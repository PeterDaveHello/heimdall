[
  {
    "id": 40540952,
    "title": "Hand-Drawn UI Components for Wireframes and Mockups with Wired Elements",
    "originLink": "https://wiredjs.com/",
    "originBody": "Wired Elements A set of common UI elements with a hand-drawn, sketchy look. These can be used for wireframes, mockups, or just the fun hand-drawn look. The elements are drawn with enough randomness that no two renderings will be exactly the same — just like two separate hand-drawn shapes. COMPONENT SHOWCASE VIEW ON GITHUB SPONSOR THIS PROJECT Here's a simple demo: Using wired-element controls to change a sketchy circle drawn using rough.js. Try It Now Play with wired-elements live in Sandbox: Wired Elements Vanilla Try it with a framework: Wired Elements in Vue Wired Elements in Svelte Wired Elements in React (using React wrapper components) Documentation Instructions on how to use wired-elements can be found on wired-elements GitHub page. API of specific elements can be found on their respective page in the Docs. Here's the list. Components Component showcase of all the wired elements. Credits wired-elements was built using RoughJS and Lit. Support this project Support development of this project by sponsoring on Open Collective or Github. License MIT License (c) Preet Shihn.",
    "commentLink": "https://news.ycombinator.com/item?id=40540952",
    "commentBody": "UI elements with a hand-drawn, sketchy look (wiredjs.com)630 points by nickca 20 hours agohidepastfavorite99 comments hgs3 15 hours agoThis looks nice and oddly nostalgic. One bit of feedback: each time I toggled the checkbox I expected the checkmark to look slightly different, as if it were being redrawn by hand each time. Adding some \"noise\" might be a neat feature for a version 2.0. reply duegreg 55 minutes agoparentAdding a noise would result differenet look on every render, I am not sure that's good reply SpaceNoodled 31 minutes agorootparentIt already does that, though. reply kissgyorgy 13 hours agoprevMy favorite tool for this is Balsamiq Wireframes: https://balsamiq.com/wireframes/ Having to write code loses the point of quick and dirty. reply riffraff 12 hours agoparentI think something like this can be useful to show your customers that something is a functional prototype and not a finished product. This works if you can switch out The design as in the old \"napkin look&feel\" for Swing.[0] Sadly I think \"change design by switching out a theme\" has long died as an idea in the web space. [0] https://napkinlaf.sourceforge.net/ reply pjmlp 7 hours agorootparentExactly, I have been in too many meetings when they get shown a click dummy in the technology stack, we get the reaction, a couple of days more and it is done. reply krsdcbl 10 hours agorootparentprevI'm asking myself if this might be a good application for a WebGL shader? The issue with \"theming to look sketched\" is the same as with \"hand drawn looking components\" - it has to be integrated for your specific DOM or App, totally defying the purpose of \"making someone understand this is a quick sketch\". Because by now, it's far beyond being a sketch. reply riffraff 12 minutes agorootparentbut that's the point, it's not a sketch, it's a prototype. Consider building a calendar app. You build part of the thing, but you have not handled error conditions and edge cases, the back-end supports only a single user and you don't have authentication working. It's enough to validate some of the flows and how UX actually works, but if you show it to a customer with the actual proper designs they will subconsciously asume that the work is finished. If the look and feel is \"this is not real\" it sets their mind in a different configuration. reply juliushuijnk 12 hours agoparentprevFor wireframing on your phone, with hand drawn style: https://www.tinyux.app It has a non-standard ux, since I didn't want drag-drop on a small touch screen. reply grodriguez100 9 hours agorootparentLooks great! No iOS version? reply pjmlp 7 hours agorootparentprevGreat, thanks for sharing. reply xixixao 12 hours agorootparentprevWow! Awesome! reply jnsie 3 hours agoparentprevAbsolutely love Balsamiq. So quick and easy. IIRC there used to be a post-processing tool by a 3rd party that would turn Balsamiq Mockups into code - wonder if it's still kicking around. Edit: it was called napkee. looks like it was open sourced but hasn't been touched in almost a decade reply McDyver 12 hours agoparentprevI really like https://wireframesketcher.com/ No cloud dependencies, and a lifetime license for $99. These are becoming rarer and rarer reply mthoms 1 hour agorootparentSadly, the \"lifetime license\" only includes one year of updates. reply Angostura 11 hours agoparentprevLove Balsamiq. The style of wireframe is so useful for conveying to people that this is just a sketch and to avoid the ‘I’m not sure about the font’ questions reply elevatedastalt 3 hours agoprevI like it for the aesthetics. It's cute. I don't care whether it's more useful for getting feedback or whatever. reply literallycancer 1 hour agoparentIf you just want the visuals and don't need the JS parts, you can use PaperCSS. reply elevatedastalt 1 hour agorootparentThanks for the tip, just checked it out. I prefer the aesthetics of the current one better. Paper CSS looks like a kid drew those elements. This one has the hand-drawn look of a more professional hand. reply aryeshalev 54 minutes agoprevhttps://jdittrich.github.io/quickMockup/ this is another GEM from the past, super easy and you can load/save as HTML file. i like the simple design for mockup reply beryilma 5 hours agoprevThis is UI theory gone wrong. The idea that users will give feedback more freely when something looks unfinished is at best a non-validated theory. This used to work with actual paper prototypes. But going the extra length to make UI designs look hand-drawn, when the users know full well they are computer generated is just busy work. Showing the users what the actual design might look like works better and it actually captures important design choices such as shapes, colors, proximity, etc. that users care about. I have been to many UX tests. Regardless of sketchy design look or not, the user will either have no problem providing feedback or they won't give useful feedback. It mostly depends on the person, not on what you show them. reply shalmanese 4 hours agoparentI've found the opposite. When users feel like a lot of work has gone into something, they'll restrict their critiques to minor elements that seem easy to change because they are worried about causing you too much rework. Putting it in a sketch format makes it seem like it was easy to generate and gives them much more freedom to make more radical suggestions. The key though is you also have to present them at least 2 - 3 very radically different versions of the same thing. Each tool has a different purpose at different parts of the design stage. When I'm in blue sky ideation phase, I don't want users to be thinking about colors and proximity (although I do, even at that stage, care deeply about information hierarchy and hate sketch tools that make information heirarchy hard to design for). It's when I'm more in the narrowing down stage that I'll move to more higher fidelity tools. reply Oarch 5 hours agoparentprevThank you for this. I couldn't agree more about this being busy work. Which there seems to be a lot of in the PM world. Personally I've found basic shapes and text boxes, or felt markers, infinitely more helpful at getting direct feedback from users. reply SteveDR 3 hours agoparentprevWho is claiming that this style improves user feedback? From the OP, “these can be used for wireframes, mockups, or just the fun hand-drawn look.” I like the style for its aesthetics alone reply wouldbecouldbe 2 hours agoparentprevIt's great for showing work to clients, not users. I've always gotten so much complaints when wireframes look too much as design, they will start making all kinds of comments about how it looks instead of focusing on structure. I used balsamiq wireframes tool a lot for that reason. reply ghnws 1 hour agoparentprevWhere are you reading this? reply DontchaKnowit 1 hour agoparentprevCounterpoint, it looks good. reply samatman 3 hours agoparentprevYou're imputing a huge amount of intention found nowhere in the link. > A set of common UI elements with a hand-drawn, sketchy look. These can be used for wireframes, mockups, or just the fun hand-drawn look. You made up all the stuff about users and feedback. It isn't on the page and it isn't in the README. reply SoftTalker 17 hours agoprevWell we used to use \"sketchy\" looking mockups so that users would understand that this wasn't a finished, working interface. I guess that's out the window now. reply pech0rin 15 hours agoparentthats the entire point of this. to generate mockup software reply samuria 14 hours agorootparentWouldn't it be easier to just decide on a UI library to be used for the project and use that instead? reply epidemian 14 hours agorootparentBut that would not convey to the user the idea of the interface being a work-in-progress. reply mirekrusin 11 hours agorootparentYou can stick “under construction” animated gif. reply worksonmine 14 hours agorootparentprevDesign can distract from the UX and there's a use for very minimal mocks. reply cl3misch 13 hours agoprevI love the sketchy UI elements. I hate the sketchy text font. Do normal sans-serif fonts and sketchy UI not fit together stylistically? reply tyingq 6 hours agoparentA lot (not all) of the things in their showcase are using a normal font, so you can have a look: https://wiredjs.com/showcase reply zogomoox 7 hours agoparentprevThe cursive font should at least lean to the right. reply lloeki 10 hours agoparentprevI liked the font myself, but as with every such \"handwriting\" font it sits in an uncanny valley of \"sketchy but regular\": a given glyph, however sketchy in isolation, always looks exactly the same! reply bdcravens 16 hours agoprevReminds me of Balsamiq, which was the rage about 10 years ago. reply ceejayoz 16 hours agoparentI still find Balsamiq super useful. reply neeleshs 14 hours agoparentprevIt's still my go-to tool for mockups. I find it to be the most powerful tool to convey product behavior to stakeholders reply fuzztester 16 hours agoparentprevYes, was about to say the same, then saw your comment. IIRC, Peldi, Balsamiq founder, had blogged about it at the time. I like it too. Also reminds me vaguely of the Comic Sans font, which was the rage even earlier. reply majikandy 14 hours agorootparentPerhaps more like Tekton, which was like a classier crisper comic sans reply TheBlight 16 hours agoparentprevWasn't there an X window manager that you could skin in the same way? reply jzemeocala 14 hours agorootparentCompiz? reply kissgyorgy 13 hours agoparentprevI use it almost every day. reply WillAdams 3 hours agoprevThere was a charming theme for Mac OS which looked something along these lines (and there was another which collapsed elements down to the bare minimum of space necessary which was a big help, and wish had been preserved, or re-created on some other platform --- in particular, being able to arrange a line of window-shaded folders and to double-click on any one to make it visible was a god-send on small displays) --- it would be nice if theming were simpler to do/implement. reply bryanrasmussen 14 hours agoprevI personally like the look of this kind of thing enough I would want to use it for a real product, or maybe for the beta version of the product and then switch it up when official release. Probably too much extra work though. reply mortenjorck 12 hours agoparentEarly in my design career, I ran a user test of a lo-fi prototype with some members of a customer success team, the visual presentation intended to convey the mutability of what we were presenting. I got a comment that “the sketch look was really nice” and since then have never relied on that as a signifier of where a concept was in the design process. reply shagie 13 hours agoparentprevThere's good reason to use it for a beta build (or an alpha) - it changes the type of feedback you get. While my google-fu is failing me as I write this, there was a blog post I read long ago about a making things look not done (I want to say it was in the context of an AWT look and feel). Related: xkcd-style Plots https://mathematica.stackexchange.com/questions/11350/xkcd-s... ( https://news.ycombinator.com/item?id=4597977 ) and https://www.chrisstucchio.com/blog/2014/why_xkcd_style_graph... If the product looks done then the feedback you will get assumes that it is done. I made the mistake of using a nice looking header (it looked better than the existing ones that were used which were stretched gifs and I had a CSS gradient) while working on the innards of some JavaScript. While I was trying to get feedback on the \"is this workflow the right sequence of pages? Are these the UI elements that need to be on the page for this functionality? For that matter, am I missing some functionality?\" .... and I got feedback about the color blue and if it should go from dark to light or light to dark in the gradient. ... And with some digging I remembered the look and feel - https://napkinlaf.sourceforge.net Which has the links to the blog post: Don't make the Demo look Done - https://headrush.typepad.com/creating_passionate_users/2006/... reply diegof79 3 hours agorootparentCheck out Bill Buxton's work, he wrote a lot about this: https://www.billbuxton.com/ reply endofreach 2 hours agoprevFinally. I have occasionally been looking for something similar & it was not important enough to do it myself. Now i have a good basis to start from. Thank you! reply vanviegen 9 hours agoprevI've drawn my inspiration from this project while creating WireText, a VSCode plugin for quickly creating mockups with a hand-drawn style, https://gitlab.com/saxion.nl/42/wiretext-code/-/blob/main/RE... reply brimstedt 5 hours agoprevNice work It would be neat if there was an option to make input and textarea use the script font as well and an image component that made a sketch of the image provided. And maybe charts, and... reply ayhanfuat 18 hours agoprevrough.js is great. It is also powering excalidraw. reply elric 10 hours agoparentI never really used Excalidraw until I discovered the Obsidian plugin. Now I have tons of markdown notes with embedded Excalidraw doodles. It's good stuff. reply majikandy 14 hours agoparentprevAh that makes sense now. I nearly said it was like excalidraw, which is really nice for architecture diagrams more than for ui/ux I find. reply j45 18 hours agoparentprevIt's also important to appreciate and congratulate someone who started and finished something they wanted to complete and release. reply ayhanfuat 17 hours agorootparentTrue. I am not sure how that relates to my comment though. Did I seem unappreciative somehow? reply kvmet 17 hours agorootparentI don't think they were saying you were unappreciative. They were just adding on that finishing a project is worth celebrating. reply throwaway290 10 hours agorootparentprevIt was finished and congratulated 6 years ago: https://news.ycombinator.com/item?id=17146451 The same author also made rough.js. So it looks like the commenter you replied to has more context than you. reply Terr_ 12 hours agoprevI wish more designers used these, precisely because it blocks them from prematurely adding too much detail. When they start doing things like exact-padding and font-size and whatnot, it either (A) front-loads implementation into the development process when the UX hasn't really been tested or at least (B) making developers spend time guessing how precisely the beta needs to look like the design. reply flemhans 3 hours agoprevThe progress bars should really be filled out like if a person was doing it, rather than re-rendering the whole contents reply lproven 9 hours agoprevReminds me of the Paper theme for MacOS 8.x-9.x. There's a screenshot in the carousel here: https://www.macintoshrepository.org/2549-appearance-manager-... Implemented using Appearance Manager, not via Kaleidoscope or one of the other add-ons. https://en.wikipedia.org/wiki/Appearance_Manager reply nonesuchluck 16 hours agoprevMaybe someone here can help me remember. I had a PalmOS app that I loved, back in the day, and I can't remember what it was called. It was a shareware clock app, with hand-drawn time that animated from one numeral to the next. I used to use it as an alarm clock, in my Sony Clie dock, by my bed. Would love to see it again. reply r1chard007 15 hours agoparentChatGPT thinks it was called 'TimeDraw' :) reply chr15m 10 hours agoprevYou might also like DoodleCSS! https://chr15m.github.io/DoodleCSS/ reply pg5 6 hours agoprevGlad to have found this! I've been looking for something like this because I'm working on a fun, quick side project and don't want to use something boring like bootstrap for it. reply danielovichdk 5 hours agoprevI would like to have something like this for \"text on stripped paper\". Anyone know if there is a library for that ? reply bkazez 9 hours agoprevPencil and paper also has a hand-drawn, sketchy look, and you don’t have to write code! reply sandGorgon 58 minutes agoprevcan i use this in htmx ? reply airstrike 18 hours agoprevPreviously https://news.ycombinator.com/item?id=17146451 (650 points, 121 comments) reply tunnuz 9 hours agoprevI use Excalidraw for sketching with a similar outcome. reply hodanli 13 hours agoprevLast commit is three years ago reply amsterdorn 4 hours agoparentHN is like Reddit, people will repost old things that aren't theirs until the end of time. reply Dalewyn 17 hours agoprevI find the deliberate choice of a left-handed font for this example intriguing. reply hammock 17 hours agoparentWas it deliberate? reply Dalewyn 7 hours agorootparentFrom the source code: > >@font-face { font-family: 'Gloria Hallelujah'; ... src: local('Gloria Hallelujah'), local('GloriaHallelujah'), url(fonts/font.woff2) format('woff2'); ... } >body { ... font-family: 'Gloria Hallelujah', sans-serif; ... } So yes, the choice of font looks to be quite deliberate. reply hammock 5 hours agorootparentDo we know they know it’s left handed, that was more my (our?) question reply Lorin 14 hours agoprevDoesn't seem super accessible tho', tab focus leaves me wanting. reply seanmcdirmid 17 hours agoprevThis brings back memories of Swing/JFC, back when it was thought to be a good thing to have pluggable look and feels. reply TeaVMFan 15 hours agoparentNapkinLAF provided a hand-drawn look for Swing: https://napkinlaf.sourceforge.net/#Snapshots reply seattle_spring 18 hours agoprevReally nice! Not sure what I'd ever use this for, but definitely putting it in my proverbial back pocket. reply xyzzy_plugh 16 hours agoprevEvery time I go to reach for rough.js I get a bit uncomfortable when I read this: > Some of the core algorithms were adapted from handy processing lib. handy is LGPLv3, but rough.js is MIT. This line makes me think parts of rough.js are actually LGPLv3, as those parts are derivative works. In practice this is unlikely to negatively impact me but I'm not super pleased by the uncertainty. reply jacobolus 15 hours agoparentAlgorithms can't be copyrighted (though code can be), so this really depends on what level of copying is implied by the word \"adapted\". reply xyzzy_plugh 15 hours agorootparent\"I read this code and then reimplemented it from scratch in a different language\" does not release you from licensing obligations. If that were the case then brb while I go reimplement unreal engine in rust. reply PaulDavisThe1st 4 hours agorootparentYou absolutely can do that. And yes, it does release you from licensing obligations (unless the license included specific clauses). reply xyzzy_plugh 43 minutes agorootparentYou should consult a lawyer. It's one thing if it's a clean-room implementation but every IP attorney I have ever interacted with would disagree with you. It doesn't take much for a court to consider something a derivative work. reply greenavocado 17 hours agoprevBring back skeuomorphism. I hate flat and material design. Those two excessively eliminate subconscious visual cues from the natural world. reply bmitc 17 hours agoparentSkeuomorphism goes in the other extreme direction though. One doesn't need flat design or skeuomorphism. There's a nice balance of UI elements that sit in the middle. I think Windows 7 was probably a good example of just straight up boring buttons and controls. reply skydhash 17 hours agorootparentOS X Maverics and iOS 6 were also nice examples of skeuomophism done right. reply bmitc 17 hours agorootparentI personally dislike Apple's skeuomorphic designs and find them very distracting. I in general do not like skeuomorphism in nearly any application. reply wkat4242 8 hours agoparentprevI hate it too. I understand what people had against it though because Apple (led by Scott Forstall) took it wayyyy too far at one point. The notepad, the game center, it was a joke. It became its own objective. I think the current bland flatness is a kneejerk reaction to that (kicked off by Jony Ive who was probably angered by Forstall's design). reply justinclift 15 hours agoprevFirefox's \"Enhanced Tracking Protection\" seems to break their React demo. Turning that off though and things seem to load ok, even with UBlock Origin and Privacy Badger running. reply jan_Sate 15 hours agoparentStrange. It works here with \"Enhanced Tracking Protection\". Maybe I'm using a different version of Firefox? reply justinclift 13 hours agorootparentI'm using FF 126.0.1 on Debian Linux x86_64, if that helps. :) reply the_gipsy 16 hours agoprev [–] Completely broken on mobile. reply HPsquared 16 hours agoparentWorks for me on Chrome Android reply throwaway_ab 14 hours agoparentprev [–] Works for me on iPhone. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Wired Elements offers UI components with a hand-drawn, sketchy look, perfect for wireframes, mockups, or a playful design.",
      "Compatible with multiple frameworks including Vanilla, Vue, Svelte, and React, and built using RoughJS and Lit.",
      "Open-source under the MIT License, with detailed documentation available on GitHub, and support options via Open Collective or GitHub."
    ],
    "commentSummary": [
      "A Hacker News discussion examines a UI library from wiredjs.com that features hand-drawn, sketchy elements, sparking nostalgia and debate on its practicality in modern web development.",
      "Users compare it to tools like Balsamiq Wireframes for quick prototyping and discuss the importance of visual presentation in user testing, mentioning tools like TinyUX, WireframeSketcher, QuickMockup, and Excalidraw.",
      "The conversation also touches on design trends such as skeuomorphism versus flat design, licensing concerns, and the impact of visual completeness on feedback focus."
    ],
    "points": 630,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1717194188
  },
  {
    "id": 40540703,
    "title": "Windows 11's Copilot+ Recall Sparks Privacy and Security Concerns",
    "originLink": "https://doublepulsar.com/recall-stealing-everything-youve-ever-typed-or-viewed-on-your-own-windows-pc-is-now-possible-da3e12e9465e",
    "originBody": "Stealing everything you’ve ever typed or viewed on your own Windows PC is now possible with two lines of code — inside the Copilot+ Recall disaster. Kevin Beaumont · Follow Published in DoublePulsar · 9 min read · 1 day ago -- 5 I wrote a piece recently about Copilot+ Recall, a new Microsoft Windows 11 feature which — in the words of Microsoft CEO Satya Nadella- takes “screenshots” of your PC constantly, and makes it into an instantly searchable database of everything you’ve ever seen. As he says, it is photographic memory of your PC life. I got ahold of the Copilot+ software and got it working on a system without an NPU about a week ago, and I’ve been exploring how this works in practice, so we’ll have a look into it that shortly. First, I want to look at how this feature was received as I think it is important to understand the context. The overwhelmingly negative reaction has probably taken Microsoft leadership by surprise. For almost everybody else, it won’t have. This was like watching Microsoft become an Apple Mac marketing department. At a surface level, it is great if you are a manager at a company with too much to do and too little time as you can instantly search what you were doing about a subject a month ago. In practice, that audience’s needs are a very small (tiny, in fact) portion of Windows userbase — and frankly talking about screenshotting the things people in the real world, not executive world, is basically like punching customers in the face. The echo chamber effect inside Microsoft is real here, and oh boy… just oh boy. It’s a rare misfire, I think. I think it’s an interesting entirely, really optional feature with a niche initial user base that would require incredibly careful communication, cybersecurity, engineering and implementation. Copilot+ Recall doesn’t have these. The work hasn’t been done properly to package it together, clearly. A lot of Windows users just want their PCs so they can play games, watch porn, and live their lives as human beings who make mistakes.. that they don’t always want to remember, and the idea other people with access to the device could see a photographic memory is.. very scary to a great many people on a deeply personal level. Windows is a personal experience. This shatters that belief. I think they are probably going to set fire to the entire Copilot brand due to how poorly this has been implemented and rolled out. It’s an act of self harm at Microsoft in the name of AI, and by proxy real customer harm. More importantly, at the time I pointed out this fundamentally breaks the promise of security in Windows. I’d now like to detail why. Strap in — this is crazy. I’m going to structure this as a Q&A with myself now, based on comments online, as it’s really interesting seeing how some people handwave the issues away. Q. The data is processed entirely locally on your laptop, right? A. Yes! They made some smart decisions here, there’s a whole subsystem of Azure AI etc code that process on the edge. Q. Cool, so hackers and malware can’t access it, right? A. No, they can. Q. But it’s encrypted. A. When you’re logged into a PC and run software, things are decrypted for you. Encryption at rest only helps if somebody comes to your house and physically steals your laptop — that isn’t what criminal hackers do. For example, InfoStealer trojans, which automatically steal usernames and passwords, are a major problem for well over a decade — now these can just be easily modified to support Recall. Q. But the BBC said data cannot be accessed remotely by hackers. A. They were quoting Microsoft, but this is wrong. Data can be accessed remotely. This is what the journalist was told for some reason: Q. Microsoft say only that user can access the data. A. This isn’t true, I can demonstrate another user account on the same device accessing the database. Q. So how does it work? A. Every few seconds, screenshots are taken. These are automatically OCR’d by Azure AI, running on your device, and written into an SQLite database in the user’s folder. This database file has a record of everything you’ve ever viewed on your PC in plain text. OCR is a process of looking an image, and extracting the letters. Q. What does the database look like? A: Q. How do you obtain the database files? A. They’re just files in AppData, in the new CoreAIPlatform folder. Q. But it’s highly encrypted and nobody can access them, right?! A. Here’s a few second video of two Microsoft engineers accessing the folder: Q. …But, normal users don’t run as admins! A. According to Microsoft’s own website, in their Recall rollout page, they do: In fact, you don’t even need to be an admin to read the database — more on that in a later blog. Q. But a UAC prompt appeared in that video, that’s a security boundary. A. According to Microsoft’s own website (and MSRC), UAC is not a security boundary: Q. So… where is the security here? A. They have tried to do a bunch of things but none of it actually works properly in the real world due to gaps you can drive a plane through. Q. Does it automatically not screenshot and OCR things like financial information? A. No: Q. How large is the database? A. It compresses well, several days working is around ~90kb. You can exfiltrate several months of documents and key presses in the space of a few seconds with an average broadband connection. Q. How fast is search? On device, really fast. Q. Have you exfiltrated your own Recall database? A. Yes. I have automated exfiltration, and made a website where you can upload a database and instantly search it. I am deliberately holding back technical details until Microsoft ship the feature as I want to give them time to do something. I actually have a whole bunch of things to show and think the wider cyber community will have so much fun with this when generally available.. but I also think that’s really sad, as real world harm will ensue. Q. What kind of things are in the database? A. Everything a user has ever seen, ordered by application. Every bit of text the user has seen, with some minor exceptions (e.g. Microsoft Edge InPrivate mode is excluded, but Google Chrome isn’t). Every user interaction, e.g. minimizing a window. There is an API for user activity, and third party apps can plug in to enrich data and also view store data. It also stores all websites you visit, even if third party. Q. If I delete an email/WhatsApp/Signal/Teams message, is it deleted from Recall? A. No, it stays in the database indefinitely. Q. Are auto deleting messages in messaging apps removed from Recall? A. No, they’re scraped by Recall and available. Q. But if a hacker gains access to run code on your PC, it’s already game over! A. If you run something like an info stealer, at present they will automatically scrape things like credential stores. At scale, hackers scrape rather than touch every victim (because there are so many) and resell them in online marketplaces. Recall enables threat actors to automate scraping everything you’ve ever looked at within seconds. During testing this with an off the shelf infostealer, I used Microsoft Defender for Endpoint — which detected the off the shelve infostealer — but by the time the automated remediation kicked in (which took over ten minutes) my Recall data was already long gone. Q. Does this enable mass data breaches of website? A. Yes. The next time you see a major data breach where customer data is clearly visible in the breach, you’re going to presume company who processes the data are at fault, right? But if people have used a Windows device with Recall to access the service/app/whatever, hackers can see everything and assemble data dumps without the company who runs the service even being aware. The data is already consistently structured in the Recall database for attackers. So prepare for AI powered super breaches. Currently credential marketplaces exist where you can buy stolen passwords — soon, you will be able to buy stolen customer data from insurance companies etc as the entire code to do this has been preinstalled and enabled on Windows by Microsoft. Q. Did Microsoft mislead the BBC about the security of Copilot? A. Yes. Q. Have Microsoft mislead customers about the security of Copilot? A. Yes. For example, they describe it as an optional experience — but it is enabled by default and people can optionally disable it. That’s wordsmithing. Microsoft’s CEO referred to “screenshots” in an interview about the product, but the product itself only refers to “snapshots” — a snapshot is actually a screenshot. It’s again wordsmithing for whatever reason. Microsoft just need to be super clear about what this is, so customers can make an informed choice. Q. Recall only applies to 1 hardware device! A. That isn’t true. There are currently 10 Copilot+ devices available to order right now from every major manufacturer: https://www.microsoft.com/en-gb/windows/copilot-plus-pcs#shop Additionally, Microsoft’s website say they are working on support for AMD and Intel chipsets. Recall is coming to Windows 11. Q. How do I disable Recall? A. In initial device setup for compatible Copilot+ devices out of the box, you have to click through options to disable Recall. In enterprise, you have to turn off Recall as it is enabled by default: WindowsAI Policy CSP - Windows Client Management Learn more about the WindowsAI Area in Policy CSP. learn.microsoft.com The Group Policy object for this has apparently been renamed (the MS documentation is incorrect): Q. What are the privacy implications? Isn’t this against GDPR? A. I am not a privacy person or a legal person. I will say that privacy people I’ve talked to are extremely worried about the impacts on households in domestic abuse situations and such. Obviously, from a corporate point of view organisations should absolutely consider the risk of processing customer data like this — Microsoft won’t be held responsible as the data processor, as it is done at the edge on your devices — you are responsible here. Q. Are Microsoft a big, evil company? A. No, that’s insanely reductive. They’re super smart people, and sometimes super smart people make mistakes. What matters is what they do with knowledge of mistakes. Q. Aren’t you the former employee who hates Microsoft? A. No. I just wrote a blog this month praising them: Breaking down Microsoft’s pivot to placing cybersecurity as a top priority My thoughts on Microsoft’s last chance saloon moment on security. doublepulsar.com Q. Is this really as harmful as you think? A. Go to your parents house, your grandparents house etc and look at their Windows PC, look at the installed software in the past year, and try to use the device. Run some antivirus scans. There’s no way this implementation doesn’t end in tears — there’s a reason there’s a trillion dollar security industry, and that most problems revolve around malware and endpoints. Q. What should Microsoft do? A. In my opinion — they should recall Recall and rework it to be the feature it deserves to be, delivered at a later date. They also need to review the internal decision making that led to this situation, as this kind of thing should not happen. Earlier this month, Microsoft’s CEO emailed all their staff saying “If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security.” We will find out if he was serious about that email. They need to eat some humble pie and just take the hit now, or risk customer trust in their Copilot and security brands. Frankly, few if any customers are going to cry about Recall not being immediate available — but they are absolutely going to be seriously concerned if Microsoft’s reaction is to do nothing, ship the product, slightly tinker or try to wordsmith around the problem in the media.",
    "commentLink": "https://news.ycombinator.com/item?id=40540703",
    "commentBody": "Recall: Stealing everything you've ever typed or viewed on your own Windows PC (doublepulsar.com)313 points by todsacerdoti 21 hours agohidepastfavorite284 comments Grimeton 1 hour agoWhat a lot of people are missing in all this is that Microsoft is coming from one, dialing it up to eleven to then dial it back to 9. By pushing this onto people in a hard way they open the door to come up with a mitigating solution that later is far beyond what we had before recall but not as bad as what they pushed onto people in the first place. So they will reach their goal, as it was never 11, it was always 9. reply walterbell 19 hours agoprevRelated threads: \"Giving Windows total recall of everything a user does is a privacy minefield\", 41 comments, https://news.ycombinator.com/item?id=40470806 \"AI PCs are the final nail in the coffin of open computing\", 60 comments, https://news.ycombinator.com/item?id=40436975 \"How the new Microsoft Recall feature fundamentally undermines Windows security\", 50 comments, https://news.ycombinator.com/item?id=40433884 \"Windows Recall sounds like a privacy nightmare\", 298 comments, https://news.ycombinator.com/item?id=40443682 reply fabiensanglard 9 hours agoprevFor a moment my perception of Microsoft was that it has become a cool company (github acquisition, Linux Sub-sytem). I was impressed by how Satya Nadella turned the company around (at least in terms of image). Between Recall and the mandatory account login to install Windows 10 I am progressively reverting to how I felt about them. reply Xeamek 8 hours agoparentI think 'parts of' microsoft really did become cool. In developers world, pushing open source, or in gaming - xbox game pass is just best value that there is. But for every cool department, there also exists one that still behaves as a predatory corpo. And it seems to me like their main target are 'normie' users. reply SoftTalker 38 minutes agorootparentMicrosoft is ultimately a for-profit publicly-owned company, and are motivated to increase shareholder value. Being perceived as cool by the open-source community might play into that in some ways, but ultimately there isn't any revenue there. reply WhyNotHugo 8 hours agoparentprev> For a moment my perception of Microsoft was that it has become a cool company I think that they've been trying very hard to give out that image without really changing who they are. It's not the first time that they try hard to look like they've changed, and it won't be the last. reply fifteen1506 8 hours agorootparentYes. Marketing. In the end, they are working for decision makers who can spend money. Typical IT guy (not head) cannot spend money without 3 approvals from different departments. reply lukan 8 hours agoparentprevTracking and ads integrated by default into the OS, who cares. But they did something with open source, they must have become nice! And the linux subsystem, well, for me a textbook example of: https://en.m.wikipedia.org/wiki/Embrace,_extend,_and_extingu... So sorry, they were never cool to me. I still use them, but if I must switch to win 11 soon, I might take that as an opportunity to finally cut loose my last dependencies with windows. reply strictnein 9 minutes agorootparent> Tracking and ads integrated by default into the OS, who cares. But they did something with open source, they must have become nice! > Be kind. Don't be snarky. Converse curiously; don't cross-examine. Edit out swipes. https://news.ycombinator.com/newsguidelines.html reply throwup238 1 hour agorootparentprevMore than 60% of Azure is run on Linux [1] and the writing was already on the wall when the Linux subsystem was conceived so I don't think it's a good example of EEE. [1] https://azure.microsoft.com/en-us/products/virtual-machines/... reply nix0n 44 minutes agorootparent> I don't think it's a good example of EEE Not yet, they're still on the \"embrace\" step. reply temac 34 minutes agorootparentWith WSL they are absolutely at the \"extend\" step. reply kenjackson 23 minutes agoparentprevWhy is Recall something that would revert your opinion? It’s a feature that provides value to the customer first and foremost. reply temp3000 8 hours agoparentprevThis is another nail in the coffin for me, as a life long Windows user. My next install is Linux single boot. reply WhackyIdeas 8 hours agorootparentI said on HN just a few days ago: “The only good thing about Recall is that it has been the definitive decider of moving away from Microsoft permanently because for them to create such a ‘feature’ shows a complete lack of care about people’s private data - they’ll be leaving a huge jackpot prize for anyone who breaks into a system.” And only 4 days later, it is shown. reply richliss 7 hours agorootparentprevI've just installed and configured KDE Neon Plasma 6 and I'm really liking it a lot. Feels closer to Windows than my other Ubuntu and Xubuntu installs. Just my 2p reply Halfwhit 2 minutes agorootparentFound a fellow Brit reply sleepybrett 9 minutes agorootparentprevthe coffin is nothing but nails at this point. reply 29athrowaway 8 hours agoparentprevGitHub became the data source for OpenAI Codex and GitHub Copilot not long after it was acquired. The #1 merit of Nadella is pushing for increased accessibility. reply WWLink 8 hours agoparentprevlol they never really sold me on it. WSL1 with its nano processes is neat, but Teams is a disgusting mess and MS has been rather consistent with treating their users like cattle. reply oefrha 18 hours agoprev> Everything a user has ever seen, ordered by application. Every bit of text the user has seen, with some minor exceptions (e.g. Microsoft Edge InPrivate mode is excluded, but Google Chrome isn’t). Microsoft was the company that used to add bespoke code to Windows to maintain compatibility with old third party software, patch explorer.exe to stop third party customizations from crashing it, etc. While the whole Recall thing is a pretty bad idea for most users, the lack of care extended to the third party browser with overwhelming market share among their users is just sad. Are they counting on people switching to Edge because “PSA: you’ll be recorded if you watch pr0n in Chrome!”? reply ajross 18 hours agoparentIt was surely just schedule pressure. There's no system-visible API for \"incognito mode\" (nor should there be, obviously, as it would defeat the purpose) so they just skipped it. I dunno, and I say this as someone who works for a competitor and has no love for MS... a lot of the responses here seem really uncharitable. This isn't bad faith, it's just a rushed product with some poor planning. If Apple had rolled this same feature out with glitz and a giant slideshow about privacy and explained how everything was encrypted and never left the device, we'd all be crowing about how great it is even if it too was screenshotting incognito windows. reply oefrha 18 hours agorootparentI thought about it for a minute, and sure, given a chrome.exe HWND, I can't think of a way to tell if it's Incognito.[1] But companies work with important vendors on major features all the time. If they think it's important enough to exclude private windows in Edge, they surely could have worked with Google to figure out something. > nor should there be, obviously, as it would defeat the purpose No it doesn't. Incognito mode is about leaving no trace on disk, that's all. Recall is the one defeating that purpose right now, if TFA is accurate. [1] Not saying it's impossible. Only that I can't think of a straightforward solution in a pinch with my limited Windows experience and hacking skills. reply twinge 18 hours agorootparentprevIt seems contrary to their docs[0]: > Recall won’t save any content from your private browsing activity when you’re using Microsoft Edge, Firefox, Opera, Google Chrome, or other Chromium-based browsers. Without a system-level incognito mode feature I could see apps allowing users to denote their windows as DRM content to avoid Recall. 0: https://support.microsoft.com/en-us/windows/privacy-and-cont... reply oefrha 18 hours agorootparent> apps allowing users to denote their windows as DRM content to avoid Recall That would prevent user-initiated screen captures as well. Not a good idea for browsers at least. reply klabb3 16 hours agorootparentprev> a lot of the responses here seem really uncharitable. This isn't bad faith, it's just a rushed product with some poor planning. But it’s not the product people are criticizing, at all. Similar tools have existed for a long time and have not raised eyebrows except when it’s been forced by an employer or a school. It’s that it’s the OS putting an always on and enabled-by-default spyware on devices that are frequently shared by family members, when their average users who barely know what a web browser is and will just accept recommended defaults. Speaking of which, the whole spiel about Edge/IE is precisely their aggressive defaults. It’s the same here. If you’re a startup building custom tools you can talk about rushed products and assume good intent. This software is built by a software company with some of the worlds best software engineers all the way up to the top. I mean, people trust them with everything from business secrets to payment details to mission critical services. This is clearly not a “rushed product oopsie”, it’s blatant disregard for privacy, and to a lesser extent, security. I’m avoiding windows like the plague, but since seeing my mom get bombarded with “recommended Microsoft defaults” over the last decade or so, I’m convinced MS is deliberately exploiting uninformed users as much as they can get away with, while leaving hidden options for power users to disable the ads and the crapware so they don’t leave. This total recall debacle is probably a similar attempt at using their unknowing user base to train their new AI models, or similar. If it was a genuinely useful product it would not be enabled by default. reply ajross 4 hours agorootparent> It’s that it’s the OS putting an always on and enabled-by-default spyware on devices that are frequently shared And I have to repeat: if Apple Computer had pushed the same product, but with a slide talking about how it was all locally encrypted and unextractable and tied to both the device and the user account, HN would be celebrating the attention to privacy even though macs too are \"frequently shared\". And the reasoning would be how strong the security engineering was around the process, because we love that stuff and we love macs. MS doesn't get the same benefit of the doubt, and it leaks into the technical content of the argument, and that's wrong. And FWIW I'm mostly just handwaving the technical details. I mean, do we know for a fact that MS is *not* encrypting this with a TPM-managed key tied to the user account? I bet they are, honestly. reply fbdab103 1 hour agorootparentAbsolutely not would I give Apple a free pass either. They can say all the nice things they want about protecting my privacy, but I do not trust any commercial entity will act in my best interest. Especially when they all have government requirements to hand over my data when a cop asks nicely. We are speed running into a neuromancer dystopia where tech companies control every facet of our lives. Why would I be ok with them making it easier to monitor my every keystroke? reply klabb3 2 hours agorootparentprev> if Apple Computer had pushed the same product, […], HN would be celebrating the attention to privacy I don’t believe so, at least not if it’s enabled-by-default. > MS doesn't get the same benefit of the doubt Apple doesn’t rely on benefit of the doubt because they are very clear about how the privacy of new products work (say Touch and Face ID), and Microsoft is not. I mean just look at this very thread, it’s super unclear how it works and interacts with other windows feature (some of which are premium) like fde/bitlocker and whether there’s telemetry/training. That obviously contributes to the “harsh” response. As it should. reply kstrauser 1 hour agorootparent> I mean just look at this very thread, it’s super unclear how it works and interacts with other windows feature I agree with you, but that’s not great evidence for your point. Bring up any random Apple feature and people will be quick to warn you about their misunderstandings of it. “Face ID means Apple has all our pictures now!” “Apple Keychain shares all your passwords with them!” Etc. reply sleepybrett 7 minutes agorootparentprevSpotlight already indexes all the text on your disk. reply cortesoft 18 hours agoparentprevHow would this system know chrome was running incognito? reply kccqzy 18 hours agorootparentDevelop an API in Windows. Contribute to the Chromium codebase to use this API. reply willcipriano 18 hours agorootparentWe already have \"Launch as Administrator\" add \"Launch in private mode\" reply btown 18 hours agorootparentprevFor one, OCR’ing the word Incognito from the screen? reply gitgud 8 hours agorootparentWhat if it’s fullscreen? What if the title bar of the window is off the screen? A better solution would be to just pause the screen capture whenever incognito is open anywhere reply Terr_ 18 hours agorootparentprevSo I just add a new custom toolbar to the bottom of the screen titled \"Incognito\" so that the text is always there, and suddenly nothing's being recorded anymore? :P On one level that's convenient, but on the other hand I'm not sure it's a very robust design. reply oefrha 18 hours agorootparentprevThat only works if you have the \"New Incognito Tab\" page active. Not when you're actually browsing something. reply chuckadams 16 hours agorootparentThere's also the profile button at the top right, just to the left of the hamburger menu, that says \"Incognito\" on it. reply oefrha 15 hours agorootparentYeah I forgot about that. reply bitwize 18 hours agoparentprevMicrosoft is still fighting the browser wars of the 90s. They're not going to give third-party browsers any quarter if they can avoid it. reply Workaccount2 18 hours agoparentprevWatching users flock to Apple's walled garden, to the point where it's a social issue to not have an iPhone, has left Microsoft (and many others) wondering why the fuck they have been so accommodating to user choices for all this time. reply leereeves 18 hours agorootparentOutside of high school, where literally everything is a reason to ostracize people who are different, where is it a social issue to not have an iPhone? reply lwhalen 18 hours agorootparentI've been married for ages (so I can't speak to this first-hand), but my single friends in their late-20s to mid-30s say that NOT having an iPhone gets them rejected fairly often. reply jay-barronville 8 minutes agorootparentI’m in that age range and I’m married. A few years ago when I was still a single man, I experienced this firsthand at least a few times. My primary device wasn’t an iPhone and I had women tell me they “don’t like texting with green bubbles”…it was pretty bizarre, to say the least. In hindsight, I’m glad those women filtered themselves out, but it’s definitely really rough out there for our society’s young men. reply ryandrake 17 hours agorootparentprevSeems to me like an effortless way for them to automatically filter out terrible, shallow partners from their lives before investing anything into a relationship. What a time-saver! reply snapplebobapple 4 hours agorootparentI think you are missing the first half the modern male reproductive lifecycle. What you are saying increasingly applies to 30's onward (when males are looking for long term family relationships) but for the 20 somethings many are looking for these shallow, vacuous women because they often can be convinced into meaningless sex. It's a pretty sad state but it appears to be this way now. reply lwhalen 17 hours agorootparentprevYou and I are in violent agreement on this, but... the younger generation seems more interested in 'smashing' these days than 'partner-seeking'. Can't say I was MUCH different, but I certainly wouldn't change the daily-driver tech I use just to peacock for a hot date. reply acheong08 8 hours agorootparentPerhaps that’s more prevalent in the US and wealthier districts? I haven’t seen any of that in the UK (everyone here seems to use WhatsApp, Instagram, and Snapchat which I similarly dislike) reply Gud 9 hours agorootparentprevNothing excites a woman more than when you bend over backwards to please her. reply thejohnconway 17 hours agorootparentprevSounds like your friends are dodging bullets without having to do anything. I say this as an iPhone user. reply ramenbytes 16 hours agorootparentprevIs there any possible steelman for this, or is it as shallow as it sounds? reply Cyphase 15 hours agorootparentPossible steelier arguments: - \"iPhone's are generally more expensive, Android phones are generally cheaper\", so having an iPhone signals financial \"goodness\". Same argument can be applied to lots of other products. - \"iPhones are generally better, so if you have Android you're compromising for some reason\", e.g. lack of money to buy one, \"weird\" political/social/other beliefs, etc. - Messaging systems are like accents; some people might prefer dating someone who speaks their language in a more similar accent, and others might prefer dating people who use the messaging system they prefer. Also, a lot depends on your definition of \"shallow\". reply operator2140 3 hours agorootparentprevmaybe women don’t want their sms/mms/phone calls being leaked unencrypted over the antiquated, legacy telephony network? reply hammyhavoc 16 hours agorootparentprevI would argue that not having an iPhone is behaving as a filter for likely incompatible pairings if not possessing a particular brand of phone is an issue to any prospective partner. reply throwaway22032 18 hours agorootparentprevIt seems to be an American thing, something to do with iMessage. Here in the UK I've literally never encountered it. reply janice1999 16 hours agorootparentOn this side of the Atlantic, not having WhatsApp installed has been far more of an issue for me. reply brookst 16 hours agorootparentprevNever seen it in the US either. It’s always these third-hand stories. reply friend_and_foe 16 hours agoparentprev> if you don't use our browser we will spy on you Good god how more obvious could it be? Anyone still using this software is a lost cause. reply jmkni 5 hours agorootparentMost people just don’t care that much about tech (they have other shit going on) and will use whatever is put in front of them. I think the tech community is responsible for keeping companies like MS in check and pushing back against literal spyware being normalised in operating systems. reply rezonant 18 hours agoparentprevThankfully it looks like they've already added support for Chrome, Firefox and a bunch of other browsers: https://support.microsoft.com/en-us/windows/retrace-your-ste... reply NewJazz 18 hours agorootparentThis is not something to be grateful for. They are normalizing spying by default. reply rezonant 14 hours agorootparentI didn't say we should be grateful about that. I'm saying that thankfully they are not using the threat of your guarded personal data being exposed because you want this feature but don't want to use Edge. reply oefrha 18 hours agorootparentprevThat's what they say in marketing materials, but TFA claims to have dumped their database and was speaking from experience dissecting that. Hard to say which should be trusted. reply wilsonnb3 18 hours agorootparentTFA is using a preview release of Windows modified to run on hardware that isn’t officially supported, I would lean towards trusting the marketing material on what will be supported at release. reply hammyhavoc 16 hours agorootparentprev\"Thankful\" for surveillance? reply rezonant 14 hours agorootparentThankful for not pushing Edge if you decide you want this feature. This feature is something the community has been independently creating for years. Personally I don't think people should actually allow this type of feature. It's too much of a risk. But my point stands on its own, that at least they aren't creating an even more perverse incentive to use Edge, which they absolutely could have done, and seems the MO of the Microsoft today who would sacrifice all else to be able to say there's 1 or 2 more Edge or Bing users. reply danpalmer 9 hours agoprevThe privacy concerns here are real and massive, and I suspect this will get worse before it gets better. However. This is the holy grail of computer usage. A good version of this could be the killer app for modern AI. Because of that, ripple are going to keep trying to make this feature happen. There’s already a popular implementation on macOS. I’m excited for the end-state, but apprehensive about getting there. reply flohofwoe 9 hours agoparentI would prefer if they get a simple search-in-local-text-files right first. That doesn't even need \"AI\", just some plain old 1970s-style coding. reply safety1st 9 hours agoparentprevI think I'll stay on Linux and let Windows users be the guinea pig for this particular experiment. reply michelsedgh 9 hours agorootparentI wonder what kinda data they will get cause most people who code and do heavy stuff I think are on mac/Linux so I wonder how good the data they gather be you know? Mostly moms and dads using their computers wrong this is what I imagine my head is the data they gather, like maybe 50-60% of it lol and heavy users with good data are nowhere to be found reply mikro2nd 8 hours agorootparent\"moms and dads using their computers wrong\" What the hell is the \"wrong\" way to use a computer? Emails, social media and doing your banking? And why attack \"moms and dads\"? Speaking as a grandfather, I find it insulting, having used and programmed computers since the 1970s. reply smeej 8 hours agorootparent> What the hell is the \"wrong\" way to use a computer? Searching for your bank's website rather than bookmarking it, and entering your credentials into the phishing site that's the top result. Installing Anydesk or something for the \"nice gentleman who called me from Microsoft to tell me my warranty had expired and he needed gift cards to pay for it.\" Those are the two most obvious ones I can think of. There's a multi-billion dollar \"industry\" separating especially older people from their money using computers. Frankly if you've even been around computers for 50+ years and haven't encountered the many and varied ridiculous ways people can use them \"wrong,\" I have to wonder whether you've ever had to deal with regular people using them in the real world at all. reply frde_me 49 minutes agorootparentEh, I'm a pretty advanced user by any mean, and I still search for my banks website a non-trivial amount of time. I have a bookmark, but it's honestly just as fast to do it that way reply matthewmacleod 8 hours agorootparentprevSearching for your bank's website rather than bookmarking it, and entering your credentials into the phishing site that's the top result. This isn’t wrong so much as a damning indictment of the tech industry’s inability to fix core issues despite having more money that god herself reply DandyDev 8 hours agorootparentprevA lot of those moms and dads are software engineers, data scientists etc. reply kmlx 8 hours agoparentprev> However. This is the holy grail of computer usage i’ve seen this often and i’m trying to understand the issue. i have never wanted to go back in history or have a comprehensive history of all my actions. the only exception is the terminal and for that ctrl+r/history is more than enough. i learn, apply and move on. what’s the use case for this recall thing? reply danpalmer 6 hours agorootparentI often have 100 tabs open so that I can find things I was looking at again. I regularly browse my browser history to try to find things I saw previously. I often remember seeing something in a chat session somewhere, but can't remember which person/channel/room it was... Being able to search everything I've seen on my screen would address all of these. I think it would fundamentally change how we interface with computers, if we could do it reliably. Silos between applications can start to break down when you have sufficient intelligence about what's on the screen too, and that's a huge opportunity. reply kmlx 6 hours agorootparentinteresting. thanks for the reply. i also used to have lots of tabs open till i realised i need to organise better. since then i keep a few tabs open at max and regularly close all my tabs. i also tend to open a tab, read what i need to and then close it. no more clutter this way. reply mmebane 1 hour agorootparentprevAs someone with ADHD, being able to have an assistant with perfect memory that I can ask extremely vague questions to about things I'm pretty sure I did some time between last week and 5 years ago sounds amazing. I'm skeptical Recall will actually be able to do that. I doubt its usefulness outweighs the legal and social concerns. But I can absolutely see the use. reply Slyfox33 9 hours agoparentprev\"Holy grail of computer usage\". Really...? reply falcor84 9 minutes agorootparentI'm somewhat with the parent on this. Getting this right will probably take a long time but can eventually bring us something like J.A.R.V.I.S, whereby an AI agent can reason about all activities you've performed on the computer in the past and give you advice or perform full tasks for you based on that. We can argue whether we want to give the AI that level of trust, but I'm very interested in the potential. reply Dalewyn 8 hours agorootparentprevOne of the most popular features ever used on every computer is the Undo button. One of the most common questions every computer user has at any given moment is some variant of \"What/Where was it?\". I agree it's a holy grail, but it's also a road paved with good intentions. reply Slyfox33 4 hours agorootparentRecall isn't an undo button. It's just screenshots lol. reply bravetraveler 8 hours agoparentprevSnake oil salesman says: \"Snakes Are Scary!\" Computers are the holy Grail of computers. Stop playing with fire. The tool is already here. reply beretguy 9 hours agoparentprevMicrosoft will probably encrypt that db or something to try to convince people it’s safe. reply plonk 9 hours agorootparentDoesn’t Windows already have a data store that’s encrypted with a key that doesn’t exist in RAM unless you’re logged on? And some kind of isolation of sensitive processes in a VM? Malware can probably read most of the user’s data in RAM, but if OS components keep getting more isolated from each other, maybe that can be secure enough. reply rzzzt 6 hours agorootparentThe Data Protection API makes this quite easy from a programming standpoint (it also makes relocating keys to another machine hard, but in this case this should count as another upside): https://en.wikipedia.org/wiki/Data_Protection_API reply cududa 9 hours agorootparentprevPlease explain to me why an OS level signed and encrypted database isn’t secure reply soraminazuki 8 hours agorootparentWhy is a padlock with the key stuck right into it secure? The encrypted data and the decryption key is on the same physical device. Sure, memory isolation techniques may serve as a deterrent with extreme care. But if Microsoft increases the attack surface by sloppily integrating that feature everywhere in Windows, the yet-to-be-implemented-if-at-all encryption is going to be ineffective. And that’s going to happen more likely than not. reply plonk 2 hours agorootparentMaybe the learning and inference can happen in a VM and the apps can only have access to a query API. (Take the equivalents if it's not all ML.) reply plonk 2 hours agorootparentAlso, Windows Store apps seem to have an identity and limited permissions, so you can probably have some kind of smartphone OS-like isolation. reply pjmlp 9 hours agorootparentprevThe same reason banks get occasionally robbed, despite all security cameras, delayed openings, biometrics, armed security, and everything else put in place. When there is a will, there is eventually a way, for anyone with enough resources. reply plonk 9 hours agorootparentprevResearchers on Twitter are saying that it’s just a plaintext SQLite. reply verdverm 8 hours agorootparentprevWhen has windows ever been a safe or secure OS environment? Seems to me, many an exploit has been installed by the user while trying to get device drivers working reply bravetraveler 7 hours agorootparentprevPeople made it, people will break it Please explain to me how anything achieved infallible nature. Consider the natural vacuum is space. reply page_fault 9 hours agorootparentprevBecause it needs to be decrypted at some point? reply anthk 8 hours agoparentprevNo. The holy grail of computing would be taking an instant snapshot as you do with emulators/vm's from anywhere, allowing you to rollback anytime, restoring the CPU and memory settings in the spot. With incremental snapshots, 'branches' and so on, switching back and forth seamlessly as you would do with save states under an emulator. And with 'no time', with a delay of less than 5-10 seconds on creating/restoring a snapshot. On search, as they stated, Recoll did it fine over 20 years. reply everdrive 9 hours agoparentprevFurther infantilization of the user, further resource requirements, enormous privacy concerns, and proprietary technology. It all seems bad to me. reply jiripospisil 9 hours agoprev2017: People are willingly buying a spy device to put into their homes. 2024: People are willingly activating a key logger. reply tetris11 8 hours agoparent2017: People are forced to swallow spy devices in order to continue to interact with their work/life communities 2024: People are forced to swallow keyloggers in order to continue to use their desktop PCs. Most don't know what they're agreeing to, because most people don't have the time to be experts in tech despite it affecting every factor of their lives, much in the same way that tech people don't have the time to be be farming and agriculture experts in their free time despite it affecting every factor of their lives. reply Dalewyn 7 hours agorootparent>Most don't know what they're agreeing to, because most people don't have the time to be experts in tech I used to think this maybe 20 years ago, but I think it's about time we shift gears to the realization that the reality is people don't care about tech privacy and security. We have to remember: The internet as most people know it (the World Wide Web) is 33 years old, personal computing is even older. The 30- and 40-years olds literally grew up with all this. The 10- and 20-years olds are living all this from the moment they were born. Even legislation like GDPR came into force. The result is still nobody cares. Lack of awareness isn't a problem anymore. Everyone knows, nobody cares.reply Beldin 5 hours agorootparentThat's a vast oversimplification. Try flipping it to anything outside your personal scope of skills and knowledge to see how wrong this position is. Example: tap water quality (assuming you live where tap water is safe to drink). Do you know how it works? What steps are being taken? Could you fix that yourself for your house if things break down? And yet, you probably care. Another example: car safety features. Could you add a crumple zone to an 80s car? A cage construction? Yet you probably care that any car you're in has those properly engineered and no part of your body will be crumpled in case of a collision. reply WhyNotHugo 8 hours agoparentprevI question the usage of the word \"willingly\". People aren't really aware of the consequences of their actions. The people of Troy didn't \"willingly\" bring in a huge wooden horse full of enemy soldiers. They \"unknowingly\" brought in a horse full of enemy soldiers. reply Shank 9 hours agoparentprevI actually think that was 2023 with Rewind. At least with Rewind you’ve gotta download the app yourself and run it. The in-progress OOBE setup for Recall does not even have an opt-out. Instead, it offers to open the settings panel after you’re done with setup, rather than just giving you an off switch: https://x.com/tomwarren/status/1796681578984182066 reply TiredOfLife 8 hours agoparentprev2600 BCE: People are willingly writing their thoughts and experiences on papyrus reply tetris11 8 hours agorootparent2600 AD: People are willingly writing their thoughts and experiences on papyrus reply 6510 7 hours agorootparentprevRight, and having your own kinda stopped being a thing right then and there. We had Ted Kaczynski I suppose but we didn't quite understand what he was on about. Crass - Where Next Columbus? (1981) https://www.youtube.com/watch?v=U8lZUxHX8e8 reply FerretFred 9 hours agoprev> The overwhelmingly negative reaction has probably taken Microsoft leadership by surprise. For almost everybody else, it won’t have. I sometimes (always) wonder which planet Microsoft leadership live on - it's certainly not the real world that you and I live on. reply jasonjayr 19 hours agoprevThere was a movie about this exact thing. Antitrust (2001) was about a Microsoft-like company monitoring everyone’s computer and stealing code. 23 years later and here we are. reply aleph_minus_one 19 hours agoparentWhen I watched this film, I immediately thought during the finale of this film that the story of this film is so \"wrong\" because the public will barely care about the misdeeds of NURV (or rather: those who do care basically already \"know\"). From todays's perspective, considering for example the Snowden and Wikileaks revelations that caused exactly these barely-nil reactions in the public, I know that I was right regarding this feeling. reply accrual 19 hours agorootparentI wouldn't say they caused barely-nil reactions. Maybe to the general public and to a random bystander if asked, but they still had implications for the public's overall trust in the government. And for users in the affected spaces (tech, security, etc.) the reaction was stronger and longer lasting. reply aleph_minus_one 18 hours agorootparent> but they still had implications for the public's overall trust in the government. In my observation it really was as I described: - those who already deeply distrusted the government continued to do so - the others (the huge majority) simply did barely care or even attempted to justify the crimes Concerning your point about tech and security sectors: those who form the inner core of the people working in these sectors basically already knew what was happening since at least the 90s. reply vasco 16 hours agorootparentprevNobody votes for net neutrality or internet rights or privacy focused groups. I agree that there were almost no reactions. June 6th people will vote for European elections, lets see how many seats the pirate party gets. reply zer00eyz 18 hours agorootparentprev>> considering for example the Snowden and Wikileaks revelations Every time someone points at this and acts like it was a revlation I shake my head. https://en.wikipedia.org/wiki/Room_641A https://en.wikipedia.org/wiki/Joseph_Nacchio (Every one screams about conspiracy's but the only people who told the Bush II government no went to prison). https://www.politico.com/blogs/politico-now/2008/02/senate-p... They did it, out in public people wrote about it, everyone shrugged and went on with their lives. All Snowden did was give it a face, some (program) names, but any one with any sense stayed away already. The American public has been warned twice, and did not care. It's gonna take something major leaking for them to do anything about it. reply pseudalopex 16 hours agorootparentKlein said he is relieved another person -- Snowden -- could corroborate his story, but with actual government documents. \"When he first came out, I was delighted. It was, first of all, vindication for what I was saying,\" Klein said. \"He also revealed the programs they were doing were vastly bigger than I ever understood at the time.\" https://www.nbcbayarea.com/news/local/bay-area-whistleblower... reply makeitdouble 18 hours agorootparentprevIt depends on what you see as an appropriate reaction to wikileaks and Snowden. It was a structural problem, and no governing parties ever tried to stop it so voting it out for instance wasn't a solution. Perhaps Trump's support could be a far reaction to it, with no clear direction but just a strong wish to screw it all. We see the same in other countries where people go to the extremes as discomfort rises with no actionable way. reply ch4ch4 19 hours agoparentprevThat was an enjoyable movie, aside for the fact that the writers thought fiber optic cables are used for hidden cameras. reply gravescale 18 hours agorootparentIn 2001, image sensors and the electronics to drive them were far larger then they are today, an endoscope-style fibre optic isn't necessarily ridiculous if you only had a tiny hole to sneak through a thick obstruction. Even today, a non-CIA-grade camera head is probably 2-3mm across and the optics on a fibre optic can be far narrower. You can only have a pinhole spy cam if you have a void directly behind the pinhole. reply kjkjadksj 16 hours agorootparentWhats the spec on the CIA grade stuff then? reply thebruce87m 11 hours agorootparentThey probably don’t even have to bother, just hack the targets phone, tv, iPad, laptop to get every bit of content they consume and conversation they have. reply gravescale 15 hours agorootparentprevWell, I assume they can do better than a £10 USB endoscope from AliExpress! But even if you have some ultratiny thing hot out of a classified lab, fibre optics also keep the active electronics further away and harder to detect. reply tomalpha 18 hours agorootparentprevhttps://www.detective-store.com/optical-fibre-micro-camera-s... reply jstummbillig 8 hours agoprevI think, roughly, having good access to information is a super power (good being a mix of mostly: quick, simple, easy, accurate, free). My personal information is most valuable to me, so having access to that is specially powerful. The only way to make that happen is to store that information somewhere. The best way to do anything, that I just want the benefits of, is automatically. And now we are here. It feels monstrous but, to me, the above still stands. How to connect the dots to get to a place that feels good, I do not know. I would not be shocked if it turned out to be mostly about adjusting ourselves to it over time. But I am almost 100% positive we will all* want this super power, in some much better and much more complete form, in our future lives. And not being able to have it will feel absolutely silly, from there on out forever. reply apantel 22 minutes agoparentIt’s only a superpower if you have full control over the data and no one else has access. If someone else has control and access, it’s THEIR superpower over you. reply temp3000 8 hours agoparentprevThe tradeoff is security and privacy. Important things. If you click the eye icon next to a password, or paste a secret to an env var you are now exploitable. If you have something you want to keep private or shred this is another place to delete (or forget to do so). So there are plenty of anti-benefits to this superpower. reply jstummbillig 7 hours agorootparentI understand. Nobody* will care. People already have traded privacy and a comprehensive personality profile for silly streams of video, photos and text on social media for the past 20 years. Imagine what happens, when you get something immensely useful out of it. Today, nobody will work with you, if you are unable to manage E-Mail. In the future, nobody will work with you, if you can't properly use the time information dimension that this technology enables. You will simply look demented by comparison. reply temp3000 7 hours agorootparentIf that is the case then like anything else let’s make it, well… good. And that includes making it safe, secure and privacy focused. A good delineation is your work PC will be spied on (assume anyone in the org can see what you are doing, even before this tech). Opting out of it on personal devices is fine. You wont look like an idiot or be refused work I am sure. reply bravetraveler 7 hours agoparentprev> How to connect the dots to get to a place that feels good, I do not know With an eraser, entirely not interested. I'm willing to make the bet on this FOMO that while people risk this, I'll still be just as employable by living like it's 1999 I provide the value, not what I did before reply poisonborz 8 hours agoparentprevThat is the saddest truth I've read for some time reply timeon 8 hours agoparentprevI prefer personal discipline and memory over this 'super power'. reply croes 7 hours agoprev>Q. Are Microsoft a big, evil company? >A. No, that’s insanely reductive. They’re super smart people, and sometimes super smart people make mistakes. What matters is what they do with knowledge of mistakes. Never attribute to malice that which can be adequately explained by neglect, ignorance or incompetence. But >Q. Did Microsoft mislead the BBC about the security of Copilot >A. Yes. >Q. Have Microsoft mislead customers about the security of Copilot? >A. Yes. For example, they describe it as an optional experience — but it is enabled by default and people can optionally disable it. That’s wordsmithing. Maybe at some point we should reconsider. reply zuminator 19 hours agoprevThat was very well written and reasonable, neither minimizing the risks nor succumbing to hysteria. The idea behind Recall, universal, machine-assisted search, is a good one but it's embarrassingly clear that in terms of implementation, this ain't it, chief. Microsoft should do what Google has done with Sky -- withdraw the product, take the hit, and hope that something can be salvaged from this debacle, both in terms of an improved product, and better company-wide testing and rollout practices. reply shostack 19 hours agoparentI think the idea is a good one, I just have zero trust in Microsoft to have my interests in mind so it becomes a question of when, not if, it becomes abused, if not by them then some bad actor. reply emmelaich 18 hours agoparentprevWhat's this Google Sky that was withdrawn? reply ambicapter 18 hours agoparentprev> In practice, that audience’s needs are a very small (tiny, in fact) portion of Windows userbase — and frankly talking about screenshotting the things people in the real world, not executive world, is basically like punching customers in the face. The echo chamber effect inside Microsoft is real here, and oh boy… just oh boy. It’s a rare misfire, I think. > I think it’s an interesting entirely, really optional feature with a niche initial user base that would require incredibly careful communication, cybersecurity, engineering and implementation. Copilot+ Recall doesn’t have these. The work hasn’t been done properly to package it together, clearly. Definitely needed some copy editing however. reply lostmsu 18 hours agoparentprevThis is poorly written and completely nonsensical. Also a clickbait: nobody is stealing anything. If you get malware or you have other admins on your computer, they can already record anything you do at their will. The data is NOT uploaded to M$. The whole fuss is stupid and I am impressed 90% comments here seem to think otherwise. reply kccqzy 15 hours agorootparentIf you get malware, these malware will only begin to record things once they are installed. They do not automatically get years of computer use history. Until Recall. It seems like you live in the same kind of detached echo chambers as the people at Microsoft who approved this feature. No wonder you find this nonsensical. I bet the product managers responsible for this also find this article nonsensical. reply lostmsu 14 hours agorootparentRight, the malware will only able to get all the documents you wrote during the years of using the computer, all the emails you ever got or sent, all your chat conversations, your bank account details, photos and videos you made, complete browsing history, access to your work account(s). Of course it is critical that it does not also know what porn you watched in incognito, or what you had in snapchat messages. Wait, scratch the snapchat. I don't think they have desktop client. reply eviks 5 hours agorootparentYou ignore this basic computer operation called delete reply fh9302 18 hours agoprevRecall can't be disabled during the Windows setup. It has to be disabled manually in Windows settings. https://x.com/tomwarren/status/1796681578984182066 reply autoexec 18 hours agoparentEventually they can just re-enable it as part of an \"important update\". They can even stop you from being able to disable it entirely. If you're using Windows, it's not really your computer. At least you aren't the administrator of that device. Microsoft can access any file, install any software, change any setting, or remove any access at any time for any reason with no notice or indication to you that it happened. They can even shutdown your computer. Any device which works like that is not one that's under your control. Every internet connected windows computer is insecure by design and cannot be trusted to protect your privacy or security. reply wilsonnb3 18 hours agorootparent> Microsoft can access any file, install any software, change any setting, or remove any access at any time for any reason with no notice or indication to you that it happened. They can even just shutdown your device. Any device which works like that is not one under your control. Having this power is inherent in making the OS. Whoever is the vendor of your particular Linux distribution has the same powers, it is just that you trust them not to use them (or, in a very small theoretical minority of cases, you’ve audited the code and binaries yourself). So yes, you shouldn’t use an OS from a vendor you don’t trust, I agree completely. I don’t understand why people are acting like this is earth shattering news though, this has always been the case since people started using software they didn’t write themselves. reply autoexec 18 hours agorootparent> Having this power is inherent in making the OS. No, it really isn't. For decades I owned computers with operating systems which didn't have that capability. Once installed and configured, the OS was consistent and (reasonably) stable. Someone would literally have to break into my house or office to modify my settings or install software against my wishes. Even after I started connecting my devices to the internet the OS itself had no ability to do these things and couldn't gain that ability unless I explicitly chose to install updates that enabled that behavior. That's entirely different from the situation today where MS forces updates and restarts, installs unwanted software on our computers, and has files and folders that we (even using administrator accounts) don't have access to. Linux too is very different. Linux is transparent about what it does, adds, or changes. You have the power to choose which updates to apply or not. You have the power to modify any part of your OS so that it does what you want. I can't speak to all distros out there, but I've never seen a linux system force a restart in the middle of the day, or reinstall applications users removed without notice. Can't say the same for Windows. Unlike Windows, linux typically respects its users and their wishes. You really don't have to write your own software in order to have software that respects you and leaves you in control of your own devices. It's kind of crazy that you'd think there could be no other way. reply wilsonnb3 18 hours agorootparent> I can't speak to all distros out there, but I've never seen a linux system force a restart in the middle of the day. My point is that there isn’t a technical reason that prevents Linus distros, or any other OS, from restarting your computer whenever it feels like it. By definition the OS has control of the hardware and software and thus whoever writes the OS inherits that control. I completely agree that there are good reasons to not trust Microsoft and people who don’t should not be using Windows. I just dislike the framing of this issue as a recent development rather than an inherent problem of running software you didn’t write. reply autoexec 17 hours agorootparent> My point is that there isn’t a technical reason that prevents Linus distros, or any other OS, from restarting your computer whenever it feels like it. Go install MS-DOS 6.22 on a computer. You can leave that system up and wait your whole life and you'll never see it suddenly restart your computer without asking. The technical reason why it can't is because there is no code in that OS designed to check for and accept an order from someone at Microsoft to restart your machine without asking. It doesn't exist. You could choose to find or write and then install new software that gives that OS the capability to do it, but that capability just isn't there otherwise. There's no rule that an OS has to include code to violate the rights and will of the people who install it on their devices. That's a choice that MS made. Far too many people have accepted that behavior from them so they keep pushing and pushing with new and increasingly user-hostile code and behavior but none of that is inevitable or unavoidable. That is what's a very recent development. For a very very long time no operating system would have dared to violate their users that way. None of them did. Yes, at a certain level you have to be able to place some level your trust in your OS. Especially one with internet access. MS has shown themselves to be entirely untrustworthy, but they could still change all of that. They could strip out every line of code that allows them to remotely access your system without your explicit permission. They could be 100% transparent about what their updates will do to your computer if they are installed and they could give you the ability to not install any update you didn't like and revert to any previous state. They could give you full access to every file and directory and process and give you the ability to control every aspect of their OS. They could vow to never modify a setting after you've changed it. They just choose not to do those things, because they don't care about you or your privacy or your wishes, or your rights. As long as people continue to use windows, Microsoft stands to make a lot of money by ignoring those things. reply ryandrake 17 hours agorootparentprevRight. There is no technical reason why the OS vendor couldn’t attack you in the past, but software industry norms have changed over the years. What has changed is trust. Today, you have to consider commercial OS vendors (and third party application developers) to be remote attackers in your threat model. More and more, they write their software to serve themselves rather than their users, and to make computers do what they want them to do, not what the users want them to do. This was not the case decades ago, even if the technical ability was there all along. reply autoexec 15 hours agorootparent> More and more, they write their software to serve themselves rather than their users Well said! I really miss when our products served us but I can't think of a recent purchase of anything internet capable that wasn't designed to work for someone else (and against me no less). I don't see \"never own an internet capable product again\" as a viable option here, and I'm not sure what else we can do to protest this besides push for government intervention. In the meantime, I try to firewall off whatever I can. reply smaudet 17 hours agorootparentprev> My point is that there isn’t a technical reason that prevents Linus distros, or any other OS, from restarting your computer whenever it feels like it. Wrong, the point of the operating system is to manage local state, hardware, etc. The point of viruses, malware, and spyware is to exfiltrate data and control from a set of systems. This is getting to the point where Windows itself is a worse virus than just downloading the random shady program from the internet, with all anti-virus turned off... And the technical distinction? You can turn off everything in linux, you can make it so the computer cannot update itself. The Operating System is unable to change itself in this configuration, the only way around this is for you to choose to update it. This cannot be done with Windows, not without resorting to technical tricks that look at lot like what malware and viruses have to do. This a is pretty, and important technical distinction: Operating Systems don't have built-in backdoors that you cannot turn off by design. Malware and botnets, have built-in backdoors that you cannot turn off by design. reply wilsonnb3 17 hours agorootparent> Wrong, the point of the operating system is to manage local state, hardware, etc. Yes, and to manage local state and hardware it needs to be able to control the hardware and other software. You can build an OS that doesn’t take advantage of those capabilities but you can’t build an OS that doesn’t have them. Hence why the key is trusting your OS vendor. > And the technical distinction? You can turn off everything in linux, you can make it so the computer cannot update itself. The Operating System is unable to change itself in this configuration, the only way around this is for you to choose to update it. Sure you can do all that but what you can’t do is make it so your Linux based OS can’t control your hardware and software. At the end of the day, the key is still trust, either in your vendor or in your own audit. You have presented a great many reasons why Linux is more trustworthy than Windows to many people but you cannot get around the problem of having to trust someone. reply smaudet 12 hours agorootparent> but you cannot get around the problem of having to trust someone. You still don't get it... At the end of the day, I don't have to trust anyone with an OS that I fully control, with hardware that I fully control, because I can verify every bit of hardware, every bit of software, even stop the kernel from doing things if I want to (yes its possible, technically). Sure, I can place some temporary trust in some components, but it doesn't matter really, because I can always swap/disable/remove audit/reaudit any component. You can choose to trust, as much or as little as you want. I don't have to use the kernel at all if I don't want to, I could swap in another one and still be good to go (more or less). This is different from the case here, where by default, not of my choosing, actively and persistently nearly every aspect of a Windows computer is obfuscated, un-auditable, actively and without consent doing things that are not operating system things but spyware, bloatware, crapware, or just straight up malware. You can wave your hands around as much as you like waffling about \"trusting someone\" but there is a big big difference between someone acting reasonably, and choosing to allow them into your home, and \"trusting\" someone with a knife to your back not to shiv you. One is reasonable, a choice, and low risk, the other is clearly none of those things. You don't have to \"trust\" low risk situations, they are just low risk, no trust involved. reply Terr_ 16 hours agorootparentprevThere's still a big difference between \"a surreptitious hack is technically possible with future development and getting you to accept a bad patch\" versus \"the company is actively using sketchy powers and trying to make them constant and socially normalized.\" In one case, someone discovering sketchy secret backdoor code causes a huge flap and damage to the company's brand and stock price etc. In the other, some corporate drone bafflegabs about it enabling superior customer satisfaction synergies, while pointing to a tiny clause in an enormous contract of adhesion to claim everybody knowingly agreed to it. reply gryn 18 hours agorootparentprevI don't think msft will give you the code for windows to review it if you ask nicely, unlike linux where it's already available. if you're paranoid about the distribution of your pre-built distro you can compile everything by hand and some do that for fun. so putting them on the same pedestal is weird mind gymnastics. reply wilsonnb3 17 hours agorootparentYes, if you compile your own binaries, audit the source code, and for good measure audit your compiler and the system you are using to compile it, then there is a meaningful difference*. Since 99% of users don’t actually do any of that, then in practice there isn’t actually a difference. * I am aware that there are shades of grey between the scenario I describe and proprietary software - I am just being hyperbolic for rhetorical reasons. reply autoexec 17 hours agorootparent> Since 99% of users don’t actually do any of that, then in practice there isn’t actually a difference. I understand the hyperbole, but in practice we have strong evidence that MS is willing to intentionally use their OS against you, while we don't for your typical linux OS. That really means a lot. When linux distros disrespect their users even a little (see for example https://www.pcworld.com/article/436097/ubuntus-unity-8-deskt...) users really don't put up with it and they can switch to another distro with very very little effort/change and even have the ability to modify the source and fork the OS. That helps to keep people a little more honest. The backdoored compiler problem is a bit harder. We can write our own, but it's turtles all the way down. Increasingly we also have to put a lot of trust in our hardware. There are only a small number of companies making CPUs and wireless chips. I imagine they're under enormous pressure from governments to compromise the privacy and security of the people using that hardware and we have less trust in our own devices the more we have \"trusted computing\" forced on us. reply throwaway22032 17 hours agorootparentprevTrust is earned. Your partner always has the capability to screw you over, cheat on you, embezzle from the shared account, whatever. Linux is like a nerdy guy who stays at home, plays with Warhammer figures and cooks you dinner. Windows is an OnlyFans model who goes on vacations for weeks at a time and ignores your calls. reply rgrmrts 18 hours agorootparentprevYeah this is my problem with windows. I’ll delete or disable things that were added to my machine only to have windows update restart my computer and those things show up again. I’m using a legit copy of Windows 11 Pro and it’s absurd that I’ve had to delete or disable random shit like social media apps multiple times. reply pcdoodle 17 hours agorootparentprevYes. This is why they renamed \"My Computer\" to \"This PC\". It's finally here. It's been fun, I love windows but this is the end IMO. reply autoexec 15 hours agorootparentI know how you feel. I was a fan of DOS, Win89SE, Windows 2000 Pro, and Windows 7 Pro (until 7's updates started including Win10's invasive telemetry). The good news is that alternatives are better than ever and the few windows applications I still use can run using wine (or worst case a VM) reply cantSpellSober 17 hours agoparentprevWindows victims (self included) are used to this. Setup takes an hour, configuring settings takes a week reply 999900000999 18 hours agoprevRecall needs to ship as a completely separate application. In fact Microsoft should also charge a nominal fee for it so no one accidentally installs it. Still this is the worst spyware ever made. Have a telehealth appointment, lawyer conference, loan application, now Recall will store all that like it or not. This made me decide to go with a non Snapdragon Laptop since I want nothing to do with this. Gonna dual boot with Fedora ( sorry Debain, but it looks like Stable is a bit behind what I need hardware support wise). Microsoft is making a serious argument for going full Linux + maybe a PS5 for competitive gaming ( since anti cheat is Windows only by design). reply pkossum 17 hours agoparentOddly enough I game on windows, and have been considering your last sentence, though I'm mostly waiting out Arm Linux for battery life considerationz. reply 999900000999 17 hours agorootparentI also use Windows for music. I really really don't want to switch to OSX here since a 4TB drive is literally a 1200$ upgrade for Macs. Compared to 200$ when you can upgrade it yourself. Music production isn't great on Linux. reply tim333 6 hours agorootparentThere are always external drives with macbooks. I guess neither system is perfect. reply aaronmdjones 15 hours agoparentprev> sorry Debain, but it looks like Stable is a bit behind what I need hardware support wise bookworm-backports has kernel 6.6, which is the very latest LTS series. Is this not new enough? reply 999900000999 9 hours agorootparentI don't understand how to install backports. I'm generally not a hardcore Linux person. Fedora seems to be more up to date out of the box. reply mikehearn 18 hours agoprevI'm trying to square the claims in this article with what Microsoft says. Article: \"This database file has a record of everything you’ve ever viewed on your PC in plain text\" Microsoft: \"Snapshots are encrypted by Device Encryption or BitLocker, which are enabled by default on Windows 11.\" https://support.microsoft.com/en-us/windows/privacy-and-cont... The article is a little bit hand-wavy about how exactly the database comes to be decrypted and remotely exfiltrated. The headline says it takes \"two lines of code\" but unless I'm missing it, I don't see those lines discussed in the article. reply MiguelHudnandez 18 hours agoparentThe database is not encrypted while the system is running. Microsoft's claim that it's encrypted is due to the machine being encrypted at rest with Bitlocker. The databases are plain-text sqlite files within the current user's %appdata% folder. So, literally anything that can grab those files and put them somewhere else can qualify as exfiltration. Any backup product worth its salt would be covering these databases. reply teraflop 18 hours agoparentprevBitLocker encrypts the hard drive contents at rest, but while the system is booted, the drive is transparently decrypted. So what Microsoft says is technically true, but doesn't necessarily present any kind of barrier to the database being exfiltrated by malware. It only protects against somebody stealing your hard drive. reply rezonant 18 hours agoparentprevWell bitlocker (ie device encryption) is only protecting you from offline attacks, ie when someone pulls your hard drive to examine it. Code running on the machine itself wouldn't be affected by it. reply walterbell 18 hours agoparentprevFrom the article: Q. Have you exfiltrated your own Recall database? A. Yes. I have automated exfiltration, and made a website where you can upload a database and instantly search it. I am deliberately holding back technical details until Microsoft ship the feature as I want to give them time to do something. I actually have a whole bunch of things to show and think the wider cyber community will have so much fun with this when generally available.. but I also think that’s really sad, as real world harm will ensue. reply jmprspret 18 hours agoparentprev1. It is encrypted at rest, once you login its decrypted with the rest of the stuff running+on your drive. All this stops is someone with physical access and that's it. 2. The article says that they are not releasing PoC (my words not theirs) because this feature isn't out, and they want to give M$ a chance to fix it: > I am deliberately holding back technical details until Microsoft ship the feature as I want to give them time to do something. reply userbinator 18 hours agoprevMeanwhile, features like \"secure\" boot will stop you from patching this spyware out when it inevitably becomes impossible to disable completely via ordinary means, and even if you manage to find an exploit to \"jailbreak\" through, remote attestation will ostracise your machine from all the services that will eventually use the \"telemetry\" gathered to grant or deny you access based on how \"human\" you are. reply 20after4 17 hours agoparentTo anyone who thinks this sounds far fetched, check back in a couple of years. I can only think of one alternative future, in which things are not headed in this generally Orwellian direction, and the alternative may be even worse (complete or nearly-complete social collapse) reply temp3000 8 hours agorootparentIn a system where everyone breaks the law (because there are so many) and everyone is spied on (so you can prove anyone did some sort of “ crime”) you can now coerce anyone you like if you get to make the decision of who to charge (like governments do) which is useful for silencing descent or opposition. reply alpha123123 19 hours agoprevThreads on similar \"apps\": \"Rem: Remember Everything (open source)\", 196 comments, https://news.ycombinator.com/item?id=38787892 \"I made an open source Windows app to rewind and search everything on screen\", 166 comments, https://news.ycombinator.com/item?id=40105371 \"Rewind: The Search Engine for Your Life\", 92 comments, https://news.ycombinator.com/item?id=33421751 reply wavemode 18 hours agoparentI thought this concept sounded familiar. I remember Rem being relatively well-received. I guess people trust Microsoft a lot less. reply zuminator 15 hours agorootparentThose third party apps are not enabled by default, won't modify their functionality without your permission, and two of the three are open source. And for me it's not so much that I trust Microsoft less than any other company. It's that using their services require so much trust, yet they give so little trust in return. reply chewz 1 hour agoprevI haven't typed or viewed anything on any Windows PC in like 15 years. And I think anyone shouldn't.... Keep pushing chaps... reply chx 8 hours agoprevThe real harm in Recall and I so much wish there was an authority who stepped in and slapped Microsoft into next week because of it: abusive relationships. Now your abuser can see everything you've done on your computer so if you try to get out your situation will worsen and also because of this you might not dare to seek help on said computer. reply mercurialsolo 8 hours agoprevBeyond the technology community I doubt that the loss of personal privacy is being discussed at large. Ad trackers had already made internet privacy a myth for the vast majority of the world; we now are seeing this permeate onto personal devices and subsequently to our environment with these always on, silently tracking devices - in the name of convenience. Perfect recall, no need for memory, available at your fingertips - with T&C that completely disregards your need for personal privacy. The challenge though is the \"better\" alternatives where a mix of privacy and convenience is there is not always convenient. We have and continue to be marketed convenience at the cost of privacy. What we need is consumer data privacy to really become a societal and government concern and for devices which infringe on this to really go through the same scruity as say a drug going thru FDA approval. reply probably_wrong 9 hours agoprevI regularly get spam claiming that a hacker has compromised my device, has been filming me jerking off through my own webcam, and will send screenshots to my friends and family if I don't send them bitcoins. You know, the usual. Once Recall is out the story will go from laughable to worrying - all they'll have to do is change their text to include instructions to open Recall (the same way old websites would open 'file:///' to show they 'knew' what's on your PC) and regular people will lose their minds (and money). And then there's the age-old adage \"if you can see it you can exfiltrate it\" - it didn't work for DRM and it won't work here. Malware will steal this data. reply croes 7 hours agoprevSeems like the Key & Peele's Computer History sketch needs an update. https://youtu.be/s0lUbqDrFWU?si=47kGfhA-PVuctWep NSFW reply rakoo 8 hours agoprevI had to make it through 3/4 of the article before realizing that \"Copilot+\" is not, in fact, Copilot, or an enhanced version of it, but a line of computers. The marketing department also has some work to do. reply xbmcuser 15 hours agoprevConsidering how stupid/unsophisticated an avg computer user is. The worlds scammers income is going to sky rocket in the next few years as more people get new computers with built in recall. reply DarkmSparks 8 hours agoprevI dont even need to read beyond the title for a change, its been more than 10 years now since I have used windows in a business environment (linux on my main machine, mac for my second) and there is already no way I would go back to it. But good grief wtf. I was mostly joking when I said before MS was intentionally trying to kill windows, but is there actually any other explanation for implementing this \"feature\"? reply makkesk8 8 hours agoprevI used to use Manictime[1] to achieve something similar, although, that was for time tracking purposes. But I can admit I've used it multiple times to find websites and documents that I forgotten the name of using the screenshots. But in essence, It suffers from the same flaws as Recall. [1] https://www.manictime.com reply MarkMarine 19 hours agoprevI just assumed windows work computers were doing this already. reply aleph_minus_one 18 hours agoparent> I just assumed windows work computers were doing this already. In most sectors the installed corporate spyware is from a different company than Microsoft. reply smaudet 17 hours agoparentprevWork computers are like this, a bit, but typically they don't exfiltrate their own data outside their own corporate borders. That's a big deal/difference. The other issue, the vendors tend not to leave glaring security holes in their software, both because of IT desire to maintain control of the operating environment, but also because there is intense awareness that corporate espionage is a constant, real, and ongoing threat. It sounds like the MS folks rushed out a \"feature\" and wanted to pretend we all live in some utopia where nobody does anything bad, ever. Possibly all snorting coke or something... reply thierrydamiba 19 hours agoprevAm I missing something or is this as big of a disaster as I think? This is terrible news right? reply ocdtrekkie 19 hours agoparentSort of but AI PCs are going to be more expensive than normal PCs so most users won't and will continue to not have a PC that does this. Pushing it out to existing machines would be a very different tier of problem. reply dmurray 19 hours agorootparentIf it works like any other electronics product I've bought in the last ten years, the \"AI\" version will soon be cheaper thanks to subsidies from advertisers, while the \"normal PC\" will be twice as expensive and the domain of tinkerers and cranks. reply __loam 18 hours agorootparentI just want to buy a screen guys. reply calgoo 19 hours agorootparentprevI wonder if MS will then just make it a online processing version, so it just ships all your data \"for free!!!!\" to MS servers and then processed remotely for you! I also wonder how \"knowledge transfer\" will happen when you get a new machine in the future? What about backups, do they sit in the cloud already? These all sounds like ways that this \"local\" AI PC will share data with the MS cloud in one form or another. With Apple doing similar things already on iPhones (I know there are differences, but its still analyzing your data etc), I wonder if Linux might actually become a more mainstream OS in the future. Could also be that people stop using computers as much and just use tablets with docking stations + keyboard & monitors (again, there is work to be done, but its a possibility from a HW level). That would leave us with MacOS & Windows for business desktops (with some Chromebook & Linux sprinkled in there). Education would probably be more Tablet & Chromebook style compute, and gaming is already moving to the cloud (I guess the positive here is that we might finally be able to get rid of AntiCheat software:) ). reply aleph_minus_one 18 hours agorootparent> I wonder if MS will then just make it a online processing version, so it just ships all your data \"for free!!!!\" to MS servers and then processed remotely for you! > I also wonder how \"knowledge transfer\" will happen when you get a new machine in the future? What about backups, do they sit in the cloud already? Honestly, I guess this will become very expensive for Microsoft, and they won't find a good business case what to do with the collected data. So Microsoft is wasting a huge load of money, and additionally their AI spyware causes a huge reputation damage for Microsoft: a lose-lose situation. :-( reply gryn 17 hours agorootparent> they won't find a good business case what to do with the collected data. ads. they've been clear about windows becoming more ads oriented. guess they are testing the water to try and get a competitive edge over google in a few years with this new data trove. reply walterbell 18 hours agorootparentprevOther use cases for on-device NPU silicon have been demonstrated, e.g. radio sensing of room geometry and human motion, breathing, gestures. reply aleph_minus_one 18 hours agorootparentThanks for the clarification of other kinds of spying that this enables. Nevetheless I still have difficulties seeing how this is supposed to make money for Microsoft. reply walterbell 18 hours agorootparent> how this is supposed to make money No idea about this specific feature, but billions are being spent to train models in the cloud (including Azure), with the expectation that some models will be used for on-device inference. It remains to be seen whether those investments will return dividends. In the meantime, cloud and GPU vendors are making money on model training. reply aodonnell2536 18 hours agorootparentprevThis seems likely to me. Considering the global market share of Windows, there would be a need to roll out such a grandiose service slowly, too. reply walterbell 19 hours agorootparentprev> AI PCs are going to be more expensive than normal PCs The Qualcomm Oryon SoC is about half the price of an Intel CPU. reply ocdtrekkie 16 hours agorootparentCurrently an Inspiron 7440 (non-Copilot+) starts at $849 and an Inspiron 7441 (with Copilot+) starts at $1,099. That's a $250 premium to get an \"AI PC\". reply walterbell 16 hours agorootparent12-core Oryon dev kit with 32GB RAM is $899, https://www.windowscentral.com/software-apps/windows-11/qual... It's a brand new device family. OEM price competition should improve as more devices ship in Aug/Sep. Oryon should run Linux with good performance-per-watt. Hopefully the NPU can be disabled to save energy. reply ocdtrekkie 13 hours agorootparentThe part where you are pointing out a dev kit and about how well it runs Linux is really just admitting you don't understand the conversation we are having here. Most users buy a cheap HP from Costco for $300. Businesses will buy the same standard line OptiPlex they bought last year. Very, very few people will magically end up with Windows Recall who didn't intend to. reply walterbell 8 hours agorootparent> how well it runs Linux i.e. PC hardware for AI inference will not be limited to Windows. > Most users buy a cheap HP from Costco for $300. Businesses will buy the same standard line OptiPlex they bought last year. Both Intel and AMD announced upcoming chips with NPUs. Mediatek and Nvidia will likely join the Arm AI PC competition in 2025. Apple's 2024 OS updates are focused on AI features, both on-device and cloud partnership with OpenAI. Intel's Computex tagline a few weeks ago was literally \"AI Everywhere\". In a few years, silicon for on-device AI inference will likely be pervasive in retail PCs, including Costco, HP and Dell Optiplex. It has been shipping in Apple Silicon Macbooks and iPads since 2020, mostly unused by software until now. reply thsksbd 19 hours agoprevAhhh, I knew it was important to keep my 20 year old SGI fuel. :P reply croes 8 hours agoprevSeems like the Key & Peele's Computer History sketch needs an update. https://youtu.be/s0lUbqDrFWU?si=47kGfhA-PVuctWep NSFW reply rafaelgoncalves 1 hour agoparentThanks for the video. Like someone commented, i think too that this sketch will not get old so soon, more with all \"AI\" getting pushed. reply cududa 9 hours agoprevNeither these two paragraphs or the source they’re paraphrasing (https://doublepulsar.com/recall-stealing-everything-youve-ev...) indicate anything matching the headline at all. The source article is a QA with himself. There’s 1 tweet references, that shows a screenshot of a truncated SQLite db that shows a log of the applications opened via the user UI shell: https://x.com/gossithedog/status/1796218726808748367?s=46&t=... Whoopty doo. There are many, many sources throughout Windows that can give you a list of recently opened applications (that have existed for 10-25 years) reply andersa 9 hours agoprevThe real question is why the hell can said infostealer malware access the file. Can we PLEASE have proper per-process file access restrictions on Windows already - like MacOS has had for a decade now ????? Why is win32 app isolation still not done? reply buro9 9 hours agoparenthttps://learn.microsoft.com/en-us/defender-endpoint/enable-c... this works surprisingly well to prevent an app from going too far into folders it should not access. It's a bit opaque though, not as simple as *nix owner/group/everyone permissions. reply andersa 8 hours agorootparentThis seems like a start, but just marking applications as \"trusted\" doesn't cut it. We need real rules like each program can only access its own installation folder, its own user data folder, and any folders the user has explicitly granted access to for that program. I may \"trust\" a video editing app, for example, so I can access my raw content folders. It should still be completely impossible for that process (or any spawned from it) to access my browser session information in case of an RCE from loading a malicious video. reply datahack 19 hours agoprevLet me be clear: I will literally burn any computer equipped with this technology I am forced to use and post the aftermath online. This is societal cancer. Total information monitoring is the death of any semblance of human independence and should be violently resisted. I have never been more disgusted by a management team. How clueless can you be? Combined with digital intelligence, this technology is profoundly dangerous to anyone who works with a computer or technology (which is almost everyone). reply CoastalCoder 19 hours agoparentI think that kind of sabotage would only work if a very large number of persons joined you. You might want to consider an alternative plan, as doing that to an employer's / government's computer could get you in a lot of trouble. reply datahack 18 hours agorootparentOBEY. reply wilsonnb3 18 hours agoparentprevWho do you anticipate forcing you to use a computer with this functionality? reply Skunkleton 17 hours agorootparentWork, school, or ignorance. reply NegativeLatency 19 hours agoparentprev“Someone else will do it” reply datahack 18 hours agorootparentWhat happened to the punk in cyberpunk? reply aleph_minus_one 18 hours agorootparent> What happened to the punk in cyberpunk? It began to wane as soon as programming and in particular working in big tech became an opportunity to get rich. reply L-four 19 hours agoprevRecall will track every you do this would be a massive invasion of privacy if Microsoft didn't already track everything you do. reply matt3210 18 hours agoprev> Hackers can't get it Same as > The Tesla will never crash in FSD Same as >out platform is hacker proof reply xbar 15 hours agoprevWell, we've got the next 6 months to make 2024 the year of the Linux desktop... reply walterbell 19 hours agoprevThe idea other people with access to the device could see a photographic memory is.. very scary to a great many people on a deeply personal level. Windows is a personal experience. This shatters that belief. How did we get here? A 20-year lifelog. 2003, https://en.wikipedia.org/wiki/DARPA_LifeLog >The objective of the LifeLog concept was \"to be able to trace the 'threads' of an individual's life in terms of events, states, and relationships\", and it has the ability to \"take in all of a subject's experience, from phone numbers dialed and e-mail messages viewed to every breath taken, step made and place gone\". 2007 Microsoft Research, https://www.microsoft.com/en-us/research/video/the-microsoft... > The SenseCam is a personal, wearable camera developed by Microsoft Research in Cambridge, UK, and used as a lifelogging device in projects like MyLifeBits.. is based on wearing the SenseCam for lifelogging of ‘events’ during your day, and generating a fast-forward movie of the event as the memory recall interface. 2010 Microsoft Research, https://www.microsoft.com/en-us/research/publication/now-let... > Lifelogging technologies can capture both mundane and important experiences in our daily lives, resulting in a rich record of the places we visit and the things we see.. Previous work has demonstrated that Lifelogs can aid recall, but that they do many other things too. They can help us look back at the past in new ways, or to reconstruct what we did in our lives, even if we don’t recall exact details. https://www.microsoft.com/en-us/research/project/mylifebits/ & https://en.wikipedia.org/wiki/MyLifeBits > MyLifeBits is a life-logging experiment begun in 2001. It is a Microsoft Research project inspired by Vannevar Bush's hypothetical Memex computer system.. The \"experimental subject\" of the project is computer scientist Gordon Bell.. For this, Bell has digitized all documents he has read or produced, CDs, emails, and so on. He continues to do so, gathering web pages browsed, phone and instant messaging conversations and the like more or less automatically. The book Total Recall describes the vision and implications for a personal, lifetime e-memory for recall, work, health, education, and immortality. Lifelogging was referenced by 10,000 academic papers over two decades, https://scholar.google.com/scholar?q=lifelogging reply delta_p_delta_x 18 hours agoprevI genuinely like Windows. I was raised on it, and have used every consumer version since 98, and have programmed on it since Windows 7. I believe that from a purely technical, systems programming, and even UI/UX perspective, it is superior to the competition—both commercial and free/libre open-source. I will be very happy to defend this statement as factually and reasonably as possible, because I also program on 'the competition OSs' at work, and every time I do so, I want to go back to a cohesive platform like VS 2022 which, for me, is unparalleled in terms of productivity. But this sort of tone-deaf move from Microsoft is irritating me. I already dislike Windows 11's UI and UX flow because it is so reminiscent of macOS; this is why I haven't updated to it on my personal main computer yet (which is running Windows 10 Education, courtesy of my alma mater's Azure subscription). I've seen rumours that Microsoft hired a bunch of UI designers who used nothing but macOS and decided it was a good idea to port macOS UI designs to Windows. What a terrible terrible thing. UI responses that are instant even on Windows 10 now have a jelly-like lag to them on Windows 11, for no good reason. Also consider the regression of the right-click context menu, the ads and Copilot everywhere, a preference for unlabelled icons over text, amongst many others. As another comment says, Windows Recall appears to be an AI evolution of the already-present Windows Timeline feature. The privacy outcomes of this are concerning and I really really wish we didn't have 'AI' and 'Copilot' stuffed down our throats all the time. I would like to opt-in to features I want, rather than have them all pre-enabled. Some of them are very useful, like clipboard history with Windows-Ctrl-V; some less so and are flagrant privacy violations. I have already disabled almost every tracking, phone-home and auto-update feature possible using group policies; this is just another thing to add to my list of disabled 'help' features. That being said, if Recall doesn't phone home—which appears to be the case here—I don't buy the argument that 'it's stealing everything you do and hackers can access it if they have physical access'. I believe that the moment a computer's physical access record is compromised, the entire computer is compromised, regardless of security theatre like disk encryption in the form of BitLocker/LUKS, Secure Boot etc. It doesn't matter whether Recall is present or not. reply 20after4 17 hours agoparent> if Recall doesn't phone home—which appears to be the case here—I don't buy the argument that 'it's stealing everything you do and hackers can access it if they have physical access' The risk is not physical access, the risk is malware installed on the machine, or a security hole in some browser feature enabling malicious actors to covertly uploading the recall database to a remote server. reply Skunkleton 17 hours agoparentprev> It doesn't matter whether Recall is present or not Yes it does. Consider getting some malware that is detected a few minutes later and removed. For most people there would be no harm. With recall, you would be instantly screwed. reply jonas_kgomo 18 hours agoprevI have shared a similar post about another app of the same kind. It is called Invisibility (already using it, but concerned)! https://news.ycombinator.com/item?id=40530354 reply autoexec 18 hours agoprevWindows already had a built in keylogger with Window 10 (https://www.pcworld.com/article/423165/how-to-turn-off-windo...) but all that data was only going to Microsoft and couldn't be accessed by you or anyone with access to your device. Law enforcement, attorneys, and three letter agencies must be extremely excited about Recall. Now they won't have to hope that MS has records of everything you've typed while using your device, because with Recall all of that evidence will be stored on the device itself. \"If one would give me six lines written by the hand of the most honest man, I would find something in them to have him hanged.\" imagine could be found using everything a person ever types on their computer. reply thebeardisred 18 hours agoprevI find the irony that I keep getting advertised the podcast* by some VP of Security ding-dong at Microsoft *delicious*. (* I am intentionally not stating the name because they don't deserve the attention or free advertising) reply jmkni 8 hours agoprevThe domestic abuse point is a chilling one An abuser being able to access their partners/kids computers and go and see every thing they have done is terrifying Can we just not do this at all? reply WhackyIdeas 8 hours agoparentYes, my ex would have loved to have used this to see everything I was doing on a computer… then dox all my private data to Facebook for a laugh. reply fragmede 7 hours agoparentprevnannyware has existed for decades before this, just like tracking devices existed before Apple air tags. reply wkat4242 8 hours agoprev> During testing this with an off the shelf infostealer, I used Microsoft Defender for Endpoint — which detected the off the shelve infostealer — but by the time the automated remediation kicked in (which took over ten minutes) my Recall data was already long gone. Yeah... This is a BIG issue with the modern generation of antimalware solutions. The old signature-based detection is great and immediate. But it has drawbacks. It only detects already-known malware. Not tailored stuff used only on one specific target. Also, malware can change its own signature adaptively. and hook into known-safe binaries. So, a modern antimalware uses AI learning and behavioural analysis. Why is notepad.exe suddenly logging all your keywords? Ban it. The problem is: This takes a while. The tool can be configured to block it before someone looks at it, but it still takes a while. At the point that this happens, the damage is often already done. At an enterprise level this is not a huge problem because it does mean the problem is detected, and by the time it is investigated it's possible to stop the source of the malware and ban it from all the other 100.000 PCs by using signature detection or other mitigations. On personal PCs this is more of an issue because they don't have a dedicated SOC (Security Operations Centre) jumping on to these things. Also, the noise level is an issue in the enterprise, set the detection threshold too high and your SOC gets overwhelmed by all the detections and becomes ineffective. Anyhow, this is indeed a good argument against Recall. When it was first introduced last week, people stated that it wasn't a big deal because malware can install its own key/screen logging. However a repository of the last 6 months of activity is indeed a very juicy target to exfiltrate quickly before detection. reply jiggawatts 19 hours agoprevIt’s just a matter of time until they introduce an InTune feature to enforce this setting and send AI-written summaries of all employee behaviour to management. https://x.com/AlexBlechman/status/1457842724128833538?lang=e... reply Animats 18 hours agoprevIs there a law enforcement analysis tool for this yet? reply xyst 16 hours agoprevThis is so wild. What kind of a-hole in the C-level suite of MS thought this was a good idea? reply at_a_remove 18 hours agoprevI am trying not to be reactionary but I swear, my usual response to the announcement of a new Windows feature is often \"... and how do I turn that off?\" Still, sqlite ... tempting. Sometimes, if I am missing something I read months or even years ago, the right query to the sqlite files Firefox keeps can give good results. reply bee_rider 19 hours agoprevThe only silver lining on this is that, previously, the idea that your computer might visually spy on everything you do and index it for easy searching was the domain of paranoid conspiracy theorists. So I guess at least marketing that as a feature now makes it obvious that it is possible, I guess. reply EGreg 19 hours agoparentIt was always possible. But now they will claim you have a way to turn it off. Who knows ! There is so much telemetry being recorded anyway. This is like people preferring Macbooks from 2016 because it was pre-touchbar. I honestly consider good tech from previous years to be far more secure than what’s around now. Who knows what’s in your products now? reply dankobgd 8 hours agoprevImagine using windows in 2024 reply fifteen1506 8 hours agoprevI don't see Recall being axed. Disabled by feature flag? Sure. Disabled by default? Sure. Once Windows stops having administrator accounts, they'll enable it by default. As Smith would say, \"it's inevitable\". reply usrbinbash 7 hours agoparent> As Smith would say, \"it's inevitable\". To which I reply: sudo rm -rf /agents && echo \"the only windows here are made of glass\" reply Xeamek 8 hours agoparentprevwym by \"Once Windows stops having administrator accounts\"? reply ok_dad 19 hours agoprevThis is something you have to specifically enable right? Right?! I’m sick of brain dead execs and product managers. What world do these people live in? reply TheRoque 19 hours agoparentFirst it's gonna be an obvious setting, then it's gonna be a setting deep in the \"features\" of windows, then it's gonna be a register you have to edit, then it's gonna be an obscure powershell script you have to execute, and in the end, it will be a default feature that you have no control on :) reply Zuiii 15 hours agorootparentDon't forget them requiring an online Microsoft account to change the setting. Don't have internet or can't set up an account? Well, tough luck. reply 0x_rs 19 hours agoparentprevRegardless of the new Recall feature, the Windows Timeline already collects vast amounts of data and is enabled by default, and I've seen the database file grow even when disabled. A trove of information for any forensic analysis. https://kacos2000.github.io/WindowsTimeline/WindowsTimeline.... reply lstamour 19 hours agoparentprevIt’s enabled by default on a select number of laptops they sell, and no one else has access to it yet. But they plan to roll it out further. I’d personally hoped for some kind of “Secure Enclave” such that only certain processes could be run, but this is about what I would expect of someone shipping a minimum viable product because AI is a hot buzzword. Weirdly, even AI could theoretically solve some of the privacy issues simply by looking for PII and removing them. Extra weirdly, this would have been more secure if they uploaded screenshots to the cloud, because access could be better monitored there and exfiltration limited. reply ocdtrekkie 19 hours agorootparentIf anything, it's likely they imagine the PII being useful. Imagine being able to ask your computer for your bank account number instead of digging it out of a website or file. Obviously useful... but also obviously exploitable. reply NekkoDroid 19 hours agoparentprev> This is something you have to specifically enable right? Right?! lol, lmao even. Some might even be so inclined to say rofl. It is actually kind of depressing the state in which Windows is and is going. For me personally the only actual \"advantage\" Windows has over my homegrown Archlinux install is app/game support. And basically all of the ones that don't work, don't work by design because they don't want the user to own their hardware/software. reply LegitShady 19 hours agoparentprevFirst it's opt in, then it's on by default, then it's mandatory and what do you have to hide anyways? The default progression of this sort of \"feature\" and business practice. reply abracadaniel 19 hours agorootparentThere’s also the classic phase where it’s technically opt-out, but it magically gets reset to default every time Windows updates. reply hansvm 19 hours agorootparentThere's a phase in between where it's reset every time you leave the settings menu and can't see them resetting it (I forget which privacy policy it was anymore, but that was the straw that kicked me over to Linux for good). reply frizlab 19 hours agorootparentprevAnd in this particular instance it did not even start with opt-in. They went straight to on by default! reply colechristensen 19 hours agoparentprev>What world do these people live in? Getting promoted by other execs based on the visibility of delivered features. reply 27 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft's new Windows 11 feature, Copilot+ Recall, automatically takes and stores searchable screenshots of user activity, raising privacy and security concerns.",
      "Critic Kevin Beaumont argues the feature is poorly implemented and communicated, making sensitive data vulnerable to hackers and malware, despite Microsoft's security claims.",
      "The feature is enabled by default, potentially leading to mass data breaches and misuse of personal information, prompting calls for Microsoft to urgently rework or recall the feature to maintain customer trust and comply with privacy regulations."
    ],
    "commentSummary": [
      "Microsoft's \"Recall\" feature for tracking user activity on Windows PCs has raised significant privacy concerns, with critics accusing the company of invasive data collection and favoring its Edge browser.",
      "The discussion suggests switching to Linux for better privacy and control, highlighting the strategic role of the Linux subsystem and the impact of GitHub's acquisition on tools like OpenAI Codex and GitHub Copilot.",
      "Users express broader dissatisfaction with Windows, preferring Linux to avoid being test subjects and to maintain better user control and privacy."
    ],
    "points": 313,
    "commentCount": 284,
    "retryCount": 0,
    "time": 1717192369
  },
  {
    "id": 40543651,
    "title": "ROOT: The Backbone of Scientific Data Analysis and Higgs Boson Discovery",
    "originLink": "https://root.cern/",
    "originBody": "Start Reference Forum Gallery √-1 ROOT enables statistically sound scientific analyses and visualization of large amounts of data: today, more than 1 exabyte (1,000,000,000 gigabyte) are stored in ROOT files. The Higgs was found with ROOT! As high-performance software, ROOT is written mainly in C++. You can use it on Linux, macOS, or Windows; it works out of the box. ROOT is open source: use it freely, modify it, contribute to it! $ _ ROOT comes with an incredible C++ interpreter, ideal for fast prototyping. Don’t like C++? ROOT integrates super-smoothly with Python thanks to its unique dynamic and powerful Python ⇄ C++ binding. Or what about using ROOT in a Jupyter notebook? RNTuple: Where are we now and what's next? (26 Jan 2024) Hello, this is Florine from the ROOT team! Over the past year, I’ve been working as a technical student funded by ATLAS to evaluate and help further develop RNTuple. As you may already be aware, RNTuple [1] is currently being developed as the successor to TTree, and is projected to be used in Run 4. I imagine you might be wondering why there is a need for a completely new (TTree-incompatible) system, and what this looks like. That’s why in this blog post, I will try to answer this question, as well as give you an overview of the current status of RNTuple, what we’re still working on before its first production release (and what we will work on beyond this), and finally how you can already try it out! Interactive, web-based canvas is now the default in ROOT (05 Jun 2023) After a long period of development and testing we decided to switch to the web-based TCanvas implementation by default in the ROOT master version. It has been present in the ROOT for a while (since 2017) and used already in the web-based TBrowser, which you have probably seen already. New class TScatter (30 May 2023) Coding in ROOT with the horsepower of an F1 (01 Mar 2022) If you’ve ever rubbed your eyes trying to decrypt C++ compilation errors from a terminal, or even have faced with your bare eye the intimidating logs of valgrind output for memory leak detection, or manually deployed gdb, you should definitely keep reading. With this post, I believe you’ll improve your productivity and experience with ROOT by using QtCreator as a development and troubleshooting environment. Debugging CERN ROOT scripts and ROOT-based programs in Eclipse IDE (30 Oct 2021) More ...Latest Releases Release 6.32/00 - 28th of May, 2024 Release 6.30.06 - 3rd of April, 2024 Release 6.30.04 - 31st of January, 2024 Release 6.30/02 - 28th of November, 2023 Release 6.30/00 - 7th of November, 2023 More ...",
    "commentLink": "https://news.ycombinator.com/item?id=40543651",
    "commentBody": "ROOT: analyzing petabytes of data scientifically (root.cern)277 points by z3phyr 11 hours agohidepastfavorite70 comments captainmuon 5 hours agoA blast from the past, I used to work in particle physics and used ROOT a lot. I had a love/hate relationship with it. On the one hand, it had a lot of technical debt and idiosyncrasies. But on the other hand, there are a bunch of things that are easier in ROOT than in more \"modern\" options like matplotlib. For example, anything that has to do with histograms. Or highly structured data (where your 'columns' contain objects with fields). Or just plotting functions (without having to allocate arrays for the x and y values). I also like the very straightforward object-oriented API. It feels like old-school C++ or Java, as opposed to pandas/matplotlib which has a lot of method chaining, abuse of [] syntax and other magic. It is not elegant, and quite verbose, but that is probably a good thing when doing a scientific analysis. I left about 5 years ago, and ROOT was in a process of change. They already ripped out the old CINT interpreter and moved to a clang-based codebase, and now you can run your analyses in Jupyter as far as I know (in C++ or Python). I heard the code quality has improved a lot, too. reply ephimetheus 8 minutes agoparentWe all have a love/hate relationship with it. It’s a bit like Stockholm syndrome. reply ilrwbwrkhv 4 hours agoparentprevI wonder if Haskell would also be a good fit for writing something like this. reply tikhonj 2 hours agorootparentHaskell would be great for designing the interface of a library like this, but not for implementing it. It would definitely not look like \"old-school C++ or Java\" but, well, that's the whole point :P I haven't used ROOT so I don't know how well it would work to write bindings for it in Haskell; it can be hard to provide a good interface to an implementation that was designed for a totally different style of use. Possible, just difficult. reply goy 3 hours agorootparentprevI think having Haskell bindings to it will be quite valuable .For implementation of core structures, though, it's better to stick to C++ to max out on performance and have a finer control on resource usage. Haskell isn't particularly good at that. EDIT: there's one at https://hackage.haskell.org/package/HROOT reply shrimp_emoji 4 hours agorootparentprevNo. reply mynameisvlad 2 hours agorootparentThis is a technical community. You really have to do better than a one word dismissal without any reasoning. In other words, why do you think it’s not a good fit? reply hackable_sand 3 hours agorootparentprevCould it though? reply BiteCode_dev 3 hours agoparentprevHonestly now with chatgpt, matplotlib terrible API is less of a problem. reply typon 1 minute agorootparentThis is a great example of why the age of truly terrible software is going to be ushered in as LLMS get better. When the cost of complexity of interacting with an API is paid by the LLM, optimizing this particular part of software design (also one of the hardest to get right) will be less fashionable. reply OutOfHere 3 hours agorootparentprevThat's true, but still, there are things you just can't do in matplotlib that you can do better in other GPT-aware packages like plotly. reply sbinet 17 minutes agoprevIMHO, ROOT[3-5] is too many things with a lot of poorly designed API and most importantly a lack of separation between ROOT-the-library and ROOT-the-program (lots of globals and assumptions that ROOT-the-program is how people should use it). ROOT 6 started to correct some of these things, but it takes time (and IMHO, they are buying too much into llvm and clang, increasing even more the build times and worsening the hackability of ROOT as a project) Also, for the longest time, the I/O format wasn't very well documented, with only 1 implementation. Now, thanks to groot [1], uproot (that was developed building on the work from groot) and others (freehep, openscientist, ...), it's to read/write ROOT data w/o bringing the whole TWorld. Interoperability. For data, I'd say it's very much paramount in my book to have some hope to be able to read back that unique data in 20, 30, ... years down the line. [1] https://go-hep.org/x/hep/groot (I am the main dev behind go-hep) reply ephimetheus 7 minutes agoparentuproot to this day doesn’t properly implement reading TEfficiency, I believe, which is a bummer, to be honest. reply elashri 6 hours agoprevThere are no many reasons why new analyses should default to using ROOT instead of more user friendly and sane options like uproot [1]. Maybe some people have some legacy workflow or their experiments have many custom patches on top of ROOT (common practice) for other things but for physics analysis you might be self torturing yourself. Also I really like their 404 page [2]. And no it is not about room 404 :) [1] https://github.com/scikit-hep/uproot5 [2] https://root.cern/404/ reply moelf 5 hours agoparentOne common criticism of uproot is that it's not flexible when per-row computation gets complicated because for-loops in Python is too slow. For that one can either use Numba (when it works), or, here's the shameless plug, use Julia: https://github.com/JuliaHEP/UnROOT.jl Past HN discussion on Julia for particle physics: https://news.ycombinator.com/item?id=38512793 reply szvsw 4 hours agorootparentA great alternative to numba for accelerated Python is Taichi. Trivial to convert a regular python program into a taichi kernel, and then it can target CUDA (and a variety of other options) as the backend. No need to worry about block/grid/thread allocation etc. at the same time, it’s super deep with great support for data classes, custom memory layouts for complexly nested classes, etc etc, comes with autograd, etc. I’m a huge fan - makes writing code that runs on the GPU and integrates with your python libraries an absolute breeze. Super powerful. By far the best tool in the accelerated python toolbox IMO. reply OutOfHere 3 hours agorootparentNegative, as Taichi doesn't even support Python 3.12, and it's unclear if it ever will. Why would I limit myself to an old version of Python? reply almostgotcaught 2 hours agorootparentHn people are so haughty https://github.com/taichi-dev/taichi/pull/8522 reply OutOfHere 2 hours agorootparentThe haughtiness is not for nothing. Since Dec 2023, they made a lame excuse that Pytorch didn't support 3.12: https://github.com/taichi-dev/taichi/issues/8365#issuecommen... Later, even when Pytorch added support for 3.12, nothing changed (so far) in Taichi. reply almostgotcaught 1 hour agorootparent>they made a lame excuse that Pytorch didn't support 3.12 how is this a lame excuse >but it fails on a bunch of PyTorch-related tests. We then figured out that PyTorch does not have Python 3.12 support they have a dep that was blocking them from upgrading. you would have them do what? push pytorch to upgrade? >Later, even when Pytorch added support for 3.12, nothing changed (so far) in Taichi. my friend that \"Later\" is feb/march of this year ie 2-3 months ago. exactly how fast would you like for this open source project to service your needs? not to mention there is a PR up for the bump. I stand by my original comment. reply elashri 5 hours agorootparentprevThat'a true and Julia might be a solution but I don't see the adoption happening anytime soon. But this particular problem (per row computation) have different options to tackle now in hep-python ecosystem. One approach is to leverage array programming with NumPy to vectorize operations as much as possible. By operating on entire arrays rather than looping over individual elements, significant speedups can often be achieved. Another possibility is to use a library like Awkward Array, which is designed to work with nested, variable-sized data structures. Awkward Array integrates well with uproot and provides a powerful and flexible framework for performing fast computations on i.e jagged arrays. reply moelf 4 hours agorootparentUproot already returns you Awkward array, so both things you mentioned are different ways of saying the same thing. The irreducible complexity of data analysis is there no matter how you do it, and \"one-vector-at-a-time\" sometimes feel like shoehorning (other terms people come up with include vector-style mental gymnastics). For the record, vector-style programming is great when it works, I mean Julia even has a dedicated syntax for broadcasting. I'm saying when the irreducible complexity arrives, you don't want to NOT be able to just write a for-loop Just a recent example, a double-for loop looks like this in Awkward array: https://github.com/Moelf/UnROOT_RDataFrame_MiniBenchmark/blo... -- the result looks \"neat\" as in a piece of art. reply leohonexus 9 hours agoprevVery cool to see large-scale software projects used for scientific discoveries. Another example: Gravitational waves were found with GStreamer at LIGO: https://lscsoft.docs.ligo.org/gstlal/ reply hkwerf 8 hours agoparentHere it's more the other way around. CERN needs a data analysis framework, so CERN develops, maintains and publishes it for other users. That being said, I don't know whether it's actually a good idea for someone external to actually use it. My experience may be a little outdated, but it's quite clunky and dated. The big advantage of using it for CERN or particle physics stuff is that it's basically a standard, so it's easy to collaborate internally. reply aulin 8 hours agoparentprevWell these are two very different examples. One, ROOT, is a powerful data analysis framework that as powerful as it is failed to be general and easy to use enough to ever get out the HEP world. The other one, gstreamer, is a beautifully designed platform with an architecture so nice it can be easily abstracted and reused in completely different scenarios, even ones that probably never occurred to the authors. reply im3w1l 5 hours agorootparentGstreamer must have been a winamp clone right? reply jakjak123 7 hours agoparentprev> Gravitational waves were found with GStreamer at LIGO: https://lscsoft.docs.ligo.org/gstlal/ Say WHAT now?! reply rubicks 33 minutes agoprevWhat I remember about ROOT Cint is that it was an absolute nightmare to work with, mostly because it couldn't do STL containers very well. It was a weird time to do language interop for physicists. reply wolfspider 1 hour agoprevThe part of Root I use is Cling the C++ interpreter along with Xeus in a Jupyter notebook. I decided one night to test the fastest n-body from benchmarkgames comparing Xeus and Python 3. With Xeus I get 15.58 seconds and running the fastest Python code with Python3 kernel, both on binder using the same instance, I get 5 minutes. Output is exactly the same for both runs. Even with an overhead tax for running dynamic C++ at ~300% for this program Cling is very quick. SIMD and vectorization were not used just purely the code from benchmarkgames. I use Cling primarily as a quick stand-in JIT for languages that compile to C++. reply Jeaye 31 minutes agoparentI'm using Cling for JIT compiling my native Clojure dialect: https://github.com/jank-lang/jank Trying to bring C++ into the Clojure world and Clojure/interactive programming into the C++ world. reply SiempreViernes 10 hours agoprevAh, root... every day it happens I am thankful I don't have to used a version older than 6. reply YakBizzarro 10 hours agoparentRoot was zone of the reasons to decide to not study particle physics reply oefrha 10 hours agorootparentYou don’t have to. I worked on data analysis (mostly cleaning and correction) for CMS (one of the two main experiments at LHC) for a while and didn’t have to touch it. Disclaimer: I was a high energy theorist, but did the aforementioned experimental work early in my PhD for funding. reply aoanla 9 hours agorootparentI mean, most of the researchers I know at least use PyRoot (or the Julia equivalent) as much as possible, rather than actually interacting with Root itself. Which probably saves their sanity... reply brnt 8 hours agorootparentprevI did my master and PhD around the time numpy/scipy got competitive for a lot of analysis (for me a complete replacement) but the Python binding for root weren't there or in beta. Root-the-data+format remained however the main output of Geant4, so I set up a tiny Python wrapper around a root script that would dump any .root contents and load it up in a numpy file. My plots looked a lot nicer ;) reply tempay 6 hours agorootparentprevThese days you can mostly avoid it. The Python HEP ecosystem is now pretty advanced so you can even read ROOT files without needing root itself. See: https://scikit-hep.org/ reply twixfel 9 hours agoparentprevI'm still waiting for the interface-breaking, let's-finally-make-root-good, version 7, which I think I first heard about in 2016 or so... true vapourware. reply usgroup 1 hour agoprevI struggle to see why one may want to use an interactive analysis toolkit via C++. Could anyone who has used ROOT enlighten me on this? I understand why you may write it in C++, but why would you want to invoke it with C++ for this sort of work? reply ephimetheus 4 minutes agoparentAll of our other code is C++. The data reconstruction framework writing ROOT files, the analysis frameworks doing stat analysis. The event data model is implemented in C++. It has its rough edges, but you do get a lot of good synergy out of this setup for sure. reply bobek 6 hours agoprevAaah, this brings memories of late night debugging sessions of code written by briliant physicists without computer science background ;) reply andrepd 33 minutes agoparentAhh I can imagine the 2000 lines-long main() :) reply codecalec 7 hours agoprevRoot is definitely the backbone of a ton of work done in experimental particle physics but it is also the nightmare of new graduate students. It's affectively engrained into particle physics and I don't expect that to change anytime soon reply elashri 6 hours agoparentIt is not that bad now with pyroot (ROOT python interface) and uproot being an option that is easy to learn for new graduate students. The problem is about legacy code which they usually have to maintain as part of experiment service reply ephimetheus 3 minutes agorootparentI can’t count the number of of times where a beginner did some stuff in pyroot that was horrifically slow and just implementing the exact same algorithm in C++ was two orders of magnitude faster. If you don’t use RDataFrame, or it’s just histogram plotting, be very careful with pyroot. reply nomilk 10 hours agoprevSource code: https://github.com/root-project reply dailykoder 10 hours agoprev>Debugging CERN ROOT scripts and ROOT-based programs in Eclipse IDE (30 Oct 2021) Oh gosh. The nightmares. - What obviously shows that you can build extraordinary stuff in horrible environments. reply BSDobelix 9 hours agoparentI don't understand is it about eclipse? reply scheme271 9 hours agoprevROOT, providing the C++ repl that no one asked for. reply Jeaye 30 minutes agoparentI definitely asked for it. I'm using Cling for JIT compiling my native Clojure dialect: https://github.com/jank-lang/jank Without Cling, this sort of thing wouldn't be feasible in C++. Not in the way which Clojure dialects work. The runtime is a library and the generated code is just using that library. reply pjmlp 5 hours agoparentprevBefore ROOT, there was Energize C++ and Visual Age for C++ v 4.0, however too expensive and resource demanding for early 1990's workstations. There are also a couple of C++ live environments in the game industry. reply fooker 7 hours agoparentprevThe researchers behind this contributed it into mainline clang as clang-repl reply lnauta 6 hours agoprevHave they released v7 yet? When I started my PhD it they announced it, and I looked forward towards the consistency between certain parts of the software they would introduce (some mismatches really dont make sense and are clearly organic) and now I'm already 2 years past my graduation. reply npalli 6 hours agoparentv6.32 reply koolala 3 hours agoprevcan they release a quantized 1bit version? i dont think anyones pc can science this reply nousernamed 3 hours agoprevthe amount of times I googled 'taxis' with predictable results reply SilverSlash 9 hours agoprevLet me guess, it only run on an IBN 5100? reply 8organicbits 8 hours agoparentNo. https://root.cern/install/ reply div72 8 hours agoparentprevOnly for the optional \"read time travel and world domination plans\" module. reply qa-wolf-bates 2 hours agoprevI think that this article is very interesting reply mjtlittle 10 hours agoprev [–] Didnt know there was a cern tld reply ragebol 9 hours agoparentHandy if they host conferences, for people worried about too many TLDs perhaps. https://con.cern is not yet used, so... reply SiempreViernes 10 hours agoparentprevYeah... according to wikipedia they've had it since 2014, but even now a lot of their pages are on .ch reply sneak 10 hours agoparentprev [–] Yes, the root zone is terribly polluted now. Unfortunately there’s no way to unring that bell, people depend on a lot of these new domains now. It was a huge mistake, borne out of greed and recklessness. https://en.wikipedia.org/wiki/ICANN#Notable_events reply 9dev 9 hours agorootparentI still wonder why we need that arbitrary restriction anyway? reply 8organicbits 8 hours agorootparentIf we allowed all possible TLDs, then we'd need a default organization to administer them. The current setup requires an organization to control each TLD, which allows us to grant control to countries or large organizations. The web should be decentralized, which means TLD ownership should be spread across multiple organizations. More TLDs with more distinct owners is a better situation than one default. reply Biganon 9 hours agorootparentprevI fail to see the problem with those new TLDs. reply oefrha 9 hours agorootparentCertain gTLDs have been borderline scams. The most infamous one might be .sucks, an extortion scheme charging an annual protection fee of $$$, complete with the pre-registration process when you could buy .sucks for $$$$ before it’s snatched up by your enemies. They also screwed up some old URL/email parsers/sniffers hardcoding TLDs. Largely the fault of bad assumptions to begin with. Other than the above, I don’t see much of a problem. Whatever problems people like to point out about gLTDs already existed with numerous sketchy ccTLDs, like .io. Guess what, the latest hotness .ai is also one of those. reply jesprenj 10 hours agorootparentprev [–] I guess ICANN needs to get money somehow. reply lambdaxyzw 9 hours agorootparent [–] Why can't it just get funding from the government? reply rnhmjoj 8 hours agorootparent [–] Aren't they already getting an outrageous amount of money for essentially supervising a txt file? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ROOT is a high-performance, open-source software written in C++ for scientific data analysis and visualization, capable of handling over 1 exabyte of data.",
      "It played a crucial role in the discovery of the Higgs boson and is compatible with Linux, macOS, and Windows, integrating well with Python and Jupyter notebooks.",
      "Recent updates include a default web-based canvas, the new TScatter class, and the upcoming RNTuple system, with the latest release being version 6.32/00 as of May 2024."
    ],
    "commentSummary": [
      "The discussion highlights ROOT, a data analysis framework in particle physics, praised for handling histograms and structured data but criticized for its complex API and technical debt.",
      "Comparisons with modern tools like matplotlib, uproot, and Julia are made, noting ROOT's specific advantages and shortcomings, and the potential use of Haskell for interfaces and C++ for performance.",
      "The evolution of ROOT, including its transition to a clang-based codebase, integration with Jupyter, and improvements in code quality, is noted, along with the impact of tools like ChatGPT on simplifying complex APIs and concerns about software quality as Large Language Models improve."
    ],
    "points": 277,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1717226748
  },
  {
    "id": 40541559,
    "title": "WWVB: The Unsung Hero of Accurate Timekeeping in the U.S.",
    "originLink": "https://ben.page/wwvb",
    "originBody": "Ben Borgers Ben Borgers Home Blog Work Projects Newsletter RSS feed Contact Twitter GitHub LinkedIn 60 kHz February 27, 2022 There’s a radio station in Colorado that broadcasts the time. It’s WWVB, the station that the National Institute of Standards and Technology uses to broadcast the current time, tuned to the 60 kHz frequency. Day and night, this station diligently continues broadcasting. 60 kHz is at a “low frequency” part of the spectrum, which means the bandwidth is just enough to broadcast a single digit of binary every second. It takes a full minute just to broadcast the current time. The WWVB station is used by every radio clock in the United States. They read the minute-long broadcast once a day or so, and set themselves to that time. Then, they offset the broadcasted time by whatever time zone you’ve set on the clock, since WWVB doesn’t know what time zone you’re in. That’s how radio clocks accurately know the current time without you needing to set them. They read it from the airwaves. Clever, isn’t it? We needed a way for clocks to set their own time, so the government set up a radio station that broadcasts the current time forever. This kind of thing is exceedingly cool to me. That the solution feels like it’s just neat. I’m not particularly patriotic, but this kind of thing feels particularly American. Perhaps my imagination of American innovation is still set in the era of lunar missions and radio. But we had a problem, and we solved it with technology. And none of that fancy newfangled technology — we solved it using solid technology, the kind that you can touch with your hands and that buzzes in the airwaves. It’s also a reminder that we owe the way the world works to an unimaginable number of invisible people who have come before us. Everything around builds atop the contributions of people, not unlike us, who tried to leave the world a little better than they’d found it.",
    "commentLink": "https://news.ycombinator.com/item?id=40541559",
    "commentBody": "60 kHz (2022) (ben.page)226 points by jer0me 18 hours agohidepastfavorite140 comments tanepiper 11 hours ago> I’m not particularly patriotic, but this kind of thing feels particularly American. Maybe that tinge of patriotism is why Americans don't even bother to see if something isn't an American invention - https://en.wikipedia.org/wiki/Time_from_NPL_(MSF) - they'll just claim it anyways since they barely read outside their borders. Radio 4 would also like a word: https://en.wikipedia.org/wiki/Greenwich_Time_Signal reply ToValueFunfetti 5 hours agoparent>The first radio time signal, sent by telegraphic code, was broadcast in September 1903 by the United States Navy. Most sources list Navesink, New Jersey as the site of this broadcast, but the reference clock was located at the United States Naval Observatory (USNO) in Washington, DC. The first regularly scheduled time broadcasts began on August 9, 1904* from the Navy Yard in Boston. By the end of 1905, the Navy had transmitted time signals from stations in several other cities, including Norfolk, Newport, Cape Cod, Key West, Portsmouth, and Mare Island in California (PDF) https://tf.nist.gov/general/pdf/2131.pdf *Another source said August 9, 1905 reply tanepiper 33 minutes agorootparentYes but it's not \"particularly American\". \"In November 1898, an optical instrument maker and inventor named Sir Howard Grubb addressed the Royal Dublin Society and proposed the concept of a radio controlled clock.\" It was already used in telegraph transmission. America just implemented first. reply zh3 10 hours agoparentprevRadio 4 is particularly interesting, as it carries additional 25bps data [0] using for radio teleswitch operation [1] - a service whose days are numbered. Quite fun to receive this signal, as it's trivial to build a 198KHz receiver and to recover the data stream (+ time signal) using a signal, e.g. mix it with a 200KHz signal and process the resultant 2Khz audio tone. [0] https://www.bbc.co.uk/rd/publications/rdreport_1984_19 [1] https://en.wikipedia.org/wiki/Radio_teleswitch reply tanepiper 6 hours agorootparentRadio 4 is also the station that if there is no communication with mainland UK, and Radio 4 no longer broadcasting, submarine commanders were to assume the country was attacked and destroyed. https://en.m.wikipedia.org/wiki/Letters_of_last_resort reply cjs_ac 10 hours agoparentprevThe US Bill of Rights is copied almost word-for-word from English legislation: https://en.wikipedia.org/wiki/Bill_of_Rights_1689 reply cjensen 3 hours agorootparentInspired by, certainly. The founders were not ignorant and were aware of precedent. But calling it \"almost copied word-for-word\" is not justifiable. On a trivial basis, it's literally not word-for-word in any way. On a more serious basis, there are significant changes in the laws, and enshrining something in an Act which can be changed by future Parliaments is a lot different that enshrining something into a Constitution which is much harder to change. reply CPLX 8 hours agorootparentprevIn fairness, the United States is a breakaway region of Great Britain. reply mixmastamyk 9 hours agorootparentprevA copy of the Magna Carta was in the National Archives for a while, and John Locke was a notable influence on founders such as Jefferson. Nihil novi sub sole. reply pohl 3 hours agorootparentI saw it there the day before yesterday! reply tompccs 8 hours agoparentprevNPL - the National Physics Laboratory in Teddington, London, also invented packet switching, the foundational technology for the internet. https://en.wikipedia.org/wiki/Packet_switching reply benhurmarcel 30 minutes agoparentprevIt’s like apple pie then reply asddubs 1 hour agoparentprevinventing a solution to a problem, only in america reply nabla9 8 hours agoparentprevExisted everywhere in the world since radios became common. I have an old solar powered Casio g-shock multi-band watch and it gets signals from transmitters located in US, Germany, UK, 2X Japan and China. I have never needed to put watch on time, or change batteries. https://gshock.casio.com/intl/technology/radio/ reply lxgr 3 hours agorootparentThese are indeed very cool! If for some reason you are a few standard deviations more paranoid about not having the correct time than the average person, they now apparently have models that can get the current time via GPS, time beacon, or Bluetooth from a connected smartphone :) https://gshock.casio.com/us/technology/3way/ reply new299 10 hours agoparentprevWWVB also broadcasts on 60KHz and a number of other frequencies. WWV (time signal on different frequency) was started a few years before the one you mention. Though I’m not sure it’s really worth arguing about… not sure who was the first radio time signal. https://en.m.wikipedia.org/wiki/Radio_clock reply anyfoo 18 hours agoprev> I’m not particularly patriotic, but this kind of thing feels particularly American. ??? America has no doubt done some very remarkable things (the moon landing, the first backward compatible color TV standard), but I don't think this is a notable example of that. I think everyone in Europe was familiar with even wristwatches getting the time from \"airwaves\". At least in Germany, watches were a very common thing since the 80s (I think everyone old enough vividly remembers the many Junghans commercials on TV). The corresponding signal has been broadcast for many decades earlier, as it's no surprise that railroad and airplane networks were in need of a common time: https://en.wikipedia.org/wiki/DCF77 > But we had a problem, and we solved it with technology. And none of that fancy newfangled technology — we solved it using solid technology, the kind that you can touch with your hands and that buzzes in the airwaves. There wasn't much \"newfangled technology\" around at the time. We essentially solved it using the simplest method there was at the time, at least I find it a bit hard to come up with a simpler one. Also, what would be an example of technology that you can't either \"touch with your hands\" or that does \"buzz in airwaves\"? reply aeonik 18 hours agoparentUS seems to be the first to pioneer this according to my very cursory research: https://timeandnavigation.si.edu/multimedia-asset/us-navy-wi... reply anyfoo 17 hours agorootparentI stand corrected, it seems to indeed have been the US Navy that pioneered time signals. France followed up a few years later. I don’t know who had the first system accessible to civilians, but at this point we’d just be arguing details. reply lloeki 11 hours agorootparentFrom the link: > Radio time signals for setting chronometers were sent from stations such as NAA in Arlington, Virginia, which began broadcasting in 1913. The time was supplied from a direct communication line with the clocks of the U.S. Naval Observatory in Washington, D.C. In combination with another low-frequency installation mounted on the Eiffel Tower in Paris, the towers had the range to cover the North Atlantic Ocean and the eastern United States. So I looked up the Eiffel tower. > The Eiffel Tower time signal broadcasts began on May 23, 1910 https://tf.nist.gov/general/pdf/2131.pdf Also, same source starts with the 1898 proposal of time sync via radio being made by an Irish bloke. > I don’t know who had the first system accessible to civilians, but at this point we’d just be arguing details. Agreed. Even non-civilian, and whoever came first, the timeline of things across various countries, I don't think the core point of TFA holds water: it's not a \"particularly American\" thing, it just made sense out of engineering requirements: time sync was transmitted over telegraph wires, and telegraph went wireless; the leap is not exactly surprising. reply aeonik 7 hours agorootparentNice find! This turns out to be quite an interesting rabbit hole. It's definitely earlier than I would have guessed regardless. reply Aloisius 16 hours agoparentprev> I think everyone in Europe was familiar with even wristwatches getting the time from \"airwaves\". At least in Germany, watches were a very common thing since the 80s Not the 80s. The first radio controlled wristwatch came out in 1990 (Junghans Mega 1). reply anyfoo 15 hours agorootparentSorry, I mistranslated “clock” (hence why I differentiated “wristwatch” in the sentence before). We owned a Funkuhr-Wecker in the 80s, so, an alarm clock synchronizing to DCF77, in the 80s. The wristwatches came a bit later, yeah (not that I’d have known without looking it up whether that was earlier or later than 1990). reply layman51 15 hours agoparentprevI wondered about its American origins too. I vaguely remember a little detail in the novel Cryptonomicon where one of the characters (a Lieutenant in the Imperial Japanese Army during WWII) sets his watch with a radio time signal. So that got me thinking that maybe the idea arose in different places. Also I was wondering about the “forever” wording. I imagine the signal is very robust, but couldn’t American politics just one day decide that it’s not worth it to broadcast the time by radio signal anymore? reply someguydave 16 hours agoparentprevDigital communications is America’s gift to the world, look up Claude Shannon reply spixy 2 hours agorootparentAC motor, steam, diesel, jet engines are european \"gifts to the world\", yet it would be cringe to write sentence like \"I’m not particularly patriotic, but this kind of thing feels particularly European\" reply sokoloff 5 hours agorootparentprevI think the Englishman George Boole deserves significant credit as well. reply Dalewyn 17 hours agoparentprev>what would be an example of technology that you can't either \"touch with your hands\" or that does \"buzz in airwaves\"? Microprocessors and lights/lasers respectively, among I'm certain countless other examples. reply anyfoo 17 hours agorootparentI can touch microprocessors, and lasers are buzzing in airwaves. It’s the same electromagnetic radiation as what we know as radio, just at much, much higher frequencies. But this was asked in the context of solving this problem, in the 1950s. reply pavel_lishin 17 hours agorootparentSure, but trench soldiers used to build their own radios. A soldier today stationed in the middle of a war isn't likely to assemble their own laser emitter. reply anyfoo 17 hours agorootparentI don’t think anybody at that time could have built a laser emitter, even. Not sure why you would want to, to solve that problem, even today. I feel like the point of my question was lost. reply Dalewyn 17 hours agorootparentprev>I can touch microprocessors, No you can't, you're just touching the box they come in. You can touch copper cables and wires, vacuum tubes, resistors, capacitors, breadboards, and (large!) printed circuit boards among other \"basic\" electronic components. You can't touch microprocessors, it's even stated on the tin they are microscopic in scale; you can't touch that in any meaningful way. reply anyfoo 17 hours agorootparentWe could argue about that, but let me rephrase then: What technology that you can neither touch, nor that buzzes in airwaves, could they have used to solve this problem back then? reply tkzed49 17 hours agoprevthe bandwidth is just enough to broadcast a single digit of binary every second. I'd be interested in some more context on this. I think it's pretty clear that you can encode more than 1bps in a 60kHz signal, but I'm curious how the encoding was chosen. There's some more detail on it here: https://en.wikipedia.org/wiki/WWVB#Modulation_format The WWVB 60 kHz carrier, which has a normal ERP of 70 kW, is reduced in power at the start of each UTC second by 17 dB (to 1.4 kW ERP). It is restored to full power some time during the second. The duration of the reduced power encodes one of three symbols: If power is reduced for one-fifth of a second (0.2 s), this is a data bit with value zero. If power is reduced for one-half of a second (0.5 s), this is a data bit with value one. If power is reduced for four-fifths of a second (0.8 s), this is a special non-data \"mark\", used for framing. This is apparently the IRIG H encoding, which dates to the 50s and is probably designed to be easily decoded. By what, I wonder? reply retrac 17 hours agoparentThe signal is not 60 kHz wide. It is at 60 kHz. A pure carrier with no modulation has the inverse of infinite bandwidth. (Assuming perfect transmitters, which can't really exist, of course.) The faster you modulate the signal, the more it spreads out across the spectrum. The frequency-time tradeoff. The time signals run at the low end of longwave. That spans from about 40 to 120 kHz or so. There is about 100 kHz of spectrum down there. Though broadcasting a broadband longwave signal like that would be an imposing task. To cover a large area with AM broadcast (maybe 5 - 10 kHz) uses 1 - 2 megawatt transmitters in Europe. To cover the continent with a 100 kHz broadband digital signal would probably take 10 - 50 megawatts, maybe with 3 transmitter sites. Though I think it could be done. Could fit maybe a megabit a sec in there. Something less ambitious maybe. A few hundred bytes a second? A kilohertz or so of spectrum used. It could work inside elevators, deep underground in parking garages, and so on. Digital alert stream for emergencies, time signal, etc.? Would not be much more complicated than the time signal transmitters, just with a more sophisticated modulation. reply wglb 17 hours agorootparentThere is also an antenna issue with regards to resonance. If the signal spreads to wide, the antenna and associated tuning equipment are going to suffer RF losses, reflections and likely heating. That is why the 17khz naval CW station has a limitation on cw speed. reply lxgr 16 hours agorootparentprevThe agency running the German equivalent to WWVB, DCF77, has looked into piggybacking an emergency warning signal onto unused bits of the time signal: https://www.ptb.de/cms/en/presseaktuelles/journals-magazines... These days though, a more popular idea seem to be to piggy back onto GNSS signals, which are also broadly available (no idea how well they compare in practice to low frequency time beacons), and for which importantly many mobile devices already have a receiver built-in: https://defence-industry-space.ec.europa.eu/galileo-emergenc... The US also already has pretty good coverage with the NOAA weather radio system, which supports a digital, zone-based alerting system for both weather and other hazards. That has the big advantage of being regionally steerable. In fact, I can't imagine many events other than weather (which is usually localized) that warrant such a low-entropy signal on a nationwide basis, and everything I can imagine has very limited actionability. (\"Asteroid inbound, prepare to hide in the basement for a few thousand years\"?) reply zh3 10 hours agorootparentDCF77 is a natural for this because it already includes a phase-modulated signal (as well as the 1bps amplitude-modulation). At the moment it's a 512-bit pseudo random sequence, but could obviously carry a data stream instead. reply morpheuskafka 15 hours agorootparentprev> a more popular idea seem to be to piggy back onto GNSS signals Don’t all GNSS systems provide a highly accurate time signal (maybe needed for computing the distance to satellite)? I know GPS does at least. reply stkdump 14 hours agorootparentAs mentioned somewhere else, these time signals also carry information about DST and adapt for leap seconds, both of which GPS doesn't do as far as I know. I remember that earlier versions of Android always set the phone clock to GPS which was then off by a number of seconds. reply kqbx 9 hours agorootparentGPS navigation messages do include information about leap seconds and other stuff necessary to convert between GPS time and UTC. See GPS Interface Specification IS-GPS-200, 30.3.3.6.2 - UTC and GPS Time reply lxgr 15 hours agorootparentprevYes, the GPS signal in a way is primarily a very precise time signal :) It’s a TDOA system! I was referring to piggying back an emergency signal onto that! reply BenjiWiebe 15 hours agorootparentprevYes, it is needed to compute a location. More accurate time = more accurate location. reply mschuster91 11 hours agorootparentprev> These days though, a more popular idea seem to be to piggy back onto GNSS signals, which are also broadly available (no idea how well they compare in practice to low frequency time beacons), and for which importantly many mobile devices already have a receiver built-in GPS chips are very very power intensive as the compute power involved to decode a signal out of something that would normally be way below the noise threshold is still very high, despite some two decades worth of optimization. And they're pretty useless outside of direct line of sight towards the sky. > In fact, I can't imagine many events other than weather (which is usually localized) that warrant such a low-entropy signal on a nationwide basis, and everything I can imagine has very limited actionability. (\"Asteroid inbound, prepare to hide in the basement for a few thousand years\"?) It's a trigger signal for a low-power receiver that can then turn on a higher-bandwidth (and thus, higher power) device to detect what is actually going on. Say you're a country at war like Ukraine - have the \"emergency bit\" on nationwide during Russian air raids so that receivers can listen for area-specific signals and blare horns if affected. No matter what, we have to prepare for war, and that includes having robust technology that is hard to take out on the sender side (which cellphone stations aren't, they're easy targets in a cyber war!) and, most importantly, can be stored in everyone's garden shed and live on a single battery for years. reply meindnoch 10 hours agorootparent>Say you're a country at war like Ukraine - have the \"emergency bit\" on nationwide during Russian air raids so that receivers can listen for area-specific signals and blare horns if affected. And what would prevent Russia from continuously broadcasting this emergency bit to cause chaos? reply kazinator 14 hours agorootparentprevIf we could have have 58 kHz to 62, that's 4 kHz. Enough for a 300 baud modem to work, which is 300 times faster than 1 bit per second. reply YZF 14 hours agorootparentWe need to know the signal to noise ratio. Not just the bandwidth. 4khz is enough to carry much higher rates on residential phone lines... reply eternauta3k 12 hours agorootparentAlternatively, it would be fun to see how much noise you can put into the signal and still decode it. Maybe using the knowledge of its structure, kind of how (I believe?) GPS reception works. reply YZF 10 minutes agorootparentGPS is spread spectrum. It's modulated using a pseudo random sequence and then demodulated using the same sequence. All the same theory still applies but it's just a different way of using the spectrum, instead of a fixed band the bandwidth is \"spread\" across a broader band in a way that allows all these different signals to be multiplexed. Specifically with GPS IIRC the phase of the pseudo random sequence is also conveying timing information because of how it relates to the precise clock on the satellites. Spread spectrum can be a little more resilient to certain kinds of noise, e.g. if you have noise just at one frequency now you've reduced the impact of that noise. reply nico 14 hours agorootparentprevCould we have something like an AM broadcast, but then the receiver “upsampling” the data? Maybe using some ML? For example, if you have an AM channel that always transmits some song from a list, then you might not even need to receive the whole song. Or maybe you can have a model that’s good at upsampling those types of songs/data? reply mlyle 14 hours agorootparentThis is pretty much what audio compression, like a MP3 file, does: it figures out how to encode perceptually-identical (or close) stuff more densely and remove redundant information. Every time you listen to \"HD Radio\" you're doing exactly this. reply vlovich123 13 hours agorootparentYes, but modern ML techniques (particularly with neural networks) are much more impressive at recovering signal with even less information than things like MP3. An example of this in the visual domain is upscaling. So it’s not a bad question at all. reply mlyle 12 hours agorootparentSure-- as we've discussed endlessly here: compression and ML are really very close cousins. I never said it was a bad question. reply keithalewis 11 hours agorootparentYou can even win a prize for that https://en.m.wikipedia.org/wiki/Hutter_Prize. reply vlovich123 3 hours agorootparentI extremely dislike the Hutter Prize. I think it's generally agreed that intelligence compresses the model better & therefore it's reasonable to assume that the better the compression, the more intelligence you have (at least for now where we are with AI, this seems to be a roughly reasonable assumption). My problem with the Hutter prize is that it makes the leap to say that lossless compression is this when everything about AI is choosing how much error you're willing to tolerate. In other words, intelligence is linked to lossy compression, not lossless - a competition for lossless compression doesn't do anything more than get us better lossless compressors / decompressors. reply YZF 14 hours agorootparentprevAM broadcast uses a \"band\" around the carrier. Whatever tricks you use Shannon's law is going to determine the upper limit of how much data you can carry which depends on the bandwidth and the signal to noise ratio. ML makes no difference from a theoretical perspective. Compression just reduces the bandwidth you'll need assuming your data compresses (with whatever method you choose, ML can be part of that)... reply nico 14 hours agorootparentI guess my usual experience is that AM transmissions sound pretty horrible, especially compared to FM So, my question is more like, can we just upgrade the equipment and keep the channels? And can AM be used more like for signal/indexing rather than full data dump? (eg. instead of transmitting the whole song, transmitting just an id of the song) reply vlovich123 13 hours agorootparentAM sounds bad because it’s amplitude modulated. So variation in amplitude (i.e. signal strength) results in noise when decoding. FM however is not sensitive to this issue because its frequency modulated - slight variations in signal strength have no effect on the encoded signal. And yes, we can always upgrade the equipment. That’s what we did when we transitioned TV to digital signals and FM to digital FM - we reserved some bandwidth for over-the-air while freeing up the rest to be used for other purposes. But it’s an expensive proposition and one that happens rarely (in fact those are the only two times I’m aware of it, they happened at basically the same time, took forever, & there was insane opposition to it). reply YZF 13 hours agorootparentprevThe limit is Shannon's law which depends on bandwidth and signal to noise. A more advanced modulation technique can get more \"information\" through up to that limit. A song for this matter can just be considered information as well. So the answer is almost certainly yes, one can get more out of existing channels by upgrading the equipment assuming the current equipment doesn't approach the theoretical limit which is almost certainly the case for this \"old\" technology we're talking about. An FM radio channel has a bandwidth of 200Khz. AM radio has a bandwidth of of 10Khz. That's a big part of the reason why AM sounds horrible compared to FM. The way noise impacts AM and FM is also different, especially given human perception. IIRC both these methods were picked for practical reasons not necessarily any optimal criteria. AM is fairly crude, there's only so much you can change the amplitude for any practical purpose. Changing the frequency makes it easier to use more bandwidth so it was a natural evolution. Depending on how you're modulating the amplitude (the naive old school radio IIRC is just using the signal you're transmitting to modulate it) you can make the bandwidth larger or smaller. Random e.g. something like Ethernet on copper can be viewed as a very very fast, high bandwidth, AM. All these things boil down to the same Shannon law just with somewhat different efficiencies depending on how well you can control the frequency domain. This problem of cramming the most bits into a given channel is something people have been working on for a long time. Some coverage of this here: https://en.wikipedia.org/wiki/Modulation That's how we got from 300bps modems to 56k modems over the same phone lines way back and how we push more out of radio spectrum today. reply cycomanic 10 hours agorootparentOne thing you forgot to mention is that in contrast to an FM receiver, or even more a digital one, AM receivers are incredibly easy to build even with very basic \"household items\", which I believe is at least part of the motivation to provide emergency services and similar broadcasts over AM. reply Dylan16807 12 hours agorootparentprev> An FM radio channel has a bandwidth of 200Khz. How gratuitous with modern tech. FM needs tens of decibels of signal to noise ratio to sound reasonable, especially if you want stereo. But a digital signal could fit perfect audio into that range at less than one bit per Hz, which would let it tolerate radio noise that's as strong as the actual signal. reply wglb 17 hours agoparentprevBy \"atomic\" clocks (for example https://www.amazon.com/Crosse-Technology-WT-3129B-Atomic-Ana...), and older brands of watches. I think there are also older industrial applications using this technology to syncronize. reply metaphor 16 hours agorootparent> ...and older brands of watches. Modern watches too, e.g. Casio digital watches marketed as integrating their \"Wave Ceptor\"/\"Multi-Band 6\" feature[1]. I still use a 17-year-old (batch code[2] 202A327G) G-Shock GW-530A (predecessor of this active offering[3]) that syncs with WWVB every morning several hours before dawn. [1] https://en.wikipedia.org/wiki/Casio_Wave_Ceptor [2] https://shockbase.org/watches/batchcode.php [3] https://www.casio.com/us/watches/gshock/product.GW-M530A-1/ reply anaidioschrono 17 hours agorootparentprevI love these clocks! I've had a few in every house I've lived in, from all along the West Coast and Midwest. reply lxgr 16 hours agorootparentSince the equivalent European signal I'm familiar with only covers a single timezone, with unified DST rules, I'm curious: Do these usually have an adjustment button for a time zone offset? And how do they handle DST – do they have an \"I'm in Arizona (but not in the Navajo Nation), leave me on standard time\" switch? reply V99 16 hours agorootparentThey usually have an offset and a DST option. So in Arizona you'd choose -7/Off, and never touch it again. In Colorado you'll have to toggle DST twice a year manually, because it doesn't know the rules for when it starts/ends. reply o11c 13 hours agorootparentYou shouldn't have to toggle DST manually; it's encoded at :57-58. That said, my clock stopped syncing for some reason, so ... reply lxgr 15 hours agorootparentprevOh, is there no “DST in effect” flag in the signal? As I understand it, the US switches at the same day across the country (for the states that do have it), so that should theoretically be possible. reply joezydeco 1 hour agorootparentThe time code format does include a DST bit. https://www.nist.gov/pml/time-and-frequency-division/time-di... reply quintushoratius 8 hours agorootparentprevNot every region of the continental US respects DST. reply lxgr 6 hours agorootparentI know, but if it does, it switches on the same day as all the others, which could be encoded in the signal, and it seems like it is [1]. So instead of a switch to “add an hour”, there could be one that says “add an hour if the signal says DST is currently in effect”. [1] https://en.wikipedia.org/wiki/WWVB#Amplitude-modulated_time_... reply esaym 15 hours agorootparentprevWow, added to wish list! reply zh3 10 hours agoparentprevIn fact there are lower-frequency signals that carry more data, generally used for submarine communication - the wikipedia article says \"uo tp 300bps\" using VLF (30KHz or so). Fun fact: sound cards in PC's are now fast enough that it's possible to receive many of these signals (including WWWV, DCF77 and MSF) directly with a sound card - basically connect a long wire (or tuned circuir) to the sound card input and do a bit of DSP. Sampling at 192KHz makes reception of any of these easy. [0] https://en.wikipedia.org/wiki/Communication_with_submarines reply mmastrac 17 hours agoparentprev> By what, I wonder? At that length of symbol encoding, by hand. :) reply throwing_away 15 hours agoparentprev> This is apparently the IRIG H encoding, which dates to the 50s and is probably designed to be easily decoded. By what, I wonder? Wonder no more! This technology is used in wall clocks, weather stations, and even wrist-watches. They're usually marketed as \"atomic clocks\". reply esaym 15 hours agoparentprevFor those of us that are visual or audio learners: https://www.youtube.com/watch?v=V0CWj9DwRBY reply esaym 15 hours agorootparentAnd a \"coverage map\" https://www.nist.gov/pml/time-and-frequency-division/time-di... reply russdill 16 hours agoparentprevReminds me of encodings for IR remote controls such as RC5 reply anyfoo 17 hours agoparentprevRelays maybe? EDIT: Not sure why I was downvoted. Relays seem like period-accurate technology (happy to be corrected), and they’re comparably slow. reply Dylan16807 12 hours agorootparentI can't say why you were downvoted but that's still pretty slow for relays. One bit per second seems more like it's aimed at casual human decoding. Especially because each digit is encoded separately, even for the hours. Also for plain old range purposes, one transmission per minute is about as wide as you can go before things get silly. reply Liftyee 1 hour agoprevCould someone please explain why the low frequency means the bandwidth is limited to 1 bps? Why can't the signal be turned on and off more times per second to transmit at a higher rate? reply sllabres 16 hours agoprevIf I think about time an American nowadays the first thing that comes into my mind is GPS. What WWVB or DCF77 (and JJY) was back then is now GPS timing capabilities. Much higher precision but of course a higher complexity for the receiver. [1] I don't know about WWVB and JJY but the German DFC77 is not only AM modulated but FM too allowing for a better accuracy. [1] https://www.hopf.com/dcf77-gps_en.php#chapter3 reply 2four2 18 hours agoprevThere are several of these stations, they are maintained and operated by NIST. Two of them were almost shut down recently due to budget concerns: https://www.radioworld.com/global/why-wwv-and-wwvh-still-mat... reply dfc 17 hours agoparentIn April one of the two WWVB antennas failed (broke) because of 90mph winds. https://www.nist.gov/pml/time-and-frequency-division/time-di... reply Eduard 17 hours agorootparenthttps://www.nist.gov/pml/time-and-frequency-division/time-di... > Official Notice: Commencing from 0000 Coordinated Universal Time (UTC) on April 7, 2024, the southern antenna of WWVB has been rendered non-operational due to damage sustained from wind gusts exceeding 90 MPH. Please be advised that WWVB continues to function at a diminished overall power, utilizing only its northern antenna. > Update 20 May 2024: The components necessary for the refurbishment of the southern antenna’s triatic are currently being manufactured and shipped. The projected timeline for the completion of these repairs is tentatively set for the latter part of June 2024. We would like to emphasize that this is an estimated timeline and may be subject to alterations based on a variety of factors. We greatly appreciate your understanding and patience during this process. Can someone using WWVB radio clocks describe what the current situation is? Is the second working antenna strong enough to provide signals throughout the US / North America? Are WWVB radio clocks popular at all? Here in Europe, DCF77-based clocks are popular, especially with respect to automatic summer time adaption and (less obvious) leap second HANDLING. It would be very noticeable eventually if the DCF77 signal had severe issues. Also, I'm very surprised about the abysmal historical uptime of Wwvb. It lists thirteen downtimes of more than 5 minutes each in 2023 alone. reply lxgr 16 hours agorootparentGiven that most clocks listen for it about once per day, if that, to save battery, and in my experience even then only manage to catch the signal every couple of days, high continuous availability doesn't really seem to be a concern. reply asix66 13 hours agoprevI wear a Citizen “AT” [1] wristwatch daily. It not only keeps perfect time, because of its daily sync at 2am to WWVB, but it also never needs a battery change since it self charges from light. These two features make it the perfect zero maintenance timepiece IMHO, and it looks stylish too. [1] https://www.citizenwatch.com/us/en/collection/mens-atomic-ti... reply jgrahamc 4 hours agoprevThe WWVB transmitter sometimes has surprisingly long propagation: https://blog.jgc.org/2021/07/receiving-wwvb-time-signal-in-p... and https://blog.jgc.org/2023/03/wave-ok-wwvb-time-signal-sets-m... reply belter 3 hours agoprevIf you go below 300 Hz, you are probably into some hardcore top secret stuff: https://en.wikipedia.org/wiki/Communication_with_submarines Assuming salt water, a 1024 Kb message should take around 3 hours to transmit. reply esaym 1 hour agoprevI see there are cheap indoor wall clocks that are syncd with WWVB. But can't find anything that use gps instead. I am wondering why? Are the receivers too power hungry or the signals just too weak? I'd think going with a gps time sync would be a more future proof design. reply LeoPanthera 14 hours agoprevSadly, WWVB is damaged. At midnight on April 7, 2024, WWVB's south antenna was disabled due to damage sustained during high winds. WWVB now broadcasts exclusively from the north antenna, at a reduced power of 30kW. This situation is expected to continue \"indefinitely\", presumably because of a lack of budget for repairs. reply michaelsshaw 14 hours agoparentImportant to note the update: Update 20 May 2024: The components necessary for the refurbishment of the southern antenna’s triatic are currently being manufactured and shipped. The projected timeline for the completion of these repairs is tentatively set for the latter part of June 2024. We would like to emphasize that this is an estimated timeline and may be subject to alterations based on a variety of factors. We greatly appreciate your understanding and patience during this process. reply jimbobimbo 14 hours agoparentprevThis looks like a great candidate for crowd funding. reply blt 3 hours agorootparentyes, in this case I'd say a good crowd funding platform is the IRS. reply kwhitefoot 2 hours agoprev1 bps on a 60 kHz carrier? That is certainly not the maximum possible data rate. There are AM stations broadcasting at only two or three times that frequency that broadcast voice and music. Perhaps they meant that it is the most practical for small receivers like watches. BBC R4 Longwave is at 198 kHz (formerly 200 kHz). The transmitter at Droitwich has been broadcasting voice and music since 1934. reply evanjrowley 3 hours agoprevEarlier this week I purchased a Casio Waveceptor watch. It features the ability to synchronize with this radio signal. I'm very happy with it. I was also happy with an early-2000s model for several years prior. The older one's receiver seemed to become unreliable after about a decade, possibly due to changes in the wireless spectrum over time. Overall, it was quite a reliable watch and so I have good confidence in this product line. reply bb88 14 hours agoprevFor those looking to hear what this sounds like without buying any radio gear, there are many public kiwisdr sites that can hear 60khz. It's a matter of choosing AM decoding and tuning to the particular frequency. http://kiwisdr.com/public/ Many of these have user limits and time limits as well. Note that you can also hear WWVB on 2.5mhz, 5mhz, 10mhz, and 15mhz. reply misstuned 10 hours agoparentYou can also pick \"timecode\" from the extensions menu in the KiwiSDR interface and actually decode the time signal live on the receiver. reply mikewarot 14 hours agoprevI have a friend with a 300' longwire antenna. A few weeks ago, after we were done repairing an old receiver that included VLF, he tuned in WWVB for me... checking it off my bucket list. It was interesting to hear the beeps of different lengths. It came in surprisingly clear, in our Chicago suburb, especially since this was after the antenna issue on their end. reply anonymousiam 15 hours agoprevWWVB data transmission format is described here: https://www.nist.gov/system/files/documents/2017/05/09/NIST-... reply lxgr 16 hours agoprevI remember my dad getting a solar-powered wristwatch that could tune in to the Western European equivalent of WWVB (DCF77, located in Germany) and adjust itself (respecting DST!) when I was a teenager. Not that I'd regularly suffer from empty watch batteries or horribly incorrect time on my quartz watch, but I always found that extremely neat :) reply vzaliva 15 hours agoprevDecoding these signals should be a fun 1st project for Software Defined Radio. Google search shows several projects which did just that. reply kqbx 10 hours agoparentCan confirm, it's a great way to get started with radio stuff. Since I live in Europe, I did this with the German DCF77 signal. The frequency is low enough that you can do direct sampling with a 192ksps sound card or with an RP2040. A nice thing about DCF77 is that once you manage to receive the amplitude modulated signal, you can move onto decoding the slightly more advanced spread-spectrum phase modulated signal, which carries (almost) the same data. You can start by buying a time signal receiver module, and once you confirm that the module can receive the signal, cut off the ferrite antenna and use it with your own receiver. Here's a nice article on which I based my own project: https://hal.science/hal-02182845/ Using the same setup (ferrite stick -> discrete transistor amplifier -> RP2040) I also managed to receive the polish AM station at 225kHz. reply dinoqqq 9 hours agoprevAmerican patriotism set aside, I share the author's enthusiasm for these types of technology! The post's main focus on how we stand on the shoulders of giants is so true! It makes me happy and proud as well. Another fascinating technology that we take for granted, is the possibility to travel with 280km/h in a train and still be able to receive digital bits and bytes. What kind of witchcraft would our grand-grand-grandparents have called that? reply khaki54 17 hours agoprevI can picture a room full of engineers in ties, wearing pocket protectors full of implements, chain smoking cigarettes while coming up with this one. reply mmastrac 17 hours agoparentMore likely a radio nerd, John Alvin Pierce, that was just really good at radio and obsessed with oscillators (https://ieeexplore.ieee.org/document/60675) that inspired a group of distinguished government-employed engineers and scientists. In fairness, there _might_ be a pocket-protector in this picture of them but it's a bit blurry. Actually, yes, there definitely is. No cigarettes, however: https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/t... on image to zoom&p=PMC3&id=4487279_jres.119.004f7.jpg [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4487279/ reply metaphor 16 hours agorootparentQuick image link fix...spaces in URL broke auto-hyperlinking: https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/t... reply denton-scratch 10 hours agorootparentDamn, he's got two pocket protectors! reply erikpukinskis 17 hours agoparentprevI can picture a black woman at a desk in a nicely pressed dress coming up with this one on pencil and paper. reply neilv 17 hours agorootparenthttps://en.wikipedia.org/wiki/Hidden_Figures reply anArbitraryOne 14 hours agoprevI like the topic very much, but why did this very basic blog post get so many upvotes? reply nicbou 10 hours agoparentIt introduces something new that tickles our curiosity. Not everything needs to be a long read. I do like short posts. reply tempodox 12 hours agoprev60 kHz is “long wave”, a frequency so low you could almost still hear. It's just thrice the upper limit of ~20 kHz for the human ear. I love how my wristwatch eats sunlight at day (solar-powered) and drinks radio waves at night. Never has to change a battery and shows time as accurately as any internet-connected device that listens to NTP. reply squarefoot 12 hours agoprevFor the makers here, you can purchase cheap small modules that interface to any microcontroller, including most popular ones like AVR (Arduino), ESP32, etc. so that they can receive and decode time signals from WWVB or the European equivalent DCF77. Just search for \"WWVB (or DCF77) module\" at the usual online shops. reply barrenko 6 hours agoprevWonderful post, thank you. reply frankus 17 hours agoprevIt's kind of baffling that in 2024 there are appliances that exist for which I have to manually set the time, or at least have to go through some cumbersome IoT set-up process, that most likely is absolute garbage security-wise if it isn't actively acting against my interests (like those fridges that only work with first-party water filters). Maybe I'm under-thinking this, but couldn't my Wi-Fi router just broadcast the current time (and time zone) every minute or so in an unencrypted fashion to all devices within range? reply ianburrell 1 hour agoparentI have thought that Matter should have a time broadcast message that can be sent over Wifi, Bluetooth, and Thread. Which should be enough to set the time for any Wifi device that doesn't need NTP, and low-power devices with Bluetooth and Thread. One problem is that need to pair with Wifi, Bluetooth, and Thread. The other problem is the cost to add time sync. And probably couldn't remove the UI for setting the clock because need it for people without technology. reply lxgr 16 hours agoparentprevIt could – but it could just as easily (either accidentally or on purpose) broadcast the wrong time, greatly annoying everybody within broadcast distance. Of course the same is also possible for WWVB, but at least there, I suspect that some agencies might take objection and convince you to stop sooner or later. reply alright2565 16 hours agoparentprevIt is baffling that in 2024 that my appliances don't have a battery backup for the time. It costs less than 50 cents and requires the battery be changed no more than every 20 years. But still every time the power goes out I need to reset my clocks. reply denton-scratch 10 hours agoparentprev> appliances that exist for which I have to manually set the time I've never owned an electric oven that could configure its own time. Anytime there's a power cut, I have to do some crazy dance - which usually involves digging out the oven's manual. It's the same for microwave ovens. I don't know why they don't at least store the time in a scrap of NVRAM, like a PC motherboard. reply em3rgent0rdr 13 hours agoparentprevNTP (https://en.wikipedia.org/wiki/Network_Time_Protocol) can be run on any OpenWRT router. Too bad it isn't a standard feature of every router and appliance. reply joshellington 17 hours agoparentprevYes but then that would require consensus on protocol, encoding/decoding, etc., aka the things large companies are the worst at aligning on. reply pavel_lishin 17 hours agorootparentPlus you have to defend yourself from the neighborhood kid who thinks that it'd be a laugh riot to randomize everyone's clocks at 5:15 in the morning. reply joshellington 17 hours agorootparentThat would have absolutely been myself, wardriving at 16 (back in the early aughts). Sending nonsensical print jobs and then setting everyone’s clocks to 4:20. reply MadnessASAP 16 hours agorootparentI'm in my mid 30s and I still regularly tell unattended Google Homes and Alexa's to set an alarm for 4 AM. reply pavel_lishin 4 hours agorootparentI'm not that malicious, I just like picking up friends' unattended phones and snapping a goofy selfie for them to discover later. reply z3phyr 14 hours agorootparentprevThere is only one government reply Talkative32 2 hours agorootparentThey servant bodies represent past abilities recent demonstrate open world reply mehulashah 15 hours agoprevWell said. Simple, solid technology for the benefit of everyone. reply rkagerer 16 hours agoprevI looked into getting a wristwatch that uses this. reply kristofferR 16 hours agoprevThis got me thinking of this video: https://www.youtube.com/watch?v=IbUhA6Vt6Sk \"We just lost the most powerful longwave transmitter in Europe, and why that's kind of a big deal\" (2023) Slight Technology Connection vibes, really fascinating stuff. reply mlindner 13 hours agoprevThis is completely incorrect. Time synchronization across the entire US is done via GPS, not radio signals. reply systemvoltage 15 hours agoprevSlightly tangent: being substantively patriotic is cool. It should be cool. America has done some incredible things like this and I’m sure it’s not just Americans that find it cool. reply servus45678981 10 hours agoparentBlind patriotism isn‘t cool. Most of those inventions were invented by multicultural people, so it has really not so much to do with the country. reply nocoiner 15 hours agoparentprevWell said. I couldn’t agree more. I think there are a lot of countries that have done some really remarkable things. I’m American, and I’m generally very proud of my country. If I were, say, French, Swedish or Japanese, I’d feel the same way. Obviously none of those countries - or any country - is perfect, and all have some distinctly dark chapters in their past, but there’s something inspiring about a society organizing itself to accomplish generally good things. reply servus45678981 10 hours agorootparentI honestly think pride only makes sense if the place feels more than home to you; the people always were more important than some man-made borders and identities. reply pcdoodle 15 hours agoprevCan you use this in basically the lower 48 with a long antenna? Free RTC L48? reply FrustratedMonky 17 hours agoprev\"\"I’m not particularly patriotic, but this kind of thing feels particularly American. Perhaps my imagination of American innovation is still set in the era of lunar missions and radio.\"\" Yes. From the first trans continental train, telegraph, up through landing on the moon. It does inspire a lot of patriotism, it was very 'can-do' time. Very innovative, huge public works. The old technology is still amazing. reply wayeq 17 hours agoparentNot to mention the voyager missions.. still zooming out into interstellar space.. reply andrewstuart 17 hours agoprev [–] Pretty sure there’ll be lots of comments on this to do with bandwidth and speed of light. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WWVB is a radio station in Colorado operated by the National Institute of Standards and Technology, broadcasting the current time on a 60 kHz frequency.",
      "This low-frequency signal enables radio clocks across the U.S. to set themselves accurately by reading the broadcast daily and adjusting for time zones.",
      "The post emphasizes the cleverness and innovation of this technology, underscoring the significance of past contributions to modern conveniences."
    ],
    "commentSummary": [
      "The discussion traces the origins of radio time signals, starting with the US Navy's first broadcast in 1903 and earlier proposals by Sir Howard Grubb.",
      "It examines the evolution of time synchronization technology, including the use of radio signals and the introduction of radio-controlled wristwatches.",
      "The conversation also explores the potential of GNSS (Global Navigation Satellite System) signals for emergency alerts and the technical aspects of low-frequency signals for timekeeping."
    ],
    "points": 226,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1717200336
  },
  {
    "id": 40545436,
    "title": "Napster's Legacy: How a 1999 Revolution Shaped Today's Music Streaming Industry",
    "originLink": "https://torrentfreak.com/napster-sparked-a-file-sharing-revolution-25-years-ago-250601/",
    "originBody": "HOME > PIRACY > On June 1, 1999, the first public release of Napster launched online, kick-starting a global piracy frenzy that never disappeared. At the same time, it can be argued that the file-sharing software paved the way for legitimate business models that would eventually evolve into subscription-based platforms such as Spotify and Netflix. The invention of the MP3 format in 1993 didn’t make any mainstream news headlines. In hindsight, however, it was a pivotal moment that would revolutionize music consumption, and more. Invented by the German engineer Karlheinz Brandenburg and colleagues at the Fraunhofer Society, the coding format made it possible to reduce the size of music files without any significant loss of audible sound quality. Due to the size reductions, these digital files could be stored on flash-memory devices. This led to the invention of dedicated MP3 players capable of playing music ripped from CDs. Many considered this a more compact and shock-resistant alternative to the Discman. At the time, music industry insiders were already fantasizing about the ‘celestial jukebox’; a tool or service that would make it possible to play any track on demand. The MP3 helped to bring this concept a step closer too, as Napster would soon prove. Napster: June 1, 1999 At the end of the nineties, technology and the Internet were a playground for young engineers and ‘hackers’. Some of them regularly gathered in the w00w00 IRC chatroom on the EFnet network. This tech-think-tank had many notable members, including WhatsApp founder Jan Koum and Shawn Fanning, who logged on with the nickname Napster. In 1998, 17-year-old Fanning shared an idea with the group. ‘Napster’ wanted to create a network of computers that could share files with each other. More specifically, a central music database that everyone in the world could access. This idea never left the mind of the young developer. Fanning stopped going to school and flanked by his friend Sean Parker, devoted the following months to making his vision a reality. That moment came on June 1, 1999, when the first public release of Napster was released online. Soon after, the software went viral. Napster was quickly embraced by millions of users, who saw the software as something magical. It was a gateway for musical exploration, one that dwarfed even the largest record stores in town. And all for free. It sounds mundane today, but some equated it to pure technological sorcery. For many top players in the music industry, Napster’s sorcery was pure witchcraft. At the time, manufacturing CDs with high profit margins felt like printing money and Napster’s appearance threatened to ruin the party. Music Industry Shocked According to the RIAA’s former CEO, Hilary Rosen, a few months after Napster’s release, the music industry shifted into full panic mode. In February 2000, all major label executives discussed the threat during an RIAA board meeting at the Four Seasons Hotel in Los Angeles. “I will never forget this day. All of the heads of the labels, literally the titans of the music business, were in that room. I had somebody wheel in a PC and put some speakers up and I started doing a name that tune,” Rosen later recalled. The major music bosses started to name tracks, including some that weren’t even released yet, and time and again Napster would come up with results. Needless to say, the board was terrified. Within a year, the RIAA sued Napster Inc. and soon after artists including Metallica and Dr. Dre followed. These high profile cases only raised the popularity of Napster and MP3 players began to sell like hotcakes. Peak Napster At the start of 2001, Napster’s user base reached a peak of more than 26.4 million worldwide. Yet, despite huge growth and backing from investors, the small file-sharing empire couldn’t overcome the legal challenges. The RIAA lawsuit resulted in an injunction from the Ninth Circuit Court, which ordered the network to shut down. This happened during July 2001, little more than two years after Napster launched. By September that year, the case had been settled for millions of dollars. While the Napster craze was over, file-sharing had mesmerized the masses and the genie was out of the bottle. Grokster, KaZaa, Morpheus, LimeWire, and many others popped up and provided sharing alternatives, for as long as they lasted. Meanwhile, BitTorrent was also knocking on the door. Ripple Effect Today, 25 years later, music piracy certainly hasn’t disappeared, but it has changed. When Napster came out, there simply weren’t any legal options to buy digital music online; let alone one that offered ‘unlimited access’. Napster paved the way for Apple’s iTunes store, to serve the demand that was clearly there. The boom in digital download sales never came close to mimicking the ‘all you can play’ experience and was soon marginalized. The current music industry generates the bulk of its revenues from online streaming subscriptions, while CDs have been downgraded to rare artifacts. This music streaming landscape was largely pioneered by a Napster ‘fan’ from Sweden, Daniel Ek. Like many others, Ek was fascinated by the ‘all you can play’ experience offered by file-sharing software, and that planted the seeds for the music streaming startup Spotify, where he still serves as CEO today. In fact, Spotify itself used file-sharing technology under the hood to ensure swift playback. Spotify is just one of the many examples of the Napster ripple effect, which reaches far beyond technology. The entire music industry has changed, for better and worse, depending on one’s perspective. And the ripples that started 25 years ago will still be felt in the decades to come.",
    "commentLink": "https://news.ycombinator.com/item?id=40545436",
    "commentBody": "Napster sparked a file-sharing revolution 25 years ago (torrentfreak.com)213 points by luuurker 5 hours agohidepastfavorite177 comments mmh0000 4 hours agoNapster was such an improvement over what it replaced, but, it’s funny by today’s standards Napster was so “basic”. I remember waiting 3+ hours for a single song to download. Then discovering it wasn’t what I wanted but a troll who renamed the `Barney The Dinosaur I love you` song. Then I’d spend another 3+ hours downloading a different song. Ah 56k internet, what fun. Today, TPB and a quick search can give you an artist’s entire discography in one go. Or if you’re into automation, lidarr , sonarr , and radarr can pull in your favorite things as soon as they’re released. What I find most strange about the modern day piracy is quality. It blows my mind how different groups fight to offer the best version of a free thing. And they’re so good at it, that the pirated product is usually substantially better than the official version. reply nvarsj 4 hours agoparent> What I find most strange about the modern day piracy is quality. For a lot of pirates, it's a niche hobby to preserve the best quality version of some media / art. I see it as an underground librarian archivist movement. In 100 years iTunes may be dead, but there will be some person with every album available in high quality lossless FLAC. Same with Netflix and it's bit starved encodes. reply pavlov 3 hours agorootparentIndeed. Many of these pirates are the most devoted fans of the music they store and distribute. They go to gigs, buy merchandise and vinyl editions, and otherwise support the artists directly. In contrast companies like Spotify don’t care about the music at all. They would just as well sell you white noise if it would somehow give them the slightest improvement in growth or margins. Daniel Ek, CEO of Spotify, regularly makes statements that suggest he’d rather see professional musicians disappear. Most recently: https://www.nme.com/news/music/music-fans-and-artists-hit-ba... reply cortesoft 47 minutes agorootparentSpotify might not care as much about the music and the artists as pirates, but they sure pay the artists more. reply AlexandrB 36 minutes agorootparentBarely. Some artists outright tell you to pirate their music since most of their money comes from touring/merch. reply buildsjets 1 hour agorootparentprevFor indie artists, the vinyl release frequently comes with a download code for a digital version. Some bands even include lossless versions. reply listenallyall 1 hour agorootparentprev> Many of these pirates are the most devoted fans of the music ...but most are simply pirates, completely detached from the music or the artist. Let's not glorify them. reply rickdicker 1 hour agorootparentPresumably if you're pirating music you aren't \"completely detached\" from the music - otherwise why would you do it? reply ineptech 35 minutes agorootparentIn the early 80s, the local pirate scene in my hometown revolved around a guy who I'll call George. George's entire basement was devoted to boxes full of diskettes (5 1/4\" in those days) of pirated software and photocopied documentation. All kinds of software - games, office apps, scientific stuff, you name it. Some was downloaded from BBS's but the majority of it was shipped USPS from god knows who. Thing is, he used virtually none of it. He collected software for the sake of collecting it. He didn't even play video games, just loaded them up once to make sure they ran. He was a hoarder basically, who had stumbled into a niche hobby, and like most hobbyists he would happily share it with anyone who asked. I'm not saying every pirate is like that, or even most, but I am saying, I don't think the pirate scene works without people like that. The music fans are spokes, but people who do it for the sake of doing it are the hubs. reply al_borland 23 minutes agorootparentFor the hoarders that don’t share, I don’t really see much of an issue. Sure, they have all that stuff, but they would have never bought it. It’s not actually a lost sale. reply dasil003 41 minutes agorootparentprevWait, what? There are so many assumptions packed into this statement. Why should we view things through the lens of whether we are glorifying pirates or not? Is your assertion we should be condemning them? Why? Is it because they are taking bread out of the mouths of musicians? Is it equally bad if Spotify uses their leverage to commoditize and devalue music so musicians can no longer make meaningful money from recorded music? What about music labels who structure deals so they get the bulk of the money for a fraction of the work? I'm curious to understand the principles you hold that give you such a black and white view on piracy. reply anal_reactor 54 minutes agorootparentprevThese two are not mutually exclusive reply acchow 4 minutes agorootparentprevStill frustrated that 1080p bluray looks and sounds significantly better than 4k Netflix. Will a lot of quality stuff be lost forever when these companies disappear? reply radley 2 hours agorootparentprev> Same with Netflix and it's bit starved encodes. Bitrates are tied to the subscription tier. If you do the 4K plan, it's visibly noticeable that 1080p video bitrates are higher. They don't have much 4K content, but the 1080p difference is worth it for my 4K projector. reply sargun 1 hour agorootparentSubscription tier and device. If your device isn’t capable of playback, there’s no way to get the content. reply eisa01 3 hours agorootparentprevIn addition, quite a lot of the backlog has never been released on iTunes I wanted to complete a set of DJ mixes, and had to purchase them on Discogs to as there were no digital copies, one cost me EUR50 for one CD... reply bratwurst3000 3 hours agorootparentHahahaha I had the same problem. Had to buy a vinyl for 80€ and digitilize it. Did upload it then. Now the artist has a bandcamp with the collection for free so sharing is caring reply sharkweek 54 minutes agorootparentprev>every album available in high quality lossless FLAC. In college I ate up an entire hard drive downloading the bootleg discography of Pearl Jam live shows in the hugest quality I could find. Not sure why I needed 50+ live versions of the song Daughter or Yellow Ledbetter but… I had ‘em reply satvikpendem 1 hour agorootparentprevYep, that is why I pirate as well, movies and shows are simply lower quality when streaming than via direct downloads. Plus, I can play them with my own media player and use software like SmoothVideoProject to interpolate the frame rate and use upscalers for sub-4k content. reply joshuaturner 1 hour agorootparentprevThis is especially true with the current wave of upscaled 4k releases that are, frankly - awful. Eventually, these will be the only versions we have access to on streaming platforms, but someone will have the original Blu-ray remux or DVD rip on their Plex server. reply jorvi 2 hours agorootparentprev> In 100 years iTunes may be dead, but there will be some person with every album available in high quality lossless FLAC. Same with Netflix and it's bit starved encodes. Yeah, nah. WhatCD’s full catalogue has never been recovered. reply eisa01 1 hour agorootparentBut the successors still have stuff iTunes don't have, and there's even new uploads of old releases that What didn't have ;) reply WarOnPrivacy 1 hour agorootparentprev>> some person with every album available in high quality lossless FLAC > nah. WhatCD’s full catalogue has never been recovered. The loss of WhatCD's torrent database didn't wipe out anyone's library. reply exe34 3 hours agorootparentprevalready, Disney has been censoring past episodes not even based on today's sensibilities but it seems purely based on keywords. there's an episode of the suite life of zach and cody where zach pretends to have dyslexia in order to get extra time in his exams, and near the end of the episode he gets busted and they have a thoughtful conversation about how it's unfair and hurts the people who need the extra time, and he should be less lazy and do the work, etc, and somehow that's too offensive now. reply cyanwave 1 hour agorootparentMy hope is human nature will prevail. SOME are that sensitive but I think there’s a growing amount of people maybe even a majority that don’t want censorship and nannie’s in media / comedy etc. reply supafastcoder 11 minutes agoparentprevI remember burning CD's full of 1 minute songs that never completed because someone was always calling our house line, disrupting the internet connection (and there were no resumable downloads). Good times ;) reply jakupovic 6 minutes agoparentprevI also remember having cable modems and parties with Napster station where people would find download and then play the song of choice. No sign-ups no artificial borders just music. This is still not possible... reply TheAceOfHearts 2 hours agoparentprevGosh this is such a throwback... One of my first experiences with Napster was trying to download Britney Spears' Oops I Did It Again and getting a shitty remix. The worst part was that I didn't check the song, so I ended up burning it to a CD only to find out after the fact that the song wasn't what I was looking for. And every song took so long to download because it was dial-up... It's crazy how much things have changed in a mere 20 years. reply anal_reactor 41 minutes agorootparentA friend of mine downloaded a bunch of videos \"funny hidden camera\" and then burned them on a CD and one turned out to be porn. Like, even back then it boggled my mind why he wouldn't have checked. reply throw0101c 1 hour agoparentprev> Ah 56k internet, what fun. Unless you were young and on a college/university campus where your dorm room probably had Ethernet jacks. reply WarOnPrivacy 1 hour agorootparent>> Ah 56k internet, what fun. > your dorm room probably had Ethernet jacks. One 64k T1 channel, so 14% more fun. Okay to be fair the modem was probably averaging closer to 32k so double the fun. reply throw0101c 11 minutes agorootparent> One 64k T1 channel, so 14% more fun. Except if you were sharing between all the students in the dorm and on campus. reply EvanAnderson 1 hour agorootparentprev> One 64k T1 channel, so 14% more fun. A ton less latency, though. T1 connectivity still felt snappy for web browsing long after cable modems pushed up average available bandwidth because the latency was so much better. reply steve1977 3 hours agoparentprev> And they’re so good at it, that the pirated product is usually substantially better than the official version How so, when most of these products are digital to begin with? If you have the quality of the studio master, then there’s nothing to improve. reply Khaine 31 minutes agorootparentI am into Jazz, and would like to buy Ella Fitzgerald's discography. No website will easily let me do that. Pirates do. reply npteljes 1 hour agorootparentprevSometimes they further optimize it for a goal, or maximize it to have everything. Sometimes organize it, provide additional material that fans might appreciate, add commentary. Port it to a platform where previously it was inaccessible. For examples, an artist's discography is sometimes neatly organized in folders, by year, and the files are tagged with metadata. In case of anime, fans improve or outright provide subtitles. They often do extra things like including karaoke for the opening and ending, color coding subtitle lines to match the speaker's personality or design, and provide stylized subtitles for signs and other letterings on the screen. In case of movies, they are often optimized to a specific kind of usage, for example to be able to be viewed on every phone, or to have a minimal file size. Sometimes, especially older releases, are digitally cleaned up and enhanced. Games install easily and is packaged in a way that it just works, without faffing about with the launchers and things like that. reply mmh0000 1 hour agorootparentprevWell. No unskippable ads for shows I don’t care about. No FBI warnings. But, I think theoatmeal sums it up best: https://theoatmeal.com/comics/game_of_thrones reply miah_ 2 hours agorootparentprevThere isn't always a studio behind a videos release. Sometimes the only remaining copy of a show is found in somebodies VHS collection and its filled with static and audio pops. These often get a standard 'as-is' release and then somebody in the community will clean it up, upscale the video, and fix the audio for a 'proper' release. Same goes for audio, sometimes its a old bootleg tape of a show that gets digitized and cleaned up. This is before even getting into 'fan edits' where people will re-add cut scenes, or do other edits to films. For an example of that, search for 'Topher Grace Star Wars Prequel'. reply Fezzik 56 minutes agorootparentFor a great example of fan edits, compare Harmey’s Despecialized Editions of Episodes 4, 5, and 6 to the butchered final copies that George tried to make be the only available versions; the Despecialized versions are astonishingly superior in content and audio/video quality. https://en.m.wikipedia.org/wiki/Harmy%27s_Despecialized_Edit... reply rightbyte 2 hours agorootparentprev> For an example of that, search for 'Topher Grace Star Wars Prequel' A funny detail in The Obi Wan series are the prequel lookbacks that, if you don't remember how bad they are, make them seem like mediocre movies instead. reply steve1977 2 hours agorootparentprevBut in most of those cases I wouldn’t really speak of „official“ versions. reply everyone 3 hours agorootparentprevIn the case of software removing DRM and shitware either bundled with the product or necessary to use it. In the case of music it's less clear but I would suggest curation and completeness. reply issafram 2 hours agoparentprevQuality? I can rarely find FLAC files. reply mmh0000 40 minutes agorootparenthttps://sharemania.us/ reply Biganon 1 hour agorootparentprevPrivate trackers are great for FLAC files. reply im3w1l 4 hours agoparentprev3+hours? I remember being able to download roughly 10 songs in an hour. reply mmh0000 4 hours agorootparentLook. It was 25 years ago. I’m an old man now. My memory of that time is a haze of multiplayer Quake strategy. How about I rephrase it: it FELT like 3+ hours. reply jl6 4 hours agorootparentIt could still have been 3 hours as you would have been constrained not only by your own download speed, but also the peer’s upload speed, and they might have been on an even crappier dial-up line or sharing their bandwidth with other uploads. reply Ylpertnodi 4 hours agorootparent25 years ago i had an engineer come out: everyone on the street was sharing a connection in the local box. We chatted about tech etc and he put me on a single connection. Sod the neighbors (their connections didn't get slower, mine got faster). reply ethbr1 3 hours agorootparentprevCurious to hear more informed thoughts from old ISP folks, but afaik one of the things that made Napster possible was that 56kbps connections were roughly symmetrical (~33kbps up?). By capitalizing on the oft unused upload bandwidth, Napster provided a benefit at little cost. Would be fascinated to hear what this looked like on the PSTN backend load side, ~2000. reply dpkirchner 3 hours agorootparentOne of the coolest things about the modem age is that you could easily try a different ISP if you weren't happy with your current ISP's performance (upstream and down). reply fsckboy 3 hours agorootparentprev>56kbps connections were roughly symmetrical I don't think so, asymmetry was the innovation that made 56K possible on POTS (plain old telephone sevice, with only enough bandwidth for squawky voice) reply ethbr1 1 hour agorootparentFrom a spec standpoint, acoustic modems were building over wires spec'd for bidirectional voice transmissions, i.e. symmetrical. I'm curious about the nuances, but it seems like the last mile download/upload imbalance was created by the originating signal mode? Download = First mile internet to ISP could be upgraded to digital, and thus grab some extra throughout by avoiding analog noise handling Upload = First mile user to ISP was inherently analog over phone lines, and so sacrificed throughput for line noise tolerance https://www.edn.com/an-introduction-to-the-v-90-56k-modem/ reply temporarely 3 hours agorootparentprevI would think symmetric bandwidth is baked into switched circuit networks by definition. A bit earlier than 2000 /g https://lineofsightgroup.com/wp-content/uploads/2017/01/phon... reply brabel 4 hours agorootparentprevIIRC Napster downloaded from multiple peers if possible. Each peer could restrict bandwidth so that they didn't get overwhelmed with outbound traffic. So speed would depend vastly on the popularity of content... if many people could provide a song, you could download it quickly (for the time: maybe a few minutes per song), but those where you had to rely on a single peer at a time could take hours. reply bawolff 4 hours agorootparentI'm not sure, but i think it was napster's succesors that introduced that. I dont think napster had that sort of thing. reply brabel 4 hours agorootparentToo long ago, you could be right, maybe KaZaA? reply bawolff 4 hours agorootparentI think so. KaZaa was somewhat famous for implementing that feature using the non-cryptographically secure UUHash algorithm (instead of something like sha1) for better performance, which allowed trolls to insert fake file parts into your downloads. reply brabel 3 hours agorootparentI found a post talking about KaZaA and Napster, and that's right: I forgot but Napster had a central server to provide files, that's why it was so easy to shut down... https://computer.howstuffworks.com/kazaa.htm KaZaA didn't store the files itself so it was thought they wouldn't be possible to shutdown. From the site above: \"While Kazaa claims to be \"completely legal,\" there are those who disagree: The free-to-download blue files are controlled by Kazaa users and include copyrighted content.\" \"Later that year, Kazaa was sued again, this time in the United States by the Recording Industry Association of America (RIAA) and the Motion Picture Association (MPAA). As of February 2005, the decision in that suit is still pending.\" I remember they started suing individual users at that time... I found an article explaining that: https://www.videoproc.com/resource/what-happened-to-kazaa.ht... \"In September 2003, the RIAA filed lawsuits against over 250 individuals, accusing them of illegally distributing about 1,000 copyright music files each, using P2P networks. RIAA sought an average compensation of $3,000 per case.\" The result of the first case: \"In July 2006, the MGM Studios, Inc. v. Grokster, Ltd. caused Sharman to settle for $100 million, the amount to compensate the loss of four major music labels – EMI, Sony BMG, Universal Music, and Warner Music. The company also agreed to pay an undisclosed amount to the studios in the industry.\" It's unclear exactly how Kazaa got down, the article concludes with \"In August 2012, the Kazaa website was no longer active.\"... \"the rise of legal streaming services such as iTunes, Spotify, and Netflix further compounded Kazaa's demise.\". Looks like the music industry managed to scare people away from pirating instead of actually succeeding in bringing them down directly, which is more or less what I remember. reply nostrademons 2 hours agorootparentNapster used a centralized server for indexing but downloads were peer-to-peer. This is what made Napster so awesome on college campuses: you could find anything, but if you chose a local peer, the actual download would happen over the college LAN at godly speeds. Gnutella brought peer-to-peer searches. Basically it used a flood-fill algorithm: your search would be broadcast to all connected peers, which would broadcast it to all peers that hadn't seen it yet, until somebody responded with the file and their IP and you could download directly from them. Interestingly Ethereum uses basically the same algorithm for block distribution, with some optimizations that were first published by RTM, who was one of the founders of YCombinator. Kazaa's innovation was to split the peer space into \"ordinary nodes\" and \"superpeers\", with the observation that not all bandwidth links were equal. It would enlist hosts on high-bandwidth connections to form quasi-centralized indexing nodes to organize the network topology for all the low-bandwidth consumer nodes. It's a similar principle to how the Lightning Network works for Bitcoin, or how L2s on Ethereum operate. This also made it easier to shutdown than Gnutella though, because being a superpeer made you a legal target for the RIAA. reply tithe 2 hours agorootparentFun fact: Kazaa's inventors (and their P2P architecture) would later go on to build \"Sky peer-to-peer\" AKA Skype. reply bawolff 2 hours agorootparentprevbit torrent is basically the succesor to all this, and torrenting sites like the pirate bay are still going strong. reply parineum 3 hours agorootparentprevNapster kept a central repository if who had what files, not the files themselves. Kazaa and ed2k were distributed. I think ed2k is still viable. reply nuancebydefault 42 minutes agorootparentprevIt was more like 4 mins per song on a 56K modem. So you could listen a full song while downloading another. Hence pretty acceptable. The less acceptable part was that nobody could use the phone. reply bloopernova 3 hours agorootparentprevYou wanna LAN party in the old people's home? Although I was better at Unreal Tournament, I like the weapons more. reply rightbyte 2 hours agorootparentprevI don't think you had figured out you needed to download from hosts with the lowest ping to get the top modem speed. I remember like 15min for one song on a 56k modem but like 6-7 min at top speed, which was rare. reply Asmod4n 3 hours agorootparentprevmaybe you are misremembering creating mp3 files from CDs, on a 486 it took quite a while to generate a mp3 from a 4 minute wav file. Pentiums with MMX made it quite alot faster. reply globular-toast 3 hours agorootparentprevNah, your memory serves you well. It really did take three hours sometimes. reply nostrademons 2 hours agorootparentprevHah, my first MP3 took about 2 weeks to download, over ZModem on a 14.4KBps link. I had to work around my parents' Internet-time restrictions - they'd say \"times up\", I had to disconnect, then I'd pick up again the next time I was allowed on the computer. Then I needed to convince them to buy a ZIP disk to free up enough hard disk space so I could decompress it (my Centris 660AV wasn't fast enough for real-time MP3 playback), then it took another 2 hours to decompress once I'd freed up the requisite 40 MB of hard disk space. It was 1995 and pretty magical to get CD-quality audio coming out of the computer, though. reply madduci 3 hours agorootparentprevYes it was for me 3+ hours. It all depended on codec and length of song. And also the fact that your phone line was kept busy while using it and loosing the connection when your parents wanted to make a telephone call. It was weird but funny at same time reply Vvector 3 hours agorootparentprevI remember being queued up to download one song from a specific host. My download with be like 6th in line. reply dclowd9901 1 hour agorootparentprevWe had rural phone lines with lots of noise. I think I got 3kB a sec max on those things even with 56k. reply romanhn 3 hours agoprevBest part of Napster was searching for that one song you were interested in, and then looking through all of that user's shared files. It was an amazing discovery mechanism which I missed dearly with all the subsequent P2P apps. Spotify now fills this void, but it's not quite the same. reply grimgrin 2 hours agoparentDefinitely enjoy browsing a user's shares, and even occasionally checking what's been downloaded from me. Soulseek goes strong https://nicotine-plus.org/ reply heed 2 hours agorootparentWhoa I totally forgot that Soulseek was a thing, thank you! I used to be able to find rare b-sides and live recordings of my favorite bands on it. reply Cockbrand 1 hour agoparentprevMy personal favorite was Audiogalaxy, which had a fairly decent recommendation engine. I discovered a lot of excellent music (which I subsequently bought) through this. reply sizzzzlerz 5 hours agoprevStill remember the very last day Napster was alive. It was an absolute feeding frenzy as thousands and thousands of people were attempting to download everything they could before it all went away. Napster was, basically, a giant middle finger to the record companies and their control over what got released and their ability to put out albums containing one or two songs people wanted and then stuffed with what ever crap they could shovel into it. The music business changed that day. reply smugma 2 hours agoprevParamount+ coming out with a two part documentary June 11 based on a book with the same name, How Music Got Free: https://www.paramountpressexpress.com/paramount-plus/shows/h... reply CaptainOfCoit 2 hours agoparent1) What kind of domain is that? Looks like a domain for a tabloid or something 2) It redirects me to https://www.paramountpressexpress.com/location-not-allowed which says \"Your geographic location is not allowed access\". Here is a mirror for others who happen to not live in the US (or whatever country it's restricted to): https://archive.is/AYgwj reply tmalsburg2 1 hour agoprevNapster may have started the file sharing revolution. But the exciting part for me was Gnutella and later Bittorrent, peer-to-peer technology in general, and the realization that we could use technology to liberate ourselves. Needless to say, I was young and naïve. That spirit is long dead, and the only remnants are crypto currencies and the community around them, which has tossed all lofty ideals in favor of blind greed. reply _fat_santa 2 hours agoprevNapster was a little before my time, though I fondly remember torrenting when I was a kid and oddly enough, it was a small time scam that got me into it. I remember when I was a kid (maybe 13 or 14) I really (and I mean really) wanted to play Grand Theft Auto. My dad picked up a copy of GTA San Andreas for me about a year prior but after seeing the violence and language he forced me to take it back, that put the taste into my mouth and I wanted more. So I searched for \"free video game downloads\" and I remember I went to this site that claimed that for just $30, you could \"download unlimited video games for free\". I begged my parents to let me sign up and pay the $30 until they finally relented and bought it for me. What it turned out to be was just a tutorial on how to use uTorrent (and they also had an \"addon\" for unlimited MP3's which taught you how to use Limewire). I realized pretty quickly I got scammed and I think to date this is the only scam that I've ever fallen for. Kinda worked out though because I learned how to torrent and after a few years I had a basic gaming PC with a hard drive full of games that I had torrented (everything from Crysis to GTA games to every Call of Duty game). I was also pretty deep into torrenting music right up until Spotify came to the USA (I still remember all the \"workarounds\" to get Spotify to work in the US before they officially released here, though I didn't try any myself) Since then Spotify has remained my longest kept subscription and for all it's faults I think Spotify got the solution to music piracy right. With Spotify it was just more convenient to pay them every month then to bother pirating music and uploading it to my iPhone. reply zulban 1 hour agoparentDoesn't sound like a scam. You wanted to buy a fish and they gave you a fishing pole. And for just 30$? Some online training today costs many thousands of dollars. Indeed you must be young because you think online tutorials must be free and instructors charging for their work is a scam. reply andai 2 hours agoparentprevHow is that a scam? You received, as promised, the ability to download infinite games for free :) reply nuancebydefault 46 minutes agoprevBuying music (singles or albums) was super expensive in 1999. It was unfair for listeners as well as artists. Napster suddenly made it 'free'. I remember thinking, if CD's were not so ridiculously expensive and i could pick the songs on it, i would just buy them instead of spending money on data traffic time/bandwidth. Today, we pay much less and the artists are still paid, while we can choose exactly what to listen to. reply AlexandrB 31 minutes agoparentThe \"long tail\" artists are paid a lot less. Even if you listen to Frog Eyes all day, every day most of your Spotify subscription is going to Taylor Swift. reply sorenjan 1 hour agoprevI rarely see it mentioned when discussing old file sharing programs, but I thought DC++ was the best. With the right hubs you could find anything you wanted, you could download parts of a file from multiple peers, you could browse a user's shared files to find new stuff, we had a server in our university network that RIAA/MPAA could find, there was a chat for each server, etc. https://en.wikipedia.org/wiki/DC%2B%2B reply joemazerino 45 minutes agoparentDC++ was a superior file sharing app. I used it often. I liked how you could control ratios from individual downloaders or servers. reply AcylicUnicorn 23 minutes agoprevWhen I look at the personalities who made a name for themselves in corporate piracy.. err.. file sharing, I keep noticing some of the people who've contributed a lot to the current suckage in tech. reply ofrzeta 1 hour agoprevThat was awesome. You would think of an song and right there you could download it, even more obscure stuff. Information right at your fingertips or something like that. reply p3rls 2 hours agoprevBack around in World of Warcraft around 2009ish, there was a rogue named Napster that someone invited to the guild I was inas a casual/friend. I never found out if he was actually the founder for real but I always wanted to add \"geared up the founder of Napster in Sunwell\" to my resume somehow. reply coretx 4 hours agoprevYet we still don't have a safe & resilient alternative for using Kademlia tables. Shame on us. reply bawolff 4 hours agoparentWhat's wrong with current generation DHT? They work. They are secure (depending on how you define that). They are reliable. Why would we want to replace them? reply coretx 8 minutes agorootparentAnyone with mediocre skills can take it down; ( And more ) that's the problem. I'd rather not mention the specifics because that might give people ideas but they can be found in academic papers. reply neilv 3 hours agoprevNapster helped motivate and justify all sorts of nasty DRM and legislation, which has adverse implications for more than just piracy. And at the time of the MP3 frenzy, there were a relatively small number of people saying, \"Wait, won't this mean a backlash of adversarial laws and technology that make things bad for everyone?\" But that was ignored by the masses of newly-arrived Internet users wanting to take stuff for free. reply 0xcde4c3db 56 minutes agoparentAn argument could be made that Napster contributed to the acceleration of that trend, but it was well underway already. The Software Publishers Association was coordinating raids of businesses for pirated software at least as far back as 1991. The WIPO Copyright Treaty was ratified in 1996, and the DMCA was passed in 1998 to implement it. CSS was introduced with DVD in 1996. United States v. LaMacchia was decided in 1994, leading to the passage of the NET Act in 1997. The first versions of SecuROM and SafeDisc were released in 1998. The same year, the Secure Digital Music Initiative was formed. reply artninja1988 2 hours agoparentprevIsn't this a bit like victim blaming? You should be criticizing the RIAA and lawmakers instead. reply neilv 2 hours agorootparentNo, it is not. I'm not blaming the artists whose work was stolen. A bunch of people took up thievery. Someone said please stop. They laughed, and were complicit in creating a tech dystopia. If you want to say they were dumb kids who didn't know any better, we can work with that. So then the question would be how do they help fix the mess they helped create? reply lupusreal 51 minutes agoparentprevYou're being naive if you think we wouldn't have those laws if not for Napster. If not that pretext it would be another. Floppy disks and flash drives could have been it. reply bradleyjg 5 hours agoprevNapster was a big part of it, credit where credit is due, but faster networks were the critical ingredient. I was in college at the time and it blew up on campus, but for my friends still in high school on DSL or even still POTS it was far less useful. reply skeeter2020 3 hours agoparentUniversities & Colleges have been ground zero for establishing pretty much every major advancement, from the internet itself to Doom, file sharing and Facebook. Lots of resources combined with young, rich people with big ideas and energy who get the benefit of knowledge & wisdom while almost universially staking out a position in opposition to the established \"way things are\". reply vlovich123 4 hours agoparentprevI was on 56k and Napster was the best and fastest thing around for a while, but more importantly had the largest library. Gnutella/Kazaa took over but only once Napster shut down and people started making their libraries available there. Torrents came in short order but that was more coupled to when broadband became much more widely adopted reply bigtex 3 hours agoprevI remember having 384KB symmetrical DSL in mid 2000 from Northpoint Communications, who would go out of business a few months later. I basically had business class service to my apartment because that it all they could provide, so I didn't complain. Obviously one of the first things I tested was Napster on my Bondi Blue iMac B computer and man could you download music so fast and easily! reply NathanielBaking 5 hours agoprevEverything is free now That's what they say Everything I ever done Gonna give it away Someone hit the big score They figured it out That we're gonna do it anyway Even if it doesn't pay Everything Is Free Song by Gillian Welch reply simonw 2 hours agoprevSomething I found fascinating about Napster at the time is that it was fiercely difficult to use... but it didn't matter, because what it gave people (access to ANY music) was so desirable that they would learn how to use it. reply swed420 2 hours agoprevExcerpts from chapter 5 of \"Free as in Freedom\" (freely available at https://www.oreilly.com/openbook/freedom/): > Although based on proprietary software, the Napster system draws inspiration from the long-held Stallman contention that once a work enters the digital realm-in other words, once making a copy is less a matter of duplicating sounds or duplicating atoms and more a matter of duplicating information-the natural human impulse to share a work becomes harder to restrict. Rather than impose additional restrictions, Napster execs have decided to take advantage of the impulse. Giving music listeners a central place to trade music files, the company has gambled on its ability to steer the resulting user traffic toward other commercial opportunities. > The sudden success of the Napster model has put the fear in traditional record companies, with good reason. Just days before my Palo Alto meeting with Stallman, U.S. District Court Judge Marilyn Patel granted a request filed by the Recording Industry Association of America for an injunction against the file-sharing service. The injunction was subsequently suspended by the U.S. Ninth District Court of Appeals, but by early 2001, the Court of Appeals, too, would find the San Mateo-based company in breach of copyright law,5 a decision RIAA spokesperson Hillary Rosen would later proclaim proclaim a \"clear victory for the creative content community and the legitimate online marketplace.\" > For hackers such as Stallman, the Napster business model is scary in different ways. The company's eagerness to appropriate time-worn hacker principles such as file sharing and communal information ownership, while at the same time selling a service based on proprietary software, sends a distressing mixed message. As a person who already has a hard enough time getting his own carefully articulated message into the media stream, Stallman is understandably reticent when it comes to speaking out about the company. Still, Stallman does admit to learning a thing or two from the social side of the Napster phenomenon. > \"Before Napster, I thought it might be OK for people to privately redistribute works of entertainment,\" Stallman says. \"The number of people who find Napster useful, however, tells me that the right to redistribute copies not only on a neighbor-to-neighbor basis, but to the public at large, is essential and therefore may not be taken away.\" . . . > \"It's a mistake to transfer answers from one thing to another,\" says Stallman, contrasting songs with software programs. \"The right approach is to look at each type of work and see what conclusion you get.\" > When it comes to copyrighted works, Stallman says he divides the world into three categories. The first category involves \"functional\" works-e.g., software programs, dictionaries, and textbooks. The second category involves works that might best be described as \"testimonial\"-e.g., scientific papers and historical documents. Such works serve a purpose that would be undermined if subsequent readers or authors were free to modify the work at will. The final category involves works of personal expression-e.g., diaries, journals, and autobiographies. To modify such documents would be to alter a person's recollections or point of view-action Stallman considers ethically unjustifiable. > Of the three categories, the first should give users the unlimited right to make modified versions, while the second and third should regulate that right according to the will of the original author. Regardless of category, however, the freedom to copy and redistribute noncommercially should remain unabridged at all times, Stallman insists. If that means giving Internet users the right to generate a hundred copies of an article, image, song, or book and then email the copies to a hundred strangers, so be it. \"It's clear that private occasional redistribution must be permitted, because only a police state can stop that,\" Stallman says. \"It's antisocial to come between people and their friends. Napster has convinced me that we also need to permit, must permit, even noncommercial redistribution to the public for the fun of it. Because so many people want to do that and find it so useful.\" > When I ask whether the courts would accept such a permissive outlook, Stallman cuts me off. > \"That's the wrong question,\" he says. \"I mean now you've changed the subject entirely from one of ethics to one of interpreting laws. And those are two totally different questions in the same field. It's useless to jump from one to the other. How the courts would interpret the existing laws is mainly in a harsh way, because that's the way these laws have been bought by publishers.\" > The comment provides an insight into Stallman's political philosophy: just because the legal system currently backs up businesses' ability to treat copyright as the software equivalent of land title doesn't mean computer users have to play the game according to those rules. Freedom is an ethical issue, not a legal issue. \"I'm looking beyond what the existing laws are to what they should be,\" Stallman says. \"I'm not trying to draft legislation. I'm thinking about what should the law do? I consider the law prohibiting the sharing of copies with your friend the moral equivalent of Jim Crow. It does not deserve respect.\" reply ww520 2 hours agoprevAh good time. Bring back memory. My involvement with that period was I wrote a p2p file sharing app using the Gnutella protocol, an alternative to the Napster. It was a fun time. reply andai 2 hours agoparentThat's what Limewire used right? Is that network still a thing? Last I checked it was, but full of bots that return fake files named after your exact search query. reply ww520 1 hour agorootparentYes. Limewire came out a bit later. All the apps with the same protocol could talk to each other. Initially mine was open sourced as well. I close sourced it after selling it to a company. reply dboreham 4 hours agoprevI briefly worked there. Like three Scaramuccis. reply washadjeffmad 4 hours agoprevI remember how out of control the lawsuits got by the mid-2000s. A friend had to drop out of college and move back home over some multi-million dollar claim on behalf of the RIAA that his family \"settled\" for $40K. It wasn't even clear he was responsible (the lawsuit was based on IP), but he was scared and admitted to using \"file sharing services\", and that was that. I'm still disappointed that laws against barratry didn't become more prevalent in the US as a result of \"copyright trolling\". Instead, exceptions have been slowly chiseled out of the DMCA to restore a sliver of the rights we used to have. reply jjtheblunt 4 hours agoparentWhat rights did you used to have? reply bdowling 4 hours agorootparentWe used to have the right to copy whatever we wanted and be accountable to no one. That all went away in 1790 when the U.S. government passed the first Copyright Act. https://www.copyright.gov/timeline/timeline_18th_century.htm... reply PaulDavisThe1st 3 hours agorootparent> We used to have the right to copy whatever we wanted and be accountable to no one. Unless you copied something of value to a rich and powerful person, in which case your accountability would likely be quite high. Also, \"copying\" before 1790 meant something rather different than cp(1). reply steve1977 3 hours agorootparentprevAnd what exactly would you have copied with your rights in, say, the year 1789? And how would you have done that? reply logicchains 2 hours agorootparentYou could copy a book, and sell it. This was a big part of the Protestant revolution; the Catholic church didn't want the common people reading the Bible, but back then they had no legal way to control how people used the printing press. reply steve1977 2 hours agorootparentThe Bible is not copyrighted. So you could still do that today. Also I‘m pretty sure the catholic church wanted no one to read the Luther Bible. Neither common people nor others ;) reply costco 18 minutes agorootparentThe copyright to the King James Version is owned by the crown. reply CPLX 3 hours agorootparentprev> We used to have the right > That all went away in 1790 Interesting use of the word “we” here. Also there’s a few other “rights” that have changed since then, not sure this argument has the power you think it does. reply sophacles 3 hours agorootparentprevThe big one is: when you bought a thing you owned it. The anti-circumvention clause is ultimately the root cause of the right to repair movement. The anti-circumvention clause also means it might be illegal to make a back up copy of media you bought, a thing that always used to be ok. reply chinathrow 2 hours agoprevNapster was awesome but Audiogalaxy was way better: it just worked and had a remote interface which was magical at the time. reply koolala 2 hours agoprevMusic sparked the revolution, not Napster. People freely sharing what they love. Sing it!According to the RIAA’s former CEO, Hilary Rosen, a few months after Napster’s release, the music industry shifted into full panic mode. In February 2000, all major label executives discussed the threat during an RIAA board meeting at the Four Seasons Hotel in Los Angeles. > “I will never forget this day. All of the heads of the labels, literally the titans of the music business, were in that room. I had somebody wheel in a PC and put some speakers up and I started doing a name that tune,” Rosen later recalled. > The major music bosses started to name tracks, including some that weren’t even released yet, and time and again Napster would come up with results. Needless to say, the board was terrified. They're about to be more terrified. Suno and Udio are just the beginning of the complete unraveling of the need for studio capital. It's going to happen to Hollywood too. reply sumedh 4 hours agoparentSteve Jobs was able to use their fear to sign up most publishers on itunes. reply 39896880 3 hours agorootparentAnd sell outrageously priced MP3 players. There’s no way people were stocking those things with music they paid for. reply echelon 2 hours agorootparentThis is exactly what happens when a reverse salient collapses and opens up a new channel of distribution. reply jdenning 2 hours agorootparentOT: thanks for introducing me to the term “reverse salient”! That’s a very useful concept. reply tbihl 3 hours agorootparentprevMostly, yeah. My dad, brothers, and I (all one household) had probably 200 CDs between us that we ripped and listened to repeatedly. Paid once per person? Doubtful, but yes, we paid. reply localfirst 1 hour agoparentprevI was reading parents comment with a bit of cynicism until I tried suno and I believe we are going to see significant garnishing of wages in creative field https://suno.com/song/dc950e10-6bbe-4649-8cdb-7a4d81762aae reply CyberDildonics 1 hour agorootparentI believe we are going to see significant garnishing of wages in creative field A wage garnishment is any legal or equitable procedure through which some portion of a person's earnings is required to be withheld for the payment of a debt. Most garnishments are made by court order. You think because of auto generated music there are going to be court orders to take money out of the paychecks of people in 'creative fields'? Garnish wages means taking money out of what someone has already earned. reply gedy 5 hours agoparentprevMaybe, but don't discount media lobbying gov't, and trying to generate moral outrage about AI stealing from artists, deep fakes, AI safety, etc to make sure only big corps control AI. reply HeatrayEnjoyer 4 hours agorootparent>AI stealing from artists, deep fakes, AI safety This isn't generated outage, it's genuine, and objectively justified. Trying to find clients was hard enough for artists to begin with. We should be automating hard labor so we can focus on art and human interaction, but instead we're automating art and interaction so we can focus on slaving away even harder. This is somehow worse than any traditional dystopia story. Government propaganda posters were at least the work of a human hand. reply devbent 1 hour agorootparent> We should be automating hard labor so we can focus on art and human interaction We have being doing that for over 100 years now. Look at industrial productivity per worker, it skyrocketed throughout the 20th century. It has plateaued only because all the low and medium hanging fruit was long ago picked and now engineers are busy doing incremental improvements. I am genuinely curious what sacrifices would have to be made of we went with a fully automated community, from growing food to putting it on store shelves, to running the store, cleaning it, everything. Obviously not all foods are possible to automate end to end, but I wonder how far we could go. reply echelon 3 hours agorootparentprev> objectively justified. Subjectively. GenAI is just a tool. Artists learn the craft of analogy, allusion, satire, and storytelling. These are portable to new forms and methods of creation, and they'll continue to serve artists that use GenAI. Software engineers have been constantly reinventing themselves since forever. The stack and frameworks you learn won't last forever, and the field continually gets easier to enter year over year. This is no different than what artists now face. And now bigger forms of automation and process are available to artists. They don't need studios or big budgets to achieve monumental works - they'll be making movies and games soon. (Without publishers!) And the 99% of non-artists aren't going to put the work in. These tools are going to be incredible for the artists that adopt them. We're going to see a whole bunch of Vivienne Medranoses, Zach Hadels, and Michael Cusacks. reply Teever 3 hours agorootparentprevThe royal we. What steps have you personally taken to automating hard labour? reply gedy 3 hours agorootparentprevDon't blame \"AI\" for all the cheapening of the arts. It's been that way for a long time, largely due to mass media giving away for very cheap. No one already wants to pay for anything, and artists don't help by putting their works in digital form on the internet. If I want to train a computer model on all this digital stuff, so be it. If you want to be an artist: perform music live, make a real object like a painting or sculpture, act in a play, etc. Don't be a digital \"artist\" it's already worth next to nothing. reply echelon 4 hours agorootparentprevIt's not a conscious choice. It's the fact that the convolutions that create human visual and auditory pleasure are simple signals relative to logic and reasoning. Nature itself is somehow shaped such that it is easier to create beautiful patterns than it is to engineer complex logical deductions. It is a fundamental aspect of physics and reality. Beauty is just simple patterns. We figured out that art is simple math. Engineering and law and planning not so much. reply skydhash 3 hours agorootparent> We figured out that art is simple math. Art is not about making things beautiful. It's about making things meaningful. Unless compelled by external reasons (Money,...) artists wish to express themselves and to do so the best way the can. > Engineering and law and planning not so much. All of these aims to be as precise as they can be and wish that they could be Math. But Nature and Humanity randomness won't let them be. reply wrl 2 hours agorootparentprev> We figured out that art is simple math. Engineering and law and planning not so much. Simple math... but mostly a ton of existing human-generated art. Don't act like generative AI isn't standing on a lot of collective shoulders. > Engineering and law and planning not so much. Maybe OpenAI should have trained it on that first? Simple math, right? reply ethbr1 3 hours agorootparentprevI'd phrase it as: pop art and culture have been optimizing themselves against the human psyche for so long (much longer than engineering and law and planning) that we have a pretty firm bead on what an every-person will like (mathematically-speaking). There's tons of avant garde art, but on average it's less popular (because that's not what it's optimized for). I wouldn't be surprised if GenAI commoditizes pop art, but in turn creates greater demand for abstract, more unique forms. reply skydhash 3 hours agorootparentBy its nature, pop art and culture reduce itself to the common expectations of everyone. And as such has only few knobs to tune. But humans can appreciate a wide range of qualities and the more you play on these dimensions, the more reduced the people that will \"get\" it and appreciate it. GenAI can be great for pop art, but try to create something unique to you or another person, and it will fail miserably. reply echelon 3 hours agorootparent> GenAI can be great for pop art, but try to create something unique to you or another person, and it will fail miserably. GenAI is just a tool. Creating unique art with unique perspectives is going to be more accessible to more people. Not everyone will put the work in, but there's a new opportunity in a world full of opportunity cost. reply ethbr1 2 hours agorootparentI'd make the argument that the nature of tools drives much of mass/low-cost art (that is, the majority). Very few people have the training and complete skill set to fully customize everything. Consequently, the further you get away from \"doing the thing\" (e.g. playing an instrument) to \"operating the tool that does the thing\" (e.g. writing music for a player piano), the stronger impression the tool leaves on your work. See also the corralling of c64 demos into hardware limitations. Or early electronic music vs 80s+. reply internet101010 2 hours agorootparentprevYep. Imagine you are a producer that doesn't sing very well but knows exactly what you want. There are TTS VSTs that allow for custom models. You can change the key, length, modulation, etc. by just dragging the mouse along the word(s) and it integrates into your DAW like any other instrument. reply jkolio 4 hours agorootparentprevOne of the funniest moments in modern music history, to me, was when Kim Dotcom got a bunch of superstars to sing on a track about how much they loved MegaUpload. One of the least funny moments was when the RIAA and MPAA essentially directed American and New Zealand authorities to raid his house, shut down the MegaUpload website, seize all the servers, and jail a bunch of his employees (Kim himself only barely escaping extradition). Episodes like that really do make me fearful of what corporations might push government to do. If they want to get you on something that isn't illegal, they'll try to make it illegal. If it's already illegal, they'll try to make the punishment as severe as possible. Oversight of, and accountability from, these entities is paramount. reply mouzogu 3 hours agoprevtorrent is really a great technology. subversive and evergreen. genuinely useful and hard to kill. reply issafram 2 hours agoparentOnly if you use a private tracker. Public ones always have seeders whose job it is to collect IP addresses. You'll still get that letter in the mail about pirating reply andai 1 hour agorootparentIsn't going after torrent makers more important than random downloaders? By that logic shouldn't big music / movie companies be targeting private trackers? (Surely they have enough materials to pass the interview!) reply mattmaroon 3 hours agoprevMan do I feel old now. I remember using that in college when it was brand new. These were in the days when Yahoo practically WAS the internet and Google was a scrappy upstart. I remember trading mp3s on AOL before it. I remember how much effort it took to download an mp3 over a 56k modem. Every time my brother picked up the phone I had to start over. The first one I succesfully downloaded, after trying for what must have been days but felt like months, was Gettin' Jiggy Wit It and of course it turned out our 386 computer couldn't even really play an mp3 without stuttering. I remember moving in with two friends, a block from campus, and putting a $2,500 Gateway computer (bought from the Gateway store in the strip mall, of course) on a credit card because we were going to split it three ways. You can probably guess how that ended up. Even though that was equivalent to about $5k today it was a midrange computer at best and, of course, was replaced a little over a year later. But, we did get broadband and it could play Starcraft and online poker just fine. Worth it! I remember after Napster exploded, it had gotten bogged down with new traffic and they started offering multiple servers, and someone wrote some third-party software that let you switch between them at will until you could find the song you wanted without too terribly much wait. I remember my first mp3 player, a Diamond Rio 500. It used a USB cord that, of course, had the pins reversed, so you couldn't just use any USB cord. It held about one hour of music, 1.5 if you bought a very expensive 32mb flash card. It ran for a week on one aaa battery. It cost almost $300, which would be like $500 today. God did I love that thing. I remember the RIAA and MPAA suing file sharers. I remember lots of tech bros saying what idiots they were for trying to use the legal system to put the toothpaste back in the tube. I remember one particularly astute, well-known tech guy saying at a YC dinner (must have been 2007 or 2008) in response to a sneering question about it that not everyone who chooses a career outside of tech is an idiot, the people who run the media companies might know more about media than you, certainly understand that selling physical media's days are numered, and that what they were doing was really a delaying tactic while they figured out how to monetize content in the digital age. (The answer, it turned out, was things like Netflix and Spotify, but those were still a few years away. Netflix's first original content wasn't until 2012!) I remember after Napster got sued into oblivion there were so many others. The toothpaste was out of the tube on file sharing. Limewire. Edonkey. Bittorrent. For every one they killed two more appeared. Fragmentation wasn't even an issue since everyone ran (and thus seeded) several. Good nostalgia for a Saturday,. reply tiahura 5 hours agoprevnext [17 more] [flagged] dividefuel 4 hours agoparentWhat's the argument for file sharing ruining music? I would think more the opposite, that it improved distribution and discovery for lesser-known artists, and so helped diversify music. reply jl6 4 hours agorootparentThere’s a theory that for a period of time in the 00s, all the “smart” people were getting their music for free online, while only “dumb” people were buying CDs, and so popular music charts began to reflect the tastes only of the “dumb” people. I don’t believe it because (a) crap music has always existed, and (b) Napster nerds were numerous but still weren’t a critical mass capable of moving the market like this. reply brabel 4 hours agorootparentI downloaded stuff from Napster (and its successors) but still bought CDs that I liked it in the store. Having a CD of vinyl collection used to be a matter of pride even if you had everything also on mp3s. reply bitwize 3 hours agorootparentprevEvery musician I know says this. Back in the day, you could actually make money selling records, which led to a flourishing of music. New hopefuls would work their butts off, playing gigs and selling small-print-run CDs to make ends meet while they waited and hoped for a major label to give them their big break. And they really had a shot at a big break, too. Then file sharing happened, and if you were any good your stuff was being pirated as soon as it was released, if not before. Major labels won't touch pirated stuff, it's radioactive. The only way to make it is to take a bath on record sales and hope that enough people buy concert tickets and swag to meet your expenses. So yeah, the likes of Taylor Swift, Katy Perry, Justin Bieber, Drake -- they all get rich, but everybody else is struggling. The idea that piracy helps listeners discover new music may well be true. But it doesn't help put food on musicians' tables -- just the opposite. See also: open source and the corrosive effect it's had on programmers' livelihoods. reply ljlolel 5 hours agoparentprevLet me guess, you’re over 40 reply lawgimenez 5 hours agorootparentI think he’s Lars of Metallica. Hey Lars reply codetrotter 5 hours agorootparentBeer good. Napster bad. reply jumpman_miya 5 hours agorootparentprevYou'll understand once you get there. reply maroonblazer 5 hours agorootparentI'm there (and then some) and hard disagree. reply scarface_74 4 hours agoparentprevEvery generation thinks that the next generations music is crap. Studies show that people stop discovering new music when they are 30. https://www.hypebot.com/hypebot/2023/03/study-shows-why-we-s...! reply bloopernova 3 hours agorootparentI've definitely discovered less new artists as I've aged. I'm near 50 now and last year only added Röyksopp, and Sleaford Mods, to my regular listening playlist. But I'm also still listening to new stuff by many artists like Aesop Rock, Orbital, and The Chemical Brothers. I don't follow radio or popular music at all. However if I hear something I like, the supercomputer in my pocket tells me just what it is so I can discover new music that way. reply narag 3 hours agorootparentprevLast study I've seen, that stopped being... I won't say true, but I'd say that it stopped being a valid counterargument to that particular complaint. Now that young people can access music from any time in YouTube, their choices are more atemporal and certain remote decades keep getting much love. If you combine that fact with current music being more marketed and aired, there's seem to be something at play here. I don't think new music is crap, but it's less catchy. reply scarface_74 3 hours agorootparentThere has always been a lot of marketing an airing to make music catchy. The entire “Motown machine” wasn’t about the artists, it was about the writers, producers, choreographers etc and making sure music was “acceptable” to a wider audience. reply cushpush 5 hours agoparentprevModern music is fire, modern music distribution is a dumpster fire. reply coretx 5 hours agorootparentThat's because of using the IPR's for market/platform/eyeballs/etc.-gatekeeping is more lucrative. Such behavior is illegal in any other market, but in media it's not for \"unknown\" reasons. reply gosub100 3 hours agorootparentprevCompletely agree. As much as I dislike modern streaming services' habit of preventing you from owning the art, I have to credit them for breaking me out of my deep rut and getting me to discover new artists. For decades I didn't know what I was missing. reply xhkkffbf 4 hours agoprev [–] And now rock stars who were once as rich as, well, rock stars, are largely forced to get by on a pittance from either Spotify or YouTube. It used to be that there were hundreds of stars getting rich. Now there's Taylor Swift and... I can't name anyone else. But, hey, the quasi-communists who thought they were somehow liberating the music from the evil rich people ended up dragging all of the music industry down into poverty. reply luuurker 3 hours agoparent> Now there's Taylor Swift and... I can't name anyone else. Ariana Grande, Adele, Billie Eilish, etc. I believe someone called Kendrick Lamar has been popular recently? Are they rock stars? No, people tastes are a bit different now, but they are stars and they are rich. You not knowing them is a different problem. reply hinkley 2 hours agorootparentEven Lily Allen is worth $4 mill and I only know - and own - one of her songs. reply eddythompson80 3 hours agoparentprev> It used to be that there were hundreds of stars getting rich. Now there's Taylor Swift and... I can't name anyone else. Just becaus you don’t know them doesn’t mean they don’t exist. There are way more than hundreds of “rock stars” that are getting rich now. There are also way more “rock stars” now than there ever was. reply redwall_hp 3 hours agorootparentThe \"rock star\" era was a bubble, at a time of rising demand and limited supply. Digital recording has democratized music, tastes have stratified (thank fuck) and now there's so much supply people have FOMO over discovery tools not finding them optimal recommendations in a sea of possibilities. On the other side, in the first half of the 20th century, artists were more likely paid a one-off pittance to come into a studio and record a song, and then the record labels profit from it to this day. Music is worth less now because the supply is vast and the demand is relatively limited. Pretty simple economics. And most artists have always made their money from touring...while concert prices are more eye watering than ever. reply hinkley 2 hours agoparentprevDavid Bowie was a billionaire when he died. It’s true that in the top 50 richest musicians, most of them were already rock gods before Napster. But they all are rock gods. But then you have people like Swift, Robbie Williams, Dave Matthews and Dave Grohl, most of which were on the cusp. But Grohl for instance worked hard for more than a decade after Nirvana broke up, and I would be very surprised if a lot of his money didn’t end up coming from a combination of the Foo Fighters and royalties. Commentary on one of those net worth sites: the Foo Fighters “have consistently been one of the highest-grossing touring acts in the world for more than two decades.” So yeah. reply wiseowise 2 hours agoparentprev> And now rock stars who were once as rich as, well, rock stars, are largely forced to get by on a pittance from either Spotify or YouTube. Boo hoo. Let me play a song on tinniest violin in the world for them. reply bitwize 3 hours agoparentprev [–] This is why we have DRM -- and need more. reply coretx 15 minutes agorootparentDRM is done by elected officials, usually parliamentarians. Whenever it's not elected individuals or institutions doing it, it's called tyranny or vigilantism. reply artninja1988 2 hours agorootparentprev [–] Disagree. DRM is evil and anti consumer. I don't care if there aren't as many billionaire artists as in the past reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Napster, launched in 1999, revolutionized music consumption by enabling global file-sharing, which led to widespread music piracy.",
      "Despite its shutdown in 2001 due to legal battles, Napster's influence inspired legal digital music services like Apple's iTunes and streaming platforms such as Spotify.",
      "The current music industry, thriving on streaming subscriptions, traces its origins back to Napster's disruptive impact."
    ],
    "commentSummary": [
      "Napster's launch 25 years ago revolutionized file-sharing, paving the way for platforms like The Pirate Bay and tools such as Lidarr, Sonarr, and Radarr, which often provide higher-quality media than official releases.",
      "The text discusses the evolution of file-sharing, from Napster to BitTorrent, and the legal challenges faced by platforms like KaZaA, highlighting the meticulous organization of pirated content by fans and the impact of digital rights management (DRM).",
      "It also explores the shift in the music industry due to streaming services like Spotify, the democratization of art through Generative AI, and the ethical considerations of copyright laws, including Richard Stallman's views on public redistribution rights."
    ],
    "points": 213,
    "commentCount": 177,
    "retryCount": 0,
    "time": 1717247327
  },
  {
    "id": 40539172,
    "title": "Heroku Unveils Postgres Essential Plans on AWS Aurora with Enhanced Performance",
    "originLink": "https://blog.heroku.com/heroku-postgres-essential-launch",
    "originBody": "Introducing New Heroku Postgres Essential Plans Built On Amazon Aurora Posted by Jonathan Brown Product Manager Last Updated: May 21, 2024 We’re thrilled to launch our new Heroku Postgres Essential database plans. These plans have pgvector support, no row count limits, and come with a 32 GB option. We deliver exceptional transactional query performance with Amazon Aurora as the backing infrastructure. One of our beta customers said: “The difference was noticeable right from the start. Heroku Postgres running on Aurora delivered a boost in speed, allowing us to query and process our data faster.” Our Heroku Postgres Essential plans are the quickest, easiest, and most economical way to integrate a SQL database with your Heroku application. You can use these fully managed databases for a wide range of applications, such as small-scale production apps, research and development, educational purposes, and prototyping. These plans offer full PostgreSQL compatibility, allowing you to use existing skills and tools effortlessly. Compared to the previous generation of Mini and Basic database plans, the Essential plans on the new infrastructure provides up to three times the query throughput performance and additional improvements such as removing the historic row count limit. The table highlights what each of the new plans include in more detail. Product Storage Max Connection Max Row Count Max Table Count Postgres Versions Monthly Pricing Essential-0 1 GB 20 No limit 4,000 14, 15, 16 $5 Essential-1 10 GB 20 No limit 4,000 14, 15, 16 $9 Essential-2 32 GB 40 No limit 4,000 14, 15, 16 $20 Our Commitment to the Developer Experience At Heroku, we deliver a world-class developer experience that’s reflected in our new Essential database plans. Starting at just $5 per month, we provide a fully managed database service built on Amazon Aurora. With these plans, developers are assured they’re using the latest technology from AWS and they can focus on what’s most important—innovating and building applications—without the hassle of database management. We enabled pg:upgrade for easier upgrades to major versions and removed the row count limit for increased flexibility and scalability for your projects. We also included support for the pgvector extension, bringing vector similarity search to the entire suite of Heroku Postgres plans. pgvector enables exciting possibilities in AI and natural language processing applications across all of your development environments. You can create a Heroku Postgres Essential database with: $ heroku addons:create heroku-postgresql:essential-0 -a example-app Migrating Mini and Basic Postgres Plans If you already have Mini or Basic database plans, we’ll automatically migrate them to the new Essential plans. We’re migrating Mini plans to Essential-0 and Basic plans to Essential-1. We’re making this process as painless as possible with minimal downtime for most databases. Our automatic migration process begins on May 29, 2024, when the Mini and Basic plans reach end-of-life and are succeeded by the new Essential plans. See our documentation for migration details. You can also proactively migrate your Mini or Basic plan to any of the new Essential plans, including the Essential-2 plan, using addons:upgrade: $ heroku addons:upgrade DATABASE heroku-postgresql:essential-0 -a example-app Exploring the Use Cases of the Essential Plans With the enhancements of removing row limits, adding pgvector support, and more, Heroku Postgres Essential databases are a great choice for customers of any size with these use cases. Development and Testing: Ideal for developers looking for a cost-effective, fully managed Postgres database. You can develop and test your applications in an environment that closely mimics production, ensuring everything runs smoothly before going live. Prototype Projects: In the prototyping phase, the ability to adapt quickly based on user feedback or test results is crucial. With Essential plans, you get the flexibility and affordability needed to iterate fast and effectively during this critical stage. Educational Projects and Tutorials: Ideal for educational setups that require access to live cloud database environments. They're perfect for hands-on learning, from running SQL queries to exploring cloud application management and operations, without managing the complex infrastructure. Low Traffic Web Apps: Ideal for experimental or low traffic applications such as small blog sites or forums. Essential plans provide the necessary reliability and performance, including daily backups and scalability options as your user engagement grows. Startups: The Essential plans offer a fully managed and scalable database solution, important for startup businesses to grow without initial heavy investments. It can help speed up time-to-market and reach customers faster. Salesforce Integration Trial: The best method to synchronize Salesforce data and Heroku Postgres is with Heroku Connect. The demo plan works with Essential database plans. Although the demo plan isn’t suitable for production use cases, it provides a way to explore how Heroku Connect can amplify your Salesforce investment. Incorporating pgvector: Essential database plans support pgvector, an open-source extension for Postgres designed for efficient vector search capabilities. This feature is invaluable for applications requiring high-performance similarity searches, such as recommendation systems, content discovery platforms, and image retrieval systems. Use pgvector on Essential plans to build advanced search functionalities such as AI-enabled applications and Retrieval Augmented Generation (RAG). Looking Forward As announced at re:Invent 2023, we’re collaborating with the Amazon Aurora team on the next-generation Heroku Postgres infrastructure. This partnership combines the simplicity and user experience of Heroku with the robust performance, scalability, and flexibility of Amazon Aurora. The launch of Essential database plans marks the beginning of a broader rollout that will soon include a fleet of single-tenant databases. Our new Heroku Postgres plans will decouple storage and compute, allowing you to scale storage up to 128 TB. They’ll also add more database connections and more Postgres extensions, offer near-zero-downtime maintenance and upgrades, and much more. The future architecture will ensure fast and consistent response times by distributing data across multiple availability zones with robust data replication and continuous backups. Additionally, the Shield option will continue to meet compliance needs with regulations like HIPAA and PCI, ensuring secure data management. Conclusion Our Heroku Postgres databases built on Amazon Aurora represent a powerful solution for customers seeking to enhance their database capabilities with a blend of performance, reliability, cost-efficiency, and Heroku’s simplicity. Whether you're scaling a high web traffic application or managing large-scale batch processes, our partnership with AWS accelerates the delivery of Postgres innovations to our customers. Eager to be part of this journey? Join the waitlist for the single-tenant database pilot program. We want to extend our gratitude to the community for the feedback and helping us build products like Essential Plans. Stay connected and share your thoughts on our GitHub roadmap page. If you have questions or require assistance, our dedicated Support team is available to assist you on your journey into this exciting new frontier. Originally published: May 21, 2024 postgres database pgvector sql essentials devex",
    "commentLink": "https://news.ycombinator.com/item?id=40539172",
    "commentBody": "Heroku Postgres is now based on AWS Aurora (heroku.com)204 points by mebcitto 23 hours agohidepastfavorite125 comments richardgill88 3 minutes agoWe're building on top of Aurora at https://xata.io. Currently our Aurora instances are in private beta. If you're interested in trying it out, drop me an email: richard@xata.io reply olau 13 hours agoprevA warning about Aurora: It's opaque tech. I've been on a project that switched to it by recommendation by the hosting provider, and had to switch away because it turns out that it does not support queries requiring temporary storage, i.e. queries exceeding the memory of the instances. It manifested the way that the Aurora instances would use up their available (meagre) memory, then start thrashing, taking everything down. Apparently the instances did not have access to any temporary local storage. There was no way to fix that, and it took some time to understand. After having read all the little material I could find on Aurora, my personal conclusion is that Aurora is perhaps best thought of as a big hack. I think it's likely there are more gotchas like that. We moved the database back to a simple VM on SSD, and Postgres handled everything just fine. reply deergomoo 1 hour agoparentWe’ve generally been happy with Aurora, but we run into gotchas every so often that don’t seem to be documented anywhere and it’s very annoying. Example: in normal MySQL, “RENAME TABLE x TO old_x, new_x TO x;” allows for atomically swapping out a table. But since we moved to Aurora MySQL, we very occasionally get stuff land in the bug tracker with “table x does not exist”, suggesting this is not atomic in Aurora. Is this documented anywhere? Not that I’ve been able to find. I’m fine with there being subtle differences, especially considering the crazy stuff they’re doing with the storage layer, but if you’re gonna sell it as “MySQL compatible” then please at least tell me the exceptions. reply orf 12 hours agoparentprevThe first result on Google shows that Aurora certainly does have temporary local storage https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide... reply alexey-salmin 11 hours agorootparentI believe this issue is (or was) real. There are important differences in how Aurora treats temporary data. Normal postgres and rds postgres write it into the main data volume (unless configured otherwise). Aurora however always separates shared storage from local storage and it's not entirely clear to me what is this local storage physically for non-read-optimized instance types. The only way to increase it is to increase the instance size. [1][2] This is indeed frustrating because with postgres or rds postgres you just increase the volume and that's it. Luckily since November 2023 it also has r6gd/r6id classes with local NVMEs for temp files. [3] This should in theory solve this problem but I haven't tried it yet. [1] https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide... [2] https://www.reddit.com/r/aws/s/sIhBQhsG80 [3] https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-au... reply p0seidon 4 hours agorootparentI think Aurora has to go through the same development process as every database. They changed essential patterns in the database, and there are severe side effects that need to be addressed. You can see the same with Aurora Serverless and the changes in V2; there were some quite quirky issues in the first versions. reply dalyons 4 hours agoparentprevCalling it a hack is pretty unfair. The log storage engine is a huge innovation, in my experience makes large MySQL/pg clusters much more reliable and performant at scale in a variety of different ways. It has a couple of quirks, but on balance it feels like the future - the next evolution of what traditional rdbms are capable of. But if you don’t have scale or resiliency needs it probably doesn’t matter to you. reply pquki4 7 hours agoparentprevIsn't Aurora mainly about their unique handling of logging and replica which leads to high availability and fast recovery? If you switch to VM, how do you handle availability in multiple locations and backups? If database checkpoints are good enough for you, sounds like Aurora is overkill in the first place. reply vips7L 7 hours agoparentprevWe’re currently struggling with switching from RDS to aurora. The replica times are absolutely bonkers long for the simplest of writes. reply dalyons 4 hours agorootparentAurora has essentially constant replica lag times, it’s one of the best features. Should be around 30-50ms always, are you seeing different? reply vips7L 1 hour agorootparent7-20 seconds depending on location. reply alexey-salmin 5 hours agorootparentprevCan you elaborate, which exact timings are bad? reply iancarroll 21 hours agoprevHaving previously been on several managed PostgreSQL providers and now on AWS Aurora -- Aurora has been pretty great in terms of reliability and performance with large row counts and upsert performance. However, Aurora isn't cheap and is at least ~80% of our monthly AWS bill. I wonder how it is cheaper than Heroku's previous offerings? Is it Aurora Serverless v2 or something like that to reduce cost? Aurora billing is largely around IOPS, and Heroku's pricing doesn't seem to reflect that. reply drusepth 21 hours agoparentHeroku Postgres has always been priced on platform convenience with very high margins. It's been many years now so I don't remember the exact numbers, but I moved a few databases from Heroku to AWS and reduced my DB costs ~90% (magnitude ~900/mo --> ~100/mo) for roughly the same specs. They probably have a lot of margins to eat into before they need to adjust prices. reply iancarroll 21 hours agorootparentI am not seeing the margins in this $5/mo instance but I could be wrong! reply eljimmy 20 hours agorootparentWe're using the highest tier Postgres instance at my work for one of our legacy Heroku apps and it costs thousands over what we'd pay for the equivalent on AWS directly. reply iancarroll 16 hours agorootparentSure, but those are not related to Aurora or this post. reply csande17 12 hours agorootparentprevAccording to https://elements.heroku.com/addons/heroku-postgresql the instances they're using for this tier have zero bytes of RAM, so presumably that's where they're getting most of their cost savings from. reply michaelmior 11 hours agorootparentI'm assuming this means that they are not providing any sort of guarantee on the amount of RAM available and packing these instances as tightly as they can. reply csande17 11 hours agorootparentI like to think they just aren't installing any RAM in the servers and running all the databases out of L3 cache reply jmspring 20 hours agorootparentprevI know Salesforce has a huge AWS presence. That said, is it possible they are doing multitenancy? I don't know myself. reply eezing 4 hours agorootparentprev“Amazon Aurora Serverless is an on-demand, autoscaling configuration for Amazon Aurora. It automatically starts up, shuts down, and scales capacity up or down based on your application's needs.” reply rnts08 13 hours agoparentprevEverything on Heroku is billed with a huge margin, plus as they're probably a partnered customer by now their pricing is a fraction of the average AWS customers pricing. I've been at companies on both sides of the partner pricing list and the difference is huge. reply alexey-salmin 19 hours agoparentprevTry the Aurora IO-optimized, it's a (relatively new) game changer price-wise. I'm migrating a 1tb database to it right now because I'm paying too much for iops even on the regular rds postgres. I'm also quite sure this is what heroku must be using or they would be out of business because of the pricing mismatch. reply dangoodmanUT 7 hours agorootparentThis, Aurora normal is useless now reply p0seidon 4 hours agorootparentprevHow much do you expect the price to drop? reply jrockway 20 hours agoparentprevAurora has treated us well. We make a self-hosted product that requires Postgres; our sales/customer engineering folks just started telling people to use Aurora, and it hasn't caused any problems despite the fact that all of our tests run against stock Postgres. Can't complain. Though a VM with Postgres would be plenty for our needs, and cost thousands of dollars less a month. But, HA is nice if you want to pay for it. reply encoderer 21 hours agoparentprevAurora has a new configuration option that changes billing from iops to higher storage costs. Might be what this is using. reply iancarroll 21 hours agorootparentYeah, that’s what we use as well but I don’t think that addresses the underlying instance cost? I’m not familiar with Serverless v2 though, if that’s what this is using. reply paulddraper 21 hours agorootparentThe instance cost is not much different then normal heroku compute reply chuckadams 21 hours agoparentprevJust curious, does Aurora scale down at all in price, i.e. if I have a test instance that's hardly ever used, does it ever end up being cheaper than a classic RDS instance? reply tudorg 14 hours agorootparentDisclaimer: I work at xata. Xata is (like Heroku) based on Aurora, but offers database branching and has different pricing. That should be ideal for lightly-used test instances, because you only pay for storage, and 15GB are included in the free tier. reply nilamo 21 hours agorootparentprevIt scales to zero, so costs nothing when it's not in use... reply hfern 21 hours agorootparentCan you share which configuration scales to $0? I am not aware of that being possible. Even the serverless option has a base ACU rate. reply bdcravens 20 hours agorootparentv1 of Serverless did scale to 0, but that's no longer an option reply bdcravens 20 hours agorootparentprevYou're thinking of Aurora Serverless, but the typical Aurora customer isn't using the Serverless offering. Additionally, the original version of Aurora Serverless scaled to 0, but v2 doesn't. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide... reply osigurdson 13 hours agorootparent>> but the typical Aurora customer isn't using the Serverless offering Just wondering, why is that? reply bdcravens 5 hours agorootparentA bit pricier, some companies have a steady load, scaling isn't instantaneous, etc. reply rnts08 13 hours agorootparentprevAWS Can't bill you on 0 usage. reply debuggerpk 20 hours agorootparentprevthat has not been my experience. reply dangoodmanUT 7 hours agoparentprevUse io optimized reply Scubabear68 18 hours agoparentprevCame here to say this. Aurora is good but also very expensive. If your queries are not very well tuned you will pay through the nose for I/O that would be unnoticeable in other Postgres implementations. For a very modest installation I saw DB bills go down from $3,000 a month on Aurora to about $100 for self managed Postgres. reply andrewstuart 20 hours agoparentprev>> Aurora isn't cheap and is at least ~80% of our monthly AWS bill. Why don't you run your own Postgres? It's not hard - why pay such a premium for the Amazon version? reply williamdclt 7 hours agorootparent“Not hard” is very relative. Is it hard to run a Postgres database? No. Is it hard to set up monitoring, automatic failover (without data loss), managed upgrades, tx wraparound protection, clustering and endpoint management, load balancing… and probably a bunch of other things I’m not thinking of? Yes. reply dalyons 4 hours agorootparentprevYou clearly don’t know what aurora is or does if you think people can just run their own. It’s not a regular Postgres setup, and nothing exists that’s equivalent for self hosted. reply szundi 20 hours agorootparentprevIs it less true for other cloud stuff? reply teaearlgraycold 17 hours agorootparentprevIf you’re using Aurora and not RDS you’re probably outside of the zone where rolling your own Postgres is easy. reply sgarland 16 hours agorootparentSo figure it out. I don’t understand why “ugh this is hard, I’ll pay someone else” has become the norm. You’re working in one of the most technically advanced fields in the world; act like it. reply vasco 16 hours agorootparentMost people aren't doing anything advanced. Also this has nothing to do with not wanting to do \"hard things\", that's ridiculous, it's a postgres cluster, you're not doing a PhD in math. People do it because there's limited time and no business advantage to operate postgres clusters. Use the time on what your business actually does. reply sgarland 7 hours agorootparent> that's ridiculous, it's a postgres cluster, you're not doing a PhD in math. It's not as difficult as a PhD (I assume; I only got as far as an MS), but based on what I've witnessed, it's up there in complexity. There are dozens of knobs to turn – not as many as MySQL/InnoDB to be fair, but still a lot – things you have to know before they matter, etc. > People do it because there's limited time and no business advantage to operate postgres clusters. Use the time on what your business actually does. I've seen this argument countless times for SaaS anything. I don't think it's accurate for a database. Hear me out. For most companies, the DB is the heart. Everything is recorded there, nearly every service's app needs it (whether it's a monolith or micro service-oriented DBs), and it's critically important to the company's survival. Worse, the same skills necessary for operating your own DB generally overlap heavily with optimally running a DB, by which I mean if you're good at things like DB backup automation, chances are you're also good at query optimization, schema design, etc. It's that latter part that seems to be missing from many engineering orgs. \"Just use Postgres,\" people say; \"just add a JSONB column and figure out the schema later,\" but later never comes. If your business uses a DB, then you do not have the luxury of running one poorly. Spend a few days learning SQL, it's an easy language to pick up. Then spend a few days going through the docs for your DB, and try the concepts out in a test instance. Your investment will be rewarded. reply vasco 6 hours agorootparentYou're at a more surface level than what I'm talking about. Your advice at the end is just common sense advice for anyone using any tool. It doesn't mean you should spend time implementing your own custom backup process with ability to go back to a specific point in time, configurable in 1 minute. The amount of work needed to operationalize postgres in the same way and expose it to other teams in a company will take long enough that you won't get an Aurora-like experience in less than a quarter with a full team. What could they be doing instead to your product? All I'm saying is it has nothing to do with difficulty. In my job for example we self-hosted HBase which is a beast compared to postgres, implemented custom backups etc, all because there was no good vendor for it. Postgres is much simpler and we always just used RDS and then switched to Aurora for the higher disk limits when it was launched. If there's a good enough vendor, you're just stroking your ego re-implementing these things when you could move on to the actual thing the business wants to release. I've also seen senior engineering leads \"proving\" self hosting \"saves money\" but then 2 companies working on the same type of problem in the same industry with a similar feature set, on one side we had 5 people maintaining what on the other company it took 6 teams of 4-8 people. So it depends if you'd like to have a lot of your labor focused on cutting costs or increasing revenue. And they never include the cost of communicating with extra 5 teams and the increased complexity and slowness to release things this creates, while also being harder to keep databases with current versions, more flimsy backup processes, etc. Ps: we got rid of hbase, do yourself a favor and stay away reply sgarland 3 hours agorootparent> Your advice at the end is just common sense advice for anyone using any tool. Common sense isn't so common. I've met a handful of devs across many separate companies who care at all how the DB works, what normalization is, and will read the docs. > It doesn't mean you should spend time implementing your own custom backup process with ability to go back to a specific point in time, configurable in 1 minute. If by implement you mean write your own software, no, of course not. Tooling already exists to handle this problem. Off the top of my head, EDB Barman [0] and Percona XtraBackup [1] can both do live backups with streaming so you can backup to a specific transaction if desired, or a given point in time. Or, if you happen to have people comfortable running ZFS, just snapshot the entire volume and ship those off with `zfs send/recv`. As a bonus, you'll also get way more performance and storage out of a given volume size and hardware thanks to being able to safely disable `full_page_writes` / `doublewrite_buffer`, and native filesystem compression, respectively. > If there's a good enough vendor, you're just stroking your ego re-implementing these things when you could move on to the actual thing the business wants to release. Focusing purely on releasing product features, and ignoring infrastructure is how you get a product that falls apart. Ignoring the cost of infrastructure due to outsourcing everything is how you get a skyrocketing cloud bill, with an employee base that is fundamentally unable to fix problems since \"it's someone else's problem.\" > Ps: we got rid of hbase, do yourself a favor and stay away HBase and Postgres are not the same thing at all. If you need the former you'll know it. If people convince management that they do need it when they don't, then yeah, that's gonna be a shitty time. The same is true of teams who are convinced they need Kafka when they really just need a queue. My overall belief, which has been proven correct at every company I've worked at, is that understanding Linux fundamentals and system administration remains an incredibly valuable skill. Time and time again, people who lack those skills have broken things that were managed by a vendor, and then were hopelessly stuck on how to recover. But hey, the teams had higher velocity (to ship products with poor performance). [0]: https://pgbarman.org [1]: https://www.percona.com/mysql/software/percona-xtrabackup reply lijok 8 hours agorootparentprevGonna use this next time I'm proposing my pet store should host an on-prem k8s cluster with a psql cluster on it ! reply teaearlgraycold 15 hours agorootparentprevHave you ever been paid to do work before? There’s a price at which a business will prefer to pay to have SaaS/PaaS solve a problem. Allocating engineering hours to setting up and maintaining a Postgres cluster has a cost. You’ll want someone senior on it. Their time could be well over $100/hour. And that’s assuming your business is small enough to only need one DBA part time. A business that’s spending a ton on Aurora might need 3 specialists. Now you’re talking about hundreds of thousands of dollars per year. It could be better to just pay AWS. However, at large scales cloud won’t make sense anymore. They do have a markup and eventually what you’re paying in markup could instead buy you a few full time employees. reply sgarland 7 hours agorootparent> Have you ever been paid to do work before? Yes, many times, which is why I've developed this opinion. > However, at large scales cloud won’t make sense anymore. They do have a markup and eventually what you’re paying in markup could instead buy you a few full time employees. The issue is once you've finally realized this stuff matters, and have hired a DB team, I can practically guarantee that your schema is a horror show, your queries are hellish, and your product teams have neither the time nor inclination to unwind any of it. Your DB{A,RE}s are going to spend months in hell as they are suddenly made the scapegoats for every performance problem, and are powerless to fix anything, since their proposals require downtime, too much engineering effort, or both. Hence my statement. Learn enough about this stuff so that when you do hire in specialists, the problems are more manageable. reply tbarbugli 7 hours agoprevRDS is such a depressing database option. It does not matter how much money you throw at it, its performance will always be limited by the awful disk IOPS. Luckily these days you can easily run PG on EC2 (or simply use CRDB). Weird for Heroku to ignore this huge efficiency opportunity. reply p0seidon 4 hours agoparentHave you tried io2 in this context? Did not have the chance yet. reply tbarbugli 2 hours agorootparentio2 is generally better than io1, one advantage is that you can scale storage size and IOPS independently. That being said, RDS with io2 is still worse than an ec2 instance with nvme (a lot worse) reply metadat 22 hours agoprevProduct Storage Max Connection Monthly Pricing Essential-0 1 GB 20 $5 Essential-1 10 GB 20 $9 Essential-2 32 GB 40 $20 The pricing looks quite competitive, although I'm not sure what the prior rates were. 10 years ago I spent 10x+ per month for 32GB (RAM) Heroku Postgres instances, IIRC they were around $400/mo, maybe even more. reply SahAssar 21 hours agoparent> 10 years ago I spent 10x+ per month for 32GB (RAM) Heroku Postgres instances, IIRC they were around $400/mo, maybe even more. Aren't you comparing RAM vs Storage there? The pricing chart here says nothing about RAM. reply ca_peterson 18 hours agoparentprevHeroku product here: the Essential 0 and 1 plans replace the older row-limited Heroku Postgres mini/basic plans at the same price points but better perf in a lot of scenarios and a storage instead of row limit - forcing people to denormalize for row count wasn't ideal under old mini/basic limits. The Essential-2 plan is a new option for a larger pre-prod/test/small-scale DB option above what we offered before. We're expanding the Aurora-backed offerings to include larger dedicated DBs in the relatively near future as well. Gail Frederick our CTO talked a bit more about it at high-level during re:Invent 2023: https://www.youtube.com/watch?v=fZLcv7rwj7Y&t=1955s reply vips7L 7 hours agorootparentWhy did I think you were leading the Apex product team? reply macNchz 21 hours agoparentprevThese \"Essential\" tiers are bare bones instances for toys/mvps, they're much different than the bigger ones. No replication, 99.5% uptime target, no maintenance windows etc. reply fweimer 22 hours agoparentprevThat's 750$/month now, I think: https://elements.heroku.com/addons/heroku-postgresql#pricing (Standard 4) reply mebcitto 14 hours agoparentprevI'm curious how the Essential plans work, given that Aurora pricing starts higher than that in monthly costs. It is probably databases in a shared multi-tenant Aurora instance, and then the single-tenant plans that are currently in pilot give you the full Aurora instance. That also explains some of the limitations and the low connection limits. reply PeterZaitsev 2 hours agoprevAh, I wish they would rather choose Neon... to get all the cool functions developers need but avoid going all the way proprietary. reply drewda 20 hours agoprevI'm surprised to read this, given that both Heroku and Salesforce more broadly have hired what felt like a good number of PostgreSQL committers. reply bingemaker 10 hours agoprevAfter Heroku pulled their stunt in India when RBI changed some credit card rules, Heroku is pretty much history for me. They screwed small customers got rid of them saying they can't charge credit cards with new regulations. However they continued to entertain large customers from India. Never recommending them to anyone anymore reply holografix 10 hours agoparentYou severely underestimate what a clusterfuck of credit card abuse comes out of India. Heroku used to lose millions a year. reply bingemaker 3 hours agorootparentCare to shed some more light? reply doutatsu 8 hours agoparentprevSame - I've been slowly migrating to Render (https://render.com) as my new favourite. reply gregorvand 9 hours agoprevCan you still see all instances with their IDs on a shared cluster when you connect? reply eranation 16 hours agoprevHappy Aurora Postgres serverless customer here. Be sure to use pgbouncer (self hosted, but it needs minimal babysitting) if you intend to use it in a serverless environment (and even if you are not, the benefits of not having to worry about connection pool exhaustion are still worth it). AWS's proxy won't work too well with prepared statements connection pooling (something known as connection pinning). EDIT: and yes, it's not cheap reply colesantiago 21 hours agoprevGenuine question, who even uses Heroku anymore? A VPS (hetzner, etc) + managed postgres DB (supabase / AWS / etc) or a local one might more more than enough these days. reply karmelapple 20 hours agoparentWe do, although we're in the middle of moving our entire Heroku Postgres spend over to Crunchy Data [1]. We were getting close to one of the big jumps on the standard pricing of Heroku Postgres, and we would have had to basically double our monthly cost to lift the max data we could store from 1.5TB to 2.0TB. On Crunchy Data, that additional disk space will be like 1% more rather than 100% more. While investigating Crunchy, I ran some benchmarks, and I found Crunchy Bridge Postgres to be running 3X faster than Heroku Postgres. Heroku seems to be working on some interesting new things, but I feel burned by the subpar performance and lack of basically any new features over many years. I don't know if the new Aurora-based database will be faster than Crunchy, but the benchmarks they're talking about sound like they're finally about to catch them. But we also have better features on Crunchy, too, such as logical replication. Logical replication is still not available on Heroku. The experience for deploying apps and having add-ons is still pretty easy, but we'll see how that improves. HTTP2 support is still in beta. 1. https://www.crunchydata.com reply cyberax 20 hours agorootparentI'm working in a new startup, and I tried several \"easy\" solutions: AWS Lightsail, Heroku, Crunchy. I settled up on AWS ECS :) My main issue with Heroku was that they have not changed anything in _years_. No support for gRPC, no IPv6, and simple VPC peering costs $1200 a month. reply davepeck 20 hours agorootparentYeah, the lack of HTTP/2 support has been a long-standing issue with Heroku. They just shipped HTTP/2 terminated at their router [0], and have it on their roadmap [1] to support HTTP/2 all the way through. But it seems like it's at minimum a few months off. (As for VPC peering: the moment you need that, it sorta feels like Heroku is no longer the right place to be, even ignoring the costs.) [0] https://blog.heroku.com/heroku-http2-public-beta [1] https://github.com/orgs/heroku/projects/130 reply karmelapple 16 hours agorootparentThey just shipped it, but it's still beta. So... I wouldn't consider that shipped yet. reply singingfish 20 hours agorootparentprev+1 - recommend crunchy. I ran a substantial oracle to postgres project recently and crunchy were great. reply almost 12 hours agorootparentprevMy experience with going from Heroku Postgres to Crunchy Data (specific Crunchy Bridge) has been really good. Their product has been absolutely rock solid but what really made the difference was their support. They provided a huge amount of pre-sales support while I planned the move (and even suggested mitigations for the problems I was having with Heroku Postgres to make moving less urgent). Post-sales support has been just as good, though mostly I don’t even have to think about the database hosting anymore. I also moved my app hosting to NorthFlank from Heroku and have been really happy with that as well. It’s got the features I always wanted on Heroku (simple things like grouping different types of instances together into projects really helps) plus again excellent responsive support. reply scraplab 11 hours agorootparentOur experience of moving from Heroku to CrunchyBridge has been very similar - excellent help with the migration including jumping on a call with us during the switchover to resolve a broken index. Would strongly recommend them to anyone looking to move off Heroku. reply almost 9 hours agorootparentI was a bit concerned about the cut-over from the old database on Heroku, really wanted to minimise downtime. So they helped me produce a step by step plan, test as much of it as possible, then had an engineer join me on Zoom while I made the switchover. They were even able to accomodate doing it in the early morning in my timezone to minimise the impact. Ended up with maybe 5 mins of downtime which I was very happy with. reply davepeck 21 hours agoparentprevI do, as do several startups I advise. Despite interesting competition, my feeling is that the Heroku of 2024 remains... Heroku. I feel this way even though -- depending on how you segment -- the list of \"interesting\" competitors is quite long at this point: Render, Railway, Northflank, Fly.io, Vercel, DO App Platform, etc. reply fellowniusmonk 21 hours agorootparentI revisit heroku alts every ~6months and I am shocked how not ergonomic they still are, I switched to DO VPS + Ansible Container & Github Actions for any project that doesn't need infinite scale after Salesforce paused heroku development but I'd go back to literally any heroku clone. It's crazy how the ergonomic still just aren't there. reply davepeck 21 hours agorootparentYes, completely agree; I'm equally surprised by the poor DXes. (And: bugs. I'm also surprised by the kinds of issues I run into on some of those sites in my list -- problems that, even if not show-stopping, feel like revealing indicators of quality.) reply MuffinFlavored 20 hours agorootparent> Yes, completely agree; I'm equally surprised by the poor DXes. Any specifics/examples? I find it hard to imagine those \"big name\" companies/platforms you just mentioned don't have entire teams dedicated to hyper-optimizing experience. reply vasco 15 hours agorootparentHaving a dedicated team doesn't mean anything about the end result. reply strix_varius 20 hours agorootparentprevIME the Heroku of 2024 is Render. reply bvirb 18 hours agoparentprev10yo+ B2B SaaS company we're still on Heroku. I think the value prop is particularly good for B2B SaaS and probably less so for consumer products. Our margin per customer is so much higher than the infra cost it just never makes sense to spend money on devops instead of building features. That said it does feel a bit like a ghost town, I'm always happy to hear when someone is doing something over there. reply cakoose 20 hours agoparentprevA few years ago I was considering Heroku for something new. But then I learned that Heroku Postgres's HA offering used async replication, meaning you could lose minutes of writes in the event that the primary instance failed. That was a dealbreaker. That was very surprising to me. Most businesses that are willing to pay 2x for an HA database are probably NOT likely to be ok with that kind of data loss risk. (AWS and GCP's HA database offerings use synchronous replication.) reply hot_gril 19 hours agorootparentNoticed this too. The master failover is marketed like a strict upgrade, and the \"async\" part is only in the fine print. Many would actually prefer the downtime over losing data. A user who's experienced with DBs should think to check on this, but still. reply toasterlovin 17 hours agoparentprevI’ll just share our experience with Hetzner from earlier this week to spare everyone the learning experience. Their prices are indeed incredible. However, we spun up a VPS in their Hillsboro, OR data center only to find out that our IP address was blocked by Cloud Flare, so there was no way for us to connect to our error logging or transactional email providers. We also found this thread indicating that their entire IP range for that DC is widely blocked[0]. So, not really acceptable for professional use, IMO, unless you just need compute. 0: https://news.ycombinator.com/item?id=39638849 reply sammy2255 6 hours agorootparentCloudflare doesn’t block any ip addresses. Its their customers that do reply devoutsalsa 2 hours agoparentprevThen you have to manage a VPS. If you want to do ops, you’re not Heroku’s target customer. reply swader999 21 hours agoparentprevIt's really easy to be SOC2 compliant for a small SaaS on heroku. We need to grow in customers and Dev resources to pull it off on Raw AWS. Am looking for options though because heroku is increasing their prices. reply WD-42 19 hours agoparentprevVPS is dandy when the application is fresh. Once it starts getting long in the tooth and the VPS OS needs to be updated… nightmare fuel. reply hot_gril 19 hours agoparentprevMe. Haven't found anything that's just as easy to use. The supposed Heroku replacements like Fly.io weren't. reply sammy2255 6 hours agoparentprevHetzner doesnt offer any SLAs which is important for some people reply Andugal 21 hours agoparentprevFor a lot of use cases, nothing beats « git push » and tada your app is deployed. reply zdragnar 21 hours agorootparentVery easy to do with GitHub actions or the equivalent in gitlab and probably other competitors by now I imagine. If you wanted to spend money on something else, circleci and others help manage ci/cd as well. reply icedchai 18 hours agorootparentNot to mention every cloud provider has something built in (AWS CodeBuild, GCP Cloud Build, etc.) reply csjh 19 hours agorootparentprevThat’s a pretty standard feature nowadays reply greenie_beans 19 hours agoparentprevme. tried to move to render but it's been a headache for some key things that i need. my heroku setup is dialed in so it makes it a no brainer versus the time i've wasted trying to get render to fit my use case. right now i'm using both for two different services, and will consider moving off both once i get enough customers. reply anurag 17 hours agorootparentWhat are you trying to do on Render? We have some stuff in the works and I'd love to find a way to help. reply greenie_beans 5 hours agorootparentNothing sophisticated. My woes might be because I'm bad at devops. But I spent several hours fighting with a DNS change, trying to host my marketing website as Cloudflare pages site from my root domain (with DNS managed by Cloudflare), and then wildcard subdomains routed to a Render server. I couldn't get it to work no matter what configs I tried. My root domain marketing site is proxied through Cloudflare and I was trying to get the wildcard subdomains as DNS-only, and I suspect this was the problem but idk. In other words, the Cloudflare pages marketing site is https://bookhead.net and I wanted my customer's subdomains to route to the Render server like https://forlornbooks.bookhead.net/ (I still get the error since I haven't finished my migration to Heroku). The subdomains worked with no problem until I tried to setup a separate marketing site at the root domain. Also, I had a hard time setting up SSH with a containerized server. It was a weird DX that was a bit confusing to document so I can remember later. Can only imagine how confusing it might be if I ever have teammates. The Render CLI looks promising, though. These are only the most recent issues. Seems like y'all improved the headaches I ran into the time I tried. reply teaearlgraycold 20 hours agoparentprevI use render.com. Heroku is stuck in the past. reply risyachka 21 hours agoparentprevprobably those who don't want to set uptime alerts, fine-tune configs, set up backups and restores (which are essential because sooner rather later someone always deletes a few rows/tables) and want to focus on business reply nikita 1 hour agoprev(CEO of Neon.tech) Aurora is one of the few real innovations in the database space recognized by SIGMOD: https://sigmod.org/sigmod-awards/citations/2019-sigmod-syste... It provides a lot of benefits to the user and also a ton more to the service provider. Specifically you don’t overprovision storage or compute. Plus at least theoretically you can provide invite IO throughput at the storage level. There has been a couple more iterations on the design since. Microsoft separated transaction log from storage: https://www.microsoft.com/en-us/research/uploads/prod/2019/0... Neon added an object store and branches so you can integrate backups and add a Time Machine. PolarDB separated memory from compute - this makes serverless compute more nimble and unties memory and CPU. reply breadwinner 3 hours agoprevGoogle has 2 Postgres implementations: Cloud SQL and AlloyDB. How do they compare against AWS Aurora, for the heroku scenario, i.e., multi-tenant database? reply mattacular 16 hours agoprevAurora is probably the best managed database solution out there but it is not cheap. reply sgarland 16 hours agoparentRDS beats it in almost every aspect. Running your own on native NVMe blows both of them out of the water for performance and price. I am not a fan of Aurora. I don’t get the appeal at all. I’ve tried MySQL and Postgres varieties; it’s just expensive for no reason. reply mattacular 4 hours agorootparentI'm not sure what you mean by \"running your own on native NVMe\" Are you talking about using the managed AWS Relational Database Service or something else? Aurora can also use instance types with NVMe. Anyway, that's also ignoring the features that Aurora offers, which is why people pay more for it. The ability to have multi-AZ deployments and auto-scaling of (what can be cross-region) read replicas make it very resilient and it's dead simple to operate what would normally be considered advanced features of a DB cluster. If you just need a managed Postgres or MySQL traditional single instance and none of those extra features, then obviously you would not need to pay the premium for Aurora. RDS exists for that reason. reply sgarland 36 minutes agorootparentI was referring to running your own DB on hardware with NVMe drives. Obviously you lose every nicety of managed services, but tooling exists to replace it, and you gain stupid amounts of performance. RDS Multi-AZ Cluster gives you much of the advantages of Aurora, but with higher performance and more tuning capabilities, though you are limited to 3 nodes. Tbf 3 nodes is almost certainly enough for most companies. A few hundred thousand QPS would be easily handled by that. Re: cross-region read replicas, eh… if you’ve somehow managed to ensure that every single aspect of your app is capable of withstanding the loss of an entire region – including us-east-1, since most of the control plane functions are there – then sure, maybe. But do you need it? If an entire AWS region drops out, half of the internet goes with it, and you can just blame that. I doubt the small possibility of higher uptime is worth the literal doubling in monthly costs. reply ghiculescu 15 hours agorootparentprevWhat makes RDS better than Aurora? reply sgarland 6 hours agorootparentTo be clear, my comment stated RDS is better \"in almost every aspect.\" Aurora is better at one [0] thing – storage scaling. You do not have to think about it, period. Adding more data? You get more storage. Cleaned out a lot of cruft? The storage scales back down. Aurora splits out the compute and storage layers; that's its secret sauce. At an extremely basic level, this is no different from, for example, using a Ceph block device as your DB's volume. However, AWS has also rewritten the DB storage code (both MySQL/InnoDB and Postgres). InnoDB has a doublewrite buffer, redo log, and undo log. Postgres has a WAL. Aurora replaces all of this [1] with something they call a hot log. Writes enter an in-memory queue, and are then durably committed to the hot log, before other asynchronous actions take place. Once 4/6 storage nodes (which are split across 3 AZs) have ACK'd hot log commit, the write is considered persisted. This is all well and good, but now you've added additional inter-process latency and network latency to the performance overhead. Additionally, the storage scaling I mentioned brings with it its own performance implications. If you're doing a lot of writes, you'll encounter periodic performance hits as the Aurora engine allocates new chunks of storage. Finally, even for reads, I do not believe their stated benchmarks. I say this because I have done my own testing with both MySQL and Postgres, and in every case, RDS matched or beat (usually the latter) Aurora's performance. These tests were fairly rigorous, with carefully tuned instances, identical workloads, realistic schema and queries, etc. For cases where pages have to be read from disk, I understand the reason – the additional network latency of the Aurora storage engine seems to be higher than that of EBS. I do not understand why a fully-cached read should take longer, though. As a further test, I threw in my quite ancient Dell servers (circa 2012) for the same tests. The DB backing disk was on NVMe over Ceph via Mellanox, so theoretical speeds _should_ be somewhat similar to EBS, albeit of course with less latency since everything is in a single rack. My ancient hardware blew Aurora out of the water every single time, and beat or matched RDS (using the latest Intel instance type) almost every time. [0]: Arguably, it's also better at globally distributed DB clusters with loose consistency requirements, because it supports write forwarding. A read replica in ap-southeast-1 can accept writes from apps running there, forward them to the primary in us-east-1, and your app can operate as though the write has been durably committed even though the packets haven't even finished making it across the ocean yet. If and only if your app can deal with this loosened consistency, you can dramatically improve performance for distant regions. [1]: https://d1.awsstatic.com/events/reinvent/2019/REPEAT_Amazon_... reply koromak 22 hours agoprevOroboros eating its tail reply canadiantim 16 hours agoprevPeople still use heroku? reply hoomanmo 17 hours agoprevhere is the English translation: Amplify Might Be AWS's Worst Service, Bar None Confusing documentation, a mix of old and new systems, and it made a mess of my AWS account. To put it simply, over the past two days, I attempted to deploy a full-stack assignment on AWS services. The front end was written in React, using Vite as the framework. For such Single-Page Apps (SPAs), I personally prefer using specialized services like Netlify or Cloudflare Pages for deployment, as these services offer very robust CI/CD services, allowing for one-click deployment and automatic updates, saving a lot of hassle. Initially, I planned to manually deploy on AWS using the S3 + CloudFront model (since it was just a one-time assignment), but later I discovered that AWS has a service very similar to Netlify called Amplify, which also offers CI/CD one-click deployment services. Amplify goes even further by including user directory services, allowing for one-click registration and login via related components. It sounds great, but you only realize how problematic it is after using it. After some research, my initial deployment method was to upload the code to GitHub and then click the deploy button in the Amplify interface. This is also the deployment method I use most often with Netlify. However, I later found something wrong. The key issue was that applications deployed this way using Amplify couldn't directly use Amplify's UI components to access Cognito user directory services. After much searching, I found that Amplify has an Amplify CLI initialization command to create a new CI/CD project in the Amplify service, which also deploys additional resources like Cognito. It seemed feasible, so I did it. Then I found some issues. The initial \"issues\" were just on the AWS account management level: after deploying the project via Amplify CLI, my AWS account quickly filled up with a bunch of \"things\"—the reason \"things\" is in quotes is that Amplify created a lot of fragmented resources, including but not limited to CloudFormation, IAM roles, etc., even creating two Cognito identity pools for me—it's hard not to call them \"junk.\" Moreover, most of these resources have names that are impossible for humans to remember or distinguish, and there are no explanations or grouping features to tell you what these things are for. If it were just like this, it wouldn't seem to impact the development process, right? The biggest problem is that the local debugging and production environment apparently don't use the same configuration files, and when I was cleaning up the automatically created resources in my AWS account earlier, I somehow deleted the roles calling the Cognito user pool in the production environment, causing the production environment to be unable to access the two user pools created by Amplify, constantly throwing 400 errors. After several rounds of \"deploy-delete-redeploy-redelete,\" I decided to start over and look for the related documentation again. Later, I found that Amplify has a set of documentation outside of AWS's own documentation system, and this documentation recommends a deployment method: clicking the deploy button on the GUI webpage—yes, you heard it right, the same deployment method I used initially. So, how do you deploy additional components/services like Cognito this way? Amplify's answer is configuration files. As long as you create a folder for configuration files in the root directory of your project and write the corresponding configuration files in it, the cloud will automatically create the resources you need in AWS once it reads them. It sounds reasonable, right? Then you go to find the part about configuration files in the documentation... What's going on? Why can't I find anything in the search box in the documentation? There's not even a sample configuration file! Algolia indexing service can't be this bad, right? Searching for \"defineAuth\" in the Amplify official documentation returns mostly irrelevant information. Is my search method incorrect? I entered keywords like \"site .amplify.aws defineAuth\" in the Kagi.com search engine but couldn't find any examples or explanations of configuration file items. At this point, I'm completely convinced that the Amplify documentation is garbage. Fortunately, the API documentation of the Amplify framework is quite good, at least reducing my urge to buy a ticket to the US and blow up Amazon's headquarters while guessing the configuration file items... Also, Amplify has a UI that is completely different and more modern than other AWS services. The discrepancy is still a minor issue; the main problem is that if you create a project using the (slightly outdated) Amplify CLI, and then try to configure the back-end services like Cognito it deployed on the webpage, you'll enter an old interface. That is, once you click in, you see a slightly ugly but familiar interface, yet it feels completely disconnected from the previous Amplify interface... So now I understand why I hadn't heard of Amplify before—it's really hard to use. Complete integration is indeed an advantage, but even being born with a silver spoon doesn't excuse Amplify's messiness, simply throwing everything together and telling users \"it just works.\" Users look at it, wondering what on earth all these things are, and then you hand them a manual that looks fancy but has zero information. Users, flipping through this tome with no useful information, can only throw this pile of stuff into the historical junk heap behind them in frustration. Some rights reserved Except where otherwise noted, content on this page is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International license. ENJOY READ THIS Short story: \"The Cry of Accelerated Demise\" in a place accelerating towards demise. reply greenie_beans 44 minutes agoparentAmplify is the worst!!!! reply muratsu 22 hours agoprev [–] I don't know if it's still the case but a few years ago all major cloud providers were easily giving away thousands of dollars in cloud credits. I expect them to stop this soon since smaller cloud players build on top of them and offer better dx and startups prefer to work with these smaller companies despite free credits from larger players. reply willsmith72 21 hours agoparent [–] > startups prefer to work with these smaller companies despite free credits from larger players Says who? My experience is the opposite - tending towards too much reliance on the main providers because of the credits reply muratsu 21 hours agorootparentVercel is going strong - 25.5M in 2022, 100M in 2024. Netlify is currently at 30M. Add Supabase, Render, Railway, ... reply rgbrenner 21 hours agorootparentAWS alone is $100B/yr now reply 8organicbits 12 hours agorootparentLargely enterprise spend. Startups are a different market segment. They initially have small budgets, and eventually fail or grow large enough to move to different products. reply teaearlgraycold 17 hours agorootparentprev [–] As a startup boy we are happily chewing through hundreds of thousands in GPU credits across all major cloud platforms + lambda labs. And once those credits run out we are planning to expand our owned training hardware. Currently we just have 3x L40S but would expand to 32x L40S. I’m excited to now be a sys admin in addition to a full stack web dev. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Heroku has introduced new Postgres Essential database plans built on Amazon Aurora, enhancing performance and removing row count limits.",
      "These plans, starting at $5 per month, support advanced search functionalities with pgvector and are fully managed, making them suitable for development, prototyping, education, and low-traffic web apps.",
      "Existing Mini and Basic plans will be migrated to Essential plans starting May 29, 2024, with future enhancements including single-tenant databases, scalable storage, and additional Postgres extensions."
    ],
    "commentSummary": [
      "Heroku Postgres has transitioned to AWS Aurora, leading to mixed reactions due to performance issues, operational challenges, and high costs compared to traditional Postgres or RDS Postgres.",
      "Aurora's advanced features, such as high availability and IO-optimized configurations, are praised, but its complexities and costs are significant concerns, prompting users to consider alternatives like Render, Crunchy Data, and AWS Aurora.",
      "Discussions emphasize the trade-offs between managed services and self-hosting, highlighting the importance of database management skills and the benefits of focusing on core business activities."
    ],
    "points": 204,
    "commentCount": 125,
    "retryCount": 0,
    "time": 1717183040
  },
  {
    "id": 40539031,
    "title": "Navigating Life Without Google: Challenges and Alternatives Explored",
    "originLink": "https://blog.nradk.com/posts/degoogling/",
    "originBody": "De-googling, so far May 29, 2024 Over the past few years, I have tried to gradually reduce or eliminate my use of Google products and services. It has been difficult to ditch Google’s products in some categories, and in some others, it has been nigh impossible. Here, I list the services I have tried to find alternatives for, discuss the ease (or lack thereof) of finding a satisfactory alternative, and rant about my experience so far. Before we begin, I feel like I should put a few important caveats out of the way. First, these are notes from personal experience, and are not a comprehensive review of alternatives to Google’s offerings. If you are looking for comparisons of alternatives, they are not here. Second, using some of the alternatives I mention requires having a self-hosted server setup of some sort, which for many people will be entirely out of the question. Search I have used DuckDuckGo as my primary search engine for the past six or seven years, and although it has its flaws, it has been adequate. The problem with DuckDuckGo is that in many cases, it doesn’t seem to show a good understanding of the search query, for lack of a better way to put it. It takes my search terms too literally. It’s saving grace, though, is the ‘bang’ feature: if you decide that the results are not what you hoped for and you wish to go to Google instead, just add a !g in your search. This works not just for Google but for a long list of websites, and I find this feature incredibly handy when I know beforehand that I want to search a particular website, like Wikipedia, YouTube or Amazon. After reading constant praise for it on HackerNews, I tried Kagi for couple of months last year. Kagi has a lot of strengths. Just out of the box, it’s results are markedly better than DuckDuckGo and perhaps even Google. Their feature to up-rank or down-rank websites of your choosing is an absolute lifesaver. Not having to wade through SEO’d junk to find what I need prevents a lot of frustration. That said, I had two major issues with Kagi. First, their cheapest subscription tier has a limit of 300 searches monthly, and I would find myself running against the limit halfway into the billing period. I would have gladly paid for the unlimited tier, if not for my second issue with Kagi: privacy-wise, I simply don’t believe that a service that requires login and thus naturally associates all your activity with your identity is better than a service which can be used without a login. So I’m sticking to DuckDuckGo for the time being. E-Mail Unlike Search and many other categories in this list, it is not difficult to find a capable alternative e-mail service. I personally use ProtonMail and Fastmail. Proton positions itself as the privacy-focused option, and claim to have end-to-end encryption. They bundle e-mail with storage, calendar, VPN and a password manager at a very reasonable rate. I will continue using ProtonMail as one of my email accounts, but I don’t like the fact it is difficult to use third-party email clients with their service. Yes, that is due to how they do end-to-end encryption, and I don’t have a reason to doubt their honesty, but is it really end-to-end encryption if you don’t have sole ownership and full control of your keys? Besides moving away from Google, I also decided I want to own the whole of my email address, including the domain name. So, a year and a half ago I started using Fastmail with a custom domain. A feature of Fastmail I like and use all the time is masked email, but in fairness a lot of services provide this feature these days. Unlike Proton, FastMail makes no claims of extreme privacy, but I lke it because it has a nice and clean web UI and it works with my favorite FOSS e-mail client on Android. Photos Back in the days when Google Photos had free unlimited photo storage, it was pretty much unbeatable. Yes, the resolution was capped to 16 megapixels, but during the time I relied on Google Photos, I didn’t have a phone with a camera capable of much higher resolution. After Google axed the unlimited storage, other photo storage services seem pretty competitive with Google Photos. At a glance, Microsoft OneDrive, iCloud, Amazon Photos and DropBox all seem like options I would at least consider. But I have not tried any of those services and have no plans to do so, because I self-host a personal instance of Immich to store all my photos. In just a couple of years, Immich has gone from zero to having a very rich feature-set, and although it doesn’t have a stable release yet, it is stable enough for me. Using the immich-go CLI tool, it is a breeze to import media from your Google Takeout export to Immich as well. Drive Cloud storage of files is an easy switch: there are many, many file storage providers available with very competitive pricing. For myself, I initially used a Nextcloud instance I self-host. I still have some of my files in Nextcloud, but I found it to be slow when syncing a large amount of files, so I opted for a simpler solution. All my important files are now in a directory in my home server, and when I need access to them I just mount the directory in my laptop with sshfs. In case I need a web interface, I have a FileBrowser instance running. Side note: I love how simple FileBrowser is. It does the things I need and not a thing more. I realize switching to a different storage provider will be more difficult for people who use Google Drive for collaboration and need to share files often with other people. Fortunately, in my case what I needed was just to not lose my files if my laptop goes belly up, and to be able to occasionally access important files from my phone. Calendar and Contacts Using most calendar and contacts clients with different providers, and switching between them, is more or less straightforward thanks to the CalDAV and CardDAV open protocols. I use my NextCloud instance as the server for both, and it works flawlessly. On the client side, I use Fossify Calendar and GrapheneOS’s default contacts app along with DavX5 to facilitate the synchronization. I recently discovered that the Calendar and Contacts apps that come with the Gnome Desktop Environment support CalDAV and CardDAV as well, so that’s neat! I can now view or update my calendar and contact list from my phone, laptop or the browser! For people who can’t or don’t want to self-host a Nextcloud instance, there are plenty of ‘cloud productivity suite’ providers that provide file storage, calendar, contact and email services with a single account. Microsoft Outlook and Apple iCloud are the most popular ones, but privacy-focused alternatives like Proton are growing. Phone Let me begin by saying that I am extremely grateful that GrapheneOS exists. It is Android but with privacy and security features that severely limit the capacity of big tech to spy on you. This includes Google itself, as GrapheneOS by default comes without all the apps and services that Google includes on a regular Android phone. Optionally, you can install Google Play Services, but as a regular, ‘sandboxed’ app: which means Google has the same privilege as any other app in your phone, and you can decide what permissions to allow it. But the catch is… well, there are several. Whether or not you’ll find them acceptable depends on where your preference lies in the privacy-convenience trade-off scale. Because unfortunately, you must sacrifice one to get more of the other. The first issue is that it only runs on Google Pixel devices. This is because the GrapheneOS project has a sizable list of security and support requirements that a device will need to fulfill to be supported, and apparently only Google Pixel devices do at the moment. If you already have a Pixel phone (like I did when I first decided to try GrapheneOS), this is not an issue. If you don’t already have one though, you’ll have to make a decision about whether you want to support Google’s business by buying a device they produce, even though you’re doing that so you can give Google less control of your digital life. To me, that feels like an acceptable trade-off. The other issue is that many apps will either not work, or will be partially broken. I have used GrapheneOS for a total of 16 months in two stints, 8 months each in two devices; first a Pixel 3a and now a Pixel 7. Until about 2 months ago, I used it without the (sandboxed) Google Play Services. In this configuration, there is a whole category of applications you cannot use: most prominently, banking and finance apps will just refuse to work. Many apps that work will have some features broken: notifications in particular will be missing for a lot of apps. Some apps will keep complaining that your phone does not have Google Play Services and thus is not supported, but still continue to mostly work (looking at you, Snapchat). But I found I could still manage. Apps like Telegram, Signal and Whatsapp use their own notification delivery mechanism, so they aren’t impacted as long as you turn off battery optimization for them. All the banks I have accounts with have mobile websites that work reasonably well. I was even able to take trips through Uber’s mobile website, which was a pleasant surprise. The location functionality did not work, but I believe that was due more to the browser than to Uber itself. I tried to get as many apps as possible from F-droid, the app store for open-source Android apps. For proprietary apps, I used Aurora store, which is an alternative open-source client for the Google Play Store. Not ideal, but better than manually downloading APKs from websites like APKPure and APKMirror. About two months ago, in a moment of frustration, I decided to install the sandboxed Google Play Services and Google Play Store to see if that would improve my experience. Some things are noticeably better: for instance, notifications work for all apps, installing new apps is way easier, and I can install and use some financial apps without issues. It is still far from the regular Android experience; for instance, apps don’t auto-update, you have to manually go into Google Play Store and update them. This is unfortunate, but you necessarily lose some functionality when you limit the god-like permissions Google has on a regular Android device. Slight tangent: one of my big gripes here is with Android Auto. Contrary to what the name might suggest, Android Auto is not part of the Android Open Source Project (AOSP). Instead it is a proprietary standard, and the Android Auto app on your phone is a proprietary app that has privileged access to your system. If I want turn-by-turn navigation in my car’s head unit out of an open-source maps app on my phone, that is simply not possible without a privileged, proprietary app in between to mediate. Before January 2024, it was not even possible to use Android Auto in GrapheneOS, but now it is at least supported, although it does depend on the sandboxed Google Play being installed as well. Every time I am frustrated with the state of things with GrapheneOS, I think about getting an iPhone, using it without any Google apps, and enjoying the ‘just works’ nature of Apple products. But then I would be handing over control of a large part of my digital life to another big tech company, so I haven’t bought an iPhone yet. Apple does like to make a big deal of at least pretending to care more about your privacy than the rest of Big Tech, but I would rather not rely on promises if I can, and where possible, use open-source software stacks I can trust. So I’m sticking with GrapheneOS on my Pixel, at least for the near future. Maps OpenStreetMap is a fantastic dataset and in many cases, is better than Google Maps in terms of map accuracy. I use it a lot. But Google Maps is much more than just the map data: it has client apps across different platforms with a consistent interface, routing and turn-by-turn navigation, public transport information, live traffic data, and most importantly for me, business reviews and opening hours. For the last one, Google’s sheer size means that the network effects are on their side, and I’m not aware of any effort to create an open dataset of business reviews. All this means that it is difficult to quit Google Maps entirely. OpenStreetMap-based apps like Organic Maps are great and I frequently use them, but I find myself using the Google Maps website a lot, for tasks like searching for restaurants to getting an estimate of how long my commute is going to take. Video The sad reality here is that YouTube essentially has no alternative. Yes, it is technologically difficult to deliver video to the internet in a way that is cost-effective, but the problem that is orders of magnitude more difficult is that of building a critical mass of creators and viewers that the network effect takes over. All the creators are on YouTube, so everyone goes there for content. All the viewers are on YouTube, so creators put their videos there. There are platforms that are trying to provide an alternative, but they are mostly unknown to the general populace. I have never given serious thought to PeerTube because I don’t know of any creators I watch that upload their videos to a PeerTube instance. I think Nebula.tv is promising, and a bunch of creators I like upload their videos on it. I do have a Nebula account but rarely end up using it, since most videos get uploaded to YouTube as well, and when I open Nebula, I find myself having already watched all the interesting videos on YouTube. Conclusion At this point in time, it is exceedingly difficult to fully avoid using Google products and services, and you’ll need to sacrifice a lot of convenience in your attempts to use alternatives. Nevertheless, capable and privacy-friendly alternatives exist for many product segments in which Google dominates, and I think it is worthwhile to reduce your dependence on Google where you can. If you are so inclined, self-hosting can provide even more freedom and privacy.",
    "commentLink": "https://news.ycombinator.com/item?id=40539031",
    "commentBody": "De-googling, so far (nradk.com)181 points by nradk 23 hours agohidepastfavorite184 comments extr 22 hours agoFor me the point of de-googling is not to literally stop using every single Google service but to disengage enough that your entire online identity isn't tied to Google such that losing your account would be catastrophic. For example, I still have a gmail account I use for lots of random things, but critical bank/financial accounts go directly to a separate fastmail address (where I am confident I could get a human to help me if there was a serious account problem). I still use Google Docs suite, but \"important\" documents are stored locally as Word/Excel files. I still use Google Drive/Google Photos, but I make periodic backups to a local SSD + other online backup services (iCloud/OneDrive). Etc. I don't understand why you would want to \"ditch every Google product and service\". Some of them are objectively pretty great, or inherently have low switching costs (like Google Maps). reply noman-land 22 hours agoparentSimple. Google is in the business of collecting and monetizing my data and removing my privacy. I want them to have as little access to my life as possible. reply extr 20 hours agorootparentDay to day what does \"monetizing my data and removing my privacy\" actually mean to you? Are advertisers following you around town? Bothering you at work? Hounding you in the loo? Personally it seems like my data has been stolen from just about every company I've ever given it to EXCEPT for Google. reply noman-land 19 hours agorootparentJust because this stalking happens invisibly in the guts of a Javascript file it doesn't make it any different than following me around every street I walk down and noting down everything I look at. Here's the metaphor. Google offers free video cameras to any person who wants them. The cameras collect thousands of data points about every single person who enters their field of view. The cameras pick up every movement on the streets, in the shops. Every word uttered. They note all conversations and relationships. They send this data into a central database where every person gets a lifetime profile where they are uniquely identified and where all the aggregate data across all the free cameras in the world is collated and stored together. Google now has a moment by moment timeline of everyone in the world, updating in real time. Now, using this cache of information, anyone can bid for my attention in real time as I go about my life. While I'm at the bank (website). While I'm at the bar (website). While I'm at the grocery store (website). While I'm at the library (website). Around every corner is a digital billboard that's watching what my hands are doing, where my eyes are looking, what I'm saying. It knows my name, where I'm from, what I do, and who my family is, and it wants to sell me something. This world is grotesque to me. It's about much more than data breaches. I didn't even get into the government overreach capabilities such a dataset would enable, nay, tempt. reply pennybanks 16 hours agorootparentokay but you do realize they give you there products to use right. and you probably use them or at least use products that need them to function. thats the deal and i doubt anyone would rather it be for cash instead. i mean would you rather pay monthly for chrome or firefox? pay per google search. pay monthy for youtube. pay for android os. monthly for maps. hell google can stare at my naked ass all day for all i care id rather not pay for any of these things reply gharper 14 hours agorootparent> id rather not pay for any of these things ...and many of us would simply rather not use Google products if that's the deal they're offering. Hence, the original article link. reply extr 17 hours agorootparentprevSo tl;dr the actual practical impact is just that you see more relevant ads when you’re visiting websites. reply lWaterboardCats 15 hours agorootparentI understand the argument of “what’s the practical impact” Practical impact is important to recognize but the concept of potential impact, otherwise known as risk, is most certainly valid; having the most intimate details of your identity stored outside of your own control can have profound risks for any bad actor especially when our methods of securing our identities haven’t kept up with the pace of it being collected. reply mhuffman 20 hours agorootparentprev>Day to day what does \"monetizing my data and removing my privacy\" actually mean to you? Are advertisers following you around town? Bothering you at work? Hounding you in the loo? They are doing all of those things if you have your phone with you! Unless you manually go about disabling notifications, turning off gps access for all apps, etc. reply godelski 20 hours agorootparentThey do all these things even if you don't. Even if you don't have a google account. The degree in which they do is higher if you do, but that doesn't remove it, it just lessens it. And it is worth noting that even turning off your GPS, you can still be followed. The very nature of how cell service works requires the ability to locate you, even if not to the same precision level (as aforementioned). There are of course trade-offs and this nature does not mean the tech is bad. But it does mean that we need to be aware of how it is used and how it can be abused. It is okay for mistakes to happen, but we need to fix them and resolve them, not ignore them or underplay them. My larger fear is that we live in a cluttered house and all these conversations end up being \"well that piece of paper on the ground isn't that bad. That doesn't mean I'm messy.\" Because that is true. But there's a million instances of that and the accumulation is what creates the messy/cluttered house, not any singular piece of trash. If we can't see the forest from the trees, we're doomed. Because most of the problems that exist in our modern world are through these larger complex chains of coupled interactions. Where singular events are not that bad, but the system is. reply belter 7 hours agorootparentprev> Day to day what does \"monetizing my data and removing my privacy\" actually mean to you? It means you can a permanent, and non erasable, record of being arrested, even if you just happen to be near the same location a crime was committed: \"Tracking Phones, Google Is a Dragnet for the Police\" - https://www.nytimes.com/interactive/2019/04/13/us/google-loc... Disclaimer: I am all for an effective and just law enforcement... > Are advertisers following you around town? Yes the are: \"Jeffrey Epstein’s Island Visitors Exposed by Data Broker\" - https://www.wired.com/story/jeffrey-epstein-island-visitors-... Disclaimer: Never visited Epstein’s Island... reply godelski 20 hours agorootparentprev> Are advertisers following you around town? Yes > Bothering you at work? Yes > Hounding you in the loo? If I'm taking a shit and open my phone. But also, sometimes at the urinal. I mean they put ads right in front of your face. > Personally it seems like my data has been stolen from just about every company I've ever given it to EXCEPT for Google. Google isn't in the business of selling ads, they are in the business of selling data. To make those ads more pervasive, targeted, and effective. A good ad goes unnoticed, it is often \"native.\" But to me, I just think about it this way. If some dude was following me around and writing down everything I do (maybe no the exact content of my words, but who I talk to, when, where I go, etc -- metadata), I'd be fucking pissed and call law enforcement for stalking. Now I don't understand why this is okay if the dude's name is Mark, Sundar, or Satya. I don't care that they're really good at hiding and that their note taking often goes unnoticed. I think it is creepy and immoral behavior. And beyond that, I know what that data can do. Yes, it can be used to make a lot of useful things. I work in ML, data is POWERFUL. But a coin is a coin, and it can be spent on good things or bad things. Hell, there are even good ads. But the truth of the matter is that there are far too many abuses. More than I think is acceptable. I worry about growing abuse and the slow boiling of a frog[0]. The temperature (both metaphorical and literal) has risen all my life, I see no reason to think that it will cool down if we just continue business as usual. Each little instance of temperature rise may be small and not uncomfortable, but that doesn't mean that given enough time the temperature doesn't rise to a deadly point. I cannot stress this enough, \"the road to hell is paved with good intentions.\" Evil is not (exclusively) created by evil men doing evil things. Evil is more often created by good men with good intentions. It is because the world is complex and because what we build and what we create takes on a life of its own, outside our hands, and outside what we imagined it could/would do. Ethan Zuckerman is not an evil man for creating something so universally hated, and he even created it with good intentions. But this is just the nature of the world and it means to be vigilant and careful. To be clear, it does not mean to stop progressing. But it does mean to be aware that the environment changes and that it is easy to get off track. It's easy to point fingers at specific people, but I think it is more important to recognize the complexity of reality. Because if that isn't considered, you just create a vacuum for the same evil/abuse to rise again. And again, often by people who have no malintent. And that's the issue: the world is too fucking complex and we want it to be simple(er) and we will often try to bend over backwards to make it so, because we humans were not designed for this. But we are capable of processing it. So, will we? [0] https://en.wikipedia.org/wiki/Boiling_frog reply pawelmurias 20 hours agorootparentprevThey are in a business of showing you ads. Showing your private data to someone is not in their interest. reply entropie 20 hours agorootparent> Showing your private data to someone is not in their interest. Showing your private data to someone is not in their interest yet reply noman-land 19 hours agorootparentprevThey are in the business of uniquely identifying me and organizing vast real-time bidding wars for every moment of my attention. The winners of the bidding war are the ones whose ads I (would) see (if I didn't block this garbage aggressively). It's also been well studied how so-called anonymized datasets can be de-anonymized in various ways and then all the historical data is de-anonymized. What you once thought was benign and not noteworthy can easily be used against you as the times and attitudes of a society change. Many examples can be seen both in modern day \"cancellations\" as well as in the historical record like Nazis using census data to find people with undesirable characteristics and murdering them (with IBM's help). This is not far fetched conspiracy theory paranoia bate. It is factual events from reality. If a company has a contract with an entity who pays them money for their services, it is literally in their interest to do it. reply m463 16 hours agoparentprevI think we should applaud the people who take these steps. These tech companies work with all the rest of the companies to use our own computers against us. Most folks have normalized the data collection, the dark patterns and the faustian bargains that are now happening on a daily basis. Treating them with ridicule is just not necessary. The folks who say no are really working for the rest of us. I can give you a couple website that are intimately intertwined with google services: - the california FTB website (califorina's IRS) - the california DMV website (not only drivers licenses, but real id) - kaiser permanente (health care) Most normal people, or your kids, or your parents are required to use these sites (or it is very difficult not to use them). Why should they be tracked by advertisers? why should nobody complain? https://www.theregister.com/2024/04/26/kaiser_patient_data/ reply pennybanks 16 hours agoparentprevfor me its not about losing anything important or practicing safety with backups. i just dont want bloat and activity everywhere. too many connections too many extra features suggested or mandatory etc etc. i guess im a minimalist in that sense. just want cleaner faster everything even if its a bit more inconvenient. reply assimpleaspossi 22 hours agoparentprevWhat happens if fastmail goes belly up? reply toomuchtodo 21 hours agorootparentMX record change and you’re off to the races. Tangentially, I’d also be interested in putting an acquisition together to acquire FastMail if ever in financial peril, in order for it to persist as a going concern and arguably, a public good (like Let’s Encrypt). reply varun_ch 22 hours agorootparentprevIf you own your domain and make regular data exports/backups, nothing. reply mfiro 22 hours agorootparentprevIf you use it with domain you own, you can just point it to another email provider. reply gessha 20 hours agorootparentprevWhat happens when Google kills Gmail? reply assimpleaspossi 19 hours agorootparentThat's my point. reply joshuaturner 22 hours agoprevI can appreciate the hesitancy because you need an account, but Kagi is such a great search engine. Their incentives align much better with their actual users and because of that they support tons of customization/filtering options Google just can't (read: won't). reply abenga 10 hours agoparentI tried Kagi once and never really felt like it was good enough to make me want to pay for it. Search is like air. Very important, but available everywhere for free. Pretty hard for me to justify adding another subscription to the already extant mountain. reply pbronez 7 hours agorootparentContinuing that metaphor… Google is the air of a teeming metropolis in a developing country. Every breath is polluted with foreign substances that someone else out there. Your capacity suffers as each action is taxed by the aerosolized tar. searX is mountain air. Pure, but cold and so so thin. You struggle to collect enough oxygen, waiting for the life-giving molecules to trickle in. A bracing adventure for most, a home for the hardy, adapted few. DuckDuckGo is the air of a rich suburb. The pollution is manageable and pressure sufficient. You breathe freely, undistracted, content in the assumption that your essential need is satisfied. It is sufficient, comfortable, unremarkable, there. Kagi is a seaside breeze, wafting the richness of a vibrant and alien ecosystem through your awareness. You may sense the oily tang of a nearby dockyard; it’s but one note in the harmony, fading behind the marine life, the ocean salts, the sizzling morsels of upshore boardwalk. You breathe deeply. Your awareness stretches, stitching scent to sound; your focus dances across the possibilities, musing which possibility to explore today. A corner of your mind resents the cost of your rented villa; another fantasizes about buying it, about making this place your home. Could I afford it…? reply Kye 4 hours agorootparentWhere's the town that's always trying to reinvent itself, with the layers of failure and renewal visible in the back alleys and unfinished remodels left behind in the last economic crisis? I want that search engine. reply pbronez 4 hours agorootparentSounds like Bing! reply least 23 hours agoprevThe only thing irreplaceable for me is Youtube. Even if it's gotten worse as a platform over time, I still consume it more than any other type of video media and there are no good alternatives for much of the content that I enjoy watching. reply autoexec 23 hours agoparentThe good news is that none of the content you care about is Google's. As soon as someone else provides a space for content creators to move to you can get rid of Youtube and never miss a single thing about it. Google has successfully pissed off enough youtubers that they're just waiting for the first chance to jump ship, just like we are as watchers. In the meantime, try to give as little as you can to Google. Use Newpipe and youtube-dl so you never see an ad. reply hot_gril 22 hours agorootparentThis generously assumes that the new provider won't take the same steps YouTube did. I feel like the only solution is to not care all that much about watching videos. reply AmpsterMan 21 hours agorootparentprevNebula is such a streaming service. From what I understand, they are a subscription based company that has a profit share incentive for it's creators. Consequently, the creators skew toward educational-esque content. reply annexrichmond 23 hours agorootparentprevYeah but operating a service like YouTube is such a money pit (given on recent HN threads I've read). It seems like they've been mostly able to work because of their huge ad network, targeting, etc. reply marcosdumay 22 hours agorootparentIt's not inherently a money pit. Google and Facebook have a complete oligopoly over the ads market and are manipulating it so nobody else can prosper. reply hot_gril 22 hours agorootparentIt seems like inherently a money pit to me. Was Google even profiting off it before they started cramming in ads and offering YT Premium? reply malfist 22 hours agorootparentprevServing video data to, potentially, billions of users is absolutely expensive. You can't, \"in a weekend\" a youtube clone that can stand up to the first viral hit. reply skeledrew 20 hours agorootparentThat's what BitTorrent is for. The main issue with torrenting is that non-viral content is going to have very low availability. reply smaudet 23 hours agorootparentprevYeah, the important 3rd wheel of the video space: 1) Hosting space/cpu/bandwidth. Video is one of the most intense data jobs out there, behind I suppose maybe AI and Big Data (if anyone is still doing that). You need Terabytes of drive space, petabytes of traffic, all the CDNs, processors capable of encoding inefficient video uploads to efficient video downloads. 2) Network effect. People need to actually want to come, so that people come. 3) Paying for all this - People don't like donating, don't like paying, but hard drives/servers/power banks are not free. So you either charge premiums or use ads. I wish more people had been willing to pay for Youtube Pro, e.g. (I still do), they're (still) getting worse because the ad-supported version(s) are still the bulk of their income. And so the ad-supported version is messing up everything, being very aggressive, etc. You/I pay for Netflix, why wouldn't you pay for Youtube? You can't influence the dynamic with capitalism at all when something is free. Sure, companies can \"en-shittify\", but at least for video this is fairly simple - when it costs too much/works too terribly on one video platform, you can move to the next one. 1) and 2) Make it harder to start a new paid platform, but not impossible - you can directly reflect the costs to your users. YT is just a black box at present...but would have been less so if Youtube Pro had been more popular (ads might have been easier to block, even). reply bbarnett 22 hours agorootparentThere's a space for Netflix here. They have the \"TV\" class subscribers, they have all the networks, knowhow. All the initial groundwork is done. All they need is a platform for the youtube class content provider. They already have ad deals too. They could, if they found some model that works, snap up loads of talent.... and that talent could even stay on youtube too. As long as they keep content in its own category... reply mobilejdral 23 hours agoparentprevThat is so wild to me. I only consume media there if I am sent a link. Every time I have gone there I find content that is literally optimized to use up and waste as much of my time as possible under the incentive of maximizing viewer minutes. I wonder what it would be like to spend a day in your shoes. reply itishappy 23 hours agorootparent> Every time I have gone there I find content that is literally optimized to use up and waste as much of my time as possible under the incentive of maximizing viewer minutes. It's a feed, and it responds to your use. I click on longform educational content, I get lots of longform educational content. I find the act of opening clickbait videos in an incognito window along with judicious use of the \"not interested\" button has kept it quite usable. reply the_af 22 hours agorootparent> It's a feed, and it responds to your use. I click on longform educational content, I get lots of longform educational content. Yup. You still get the occasional dud, but most of the feed is relevant in my case. I once browsed YouTube without being logged in. Do not recommend. I wanted to gouge my eyes out. I imagine it's a similar experience to browsing the web without an adblocker. reply corytheboyd 19 hours agorootparentprevMindlessly consuming the video feed is not the only way to use YouTube. If you are looking for something in particular, it is very hard to beat. Besides, at that point, is it really any different than TV, or other streaming services? reply thebruce87m 11 hours agorootparent> it is very hard to beat For any content I’m interested in, a blog post with pictures beats it. If it needs video then have it interspersed throughout the page. LLM summarisers are the only thing that make YouTube usable for me in that I can still get the content without wasting time. reply waveBidder 20 hours agorootparentprevI more or less ignore the front page and use the bell as an rss, having found channels I like via recs from channels I like and friends reply hot_gril 22 hours agorootparentprevSame, with a few exceptions. Once in a blue moon, I check if one of the two creators I care about has something I want to watch. reply readthenotes1 22 hours agorootparentprevLive the speedup ability in YouTube videos. Minimum 1.5x, some faster (because it's pretty obvious the creators slow the content down) reply the_af 22 hours agorootparentprev> Every time I have gone there I find content that is literally optimized to use up and waste as much of my time as possible under the incentive of maximizing viewer minutes. Depends on what you're watching. I mean, everyone on YouTube is playing the algorithm game somewhat -- assuming they want to have their videos watched, and they do, or they wouldn't be there to begin with. Having said that, I watch a ton of YouTube videos. Not randomly, like zapping through TV channels (do people still do that, by the way?). I watch videos from the dozens of channels I'm subscribed to, by authors I enjoy watching and about topics that are relevant to me. YouTube is unbeatable for this, because almost everyone is on YouTube, but few other video platforms are as universal. And yes, I pay for YouTube Premium because ads absolutely break YouTube. Ads make everything worse. I'm still upset about the recent trend of authors placing \"sponsor segments\" embedded in their videos (looking at you, Squarespace -- you can go f*ck yourself); I wish paying for Premium automatically skipped these segments too, but oh well. At least some authors make self-deprecating jokes about their sponsored content. reply teddyh 19 hours agorootparent> I'm still upset about the recent trend of authors placing \"sponsor segments\" embedded in their videos […] ; I wish paying for Premium automatically skipped these segments too, but oh well. May I introduce you to SponsorBlock:reply the_af 13 hours agorootparentWow. Didn't know this existed. Will definitely try it, thanks! reply tcfhgj 19 hours agorootparentprevReVanced (phone) rsp. SponsorBlock allows to skip ads and sponsorship segments (and more) reply guyzero 22 hours agoparentprevI find it crazy that someone will say \"I still consume it more than any other type of video media\" but somehow just paying for it isn't an option. You can pay to make YT ads go away. And the payments go to the video creators (eventually). reply lc5G 22 hours agorootparentI would pay for youtube if they provided an official API to download videos and stream videos in a client of my choice. I prefer to watch youtube with a custom player (mpv+yt-dlp on desktop, newpipe on android). I do not want to watch youtube on the youtube website or official youtube android app. The custom clients I am using are unofficial. They reverse engineer youtube to make it work. They often break when youtube changes something. reply zamadatix 22 hours agorootparentprevInterestingly the comment you're responding and the article the comment is responding do not mention ads as the problem being solved by de-Googling. I also think YouTube has gotten worse over time and I'd be happy to find a workable replacement despite being a long time YouTube Premium subscriber. I do use SponsorBlock though as I found supporting creators directly while paying YouTube for serving the content left still getting sponsorship ads as a bit ridiculous. It's a good product, just often intentionally shy of being great in interest of metrics reply tomrod 22 hours agorootparentprevFlex the steelmaning and incentivize me to buy YouTube Premium when adblocking gives me the same functionality and I own the process to block. I'm happy to watch an occasional ad. Maybe 2 minutes for every day, curated and high quality advertisements relevant to everyone. No, I will not pay with my data. No, I will not accept interruption in flow, within or between videos. Interrupting during a video means adblocking is justified. Adding adverts to nonmonetized channels means adblocking is justified. It's my time. Google isn't paying for the use of my eyeballs, and I actively don't purchase things I see advertised via YouTube. I'm not alone in this; most people don't buy for digital ads. They already nefariously steal my data at every opportunity. reply zamadatix 22 hours agorootparentI've never found defaulting to paying for services I consume in need of steelmaning but I suppose it ultimately comes down to how much you care about that sort of thing than the particular reasoning in the end. The biggest reason I won't pay for content is because it's harder to consume that way. E.g. actual Blu-Rays are often impossible to legally play the way you want on the devices you want, especially in comparison to a pirated mkv. YouTube Premium is extremely easy to pay for though, even for the whole family and all my devices. Solutions like uBlock Origin (which I use on many other sites) are actually harder and more fragmented in approach if I want to cover all the same usecases so, in the continued spirit of steelmaning each side, why should I seek to \"pirate it\" (for lack of a better term) instead? By the same logic I use SponsorBlock when I can because despite paying the creator and paying YouTube the hardcoded sponsors are still present (plus it has uses outside skipping just ads anyways). reply tomrod 21 hours agorootparent> By the same logic I use SponsorBlock when I can because despite paying the creator and paying YouTube the hardcoded sponsors are still present (plus it has uses outside skipping just ads anyways) Got it. So you are fine with adblocking; your take comes down to practicality. Debate aside, I appreciate your thoughtful response. reply zamadatix 19 hours agorootparentYeah definitely a bit of practicality but mostly the factor of having already paid everyone that would be getting paid yet the system not being built to adjust to that properly (YouTube only has one version of the video it'll serve and no knowing that I'm subscribed externally to the person either). Excluding some other somewhat reasonable scenarios which break the mold in corner cases (like accessing the dozens of subscription news sites that get posted on HN with no way to actually pay for them properly except spending more time managing the subscriptions than actually reading article or two a day) my \"I paid everyone for the content so I can consume it the way that works for me\" stance and reasoning doesn't really apply when you intentionally skip the first step \"I paid everyone for\" out of disinterest in doing so. reply stnmtn 22 hours agorootparentprevThe main reason (imo) would be if you wanted no ads on Mobile without a lot of hassle, and the ability to play audio from youtube with your phone screen off. reply tomrod 21 hours agorootparentAh, Newpipe! reply violet13 20 hours agorootparentprevThey by and large don't. Most creators on YouTube aren't even eligible to monetize, especially if they're making niche educational content. Google just pockets all the revenue if you have fewer than 4,000 \"public watch hours\". If you put together an a concise 4-minute DIY video, an average view time will probably hover around 1 minute, and you will need 240k views to qualify. Most videos on YouTube get under 1k views. There is a negligibly small percentage of content creators / content farmers who live in a symbiotic relationship with the platform and actually make good money, but let's not pretend you're patronizing the creators when you're paying for YouTube Premium. reply Jensson 13 hours agorootparent> Google just pockets all the revenue if you have fewer than 4,000 \"public watch hours\". If you put together an a concise 4-minute DIY video, an average view time will probably hover around 1 minute, and you will need 240k views to qualify. Most videos on YouTube get under 1k views. With less than 4k public watch hours your channel has barely made any money. It is accumulated for your videos for the entire channel, that isn't per video. And no, youtube isn't making money on those videos, it costs them more to store etc and most of them aren't getting any ads anyway. Instead you get free video upload on youtube, normally you pay for that. Youtube makes their money from the big creators that draws in a lot of views per video, there efficiency of scale lets them get over the profitability hump. reply PodgieTar 22 hours agorootparentprevI’m conflicted on this. I pay for YouTube premium - for background playing on my phone and for Ad-less on my TV. I pay more for Netflix, and I use it a lot less. So I’m airing on the side of agreeing, but I also think Googles tactics have been incredibly shoddy, and I can understand someone not wanting to support that. reply fossuser 21 hours agorootparentprevMaybe they do pay for it? It's not obvious they don't. (edit: they mention they do pay for it below) I would have written a similar comment and I pay for Premium. I don't use any other Google service at this point, but I do have an account just for YouTube (I dig into the privacy settings to disable all the things I can). It wasn't really that hard if you use Apple for almost everything and Fastmail for mail. I use DuckDuckGo for search and occasional search again on Google, but that's becoming less relevant over time with LLMs (and generally reddit or youtube is the best place to search anyway for most things). reply least 21 hours agorootparentprevI do pay for Youtube Premium precisely because it is something that I use basically every day. It's just kind of an opaque system where it's not really clear how much it is actually supporting video creators, how much of it is maintaining the service, and how much of it is lining Google's pockets. I watch quite a bit of videos by creators with very lower viewership and sub counts and I suspect they're not seeing a penny. reply YurgenJurgensen 20 hours agorootparentprevPaying won't make Shorts go away. Nor will it remove the 8 other elements of the YouTube UI I have to block to make the site somewhat useable. Also I believe the payouts are split entirely by watchtime. Three minutes of watching an original song with a high-quality music video is worth more to me than 8 hours of some game stream or rambling video essay I happened to have on in the background while doing chores, but if YTP values both equally per minute it'll basically send no money to the creators who I believe bring the most value to the platform. There's even a like/dislike system, but there's no indication that it matters for Premium money. Channel memberships and superchats don't even remove ads, so even if I did decide to sub on a per-channel basis, I'd still need an ad blocker. reply jemmyw 20 hours agorootparentHow should they split the money then? By time seems the fairest way. How else should they judge the quality? That music video you mentioned is likely to get more views so more money overall. But if the gamer stream is getting more views and watch time why shouldn't it get more money? reply YurgenJurgensen 19 hours agorootparentLikes and dislikes, like I said. It's literally a feature where the users judge the quality of the video. reply jemmyw 7 hours agorootparentThat doesn't seem fair. If you produced something and everyone watched it and disliked it... well a) why did they watch it then and b) they won't watch your next thing... Or they will because they disapprove but cannot help themselves. Either way, your content drove the traffic regardless. reply rc_mob 22 hours agorootparentprevI don't want to send google money. I happily pay nebula though. reply riddley 22 hours agorootparentprevIf only that were true! Like the OP I consume a ton of YouTube and despite my misgivings about Google, I was a long time subscriber to the YouTube/music combo deal until they started showing ads despite my subscription. It would happen a few times a week, and if I'm being charitable I could chalk it up to a deployment error, but it happened too many times for me to not get the impression that no one at YouTube gives a shit about subscribers. reply sneak 22 hours agorootparentprevYou can’t pay YouTube to remove in-video ads that the video creators put in. For that you need SponsorBlock anyway, so might as well use a 3p client that supports blocking both kinds of ads, without giving money to Google to continue building ad surveillance software. reply ok_dad 22 hours agorootparentI don’t mind ads in the videos for creators I enjoy, it pays their bills. There’s a difference between those and YouTube’s ads which the creators don’t get a big percentage of. I can easily fast forward those ads if I want to, anyways. How do you expect all this content to be made without any sort of money? You’re basically leeching off the rest of us, in this case, IMO. I hate ads, but right now that’s the only ticket, or paying for premium. Perhaps that will change, I hope. reply jsnell 22 hours agorootparentThe creators get the majority of the ad revenue from YouTube's ads (it's a 55/45 split). reply ok_dad 20 hours agorootparent55% isn’t big to be when you consider they made the content, though the cost’s for video are probably higher than most things online. reply sneak 22 hours agorootparentprevI mind all ads. They rob you of your most precious and nonrenewable resource without consent. Ads are cancer and should be excised at every opportunity. Ad companies should be destroyed. Ad tech workers should be blackballed. Shilling is shameful. reply guyzero 21 hours agorootparentThis entire forum is an advertisement for an investment firm. Many posts are advertisements for startups. reply sneak 10 hours agorootparentNot really. HN would still be excellent and useful and valuable even if pg and yc ceased to exist. reply ok_dad 20 hours agorootparentprevI’ll agree in some capacity that ads are a scourge today, but what’s your alternative? You can’t just burn down the world ave start fresh, there has to be a better way and a plan to transition. Those who would just burn the world down are not mature. This applies to just parts of the world, too, such as “advertising must burn” or “let Y country burn because of X problem” (paraphrasing). I cannot agree on blackballing workers. That’s just another form of control we shouldn’t exercise. Imagine if someone blackballed you for your career doing whatever you do? They’re just trying to survive and earn a living. Maybe I’ll agree on shaming and calling out execs though. reply sneak 10 hours agorootparentIt’s possible to survive doing things that aren’t shameful and destructive. If we don’t shame and blackball people for their freely taken adult decisions about how to spend their time and direct their effort, what can we shame or blackball people for? It should be hazardous to your career to engage in certain types of business. Mass murder, advertising, spyware, spam, network abuse (DDoS), or conspiracy to commit same should always make people think twice about hiring you. reply guyzero 22 hours agorootparentprevDo you walk out of a movie once someone drinks a Coke on screen? This seems like a level of ideological devotion that makes doing anything pretty difficult. reply SadTrombone 21 hours agorootparentDo you have any examples of a movie where someone spends a couple of minutes drinking Coke while talking about how delicious it is and why you should buy it? reply annexrichmond 23 hours agoparentprevI've personally been watching YouTube less and less. The ads are insane. I would normally spend more time watching videos but it's become so disruptive I think most of us can agree that many videos are unnecessarily long (as it's incentivized by the creator for ad revenue), so there's far more fluff in it. With the rise of LLMs, etc for getting knowledge fast , I just can't see how this model is sustainable. I don't want to waste time watching videos that could have been a short paragraph. And when you add 30 second ads every 5 mins it makes it even less compelling reply lawn 23 hours agorootparentDon't everyone on HN use ublock origin? I haven't seen an ad in years. reply instagib 22 hours agorootparentSideloaded the latest and greatest YouTube++/uYou/uYouPlus/(currently uYouEnhanced) on my iPhone and re-sign it weekly with AltStore, Trollstore, or use a jailbroken equivalent on a device that managed to stay jailbroken. There’s a few other new options I’ve seen through Reddit for web viewing, iOS content blockers, or hosting your own VPS invidious/piped instance with rotating ipv6 servers (not google/aws/digital ocean as they require using a web panel). You can leave ads on for whitelisted creators sponsored advertising content also. reply rav3ndust 22 hours agorootparentprevsame. firefox with ubO is my best friend. reply rossjudson 22 hours agorootparentprevI don't want the ads, but I'd rather see the carefully curated list of creators I care about be able to get paid, and Youtube Music works for me. Youtube Premium is worth it. reply usmcnapier 23 hours agorootparentprevSame here. reply riddley 22 hours agorootparentprevPhones and TVs. reply bsder 22 hours agorootparentprevYouTube seems to be doing subtle dark patterns if you use uBlock origin. For example, I can watch videos, but I often get an \"error\" if I try to seek in the video. reply wolpoli 22 hours agorootparentprevIt's harder to avoid bad quality videos that now you can't see downvotes. As for the video being unnecessarily long - I have played with copying the transcript from the video into ChatGPT, but they tend to be too long for ChatGPT to handle. reply lawn 22 hours agorootparentKagi has an Universal Summerized function that can give you a nice summary of a video. reply throw_pm23 23 hours agorootparentprevWhere do you see ads on Youtube? I have seen maybe one or two in 10 years. reply SoftTalker 22 hours agorootparentMore and more content creators are just embedding them directly into their videos. Either by just mentioning sponsor names in the flow of what they are talking about, or doing little sidebars where they talk about/promote their sponsors. I don't mind these so much because they are easy to skip, but ad-blockers don't catch them. reply the_af 22 hours agorootparentprevI see them almost guaranteed unless I'm logged in. I ended up paying for Premium to remove them. reply the_af 22 hours agorootparentprev> The ads are insane. The ads are indeed insane. They break YouTube in practice. YouTube, in its wisdom, much like a mobster in the protection racket, offers you an out: pay for Premium. I ended up doing this because YT is one of the things I use the most. The alternative was having Firefox with uBlock Origin on every mobile device in my household, I suppose, but this wasn't feasible. YouTube Premium took the win :( > I just can't see how this model is sustainable. I don't want to waste time watching videos that could have been a short paragraph Depends on what you're watching. I always disliked, say, programming tutorials in video form -- give me text, to read and understand at my own pace. Many video tutorials are unnecessarily longwinded and always go at the wrong pace for me, either too fast or too slow. But I watch tons of videos about other topics where video is the right form. I watch hobby tutorials, I watch videos about cinema/art, etc, and I feel video is the right format for me. reply cjk2 22 hours agoparentprevI use yt-dlp to download the videos, then play it on VLC on my iPad. Much nicer experience! reply qwerpy 20 hours agorootparentThis is also my preferred way of consuming YouTube. In addition to zero ads, you get instant seek, consistently high bitrate, ability to watch anywhere, and no autoplay for the kids to get sent down an addiction spiral. reply neilv 23 hours agoprev> OpenStreetMap-based apps like Organic Maps are great and I frequently use them, but I find myself using the Google Maps website a lot, for tasks like searching for restaurants to getting an estimate of how long my commute is going to take. Agreed, as someone who walks almost everywhere, Organic Maps is great on GrapheneOS. The only things I use Google Maps for are more rare: (1) see what hours a store is open, which IIUC is being worked on by OSM; (2) planning a trip via public transit; (3) StreetView, which is really an awesome feature. reply NoboruWataya 22 hours agoparent(1) in particular is something that you can easily contribute to fixing yourself. Organic Maps lets you edit store details from within the app. Personally, (2) and (3) are the ones that keep me on Google Maps, along with the real killer feature for me which is that I can just search \"good {coffee,food,whatever} near me\" and get a list of pretty good recommendations. reply jiayo 22 hours agoparentprevFor what it's worth, both Apple Maps and Bing Maps now have street-view like features, at least in my neck of the woods (Canada). reply colingoodman 22 hours agorootparentJust about all of Apple's alternates to Google services are fine in my experience. I recently switched from Proton Mail to Apple Mail and have been satisfied saving a few bucks a month there. I don't know how to evaluate Apple's claims to privacy though. reply kelnos 22 hours agorootparentprevI guess that's fine for diversifying the big corporate overlords you rely on, which I'll agree is an improvement over relying on Google for everything. But still, I wish there were open alternatives. reply settsu 20 hours agoprevThe impetus for my years long (i.e., lazy) effort to de-Google wasn't just about the privacy issues but that those seemed to stem from an utter lack of empathy for users overall. In other words, it was recognizing an intolerable pattern of behavior, which seemed inevitable that it would cause me actual, material harm in some way eventually. reply lotophage 16 hours agoparentI was very much in the Google ecosystem and although privacy became an increasing concern, the final push for me was the horror stories who had their accounts suspended or were otherwise locked out with no recourse. reply barfbagginus 18 hours agoparentprevDoes the collapse of all knowledge as we know it count as a material harm? Or is it a revolutionary opportunity to craft a new Knowledge Society independent and protected from the abuses of the old and failed knowledge order? reply blauditore 22 hours agoprevAlthough this author seems pretty reasonable with regards to actual privacy (preferring self-hosted software), I find the popular echo chamber of \"avoid Google, they steal your data\" quite misguided. Yes, Google accumulates data and does stuff with it. But Google also has rigorous processes to lock down data and access to it, unlike virtually any small-to-medium cloud software provider. I've heard crazy stories like people looking up their friends' health insurance details for fun, just because almost everyone in the engineering part of that company had access to the production database. Plus, Google is so large that it constantly receives attention by public institutions, which makes it harder to pull off shady stuff without getting caught. Ifsells your data to the highest bidder who will spam you with cold calls, no one's gonna bat an eye. reply noman-land 21 hours agoparentDon't forget any large hoard of data is ripe for government abuse. It might not be a cold caller but a police department parallel constructing you into a crime conviction using faulty GPS data. reply blauditore 21 hours agorootparentNever though of it that way: With enough data, it's likely to find something that looks suspicious, for some definition thereof, even if just due to software errors. When looked at in isolation, this could convince people like real evidence. This would basically be the same as p-hacking. reply bogwog 22 hours agoprevOP and I are very similar lol. I bought a Pixel 7 Pro on eBay specifically so I could use Graphene (screw buying new; used is good for the planet and it doesn't give money to Google). So far, I have been loving it! Also switched to Organic Maps, but still use Google Maps for finding businesses/hours. Organic Maps is nice, but searching for directions is awful (at least in my South Florida area). Typing in any address tends to return results that only show the name of the street, and they're all identical. E.g. just a bunch of \"Southwest 123rd Street, Florida\", no house number or anything useful to distinguish them. Often times I'm forced to use Google Maps. This is currently the hardest Google service to quit for me. I went from DDG to Kagi, and stuck with Kagi because of the superior results. Luckily I got in early, and have an early adopter plan. It's a shame they can't offer better pricing though because it really is a superior service. Recently I've had some privacy concerns that have made me a bit skeptical of Kagi, but it's hard for me to go back to DDG or even Google now. It's just that good. For files I use Nextcloud exclusively. While it is slow and bloated as hell, I like it because I share the instance with family members, and it's easier for non-techie people to use it. It's also possible to mount it as a network drive using WebDAV so you can skip the bloated web interface. It works very well for me on KDE Dolphin. For email I use ProtonMail with custom domain too. They're overpriced and overrated, but it's too much work for me to switch at this point, and their app isn't that bad. I would recommend FastMail to anyone looking for a new email provider though. For YouTube, I use FreeTube. It's basically just a custom front end l. No ads, no spyware. Sure, I deprive creators of ad revenue, but that's a good thing. The less money they make on YouTube, the more likely they are to post on different services. I haven't heard of Nebula before, so I'll definitely be checking that out. reply Andrex 19 hours agoparent> For email I use ProtonMail with custom domain too. They're overpriced and overrated, but it's too much work for me to switch at this point, and their app isn't that bad. I would recommend FastMail to anyone looking for a new email provider though. I went the opposite route, but I feel like Proton is better geared to business use (the plans and included products just seem more attractive to me). And yeah the Swiss angle is mostly PR but eh, maybe that could be useful and it's one more thing. \"Defense in depth,\" kinda. When I switched I didn't really notice Proton being that much more expensive for my uses either. reply ezekiel68 21 hours agoprevThis seems like an awful lot of work for little benefit. Although I use and pay for kagi, it's not to de-Google. It simply is a superior search engine which honors semi-advanced search filters better. However, when I want to search sholarly refernces, Google search reigns supreme. I guess I'm just a rube, but it never occurred to me to demand much 'something' (all the stuff Google provides me) for 'nothing' (refusal to allow use of my annonomized data). Since I bother setting my cookie settings on all new websites, I don't really get bombarded with personalized advertising anymore. That's my middle ground. (I also pay for YouTube Premium, so I don't see the ads there). reply neilv 22 hours agoprev> I was even able to take trips through Uber’s mobile website, which was a pleasant surprise. The location functionality did not work, but I believe that was due more to the browser than to Uber itself. I'm delighted that this works now. Of course companies like the proprietary-platform apps, in some ways. So it's great to see a tech company also embrace Web open standards as an option for users/customers. reply yaky 22 hours agoparentLyft's website used to work really well circa 2019, but stopped shortly after. reply hfe 20 hours agoprev> my second issue with Kagi: privacy-wise, I simply don’t believe that a service that requires login and thus naturally associates all your activity with your identity is better than a service which can be used without a login. I don't get this argument. Do you want to be protected from the services you use, or do you want a partnership based on trust? With aligned incentives, trust is easier. Without trust, not logging in won't save your privacy. Just pay the nice people money for the very nice service they provide, and let them build trust with you. reply demosthanos 23 hours agoprev> I think Nebula.tv is promising, and a bunch of creators I like upload their videos on it. I do have a Nebula account but rarely end up using it, since most videos get uploaded to YouTube as well, and when I open Nebula, I find myself having already watched all the interesting videos on YouTube. I've just gotten in the habit of opening up Nebula first and only then switching to YouTube when there's nothing new there. Even if you do this only once every few days it's usually enough, because a lot of creators upload to Nebula a week before they do YouTube. reply riddley 22 hours agoparentNebula drives me insane. I want so much to like it, but the TV app is so terrible. You can follow creators, but it won't show you when they release a new video. Did you have to pause a video and go away for a bit? Well too bad. Unless you search the name or use their bookmark feature, you can't get back to it. There's no way to tell it that it's suggestions aren't interesting. If the thing they've put a huge banner across your screen about is a podcast and not a video, clicking it does nothing. No error, no nothing (again on the TV app.) reply cyberpunk 23 hours agoparentprevSadly none of my 'creators' are on nebula; But one thing worth trying if you do want to avoid going to youtube -- just have some cronjobs running yt-dlp every few hours on each channel you're interested in on YouTube, served up with a little auto index httpd via Tailscale -- makes for a pretty nice experience. (Seems to be limited to 1080p, but I don't really care about the quality of those videos tbh). reply bogwog 22 hours agorootparentI just use FreeTube. It's perfect. reply derlvative 23 hours agoprevI liked duckduckgo a couple years ago before it started insisting on showing results in the language associated to my IP address. Since then I've liked brave a lot better. reply autoexec 23 hours agoparentDuckduckgo used to advertise that they were all about getting out of the filter bubble but they insist on throwing a bunch of irrelevant totally unrelated garbage in their search results based on my IP address. The service has really gone downhill and I can't even blame it on bing reply SassyBird 22 hours agoparentprevI’ve been getting weird results for a while: somewhere around the end of the first page of results or beginning of the second I get results which have nothing to do with my queries. Instead they look as if someone entered geoip result for my ip address as text of the query. Maybe those are some LLM experiments and that’s part of the prompt now? reply neilv 23 hours agoparentprevIf you're using the DDG Web site, and a bookmark, can you force the language in the bookmark? Example: https://duckduckgo.com/?kl=us-en&kad=en_US reply causal 22 hours agoprevI appreciate that this also avoids other big tech solutions; i.e. MS and Apple. reply rav3ndust 22 hours agoprevi 'de-googled' my life quite a bit a long time ago - switched to an android distribution with no G services (lineageOS), moved from gmail to tuta, switched search engines, and like mentioned here, the one i can't fully leave is youtube, for the moment. i like odysee a lot as a platform, but it can't fully replace YT just yet for me, as a good deal of the channels i watch are still YT-only. i'd say my video-watching time is split maybe 80% yt, and 20% odysee at this point. reply mfiro 22 hours agoprevMoving E-mail is the easiest part on this journey. For me the biggest reason, was the risk of being locked out of my E-mail accounts and reaching no (human) support. What I found the most difficult was the search engine. Although Google search has become worse, it's still far better than Duckduckgo in my opinion. I've found DDG very inaccurate for non-English queries. reply _kidlike 21 hours agoparentkagi.com you're welcome :) reply kelnos 22 hours agoprevI would really love to run GrapheneOS (or some other Android OS that can prevent a lot of Google's built-in spying), but broken Android Auto, Google Pay, and a bunch of other things would be a complete showstopper for me. It makes me really sad that I've come to rely on a bunch of things that just aren't available at all through open software. reply strcat 15 hours agoparentThe official Android Auto app works on GrapheneOS as an extension to sandboxed Google Play. We don't have our own implementation yet and it's not planned in the near term but it is available. Google Pay doesn't allow alternate operating systems but it does work on GrapheneOS from a technical perspective, it just doesn't allow using it in practice without pretending to be the stock OS on a legacy Google-certified Android device without support for hardware attestation. Most banks have their own tap-to-pay implementation in Europe but that largely got phased out in the US. reply nani8ot 21 hours agoparentprevIs there no bank with their own mobile payment app where you live? reply strcat 15 hours agorootparentIn some regions, banks stopped supporting their own tap-to-pay systems or they never even got widely adopted in the first place. It's much better in the EU than the US, where most banks still have their own tap-to-pay. reply zeagle 22 hours agoprevNice write up. I've had a similar journey. I've been using Android auto on grapheneos without issues FYI. I did switch from nextcloud to Baikal as was using seafile for the file storage anyway. I gave up on protonmail and am using migadu with great effect. My plan was to eventually to switch to immich so perhaps a weekend evening project. reply icar 4 hours agoprevAuto updates work for me with the same setup. I'm curious as to why it wouldn't work for them. reply ein0p 16 hours agoprevPSA: DuckDuckGo is just a Bing frontend. So you’re out of the frying pan and into the fire. The only really independent option is Brave Search. It also happens to have better relevance than Bing, in my experience. reply skeledrew 19 hours agoprevI'm mostly degoogled myself, and disable/firewall every Google app on my device. Pretty hard to switch away from GMail as it's essentially everywhere for me, and YouTube doesn't have a viable replacement. I use 3rd party apps for both though, as well as the store. Re the store, I endured doing manual updates weekly for several years. Then recently when I upgraded my device I discovered Shizuku (allows elevated access without rooting), which connects nicely with Aurora Store and handles the installs. It's like a huge breath of fresh air. reply schaefer 23 hours agoprevFor me the degoogling sticking point is that I have a custom domain (registered through Google domains) that i use for both for my email addresses and my small personal home page. It would be nice to find a host that offers both hosting and email through a custom domain too. reply jiayo 22 hours agoparentGoogle Domains was sold to squarespace. Your domain will be migrated soon/already has been. Check your inbox. > Your Google domain has been migrated to your Squarespace Domains account. You can now log in at Squarespace Domains with your Google account to manage your domain, example.org. This is part of Squarespace's acquisition of all domain registrations and customer accounts from Google Domains. reply jdpedrie 23 hours agoparentprevFastmail lets you use custom domains for email and offers DNS and simple hosting. https://www.fastmail.help/hc/en-us/articles/1500000280141-Ho... Not affiliated, just a longtime customer and fastmail enjoyer. reply stvltvs 22 hours agoparentprevLots of hosting services would gladly provide webmail and web hosting on custom domains. I currently use Dreamhost. reply jorams 22 hours agoparentprevThat's an odd sticking point. Domain registrars that don't do all those things are an exception. For the vast, vast majority of them the primary things they do aside from registering domains is offering email and hosting. reply pyrophane 22 hours agoparentprevTo host email for your personal domain on Google don't you need to sign up for a business workspace plan? reply marssaxman 22 hours agoparentprevI use pair.com for that and they have served me well for many years. reply yaky 22 hours agoprevTo add to your list (I made very similar choices): Standard Notes was pretty decent for acessing notes between multiple devices when I used it. Now that I am back on Android, I use Syncthing for both pictures and notes, without a cloud intermediary. Aegis instead of Google Authenticator. I found it the most straightforward, and it has backups/transfers not tied to any account. QKSMS instead of Google Messages. It doesn't support RCS and reactions come across as texts, but no issues otherwise. Fossify apps for other basic functionality. Something to keep in mind with custom ROMs and OSes is that only some have VoLTE support. Which, with 3G shutdown, is a must in the US, unless you are in a great 2G coverage zone. reply whatindaheck 19 hours agoprevIt really takes effort to scrub oneself of big-tech. I don’t know how non-HN-types are supposed to do it. My weak point has been Apple Photos. It “just works” for me and I really appreciate that it does all processing for faces, cats, etc. locally and in a native app. Every other option I find either doesn’t do syncing, offers an Electron app, or forces viewing through a browser. Does anyone know of a solution that fits the bill? I will happily self-host. reply BetterWhisper 19 hours agoparentWell, do you have the hardware to self-host? People nowadays usually are not okay with 1 week of downloading a torrent like it was in 2005. That is probably how long the embedding will take on your cpu for all the video/picture files? reply whatindaheck 14 hours agorootparentYa I have hardware and already self-host some services. reply poochkoishi728 23 hours agoprevDoes anyone know how private ProtonMail is? I heard admittedly crackpot-sounding rumors that it was some sort of NSA honeypot, but verifying them seems like proving the existence of a divine being. reply balderdash 22 hours agoparentI mean after the Crypto AG / Project rubicon came to light (let alone many of the disclosures since) I don’t think you can rule it out - but at least it’s unlikely your data is for sale /being actively used against you for commercial purposes reply jb1991 23 hours agoprevThe problem is all the shared spreadsheets that are ubiquitous in many jobs and you typically need a Google account to participate in those. reply timbit42 19 hours agoparentYou can still move your personal life off of Google. reply twobitshifter 20 hours agoprevI was an Android user, looked at this mountain to climb 6 years ago and just decided to go with Apple. They offer email, calendar, etc, but don’t run a business model based on ads and data mining. That said, I can’t get away from YouTube and I still keep my Gmail account but only as a backup. reply standardUser 18 hours agoparentSomehow my temperament makes me much more comfortable battling an exploitative captor (Google) than succumbing to a coercive one. The fact that these are the only workable options depresses me. reply daft_pink 19 hours agoprevKagi is totally the only way to degoogle. It’s the only product where the results are clearly better or on par with google. Everything else I’ve ever use I found myself subconsciously back at google a few days later. reply terr-dav 23 hours agoprevhttps://web.archive.org/web/20240531193815/https://blog.nrad... reply 1vuio0pswjnm7 8 hours agoprevThese de-Googling or un-Googling submissions always seem to follow the same pattern: try to find a \"replacement\" for every company Google has acquired. What I never see is someone who proposes \"de-Googling\" and foregoing a replacement. Consider the following generic example. Google acquires some company that intermediates some task that people perform using a computer. Now Google collects data about its users. A hypothetical computer user makes use of this intermediary \"service\" as well as others offered by Google. Eventually user decides he wants to stop giving away data to Google. User then publishes to the web the idea of ceasing to use Google-owned intermediaries by using other, \"equivalent\" intermediaries for each and _every_ service. Or he tries to replicate role of the intermediary himself through \"self-hosting\". But he never considers that in some cases using an intermediary is itself a problem, or may be unnecessary, irrespective of whether it is Google or some other entity. Why does he need to preserve each and _every_ Google acquisition by finding a replacement. The use of an intermediary allows data collection and in almost all cases that's why someone started a so-called \"tech\" company. They saw opportunity to profit from being an intermediary (middleman). The attempt to find a replacement for _every_ Google-owned intermediary seems rather brainless. As if every Google acquisition is something that must exist and must be used. Certainly there is an arguable privacy benefit from using disparate third parties for different tasks instead of using the same one for every task. However there is also a benefit from not using a third party at all, or even from not performing the task via a server (self-hosting). reply hi-v-rocknroll 21 hours agoprevDoes anyone host their own email in a jurisdiction with a domain and VPS US Feds can't seize arbitrarily? If so, which webmail software do you use? Do you handle backups and/or high-availability? reply doublerabbit 19 hours agoparentI bought Zimbra Network Edition, around £2k with 25 mailboxes. Professional enough to resell accounts and works nicely on phones. It supports high availability, mx relays and takes scheduled backups to directory of choosing. I've been satisfied with support and it's a rock solid product that's finally starting to mature after many company take overs. reply thih9 20 hours agoprevRecently doing this got significantly easier for me. In the past years I had to actively work on not using too many Google products. Today Google products got bad that it feels like Google themselves are doing the discouraging. reply christophilus 22 hours agoprevI’ve done a decent job de-googling my life. YouTube is the only real holdout. That’s mainly because I’m too cheap to pay for Spotify. I guess I should get around to remedying that. reply er0k 22 hours agoprevIs there any good alternative to google voice? It's the only google service I haven't been able to replace yet. reply jasonriddle 21 hours agoparentI've been using Numberbarn. I've been liking it so far, no issues to report. However, I've only been using it for two months now. Also, similar to Google Voice, some businesses won't let you use a VOIP number for your phone number. reply anonymousiam 15 hours agorootparentI've been using GV for over a decade. It's the last Google service that I still use regularly. I use mailinabox for my email/contacts/calendar/drive. I've looked, and there aren't any drop-in substitutes for GV. I've had a Number Barn account with two phone numbers for a few years. They seem to provide better voice quality than most alternatives, but their SMS integration is poor. I'm not sure if they even do voicemail, but I don't think they do. reply Dudhbbh3343 21 hours agoparentprevI switched to jmp.chat a few months ago and have been impressed. You can even pay anonymously with crypto and have multiple numbers using a single app and billing account. reply anonymousiam 15 hours agorootparentLooks interesting. I got as far as where it asked me to pay them $15, which I'm reluctant to do without first trying out the service. How does one receive a Referral code? reply Dudhbbh3343 4 hours agorootparentLooks like I have 2 referral codes that give new users a free month, I believe without having to pay anything. If you reply with a way to send it to you privately, you can have one. reply anonymousiam 3 hours agorootparentThanks! Please send one to anonymousiam@zeptodata.net. reply timenova 23 hours agoprev+1 for Nebula [0]. It's a great streaming service, and while the author primarily watches YouTube, and goes to Nebula occasionally, I do the opposite. I use it as my primary source for news (TLDR Daily), and watch content from creators such as RealLifeLore and Half As Interesting, among many others. Because of that, I rarely open YouTube, and even then I use SmartTube [1] on my Chromecast which gives a great experience. [0] https://nebula.tv [1] https://github.com/yuliskov/SmartTube reply kkfx 4 hours agoprevFor me the point is not privacy, simply this war is lost. The point for me is data availability, I do want to own my data. So for emails and service I do own my domain name, so I can change third parties services, if I use any (email) in a transparent manner for my correspondent. I own the domain, I own the mail address, my website URL etc, I have a \"home address\" on the net. Similarly I do use third party email services, simply because hosting one myself meaning ending up too often in GMail and co spam, BUT, I own my mails because I do not keep them on third party IMAPs but on my homeserver, the mail provider is just a relay of my \"incomplete\" mail infra. For contact I've used Radicale, now testing https://github.com/tchapi/davis For files I have a desktop centric setup, the homeserver run a paperless-ngx instance with \"a certain set of docs\" meaning I do not mirror my org-mode/org-attach entire home structure to paperless but only some selected docs to have them available on the go via mobile if I need them and simple webdav dir to share some if needed. I do not want OwnCloud considering it a monster I can't really know by it's mere size so I prefer giving up with it. I've tried Immich and Jellyfin for my photos and movies/music but they end up essentially unused. For RSS TT-RSS is far from perfect but can handle conveniently a significant number of posts quickly skimming them also on the go. Wireguard is nice enough to encapsulate my services To make it short, my point is not avoid a company like Google than choosing another like Proton, my point is being able to have and work with my data if some services disappear in a snap. reply gfourfour 23 hours agoprevI “de-reddited” with HN. reply the_af 22 hours agoparentHN is not a replacement for Reddit in general though. Reddit is a collection of forums/communities with wildly varying topics. HN is just one specific forum. Some of the worst behaviors that are common in many Reddit subs are discouraged here in HN, and for that I'm thankful. But the topics aren't the same, so one cannot replace the other in general. reply gfourfour 21 hours agorootparentA farmers market has a narrower selection than a supermarket, but you’d be healthier in the long run only shopping at the farmers market than going to the grocery store and being bombarded by processed garbage, even if you try to stick to the good aisles. The problem with Reddit is that the worst of Reddit inevitably pollutes the best of Reddit. Better for your mental diet to avoid altogether. reply the_af 13 hours agorootparentContinuing the analogy, what if instead of groceries/vegetables you want to buy a car, or discuss a movie, or exchange ideas about watercolor techniques? What use is a farmers market then? > The problem with Reddit is that the worst of Reddit inevitably pollutes the best of Reddit. Better for your mental diet to avoid altogether. Reddit does attract some unsavory types, but on the other hand, HN has a very narrow focus while Reddit covers more topics I'm interested in. It wouldn't make sense to avoid it, or to claim HN is a way to de-reddit -- they don't cover the same niches! And I don't go randomly browsing Reddit looking for subs to read; instead I seek out subs about specific topics I'm interested in. I would say it's HN that I browse randomly, without looking for anything in particular... HN is a \"random news\" portal for me. reply gfourfour 10 hours agorootparentI still use Reddit in the way you’re suggesting, which is when I occasionally have a specific use. What I meant was that I don’t visit Reddit by instinct anymore when I’m bored - even “good” subreddits. The general user base is not high quality. Even for topics I’m interested in, I really can’t stand the Reddit vibe that inevitably seeps through. It’s incredibly predictable. reply basch 21 hours agorootparentprevIf the goal is to fill some time with something mildly educational, it replaces the dopamine/serotonin hit all the same. reply the_af 13 hours agorootparentWhat if the goal is not to fill some time? reply LAC-Tech 22 hours agoprevDuckduckgo has been my search engine for years now, never use google anymore. Only other search engine I use is yandex, which is very useful for search terms that US-hased search engines mess with the results of. reply rc_mob 22 hours agoprevMan yeah I'm trying my best to do the same. it is so hard. I pay for kagi and email services. Buut yeah can one of you hackernews geniuses please create an alternative to youtube? The only alternatives I can find are crazy republican right wing websites that I have no desire to help or support. reply Animats 23 hours agoprev [–] Why is this hard? Since I quit offering a Chrome plug-in, my last connection to a Google account is gone. I should cancel my Google account, if and when I find the password. I post videos on Hardlimit, which uses PeerTube. I don't care about \"discovery\"; they're to illustrate blog postings elsewhere. So all I need is hosting and playout, which Peertube does just fine. (Peertube handles playout by mooching bandwidth off of other people using Peertube. So it can scale if many people are watching the same video. Permanent storage, though, is not distributed. It's not Bittorrent.) I've been using LibreOffice and its predecessors for decades now. My last purchase of a Microsoft product was Word 97. Desktop browser is Firefox. Mail client is Thunderbird. Backups are IDrive. Add blocking is the EFF's Privacy Badger, which breaks many ad systems while enforcing privacy. My phone uses F-Droid. It is not connected to a Google account. Browser is Fennec. App store is F-Droid. Search is DuckDuckGo. What's the problem? reply kelnos 21 hours agoparentThe \"problem\" is that other people have different needs and usage patterns than you do. Acting like your experience should be universal is a bit weird. reply hot_gril 22 hours agoparentprevWhat's your email provider? reply Animats 15 hours agorootparentOutgoing is Sonic. Incoming is a site of my own, \"animats.com\". reply afavour 23 hours agoparentprev> What’s the problem? …did you read the article? The author lists the Google alternatives they’ve been using, along with both the successes and the problems with those alternatives. reply stalfosknight 22 hours agoparentprev [–] It's not hard but many don't want to let go of YouTube or Gmail for some reason. reply yjftsjthsd-h 21 hours agorootparent [–] > for some reason Because nobody else has a content library to rival YT. (Gmail I don't know; this should be relatively easy to switch.) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author is attempting to reduce reliance on Google products, exploring alternatives like DuckDuckGo and Kagi for search, and ProtonMail and Fastmail for email.",
      "They self-host Immich for photo storage, use a home server for file storage, and employ NextCloud for calendar and contacts, appreciating its app compatibility.",
      "Despite using GrapheneOS for privacy, they still rely on Google Maps and YouTube for their comprehensive features, concluding that while avoiding Google is challenging, privacy-friendly alternatives and self-hosting are viable options."
    ],
    "commentSummary": [
      "The discussion focuses on reducing dependence on Google to improve privacy and avoid account loss, weighing partial versus complete abandonment of Google services.",
      "Users criticize Google's extensive data tracking and targeted advertising, comparing it to stalking and highlighting potential abuse, while exploring alternatives like FastMail, ProtonMail, and different search engines.",
      "The debate reveals the complexity of moving away from big tech, especially for non-tech-savvy users, and emphasizes the need for increased awareness and solutions to prevent data misuse."
    ],
    "points": 181,
    "commentCount": 184,
    "retryCount": 0,
    "time": 1717182173
  },
  {
    "id": 40539700,
    "title": "Go Error Handling: Sentinel Errors and errors.Is() Can Slow Code by Over 500%",
    "originLink": "https://www.dolthub.com/blog/2024-05-31-benchmarking-go-error-handling/",
    "originBody": "Blog Sentinel errors and errors.Is() slow your code down by 500% Zach Musgrave May 31, 2024 GOLANG 16 min read In this blog post, we benchmark different strategies for handling errors in Go and discuss their relative performance and other tradeoffs. The difference in performance between different strategies was very surprising to us, and we'd like to share the results. The original publication of this blog overstated the difference in performance between error handling strategies due to poorly configured benchmarks. The author regrets the error. In particular, we were shocked to learn that naively using the sentinel error pattern combined with errors.Is() slows your code down by over 5x. Methodology I wrote several different fake object stores with similar methods to retrieve an object, with different method signatures and different ways to represent the value being not found. Here's one of them, which follows a common recommendation in the Go community to use a sentinel error to represent the \"value not found\" condition. var notFoundErr = errors.New(\"not found\") type resultType struct {} type errStore struct {} //go:noinline func (b *errStore) GetValue(found bool) (*resultType, error) {if found { return &resultType{}, nil} else { return nil, notFoundErr} } Then I set up a benchmark that calls this function over and over, testing both the case where the value is found and where it's not found, in the same way a client calling this method would need to do. func BenchmarkNotFoundErrEqual(b *testing.B) {var es errStorefor i := 0; i = 0) if err == notFoundErr {b.Fatal(\"expected found\") } else if err != nil {b.Fatal(err) } if val == nil {b.Fatal(\"expected not nil\") }} } Let's look at the results. Benchmark results Here's the raw benchmark output: $ go test not_found_test.go -run='.*' -bench=. -count=10 > benchresults.txt $ benchstat benchresults.txt goos: linux goarch: amd64 cpu: AMD EPYC 7571 │ benchresults.txt │ │ sec/op │ NotFoundBool-16 3.423n ± 0% NotFoundErrorsIs-16 19.35n ± 0% NotFoundErrorsIsNilCheck-16 19.34n ± 0% NotFoundErrEqual-16 7.366n ± 1% NotFoundErrEqualNilCheck-16 8.293n ± 0% NotFoundWrappedErr-16 1.374µ ± 0% NotFoundWrappedErrNilCheck-16 1.375µ ± 0% NotFoundWrappedBool-16 11.69n ± 0% NotFoundPanic-16 241.5n ± 1% FoundBool-16 3.050n ± 2% FoundErrorsIs-16 18.04n ± 1% FoundErrorsIsNilCheck-16 2.939n ± 2% FoundErrEqual-16 3.240n ± 2% FoundErrEqualNilCheck-16 2.994n ± 1% FoundWrappedErr-16 23.77n ± 2% FoundWrappedErrNilCheck-16 9.877n ± 0% FoundWrappedBool-16 9.082n ± 0% FoundPanic-16 12.77n ± 0% geomean 17.22n We'll look in depth at each of these strategies in a moment. The tables below list the error handling strategies from fastest to slowest. We break the results down into the \"found\" and \"not found\" scenarios, since their relative performance is different in each one. First, here's the speed and relative performance of each strategy when the value is not found: Strategy Speed (less is better) Multiple of fastest strategy Bool 3.423 ns/op 1.00 ErrEqual 7.366 ns/op 2.15 ErrEqualNilCheck 8.293 ns/op 2.42 ErrorsIs 19.35 ns/op 5.65 ErrorsIsNilCheck 19.34 ns/op 5.65 Panic 241.5 ns/op 70.55 And here's the same performance data when the value is found: Strategy Speed (less is better) Multiple of fastest strategy ErrorsIsNilCheck 2.939 ns/op 1.00 ErrEqualNilCheck 2.994 ns/op 1.02 Bool 3.05 ns/op 1.04 ErrEqual 3.24 ns/op 1.10 Panic 12.77 ns/op 4.35 ErrorsIs 18.04 ns/op 6.14 As you can see, it matters quite a lot for performance how you you design your APIs and handle errors in Go. Let's examine each strategy, going in order from fastest to slowest. Bool: don't return an error The fastest way to handle a \"not found\" condition in Go is to not represent it as an error. This is labled as the Bool strategy above. Here's how it's implemented: type boolStore struct {} //go:noinline func (b *boolStore) GetValue(found bool) (*resultType, bool, error) {if found { return &resultType{}, true, nil} else { return nil, false, nil} } As you can see, this method does not return an error when the value isn't found, instead returning a boolean found result. Here's how it's benchmarked: func BenchmarkNotFoundBool(b *testing.B) {var bs boolStorefor i := 0; i = 0) if err != nil {b.Fatal(err) } else if !found {b.Fatal(\"expected found\") } if val == nil {b.Fatal(\"expected not nil\") }} } This strategy performs the best in both scenarios, where the value is found and where it's not. Technically speaking this isn't even an \"error handling\" strategy, since no error is returned. Rather, it's a baseline to serve as a point of comparison for other strategies that do return errors. ErrEqual: sentinel errors with direct equality check This strategy uses classic Go sentinel errors to represent the \"value not found\" condition. It looks like this (duplicated from the \"Methodology\" section above). type errStore struct {} //go:noinline func (b *errStore) GetValue(found bool) (*resultType, error) {if found { return &resultType{}, nil} else { return nil, notFoundErr} } When we benchmark this strategy, we use direct == comparison with an error constant to detect the not found condition. func BenchmarkNotFoundErrEqual(b *testing.B) {var es errStorefor i := 0; i = 0) if err == notFoundErr {b.Fatal(\"expected found\") } else if err != nil {b.Fatal(err) } if val == nil {b.Fatal(\"expected not nil\") }} } In the \"not found\" scenario, this approach is about 2x slower than using boolean existence checks. For the case where the value is found, it performs the same. Note that checking for sentinel errors with == is no longer recommended for correctness reasons, more details in following sections. ErrEqualNilCheck: sentinel errors wrapped with a err != nil check There are two roughly equally idiomatic ways to check for a sentinel error in Go. The first is more common in our experience, and is what's done in the ErrEqual strategy: if err == notFoundErr {// nothing to do } else if err != nil {b.Fatal(err) } So you begin by checking for all sentinel error values you want to handle, either in a switch or an if / else chain, and then if the error isn't one of the handled types, pass the error up the stack, panic, or otherwise handle it. Alternately, you can perform the above logic conditionally after first checking if the error is non-nil. This code is slightly longer and more indented, and is what the ErrEqualNilCheck does. func BenchmarkNotFoundErrEqualNilCheck(b *testing.B) {var es errStorefor i := 0; i = 0) if err != nil {if err == notFoundErr { b.Fatal(\"expected found\")} else { b.Fatal(err)} } if val == nil {b.Fatal(\"expected not nil\") }} } This stategy is slightly worse than the previous one in the \"not found\" case because of the added cost of the nil check, weighing in at about 2.5x slower than boolean existence checks. But in the (usually more common) case when the value is found, it's also performs just as well as the boolean strategy. ErrorsIs: more robust sentinel error detection The original sentinel error pattern using == caused problems when other parts of a library wanted to wrap the original error in their own error type to add additional information, which causes the equality check to fail. Many linters will warn about using == for sentinel errors, and my JetBrains IDE puts a yellow squiggly under it for the same reason. Modern best practice is to instead use errors.Is() for sentinel error checks, which correctly detects wrapped errors. In our benchmarks, this strategy looks like this: func BenchmarkNotFoundErrorsIs(b *testing.B) {var es errStorefor i := 0; i = 0) if errors.Is(err, notFoundErr) {b.Fatal(\"expected found\") } else if err != nil {b.Fatal(err) } if val == nil {b.Fatal(\"expected not nil\") }} } Unfortunately, this strategy has bad performance, at 5.5x slower than the Bool strategy when the value is not found, and 6x slower when it is. ErrorsIsNilCheck: better performance on the happy path Just as we saw with ErrEqualNilCheck, we can avoid a large performance penalty in the case that the value is found by only calling errors.Is() after first determining the error is non-nil. Here's the benchmark code: func BenchmarkNotFoundErrorsIsNilCheck(b *testing.B) {var es errStorefor i := 0; i = 0) if err != nil {if errors.Is(err, notFoundErr) { b.Fatal(\"expected found\")} else { b.Fatal(err)} } if val == nil {b.Fatal(\"expected not nil\") }} } Just as with ErrEqualNilCheck, this strategy does very slightly worse for the not found scenario at about 5.5x slower than Bool, but is just as fast as Bool when the value is found. Panic: the slowest error handling technique Usually errors in Go are returned as values, but there is also another technique: you can panic instead, then recover that panic at a higher layer of the stack. Most people don't recommend doing this as an error handling technique, and we don't either. But there are some limited cases where it can be more performant than returning errors. That's not the case for this benchmark though. Panic performs the worst of any strategy. Here's what it looks like: type panicStore struct{} //go:noinline func (b *panicStore) GetValue(found bool) *resultType {if found { return &resultType{}} else { panic(notFoundErr)} } func BenchmarkNotFoundPanic(b *testing.B) {var es panicStorefor i := 0; i = 0) }() if val == nil {b.Fatal(\"expected not nil\") }} } This code is 240x slower than boolean return results in the not found case, and 13x slower in the found case. So in addition to being risky (can halt your program if you forget to recover), panics are generally slower than any other error handling strategy. What about error wrapping? These are microbenchmarks specifically engineered to zero in on the differences between the error handling strategies. But real application code looks very different. One of the biggest differences is that errors are typically passed through several layers of the stack in between other pieces of business logic. I was curious to know how this difference impacted the benchmarks. Specifically, I wanted to know how the practice of error wrapping impacted the performance of sentinel errors, and how it related to the Boolean return strategy. So I wrote two additional implementations of object stores that simulate an error deeper in the stack, getting wrapped at every layer. Here's what it looks like: type wrappedErrStore struct{} //go:noinline func (b *wrappedErrStore) GetValue(found bool) (*resultType, error) {result, err := b.queryValueStore(found)if err != nil { return nil, fmt.Errorf(\"GetValue couldn't get a value: %w\", err)}return result, nil } //go:noinline func (b *wrappedErrStore) queryValueStore(found bool) (*resultType, error) {result, err := b.queryDisk(found)if err != nil { return nil, fmt.Errorf(\"queryValueStore couldn't get a value: %w\", err)}return result, nil } //go:noinline func (b *wrappedErrStore) queryDisk(found bool) (*resultType, error) {result, err := b.readValueFromDiskFake(found)if err != nil { return nil, fmt.Errorf(\"queryDisk couldn't get a value: %w\", err)}return result, nil } //go:noinline func (b *wrappedErrStore) readValueFromDiskFake(found bool) (*resultType, error) {if found { return &resultType{}, nil} else { return nil, notFoundErr} } I also have another identical implementation that returns a boolean value to indicate presence or absence of the value, rather than notFoundErr (omitted for brevity), symmetrical to the Bool strategy. When I run the same benchmarks on these two implementations, I get the following results. When the value is not found: Strategy Speed (less is better) Multiple of fastest strategy WrappedBool 11.69 ns/op 1.00 WrappedErr 1374 ns/op 117.54 WrappedErrNilCheck 1375 ns/op 117.62 When the value is found: Strategy Speed (less is better) Multiple of fastest strategy WrappedBool 9.082 ns/op 1.00 WrappedErrNilCheck 9.877 ns/op 1.09 WrappedErr 23.77 ns/op 2.62 As you can see, wrapped errors severely degrade the performance of tihs strategy, making sentinel error checks 120x slower than the boolean check when the error is non-nil. That's bad! Much of the difference in time on this benchmark doesn't come directly from errors.Is() (although it is slower for wrapped errors, that's to be expected). Instead, this difference primarily reflects the fact that creating the wrapped errors is itself expensive. But this is a fair comparison, since in practice your code must also wrap errors to use this strategy. On the other hand, the difference in strategies is partially smeared by the introduction of more stack layers in the case the object exists, to the point that the nil-guarded errors.Is() check is only 10% slower than boolean checks. Takeaways and discussion If you want to reproduce this result or variations of it yourself, you can find the full source here. So what's the takeaway from all this? There are four main points in my view: errors.Is() is expensive. If you use it, check the error is non-nil first to avoid a pretty big performance penalty on the happy path. Using == to check for sentinel errors is likewise expensive, but less so. If you do this, check the error is non-nil first to make it cheaper on the happy path. But because of error wrapping, you probably shouldn't do this at all. Error wrapping makes using sentinel errors much more expensive, including making errors.Is() more expensive when the error is non-nil. Using sentinel errors is as performant as other techniques on the happy path if you take the above precautions, but unavoidably much more expensive on the error path. So the main takeaway is really that sentinel errors can be expensive, and you should consider this when deciding whether to use them. The standard caveat for performance-based arguments applies here: measure what difference it actually makes in your case, and don't sweat stuff that isn't on the hot path most of the time. We are talking about nanoseconds here, and it takes a lot of those to add up to something noticeable. If sentinel errors are measurably slow for you in practice, it's probably because the ones you're using are some combination of commonly triggered and expensive to construct. YMMV. That said: we have found expensive-to-construct, commonly triggered sentinel errors when profiling our database, and we changed the code to remove them, squeezing several percentage points improvement in the process. But there are also non-performance reasons you might want to avoid sentinel errors, which are more philosophical or aesthetic in nature. Earlier this month I kicked the hornet's nest by suggesting that you shouldn't name boolean map check variables ok. It got picked up a few places in the Golang world and generated a lot of discussion on Reddit and elsewhere, which I had expected. But I hadn't expected that so much of the conversation would focus on an almost tangential point I made about API design, specifically about an API returning a NotFound sentinel error: Separating out an existence check from an error condition is a good thing, actually. You never want to force clients to check for a particular error type in business logic. ... An error should by default be considered non-recoverable, to be returned when something goes very wrong. Semantically, a table not existing isn't really an error, it's something you expect to happen all the time. An interface that returns an error in the course of normal operation forces clients to understand a lot of details to use the interface correctly (looking at you, io.EOF). And errors can be expensive to construct and examine, so you don't want them constructed or interpreted on your hot path. I had thought this was a commonly understood nugget of best practice wisdom, but this opinion got a lot of pushback. People said some very hurtful, arguably accurate things about my character. Here's one of the more thoughtful examples from the discussion on Lobste.rs: Now, obviously I disagree. And I think these numbers pretty clearly refute the classical view of sentinel error handling in Go, summarized by the comment above: Errors are normal outcomes of any operation. They aren’t special and aren’t any more or less expensive to deal with than any other type. They are, classically, just values. The problem is that sentinel errors, as typically and idiomatically used, in fact are special, and are more expensive to deal with than other values. My suggestion to use boolean values outperforms them by a lot, 6x in fairly common idiomatic usage and potentially much more if they're expensive to construct. And performance considerations aside, sentinel errors have a lot of other issues. I'm not the first person to note this. Here's Dave Cheney back in 2016 on this topic: My advice is to avoid using sentinel error values in the code you write. There are a few cases where they are used in the standard library, but this is not a pattern that you should emulate. Instead, he recommends never inspecting the details of an error: Now we come to the third category of error handling. In my opinion this is the most flexible error handling strategy as it requires the least coupling between your code and caller. I call this style opaque error handling, because while you know an error occurred, you don’t have the ability to see inside the error. As the caller, all you know about the result of the operation is that it worked, or it didn’t. This is all there is to opaque error handling – just return the error without assuming anything about its contents. Dave didn't invent this advice. Going back further, lots of luminaries and philosophers in the field have given the same advice in other contexts and for other languages: don't use errors for control flow. I'm old enough to have seen Josh Bloch in person at a Java conference, where he was something of a celebrity. He gave this advice on error handling in Effective Java way back in 2001: Exceptions are, as their name implies, to be used only for exceptional conditions; they should never be used for ordinary control flow. ... This principle also has implications for API design. A well-designed API must not force its clients to use exceptions for ordinary control flow. A class with a “state-dependent” method that can be invoked only under certain unpredictable conditions should generally have a separate “state-testing” method indicating whether it is appropriate to invoke the state-dependent method. Now obviously Exceptions in Java are not the same as errors in Go. But just as obviously, they serve the same purpose. Functionally and semantically, there is not much difference between these two code snippets: try { val = store.getValue(); } catch (NotFoundException e) { // handle not found } val, err := store.GetValue() if errors.Is(err, notFoundErr) { // handle not found } They're both workable and idiomatic. But they also both have serious drawbacks, for different but related reasons. And they're both improved by changing the API to eliminate the need for the client to interpret the not found case as an error during normal operation. Here the two languages diverge: because Java lacks multiple return values, it's not ergonomic to add additional metadata like a found boolean into the return result in most cases, so you tend to do something like this instead: if (store.hasValue()) { val = store.getValue(); } Bloch anticipates that people might be tempted to use exceptions for control flow for performance reasons: More generally, use standard, easily recognizable idioms in preference to overly clever techniques that purport to offer better performance. After all, it really is more expensive to call two methods (Has(), Get()) than one (Get()), even with caching. But because Go does have multiple return values, you don't have to make this tradeoff the way you do in Java. You can just return more information and save yourself the extra method call, while not forcing clients to examine the error value. val, found, err := store.GetValue() if err != nil { return err // opaque error handling } if !found { // handle not found } Some people are bothered by the fact that a method named GetValue() may not succeed in getting a value. If that sounds like you, then name it MaybeGetValue() instead. Other people are bothered by the code overhead of a found boolean in the return. If that sounds like you, you might be able to get away with using a nil return value instead (as long as nil isn't a valid object in your API). Either way, understand that inspecting errors for business logic can come at a substantial performance cost. Conclusion We're building Dolt, the world's first version-controlled SQL database. We have been writing it for over five years now, and our codebase has quite a few sentinel errors, many of which we inherited from other libraries, but some of which we wrote ourselves. These days we avoid sentinel errors and generally treat all errors as opaque. We also wrap errors very sparingly, prefering to use stack traces instead where appropriate. Have questions or comments about Go error handling? Or maybe you are curious about the world's first version-controlled SQL database? Join us on Discord to talk to our engineering team and other Dolt users. SHARE Blog",
    "commentLink": "https://news.ycombinator.com/item?id=40539700",
    "commentBody": "Go: Sentinel errors and errors.Is() slow your code down by 3000% (dolthub.com)172 points by ingve 22 hours agohidepastfavorite116 comments zachmu 17 hours agoBlog author here with a regretful correction. This result is not accurate, mea culpa. The headline should read: errors.Is() is 500% slower. Basically: the critical functions in the benchmark are small enough they were being inlined by the compiler, which means it's possible for the compiler to further optimize the loop to avoid all comparisons in some cases, producing an inaccurate result for some of the benchmarks. You can fix this by adding noinline directives to the methods. I'll be publishing an update to the article and a post-mortem on how this slipped past review. The rank ordering of techniques in the article is unchanged, but the magnitude of the difference is not nearly so large. reply rsc 16 hours agoparentThanks for posting the update here. I was going to point out that 0.5ns for anything is almost always a sign of benchmarks being optimized away. But even if those numbers had been accurate, it is important to think about how they compare to the operations being performed. Taking 4 nanoseconds to check for an error sentinel is not a huge deal when you've spent microseconds or even milliseconds waiting for a network service. reply fl0ki 5 hours agorootparentIt's a relatively bigger deal for many other potential sources of errors, like parsing a string as a number. Also, even if the caller does not unwrap the error values, producing errors often requires at least one heap allocation. These are all fair tradeoffs for the simplicity and ergonomics of Go's error handling. They rarely matter for performance because the happy critical path of most programs does not produce lots of errors. These are just nuances worth recognizing in a thread specifically about the isolated overheads of errors. reply zachmu 16 hours agorootparentprevYou'll never get those 4 nanoseconds of your life back though reply philosopher1234 10 hours agorootparent4 nanoseconds spent calling high quality functions is 4 nanoseconds well spent reply neonsunset 4 hours agorootparent4 nanoseconds is how much it takes for unoptimized errors.Is version of this code[0] to execute in C#, instead of 15-17ns in Go. I don't know how long it will take the industry to learn not to use Go for writing databases. [0]: https://gist.github.com/neon-sunset/65a8deeaac745ba159f990c7... reply zachmu 2 hours agorootparentOur database is within spitting distance of MySQL's raw performance and getting faster all the time. Certainly there are some applications where that will not be fast enough. That didn't prevent MySQL from becoming the worlds most widely deployed free database. And yes, postgres is faster and will eventually pass MySQL in deployments, but that's success also has very little to do with its speed. reply vips7L 1 hour agorootparentprevIs that AOT or JIT? reply hinkley 12 hours agoparentprevThe thing with programming language evolution is that quite often everything except the error handling gets faster over time. If you have an app that makes aggressive use of errors, you’re not going to see the same speedups from version to version that others are seeing. Particularly in JITed languages, if the multiplier is hyperbole today, there may be a day in the future where it’s accurate. reply rafaelmn 5 hours agorootparent> The thing with programming language evolution is that quite often everything except the error handling gets faster over time. Funny I was just reading this the other day : https://learn.microsoft.com/en-us/dotnet/core/whats-new/dotn... It's probably not the first thing you spend effort on when optimizing but it gets better over time. reply samatman 2 hours agorootparentprevThe minimum cost of an error is one cold branch. Which is effectively free on the happy path, and as cheap as it possibly can be on the error path. A well-designed error handling system can feasibly reach this goal, at which point no more optimization is possible. Edit @hinkley: I am unfortunately in hacker jail, and have used up my post quota, so I have to edit my post rather than reply to yours. Reply follows. Throwing, the way Java and C++ do it, exceeds the minimum cost of an error, yes. Not that this is some sort of show-stopping problem, I'm discussing the minimum cost, not what level of cost makes errors too expensive. A branch-not-taken, which is the opposite of processing an error, caught or otherwise, is what I described as practically free. Not always. If the only path forward is, for instance, to reference a pointer, and the error would mean that the pointer is a null pointer, the chip would have to wait for the branch to coalesce, which might mean a minor pipeline stall. But if the processor can do further work, such as checking for an error on malformed input, where it's legal to speculatively execute the next decode on the predicted branch, it really is practically free. It costs considerably less than one instruction. If the branch triggers, the processor spills all the speculative work and switches. Since errors are expected less frequently than success, more or less by definition, that's the correct tradeoff. This is why I described the error branch as \"cold\". The processor should predict and speculate on the happy path. Zig is an example of a language where errors can be as inexpensive as a superscalar check on a given register while the main path moves forward. This is also possible in C when checking errno(). I can't speak for Rust, but enormous amounts of work have gone into that compiler, and it should indeed be possible to compile a match-on-Result to the same efficient level. Perhaps trickier since Rust doesn't \"know\" what an error is, but I suspect that in practice it can get this right. Not that it's even mostly up to the compiler: there are ASM hints for the branch predictor in some instruction sets, but modern branch predictors don't need those to handle the case where 99+% of the time execution proceeds on one branch. It's conventional for compilers to emit \"branch on cold, proceed on hot\", but it's been a long time since that has mattered for execution speed. You'll get a bit tighter on a chip with no predictor and a shallow pipeline, like AVR, but for anything which can run an operating system it's almost irrelevant now. I see no a priori reason why Go shouldn't achieve the minimum level either. Languages which use exceptions and try/catch must do some bookkeeping, which imposes a cost on the code whether an exception is raised, or not. That can be minimized, but not to the limit I'm talking about. You might find this link interesting: https://ziglang.org/documentation/0.12.0/#Implementation-Det... Note that the costs described are for the ability to generate an error trace, not error handling itself. This cost is only paid in debug and ReleaseSafe modes. reply hinkley 2 hours agorootparentAre you talking about errors that are not thrown? What language are you using where catching and processing an error are free? reply zachmu 14 hours agoparentprevCorrections and new title are now live. Thank you for your forbearance. reply jay-barronville 16 hours agoparentprevDo you notice any difference if you update `BenchmarkFoundErrorsIs()` to the following? func BenchmarkFoundErrorsIs(b *testing.B) { var es errStore for i := 0; iOptimized Go code will generally perform on par with optimized Rust code. This is an incredibly dangerous assumption. Go and Rust are in a completely different weight class of compiler capability. Before making an assumption like this, I strongly suggest building a sample application (that is more complex than hello world) in Go and Rust, then compiling both with optimizations and taking a peek at what they compile to with Ghidra/IDA Pro/any other disassembler. C# + .NET 8 can sometimes trade blows with Rust + LLVM, particularly with struct generics, but Go definitely can't, is far behind in compiler features and doesn't leverage techniques that reduce cost of non-free abstractions that .NET and OpenJDK employ. reply Thaxll 1 hour agorootparentI've seen Go code faster than Rust one, for example there was an interesting article years ago about counting words in a file: https://benhoyt.com/writings/count-words/ reply kbolino 2 hours agorootparentprevI should clarify that by \"optimized\" I meant hand optimized, not just compiler optimized. You can easily code yourself into surprisingly poor performance with Go, though usually for the same/similar reasons as C#/Java. Rust makes it a little harder to get into the same situations though it's by no means impossible. As to the zero/low-cost abstractions, Go has very few abstractions at a language level and that is what I meant by \"much less expressive\". If you try too hard to write Go as though it were another language, you will shoot yourself in the foot. However the Go compiler, standard library, and broader ecosystem have come a long way and enable very similar performance profiles to most other compiled languages. It has not been my experience, at least in micro-benchmarks, to see any major performance differences in Go vs e.g. C on things like encryption, hashing, RNG, SIMD, etc. You do need to avoid allocations in hot code paths but that's true of any garbage-collected language. The outstanding performance issues I'm aware of are around deferring, calling methods through an interface, and calling out to cgo. These are pretty minor in most situations but can eat up a lot of CPU cycles if done often in tight loops. They've also been getting better over time and can sometimes be optimized away with PGO. reply neonsunset 1 hour agorootparentThis is fair but performant data processing in Go simply has much lower performance ceiling than Rust, C#, Swift or C++. Which is why all standard library routines that do so (text search, hashing, encryption, etc.) are written in Go's custom \"\"\"portable\"\"\" ASM dialect. It is unwieldy and has API omissions because it takes this half-assed approach many things in Golang ecosystem do. The closer, and better, example is hand-optimized C# where you still (mostly) write portable and generic code for SIMD routines and they get compiled to almost the same codegen as hand-intrinsified C++. I understand where this belief comes from, but the industry at large does itself a huge disservice by tunnel visioning at a handful of (mostly subpar) technologies. reply kbolino 20 minutes agorootparentUnfortunately, you are right that SIMD support in Go isn't great and basically requires writing platform-specific code in their quirky assembly language or using a third-party library which does that for you. I don't think the existing use of assembly under the hood to accelerate performance when possible is something to be frowned upon though. It's an implementation detail that almost never matters. One issue with the performance of their assembly code though is that it passes arguments and return values on the stack. I saw something about allowing code to use registers instead but I don't think it's available yet. reply Thaxll 1 hour agorootparentprevI think you over hype c# and swift, we did benchmark a grpc api server in c# ( net core ) and Go, same implementation in both languages and Go while faster was using way less memory. reply neonsunset 1 hour agorootparentWas that with .NET 8? There have been significant performance improvements with each subsequent version in the last 5 years. reply cedws 3 hours agorootparentprevRuntime cost or compile time cost, take your pick. reply teepark 1 hour agoprevComparing to the speed of a direct boolean check is a great way to sensationalize really small numbers. Nobody's real-world code is being slowed down by 500% because all real world code is doing much more than just checking errors. All I see from these results is a 15-16ns cost to using errors.Is versus an additional boolean. Even the examples (\"GetValue\") hint at an extremely common use case: reads from a data store. A case where single-digit milliseconds is considered \"all good performance-wise\" for the most common SQL databases, clocking in at 100000x the time scale of errors.Is. reply ReleaseCandidat 19 hours agoprevSorry, that's still not a reason to use bools instead of \"errors\" and An error should by default be considered non-recoverable, to be returned when something goes very wrong. Semantically, a table not existing isn't really an error, it's something you expect to happen all the time. remains a bad idea. Seems like the reason for this article is the reaction to that one: https://www.dolthub.com/blog/2024-05-10-ok-considered-harmfu... reply zachmu 17 hours agoparentAgree to disagree here. And yes, this article is a follow-up to the linked one. The last section of the blog has a discussion of aesthetic and philosophical concerns around using errors for control flow. I'm against it, for the reasons I provide. YMMV. reply sethammons 9 hours agorootparentYour take away in the \"ok is harmful\" article is incorrect. There is nothing wrong with \"ok\" - your problem is with shadowing variables. The better organized code couldn't use shadowing. Your example of moving the ok-checks to the end makes as much sense as moving all your err checks to the bottom. If you overwrite a shadow variable before using the value, of course that's bad. The real takeaway should be that one must always be careful with shadow variables and generally use them as immediately as possible. reply LoganDark 18 hours agoparentprevThat quote conflates errors and exceptions. An error is an unsuccessful result. An exception, however, precludes any result at all. Sometimes they are the same, but in many cases they are different. Asking to retrieve a table that doesn't exist is an error, because the answer is that there is no table; the operation was unsuccessful. There failed to be retrieved any table, but as the table always could have either existed or not, this is not an exceptional case. Nothing went wrong: you simply asked to retrieve a table and no table was found, so you get that back as an error. However, trying to perform a table operation on that unsuccessful result generates an exception, because you violated a contract: you can't operate on something that doesn't exist. Sure, you can operate on the representation of nonexistence (that is, null), but you attempted to operate on a table and you don't have one. The computation stops prematurely with an exception because the operation expects the table to necessarily exist. There is no unsuccessful result to be returned because the operation didn't even make it to the point of failing; the performance itself of the operation failed. The operation is entirely invalid and returning any result would be incorrect. At least, that is my personal understanding of the terms. For example, most things in Rust are errors and not exceptions, because exceptions (panics) generally terminate the thread, or abort the process, depending on user preference or runtime environment. Anything that isn't fatally unrecoverable deserves to be an error - in other words, a `Result::Err` - which can be handled by the caller, as opposed to panics, which are not guaranteed to be catchable (unwinding is not a required part of the language, it is an optional convenience). Most failures in JavaScript are exceptions, because you are not made to check anything, and everything is dynamically typed, so if you make an assumption that isn't true, an exception has to be raised to stop you. Luckily, there is try/catch. It can also be difficult to represent errors without exceptions, you instead have things called \"error states\" which are basically certain non-errors being treated as errors by user code, when either that result is undesired even if perfectly normal, or that result is the only/easiest way for the error to be reported by the source. Anyway, my point is that the article probably means to use the term exception and say that a boolean is better than an exception when the error isn't an exceptional case. And in that case I would agree. In Rust, I don't want my program to crash when I ask for a nonexistent table - I only want that to happen later if I decide that the table's nonexistence is a fatal error in the context of the whole thread. reply tsimionescu 11 hours agorootparentYou are explaining the diferrence between error results and panics in Rust, and maybe this also applies to Go. But this is very much not how exceptions (a word these languages don't even use) are understood in languages which have them. In Java, C#, C++, for example, exceptions are meant for any situation where the operation you asked for can't be completed successfully, for whatever reason. The most common example is IOException: if you asked to read 200 bytes from a socket, but the connection got interrupted after 100 bytes, that's an IOException. This would not be appropriate as a panic in Go or Rust, for a comparison of the difference in philosophy. reply spockz 11 hours agorootparentI like your example. Although I do remember that there are also read operations where you ask for 200 bytes and you can get 100 back. And it is up to the caller to check for this. Otoh, if the socket is closed you indeed get a socket closed exception. But that is because the docker was closed while reading. What is the rust behaviour in this case? reply LoganDark 10 hours agorootparent> I do remember that there are also read operations where you ask for 200 bytes and you can get 100 back. Those are read operations where you say you have 200 bytes to store the result, rather than where you ask for exactly 200 bytes. But yes, they certainly exist, the ones that ask for exact numbers of bytes are typically built on repeated calls to the ones that can return less. > Otoh, if the socket is closed you indeed get a socket closed exception. But that is because the docker was closed while reading. I think this depends on how it closed, sometimes you just get EOF, which is an error state but not an exception (unless your language is clinically insane like Python). Obviously if it's interrupted in such a way that there would probably have been more data but you can no longer get it, that's a different error than EOF (and some languages use exceptional control flow to report mere errors, it is just that Rust does not do this). > What is the rust behaviour in this case? Read calls can return less data than you asked for. If they return no data, that is a successful EOS. They can also report I/O errors as distinct from a successful result. There is an I/O error for \"unexpected EOF\", where you require more data than is present, and the fact that it's not present is an error. I believe `read_exact` uses this if anything happens before it has entirely filled the buffer, including completely normal connection closure. reply LoganDark 10 hours agorootparentprev> You are explaining the diferrence between error results and panics in Rust That is indeed an example I included, yes > In Java, C#, C++, for example, exceptions are meant for any situation where the operation you asked for can't be completed successfully, for whatever reason. The most common example is IOException: if you asked to read 200 bytes from a socket, but the connection got interrupted after 100 bytes, that's an IOException. - Java has checked exceptions, which... ugh, I'm not going to get too much into this, but this is basically their terrible way of representing expected errors while reusing the try/catch syntax. Yes, the thing is called exceptions at the language level. Yes, they do interrupt control flow. But fundamentally I consider them mere errors. - C++ overuses exceptions in a similar way, except they're not checked because fuck you, nothing in C++ is checked. The relevant quote's talking about semantics, so I'm talking about errors and exceptions as something that transcends the individual programming language. Different languages are different and don't always (even usually don't) map these concepts 1:1. reply tsimionescu 3 hours agorootparentThe behavior in Java and C++ is just a different philosophy of what exceptions mean, that is what I was trying to say. The semantics of an exception in these languages is that functions should either return a simple type or throw an exception if they are unable to return that type for whatever reason (be it a bug, an unmet precondition, a temporary error, an unmet postcondition, etc). Exceptions are not extraordinary events, and it is certainly never appropriate to stop execution because some exception was raised: the program itself is supposed to, at the right level, specify what should happen when an exception is encountered. The difference between different levels of error, such as fatal errors vs others, is captured by the type of the exception itself. This all has little to do with Java's checked exceptions support. That is a feature that is supposed to help with verification that the program indeed specifies what to do when an exception happened, for certain classes of errors that are considered more likely to occur in production. C#, which learned from both C++ and Java, doesn't include these, but still has the same semantics for exceptions. reply randomdata 15 hours agorootparentprev> Sometimes they are the same Your understanding otherwise seems on point, but I'm not sure they can ever be the same. An exception is the representation of programmer fault. An error, on the other hand, is the representation of faults outside of the programmer's control. Like you point out, operating on a missing table is a mistake made by the programmer. This fault could have been avoided when the code was written. The table not being there is not within the control of the developer (with some assumptions of what table is meant to mean here) and no amount of perfect coding could have avoided the situation. > Most failures in JavaScript are exceptions Idiomatic Javascript suggests handling errors using its exception handling mechanism, but the payload is still error in nature. In fact, the base object the idioms suggest you use to hold error data, passed using the exception handler, is literally called Error. Javascript may be confused by having no official \"Exception\" type. Although arguably it does not need one as you can simply throw a string, which is all you need for an exception. The exception handler will tack on the rest of the information you need to know automatically. Java takes a different opinion, though, having two different, albeit poorly named, built-in classes for errors (Exception) and exceptions (RuntimeException). reply trustno2 10 hours agoparentprevGo returns bool for map access, not errors... reply aaomidi 16 hours agoparentprevI agree with you. The go standard library uses sentinel errors. ErrNotFound is actually good to use, because otherwise, a `Get` call can end up returning `nil, nil` (*Value, error). Which is just bad. reply eptcyka 19 hours agoprevThere's a tangential issue in Rust where there's a concern around if a library should be exposing the error types of its dependencies. If they are exposed, semver is broken anytime a dependency is updated in a semver breaking fashion. I, in general, agree that this breaks encapsulation and is not good. However, and maybe I'm doing things wrong, but often enough there are edge cases where it is important to distinguish between different errors and adjust control flow. Often, library authors will not be able to foresee all possible usecases for their error types, so one can end up doing some horrible type casting or worse hacks to get at the actual underlying data. I'm still torn as to what is the better way forward. reply tempaccount420 17 hours agoparentI don't think errors should have to follow semver. Please, break my match on non-major version bumps, if I didn't explicitly add a catch-all case. reply tialaramex 5 hours agorootparentThat's what the #[non_exhaustive] attribute is for. If a struct or enum is given this attribute then everybody else is treated as though they cannot exhaustively match it, they must write the catch all case. You (as the owner of the type) can still exhaustively match it, after all you're the one who'll get broken by your own change if you screw up - but everybody else gets told they need to write the \"catch all\". So, no, errors aren't special and should obey semver but you deliberately can (and should) signal that your users can't exhaustively match a thing you know will change. reply tempaccount420 3 hours agorootparentI want to be exhaustive on my error checks. I want my match to fail if a new variant is added, so that I can handle it (even if it's just to ignore it). A big reason why refactors can be so fearless in Rust is thanks to exhaustive checks. While the 'refactor' part applies mostly to your own, 1st party code, it's also useful when upgrading 3rd party dependencies. For this reason I personally don't like `#[non_exhaustive]`. I don't expect my code to keep compiling when I upgrade the minor version of my dependencies, and as long as the reason is simple, like a new enum variant was added, then I'm happy that it errored and will happily fix it. This perfectionist attitude towards semver is why so many projects get stuck forever at v0. https://0ver.org/ reply fl0ki 5 hours agorootparentprevIt's only fair to call out that #[non_exhaustive] only helps you for the very specific case of adding new enum variants or struct fields. It does nothing for the burden of keeping all existing variants and fields compatible with any way they could have been matched. However it's also only fair to note that the Rust ecosystem does not consider every [potential] breaking change a major one. https://predr.ag/blog/some-rust-breaking-changes-do-not-requ... reply paulmd 18 hours agoparentprevjava has the \"rootCause\" constructor which wraps another Exception (actually Throwable) of any type passed in. so the idiomatic way would be to create your own error types, but to pass the lower-level exception in as the rootCause so you can trace the exceptions down the stack if desired. Since the type is Throwable rather than a dependency-specific exception, this isn't exposed in the interface unless you go out of your way to cast things in an obviously dangerous way. Does that not exist in Rust? reply Dylan16807 16 hours agorootparentWhen you say \"obviously dangerous\" do you mean \"obviously breaking encapsulation\" or is there a bigger issue? If there's a bigger issue then I'm not sure how it's useful for solving this problem of wanting the details. reply paulmd 13 hours agorootparent\"obviously breaking encapsulation\", like if you pull some interface from the dependency through yours, then yeah, you're broken when they break things. That's a generalized interface problem and java can't help you anymore than rust can. But I guess it's true that it may not be obvious that library interfaces may churn unexpectedly etc, you are dependent on the good behavior of others unless you specifically take steps to abstract it. (although you do absolutely have to be careful about what you are returning in a client-facing error message etc. don't return the parts of the stack trace that leak information about your application to an actual client, they should be caught by your framework and turned into generic \"4xx git rekt\"/\"5xx we made a fucky wucky\" messages that don't reveal too much about your environment.) not a java problem specifically for either of those, it just seems odd coming from that world that there's not a concept of a \"recursive exception stack\" like that in rust? reply eightnoteight 15 hours agoprev> The problem is that sentinel errors, as typically and idiomatically used, in fact are special, and are more expensive to deal with than other values. My suggestion to use boolean values outperforms them by a lot, 30x in fairly common idiomatic usage. while I agree to some degree, but when performance comes into picture, what really matters more is normal path vs surprise path rather than happy path vs error path it's hard to argue what is a happy path in the code, but it's not wrong to say that io.EOF check is a normal path of the code i.e. not a surprise in production. the bad performance of errors.Is is something to be improved upon but it's not a surprise in production when there are a large number of `errors.Is` checks during normal path of the code now coming to the surprise path of the code, here's where performance gets really important, no one wants their code to suddenly hog 100% CPU because of some special error case - https://blog.cloudflare.com/cloudflare-outage . but such surprise paths often contain a large amount of business logic that weigh much more than how slow errors.Is function is compared to a boolean check it would be interesting to see where this line of reasoning is valid but IMO performance isn't a good argument against why errors are not normal outcomes of operations in production but thumbs up for the article, now I know what to reference for backing the below pattern that I often use, when I first saw the errors.Is it was pretty obvious that its going to be slow but just didn't have time to prove it and use below pattern ``` if err != nil && errors.Is(err, x) { } else if err != nil && errors.Is(err, y) { } else if err != nil { // handling unknown error } ``` reply L-four 18 hours agoprevIt shouldn't surprise anyone that a checking a bool is faster than calling a function and checking the bool it returns. reply jay-barronville 17 hours agoparentThe function call itself isn’t really the culprit (the cost of the function call itself is negligible in this case). It’s the implementation of the function [0]. [0]: https://news.ycombinator.com/item?id=40542102 reply marcrosoft 3 hours agoprevI’m still using if err == something. I’ve never bothered to use error.Is. Looks like another reason to avoid reply zachmu 1 hour agoparentThe problem is error wrapping. Somebody deeper in the stack can break your equality check. reply leetrout 20 hours agoprevI like posts like this. I learn something about a tool I love and have used for 10 years and it isn't snarky, flamebait nor preachy. reply liampulles 9 hours agoprevThis is an interesting experiment. I have to say that nearly all the production code I write in Go is typically stuff interacting with external services like databases and SaaS APIs, so the difference in nanoseconds here is not going to move me away from using the safe idiomatic approach, but it is something to bear in mind for whenever I might use Go for very computationally intense tasks. reply cedws 3 hours agoprevThis isn't really relevant unless you're error checking inside a hot function - in which case, you need to rewrite your hot function. The majority of code out there is I/O bound (network/disk) anyway. reply lenkite 17 hours agoprevWish `errors.Is` immediately checked whether err is `nil` and returned `false`. Could have simply solved this issue I think. reply jay-barronville 17 hours agoparentThe first 3 lines of `errors.Is()` [0] are the following: if err == nil || target == nil { return err == target } [0]: https://github.com/golang/go/blob/9b43bfbc51c469ec13fca24960... reply lenkite 16 hours agorootparentGlad to know they have fixed this. In Go 1.22.3 (the latest release), the first 3 lines of `errors.is` [0] are the following: if target == nil { return err == target } [0]: https://github.com/golang/go/blob/go1.22.3/src/errors/wrap.g... reply zachmu 15 hours agorootparentThat's interesting, because I ran the benchmarks on that version of Go. reply richbell 15 hours agorootparentThe `err == nil` check was added in April to avoid expensive and unnecessary reflection. Perhaps 1.23, or whichever release that ends up in, would be faster? https://github.com/golang/go/commit/af43932c20d5b59cdffca454... reply tommiegannert 12 hours agoprevNice write-up. It's a shame that errors.Is is slow for general use, and at least some of that seems attributable to the Comparable change requiring reflection. Multi-errors seems to have bloated the switch. And of course the lack of a happy-path that was fixed in [1]. Since Go already has two ways of handling exceptional state: return or panic, it does feel like a stretch to also introduce a \"not found\" path too. All bets are off in tight inner loops, but I think as a general coding practice, it'll make the language (de facto) more complicated/ambiguous. But my take away is that the question has been kicked off: can wrapped errors be made more efficient? 1. https://github.com/golang/go/commit/af43932c20d5b59cdffca454... reply boomlinde 2 hours agoparentI would argue that \"not found\" is already present in the form of boolean \"ok\" return values, in cases where it really isn't an exceptional error state. On the other hand, the standard library implements things like errors.NewNotFound reply manlobster 19 hours agoprev> Here's one of them, which follows a common recommendation in the Go community to use a sentinel error to represent the \"value not found\" condition. Is this really a common recommendation in the Go community? Seems like returning `bool` to indicate if the value was found is somewhat of a no-brainer, since it follows the familiar approach used for map lookups and type assertions. reply segfaltnh 19 hours agoparentI've certainly seen and emulated this pattern in a lot of Go code. I don't know if I am representative of the larger community. I often use bool unless there are other error conditions (and there often are), but this article is definitely making me wonder if this is ideal. reply abound 18 hours agoparentprevThere are at least two places in the standard library that behave like this: sql.ErrNoRows and the error thrown by os.Open when a file doesn't exist. reply manlobster 17 hours agorootparentTrue, but both of these are for relatively slow operations, where the performance of the error checking is relatively insignificant. reply zachmu 1 hour agorootparentIt has also become somewhat common to use io.EOF to signal the end of iteration, like the reader libraries do. reply zitterbewegung 18 hours agoprevIn error handling or even reporting status there seems to be a part of being unaware that using stdout or some other strategy is either designed incorrectly which leads to slowdowns or people don’t understand when to use it. Sure languages also have this slowdown or have some documentation about it but many people just use what seems to be correct. reply amelius 5 hours agoprevError handling is one of the reasons why I lost interest in Go. reply randomdata 4 hours agoparentRegardless of language, error handling is my favourite part of programming. It’s where the most interesting problems lie. What motivates the “happy path only” programmer? reply amelius 4 hours agorootparentPerhaps, but without the proper tools/primitives it is certainly not my favorite part of programming. reply randomdata 4 hours agorootparentWhat tools or primitives are missing that you wouldn't also need in the \"happy path\"? It is not like error handling is any different than any other type of handling from a programming point of view. It's the \"business\" challenges that makes it interesting. Most everyone has already figured out how to deal with success, but few want to think about failure, leaving all kinds of interesting problems to solve. reply emmelaich 16 hours agoprevIs the performance lost in the panic recover() not the actual panic? Just a guess, I'm not a Go person. If there's no attempt to recover in the code, then panic() wouldn't need to save any state and the compiler might just optimise out anything needed for recover()y. reply ongy 12 hours agoprev@zachmu can you upload the code you ran to a public repo? I'd like to play with it a bit, to compare patterns I've done before and am too lazy to copy+paste ;) reply zachmu 2 hours agoparentThere is a link in the blog to a public gist, it's just a single file you can download and run. reply richbell 20 hours agoprev> errors.Is() is expensive. If you use it, check the error is non-nil first to avoid a pretty big performance penalty on the happy path. Am I missing something, or could this be partially mitigated by adding a check for `err == nil && target != nil` and vice versa, rather than `err == target`? https://github.com/golang/go/blob/9b43bfbc51c469ec13fca24960... reply ReleaseCandidat 18 hours agoparentWhat he is missing is that if the max. 3 pointer comparisons in (together with the function call of `Is`) if err == nil || target == nil { return err == target } are a \"pretty big performance penalty\", there isn't much work going on. I mean, of course, these 2 or 3 comparisons + the function call of `Is` are at least double that of err != nil even if `Is` would have been inlined by the compiler. reply richbell 14 hours agorootparentActually, the `err == nil` check doesn't exist in the latest release. It was added recently, which would explain the significant performance cost. If err is nil, it doesn't return early. https://github.com/golang/go/blame/adbfb672ba485630d75f8b559... reply tommiegannert 12 hours agorootparentNice find. This commit: https://github.com/golang/go/commit/af43932c20d5b59cdffca454... from April 4. reply metadat 18 hours agoparentprevI wonder why the compiler can't be smart enough to optimize such a case. reply Tomis02 15 hours agorootparentWe probably don't need programmers either, the compiler should be smart enough to know what the stakeholders mean. reply richbell 14 hours agorootparentprevThe answer is apparently that the code I linked isn't released yet. The latest available release only checks if target is nil. reply flexagoon 16 hours agorootparentprevIt already does reply SPascareli13 17 hours agoprevHow is this not a compiler bug? What would justify a nil check being many times more expensive then a bool check? reply tedunangst 16 hours agoparentThe nil check is not more expensive. reply SPascareli13 15 minutes agorootparentDid you read the article? It clearly show that it is. I just don't understand why. reply jakjak123 7 hours agoprevInteresting enough, but this misses most of the point. In any case, its not really that interesting if the value you looked for is missing or not, its way more important that I can see _which_ value was not found. Any solution that does not capture which value was missing, is kind of besides the point for most practical applications. reply zachmu 1 hour agoparentYou can get that by implementing an error type, or otherwise constructing a new error every time one is returned. But doing this adds to the cost of returning the error. This is how we use most errors in our code base. But we tend to not treat them as sentinels, we are only interested in the descriptive error messages. reply Groxx 16 hours agoprevFor other fun surprises that are rather obvious from the implementation but still surprise people quite regularly: wait until you see how much slower context keys are when compared to thread locals, particularly since it's normal for contexts to contain MANY layers (as opposed to errors, which are frequently* returned without wrapping at almost any level). It's a glorified linked-list lookup, and it re-walks the whole path every time. But for both contexts and errors: imo the tradeoff is overwhelmingly in favor of wrapping more, not less, in the vast majority of code. Context is useful and standard for many things, and wrapped errors carry a lot more useful information, and both of those are absolutely crucial for healthy interop with other code. Performance sensitive stuff can do whatever it needs, these are obviously terrible choices there so just don't even consider it. * but not always! and it may change with time. Still, my contexts are regularly 10+ layers deep while my errors are rarely more than one or two. reply neild 16 hours agoprevThere are some interesting results in here, but the slower cases are a bit misleading. The majority of time in the slow cases is spent constructing errors, not in errors.Is. Some background for anyone not familiar with Go errors: A Go error is an interface value with an Error method that returns the error's text. A simple error can be constructed with the errors.New function: var ErrNotFound = errors.New(\"not found\") A nil error indicates success, and a non-nil error indicates some other condition. This is the infamous \"if err != nil {}\" check. Comparing an error to nil is pretty fast, since it's just a single pointer comparison. On my laptop, it's about 0.5ns. Comparing a bool is about 0.3ns, so \"err != nil\" is quite a bit slower than \"!found\", but it's really unlikely the 0.2ns is going to be relevant outside of extremely hot loops. We can also compare an error to some value: \"if err == ErrNotFound {}\". In this case, we say that ErrNotFound is a \"sentinel\" (some error value that you compare against). This is about 2.3ns on my laptop; there are two pointer comparisons in this case and a bit more overhead in comparing interface values. (You can actually make this check almost arbitrarily expensive; you could have an error value that's a gigabyte-large array, for example.) It's common to annotate an error, adding some more useful information to it. For example, we might want our \"not found\" error to say what was not found: return fmt.Errorf(\"%q: not found\", name) // \"foo\": not found This is quite a bit more expensive than \"return ErrNotFound\". The fmt.Errorf function will parse a format string, produce the error text, and make two allocations (one for the error string, one for a small struct that holds it). This is about 84ns on my laptop--168 times slower than the fast path! But 84ns is still pretty fast, and you can't get away from the need for at least one allocation if you want to return an error that's varies based on the inputs of the function that produced it. (You can get faster than fmt.Errorf if it matters, but this comment is already getting large.) A problem with using fmt.Errorf in this way is that you can't test the error against a sentinel any more. This was addressed a while back in Go 1.13 with the addition of error wrapping. You can return an error that wraps the sentinel (note the %w format verb): return fmt.Errorf(\"%q: %w\", name, ErrNotFound) // \"foo\": not found And you can then use the errors.Is function to ask whether an error is equal to ErrNotFound, or if it wraps ErrNotFound: if errors.Is(err, ErrNotFound) { ... } On my laptop, producing a wrapping error like this and testing it with \"err != nil\" is about 91ns, and testing it with \"errors.Is(err, ErrNotFound)\" is about 98ns. So using Is is adding 7ns of overhead, which is not nothing, but is also pretty much lost in the noise compared to creating the error in the first place. The example in this blog post went a step further, though, and created an error with not just a single layer of wrapping but one with four. The error text in the wrapped error cases is: GetValue couldn't get a value: queryValueStore couldn't get a value: queryDisk couldn't get a value: not found (That is, by the way, a very difficult error to read. Don't hand users errors that look like that.) Creating a stack of four wrapped errors like this on my laptop is 396ns, and inspecting it with errors.Is is another 21ns. 21ns is waaaaay more than the 0.5ns for a simple \"err != nil\" check, but again the runtime here is massively dominated by the expense of creating the error--which in this case involves repeatedly creating formatted strings and throwing them away, and two allocations for each layer in the stack. In general, when doing low level optimization of Go code, avoiding allocations is the biggest bang for your buck. If microseconds matter, you absolutely should pay attention to the cost of constructing error values. But the cost of inspecting those values doesn't usually become an issue unless nanoseconds count, and will generally be dominated by the cost of construction. Also, even the slowest cases here are running about 0.5-1.5μs, which absolutely matters in some cases, but is irrelevant in many others. reply beautron 10 hours agoparent> In general, when doing low level optimization of Go code, avoiding allocations is the biggest bang for your buck. I have found this to be the truth. I'm making a game in Go, and have learned that a smooth framerate depends on programming with allocation awareness. Always know where and when your memory is coming from. reply zachmu 13 hours agoparentprevThis is true. We have in the past eliminated sentinel errors and gotten measurable gains from doing so, but this is mostly because our errors were both common (occurring several times on every request) and expensive to construct. The direct cost of errors.Is() was minor compared to the cost of building the error itself. And you might be surprised how common it is for people to insist on wrapping an error at every layer of the stack. I've gotten in arguments with these people online, they're out there. reply sethammons 8 hours agorootparentI insist on wrapping all errors each time and only removing that when performance testing shows it to be a bottleneck. A top concern of my systems is debugability which includes descriptive, wrapped errors with structured logging (this is a super power for system development and I am surprised when folks don't give love to structured logs and detailed, reproducible errors). I want organizational velocity in the general case. If wrapping an error is in a hot path and shows up in metrics, yeah, remove the wrapping. Otherwise, wrap the error. What is your argument against that? It would seem you find the compute savings of non-wrapped errors outweighs developer time and customer impact. If that is not what you are saying, please correct me. My creds are using Go since 1.2 and writing massively scaled systems processing multibillion events daily for hundreds of thousands of users with 4 to 5 9s of uptime across dozens of services maintained by hundreds of developers earning the company hundreds of millions of dollars. reply zachmu 2 hours agorootparentMy main argument against this practice isn't performance, it is that it makes error handling more difficult to write, review, and maintain. Treating errors as opaque and passing them up the stack in the general case is automatic and trivial to get right. Wrapping them is not. I agree with your point about debugging, but I have a different idea how to best achieve it. Rather than wrapping an error at every stack layer, just take a stack trace when the error is created. This works great as long as... you don't design the system to require sentinel errors. Treating errors as rare, exceptional events rather than normal values used for control flow changes how you approach them. reply azurelake 3 hours agorootparentprevMy argument against wrapping for backend services is that is: 1. I think that it is preferable to handle the error where it happened instead of at the top of the stack. For a backend service, there's really only three things you want to do with an error: log it, maybe bump some metrics, and return an error code and ID to the client. You have a lot more information available (including a stack trace if desired) if you handle it at this point. 2. By wrapping the error up the call stack, you're building an ad hoc stack trace. Performance wise, this is (probably, haven't measured) a lot better than an actual stack trace, but as you said yourself, the top concern is debug-ability and developer velocity. 3. Wrapping an error doesn't provide just a stack though, you can add values to the error! Except...what does that really buy you vs. just adding the values to your structured logging system going down the stack vs. doing it on the way back up in an ad-hoc way? Those wrapped error values are a lot more difficult to work with in Grafana vs. searching based on fields. 4. If I have a stack trace, structured log fields, and a correlation ID, I personally don't get any value out of messages like (\"could not open file), as I can just use the stack trace to go look at exactly what the line of code is doing. You could argue that with good enough wrapping, looking at the code wouldn't even be necessary, but I think that's pretty rare in practice. It also seems like a lot of extra work to spend a minute loading up the code in an IDE. 5. As mentioned in 1), what the client gets is just an error code and trace ID anyways. In fact, we actively don't want the wrapped context to be sent back to the client since it can be a security concern. If that's the case, we need to remove it and log it anyways. Why not just log the information in the first place? Anyways, curious to hear your thoughts. I used to advocate for wrapping errors, FWIW. reply 12345hn6789 14 hours agoprevThanks for the updates and article. If this is s big deal perhaps you should be choosing a different language :) Also BTW the nested `errors.Is` bench is wrong. Each function is wrapping the errors instead of constructing them preemptively and this is benching the time to create nested errors. Don't do this, in the test or real code. It makes it tough to read as a client. reply -mlv 14 hours agoprevI really don't want to be that guy, but I think the author meant to say '96.77%' instead of '3000%', or '96.66%' instead of '30x'. reply larsrc 9 hours agoparentYou understood what they meant, though. But yeah, it's confusing. I argued with my PhD advisor over this several times. \"50% slower\" or \"10% slower\" is ready to understand, but \"96% slower\" is harder to figure out, and it's easy to dismiss the difference between 96% and 98% because the numbers are similar reply samatman 1 hour agorootparentAs far as I can tell, this is an aspect of language which people merely pretend to have problems with, out of pedantry, rather than a barrier to understanding in any possible case. Everyone understands that if A is 2x faster than B, then B is 2x slower than A. If you say either of these things to someone with a basic grasp of sums, and then tell them A takes one second, they'll know that B takes two seconds. It doesn't matter which one you supply. No one would get this wrong. reply neonsunset 14 hours agoprev [–] The numbers in the benchmark are unsurprising for Go. Comparable code (for errors.Is()) written in C# takes about 4ns on M1 Pro: (made unidiomatic and abstracion-heavy-ish for demonstration purposes) https://gist.github.com/neon-sunset/65a8deeaac745ba159f990c7... Of course were it rewritten to a TryGet or (object, bool), it would bring that to 1-3 CPU cycles, as everything would get inlined and lowered to a few branches. reply randomdata 4 hours agoparent [–] For gc. The was no attempt to benchmark under gccgo, tinygo, etc. so we don’t really know how Go fares. Go is, by explicit design, multi-implementation. reply neonsunset 4 hours agorootparent [–] Do you have numbers for any of these on hand? I'm not holding my breath (after all, there's still issue of this being Go itself, which is inadequate at systems programming) but it's interesting to see how alternative implementations fare nonetheless. (this does not make the point any less valid as the solution this is applied to discussed in the blog post uses \"vanilla\" runtime flavour) reply randomdata 3 hours agorootparent [–] > Do you have numbers for any of these on hand? If 19ns is critical to my existence, I'm certainly not going to spend hours of it finding out if a handful of ns might be shaved off by using another implementation. > which is inadequate at systems programming Implying that Go is a scripting language? reply neonsunset 3 hours agorootparent [–] There are plenty of good languages which offer better expressiveness and performance in systems programming domain: C#, Swift, Rust, Zig, D, Nim. The biggest lie is Go looks like a low-level-ish language whilst it's anything but, so you end up with the worst of both worlds. It's tooling starts fast, which gives you a false belief that it will continue perform fast, and time and time again people learn the hard way that it's just not the case. It's ironic the story with C# is the opposite, that people expect it to underperform, and avoid it on the premise of criticism based on their imagination, only to be repeatedly proven wrong that it's much faster and extremely capable at systems programming tasks. reply randomdata 3 hours agorootparent [–] > There are plenty of good languages which offer better expressiveness and performance: C#, Swift, Rust, Zig, D, Nim. If we're sharing random tangents, I hear a lot of people like ice cream. > The biggest lie is Go looks like a low-level-ish language In what way does it look low level? I don't see it. Its closest language analog is Python, which I don't think anyone would see as being low level. Also, I assume this is meant to be in response to the \"systems\" bit. Systems are not defined by how \"low level\" they are. You most definitely could create a \"low level\" scripting language. You probably wouldn't, for various reasons, but you could. reply neonsunset 3 hours agorootparent [–] You're responding to something that isn't in my comments. Neither I said it's a scripting language, nor you are being honest in pretending that the industry does not associate whether a language is perceptibly low-level with its applicability in systems programming tasks. reply randomdata 3 hours agorootparent [–] You said systems, which implies a converse of scripts. There is nothing else in the category. If something is not a systems language, it is necessarily a scripting language. The vast majority of low level tasks are systems in nature (obviously; hence why a low level scripting language would be mostly pointless), so you're not wrong, but you're not telling the whole story. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Zach Musgrave's blog post benchmarks various error handling strategies in Go, revealing significant performance differences, particularly with the sentinel error pattern using errors.Is(), which can slow down code by over 5x.",
      "The study found that boolean checks were the fastest method, while using panic for error handling was the slowest. The blog provides detailed performance metrics and discusses the trade-offs of each approach.",
      "The author supports avoiding sentinel errors for better performance and code clarity, aligning with expert opinions like Dave Cheney's, and highlights the benefits of Go's multiple return values for efficient error handling."
    ],
    "commentSummary": [
      "The blog post initially claimed that using `errors.Is()` in Go slowed down code by 3000%, later corrected to 500% due to compiler optimizations, but the rank ordering of techniques remained unchanged.",
      "The discussion concluded that Go's error handling overhead is generally negligible, with trade-offs for simplicity and ergonomics often being worth it, and compared error handling performance across various programming languages.",
      "The text emphasizes the trade-offs in Go between wrapping errors for more information and using simple errors for faster comparison, highlighting the importance of efficient error handling and varying approaches across different languages."
    ],
    "points": 172,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1717186345
  },
  {
    "id": 40544875,
    "title": "Hugging Face Responds to Security Breach, Revokes Tokens and Enhances Measures",
    "originLink": "https://huggingface.co/blog/space-secrets-disclosure",
    "originBody": "Back to Articles Space secrets leak disclosure Published May 31, 2024 Update on GitHub Upvote 32 +26 huggingface Hugging Face Earlier this week our team detected unauthorized access to our Spaces platform, specifically related to Spaces secrets. As a consequence, we have suspicions that a subset of Spaces’ secrets could have been accessed without authorization. As a first step of remediation, we have revoked a number of HF tokens present in those secrets. Users whose tokens have been revoked already received an email notice. We recommend you refresh any key or token and consider switching your HF tokens to fine-grained access tokens which are the new default. We are working with outside cyber security forensic specialists, to investigate the issue as well as review our security policies and procedures. Over the past few days, we have made other significant improvements to the security of the Spaces infrastructure, including completely removing org tokens (resulting in increased traceability and audit capabilities), implementing key management service (KMS) for Spaces secrets, robustifying and expanding our system’s ability to identify leaked tokens and proactively invalidate them, and more generally improving our security across the board. We also plan on completely deprecating “classic” read and write tokens in the near future, as soon as fine-grained access tokens reach feature parity. We will continue to investigate any possible related incident. Finally, we have also reported this incident to law enforcement agencies and Data protection authorities. We deeply regret the disruption this incident may have caused and understand the inconvenience it may have posed to you. We pledge to use this as an opportunity to strengthen the security of our entire infrastructure. For any question, please contact us at security@huggingface.co. Upvote 32 +20",
    "commentLink": "https://news.ycombinator.com/item?id=40544875",
    "commentBody": "Space secrets leak disclosure (huggingface.co)166 points by markyg 7 hours agohidepastfavorite61 comments Aachen 4 hours agoJust two days ago I flipped through a slide deck from a security conference where the author, Jossef Harush Kadouri, found that using a model from a place like Huggingface means the author of the model can execute any code on your machine. Not sure if the slides are uploaded elsewhere, I got them sent as file: https://dro.pm/c.pdf (45MB) slide 188 I didn't realise at the time that I flipped through the slides that this means not only the model's author gets to run code on your machine, but also if Huggingface got a court-signed letter or if someone hacked them (especially if they don't notice for a while¹) As someone not in the AI scene, I've never run these models but was surprised at how quickly the industry standardised the format. I had assumed model files were big matrices of numbers and some metadata perhaps, but now I understand how they managed so quickly: a model is (eyeing slides 186 and 195) a Python script that can do whatever it wants. That makes \"standardisation\" exceedingly easy: everyone can do their own thing and you sidestep the problem altogether. But that comes with a cost. ¹ https://www.verizon.com/business/resources/articles/s/how-to... says 20% doesn't notice for months; of course, it depends on the situation and what actions the attackers take reply strangecasts 4 hours agoparent> I had assumed model files were big matrices of numbers and some metadata perhaps ONNX [1] is more or less this, but the challenge you immediately run into is models with custom layers/operators with their own inference logic - you either have to implement those operators in terms of the supported ops (not necessarily practical or viable) or provide the implementation of the operator to the runtime, putting you back at square one. [1] https://onnx.ai/ reply addandsubtract 4 hours agoparentprevIsn't that why we have the .safetensors format, which can't execute code on the host? reply dartos 3 hours agorootparentThis is my understanding as well reply SV_BubbleTime 3 hours agorootparentI was at Info sec local meet up six months ago, and mentioned the Tensor models get control of your CPU and GPU… I was basically passed over like I was some idiot making LLM == TERMINATOR claims. Good stuff. reply mistrial9 2 hours agorootparent> Tensor models get control of your CPU and GPU \"get control\" is not accurate.. there is a code execution model on the CPU based on an operating system; the GPU is a different story. Executed code with an operating system has execution context and protections.. this is basic to operating system theory, all the common operating systems implement related concepts. reply d-z-m 2 hours agoparentprev> using a model from a place like Huggingface means the author of the model can execute any code on your machine To my knowledge this is only a problem if the model is serialized/de-serialized via pickle[0]. [0]: https://huggingface.co/docs/hub/en/security-pickle reply hehdhdjehehegwv 1 hour agorootparentThe fact that pickle even exists is fundamentally wrong to start with. They should not be permitted as a distribution format, period. reply 1oooqooq 5 minutes agorootparentit's pretty neat and functional if you are the one writing the code. you can do things like \"hibernate\" with zero developer cost. reply bongodongobob 4 hours agoparentprevAre you telling me that when I run software on my computer I could potentially be running software on my computer? reply hn92726819 3 hours agorootparentThis is more akin to downloading a jpeg and the jpeg running arbitrary code. Models should be like jpegs and I believe safetensors treat them that way, while the old pickle format didn't. reply amelius 2 hours agorootparentYou typically need more than the model to actually use it, as you might need to \"massage\" the data into the right form before and after the model call. reply catgary 2 hours agorootparentprevThe weights are like jpegs, the model itself is still a piece of software that executes code. reply pheatherlite 2 hours agorootparentprevData intended to be read as instructions for the interpreter or the cpu, is a whole different ballgame than data intended to convey values of something. High order sparse/dense matrices serialized in some xyz format is what most people think of when they hear the word \"model\". To switch it up and send some arbitrary python file and execute it on the client is a security nightmare. This outrageous. reply foobiekr 4 hours agorootparentprevNobody expects a model file to be code thrat executes whatever. reply foolishbard 4 hours agorootparentI'd say anybody who is working in the field has this expectation. But the outside observer who is excited to try a new model does not expect it. reply squigz 4 hours agorootparentprevThis is like expecting any random binary to only do what it 'claims' to do. reply erikerikson 4 hours agorootparentNo, this is like PDFs that can contain and will execute PostScript but without a sandbox and with more general libraries reply LunaSea 4 hours agorootparentprevThat is borderline incompetence reply koolala 3 hours agoparentprevHugging face standing right behind you, ready for hugs reply fieldcny 5 hours agoprevThat’s a very weasley worded statement, to begin with “they have suspicions” is not a statement that should be in a communication of this type reply erhaetherth 55 minutes agoparentI thought it was pretty good actually. Most of these leak disclosures usually say things like \"We do not have evidence they accessed any secrets\" or something like that, because they don't \"know\" what the hackers did once they were in. At least huggingface is saying \"Yeah, they probably accessed secrets but we can't confirm it\" reply afro88 2 hours agoprev> Over the past few days, we have made other significant improvements to the security of the Spaces infrastructure, including completely removing org tokens (resulting in increased traceability and audit capabilities), implementing key management service (KMS) for Spaces secrets, robustifying and expanding our system’s ability to identify leaked tokens and proactively invalidate them, and more generally improving our security across the board. That's a serious amount of non-trivial work to be done in \"a few days\". The kind of work that should trigger more time consuming activities like security audits, pen tests and the like, before going live, right? reply erhaetherth 53 minutes agoparentHopefully the work was underway for awhile already, and maybe they just launched it now because the damage is already done? reply foolishbard 4 hours agoprevMy anthropic key was leaked and someone ran up a 10k bill on it. Are HF going to cover that? reply Tiberium 6 minutes agoparentAre you sure it was only stored in your space secrets? Not variables (which are public) or stored in the .env file (also public). reply mrkramer 4 hours agoparentprevI always thought you could set your \"maximum limit\" for spending on cloud providing platforms. reply a1o 3 hours agorootparentThat's surprisingly not a thing in many platforms. reply deusum 3 hours agorootparentprevThat $10k was probably the limit for their work, not someone else’s stolen time. reply Liftyee 2 hours agoprevThe title made me think this was an article about space, but instead I got an article about Space. reply Mo3 1 hour agoparentI legit thought someone leaked proof of extraterrestrial life and disclosure began. Another day.. reply jerpint 1 hour agoprevI noticed a few weeks ago that some of my OpenAI keys got compromised, they were only active as secrets on a huggingface space. I got an email a few days ago informing me that the spaces were compromised , so I suspect this issue has been going on for at least a few weeks reply nmstoker 1 hour agoprevThere's no mention of handling with regard to costs inappropriately incurred - wouldn't access to the secrets let people call APIs and run up costs? Or is this purely about theft of data/code? reply TekMol 5 hours agoprevWhy does HF store \"secrets\"? Couldn't they just store a public key, the user has the secret key and signs their requests with that? reply foolishbard 4 hours agoparentYou can build apps hosted on HF which access third party APIs, e.g. OpenAI or Anthropic. The api keys for these are then stored in the HF secrets reply cushpush 5 hours agoparentprevUsually you need some sort of \"token\" that lets you practically operate within a browser session. It seems like this is about tokens they had to revoke, which is kinda like a password but not. reply WhackyIdeas 6 hours agoprevWhat is ‘Space’ ? reply belter 6 hours agoparentIts shortcut for their Spaces. https://huggingface.co/docs/hub/en/spaces-overview The front end/portal. I speculate that is coded in Python. Maybe some Django thing... reply jimnotgym 5 hours agorootparentWhy is being coded in Python relevant? reply belter 5 hours agorootparentMight be relevant or not. Depends what you want to know about it. reply bn-l 4 hours agoparentprevIt’s a vm where they run your code reply white_beach 6 hours agoprevgiven how difficult it was do a simple thing -- this was not a surprise reply swader999 6 hours agoprevFor all those wondering, this is not about aliens. reply belter 6 hours agoparentIt's just occurred to me that if Aliens wanted to take over Earth...They could progressively leak scientific secrets, under the disguise of normal scientific progress. This would lead us create a Trojan-ed AGI, that would take over everything, and just build spaceships to ship them all our Palladium...Just imagine a giant spaceship on the way to Proxima Centauri full of stolen catalytic converters.... Can't get into the details, but it seems there is a way to convert Palladium into Dilithium Crystals. When you achieve that all hell breaks loose.... reply doctorhandshake 5 hours agorootparentThis figures in the plot of A Deepness in the Sky by Vernor Vinge reply belter 4 hours agorootparentNever read it. Sound interesting, and added to reading list. Maybe I am a just an LLM based Replicant... reply pavel_lishin 3 hours agorootparentHighly recommend it, as well as it's ... sequel, sort of? A Fire Upon The Deep. (Though I would skip the third book in the series, personally.) reply pavel_lishin 3 hours agorootparentprevAlso, somewhat in The Lives of Tao, by Wesley Chu. reply nilamo 4 hours agorootparentprevSort of the opposite of the Three Body Problem, haha reply portaouflop 6 hours agorootparentprevAliens already took over earth, we are the aliens. reply belter 6 hours agorootparentYeah, but we have no Tentacles, so we are considered a kind of subspecies, and ostracized in Galactic social circles... reply codetrotter 4 hours agorootparentThe cuttlefish and the octopi are the real aliens. You might not think they pose much of a threat. But of course, that’s merely because they are intentionally dormant at the moment. Once the AGI is complete, and the spaceships have been built.. it’ll be “so long and thanks for the fish” just like Douglas Adams predicted except he suspected the wrong species. reply yencabulator 2 hours agorootparentInventing technology is hard under water, hence octopuses created us to build them spaceships so they can go back home. reply pogue 5 hours agorootparentprevI like it! reply Oarch 2 hours agoparentprevHow can you be sure? Have you asked them? reply Rucadi 6 hours agoparentprevThat's what NASA wants us to think. reply wslh 5 hours agoparentprevBefore reading it I thought their AI components detected life beyond Earth, basically what SETI has been doing for decades [1]. [1] https://www.seti.org/ reply trashface 3 hours agoparentprevDisappointed. reply moose44 4 hours agoprevhttps://news.ycombinator.com/item?id=40539993 reply Macuyiko 5 hours agoprev [–] Very disheartening. HF is doing so much good in the AI community, much more than regulators understand at the moment. reply beardedwizard 5 hours agoparent [–] What does this comment mean? Why is it disheartening? What do regulators have to do with it? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hugging Face identified unauthorized access to their Spaces platform, potentially compromising some Spaces secrets, and revoked affected HF tokens.",
      "Users are advised to refresh their keys and switch to fine-grained access tokens as part of enhanced security measures.",
      "The company is working with cybersecurity experts to investigate and improve security, including removing org tokens, implementing key management services, and enhancing token leak detection."
    ],
    "commentSummary": [
      "Jossef Harush Kadouri disclosed a security risk with AI models from platforms like Huggingface, which can execute arbitrary code on a user's machine if compromised.",
      "The risk is due to these models being Python scripts rather than just data matrices, despite safer formats like ONNX and .safetensors being available.",
      "Huggingface acknowledged a security breach and has since upgraded their security measures, emphasizing the need for robust security audits and caution with insecure serialization formats like pickle."
    ],
    "points": 166,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1717241923
  },
  {
    "id": 40543990,
    "title": "Windows 11's Recall Feature Raises Major Security and Privacy Concerns",
    "originLink": "https://simonwillison.net/2024/Jun/1/stealing-everything-youve-ever-typed/#atom-everything",
    "originBody": "Simon Willison’s Weblog Subscribe Stealing everything you’ve ever typed or viewed on your own Windows PC is now possible with two lines of code — inside the Copilot+ Recall disaster (via) Recall is a new feature in Windows 11 which takes a screenshot every few seconds, runs local device OCR on it and stores the resulting text in a SQLite database. This means you can search back through your previous activity, against local data that has remained on your device. The security and privacy implications here are still enormous because malware can now target a single file with huge amounts of valuable information: During testing this with an off the shelf infostealer, I used Microsoft Defender for Endpoint — which detected the off the shelve infostealer — but by the time the automated remediation kicked in (which took over ten minutes) my Recall data was already long gone. I like Kevin Beaumont's argument here about the subset of users this feature is appropriate for: At a surface level, it is great if you are a manager at a company with too much to do and too little time as you can instantly search what you were doing about a subject a month ago. In practice, that audience’s needs are a very small (tiny, in fact) portion of Windows userbase — and frankly talking about screenshotting the things people in the real world, not executive world, is basically like punching customers in the face. Posted 1st June 2024 at 7:48 am Recent articles Training is not the same as chatting: ChatGPT and other LLMs don't remember everything you say - 29th May 2024 Weeknotes: PyCon US 2024 - 28th May 2024 ChatGPT in \"4o\" mode is not running the new features yet - 15th May 2024 Slop is the new name for unwanted AI-generated content - 8th May 2024 Weeknotes: more datasette-secrets, plus a mystery video project - 7th May 2024 Weeknotes: Llama 3, AI for Data Journalism, llm-evals and datasette-secrets - 23rd April 2024 microsoft 101 privacy 36 security 450 sqlite 246 Source code © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40543990",
    "commentBody": "[dupe] Stealing everything you've ever typed on your Windows Recall PC is now possible (simonwillison.net)160 points by serhack_ 10 hours agohidepastfavorite20 comments iceyest 9 hours agoDuplicate? This was already posted 12 hours ago, though not the same link. All the discussion seems to be here: \"Recall: Stealing everything you've ever typed or viewed on your own Windows PC\" 171 comments https://news.ycombinator.com/item?id=40540703 reply dang 1 hour agoparentComments moved thither (except the ones that only make sense here). Thanks! reply WhackyIdeas 7 hours agoparentprevYeah, adding DUPE to this all of a sudden has made it disappear off the top spot (and main page) so people will be less likely to find out about this today. And it’s kinda important (understatement) for everyone to know about… especially if they use Windows or have family and friends who use it. reply politelemon 9 hours agoprev [–] IMO the link should point at the original source: https://doublepulsar.com/recall-stealing-everything-youve-ev... The blog post is insanely reductive and clickbaitish reply simonw 9 hours agoparentI agree that the original article should be linked here, not my summary. I take offense to the clickbait accusation. This post wasn't intended to be shared elsewhere, it was a bookmark on my personal website. I have 7,000 more of those here: https://simonwillison.net/search/?q=&type=blogmark reply temp3000 8 hours agorootparentI agree, not clickbait. It probably needs a heading or something to make it clear it is a summary of another blog. I was a bit confused I assumed this was your blog post at first. Maybe because I am on mobile there is some missing context about what this is. reply breck 8 hours agorootparentprevI had not heard the term \"blogmarks\" before, but looks interesting, what you are doing there. Have you written anything about that yet? reply simonw 8 hours agorootparentI started using the term about twenty years ago: https://simonwillison.net/2003/Nov/24/blogmarks/ It's basically my name for a link blog. My biggest inspiration for this is https://daringfireball.net where most of the posts are links plus commentary but sometime they are full blown essays in their own right. https://waxy.org is another link blog that I really like. reply lagniappe 8 hours agorootparentprevI had a similar idea and built linkfeed for it - though blogmark probably makes more sense. Here's mine: https://linkfeed.net/u/5rh44yzy And RSS: https://linkfeed.net/u/5rh44yzy/feed reply nonameiguess 8 hours agorootparentprevI understand and don't disagree with this sentiment, but I also think it's worth considering the Internet fame you've developed over the decades. Your blog isn't a diary. A lot of people are reading it and you might take that into consideration. It's going to be shared far and wide whether you intend it or not. I'm not really a fan of Scott Alexander any more, but something he used to do years back that I liked was putting an \"epistemic status\" disclaimer at the top of every new post. Some were well-researched, evidence-backed treatises. Some were barely idle shower thoughts. Everyone's musings, whether they write them down or not, probably fall into categories like these. But when you know others are reading it, it can be helpful to make very explicit which is which so readers know what to take seriously and what to consider exploratory and possible but also only preliminary and maybe a deadend or even wrong. reply danieldk 8 hours agorootparentI understand and don't disagree with this sentiment, but I also think it's worth considering the Internet fame you've developed over the decades. Your blog isn't a diary. A lot of people are reading it and you might take that into consideration. Their blog can be anything they want? The post literally starts with the link to the original source. It's not like those typical clickbait sites that bury the link somewhere deep down. IMO, it's up to the submitter to link to the original source. I see it more as quote-Tweets, someone with reach amplifying useful/interesting articles. reply usrbinbash 8 hours agorootparentprev> Your blog isn't a diary A blog is exactly what the one who writes it wants it to be. reply waihtis 8 hours agorootparentprevOn the other hand, you could thank simonw for writing over 7000 instances of free summaries. reply fragmede 8 hours agorootparentprevthe short entries are tagged with #blogmark. reply rmbyrro 9 hours agoparentprevIt is a summary, not an insane reduction. Summaries can have sane usefulness. I didn't find it clickbaity at all. reply dom96 8 hours agorootparentSame, I enjoyed the terseness of this summary. reply timeon 8 hours agorootparentAnd was not present with pop-ups like on that Medium site. reply Nevolihs 8 hours agoparentprev [–] I've been wanting something like this for my entire life and it seems like computers are finally heading in a direction of becoming extensions of ourselves instead of just clunky tools we have to adapt ourselves to. This has been the kind of sci-fi I've dreamt of since I was a kid. I get the privacy concerns. And security concerns. But honestly, if somebody has this kind of access to your computer, they have access to your entire life anyway. Setting up a keylogger, getting all your passwords, getting any vital documents, etc. It's trivial. When is it just pointless fear mongering? It's subject to the same security concerns as everything else and we don't suggest people switch to pen and paper instead. reply usrbinbash 8 hours agorootparent [–] > But honestly, if somebody has this kind of access to your computer, they have access to your entire life anyway. Exactly. So please tell me why I would want to essentially allow something like this on my machine? As for the scifi-esque abilities: All my important information is stored in a personal wiki, which is automatically backed up and encrypted on a backup server. It's fully tagged, its fully searchable. And you know what? I have even set up a pipeline to feed it into a locally run LLM if I want to. Yes, I can do RAG on my personal wiki. reply Nevolihs 6 hours agorootparent [–] > All my important information is stored in a personal wiki Ah, there's the crux, no? I dream of a system that automatically captures and allows me to interact with and query against everything I do, every ebook I download, every image, every document, every video, every site. What you're describing isn't this, it's not even the same ballpark. Recall, even if it isn't perfect, is in the same ballpark. > Exactly. So please tell me why I would want to essentially allow something like this on my machine? Because it's useful. I don't understand this. It's like a circular argument. I want a useful feature, I install it on my PC. The security concerns apply regardless of Recall existing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Simon Willison's blog highlights a new Windows 11 feature called Recall, which captures periodic screenshots, performs Optical Character Recognition (OCR), and stores the text in a SQLite database for easy searching.",
      "While potentially useful for busy professionals, the feature poses significant security and privacy risks, as malware could target the database containing sensitive information.",
      "Despite protections like Microsoft Defender for Endpoint, data could be compromised before remediation, making the feature beneficial for a small subset of users but potentially harmful for the general user base."
    ],
    "commentSummary": [
      "Hacker News users are discussing \"Recall,\" a tool that can capture everything typed or viewed on a Windows PC, raising significant privacy and security concerns.",
      "The debate includes whether Simon Willison's summary is sufficient or if linking to the original, more detailed article is necessary for proper context.",
      "The conversation also explores the broader implications of such surveillance tools, with some users acknowledging potential benefits despite the associated risks."
    ],
    "points": 160,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1717231409
  },
  {
    "id": 40543330,
    "title": "NGINX Unit 1.32.1: Versatile Open-Source Web Application Server Released",
    "originLink": "https://unit.nginx.org/",
    "originBody": "v. 1.32.1 About Key features News Installation Control API Configuration Scripting SSL/TLS certificates Usage statistics How-to Troubleshooting Community Universal web app server§ NGINX Unit is a lightweight and versatile application runtime that provides the essential components for your web application as a single open-source server: running application code (including WebAssembly), serving static assets, handling TLS and request routing. Unit was created by nginx team members from scratch to be highly efficient and fully configurable at runtime. The latest version is 1.32.1, released on Mar 26, 2024. See a quickstart guide on our GitHub page. Browse the changelog, see the release notes in the news archive, or subscribe to our RSS feed. Check out the discussion of our key features for further details. Peek at our future plans with a GitHub-based roadmap. Watch the entire NGINX Unit tutorial course here. Edit this page © 2017-2024 NGINX, Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=40543330",
    "commentBody": "Nginx Unit: open-source, lightweight and versatile application runtime (nginx.org)132 points by thunderbong 12 hours agohidepastfavorite84 comments geenat 10 hours agoAt this point I'd rather be good at Caddyfile and have a project folder of: /home/me/project/caddy /home/me/project/Caddyfile No sudo, no config spew across my filesystem. Competition is good, and I had a lot of fun with nginx back in the day but it's too little too late for me. reply klabb3 3 hours agoparentThis is more of a linuxism no? I agree though, I have used Linux for decades but I never remember the usr, bin, local, etc permutations of magical paths, nor do I think it makes any sense. It’s a mess honestly, and is almost never what I want. When I was younger I thought I was holding it wrong but these days I’m sure it will never, ever map well to my mental model. It feels like a lot of distro-specific trivia that’s leaking all over the floor. It seems like a lot of the hipster tooling dropped those things from the past and honestly it’s so much nicer to have things contained in a single file/dir or at most two. That may be a big reason why kids these days prefer them, honestly. As for nginx itself it’s actually much better suited for high performance proxies with many conns, imo. I ran some benchmarks and the Go variants (traefik, caddy) eat a lot of memory per conn. Some of that’s unavoidable because of minimum per-goroutine stacks. Now I’m sure they’re better in many ways but I was very impressed with nginxs footprint. reply kbenson 1 hour agorootparentWindows has the same thing, it's just much less exposed, and none of the paths are magical, they're well defined and mostly adhered to for all major distros. The core of how it works in Linux is fairly straightforward. The main difference is in how additional software is handled. Windows, because of its history with mostly third party software being installed, generally installed applications into a folder and that folder contained the application... Mostly. Uninstalling was never as simple as that might imply. Linux distros had an existing filesystem layout (from Unix) to conform to, so when they started developing package managers, they had to support files all over the place, so they make sure packages include manifests. Want to know where use executable are? Check bin. Superuser.com executables? Check sbin (don't want those cluttering the available utils in the path of regular users). Libs for in libs. /bin and /usr/bin and the others are holdovers from the long past when disks were small, and recent distros often symlink usr to / so they're different in name only. /usr/local is for admin local modifications that are not handled through a package. /opt is for whatever, and often used for software installed into a contained folder, like in windows. Just know what bin, sbin, lib, opt and etc are for and most the rest is irrelevant as long as you know how to query the package manager for what files a package provides or as it what package a specific filer belongs to. If you looked I to windows and the various places it puts things I suspect you'd find it at least complicated, if not much more. Note: what I said may not match the LSB (which someone else usefully posted) perfectly, but for the most part it should work as a simple primer. reply gjvc 3 hours agorootparentprevhonestly reply klabb3 3 hours agorootparentMaking me self conscious, honestly. I’m not a patient and careful writer. That’s why I’m lurking in the comments. reply nevermore24 3 hours agorootparentprevI don't know if not being able to remember filesystem conventions is Linux's fault. Computers have a lot of esoterica and random facts to recall. How is this one any different? See also: https://refspecs.linuxfoundation.org/FHS_3.0/fhs/index.html reply sofixa 24 minutes agoparentprevI'm really not a fan of Caddy. It tries to be too smart and make things simple for users, which means that the second something goes wrong or you step out of the expected box, you get weird behaviour which is hard to debug. Fun example from last week, a colleague was trying to try out ACME with a custom ACME server, and configured it. For some reason Caddy was not using it and instead used its own internal cert issuer, even if explicitly told to use the ACME provider as configured. Turns out that if you use the .local domain, Caddy will insist on using its own cert issuer even if there's an ACME provider configured. Does that make sense? Yeah, somewhat, but it's the kind of weird implicit behaviour that makes me mistrust it. My go-tos are nginx for static stuff, Traefik for dynamic stuff. reply cpach 7 hours agoparentprevAFAIK, nginx doesn’t require root. If you’re thinking about the ability to bind port 80/443, you should be able to do that via CAP_NET_BIND_SERVICE. With that said, Caddy is pretty rad. reply kaptainscarlet 10 hours agoparentprevCaddy sounds like the go to tool for people who care a lot about getting things done. It's time for me to try it. reply NetOpWibby 4 minutes agorootparentI recently setup a Flarum forum and the instructions mentioned Apache and Nginx. I sighed until I saw Caddy immediately below. Caddy really is the most pleasant webserver software I’ve ever used. reply diarrhea 10 hours agorootparentprevA coworker of mine dislikes it as it bundles everything into a single binary. For example, to have ACME DNS-01 challenges for certificate issuance working, I need to compile in a Google DNS-specific plugin. But then it... just works. Try the same with most other web servers/proxies and you're in for a world of pain. Having this much functionality bundled into a single binary is as much a curse as it is a blessing. That said, having your own little 'Cloudflare Workers' in the form of Nginx Unit with wasm sounds great. Not sure Caddy can do that. reply TheCapeGreek 8 hours agorootparentFor me, the promise of Caddy and especially tools around it like FrankenPHP make the \"everything in a single binary\" idea the MORE enticing option, not less. Sure we already have repeatable infrastructure, containers, etc. but I also love the idea of just building and shipping a PHP app binary that includes the webserver. It makes server provisioning even less of a priority, especially if I have reasons to not use serverless or PaaS tools. reply AbraKdabra 2 hours agorootparentprevHaving a single binary is definitely what drives me to use certain software, Deno is one of them. reply beeboobaa3 1 hour agorootparentIt's great until you want to include a non-standard plugin and need to compile your own binaries. Now that single binary deployment requires you to compile the software yourself. Caddy has nice tooling for this but it'd be far more convenient to just drop a dll/so file in the right directory. Single binary deployments are great if someone else did the compiling for you. If you need to compile yourself it truly does not matter if you need to ship a single binary or a directory or whatever. reply 9dev 9 hours agorootparentprevIf you want to see a real-life example of what Caddy can do, feel free to check the configuration of my iss-metrics project: https://github.com/Radiergummi/iss-metrics/blob/main/caddy/C... I was in the same boat as you and wanted to try out what Caddy is capable of. I was immediately convinced. So many features, where you expect them. Consistent configuration language. Environment interpolation, everywhere. Flexible API. It’s really all there. reply ac130kz 7 hours agorootparentFrom the first glance it doesn't look convincingly better than a generic and manually polished nginx configuration. Are there any other benefits to Caddy? reply 9dev 2 hours agorootparentIf you choose to start the project with docker compose, you’ll notice how it will immediately bring up a fully functional reverse proxy setup with TLS support on localhost; set the SITE_DOMAIN environment variable to your proper domain instead, and you’ll find that configured as well, along with a proper, ACME-issued certificate. Add a bit more effort, and you’ll also get mTLS for all services automatically. All of this is more or less doable with nginx, I’ve done it often enough. But read the Caddyfile and tell me this isn’t miles ahead in clarity. reply otabdeveloper4 48 minutes agorootparentprevBetter Docker integration out of the box, I guess. I don't use docker so I don't care. reply corobo 5 hours agorootparentprevIt does all the letsencrypt stuff for you - certbot is not a massive hassle if you're just serving the one domain of course but I really liked it for that when I was setting up a redirect server (corps do love buying TheirBrand.everytld haha) Set the config up with CI/CD and can now just edit the config and git push knowing Caddy will just handle the rest reply ac130kz 3 hours agorootparentSeems to be a middleground between doing certs on a small scale with cronjobs and a fully fledged automated Kubernetes cluster. reply page_fault 9 hours agorootparentprevIt's a fine project right up to the point of you needing additional functionality that's split into one of the plugins. Since Go applications do not support proper .so in practice, you have to build your own binaries or rely on their build service, and this puts the responsibility of supporting and updating such custom configuration on you. So no setting up unattended-upgrades and forgetting about it. reply thegeekpirate 2 hours agorootparentI think that's what https://caddyserver.com/docs/command-line#caddy-upgrade (and the following commands) are for ;) reply beeboobaa3 1 hour agorootparent> experimental also totally non-standard, apt unattended-upgrades won't be doing that for you. sure you can do a cronjob, but, non-standard reply bavell 4 hours agorootparentprevEh, it's a bit over hyped imo although I do like the config format and built-in acme. My production clusters all run nginx though and give me minimal fuss with a lot of flexibility. reply asmor 8 hours agoparentprevhas anyone figured out why caddy is substantially slower (thoughput, not latency) than nginx at reverse proxying? i've switched it around for my seafile and it's a night and day difference. reply ac130kz 7 hours agorootparentGarbage collection pauses might have something to do with that. reply tempest_ 4 hours agorootparentAlso the lack of 20 years of optimization where it spent a lot of time as one of the larger open source web servers and so got a lot of attention. In 2024 people are more likely to turn the cloud knob up to pay for throughput (if they need it) and save on dev time with the comparably better dev ex that caddy offers. reply nickjj 3 hours agorootparent> In 2024 people are more likely to turn the cloud knob up to pay for throughput (if they need it) and save on dev time with the comparably better dev ex that caddy offers. This seems like a weird trade off to me. The \"learning tax\" is really only paid once with nginx. Once you understand how it works and configured a reasonably end-to-end example with it then you can carry that over to your next project with minimal changes. I've hosted countless Flask, Django, Rails, etc. apps over the years and very little changes on the nginx side of things. I'd rather learn this tool once and have better runtime performance all the time across all projects. With that said, the performance difference probably won't be very noticeable for most sites but still, I personally wouldn't want to give in to running a less efficient solution when I know a more efficient solution exists right around corner that requires no application code changes to use -- just a little elbow grease to configure nginx once. This is especially true when nginx has a ~20 year track record of stability and efficiency. reply jacob019 1 hour agorootparentRight. Everytime nginx comes up someone has to bring up how much better caddy is. After using nginx everywhere for nearly two decades I have no desire to learn a new tool. Nginx does everything I want, even some exotic stuff and plugins for certain use cases. It is highly configurable, has plenty of good documentation, is well supported in every distro, and is extremely performant. I don't care how much easier caddy is and that it can configure certs for me. I prefer the unix philosophy anyway, and it's not like I'm spending a significant amount of time on nginx configs or certs. I use acme.sh for certs it only takes a couple minutes to provision a new instance with nginx and acme.sh, just the way I want it. End rant. reply tempest_ 38 minutes agorootparentSure, but you list of things there describes a work flow that is becoming a bit old. I would wager the vast majority of Nginx \"installs\" are running in a container now a days. The distro doesnt matter and few are provisioning an instance of anything, that's some container orchestration job. Last week trying to coax nginx to be able to set a CSP nonce in a web apps index.html which apparently meant I would need to custom build Nginx or a custom build a container with a custom built Nginx to install a plug-in to do it. This type of stuff adds up and having a bunch of stuff hidden in Nginx Plus doesn't help either. I think Nginx is a great piece of software its just that people don't need all its offerings, they just want to host some tiny js and proxy to an API and things like caddy were built for that. The limited throughput doesnt matter when CloudFlare or Cloud front cache most of the things it is serving anyway. reply jacob019 13 minutes agorootparentI'm pretty much always in a container too, and I'm perfectly happy with the workflow, it scales just fine and can be orchestrated as well. When I'm deploying a new project, bringing up a new container with nginx is like 0.1% of the work, why mess with that? I like Debian, I use it in my containers. Yes there are slimmer and lighter things, I don't care, my stack is rock solid. As for the CSP nonce, I'm surprised that there isn't a plugin, but compiling isn't a big deal, just annoying. Alternately NGINX is scriptable or you can do it at the application level as well. If caddy is easier for you or for that use case, then that's great, use it with my blessing. reply lelanthran 1 hour agorootparentprev> I don't care how much easier caddy is and that it can configure certs for me. I always find this a weird selling point, TBH. It's probably a selling point for people who don't already know the existing $FOO. For me, putting effort into learning the new thing only to use it exactly as the old thing is wasted effort. I don't know what I gain by moving to the new $FOO, usually. reply asmor 19 minutes agorootparentprevI'd consider only pushing 20Mbps on a 2.5GbE network more than a lack of optimization. Supposedly you can tune some buffer sizes to make it better, but it's still laughably bad for serving larger files. reply asmor 9 minutes agorootparentprevGo isn't Java, especially after the rewrite in 1.5 (and smaller scale changes in 1.19) the GC doesn't pause long or often enough to affect throughput. reply dboreham 4 hours agorootparentprevUnlikely. reply renewiltord 1 hour agoparentprevThese are things for sure, but nginx config files are well understood by LLMs so I get good advice from them. That's really the limiting factor for most equivalent tools for me these days, how well the LLM handles it. If someone hooks them up to a man page I think it might level the playing field. reply callahad 1 hour agoprevHi! I'm currently in charge of Unit. If you're using it, I'd love to chat with you to understand what sucks, what doesn't, and what's missing. I have my own ideas, but external validation is always nice. :) Contact info is in my profile. reply simonw 10 hours agoprevFor some reason I had a thought lodged in my head that Unit wasn't open source, but I just checked their GitHub repo and it's been Apache 2 since they first added the license file seven years ago. I must have been confusing it with NGINX Plus. reply rvnx 4 hours agoparent“Oops, sorry, thank you for letting us know, we will change that to the proprietary license instead” reply callahad 2 hours agorootparentI wouldn't bet on that. :) F5 isn't the most visible corporation in terms of grassroots engagement, but NGINX itself has remained F/OSS all these years and newer projects like the Kubernetes Ingress Controller [0], Gateway Fabric [1], and NGINX Agent [2] are all Apache 2.0 licensed. Just like Unit. We do have commercial offerings, including the aforementioned NGINX Plus, but I think we've got a decent track record of keeping useful things open. [0]: https://github.com/nginxinc/kubernetes-ingress [1]: https://github.com/nginxinc/nginx-gateway-fabric [2]: https://github.com/nginx/agent reply rvnx 2 hours agorootparentOk, seems better than the industry then :) I have trauma from Aerospike, Redis, and couple of others, so it may have affected my perception. reply g15jv2dp 46 minutes agorootparentI don't the other one, but what's your gripe with redis, exactly? Can you articulate it? reply vdfs 3 hours agorootparentprevUsually it's done on purpose, they wait until it get very popular and used everywhere before pulling the carpet reply ngrilly 11 hours agoprevNeat! What is the benefit of using this over \"standalone\" nginx? The HTTP API enabling configuration change at runtime without downtime (like Caddy)? No need for a process supervisor like supervisord or systemd as nginx Unit is managing the backends? reply callahad 2 hours agoparentExactly, they're complements: you'd deploy your application on Unit and put that behind NGINX or another reverse proxy like Caddy or Traefik. Unit can serve static assets, directly host Python / PHP / WebAssembly workloads, automatically scale worker processes on the same node, and dynamically reconfigure itself without downtime. Unit cannot do detailed request/response rewriting, caching, compression, HTTP/2, or automatic TLS... yet. ;) reply 9dev 10 hours agoparentprevIt’s pretty much like Caddy vs. nginx: Language runtime, static asset serving, TLS, routing and so on bundled in a single package. That makes it very easy to deploy a container, for example. Thinking of a typical PHP app, which exposes both dynamically routed endpoints and static asset. With a traditional setup, you’d let nginx handle all paths as static assets and fallback to the index.php file to serve the app. When you package that as a container, you’ll either have to use separate PHP-FPM and nginx containers, or run two processes in a single container. Both of which is not ideal. And it gets ever more complex with TLS, and so on. Using unit or caddy, you can simplify this to a single container that achieves it all, easily. reply SahAssar 8 hours agorootparentWhat is caddys language runtime? Which languages does it support? Or are you thinking of the frankenphp plugin? reply 9dev 8 hours agorootparentCaddy can either use something sophisticated like FrankenPHP, which I very much look forward to using soon now that it seems stable, or a regular old FastCGI SAPI. reply SahAssar 6 hours agorootparentBut nginx also supports FastCGI, and you need to run the FastCGI server as a separate process (like php-fpm), right? I don't see how caddy (without stuff like frankenphp) is any closer to a complete single binary reverse-proxy AND language runtime than nginx. reply chuckadams 4 hours agorootparentUnit has its own SAPI for PHP and executes it directly, no php-fpm needed. I’m using it to serve Wordpress right now, works pretty well. reply SahAssar 3 hours agorootparentRight, but the parent comment seemed to imply that the same was true for caddy. I was asking what caddys language runtime was (besides via plugins like frankenphp)? reply chuckadams 2 hours agorootparentPlugins would be it, so you could say Go is Caddy's runtime. Which is of course duh, but since the official mechanism to extend it is by statically compiling in go code, it's also accurate. It's not like nginx and apache are that much different, their \"language runtimes\" also boil down either to extensions linked into the server or proxying to a backend through another protocol like FastCGI. Caddy supports fcgi out of the box, even using PHP's default settings with one line of config, but I'm not a big fan of php-fpm, and I like having just one daemon to supervise. reply SahAssar 1 hour agorootparentSo walking back to the comment (https://news.ycombinator.com/item?id=40543839) this started at: > It’s pretty much like Caddy vs. nginx: Language runtime, static asset serving, TLS, routing and so on bundled in a single package. That makes it very easy to deploy a container, for example. > Using unit or caddy, you can simplify this to a single container that achieves it all, easily. With caddy this is not true, unless you have compiled in your own plugin (custom or frankenphp), right? All I was asking is what they thought the language runtime for caddy was. reply attentive 8 hours agoparentprevIt's an app server. It can run your asgi or wsgi app. reply supriyo-biswas 10 hours agoparentprevFor me I'd rather ship a single binary with PHP support in it when using containers. reply la_fayette 10 hours agorootparentCan you elaborate on that? Especially, where is the php runtime and the webserver? reply casperb 9 hours agoprevI tried a setup with Nginx Unit and php-fpm inside a Docker container, but the way to load the config is so combersome I never was confident to use it in production. It feels like I am doing something wrong. Is there a way to just load a config file from the filesystem? reply callahad 2 hours agoparentWe're very actively working on improving Unit's UX/DX along those lines. Our official Docker images will pick up and read configuration files from `/docker-entrypoint.d/`, so you can bind mount your config into your container and you should be off to the races. More details at https://unit.nginx.org/installation/#initial-configuration But that's still kinda rough, so we're also overhauling our tooling, including a new (and very much still-in-development) `unitctl` CLI which you can find at https://github.com/nginx/unit/tree/master/tools/unitctl. With unitctl today, you can manually run something like `unitctl --wait-timeout-seconds=3 --wait-max-tries=4 import /opt/unit/config` to achieve the same thing, but expect further refinements as we get closer to formally releasing it. reply casperb 1 hour agorootparentThat sounds much better, thanks for the effort. reply jonatron 8 hours agoparentprevhttps://unit.nginx.org/howto/docker/#apps-in-a-containerized... > We’ve mapped the source config/ to /docker-entrypoint.d/ in the container; the official image uploads any .json files found there into Unit’s config section if the state is empty. reply casperb 6 hours agorootparentI saw that, but I do like to make my own container. So I did roughly the same steps as they do. But it feels complicated. reply jonatron 5 hours agorootparentCan you copy the official image's script? https://github.com/nginx/unit/blob/0e79d961bb1ea68674961da17... reply ajayvk 5 hours agoparentprevI am building https://github.com/claceio/clace. It allows you to install multiple apps. Instead of messing with routing rules, each app gets a dedicated path (can be a domain). That way you cannot break one app while working on another. Clace manages the containers (using either Docker or Podman), with a blue-green (staged) deployment model. Within the container, you can use any language/framework. reply gawa 8 hours agoparentprevThe docs mentions: > The control API is the single source of truth about Unit’s configuration. There are no configuration files that can or should be manipulated; this is a deliberate design choice (https://unit.nginx.org/controlapi/#no-config-files) So yeah, the way to go is to run something like `curl -X PUT --data-binary @/config.json --unix-socket /var/run/control.unit.sock http://localhost/config/` right after you start your nginx-unit. The way to manage a separate config step depends on how you manage to run the process nginx-unit (systemd, docker, podman, kubernetes...). Here's an example I found where the command is put in the entrypoint script of the container (see toward the end): https://blog.castopod.org/containerize-your-php-applications... reply casperb 6 hours agorootparentI did that, but sometimes it takes a short moment before Unit is started, so you need a loop to check if Unit is responding before you can send the config. In total it was around 20 lines just to load the config. It feels like doing something wrong. Or using the wrong tool. reply random_savv 9 hours agoprevHow does this compare to OpenResty? Could it somehow help with OIDC support (e.g. by integrating a relevant nodejs lib)? reply RantyDave 9 hours agoprevDjango without gunicorn? I’ll give it a go… reply anentropic 4 hours agoparentI'm wondering what are the pros and cons of that vs this Years ago every website needed Apache or Nginx, then lately I've hardly used it at all... usually have a container with gunicorn behind a load balancer that is part of the cloud platform It's easy to see how to get Nginx Unit working, but not sure exactly how it fits into the utility picture vs other options reply teitoklien 2 hours agorootparentNginx is a reverse proxy, it can work as the load balancer, the static asset server, a response caching server, an authorization server, HLS streaming server, etc. Nginx has a lot of usecases in one package, most larger companies and more technical workplaces, use Nginx or similar alternatives. Nginx Unit typically is meant for unifying the entire application space of multiple programming languages served by 1 Server (which also acts as a static server) So serving golang, python, php code and application all under 1 single wsgi/asgi application server called nginx unit and be able to dynamically change its routes with api calls too. This allows you to have 1 root process in a docker container to control your python fastapi or golang api processes, without needing 1 container for nginx and 1 container for Python/Golang process or a supervisord like init system controlling 2 or more processes inside 1 container Everything is under nginx unit and nginx unit is being triggered as the main process inside the container. Moreover it is also much much much more faster in terms of response time than most language dependent application servers like gunicorn/unicorn for python3, etc [1] [1](https://medium.com/@le_moment_it/nginx-unit-discover-and-ben...) reply move-on-by 5 hours agoparentprevI’ve largely enjoyed gunicorn. What do you dislike about it? reply DataDive 2 hours agorootparentI largely agree here that of all the components in a stack, unicorn seems to be the least troublesome and almost invisible. I have never had a problem that I would have traced back to gunicorn not working ... On the other hand not having to run gunicorn as another separate service might be an advantage. reply pjmlp 7 hours agoprevThere is a certain irony that after the application servers bashing phase, a decade later everyone is doing their own version. reply throwaway894345 5 hours agoparentWhat is an application server exactly and who besides nginx is building one? reply pjmlp 5 hours agorootparentApache with mod_tcl, mod_perl, mod_php, Websphere, JBoss (now Wildfly), Payara, IIS/.NET, Erlang/BEAM... Basically a full stack experience from programming language, web framework and networking protocols, possibly with a management dashboard. As for who is building one everyone that is now trying to sell the idea of packaging WebAssembly into containers, deploying them into managed Kubernetes clusters, or alternatively cloud managed applications, like Vercel, Nelify, Azure Container Apps, Cloud Run,.... reply anentropic 3 hours agorootparent> everyone that is now trying to sell the idea of packaging WebAssembly into containers, deploying them into managed Kubernetes clusters, or alternatively cloud managed applications Does Nginx Unit really fit into this picture though? Is there a place for an all-in-one app server in that scenario, I would have thought they want each component to be separated (wasm host, load balancer, etc etc) for commoditisation and independent scaling of different layers (This is not a criticism in form of a question... I am honestly curious) reply callahad 1 hour agorootparentAbsolutely keep your load balancer for multi-node scaling, but how are you going to run you WebAssembly workloads within a given node? Unit can do that. Or what if you have a single logical service that's composed of a mix of Wasm endpoints and static assets augmenting a traditional Python application? Unit pulls that all together into a single, unified thing to configure and deploy. If you're writing Node, Go, or Rust you haven't had to think about application servers for a long time. Folks writing Python and PHP still do, and WebAssembly will require the same supporting infrastructure since Wasm -- by definition -- is not a native binary format for any existing platform. :) reply anentropic 36 minutes agorootparentWell there are other dedicated \"WASM in k8s\" solutions like SpinKube and my Python apps have not been behind Nginx for a long time, they're mostly wrapped in a zero-config gunicorn runner in a Docker container, static assets in S3 via a CDN am wondering who wants a single-node heterogenous application server these days TBH the simplicity of it is appealing though reply callahad 3 minutes agorootparentIMHO, it's still a few years a early for pure-play Wasm solutions, though Fermyon is doing exceptional work to manifest that future. My hope is that Unit can offer a pragmatic bridge: run your existing applications as-is, and when you want to sprinkle in some Wasm, we're ready. That's not to say Wasm is Unit's only use case, but do believe it's what will get people thinking about application servers again. :) > my Python apps have not been behind Nginx for a long time, they're mostly wrapped in a zero-config gunicorn runner in a Docker container, static assets in S3 via a CDN ...and are there any reverse proxies, load balancers, or caches on the network path between your end user and your container? ;) fastball 2 hours agoprevWe've been using Nginx Unit in production for a Python backend for about a year now and it's been working pretty well. Some thoughts in no particular order: - \"Nginx Unit\" is an annoying name to google when you have a problem. You get a lot of nginx results that are of course completely irrelevant to what you're looking for, as there is zero actual overlap between the two things. Using quoted search terms is not sufficient to overcome this. - When it works, the all-in-one system feels great. - However sometimes the tightly-coupled nature can be slightly annoying. For example, they publish packages for the various runtimes (ours is python) in the various registries, but only for the \"defaults. Concrete example: we are currently running Ubuntu 23.04 but wanted to upgrade to Python 3.12. However Nginx Unit only pre-packages a Python 3.11 package for unit on Ubuntu 23.04 as that is the system-included Python. Had to build our own support from source, which was fairly easy, but still more difficult than our pre-Nginx Unit infra, where all I would have to do is install Python 3.12 and I'm good to go (because the python runtime wasn't at all coupled with the webserver when our stack was Nginx + Python + Gunicorn) - I never properly benchmarked the two in a comprehensive comparison, but Nginx Unit is definitely faster than the aforementioned previous stack. I tested some individual routes (our heaviest/most important) and the speedup was 20-40%. - Even when I tell it to keep a minimum number of worker processes around, it kinda seems... not to? I haven't properly tested, but sometimes it feels more like serverless, where if I haven't sent a request in a while it takes a bit of extra time to spin up a process, but after that requests are snappier. Definitely need to properly investigate this but haven't gotten around to it yet. It might just be the difference between allocated memory and not rather than spinning up processes. - It's a shame it doesn't support SSL via Let's Encrypt out-of-the-box, like Caddy. To me that is the biggest (really only) missing piece at the moment. - I much prefer using the HTTP system to manage config than files, and find the Nginx Unit JSON config much, much more readable than either Nginx or Apache configs I've worked with in the past. I'd also give it a slight edge over caddy configuration. - That said, managing the config (and system in general) can sometimes be annoyingly opaque. The error messages are somewhat terse when updating config fails, so you need to dig into the actual logs to see the error. Just feels a little cat-and-mousey when you could just tell me what the error is up-front, when I'm sending the config request. In summary, overall I've liked using Nginx Unit, but wish they would: change the name to something less confusing, add built-in Let's Encrypt support ala Caddy, and make the errors and overall stack feel a little less opaque / black boxy. reply xandrius 6 hours agoprevI read this and expected some sort of unit testing for nginx configurations. I'd love to have something like that: provide a configuration and automatically check all the paths that the configuration enables. Maybe throw in some LLM for some comments and tips to improve performance/security. reply nginxdud 5 hours agoprev [–] Another dud from the people that bought stolen code. Nginx is awful, archaic software. reply OneOffAsk 5 hours agoparentNginx is a state machine that efficiently handles lots of L4-L7 protocols. Seems weird to feel any emotions about it. reply g15jv2dp 42 minutes agoparentprevDid you really create an account just to hate on nginx? That's sad. reply victorbjorklund 5 hours agoparentprev [–] What do you use instead? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NGINX Unit is a lightweight, versatile, open-source web application server that integrates application runtime, static asset serving, TLS handling, and request routing.",
      "The latest version, 1.32.1, was released on March 26, 2024, offering enhanced efficiency and runtime configurability.",
      "For more details, users can refer to the GitHub page for a quickstart guide, changelog, release notes, and future roadmap."
    ],
    "commentSummary": [
      "The discussion compares web servers and application runtimes, focusing on Nginx Unit, Caddy, and traditional Nginx, highlighting their respective strengths and weaknesses.",
      "Nginx Unit is praised for dynamic reconfiguration and multi-language support but criticized for lacking built-in Let's Encrypt SSL support and having configuration challenges.",
      "Caddy is noted for ease of use and automatic TLS, though some users find it overhyped and less performant than Nginx, while traditional Nginx is favored for stability and performance."
    ],
    "points": 132,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1717222681
  },
  {
    "id": 40544219,
    "title": "Ticketmaster Data Breach Exposes 560M Customers' Information, Hackers Demand Ransom",
    "originLink": "https://www.bbc.co.uk/news/articles/cw99ql0239wo",
    "originBody": "Ticketmaster confirms data hack which could affect 560m globally IMAGE SOURCE, GETTY IMAGES Published 1 June 2024, 09:32 BST Updated 09:53 BST Ticketmaster owner Live Nation confirmed \"unauthorised activity\" on its database after a group of hackers said they had stolen the personal details of 560 million customers. ShinyHunters, the group claiming responsibility, says the stolen data includes names, addresses, phone numbers and partial credit card details from Ticketmaster users worldwide. The hacking group is reportedly demanding a $500,000 (£400,000) ransom payment to prevent the data from being sold to other parties. In a filing to the US Securities and Exchange Commission, Live Nation said that on 27 May \"a criminal threat actor offered what it alleged to be Company user data for sale via the dark web\", and that it was investigating. The number of customers affected by the data breach has not been confirmed by Live Nation. The Ticketmaster breach was first revealed by hackers who posted an advert for the data on Wednesday evening. Ticketmaster refused to confirm it to reporters or customers and instead notified shareholders late on Friday. The Australian government said it is working with Ticketmaster to address the issue. The FBI has also offered to assist, a spokesperson for the US Embassy in Canberra told Agence France-Presse. A spokesperson for the FBI told the BBC it \"has no comment on this matter\". In its filing, Live Nation said it was working to \"mitigate risk\" to its customers and that it was notifying users about the unauthorised access to their personal information. \"As of the date of this filing, the incident has not had, and we do not believe it is reasonably likely to have, a material impact on our overall business operations or on our financial condition or results of operations. We continue to evaluate the risks and our remediation efforts are ongoing\", it added. American website Ticketmaster is one of the largest online ticket sales platforms in the world. This hack is one of the biggest in history in terms of global victims but it’s not yet clear how sensitive the data is that is in the hands of cyber criminals. Researchers are also warning that it’s part of a larger ongoing hack involving a cloud service provider called Snowflake which is used by many large firms to store data in the cloud. Snowflake notified customers of an increase in cyber threat activity targeting some of its customers’ accounts. On Friday Santander confirmed it had data from an estimated 30m customers stolen which was being sold by the same hacking group as the Ticketmaster hackers. It’s thought these hacks are all linked and many others could become public. An advert with some data samples allegedly obtained in the breach have been posted on the website BreachForums - a newly relaunched hacking forum on the dark web where other hackers buy and sell stolen material, and information to enable hacks to take place. ShinyHunters has been linked to a string of high-profile data breaches resulting in millions of dollars in losses to the companies involved. In 2021 the group sold a genuine database of stolen information from 70 million customers of US telecoms firm AT&T. In September last year, almost 200,000 Pizza Hut customers in Australia had their data breached. The FBI cracked down on the domain in March 2023, arresting its administrator Conor Brian Fitzpatrick, but it has reappeared, according to tech media. Users of hacking forums often inflate the scale of their hacking to attract attention from other hackers. They are often where large stolen databases first appear but can also feature false allegations and claims. Individuals declaring large batches of data in the past have proven to be duplicates of previous hacks rather than newly stolen information. If the data hack is as large as claimed by ShinyHunters, the hack could be the most significant breach ever in terms of numbers and the extent of the data stolen. This is not the first time Ticketmaster has been hit with security issues. In 2020 it admitted it hacked into one of its competitors and agreed to pay a $10m fine. In November it was allegedly hit by a cyber attack which led to problems selling tickets for Taylor Swift's Era's tour. Earlier this month, US regulators sued Live Nation accusing the entertainment giant of using illegal tactics to maintain a monopoly over the live music industry. The lawsuit from the Department of Justice said the firm's practices had kept out competitors, and led to higher ticket prices and worse service for customers. What to do if you are worried you have been affected Experts say it’s important not to panic but to be alert, if you think you may be a victim. Watch out for bogus emails, messages and phone calls - hackers can sometimes use the details they have to trick victims into revealing more information. In some cases scammers may try and exploit the fear caused by the hack as a way of trying to persuade you to share information. Be especially suspicious of: official-sounding messages about \"resetting passwords\", \"receiving compensation\", \"scanning devices\" or \"missed deliveries\" emails full of \"tech speak\", designed to sound more convincing being urged to act immediately or within a limited timeframe In 2018 when a hack put some Ticketmaster customer information at risk, UK officials also suggested users , external kept an eye on their financial accounts for suspicious activity. They also advised changing your password for Ticketmaster and on any other sites using the same password. Get in touch Are you worried your Ticketmaster account has been hacked? Contact form Related Topics Computer hacking Ticketing More on this story Data allegedly stolen from 560 million Ticketmaster users Published 1 day ago Santander staff and '30 million' customers hacked Published 21 hours ago",
    "commentLink": "https://news.ycombinator.com/item?id=40544219",
    "commentBody": "Ticketmaster confirms data hack which could affect 560M globally (bbc.co.uk)128 points by simonjgreen 9 hours agohidepastfavorite4 comments redbell 6 hours ago [–] The HN ranking algorithm seems to be missing out something here! There's already another, older, story [1] on the same topic currently on the front page, ranked #24. I believe it would make more sense if @dang merged both threads in a single, homogenous one for more value. _______________ 1. https://news.ycombinator.com/item?id=40543408 reply dang 4 hours agoparent [–] As you wish. reply razodactyl 3 hours agorootparent [–] What are you. Some sort of omnipotent genie admin? Haha Does the HN system surface things like the comment above to you or are you just everywhere. reply slater 2 hours agorootparent [–] I think dang has often mentioned that \"@dang\" doesn't work, but I wouldn't put it past him to have an automated search for that ;) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Live Nation, Ticketmaster's parent company, confirmed a data breach affecting potentially 560 million customers globally, with hackers ShinyHunters claiming responsibility.",
      "The stolen data includes names, addresses, phone numbers, and partial credit card details, with hackers demanding a $500,000 ransom to prevent the sale of the data.",
      "The breach is linked to a larger hack involving cloud service provider Snowflake, affecting other companies like Santander, and users are advised to be vigilant against phishing attempts and monitor their financial accounts."
    ],
    "commentSummary": [
      "Ticketmaster has confirmed a data breach that could potentially impact 560 million people worldwide.",
      "Discussions on Hacker News suggest merging two separate threads about the incident for better coherence, with an admin agreeing to the suggestion.",
      "The conversation includes light-hearted remarks about the admin's responsiveness and the platform's functionality."
    ],
    "points": 128,
    "commentCount": 4,
    "retryCount": 0,
    "time": 1717234419
  },
  {
    "id": 40543408,
    "title": "The Best Web Browsers for Privacy and Security: Tor, Brave, and Firefox Lead the Pack",
    "originLink": "https://stackdiary.com/ticketmaster-confirms-data-breach-with-a-sec-filing/",
    "originBody": "The Best Web Browsers for Privacy and Security",
    "commentLink": "https://news.ycombinator.com/item?id=40543408",
    "commentBody": "Ticketmaster confirms data breach with a SEC filing (stackdiary.com)123 points by skilled 12 hours agohidepastfavorite63 comments baxtr 6 hours ago> Researchers are also warning that it’s part of a larger ongoing hack involving a cloud service provider called Snowflake which is used by many large firms to store data in the cloud. Now that’s interesting. Anyone know how they’re related? reply dddddaviddddd 6 hours agoparenthttps://news.ycombinator.com/item?id=40534868 reply kmfrk 7 hours agoprevOne of the fun things about these messes is that sometimes you think you're unaffected, but then it turns out the company bought one of the companies you actually did sign up with, and all of a sudden you're a part of a database you never signed up for. Had that happen a few times with my +company@gmail filters that went on to reveal the trail of acquisition breadcrumbs. reply sys_64738 6 hours agoprevIf somebody becomes a victim of this Ticketmaster fraud then the company should be fully liable. We really need an event such as this to bankrupt a company and for all the C-level suits to be made financially bankrupt too. Let's use the like of Ticketmaster fraud as a starting point to make these companies prevent fraud in the first place, instead they pass the buck to those customers defrauded. reply skeeter2020 3 hours agoparentWho should \"go bankrupt\" in this case? Ticketmaster? Snowflake? Snowflake's data subprocessors? The manufacturer(s) of the desktop or security apps on the compromised employee's machine? I get your natural but naive reaction, but as someone who just lived through the Sisense breach as neither the original source or end-user, it's much more complicated than you try and present it. reply ChildOfChaos 8 hours agoprevNext week. Ticketmaster introduces new 'data protection fee' to all ticket purchases. reply GrumpyNl 7 hours agoparentWill include AI to make it more safe. reply ixaxaar 7 hours agoparentprevCool, rise of concert tourism? At least better than medical tourism ig. reply doctor_eval 5 hours agoparentprevhttps://theshovel.com.au/2024/05/30/ticketmaster-hacker-dema... “Ticketmaster Hacker Demands $500K Ransom (Plus $300K Ransom Processing Fee, $220K Ransom Handling Fee)” Whole (joke) article is hilarious. reply flawn 7 hours agoprev2 weeks ago, when attending a concert, i got forced TWICE back to back to reset my password when logging in. I jokingly called then that Ticketmaster got breached. Crazy to see this being actually true. reply wombat-man 6 hours agoparentI feel like I often have to login again with ticketmaster and I'm never sure of they can't figure out token refresh logic or if it's some kind of anti theft measure or what. reply HeckFeck 5 hours agorootparentHappens all the time: I need the wretched app to go to a QR code-only concert. Would love to take up a stallman-esque stand against this, but the bands I love aren't getting any younger and neither am I. Need to show my digital ticket to the steward. Only to find the app has again signed out, spend ages fumbling with my phone and password manager to sign in, while standing in the rain outside of the venue. Not the seamless experience they promise! reply a012 5 hours agorootparentIdk if you could take a screenshot of the QR code for offline use? I always do this when buying public transport tickets in Europe. reply pc86 4 hours agorootparentThe last half-dozen concerts I've been to have some sort of live code or something where screenshots don't work. I haven't spent enough time trying to deduce if the code itself changes or if the scanner just reads animation over the code, but at least for the moment you do need the live app to get into most concerts if they're using the Ticketmaster or Live Nation apps. reply crazygringo 4 hours agorootparentIt's almost certainly the code that changes. That's easy to program and is very common in apps that produce scannable QR codes that they don't want duplicated, that will only work for the next e.g. 60 seconds. Any animation that could reliably be picked up by a scanning camera would necessarily be quite visible to the user. Just think of how QR codes often don't even work until you increase your phone's brightness, and they're as high-contrast as you can get... reply portaouflop 6 hours agorootparentprevIf you have refresh tokens in your first-party authentication flow you are already a good way down complexity lane - best to avoid IMO. reply timothyduong 11 hours agoprevYou’d think for all the additional booking fees that they’re known for charging that they’d invest a little more into security of their infrastructure both internally and externally… reply portaouflop 9 hours agoparentWhy should they invest? Most likely nothing bad will happen to their bottom line or execs so no reason to invest in security. We decided that profits are more important than accountability. reply mholt 7 hours agoparentprevThey recently got sued for being a monopoly. They have no incentive to do better. And unfortunately this probably won’t change anything. https://www.justice.gov/opa/pr/justice-department-sues-live-... reply nerdawson 4 hours agoparentprevTheir data was exposed by breaching a third party service. I'm not sure how investing more in security could have helped prevent this. You could argue that they shouldn't be housing this data with Snowflake but then you could say the same about a service like Amazon S3. At what point is a company able to rely on a third-party vs being expected to run it in-house? reply skeeter2020 3 hours agorootparentYou're right, but nobody here cares about the details of data subprocessors and how a lot of nodes in the chain get impacted. It doesn't play well on the internet where short & pithy moral outrage is the level of discourse. It makes me sad that the initial reaction is gleeful & mean-spirited towards the targets, when if you've ever been involved in something like this you'd hope it would be empathy for what a lot of Snowflake and TM employees are working on this weekend, and anger towards the hackers. It's like people forget that because it's data and the targets are big companies these criminals aren't stealing from real people and making the world a worse place. reply TeMPOraL 5 hours agoparentprevLike anything ever happened to any company that had a data breach. It's been shown time and again that it's a waste of money to care about this. In contrast, what is worth it is to do the minimum necessary to check the boxes on whatever certification the business is trying to get. reply safety1st 6 hours agoparentprevOn the contrary, economists have known for some time [1] that this (investing in improvements to their product) is exactly what monopolies DON'T do well. And the DoJ is suing Ticketmaster for being an abusive monopoly as we speak, so..... [1] Actually, we've known this as far back as Adam Smith! He talked a fair bit of shit about monopolies in The Wealth of Nations. His \"invisible hand,\" after all, was competition, which monopolies by definition don't have. Remember this every time someone talks shit about capitalism and points to corrupt and abusive American monopolies as evidence of why it's a failed system. America's government only really started leaning into its modern love for monopolies from the Reagan administration onward - in defiance of centuries of evidence that it was a bad idea. reply sa-code 9 hours agoparentprevNope those fees might as well be called \"yachts for executives fee\" reply CaptainOfCoit 6 hours agoparentprevWho would think that? Extra earned resources never goes into improving security, why would it be different with Ticketmaster? Live Nation Entertainment / Live Nation, owner of Ticketmaster, is a public for-profit company, 100% of their focus will be on \"more money now\", and security doesn't contribute to that so of course they wouldn't focus on that. reply nine_zeros 8 hours agoparentprevThose yachts won't get bought themselves. reply mholt 7 hours agoprevTicketmaster has recently been sued by the DoJ for monopolizing: https://www.justice.gov/opa/pr/justice-department-sues-live-... Too bad nothing will improve because of this. reply speedylight 1 hour agoprevSituation sounds ripe for a new credit monitoring service, “Creditmaster”. Be the problem, and sell the solution! reply prophesi 5 hours agoprevThis is potentially related to the Snowflake breach reported yesterday. https://www.theverge.com/2024/5/31/24168984/ticketmaster-san... reply _tk_ 4 hours agoparentThere is no confirmation of a \"Snowflake breach\". Single accounts might have been compromised, but that's all we know for sure so far. reply ggm 11 hours agoprevAnd in Australia ticketek who has also filed an Australian cyber crime report. So far only name DOB and address. reply skissane 2 hours agoprevTicketmaster historically has had a very unusual architecture – they wrote their own operating system for the VAX, and ran their core application on top of it. When the VAX reached end-of-life, most users had a migration path elsewhere – VMS users went to Alpha, Unix could either go to Alpha or to a competing Unix platform – Ticketmaster didn't have any such path, because when you write your own operating system, porting it to a new platform is all on you. It didn't help that most (all?) of it was written in assembly language. So they ended up writing their own VAX emulator for Linux, and hence their in-house VAX operating system could continue to run as an app under Linux. And they've been trying for years to get away from all that legacy on to a more mainstream platform. (And for all I know that legacy system is still running today.) But this breach clearly has nothing to do with all that, since it involved a breach of the Snowflake cloud database. So either the legacy system's replacement, or some supporting system. Side note: Terry Davis was one of the developers of Ticketmaster's OS, before he wrote TempleOS, although they created their OS years before he was hired. He credited working on Ticketmaster's OS as part of his inspiration for writing his own reply dimask 10 hours agoprevAnybody who is concerned and wants to learn what personal info of them ticketmaster holds and hence may have been compromised, and lives in a country where GDPR applies, they can send them a GDPR subject access request, eg something like [0]. Their emails in general look like privacy@ticketmaster.XXX where XXX the country code but probably also double check to make sure. [0] https://www.linkedin.com/pulse/nightmare-letter-subject-acce... reply Tblue468 8 hours agoparentAs per the Ticketmaster website [1], there are also the following forms to request a copy of your data: - US: https://privacyportal.onetrust.com/webform/ba6f9c5b-dda5-43b... - Non-US countries: https://privacyportal.onetrust.com/webform/ba6f9c5b-dda5-43b... [1] https://privacy.ticketmaster.com/en/privacy-policy#your-choi... reply Ylpertnodi 7 hours agorootparentThe non-us sounds like a type of mediation. Gdpr requests have teeth. reply mulmen 11 hours agoprevWhat data was leaked specifically? Were my payment credentials or bank account information leaked? How do customers know how to respond to this? reply a012 10 hours agoparent> The hackers also shared a sample of the data, which includes hashed credit card numbers, the last four digits of credit cards, credit card expiration dates, and fraud details, as well as customer names, addresses, and emails. https://www.cyberdaily.au/security/10632-hackers-claim-ticke... reply saulrh 7 hours agorootparentDoes anyone know what \"fraud details\" are? Is this just (\"just\", I say) the answers to security questions, or is this, like, your mother's maiden name and every address you've ever lived at and all the other stuff that would let anyone on earth open a bank account in your name? reply a012 7 hours agorootparentI don’t know about the sample of leaked data, but I presume the fraud details is just a boolean value true or false which means the transaction was rated legitimate or not. reply cushpush 5 hours agoprevWhere does my convenience fee go if not to securing my data? Court should order a rebrand to Ticketchump reply demondemidi 5 hours agoprevWouldn’t the data be worth more than £400k on the black market? reply simonjgreen 4 hours agoparentIt’s an interesting question. What’s the going $:identity exchange rate I wonder? Based on this ransom $0.0009 per identity? reply CaptainZapp 7 hours ago [flagged]prevnext [20 more] \"As of the date of this filing, the incident has not had, and we do not believe it is reasonably likely to have, a material impact on our overall business operations or on our financial condition or results of operations. We continue to evaluate the risks and our remediation efforts are ongoing\" Ah, All is well then. It has no material impact on your business. So, the 600'000'000 affected customers can kindly go and fuck themselves? reply chollida1 6 hours agoparentLook, I get just being blindly outraged at a hack and the hate that Ticketmaster seems to generate, but the company wrote this for the regulators and shareholders as much as anything. They have a legal duty to report hacks and report if it will have any affect on their business. And this is what they are doing here. Being outraged because a company does what they are legally obliged to do just makes you angry for no reason. They'll address the hack to customers no doubt, in a separate email/communication. At no point did they say the 600M people \"can kindly go and fuck themselves\" and contorting things to try and get there will just make you angry. If you want to blindly feel anger, you're welcome to, but the company did what they should here. reply tigerBL00D 5 hours agorootparentI see both sides of this, but I think I would side with the original comment. It's not just blind outrage or something like that. The point is that there ought to be impact on Ticketmaster business. The messed up and there should be a cost to the company. They should be paying big fines or compensating their users. It should be entirely possible for a company to go bankrupt in such a situation which would incentivise better internal practices. reply vlovich123 4 hours agorootparentSo this is an interesting case because by all accounts it was Snowflake that got compromised, not Ticketmaster themselves. So this isn’t them messing up but their vendor. I get the argument that you are responsible for your vendors, but at the same time the whole point of buying a SaaS product is precisely to hire domain experts to run a piece of infrastructure that you have less experience with. Security is very hard even for major companies with a stronger culture of it who spend much more resources on it like Apple and Google. It’s not clear that if Ticketmaster would have done a better job. I think the view I’m most sympathetic too is that customer information should be viewed and reported on as a toxic asset/liability to discourage gathering of personal information in the first place. reply fuzzfactor 6 hours agorootparentprev>At no point did they say the 600M people \"can kindly go and fuck themselves\" With nary a word this was fully implied way before they even started collecting customer data. Companies can have this attitude regardlless of whether there is any data at all. reply Timshel 4 hours agorootparentprevThey basically said: - no expected damage for the leak - no cost for any remediation - no cost for any legal issue - no expected cost to ensure it does not happen again If you believe that they should be impacted then it's kind of a Fuck you. They are not the ones making the law but it certainly looks like they will plainly use the fact that they can continue business as usual with almost no impact. reply LightBug1 5 hours agorootparentprev>> At no point did they say the 600M people \"can kindly go and fuck themselves\" That is exactly what they said. Verbatim. That's how my brain received it, anyway. reply pc86 4 hours agorootparentThen you should see a doctor because you're either hallucinating or don't know what verbatim means. This is a specific communication for a specific audience and there are legal requirements around what is reported. reply nothercastle 5 hours agorootparentprevThe problem is that there is no material impact due to these issues. The shareholders should absolutely be impacted and the company on the hook for damages reply ChildOfChaos 4 hours agorootparentprevThe more damning part is this though 'Ticketmaster refused to confirm it to reporters or customers and instead notified shareholders late on Friday.' They cared more about there shareholders than users. reply toast0 4 hours agorootparentprev> At no point did they say the 600M people \"can kindly go and fuck themselves\" It's TicketMaster, that's their business model. No need to say it again in this message to the SEC. reply bobmcnamara 7 hours agoparentprevHey now, Ticketmaster will continue to fuck us too! reply nehal3m 7 hours agoparentprevThe PR is for shareholders, not the peasants. reply keiferski 4 hours agoparentprevThis statement is for investors and managers, not customers. Customers don't typically read statements filed with the SEC. Companies get hacked all the time and the material effect on the underlying business is, indeed, often not significant. Especially for something as monstrously huge as TicketMaster. reply mmsc 6 hours agoparentprevThat's just a requirement for the SEC filing. If it has interrupted business operation, it needs to be specified. Think if a hospital or a manufacturer has a breach. reply anotheraccount9 7 hours agoparentprevSounds like the Equifax breach response ;) nothing to worry I guess reply tyingq 5 hours agoparentprevThey aren't wrong, consequences for them are likely light. There's simultaneously a huge breach with Snowflake, which is high impact as many companies would have their customer data in a platform like that. And past breaches like Equifax that exposed almost everyone. They are mostly just acknowledging that the general public has \"breach fatigue\". Nobody cares anymore. It's just another 12 months of free monitoring on top of the others you already have. So now you just freeze your credit until you need a loan, unfreeze it, put it back. reply cqqxo4zV46cp 6 hours agoparentprevOh. Come on. This isn’t the Joe Bloggs News comment section. That quote is from an SEC filing. The literal purpose is to update shareholders on exactly this. There’s so much to hate about TM without stooping to this. reply tomrod 5 hours agorootparentIt wasn't the shareholders' data that was leaked. A bit of contrition is required. Not a nice to have. Not contextual. Required. reply pc86 4 hours agorootparentNot in an SEC filing, it's not. reply antonioevans 6 hours agoprev [–] Isn't this timing weird in that they just got charged by the DOJ with monopoly? \"Oh we had a hack, that's why we cannot find the documents, your honor.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article reviews web browsers focusing on privacy and security, highlighting Tor, Brave, and Firefox for their strong privacy tools like ad-blocking, anti-tracking, and encryption.",
      "It emphasizes the significance of regular updates and user control over settings to improve online safety.",
      "The review aims to guide users in choosing browsers that best protect their privacy and security."
    ],
    "commentSummary": [
      "Ticketmaster confirmed a data breach in an SEC filing, connected to a larger hack involving Snowflake, raising significant concerns about data security and corporate accountability.",
      "Users are frustrated over potential fraud and perceive inadequate security investments despite high fees, sparking discussions on the implications for customers and the inefficacy of corporate responses.",
      "The incident has led to debates about Ticketmaster's response, the prioritization of shareholders over customers, and signs of public \"breach fatigue,\" indicating desensitization to such news."
    ],
    "points": 123,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1717223643
  }
]
