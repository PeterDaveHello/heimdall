[
  {
    "id": 39444500,
    "title": "Enhance Privacy on Signal: Hide Your Phone Number with Usernames",
    "originLink": "https://signal.org/blog/phone-number-privacy-usernames/",
    "originBody": "Keep your phone number private with Signal usernames Randall Sarafa on 20 Feb 2024 Signal’s mission and sole focus is private communication. For years, Signal has kept your messages private, your profile information (like your name and profile photo) private, your contacts private, and your groups private – among much else. Now we’re taking that one step further, by making your phone number on Signal more private. Here’s how: New default: Your phone number will no longer be visible to everyone in Signal If you use Signal, your phone number will no longer be visible to everyone you chat with by default. People who have your number saved in their phone’s contacts will still see your phone number since they already know it. Connect without sharing your phone number If you don’t want to hand out your phone number to chat with someone on Signal, you can now create a unique username that you can use instead (you will still need a phone number to sign up for Signal). Note that a username is not the profile name that’s displayed in chats, it’s not a permanent handle, and not visible to the people you are chatting with in Signal. A username is simply a way to initiate contact on Signal without sharing your phone number. Control who can find you on Signal by phone number If you don’t want people to be able to find you by searching for your phone number on Signal, you can now enable a new, optional privacy setting. This means that unless people have your exact unique username, they won’t be able to start a conversation, or even know that you have a Signal account – even if they have your phone number. Right now, these options are in beta, and will be rolling out to everyone in the coming weeks. Note that even once these features reach everyone, both you and the people you are chatting with on Signal will need to be using the most updated version of the app to take advantage of them.1 Importantly, all of this is optional. While we changed the default to hide your phone number from people who don’t have it saved in their phone’s contacts, you can change this setting. You are not required to create a username and you have full control over whether you want people to be able to find you by your phone number or not. Whatever choices work for you and your friends, you’ll still be able to communicate with your connections in Signal, past and present. Keeping your phone number private on Signal Once these features roll out, your phone number will no longer be visible in Signal to anyone running the latest version of Signal who doesn’t already have it saved in their phone’s contacts. This means that when you participate in group chats, message people 1-1, and make Signal calls, your phone number won’t show up unless the person has it saved (you can also limit this further, as detailed below). Your Signal profile name and photo will continue to be visible. If you’d still like everyone to see your phone number when messaging them, you can change the default by going to Settings > Privacy > Phone Number > Who can see my number. You can either choose to have your phone number visible to “Everyone” you message on Signal or “Nobody.” If you select “Nobody,” the only people who will see your phone number in Signal are people who already have it saved to their phone’s contacts. Changing your phone number privacy settings by going to Settings > Privacy > Phone number will change whether they see your phone number in your profile. We’re also introducing a setting that lets you control who can find you by your phone number on Signal. Up until today, anyone who had your phone number–from a party flier, a business card, or somewhere else–could look you up on Signal by phone number and message you. You can now restrict this by going to Settings > Privacy > Phone Number > Who can find me by my number and setting it to “Nobody.” Selecting “Everybody” means that anyone who has your phone number can type it into Signal and send you a message request (which you can accept, reject, or block). This is still the default setting, and is how Signal has worked for years. Changing your phone number privacy settings by going to Settings > Privacy > Phone number will change how people can connect with you on Signal. Selecting “Nobody” means that if someone enters your phone number on Signal, they will not be able to message or call you, or even see that you’re on Signal. And anyone you’re chatting with on Signal will not see your phone number as part of your Profile Details page – this is true even if your number is saved in their phone’s contacts. Keep in mind that selecting “Nobody” can make it harder for people to find you on Signal. If your friend downloads Signal and opens the app to see who they can message, they won’t know that they can message you. Instead, in order to connect on Signal you will need to share your full, unique username with them.2 You can change these settings at any time to best suit the ways you want to connect with others on Signal. Usernames: Another way to start a conversation Until now, someone needed to know your phone number to reach you on Signal. Now, you can connect on Signal without needing to hand out your phone number. (You will still need a phone number to register for Signal.) This is where usernames come in. Instead of giving out your phone number, you can now share a username. You can also generate a QR code or link that directs people to your username, letting them quickly connect with you on Signal. Generate a QR code or unique URL to connect on Signal without sharing your phone number by going to Profile > QR Code or Link. Usernames in Signal do not function like usernames on social media platforms. Signal usernames are not logins or handles that you’ll be known by on the app – they’re simply a quick way to connect without sharing a phone number. Your profile name remains whatever you set it to. Your username is not displayed on your Profile Details page, and people you message can’t see or find your username without your sharing it. Put another way, someone will need to know your exact unique username in order to start a chat with you on Signal. And Signal does not provide a searchable directory of usernames. To connect via username, type someone's exact username into the New Chat bar and send them a message. Once they accept your message request, you'll see their profile name in the chat. We have also worked to ensure that keeping your phone number private from the people you speak with doesn’t necessitate giving more personal information to Signal. Your username is not stored in plaintext, meaning that Signal cannot easily see or produce the usernames of given accounts.3 Usernames simply allow you to initiate a connection on Signal without sharing your phone number, and Signal’s robust privacy safeguards remain unchanged. Signal is built so that we do not know who you message, what you say, which group chats you participate in, who’s in your contact list, and more. Set it, share it, change it If you want to create a username, you can do so in Settings > Profile. A username on Signal (unlike a profile name) must be unique and must have two or more numbers at the end of it; a choice intended to help keep usernames egalitarian and minimize spoofing. Usernames can be changed as often as you like, and you can delete your username entirely if you prefer to no longer have one. To create a username, go to Settings > Profile. Once you’ve created a username, you can share it with others who can use it to connect with you. To connect with someone via their username, simply open the New Chat screen on Signal and type in their username. Since Signal does not provide a searchable directory of usernames, only people who have your exact unique username will be able to start a conversation with you. And you can share it with as few or as many people as you want. You can also share a QR code or unique URL that shortcuts to your username in Signal. You can reset these at any time without having to change your username, much like a group invite link. Usernames in Signal are designed to be easily changeable. For example, you can make a username to connect with people at a conference or to plan a group trip. Then, when it’s over, change it if you want to. Just click on your username from your Profile Details page to make the changes you want. When you change your username, your Signal contacts are not notified because your username is not visible to the people you are chatting with 1-1 or in groups. To Recap Starting soon, your phone number will no longer be visible to people you chat with on Signal, unless they have it in their phone’s contacts. You will also be able to configure a new privacy setting to limit who can find you by your phone number on Signal. And, you’ll now be able to create an optional username that you can share with the people you want to connect with on Signal. For more information, you can take a look at our support center articles. Currently these new features are in beta but will be rolling out to everyone in a few weeks. Thanks to Nina Berman, Jun Harada, Ehren Kret, Joshua Lund, Jim O’Leary, Alex Ristevski, and Meredith Whittaker for helping to author and edit this post. Each version of the Signal app expires after about 90 days, after which people on the older version will need to update to the latest version of Signal. This means that in about 90 days, your phone number privacy settings will be honored by everyone using an official Signal app. ↩ In order to authenticate that whoever you’re chatting with on Signal is the person they’re claiming to be, we encourage you to use tools like comparing safety numbers (ideally in person), or even checking in with them via another channel (DMs, a phone call, or similar). ↩ Usernames in Signal are protected using a custom Ristretto 25519 hashing algorithm and zero-knowledge proofs. Signal can’t easily see or produce the username if given the phone number of a Signal account. Note that if provided with the plaintext of a username known to be in use, Signal can connect that username to the Signal account that the username is currently associated with. However, once a username has been changed or deleted, it can no longer be associated with a Signal account. ↩ Tweet Facebook Don't have Signal? Give it a try!",
    "commentLink": "https://news.ycombinator.com/item?id=39444500",
    "commentBody": "Keep your phone number private with Signal usernames (signal.org)1183 points by Josely 15 hours agohidepastfavorite704 comments godelski 14 hours agoThis is fantastic! I also love that there is the QR code generator. It'll make connecting easier. I hope moving forward we can have multiple usernames and profiles. This would greatly increase privacy since we may have different identities in different social groups. Even on HN a lot of us have multiple personas. I find one of the big challenges is actually handling these different identities as most software only assumes you have one. Though it seems to be common on social media like twitter or instagram. But bitwarden still doesn't know how to differentiate microsoft logins lol Edit: I'd love in the future to also see things like self destructing or one time links. I don't think these should be hard to implement, especially if one can have multiple usernames. Certainly a limit like 3 would be fine with the numbers, right? Personally I wouldn't be upset if multiple names became a premium feature but I'd strongly prefer if it wasn't. I get that signal still needs money (https://news.ycombinator.com/item?id=39446053) reply LtWorf 12 hours agoparentTelegram has had all of these features for a while… too bad it isn't as secure as signal or it'd be perfect, since it's also written in a real GUI toolkit and present in distribution repositories. I do wonder how telegram and signal are planning to finance it long term. Telegram is adding absurd paid features like exclusive animations, which won't earn nearly enough to cover the costs. I wonder where signal is about keeping the servers up, since they hate federation so much. reply vld_chk 10 hours agorootparentTelegram and Signal solves very different types of privacy issues. Telegram is good, as you mention, to be relatively private in groups/chats/channels without a need to expose neither your phone nor even a nickname (unless you live in autocratic countries — will come to this later). But it comes with costs. First, their p2p communication is not e2e encrypted by default. Not to say that all comments/group chats are not encrypted too, unlike let’s say WA. Second, Telegram API. It gives too much information. You can do a lot with it: read history, track changes of usernames, etc. For example, it is quite easy to obtain an internal user ID and there are black market services and databases where they promise to connect that ID with phone number if that account ever had privacy settings switched off in the past. Claimed that they kind of scrape all accounts and pair ID for those where privacy settings set poorly. Even if you change it later — your internal ID and that scrape will state forever. Third, Telegram was funded by Russian government since Durov had issues with SEC. He raised money from different Russian state-owned banks like VTB, issued bonds which are traded in Saint-Petersburg stock exchange, and even take some money directly from Russian government though a Qatar proxy-company. Not to say, that there are cases when TG was involved in criminal charges against people (the most famous one is story with Ryanair plane being forced to land in Minsk to arrest Lukashenko’s critique) and it was never directly addressed and explained by company how exactly those people was caught and how company protect against “SIM card replacement” cases (Signal at least inform me everytime my peer logged to new device). Selecting between Signal with AFAIK no known cases of charges in dictatorship countries like Russia, funded by non-profitable charity, and TG without default e2e encryption, public API and Russian-state funding, is quite obvious for me. reply grayfaced 10 hours agorootparentIt was also banned and blocked in Russia for several years. It was only unbanned when they agreed to cooperate with security services. https://en.wikipedia.org/wiki/Blocking_of_Telegram_in_Russia reply vld_chk 9 hours agorootparentMore to this “lucky coincidence” it was unbanned exactly when Durov failed in trouble with SEC and raised Russian-state money to solve his problems. Around same time almost all official Russian institutions open TG accounts and Russian Parliament (if we can call that silly thing like this) representatives was saying like “we solved all problems with them”. When war started, and Russia banned a lot of services like FB, they created list of communication platforms they have questions about loyalty and cooperation with Russian government. TG was not on that list and through the whole war the only issue was about Telegraph — supplementary platform to publish long notes. AFAIK there was 0 questions or criticisms to TG in those 2 years. As for me, it says a lot reply pcchristie 9 hours agorootparentprevI didn't know a lot of this. I thought Telegram was mostly funded through Durov's Bitcoin and VK money? It feels strange that he'd be so \"in bed\" with the Russian govt when the whole reason he left was because of his staunch opposition to taking down Navalny's VK page. But I haven't done extensive reading on this. reply vld_chk 8 hours agorootparentDurov was indeed an opposition to Russian govt for some time and TG was banned in Russia for some time. But then “SEC-incident” happened. He and his brother wanted to build TON and fund it by kind of ICO (without naming it ICO). SEC decides enough is enough and blocked launch of TON with charging Durov for selling unregistered securities. At the end, issue was settled, Durov returned all money and settle the deal with SEC, but it shrinks his finance by a lot and he ran out of money for TG. Then he was seen in Russia and issued bonds for $1 bln. According to Russian financial press [1], bonds were underwritten by Russian banks closely affiliated with government or directly stated-owned (all of them are in sanctions list now), and even some money was invested by Russian Fund of Direct Investments [2]. Last summer he again issued bonds for TG for $270 mln. You can buy TG bonds at SPB stock exchange where they were listed 2 weeks after the issuing [3]. Surprisingly (repeating my comment below), around same time, Russian govt withdrew all their claims to Telegram and started to use as the official communication channel. Not to say that other “transformations” happened like Duriv publicly denounce US declaring it is a “police state” [4] All links in Russian, sorry: [1] https://www.rbc.ru/finances/15/03/2021/604f11019a79478034130... [2] https://www.bbc.com/russian/news-56501991.amp [3] https://www.forbes.ru/finansy-i-investicii/424665-shirokiy-k... [4] https://te.legra.ph/7-prichin-ne-pereezzhat-v-Kremnievuyu-do... reply Lockal 3 hours agorootparentprevDurov personally blocked Navalny channels in Telegram during 2021 elections - https://www.rferl.org/a/telegram-navalny-smart-voting/314662... even though \"technically\" as a foreign legal entity they had no obligation to follow orders of Russian censorship agencies. Also, if you look up the results of court decisions in Russia, Telegram leads by a significant margin among other messengers. Yes, of course, it is the most popular messenger in Russia, but it is designed from the ground up to tie and control the circle of communication to specific people as precisely as possible. reply psuresh 3 hours agorootparentprevDictatorship exists in varous forms. Russia has democracy though in bad shape. There various flavours of democracy. But what about total dictatorship in China has no opposition and many countries with theocratic monarchy. reply vertis 45 minutes agorootparentIt's really easy to tell the difference between a democracy and a fake democracy. Democracies are messy, people never agree. Anywhere that get's consistent landslides for one person or party is not a democracy. Take for example France vs Russia. In the 2022 election, Macron managed to get just ~30% of the voters that wanted him as President. In the second round where only two options remained, only 58%. Without any serious opposition (with the murder of Boris Nemtsov and jailing/deregistration of Alexei Navalny), the 2018 was again a landslide for Putin with 76.69% of the vote. There are of course other easy ways to tell, but this serves as a pretty easy heuristic. This is, of course, a gross simplification, of everything that makes up a democracy. For example, the US is at best a flawed democracy because of all the lobbying, money and gerrymandering (and things like the Electoral College). Disclaimer: Not American, I'm a Kiwi, so outsiders view of US politics. reply walteweiss 2 hours agorootparentprevBullshit. Russia has no democracy, even in the minds of its citizens, not to say in the government. It never had and it may never have democracy. At least, until Russia exists in its current shape of form. My bet is that they have a chance for democracy only when Russia becomes a set of little independent states. As Russia in a nutshell, is just a Muscovy that occupied other sovereign states. It was exactly like they’re trying it with Ukraine currently, again. Again, as the previous one was in 1918, when Russia ‘incorporated’ other states, what we know as ussr. reply Nuzzerino 12 hours agorootparentprevDon’t worry, telegram is now gatekeeping certain privacy settings behind the premium subscription like it’s 2003. They also make it difficult to hide your pseudo identity from your phone contacts. I’ve had all the “discover contacts” settings turned off, and simply reinstalling the app caused people to be given my username without my consent. Settings somehow magically switched themselves back on and I couldn’t turn them off until after the damage was done. There was no confirmation prompt. Pretty sure this happened to me more than once. Please don’t ever compare Telegram with Signal. reply hanniabu 11 hours agorootparent> telegram is now gatekeeping certain privacy settings behind the premium subscription Such as? reply vld_chk 10 hours agorootparentFor example, now you can’t restrict who can send you a message unless you have a premium. Also they added a “feature” that premium users can bypass non-premium users privacy setting “last seen and online” and TG will tell that info regardless of your choice unless you are premium too. reply flexagoon 10 hours agorootparentYou're significantly misunderstanding the changes. > now you can’t restrict who can send you a message unless you have a premium. And before that you just weren't able to restrict that at all, there was no such feature. They didn't remove this feature for free users - it never existed. They just added it right now only for paid users. > premium users can bypass non-premium users privacy setting “last seen and online” That is absolutely not what the feature is. If you hide YOUR OWN last seen time, you won't be able to see last seen time of other users, even when they have it public. Now, premium users will be able to see public last seen times of other people if they hide their own. But they obviously still can't see last seen time of people who set it to private, that would've been very dumb. reply vld_chk 9 hours agorootparentThanks for the clarification on last seen, I certainly misread it. About messages: hm, I was sure it existed before but maybe again my brain just lags. As someone who for some time created and moderated fairly popular chat (200+ people) for anti-war Russians, I have very long and complicated history of relationship with this service and have a lot of different grey-zone stories where it is hard to understand whether it is a mistake from users and whether it is a leak from the service. Hence I have a little low expectation and overreact on their recent changes reply flexagoon 1 hour agorootparentI have three Telegram channels with a few hundred subscribers each, and I also use the service daily, as I'm Russian as well. I generally agree with you that Durov makes a lot of incredibly stupid decisions. I think pretty much everyone in the \"Telegram community\" (eg. channel administrators, bot/client developers, etc.) would agree that the changes Telegram is introducing are often bad. The issue, though, is that there isn't any alternative right now - Telegram is the best messenger out there in terms of general usage. So while I do hate what they're doing sometimes, I still use the product and even pay for Telegram Premium. It's bad enough to be mildly annoying, but not bar enough to actually make people leave the platform. Edit: just as I was writing this, Telegram introduced a new feature. I'm not sure if I love it or hate it to be honest, it's a smart way for them to save money, but it is pretty weird: https://t.me/tginfo/3942 reply Lockal 3 hours agorootparentprevIf you consider Telegram as a product to be a logical continuation of the VK message system, then all of these \"features\" existed. Restricting of incoming messages existed (cloned from Facebook as usual). Restricting of \"last seen and online\" existed in third-party clients. Later on VK started to actively destroy this functionality, by moving manual \"is online\" management from designated API into all data-fetching APIs. Not to mention that VK and Telegram are now actively fighting with third-party clients. In which world they would not fight Ninjagram/AyuGram/Plus Messenger/other forks, which allow to add multiple accounts, hide online/reading (to some extent), show message editing history and so on? reply stavros 7 minutes agorootparentIf you consider technology to be a logical continuation of earlier technology, then all features existed. Systemling0815 10 hours agorootparentprevLast online status reply kome 11 hours agorootparentprevi've been using Telegram on and off since 2015 or so, and i've never shared my contacts. never! re-installing Telegram has never changed that setting. The real problem with cellphones is that a lot of privacy-threatening issues are literally one fat finger away. And clearly, that's a feature, not a bug. That's why I prefer to work and message on my laptop anyway. but again, Telegram has been, in many practical ways, much more privacy-oriented than all the other messengers, exactly because you don't have to share your phone number to participate in groups and chats. reply LtWorf 11 hours agorootparentprevCome on signal until today had no way to keep the phone number private. Which is the topic here. reply fidelramos 11 hours agorootparentBecause unlike Telegram they strive to do things in a privacy-respecting way, and that's hard to get right. reply oli-g 9 hours agorootparentprevI don't get why people who are so paranoid about someone associating their Telegram handle with their phone number simply don't go and grab a burner SIM at Tesco. I mean I'm all down with the idea of tech companies respecting our privacy. But here we are, complaining that corporations that are at least trying (and that are operating at a loss since their conception for our convenience) aren't giving us \"Snowden hiding in Russia\" level of security out of the box, for free, just because we deserve it. All while we could easily implement it ourselves for like $8 and with no online trace whatsoever. It's like, Tails Linux exists, but FUCK GOOGLE for forcing me to Ctrl+Shift+Delete in Chrome if I want to erase a cookie. I'm so significant and certainly not a criminal, why do they hate me so much?? reply guhcampos 8 hours agorootparentIt's not always that simple. In many countries, like Brazil, you need a valid ID document to buy a SIM card, and the number is then and always linked to your government ID. This is the case for quite a few relatively free countries as a means to fraud prevention (not that it's particularly effective though). reply kojoru 4 hours agorootparentSpecifically for telegram there's a (rather expensive) crypto-based no-sim option: https://telegram.org/blog/ultimate-privacy-topics-2-0/ru?set... reply palmfacehn 2 hours agorootparentprevI've tried 4 different sim cards in telegram. None of them seem to work. Not sure why a \"privacy\" app is asking for a phone number in the first place. reply Nuzzerino 5 hours agorootparentprevAh, the good ol “just get a burner sim bro” argument. Tried that once, they did KYC. reply mtnGoat 5 hours agorootparentI hadn’t used a burner in years, last year my phone broke on a trip and I just wanted to grab a phone, to get me through the week. I can say it’s not like it used to be! Can’t just grab one at the gas station and pop it in a phone. Gotta give ID, sign up for accounts, etc. reply prmoustache 1 hour agorootparentIt depends of the country. You can buy a sim card at an Oxxo in Mexico like you would buy a bag of doritos. I did it precisely last year. Having said that if you leave the country I am pretty sure that sim card and number would be deactivated after a few months if not connected. I am not sure how fast a number can be reused. reply snotrockets 10 hours agorootparentprevTelegram isn't a messaging service. It's a social network with a messenger UI. Quite ingenious, if you'd ask me, but a social network and a private messenger can't really be reconciled into a single product. reply AnonHP 5 hours agorootparentWhat would you classify Signal as, with its stickers, cryptocurrency (MobileCoin), etc.? reply hunter2_ 1 hour agorootparentI think \"social\" in this context refers to frictionless friend finding, not stickers. Good privacy involves a certain level of friction, with PGP verification being a classic example of the UX problem space. reply contact9879 12 hours agorootparentprevYou're in luck because Signal had a whole blog post about long term financing a couple months ago. https://signal.org/blog/signal-is-expensive/ reply nicce 11 hours agorootparentGood reminder that need to make a new donation. reply brewdad 9 hours agorootparentI kick in $5 a month because that's about what I figure self-hosting a messaging service would cost me. I don't want the hassle of self-hosting and I trust Signal more than the other remote hosted options. reply Faizan711 4 hours agorootparentprevWhy do you say that Telegram isn't as secure as signal? reply jknutson 4 hours agorootparentI’m not who you replied to, but I agree with his sentiment about signal being superior to telegram in terms of security (or more specifically, privacy). For me, there’s two big reasons for this: Signal chats are E2E at all times, while Telegram is only E2E when you explicitly create a “secret chat” with whoever you’re conversing with. I don’t fault Telegram too much for this, because they still provide the option to use E2E for everything, but Signal gets brownie points in my book because they just do it by default without getting in the way of the User. Secondly, as far as I know, Telegram uses their own in house encryption techniques as opposed to industry standards. I am not at all knowledgeable about encryption or cryptography— I only know what’s required of me in my job (basically the bare minimum), and so I don’t actually know whether this is anything of serious concern. It could very well be that Telegram’s encryption techniques are just as effective as the established norms, but I do see the general consensus trending towards “roll your own encryption = bad, use established norms = good”, which is primarily what I am basing my opinion on here. To further detract from my own point, it actually seems like Telegram might be using “established norms” for encryption nowadays anyways [1], although I couldn’t really tell from the brief description I read on Wikipedia. Overall, I think Telegram is perceived as being less secure than Signal primarily because of the reputation Telegram has for implementing their own in house encryption techniques, even if they don’t use those techniques anymore— their name has become associated with their known history of using ad hoc encryption. [1]: https://en.m.wikipedia.org/wiki/Telegram_(software)#Architec... reply LtWorf 1 hour agorootparentprevChats are not e2e encrypted by default, they are just encrypted in transit. However this allows chats to be synced across many devices, so it is very very convenient. Telegram has e2e encrypted chats but only on mobile and not on desktop for some reason. reply fsflover 1 hour agorootparent> However this allows chats to be synced across many devices I use Matrix with e2e encryption, and my chats are synced just fine. reply vel0city 12 hours agoparentprev> But bitwarden still doesn't know how to differentiate microsoft logins To be fair to Bitwarden even Microsoft doesn't know how to differentiate between multiple Microsoft logins. As of at least a year ago, you can technically have different logins with the same username/email identifier, and different login prompts will behave differently. reply smingo 33 minutes agorootparentindeed, with an incoming Teams meeting invite, it should be determinable from the sender's context which account should work on the meeting. Instead there is 2 minutes of waiting, and what seems like pot luck with the account. reply folmar 9 hours agorootparentprevAlso nice to mention that some of those are connected and some are not. For example I have a personal account (that I did not create but appeared magically at some point; it behaves as totally separate), a work account (main work tenant) and three guest work tenants that share the password, but don't share the 2fa. For some apps you chose the tenant, but not for all. reply godelski 11 hours agorootparentprevOh yeah it was more a joke than anything. Microsoft is just creating such a shitty environment. I can be logging in from my company portal where they know the identifier yet I still have to add @company.com. I mean I got one for my job, for my university, for conferences (CMT), and I swear I'm forgetting 30 others that I only use once in a blue moon. They also are real shady with yubikeys. You can't set them as default but you can set \"security key.\" So the process ends up being it assuming you want to use Hello (which breaks my Outlook... wtf), clicking use another device, security key, clicking next, then finally typing in your credentials. The next part makes me real suspicious since all the other dialogues go to the next page without clicking next. Why just this page? It's some weird dark pattern bs. I'd call it malicious, but I think maliciousness requires intent. A chicken running around with its head cut off isn't really malicious if it runs into you. reply mtnGoat 5 hours agorootparentprevYou can use these “features” to hijack accounts too ;) I’d call them bugs, but they’ve been reported and didn’t get fixed. reply Geisterde 9 hours agoparentprevMatrix might interest you, but it doesnt solve telephone numbers (i think) reply godelski 8 hours agorootparentI don't want to be too dismissive of Matrix, but I also see these types of comments as understanding what problem Signal is actually addressing: security for the masses. There's no way I'm getting my grandma on Matrix and you're delusional if you think she can setup a server. But it isn't hard to get my grandma on Signal and that's a much better security feature than federation or even not having phone numbers. If I want extreme security, you're right that there are better tools. But my threat model isn't trying to avoid nation state actors, it's mostly about avoiding mass surveillance, surveillance capitalism, and probably most importantly: sending a message to the gov to fuck off with all this spying. At the end of the day, there's no other app that's even close to fulfilling those needs. I didn't realize my comment rose to the top. When I had written this I had also written this comment[0] which was the grandchild of the top comment at the time. It has a bit more details on my thoughts/reservations of federation. tldr is mostly about avoiding centralization. This remains an open problem and I think it is far too easily dismissed. But federation isn't solving the problems people want it to if it's federated like email and web browsers. That's just mostly centralization with all the headaches of federation. And to anyone complaining about lack of federation, what's stopping you from running your own Signal server? Sure, it won't connect to the official channel, but is that a roadblock? Even Matrix started with one server. This is a serious question, is there something preventing this? Because if the major problem with Signal is lack of federation, I don't see why this is not solvable building off of Signal and not needing to create a completely different program. Who knows, if it becomes successful why wouldn't Signal allow a bridge or why can't apps like Molly allow access to both the official and federated networks? [0] https://news.ycombinator.com/item?id=39446183 reply Geisterde 7 hours agorootparentOh, I agree completely with everything in the top paragraph, and I certainly have seen a natural trend towards central nodes/relays in all the federated networks I can think of. I think the appeal is that for the average user its about as good security as anything else available, and it has the option to work off the centralized network. reply BlueTemplar 2 hours agorootparentprev> There's no way I'm getting my grandma on Matrix Why ? Have you tried ? reply staunton 1 hour agorootparentIndeed, my grandma is on Martix (I did help her set it up though) reply tcmb 14 hours agoprevI like the idea, but they should have called it something else instead of ‚usename‘. Maybe ‚connection string‘ or ‚discovery phrase‘. Right now they have to explain at length in what ways it’s different from regular usernames. reply nsxwolf 11 hours agoparentIs ,comma-backtick` some personal quirk of yours, or is it some standard I'm not aware of? reply lock-the-spock 31 minutes agorootparentTo give a definite answer to the discussion below - it seems Czech, Slovak, German, Slovenian and Croatian sometimes use this format. Here an authoritative source: the EU publications office: https://op.europa.eu/en/web/eu-vocabularies/formex/physical-... reply loeber 11 hours agorootparentprevEuropean quotation marks commonly have the left one down low and the right one up high. The same applies for single quotes. But using comma-backtick is deeply unorthodox. reply sph 10 hours agorootparentGermany != Europe. The French use « », Italians use ‘regular’ “quotes”, etc. Strangely enough, this is the first time I see your style of quote, in two decades on the Internet. reply lock-the-spock 30 minutes agorootparenthttps://op.europa.eu/en/web/eu-vocabularies/formex/physical-... reply replwoacause 5 hours agorootparentprevYeah I’m surprised at how rare this is to see. I guess that means all Germans don’t follow this convention? reply illiac786 2 hours agorootparentI believe it should be double, „like this“, not single quotes. reply fredoliveira 11 hours agorootparentprev> European quotation marks commonly have the left one down low and the right one up high Wouldn't say it's \"common\", because IIRC that's only the case in Germany and Austria. reply pitkali 10 hours agorootparentAlso in Polish, actually. reply Jenda_ 10 hours agorootparentAnd some others. https://en.wikipedia.org/wiki/Quotation_mark#Summary_table reply replwoacause 5 hours agorootparentprevInterestingly, the author does not follow this convention on his personal site (first link in profile) … instead option for the ‘single quote’ form instead. reply stavros 11 hours agorootparentprevIt's ‚comma-apostrophe‘, actually. reply godelski 11 hours agorootparent,comma-apostrophe'? Only place I've see the backtick used for apostrophe is latex. And even then half the people don't know about it. reply stavros 11 hours agorootparentSure, but there's no backtick in the GP's comment. Only an apostrophe. reply godelski 11 hours agorootparentWait what? I see ,comma-backtick` whereas I wrote ,comma-apostrophe' I copy pasted both btw. You see them both as '? I see GP as having ` and me having ' https://en.wikipedia.org/wiki/Backtick reply stavros 11 hours agorootparenttcmb used ‚comma-apostrophe‘. nsxwolf asked \"Is ,comma-backtick` some personal quirk of yours, or is it some standard I'm not aware of?\" I'm pointing out that nsxwolf was wrong to ask about comma-backtick, because tcmb used comma-apostrophe. reply roryokane 10 hours agorootparentBoth are wrong. tcmb didn’t use ‚comma-apostrophe’ – they opened with , U+201A SINGLE LOW-9 QUOTATION MARK (not U+002C COMMA) and closed with ‘ U+2018 LEFT SINGLE QUOTATION MARK (otherwise known as an open single quotation mark). This matches the German convention described on https://en.wikipedia.org/wiki/Quotation_mark#German. reply godelski 10 hours agorootparentSorry I was quoting nsxwolf. But now that you point it out, I can see the difference. It's subtle so I'll copy paste so others can see. tcmb: ‚usename‘ nsxwolf: ,comma-backtick` stavros: ‚comma-apostrophe‘ godelski: ,comma-apostrophe' Though while copy pasting I see tcmb and stavros as having the same character which is different from the longer character you pasted. Seems my clipboard doesn't like that character. I also seem to have crashed OSX's emoji and symbol tray. No longer pops up if I press the button (bottom left) or select from firefox but got it back by opening safari. Fuck man, I do not envy you people working on ligatures. Or timezones. I'm always impressed by these random rabbitholes and complexities in things that always look very simple. It's beautiful in a weird way. reply crazygringo 8 hours agorootparentWow this is like the most HN thread I've ever seen, I love it! It's almost like a punctuation version of \"Who's on first?\" Everybody's arguing, then finally all is revealed, and I learned a ton of stuff along the way about German quotation marks and the subtle difference between backticks and opening curly quotes, and low quotation marks and commas, in the Verdana font! (If this had been a serif font with actual curly quotes the differences would have been much more obvious...) reply tcmb 3 hours agorootparentprevIt‘s what my phone made out of two presses of the same (single quote) button. reply m12k 13 hours agoparentprev\"friendcode\" seems to be pretty standard in multiplayer video games reply b1n 11 hours agorootparentMaybe \"contactcode\" would be better in this situation, as it doesn't imply any specific relationship between participants. reply duxup 13 hours agorootparentprevYeah that seems to be the standard and very descriptive. reply weikju 11 hours agorootparentprevNot everyone I connect to on signal is a friend. same for e.g. journalists or government people who use Signal. reply samstave 12 hours agorootparentprevHellDivers 2 LFG rn is all about sharing Friendcodes... you can get a ton of them on discord or reddit... but then you end up haveing a \"friendcode\" cybermentally-distributed DNS system for them over time. Six degrees will still exist. (funny weird thing is that with HD2's server issues due too demand, one way to harvest this would be to create a fake LFG host game and have tons and tons of accounts bang against your HellDiver-Pot - and get whatever you can scrape from that? --- OK - I actually went down this hole the other daty... you look at the reddit thread on helldrivers for LFG - or the discord... So on reddit, you just put .json at end of thread - DL the entire thread as json, now you have reddit id, location, play style, etc, details AND their friendcode on HD2... but since they can individually generate random friend codes on any game/system that allows such... you have a breadcrump (with enough attention span to just correlate all the shared info between these friend codes and data received... still - even with random friend codes - six degrees is still available, easily.?? --- I deeply hope they do a Tech Talk on the post-mortem of this lauch success spiral - its fascinating.... But one thing I am really interested in, this is based on the Autodesk Engine, I know they co-dev-dog-fooded, but I hadnt really known of this engine at all... what little I do know, is that - its amazing... But I'd really like to know more about the arch and overall traffic flows etc of this game. Its beautiful see \"problems\" like this explode in like ~2 weeks. What do internet traffic graphs look like since growth, per carrier? reply solardev 12 hours agorootparentDoes it not have built-in public matchmaking? reply pfych 11 hours agorootparentThe developers last game had an all time peak of 7,000 users. They planned worst case scenario of 250,000 users for the sequel expecting more realistically 50,000 users. They're currently at 394,686 players on steam alone - not including Playstation players. The servers are doing their best right now. reply solardev 9 hours agorootparentSorry, I don't quite understand this in the context of \"friend code\" vs \"matchmaking\". Are you saying that friend codes bypass their servers, allowing peer-to-peer play even when the servers are overloaded (the way direct IP addresses used to do in old PC games)? I apologize for not asking a clearer question. I was actually just interested in buying the game, but only if it has public matchmaking built-in for finding anonymous pick-up groups, instead of needing an external Discord server to swap friend codes on. reply samstave 5 hours agorootparentFriendcode is basically a token: lets have a game - call me on this burner number. we have game. .>..x###.////3~~E`~,~X>>----- XXNXN x0x then I know that youre solardev.. and we can be friends in future (but this model is exploitable in ways, which is premise of many threads here) reply WolfeReader 13 hours agoparentprev\"Connection string\" already means something else. I'm partial to \"Identifier\", myself. reply msm_ 11 hours agorootparentBut identifier already means something else (i'm used to identifiers being unique, constant, and useful for actually identifying someone). reply WolfeReader 11 hours agorootparentGood point! The former C++ programmer in me wants to call them \"user pointers\" but that would just confuse people who haven't learned pointers. reply folmar 9 hours agoparentprevThere is old-now-unused \"nickname\". reply jamwil 6 hours agorootparentI like “handle”. It’s short and conveys some mutability. reply Vinnl 15 hours agoprev> Note that even once these features reach everyone, both you and the people you are chatting with on Signal will need to be using the most updated version of the app to take advantage of them. > Each version of the Signal app expires after about 90 days, after which people on the older version will need to update to the latest version of Signal. This means that in about 90 days, your phone number privacy settings will be honored by everyone using an official Signal app. Which is also an example of a challenge for open ecosystems where everyone can create apps. I understand that it doesn't outweigh the benefits to everyone, but it is a valid reason. reply smt88 14 hours agoparentIs Signal considered to be (or attempting to be) an open ecosystem? My understanding is that Signal (the app) is private, not anonymous, centralized, and closed. The underlying protocol is open and could be used for an open ecosystem, but I didn't think Signal aspired to do that. reply lima 14 hours agorootparentThe apps and most of the backend are open source too, not just the protocol. The important distinction is that it's not decentralized like XMPP or email, which is a conscious decision: it would become very difficult to change it to add new features and they'd be left behind by closed-source competitors (see: XMPP). reply ezst 11 hours agorootparentI see that it is a ton of wishful thinking and FUD on the side of Signal to claim that: XMPP is alive and kicking, has all the features one needs, runs everywhere, at scale, offers the same or better crypto, better privacy, better resilience and is more sustainable. When Signal will inevitably fail/turn against its users/enshittify itself or get acquired, all federated and P2P protocols will keep on going. For decades. That's the kind of communications systems we should be demanding in the present era, nothing less. reply kiwijamo 11 hours agorootparentYet I'd wager most HN readers have a grand total of zero XMPP contacts. Myself included. Proving the GPs point. reply AJ007 9 hours agorootparentBecause of what Google did with Google Talk. https://ploum.net/2023-06-23-how-to-kill-decentralised-netwo... XMPP is underrated. A lot of people are imagining Pidgen in 2011, but the protocol has been extended, the actively developed clients are good, and it avoids the heavier parts of Matrix (both client and server side.) I wouldn't be surprised if Slack's replacement when Salesforce inevitably fucks it up will be XMPP based rather than Matrix. reply smt88 7 hours agorootparentSlack's replacement is going to be Teams. No corporation chooses internal chat clients based on interoperability or openness of source code. reply zaik 4 hours agorootparentprevI kicked out all the walled-garden apps like Signal and went standard XMPP only. I have a lot of XMPP contacts now. You just need to commit to it. reply cortesoft 4 hours agorootparentAnd have friends who are all willing to commit to it, too reply zaik 4 hours agorootparentNot really, my friends are still using proprietary apps besides their XMPP client. reply kaanyalova 14 hours agorootparentprevBoth the app and the server is open source https://github.com/signalapp/Signal-Android https://github.com/signalapp/Signal-Server There are forks like Session which doesn't require a phone number to sign up https://github.com/oxen-io/session-android reply smt88 11 hours agorootparentI understand this, but Signal doesn't attempt to tolerate third-party apps on their servers as far as I know. They don't support interoperability. reply godelski 11 hours agorootparentYou can run Signal app forks on the Signal server. Molly is a popular one. You just can't create new servers. I wish you could, but I get the reasoning of not wanting honeypots. But that doesn't stop you from running your own network of Signal servers. So I don't see anything stopping anyone. I mean Mullvad runs their own stuff and I don't see half the complaints about them. I've always been curious why Signal is so unique here. If 1/100th the people that made these concerns developed a open community of signal servers, I'm sure we'd have a viable alternative network. What's stopping everyone? reply AJ007 9 hours agorootparentOne of the big lessons from Twitter and Reddit was third party apps are tolerated or even encouraged until they are not. Unlike, for example Discord, I haven't see any indication that third party clients are causing account bans, yet. The status of open source, privacy respecting messaging apps looks really healthy to me, compared to where we've been over the past 30+ years (thinking starting with ICQ.) Signal was a big leap toward getting average people using much more secure messaging, although it is pretty clear even most 'tech' people don't grasp what is going on or why it is important to be able to use e2ee separate from a combined client+server provider. reply godelski 9 hours agorootparentYes, but my argument is more in the realms of \"why are there no projects to create an open network using the existing architecture\" not \"we shouldn't have an open network and completely rely on Signal forever.\" reply Vinnl 14 hours agorootparentprevThey've described what they're attempting to be here: https://signal.org/blog/the-ecosystem-is-moving/ reply rstuart4133 11 hours agorootparentMoxie's post looks solid, but there is a counter example: bitcoin nodes. They are a very loose federation of nodes that go through regular upgrades in the protocol. So it is possible. But yes, it's also very hard. The bitcoin protocol didn't start out that way. It took a lot of knocks and bruises to get to the point they could upgrade all the servers in the federation. Interestingly, the method bitcoin came up with allows protocol changes to fail, meaning the bulk of the federation never takes them up. Everyone gets a vote, and it only succeeds if the bulk of the federation upgrades. Perhaps from Moxie's point of view that's unacceptable, as it means he is no longer the dictator of the protocol. Nonetheless, it is possible to design a protocol so it can be upgraded relatively quickly. Even if you don't do add \"quick transition\" features to a protocol transitions can still haven. IPv6 will replace IPv4. But as Moxie says, it's painfully slow. reply greyface- 12 hours agorootparentprevThe author is no longer CEO, though, and there are a lot of \"I\" statements in the post. Is it still accurate? Has the current CEO made any comment on it? reply sdenton4 11 hours agorootparentIt's a great encapsulation of why Signal is not federated, and, unless you find the current CEO stating otherwise, is unlikely to change. Changes like the one detailed in the link simply wouldn't be possible to roll out efficiently in a federated ecosystem. Signal has consistently focused on helping /most/ users do what they want with the app without sacrificing security. This change - away from requiring phone numbers - helps plug one of the biggest criticisms, both on the security and product side. Nothing about their mission requires federation, so I respect that they haven't sacrificed their mission in order to do it. reply fsflover 11 hours agorootparentprevMatrix debunked these arguments: https://matrix.org/blog/2020/01/02/on-privacy-versus-freedom... reply vlovich123 15 hours agoparentprevProtocol ratcheting, but 90 days would be quick if there’s a lot of apps. reply unethical_ban 15 hours agoparentprevI wish it were more obvious that Signal expires its apps every 90 days. My mom couldn't receive signal calls on the backup phone I gave her. I had disabled auto-updates since apps break UI sometimes and she gets confused by things moving around. When I visited, I opened the signal app and was told I had to update. reply gnicholas 11 hours agorootparentI have been bitten by this in the past. At least now they give warnings in-app that the app will expire soon. But if you don't use the app regularly, you wouldn't even know. Also, I'm not aware of any other apps that die in this way, so it's not like people are in the habit of periodically checking the app to make sure they're still on a version that can receive incoming messages. reply int_19h 3 hours agorootparentThis has more sinister implications in some places. For example, Apple app store in Russia can get banned at any time. So if I understand this correctly, if that happens, Signal will stop working for all iPhone owners in Russia in 6 months. And guess where you really need something like Signal? reply KennyBlanken 14 hours agorootparentprevIt's patently unforgivable that a message would not be delivered because the client is out of date. The Signal team is incredibly clueless and arrogant toward its userbase. It seems to simply not have occurred to them that many people rarely/never have wifi, may not be on AC power when they are on wifi which means the phone may not check for / apply updates, etc. In the US, cellular is often expensive and slow. In underdeveloped countries where software like Signal could be really important, all this is even more true. We get shit crammed down our throats to protect the most obscure edge cases for the smallest percentage of the most vulnerable users - such as not being able to sync messages between devices - but then they pull shit like this which has a huge impact for people in rural areas and underdeveloped countries? reply __MatrixMan__ 14 hours agorootparentDelivering a message to a client which is known to be less secure than the sender expected it to be is unforgivable. Refusing to deliver is inconvenient. reply jjav 12 hours agorootparent> Delivering a message to a client which is known to be less secure than the sender expected it to be is unforgivable. That is inconsistent with the threat model of a messaging system! Inherently, a messaging system will deliver a plaintext copy of the message to the recipient(s). Wouldn't be much of a messaging system otherwise. Once you sent something and it was delivered in plaintext to the recipient, the information disclosure risk is completely out of your control (and out of control of the application in use). The recipient is free to leak it however they wish. If you don't trust the recipient to keep it private, don't send it. reply eszed 12 hours agorootparentJust curious, since I'm not really active in this space, but wouldn't the threat model of most concern be that an external actor breaks (maybe an outdated version of) the app or protocol? This would leak data without you or the recipient being any the wiser. It seems like that's the threat the app-expiry policy is intended to address. reply jjav 12 hours agorootparentYou could update the protocol version if and when a protocol weakness is discovered and then stop talking the previous protocol version after a transition period. No need to continuously expire apps in the absence of a protocol breach. reply eszed 2 hours agorootparentWhat if there's a vulnerability in the app itself? I have no idea if that's what they're concerned about - they may just be being arseholes in this case - but from the outside it seems like a legit reason to build in the capability for app expiration. reply __MatrixMan__ 10 hours agorootparentprevBut you don't know, at the time of sending, which version of the client will show up to retrieve it. Otherwise both clients would need to be connected at the same time before you were allowed to send. reply joshuamorton 10 hours agorootparentprev> That is inconsistent with the threat model of a messaging system! I disagree, the worst thing that a messaging system that aims to be \"private\" can do is to actually not be private. Sending to a known-insecure client is a violation of, like, the one thing signal claims to do. > If you don't trust the recipient to keep it private, don't send it. My threat model is some combination of \"third party actors who I don't trust\" and \"second parties who I trust but who are non-experts\"[1]. I would like Signal to protect me from the first (by not delivering things to known-insecure clients that can be middlemanned or otherwise discovered) and the second, by having privacy-respecting and mistake-preventing defaults. Things like disappearing messages and such. Keeping my trusted-but-nonexpert peers from making mistakes that can harm either of us in the future is a key part of my threat model. For example, disappearing messages prevent me from being harmed by my friend, who I trust to discuss things with, not having a lockscreen password and getting warrented by the police. An outdated or third party client that lets you keep them forever, even if well intentioned, can break that aspect of the threat model. And yes, a peer who is actually nefarious can still do that, but that's not my threat model. I think my friends aren't privacy-experts, I don't think they're feds. [1]: This is, for example, the reason that I think PGP is not a good tool. Even if I do everything right, a well meaning peer who is using the PGP application can unintentionally leak my plaintext when they don't mean to, because of the tool's sharp edges. reply sunshowers 10 hours agorootparentprevI think this is the tradeoff that Signal makes versus the messenger most similar to it, WhatsApp. Though of course everyone in a group chat must pick one or the other, so it's not much of a free choice. (My friend group in the bay area is entirely on Signal, for example, though I also have a WhatsApp account.) reply vel0city 12 hours agorootparentprev> In the US, cellular is often expensive and slow. Mint will sell you a plan for 5GB of data for $15/mo. Its not that expensive to have a basic cellular plan. And that's assuming you're not poor enough to have your cellular plan almost entirely subsidized. And also assuming you're pretty much never anywhere with wifi. In the vast majority of markets in the US it'll take a minute or less to download, it'll probably take more time unpacking on your device and installing. reply ambichook 11 hours agorootparent5gb for $15USD/mo is expensive relative to other areas of the world. in aus, for example, my phone plan is $30AUD/mo for 55gb reply vel0city 9 hours agorootparentSure, but the thing I was responding to was \"in the US\". There's cheaper per-gig plans in the US. Visible has unlimited plans for $30/mo which is cheaper per-gig if you use a lot but more if you're using less than 5GB anyways. And if 200MB/yr currently seems like an expensive amount of data to you, you're probably already using less than 5GB a month. reply Klaus23 12 hours agorootparentprevWe are talking about 85 MB four times a year to keep the application up to date and running smoothly. Don't be ridiculous. reply hot_gril 15 hours agoparentprevHackers can always create apps. reply verandaguy 14 hours agorootparentThis is a common, but terrible argument. Anyone can (mis)use, make, or weaponise technology given enough time and funding. Following this reasoning to its logical extreme, nobody should ever do anything. The problem something like this solves is to raise the bar somewhat and discourage a fraction of those who would. Done right, that fraction will be significant. reply hot_gril 13 hours agorootparentIt's not a big expensive task to look at what data an app is sending/receiving. Anyone with minimal reverse-engineering skill will know how to intercept HTTPS to/from their own phone in 5 minutes. Signal uses some other protocol, but it's also doable, also it's open source anyway. The conclusion isn't that Signal should be closed-source, it's that Signal's servers should not trust the clients not to be tampered with. So after 90 days, they will remove phone numbers from the protocol for users who have hidden them, breaking old clients, which is fine. What is the alternative solution you're thinking of? reply saurik 6 hours agorootparentprevI mean, if WhatsApp said this about the privacy of messages, Signal would be running billboard ads about how they don't care about privacy and look at how much better Signal is, right? This is the company that goes out of their way to pile on advanced encryption and insists on using dangerous secure enclaves to get this kind of thing right... until they are asked the hide phone numbers, at which point they are selling people a false bill of goods that WILL confuse someone into giving their phone number to someone who they really shouldn't have. It isn't as if it is somehow impossible to hide anyone's number at the protocol level: hell... even Snapchat does this, right? reply LoganDark 15 hours agoparentprevDoes this mean the protocol still exposes your phone number and it's hidden only by the client side? reply varenc 15 hours agorootparentThe answer is almost certainly no. It means the old APIs that expose phone numbers will stop working in 90 days. And old clients along with them. I have not investigated this at all, but I have enough faith in Signal/Whisper Systems to be optimistic. reply hot_gril 15 hours agorootparentFound out the hard way that the old versions do stop working. You don't even get message notifications if your app is out of date. reply jcollins1991 15 hours agorootparentYup, I was on an international trip with hardly any data allowance when all of a sudden my messages stopped sending, and I couldn't receive any new ones... That'll never happen with SMS. I love Signal, but some of their product decisions have been questionable. reply hot_gril 14 hours agorootparentTheir decisions seem right for the use case of a secure messaging app, but I don't care about that use case and would rather use a non-e2ee app that'll be reliable, not lock me out, and work seamlessly across devices. Also, for those who truly care about e2ee, it's pointless if you aren't checking all the safety numbers out-of-band. reply freedomben 14 hours agorootparentYes, this is a compromise on the CIA triad. It prefers integrity and confidentiality over availability. That is a fine decision to make for a security-minded app, but signal has always presented themselves as a full alternative to SMS and other messaging systems where availability is prioritized over confidentiality and integrity. It should really be made more clear so that users are making an informed decision. They could also do wonders for the user experience by having the app inform the user of the problem and how to remedy it. reply hot_gril 13 hours agorootparentYeah, but I wouldn't call SMS super available either since it relies a lot on the ends too. Had a lot of those drop when I traveled. Something like Facebook Messenger has a whole server storing messages, so it's solid, you'll receive them later even if your phone breaks. reply londons_explore 15 hours agorootparentprevThe way they say \"privacy settings will be honored by everyone using an official Signal app.\" kinda suggests they're gonna let third parties keep getting this info... reply contact9879 12 hours agorootparentThey won't. It'll be similar to message timers or delete for everyone. You can revoke sharing your number and it will be hidden in official apps but third party apps won't magically forget the number that was previously shared. However if you choose not to share your number from the start, no one will be able to see your number. reply jenny91 15 hours agoprevI've been a Signal beta tester on iOS for as long as I remember, knowing that they were going to introduce usernames, and I wanted to get my (relatively common) name as my username. Now they finally introduced it, but they require it to end in at least 2 digits \"a choice intended to help keep usernames egalitarian and minimize spoofing\". Edit: this is not actually a serious problem for me, don't worry! Rather, I think it's funny. And honestly I kind of like having the numbers required, it's a good idea. It does remove a lot of the vanity from usernames. reply hnarn 14 hours agoparentIt’s an excellent design choice, it more or less completely eliminates “vanity names” and the “value” of shorter names. reply zestyping 6 hours agorootparentIt's a brilliant design choice. At first I was like \"What?\" and now the more I think about it, the more I realize it is an absolute genius move. People need to get trained out of (even informally) assuming they can identify someone because their username looks familiar, and this is a great way to do it. reply kelvie 14 hours agoparentprevAs you may already know, getting a commonly used username is also somewhat of a curse (do you like getting \"forgot your password\" emails every hour?) Or tons of (mistaken) conversation requests? reply Marsymars 14 hours agoparentprevUsernames are only used for the initial connection, so \"getting\" a username doesn't really gain you anything other than the \"username\" you give to people who don't already have you as a contact: \"a username is not the profile name that’s displayed in chats, it’s not a permanent handle, and not visible to the people you are chatting with in Signal\" reply baq 15 hours agoparentprevI’m politely putting it away into the not-a-problem drawer. reply stavros 15 hours agoparentprevWell, I got stavros.01, if anyone wants to chat. reply ggrelet 10 hours agorootparentCould have gotten stavr.05 reply stavros 10 hours agorootparentI thought of that, but it's much harder to say. \"stavr dot zero five\" is going to confuse people. reply canaus 15 hours agoparentprevI don’t think this is necessarily something to lose sleep over. reply password4321 12 hours agoparentprev> require it to end in at least 2 digits ... notes HN user jenny91 reply ThePowerOfFuet 13 hours agoparentprevAt least 8675309 ends in two digits! reply giantrobot 15 hours agoparentprevI can't wait to talk to elonmusk420! I'm sure it'll be the real Elon. His online antics are such anyone with that username will instantly trigger Poe's Law. Getting rid of phone numbers as identifiers is a good idea but I think it would be better to just assign user IDs or generate hashes based on user inputs or something. reply vel0city 14 hours agorootparent> generate hashes based on user inputs or something. Because friend codes were so popular on Nintendo. Hey add me real quick, my id is 12716472-83647281746-8172649! Or use the hash code, 0x28A56ED9! Super easy to remember, way better than giantrobot22 or vel0city66. reply KennyBlanken 13 hours agorootparentGiven nintendo's user base includes a LOT of children who are very young, the long codes may have been a feature, not a bug - the equivalent of a child latch - to slow down/discourage young users from adding people themselves so their parents have a better idea of who they are interacting with. reply JoshTriplett 11 hours agorootparentI expect it's more a combination of several factors: - if we don't have usernames we don't have to deal with obscene usernames, trademarked usernames, impersonation claims, and similar - if we don't have usernames and our generated friend codes aren't guessable, we don't have to worry about people getting random unexpected friend requests from people they don't know reply vel0city 12 hours agorootparentprevDon't get me wrong I get there were intentional reasons for it in regard to friend codes and I don't necessarily fully mind with that in mind in that use case. I do kind of wish there was an \"I'm 13/18+, let's take the training wheels off\" feature though. reply giantrobot 11 hours agorootparentprevThe issue there is \"veI0city66\". Depending on the font that capital \"I\" might look identical to a lower case \"l\". A hash with an alphabet that doesn't include homoglyphs would reduce ambiguity. There's also the \"weedlordbonerhitler69\" issue. A user name that seemed hilarious at 16 likely seems less hilarious at 26. If users were identified with a hash derived from an input user name you could type in \"weedlordbonerhitler69\" and what would be displayed is a hash on the client side. The contact add UI could simply return the UID for the input username. So you could give out the UID or username and another user could still add you. reply vel0city 9 hours agorootparent> The issue there is \"veI0city66\". Depending on the font that capital \"I\" might look identical to a lower case \"l\". A hash with an alphabet that doesn't include homoglyphs would reduce ambiguity. They're not going to get mixed up typing it in from me verbally telling me the name. They're not going to get confused typing it in. And even then, validate the user after, that's another feature of signal is in person/out of band validation of the ends. So start the convo the verify through a channel you otherwise trust. > There's also the \"weedlordbonerhitler69\" issue. A user name that seemed hilarious at 16 likely seems less hilarious at 26. And with their setup you can change it at any time, so once again not really an issue. reply v7p1Qbt1im 12 hours agoprevNice. Now please finally give us iOS cloud backups before i break or loose my phone and years of conversations get evaporated. reply JoshTriplett 11 hours agoparentI'd settle for full sync of chats between my own devices. If I can sync between my laptop and my phone, that's sufficient, since I already back up my laptop. reply harry8 8 hours agoparentprevCounterpoint: I don't want backups for IM. I don't want my counter-parties to have backups for e2e encrypted IM. I don't want IM to last. Why record every conversation on your permanent record? It's nuts. For me, having a searchable record of everything said defeats the whole purpose if IM and e2e encryption. I'm sure the NSA like it. Reasonable people may differ on it. reply Zuiii 7 hours agorootparent> I don't want my counter-parties to have backups for e2e encrypted IM. That's not your choice to make. reply harry8 6 hours agorootparentAre you happy with anyone you talk to secretly bugging the conversation and transcribing it? I don't want to be randomly assaulted on the street, also not my choice to make. Doesn't make it ok imho. reply int_19h 3 hours agorootparentThey can do that in any case if they want to, just by taking photos of their phone screen. reply worez 1 hour agorootparentprevwhat's stopping someone from just showing another person their phone screen with your messages? reply nikisweeting 3 hours agorootparentprevOk but I can already do it on desktop (and it's even easier on Android), it's only missing on iOS. So this point is kinda moot... The encryption key is in cleartext on desktop and the SQLite db is right next to it: ~/Library/Application Support/Signal/config.json reply infotainment 11 hours agoparentprevThe lack of any kind of backup/export for iOS is the main thing keeping me from recommending Signal. Sadly, from what I’ve seen in similar threads online, it seems the devs are opposed to backups in principle (they believe that chats should be ephemeral and backing up is antithetical to this). reply erichocean 11 hours agorootparent> The lack of any kind of backup/export for iOS is the main thing keeping me from recommending Signal. \"No one can read your chats, including you.\" — Signal reply jtriangle 6 hours agorootparentprevRun a windows VM, install signal desktop, bob's your auntie. reply infotainment 4 hours agorootparentThis is basically what I do, just replace Windows vs Mac. Still, I feel it's much more inconvenient than it really needs to be; the correct UX is a button press. reply gitaarik 5 hours agoparentprevWhy iOS cloud backup? Why not a universal backup way, OS / cloud vendor independent? reply simonklitj 12 hours agoparentprevJust happened to me a couple of months ago. Cannot agree with you more. reply laktak 10 hours agoparentprevYou may be able to install something like https://github.com/mollyim/mollyim-android in the EU ... eventually. reply zuhsetaqi 11 hours agoprevIf I understand correctly it’ll still not be possible to create an account without entering a phone number? For me this is a requirement to call a service a private service because in Germany at least every phone number is connected with a persons identity. To get a phone number you need to connect it to an identity using a identity card reply wraptile 6 hours agoparentHere in Thailand it's the same but phone numbers get recycled and expire very aggressively. I just got a new phone number and I can login to many platforms of some 20 year old guy who really likes pc gaming. Phone numbers should have NEVER became an ID. Incredibly hypocritical of Signal to claim \"privacy focus\" when the lowest layer of the system is literally the least secure identification method we have. reply 123yawaworht456 3 hours agorootparentsame in my country. I had two SIM cards dedicated to online crap - one for important stuff like banking, another for social media and such. both have expired after ≈ 3 months of inactivity, when my 2 week trip unexpectedly took 4 months. those SIM cards weren't physically inserted into my phone - I used to do that once a month to call someone and get billed a few cents so it would remain active, until that trip. there's no way to get those phone numbers back and it's been an enormous pain the dick. I hate this fucking system, but I hate the fact that fucking everything requires a phone number even more. reply crotchfire 4 hours agoparentprevin Germany at least every phone number is connected with a persons identity. To get a phone number you need to connect it to an identity using a identity card Personally, I am totally baffled by this. Due in large part to C3's positive influence, Germany is at the forefront of privacy issues and legislation on so many areas, except for this one, which ends up turning into a massive backdoor in the whole edifice. Okay, we can't ask for a copy of your identification card... we'll just use a telephone number or SIM code or something trivially tied back to your IMSI (like an app store account or IMEI) instead. Because of the absurd 2017 law, these are equivalent to your government ID card. I really don't understand why Germans put up with this while simultaneously pushing so hard for positive changes in every other aspect of online privacy. Especially when so many other developed Western countries do not tie SIM cards to identities: Netherlands, Denmark, Finland, Iceland, Ireland, US, UK, Canada, and many many others. It's like a giant `sudo gimme-your-identity` backdoor in all the other data collection protections. And nobody seems to care about closing the backdoor. reply moepstar 3 hours agorootparentIt wasn't always like this - the requirement to give your ID to get a SIM card, as you noted, was only introduced in 2017 (though it certainly feels way longer ago for me). Anyways - why does nobody care? Simple: most don't feel this being an issue. Some may even say that they \"don't have anything to hide\" and there goes the erosion of privacy, bit by bit - by the time someone notices \"ok, this may become a problem\" - it'll be too late :( reply junto 2 hours agorootparentprevOn the flip side, SMS fraud is almost nonexistent from German mobile numbers, which is why scammers just send from other countries to German mobile phone owners. Mostly from France. reply vld_chk 10 hours agoparentprevThis is a fundamentally different problem for a fundamentally different audience. If we take privacy issue, it can be divided into 3 segments: * Privacy of user data. The basic level. When you use Google or Apple, they collect data. Even if you minimize all settings — data is still collected. This data is used to train models and models is used to sell ads, target you or do anything else you have no clue about (like reselling it to hundred of “partners”). * Privacy against undesired identification. Next layer of privacy. When you want to have some personal life online without sharing much about you. Like Reddit, anonymous forums, or Telegram (to some degree). * Privacy against governments. The ultimate boss of privacy. When you want to hide from all governments in the world your identity. Signal was perfect at first layer strong but not perfect at 3rd layer (e2e encryption, no data collection to share nothing with governments who seek for data, good privacy settings, always tell you if your peer logged to new device to protect from cases when government operates with telecom companies and use sms password to make a new login), and almost non present at 2nd because they have no public features except group chats where you share your number. Now they in one move close gaps at 2nd layer — you can hide phone number and stay fully anonymous, and strength their positions in 3rd layer, leaving the last piece open: government still will know that you have some Signal account. As for me, this setup solves 99,999% cases for regular people in democratic and semi-democratic countries and address the most fundamental one: privacy of data and actions online. Yes it is not perfect but barrier for government to spy on me is that high that I reasonably can believe that in most cases you should never be worried about being spied, especially if you live in some places which are named not as Iran or Russia. The only scenario, in my perspective, you can want to have a login without phone (with all sacrifices to spam accounts, quality of peers and usual troll fiesta in such places) is when you want to do something you don’t want ever be found in your current country. But in this case, IMO, Signal is the last worry you usually have on your mind and there are a lot of specialized services and protocols to address your need. reply cookiengineer 5 hours agorootparent1,2 and in part 3 were already fixed with the Signal FOSS fork back then, but Moxie and his army of lawyers decided to send out multiple cease and desist letters against those projects. Which, in return, makes Signal not open source, no matter what the claims are. If they don't hold up their end of the license and argue with their proprietary (and closed to use) infrastructure then I'd argue they are no better than Telegram or WhatsApp. Signal's backup problem is another story which might blow up my comment too much. Because of your mentioned points I would never recommend Signal, and rather point to Briar as a messenger and group/broadcast platform. Currently, it's still a little painful to use and e.g. QR Codes would already help so much with easing up the connection and discovery/handshake process. But it has huge potential as both a messenger and a federated and decentralized platform. reply idatum 6 hours agorootparentprevI just don't want my metadata (contact graph) hoovered because I send a (encrypted) message to someone that may be an over sharer on FB, etc. I use Signal because I am a \"nothing to hide and I like to own my privacy as much as possible\" type online person. Signal == more peace of mind just generally in this online world we have. reply crotchfire 4 hours agorootparentprevIf we take privacy issue, it can be divided into 3 segments: This sounds like a bunch of bullshit. reply egberts1 1 hour agorootparentSounds like your username. reply autoexec 7 hours agorootparentprev> no data collection to share nothing with governments who seek for data, That isn't true anymore and hasn't been for years. Signal collects your data and keeps it forever in the cloud. reply khimaros 6 hours agorootparentcitation needed. care to elaborate on this? reply autoexec 6 hours agorootparentCheck out my post here: https://news.ycombinator.com/threads?id=autoexec#39445866 reply barsonme 3 hours agorootparentSignal is not a VPN. How is this relevant? Or did you link to the wrong comment? reply fsflover 55 minutes agorootparentprevNot exactly that but looks relevant unless you trust Amazon: https://news.ycombinator.com/item?id=39414322 reply joker99 3 hours agoparentprevJust use Wire (wire.com). True end to end encrypted multi device messenger, open source, federated and based on MLS. All you need is an email address, no phone number required. And based in Europe. They allow building your own clients (with some stipulations) and seem to solve everyone’s issues with signal here reply godelski 8 hours agoparentprevI think it is a holdover from the Text Secure days. And like others say, it's a different problem. But for solutions, can't you just buy a voip number? You just need it for registration and then can dump it. I'm sure you can buy one with cash or zcash if you're really paranoid. While in the US I don't have to show my gov ID to get a phone number, I don't know anyone who buys a phone with cash except international students. So practically everyone is identifiable anyways. But I'm not sure this is a deal breaker since all I'm leaking is that I have registered a Signal account. AFAIK Signal only has logs of an account existing and last online with 24hr resolution (which avoids many collision deanonymization methods). Even paying with cash is hard as I'm probably caught on camera (but these usually get flushed). So I'm legitimately curious, why is this a dealbreaker? It doesn't seem like a concern for the vast majority of people, and the problem Signal is solving is secure communication for the masses, not the most secure method possible with unbounded complexity. It's being as secure as possible while being similar in complexity to the average messenger. reply freddie_mercury 8 hours agorootparent> But for solutions, can't you just buy a voip number? No, how would my uncle in the countryside of Vietnam do that? He doesn't have a credit card -- not many here do. He doesn't speak English -- can you find a website that sells voip numbers in Vietnamese? Buying a voip number from a provider in Vietnam has the same exact KYC requirements as buying a SIM, so it is still tied to your government ID and registered forever. Also buying a VOIP for 1 month costs something like $10 from a quick Google. Average salaries are like $1.50/hour. Nobody is going to pay an entire day's salary to buy an VOIP number they throw for a month just so they can register anonymously for chat. So, not you can't \"just\" buy a voip number unless you're a rich Westerner. But who needs privacy more? People in liberal democracies or people in places like Vietnam (literally an authoritarian country where people are routinely imprisoned for speaking against the government)? > I don't know anyone who buys a phone with cash except international students. Everyone buys a phone with cash here because few people have credit cards, since there is no such thing as \"credit ratings\" and it is easy for people to disappear from their debts. There are more people in Vietnam than any country in Europe. We all use smartphones and messenger apps here, too. reply graphe 7 hours agorootparentHe’d ask you to do it then like every non technical older person. It’s a non issue. reply freddie_mercury 7 hours agorootparentNone of my non technical older relatives in Vietnam have asked for anyone's help signing up for the chat accounts they use. reply gitaarik 5 hours agorootparentIndeed. Even most technical people don't have experience setting up VOIP stuff. And needing some techie's intervention just to create an account is not beneficial for a company's user base. Calling this a non-issue is being ignorant about how usability works and influences user engagement. reply graphe 41 minutes agorootparentprevIf they needed signal it be because someone like you told them to get it. Non issue for the billions that use WhatsApp and Facebook. Your uncle in Vietnam has a smartphone, no internet, no number, and NEEDS the signal app? He might need solar, electricity and internet first. reply ossguy 3 hours agoparentprevWhy do you need a German phone number? Many countries let anyone have a phone number, with no proof of address or other identifying information. Just use one of those numbers instead. One example service is https://jmp.chat/ but there are many others. reply illiac786 2 hours agoparentprevThis is not correct. Go to a phone booth, get Signal, never need the phone number again. Any phone will do. Get a phone number from a different country online and without identity check, who cares, you will never need it again. reply boobsbr 1 hour agorootparentI haven't seen a phone booth in Europe for the last 7 years. reply illiac786 1 hour agorootparentJust use the wonderful openstreetmap to find the nearest one, it will be closer than you think. reply jesterson 2 hours agorootparentprevwouldn't the next bloke using the booth for same cause get the whole account? reply illiac786 2 hours agorootparentNot if you set a PIN no. But I think the next bloke can't use the booth to create a signal account anymore. I don't think we'll run out of booth though considering how rare the use case is ;) reply giireon 1 hour agoparentprevIt's still preferable to use a burner number for signal/telegram if you want privacy. reply the_gipsy 57 minutes agorootparentThere are many countries where it's completely impossible to get a burner phone. reply outime 11 hours agoparentprevSame in Spain since 2004 Madrid train bombings IIRC. reply int_19h 10 hours agorootparentThis is the case in most countries these days. There are very few places left where you can get a mobile phone number without identifying yourself at some point. reply Gigachad 9 hours agorootparentI used to care, but at this point it’s obvious that taking a phone number is by far the most effective anti spam and anti trolling method in existence. reply Springtime 8 hours agorootparentThere was a forum that used to have as a requirement a non-free email account and seemed to have no issues with spam accounts with tens of thousands of members for more than 10 years. In that use case it seemed the non-free account aspect to sign-up was the threshold which seemed to keep spammers out vs the fact such an email account could be (with relevant authority) traced back to a real identity. I'd be curious if there is a study that has looked into the thresholds for different use cases at which spam account creation drops to negligible amounts and how much price vs anonymity vs difficulty factors into it. reply someplaceguy 9 hours agorootparentprevWhich is great when databases leak. Absolutely brilliant. reply nottorp 11 hours agoparentprev... but then Signal wouldn't have your phone number either. What they need it for is ... dubious if you ask me. reply tivert 11 hours agorootparent> ... but then Signal wouldn't have your phone number either. What they need it for is ... dubious if you ask me. The reasons they need it aren't really that dubious to me: they want to create a service that actual people will actually use, not just weird privacy geeks who never gave up on PGP. Using phone numbers allows for the kind of user discovery that most people expect in 2024, and requiring them inserts a barrier to mass account creation that can keep spam accounts down to a manageable level (especially given the whole point is they can't do content-based spam-filtering in the way that makes email managable). Personally, my understanding is they've always been trying to develop the maximally private usable chat app, which requires some compromises from the theoretically maximally private chat app. reply nottorp 26 minutes agorootparent> not just weird privacy geeks who never gave up on PGP Looks like you're thinking about key exchanges as opposed to phone number exchanges. Ever heard of user nicknames? reply garbanz0 10 hours agorootparentprevYeah, privacy is weird and cringe! Let's call 'em \"privacy-bros\" or maybe \"encryption-bros\" to signify that they are low status (I don't want to be like them, ew!) reply fastball 4 hours agorootparentIf you need privacy without usability just exchange pubkeys with your friends? reply verticalscaler 2 hours agorootparentI think the remark is more about these sort of rhetorical tactics which permeate every topic. It is a fair remark. reply tivert 2 hours agorootparent> I think the remark is more about these sort of rhetorical tactics which permeate every topic. It is a fair remark. It's not a fair remark though, all it did was twist what I said into a inflammatory derailment. The point is there are a lot of (usually technical) people who are too focused one aspect, but are missing the bigger picture. If you follow them, you'll probably get a communication app that only those people can/will use, which has deal breakers for mass-market adoption. And once that happens, those people probably won't use it either, since they want to communicate outside their group. reply verticalscaler 2 hours agorootparentBoth his and your comments come off as inflammatory derailment to me. That's how it reads, I'm not ascribing malintent. People didn't use to talk like this, I hope you reconsider. \"not just weird privacy geeks who never gave up on PGP.\" is simply not conducive towards making your point. You can make your (otherwise solid) point and even win the argument on merit without this sort of thing. reply actionfromafar 1 hour agorootparentI try to figure out another shorthand which communicates as effectively. \"Privacy minded geeks with a deep understanding of E2E encryption\"? reply makeitdouble 9 hours agorootparentprev> Using phone numbers allows for the kind of user discovery that most people expect in 2024 Do people really expect to still exchange phone numbers ? Fundamentally I don't want people to call me nor SMS me (that's for spam only), most messaging services will allow contact exchange through a QR code inside the app, and if everything else fail an email address will be the most stable fallback. reply op00to 7 hours agorootparent> Do people really expect to still exchange phone numbers Yes. This is the norm in the US. reply fastball 4 hours agorootparentAnd everywhere else on earth. reply makeitdouble 1 hour agorootparentNot really, for better or worse. In many countries SMS was either crazy expensive, unreliable, wall gardened to death (can't message people on other carriers...) and had no traction in the first place. Then phone calls are also crazy expensive: I'm looking at the phone plans right now and the main focus is the data amount. Phone call options are either to only allow for super short conversations for a flat fee (less than 5min per call, for a 25% increase in the monthly plan) or 30 min to an hour of phone call for double to triple the price of the plans. Moving to an alternative is just the normal course given these incentives, and that's what people did in droves (looking at Japan for instance) reply nottorp 11 hours agorootparentprevBut then it's not private. It's linked to your phone number. reply WithinReason 11 hours agorootparentYou can now hide you phone number, according to the blog post. [...] Selecting “Nobody” means that if someone enters your phone number on Signal, they will not be able to message or call you, or even see that you’re on Signal. And anyone you’re chatting with on Signal will not see your phone number as part of your Profile Details page – this is true even if your number is saved in their phone’s contacts. Keep in mind that selecting “Nobody” can make it harder for people to find you on Signal. reply nottorp 10 hours agorootparentI can only hide my phone number from other people, and even for that it should have been hidden by default from the start. Can't hide it from some thought police which may or may not need a court order. reply borski 10 hours agorootparentBut it’s irrelevant, as the chats are end to end encrypted regardless. So sure, they’d know you had a Signal account, but not the contents thereof. reply nottorp 10 hours agorootparentWell, to link with recent news, do you think talking with the late Alexey Navalni over Signal would protect you from russian police? They'd still be able to see that you talked to him. And then what's the point of the super duper encryption? reply ivlad 2 hours agorootparentIn Signal, probably no. Signal has this sealed sender functionality hiding significant amount of metadata from passive observer and active examination post-communication: https://signal.org/blog/sealed-sender/ What Russian police would be able to see, that in a given time period of certificate rotation at most X people communicated to Navalny. reply Summershard 2 hours agorootparentprevSignal does not know who you correspond with. The only information they keep is the account creation timestamp, and the date that the account last connected to the Signal service. You may have confused this information with WhatsApp which indeed keeps a lot of metadata on each user. reply xorcist 30 minutes agorootparentSignal absolutely knows who you correspond with. How could they otherwise route your chat messages? They promise to throw this information away, which is nice but not possible to verify. They also employ a roundabout way of encrypting this data, but as they rightly point out in their article that describes the scheme, encrypting or hashing phone numbers is not safe from a malicious attacker. The space of all possible phone numbers is so small that it could be brute forced in the blink of an eye. You place all your trust in Signal (and Google/Apple) when you use them. That may be better than the alternatives, but it's still something we should be honest about. That said, keep in mind that Signal and Google/Apple can also trivially backdoor your software, so unless you take specific precautions against that, the details of their middleman protection isn't terribly important. reply Summershard 21 minutes agorootparentI guess you are right. It's trust-based. For an actual obfuscation Signal would need to implement something like onion routing, right? I think Session does it. fsflover 47 minutes agorootparentprevhttps://news.ycombinator.com/item?id=39414322 reply Summershard 33 minutes agorootparentWell, TIL. That does not refute my comment, though. Signal still does not know who you chat with. It's the cloud provider who might log the IP address of the sender. Identifying the person based on that information alone would be non-trivial if not simply impossible. reply wolverine876 2 hours agorootparentprev> They'd still be able to see that you talked to him. Signal has no access to metadata, including participants in a conversation. All they know is the date of account creation and the date of the last connection. However, if they got access to Navalni's phone, then they of course can see everything Navalni can. reply nottorp 1 hour agorootparent> However, if they got access to Navalni's phone, then they of course can see everything Navalni can. Aha :) Do you people also want the relevant xkcd? The one about the wrench... reply Geisterde 8 hours agorootparentprevEven encrypted data is not irrelevant. The frequency of messages is relevant, as is how many messages are sent how quickly, the total package size can be revealing if they arent hella padding the data, there is a lot you can learn just from the data. Total obfuscation is ideal. reply op00to 7 hours agorootparentIf you are worried of an adversary that is using numerical analysis on the frequency of messages to somehow undermine you, I’d recommend not using a smartphone or internet connected device. And perhaps medication. reply miramba 5 hours agorootparentGood to hear that you have nothing to hide, comrade. reply nl 10 hours agorootparentprevIt's not irrelevant, but the exposure is reduced. If a person is a member of a terrorist network - or friends with someone who is - the fact that a warrant could force Signal to expose that link could mean that a court is then more likely to approve increased surveillance of your (non-Signal) communications because of that link. On the other hand if you are a woman on Tinder and using Signal to communicate with matches, this doesn't expose you to the person you have just matched with adding your number to their phone book, uploading it to LinkedIn and then finding where you work (which is what you can do with a phone number). My feeling is this is a reasonable compromise, but it is important people understand what it does and doesn't protect you from. reply wyre 11 hours agorootparentprevLuckily there are other messaging services that are private if you’re going to be that pedantic about it. reply egberts1 1 hour agorootparentBut none will be as private as Signal. reply fsflover 45 minutes agorootparentMatrix is more private, depending on your threat model. reply hexage1814 10 hours agorootparentprev>and requiring them inserts a barrier to mass account creation that can keep spam accounts Well, an even better barrier to reduce spam would be Signal to require some official ID of people... reply kQq9oHeAz6wLLS 10 hours agorootparentBut that's also a barrier to actual users, which would be counter-productive. reply Angostura 10 hours agorootparentprevI mean, a phone number is an arbtrary sequence of digits. I'm very happy to use a chat app where I say to someone 'what's your username?'. I'm not giving a chat app free access to all my contacts - and that includes things like Whatsapp reply aqfamnzc 11 hours agorootparentprevThe claim (which generally I'm inclined to believe) is that requiring a phone number drastically increases the cost to sending spam. That in turn drastically reduces the spam amount. reply TacticalCoder 8 hours agorootparentprevTo me Signal is in the business of collecting metadata and nothing else (for whom, that is a good question: probably some three letter agency). reply egberts1 1 hour agorootparentPerhaps you need a refresher in Signal Protocol. Do not be sprouting on about things that you do not understand. https://eprint.iacr.org/2016/1013.pdf reply fsflover 43 minutes agorootparentThe parent is right: https://news.ycombinator.com/item?id=39414322 reply marssaxman 9 hours agorootparentprevWhat they need it for is simply that it's the way the system has always worked, because Signal started life as an encrypted replacement for SMS. The point was that you could switch from the standard SMS app you were already using over to Signal (which was called \"TextSecure\" at the time) without having to change your habits, because sending messages to people's phone numbers was simply what people did then. There's nothing nefarious about it. reply thisislife2 11 hours agoparentprevYes, this is just Apple level bullshit - trust us with your private data even though no law prevents us from exploiting it ... reply stavros 11 hours agorootparentDamn, people will never be satisfied, will they. It's not meant to be an anonymous messenger, because those have spam issues. reply tentacleuno 11 hours agorootparentSignal has spam issues even with the phone number requirement, as I've experienced lately (though nothing on the scale of Twitter). I dread to think what the spam would be like without the requirement of a phone number. reply Stephen304 10 hours agorootparentAt least now you can solve the existing spam problem if you want by disallowing people from using your number to message you in the privacy settings and randomizing your username after anyone new adds you - that way your username is like a one time password to add you, kind of like what lots of people here wish existed for phone calls. reply prmoustache 1 hour agorootparentprevNot true I don't have spam on Tox or Briar. But sadly I don't have contacts either! reply codedokode 10 hours agorootparentprevThey could collect a small amount in cryptocurrency to prove user is not a spammer. Telegram tried this but the price for not providing a phone number was too high. Does it mean knowing user's number is so valuable? reply filleduchaos 9 hours agorootparentIt strikes me as hopelessly naive to think that keeping a personal phone number private is the only reason a user would want to be able to sign up for a service completely anonymously. The question is not whether knowing a user's number is worth $X, the question is whether _anonymous access to your platform_ is worth $X; a question that applies equally to both innocent good-faith users and to spammers/phishers/etc. If your platform is actually worth anything, $X is not going to be a small amount. And yet many people seem to earnestly believe that a tiny token fee will be enough to deter spam, despite clear evidence to the contrary (see for instance how Twitter's \"verification\" fee has completely failed to stop bots from overrunning the platform, many of which proudly display their blue checks). reply fsflover 11 hours agorootparentprevI never received any spam in Matrix. reply stavros 11 hours agorootparentThat's like saying you've never seen any advertisements in the desert. reply cqqxo4zV46cp 10 hours agorootparentprevJust like you haven’t received any communication from anyone about any topic other than talking about Matrix. It’s not that Matrix has a magic formula, it’s that a fraction of a fraction a percent of people care even an iota about it. reply cqqxo4zV46cp 10 hours agorootparentprevI could certainly point out the differences, but the fact that you yourself aren’t acknowledging them indicates to me that you’re throwing intellectual integrity out the window because this product doesn’t work in the way that you want it to work. Engineering is about tradeoffs, and not every company serves to build something that does exactly what YOU want it to. I prefer Signal the way it is. I understand the tradeoffs. reply 447 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Signal is boosting privacy by enabling users to conceal their phone numbers by default and use unique usernames for communication, controlling who can discover them by phone number.",
      "Usernames are kept private in chats and can be exchanged to connect without revealing phone numbers, with users having the flexibility to adjust settings and usernames anytime.",
      "These new features are optional and aim to preserve privacy during interactions on the Signal platform."
    ],
    "commentSummary": [
      "Discussions delve into privacy, security, encryption, and user identification in messaging apps like Telegram and Signal, addressing concerns over financial sustainability, government affiliations, and security implementation.",
      "Users debate the delicate balance between privacy and convenience, considering the use of usernames and friend codes as alternatives to phone numbers for identification on messaging platforms.",
      "Topics include encryption techniques, app expiration rules, data collection, and the trade-offs between usability and privacy, as well as exploring identity verification, spam prevention, and proposing alternative methods for online communication security."
    ],
    "points": 1183,
    "commentCount": 704,
    "retryCount": 0,
    "time": 1708452065
  },
  {
    "id": 39447041,
    "title": "Kagi Sidekick: Fast Search and AI Chat Tool",
    "originLink": "https://sidekick.kagi.com/",
    "originBody": "Kagi Sidekick (alpha) Add fast search with Kagi smarts and AI to your personal or documentation website with just a couple lines of code. We'll sort out the hard stuff - indexing, embeddings, vector search, and AI chat with your pages. Just integrate with two lines of code using our lightweight Web Components (20kB min+gzip) or our Docusaurus, Hugo, or VitePress plugins. See demo in Kagi's own documentation at help.kagi.com What is the big idea? We intend to offer Sidekick as a free service for small/personal websites and reasonably priced search option for commercial websites. The added benefit of using Kagi Sidekick is that your website will automatically (after opt-in) be indexed to show in Kagi search. We are currently in the process of gauging demand for this service. Please register your interest and we'll notify you when it launches. You can also submit feedback in the Sidekick discussion thread. h Follow @KagiHQ for updates or join our Discord to discuss",
    "commentLink": "https://news.ycombinator.com/item?id=39447041",
    "commentBody": "Kagi Sidekick (alpha) (kagi.com)562 points by jviide 12 hours agohidepastfavorite130 comments freediver 12 hours agoKagi founder here. This is very early (alpha) concept built by a single Kagi Labs developer in a few weeks. The proper infrastruture and product is not built yet. We are launching a prototype to get feedback and gauge demand. Why does this exist? It would be an efficient way for us to build and expand our own index. Assuming users of this would be Kagi users, we would expand our index by tens of thousands of high quality? personal websites, hobby projects, startups, documentation websites, etc., also helping to surface them in our results (where relevant, like we already do with Kagi Small Web initiative [1]). It is a win-win for both our users and Kagi. It would also be a way for Kagi to get some exposure outside of kagi.com (provided the search widget has some branding on it). This is why it makes sense to offer it for free for smaller sites/projects. And crowdsourcing index is completely opposite direction of one that causes deterioration of web search results in ad-supported search where few entities control the majority of space [2], so we like it. That is the plan - and since this is a \"Labs\" project, we are open to it crashing and burning. Know we do not, until we try. Try and try again, we must. [1] https://kagi.com/smallweb [2] https://detailed.com/google-control/ reply quinncom 11 hours agoparentHi Vlad, How will you prevent someone from connecting Sidekick to a website that appears at first to be a small website but eventually fills with LLM-generated SEO keywords and ads? You might be able to manually review sites when they first sign up for Sidekick, but once the channel is open for them to inject content into Kagi's index, what's to stop them from abusing their privileged position? reply freediver 10 hours agorootparentThat is a great question. Well we first have to ask what would be the purpose of someone going through the trouble to create such (LLM generated) spammy SEO content? The answer (for the majority of web at least now) is to monetize it with ads/affiliate links. If that is the case then the answer is easy as we already penalize sites with ads/trackers on them for our general web search, and completely boot them out of our own index. In parallel, we are developing LLM-content detector technology to be more efficient at detecting such content regardless of how it is monetised (and we will offer this as an API once developed). reply hackeruse 9 hours agorootparentThis is a naive take. SEO schemes are attractive for companies that sell products themselves (e.g. try searching anything related to ETL tools). The content itself is the ad and you won’t find any ad serving scripts or affiliate links in there. (Source: have created such schemes, although would generally not recommend them to my customers nowadays) reply freediver 8 hours agorootparentUnderestimate the average Kagi user at your own peril. I do not think many would fall prey to an LLM generated content marketing page and end up buying a product from such site. Much likelier scenario is the page gets instantly blocked/reported. reply mentalpiracy 4 hours agorootparentnot really underestimating Kagi users, just the inexorable push to colonize every last useful network. This could also be an attack vector for adversaries looking to pollute Kagi's search results and/or force you to divert resources to policing it. reply Aeolun 1 hour agorootparentprevThat is, until eternal september. reply Repulsion9513 8 hours agorootparentprevThey want to index companies that sell products. I don't see a big problem here if a company that sells a product I'm searching for, who happens to also have low-quality SEO content, shows up in that search. In fact, I would rather they not get penalized for it, since low-quality SEO content is a good way to show up in certain other search engines (Google), and every business wants to show up in Google, making that content quite common even from reputable businesses making a quality product. reply bootsmann 1 hour agorootparentprevYou can block the site in kagi if you don't like it, that's 50% of the entire moat of the search engine. reply daco 7 hours agorootparentprevIs there any reliable AI generated content detector today ? I've tried many free and paying ones online, but they're aren't reliable reply vasco 6 hours agorootparentIt's impossible to make. You cannot prove any sentence was created by an LLM and you can't prove it wasn't. reply yunwal 5 hours agorootparentUnless you design the LLM yourself and purposefully watermark the output. https://arxiv.org/pdf/2306.04634.pdf reply vasco 4 minutes agorootparentThat's a digital signature, same as sending an email with GPG to prove you sent it. You wouldn't say that because some people use GPG you can somehow detect who wrote every email on earth, it's a push model vs pull. This is why I wrote \"any sentence\" vs \"some sentences\". WirelessGigabit 7 hours agorootparentprevProblem is that many webites used to hire writers which wrote tangentially related posts to get their main product higher ranked. Like LogRocket and Partition Minitool do. Combine that with that guy who boasted about his 'SEO heist', I think it's a very valid concern. reply adiabatty 7 hours agorootparentMy general impression of the LogRocket site is that they have decent articles on how to do frontend development. At least that what I remember from the times I've been directed there by a search engine. And we…want to discourage writing useful web pages, even though articles on understanding TypeScript's type system aren't all that closely related to their main product…? What am I missing? reply BadHumans 6 hours agorootparentprevI have solved many problems because of a blog post created by company that wanted to get their product name out there and I don't think they should be looked at negatively for doing that. Are upset whenever a companies tech blog lands on HN? Because it is virtually the same thing. If you use Kagi and come across a site that you find is low quality and spammy then just block it. That's the cool thing about using Kagi. reply pests 3 hours agorootparentAgreed. I've also found that type of developer marketing valuable many times in the past. It's sometimes obvious its going to end in a pitch for the product, but often it does a good job summarizing the key problems in the space, mentioning or showing other solutions / offerings, and pitching which tradeoffs they made for their own product and how they solved issues. Even if you don't go with the ad, you can quickly pivot to other named players or get a better understanding of the terminology or jargon to start searching more. reply thereticent 5 hours agorootparentprevIsn't this exactly what 37signals and even joelonsoftware were? Isn't HN essentially a free conduit to YCombinator awareness? I don't see the problem with what you're describing. It seems like one of the most contributory ways to market well. reply paranoidxprod 12 hours agoparentprevThank you for the explanation! While I agree with others that AI can take away from a products core vision, I’ve been very happy with Kagi’s path and roadmap. I feel like the AI products that you guys have released have served well as complements to search, and hope the trend continues. Hopefully this helps with indexing while offering a cool service to small creators! Edit: I forgot to say, the change where a `?` appended to a search triggering the quick answers was an amazing change. I would love to see more features that can be invoked by appending or prepending to the search query. reply ubutler 10 hours agorootparentRE your edit, I’m assuming you’re already aware that Kagi lets you create custom bangs, but just in case you’re not, you can create your own shortcuts that when preceded by an exclamation mark like !so can redirect to or search other websites. I use this to append ‘site:reddit.com’ when I add !r to a query, for example. reply spenczar5 11 hours agoparentprevI love your transparency. Saying how it benefits Kagi, not just how it is a cool feature for users, is refreshing. It makes me trust more of what you say, and builds some sense of what the product’s direction could be. Thanks. reply crooked-v 7 hours agoparentprevWhat would your approach be for pricing for wiki-type sites that are nonprofit but may have hundreds or thousands of pages with assorted media? I know that decent search beyond just name matching is a recurring issue for independent fandom wikis, which rarely have the funding or coordination to do anything fancier than just a Mediawiki site. For a random example, there's the Baldur's Gate 3 wiki (https://bg3.wiki), which has upwards of 8,000 pages often with pretty dense text (see https://bg3.wiki/wiki/D%26D_5e_rule_changes for an example) and is funded entirely off donations. reply esperent 7 hours agorootparentIt would be great if there was a free version for charity/non profit/open source. I don't know if this is feasible for Kagi. But I do know that many of these types of wiki/forum/blog are run on a shoestring. reply goplayoutside 36 minutes agoparentprevSatisfied Kagi early adopter here. Can you make a Mediawiki extension also? MW search leaves something to be desired, and I'd love to have Kagi on my wiki site. Keep up the great work, you have an incredible product. reply nicoburns 10 hours agoparentprevIs it possible to use the search functionality without the \"AI smarts\"? I can see a good site search service being a great addition to some websites I run, but I would absolutely not want to push an AI chatbot on my users. reply freediver 9 hours agorootparentYes, glad to see the skepticism towards AI, this is why it is turned off by default even in our demo. reply greazy 11 hours agoparentprevKagi user here and scientist. I think kagi sidekick would be very well received in the bioinformatics space. Lots of complex docs that require end users to digest large complex data. Can it be tuned to only point users to the docs and not answer questions? reply freediver 9 hours agorootparentYes, summary mode is completely optional (and turned off by default as you can see in our demo). reply kazinator 10 hours agoparentprevKagi is Japanese for key, right? We search with a search key. If I'm getting it right. Do you know that \"sidekick\" is aibó in Japanese (相棒)? Notice the \"AI\" in it? reply freediver 9 hours agorootparentDidn't know that and thanks for giving us the idea. Aibó sounds much better than Sidekick, we may need to rename :) reply niederman 4 hours agorootparentPlease don't write it Aibó, though. The proper romanization is Aibō or Aibou. See https://en.m.wikipedia.org/wiki/Hepburn_romanization#Long_vo... reply kazinator 7 hours agorootparentprevEven though Sony has taken the sidekick word, there are lots. The characters 愛(love), 相(together) have \"ai\" readings and are productive. The verb あう goes to an あい noun form that is productive for forming words like aizuchi. Plus various others. aite 愛犬 aiken: beloved dog 相手 aite: companion, other party, opponent. 藍色 aiiro: indigo blue; 濃藍 koai: deep indigo. ... https://jisho.org/search/あい%3F reply aledalgrande 6 hours agorootparentand \"jisho\" means \"dictionary\" haha! loved the explanation, although I can only read the hiragana and love jisho too! have to get on with those wanikani exercises... reply solardev 8 hours agorootparentprevIsn't that already the name of Sony's robot dog? reply adiabatty 7 hours agorootparentIt is. reply neilv 10 hours agorootparentprevSony AIBO. https://en.wikipedia.org/wiki/AIBO reply kazinator 9 hours agorootparentYeah, what are the odds that the Japanese would use up the obvious Japanese words for tech stuff. reply davidthewatson 6 hours agoparentprevI'd use this tomorrow... If it worked in a shell script or similar old-school unix architectural style on my bespoke static site generator, which is a slow-motion train wreck of a weekend hack-fest being ported from python/staticjinja to rust/minijinja. Is kagi competing with whoogle? Whoogle gives me the old-school, seemingly linear algebra of pagerank, hauntology that I expect. reply kbumsik 7 hours agoparentprevGreat work! How it would be different from Algolia DocSearch? https://docsearch.algolia.com reply meantub 11 hours agoparentprevI think it would be interesting if like the website ranking that is done on Kagi there was a way to rate the search results to lower or higher it's ranking in search results. It would be a little different though since the website ranking on Kagi is for users but ranking the search results might just improve the intended search result that many people are looking for. I guess this assumes that you aren't already doing that when they click one option over another for a certain search term. Just thinking about searching through some documentation sites and you get a dumb result you weren't looking for at the top, and would want to deprioritize that result. reply aheilbut 6 hours agoparentprevthis is a great idea and should have happened long ago.. https://news.ycombinator.com/item?id=19713604#19714732 reply gavinhoward 6 hours agoparentprevIt sounds good, except I don't want AI on my site. Any way to not have the AI part? reply Kbelicius 38 minutes agorootparentAI is optional and is turned off by default. reply Repulsion9513 8 hours agoparentprevAh yes, \"The 16 Companies\" - except it's actually five hundred... Seems like the problem in [2] is a few entities controlling the majority of spaces other than search, to me. Shame we don't have any real laws against anti-competitive behavior (just the way YC likes it). inb4 flagged reply tangmonk 7 hours agoparentprevYou should launch a crypto project for that reply kaycebasques 10 hours agoprevQuestions as a technical writer who maintains docs sites: * What pages would get included in the index? Everything on the same domain? Or only pages where the search widget is included? Your demo looks like it's pulling in answers from kagi.com whereas if I were maintaining those docs I might want it to only look at help.kagi.com * Do I get logs of the things that people type into the box? How? Also the generated summaries, do I get logs of those? I would need to know what these LLMs are saying to my readers... * Do I get access to the embeddings that you've generated for my site? (Probably not but I have some use cases where it'd be really cool if there are embeddings for my site \"just out there\" with no further work on my part needed.) Edit: assuming that embeddings are involved which I realized later might not be true... * How frequently will you index my site? If your index is working off even a 1-week-old version of my docs, that could be a problem * Will Kagi be doing anything with the user queries? reply sedatk 12 hours agoprevI've been a paying Kagi user for a few months now. The only thing that made me miss Google results was immediate answers: the answers that are extracted from a prominent web page and shown directly, so you don't have to click the link. Actually, Kagi has some of that for certain queries, but it's not as extensive as Google's. So, contrary to the most of the comments here, I support their AI endeavors for the sake of providing answers directly, saving us from clicks. Their search results are already very good. Can't wait to see Kagi flourish. reply nicce 10 hours agoparentI have personally started to avoid Google’s and others immediate answers, because they are often very wrong because they are picked out of the contex. They try to show what you want to see, but it often means something else. reply kosmozaut 3 hours agorootparentThis is so true! While planning a trip abroad last year we were unsure about whether $thing is legal in $country. Google proclaimed in bold letters that, yes, $thing is legal in $country, but this line was taken from a site with the title \"common misconceptions about traveling in $country\" an in fact $thing was not legal. Such a basic mistake, I haven't trusted the instant results ever since. reply Zambyte 8 hours agorootparentprevI have used Kagi for several months now, and I find that the ability to decide which searches I want quick answers for to be useful. There are certain searches where I feel comfortable with accepting a quick answer, and others where I don't think it would be useful. Being able to avoid the clutter unless I want it is nice. reply kuchenbecker 6 hours agorootparentSports scores and stocks. If kagi inlined those results I would be very happy to not need to use the Google bang. Additionally, the google bang destroys suggested terms and with no kagi history I'm typing the whole query. reply al_borland 11 hours agoparentprevI’ve been really liking the quick answers[1] that Kagi added. The doc gives some info as to what triggers it so it seems less random. [1] https://help.kagi.com/kagi/ai/quick-answer.html reply sedatk 11 hours agorootparentOh that's awesome. I think I bumped into it once, but my search flow is usually always in a rush that I haven't had time to stop and think about it. reply mastercheif 7 hours agorootparentYou can click the \"Quick Answer\" button on the search results page to trigger the same feature. I think it appears whenever there is more than one word in the search query. reply paradox460 6 minutes agorootparentYou can also force its appearance by sticking a ? at the end of a query, either space separated or otherwise. reply almyk 10 hours agorootparentprevThanks for linking to the docs. I really like this feature, but as you said, it felt like it showed up at random reply ashenke 8 hours agorootparentSince last week, it will trigger whenever your search query ends with a ? or starts with How or Why or other interrogative words https://kagi.com/changelog#3179 reply unshavedyak 11 hours agoparentprevUltimate/normal customer, and i agree. Personally i think AI _is_ Search, and while we don't need to force them together in some massive behemoth now - laying foundation for being familiar, comfortable and well integrated in the future seems foundational to today. In my eyes at least. reply bbor 10 hours agorootparentStrongly held belief that I think is backed up by academic consensus: we should not treat LLMs as stores of knowledge. As my mantra goes, “language models, not knowledge models”. So the future of search might involve LLMs at a very fundamental level (and I think it will!), but they’ll never be the central component. Humans will never ever ever invent a better knowledge system than a database / a piece of paper, I guarantee. reply lutoma 9 hours agoparentprevOne thing I miss from Google is a reliable built-in calculator. I know I could just use some other calculator app but the habit of just typing stuff into my address bar is hard to shake. Kagi also has a calculator, but for a lot of queries it gives questionable results, for example for `210/8` it returns `105/4`. Technically correct, sure, but almost never what I want. reply paradox460 4 minutes agorootparentJust tested this out, you can add \"in decimal\" to get it as a decimal. reply freedomben 9 hours agorootparentprevFWIW I think this has improved radically in the past few weeks thanks to their integration with Wolfram. I haven't tried it yet though but was pleased to see that in the changelog reply sedatk 8 hours agorootparentIt's apparently the Wolfram|Alpha integration that returns 105/4. reply sedatk 8 hours agorootparentprevI agree. I've also found out that \"210/8.0\" returns the desired result. reply OJFord 10 hours agoparentprevIf anyone from DDG is here, please sort out your calendar one? I reported it in early January that it was (understandably) erroneously assuming 'jan calendar' still meant current_year+1; still doing it for February. March is ok though, so I think actually the implementation was always buggy, not just now it's 2024, but it's checking if month isthe same SEO site gaming/structuring. This isn’t totally true. They can dodge a lot of stuff, because optimisations target Google’s algorithms, not Kagis. And Google also has conflicting intrests on same cases to let them just be. I have paid for long time as well. But this brings new ”attack vector”, which is important to consider. reply janfoeh 10 hours agorootparentprevPaying Kagi customer here as well. Kagi is leagues better than now-Google. It is about on par, slightly worse than the Google of yore — not by their own fault, but because they operate in a much more professionalized, difficult, hostile web. They didn't dodge the SEO bullet — nobody has aimed at them yet. reply al_borland 11 hours agorootparentprevWith Kagi’s customers being users, not advertisers, they can put in features to mitigate against problems caused by SEO, like how Kagi bundles up all the “Top X abcdefg” lists that turned Google into trash. reply nicce 10 hours agorootparentThe parent context was: > Ie user activity on embedded sites helps indicate interest in Kagi index updates. Doesn’t this mean anyone? And once someone fakes user activity, that make it useless measure in the end. reply al_borland 7 hours agorootparentIf that becomes a problem, they can alter the plan. Kagi doesn’t have the same conflict as pretty much every other search engine when evaluating this stuff. If it starts making things worse, stop factoring it in, or reduce the influence in the ranking. reply dvaun 11 hours agorootparentprevThis seems like a problem of the search space rather than solely Google's. How are you supposed to guard a system which, by its nature, provides financial incentive to others to game it? No matter what weights and checks are put in place, some observers will notice how their rankings change and make appropriate modifications. This isn't solved by Kagi's product, either. I'm a happy user and use it as my daily driver. That doesn't mean that, if it increases in popularity, the results will remain unskewed. reply nicce 10 hours agorootparentThat is true. But you can still try to learn from others mistakes to make it as hard as possible? reply ipaddr 9 hours agorootparentprevNo one would want to target developers with their warez. reply t00 1 hour agoprevIn Kagi example search results I found some results being considerably not relevant to the search query. Is it like an option \"discover\" which can be disabled? Headphones search returned a MacOS app when searching on a phone. Does actual search vary reaults based on device user-agent class or platform? reply dannyobrien 2 hours agoprevI think everyone has some point where they determined that Google \"went bad\" (or at least everyone who thought they were good at some point). Mine is Google Plus, because they decided to make it, like Facebook, a destination. Before it was unveiled, I assumed it was going to be, like AdWords, a thing you could add to your website: adding a social layer to the Web would have increased its usefulness, allowed Google to do its usual thing of providing a feature that informed it of where the interesting stuff was, and undermined Facebook's main advantage over the rest of the web. I'm cheered that Kagi gets what Google forgot. reply ggoo 12 hours agoprevI just want good search results. This feels like a distraction. reply Kerrick 11 hours agoparentGiven that their motivation for this is to find more sites and pages to index, it seems like it should improve the quality (or at least quantity) of results thanks to a larger index. reply ggoo 11 hours agorootparentHm, I may have judged too quickly then - I do hope this translates to better quality. Will wait and see. reply laweijfmvo 12 hours agoparentprevAgree, I thought I was paying for search so they could focus on building search. Anything that feels like a money-raising or God-forbid acquisition play is concerning. reply evanharwin 12 hours agoparentprevAgreed. Seems like web search implementation (both from Kagi and all its competitors!) could be almost endlessly improved upon, and any non-search feature is at odds with that. Maybe there’s an argument that people who might use this, might also be people with sites that’d be valuable to index, and thus it’d both be nice for them and improve search for all users? :) reply MostlyStable 12 hours agoprevAs a paying customer, I agree that the only thing I care about from Kagi is search, but...I think that there is a very non-trivial chance that in the near future, search almost entirely gets eaten by AI. It makes sense to me that a search company would be exploring AI and its interaction with search. While I hope this doesn't become the main focus (until and unless it has to be), the history of companies like Kodak (who was a \"film\" company and therefore chose to ignore digital cameras) should be instructive. If they completely ignore the potential replacement to search, they might get good at search just in time for that to become irrelevant. reply jcul 10 hours agoparentThis feature isn't specifically to do with AI though is it? Unless I missed something (likely). My understanding was that it was about adding a \"search with Kagi\" thing to your site and triggering Kagi's indexer to index your site as a side effect. reply Shadowmist 4 hours agoparentprevI pay for search, and would also pay for email (currently paying Google). Speaking of Google, I hate the Kagi “g” logo. reply stefandesu 2 hours agorootparentSame, it sometimes confuses me and I think I'm on Google. But maybe that's exactly the point. reply reddalo 1 hour agorootparentHow do you get confused? It's orange and black, completely different from Google's. But... it reminds me of old-school Google logo, before they changed it to the simple sans-serif new one. reply stpe 2 hours agoprevFunny. Kagi was the payment solution I used to sell shareware in the late 90'ies. I believe the company later disappeared due a fraud case that made it bankrupt. Seems like both domain and company name is recycled into this now. reply lauriewired 9 hours agoprevAny chance you'd consider making a version of this as a browser extension for paid subscribers? What I really want is this functionality on unsupported websites/documentation. It would be killer to \"ask\" kagi a fact about the article or docs I'm already reading, even if it only traverses/parses the single page. reply lurking_swe 9 hours agoparenti think this would do what you want: https://kagi.com/summarizer/index.html now just need to build an extension :) reply jackson1442 8 hours agorootparentIt's already a feature in the Kagi browser extensions! Just summarization (and their FastGPT), not the website Q&A part. chrome: https://chromewebstore.google.com/detail/kagi-search-for-chr... firefox: https://addons.mozilla.org/en-US/firefox/addon/kagi-search-f... reply qwertox 5 hours agoprevI didn't learn anything from that screencast. It's not clear what action causes which reaction. reply dcchambers 4 hours agoprevFree for small/personal websites is great. reply numbers 12 hours agoprevThis is tangential to search but I wonder if this is a way to push more indexing info into the kagi index or if it's just another fun little project? reply Springtime 8 hours agoprevThe demo docs site says the feature is only available to 'Ultimate' members, while the linked page is ambiguous. Is it the case that Sidekick is only available for such Kagi members? reply syntaxing 10 hours agoprevI love this. Llamaindex has (had?) a discord bot that did something similar. I forgot who the exact provider was but it solved my problem. Sometimes small projects just doesn’t have the people power for support and features like this go a long way. reply runamuck 11 hours agoprevI pay for Kagi. I also have a small hobby site and would love to try this out. reply metabagel 10 hours agoprevI am a paid subscriber. I love Kagi search. I love that I can lower my personal ranking of a website which doesn't provide value for me - SEO spam, AI nonsense, etc. reply RagnarD 9 hours agoprevPaying Kagi search customer here. Thanks for letting me dump Google and providing better features as well. reply nabogh 11 hours agoprevI cancelled my Kagi subscription because it's a little too expensive for me in the unlimited search tier. And I search too much for the limited search tier. I can't help but wonder if my money was being spent on pet projects like this instead of improving/maintaining the search. reply chias 11 hours agoparentAll companies allocate some fraction of their resources towards innovation outside of what is currently their core feature-set. reply Onawa 11 hours agorootparentOn top of that, the top comment from the Kagi CEO says this was developed by a single developer in a couple of weeks... Yes they will have to finish building up infrastructure behind it, but as a paying Kagi user I'm more than happy to support any side ventures that ultimately end up making search better in the long run. reply nabogh 9 hours agorootparentprevYeah like it's fine. I just felt the value wasn't quite there for me personally and I found a few too many projects coming out that didn't benefit me. So I cancelled. No harm done in the end, just a little sad nothing is filling my personal niche of the market. reply samatman 9 hours agoparentprevThey don't spend your money, fella. They spend their money, some of which you paid them in exchange for services. Which you've stopped doing. reply nabogh 9 hours agorootparentOf course I understand that it's not my money. Weirdly hostile comment. I'm just expressing that the value wasn't quite there for myself. And these projects kept coming out so I wondered if they were the reason for the higher prices. reply BeetleB 10 hours agoprevOK - I must be dumb. > Just integrate with two lines of code using our lightweight Web Components (20kB min+gzip) or our Docusaurus, Hugo, or VitePress plugins. See demo in Kagi's own documentation at help.kagi.com How do I do this on my site? I couldn't find the code anywhere. reply justusthane 9 hours agoparentMaybe you didn’t read to the end, or maybe the page was updated after you commented? > We are currently in the process of gauging demand for this service. Please register your interest and we'll notify you when it launches. reply kelvie 12 hours agoprevDisclaimer: Happy paid kagi user for over a year now. Why would they offer this for free for small websites? This isn't some VC-backed company getting ready to data-mine us and collect users for enshittification purposes, and in general, Kagi is the site people recommend when they say \"if you're not the customer, you're the product\". reply atestu 12 hours agoparentMy take is this helps them index the web, and they're particularly interested in small website with great, niche, organic content. See also: https://kagi.com/smallweb reply psytrx 11 hours agorootparentI've had the same thoughts on those. Maybe it also allows them to gather data on whether/how people interact with a webpage itself, which may be an indicator for its quality. reply atestu 7 hours agorootparentYes! It tells them what people search for on that particular website… all very valuable data. reply hungariantoast 12 hours agoparentprevMaybe because they can comfortably afford to? Maybe because some people who operate businesses are actually kind and generous? Maybe because this service doesn't cost them much to operate for \"small/personal websites\"? Maybe because it's a clever way to get more people to pay for the service in the long run, after they initially try it out for free? reply ipaddr 8 hours agoparentprevBecause their main business is taking content ranking it and charging you for it. This allows them to discover sites cheaper in a more targeted way. It increases awareness of kagi. It's a win/win idea. Don't mistake Kagi for something it's not. You are still the product. All of your data is feed into the system (what you search for, what you rank, what you filter etc) and used to sell others. reply lelandbatey 9 hours agoprev [–] Light fast response, fuzzy search, low-difficulty include. If this was available and free, I'd be working hard to use this to search all my note/docs. If it were cheap, I'd use it for simple search @ dayjob on a regular basis. What a great little feature! I am not a paying customer for Kagi, though I keep thinking about it. Maybe this'll turn me into a paying customer ahead of schedule. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Kagi Sidekick enables fast search, AI chat, and indexing on websites using minimal code, targeting personal or documentation sites.",
      "It's free for small/personal websites and offers commercial sites reasonably priced options, including inclusion in Kagi search.",
      "Users interested in the tool can engage in the Sidekick discussion thread or join Kagi's Discord community for updates and feedback."
    ],
    "commentSummary": [
      "The conversation focuses on Kagi Sidekick, an alpha concept by Kagi Labs to broaden their collection of personal websites and startups, touching on AI-generated content risks, SEO tactics, and nonprofit wiki difficulties.",
      "Users raise technical queries about the search tool's operations, exchange experiences, and propose enhancements, while some question the worth of Kagi's services and ponder their intentions behind offering free new initiatives.",
      "The discussion underscores the significance of delivering valuable content and transparent marketing for effective promotion in the tech industry."
    ],
    "points": 562,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1708463900
  },
  {
    "id": 39443965,
    "title": "Let's Enhance GPT Tokenization Efficiency",
    "originLink": "https://www.youtube.com/watch?v=zduSFxRajkE",
    "originBody": "hi everyone so in this video I'd like us to cover the process of tokenization in large language models now you see here that I have a set face and that's because uh tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it it is fairly hairy gnarly and there's a lot of hidden foot guns to be aware of and a lot of oddness with large language models typically traces back to tokenization so what is tokenization now in my previous video Let's Build GPT from scratch uh we actually already did tokenization but we did a very naive simple version of tokenization so when you go to the Google colab for that video uh you see here that we loaded our training set and our training set was this uh Shakespeare uh data set now in the beginning the Shakespeare data set is just a large string in Python it's just text and so the question is how do we plug text into large language models and in this case here we created a vocabulary of 65 possible characters that we saw occur in this string these were the possible characters and we saw that there are 65 of them and then we created a a lookup table for converting from every possible character a little string piece into a token an integer so here for example we tokenized the string High there and we received this sequence of tokens and here we took the first 1,000 characters of our data set and we encoded it into tokens and because it is this is character level we received 1,000 tokens in a sequence so token 18 47 Etc now later we saw that the way we plug these tokens into the language model is by using an embedding table and so basically if we have 65 possible tokens then this embedding table is going to have 65 rows and roughly speaking we're taking the integer associated with every single sing Le token we're using that as a lookup into this table and we're plucking out the corresponding row and this row is a uh is trainable parameters that we're going to train using back propagation and this is the vector that then feeds into the Transformer um and that's how the Transformer Ser of perceives every single token so here we had a very naive tokenization process that was a character level tokenizer but in practice in state-ofthe-art uh language models people use a lot more complicated schemes unfortunately uh for constructing these uh token vocabularies so we're not dealing on the Character level we're dealing on chunk level and the way these um character chunks are constructed is using algorithms such as for example the bik pair in coding algorithm which we're going to go into in detail um and cover in this video I'd like to briefly show you the paper that introduced a bite level encoding as a mechanism for tokenization in the context of large language models and I would say that that's probably the gpt2 paper and if you scroll down here to the section input representation this is where they cover tokenization the kinds of properties that you'd like the tokenization to have and they conclude here that they're going to have a tokenizer where you have a vocabulary of 50,2 57 possible tokens and the context size is going to be 1,24 tokens so in the in in the attention layer of the Transformer neural network every single token is attending to the previous tokens in the sequence and it's going to see up to 1,24 tokens so tokens are this like fundamental unit um the atom of uh large language models if you will and everything is in units of tokens everything is about tokens and tokenization is the process for translating strings or text into sequences of tokens and uh vice versa when you go into the Llama 2 paper as well I can show you that when you search token you're going to get get 63 hits um and that's because tokens are again pervasive so here they mentioned that they trained on two trillion tokens of data and so on so we're going to build our own tokenizer luckily the bite be encoding algorithm is not uh that super complicated and we can build it from scratch ourselves and we'll see exactly how this works before we dive into code I'd like to give you a brief Taste of some of the complexities that come from the tokenization because I just want to make sure that we motivate it sufficiently for why we are doing all this and why this is so gross so tokenization is at the heart of a lot of weirdness in large language models and I would advise that you do not brush it off a lot of the issues that may look like just issues with the new network architecture or the large language model itself are actually issues with the tokenization and fundamentally Trace uh back to it so if you've noticed any issues with large language models can't you know not able to do spelling tasks very easily that's usually due to tokenization simple string processing can be difficult for the large language model to perform natively uh non-english languages can work much worse and to a large extent this is due to tokenization sometimes llms are bad at simple arithmetic also can trace be traced to tokenization uh gbt2 specifically would have had quite a bit more issues with python than uh future versions of it due to tokenization there's a lot of other issues maybe you've seen weird warnings about a trailing whites space this is a tokenization issue um if you had asked GPT earlier about solid gold Magikarp and what it is you would see the llm go totally crazy and it would start going off about a completely unrelated tangent topic maybe you've been told to use yl over Json in structure data all of that has to do with tokenization so basically tokenization is at the heart of many issues I will look back around to these at the end of the video but for now let me just um skip over it a little bit and let's go to this web app um the Tik tokenizer bell.app so I have it loaded here and what I like about this web app is that tokenization is running a sort of live in your browser in JavaScript so you can just type here stuff hello world and the whole string rokenes so here what we see on uh the left is a string that you put in on the right we're currently using the gpt2 tokenizer we see that this string that I pasted here is currently tokenizing into 300 tokens and here they are sort of uh shown explicitly in different colors for every single token so for example uh this word tokenization became two tokens the token 3,642 and 1,634 the token um space is is token 318 so be careful on the bottom you can show white space and keep in mind that there are spaces and uh sln new line characters in here but you can hide them for clarity the token space at is token 379 the to the Token space the is 262 Etc so you notice here that the space is part of that uh token chunk now so this is kind of like how our English sentence broke up and that seems all well and good now now here I put in some arithmetic so we see that uh the token 127 Plus and then token six space 6 followed by 77 so what's happening here is that 127 is feeding in as a single token into the large language model but the um number 677 will actually feed in as two separate tokens and so the large language model has to sort of um take account of that and process it correctly in its Network and see here 804 will be broken up into two tokens and it's is all completely arbitrary and here I have another example of four-digit numbers and they break up in a way that they break up and it's totally arbitrary sometimes you have um multiple digits single token sometimes you have individual digits as many tokens and it's all kind of pretty arbitrary and coming out of the tokenizer here's another example we have the string egg and you see here that this became two tokens but for some reason when I say I have an egg you see when it's a space egg it's two token it's sorry it's a single token so just egg by itself in the beginning of a sentence is two tokens but here as a space egg is suddenly a single token uh for the exact same string okay here lowercase egg turns out to be a single token and in particular notice that the color is different so this is a different token so this is case sensitive and of course a capital egg would also be different tokens and again um this would be two tokens arbitrarily so so for the same concept egg depending on if it's in the beginning of a sentence at the end of a sentence lowercase uppercase or mixed all this will be uh basically very different tokens and different IDs and the language model has to learn from raw data from all the internet text that it's going to be training on that these are actually all the exact same concept and it has to sort of group them in the parameters of the neural network and understand just based on the data patterns that these are all very similar but maybe not almost exactly similar but but very very similar um after the EG demonstration here I have um an introduction from open a eyes chbt in Korean so manaso Pang uh Etc uh so this is in Korean and the reason I put this here is because you'll notice that um non-english languages work slightly worse in Chachi part of this is because of course the training data set for Chachi is much larger for English and for everything else but the same is true not just for the large language model itself but also for the tokenizer so when we train the tokenizer we're going to see that there's a training set as well and there's a lot more English than non-english and what ends up happening is that we're going to have a lot more longer tokens for English so how do I put this if you have a single sentence in English and you tokenize it you might see that it's 10 tokens or something like that but if you translate that sentence into say Korean or Japanese or something else you'll typically see that the number of tokens used is much larger and that's because the chunks here are a lot more broken up so we're using a lot more tokens for the exact same thing and what this does is it bloats up the sequence length of all the documents so you're using up more tokens and then in the attention of the Transformer when these tokens try to attend each other you are running out of context um in the maximum context length of that Transformer and so basically all the non-english text is stretched out from the perspective of the Transformer and this just has to do with the um trainings that used for the tokenizer and the tokenization itself so it will create a lot bigger tokens and a lot larger groups in English and it will have a lot of little boundaries for all the other non-english text um so if we translated this into English it would be significantly fewer tokens the final example I have here is a little snippet of python for doing FS buuz and what I'd like you to notice is look all these individual spaces are all separate tokens they are token 220 so uh 220 220 220 220 and then space if is a single token and so what's going on here is that when the Transformer is going to consume or try to uh create this text it needs to um handle all these spaces individually they all feed in one by one into the entire Transformer in the sequence and so this is being extremely wasteful tokenizing it in this way and so as a result of that gpt2 is not very good with python and it's not anything to do with coding or the language model itself it's just that if he use a lot of indentation using space in Python like we usually do uh you just end up bloating out all the text and it's separated across way too much of the sequence and we are running out of the context length in the sequence uh that's roughly speaking what's what's happening we're being way too wasteful we're taking up way too much token space now we can also scroll up here and we can change the tokenizer so note here that gpt2 tokenizer creates a token count of 300 for this string here we can change it to CL 100K base which is the GPT for tokenizer and we see that the token count drops to 185 so for the exact same string we are now roughly having the number of tokens and roughly speaking this is because uh the number of tokens in the GPT 4 tokenizer is roughly double that of the number of tokens in the gpt2 tokenizer so we went went from roughly 50k to roughly 100K now you can imagine that this is a good thing because the same text is now squished into half as many tokens so uh this is a lot denser input to the Transformer and in the Transformer every single token has a finite number of tokens before it that it's going to pay attention to and so what this is doing is we're roughly able to see twice as much text as a context for what token to predict next uh because of this change but of course just increasing the number of tokens is uh not strictly better infinitely uh because as you increase the number of tokens now your embedding table is um sort of getting a lot larger and also at the output we are trying to predict the next token and there's the soft Max there and that grows as well we're going to go into more detail later on this but there's some kind of a Sweet Spot somewhere where you have a just right number of tokens in your vocabulary where everything is appropriately dense and still fairly efficient now one thing I would like you to note specifically for the gp4 tokenizer is that the handling of the white space for python has improved a lot you see that here these four spaces are represented as one single token for the three spaces here and then the token SPF and here seven spaces were all grouped into a single token so we're being a lot more efficient in how we represent Python and this was a deliberate Choice made by open aai when they designed the gp4 tokenizer and they group a lot more space into a single character what this does is this densifies Python and therefore we can attend to more code before it when we're trying to predict the next token in the sequence and so the Improvement in the python coding ability from gbt2 to gp4 is not just a matter of the language model and the architecture and the details of the optimization but a lot of the Improvement here is also coming from the design of the tokenizer and how it groups characters into tokens okay so let's now start writing some code so remember what we want to do we want to take strings and feed them into language models for that we need to somehow tokenize strings into some integers in some fixed vocabulary and then we will use those integers to make a look up into a lookup table of vectors and feed those vectors into the Transformer as an input now the reason this gets a little bit tricky of course is that we don't just want to support the simple English alphabet we want to support different kinds of languages so this is anango in Korean which is hello and we also want to support many kinds of special characters that we might find on the internet for example Emoji so how do we feed this text into uh Transformers well how's the what is this text anyway in Python so if you go to the documentation of a string in Python you can see that strings are immutable sequences of Unicode code points okay what are Unicode code points we can go to PDF so Unicode code points are defined by the Unicode Consortium as part of the Unicode standard and what this is really is that it's just a definition of roughly 150,000 characters right now and roughly speaking what they look like and what integers um represent those characters so it says 150,000 characters across 161 scripts as of right now so if you scroll down here you can see that the standard is very much alive the latest standard 15.1 in September 2023 and basically this is just a way to define lots of types of characters like for example all these characters across different scripts so the way we can access the unic code code Point given Single Character is by using the or function in Python so for example I can pass in Ord of H and I can see that for the Single Character H the unic code code point is 104 okay um but this can be arbitr complicated so we can take for example our Emoji here and we can see that the code point for this one is 128,000 or we can take un and this is 50,000 now keep in mind you can't plug in strings here because you uh this doesn't have a single code point it only takes a single uni code code Point character and tells you its integer so in this way we can look up all the um characters of this specific string and their code points so or of X forx in this string and we get this encoding here now see here we've already turned the raw code points already have integers so why can't we simply just use these integers and not have any tokenization at all why can't we just use this natively as is and just use the code Point well one reason for that of course is that the vocabulary in that case would be quite long so in this case for Unicode the this is a vocabulary of 150,000 different code points but more worryingly than that I think the Unicode standard is very much alive and it keeps changing and so it's not kind of a stable representation necessarily that we may want to use directly so for those reasons we need something a bit better so to find something better we turn to encodings so if we go to the Wikipedia page here we see that the Unicode consortion defines three types of encodings utf8 UTF 16 and UTF 32 these encoding are the way by which we can take Unicode text and translate it into binary data or by streams utf8 is by far the most common uh so this is the utf8 page now this Wikipedia page is actually quite long but what's important for our purposes is that utf8 takes every single Cod point and it translates it to a by stream and this by stream is between one to four bytes so it's a variable length encoding so depending on the Unicode Point according to the schema you're going to end up with between 1 to four bytes for each code point on top of that there's utf8 uh utf16 and UTF 32 UTF 32 is nice because it is fixed length instead of variable length but it has many other downsides as well so the full kind of spectrum of pros and cons of all these different three encodings are beyond the scope of this video I just like to point out that I enjoyed this block post and this block post at the end of it also has a number of references that can be quite useful uh one of them is uh utf8 everywhere Manifesto um and this Manifesto describes the reason why utf8 is significantly preferred and a lot nicer than the other encodings and why it is used a lot more prominently um on the internet one of the major advantages just just to give you a sense is that utf8 is the only one of these that is backwards compatible to the much simpler asky encoding of text um but I'm not going to go into the full detail in this video so suffice to say that we like the utf8 encoding and uh let's try to take the string and see what we get if we encoded into utf8 the string class in Python actually has do encode and you can give it the encoding which is say utf8 now we get out of this is not very nice because this is the bytes is a bytes object and it's not very nice in the way that it's printed so I personally like to take it through list because then we actually get the raw B of this uh encoding so this is the raw byes that represent this string according to the utf8 en coding we can also look at utf16 we get a slightly different by stream and we here we start to see one of the disadvantages of utf16 you see how we have zero Z something Z something Z something we're starting to get a sense that this is a bit of a wasteful encoding and indeed for simple asky characters or English characters here uh we just have the structure of 0 something Z something and it's not exactly nice same for UTF 32 when we expand this we can start to get a sense of the wastefulness of this encoding for our purposes you see a lot of zeros followed by something and so uh this is not desirable so suffice it to say that we would like to stick with utf8 for our purposes however if we just use utf8 naively these are by streams so that would imply a vocabulary length of only 256 possible tokens uh but this this vocabulary size is very very small what this is going to do if we just were to use it naively is that all of our text would be stretched out over very very long sequences of bytes and so um what what this does is that certainly the embeding table is going to be tiny and the prediction at the top at the final layer is going to be very tiny but our sequences are very long and remember that we have pretty finite um context length and the attention that we can support in a transformer for computational reasons and so we only have as much context length but now we have very very long sequences and this is just inefficient and it's not going to allow us to attend to sufficiently long text uh before us for the purposes of the next token prediction task so we don't want to use the raw bytes of the utf8 encoding we want to be able to support larger vocabulary size that we can tune as a hyper but we want to stick with the utf8 encoding of these strings so what do we do well the answer of course is we turn to the bite pair encoding algorithm which will allow us to compress these bite sequences um to a variable amount so we'll get to that in a bit but I just want to briefly speak to the fact that I would love nothing more than to be able to feed raw bite sequences into uh language models in fact there's a paper about how this could potentially be done uh from Summer last last year now the problem is you actually have to go in and you have to modify the Transformer architecture because as I mentioned you're going to have a problem where the attention will start to become extremely expensive because the sequences are so long and so in this paper they propose kind of a hierarchical structuring of the Transformer that could allow you to just feed in raw bites and so at the end they say together these results establish the viability of tokenization free autor regressive sequence modeling at scale so tokenization free would indeed be amazing we would just feed B streams directly into our models but unfortunately I don't know that this has really been proven out yet by sufficiently many groups and a sufficient scale uh but something like this at one point would be amazing and I hope someone comes up with it but for now we have to come back and we can't feed this directly into language models and we have to compress it using the B paare encoding algorithm so let's see how that works so as I mentioned the B paare encoding algorithm is not all that complicated and the Wikipedia page is actually quite instructive as far as the basic idea goes go what we're doing is we have some kind of a input sequence uh like for example here we have only four elements in our vocabulary a b c and d and we have a sequence of them so instead of bytes let's say we just have four a vocab size of four the sequence is too long and we'd like to compress it so what we do is that we iteratively find the pair of uh tokens that occur the most frequently and then once we've identified that pair we repl replace that pair with just a single new token that we append to our vocabulary so for example here the bite pair AA occurs most often so we mint a new token let's call it capital Z and we replace every single occurrence of AA by Z so now we have two Z's here so here we took a sequence of 11 characters with vocabulary size four and we've converted it to a um sequence of only nine tokens but now with a vocabulary of five because we have a fifth vocabulary element that we just created and it's Z standing for concatination of AA and we can again repeat this process so we again look at the sequence and identify the pair of tokens that are most frequent let's say that that is now AB well we are going to replace AB with a new token that we meant call Y so y becomes ab and then every single occurrence of ab is now replaced with y so we end up with this so now we only have 1 2 3 4 5 6 seven characters in our sequence but we have not just um four vocabulary elements or five but now we have six and for the final round we again look through the sequence find that the phrase zy or the pair zy is most common and replace it one more time with another um character let's say x so X is z y and we replace all curses of zy and we get this following sequence so basically after we have gone through this process instead of having a um sequence of 11 uh tokens with a vocabulary length of four we now have a sequence of 1 2 3 four five tokens but our vocabulary length now is seven and so in this way we can iteratively compress our sequence I we Mint new tokens so in the in the exact same way we start we start out with bite sequences so we have 256 vocabulary size but we're now going to go through these and find the bite pairs that occur the most and we're going to iteratively start minting new tokens appending them to our vocabulary and replacing things and in this way we're going to end up with a compressed training data set and also an algorithm for taking any arbitrary sequence and encoding it using this uh vocabul and also decoding it back to Strings so let's now Implement all that so here's what I did I went to this block post that I enjoyed and I took the first paragraph and I copy pasted it here into text so this is one very long line here now to get the tokens as I mentioned we just take our text and we encode it into utf8 the tokens here at this point will be a raw bites single stream of bytes and just so that it's easier to work with instead of just a bytes object I'm going to convert all those bytes to integers and then create a list of it just so it's easier for us to manipulate and work with in Python and visualize and here I'm printing all of that so this is the original um this is the original paragraph and its length is 533 uh code points and then here are the bytes encoded in ut utf8 and we see that this has a length of 616 bytes at this point or 616 tokens and the reason this is more is because a lot of these simple asky characters or simple characters they just become a single bite but a lot of these Unicode more complex characters become multiple bytes up to four and so we are expanding that size so now what we'd like to do as a first step of the algorithm is we'd like to iterate over here and find the pair of bites that occur most frequently because we're then going to merge it so if you are working long on a notebook on a side then I encourage you to basically click on the link find this notebook and try to write that function yourself otherwise I'm going to come here and Implement first the function that finds the most common pair okay so here's what I came up with there are many different ways to implement this but I'm calling the function get stats it expects a list of integers I'm using a dictionary to keep track of basically the counts and then this is a pythonic way to iterate consecutive elements of this list uh which we covered in the previous video and then here I'm just keeping track of just incrementing by one um for all the pairs so if I call this on all the tokens here then the stats comes out here so this is the dictionary the keys are these topples of consecutive elements and this is the count so just to uh print it in a slightly better way this is one way that I like to do that where you it's a little bit compound here so you can pause if you like but we iterate all all the items the items called on dictionary returns pairs of key value and instead I create a list here of value key because if it's a value key list then I can call sort on it and by default python will uh use the first element which in this case will be value to sort by if it's given tles and then reverse so it's descending and print that so basically it looks like 101 comma 32 was the most commonly occurring consecutive pair and it occurred 20 times we can double check that that makes reasonable sense so if I just search 10132 then you see that these are the 20 occurrences of that um pair and if we'd like to take a look at what exactly that pair is we can use Char which is the opposite of or in Python so we give it a um unic code Cod point so 101 and of 32 and we see that this is e and space so basically there's a lot of E space here meaning that a lot of these words seem to end with e so here's eace as an example so there's a lot of that going on here and this is the most common pair so now that we've identified the most common pair we would like to iterate over this sequence we're going to Mint a new token with the ID of 256 right because these tokens currently go from Z to 255 so when we create a new token it will have an ID of 256 and we're going to iterate over this entire um list and every every time we see 101 comma 32 we're going to swap that out for 256 so let's Implement that now and feel free to uh do that yourself as well so first I commented uh this just so we don't pollute uh the notebook too much this is a nice way of in Python obtaining the highest ranking pair so we're basically calling the Max on this dictionary stats and this will return the maximum key and then the question is how does it rank keys so you can provide it with a function that ranks keys and that function is just stats. getet uh stats. getet would basically return the value and so we're ranking by the value and getting the maximum key so it's 101 comma 32 as we saw now to actually merge 10132 um this is the function that I wrote but again there are many different versions of it so we're going to take a list of IDs and the the pair that we want to replace and that pair will be replaced with the new index idx so iterating through IDs if we find the pair swap it out for idx so we create this new list and then we start at zero and then we go through this entire list sequentially from left to right and here we are checking for equality at the current position with the pair um so here we are checking that the pair matches now here is a bit of a tricky condition that you have to append if you're trying to be careful and that is that um you don't want this here to be out of Bounds at the very last position when you're on the rightmost element of this list otherwise this would uh give you an autof bounds error so we have to make sure that we're not at the very very last element so uh this would be false for that so if we find a match we append to this new list that replacement index and we increment the position by two so we skip over that entire pair but otherwise if we we haven't found a matching pair we just sort of copy over the um element at that position and increment by one then return this so here's a very small toy example if we have a list 566 791 and we want to replace the occurrences of 67 with 99 then calling this on that will give us what we're asking for so here the 67 is replaced with 99 so now I'm going to uncomment this for our actual use case where we want to take our tokens we want to take the top pair here and replace it with 256 to get tokens to if we run this we get the following so recall that previously we had a length 616 in this list and now we have a length 596 right so this decreased by 20 which makes sense because there are 20 occurrences moreover we can try to find 256 here and we see plenty of occurrences on off it and moreover just double check there should be no occurrence of 10132 so this is the original array plenty of them and in the second array there are no occurrences of 1032 so we've successfully merged this single pair and now we just uh iterate this so we are going to go over the sequence again find the most common pair and replace it so let me now write a y Loop that uses these functions to do this um sort of iteratively and how many times do we do it four well that's totally up to us as a hyper parameter the more um steps we take the larger will be our vocabulary and the shorter will be our sequence and there is some sweet spot that we usually find works the best in practice and so this is kind of a hyperparameter and we tune it and we find good vocabulary sizes as an example gp4 currently uses roughly 100,000 tokens and um bpark that those are reasonable numbers currently instead the are large language models so let me now write uh putting putting it all together and uh iterating these steps okay now before we dive into the Y loop I wanted to add one more cell here where I went to the block post and instead of grabbing just the first paragraph or two I took the entire block post and I stretched it out in a single line and basically just using longer text will allow us to have more representative statistics for the bite Pairs and we'll just get a more sensible results out of it because it's longer text um so here we have the raw text we encode it into bytes using the utf8 encoding and then here as before we are just changing it into a list of integers in Python just so it's easier to work with instead of the raw byes objects and then this is the code that I came up with uh to actually do the merging in Loop these two functions here are identical to what we had above I only included them here just so that you have the point of reference here so uh these two are identical and then this is the new code that I added so the first first thing we want to do is we want to decide on the final vocabulary size that we want our tokenizer to have and as I mentioned this is a hyper parameter and you set it in some way depending on your best performance so let's say for us we're going to use 276 because that way we're going to be doing exactly 20 merges and uh 20 merges because we already have 256 tokens for the raw bytes and to reach 276 we have to do 20 merges uh to add 20 new tokens here uh this is uh one way in Python to just create a copy of a list so I'm taking the tokens list and by wrapping it in a list python will construct a new list of all the individual elements so this is just a copy operation then here I'm creating a merges uh dictionary so this merges dictionary is going to maintain basically the child one child two mapping to a new uh token and so what we're going to be building up here is a binary tree of merges but actually it's not exactly a tree because a tree would have a single root node with a bunch of leaves for us we're starting with the leaves on the bottom which are the individual bites those are the starting 256 tokens and then we're starting to like merge two of them at a time and so it's not a tree it's more like a forest um uh as we merge these elements so for 20 merges we're going to find the most commonly occurring pair we're going to Mint a new token integer for it so I here will start at zero so we'll going to start at 256 we're going to print that we're merging it and we're going to replace all of the occurrences of that pair with the new new lied token and we're going to record that this pair of integers merged into this new integer so running this gives us the following output so we did 20 merges and for example the first merge was exactly as before the 10132 um tokens merging into a new token 2556 now keep in mind that the individual uh tokens 101 and 32 can still occur in the sequence after merging it's only when they occur exactly consecutively that that becomes 256 now um and in particular the other thing to notice here is that the token 256 which is the newly minted token is also eligible for merging so here on the bottom the 20th merge was a merge of 25 and 259 becoming 275 so every time we replace these tokens they become eligible for merging in the next round of data ration so that's why we're building up a small sort of binary Forest instead of a single individual tree one thing we can take a look at as well is we can take a look at the compression ratio that we've achieved so in particular we started off with this tokens list um so we started off with 24,000 bytes and after merging 20 times uh we now have only 19,000 um tokens and so therefore the compression ratio simply just dividing the two is roughly 1.27 so that's the amount of compression we were able to achieve of this text with only 20 merges um and of course the more vocabulary elements you add uh the greater the compression ratio here would be finally so that's kind of like um the training of the tokenizer if you will now 1 Point I wanted to make is that and maybe this is a diagram that can help um kind of illustrate is that tokenizer is a completely separate object from the large language model itself so everything in this lecture we're not really touching the llm itself uh we're just training the tokenizer this is a completely separate pre-processing stage usually so the tokenizer will have its own training set just like a large language model has a potentially different training set so the tokenizer has a training set of documents on which you're going to train the tokenizer and then and um we're performing The Bite pair encoding algorithm as we saw above to train the vocabulary of this tokenizer so it has its own training set it is a pre-processing stage that you would run a single time in the beginning um and the tokenizer is trained using bipar coding algorithm once you have the tokenizer once it's trained and you have the vocabulary and you have the merges uh we can do both encoding and decoding so these two arrows here so the tokenizer is a translation layer between raw text which is as we saw the sequence of Unicode code points it can take raw text and turn it into a token sequence and vice versa it can take a token sequence and translate it back into raw text so now that we have trained uh tokenizer and we have these merges we are going to turn to how we can do the encoding and the decoding step if you give me text here are the tokens and vice versa if you give me tokens here's the text once we have that we can translate between these two Realms and then the language model is going to be trained as a step two afterwards and typically in a in a sort of a state-of-the-art application you might take all of your training data for the language model and you might run it through the tokenizer and sort of translate everything into a massive token sequence and then you can throw away the raw text you're just left with the tokens themselves and those are stored on disk and that is what the large language model is actually reading when it's training on them so this one approach that you can take as a single massive pre-processing step a stage um so yeah basically I think the most important thing I want to get across is that this is completely separate stage it usually has its own entire uh training set you may want to have those training sets be different between the tokenizer and the logge language model so for example when you're training the tokenizer as I mentioned we don't just care about the performance of English text we care about uh multi many different languages and we also care about code or not code so you may want to look into different kinds of mixtures of different kinds of languages and different amounts of code and things like that because the amount of different language that you have in your tokenizer training set will determine how many merges of it there will be and therefore that determines the density with which uh this type of data is um sort of has in the token space and so roughly speaking intuitively if you add some amount of data like say you have a ton of Japanese data in your uh tokenizer training set then that means that more Japanese tokens will get merged and therefore Japanese will have shorter sequences uh and that's going to be beneficial for the large language model which has a finite context length on which it can work on in in the token space uh so hopefully that makes sense so we're now going to turn to encoding and decoding now that we have trained a tokenizer so we have our merges and now how do we do encoding and decoding okay so let's begin with decoding which is this Arrow over here so given a token sequence let's go through the tokenizer to get back a python string object so the raw text so this is the function that we' like to implement um we're given the list of integers and we want to return a python string if you'd like uh try to implement this function yourself it's a fun exercise otherwise I'm going to start uh pasting in my own solution so there are many different ways to do it um here's one way I will create an uh kind of pre-processing variable that I will call vocab and vocab is a mapping or a dictionary in Python for from the token uh ID to the bytes object for that token so we begin with the raw bytes for tokens from 0 to 255 and then we go in order of all the merges and we sort of uh populate this vocab list by doing an addition here so this is the basically the bytes representation of the first child followed by the second one and remember these are bytes objects so this addition here is an addition of two bytes objects just concatenation so that's what we get here one tricky thing to be careful with by the way is that I'm iterating a dictionary in Python using a DOT items and uh it really matters that this runs in the order in which we inserted items into the merous dictionary luckily starting with python 3.7 this is guaranteed to be the case but before python 3.7 this iteration may have been out of order with respect to how we inserted elements into merges and this may not have worked but we are using an um modern python so we're okay and then here uh given the IDS the first thing we're going to do is get the tokens so the way I implemented this here is I'm taking I'm iterating over all the IDS I'm using vocap to look up their bytes and then here this is one way in Python to concatenate all these bytes together to create our tokens and then these tokens here at this point are raw bytes so I have to decode using UTF F now back into python strings so previously we called that encode on a string object to get the bytes and now we're doing it Opposite we're taking the bytes and calling a decode on the bytes object to get a string in Python and then we can return text so um this is how we can do it now this actually has a um issue um in the way I implemented it and this could actually throw an error so try to think figure out why this code could actually result in an error if we plug in um uh some sequence of IDs that is unlucky so let me demonstrate the issue when I try to decode just something like 97 I am going to get letter A here back so nothing too crazy happening but when I try to decode 128 as a single element the token 128 is what in string or in Python object uni Cod decoder utfa can't Decode by um 0x8 which is this in HEX in position zero invalid start bite what does that mean well to understand what this means we have to go back to our utf8 page uh that I briefly showed earlier and this is Wikipedia utf8 and basically there's a specific schema that utfa bytes take so in particular if you have a multi-te object for some of the Unicode characters they have to have this special sort of envelope in how the encoding works and so what's happening here is that invalid start pite that's because 128 the binary representation of it is one followed by all zeros so we have one and then all zero and we see here that that doesn't conform to the format because one followed by all zero just doesn't fit any of these rules so to speak so it's an invalid start bite which is byte one this one must have a one following it and then a zero following it and then the content of your uni codee in x here so basically we don't um exactly follow the utf8 standard and this cannot be decoded and so the way to fix this um is to use this errors equals in bytes. decode function of python and by default errors is strict so we will throw an error if um it's not valid utf8 bytes encoding but there are many different things that you could put here on error handling this is the full list of all the errors that you can use and in particular instead of strict let's change it to replace and that will replace uh with this special marker this replacement character so errors equals replace and now we just get that character back so basically not every single by sequence is valid utf8 and if it happens that your large language model for example predicts your tokens in a bad manner then they might not fall into valid utf8 and then we won't be able to decode them so the standard practice is to basically uh use errors equals replace and this is what you will also find in the openai um code that they released as well but basically whenever you see um this kind of a character in your output in that case uh something went wrong and the LM output not was not valid uh sort of sequence of tokens okay and now we're going to go the other way so we are going to implement this Arrow right here where we are going to be given a string and we want to encode it into tokens so this is the signature of the function that we're interested in and um this should basically print a list of integers of the tokens so again uh try to maybe implement this yourself if you'd like a fun exercise uh and pause here otherwise I'm going to start putting in my solution so again there are many ways to do this so um this is one of the ways that sort of I came came up with so the first thing we're going to do is we are going to uh take our text encode it into utf8 to get the raw bytes and then as before we're going to call list on the bytes object to get a list of integers of those bytes so those are the starting tokens those are the raw bytes of our sequence but now of course according to the merges dictionary above and recall this was the merges some of the bytes may be merged according to this lookup in addition to that remember that the merges was built from top to bottom and this is sort of the order in which we inserted stuff into merges and so we prefer to do all these merges in the beginning before we do these merges later because um for example this merge over here relies on the 256 which got merged here so we have to go in the order from top to bottom sort of if we are going to be merging anything now we expect to be doing a few merges so we're going to be doing W true um and now we want to find a pair of byes that is consecutive that we are allowed to merge according to this in order to reuse some of the functionality that we've already written I'm going to reuse the function uh get stats so recall that get stats uh will give us the we'll basically count up how many times every single pair occurs in our sequence of tokens and return that as a dictionary and the dictionary was a mapping from all the different uh by pairs to the number of times that they occur right um at this point we don't actually care how many times they occur in the sequence we only care what the raw pairs are in that sequence and so I'm only going to be using basically the keys of the dictionary I only care about the set of possible merge candidates if that makes sense now we want to identify the pair that we're going to be merging at this stage of the loop so what do we want we want to find the pair or like the a key inside stats that has the lowest index in the merges uh dictionary because we want to do all the early merges before we work our way to the late merges so again there are many different ways to implement this but I'm going to do something a little bit fancy here so I'm going to be using the Min over an iterator in Python when you call Min on an iterator and stats here as a dictionary we're going to be iterating the keys of this dictionary in Python so we're looking at all the pairs inside stats um which are all the consecutive Pairs and we're going to be taking the consecutive pair inside tokens that has the minimum what the Min takes a key which gives us the function that is going to return a value over which we're going to do the Min and the one we care about is we're we care about taking merges and basically getting um that pairs index so basically for any pair inside stats we are going to be looking into merges at what index it has and we want to get the pair with the Min number so as an example if there's a pair 101 and 32 we definitely want to get that pair uh we want to identify it here and return it and pair would become 10132 if it occurs and the reason that I'm putting a float INF here as a fall back is that in the get function when we call uh when we basically consider a pair that doesn't occur in the merges then that pair is not eligible to be merged right so if in the token sequence there's some pair that is not a merging pair it cannot be merged then uh it doesn't actually occur here and it doesn't have an index and uh it cannot be merged which we will denote as float INF and the reason Infinity is nice here is because for sure we're guaranteed that it's not going to participate in the list of candidates when we do the men so uh so this is one way to do it so B basically long story short this Returns the most eligible merging candidate pair uh that occurs in the tokens now one thing to be careful with here is this uh function here might fail in the following way if there's nothing to merge then uh uh then there's nothing in merges um that satisfi that is satisfied anymore there's nothing to merge everything just returns float imps and then the pair I think will just become the very first element of stats um but this pair is not actually a mergeable pair it just becomes the first pair inside stats arbitrarily because all of these pairs evaluate to float in for the merging Criterion so basically it could be that this this doesn't look succeed because there's no more merging pairs so if this pair is not in merges that was returned then this is a signal for us that actually there was nothing to merge no single pair can be merged anymore in that case we will break out um nothing else can be merged you may come up with a different implementation by the way this is kind of like really trying hard in Python um but really we're just trying to find a pair that can be merged with the lowest index here now if we did find a pair that is inside merges with the lowest index then we can merge it so we're going to look into the merger dictionary for that pair to look up the index and we're going to now merge that into that index so we're going to do tokens equals and we're going to replace the original tokens we're going to be replacing the pair pair and we're going to be replacing it with index idx and this returns a new list of tokens where every occurrence of pair is replaced with idx so we're doing a merge and we're going to be continuing this until eventually nothing can be merged we'll come out here and we'll break out and here we just return tokens and so that that's the implementation I think so hopefully this runs okay cool um yeah and this looks uh reasonable so for example 32 is a space in asky so that's here um so this looks like it worked great okay so let's wrap up this section of the video at least I wanted to point out that this is not quite the right implementation just yet because we are leaving out a special case so in particular if uh we try to do this this would give us an error and the issue is that um if we only have a single character or an empty string then stats is empty and that causes an issue inside Min so one way to fight this is if L of tokens is at least two because if it's less than two it's just a single token or no tokens then let's just uh there's nothing to merge so we just return so that would fix uh that case Okay and then second I have a few test cases here for us as well so first let's make sure uh about or let's note the following if we take a string and we try to encode it and then decode it back you'd expect to get the same string back right is that true for all strings so I think uh so here it is the case and I think in general this is probably the case um but notice that going backwards is not is not you're not going to have an identity going backwards because as I mentioned us not all token sequences are valid utf8 uh sort of by streams and so so therefore you're some of them can't even be decodable um so this only goes in One Direction but for that one direction we can check uh here if we take the training text which is the text that we train to tokenizer around we can make sure that when we encode and decode we get the same thing back which is true and here I took some validation data so I went to I think this web page and I grabbed some text so this is text that the tokenizer has not seen and we can make sure that this also works um okay so that gives us some confidence that this was correctly implemented so those are the basics of the bite pair encoding algorithm we saw how we can uh take some training set train a tokenizer the parameters of this tokenizer really are just this dictionary of merges and that basically creates the little binary Forest on top of raw bites once we have this the merges table we can both encode and decode between raw text and token sequences so that's the the simplest setting of The tokenizer what we're going to do now though is we're going to look at some of the St the art lar language models and the kinds of tokenizers that they use and we're going to see that this picture complexifies very quickly so we're going to go through the details of this comp complexification one at a time so let's kick things off by looking at the GPD Series so in particular I have the gpt2 paper here um and this paper is from 2019 or so so 5 years ago and let's scroll down to input representation this is where they talk about the tokenizer that they're using for gpd2 now this is all fairly readable so I encourage you to pause and um read this yourself but this is where they motivate the use of the bite pair encoding algorithm on the bite level representation of utf8 encoding so this is where they motivate it and they talk about the vocabulary sizes and everything now everything here is exactly as we've covered it so far but things start to depart around here so what they mention is that they don't just apply the naive algorithm as we have done it and in particular here's a example suppose that you have common words like dog what will happen is that dog of course occurs very frequently in the text and it occurs right next to all kinds of punctuation as an example so doc dot dog exclamation mark dog question mark Etc and naively you might imagine that the BP algorithm could merge these to be single tokens and then you end up with lots of tokens that are just like dog with a slightly different punctuation and so it feels like you're clustering things that shouldn't be clustered you're combining kind of semantics with uation and this uh feels suboptimal and indeed they also say that this is suboptimal according to some of the experiments so what they want to do is they want to top down in a manual way enforce that some types of um characters should never be merged together um so they want to enforce these merging rules on top of the bite PA encoding algorithm so let's take a look um at their code and see how they actually enforce this and what kinds of mergy they actually do perform so I have to to tab open here for gpt2 under open AI on GitHub and when we go to Source there is an encoder thatp now I don't personally love that they call it encoder dopy because this is the tokenizer and the tokenizer can do both encode and decode uh so it feels kind of awkward to me that it's called encoder but that is the tokenizer and there's a lot going on here and we're going to step through it in detail at one point for now I just want to focus on this part here the create a rigix pattern here that looks very complicated and we're going to go through it in a bit uh but this is the core part that allows them to enforce rules uh for what parts of the text Will Never Be merged for sure now notice that re. compile here is a little bit misleading because we're not just doing import re which is the python re module we're doing import reex as re and reex is a python package that you can install P install r x and it's basically an extension of re so it's a bit more powerful re um so let's take a look at this pattern and what it's doing and why this is actually doing the separation that they are looking for okay so I've copy pasted the pattern here to our jupit notebook where we left off and let's take this pattern for a spin so in the exact same way that their code does we're going to call an re. findall for this pattern on any arbitrary string that we are interested so this is the string that we want to encode into tokens um to feed into n llm like gpt2 so what exactly is this doing well re. findall will take this pattern and try to match it against a string um the way this works is that you are going from left to right in the string and you're trying to match the pattern and R.F find all will get all the occurrences and organize them into a list now when you look at the um when you look at this pattern first of all notice that this is a raw string um and then these are three double quotes just to start the string so really the string itself this is the pattern itself right and notice that it's made up of a lot of ores so see these vertical bars those are ores in reg X and so you go from left to right in this pattern and try to match it against the string wherever you are so we have hello and we're going to try to match it well it's not apostrophe s it's not apostrophe t or any of these but it is an optional space followed by- P of uh sorry SL P of L one or more times what is/ P of L it is coming to some documentation that I found um there might be other sources as well uh SLP is a letter any kind of letter from any language and hello is made up of letters h e l Etc so optional space followed by a bunch of letters one or more letters is going to match hello but then the match ends because a white space is not a letter so from there on begins a new sort of attempt to match against the string again and starting in here we're going to skip over all of these again until we get to the exact same Point again and we see that there's an optional space this is the optional space followed by a bunch of letters one or more of them and so that matches so when we run this we get a list of two elements hello and then space world so how are you if we add more letters we would just get them like this now what is this doing and why is this important we are taking our string and instead of directly encoding it um for tokenization we are first splitting it up and when you actually step through the code and we'll do that in a bit more detail what really is doing on a high level is that it first splits your text into a list of texts just like this one and all these elements of this list are processed independently by the tokenizer and all of the results of that processing are simply concatenated so hello world oh I I missed how hello world how are you we have five elements of list all of these will independent independently go from text to a token sequence and then that token sequence is going to be concatenated it's all going to be joined up and roughly speaking what that does is you're only ever finding merges between the elements of this list so you can only ever consider merges within every one of these elements in individually and um after you've done all the possible merging for all of these elements individually the results of all that will be joined um by concatenation and so you are basically what what you're doing effectively is you are never going to be merging this e with this space because they are now parts of the separate elements of this list and so you are saying we are never going to merge eace um because we're breaking it up in this way so basically using this regx pattern to Chunk Up the text is just one way of enforcing that some merges are not to happen and we're going to go into more of this text and we'll see that what this is trying to do on a high level is we're trying to not merge across letters across numbers across punctuation and so on so let's see in more detail how that works so let's continue now we have/ P ofn if you go to the documentation SLP of n is any kind of numeric character in any script so it's numbers so we have an optional space followed by numbers and those would be separated out so letters and numbers are being separated so if I do Hello World 123 how are you then world will stop matching here because one is not a letter anymore but one is a number so this group will match for that and we'll get it as a separate entity uh let's see how these apostrophes work so here if we have um uh Slash V or I mean apostrophe V as an example then apostrophe here is not a letter or a number so hello will stop matching and then we will exactly match this with that so that will come out as a separate thing so why are they doing the apostrophes here honestly I think that these are just like very common apostrophes p uh that are used um typically I don't love that they've done this because uh let me show you what happens when you have uh some Unicode apostrophes like for example you can have if you have house then this will be separated out because of this matching but if you use the Unicode apostrophe like this then suddenly this does not work and so this apostrophe will actually become its own thing now and so so um it's basically hardcoded for this specific kind of apostrophe and uh otherwise they become completely separate tokens in addition to this you can go to the gpt2 docs and here when they Define the pattern they say should have added re. ignore case so BP merges can happen for capitalized versions of contractions so what they're pointing out is that you see how this is apostrophe and then lowercase letters well because they didn't do re. ignore case then then um these rules will not separate out the apostrophes if it's uppercase so house would be like this but if I did house if I'm uppercase then notice suddenly the apostrophe comes by itself so the tokenization will work differently in uppercase and lower case inconsistently separating out these apostrophes so it feels extremely gnarly and slightly gross um but that's that's how that works okay so let's come back after trying to match a bunch of apostrophe Expressions by the way the other issue here is that these are quite language specific probably so I don't know that all the languages for example use or don't use apostrophes but that would be inconsistently tokenized as a result then we try to match letters then we try to match numbers and then if that doesn't work we fall back to here and what this is saying is again optional space followed by something that is not a letter number or a space in one or more of that so what this is doing effectively is this is trying to match punctuation roughly speaking not letters and not numbers so this group will try to trigger for that so if I do something like this then these parts here are not letters or numbers but they will actually they are uh they will actually get caught here and so they become its own group so we've separated out the punctuation and finally this um this is also a little bit confusing so this is matching white space but this is using a negative look ahead assertion in regex so what this is doing is it's matching wh space up to but not including the last Whit space character why is this important um this is pretty subtle I think so you see how the white space is always included at the beginning of the word so um space r space u Etc suppose we have a lot of spaces here what's going to happen here is that these spaces up to not including the last character will get caught by this and what that will do is it will separate out the spaces up to but not including the last character so that the last character can come here and join with the um space you and the reason that's nice is because space you is the common token so if I didn't have these Extra Spaces here you would just have space you and if I add tokens if I add spaces we still have a space view but now we have all this extra white space so basically the GB to tokenizer really likes to have a space letters or numbers um and it it preens these spaces and this is just something that it is consistent about so that's what that is for and then finally we have all the the last fallback is um whites space characters uh so um that would be just um if that doesn't get caught then this thing will catch any trailing spaces and so on I wanted to show one more real world example here so if we have this string which is a piece of python code and then we try to split it up then this is the kind of output we get so you'll notice that the list has many elements here and that's because we are splitting up fairly often uh every time sort of a category changes um so there will never be any merges Within These elements and um that's what you are seeing here now you might think that in order to train the tokenizer uh open AI has used this to split up text into chunks and then run just a BP algorithm within all the chunks but that is not exactly what happened and the reason is the following notice that we have the spaces here uh those Spaces end up being entire elements but these spaces never actually end up being merged by by open Ai and the way you can tell is that if you copy paste the exact same chunk here into Tik token U Tik tokenizer you see that all the spaces are kept independent and they're all token 220 so I think opena at some point Point en Force some rule that these spaces would never be merged and so um there's some additional rules on top of just chunking and bpe that open ey is not uh clear about now the training code for the gpt2 tokenizer was never released so all we have is uh the code that I've already shown you but this code here that they've released is only the inference code for the tokens so this is not the training code you can't give it a piece of text and training tokenizer this is just the inference code which Tak takes the merges that we have up above and applies them to a new piece of text and so we don't know exactly how opening ey trained um train the tokenizer but it wasn't as simple as chunk it up and BP it uh whatever it was next I wanted to introduce you to the Tik token library from openai which is the official library for tokenization from openai so this is Tik token bip install P to Tik token and then um you can do the tokenization in inference this is again not training code this is only inference code for tokenization um I wanted to show you how you would use it quite simple and running this just gives us the gpt2 tokens or the GPT 4 tokens so this is the tokenizer use for GPT 4 and so in particular we see that the Whit space in gpt2 remains unmerged but in GPT 4 uh these Whit spaces merge as we also saw in this one where here they're all unmerged but if we go down to GPT 4 uh they become merged um now in the gp4 uh tokenizer they changed the regular expression that they use to Chunk Up text so the way to see this is that if you come to your the Tik token uh library and then you go to this file Tik token X openi public this is where sort of like the definition of all these different tokenizers that openi maintains is and so uh necessarily to do the inference they had to publish some of the details about the strings so this is the string that we already saw for gpt2 it is slightly different but it is actually equivalent uh to what we discussed here so this pattern that we discussed is equivalent to this pattern this one just executes a little bit faster so here you see a little bit of a slightly different definition but otherwise it's the same we're going to go into special tokens in a bit and then if you scroll down to CL 100k this is the GPT 4 tokenizer you see that the pattern has changed um and this is kind of like the main the major change in addition to a bunch of other special tokens which I'll go into in a bit again now some I'm not going to actually go into the full detail of the pattern change because honestly this is my numbing uh I would just advise that you pull out chat GPT and the regex documentation and just step through it but really the major changes are number one you see this eye here that means that the um case sensitivity this is case insensitive match and so the comment that we saw earlier on oh we should have used re. uppercase uh basically we're now going to be matching these apostrophe s apostrophe D apostrophe M Etc uh we're going to be matching them both in lowercase and in uppercase so that's fixed there's a bunch of different like handling of the whites space that I'm not going to go into the full details of and then one more thing here is you will notice that when they match the numbers they only match one to three numbers so so they will never merge numbers that are in low in more than three digits only up to three digits of numbers will ever be merged and uh that's one change that they made as well to prevent uh tokens that are very very long number sequences uh but again we don't really know why they do any of this stuff uh because none of this is documented and uh it's just we just get the pattern so um yeah it is what it is but those are some of the changes that gp4 has made and of course the vocabulary size went from roughly 50k to roughly 100K the next thing I would like to do very briefly is to take you through the gpt2 encoder dopy that openi has released uh this is the file that I already mentioned to you briefly now this file is uh fairly short and should be relatively understandable to you at this point um starting at the bottom here they are loading two files encoder Json and vocab bpe and they do some light processing on it and then they call this encoder object which is the tokenizer now if you'd like to inspect these two files which together constitute their saved tokenizer then you can do that with a piece of code like this um this is where you can download these two files and you can inspect them if you'd like and what you will find is that this encoder as they call it in their code is exactly equivalent to our vocab so remember here where we have this vocab object which allowed us us to decode very efficiently and basically it took us from the integer to the byes uh for that integer so our vocab is exactly their encoder and then their vocab bpe confusingly is actually are merges so their BP merges which is based on the data inside vocab bpe ends up being equivalent to our merges so uh basically they are saving and loading the two uh variables that for us are also critical the merges variable and the vocab variable using just these two variables you can represent a tokenizer and you can both do encoding and decoding once you've trained this tokenizer now the only thing that um is actually slightly confusing inside what opening ey does here is that in addition to this encoder and a decoder they also have something called a bite encoder and a bite decoder and this is actually unfortunately just kind of a spirous implementation detail and isn't actually deep or interesting in any way so I'm going to skip the discussion of it but what opening ey does here for reasons that I don't fully understand is that not only have they this tokenizer which can encode and decode but they have a whole separate layer here in addition that is used serially with the tokenizer and so you first do um bite encode and then encode and then you do decode and then bite decode so that's the loop and they are just stacked serial on top of each other and and it's not that interesting so I won't cover it and you can step through it if you'd like otherwise this file if you ignore the bite encoder and the bite decoder will be algorithmically very familiar with you and the meat of it here is the what they call bpe function and you should recognize this Loop here which is very similar to our own y Loop where they're trying to identify the Byram uh a pair that they should be merging next and then here just like we had they have a for Loop trying to merge this pair uh so they will go over all of the sequence and they will merge the pair whenever they find it and they keep repeating that until they run out of possible merges in the in the text so that's the meat of this file and uh there's an encode and a decode function just like we have implemented it so long story short what I want you to take away at this point is that unfortunately it's a little bit of a messy code that they have but algorithmically it is identical to what we've built up above and what we've built up above if you understand it is algorithmically what is necessary to actually build a BP to organizer train it and then both encode and decode the next topic I would like to turn to is that of special tokens so in addition to tokens that are coming from you know raw bytes and the BP merges we can insert all kinds of tokens that we are going to use to delimit different parts of the data or introduced to create a special structure of the token streams so in uh if you look at this encoder object from open AIS gpd2 right here we mentioned this is very similar to our vocab you'll notice that the length of this is 50257 and as I mentioned it's mapping uh and it's inverted from the mapping of our vocab our vocab goes from integer to string and they go the other way around for no amazing reason um but the thing to note here is that this the mapping table here is 50257 where does that number come from where what are the tokens as I mentioned there are 256 raw bite token tokens and then opena actually did 50,000 merges so those become the other tokens but this would have been 50256 so what is the 57th token and there is basically one special token and that one special token you can see is called end of text so this is a special token and it's the very last token and this token is used to delimit documents ments in the training set so when we're creating the training data we have all these documents and we tokenize them and we get a stream of tokens those tokens only range from Z to 50256 and then in between those documents we put special end of text token and we insert that token in between documents and we are using this as a signal to the language model that the document has ended and what follows is going to be unrelated to the document previously that said the language model has to learn this from data it it needs to learn that this token usually means that it should wipe its sort of memory of what came before and what came before this token is not actually informative to what comes next but we are expecting the language model to just like learn this but we're giving it the Special sort of the limiter of these documents we can go here to Tech tokenizer and um this the gpt2 tokenizer uh our code that we've been playing with before so we can add here right hello world world how are you and we're getting different tokens but now you can see what if what happens if I put end of text you see how until I finished it these are all different tokens end of text still set different tokens and now when I finish it suddenly we get token 50256 and the reason this works is because this didn't actually go through the bpe merges instead the code that actually outposted tokens has special case instructions for handling special tokens um we did not see these special instructions for handling special tokens in the encoder dopy it's absent there but if you go to Tech token Library which is uh implemented in Rust you will find all kinds of special case handling for these special tokens that you can register uh create adds to the vocabulary and then it looks for them and it uh whenever it sees these special tokens like this it will actually come in and swap in that special token so these things are outside of the typical algorithm of uh B PA en coding so these special tokens are used pervasively uh not just in uh basically base language modeling of predicting the next token in the sequence but especially when it gets to later to the fine tuning stage and all of the chat uh gbt sort of aspects of it uh because we don't just want to Del limit documents we want to delimit entire conversations between an assistant and a user so if I refresh this sck tokenizer page the default example that they have here is using not sort of base model encoders but ftuned model uh sort of tokenizers um so for example using the GPT 3.5 turbo scheme these here are all special tokens I am start I end Etc uh this is short for Imaginary mcore start by the way but you can see here that there's a sort of start and end of every single message and there can be many other other tokens lots of tokens um in use to delimit these conversations and kind of keep track of the flow of the messages here now we can go back to the Tik token library and here when you scroll to the bottom they talk about how you can extend tick token and I can you can create basically you can Fork uh the um CL 100K base tokenizers in gp4 and for example you can extend it by adding more special tokens and these are totally up to you you can come up with any arbitrary tokens and add them with the new ID afterwards and the tikken library will uh correctly swap them out uh when it sees this in the strings now we can also go back to this file which we've looked at previously and I mentioned that the gpt2 in Tik toen open I.P we have the vocabulary we have the pattern for splitting and then here we are registering the single special token in gpd2 which was the end of text token and we saw that it has this ID in GPT 4 when they defy this here you see that the pattern has changed as we've discussed but also the special tokens have changed in this tokenizer so we of course have the end of text just like in gpd2 but we also see three sorry four additional tokens here Thim prefix middle and suffix what is fim fim is short for fill in the middle and if you'd like to learn more about this idea it comes from this paper um and I'm not going to go into detail in this video it's beyond this video and then there's one additional uh serve token here so that's that encoding as well so it's very common basically to train a language model and then if you'd like uh you can add special tokens now when you add special tokens you of course have to um do some model surgery to the Transformer and all the parameters involved in that Transformer because you are basically adding an integer and you want to make sure that for example your embedding Matrix for the vocabulary tokens has to be extended by adding a row and typically this row would be initialized uh with small random numbers or something like that because we need to have a vector that now stands for that token in addition to that you have to go to the final layer of the Transformer and you have to make sure that that projection at the very end into the classifier uh is extended by one as well so basically there's some model surgery involved that you have to couple with the tokenization changes if you are going to add special tokens but this is a very common operation that people do especially if they'd like to fine tune the model for example taking it from a base model to a chat model like chat GPT okay so at this point you should have everything you need in order to build your own gp4 tokenizer now in the process of developing this lecture I've done that and I published the code under this repository MBP so MBP looks like this right now as I'm recording but uh the MBP repository will probably change quite a bit because I intend to continue working on it um in addition to the MBP repository I've published the this uh exercise progression that you can follow so if you go to exercise. MD here uh this is sort of me breaking up the task ahead of you into four steps that sort of uh build up to what can be a gp4 tokenizer and so feel free to follow these steps exactly and follow a little bit of the guidance that I've laid out here and anytime you feel stuck just reference the MBP repository here so either the tests could be useful or the MBP repository itself I try to keep the code fairly clean and understandable and so um feel free to reference it whenever um you get stuck uh in addition to that basically once you write it you should be able to reproduce this behavior from Tech token so getting the gb4 tokenizer you can take uh you can encode the string and you should get these tokens and then you can encode and decode the exact same string to recover it and in addition to all that you should be able to implement your own train function uh which Tik token Library does not provide it's it's again only inference code but you could write your own train MBP does it as well and that will allow you to train your own token vocabularies so here are some of the code inside M be mean bpe uh shows the token vocabularies that you might obtain so on the left uh here we have the GPT 4 merges uh so the first 256 are raw individual bytes and then here I am visualizing the merges that gp4 performed during its training so the very first merge that gp4 did was merge two spaces into a single token for you know two spaces and that is a token 256 and so this is the order in which things merged during gb4 training and this is the merge order that um we obtain in MBP by training a tokenizer and in this case I trained it on a Wikipedia page of Taylor Swift uh not because I'm a Swifty but because that is one of the longest um Wikipedia Pages apparently that's available but she is pretty cool and um what was I going to say yeah so you can compare these two uh vocabularies and so as an example um here GPT for merged I in to become in and we've done the exact same thing on this token 259 here space t becomes space t and that happened for us a little bit later as well so the difference here is again to my understanding only a difference of the training set so as an example because I see a lot of white space I supect that gp4 probably had a lot of python code in its training set I'm not sure uh for the tokenizer and uh here we see much less of that of course in the Wikipedia page so roughly speaking they look the same and they look the same because they're running the same algorithm and when you train your own you're probably going to get something similar depending on what you train it on okay so we are now going to move on from tick token and the way that open AI tokenizes its strings and we're going to discuss one more very commonly used library for working with tokenization inlm and that is sentence piece so sentence piece is very commonly used in language models because unlike Tik token it can do both training and inference and is quite efficient at both it supports a number of algorithms for training uh vocabularies but one of them is the B pair en coding algorithm that we've been looking at so it supports it now sentence piece is used both by llama and mistal series and many other models as well it is on GitHub under Google sentence piece and the big difference with sentence piece and we're going to look at example because this is kind of hard and subtle to explain is that they think different about the order of operations here so in the case of Tik token we first take our code points in the string we encode them using mutf to bytes and then we're merging bytes it's fairly straightforward for sentence piece um it works directly on the level of the code points themselves so so it looks at whatever code points are available in your training set and then it starts merging those code points and um the bpe is running on the level of code points and if you happen to run out of code points so there are maybe some rare uh code points that just don't come up too often and the Rarity is determined by this character coverage hyper parameter then these uh code points will either get mapped to a special unknown token like ank or if you have the bite foldback option turned on then that will take those rare Cod points it will encode them using utf8 and then the individual bytes of that encoding will be translated into tokens and there are these special bite tokens that basically get added to the vocabulary so it uses BP on on the code points and then it falls back to bytes for rare Cod points um and so that's kind of like difference personally I find the Tik token we significantly cleaner uh but it's kind of like a subtle but pretty major difference between the way they approach tokenization let's work with with a concrete example because otherwise this is kind of hard to um to get your head around so let's work with a concrete example this is how we can import sentence piece and then here we're going to take I think I took like the description of sentence piece and I just created like a little toy data set it really likes to have a file so I created a toy. txt file with this content now what's kind of a little bit crazy about sentence piece is that there's a ton of options and configurations and the reason this is so is because sentence piece has been around I think for a while and it really tries to handle a large diversity of things and um because it's been around I think it has quite a bit of accumulated historical baggage uh as well and so in particular there's like a ton of configuration arguments this is not even all of it you can go to here to see all the training options um and uh there's also quite useful documentation when you look at the raw Proto buff uh that is used to represent the trainer spec and so on um many of these options are irrelevant to us so maybe to point out one example Das Das shrinking Factor uh this shrinking factor is not used in the B pair en coding algorithm so this is just an argument that is irrelevant to us um it applies to a different training algorithm now what I tried to do here is I tried to set up sentence piece in a way that is very very similar as far as I can tell to maybe identical hopefully to the way that llama 2 was strained so the way they trained their own um their own tokenizer and the way I did this was basically you can take the tokenizer model file that meta released and you can um open it using the Proto protuff uh sort of file that you can generate and then you can inspect all the options and I tried to copy over all the options that looked relevant so here we set up the input it's raw text in this file here's going to be the output so it's going to be for talk 400. model and vocab we're saying that we're going to use the BP algorithm and we want to Bap size of 400 then there's a ton of configurations here for um for basically pre-processing and normalization rules as they're called normalization used to be very prevalent I would say before llms in natural language processing so in machine translation and uh text classification and so on you want to normalize and simplify the text and you want to turn it all lowercase and you want to remove all double whites space Etc and in language models we prefer not to do any of it or at least that is my preference as a deep learning person you want to not touch your data you want to keep the raw data as much as possible um in a raw form so you're basically trying to turn off a lot of this if you can the other thing that sentence piece does is that it has this concept of sentences so sentence piece it's back it's kind of like was developed I think early in the days where there was um an idea that they you're training a tokenizer on a bunch of independent sentences so it has a lot of like how many sentences you're going to train on what is the maximum sentence length um shuffling sentences and so for it sentences are kind of like the individual training examples but again in the context of llms I find that this is like a very spous and weird distinction like sentences are just like don't touch the raw data sentences happen to exist but in raw data sets there are a lot of like inet like what exactly is a sentence what isn't a sentence um and so I think like it's really hard to Define what an actual sentence is if you really like dig into it and there could be different concepts of it in different languages or something like that so why even introduce the concept it it doesn't honestly make sense to me I would just prefer to treat a file as a giant uh stream of bytes it has a lot of treatment around rare word characters and when I say word I mean code points we're going to come back to this in a second and it has a lot of other rules for um basically splitting digits splitting white space and numbers and how you deal with that so these are some kind of like merge rules so I think this is a little bit equivalent to tick token using the regular expression to split up categories there's like kind of equivalence of it if you squint T it in sentence piece where you can also for example split up split up the digits uh and uh so on there's a few more things here that I'll come back to in a bit and then there are some special tokens that you can indicate and it hardcodes the UN token the beginning of sentence end of sentence and a pad token um and the UN token must exist for my understanding and then some some things so we can train and when when I press train it's going to create this file talk 400. model and talk 400. wab I can then load the model file and I can inspect the vocabulary off it and so we trained vocab size 400 on this text here and these are the individual pieces the individual tokens that sentence piece will create so in the beginning we see that we have the an token uh with the ID zero then we have the beginning of sequence end of sequence one and two and then we said that the pad ID is negative 1 so we chose not to use it so there's no pad ID here then these are individual bite tokens so here we saw that bite fallback in llama was turned on so it's true so what follows are going to be the 256 bite tokens and these are their IDs and then at the bottom after the bite tokens come the merges and these are the parent nodes in the merges so we're not seeing the children we're just seeing the parents and their ID and then after the merges comes eventually the individual tokens and their IDs and so these are the individual tokens so these are the individual code Point tokens if you will and they come at the end so that is the ordering with which sentence piece sort of like represents its vocabularies it starts with special tokens then the bike tokens then the merge tokens and then the individual codo tokens and all these raw codepoint to tokens are the ones that it encountered in the training set so those individual code points are all the the entire set of code points that occurred here so those all get put in there and then those that are extremely rare as determined by character coverage so if a code Point occurred only a single time out of like a million um sentences or something like that then it would be ignored and it would not be added to our uh vocabulary once we have a vocabulary we can encode into IDs and we can um sort of get a list and then here I am also decoding the indiv idual tokens back into little pieces as they call it so let's take a look at what happened here hello space on so these are the token IDs we got back and when we look here uh a few things sort of uh jump to mind number one take a look at these characters the Korean characters of course were not part of the training set so sentence piece is encountering code points that it has not seen during training time and those code points do not have a token associated with them so suddenly these are un tokens unknown tokens but because bite fall back as true instead sentence piece falls back to bytes and so it takes this it encodes it with utf8 and then it uses these tokens to represent uh those bytes and that's what we are getting sort of here this is the utf8 uh encoding and in this shifted by three uh because of these um special tokens here that have IDs earlier on so that's what happened here now one more thing that um well first before I go on with respect to the bitef back let me remove bite foldback if this is false what's going to happen let's retrain so the first thing that happened is all the bite tokens disappeared right and now we just have the merges and we have a lot more merges now because we have a lot more space because we're not taking up space in the wab size uh with all the bytes and now if we encode this we get a zero so this entire string here suddenly there's no bitef back so this is unknown and unknown is an and so this is zero because the an token is token zero and you have to keep in mind that this would feed into your uh language model so what is a language model supposed to do when all kinds of different things that are unrecognized because they're rare just end up mapping into Unk it's not exactly the property that you want so that's why I think llama correctly uh used by fallback true uh because we definitely want to feed these um unknown or rare code points into the model and some uh some manner the next thing I want to show you is the following notice here when we are decoding all the individual tokens you see how spaces uh space here ends up being this um bold underline I'm not 100% sure by the way why sentence piece switches whites space into these bold underscore characters maybe it's for visualization I'm not 100% sure why that happens uh but notice this why do we have an extra space in the front of hello um what where is this coming from well it's coming from this option here um add dummy prefix is true and when you go to the documentation add D whites space at the beginning of text in order to treat World in world and hello world in the exact same way so what this is trying to do is the following if we go back to our tick tokenizer world as uh token by itself has a different ID than space world so we have this is 1917 but this is 14 Etc so these are two different tokens for the language model and the language model has to learn from data that they are actually kind of like a very similar concept so to the language model in the Tik token World um basically words in the beginning of sentences and words in the middle of sentences actually look completely different um and it has to learned that they are roughly the same so this add dami prefix is trying to fight that a little bit and the way that works is that it basically uh adds a dummy prefix so for as a as a part of pre-processing it will take the string and it will add a space it will do this and that's done in an effort to make this world and that world the same they will both be space world so that's one other kind of pre-processing option that is turned on and llama 2 also uh uses this option and that's I think everything that I want to say for my preview of sentence piece and how it is different um maybe here what I've done is I just uh put in the Raw protocol buffer representation basically of the tokenizer the too trained so feel free to sort of Step through this and if you would like uh your tokenization to look identical to that of the meta uh llama 2 then you would be copy pasting these settings as I tried to do up above and uh yeah that's I think that's it for this section I think my summary for sentence piece from all of this is number one I think that there's a lot of historical baggage in sentence piece a lot of Concepts that I think are slightly confusing and I think potentially um contain foot guns like this concept of a sentence and it's maximum length and stuff like that um otherwise it is fairly commonly used in the industry um because it is efficient and can do both training and inference uh it has a few quirks like for example un token must exist and the way the bite fallbacks are done and so on I don't find particularly elegant and unfortunately I have to say it's not very well documented so it took me a lot of time working with this myself um and just visualizing things and trying to really understand what is happening here because uh the documentation unfortunately is in my opion not not super amazing but it is a very nice repo that is available to you if you'd like to train your own tokenizer right now okay let me now switch gears again as we're starting to slowly wrap up here I want to revisit this issue in a bit more detail of how we should set the vocap size and what are some of the considerations around it so for this I'd like to go back to the model architecture that we developed in the last video when we built the GPT from scratch so this here was uh the file that we built in the previous video and we defined the Transformer model and and let's specifically look at Bap size and where it appears in this file so here we Define the voap size uh at this time it was 65 or something like that extremely small number so this will grow much larger you'll see that Bap size doesn't come up too much in most of these layers the only place that it comes up to is in exactly these two places here so when we Define the language model there's the token embedding table which is this two-dimensional array where the vocap size is basically the number of rows and uh each vocabulary element each token has a vector that we're going to train using back propagation that Vector is of size and embed which is number of channels in the Transformer and basically as voap size increases this embedding table as I mentioned earlier is going to also grow we're going to be adding rows in addition to that at the end of the Transformer there's this LM head layer which is a linear layer and you'll notice that that layer is used at the very end to produce the logits uh which become the probabilities for the next token in sequence and so intuitively we're trying to produce a probability for every single token that might come next at every point in time of that Transformer and if we have more and more tokens we need to produce more and more probabilities so every single token is going to introduce an additional dot product that we have to do here in this linear layer for this final layer in a Transformer so why can't vocap size be infinite why can't we grow to Infinity well number one your token embedding table is going to grow uh your linear layer is going to grow so we're going to be doing a lot more computation here because this LM head layer will become more computational expensive number two because we have more parameters we could be worried that we are going to be under trining some of these parameters so intuitively if you have a very large vocabulary size say we have a million uh tokens then every one of these tokens is going to come up more and more rarely in the training data because there's a lot more other tokens all over the place and so we're going to be seeing fewer and fewer examples uh for each individual token and you might be worried that basically the vectors associated with every token will be undertrained as a result because they just don't come up too often and they don't participate in the forward backward pass in addition to that as your vocab size grows you're going to start shrinking your sequences a lot right and that's really nice because that means that we're going to be attending to more and more text so that's nice but also you might be worrying that two large of chunks are being squished into single tokens and so the model just doesn't have as much of time to think per sort of um some number of characters in the text or you can think about it that way right so basically we're squishing too much information into a single token and then the forward pass of the Transformer is not enough to actually process that information appropr",
    "commentLink": "https://news.ycombinator.com/item?id=39443965",
    "commentBody": "Let's Build the GPT Tokenizer [video] (youtube.com)547 points by davidbarker 16 hours agohidepastfavorite33 comments seanbethard 5 minutes agoIt’s pretty wild how little discussion there's been about the core feature of these models. It's as if this aspect of their development has been solved. Basically all NLP publications today take these BPE tokens as a starting point and if they are mentioned at all they’re mentioned in passing. https://blog.seanbethard.net/meanings-are-tiktokens-in-space reply threesevenths 15 hours agoprevAndrej's video on building GPT nano is an excellent tutorial of all of the steps involved in a modern LLM. reply xdennis 12 hours agoparenthttps://www.youtube.com/watch?v=kCc8FmEb1nY reply pests 6 hours agorootparentHis earlier videos on micrograd and makemore are a gold mine as well. reply mrtksn 15 hours agoprevI can't recommend enough the whole series, zero to hero: https://karpathy.ai/zero-to-hero.html No metaphors trying explain \"complex\" ideas, making them scary and seem overly complex. Instead, hands on implementations with analogy explainers where you can actually understand the ideas and see how simple it is. Steeper learning curve at first but it is much more satisfying and you actually earn the ability to reason about this stuff instead of writing over the top influencer BS. reply yen223 9 hours agoparentOne thing I like about that zero-to-hero series is how he almost never handwave over seemingly minor details. Definitely recommend watching those videos and doing the exercises, if you have any interest in how LLMs work. reply MPSimmons 14 hours agoparentprevThanks for this link - I have some free time coming up, and this seems like a great use of it! reply mynameisure 4 hours agoprevA noob question? Do you all intend to work on LLM’s or watching the content for the curious mind.I am asking how anyone like me as a software generalist can make use of this amazing content.Anyone with insights on how to transition from a generalist backend engineer to an AI engineer ? Or its a niche and the only path is the route of PHD … reply brainless 2 hours agoparentI was not really interested in LLMs till a month back. I had an earlier product where I wanted a no-code app for business insights on any data source. Plug in MySQL, PostgreSQL, APIs like Stripe, Salesforce, Shopify, even CSV files and it would be able to generate queries from user's GUI interactions. Like Airtable but for own data sources. I was generating SQLs including JOINs, or HTTPS API calls. Then I abandoned it in 2021. This year, it struck me that LLMs would be great to infer business insights from the schema. I could create reports and dashboards automatically, surface critical action points straight from the schema/data and users chatting with the app. So for the last couple weeks, I have been building it, running test on LLMs (CodeLlama, Zephyr, Mistral, Llama 2, Claude and ChatGPT). The results are quite good. There is a lot of tech that I need to handle: schema analysis, SQL or API calls, and the whole UI. But without LLMs, there was no clear way for me to infer business insights from schema + user chats. To me, this is not a niche anymore now that I have found a problem I wanted to tackle already. reply lb4r 2 hours agoparentprevSpeaking for myself, and except for just being curious, it's mostly for similar reasons as to why you'd want to read, for example, CLRS, even though you'll probably never implement an algorithm like that in a real production environment yourself. It's not so much about learning how, but rather why, because it'll help you answer your why's in the future (not that the how can't also be important, of course). reply elbear 3 hours agoparentprevJust a guess, but understanding how LLMs are built may also help you if you want to fine-tune a model. Someone who knows more may confirm or contradict this. reply timzaman 4 hours agoprevThe best thing is that I know Andrej reads all these comments. Hi Andrej! This is your calling. Miss you though! reply theptrk 5 hours agoprevThere should be awards for this type of content. Andrew Ng series and Karpathy series as first inductees to the hall of fame. reply albert_e 5 hours agoprevHis video on Backpropagation was a revelation to me : https://www.youtube.com/watch?v=q8SA3rM6ckI reply 098799 1 hour agoprevProbably more coming soon given he just left openai to pursue other things. reply sabareesh 15 hours agoprevEven if you pay it is hard to get such a high quality content! reply progbits 14 hours agoparentI've been learning a few new CS things recently and honestly I mostly find inverse correlation between cost and quality. There are books from oreilly and paid MOOC courses that are just padded with lots of unnecessary text or silly \"concept definition\" quizzes to make them seem worth the price. And there are excellent free YT video lectures, free books or blog posts. Andrej's YT videos are one great example. https://course.fast.ai is another. reply GaneshSuriya 13 hours agorootparentIt's not only about the cost, though. There's an inverse correlation with the glossiness of the content as well. If the web page /content is too polished, they're most likely optimizing for wooing users. Unlike a lot of the examples I gave in the sibling comments. Where the optimization is only on the love for the topic being discussed reply rahimnathwani 12 hours agorootparentThere's an inverse correlation with the glossiness of the content as well. This is probably due to survivorship bias. Sites that have poor content and poor visual appeal (glossiness) never get on your radar. i.e. Berkson's Paradox: https://en.wikipedia.org/wiki/Berkson%27s_paradox reply danielmarkbruce 12 hours agorootparentprevThere are some extremely good CS textbooks which cost money. That being said, many good ML/AI texts are free. But it's not easy reading. reply thfuran 13 hours agorootparentprev>And there are excellent free YT video lectures, free books or blog posts. There's also a tremendous amount of extremely low quality YouTube and blog content. reply progbits 13 hours agorootparentSure. I don't claim the free content is all good. But from my limited sample size, the best free content is better than the best paid content. reply simmanian 14 hours agorootparentprevDo you have recommendations for other high quality courses teaching CS things? reply GaneshSuriya 13 hours agorootparent- operating system in three easy pieces (https://pages.cs.wisc.edu/~remzi/OSTEP) is incredible for learning OS internals - beej's networking guide is the best thing for network layer stuff https://beej.us/guide/ - explained from first principles great too https://explained-from-first-principles.com/ - pintos from Stanford https://web.stanford.edu/class/cs140/projects/pintos/pintos_... reply vb234 10 hours agorootparentWow. Thanks for sharing. I had no idea that Professor Remzi and his wife Andrea wrote a book on Operating Systems. I loved his class (took it almost 22 years ago.) Will have to check his book out. reply davidbarker 13 hours agorootparentprevI can highly recommend CS50 from Harvard (https://www.youtube.com/@cs50). Even after being involved in tech for 25+ years, I learnt a lot from just the first lecture alone. Disclosure: Professor Malan is a friend of mine, but I was a fan of CS50 long before that! reply codelobe 10 hours agorootparentprevReplying to bookmark(hoard) all the thread links later. Fellow hackers might also enjoy: https://www.nand2tetris.org/ reply Tomte 3 hours agorootparentprevnand2tetris: https://www.nand2tetris.org/ I like the book better than the online course. reply diimdeep 12 hours agorootparentprevBuild an 8-bit computer from scratch https://eater.net/8bit/ https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2... Andreas Kling. OS hacking: Making the system boot with 256MB RAM https://www.youtube.com/watch?v=rapB5s0W5uk MIT 6.006 Introduction to Algorithms, Spring 2020 https://www.youtube.com/playlist?list=PLUl4u3cNGP63EdVPNLG3T... MIT 6.824: Distributed Systems https://www.youtube.com/@6.824 MIT 6.172 Performance Engineering of Software Systems, Fall 2018 https://www.youtube.com/playlist?list=PLUl4u3cNGP63VIBQVWguX... CalTech cs124 Operating Systems https://duckduckgo.com/?t=ffab&q=caltech+cs124&ia=web try searching here at HN for recommendations https://hn.algolia.com reply nojvek 11 hours agorootparentThank you a ton for the links. reply 3abiton 14 hours agoparentprevHis previous video onLLM tramsformer foundation is extremely useful. reply ShamelessC 8 hours agoprevHad to double check my playback speed - he talks like a 1.25x playback speaker sounds. reply moffkalast 11 hours agoprev [–] > you see when it's a space egg, it's a single token I'm not sure if the crew of the Nostromo would agree ;) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text emphasizes the significance of tokenization in large language models such as GPT-2, showcasing its impact on model performance and language processing.",
      "Techniques like byte pair encoding are explained to underline the importance of efficient tokenization for enhanced model performance, while various tokenization algorithms and encoding methods like utf8 are explored.",
      "Special focus is given to handling special tokens, training tokenizer vocabularies, and overcoming challenges posed by large vocabulary sizes in Transformers."
    ],
    "commentSummary": [
      "The forum highlights the overlooked aspect of token usage in GPT models, suggesting tutorials and resources for understanding transformer models and AI engineering.",
      "Users share their experiences leveraging LLMs for different applications and recommend top-notch computer science learning materials.",
      "Mentions include Andrej Karpathy's videos and insights into shifting from traditional back-end engineering to the exciting field of AI engineering."
    ],
    "points": 547,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1708449630
  },
  {
    "id": 39440179,
    "title": "In Defense of Simple Architectures: The Case of $1.7B Company, Wave",
    "originLink": "https://danluu.com/simple-architectures/",
    "originBody": "Wave is a $1.7B company with 70 engineers1 whose product is a CRUD app that adds and subtracts numbers. In keeping with this, our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres. Starting with a simple architecture and solving problems in simple ways where possible has allowed us to scale to this size while engineers mostly focus on work that delivers value to users. Stackoverflow scaled up a monolith to good effect (2013 architecture / 2016 architecture), eventually getting acquired for $1.8B. If we look at traffic instead of market cap, Stackoverflow is among the top 100 highest traffic sites on the internet (for many other examples of valuable companies that were built on top of monoliths, see the replies to this Twitter thread. We don’t have a lot of web traffic because we’re a mobile app, but Alexa still puts our website in the top 75k even though our website is basically just a way for people to find the app and most people don’t even find the app through our website). There are some kinds of applications that have demands that would make a simple monolith on top of a boring database a non-starter but, for most kinds of applications, even at top-100 site levels of traffic, computers are fast enough that high-traffic apps can be served with simple architectures, which can generally be created more cheaply and easily than complex architectures. Despite the unreasonable effectiveness of simple architectures, most press goes to complex architectures. For example, at a recent generalist tech conference, there were six talks on how to build or deal with side effects of complex, microservice-based, architectures and zero on how one might build out a simple monolith. There were more talks on quantum computing (one) than talks on monoliths (zero). Larger conferences are similar; a recent enterprise-oriented conference in SF had a double-digit number of talks on dealing with the complexity of a sophisticated architecture and zero on how to build a simple monolith. Something that was striking to me the last time I attended that conference is how many attendees who worked at enterprises with low-scale applications that could’ve been built with simple architectures had copied the latest and greatest sophisticated techniques that are popular on the conference circuit and HN. Our architecture is so simple I’m not even going to bother with an architectural diagram. Instead, I’ll discuss a few boring things we do that help us keep things boring. We’re currently using boring, synchronous, Python, which means that our server processes block while waiting for I/O, like network requests. We previously tried Eventlet, an async framework that would, in theory, let us get more efficiency out of Python, but ran into so many bugs that we decided the CPU and latency cost of waiting for events wasn’t worth the operational pain we had to take on to deal with Eventlet issues. The are other well-known async frameworks for Python, but users of those at scale often also report significant fallout from using those frameworks at scale. Using synchronous Python is expensive, in the sense that we pay for CPU that does nothing but wait during network requests, but since we’re only handling billions of requests a month (for now), the cost of this is low even when using a slow language, like Python, and paying retail public cloud prices. The cost of our engineering team completely dominates the cost of the systems we operate2. Rather than take on the complexity of making our monolith async we farm out long-running tasks (that we don’t want responses to block on) to a queue. A place where we can’t be as boring as we’d like is with our on-prem datacenters. When we were operating solely in Senegal and Côte d'Ivoire, we operated fully in the cloud, but as we expand into Uganda (and more countries in the future), we’re having to split our backend and deploy on-prem to comply with local data residency laws and regulations. That's not exactly a simple operation, but as anyone who's done the same thing with a complex service-oriented architecture knows, this operation is much simpler than it would've been if we had a complex service-oriented architecture. Another area is with software we’ve had to build (instead of buy). When we started out, we strongly preferred buying software over building it because a team of only a few engineers can’t afford the time cost of building everything. That was the right choice at the time even though the “buy” option generally gives you tools that don’t work. In cases where vendors can’t be convinced to fix showstopping bugs that are critical blockers for us, it does make sense to build more of our own tools and maintain in-house expertise in more areas, in contradiction to the standard advice that a company should only choose to “build” in its core competency. Much of that complexity is complexity that we don’t want to take on, but in some product categories, even after fairly extensive research we haven’t found any vendor that seems likely to provide a product that works for us. To be fair to our vendors, the problem they’d need to solve to deliver a working solution to us is much more complex than the problem we need to solve since our vendors are taking on the complexity of solving a problem for every customer, whereas we only need to solve the problem for one customer, ourselves. A mistake we made in the first few months of operation that has some cost today was not carefully delimiting the boundaries of database transactions. In Wave’s codebase, the SQLAlchemy database session is a request-global variable; it implicitly begins a new database transaction any time a DB object’s attribute is accessed, and any function in Wave’s codebase can call commit on the session, causing it to commit all pending updates. This makes it difficult to control the time at which database updates occur, which increases our rate of subtle data-integrity bugs, as well as making it harder to lean on the database to build things like idempotency keys or a transactionally-staged job drain. It also increases our risk of accidentally holding open long-running database transactions, which can make schema migrations operationally difficult. Some choices that we’re unsure about (in that these are things we’re either thinking about changing, or would recommend to other teams starting from scratch to consider a different approach) were using RabbitMQ (for our purposes, Redis would probably work equally well as a task queue and just using Redis would reduce operational burden), using Celery (which is overcomplicated for our use case and has been implicated in several outages e.g. due to backwards compatibility issues during version upgrades), using SQLAlchemy (which makes it hard for developers to understand what database queries their code is going to emit, leading to various situations that are hard to debug and involve unnecessary operational pain, especially related to the above point about database transaction boundaries), and using Python (which was the right initial choice because of our founding CTO’s technical background, but its concurrency support, performance, and extensive dynamism make us question whether it’s the right choice for a large-scale backend codebase). None of these was a major mistake, and for some (e.g. Python) the downsides are minimal enough that it’s cheaper for us to continue to pay the increased maintenance burden than to invest in migrating to something theoretically better, but if we were starting a similar codebase from scratch today we’d think hard about whether they were the right choice. Some areas where we’re happy with our choices even though they may not sound like the simplest feasible solution is with our API, where we use GraphQL, with our transport protocols, where we had a custom protocol for a while, and our host management, where we use Kubernetes. For our transport protocols, we used to use a custom protocol that runs on top of UDP, with an SMS and USSD fallback, for the performance reasons described in this talk. With the rollout of HTTP/3, we’ve been able to replace our custom protocol with HTTP/3 and we generally only need USSD for events like the recent internet shutdowns in Mali. As for using GraphQL, we believe the pros outweigh the cons for us: Pros: Self-documentation of exact return type Code generation of exact return type leads to safer clients GraphiQL interactive explorer is a productivity win Our various apps (user app, support app, Wave agent app, etc.) can mostly share one API, reducing complexity Composable query language allows clients to fetch exactly the data they need in a single packet roundtrip without needing to build a large number of special-purpose endpoints Eliminates bikeshedding over what counts as a RESTful API Cons: GraphQL libraries weren’t great when we adopted GraphQL (the base Python library was a port of the Javascript one so not Pythonic, Graphene required a lot of boilerplate, and Apollo-Android produced very poorly optimized code) Default GQL encoding is redundant and we care a lot about limiting size because many of our customers have low bandwidth As for Kubernetes, we use Kubernetes because knew that, if the business was successful (which it has been) and we kept expanding, we’d eventually expand to countries that require us to operate our services in country. The exact regulations vary by country, but we’re already expanding into one major African market that requires we operate our “primary datacenter” in the country and there are others with regulations that, e.g., require us to be able to fail over to a datacenter in the country. An area where there’s unavoidable complexity for us is with telecom integrations. In theory, we would use a SaaS SMS provider for everything, but the major SaaS SMS provider doesn’t operate everywhere in Africa and the cost of using them everywhere would be prohibitive3. The earlier comment on how the compensation cost of engineers dominates the cost of our systems wouldn’t be true if we used a SaaS SMS provider for all of our SMS needs; the team that provides telecom integrations pays for itself many times over. By keeping our application architecture as simple as possible, we can spend our complexity (and headcount) budget in places where there’s complexity that it benefits our business to take on. Taking the idea of doing things as simply as possible unless there’s a strong reason to add complexity has allowed us to build a fairly large business with not all that many engineers despite running an African finance business, which is generally believed to be a tough business to get into, which we’ll discuss in a future post (one of our earliest and most helpful advisers, who gave us advice that was critical in Wave’s success, initially suggested that Wave was a bad business idea and the founders should pick another one because he foresaw so many potential difficulties). Thanks to Ben Kuhn, Sierra Rotimi-Williams, June Seif, Kamal Marhubi, Ruthie Byers, Lincoln Quirk, Calum Ball, John Hergenroeder, Bill Mill, Sophia Wisdom, and Finbarr Timbers for comments/corrections/discussion. If you want to compute a ratio, we had closer to 40 engineers when we last fundraised and were valued at $1.7B. [return] There are business models for which this wouldn't be true, e.g., if we were an ad-supported social media company, the level of traffic we'd need to support our company as it grows would be large enough that we'd incur a significant financial cost if we didn't spend a significant fraction of our engineering time on optimization and cost reduction work. But, as a company that charges real money for a significant fraction of interactions with an app, our computational load per unit of revenue is very low compared to a social media company and it's likely that this will be a minor concern for us until we're well over an order of magnitude larger than we are now; it's not even clear that this would be a major concern if we were two orders of magnitude larger, although it would definitely be a concern at three orders of magnitude growth. [return] Despite the classic advice about how one shouldn’t compete on price, we (among many other things) do compete on price and therefore must care about costs. We’ve driven down the cost of mobile money in Africa and our competitors have had to slash their prices to match our prices, which we view as a positive value for the world [return]",
    "commentLink": "https://news.ycombinator.com/item?id=39440179",
    "commentBody": "In Defense of Simple Architectures (2022) (danluu.com)547 points by Brajeshwar 22 hours agohidepastfavorite409 comments from-nibly 19 hours agoThis is what I tell engineers. Microservices aren't a performance strategy. They are a POTENTIAL cost saving strategy against performance. And an engineering coordination strategy. Theoretically If you have a monolith that can be scaled horizontally there isn't any difference between having 10 replicas of your monolith and having 5 replicas of two microservices with the same codebase. UNLESS you are trying to underscale part of your functionality. You can't underscale part of your app with a monolith. Your pipe has to be big enough for all of it. Generally speaking though if you are talking about 10 replicas of something there's very little money to be saved anywhere. Even then though the cost savings only start at large scales. You need to have a minimum of 3 replicas for resiliency. If those 3 replicas are too big for your scale then you are just wasting money. The place where I see any real world benefit for most companies is just engineering coordination. With a single repo for a monolith I can make 1 team own that repo and tell them it's their responsibility to keep it clean. In a shared monolith however 0 people own it because everyone owns it and the repo becomes a disaster faster than you can say \"we need enterprise caching\". reply wg0 17 hours agoparentYou can scale those things with libraries also. The very browser that you're using to read this comment is an example of it. FreeType, Hurfbaz, Pango, Cairo, Uniscribe, GDI, zlib this that and a deep deep dependency tree built by people who never have talked to each other directly other than the documentation of their libraries - works perfectly well. I assure you 98% of the companies have simpler and shallower code base than that of a modern A class browser. Microservices was a wrong turn in our industry for 98% of the use cases. reply Tainnor 13 hours agorootparent> Hurfbaz This made me chuckle. :) It's HarfBuzz. Yours sounds like a goblin. reply wg0 1 hour agorootparentI could't recall the correct name but originally, it is in Persian حرف باز and in that باز would be transliterated as Baz. But yes, you're right. reply scient 19 hours agoparentprevServices, or even microservices, are more of a strategy to allow teams to scale than services or products to scale. I think thats one of the biggest misconceptions for engineers. On the other end you have the monorepo crew, who are doing it for the same reasons. On your note about resiliency and scale - its always a waste of money until shit hits the fan. Then you really pay for it. reply camgunz 17 hours agorootparent> Services, or even microservices, are more of a strategy to allow teams to scale than services or products to scale. I've never really understood why you couldn't just break up your monolith into modules. So like if there's a \"payments\" section, why isn't that API stabilized? I think all the potential pitfalls (coupling, no commitment to compatibility) are there for monoliths and microservices, the difference is in the processes. For example, microservices export some kind of API over REST/GraphQL/gRPC which they can have SDKs for, they can version them, etc. Why can't you just define interfaces to modules within your monolith? You can generate API docs, you can version interfaces, you can make completely new versions, etc. I just feel like this would be a huge improvement: - It's so much more engineering work to build the service handler scaffolding (validation, serialization/deserialization, defining errors) - You avoid the runtime overhead of serialiation/deserialization and network latency - You don't need to build SDKs/generate protobufs/generate clients/etc. - You never have the problem of \"is anyone using this service?\" because you can use code coverage tools - Deployment is much, much simpler - You never have the problem of \"we have to support this old--sometimes broken--functionality because this old service we can't modify depends on it\". This is a really undersold point: maybe it's true that microservice architectures let engineers build things without regard for other teams, but they can't remove things without regard for other teams, and this dynamic is like a no limit credit card for tech debt. Do you keep that service around as it slowly accretes more and more code it can't delete? Do you fork a new service w/o the legacy code and watch your fleet of microservices grow ever larger? - You never have the problem of \"how do we update the version of Node on 50 microservices?\" reply lll-o-lll 13 hours agorootparent> I've never really understood why you couldn't just break up your monolith into modules You can! We used to do this! Some of us still do this! It is, however, much more difficult. Not difficult technically, but difficult because it requires discipline. The organisations I’ve worked at that have achieved this always had some form of dictator who could enforce the separation. Look at the work done by John Lakos (and various books), to see how well this can work. Bloomberg did it, so can you! Creating a network partition makes your system a distributed system. There are times you need this, but the tradeoff is at least an order of magnitude increase in complexity. These days we have a lot of tooling to help manage this complexity, but it’s still there. The combination of possible failure states is exponential. Having said all this, the micro service architecture does have the advantage of being an easy way to enforce modularity and does not require the strict discipline required in a monolith. For some companies, this might be the better tradeoff. reply default-kramer 8 hours agorootparent> easy way to enforce modularity and does not require the strict discipline required in a monolith In my experience, microservices require more discipline than monoliths. If you do a microservice architecture without discipline you end up with the \"distributed monolith\" pattern and now you have the worst of both worlds. reply Fanmade 2 hours agorootparentYes, I completely agree. If your team doesn't have the skills to use a proper architecture within a monolith, letting them loose on a distributed system will make things a lot worse. I've seen that happen multiple times. reply gmfawcett 9 hours agorootparentprev> does not require the strict discipline required in a monolith How so? If your microservices are in a monorepo, one dev can spread joy and disaster across the whole ecosystem. On the other hand, if your monolith is broken into libraries, each one in its own repo, a developer can only influence their part of the larger solution. Arguably, system modularity has little to do with the architecture, and much to do with access controls on the repositories and pipelines. reply lll-o-lll 4 hours agorootparent> Arguably, system modularity has little to do with the architecture, and much to do with access controls on the repositories and pipelines. Monoliths tend to be in large monolithic repos. Microservices tend to get their own repo. Microservices force an API layer (defined module interface) due to imposing a network boundary. Library boundaries do not, and can generally be subverted. I agree that modularity has nothing to do with the architecture, intrinsically, simply that people are pushed towards modularity when using microservices. reply camgunz 2 hours agorootparentPeople make this argument as though it's super easy to access stuff marked \"private\" in a code base--maybe this is kind of true in Python but it really isn't in JVM languages or Go--and as though it's impossible to write tightly coupled microservices. The problem generally isn't reaching into internal workings or coupling, the problem is that fixing it requires you to consider the dozens of microservices that depend on your old interface that you have no authority to update or facility to even discover. In a monolith you run a coverage tool. In microservices you hope you're doing your trace IDs/logging right, that all services using your service used it in the window you were checking, and you start having a bunch of meetings with the teams that control those services to coordinate the update. That's not what I think of when I think of modularity, and in practice what happens is your team forks a new version and hopes the old one eventually dies or that no one cares how many legacy microservices are running. reply Izkata 7 hours agorootparentprev> It is, however, much more difficult. Not difficult technically, but difficult because it requires discipline. Before that, people need to know it's even an option. Years ago when I showed a dev who had just switched teams how to do this with a feature they were partway through implementing (their original version had it threading through the rest of the codebase) it was like one of those \"mind blown\" images. He had never even considered this as a possibility before. reply Cthulhu_ 13 hours agorootparentprev> some form of dictator who could enforce the separation. Like a lead developer or architect? Gasp! I wonder if the microservices fad is so that there can be many captains on a ship. Of course, then you need some form of dictator to oversee the higher level architecture and inter-service whatnots... like an admiral. reply dalyons 10 hours agorootparentprevi agree that its possible. From what i've seen its probably harder though than just doing services. You are fighting against human nature, organizational incentives, etc. As soon as the discipline of the developers, or vigilance of the dictator lapses, it degenerates. reply Fanmade 1 hour agorootparentIt is really hard to read this for me. How can anyone think that it is harder to write a proper monolith than implementing a distributed architecture? If you just follow the SOLID principles, you're already 90% there. If your team doesn't have the knowledge (it's not just \"discipline\", because every proper developer should know that they will make it harder for everyone including themselves if they don't follow proper architecture) to write structured code, letting them loose on a distributed system will make things much, much worse. reply mjr00 17 hours agorootparentprev> You never have the problem of \"how do we update the version of Node on 50 microservices?\" And instead you have the problem of \"how do we update the version of Node on our 10 million LOC codebase?\" Which is, in my experience, an order of magnitude harder. Ease of upgrading the underlying platform versions of Node, Python, Java, etc is one of the biggest benefits of smaller, independent services. reply camgunz 12 hours agorootparent> And instead you have the problem of \"how do we update the version of Node on our 10 million LOC codebase?\" I think if you get to that scale everything is pretty hard. You'll have a hard time convincing me that it's any easier/harder to upgrade Node on 80 125K LOC microservices than a 10M LOC monolith. Both of those things feel like a big bag of barf. reply naasking 16 hours agorootparentprevUpgrading the platform also happens at least 10x less frequently, so that math doesn't necessarily work out in your favour though. reply mjr00 16 hours agorootparentIt's much easier to make smaller scope changes at higher frequency than it is to make large changes at lower frequency. This is the entire reason the software industry adopted CI/CD reply naasking 16 hours agorootparentI'm not sure that's measuring what you think. The CI pipeline is an incentive for a good test suite, and with a good test suite the frequency and scope of changes matters a lot less. CI/CD is also an incentive to keep domain-level scope changes small (scope creep tends to be a problem in software development) in order to minimize disruptions to the pipeline. These are all somewhat different problems than upgrading the platform you're running, which the test suite itself should cover. reply groby_b 7 hours agorootparentCI/CD is usually a component of DevOps, and any decent DevOps team will have DORA metrics. Time-to-fix, frequency of deploys are both core metrics, and mirror frequency and scope of changes. You want change often, and small. Yes, change failure rate is also measured, and that's why good test suites matter, but if you think frequency and scope of change don't matter for successful projects, you haven't looked at the data. That means frequently updating your dependencies against a small code base is much more useful (and painless) than occasional boil-the-ocean updates. (As always, excepting small-ish teams, because direct communication paths to everybody on the team can mitigate a lot of problems that are painful at scale) reply Tainnor 13 hours agorootparentprev> I've never really understood why you couldn't just break up your monolith into modules. I think part of it is that many just don't know how. Web developers deal with HTTP and APIs all the time, they understand this. But I suspect that a lot of people don't really understand (or want to understand) build systems, compilers, etc. deeply. \"I just want to press the green button so that it runs\". reply Cthulhu_ 13 hours agorootparentCounterpoint, most monoliths are built like that; I wonder if they think that pressing a green button is too easy, like, it HAS to be more complicated, we HAVE to be missing something. reply dns_snek 19 hours agorootparentprevHow do you square that with the fact that shit usually hits the fan precisely because of this complexity, not in spite of it? That's my observation & experience, anyway. Added bits of \"resiliency\" often add brand new, unexplored failure points that are just ticking time bombs waiting to bring the entire system down. reply Eridrus 14 hours agorootparentMicroservices almost always increase the amount of partial failures, but if used properly can reduce the amount of critical failures. You can certainly misapply the architecture, but you can also apply it well. It's unsurprising that most people make bad choices in a difficult domain. reply Tainnor 13 hours agorootparentFault tolerance doesn't necessarily require microservices (as in separate code bases) though, see Erlang. Or even something like Unison. But for some reason it seems that few people are working on making our programming languages and frameworks fault tolerant. reply Eridrus 11 hours agorootparentBecause path dependence is real so we're mostly building on top of a tower of shit. And as computers got faster, it became more reasonable to have huge amounts of overhead. Same reason that docker exists at all. reply bluGill 18 hours agorootparentprevNot adding that resiliency isn't the answer though - it just means known failures will get you. Is that better than the unknown failures because of your mitigation? I cannot answer that. I can tell you 100% that eventually a disk will fail. I can tell you 100% that eventually the power will go out. I can tell you 100% that even if you have a computer with redundant power supplies each connected to separate grids, eventually both power supplies will fail at the same time - it just will happen a lot less often than if you have a regular computer not on any redundant/backup power. I can tell you that network cables do break from time to time. I can tell you that buildings are vulnerable to earthquakes, fires, floods, tornadoes and other such disasters). I can tell you that software is not perfect and eventually crashes. I can tell you that upgrades are hard if any protocol changed. I can tell you there is a long list of other known disasters that I didn't list, but a little research will discover. I could look up the odds of the above. In turn this allows calculating the costs of each mitigation against the likely cost of not mitigating it - but this is only statistical you may decide something statistically cannot happen and it does anyway. What I cannot tell you is how much you should mitigate. There is a cost to each mitigation that need to be compared to the value. reply camgunz 17 hours agorootparentAbsolutely yeah, these things are hard enough to test in a controlled environment with a single app (e.g. FoundationDB) but practically impossible to test fully in a microservices architecture. It's so nice to have this complexity managed for you in the storage layer. reply troupe 17 hours agorootparentprev> How do you square that with the fact that shit usually hits the fan precisely because of this complexity The theoretical benefit may not be what most teams are going to experience. Usually the fact that microservices are seen as a solution to a problem that could more easily be solved in other much simpler ways, is a pretty good indication that any theoretical benefits are going to be lost through other poor decision making. reply danielovichdk 13 hours agorootparentprevMicroservices is more about organisation than it is about technology. And that is why developers have so much trouble getting it right. They can't without having the organisational fundamentals in place. It is simply not possible. The architectural constraints of microservices will show the organisational weaknesses in a much higher rate because of the pressure it puts on having the organisation be very strict about ownership, communication and autonomy. The takes a higher level of maturity as an organisation to enable the benefits of microservies, which is also why most organisations shouldn't even try. Stop all the technical nonsense because it won't solve the root cause of the matter. It's the organisation. Not the technology reply manicennui 16 hours agorootparentprevExcept that most people build microservices in a way that ignores the reality of cloud providers and the fact that they are building (more) distributed systems, and often end up with lower resiliency. reply upupupandaway 9 hours agoparentprev> You can't underscale part of your app with a monolith. Your pipe has to be big enough for all of it. Not necessarily. I owned one of Amazon’s most traffic-heavy services (3 million TPS). It was a monolith with roughly 20 APIs, all extremely high volume. To keep the monolith simple and be able to scale it up or down independently, the thousands of EC2 instances running it were separated in fleets, which serving a specific use case. Then we could for example scale the website fleet while keeping the payment fleet stable. The only catch is that teams needed to be allowed to call only the APIs representing the use case of that fleet (no calling payment-related APIs on the website fleet). Given the low number of APIs and basic ACL control, it was not a challenge. reply duped 18 hours agoparentprev> Microservices aren't a performance strategy Who thinks that more i/o and more frequent cold starts are more performant? Where I see micoservices as very useful is for elasticity and modularization. It's probably slower than a monolith at your scale but you don't want to fallover when loads start increasing and you need to scale horizontally. Microservices with autoscaling can make that very useful. But of course, updating services can be a nightmare. It's a game of tradeoffs. reply jakey_bakey 17 hours agoparentprevSurely one could split a shared monolith into many internal libraries and modules, facilitating ownership? reply troupe 17 hours agorootparentYes, but you are still dealing with situations where other teams are deploying code that you are responsible for. With microservices you can always say, \"our microservice wasn't deployed, so it is someone else's problem.\" But I think you are pointing out one of the reasons most places don't get benefits from microservices. If their culture doesn't let them do as you describe with a monolith, the overhead of microservices just bring in additional complexities to an already dysfunctional team/organization. reply manicennui 16 hours agorootparentprevIt's crazy to me how opposed to building libraries everyone seems these days. We use a fuckton of libraries, but the smallest division of software we'll build is a service. reply dalyons 10 hours agorootparentits not crazy to me... ive worked at a couple of places that decided to try to use libraries in a web services context. They were all disasters, and the libraries ended up being considered severe tech debt. The practical aspects of updating all the consumers of your library, and how the libraries interact with persistence stores end up being their undoing. reply manicennui 9 hours agorootparentHow do you square this with the fact that you likely used dozens of libraries from third parties? reply dalyons 5 hours agorootparentprobably 1000s hah. It's the speed of change. third party libraries dont change much, or fast - and you dont really want them to. The software you build internally at a product company with 500+ engineers changes much much faster. And you want it to. reply lmm 11 hours agorootparentprevIt's difficult to own a library end-to-end. Ownership needs to include deployment and runtime monitoring. reply roenxi 10 hours agoparentprev> The place where I see any real world benefit for most companies is just engineering coordination. With a single repo for a monolith I can make 1 team own that repo and tell them it's their responsibility to keep it clean. In a shared monolith however 0 people own it because everyone owns it and the repo becomes a disaster faster than you can say \"we need enterprise caching\". Microservices supposed to be so small that they are much smaller than a team's responsibility; so I don't think that is a factor. The bottleneck in a software engineering company is always leadership and social organisation (think Google - they tried to muscle in on any number of alternative billion dollar businesses and generally failed despite having excellent developers who assembled the required parts rapidly). Microservices won't save you from a a manger who doesn't think code ownership is important. Monoliths won't prevent such a manger from making people responsible for parts of the system. I've worked on monoliths with multiple teams involved. It seems natural that large internal modules start to develop and each team has their own areas. A little bit like an amoeba readying to split, but never actually going through with it. reply jmull 12 hours agoparentprev> In a shared monolith however 0 people own it because everyone owns it and the repo becomes a disaster faster than you can say \"we need enterprise caching\". * I give two teams one repo each and tell each one: \"This is your repo, keep it clean.\" -or- * I give two teams one folder each in a repo and tell each one: \"This is your folder, keep it clean.\" If you've got a repo or folder (or anything else) that no one is responsible for, that's a management problem, and micro services won't solve it. Repos (and folders) don't really have anything to do with organizing a complex system of software -- they are just containers whose logical organization should follow from the logical organization of the system. Microservices back you into position where when the code of one team to calls the component of another team, it has to be high-latency, fault-prone, low-granularity, config-heavy. Some stuff falls the way anyway, so no problem in those case, but why burn that limitation in to your software architecture from the start? Just so you don't have to assign teams responsibility for portions of a repo? reply leeoniya 12 hours agorootparenti think doing releases, deployments, and rollbacks is trickier in a monorepo. with multiple services/repos, each team can handle their own release cycles, on-call rotation, and only deal with the bits, tests, and failures they wrote. you lose some CPU/RAM/IO efficiency, but you gain autonomy and resilience, and faster time to resolution. https://blog.nelhage.com/post/efficiency-vs-resiliency/ e.g. at Grafana we're working through some of these decoupling-to-microservices challenges, because the SREs that deploy to our infra should not need to deal with investigating and rolling back a whole monolith due to some regression introduced by some specific team to a core datasource plugin, or frontend plugin/panel. the pain at scale is very real, and engineering hours are by far more expensive than the small efficiency loss you get by over-provisioning the metal a bit. reply jmull 11 hours agorootparentI didn't mean to say there are no reasons to have separate repos, just more responding to the post above mine. But I will note that almost all the benefits listed aren't specific to micro services. (Also, to me, it's flawed that a dependency can really push to production without coordination. At the least, dependent components should decide when to \"take\" a new version. There are lot of ways to manage that -- versioned endpoints, e.g., so that dependents can take the new when they are ready. But it's extra complexity that's tricky in practice. e.g., you really ought to semver the endpoints, and keep versions as long as there could be any dependents... but how many is that, and how do you know? From what I've seen, people don't do that and instead dependencies push new versions, ready or not, and just deal with the problems as breakages in production, which is pretty terrible.) reply otabdeveloper4 14 hours agoparentprev> Your pipe has to be big enough for all of it. What do you mean by \"pipe\" here? It's easier to share CPU and network bandwidth across monolith threads than it is across microservice instances. (In fact, that is the entire premise of virtualization - a VM host is basically a way to turn lots of disparate services into a monolith.) reply chasd00 16 hours agoparentprev> And an engineering coordination strategy. i've always felt the org chart defines the microservice architecture. It's a way to keep teams out of each other's hair. When you have the same dev working on more than one service then that's an indication you're headed for trouble. reply wesselbindt 15 hours agorootparentThat's not just a feeling, it's a direct consequence of Conway's law. You feel correctly. reply danenania 15 hours agoparentprevI mostly agree. I'd add though that isolation can be a legitimate reason for a microservice. For example, if you have some non-critical logic that potentially uses a lot of CPU, splitting that out can make sense to be sure it's not competing for CPU with your main service (and bringing it down in the pathological case). Similarly, if you have any critical endpoints that are read-only, splitting those out from the main service where writes occur can improve the reliability of those endpoints. reply camgunz 17 hours agoparentprev> Microservices aren't a performance strategy. They are a POTENTIAL cost saving strategy against performance. Yeah they were a way to handle databases that couldn't scale horizontally. You could move business logic out of the database/SQL and into Java/Python/TypeScript app servers you could spin up. Now that we have databases like BigQuery and CockroachDB we don't need to do this anymore. reply thesnide 10 hours agorootparentIf you plan to scale horizontally with a microservice strategy, you'll in a lot of pain. As if you have say 100x more customers, will you divide your application in 100x more services? As others said, microservices scale manpower, not performance. It only comes sometimes as a bonus. reply mhh__ 15 hours agoparentprevI view micro services as a risk mainly in a Conway sense rather than technology. Most companies can run on a VPS. reply nijave 19 hours agoparentprevMicroservices can help with performance by splitting off performance critical pieces and allowing you to rewrite in a different stack or language (Rust or go instead of Ruby or Python) But yeah, they also tend to explode complexity reply dns_snek 19 hours agorootparentAn important point that people seem to forget is that you don't need microservices to invoke performant native code, just a dynamic library and FFI support. The entire Python ecosystem is built around this idea. reply bunderbunder 19 hours agorootparentThis is why Python stole enterprise big data and machine learning from Java. It actually has a higher performance ceiling for certain specific situations because, almost uniquely among garbage collected high-level languages, it can call native code without marshaling or memory pinning. reply nijave 18 hours agorootparentprevYou don't need it but you can also explode your repo, test, and build complexity when it'd be easier to keep them isolated. For instance, you might not want to require all develops have a C tool chain installed with certain libraries for a tiny bit of performance optimized code that almost never gets updated. reply dns_snek 17 hours agorootparentI don't know, that seems like confusion of runtime and code organization boundaries. Adding a network boundary in production just to serve some code organization purpose during development seems completely unnecessary to me. For development purposes you could build and distribute binary artifacts just like you would for any other library. Developers who don't touch native code can just fetch the pre-built binaries corresponding to current commit hash (e.g. from CI artifacts). reply jen20 9 hours agorootparentprevOne would imagine that by 2024 there would be a solution for every dynamic language package manager for shipping pre-compiled native extensions, _and_ for safely allowing them to be opted out of in favour of native compilation - I don't use dynamic languages though so don't really know whether that has happened. My overriding memory of trying to do literally anything with Ruby was having Nokogiri fail to install because of some random missing C dependency on basically every machine... reply rqtwteye 18 hours agorootparentprevI am good with splitting off certain parts as services once there is a performance problem. But doing microservices from the start is just a ton of complexity for no benefit and most likely you'll get the service boundaries wrong to some degree so you still have to refactor when there is a need for performance. reply bluGill 18 hours agorootparentOften you can get your boundaries close enough. There will always be cases where if you knew then what you know now. However you don't need to be perfect, just close enough. Web apps have been around in all sizes for 30 years - there is a lot of culture knowledge. Do not prematurely pessimize just because we don't know what is perfect. I'm not saying microservices are the right answer. They are a useful tool that sometimes you need and sometimes you don't. You should have someone on your team with enough experience to get the decisions close enough up front. This isn't anything new. reply AlchemistCamp 16 hours agorootparentprevGithub scaled all the way to acquisition with Ruby on Rails, plus some C for a few performance critical modules and it was a monolith. It doesn’t take a microservice to allow rewriting hot paths in a lower level language. Pretty much everything has a C-based FFI. This is what makes Python useable for ML—the libraries are written in C/C++. reply danmaz74 12 hours agorootparentprevBut you don't need micro services for that. You can always split things when useful, the issue with microservices is the idea that you should split things also when not necessary. reply throwaway2037 19 hours agoprev> For example, at a recent generalist tech conference, there were six talks on how to build or deal with side effects of complex, microservice-based, architectures and zero on how one might build out a simple monolith. Queue my favourite talk about microservices: David Schmitz - 10 Tips for failing badly at Microservices [1] This guy has amazing delivery -- so dry and funny. He spends 45 minutes talking about all of his microservices mistakes! [1] https://www.youtube.com/watch?v=r8mtXJh3hzM reply corpMaverick 10 hours agoparentMicroservices are an organizational technology. They allow small teams to work independently from other teams. The most frequent problem that I see is that people design Microservices that are \"small as possible\" instead of \"Small enough than a small team can own them\". For example, I have seen teams of 5-7 developers owning 20 microservices which is crazy IMHO. I blame the pre-fix micro which is highly miss leading. Now, if you merge together these 20 tiny microservices doesn't make monolith. A monolith would be a service or application that is owned by multiple/many teams. The other important aspect is that microservices should be loosely coupled and what I see is highly coupled microservices. How do you know you have highly coupled microservices? I have seen a few things. - They share business logic and developers end up creating libraries with business logic which is very problematic. - They share access to the tables. - Changes in one often require changes in other microservices. - Integration and unit tests are not enough. You need E2E tests, but these are hard to write and brittle. reply devjab 18 hours agoparentprevMonoliths aren’t very useful in many organisations where you need to build and connect 300+ systems. They also stop having simple architecture if you try. Most architecture conferences and talks tend to focus more on the enterprise side of things, and really, why would you need full time software focused architects if you’re building something like stackoverflow. I do think things have gotten a little silly in many places with too much “building like we’re Netflix” because often your microservices can easily be what is essentially a bunch of containerised monoliths. I think the main issue is that your IT architecture has or should have) very little to do with tech and everything to do with your company culture and business processes. Sometimes you have a very homogeneous focus maybe even on a single product, in which case microservices only begin to matter when you’re Netflix. Many times your business will consist of tens-thousands of teams with very different focuses and needs, and in these cases you should just never do monoliths unless you want to end up with a technical debt that will hinder your business from performing well down the line. reply CharlieDigital 18 hours agorootparent> Monoliths aren’t very useful in many organisations where you need to build and connect 300+ systems Seems like the mistake is building 300+ systems instead of a handful of systems. A Google team published a paper on this last year: https://dl.acm.org/doi/10.1145/3593856.3595909 > When writing a distributed application, conventional wisdom says to split your application into separate services that can be rolled out independently. This approach is well-intentioned, but a microservices-based architecture like this often backfires, introducing challenges that counteract the benefits the architecture tries to achieve. Fundamentally, this is because microservices conflate logical boundaries (how code is written) with physical boundaries (how code is deployed). In this paper, we propose a different programming methodology that decouples the two in order to solve these challenges. With our approach, developers write their applications as logical monoliths, offload the decisions of how to distribute and run applications to an automated runtime, and deploy applications atomically. Our prototype implementation reduces application latency by up to 15× and reduces cost by up to 9× compared to the status quo. Worth the read. reply devjab 16 hours agorootparent> Seems like the mistake is building 300+ systems instead of a handful of systems. But that’s not what happens on enterprise organisations. 90% of those are bought “finished” products, which then aren’t actually finished and you can be most certain that almost none of them are capable of sharing any sort of data without help. Hell, sometimes you’ll even have 3 of the same system. You may think it’s silly, but it is what it is in non-tech enterprise where the IT department is viewed as a cost-center similar to HR but without the charisma and the fact that most managers think we do magic. Over a couple of decades I’ve never seen an organisation that wasn’t like this unless it was exclusively focused on doing software development, and even in a couple of those it’s the same old story because they only build what they sell and not their internal systems. One of the things I’m paid well to do is help transitions startups from their messy monoliths into something they can actually maintain. Often with extensive use of the cheaper external developers since the IT department is pure cost (yes it’s silly) and you just can’t do that unless you isolate software to specific teams and then set up a solid architecture for how data flows between systems. Not because you theoretically can’t, but because the teams you work with often barely know their own business processes. I currently know more about specific parts of EU energy tariffs than the dedicated financial team of ten people who work with nothing else, because I re-designed some of the tools they use and because they have absolutely no process documentation and a high (I’m not sure what it’s called in English, but they change employees all the time). Which is in all regards stupid, but it’s also the reality of sooo many places. Like, the company recently fired the only person who knows how HubSpot works for the organisation during down sizing… that’s the world you have to design systems for, and if you want it to have even a fraction of a chance to actually work for them, you need to build things as small and isolated as possible going all in on team topologies even if the business doesn’t necessarily understand what that is. Because if you don’t, you end up with just one person who knows how the HubSpot integrations and processes work. It’s typically the same with monoliths, they don’t have to be complicated messes that nobody knows how work… in theory… but then they are build and maintained by a range of variously skilled people over 5 years and suddenly you have teams who hook directly into the massive mess of a DB with their Excel sheets. And what not. reply SamuelAdams 15 hours agorootparent> a high (I’m not sure what it’s called in English, but they change employees all the time). To help you out, the word is attrition or turnover. Turnover would be more appropriate if the roles are refilled, attrition if the roles are never replaced. https://www.betterup.com/blog/employee-attrition reply hcarvalhoalves 12 hours agorootparentprevFull-circle back to mainframe programming model. reply CharlieDigital 10 hours agorootparentNot really; it's not monolithic compute. It's monolithic codebase, but deployment can be piecemeal. reply bluGill 18 hours agorootparentprevBuilding like NetFlix is better than random unguided architectures that result from not thinking. It might not be the best for your problem though. If you don't need thousands of servers, then the complexity that Netflix has to put into their architecture to support that may not be worth the cost. However if you do scale that far you will be glad you choose an architecture proven to scale that large. However I doubt Netflix has actually documented their architecture in enough detail that you could use it. Even if you hire Netflix architects they may not themselves know some important parts (they will of course know the parts they worked on) reply devjab 18 hours agorootparentI mostly use Netflix as somewhere you’ve reached a technical point where you need to spread horizontally. As StackOverflow you can scale rather far without doing so if your product isn’t streaming billions of gigabytes of video to the entire world through numerous platforms. So what I mean by it is that many of will never reach those technical requirements. Sorry that I wasn’t clear. I don’t disagree with what you say at all, but I do think you can very easily “over design” your IT landscape. Like we have a few Python services which aren’t build cleverly and run on docker containers without clever monitoring. But they’ve only failed once in 7 years and that was due to a hardware failure on a controller that died 5 years before it should’ve. reply bluGill 18 hours agorootparentThat is how I use Netflix or stackoverflow. Choosing either (despite how different they are!) is better than random unstructured building code with no thought to the whole system. reply renegade-otter 15 hours agorootparentprevIt's a very bold assumption that a team that cannot manage a monolith will somehow lay a robust groundwork that will be the foundation of a future Netflix-like architecture. By the way - Netflix started as a monolith, and so did most other big services that are still around. The rest faded away, crushed by the weight of complexity, trying to be \"like Netflix\". reply bluGill 14 hours agorootparentThere are much better options above copying someone else. Copy is better than letting anything happen, but you should do better. You should learn from Netflix, stackoverflow and the like - no need to remake the same mistakes they did - but your situation is different so copy isn't right either. reply bunderbunder 19 hours agoprevI think a lot of this comes from the additive cognitive bias, which is really deeply entrenched in most of our thinking. Here's a concrete example, stolen from JD Long's great NormConf talk, \"I'd have written a shorter solution but I didn't have the time\", which is itself drawing from \"People systematically overlook subtractive changes\" by Adams, et al., 2021: Give people a Lego construction consisting if a large, sturdy plinth with single small spire supporting a platform on top. The challenge is to get it to reliably support a brick. As is, it will collapse because that little spire is so flimsy. Ask participants to modify the structure so that it will support the brick. Incentivize simpler solutions by saying that each additional brick costs $0.50. Given just this information, people universally try to fix the problem by adding a bunch more bricks to overbuild the structure, and they spend a lot of energy trying to come up with clever ways to reduce the number of bricks they add. But then the research team makes one small tweak to the instructions: they explicitly point out that removing bricks doesn't cost anything. With that nudge, people are much more likely to hit on the best solution. reply perrygeo 18 hours agoparentIn software, it's clear why we don't prefer subtractive changes - complexity. If there's any chance that the code in question has non-local effects that can't be reasoned about at the call site, it's risky to make subtractive changes. Additive changes have the advantage of having a corresponding additive feature to test - you don't need to touch or even understand any of the existing complexity, just glue it on top. So the cost structure is likely inverted from the study you describe. Additive changes are (locally) cheap. Subtractive changes are potentially expensive. reply hiAndrewQuinn 18 hours agorootparentThis is a big factor in why Haskell is widely considered a really fun language to refactor in. The enforced purely functional paradigm makes adding nonlocal effects much more annoying, so later contributors can much more confidently reason about the state of their code (assuming you can find any later contributors in such a small community!). I don't know whether the Lisps have a similar \"fun to refactor\" quality to them, since while they are in the functional camp, they're totally different and more flexible beasts architecturally. reply packetlost 17 hours agorootparentIMO Lisps are not nice to refactor in that sense, but a dream to modify in general. In the case of Haskell and Rust, the compiler creates really tight and more importantly, global, feedback loops that have pretty good guarantees (obviously not perfect, but they're stronger than most everything else out there), while Lisp has basically no guarantees and forces you to execute code to know if will even work at all. This doesn't become apparent until you try to do large scale refactors on a codebase and suddenly you actually need good test coverage to have any confidence that the refactor went well vs in Rust or Haskell if it compiles there's already a pretty reasonable guarantee you didn't miss something. Fortunately, Lisp codebases IME tend to be smaller and use small, unlikely-to-need-to-be-modified macros and functions, so large refactors may be less common. reply jodrellblank 15 hours agoparentprevI'm not convinced that challenge shows what you say it shows. If someone set me a challenge to \"modify this structure to support a brick\" and the solution was \"take the structure away and put the brick on the ground\" I would feel like it was a stupid trick. \"Remove the spire and put the brick lower\" is less clear, but it's along those lines; \"make this church and tower reliably support a clock at the top where everyone can see it\", solution: \"take the tower away and put the clock on the church roof\", no, it's a Captain Kirk Kobayashi-Maru cheat where the solution to the \"engineering challenge\" is to meta-change-the-goal. Yes you might get to change the goal in business software development, but you also know that the goal is 'working solutions' not 'build up this existing structure to do X'. reply palata 19 hours agoprevI believe it is as easy to over-engineer as it is to under-engineer. Architecture is the art of doing \"just enough\"; it must be as simple as possible, but as complex as necessary. But that is hard to do, and it takes experience. And one thing that does not describe the IT industry well is \"experience\": don't most software developers have less than 5 years experience? You can read all the books you want and pass all the certifications you can like a modern Agile manager, but at the end of the day you will have the same problem they have: experience takes time. Inexperienced engineers throw sexy tech at everything, inexperienced manager throw bullshit Agile metrics at everything. Same fight. reply bjornsing 18 hours agoparent> Architecture is the art of doing \"just enough\"; it must be as simple as possible, but as complex as necessary. Or as Albert Einstein phrased it: “Everything should be made as simple as possible, but not simpler!” reply geodel 18 hours agoparentprevWell I have seen lot of people with 20 years of experience which is actually 2 year experience repeated 10 times. reply palata 17 hours agorootparentThat's not what I am saying. I am saying that you cannot make 20 years of experience in 2 years. You are saying that some people with 20 years of experience are worse architects than others. reply valenterry 16 hours agoparentprevWell summarized. Also, the area of experience matters. I myself for instance have become pretty good at designing systems and infrastructure both from scratch up to corporate level. However, that will not help me too much when it comes to mega-corp level (think: FAANG) and other things such as designing public opensource libraries. (libraries and systems/applications require very different tradeoffs). reply whiterknight 7 hours agoparentprevWhen was the last time you saw a system fail because it was under engineered? I don’t mean bad code. I mean something like a single file with 100k lines with structs and functions, or a MySQL instance serving millions of customers a day. reply IshKebab 16 hours agoparentprevIn my experience over-engineering is far less of a problem than badly engineering. I see way more total messes than \"enterprise\" engineering. reply gherkinnn 21 hours agoprevI'm going to agree with Dan Luu by asking where I can find more of these sane companies. I want to spend an honest 6h a working day improving a product (Terraform and Webpack are not the product) and spend the rest of my time tending to my garden. reply corpMaverick 20 hours agoparentI am with you. Complex architectures where you have to fight it even for simple changes is a recipe for burn out as you paddle and paddle and you are stuck in the same place. reply LispSporks22 3 hours agoparentprevI think about this almost daily while slogging away in hundreds of lambdas, working on the world’s slowest app server. I think maybe 10% of our effort rubs off as business value. Maybe less. reply zer00eyz 21 hours agoparentprevYou aren't looking for a job. You're looking for a lifestyle business. They are great. You will get time in the garden, that's awesome. Vacation someplace with no phones? Out of the question. Weekend in a 3rd world country with character and food and sketchy internet. Not gonna happen. You want to optimize for free time in the garden by all means you can do it, but you loose out in other places, you pick up other work (taxes, systems etc). Edit: Down vote away, I live this life now, my tomatoes are delicious and I make yogurt too! reply nickserv 20 hours agorootparentNot sure I understand your point. I do my work, doing development work and managing a small team. Still never work weekends (no phone, no email, no slack), spend time in my garden and just came back from a no-phone vacation. Salary is above the going rate where I live - the work is remote and salary is based on company headquarters. Taxes are in line with the country I live in. Not really seeing any downsides here, and as expected morale is quite good overall at work... But finding this company was extremely lucky/difficult. reply bluGill 19 hours agorootparentprevI learned long ago that I cannot write quality code for 8 hours per day. I need to have several hours of meetings every day just to ensure I don't overdo the coding and write bad code. Sure I can write code for 14 hours a day, but it will be bad code, sometimes even negative productivity as I introduce so many bugs. reply PeterisP 18 hours agorootparentprevRunning a business, even a \"lifestyle business\" is so substantially different from a job - requiring all kinds of very different tasks, and risk-taking profile - that it doesn't seem reasonable to assume that someone who's looking for such a job is actually looking to run some business. reply lovebes 18 hours agorootparentprevMay I get some more information on how you went into lifestyle business? Looking to get into that as well. reply zer00eyz 15 hours agorootparentI know several folks with niche business that pay various levels of their bills. Software for youth sports, photography, asset tracking, vendor tracking, niche issues in CC Processing, facets of insurance and billing (coding). Niche businesses happen all over the place, and finding one (for me) was a lot of trial and error, that niche business pays my \"bills\" and I do consulting work (sporadic, interesting and high value) to round it out (and keep me on my game). Dont think of it as a business right away. You're going to \"play\", you want to build them quickly, you want to host them cheaply, you want to toy with selling them. Your managing disappointment and failure, your learning lessons and keeping it FUN. The moment you start dreaming that it's going to \"make it big\" is the moment you have to reel yourself back to reality. If you can say \"what did I learn\" and have a list of things you got from it then it was a success. At some point you just find one that clicks and it grows. reply benreesman 14 hours agoprevThis one is a classic (instant classic), I can't say anything about this better than Dan did. What I can offer is a personal anecdote about what an amazing guy he is. I had been a fan of his blog for a while, and at one point I decided to just email him and offer to fly to wherever he was because I wanted to hear his thoughts on a number of topics. This was in maybe 2016, but even then I didn't expect someone who must get a zillion such emails to even reply, let alone invite me up to Seattle at my leisure! I think I had the flight booked on a Wednesday for a departure on Friday, for one night's stay, and Dan was profoundly generous with his time, we stayed up late into the night chatting and I learned a great deal, especially about the boundary between software and hardware (a topic on which he is a first-order expert with an uncommon gift for exposition). I had the great fortune of spending some real time with Dan not just once but twice! When I went to NYC to get involved with what is now the Reels ML group, he happened to also be in NYC, and I had the singular pleasure to speak with him at length on a number of occasions: each illuminating and more fun than you can have without a jet ski. Dan is a singularly rigorous thinker with the dry and pithy wit of a world-class comedian and a heart of gold, truly generous with his expertise and insight. I'm blessed to have met and worked with a number of world-class hackers, but few, few if any are such a joy to learn from. reply Shrezzing 21 hours agoprevI'd argue that this article could be read more like \"start as a monolith, and then move towards microservices when sensible\", except the penny hasn't fully dropped for the author that the sensible time for their organisation is right now. The company appears dogmatically locked into their idling python code, their SQL making unpredictable commits, and their SQL framework making it difficult to make schema migrations. This is a financial services company describing data-integrity bugs in their production platforms. reply wavemode 20 hours agoparentIn what way does \"moving towards microservices\" relate to solving \"data-integrity bugs\"? I would argue the opposite - the more distributed you make the system, the more subtle consistency bugs tend to creep in. reply DonnyV 20 hours agorootparentFor a financial services company they should be using a compiled language. Something like C# or Java or Rust or Go with Postgres. reply artimaeis 18 hours agorootparentHe's been talking about the problems/solutions around Wave for a while. Every time I can't help but think if they'd just started with C# or Java, maybe Go or Rust, they'd be in a better position. Here's what I see them as providing: - Simple async, no wasted CPU on I/O - Strong typing defaults - Reliable static analysis Half of the \"pros\" he lists for using GraphQL are just provided out of the box using ASP.NET Core with NSwag to document the endpoints. If they want to keep the client-side composability, create some OData endpoints and you've got it. > Self-documentation of exact return type > Code generation of exact return type leads to safer clients > Our various apps (user app, support app, Wave agent app, etc.) can mostly share one API, reducing complexity > Composable query language allows clients to fetch exactly the data they need in a single packet roundtrip without needing to build a large number of special-purpose endpoints Bias flag: I'm primarily a C#/.NET developer. I came up on python and ruby but have had more success in my region getting paid to work on .NET code. reply quaunaut 19 hours agorootparentprevWhy? What inherent advantage do those languages have with financial logic? reply DonnyV 18 hours agorootparentPython will let you use float division on integers. Compiled type languages won't do that. This would be solved by using a double data type. But python doesn't have it. reply quaunaut 16 hours agorootparentYou'd almost certainly just use a library dedicated to mitigating this problem though, of which there are many. reply notpachet 18 hours agorootparentprevBetter static analysis, for one. reply jakewins 27 minutes agorootparentWe have services in Python and C# at $dayjob, and my experience is that Python has mostly left C# behind here - what is something you can express in the C# type system that MyPy cannot statically check? Conversely, I find myself constrained by C#'s typesystem not having features now widely popular from Rust and Typescript and available in modern Python: const generics / literals, protocols/structural interfaces and matching over algebraic datatypes, to name a few examples. reply quaunaut 16 hours agorootparentprevAnd that helps in financial analysis how? reply notpachet 16 hours agorootparentI didn't say anything about financial analysis. But static typing is a good defense mechanism against type errors at runtime (I don't think anyone would argue against that). When you're running a financial services product, the cost of a type error in production can be dramatically higher than in non-financial code. Speaking from firsthand experience. reply IshKebab 16 hours agorootparentprevI don't think it's specific to financial services. He just meant that financial services is an area where you really don't want bugs! And one of the easiest ways to eliminate entire classes of bugs is to use static typing. reply jakewins 16 hours agoparentprevThe author is ex Microsoft, Google and was a senior staff eng at Twitter; i don’t know them but my experience with their blogging over the last decade is that they seem generally very well informed. To me it seems unlikely “the penny hasn’t dropped”, like you say. On the actual criticism you’re raising: In what concrete way would moving to micro services help with reducing data integrity bugs? In my experience I’d expect the exact opposite - micro services systems generally being harder to test and generally having more possible interleavings and partial failure paths. reply mousetree 20 hours agoparentprevThese are all specific problems that can be individually solved. Personally I don’t see how changing their architecture or programming language would solve those reply nijave 18 hours agorootparentIn addition, they risk introducing new problems they already solved with their current setup reply lstodd 19 hours agoparentprevWhere does the \"idling python code\" come from? If it blocks, it blocks, that's not \"idling\". And I doubt they are running a process per core. reply whiterknight 7 hours agoparentprevDaily reminder that when one process is blocked others can be working so nothing is “idling” on a web server properly configured. reply cityguy33 17 hours agoprevThe most sophisticated arch ever needed for any scale in my FAANG and F500 jobs are a ssl-supported load balancer, multiple app servers with thread pools, a sharded database, and message queues. Everything else is just a dressed up version of this reply nonethewiser 15 hours agoprevWow, this guy's patreon is something else. https://www.patreon.com/danluu $16/month \"Short-form posts\" $256/month \"Sponsor\" $2,048/month \"Major Sponsor\" $16,384/month \"Patron\" You certainly can't fault the guy for trying, although it does make him seem a bit crazy. And I'm aware of scenarios where raising prices doesn't have the negative effect you might expect. You never know what people might spend but I can't imagine this is a tempting proposition for anyone. Maybe it's all a ploy to get you to spend $16/month to see how many \"Sponsor\", \"Major\", and \"Patron\" level subscribers there are. reply neilk 13 hours agoparentIn this very comment page, there is a guy who flew Dan to a different city, just to hear this thoughts on random issues. Also, if you look at his other writings, he's often noted trends in how software developers are paid. And the paradox that with even small tweaks, he's sometimes saved his employers far more than he could have earned in ten lifetimes. He's a free agent now... so why not charge what his advice is actually worth? Dan is a very unusual person, in that he tries to do what is predicted by the data he has, rather than waiting for it to become conventional wisdom. reply 2d8a875f-39a2-4 20 hours agoprevI agree with the general sentiment that simple architectures are better and monoliths are mostly fine. But. I've dealt with way too many teams whose shit is falling over due to synchronous IO even at laughably low volumes. Don't do that if you can avoid it. \"Subtle data-integrity bugs\" are not something we should be discussing in a system of financial record. Avoiding them should have been designed in from the start. reply renegade-otter 15 hours agoparentSo they cannot get data integrity constraints done properly with a single database? Wait until they have to do seven. Also, sounds like not even proper indexes were in place, so database amateur hour. reply lelanthran 19 hours agoparentprev> shit is falling over due to synchronous IO even at laughably low volumes. Like? I ask because even synchronous IO let's you serve millions of requests per month on a cheap VPS. That's enough in the b2b space to keep a company of 1000 employees in business. reply default-kramer 7 hours agorootparentI had a similar thought when C# introduced async/await. \"Why all this complexity? What was wrong with good old fashioned blocking calls?\" I don't know the answer, but I would like to. I think it has to do with the limit on the number of processes/threads that the OS/framework can manage. Once you reach this limit, using async/await somehow allows the OS/framework to secretly use your (technically not) blocked threads to do some other work while they are (technically not) blocked. reply Tainnor 5 hours agorootparent1. Async allows you to parallelise tasks. If a request from a user needs to hit three logically independent endpoints, you don't need to do that in sequence, you can do them in parallel and thus the user will get a response much quicker. 2. OS threads can be expensive in the sense that they can block a lot of memory, to the point where you could run out of threads at some point. This is worse in some environments than in others. Apart from async, another solution for this is virtual / green threads (as in Erlang, Haskell, and much more recently, Java). 3. Some async implementations enable advanced structured concurrency patterns, such as cancellation, backpressure handling, etc. reply 2d8a875f-39a2-4 19 hours agorootparentprevUntil one of your IO destinations develops some latency. Or your workflow adds a few more sync IOs into each request. Or you suddenly run outta threads. Then even if you're only at millions per month you've probably got problems. reply lelanthran 18 hours agorootparent> Then even if you're only at millions per month you've probably got problems. Not in my experience. You may be using metrics for B2C websites which make $1 for each 1 million hits. B2B works a little differently: you're not putting everyone on the same box, for starters. I did some contract maintenance for a business recently (had no tech staff of their own, had contracted out their C# based appdev to someone else decades ago and just need some small changes now), and a busy internal app serving about 8000 employees was running just fine off a 4GB RAM VPS. Their spend is under $100/m to keep this up. No async anywhere. No performance problems either. So, sure, what you say makes sense if your business plan is \"make $1 of each 1 million visitors\". If you business plan is \"sell painkillers, not vitamins\" you need maybe 10k paying users to pay yourself a f/time salary. reply marginalia_nu 18 hours agorootparentprevTo be fair, a million requests per month is 20 requests per minute... reply lelanthran 18 hours agorootparent> To be fair, a million requests per month is 20 requests per minute... Which, in B2B, is insanely profitable. At 20 rqsts/min, for a paying customer paying you $200/m/user, those numbers are fantastic! I can only dream of having those numbers! reply marginalia_nu 18 hours agorootparentSure, but in terms of load, dealing with the requests, a single raspberry pi 2 will barely register that even if you deploy it with a CGI stack. reply philipbjorge 15 hours agorootparentprevRecently I worked on a project that was using synchronous IO in an async framework -- That tanked performance immediately and effectively meant that the application could service one request at a time while subsequent requests started queuing. (Agreed that synchronous IO can serve hundreds of requests per second with the right threading model) reply from-nibly 20 hours agoparentprevYeah I'm with this guy right up until that last statement. You should have 0 of these. Not an increasing rate of them. reply mr_tristan 17 hours agoprevI find that architecture should benefit the social structure of the engineering team, and there are limits. I work on one of these “simple architectures” at large scale… and it’s absolute hell. But then, the contributor count to this massive monorepo + “simple architecture” hell numbers in the thousands. Wave financial is only 350 people according to wikipedia - I doubt that’s 350 engineers. I know only of Google and Meta that can even operate with a massive monorepo, but I wouldn’t call their architecture “simple”. And even they do massive internal tooling investments - I mean, Google wrote their own version control system. So I tend to think “keep it simple until you push past Dunbar’s number, then reorganize around that”. Once stable social relationships break down, managing change at this scale becomes a weird combination of incredible rigidity and absolute chaos. You might make some stopgap utility and then a month later 15 other teams are using it. Or some other team wants to change something for their product and just submits a bunch of changes to your product with unforseen breakage. Or some “cost reduction effort” halves memory and available threads slowing down background processes. Keeping up with all this means managing hundreds of different threads of communication happening. It’s just too much and nobody can ever ask the question “what’s changed in the last week” because it would be a novel. This isn’t an argument for monoliths vs microservices, because I think that’s just the wrong perspective. It’s an argument to think about your social structure first, and I rarely see this discussed well. Most companies just spin up teams to make a thing and then don’t think about how these teams collaborate, and technical leadership never really questions how the architecture can supplement or block that collaboration until it’s a massive problem, at which point any change is incredibly expensive. reply mattbuilds 17 hours agoparentThe way I tend to look at it is to solve the problem you have. Don't start with a complicated architecture because \"well once we scale, we will need it\". That never works and it just adds complexity and increases costs. When you have a large org and the current situation is \"too simple\", that's when you invest in updating the architecture to meet the current needs. This also doesn't mean to not be forward thinking. You want the architecture to support growth that will more than likely happen, just keep the expectations in check. reply fl0ki 15 hours agorootparent> Don't start with a complicated architecture because \"well once we scale, we will need it\". > You want the architecture to support growth that will more than likely happen The problem is even very experienced people can disagree about what forms of complexity are worth it up-front and what forms are not. One might imagine that Google had a first generation MVP of a platform that hit scaling limits and then a second generation scaled infinitely forever. What actually happens is that any platform that lives long enough needs a new architecture every ~5 years (give or take), so that might mean 3-5 architectures solving mostly the same problem over the years, with all of the multi-year migration windows in between each of them. If you're very lucky, different teams maintain the different projects in parallel, but often your team has to maintain the different projects yourselves because you're the owners and experts of the problem space. Your leadership might even actively fend off encroachment from other teams \"offering\" to obsolete you, even if they have a point. Even when you know exactly where your scaling problems are today, and you already have every relevant world expert on your team, you still can't be absolutely certain what architecture will keep scaling in another 5 years. That's not only due to kinds of growth you may not anticipate from current users, it's due to new requirements entirely which have their own cost model, and new users having their own workload whether on old or new requirements. I've eagerly learned everything I can from projects like this and I am still mentally prepared to have to replace my beautifully scaling architectures in another few years. In fact I look forward to it because it's some of the most interesting and satisfying work I ever get to do -- it's just a huge pain if it's not a drop-in replacement so you have to maintain two systems for an extended duration. reply zer00eyz 21 hours agoprevEarly in my career one of the IT guys told me that one of the people on staff was \"a technical magpie\". I looked at him with a raised eyebrow and he said \"He has to grab every shiny piece of tech that shows up and add it to the pile\". This is where we are. I can't tell you how many times I have seen projects get done just to pad a PM or developers resume. Just because it was the lastest and greatest hot shit thing to use. No sense of if it would be better, faster, cheaper, useful. When cloud was the hot new thing the company that I worked with launched a replatform on AWS. It gave us the ability to get through the initial scaling and sizing with ease. We left right away, because even then the costs did not make sense. Now we see folks crying about \"exit fees\" that were always there. That assumes that your institution even has the gall to own years of pissing away money. Workman like functionality isnt sexy, it wont be the hot bullet point on your resume, it wont get you your next job, but it is dam effective. reply TeMPOraL 19 hours agoparent> Workman like functionality isnt sexy, it wont be the hot bullet point on your resume, it wont get you your next job, but it is dam effective. So, not fun, not rewarding, no intellectual challenge, no career benefit. Why exactly should I want to do it? This isn't the goddamn United Federation of Planets, nor is the company a church - why exactly should I go above and beyond what I agreed to in exchange for my salary? It's not like the bosses go above and beyond either, nor do they believe in company \"mission\". To be clear: I understand the importance of actually doing your job right, and benefits of using boring tech, but you are not selling that well here. Employees need food and shelter and creature comforts, and so do their families. They are going to think beyond the current job, because if they won't, nobody else will. reply JAlexoid 17 hours agorootparentIn my experience the boring \"I delivered a project very fast with X,Y and Z and saved the company $100mil\" will win over \"I rearchitected a massive system to run on microservices\" At a certain point in your career, you'll realize that the business manager can override any technical hiring manager. Because at the end of the day delivering results is sexier, than bells and whistles in your resume. reply hliyan 16 hours agorootparentThere's an old IEEE article about the billions of dollars lost due to software project failures: https://spectrum.ieee.org/why-software-fails We don't hear of such failures any more because software projects (or products) no longer \"fail\" in the traditional sense -- they turn into endless money sinks of re-architectures, re-platforming, tech debt repayment or expensive maintenance, that can continue as long as the company has cash. When the company does run out of cash, it is difficult to say to what extent tech expenses or lack of revenue due to slow software delivery played a part. reply loup-vaillant 15 hours agorootparentprev> In my experience the boring \"I delivered a project very fast with X,Y and Z and saved the company $100mil\" will win over \"I rearchitected a massive system to run on microservices Good luck having the opportunity to work in a project where you have even the faintest idea how much money your contribution will make or save. I don't know about you, but never in my 17 year career have I had enough information to even attempt computing these numbers. And even if I could have, it was never part of my job description. So how did you know your numbers? Or if you didn't, how did you made them up for your interviews? reply aprdm 13 hours agorootparentIt's crazy that you don't know. I've been in this industry for 20y and apart from when I was extremely junior I always had a sense of the business impact of my work. reply loup-vaillant 12 hours agorootparentYeah, a sense. A guess. A gut feeling. Based on what exactly? I sure do get a sense of what will require less effort in the long run, or what will makes the user's life easier, or even what is likely to decrease the defect rate… but I dare 95% of programmers, even the subset active here on HN, to reliably assess the monetary impacts of those decisions within one order of magnitude, especially compared to the alternatives. Not to mention the monetary impacts of decisions totally outside my control. I can tell the architect \"you suggest A, but B is simpler to use and makes the API 3 times simpler at no loss of functionality\", what's the point of estimating the impact of such a no-brainer when the architect answers is \"you're correct, but we'll do it my way\" (real story)? And how do you expect me to estimate the monetary impact of pointing out that their TPM provisioning is missing a verification step? That stuff happens inside the factory, a problem at this stage is unlikely anyways. And even if I could somehow divine my monetary impact, the best I can say now is \"I did good work for this company, they didn't listen to me, and now they're going under\". Not even kidding, they are going under. I just ended my gig there because I couldn't take it any more. What are those wonderful places you worked at where you could estimate your impact with reasonable accuracy? reply aprdm 12 hours agorootparentNapkin math and ROI, no one is asking for the cents. For example, build system improvement on 10% of build time across 200 developers who on average get paid 300k a year - that's a very easy math, no ? Same for time to deploy, improvements on time to fix a bug, etc. etc. You can extrapolate and compare initiatives and projects on t-shirt sizes and ROIs. Knowing where yours sit as well. What places I've worked at ? Mostly business that made some money and were stable.. apart from a start up that was VC funded and made no money reply loup-vaillant 11 hours agorootparentI rarely got to know the actual deployment scale of anything I've done. Let's make a list: Ground software for an observation satellite. My internship was about implementing a dead simple neural \"network\" (2 hidden layers, no feedback), everything was specified from up top, we didn't even get to touch the learning algorithms. Impact? I guess a big flat zero, since all the differentiators was in the learning parameters. Peer-to-peer social network before Facebook. Never made a cent. Geographic Information System for the military. I was for obvious reasons not allowed to know enough to estimate the impact of my work. And even then all decisions was made by the customer, and once the user (a different entity) saw the Rube Goldberg contraption we dully made for them they predictably balked, and we did what we could from there. Which was, not that much. I did some useful stuff for sure, but mostly I participated in a system that was arguably worse than the one that preceded it. A visualiser for civil radar data. Data in, little planes in the screen out. And other nice stuff. I designed a simple C++ API that allowed the client to write business code faster than we would have ourselves (if only because of communication overhead), saving weeks of work. That contribution was utterly ignored for personal reasons, and I was eventually out. I have no idea what my actual impact was, because I don't know how far the project even went, and how widely it was eventually deployed. The maintenance of ground software for small civil observation drones. I did some cool stuff, but then was asked to transfer ownership of this software to a recently bought team (that did stuff similar to the company I worked for). I could have known how many drones were actually deployed, but to be honest my thing just saved a few minutes of flight, while most of the cost is to get the drone and its operator on site. That company was never really profitable, I hope the good people I met there are doing well. Scripting language for a programmable logic controller test environment. For the military, so I don't think I was allowed to even know the size of the team we'd deliver the software to. I got good feedback from them (they were happy about what I did), and I'm pretty sure my static typing made things easier for them than if I had just picked Lua or something, but how easier, and how much money it will save in the long run I have no freaking clue. Stuff in a missile company I cannot disclose. I believe my impact was almost nil, I couldn't stand their abysmal tech environment. Maintenance of a ground troops training system (a glorified laser tag, with debrief helpers and so on). Good luck assessing the impact of a better system, and I was just asked to do small tasks I could barely chose anyway. Prototype ADAS system. It was never deployed. Actual impact was therefore basically nil. Cool stuff to work on though, the CAN bus is a think of beauty. One of the rare instances where I could actually learn from example, instead of seeing yet again one of the gazillion obvious ways how not to do stuff. Ground software for some IoT device. Impact fundamentally uncertain, we had yet to sell it to anyone. Incident reporting software, based upon a more generic distributed base. I made the encryption layer (between users & company server), with a security based on PAKE (thus avoiding a PKI, which simplified the work of the sysadmin, at a slight loss of security). Impact fundamentally uncertain, we had yet to sell it to anyone. Charging stations for electric vehicles. I did the TPM provisioning, and mentioned a low-key security issue along the way. I participated in a questionable micro-service that was meant to help user interfaces (yeah, their IoT stuff had a micro-service architecture). Impact: whatever I did didn't save them: one year after I left, they're now going under. Preliminary study on the possible use of AMD-SEV to prevent users from peeking at our secret sauce (DRM). I don't think I was allowed to know the list of clients, and it's not even the only alternative. I don't think I could ever have assessed the long term impact of my work there. Flight recorder for trains (not a flight recorder then, but you get the idea). I just did little tasks here and there, didn't get the chance to have a good bird's eye view of the thing or its environment. Deployment base was knowable, but the business impact of my work was likely minimal, beyond \"finish this step so we can show the client we're on track for the next short term milestone\". The whole thing is a heap of technical debt, common components are impossible to update (user projects aren't locked to a given revision, they all pull from trunk), the build system is a home made monstrosity that doesn't help more than the standard monstrosities (I hate build systems)… and I was just axed from a round of layoffs. Cryptographic library I did on my free time: https://monocypher.org/ Nice little thing with a significant user base in the embedded ecosystem (not even my primary target). I controlled everything from start to finish, and I have no idea how many users I have, let alone how much time and money I saved them. In part because it is so simple, with such an outstanding documentation (which I mostly didn't write), that most users don't even have to bug me. --- To sum this up, my resume looks fairly horrible with respect to what I know of my actual business impact. Most of it, I think, was entirely outside my control. And I don't think I'm exceptional in this. reply yen223 9 hours agorootparentIf I could offer one piece of unsolicited advice: in whatever you do next, make it a point to understand a) how the business is doing and b) what your impact to the business will be. Make it a point to gather numbers like e.g. revenue figures, growth rates, costs, usage metrics. It is true that us underlings aren't usually handed these numbers, but you might be surprised by how easy it is to get them once you start looking. And once you have those numbers, you can derive some good estimates around your own impact towards the business. Having those numbers will help you understand your own impact. They will also help you avoid companies that, for a lack of a better word, are destined to fail. (I'm a fan of your blog btw. I really liked the article on the Monty Hall problems!) reply loup-vaillant 2 hours agorootparent(Thanks for the compliment!) I should be able to try this the next few weeks (new gig). But again, I'll be working on a prototype where such projections have already been made, the plan have already been laid out, and I'll mostly have to follow orders with relatively little wiggle room (they also expect me to hit the ground running). My only expected impact there is whether the stuff gets done on time or not. And assuming it is, the main company will throw the prototype away and rewrite everything anyway (at least they're supposed to). reply mlhpdx 13 hours agorootparentprevHonest question: if you’ve never known the tangible value of your work, how did you decide what to do? It’s an uncomfortable question to ask, but I genuinely don’t understand how that would be possible. reply TeMPOraL 12 hours agorootparentYour manager tells you? Or, higher up the career ladder, whatever is most urgent for the best-paying customer? Like, I know what's on the list to finish for the next multi-million-dollar payout from a major customer, but how my work contributes to it, compared to work done by 20+ other people involved in dev, operations, qa, deployment, customer negotiations, etc.? Who the fuck knows? Best I can estimate is how much it'll cost the company if I fail to deliver my piece on time. reply aprdm 11 hours agorootparentAt a lot of companies I've worked at, the engineers are empowered to decide on what to work at, it's not like they're 100% booked by PMs. Even if PMs, they can argue. \"Your manager tells you what to work on\" isn't how big tech runs for the most part, in fact it's a bit sad to think that some people work like that reply loup-vaillant 11 hours agorootparentIt's how most people had me work. It's how most of my colleagues had to work too. Just doing what my manager tells me to work on is what is considered normal and expected in… 8 of the 10 or so places I've worked at. The empowerment you speak of is but a faint dream in my town. reply aprdm 9 hours agorootparentInteresting. That's different from companies I've worked at, where the sw developer usually asks for clarifications and can fight back if they have something else that they feel should be worked on that has a bigger ROI for the company. Companies I've worked at recently have been much more bottom up than top down reply loup-vaillant 2 hours agorootparentWe can definitely ask for clarification. But fighting back is a grounds for being moved up the ax list. One workaround is trying stuff for free and present the results. Free labour for the employer, yay! reply mkl95 17 hours agorootparentprev> \"I rearchitected a massive system to run on microservices\" Saving a company from political mayhem is a pretty good achievement to have on your resume. It's also impressive because most engineering teams give up early on. reply tomaskafka 17 hours agorootparentprevThat depends on interest rates - right now it's a rare time when saved millions suddenly appear worth more than freshly rewritten pile of microservices. reply Spivak 17 hours agorootparentprev> delivered a project very fast with X,Y and Z and saved the company $100mil The problem is that $100mil is all pixie fairy dust when you're working on a new project. I wish this wasn't true but it works out better for you to implement it as costly and complex as possible, show off how smart you are, then simplify it during a cost cutting initiative (wow they must be so smart to make such an obviously complex system so simple). The secret is that while you think you're getting away with something playing this game you're actually doing exactly what the business wants. reply heyodai 14 hours agorootparent> ...while you think you're getting away with something playing this game you're actually doing exactly what the business wants. How so? I would think the business wants to spend as little money as possible. reply HeyLaughingBoy 13 hours agorootparentA bit of an aside, but one of the most important things that I've learned over my career is that the business wants to make as much money as possible. This may seem similar to \"wants to spend as little money as possible,\" but there's a big difference. Your floor is limited because you can only drop your costs to zero, but there's no ceiling on how much revenue you can make. reply wholinator2 14 hours agorootparentprevWell maybe not what it wants, but probably (depending on culture) what it _rewards_. reply Spivak 13 hours agorootparentprevNah, they want to bring in as much money as possible, subtle difference. High complexity (tech debt) and high costs (paying for expensive managed services) in service to time-to-ship is actually great. If it turns out that the market they predicted doesn't pan out they find out faster and just shut it down chalk it up to r&d costs for the tax break, and if it's so successful it costs them an arm and a let it's \"good problems to have.\" reply swader999 15 hours agorootparentprevYes, and add in a couple, I saved the project or successfully competled the previously failing project... reply mattgreenrocks 18 hours agorootparentprev> Why exactly should I want to do it? Because you're a professional, and part of that means doing things to help your team succeed. > They are going to think beyond the current job, because if they won't, nobody else will. This is also right, and a good thing to hold in the other hand. Reconciling these two forces needs a healthy org that allows employees to grow, along with a recognition that sometimes the forces conflict, and sometimes they don't. All we can do is play the cards we're dealt in the best way. If you really want to learn new tech, that's what the off hours are for. I say this as someone who has a lot of things that intrude into those hours. I'm (slowly) learning frontend after being a backend/compiler dev for a long time. It's...not easy, but I like it! reply rizzom5000 17 hours agorootparentprev> no intellectual challenge I tend to think that cargo cult programming and resume-driven development are the intellectual path of least resistance. Perhaps it's analogous to, \"I'd rather rewrite this than understand how it works\", because that requires less intellectual effort. Quality engineering is not achieved by the intellectually lazy, from what I've seen. reply isoprophlex 16 hours agorootparentYou're not wrong, but when you're inheriting a convoluted 50 file React shitfest that could have been a single HTML page and 20 lines of javascript... what are you going to do? Invest time in understanding that, or radically simplify in 20% of the time it takes to grok what you get thrown at you? reply codelobe 15 hours agorootparentAh, I see you are also a coder of culture... The trick is to get the Project Management to migrate to a hot new framework: Vanilla JS... http://vanilla-js.com/ reply groestl 15 hours agorootparentprevNo, a single HTML page and 20 lines of Javascript is clear cut. But there's a _lot_ of instances where it's not that way, and still rewrites are being proposed. reply asalahli 13 hours agorootparentprevWell I still need to understand what it is doing in order to radically simplify it and still have it do the exact same thing. reply waynesonfire 15 hours agorootparentprevstrawman. why do you even have a 50 file react shitfest to begin with? Hint: perhaps because someone want to pad their resume? reply TeMPOraL 15 hours agorootparentHint: because almost every web developer is a junior who doesn't know what they're doing. Proof: that's literally what a significant positive growth rate of an occupation means - if the doubling period is N years, then at any given moment, half the workforce has N years of experience or less. I don't remember the estimate for webdev, but I think N was something between 3 to 5 years. reply groestl 15 hours agorootparentprevI've seen this. Usually a combination of no economical constraints and technical curiosity on the engineers side. reply groestl 15 hours agorootparentprev> I'd rather rewrite this than understand how it works Sounds like \"how should I know what I think before I hear what I say\" ;) reply TeMPOraL 15 hours agorootparentI mean yes, it works that way? Hence inner narrative, for those who have it, and/or talking to yourself via notebook or a text file. reply Jtsummers 17 hours agorootparentprev> no intellectual challenge If it's not intellectually challenging, you're not working on interesting systems. If you have to find interesting tools to find intellectual stimulation, consider a different system to work on. As an example, I got to learn astrodynamics as part of my last job. Maybe not intellectually stimulating to everyone, but it didn't require me to learn the latest tooling just an interesting area of math and physics. The tooling and the language for the software wasn't that interesting, but that's fine. reply feoren 17 hours agorootparentI use boring architectures: JS/TS hitting a single C# server hitting a single database. I have had to (and gotten the opportunity to) learn about: - Environmental chemistry - Mass balance simulations - Wastewater treatment processes - Geotechnical engineering - Ecology - Mass transit systems And quite a bit more. I could not agree with you more. Even without the broad range of interesting subject matter, there's no end to intellectually stimulating work simply trying to architect a system well. reply Philip-J-Fry 18 hours agorootparentprevA boring and simple tech stack can mean you focus on delivering features rather than trying to work out which part of your complicated system is broken. The career benefit to me is that a simple tech stack allows a company to move fast and prosper. A prosperous company is usually financially rewarding even if it's not the most mentally rewarding. Getting tangled up in shiny new toys can harm your ability to move fast and it can have a negative effect on your career at that particular company. Especially since the shiny new toy today is old and rusty tomorrow, but boring stacks will always be old reliable. reply evantbyrne 13 hours agorootparentIt is difficult to overestimate the value of being able to actually take time off because changes happen in a reasonable time and your software just works without any surprises. Give me a boring tech stack, please! reply eadmund 15 hours agorootparentprev> So, not fun, not rewarding, no intellectual challenge, no career benefit. Why exactly should I want to do it? … why exactly should I go above and beyond what I agreed to in exchange for my salary? I think that delivering a solution which works, even if it is not sexy, is exactly what one agreed to in exchange for one’s salary. It may not be fun, it may have no intellectual challenge, and it may have no career benefit, but it is rewarding: the reward is the salary. reply olau 17 hours agorootparentprevIMHO it's about society. If you're asking on a personal level, I think that if you keep to the high ground, you're more likely to find long-lasting happiness. David Graeber spends a good deal of the pages in Bullshit Jobs on this topic. reply paulddraper 18 hours agorootparentprevHopefully, hopefully your incentives are aligned with your team's success. If they are not, I am truly sorry. reply underdeserver 18 hours agorootparentIn almost every business setting, your incentives are _partially_ aligned with your employer's. For instance, you usually both want to build a good product; conversely, you want to get paid as much as possible while your employer wants to pay you as little as possible. If it's all above board, and the non-aligned parts are agreed-to, all is well. reply muyuu 14 hours agorootparentprevhuman factors like drive are more important than most project managers would like to believe if you have people who are effective, allow them some space for fun and intellectual challenge even if it takes a bit away from the workload - if you disregard those human factors something will give up at the end, perhaps catastrophically as efforts are made to add \"sexiness\" to the core of the mission critical workload reply PH95VuimJjqBqy 16 hours agorootparentprevif the pride of a good job done isn't enough motivation for you then you'll never understand because you simply don't have the ability to. reply TeMPOraL 15 hours agorootparentUnless you're working pro bono, the \"pride of a good job done\" isn't enough motivation for you either. Your employer may wish it was, though. Point is, there is more to the equation. Employees don't exist in isolation, and when jobs actively refuse to take into account that the workers are real, living human beings, with needs and plans and dependents to feed, then resume-driven work will continue. reply zer00eyz 15 hours agorootparentprev>>> but you are not selling that well here I did not sell it well, that's fair. >> Why exactly should I want to do it? This isn't the goddamn United Federation of Planets, nor is the company a church - why exactly should I go above and beyond HN doesn't want to hear this answer: you do it for the PEOPLE around you. If you build sexy tech, and then get sexy job and I have to clean up your turds... well you can go fuck yourself. Hope that I'm never going to be the one answering the linked in request for a recommendation or sitting on the other side of the table when you come in for an interview. The tech job market is bad, and getting worse. You're gonna need an advocate on the inside if you want or need work quickly. That means former co-workers and bosses. NO one is gonna hire clock puncher who did selfish resume building project and left. Dont be that guy. reply manicennui 16 hours agorootparentprevMost jobs aren't a source of those things. Why should software development be any different? Introducing unnecessary technical challenges just to make your work interesting often has a negative impact on the end user/customer, which you should give a shit about. Do you think lawyers and architects complain if they aren't allowed to jump on every fad and make their work needlessly complex? reply Swizec 18 hours agorootparentprev> So, not fun, not rewarding, no intellectual challenge, no career benefit. Why exactly should I want to do it? It does help you get the next job. You’re just pitching it wrong. Instead of “Built boring tech” try “Delivered $5,000,000 return 2 months early”. Watch your inbox blow up. Business leaders don’t care about what you do, they care about results. What you do to get those results is just an unfortunate cost and obstacle to overcome on the way to the outcome. reply reader_1000 14 hours agorootparentMost companies out there want you to have certain technologies / keywords in your resume and will automatically reject you if you don't have them. Yes, building a solid project with boring technology that delivers real business value sounds good in theory but not so good when applying for a new job. Maybe it can help after you somehow manage to pass some initial screening. reply loup-vaillant 15 hours agorootparentprev> Instead of “Built boring tech” try “Delivered $5,000,000 return 2 months early”. How do I do that without lying through my teeth? 17 years on the job, I never had the data to even begin estimate that kind of things. It was never my job to know it (I'm a programmer, not an accountant), and it was often actively hidden from me. And how did you do it? How did you get your numbers, and what did you tell recruiters when you didn't? reply hoosieree 13 hours agorootparentThe usual resume advice to quantify everything so each bullet point conveys \"number go up\" also falls apart when you invent something, create a new revenue stream, new product, etc. The previous value was nil, therefore I improved it by... infinity percent? reply Swizec 14 hours agorootparentprevMaybe I’ve been extraordinarily lucky, but I’ve always just asked and people were so excited that an engineer would actually care about things that are on their mind all day every day. Might be more common in companies managed by OKR where you always know the business impact of your work. The business impact is your prime objective and you’re free to figure out the implementation. reply HeyLaughingBoy 13 hours agorootparentRight? I was going to ask OP \"have you ever asked anyone?\" Because, IME, managers, etc. love it when you show an interest in how the business works and where your part fits in. It also makes their job easier if they can relate the crappy stuff they have to assign you to how much benefit the business gets from it. reply loup-vaillant 12 hours agorootparentI must be doing something wrong because most of the time, getting interested in the wider impact of my work is held against me. I just ask why they want the stuff, suggest alternatives, point out issues, and the next day I'm an uncontrollable Maverick that means to rewrite everything and waste a ton of time… This also happens after explicit requests for feedback. Maybe they didn't actually meant it and I'm supposed to \"take the hint\" or some neurotypical bullshit, but when I hear such requests I tend to take them literally, and provide real feedback on the real issues. Unfortunately for all of us those tend to be stuff that ossified years ago and cannot (or will not) be fixed any time soon. Ever, in my experience. Quite the downer. Last time this happened I ended up being axed from a wave of layoffs. Whether their short term workflow and subsequent technical debt finally caught up to them, or their parent company just wanted to cut costs, I will never know. I do know my technical expertise was highly praised, and middle management felt I wasn't quite aligned with the goals of the company, whatever those were. (I think one significant cause was that I only emailed them about significant problems, and kept to myself and my team lead when it was smooth. I think in the future I will stop trusting them with anything negative.) So yeah, I can bet they love it when you act interested about their work, but start questioning their decisions (even just the one directly related to your own work and its likely impact on theirs), and the tone of the conversation changes very, very quickly. At least where I've worked. reply HeyLaughingBoy 11 hours agorootparent>start questioning their decisions I'm not privy to any discussions you've had, of course, but I will comment on this because I see it in many tech people: don't question people's decisions. No matter how you phrase it, it will appear critical. And people (neurotypical or otherwise!) don't like criticism, no matter how much they claim to welcome it. Those \"explicit\" requests for feedback? They want positive feedback. If you really must say something negative, suggest the opposite in a positive sense instead. i.e., instead of saying \"we should not have done X. It was a bad idea,\" try saying \"I think if we had done Y, things would have worked well\" while avoiding any potentially embarrassing references to X.",
    "originSummary": [
      "Wave, a $1.7B company, opts for a simple CRUD app architecture with a Python monolith on Postgres, akin to Stackoverflow, showcasing a preference for simplicity over complexity in scalability.",
      "The company processes billions of monthly requests with synchronous Python and a task queue, encountering hurdles with telecom integrations and regulatory compliance during global expansion.",
      "Despite tech trends favoring intricate setups, Wave utilizes GraphQL, Kubernetes, and custom transport protocols for their API, focusing on operational ease and cost-efficiency with a small engineering team."
    ],
    "commentSummary": [
      "The article examines the advantages and drawbacks of microservices versus monolithic architectures in software engineering, stressing discipline, organizational setup, and communication.",
      "Discussions cover scalability, maintainability, complexity, and how architectural decisions influence performance and data integrity, particularly in financial services.",
      "Emphasis is placed on striking a balance between simplicity and complexity, meeting business objectives, achieving tangible outcomes, and managing transitions between roles and technologies in the field."
    ],
    "points": 547,
    "commentCount": 409,
    "retryCount": 0,
    "time": 1708429834
  },
  {
    "id": 39443679,
    "title": "SSD Advancements Outpace Cloud Offerings",
    "originLink": "http://databasearchitects.blogspot.com/2024/02/ssds-have-become-ridiculously-fast.html",
    "originBody": "Database Architects A blog by and for database architects. Monday, February 19, 2024 SSDs Have Become Ridiculously Fast, Except in the Cloud In recent years, flash-based SSDs have largely replaced disks for most storage use cases. Internally, each SSD consists of many independent flash chips, each of which can be accessed in parallel. Assuming the SSD controller keeps up, the throughput of an SSD therefore primarily depends on the interface speed to the host. In the past six years, we have seen a rapid transition from SATA to PCIe 3.0 to PCIe 4.0 to PCIe 5.0. As a result, there was an explosion in SSD throughput: At the same time, we saw not just better performance, but also more capacity per dollar: The two plots illustrate the power of a commodity market. The combination of open standards (NVMe and PCIe), huge demand, and competing vendors led to great benefits for customers. Today, top PCIe 5.0 data center SSDs such as the Kioxia CM7-R or Samsung PM1743 achieve up to 13 GB/s read throughput and 2.7M+ random read IOPS. Modern servers have around 100 PCIe lanes, making it possible to have a dozen of SSDs (each usually using 4 lanes) in a single server at full bandwidth. For example, in our lab we have a single-socket Zen 4 server with 8 Kioxia CM7-R SSDs, which achieves 100GB/s (!) I/O bandwidth: AWS EC2 was an early NVMe pioneer, launching the i3 instance with 8 physically-attached NVMe SSDs in early 2017. At that time, NVMe SSDs were still expensive, and having 8 in a single server was quite remarkable. The per-SSD read (2 GB/s) and write (1 GB/s) performance was considered state of the art as well. Another step forward occurred in 2019 with the launch of i3en instances, which doubled storage capacity per dollar. Since then, several NVMe instance types, including i4i and im4gn, have been launched. Surprisingly, however, the performance has not increased; seven years after the i3 launch, we are still stuck with 2 GB/s per SSD. Indeed, the venerable i3 and i3en instances remain the best EC2 has to offer in terms of IO/$ and SSD capacity/$, respectively. Personally, I find this very surprising given the SSD bandwidth explosion and cost reductions we have seen on the commodity market. At this point, the performance gap between state-of-the-art SSDs and those offered by major cloud vendors, especially in read throughput, write throughput, and IOPS, is nearing an order of magnitude. (Azure's top NVMe instances are only slightly faster than AWS's.) What makes this stagnation in the cloud even more surprising is that we have seen great advances in other areas. For example, during the same 2017 to 2023 time frame, EC2 network bandwidth exploded, increasing from 10 Gbit/s (c4) to 200 Gbit/s (c7gn). Now, I can only speculate why the cloud vendors have not caught up on the storage side: One theory is that EC2 intentionally caps the write speed at 1 GB/s to avoid frequent device failure, given the total number of writes per SSD is limited. However, this does not explain why the read bandwidth is stuck at 2 GB/s. A second possibility is that there is no demand for faster storage because very few storage systems can actually exploit tens of GB/s of I/O bandwidth. See our recent VLDB paper. On the other hand, as long as fast storage devices are not widely available, there is also little incentive to optimize existing systems. A third theory is that if EC2 were to launch fast and cheap NVMe instance storage, it would disrupt the cost structure of its other storage service (in particular EBS). This is, of course, the classic innovator's dilemma, but one would hope that one of the smaller cloud vendors would make this step to gain a competitive edge. Overall, I'm not fully convinced by any of these three arguments. Actually, I hope that we'll soon see cloud instances with 10 GB/s SSDs, making this post obsolete. Posted by Viktor Leis at 9:00 AM Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest 16 comments: AnonymousFebruary 20, 2024 at 6:42 PM Related Hackernews Discussion: https://news.ycombinator.com/item?id=39443679 ReplyDelete Replies Reply AdamKFebruary 20, 2024 at 7:27 PM Cloud providers buy only high capacity drives, so they have less transfer speed per TB of storage available (compared to comodity drives). If the drive is share between multiple VMs, throughput is shared between them, and they have to obey SLAs. This could be also a way to manage wear of the drives. ReplyDelete Replies Reply AnonymousFebruary 20, 2024 at 7:54 PM Fucking nerds. Fuck your databases. ReplyDelete Replies AnonymousFebruary 20, 2024 at 9:06 PM Reason ? Delete Replies Reply AnonymousFebruary 20, 2024 at 10:54 PM databases must have fucked your mother for you to be so angry Delete Replies Reply AnonymousFebruary 21, 2024 at 12:41 AM A database baby in the wild? How precious Delete Replies Reply AnonymousFebruary 21, 2024 at 9:24 AM Is that why we have child-parent relationship in database? Delete Replies Reply Reply AnonymousFebruary 20, 2024 at 9:19 PM I've seen speeds of 60k iops and 7 GB/s speeds on akamai cloud/linode. Even in the 5$ nanodes. Varies depending on the class of system you get, you get a random Zen 2 or Zen 3 class core and the better disks are on the Zen 3 instances. Still pretty slow for databases compared to bare metal. The fractional vCPUs they sell are comparable with the disk difference. Cloud resources are a pretty bad deal right now and don't reflect the gains from the last 3 years - which have been huge on the CPU side too. ReplyDelete Replies Reply AnonymousFebruary 20, 2024 at 9:20 PM Interesting. I can say that locally on my workstation, SSDs are beneficial for searching with tools like FileSearchEX, where one has to load the contents in memory of thousands of files over and over to find keywords. But I wonder, as the HN article states, is the reason because of a protocol in front of the actual drives? ReplyDelete Replies Reply AnonymousFebruary 20, 2024 at 11:12 PM Cloud just means someone elses hardware and your virtual machine shares I/O with everyone. If you want faster, use your own hardware or keep syncing your cloud instance around until you find hardware without noisy neighbors. ReplyDelete Replies AnonymousFebruary 21, 2024 at 1:04 AM The important thing here is that even on the AWS metal instances which are supposed to give you one entire host and even if you choose an instance that has LOCAL NVMe SSDs, those are still ridiculously slow. Delete Replies Reply Reply AnonymousFebruary 20, 2024 at 11:36 PM This link isn't directly salient to this post, as it's about CXL adoption, but the preamble does a nice job framing why folks see what they see with cloud services re:performance. https://blog.jrlabs.io/posts/2024-02-19-what-is-cxl/ ReplyDelete Replies Reply AkashFebruary 21, 2024 at 12:51 AM SSD's on all the clouds are Network SSD's and will never be performant as local disk on bare metal. Network will always be slower than local disk. Its just matter of time when these network volume will fall behind in numbers compare to local SSD's. ReplyDelete Replies AnonymousFebruary 21, 2024 at 1:09 AM This is not talking about EBS. This is talking about the \"instance-local\" SSDs. Also Networking on AWS is now faster than the supposed LOCAL NVMe SSDs that you are supposed to get with the instance types offering them. You can read at 80 GBit/s from S3 if you choose an instance with a lot of network bandwidth but only at 64 GBit/s from \"local NVMe SSD\" if you have an instance with 4 of them and put them in RAID 0... This is clearly ridiculous. Delete Replies Reply Reply AnonymousFebruary 21, 2024 at 5:51 AM Hmm ReplyDelete Replies Reply AnonymousFebruary 21, 2024 at 5:57 AM Could you increase blog's font size? Just a readibility comment. ReplyDelete Replies Reply Add comment Load more... Older Post Home Subscribe to: Post Comments (Atom) Contributors Peter Boncz Thomas Neumann Viktor Leis Blog Archive ▼ 2024 (1) ▼ February (1) SSDs Have Become Ridiculously Fast, Except in the ... ► 2023 (3) ► April (1) ► February (1) ► January (1) ► 2022 (7) ► June (5) ► April (1) ► January (1) ► 2021 (2) ► July (1) ► June (1) ► 2020 (4) ► November (1) ► October (1) ► April (1) ► January (1) ► 2019 (3) ► July (1) ► May (1) ► February (1) ► 2018 (2) ► June (1) ► April (1) ► 2017 (2) ► December (1) ► February (1) ► 2016 (2) ► August (1) ► April (1) ► 2015 (7) ► December (1) ► September (1) ► July (1) ► June (1) ► April (1) ► February (1) ► January (1) ► 2014 (11) ► December (1) ► September (1) ► August (1) ► July (2) ► June (2) ► May (4) Simple theme. Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=39443679",
    "commentBody": "SSDs have become fast, except in the cloud (databasearchitects.blogspot.com)488 points by greghn 17 hours agohidepastfavorite345 comments pclmulqdq 16 hours agoThis was a huge technical problem I worked on at Google, and is sort of fundamental to a cloud. I believe this is actually a big deal that drives peoples' technology directions. SSDs in the cloud are attached over a network, and fundamentally have to be. The problem is that this network is so large and slow that it can't give you anywhere near the performance of a local SSD. This wasn't a problem for hard drives, which was the backing technology when a lot of these network attached storage systems were invented, because they are fundamentally slow compared to networks, but it is a problem for SSD. reply jsnell 16 hours agoparentAccording to the submitted article, the numbers are from AWS instance types where the SSD is \"physically attached\" to the host, not about SSD-backed NAS solutions. Also, the article isn't just about SSDs being no faster than a network. It's about SSDs being two orders of magnitude slower than datacenter networks. reply pclmulqdq 16 hours agorootparentIt's because the \"local\" SSDs are not actually physically attached and there's a network protocol in the way. reply jsnell 16 hours agorootparentI think you're wrong about that. AWS calls this class of storage \"instance storage\" [0], and defines it as: > Many Amazon EC2 instances can also include storage from devices that are located inside the host computer, referred to as instance storage. There might be some wiggle room in \"physically attached\", but there's none in \"storage devices located inside the host computer\". It's not some kind of AWS-only thing either. GCP has \"local SSD disks\"[1], which I'm going to claim are likewise local, not over the network block storage. (Though the language isn't as explicit as for AWS.) [0] https://aws.amazon.com/ec2/instance-types/ [1] https://cloud.google.com/compute/docs/disks#localssds reply reactordev 3 hours agorootparentAWS is so large, every concept of hardware is virtualized over a software layer. “Instance storage” is no different. It’s just closer to the edge with your node. It’s not some box in a rack where some AWS tech slots in an SSD. AWS has a hardware layer, but you’ll never see it. reply pclmulqdq 14 hours agorootparentprevThat's the abstraction they want you to work with, yes. That doesn't mean it's what is actually happening - at least not in the same way that you're thinking. As a hint for you, I said \"a network\", not \"the network.\" You can also look at public presentations about how Nitro works. reply jsnell 13 hours agorootparentI've linked to public documentation that is pretty clearly in conflict with what you said. There's no wiggle room in how AWS describes their service without it being false advertising. There's no \"ah, but what if we define the entire building to be the host computer, then the networked SSDs really are inside the host computer\" sleight of hand to pull off here. You've provided cryptic hints and a suggestion to watch some unnamed presentation. At this point I really think the burden of proof is on you. reply stingraycharles 7 hours agorootparentYou are correct, and the parent you’re replying to is confused. Nitro is for EBS, not the i3 local NVMe instances. Those i3 instances lose your data whenever you stop and start them again (ie migrate to a different host machine), there’s absolutely no reason they would use network. EBS itself uses a different network than the “normal” internet, if I were to guess it’s a converged Ethernet network optimized for iSCSI. Which is what Nitro optimizes for as well. But it’s not relevant for the local NVMe storage. reply fcsp 2 hours agorootparentprevI see wiggle room in the statement you posted in that the SSD storage that is physically inside the machine hosting the instance might be mounted into the hypervised instance itself via some kind of network protocol still, adding overhead. reply jng 14 hours agorootparentprevNitro \"virtual NVME\" device are mostly (only?) for EBS -- remote network storage, transparently managed, using a separate network backbone, and presented to the host as a regular local NVME device. SSD drives in instances such as i4i, etc. are physically attached in a different way -- but physically, unlike EBS, they are ephemeral and the content becomes unavaiable as you stop the instance, and when you restart, you get a new \"blank slate\". Their performance is 1 order of magnitude faster than standard-level EBS, and the cost structure is completely different (and many orders of magnitude more affordable than EBS volumes configured to have comparable I/O performance). reply jasonwatkinspdx 12 hours agorootparentprevBoth the documentation and Amazon employees are in here telling you that you're wrong. Can you resolve that contradiction or do you just want to act coy like you know some secret? The latter behavior is not productive. reply stingraycharles 4 hours agorootparentThe parent thinks that AWS' i3 NVMe local instance storage is using a PCIe switch, which is not the case. EBS (and the AWS Nitro card) use a PCIe switch, and as such all EBS storage is exposed as e.g. /dev/nvmeXnY . But that's not the same as the i3 instances are offering, so the parent is confused. reply dekhn 12 hours agorootparentprevit sounds like you're trying to say \"PCI switch\" without saying \"PCI switch\" (I worked at Google for over a decade, including hardware division). reply pclmulqdq 7 hours agorootparentThat is what I am trying to say without actually giving it out. PCIe switches are very much not transparent devices. Apparently AWS has not published anything about this, and doesn't have Nitro moderating access to \"local\" SSD, though - that I did get confused with EBS. reply pzb 5 hours agorootparentAWS has stated that there is a \"Nitro Card for Instance Storage\"[0][1] which is a NVMe PCIe controller that implements transparent encryption[2]. I don't have access to an EC2 instance to check, but you should be able to see the PCIe topology to determine how many physical cards are likely in i4i and im4gn and their PCIe connections. i4i claims to have 8 x 3,750 AWS Nitro SSD, but it isn't clear how many PCIe lanes are used. Also, AWS claims \"Traditionally, SSDs maximize the peak read and write I/O performance. AWS Nitro SSDs are architected to minimize latency and latency variability of I/O intensive workloads [...] which continuously read and write from the SSDs in a sustained manner, for fast and more predictable performance. AWS Nitro SSDs deliver up to 60% lower storage I/O latency and up to 75% reduced storage I/O latency variability [...]\" This could explain the findings in the article - they only meared peak r/w, not predictability. [0] https://perspectives.mvdirona.com/2019/02/aws-nitro-system/ [1] https://aws.amazon.com/ec2/nitro/ [2] https://d1.awsstatic.com/events/reinvent/2019/REPEAT_2_Power... reply rowanG077 3 hours agorootparentprevWhy are you acting as if PCIe switches are some secret technology? It was extremely grating for me to read your comments. reply wstuartcl 12 hours agorootparentprevthe tests were for these local (metal direct connect ssds). The issue is not network overhead -- its that just like everything else in cloud the performance of 10 years ago was used as the baseline that carries over today with upcharges to buy back the gains. there is a reason why vcpu performance is still locked to the typical core from 10 years ago when every core on a machine today in those data scenters is 3-5x or more speed basis. Its cause they can charge you for 5x the cores to get that gain. reply flaminHotSpeedo 4 hours agorootparent> there is a reason why vcpu performance is still locked to the typical core from 10 years ago That is transparently nonsense. You can disprove that claim in 5 minutes, and it makes literally zero sense for offerings that aren't oversubscribed reply wmf 10 hours agorootparentprevvcpu performance is still locked to the typical core from 10 years ago No. In some cases I think AWS actually buys special processors that are clocked higher than the ones you can buy. reply phanimahesh 4 hours agorootparentThe parent claims that though aws uses better hardware, they bill in vcpus whose benchmarks are from a few years ago, so that they can sell more vcpu units per performant physical cpu. This does not contradict your claim that aws buys better hardware. reply wmf 4 hours agorootparentIt's so obviously wrong that I can't really explain it. Maybe someone else can. To believe that requires a complete misunderstanding of IaaS. reply gowld 7 hours agorootparentprevYou are talking about real CPU not virtual cpu reply wmf 6 hours agorootparentGenerally each vCPU is a dedicated hardware thread, which has gotten significantly faster in the last 10 years. Only lambdas, micros, and nanos have shared vCPUs and those have probably also gotten faster although it's not guaranteed. reply jandrewrogers 3 hours agorootparentIn fairness, there are a not insignificant number of workloads that do not benefit from hardware threads on CPUs [0], instead isolating processes along physical cores for optimal performance. [0] Assertion not valid for barrel processors. reply 20after4 16 hours agorootparentprevIf the SSD is installed in the host server, doesn't that still allow for it to be shared among many instances running on said host? I can imagine that a compute node has just a handful of SSDs and many hundreds of instances sharing the I/O bandwidth. reply discodave 15 hours agorootparentIf you have one of the metal instance types, then you get the whole host, e.g. i4i.metal: https://aws.amazon.com/ec2/instance-types/i4i/ reply aeyes 15 hours agorootparentprevOn AWS yes, the older instances which I am familiar with had 900GB drives and they sliced that up into volumes of 600, 450, 300, 150, 75GB depending on instance size. But they also tell you how much IOPS you get: https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/stora... reply queuebert 14 hours agorootparentprevHow do these machines manage the sharing of one local SSD across multiple VMs? Is there some wrapper around the I/O stack? Does it appear as a network share? Geniuinely curious... reply felixg3 13 hours agorootparentProbably NVME namespaces [0]? [0]: https://nvmexpress.org/resource/nvme-namespaces/ reply bravetraveler 11 hours agorootparentLess fancy, quite often... at least on VPS providers [1]. They like to use reflinked files off the base images. This way they only store what differs. 1: Which is really a cloud without a certain degree of software defined networking/compute/storage/whatever. reply icedchai 12 hours agorootparentprevWith Linux and KVM/QEMU, you can map an entire physical disk, disk partition, or file to a block device in the VM. For my own VM hosts, I use LVM and map a logical volume to the VM. I assumed cloud providers did something conceptually similar, only much more sophisticated. reply bravetraveler 11 hours agorootparentFiles with reflinks are a common choice, the main benefit being: only storing deltas. The base OS costs basically nothing LVM/block like you suggest is a good idea. You'd be surprised how much access time is trimmed by skipping another filesystem like you'd have with a raw image file reply dan-robertson 13 hours agorootparentprevAWS have custom firmware for at least some of their SSDs, so could be that reply magicalhippo 12 hours agorootparentprevIn say VirtualBox you can create a file backed on the physical disk, and attach it to the VM so the VM sees it as a NVMe drive. In my experience this is also orders of magnitude slower that true direct access, ie PCIe pass-through, as all access has to pass through the VM storage driver and so could explain what is happening. reply bravetraveler 11 hours agorootparentThe storage driver may have more impact on VBox. You can get very impressive results with 'virtio' on KVM reply magicalhippo 9 hours agorootparentYeah I've yet to try that. I know I get a similar lack of performance with Bhyve (FreeBSD) using VirtIO, so it's not a given it's fast. I have no idea how AWS run their VMs, was just saying a slow storage driver could give such results. reply bravetraveler 8 hours agorootparent> just saying a slow storage driver could give such results Oh, absolutely - not to contest that! There's a whole lot of academia on 'para-virtualized' and so on in this light. That's interesting to hear about FreeBSD; basically all of my experience has been with Linux/Windows. reply ownagefool 15 hours agorootparentprevPCI bus, etc too reply throwawaaarrgh 14 hours agorootparentprevInstance storage is not networked. That's why it's there. reply Hewitt821 8 hours agorootparentprevLocal SSD is part of the machine, not network attached. reply yolovoe 13 hours agorootparentprevYou’re wrong. Instance local means SSD is physically attached to the droplet and is inside the server chassis, connected via PCIe. Sourece: I work on nitro cards. reply tptacek 12 hours agorootparent\"Attached to the droplet\"? reply sargun 11 hours agorootparentDroplets are what EC2 calls their hosts. Confusing? I know. reply tptacek 11 hours agorootparentYes! That is confusing! Tell them to stop it! reply kiwijamo 9 hours agorootparentFYI it's not a AWS term, it's a DigitalOcean term. reply tptacek 8 hours agorootparentI could not be more confused. Does EC2 quietly call their hosting machines \"droplets\"? I knew \"droplets\" to be a DigitalOcean team, but DigitalOcean doesn't have Nitro cards. reply apitman 8 hours agorootparentNow I'm wondering if that's where DO got the name in the first place reply chatmasta 7 hours agorootparentSurely \"droplet\" is a derivative of \"ocean?\" reply arrakeenrevived 6 hours agorootparentClouds (like, the big fluffy things in the sky) are made up of many droplets of liquid. Using \"droplet\" to refer to the things that make up cloud computing is a pretty natural nickname for any cloud provider, not just DO. I do imagine that DO uses \"droplet\" as a public product branding because it works well with their \"Ocean\" brand, though. ...now I'm actually interested in knowing if \"droplet\" is derived from \"ocean\", or if \"Digital Ocean\" was derived from having many droplets (which was derived from cloud). Maybe neither. reply sargun 7 hours agorootparentprevI believe AWS was calling them droplets prior to digital ocean. reply hipadev23 12 hours agorootparentprevdigitalocean squad reply jbnorth 7 hours agorootparentNo, that’s AWS. reply jrullman 16 hours agorootparentprevI can attest to the fact that on EC2, \"instance store\" volumes are actually physically attached. reply dekhn 15 hours agorootparentprevI suspect you must be conflating several different storage products. Are you saying https://cloud.google.com/compute/docs/disks/local-ssd devices talk to the host through a network (say, ethernet with some layer on top)? Because the documentation very clearly says otherwise, \"This is because Local SSD disks are physically attached to the server that hosts your VM. For this same reason, Local SSD disks can only provide temporary storage.\" (at least, I'm presuming that by physically attached, they mean it's connected to the PCI bus without a network in between). I suspect you're thinking of SSD-PD. If \"local\" SSDs are not actually local and go through a network, I need to have a discussion with my GCS TAM about truth in advertising. reply Hewitt821 8 hours agorootparentLocal SSD is part of the machine. reply op00to 15 hours agorootparentprev> physically attached Believe it or not, superglue and a wifi module! /s reply mint2 14 hours agorootparentprevI don’t really agree with assuming the form of physical attachment and interaction unless it is spelled out. If that’s what’s meant it will be stated in some fine print, if it’s not stated anywhere then there is no guarantee what the term means, except I would guess they may want people to infer things that may not necessarily be true. reply dekhn 12 hours agorootparent\"Physically attached\" has had a fairly well defined meaning and i don't normally expect a cloud provider to play word salad to convince me a network drive is locally attached (like I said, if true, I would need to have a chat with my TAM about it). Physically attached for servers, for the past 20+ years, has meant a direct electrical connection to a host bus (such as the PCI bus attached to the front-side bus). I'd like to see some alternative examples that violate that convention. reply adgjlsfhk1 11 hours agorootparentEthernet cables are physical... reply dekhn 11 hours agorootparentThe NIC is attached to the host bus through the north bridge. But other hosts on the same ethernetwork are not considered to be \"local\". We dont need to get crazy about teh semantics to know that when a cloud provider says an SSD is locally attached, that it's closer than an ethernetwork away. reply SteveNuts 5 hours agorootparentprevIf that’s the game we’re going to play then technically my driveway is on the same road as the White House. reply adgjlsfhk1 5 hours agorootparentexactly. it's not about what's good for the consumer, it's about what they can do without losing a lawsuit for false advertising. reply candiddevmike 16 hours agorootparentprevDepends on the cloud provider. Local SSDs are physically attached to the host on GCP, but that makes them only useful for temporary storage. reply amluto 15 hours agorootparentWhich is a weird sort of limitation. For any sort of you-own-the-hardware arrangement, NVMe disks are fine for long term storage. (Obviously one should have backups, but that’s a separate issue. One should have a DR plan for data on EBS, too.) You need to migrate that data if you replace an entire server, but this usually isn’t a very big deal. reply supriyo-biswas 15 hours agorootparentThis is Hyrum’s law at play: AWS wants to make sure that the instance stores aren’t seen as persistent, and therefore enforce the failure mode for normal operations as well. You should also see how they enforce similar things for their other products and APIs, for example, most of their services have encrypted pagination tokens. reply pclmulqdq 16 hours agorootparentprevIf you're at G, you should read the internal docs on exactly how this happens and it will be interesting. reply seedless-sensat 3 hours agorootparentWhy are you protecting Google's internal architecture onto to AWS? Your Google mental model is not correct here reply jsolson 5 hours agorootparentprevIn most cases, they're physically plugged into a PCIe CEM slot in the host. There is no network in the way, you are either misinformed or thinking of a different product. reply rfoo 15 hours agorootparentprevWhy would I lose all data on these SSDs when I initiate a power off of the VM on console, then? I believe local SSDs are definitely attached to the host. They are just not exposed via NVMe ZNS hence the performance hit. reply res0nat0r 15 hours agorootparentYour EC2 instance with instance-store storage when stopped can be launched on any other random host in the AZ when you power it back on. Since your rootdisk is an EBS volume attached across the network, so when you start your instance back up you're going to be launched likely somewhere else with an empty slot, and empty local-storage. This is why there is always a disclaimer that this local storage is ephemeral and don't count on it being around long-term. reply mrcarrot 13 hours agorootparentI think the parent was agreeing with you. If the “local” SSDs _weren’t_ actually local, then presumably they wouldn’t need to be ephemeral since they could be connected over the network to whichever host your instance was launched on. reply manquer 15 hours agorootparentprevIt is because on reboot you may not get the same physical server . They are not rebooting the physical server for you , just the VM Same VM is not allocated for a variety of reasons , scheduled maintenance, proximity to other hosts on the vpc , balancing quiet and noisy neighbors so on. It is not that the disk will always wiped , sometimes the data is still there on reboot just that there is no guarantee allowing them to freely move between hosts reply mr_toad 9 hours agorootparentData persists between reboots, but not shutdowns: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-inst... https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-inst... reply throwawaaarrgh 14 hours agorootparentprevYes, that's what their purpose is in cloud applications: temporary high performance storage only. If you want long term local storage you'll have to reserve an instance host. reply crotchfire 14 hours agorootparentprevThis is incorrect. Amazon offers both locally-attached storage devices as well as instance-attached storage devices. The article is about the latter kind. reply hathawsh 13 hours agorootparentprevThat seems like a big opportunity for other cloud providers. They could provide SSDs that are actually physically attached and boast (rightfully) that their SSDs are a lot faster, drawing away business from older cloud providers. reply jbnorth 7 hours agorootparentThis is already a thing. AWS instance store volumes are directly attached to the host. I’m pretty sure GCP and Azure also have an equivalent local storage option. reply ddorian43 11 hours agorootparentprevNext thing the other clouds will offer is cheaper bandwidth pricing, right? reply solardev 12 hours agorootparentprevFor what kind of workloads would a slower SSD be a significant bottleneck? reply jandrewrogers 6 hours agorootparentI run very large database-y workloads. Storage bandwidth is by far the throughput rate limiting factor. Cloud environments are highly constrained in this regard and there is a mismatch between the amount of CPU you are required to buy to get a given amount of bandwidth. I could saturate a much faster storage system with a fraction of the CPU but that isn’t an option. Note that latency is not a major concern here. This has an enormous economic impact. I once did a TCO study with AWS to run data-intensive workload running on purpose-built infrastructure on their cloud. AWS would have been 3x more expensive per their own numbers, they didn’t even argue it. The main difference is that we had highly optimized our storage configuration to provide exceptional throughput for our workload on cheap hardware. I currently run workloads in the cloud because it is convenient. At scale though, the cost difference to run it on your own hardware is compelling. The cloud companies also benefit from a learned helplessness when it comes to physical infrastructure. Ironically, it has never been easier to do a custom infrastructure build, which companies used to do all the time, but most people act like it is deep magic now. reply solardev 5 hours agorootparentThanks for the details! Does this mean you're colocating your own server in a data center somewhere? Or do you have your own data center/running it off a bare metal server with a business connection? Just wondering if the TCO included the same levels of redundancy and bandwidth, etc. reply jandrewrogers 3 hours agorootparentWe were colocated in large data centers right on the major IX with redundancy. All of this was accounted for in their TCO model. We had a better switch fabric than is typical for the cloud but that didn’t materially contribute to cost. We were using AWS for overflow capacity when we exceeded the capacity of our infrastructure at the time; they wanted us to move our primary workload there. The difference in cost could be attributed mostly to the server hardware build, and to a lesser extent the better scalability with a better network. In this case, we ended up working with Quanta on servers that had everything we needed and nothing we didn’t, optimizing heavily for bandwidth/$. We worked directly with storage manufacturers to find SKUs that stripped out features we didn’t need and optimized for cost per byte given our device write throughput and durability requirements. They all have hundreds of custom SKUs that they don’t publicly list, you just have to ask. A hidden factor is that the software was designed to take advantage of hardware that most enterprises would not deign to use for high-performance applications. There was a bit of supply chain management but we did this as a startup buying not that many units. The final core server configuration cost us just under $8k each delivered, and it outperformed every off-the-shelf server for twice the price and essentially wasn’t something you could purchase in the cloud (and still isn’t). These servers were brilliant, bulletproof, and exceptionally performant for our use case. You can model out the economics of this and the zero-crossing shows up at a lower burn rate than I think many people imagine. We were extremely effective at using storage, and we did not attach it to expensive, overly-powered servers where the CPUs would have been sitting idle anyway. The sweet spot was low-clock high-core CPUs, which are typically at a low-mid price point but optimal performance-per-dollar if you can effectively scale software to the core count. Since the software architecture was thread-per-core, the core count was not a bottleneck. The economics have not shifted much over time. AWS uses the same pricing model as everyone else in the server leasing game. Roughly speaking, you model your prices to recover your CapEx in 6 months of utilization. Ignoring overhead, doing it ourselves pulled that closer to 1.5-2 months for the same burn. This moves a lot of the cost structure to things like power, space, and bandwidth. We definitely were paying more for space and power than AWS (usually less for bandwidth) but not nearly enough to offset our huge CapEx advantage relative to workload. All of this can be modeled out in Excel. No one does it anymore but I am from a time when it was common, so I have that skill in my back pocket. It isn’t nearly as much work as it sounds like, much of the details are formulaic. You do need to have good data on how your workload uses hardware resources to know what to build. reply vidarh 1 hour agorootparent> All of this can be modeled out in Excel. No one does it anymore but I am from a time when it was common, so I have that skill in my back pocket. It isn’t nearly as much work as it sounds like, much of the details are formulaic. You do need to have good data on how your workload uses hardware resources to know what to build. And this is one of the big \"screts\" AWS success: Shifting a lot of resource allocation and power from people with budgeting responsibility to developers who have usually never seen the budget or accounts, don't keep track, and at most retrospectively gets pulled in to explain line items in expenses, and obscuring it (to the point where I know people who've spent 6 figure amounts worth of dev time building analytics to figure out where their cloud spend goes... tooling has gotten better but is still awful) I believe a whole lot of tech stacks would look very different if developers and architects were more directly involved in budgeting, and bonuses etc. were linked at least in part to financial outcomes affected by their technical choices. A whole lot of claims to low cloud costs come from people who have never done actual comparisons and who seem to have a pathological fear of hardware, even when for most people you don't need to ever touch a physical box yourself - you can get maybe 2/3's of the savings with managed hosting as well. You don't get the super-customized server builds, but you do get far more choice than with cloud providers, and you can often make up for the lack of fine-grained control by being able to rent/lease them somewhere where the physical hosting is cheaper (e.g. at a previous employer what finally made us switch to Hetzner for most new capacity was that while we didn't get exactly the hardware we wanted, we got \"close enough\" coupled with data centre space in their locations in Germany being far below data centre space in London - it didn't make them much cheaper, but it did make them sufficiently cheaper to outweigh the hardware differences with a margin sufficient for us to deploy new stuff there but still keep some of our colo footprint) reply lolc 10 hours agorootparentprevI tend some workloads that transform data grids of varying sizes. The grids are anon mmaps so that when mem runs out, they get paged out. This means processing stays mostly in-mem yet won't abort when mem runs tight. The processes that get hit by paging slow to a crawl though. Getting faster SSD means they're still crawling but crawling faster. Doubling SSD throughput would pretty much half the tail latency. reply solardev 9 hours agorootparentI see. Thanks for explaining! reply ReflectedImage 8 hours agorootparentprevPretty much all work loads, work loads that are not affected would be the exception reply mike_hearn 16 hours agorootparentprevThey do this because they want SSDs to be in a physically separate part of the building for operational reasons, or what's the point in giving you a \"local\" SSD that isn't actually plugged into the real machine? reply yolovoe 13 hours agorootparentThe comment you’re responding to is wrong. AWS offers many kinds of storage. Instance local storage is physically attached to the droplet. EBS isn’t but that’s a separate thing entirely. I literally work in EC2 Nitro. reply ianburrell 15 hours agorootparentprevThe reason for having most instances use network storage is that it makes possible migrating instances to other hosts. If the host fails, the network storage can be pointed at the new host with a reboot. AWS sends out notices regularly when they are going to reboot or migrate instances. Their probably should be more local instance storage types for using with instances that can be recreated without loss. But it is simple for them to have a single way of doing things. At work, someone used fast NVMe instance storage for Clickhouse which is a database. It was a huge hassle to copy data when instances were going to be restarted because the data would be lost. reply mike_hearn 14 hours agorootparentSure, I understand that, but this user is claiming that on GCP even local SSDs aren't really local, which raises the question of why not. I suspect the answer is something to do with their manufacturing processes/rack designs. When I worked there (pre GCP) machines had only a tiny disk used for booting and they wanted to get rid of that. Storage was handled by \"diskful\" machines that had dedicated trays of HDDs connected to their motherboards. If your datacenters and manufacturing processes are optimized for building machines that are either compute or storage but not both, perhaps the more normal cloud model is hard to support and that pushes you towards trying to aggregate storage even for \"local\" SSD or something. reply deadmutex 14 hours agorootparentThe GCE claim is unverified. OP seems to be referring to PD-SSD and not LocalSSD reply rwiggins 5 hours agorootparentGCE local SSDs absolutely are on the same host as the VM. The docs [0] are pretty clear on this, I think: > Local SSD disks are physically attached to the server that hosts your VM. Disclosure: I work on GCE. [0] https://cloud.google.com/compute/docs/disks/local-ssd reply flaminHotSpeedo 4 hours agorootparentprevThey're claiming so, but they're wrong. reply wiredfool 12 hours agorootparentprevAre you saying that a reboot wipes the ephemeral disks? Or a stop the instance and start the instance from AWS console/api? reply ianburrell 12 hours agorootparentReboot keeps the instance storage volumes. Restarting wipes them. Starting frequently migrates to new host. And the \"restart\" notices AWS sends are likely cause the host has a problem and need to migrate it. reply youngtaff 13 hours agorootparentprev> At work, someone used fast NVMe instance storage for Clickhouse which is a database. It was a huge hassle to copy data when instances were going to be restarted because the data would be lost. This post on how Discord RAIDed local NVMe volumes with slower remote volumes might be on interest https://discord.com/blog/how-discord-supercharges-network-di... reply ianburrell 12 hours agorootparentWe moved to running Clickhouse on EKS with EBS volumes for storage. It can better survive instances going down. I didn't work on it so don't how much slower it is. Lowering the management burden was big priority. reply colechristensen 16 hours agorootparentprevFor AWS there are EBS volumes attached through a custom hardware NVMe interface and then there's Instance Store which is actually local SSD storage. These are different things. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Instance... reply kwillets 15 hours agorootparentEBS is also slower than local NVMe mounts on i3's. Also, both features use Nitro SSD cards, according to AWS docs. The Nitro architecture is all locally attached -- instance storage to the instance, EBS to the EBS server. reply Salgat 12 hours agorootparentprevAt first you'd think maybe they can do a volume copy from a snapshot to a local drive on instance creation but even at 100gbps you're looking at almost 3 minutes for a 2TB drive. reply ddorian43 16 hours agorootparentprevDo you have a link to explain this? I dont think its true. reply zokier 16 hours agorootparentprevWhat makes you think that? reply bfncieezo 13 hours agorootparentprevinstances can have block storage which is network attached, or local attached ssd/nvme. its 2 separate things. reply choppaface 13 hours agorootparentprevNope! Well not as advertised. There are instances, usually more expensive ones, where there are supposed to be local NVME disks dedicated to the instance. You're totally right that providing good I/O is a big problem! And I have done studies myself showing just how bad Google Cloud is here, and have totally ditched Google Cloud for providing crappy compute service (and even worse customer service). reply karmakaze 9 hours agorootparentprevI've run CI/CD pipelines on EC2 machines with local storage, typically running Raid-0, btrfs, noaccestime. I didn't care if the filesystem got corrupt or whatever, I had a script that would rebuild it in under 30mins. In addition to the performance you're not paying by IOPs. reply crazygringo 16 hours agorootparentprev> It's about SSDs being two orders of magnitude slower than datacenter networks. Could that have to do with every operation requiring a round trip, rather than being able to queue up operations in a buffer to saturate throughput? It seems plausible if the interface protocol was built for a device it assumed was physically local and so waited for confirmation after each operation before performing the next. In this case it's not so much the throughput rate that matters, but the latency -- which can also be heavily affected by buffering of other network traffic. reply Nextgrid 16 hours agorootparentUnderlying protocol limitations wouldn't be an issue - the cloud provider's implementation can work around that. They're unlikely to be sending sequential SCSI/NVMe commands over the wire - instead, the hypervisor pretends to be the NVME device, but then converts to some internal protocol (that's less chatty and can coalesce requests without waiting on individual ACKs) before sending that to the storage server. The problem is that ultimately your application often requires the outcome of a given IO operation to decide which operation to perform next - let's say when it comes to a database, it should first read the index (and wait for that to complete) before it knows the on-disk location of the actual row data which it needs to be able to issue the next IO operation. In this case, there's no other solution than to move that application closer to the data itself. Instead of the networked storage node being a dumb blob storage returning bytes, the networked \"storage\" node is your database itself, returning query results. I believe that's what RDS Aurora does for example, every storage node can itself understand query predicates. reply _Rabs_ 16 hours agoparentprevSo much of this. The amount of times I've seen someone complain about slow DB performance when they're trying to connect to it from a different VPC, and bottlenecking themselves to 100Mbits is stupidly high. Literally depending on where things are in a data center... If you're looking for closely coupled and on a 10G line on the same switch, going to the same server rack. I bet you performance will be so much more consistent. reply bugbuddy 16 hours agorootparentAren’t 10G and 100G connections standard nowadays in data centers? Heck, I thought they were standard 10 years ago. reply geerlingguy 16 hours agorootparentDatacenters are up to 400 Gbps and beyond (many places are adopting 1+ Tbps on core switching). However, individual servers may still operate at 10, 25, or 40 Gbps to save cost on the thousands of NICs in a row of racks. Alternatively, servers with multiple 100G connections split that bandwidth allocation up among dozens of VMs so each one gets 1 or 10G. reply KaiserPro 13 hours agorootparentprevYes, but you have to think about contention. Whilst the Top of rack might have 2x400 gig links to the core, thats shared with the entire rack, and all the other machines trying to shout at the core switching infra. Then stuff goes away, or route congested, etc, etc, etc. reply pixl97 16 hours agorootparentprevBandwidth delay product does not help serialized transactions. If you're reaching out to disk for results, or if you have locking transactions on a table the achievable operations drops dramatically as latency between the host and the disk increases. reply bee_rider 15 hours agorootparentThe typical way to trade bandwidth away for latency would, I guess, be speculative requests. In the CPU world at least. I wonder if any cloud providers have some sort of framework built around speculative disk reads (or maybe it is a totally crazy trade to make in this context)? reply Nextgrid 10 hours agorootparentYou'd need the whole stack to understand your data format in order to make speculative requests useful. It wouldn't surprise me if cloud providers indeed do speculative reads but there isn't much they can do to understand your data format, so chances are they're just reading a few extra blocks beyond where your OS read and are hoping that the next OS-initiated read will fall there so it can be serviced using this prefetched data. Because of full-disk-encryption, the storage stack may not be privy to the actual data so it couldn't make smarter, data-aware decisions even if it wanted to, limiting it to primitive readahead or maybe statistics based on previously-seen patterns (if it sees that a request for block X is often followed by block Y, it may choose to prefetch that next time it sees block X accessed). A problem in applications such as databases is when the outcome of an IO operation is required to initiate the next one - for example, you must first read an index to know the on-disk location of the actual row data. This is where the higher latency absolutely tanks performance. A solution could be to make the storage drives smarter - have an NVME command that could say like \"search in between this range for this byte pattern\" and one that can say \"use the outcome of the previous command as a the start address and read N bytes from there\". This could help speed up the aforementioned scenario (effectively your drive will do the index scan & row retrieval for you), but would require cooperation between the application, the filesystem and the encryption system (typical, current FDE would break this). reply treflop 13 hours agorootparentprevOften times it’s the app (or something high level) that would need speculative requests, which it may not be possible in the given domain. I don’t think it’s possible in most domains. reply pixl97 14 hours agorootparentprevI mean we already have readahead in the kernel. This said the problem can get more complex than this really fast. Write barriers for example and dirty caches. Any application that forces writes and the writes are enforced by the kernel are going to suffer. The same is true for SSD settings. There are a number of tweakable values on SSDs when it comes to write commit and cache usage which can affect performance. Desktop OS's tend to play more fast and loose with these settings and servers defaults tend to be more conservative. reply nixass 16 hours agorootparentprev400G is fairly normal thing in DCs nowadays reply silverquiet 16 hours agorootparentprev> Literally depending on where things are in a data center I thought cloud was supposed to abstract this away? That's a bit of a sarcastic question from a long-time cloud skeptic, but... wasn't it? reply doubled112 16 hours agorootparentReality always beats the abstraction. After all, it's just somebody else's computer in somebody else's data center. reply bombcar 15 hours agorootparentWhich can cause considerable \"amusement\" depending on the provider - one I won't name directly but is much more centered on actual renting racks than their (now) cloud offering - if you had a virtual machine older than a year or so, deleting and restoring it would get you on a newer \"host\" and you'd be faster for the same cost. Otherwise it'd stay on the same physical piece of hardware it was allocated to when new. reply doubled112 15 hours agorootparentAmusing is a good description. \"Hardware degradation detected, please turn it off and back on again\" I could do a migration with zero downtime in VMware for a decade but they can't seamlessly move my VM to a machine that works in 2024? Great, thanks. Amusing. reply bombcar 15 hours agorootparentI have always been incredibly saddened that apparently the cloud providers usually have nothing as advanced as old VMware was. reply wmf 14 hours agorootparentprevCloud providers have live migration now but I guess they don't want to guarantee anything. reply bombcar 12 hours agorootparentIt's better (and better still with other providers) but I naively thought that \"add more RAM\" or \"add more disk\" was something they would be able to do with a reboot at most. Nope, some require a full backup and restore. reply wmf 11 hours agorootparentResizing VMs doesn't really fit the \"cattle\" thinking of public cloud, although IMO that was kind of a premature optimization. This would be a perfect use case for live migration. reply treflop 13 hours agorootparentprevCloud makes provisioning more servers quicker because you are paying someone to basically have a bunch of servers ready to go right away with an API call instead of a phone call, maintained by a team that isn’t yours, with economies of scale working for the provider. Cloud does not do anything else. None of these latency/speed problems are cloud-specific. If you have on-premise servers and you are storing your data on network-attached storage, you have the exact same problems (and also the same advantages). Unfortunately the gap between local and network storage is wide. You win some, you lose some. reply silverquiet 11 hours agorootparentOh, I'm not a complete neophyte (in what seems like a different life now, I worked for a big hosting provider actually), I was just surprised that there was a big penalty for cross-VPC traffic implied by the parent poster. reply kccqzy 15 hours agorootparentprevIt's more of a matter of adding additional abstraction layers. For example in most public clouds the best you can hope for is to place two things in the same availability zone to get the best performance. But when I worked at Google, internally they had more sophisticated colocation constraint than that: for example you can require two things to be on the same rack. reply boulos 16 hours agoparentprevI'm not sure which external or internal product you're talking about, but there are no networks involved for Local SSD on GCE: https://cloud.google.com/compute/docs/disks/local-ssd Are you referring to PD-SSD? Internal storage usage? reply vidarh 1 hour agoparentprevIt does not fundamentally have to be. That's an architectural choice driven by cloud providers backing off from the \"instances can die on you\" choice that AWS started with and then realized customers struggled with, towards attempting to keep things running in the face of many types of hardware failures. Which is ironic given that when building on-prem/colo'ed setups you'll replicate things to be prepared for unknown lengths of downtime while equipment is repaired or replaced, so this was largely cloud marketing coming bak to bite cloud providers' collective asses. Not wanting instances to die \"randomly\" for no good reason does not always mean wanting performance sacrifices for the sake of more resilient instances. But AWS at least still offers plenty of instances with instance storage. If I'm setting up my own database cluster, while I don't want it running on cheap consumer-grade hardware without dual power supplies and RAID, I also don't want to sacrifice SSD speed for something network-attached to survive a catastrophic failure when I'm going to have both backups, archived shipped logs and at least one replica anyway. I'd happily pick network-attached storage for many things if it gets me increased resilience, but selling me a network-attached SSD, unless it replicates local SSD performance characteristics, is not competitive for applications where performance matters and I'm set up to easily handle system-wide failures anyway. reply jsolson 5 hours agoparentprevThis is untrue of Local SSD (https://cloud.google.com/local-ssd) in Google Cloud. Local SSDs are PCIe peripherals, not network attached. There are also multiple Persistent Disk (https://cloud.google.com/persistent-disk) offerings that are backed by SSDs over the network. (I'm an engineer on GCE. I work directly on the physical hardware that backs our virtualization platform.) reply jiggawatts 3 hours agorootparentIt's notable that your second link has a screenshot for 24(!) NVMe SSDs totalling 9 terabytes, but the aggregate performance is 2.4M IOPS and 9.3 GB/s for reads. In other words, just 100K/400MB per individual SSD, which is very low these days. For comparison, a single 1 TB consumer SSD can deliver comparable numbers (lower IOPS but higher throughput). If I plugged 24 consumer SSDs into a box, I would expect over 30M IOPS and near the memory bus limit for throughput (>50 GB/s). reply jsolson 2 hours agorootparentI should've aimed for more clarity in my original comment -- the first link is to locally attached storage. The second is network attached storage (what the GP was likely referring to, but not what is described in the article). Persistent Disk is not backed by single devices (even for a single NVMe attachment), but by multiple redundant copies spread across power and network failure domains. Those volumes will survive the failure of the VM to which they are attached as well as the failure of any individual volume or host. reply scottlamb 15 hours agoparentprev> The problem is that this network is so large and slow that it can't give you anywhere near the performance of a local SSD. This wasn't a problem for hard drives, which was the backing technology when a lot of these network attached storage systems were invented, because they are fundamentally slow compared to networks, but it is a problem for SSD. Certainly true that SSD bandwidth and latency improvements are hard to match, but I don't understand why intra-datacenter network latency in particular is so bad. This ~2020-I-think version of the \"Latency Numbers Everyone Should Know\" says 0.5 ms round trip (and mentions \"10 Gbps network\" on another line). [1] It was the same thing in a 2012 version (that only mentions \"1 Gbps network\"). [2] Why no improvement? I think that 2020 version might have been a bit conservative on this line, and nice datacenters may even have multiple 100 Gbit/sec NICs per machine in 2024, but still I think the round trip actually is strangely bad. I've seen experimental networking stuff (e.g. RDMA) that claims significantly better latency, so I don't think it's a physical limitation of the networking gear but rather something at the machine/OS interaction area. I would design large distributed systems significantly differently (be much more excited about extra tiers in my stack) if the standard RPC system offered say 10 µs typical round trip latency. [1] https://static.googleusercontent.com/media/sre.google/en//st... [2] https://gist.github.com/jboner/2841832 reply dekhn 14 hours agorootparentModern data center networks don't have full cross connectivity. Instead they are built using graphs and hierarchies that provide less than the total bandwidth required for all pairs of hosts to be communicating. This means, as workloads start to grow and large numbers of compute hosts demand data IO to/from storage hosts, the network eventually gets congested, which typically exhibits as higher latencies and more dropped packets. Batch jobs are often relegated to \"spare\" bandwidth while serving jobs often get dedicated bandwidth At the same time, ethernetworks with layered network protocols on top typically have a fair amount of latency overhead, that makes it much slower than bus-based direct-host-attached storage. I was definitely impressed at how quickly SSDs reached and then exceeded SATA bandwidth. nvme has made a HUGE difference here. reply KaiserPro 13 hours agorootparentprevNetworks are not reliable, despite what you hear, so latency is used to mask re-tries and delays. The other thing to note about big inter-DC links are heavily QoS'd and contented, because they are both expensive and a bollock to maintain. Also, from what I recall, 40gig links are just parallel 10 gig links, so have no lower latency. I'm not sure if 100/400 gigs are ten/fourty lines of ten gigs in parallel or actually able to issue packets at 10/40 times a ten gig link. I've been away from networking too long reply wmf 12 hours agorootparent40gig links are just parallel 10 gig links, so have no lower latency That's not correct. Higher link speeds do have lower serialization latency, although that's a small fraction of overall network latency. reply scottlamb 13 hours agorootparentprev> Networks are not reliable, despite what you hear, so latency is used to mask re-tries and delays. Of course, but even the 50%ile case is strangely slow, and if that involves retries something is deeply wrong. reply KaiserPro 13 hours agorootparentYou're right, but TCP doesn't like packets being dropped halfway through a stream. If you have a highly QoS'd link then you'll see latency spikes. reply scottlamb 13 hours agorootparentAgain, I'm not talking about spikes (though better tail latency is always desirable) but poor latency in the 50%ile case. And for high-QoS applications, not batch stuff. The snap paper linked elsewhere in the thread shows 10 µs latencies; they've put in some optimization to achieve that, but I don't really understand why we don't expect close to that with standard kernel networking and TCP. reply vitus 9 hours agorootparent> The snap paper linked elsewhere in the thread shows 10 µs latencies; they've put in some optimization to achieve that, but I don't really understand why we don't expect close to that with standard kernel networking and TCP. You can get similar results by looking at comparisons between DPDK and kernel networking. Most of the usual gap comes from not needing to context-switch for kernel interrupt handling, zero-copy abstractions, and busy polling (wherein you trade CPU for lower latency instead of sleeping between iterations if there's no work to be done). https://talawah.io/blog/linux-kernel-vs-dpdk-http-performanc... goes into some amount of detail comparing request throughput of an unoptimized kernel networking stack, optimized kernel networking stack, and DPDK. I'm not aware of any benchmarks (public or private) comparing Snap vs DPDK vs Linux, so that's probably as close as you'll get. reply CoolCold 4 hours agorootparentGreat reading, thanks for the link on vanilla vs dpdk reply kccqzy 14 hours agorootparentprevThat document is probably deliberately on the pessimistic side to encourage your code to be portable across all kinds of \"data centers\" (however that is defined). When I previously worked at Google, the standard RPC system definitely offered 50 microseconds of round trip latency at the median (I measured it myself in a real application), and their advanced user-space implementation called Snap could offer about 10 microseconds of round trip latency. The latter figure comes from page 9 of https://storage.googleapis.com/gweb-research2023-media/pubto... > nice datacenters may even have multiple 100 Gbit/sec NICs per machine in 2024, Google exceeded 100Gbps per machine long before 2024. IIRC it had been 400Gbps for a while. reply scottlamb 14 hours agorootparentInteresting. I worked at Google until January 2021. I see 2019 dates on that PDF, but I wasn't aware of snap when I left. There was some alternate RPC approach (Pony Express, maybe? I get the names mixed up) that claimed 10 µs or so but was advertised as experimental (iirc had some bad failure modes at the time in practice) and was simply unavailable in many of the datacenters I needed to deploy in. Maybe they're two names for the same thing. [edit: oh, yes, starting to actually read the paper now, and: \"Through Snap, we created a new communication stack called Pony Express that implements a custom reliable transport and communications API.\"] Actual latency with standard Stubby-over-TCP and warmed channels...it's been a while, so I don't remember the number I observed, but I remember it wasn't that much better than 0.5 ms. It was still bad enough that I didn't want to add a tier that would have helped with isolation in a particularly high-reliability system. reply kccqzy 13 hours agorootparentSnap was the external name for the internal project known as User Space Packet Service (abbreviated USPS) so naturally they renamed it prior to publication. I deployed an app using Pony Express in 2023 and it was available in the majority of cells worldwide. Pony Express supported more than just RPC though. The alternate RPC approach that you spoke of was called Void. It had been experimental for a long time and indeed it wasn't well known even inside Google. > but I remember it wasn't that much better than 0.5 ms. If you and I still worked at Google I'd just give you an automon dashboard link showing latency an order of magnitude better than that to prove myself… reply scottlamb 12 hours agorootparentInteresting, thanks! > If you and I still worked at Google I'd just give you an automon dashboard link showing latency an order of magnitude better than that to prove myself… I believe you, and I think in principle we should all be getting the 50 µs latency you're describing within a datacenter with no special effort. ...but it doesn't match what I observed, and I'm not sure why. Maybe difference of a couple years. Maybe I was checking somewhere with older equipment, or some important config difference in our tests. And obviously my memory's a bit fuzzy by now but I know I didn't like the result I got. reply Szpadel 14 hours agorootparentprevwith such speed and CXL gaining traction (think ram and GPUs over network) why network SSD is still issue? you could have like one storage server per rack that would serve storage only for that particular rack you could easily have like 40GB/s with some over provisioning / bucketing reply vlovich123 16 hours agoparentprevWhy do they fundamentally need to be network attached storage instead of local to the VM? reply drewda 16 hours agorootparentThe major clouds do offer VMs with fast local storage, such as SSDs connected by NVMe connections directly to the VM host machine: - https://cloud.google.com/compute/docs/disks/local-ssd - https://learn.microsoft.com/en-us/azure/virtual-machines/ena... - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ssd-inst... They sell these VMs at a higher cost because it requires more expensive components and is limited to host machines with certain configurations. In our experience, it's also harder to request quota increases to get more of these VMs -- some of the public clouds have a limited supply of these specific types of configurations in some regions/zones. As others have noted, instance storage isn't as dependable. But it can be the most performant way to do IO-intense processing or to power one node of a distributed database. reply Filligree 16 hours agorootparentprevThey don't. Some cloud providers (i.e. Hetzner) let you rent VMs with locally attached NVMe, which is dramatically faster than network-attached even factoring in the VM tax. Of course then you have a single point of failure, in the PCIe fabric of the machine you're running on if not the NVMe itself. But if you have good backups, which you should, then the juice really isn't worth the squeeze for NAS storage. reply ssl-3 16 hours agorootparentA network adds more points of failure. It does not reduce them. reply supriyo-biswas 16 hours agorootparentA network attached, replicated storage hedges against data loss but increases latency; however most customers usually prefer higher latency to data loss. As an example, see the highly upvoted fly.io thread[1] with customers complaining about the same thing. [1] https://news.ycombinator.com/item?id=36808296 reply ssl-3 15 hours agorootparentLocally-attached, replicated storage also hedges against data loss. reply supriyo-biswas 15 hours agorootparentRAID rebuild times make it an unviable option and customers typically expect problematic VMs to be live-migrated to other hosts with the disks still having their intended data. The self hosted version of this is GlusterFS and Ceph, which have the same dynamics as EBS and its equivalents in other cloud providers. reply mike_hearn 14 hours agorootparentWith NVMe SSDs? What makes RAID unviable in that environment? reply dijit 14 hours agorootparentThis depends, like all things. When you say RAID, what level? Software-raid or hardware raid? What controller? Let's take best-case: RAID10, small enough (but many) NVMe drives and an LVM/Software RAID like ZFS, which is data aware so only rebuilds actual data: rebuilds will degrade performance enough potentially that your application can become unavailable if your IOPS are 70%+ of maximum. That's an ideal scenario, if you use hardware raid which is not data-aware then your rebuild times depend entirely on the size of the drive being rebuilt and it can punish IOPs even more during the rebuild. But it will affect your CPU less. There's no panacea. Most people opt for higher latency distributed storage where the RAID is spread across an enormous amount of drives, which makes rebuilds much less painful. reply timc3 4 hours agorootparentWhat I used to do was swap machines over from the one with failing disks to a live spare (slave in the old frowned upon terminology), do the maintenance and then replicate from the now live spare back if I had confidence it was all good. Yes it’s costly having the hardware to do that as it mostly meant multiple machines as I always wanted to be able to rebuild one whilst having at least two machines online. reply dijit 2 hours agorootparentIf you are doing this with your own hardware it is still less costly than cloud even if it mostly sits idle. Cloud is approx 5x sticker cost for compute if its sustained. Your discounts may vary, rue the day those discounts are taken away because we are all sufficiently locked in. reply crazygringo 16 hours agorootparentprevA network adds more points of failures but also reduces user-facing failures overall when properly architected. If one CPU attached to storage dies, another can take over and reattach -- or vice-versa. If one network link dies, it can be rerouted around. reply bombcar 15 hours agorootparentUsing a SAN (which is what networked storage is, after all) also lets you get various \"tricks\" such as snapshots, instant migration, etc for \"free\". reply SteveNuts 16 hours agorootparentprevBecause even if you can squeeze 100TB or more of SSD/NVMe in a server, and there are 10 tenants using the machine, you're limited to 10TB as a hard ceiling. What happens when one tenant needs 200TB attached to a server? Cloud providers are starting to offer local SSD/NVMe, but you're renting the entire machine, and you're still limited to exactly what's installed in that server. reply vel0city 15 hours agorootparentGiven AWS and GCP offer multiple sizes for the same processor version with local SSDs, I don't think you have to rent the entire machine. Search for i3en API names and you'll see: i3en.large, 2x CPU, 1250GB SSD i3en.xlarge, 4x CPU, 2500GB SSD i3en.2xlarge, 8x CPU, 2x2500GB SSD i3en.3xlarge, 12x CPU, 7500GB SSD i3en.6xlarge, 24x CPU, 2x7500GB SSD i3en.12xlarge, 48x CPU, 4x7500GB SSD i3en.24xlarge, 96x CPU, 8x7500GB SSD i3en.metal, 96x CPU, 8x7500GB SSD So they've got servers with 96 CPUs and 8x7500GB SSDs. You can get a slice of one, or you can get the whole one. All of these are the ratio of 625GB of local SSD per CPU core. https://instances.vantage.sh/ On GCP you can get a 2-core N2 instance type and attach multiple local SSDs. I doubt they have many physical 2-core Xeons in their datacenters. reply jalk 16 hours agorootparentprevHow is that different from how cores, mem and network bandwidth is allotted to tenants? reply baq 15 hours agorootparentIt isn't. You could ask for network-attached CPUs or RAM. You'd be the only one, though, so in practice only network-attached storage makes sense business-wise. It also makes sense if you need to provision larger-than-usual amounts like tens of TB - these are usually hard to come by in a single server, but quite mundane for storage appliances. reply pixl97 16 hours agorootparentprevBecause a fair number of customers spin up another image when cores/mem/bandwidth run low. Dedicated storage breaks that paradigm. Also, adding, if I am on an 8 core machine and need 16, network storage can be detached from host A and connected to host B. In dedicated storage it must be fully copied over first. reply taneq 13 hours agorootparentprev> What happens when one tenant needs 200TB attached to a server? Link to this mythical hosting service that expects far less than 200TB of data per client but just pulls a sad face and takes the extra cost on board when a client demands it. :D reply pclmulqdq 16 hours agorootparentprevReliability. SSDs break and screw up a lot more frequently and more quickly than CPUs. Amazon has published a lot on the architecture of EBS, and they go through a good analysis of this. If you have a broken disk and you locally attach, you have a broken machine. RAID helps you locally, but fundamentally relies on locality and low latency (and maybe custom hardware) to minimize the time window where you get true data corruption on a bad disk. That is insufficient for cloud storage. reply vlovich123 5 hours agorootparentSure, but there's plenty of software that's written to use distributed unreliable storage similar to how cloud providers write their own software (e.g. Kafka). I can understand if many applications are just need something like EBS that's durable but looks like a normal disk, but not so sure it's a fundamentally required abstraction. reply Retric 16 hours agorootparentprevRedundancy, local storage is a single point of failure. You can use local SSD’s as slow RAM, but anything on it can go away at any moment. reply cduzz 15 hours agorootparentI've seen SANs get nuked by operator error or by environmental issues (overheated DC == SAN shuts itself down). Distributed clusters of things can work just fine on ephemeral local storage (aka local storage). A kafka cluster or an opensearch cluster will be fine using instance local storage, for instance. As with everything else.... \"it depends\" reply Retric 14 hours agorootparentSure distributed clusters get back to network/workload limitations. reply wmf 16 hours agoparentprevNo, the i3/i4 VMs discussed in the blog have local SSDs. The network isn't the reason local SSDs are slow. reply zokier 16 hours agoparentprev> SSDs in the cloud are attached over a network, and fundamentally have to be Not on AWS. Instance stores (what the article is about) are physical local disks. reply jsolson 5 hours agorootparentSame for Google (the GP is incorrect about how GCE Local SSD works). reply ec109685 5 hours agoparentprevThis is an interesting benchmark that compares disk read latency across different clouds across various disk configurations, including SSD's and EBS: https://github.com/scylladb/diskplorer Google's disk perform quite poorly. And how Discord worked around it: https://discord.com/blog/how-discord-supercharges-network-di... reply mkoubaa 16 hours agoparentprevDumb question. Why does the network have to be slow? If the SSDs are two feet away from the motherboard and there's an optical connection to it, shouldn't it be fast? Are data centers putting SSDs super far away from motherboards? reply supriyo-biswas 15 hours agorootparentIt’s not the network being slow, but dividing the available network bandwidth amongst all users, while also distributing the written data to multiple nodes reliably so that one tenant doesn’t hog resources is quite challenging. The pricing structure is meant to control resource usage; a discussion of the exact prices and how much profit AWS or any other cloud provider makes is a separate discussion. reply formercoder 16 hours agorootparentprevWhat happens when your vm is live migrated 1000 feet away or to a different zone? reply bugbuddy 16 hours agorootparentprev> One theory is that EC2 intentionally caps the write speed at 1 GB/s to avoid frequent device failure, given the total number of writes per SSD is limited. This is the theory that I would bet on because it lines up with their bottom line. reply Dylan16807 16 hours agorootparentBut the sentence right after undermines it. > However, this does not explain why the read bandwidth is stuck at 2 GB/s. Faster read speeds would give them a more enticing product without wearing drives out. reply bugbuddy 14 hours agorootparentThey may be limiting the read artificially to increase your resource utilization else where. If you have disk bottleneck then you would be more likely to use more instances. It is still about the bottom line. reply Dylan16807 14 hours agorootparentThat could be. But it's a completely different reason. If you summarize everything as \"bottom line\", you lose all the valuable information. reply tw04 15 hours agoparentprev>The problem is that this network is so large and slow that it can't give you anywhere near the performance of a local SSD. *As implemented in the public cloud providers. You can absolutely get better than local disk speeds from SAN devices and we've been doing it for decades. To do it on-prem with flash devices will require NVMe over FC or Ethernet and an appropriate storage array. Modern all-flash array performance is measured in millions of IOPS. Will there be a slight uptick in latency? Sure, but it's well worth it for the data services and capacity of an external array for nearly every workload. reply folmar 9 hours agorootparentA quick comparison of marketing slides lands you with 0.5 MOPS for FC-NVMe for Qlogic 2770 and 0.7 MOPS for PCIe Micron 9100 PRO, so the better speeds are not quite there, although spending quite a lot on server gear lands you near the performance of a workstation-grade drive from 2017. Which is still not bad, when I was shopping around in 2018 no money could buy performance comparable to a locally-attached NVMe in a more professional/datacenter-ready form. [1] https://media-www.micron.com/-/media/client/global/documents... [2] https://www.marvell.com/content/dam/marvell/en/public-collat... reply nostrademons 16 hours agoparentprevMakes me wonder if we're on the crux of a shift back to client-based software. Historically changes in the relative cost of computing components have driven most of the shifts in the computing industry. Cheap teletypes & peripherals fueled the shift from batch-processing mainframes to timesharing minicomputers. Cheap CPUs & RAM fueled the shift from minicomputers to microcomputers. Cheap and fast networking fueled the shift from desktop software to the cloud. Will cheap SSDs & TPU/GPUs fuel a shift back toward thicker clients? There are a bunch of supporting social trends toward this as well. Renewed emphasis on privacy. Big Tech canceling beloved products, bricking devices, and generally enshittifying everything - a lot of people want locally-controlled software that isn't going to get worse at the next update. Ever-rising prices which make people want to lock in a price for the device and not deal with increasing rents for computing power. reply davkan 13 hours agorootparentI think a major limiting factor here for many applications is that mobile users are a huge portion of the user base. In that space storage, and more importantly battery life, are still at a premium. Granted the storage cost just seems to be gouging from my layman’s point of view, so industry needs might force a shift upwards. reply mike_hearn 10 hours agorootparentMobile devices are the desktop computers of the 2010s though. They are mostly used with very thick clients. reply PaulHoule 12 hours agoparentprevThey don’t have to be. Architecturally there are many benefits to storage area networks but I have built plenty of systems which are self-contained, download a dataset to a cloud instance with a direct attached SSD, load it into a database and provide a different way. reply ejb999 16 hours agoparentprevHow much faster would the network need to get, in order to meet (or at least approach) the speed of a local SSD? are we talking about needing to 2x or 3x the speed, or by factors of hundreds or thousands? reply Nextgrid 16 hours agorootparentThe problem isn't necessarily speed, it's random access latency. What makes SSDs fast and \"magical\" is their low random-access latency compared to a spinning disk. The sequential-access read speed is merely a bonus. Networked storage negates that significantly, absolutely killing performance for certain applications. You could have a 100Gbps network and it still won't match a direct-attached SSD in terms of latency (it can only match it in terms of sequential access throughput). For many applications such as databases, random access is crucial, thus why nowadays' mid-range consumer hardware often outperforms hosted databases such as RDS unless they're so overprovisioned on RAM that the dataset is effectively always in there. reply baq 15 hours agorootparent100Gbps direct shouldn't be too bad, but it might be difficult to get anyone to sell it to you for exclusive usage in a vm... reply Ericson2314 11 hours agorootparentprevUm... why the hell does the network care whether I am doing random or sequential access? Your left that part out of your argument. reply Nextgrid 10 hours agorootparentAh sorry, my bad. You are correct that you can fire off many random access operations in parallel and get good throughput that way. The problem is that this is not possible when the next IO request depends on the result of a previous one, like in a database where you must first read the index to know the location of the row data itself. reply Ericson2314 10 hours agorootparentOK thanks yes that makes sense. Pipelining problems are real. reply Filligree 16 hours agorootparentprevThe Samsung 990 in my desktop provides ~3.5 GB/s streaming reads, ~2 GB/s 4k random-access reads, all at a latency measured at around 20-30 microseconds. My exact numbers might be a little off, but that's the ballpark you're looking at, and a 990 is a relatively cheap device. 10GbE is about the best you can hope for from a local network these days, but that's 1/5th the bandwidth and many times the latency. 100GbE would work, except the latency would still mean any read dependencies would be far slower than local storage, and I'm not sure there's much to be done about that; at these speeds the physical distance matters. In practice I'm having to architecture the entire system around the SSD just to not bottleneck it. So far ext4 is the only filesystem that even gets close to the SSD's limits, which is a bit of a pity. reply ants_a 1 hour agorootparentNetworking doesn't have to have high latency. You can buy network hardware that is able to provide sub-microsecond latency. Physical distance still matters, but 10% of typical NVMe latency gets you through a kilometer of fiber. reply wmf 16 hours agorootparentprevAround 4x-10x depending on how many SSDs you want. A single SSD is around the speed of a 100 Gbps Ethernet link. reply selectodude 16 hours agorootparentprevSATA3 is 6 Gbit, so each VM on a machine multiplied by 6 Gbit. For NVMe, probably closer to 4-5x that. You’d need some serious interconnects to get a server rack access to un-bottlenecked SSD storage. reply dan-robertson 13 hours agoparentprevI can see network attached SSDs having poor latency, but shouldn’t the networking numbers quoted in the article allow for higher throughput than observed? reply brucethemoose2 16 hours agoparentprevYeah this was my impression. I am but an end user, but I noticed that disk IO for a certain app was glacial compared to a local test deployment, and I chalked it up to networking/VM overhead reply Dylan16807 16 hours agoparentprevEven assuming that \"local\" storage is a lie, hasn't the network gotten a lot faster? The author is only asking for a 5x increase at the end of the post. reply fngjdflmdflg 16 hours agoparentprevSo do cloud vendors simply not use fast SSDs? If so I would expect the SSD manufacturers themselves to work on this problem. Perhaps they already are. reply adrr 16 hours agoparentprevIf the local drives are network drives(eg: SAN) then why are they ephemeral? reply baq 15 hours agorootparentlive vm migrations, perhaps reply waynesonfire 8 hours agoparentprevIn other words, they're saving money. This is a fundamental problem with cloud providers. Value created from technological innovation is captured by the cloud provider and bare minimum is shared to reduce prices. The margins are ridiculous. reply amir734jj 5 hours agoparentprevKind of funny but we use similar idea in Azure. reply paulddraper 12 hours agoparentprev> and fundamentally have to be Can you expound? reply nimbius 8 hours agoparentprevOnce again getting a sensible chuckle on hn listening to the cloud crowd whine and complain about what we used to call \"the SAN being slow\" with the architects argument \"the ssds will make things faster\" reply samstave 13 hours agoparentprevForgive if stupid, but when netflix was doing all their edge content boxes, where they were putting machines much more latency-close to customers... is/does/can this model kinda work for SSDs in a SScDN type of network (client---->CDN->SScDN-------SSD? reply throwawaaarrgh 14 hours agoparentprev> SSDs in the cloud are attached over a network, and fundamentally have to be SANs can still be quite fast, and instance storage is fast, both of which are available in cloud providers reply AlgorithmicTime 15 hours agoparentprev> SSDs in the cloud are attached over a network, and fundamentally have to be. No they don't. I work for a cloud provider and I can guarantee that your SSD is local to your VM. reply jsolson 5 hours agorootparentThis is also true for GCE Local SSD: https://cloud.google.com/local-ssd The GP is incorrect. reply kwillets 14 hours agoprevAWS docs and blogs describe the Nitro SSD architecture, which is locally attached with custom firmware. > The Nitro Cards are physically connected to the system main board and its processors via PCIe, but are otherwise logically isolated from the system main board that runs customer workloads. https://docs.aws.amazon.com/whitepapers/latest/security-desi... > In order to make the [SSD] devices last as long as possible, the firmware is responsible for a process known as wear leveling.... There’s some housekeeping (a form of garbage collection) involved in this process, and garden-variety SSDs can slow down (creating latency spikes) at unpredictable times when dealing with a barrage of writes. We also took advantage of our database expertise and built a very sophisticated, power-fail-safe journal-based database into the SSD firmware. https://aws.amazon.com/blogs/aws/aws-nitro-ssd-high-performa... This firmware layer seems like a good candidate for the slowdown. reply dan-robertson 13 hours agoparentYeah, I’m curious how they would respond to the claims in the article. In [1], they talk about aiming for low latency, for consistent performance (apparently other SSDs could stall at inopportune times), and support on-disk encryption. Latency is often in direct conflict with throughput (eg batching usually trades one for the other), and also matters a lot for plenty of filesystem or database tasks (indeed the OP links to a paper showing that popular databases, even column stores, struggle to use the full disk throughput, though I didn’t read why). Encryption is probably not the reason – dedicated hardware on modern chips can do AES at 50GB/s, though maybe it is if it increases latency? So maybe there’s something else to it like sharing between many vms on one host [1] https://m.youtube.com/watch?v=Cxie0FgLogg reply kwillets 13 hours agorootparentThe Nitro chipset claims 100 GB/s encryption, so that doesn't seem to be the reason. reply someguydave 12 hours agoparentprevYeah I wonder how Nitro balances the latency & bandwidth demands of multiple VMs while also minimizing memory cache misses on the CPU (I am assuming it uses DMA to talk to the main CPU cores) reply akira2501 11 hours agoparentprevI've noticed the firmware programming positions have been more common in their job listings lately. reply c0l0 16 hours agoprevSeeing the really just puny \"provisioned IOPS\" numbers on hugely expensive cloud instances made me chuckle (first in disbelief, then in horror) when I joined a \"cloud-first\" enterprise shop in 2020 (having come from a company that hosted their own hardware at a colo). It's no wonder that many people nowadays, esp. those who are so young that they've never experienced anything but cloud instances, seem to have little idea of how much performance you can actually pack in just one or two RUs today. Ultra-fast (I'm not parroting some marketing speak here - I just take a look at IOPS numbers, and compare them to those from highest-end storage some 10-12 years ago) NVMe storage is a big part of that astonishing magic. reply Aurornis 10 hours agoparent> It's no wonder that many people nowadays, esp. those who are so young that they've never experienced anything but cloud instances, seem to have little idea of how much performance you can actually pack in just one or two RUs today. On the contrary, young people often show up having learned on their super fast Apple SSD or a top of the line gaming machine with NVMe SSD. Many know what hardware can do. There’s no need to dunk on young people. Anyway, the cloud performance realities are well know to anyone who works in cloud performance. It’s part of the game and it’s learned by anyone scaling a system. It doesn’t really matter what you could do if you build a couple RUs yourself and hauled them down to the data center, because beyond simple single-purpose applications with flexible uptime requirements, that’s not a realistic option. reply EB66 8 hours agorootparent> because beyond simple single-purpose applications with flexible uptime requirements, that’s not a realistic option. I frequently hear this point expressed in cloud vs colo debates. The notion that you can't achieve high availability with simple colo deploys is just nonsense. Two colo deploys in two geographically distinct datacenters, two active physical servers with identical builds (RAIDed drives, dual NICs, A+B power) in both datacenters, a third server racked up just sitting as a cold spare, pick your favorite container orchestration scheme, rig up your database replication, script the database failover activation process, add HAProxy (or use whatever built-in scheme your orchestration system offers), sprinkle in a cloud service for DNS load balancing/failover (Cloudflare or AWS Route 53), automate and store backups off-site and you're done. Yes it's a lot of work, but so is configuring a similar level of redundancy and high availability in AWS. I've done it both ways and I prefer the bare metal colo approach. With colo you get vastly more bang for your buck and when things go wrong, you have a greater ability to get hands on, understand exactly what's going on and fix it immediately. reply zten 9 hours agorootparentprev> On the contrary, young people often show up having learned on their super fast Apple SSD or a top of the line gaming machine with NVMe SSD. Yes, this is often a big surprise. You can test out some disk-heavy app locally on your laptop and observe decent performance, and then have your day completely ruined when you provision a slice of an NVMe SSD instance type (like, i4i.2xlarge) and discover you're only paying for SATA SSD performance. reply seabrookmx 3 hours agorootparentThis doesn't stop at SSD's. Spin up an E2 VM in Google Cloud and there's a good chance you'll get a nearly 9 year Broadwell architecture chip running your workload! reply dboreham 6 hours agoparentprevSome of us are making a good living offboarding workloads from cloud onto bare metal with on-node NVMe storage. reply dijit 25 minutes agorootparentReally? I'd like to do this as a job. Are you hiring? Cloud is great for prototyping or randomly elastic workloads, but it feels like people are pushing highly static workloads from on-prem to cloud. I'd love to be part of the change going the other way. Especially since the skills for doing so seem to have dried up completely. reply jauntywundrkind 10 hours agoparentprevNVMe has been ridiculously great. I'm excited to see what happens to prices as E1 form factor ramps up! Much physically bigger drives allows for consolidation of parts, a higher ratio of flash chips to everything else, which seems promising. It's more a value line, but Intel's P5315 is 15TB at a quite low $0.9/GB. It might not help much with oops though. Amazing that we have PCIe 5.0 16GB/s and already are so near theoretical max (some lost to overhead), even on consumer cards. Going enterprise for the drive-writes-per-day (DWPD) is 100% worth it for most folks, but I am morbidly curious how different the performance profile would be running enterprise vs non these days. But reciprocally the high DWPD drives (Kioxia CD8P-V for example is DWPD of 3) seems to often come with somewhat more mild sustained 4k write oops, making me think maybe there's a speed vs reliability tradeoff that could be taken advantage of from consumer drives in some cases; not sure who wants tons of iops but doesn't actually intend to hit their Total Drive Writes, but it save you some iops/$ if so. That said, I'm shocked to see the enterprise premium is a lot less absurd than it used to be! (If you can find stock.) reply bcaxis 8 hours agorootparentThe main problem with consumer drives is the missing power loss protection (plp). M.2 drives just don't have space for the caps like an enterprise 2.5 u.2/u.3 drive will have. This matters when the DB calls a sync and it's expecting the data to be written safely to disk before it returns. A consumer drive basically stops everything until it can report success and your IOPS falls to like 1/100th of what the drive is capable of if it's happening alot. An enterprise drive with plp will just report success knowing it has the power to finish the pending writes. Full speed ahead. You can \"lie\" to the process at the VPS level by enabling unsafe write back cache. You can do it at the OS level by launching the DB with \"eatmydata\". You will get the full performance of your SSD. In the event of power loss you may well end up in an unrecoverable corrupted condition with these enabled. I believe that if you buy all consumer parts - an enterprise drive is the best place to up spend your money profitably on an enterprise bit. reply tumult 8 hours agorootparentMy experience lately is that consumer drives will also lie and use a cache, but then drop your data on the floor if the power is lost or there’s a kernel panic / BSOD. (Samsung and others.) reply bcaxis 6 hours agorootparentRumors of that. I've never actually seen it myself. reply tumult 36 minutes agorootparentI can get it to happen easily. 970 Evo Plus. Write a text file and kill the power within 20 seconds or so, assuming not much other write activity. File will be zeroes or garbage, or not present on the filesystem, after reboot. reply dijit 21 minutes agorootparentprevEh, I've definitely seen it. I buy Samsung drives relatively exclusively if that makes any difference. All that to say though: this is why things like journalling and write-ahead systems exist. OS design is mostly about working around physical (often physics related) limitations of hardware and one of those is what to do if you get caught in a situation where something is incomplete. The prevailing methodology is to paper over it with some atomic actions. For example: Copy-on-Write or POSIX move semantics (rename(2)). Then some spiffy young dev comes along and turns off all of those guarantees and says they made something ultra fast (*cough*mongodb*cough*) then maybe claims those guarantees are somewhere up the stack instead. This is almost always a lie. Also: Beware any database that only syncs to VFS. reply hypercube33 4 hours agorootparentprevOnly thing I ever have seen is some cheap Samsung drives slow to a crawl when their buffer fills or those super old Intel ssds that power loss to 8mb due to some firmware bug. reply Twirrim 16 hours agoprevDisclaimer: I work for OCI, opinion my own etc. We offer faster NVMe drives in instances. Our E4 Dense shapes ship with SAMSUNG MZWLJ7T6HALA-00AU3, which supports Sequential Reads of 7000 MB/s, and Sequential Write 3800 MB/s. From a general perspective, I would say the _likely_ answer to why AWS doesn't have faster NVMes at the moment is likely to be lack of specific demand. That's a guess, but that's generally how things go. If there's not enough specific demand being fed in through TAMs and the like for faster disks, upgrades are likely to be more of an after-thought, or reflecting supply chain. I know there's a tendency when you engineer things, to just work around, or work with the constraints, and grumble amongst your team, but it's incredibly invaluable if you can make sure your account manager knows what shortcomings you've had to work around. reply e12e 9 hours agoparent> I work for OCI Ah: Oracle cloud infra https://blogs.oracle.com/cloud-infrastructure/post/announcin... I keep forgetting Oracle is in the cloud business too. \"Make it rain\", I guess :) reply Twirrim 8 hours agorootparentDoh, sorry. I'm usually good about not using that acronym! reply nine_k 9 hours agoparentprevI very much expect AWS SSD / NVMe upgrades to be well thought-out ahead of time, and optimized for both upfront cost and for longevity / durability. Speed may be a third consideration. reply flaminHotSpeedo 4 hours agorootparentYeah, hardware and forecasting for cloud providers is basically the definition of deliberate. If anything I'd guess it's a procurement issue, parity between regions is a big thing and it's hard to supply dozens of regions around the world with the latest hardware hotness reply siliconc0w 16 hours agoprevCore count plus modern nvme actually make a great case for moving away from the cloud- before it was, \"your data probably fits into memory\". These are so fast that they're close enough to memory so it's \"your data surely fits on disk\". This reduces the complexity of a lot of workloads so you can just buy a beefy server and do pretty insane caching/calculation/serving with just a single box or two for redundancy. reply malfist 16 hours agoparentI keep hearing that, but that's simply not true. SSDs are fast, but they're several orders of magnitude slower than RAM, which is orders of magnitude slower than CPU Cache. Samsung 990 Pro 2TB has a latency of 40 μs DDR4-2133 with a CAS 15 has a latency of 14 nano seconds. DDR4 latency is 0.035% of one of the fastest SSDs, or to put it another way, DDR4 is 2,857x faster than an SSD. L1 cache is typically accessible in 4 clock cycles, in 4.8 ghz cpu like the i7-10700, L1 cache latency is sub 1ns. reply LeifCarrotson 16 hours agorootparentI wonder how many people have built failed businesses that never had enough customer data to exceed the DDR4 in the average developer laptop, and never had so many simultaneous queries it couldn't be handled by a single core running SQLite, but built the software architecture on a distributed cloud system just in case it eventually scaled to hundreds of terabytes and billions of simultaneous queries. reply Szpadel 14 hours agorootparentIn may day job I often see systems that have the opposite. Especially for database queries, developers tested on local machine with 100s of records and everything was quick and snappy and on production with mere millions of records I often see queries taking minutes up to a hour just because some developer didn't see need for creating indexes or created query in a way there is no way to even create any index that would work reply layer8 13 hours agorootparentThat’s true, but has little to do with distributed cloud architecture vs. single local instance. reply icedchai 14 hours agorootparentprevMany. I regularly see systems built for \"big data\", built for scale using \"serverless\" and some proprietary cloud database (like DynamoDB), storing a few hundred megabytes total. 20 years ago we would've built this on PHP and MySQL and called it a day. reply malfist 15 hours agorootparentprevI totally hear you about that. I work for FAANG, and I'm working on a service that has to be capable of sending 1.6m text messages in less than 10 minutes. The amount of complexity the architecture has because of those constraints is insane. When I worked at my previous job, management kept asking for that scale of designs for less than 1/1000 of the throughput and I was constantly pushing back. There's real costs to building for more scale than you need. It's not as simple as just tweaking a few things. To me there's a couple of big breakpoints in scale: * When you can run on a single server * When you need to run on a single server, but with HA redundancies * When you have to scale beyond a single server * When you have to adapt your scale to deal with the limits of a distributed system, i.e. designing for DyanmoDB's partition limits. Each step in that chain add irrevocable complexity, adds to OE, adds to cost to run and cost to build. Be sure you have to take those steps before you decide too. reply kuschku 13 hours agorootparentMaybe I'm misunderstanding something, but that's about 2700 a second. Or about 3Mbps. Even a very unoptimized application running on a dev laptop can serve 1Gbps nowadays without issues. So what are the constraints that demand a complex architecture? reply rdoherty 10 hours agorootparentI'm not the OP but a few things: * Reading/fetching the data - usernames, phone number, message, etc. * Generating the content for each message - it might be custom per person * This is using a 3rd party API that might take anywhere from 100ms to 2s to respond, and you need to leave a connection open. * Retries on errors, rescheduling, backoffs * At least once or at most once sends? Each has tradeoffs * Stopping/starting that many messages at any time * Rate limits on some services you might be using alongside your service (network gateway, database, etc) * Recordkeeping - did the message send? When? reply goguy 13 hours agorootparentprevThat really doesn't require that much complexity. I used to send something like 250k a minute complete with delivery report processing from a single machine running a bunch of other services like 10 years ago. reply nine_k 9 hours agorootparentNice. But average latency is not the whole picture; tail latency is. For good tail latency and handling of spikes, you have to have a sizable \"untapped\" reserve of performance. reply disqard 13 hours agorootparentprevI'm trying to guess what \"OE\" stands for... over engineering? operating expenditure? I'd love to know what you meant :) reply madisp 13 hours agorootparentprobably operating expenses reply malfist 12 hours agorootparentprevSorry, thought it was a common term. Operational Excellence. All the effort and time it takes to keep a service online, on call included reply Repulsion9513 15 hours agorootparentprevA LOT... especially here. reply kristopolous 11 hours agorootparentprevYou're not considered serious if you don't. Kinda stupid. reply nine_k 9 hours agorootparentIn the startup world, this is correct. The success that VCs are after is when your customer base doubles every month. Better yet, every week. Having a reasonably scalable infra at the start ensures that a success won't kill you. Of course, the chances of a runaway success like this are slim, so 99% or more startups overbuild, given their resulting customer base. But it's like 99% or more pilots who put on a parachute don't end up using it; the whole point is the small minority who do, and you never know. For a stable, predictable, medium-scale business it may make total sense to have a few dedicated physical boxes and run their whole operation from them comfortably, for a fraction of cloud costs. But starting with it is more expensive than starting with a cloud, because you immediately need an SRE, or two. reply kristopolous 8 hours agorootparentYou aren't going to get there. The risks and complexity of a startup are high to begin with. Adding artificial roadblocks because of aspirational fantasies is going to hold you back. Look at the big successes such as youtube, twitter, facebook, airbnb, lyft, google, yahoo - exactly zero of them did this preventatively. Even altavista and babelfish, done by DEC and running on Alphas, which they had plenty of, had to be redone multiple times due to growth. Heck, look at the first 5 years of Amazon. AWS was initially ideated in a contract job for Target. Address the immediate and real needs and business cases, not pie in the sky aspirations of global dominance - wait until it becomes a need and then do it. The chances of getting there are only reasonable if you move instead of plan, otherwise you'll miss the window and product opportunity. I know it ruffles your engineering feathers - that's one of the reasons most attempts at building these things fails. The best ways feel wrong, are counterintuitive and are incidentally often executed by young college kids who don't know any better. It's why successful tech founders tend to be inexperienced; it can actually be advantageous if they make the right \"mistakes\". Forget about any supposedly inevitable disaster until it's actually affecting your numbers. I know it's hard but the most controllable difference between success and failure in the startup space is in the behavioral patterns of the stakeholders. reply esafak 6 hours agorootparentDo you remember the companies that did not scale? friendster did well until it failed to scale, and Facebook took over. So the converse argument might be: don't bungle it up because you failed to plan. Provision for at least 10x growth with every (re-)implementation. https://highscalability.com/friendster-lost-lead-because-of-... reply kristopolous 4 hours agorootparentHold on... You think Facebook took over from Friendster because of scaling problems?! MySpace was the one that took the lead over Friendster and it withered after it got acquired for $500 million by news corp because that was the liquidity event. That's when Facebook gained ground. Your timeline is wrong. The MySpace switch was because of themes and other features the users found more appealing. Twitter had similar crashes with its fail whale for a long time and they survived it fine. The teen exodus of Friendster wasn't because of TTLB waterfall graphs. Also MySpace did everything on cheap Microsoft IIS 6 servers in ASP 2.0 after switching from Coldfusion in Macromedia HomeSite, they weren't genuises. It was a knockoff created by amateurs with a couple new twists. (A modern clone has 2.5 mil users: see https://spacehey.com/browse still mostly teenagers) Besides, when the final Friendster holdout of the Asian market had exponential decline in 2008, the scaling problems of 5 years ago had long been fixed. Faster load times did not make up for a product consumers no longer found compelling. Also Facebook initially was running literally out of Mark's dorm room. In 2007, after they had won the war, their code got leaked because they were deploying the .svn directory in their deploy strategy. Their code was widely mocked. So there we are again. I don't care if you can find someone who agrees with you on the Friendster scaling thing, almost every collapsed startup has someone that says \"we were just too successful and couldn't keep up\" because thinking you were just too awesome is the gentler on the ego than realizing a bunch of scrappy hackers just gave people more of what they wanted and either you didn't realize it or you thought your lack of adaption was a virtue. reply esafak 2 hours agorootparentHow sure are you that they switched because of themes? Did you see user research? I left because of its poor performance, and MySpace was no substitute for friendster; it targeted an artsy demographic. But Facebook was. reply kristopolous 1 hour agorootparentYes. I worked in social networks 15 years ago. It was a heavy research topic for me. You're a highly technical user. Non-technical people are weird - part of the MySpace exodus was the belief that it spread \"computer viruses\", really There was more to the switches but I'd have to dredge it up probably through archive sites these days. The reasons the surveys supported I considered ridiculous but it doesn't matter it's better to understand consumer behavior - we can't easily change it. Especially these days. It was not possible for me to be a teenager with high speed wi-fi when I was one 30 years ago. I've got near zero understanding of the modern consumer youth market or what they think. Against all my expectations I've become an old person. Anyways, the freeform HTML was a major driver - it was geocities with less effort, which had also exited through a liquidity event and currently has a clone these days https://neocities.org/browse reply BackBlast 12 hours agorootparentprevYou're missing the purpose of the cache. At least for this argument it's mostly for network responses. HDD was 10ms, which was noticeable for cached network request that needs to go back out on the wire. This was also bottle necked by IOPS, after 100-150 IOPS you were done. You could do a bit better with raid, but not the 2-3 orders of magnitude you really needed to be an effective cache. So it just couldn't work as a serious cache, the next step up was RAM. This is the operational environment which redis and such memory caches evolved. 40 us latency is fine for caching. Even the high load 500-600us latency is fine for the network request cache purpose. You can buy individual drives with > 1 million read IOPS. Plenty for a good cache. HDD couldn't fit the bill for the above reasons. RAM is faster, no question, but the lower latency of",
    "originSummary": [
      "The blog post highlights SSD technology advancements in speed and capacity, contrasting improvements in commodity SSDs with stagnant cloud-based SSD performance from major vendors like AWS and Azure.",
      "Possible reasons for the cloud SSD performance gap are speculated, such as concerns about device failure, limited demand for faster storage, and potential disruption to existing storage services.",
      "The post advocates for the future introduction of speedier SSDs in cloud instances, pointing towards a hopeful outlook for enhanced cloud storage capabilities."
    ],
    "commentSummary": [
      "The discussion delves into performance challenges of SSDs in the cloud, addressing network protocol limitations, storage optimization in platforms like AWS, CPU configurations, and economic impacts of storage settings.",
      "Topics include the impact of storage configurations on VM performance, network latency, strategies to enhance cloud performance, and debates on communication protocol deployment and hardware abstraction in cloud services.",
      "It also emphasizes the significance of locally-attached SSDs, network latency issues, reliable enterprise drives, and the balance between performance and resilience in database cluster storage options."
    ],
    "points": 488,
    "commentCount": 345,
    "retryCount": 0,
    "time": 1708448383
  },
  {
    "id": 39446537,
    "title": "DIY LED Matrix Earrings Shine Bright",
    "originLink": "https://mitxela.com/projects/ledstud",
    "originBody": "LED Matrix Earrings 20 Feb 2024 Progress: Complete I originally imagined the LED Industrial Piercing as a project specifically to make use of 0201 LEDs. In the end, they weren't even necessary. 0201 LEDs are just too small! Evidently, we needed to go deeper, so the purpose of this next project was to stick as many as possible of them onto the face of a stud earring. Watch the following youtube video for an extensively narrated journey. Continue reading this page for an in-depth explanation. Design These off-the-shelf LED stud earrings are very cheap. They have a tiny machined battery holder for two LR521 cells. The idea was to replace the plastic gemstone and single LED with our own circuit board, re-using the metalwork. Electrically, the circuit and firmware would be identical to the badge I recently created using a CH32V003. Read that page for the story behind it and an explanation of how it works. The challenge here is shrinking the design of the badge by a factor of 3 (or one-ninth the area) by shifting from an LED pitch of 3mm down to 1mm. We also insisted on not having a thick border around the board. This poses quite a challenge for routing the tracks. To give a sense of scale, here's three 0201 LEDs next to a regular surface mount LED: Or, to really put it in perspective, an 0201 LED on the back of an ATtiny85: Homebrew blind and buried vias It's possible to get multilayer boards made with so-called blind and/or buried vias. The limit is far, far beyond what we're working with here. The \"redistribution layer\" of BGA packages is normally built as a very small, high-density circuit board. These often have very fine pin pitches and use blind vias, via-in-pad, and so on. The point is we could easily order a circuit board to these tolerances – if we had the cash for it. A small batch of HDI boards starts at several hundred dollars, and the more complex the stackup the more the price goes up. I would very much like to keep this entire project under $50 total. I decided to craft my own circuit board sandwich, two two-layer boards held together by an array of solder pads. I wasn't sure if this was going to work. I put all the pads around around the perimeter, partly with the idea that I could inspect all the connections, of course that would mean removing the boards from the panel to do so, which I didn't want to do until the rest of the board is assembled. I suppose we could have started by fitting the components, and sandwiching the boards last, but that sounds like a recipe for disaster. The drill diameter for the vias is 0.25mm, which puts us just into the second tier of board pricing (almost all of the cheapest PCB offers have minimum 0.3mm). It may have been possible to do a layout of 0201 LEDs within the cheapest tolerance, but I didn't want to risk it. Remember the distance from one LED to the next is 1mm. At this sort of scale, the copper thickness around a plated hole is noticeably uneven. Solder sandwich As usual, I combined postage with several other orders, which leads to a mini-christmas when they all arrive. The main display is an 8x8 matrix at 1mm pitch. With the corners cut off to fit into the round profile, that adds up to 52 LEDs within a 9mm diameter circle. The panel top and bottom is 5mm wide. The whole panel is 23mm by 28.5mm. I stencilled paste onto one of the boards, and sat an unpasted board on top of it. The symmetrical panel helps ensure perfect alignment. The total board thickness is 0.6mm, so our sandwich ends up a little over 1.2mm. This was reflowed in a fairly uneventful way. I probably should have used higher temperature solder for this step, as it would reduce the risk of these joints separating later on. On the other hand, I didn't want to risk messing this up, those pads are still quite small at 0.5mm diameter and it's not like I keep a whole range of solder pastes in stock. (They have a shelf life, and go off after a few months anyway.) Paste paste paste I ordered a steel stencil along with the boards. I fit about six other designs into it, but even ordering a stencil for one board is normally viable, they're unbelievably cheap for what they are. My first mistake was not putting the fiducials on the stencil. Aligning the first board was easy enough, we could use the QFN pads to confirm it, but after cutting one board out of the panel, aligning the second board was very difficult. My second mistake is possibly making the holes for the 0201 pads too small. I probably could have gotten away with a smidgen larger, which would have made the stencilling process less stressful. The first attempt wasn't too shabby, most of those pads are perfect with just a few under-pasted. I wiped this off and tried again a few times, I feel like with exactly the right amount of pressure and speed with the squeegee it should be possible to get it perfect. I didn't quite manage perfection, but got close enough to feel like moving onwards. I did clean the stencil at this point, but not sufficiently. The remaining residue would dry up and cause me to have a much less fun time when it came to the second earring. Pick and place Presuming the reader has already watched the video, or at least perused the previous projects of the badge and the volumetric display, it goes without saying that I planned to position the parts with the pick and place machine. 0201 is well beyond the specified tolerance of it, so this was something of a gamble. It's not impossible to place 52 0201 LEDs by hand if it came to it, but I was really curious to see if I could do this, and if so, what further projects that opens up in future. The machine came with a selection of vacuum nozzles, most of which are cylindrical, but the very smallest is this shape. That oblong tip is a comparable size to the parts we're trying to pick up, so it wasn't clear if this would work, but in the end it didn't seem to have any problems in this aspect. The construction of the machine appears to be little more than a glorified 3D printer, but evidently it's built to a much sturdier tolerance. Once I'd calibrated the position of the components, it placed the first one absolutely spot-on. The problems I initially encountered were less about component placement and more about the tape misbehaving. I have the machine on a very sturdy desk, but it still shakes about as it operates. This means parts as small as an 0201 LED are likely to go flying if the protective film on the tape is pulled back. The holes in the tape are at a 4mm pitch, just like all the other tapes, and most parts, even quite small ones, are placed at a 4mm spacing. It's only when you get to 0402 and 0201 that parts are placed at a closer spacing than the holes in the tape. This poses something of a problem for the mechanism used to advance the tape: the head has a solenoid pin which it inserts into a hole and pulls it forwards. For tapes with a 2mm spacing, it needs to advance by half a step instead of a full one. The machine is able to do this, but the key factor in getting it to pick parts reliably was to offset the position of where it picks up the part out of the tape. The zero position is almost 4mm away from the area where the film is peeled back, which offered far too much leeway for parts to bounce away. I'll fast forward to the end of the first attempt. All in all I was pretty impressed with that. There's an obvious systematic error, all of the parts are a little too low, but that's just my inaccurate calibration, the placement is very consistent with itself. A few of the parts are skewwhiff, it's not clear if they jumped as they were put down, or if there was an error in the vision system. Even a small rounding error in the OpenCV outline logic could lead to it placing the parts at a weird angle. Luckily it's trivial to push the odd part back into place. After reflow, virtually all of them snapped into a perfect grid alignment. The result is quite a bit more consistent than my second attempt on video, just because I did a much worse job of stencilling the paste that time. Populating the back of the board was somewhat uneventful. Uneventful, but not without suspense! It wasn't until this point we get to check if any of it works of course. Sweet. After chopping this out of the surrounding frame, we took a moment to do some vanity shots with a coin for scale. The 50p coin is of course a little large, so here's another shot on a 5p coin, much more appropriate. Stud preparation Under the plastic gemstone, the stud earring has a tiny circuit board with two components on it. The case is the positive terminal of the battery. The negative terminal is a fine wire through the middle of the post. We just need to connect our circuit to these two points. Rather short-sightedly, I soldered the connections in place and set the circuit without fully testing it. I was exceptionally careful with setting the prongs, which was silly as I soon had to undo this and modify things. The power draw of my original firmware was far too high, it's dumb I didn't see it coming. The teeny battery can't cope with the current draw, the voltage drops and the CH32V003's brownout detector resets the chip. With some further digging into the datasheet it might be possible to disable the brownout detector. I'm more used to the ATtiny chips, many of which are happy to run down to 1.8V. It is just about possible to reprogram the circuit with it set in place, I use a crocodile clip to connect the positive terminal, touch one wire to the end to make the ground connection, and then touch one more wire inside to make contact with the debug pin. However, doing this more than once is a right pain, so I opened up the earring again to do things properly. While we're at it, adding some capacitors is bound to help. That's a 22uF and a 0.1uF in parallel. Sometimes large-value, physically small ceramic capacitors have a high internal resistance so it can be helpful to combine multiple values in parallel like this. I dropped the clock speed of the chip from 48MHz to 1.5MHz, and one of the things I found is that this affects the debugger connection. Ordinarily this is fine, there's a utility that lets you \"unbrick\" the chip by power cycling it and immediately connecting. This unfortunately necessitates debug control of the power supply, which is why it becomes very fiddly in our situation, and is what led to me trying to connect to the earring post with crocodile clips. In the end I got the display into a state I was happy with, a duty cycle of about 0.25% and the reduced clock speed means the overall power draw is something like 8mA. With that, we were able to call it done, and begin work on the second one, which had the added fun of a video camera in the way of everything. Vanity shots You may have clocked a certain amount of self-doubt I expressed as I came to set the circuit. It's an impressive electronics project, but is it an earring anyone would want to wear? To be fair, it's not like that's stopped me in the past from building utterly tasteless earrings that no one would be seen dead with. But my friend seemed quite keen to try on more glowy glowy accessories. I suspect she secretly enjoyed being an ear model. One idea I'd meant to try with the industrial piercing is combining a long exposure with flash, to try and get a still image with LED trails. This led to a somewhat inevitable conclusion. Believe it or not, it was her idea. (click for full size) The source code for this project is identical to the badge, and is available on git.mitxela.com and github. ~ mitxela.com » Projects » Hardware » LED Matrix Earrings Questions? Comments? Check out the Forum Support mitxela.com",
    "commentLink": "https://news.ycombinator.com/item?id=39446537",
    "commentBody": "LED Matrix Earrings (mitxela.com)447 points by nrabulinski 13 hours agohidepastfavorite65 comments askvictor 24 minutes agoThe led trails photo got me thinking you could make some cool persistence of vision stuff with these. As long as the wearer is going fast enough. reply kombookcha 11 minutes agoparentAlso ripe for use in any number of accessories. You could plausibly decorate just about any kind of clothing or jewelry with these if you make them detachable. You could probably have a whole glowing silhouette of your body when you dance, if you space them out like the markers on those motion capture suits. 1999 hangar raves would have loved these. reply zokier 9 hours agoprevDesign-wise I liked the industrial more, it was more cyberpunk. But this is very neat too. What I find pretty crazy is that there are .65x.65mm RGB LEDs, equal to the long side of articles LEDs squared. They are also only .25mm thick, that's same as few sheets of paper. One cyberpunk look I'm thinking you could do with those is just to glue them on your face and blend in with makeup; I'm sure someone could do fancy things here. Wire you can get pretty much as thin as you possibly want. reply wiml 7 hours agoparentFive-ten years ago there was a thing for LED eyelashes - glued to your upper eyelid, super-fine wire running to a battery by your ear. I'm not sure how they worked electrically, but the fancier ones appeared to have individually addressable LEDs along the upper eyelid. I thought they looked cool. reply poulpy123 1 hour agoprevI saw that yesterday and immediately googled if I could buy a similar led matrix for making a set. And I'm a man that has never wear any earrings and do not plan to wear some. So you can say that the marker exists reply The5thElephant 12 hours agoprevThis is very cool and I know a LOT of people who would buy and wear these. If you productized these (possibly make them rechargeable and simply way to program the LEDs) they would definitely sell. reply BHSPitMonkey 7 hours agoparentFortunately for the OP, that part's really easy; Simply wait for the tech news outlets to pick up this story, then sit back and wait for them to start showing up on Temu. reply portpecos 5 hours agorootparentLike this? https://www.temu.com/2pcs-led-earrings-illuminated-with-whit... reply alonsonic 3 hours agorootparentYou probably didn't watch the video. These are exactly the earrings used in the video to create the matrix LED ones. These earrings are just a fake jewel with an LED back light. The article showcases a programable LED matrix, which opens up a lot more possibilities. reply xeckr 4 hours agorootparentprevThose appear to be static. The ear rings in the OP are way cooler. But damn, for 4 dollars? I didn't expect even those to be that cheap. reply speleding 1 hour agorootparentI just got a few for my daughter, on Ali Express they are less than $2! reply mysteria 10 hours agoparentprevI'm not sure about the rechargeable aspect, but for programmability you could use a small IR receiver placed in the center of the LED array. You really only need a one way data transmission and something like Bluetooth is overkill. reply barake 9 hours agorootparentCould use an optical sensor like the Timex Datalink watches from the 90s[1] Could set the earrings on a phone screen and transfer the patterns over. Sounds like a fun project. 1: https://en.m.wikipedia.org/wiki/Timex_Datalink reply zokier 8 hours agorootparentLEDs are actually sensitive to light themselves so with some wizardy you might not even need separate sensor reply mattegan 6 hours agorootparentprevI love simple light-based data transmission stuff. I've seen it included in things like guitar pedals that have just a few config bits that someone might want to change infrequently. An app to change some settings can be as simple as just a little webpage! So simple! Here's a (tiny) demo of this for my PCB business card project from years ago [1]. If IIRC this proof of concept was as simple as using a phototransitor on a GPIO connected to the UART peripheral with a very low baud rate. [1]: https://www.matt.egan.me/entry/electronic-business-card-pt3#... reply joshspankit 8 hours agorootparentprevNow that you made that connection: Concerts with audience-worn LED bracelets regularly shoot out LED floodlights to the IR sensors on each wrist. Being able to do something similar on people’s ears could be cool. reply zokier 9 hours agoparentprevthe weight might start getting bit of a problem here too? Hiding the battery somewhere behind ear could help, but ruins the conceptual elegance reply crote 8 hours agorootparentProbably not worth the effort. Some women wear pretty large earrings[0] without too much trouble, after all. [0]: https://www.regalrose.co.uk/products/glory-huge-star-hoop-ea... reply zokier 8 hours agorootparentNot without any trouble though https://www.self.com/story/heavy-earrings-stretch-out-your-e... reply crote 8 hours agorootparentThe patient in question wore heavy earrings for years. Wearing moderately-heavy LED earrings every once in a blue moon isn't comparable to that. reply petesergeant 8 hours agorootparentprevHairclip the battery with tiny wires behind the ear. You could put the controller there too if you wanted something fancier reply littlestymaar 2 hours agoparentprevI'm afraid they will eventually be mainstream, because we definitely need more e-wastes for vanity purpose… reply mysteria 7 hours agoprevI ended up doodling up some ideas [1] based off this, with a pendant, OLED display, and a separate battery holder hooked to the ears coming to mind. A similar design would also be cool for earbuds and IEMs especially if the lights could be synchronized to the music. (Seriously though, everyone should learn how to sketch the designs in their head. You don't have to be good, you just have to be adequate enough to get your point across). 1. https://i.ibb.co/wwTKNJ2/image.jpg reply crote 8 hours agoprev> It's an impressive electronics project, but is it an earring anyone would want to wear? Are you kidding me? Just about every STEM gal would pay a pretty penny for a set of those! The industrial[0] is a bit too obscure, the led-ring variant[1] is a little underwhelming, but a full LED matrix? Absolutely breathtaking. [0]: https://mitxela.com/projects/scaffold [1]: https://mitxela.com/projects/charliestar reply crtified 10 hours agoprevI don't know if it's feasible at this scale, but a version that could optionally animate or pulse in sync with sound (music) would be a big hit in nightclubs. reply max_ 50 minutes agoprevI will declare my self successful in life if my website looks like the author's https://mitxela.com/projects page reply matthewfelgate 45 minutes agoparentI know right, so many interesting projects. And many of them complete! reply k__ 29 minutes agoprevAwesome! I have circuit plugs, LED matrix plugs would be the next level :D reply jbl0ndie 42 minutes agoprevI'd be interested to see an algorithm for the blinkenlights effect he demos. reply saiya-jin 10 minutes agoprevThe last thing anybody concerned about their looks wants is to have strong side light shining up close on all the imperfections, scars, bumps and overall messy skin that looked so nice and smooth before. Guys really don't want to see woman's pores and pimples 3D facial structure. Or anybody's else for that matter. reply shadowpho 11 hours agoprevPretty cool to see 2x2 layer sandwich of pcbs. It’s going to be cheaper than 1-2-1 pcb at small scale. That’s the same thing Apple does. reply Aissen 3 hours agoprevLook at Ayke's approach for another opens source earring style, using RGB LEDs and running TinyGo: https://github.com/aykevl/things/tree/master/earring-ring https://hachyderm.io/@ayke/110318083332958798 reply Retr0id 9 hours agoprevAt this point it seems like the biggest limiting factor of scale is the vias, not the LEDs themselves. If they made LEDs with 3 pads, where the outer 2 pads are connected together, you could make a matrix on a single-sided board with no vias. reply modeless 5 hours agoparentHow about OLED microdisplays? You could have full HD and thousands of nits of brightness. reply tomn 9 hours agoparentprevI wonder how it would look to connect the columns with 0201 zero-ohm resistors between the LEDs (jumping over the rows lines). If you want a square pixel pitch it might work out nicely. reply crote 7 hours agorootparentYou don't really have the clearance for that. The gap between the two pads of the resistor should be at most 0.3mm, and more realistically 0.2mm - any larger and they're going to tombstone. 0.1mm traces with 0.1mm clearance are manufacturable so it could fit in a 0.3mm gap, but realistically you probably want to do 0.15mm traces & clearance to keep the cost acceptable. reply zokier 9 hours agoparentprevFor one-off, I wonder if you could just glue the leds together and hand-wire (very carefully) with some magnet-wire. Throw in some kapton tape to insulate everything. reply liamkinne 11 hours agoprevI wonder what the smallest circular OLED display is. There are 01005 LEDs (the next size down from the 0201's used here) but at that point an OLED display is essentially the same thing. reply dekhn 11 hours agoparentI see models down to 0.5\" (12.7mm) but they all seem to have large connectors you'd have to work to hide/connect an microcontroller to. reply kragen 9 hours agoparentprevindium gallium arsenide and gallium nitride probably have a much longer lifetime than oled and will probably withstand higher temperatures (though the encapsulating resin may not) reply jbl0ndie 11 hours agoprevAny video that references Mike Harrison as inspiration is all right with me. https://electricstuff.co.uk/ reply zakki 10 hours agoparentI don't understand your comment relation with the posted article reply prophesi 10 hours agorootparentThe video in the article detailing the build shows Mike Harrison's badge in the intro that they used as a reference for their prototypes. reply jbl0ndie 43 minutes agorootparentExactly that! Blink and you'll miss it though, hence my post with a link for anyone who is interested. reply UberFly 7 hours agoprevVery cool. I could see these being popular if sold. Expect to see these on Ali or Temu by the end of the week. reply tdudhhu 3 hours agoprevAlso check his YouTube channel to see them in action and for more background information: https://youtu.be/CHoGIvOi-jw reply tomrod 8 hours agoprevI'd like these as cuff links. Imagine it as a status symbol! reply jorticka 2 hours agoparentOnly if made by Apple. reply sangnoir 1 hour agorootparentThe cuff-links would turn into green bubbles when you're near \"poor\" people (by detecting Bluetooth MAC addresses of non-Apple smartphones) reply graphe 7 hours agoparentprevFor now. Seiko invented the amazing quartz watch which was a marvelous invention of timekeeping with a crystal rather than mechanical links. Now a quartz watch is a sure sign of a cheap commodity. LED cuff links will be outdated quickly, probably less than a few weeks, maybe even after the first few wears it would be gaudy. reply kragen 5 hours agorootparentwp sez > The National Bureau of Standards (now NIST) based the time standard of the US on quartz clocks between the 1930s and the 1960s, after which it transitioned to atomic clocks.[43] In 1953, Longines deployed the first quartz movement.[44] The wider use of quartz clock technology had to await the development of cheap semiconductor digital logic in the 1960s. ... In 1966, prototypes of the world's first quartz pocket watch were unveiled by Seiko and Longines in the Neuchâtel Observatory's 1966 competition. but certainly it is common for today's technological miracle to become taken for granted tomorrow reply pests 4 hours agoprevI wonder how much the \"staggered blind vias\" PCB would have been. reply TazeTSchnitzel 11 hours agoprevVery happy to see someone did this, because I had the same idea many years ago, but it seemed infeasible. reply iancmceachern 2 hours agoprevI was just watching this! reply imzadi 11 hours agoprevCould you include ir LEDs to blind cameras? Is that a thing? reply Aurornis 11 hours agoparentWith the size of that battery, it might work for a second or two. reply jes5199 6 hours agoprevI am impressed! It’s startling how small things have gotten reply ergonaught 10 hours agoprevSuper cool. reply shaunxcode 12 hours agoprevwould definitely buy these! reply BugsJustFindMe 12 hours agoparentSame. These could easily sell for a few hundred USD. reply MivLives 11 hours agoparentprevI would if I could program them relatively easily. And if there were other colors then just red LEDs. reply crote 8 hours agorootparentMulti-color is probably not really possible at a reasonable price point because you'd need twice as many vias and traces in the same space for RGB compared to just R. You _can_ get it manufactured, just not in low quantities at a price anyone would be willing to pay. There are 1.1x1.1mm addressable LEDs available[0] which should be quite doable, but those require quite a high voltage and have an unacceptably high idle power consumption: a 52-LED matrix would draw 15mA with all the LEDs off! As to single-color non-red: the main advantage of red LEDs is that they can operate on a very low voltage. The exact same board with blue LEDs would have a significantly shorter battery life - if it's even possible at all. These earrings are probably using two SR521 batteries, which start at about 1.55V and discharge to about 1V[1] - so the earring is operating on 3.1V to 2V. You can get red LEDs which work with as little as 1.7V, but blue LEDs need about 3V to operate _at all_. You'd either have about 1/3rd of the battery life, or you'd have to add a third battery. [0]: https://www.adafruit.com/product/5849 [1]: https://img5083.weyesimg.com/uploads/xtk5s4z6.allweyes.com/i... reply dekhn 7 hours agorootparentBoost circuits are common for driving LEDs, for example a joule thief would be perfect for running this on fewer batteries. reply kragen 5 hours agorootparenttrue, but it's challenging to get a boost converter, even a very simple one like a joule thief, into the space of this earring; i think it's too small for a millihenry. using a larger number of smaller batteries might be a better option reply dr_dshiv 11 hours agoprev [–] wowee, Mitxela is a really fun rabbithole!! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LED matrix earrings are crafted using a custom circuit board with 0201 LEDs, replacing traditional components for improved functionality and aesthetics.",
      "Power consumption and clock speed challenges are overcome, culminating in a functional and visually pleasing final earring design.",
      "The project offers vanity shots of the finished product and provides access to the complete source code on git.mitxela.com and GitHub for further exploration."
    ],
    "commentSummary": [
      "Users engage in discussions on LED accessories like earrings and cuff links, including data transmission and music synchronization features.",
      "Creative LED earrings applications, cuff links' technical details, and battery efficiency are debated.",
      "The conversation extends to the history and advancements in quartz technology for timekeeping in clocks and watches."
    ],
    "points": 447,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1708461252
  },
  {
    "id": 39442273,
    "title": "Python Web Scraping: Techniques and Best Practices",
    "originLink": "https://proxiesapi.com/articles/web-scraping-in-python-the-complete-guide",
    "originBody": "In this tutorial you'll build robust web crawlers using libraries like BeautifulSoup, learn techniques to overcome real-world scraping challenges and best practices for large scale scraping. You'll gain the skills to scrape complex sites, handle issues like rate limits, blocks, and javascript pages. Why Python for Web Scraping? Python is a popular language for web scraping due to its advantages: Simple Syntax: Python's intuitive syntax allows quick coding for scraping. Built-in Libraries: Python comes with built-in libraries and modules, like urllib and lxml, that aid in scraping. Mature Scraping Libraries: Libraries like Beautiful Soup and Scrapy simplify scraping at any scale. General Purpose: Python can be used to build complete data pipelines around scraping. Interoperability: Python integrates well with other languages and performs well when performance is crucial. In contrast, languages like C++ require more effort for basic scraping tasks. JavaScript platforms like Node.js can be complex for beginners when building scraping scripts. Python's simplicity, power, and interoperability makes it ideal for scraping needs. Its high-quality libraries allow quick start to scraping at scale. Best Python Web Scraping Libraries Some of the most popular and robust Python libraries for web scraping are: BeautifulSoup Features: Excellent HTML/XML parser, easy web scraping interface, flexible navigation and search. We will be using this library in our example scraper below. Use Case: Small to medium scale web scraping. Link to BeautifulSoup docs Scrapy Features: Fast and scalable, middlewares, distributed crawling capability. Use Case: Large scale advanced web scraping projects. Link to Scrapy docs Selenium Features: Full browser automation, handles javascript heavy sites. Use Case: Sites with highly dynamic content loaded by JS. Link to Selenium docs lxml Features: Very fast XML and HTML parser. Use Case: Lightning fast parsing of XML/HTML data. Link to lxml docs pyquery Features: jQuery-style syntax for accessing HTML elements. Use Case: Makes scrape code look cleaner and more readable. Link to pyquery docs Prerequisites To follow along with the code examples in this article, you will need: Virtual Environment (Recommended) While optional, we highly recommended creating a virtual env for the project: Copy python -m venv my_web_scraping_env The Libraries We will be using the Requests, BeautifulSoup and OS libraries primarily: Copy pip install requests beautifulsoup4 This will fetch the libraries from PyPI and install them locally. With the prerequisites installed, you are all setup! Let's start scraping. Lets pick a target website For demonstration purposes, we will be scraping the Wikipedia page List of dog breeds to extract information about various dog breeds. The rationale behind choosing this page is: Well structured HTML layout that makes scraping easy Nice table layout with one breed per row Contains mulitple data fields per breed including names, breed group, alternate names and images Images can allow us to showcase scraping binary files as well This is the page we are talking about… Other great pages to practice web scraping include: Wikipedia category pages like Lists of films Ecommerce product listings like Amazon books Real estate listings like Zillow rentals The concepts covered will be applicable across any site. Write the scraping code Let's now closely examine the full code to understand how to systematically scrape data from the dogs breed webpage. Copy # Full code import os import requests from bs4 import BeautifulSoup url = '' # Headers to masquerade as a browser headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\" } # Download page HTML using requests response = requests.get(url, headers=headers) # Check valid response received if response.status_code == 200: # Parse HTML using Beautiful Soup soup = BeautifulSoup(response.text, 'html.parser') # CSS selector for the main tables table = soup.find('table', {'class': 'wikitable sortable'}) # Initialize data lists to store scraped info names = [] groups = [] local_names = [] photographs = [] # Create directory to store images os.makedirs('dog_images', exist_ok=True) # Loop through rows omitting header for row in table.find_all('tr')[1:]: # Extract each column data using CSS selectors columns = row.find_all(['td', 'th']) name = columns[0].find('a').text.strip() group = columns[1].text.strip() # Extract local name if exists span_tag = columns[2].find('span') local_name = span_tag.text.strip() if span_tag else '' # Extract photo url if exists img_tag = columns[3].find('img') photograph = img_tag['src'] if img_tag else '' # Download + Save image if url exists if photograph: response = requests.get(photograph) if response.status_code == 200: image_filename = os.path.join('dog_images', f'{name}.jpg') with open(image_filename, 'wb') as img_file: img_file.write(response.content) names.append(name) groups.append(group) local_names.append(local_name) photographs.append(photograph) print(names) print(groups) print(local_names) print(photographs) The imports include standard Python libraries that provide HTTP requests functionality (requests), parsing capability (BeautifulSoup), and file system access (os) which we will leverage. The requests library allows us to make HTTP requests to the web page and check if the response is valid before parsing. BeautifulSoup then enables us to parse the full HTML content and isolate the main data table using CSS selectors. Finally, os provides file system access to save images locally. Together they form a very handy toolkit for scraping! Downloading the page We first construct the target URL and initialize a requests Session which allows connection reuse and efficiencies when making multiple HTTP requests to the same domain: Copy url = '' headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\" } response = requests.get(url, headers=headers) Tired of getting blocked while scraping the web? Get access to 1,000 free API credits, no credit card required! Try for free We also setup a custom User-Agent HTTP header to masquerade as a Chrome browser. This helps avoid blocks from servers trying to prevent scraping. After getting the response, we can check the status code to ensure we received a proper HTML document: Copy if response.status_code == 200: # Success! print(response.text) In case of errors (e.g. 404 or 500), we do not proceed with scraping and handle the failure. Parsing the html Since we received a valid HTML response, we can parse the text content using Beautiful Soup: Copy soup = BeautifulSoup(response.text, 'html.parser') BeautifulSoup accepts the raw HTML text and an optional parser like lxml or the built-in html.parser, and provides simple methods and Pythonic idioms for navigating, searching, and modifying the parse tree. Beautiful Soup transforms the messy HTML into a parse tree that mirrors the DOM structure of tags, attributes and text. We can use CSS selectors and traversal methods to quickly isolate the data we need from this tree. The Magic of Selectors for Data Extraction One of the most magical parts of web scraping with Python's BeautifulSoup library is using CSS selectors to extract specific content from HTML pages. Selectors allow us to visually target the tags enclosing the data we want scraped. BeautifulSoup makes selecting elements a breeze. For example, consider extracting book titles from this snippet: Copy Harry Potter and the Goblet of Fire 9.1The Fellowship of the Ring 9.3We can directly target the span with class title through the CSS selector: Copy soup.select(\"div.book-listing > span.title\") This says - find all span tags having class title which are direct children of any div tag having book-listing as the CSS class. And voila, we have exactly the titles isolated: Copy [Harry Potter and the Goblet of Fire, The Fellowship of the Ring] We can chain .text to extract just the readable text within the tags: Copy [Harry Potter and the Goblet of Fire, The Fellowship of the Ring] Selectors provide incredible precision during data extraction by leveraging the innate hierarchy of structured HTML tags surrounding it. Some other examples of selectors: Copy # Select id attribute soup.select(\"#book-title\") # Attribute equality match soup.select('a[href=\"/login\"]') # Partial attribute match soup.select('span[class^=\"title\"]') # Select direct descendant soup.select(\"ul > li\") As you can see, by mastering different selector types and combining multiple selectors where needed - you gain immense power to zone in on and extract the exact data you need from any HTML document, eliminating nearly all guesswork. Lets get back to the task at hand now… Finding the table Looking at the Raw HTML, we notice a table tag with CSS class wikitable sortable contains the main breed data. We can simply select this using: Copy table = soup.find('table', {'class': 'wikitable sortable'}) This searches the parse tree for any table tag having a class attribute matching wikitable sortable. Beautiful soup makes Selection using CSS selectors super easy! Extracting all the fields With the table isolated, we loop through every tr row after the header row to extract the data from each breed: Copy for row in table.find_all('tr')[1:]: columns = row.find_all(['td', 'th']) name = columns[0].find('a').text.strip() group = columns[1].text.strip() Here, .find_all() helps search all the row children tags for any td or th elements, which represent table cells. We select these into a list columns. Using positional indexes in this columns list, we can extract the data within each cell cleanly: Copy name = columns[0].find('a').text.strip() This grabs the anchor a tag inside the first table cell, gets .text property to extract raw string content and chains .strip() to remove whitespace. Beautiful Soup chains such operations elegantly. Similarly for cells containing just text: Copy group = columns[1].text.strip() We fetch .text property directly on table cell element. The power of CSS selectors in quickly isolating specific tags, ids, classes or attributes makes data extraction very precise and straightforward in Beautiful Soup. Downloading and saving the images After scraping textual data like names, groups etc in each row, we check the last cell for an image link: Copy img_tag = columns[3].find('img') photograph = img_tag['src'] if img_tag else '' This tries detecting and fetching src attribute on any image tag if exists. We can then download and save images using this url if present: Copy if photograph: response = requests.get(photograph) image_filename = os.path.join('dog_images', f'{name}.jpg') with open(file_path, 'wb') as img_file: img_file.write(response.content) We reuse the requests library to make another GET request - this time to download the image binary content and save it locally using built-in file handling capability. Pretty nifty! And that's it! By using requests and BeautifulSoup together with Python's intuitive standard library, we were able to build a complete web scraper to extract complex data! Alternative libraries and tools for web scraping While requests and BeautifulSoup form the most popular combination, here are some alternatives worth considering: Scrapy An open source modular scraping framework meant for large scale crawling that handles throttling, cookies, proxy rotation automatically. Recommended for complex needs. Selenium Performs actual browser automation by controlling Chrome, Firefox etc. Enables scraping dynamic content that renders via JavaScript. More complex setup. pyppeteer Headless browser automation like Selenium driven through Python code. Good for javascript rendered websites. pyquery Offers jQuery style element selection. Scrape code looks very clean due to chaining syntax similar to jQuery. lxml A very fast XML/HTML parser. Great when raw parsing performance is critical. Challenges of Web Scraping in the real world: Some tips & best practices While basic web scraping is easy, building robust production-grade scalable crawlers brings its own challenges: Handling Dynamic Content Many websites rely heavily on JavaScript to render content dynamically. Static scraping then fails. Solutions: Use browser automation tools like Selenium or scraper specific solutions like Scrapy's splash integration. Here is a simple Hello World example to handle dynamic content using Selenium browser automation: Copy from selenium import webdriver from selenium.webdriver.common.by import By # Initialize chrome webdriver driver = webdriver.Chrome() # Load page driver.get(\"\") # Wait for title to load from dynamic JS execution driver.implicitly_wait(10) # Selenium can extract dynamically loaded elements print(driver.title) # Selenium allows clicking buttons triggering JS events driver.find_element(By.ID, \"dynamicBtn\").click() # Inputs can be handled as well search = driver.find_element(By.NAME, 'search') search.send_keys('Automate using Selenium') search.submit() # Teardown browser after done driver.quit() The key capabilities offered by Selenium here are: Launches a real Chrome browser to load JavaScript Finds elements only available after execution of JS Can interact with page by clicking, entering text etc thereby triggering JavaScript events Experience mimics an actual user browsing dynamically generated content Together this allows handling complex sites primarily driven by JavaScript for dynamic content. Selenium provides full programmatic control to automate browsers directly thereby scraping correctly. Getting Blocked Websites often block scrapers via blocked IP ranges or blocking characteristic bot activity through heuristics. Solutions: Slow down requests, properly mimic browsers, rotate user agents and proxies. Rate Limiting Servers fight overload by restricting number of requests served per time. Hitting these limits lead to temporary bans or denied requests. Solutions: Honor crawl delays, use proxies and ration requests appropriately. Here is sample code to handle rate limiting while scraping: Many websites have protection mechanisms that temporarily block scrapers when they detect too many frequent requests coming from a single IP address. We can counter getting blocked by rate limits by adding throttling, proxies and random delays in our code. Copy import time import random import requests from urllib.request import ProxyHandler, build_opener # List of free public proxies PROXIES = [\"104.236.141.243:8080\", \"104.131.178.157:8085\"] # Pause 5-15 seconds between requests randomly def get_request(): time.sleep(random.randint(5, 15)) proxy = random.choice(PROXIES) opener = build_opener(ProxyHandler({'https': proxy})) resp = opener.open(\"\") return resp for i in range(50): response = get_request() print(\"Request Success\") Here each request first waits for a random interval before executing. This prevents continuous rapid requests. We also route every alternate request through randomly chosen proxy servers via rotated IP addresses. Together, throttling down overall crawl pace and distributing requests over different proxy IPs prevents hitting site-imposed rate limits. Additional improvements like automatically detecting rate limit warnings in responses and reacting accordingly can enhance the scraper's resilience further. Rotating User Agents Websites often try to detect and block scraping bots by tracking characteristic user agent strings. To prevent blocks, it is good practice to rotate multiple well-disguised user agents randomly to mimic a real browser flow. Here is sample code to pick a random desktop user agent from a predefined list using Python's random library before making each request: Copy import requests import random # List of desktop user agents user_agents = [ \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/604.4.7 (KHTML, like Gecko) Version/11.0.2 Safari/604.4.7\" ] # Pick a random user agent string user_agent = random.choice(user_agents) # Set request headers with user agent before making request headers = {\"User-Agent\": user_agent} response = requests.get(url, headers=headers) By varying the user agent across requests in code runs, websites have a tougher time profiling traffic as coming from an automated bot using a static user agent. This allows the scraper to fly under the radar without getting blocked. Some additional enhancements include: Having separate user agent lists for mobile, tablets, desktop browsers Updating the lists with latest user agents periodically Dynamically generating user agents to match genuine browser attributes With effective user agent rotation and an ever expanding list of strings, scrapers enjoy better longevity undetected before site administrators can profile and actively block them. Browser Fingerprinting Beyond simplistic user agent checks, websites have adopted advanced browser fingerprinting techniques to identify bots. This involves browser attribute profiling - collecting information regarding device screen size, installed fonts, browser plugins etc. together called browser fingerprints. These fingerprints tend to remain largely consistent, stable and unique for standard tool-based bots and automation software. Dynamic websites track fingerprints of scrapers accessing them. By detecting known crawler fingerprints they can block them even if the user agents are rotated constantly. Minimizing detection risks Some ways to minimize exposing scraper fingerprints: Use Selenium to automate a standard desktop browser like Chrome or Firefox instead of custom bot agents Dynamically generate randomized attributes like viewport size, screen resolution, font lists within ranges of variety exhibited by human browsers Utilize proxy rotation and residential IP proxies to prevent tracking of IP specific attributes Limit number of parallel requests from a single proxy to site to make traffic volume seem manual Essentially by mimicking the natural randomness and variability across genuine user browsers, scraper fingerprints can avoid easy profiling by sites simply as another standard browser. Here is a code example to dynamically modify browser attributes to avoid fingerprinting: Copy from selenium import webdriver import random # List of common screen resolutions screen_res = [(1366, 768), (1920, 1080), (1024, 768)] # List of common font families font_families = [\"Arial\", \"Times New Roman\", \"Verdana\"] #Pick random resolution width, height = random.choice(screen_res) #Create chrome options opts = webdriver.ChromeOptions() # Set random screen res opts.add_argument(f\"--window-size={width},{height}\") # Set random user agent opts.add_argument(\"--user-agent=Mozilla/5.0...\") # Set random font list random_fonts = random.choices(font_families, k=2) opts.add_argument(f'--font-list=\"{random_fonts[0]};{random_fonts[1]}\"') # Initialize driver with options driver = webdriver.Chrome(options=opts) # Access webpage driver.get(target_url) # Webpage sees every scraper request originating # from distinct unpredictable browser profiles Here we randomly configure our Selenium controlled Chrome instance with different screen sizes, user agents and font sets per request. and here is how you do it using Python Requests… Copy import requests import random # Device profiles desktop_config = { 'user-agent': 'Mozilla/5.0...', 'accept-language': ['en-US,en', 'en-GB,en'], 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'accept-encoding': 'gzip, deflate, br', 'upgrade-insecure-requests': '1', 'sec-fetch-site': 'none', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'cache-control': 'max-age=0' } mobile_config = { 'user-agent': 'Mozilla/5.0... Mobile', 'accept-language': ['en-US,en', 'en-GB,en'], 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'x-requested-with': 'mark.via.gp', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'referer': '', 'accept-encoding': 'gzip, deflate, br', 'cache-control': 'max-age=0' } device_profiles = [desktop_config, mobile_config] def build_headers(): profile = random.choice(device_profiles) headers = { 'User-Agent': random.choice(profile['user-agent']), 'Accept-Language': random.choice(profile['accept-language']), # Other headers ... } return headers Now instead of hard coding, the scraper randomly selects from plausible configuration profiles including several identifying request headers - providing realistic and human-like mutations necessary to avoid fingerprint tracking. Parsing Complex HTML Scrape targets often have complex HTML structures, obfuscated tags and advanced client side code packing logic which break parsers. Solutions: Careful inspection of rendered source, using robust parsers like lxml and enhancing selectors. Here are some common types of bad HTML scrape targets exhibit and techniques to handle them: Improper Nesting HTML can often have incorrectly nested tags: Copy Latest News Impact of oil prices fall... Solution: Use a parser like lxml that handles bad nesting and uneven tags more robustly. Broken Markup Tags could be unclosed: CopyPython Web ScrapingLorem ipsum...Solution: Specify tag close explicitly while parsing: Copy title = soup.find(\"span\", class_=\"title\").text Non-standard Elements Vendor specific unrecognized custom tags may exist: CopyBelieverSolution: Search for standard tags in namespace: Copy song = soup.find(\"cisco:song\").text Non-text Content Tables, images embedded between text tags: CopyTrending Now ...Solution: Select child tags specifically: Copy paras = soup.select(\"p > text()\") This picks only text nodes as children ignoring other elements present under tag. As you can see, liberal use of selectors along with robust parsers provides the tools to handle even badly designed HTML and extract the required data reliably. Other guidelines worth following: Respect robots.txt rules Check if API access is available before scraping sites without permission Scrape data responsibly in moderate volumes Adopting these practices ensures reliable, resilient and responsible scraping operations. Conclusion In this comprehensive guide, we took an in-depth look into web scraping using Python. We covered: Why Python and libraries like BeautifulSoup are ideal for scraping most targets Common scraping patterns like making requests, parsing responses, handling dynamic content using Selenium Best practices around mimicry, circumventing blocks, respecting crawl delays and auto-throttling How to build resilient, production-grade scalable scrapers By learning core scraping paradigms, structuring code properly and applying optimization techniques, extracting accurate web data in Python at scale has become an achievable skill! While these examples are great for learning, scraping production-level sites can pose challenges like CAPTCHAs, IP blocks, and bot detection. Rotating proxies and automated CAPTCHA solving can help. Proxies API offers a simple API for rendering pages with built-in proxy rotation, CAPTCHA solving, and evasion of IP blocks. You can fetch rendered pages in any language without configuring browsers or proxies yourself. This allows scraping at scale without headaches of IP blocks. Proxies API has a free tier to get started. Check out the API and sign up for an API key to supercharge your web scraping. With the power of Proxies API combined with Python libraries like Beautiful Soup, you can scrape data at scale without getting blocked. Related articles: Web Scraping New York Times News Headlines in Go Python: The Go-To Language for Web Scraping How to write URL in Python? Avoiding Excess Characters When Writing Files in Python How to Use Proxy in WGet in 2024 Does Amazon allow web scraping? Using Rotating Proxies in rvest in 2024 Browse by tags: web scraping Python libraries BeautifulSoup best practices scraping challenges large scale scraping Browse by language: C# PHP Python JavaScript Rust Ruby Go C++ Objective-C Scala Elixir Kotlin Perl R Java Popular articles: Downloading Images from a Website with C++ and cpp-selector Scraping Multiple Pages in C++ with cpp-netlib and cppxpath Scraping eBay Listings with JavaScript and DOM Parsing in 2023 Scrape Any Website with OpenAI Function Calling in Ruby Logging and Debugging with Requests Mastering Python Requests Sessions for Power Users Scrape Websites with OpenAI Function Calling in JavaScript",
    "commentLink": "https://news.ycombinator.com/item?id=39442273",
    "commentBody": "Web Scraping in Python – The Complete Guide (proxiesapi.com)352 points by anticlickwise 18 hours agohidepastfavorite131 comments zopper 16 hours agoThis guide (and most other guides) are missing a massive tip: Separate the crawling (finding urls and fetching the HTML content) from the scraping step (extracting structured data out of the HTML). More than once, I wrote a scraper that did both of these steps together. Only later I realized that I forgot to extract some information that I need and had to do the costly task of re-crawling and scraping everything. If you do this in two steps, you can always go back, change the scraper and quickly rerun it on historical data instead of re-crawling everything from scratch. reply powersnail 15 hours agoparentWhat I find most effective, is to wrap `get` with local cache, and this is the first thing I write when I start a web crawling project. Therefore, from the very beginning, even when I'm just exploring and experimenting, every page only gets downloaded once to my machine. This way I don't end up accidentally bother the server too much, and I don't have to re-crawl if I make a mistake in code. reply bckygldstn 14 hours agorootparentrequests-cache [0] is an easy way to do this if using the requests package in python. You can patch requests with import requests_cache requests_cache.install_cache('dog_breed_scraping') and responses will be stored into a local sqlite file. [0] https://requests-cache.readthedocs.io/en/stable/ reply jot 15 hours agoparentprevThis is how I do it. I send the URLs I want scraped to Urlbox[0] it renders the pages saves HTML (and screenshot and metadata) to my S3 bucket[1]. I get a webhook[2] when it's ready for me to process. I prefer to use Ruby so Nokogiri[3] is the tool I use for scraping step. This has been particularly useful when I've want to scrape some pages live from a web app and don't want to manage running Puppeteer or Playwright in production. Disclosure: I work on Urlbox now but I also did this in the five years I was a customer before joining the team. [0]: https://urlbox.com [1]: https://urlbox.com/s3 [2]: https://urlbox.com/webhooks [3]: https://nokogiri.org reply nkko 1 hour agorootparentDoes it save the whole page or just the viewport? Just checked the landing page it looks targeted to a specific case of saving “screenshots” and this is also obvious from limitations in the pricing page so it would be unfeasible for larger projects? reply jjice 16 hours agoparentprevI've found this to be a good practice for ETL in general. Separate the steps, and save the raw data from \"E\" if you can because it makes testing and verifying \"T\" later much easier. reply iamacyborg 16 hours agorootparentI realise from working a few places that this isn't entirely common practice, but when we built the data warehouse at a startup I worked at, we engaged with a consultancy who taught us the fundamentals of how to do it properly. One of those fundamentals was separating out the steps of landing the data vs subsequent normalisation and transformation steps. reply ethbr1 16 hours agorootparentIt's unfortunate that \"ETL\" stuck in mindshare, as afaik almost all use cases are better with \"ELT\" I.e. first preserve your raw upstream via a 1:1 copy, then transform/materialize as makes sense for you, before consuming Which makes sense, as ELT models are essentially agile for data... (solution for not knowing what we don't yet know) reply dragonwriter 13 hours agorootparentI think ETL is right from the perspective where E refers to “from the source of data” and L refers to “to the ultimate store of data”. But the ETL functionality should itself lives in a (sub)system that has its own logical datastore (which may or may not be physically separate from the destination store), and things should be ELT where the L is with respect to that store. So, its E(LTE)L, in a sense. reply appplication 3 hours agorootparentFor those confused as to whether ETL or ELT is ultimately more appropriate for you… almost everyone is really just doing ETLTLTLT or ELTLTLTL anyways. The distinction is really moot. reply greenie_beans 14 hours agorootparentprevthis is what i try to do but i want to learn more about approaches like this, do you know any good resources about how to design ETL pipelines? reply 65 12 hours agorootparentI built an ETL pipeline for a government client using just AWS, Node, and Snowflake. All Typescript. To cache the data I store responses in S3. If there's a cache available, use the S3 data, if not get the new data. We can also clean the old cache occasionally with a cron job. Then do transforms and put it in Snowflake. Sometimes we need to do transforms before caching the data in S3 (e.g. adding a unique ID to CSV rows), or doing things like splitting giant CSV files into smaller files that can then be inserted into Snowflake (Snowflake has a 50mb payload limit). We have alerts, logging, and metadata set up as well in AWS and Snowflake. Most of this comes down to your knowledge of cloud data platforms. It's honestly not that difficult to build ETL pipelines from scratch. We're using a ton of different sources with different data formats as well. Using the Serverless framework to set up all the Lambda functions and cron jobs also makes things a lot easier. reply jjice 13 hours agorootparentprevI wish I did. I currently work at a startup with our core offering being ETL, so I've learned along the way as we've continued. If anyone has any, I'd love to hear as well. Keeping raw data when possible has been huge. We keep some in our codebase for quick tests during development and then we keep raws from production runs that we can evaluate with each change, giving us an idea of the production impact of the change. reply computershit 13 hours agorootparentprevThere's quite a bit of new tooling in this space, selecting the right one is going to depend on your needs then you can spike from there. Check out Prefect, Dagster, Windmill, Airbyte (although the latter is more ELT than ETL). reply NortySpock 7 hours agorootparentprevDisclaimer: previous job had a lot of cases where CSVs were dropped by SFTP, your milage may vary..., JSON APIs are said to be a different flavor of crazy... Haven't heard much beyond \"ask the Old Ones\", but \"Murphy's law strikes again\", \"eventually someone will want that data even though they swore it was unnecessary\", \"eventually someone will ask for a backfill/replay\", \"eventually someone will give you a duplicate file\", \"eventually someone will want to slice-and-dice the data a different way\" and \"eventually someone will change the schema without telling you\" have been some things I have noticed. Even de-duplicating data is, in a sense, deletion (or someone will eventually want to get at the data with duplicates -- e.g. for detecting errors or repeats or fraud or some other analysis that mirrors looking at the bullet holes in World War 2 bombers) Store the data as close to the original form as you can. Keep a timestamp of when you landed the data. Create a UUID for the record. Create a hash of the record if you can. Create a batch_id if you load multiple things at once (e.g. multiple CSVs). Don't truncate and reload a table - rather, append to it. If you still need something that looks like atomic table changes, I've gotten away with something close: \"a view that shows only the most recent valid batch\". (Yes this is re-inventing the database wheel, but sometimes you make do with the tools you are forced to use.) Someone, somewhere, will hand you a file that does not conform to the established agreement. You want to log that schema change, with a timestamp, so you can complain to them with evidence that they ain't sending you what they used to, and they didn't bother sending you an email beforehand... They're not going to fix it on your timeline, so you're probably going to end up hacking your code to work a different way... Until, you know, they switch it back... So, yeah. Log it. Timestamp it. Hash it. UUID it. Don't trust the source system to do it right, because they will eventually change the script on you. Keep notes, and plan in such a way that you have audit logs and can move with agility. reply NortySpock 6 hours agorootparentIn conclusion... I find, in data engineering ,the goal is not to prevent everything, it's to be flexible and prepared to handle lots of change, even silly changes, and be able to audit it, observe it, maneuver around it, and keep the mean-time-to-resolution low. reply nathell 15 hours agoparentprevYes! My Clojure scraping framework [0] facilitates that kind of workflow, and I’ve been using it to scrape/restructure massive sites (millions of pages). I guess I’m going to write a blog post about scraping with it at scale. Although it doesn’t really scale much above that – it’s meant for single-machine loads at the moment – it could be enhanced to support that kind of workflow rather easily. [0]: https://github.com/nathell/skyscraper reply aviperl 15 hours agoparentprevAn easy way to do this that I've used is to cache web requests. This way, I can run the part of the code that gets the data again with say a modification to grab data from additional urls, and I'm not unnecessarily rerunning my existing URLs. With this method, I don't need to modify existing code either, best of both worlds. For this I've used the requests-cache lib. reply arbuge 15 hours agorootparentLooks like a really useful library - thanks for the tip. reply bbkane 16 hours agoparentprevYes!! https://beepb00p.xyz/unnecessary-db.html really changed how I think about data manipulation, mostly with this principle. reply gdcbe 15 hours agoparentprevI talked about exactly that on a conference in 2022: https://youtu.be/b0lAd-KEUWg?feature=shared free to watch. reply generalizations 14 hours agoparentprevCan confirm. A few discrete scripts each focused on one part of the process can make the whole thing run seamlessly async, and you naturally end up storing the pages for processing by subsequent scripts. Especially if you write a dedicated downloader - then you can really go nuts optimizing and randomizing the download parameters for each individual link in the queue. \"Do one thing and do it well\" FTW. reply nkozyra 15 hours agoparentprevAlthough in general I like the idea of a queue for a scraper to access separately, another option - assuming you have the storage and bandwidth - is to capture and store every requested page, which lets you replay the extraction step later. reply BiteCode_dev 16 hours agoparentprevThe problem is crawling is generally optimized with info you find in the page. reply fragmede 15 hours agoparentprevif you're using requests in python, requests-cache does exactly this for you, saving the data to an sqlite db, and is compatible with your code using requests. reply tussa 14 hours agoparentprevIt applies to many other project too: cling on to the raw data as long as it isn't bogging you down too much. reply throwaway81523 14 hours agoparentprevGenerally it's enough to archive the retrieved HTML just in case. reply photochemsyn 16 hours agoparentprevI've found this approach works really well using JavaScript and puppeteer for the first stage, and then Python for the second stage (the re module for regular expressions is nice here IMO). JS/puppeter seems a bit easier for things like rotating user agents, from article: > \"Websites often block scrapers via blocked IP ranges or blocking characteristic bot activity through heuristics. Solutions: Slow down requests, properly mimic browsers, rotate user agents and proxies.\" reply black3r 13 hours agorootparentIf you're using JS in the first step just because you need puppeteer, check out playwright. It's what the original authors of puppeteer are working on now and it's been more actively developed in the past few years, very similar in usage and features, but it also has an official python wrapper package. reply greenie_beans 14 hours agoparentprevthis is the way. reply simonw 18 hours agoprevI strongly recommend adding Playwright to your set of tools for Python web scraping. It's by far the most powerful and best designed browser automation tool I've ever worked with. I use it for my shot-scraper CLI tool: https://shot-scraper.datasette.io/ - which lets you scrape web pages directly from the command line by running JavaScript against pages to extract JSON data: https://shot-scraper.datasette.io/en/stable/javascript.html reply BeetleB 18 hours agoparentI actually use your shot-scraper tool (coupled with Mozilla's Readability) to extract the main text of a site (to convert to audio and listen via a podcast player). I love it! Some caveats though: - It does fail on some sites. I think the value of scrapy is you get more fine grained control. Although I guess if you can use any JS with shot-scraper you could also get that fine grained control. - It's slow and uses up a lot of CPU (because Playwright is slow and uses up a lot of CPU). I recently used shot-scraper to extract the text of about 90K sites (long story). Ran it on 22 cores, and the room got very hot. I suspect Scrapy would use an order of magnitude less power. On the plus side, of course, is the fact that it actually executes JS, so you can get past a lot of JS walls. reply simonw 17 hours agorootparentWow, you're really putting it through its paces! When you ran it against 90,000 sites were you running the \"shot-scraper\" command 90,000 times? If so, my guess is that most of that CPU time is spent starting and stopping the process - shot-scraper wasn't designed for efficient start/stop times. I wonder if that could be fixed? For the moment I'd suggest writing Playwright code for 90,000 site scraping directly in Python or JavaScript, to avoid that startup overhead. reply BeetleB 15 hours agorootparentYes, indeed I launched shot-scraper command 90K times. Because it's convenient :-) I didn't realize starting/stopping was that expensive. I thought it was mostly the fact that you're practically running a whole browser engine (along with a JS engine). If I do this again, I'll look into writing the playwright code directly (I've never used it). reply ravenstine 16 hours agorootparentprevReadability is great, and I use it, but it's odd how half-assed the maintenance for it has been. I've haven't seen any noticeable improvements to it in quite some time, and when I've looked for alternatives, it usually turns out they're using it under the hood in some capacity. Perhaps it's already being made obsolete by LLM technologies? I'd be curious to hear from anyone who's used a locally running LLM to extract written content, especially if it's been built specifically for that task. reply BeetleB 14 hours agorootparent> Readability is great, and I use it, but it's odd how half-assed the maintenance for it has been. I've haven't seen any noticeable improvements to it in quite some time What improvements are you looking for? For me, it works over 95% of the time, so I'm happy. Occasionally it excises a section (e.g. \"too short\" heuristic), and I wish it was smarter about it. But like you, I haven't found better alternatives. I also need something I can run in a script. > Perhaps it's already being made obsolete by LLM technologies? I'd be curious to hear from anyone who's used a locally running LLM to extract written content, especially if it's been built specifically for that task. It would be good to benchmark this across, say, 50 sites and see which one performs better. At the moment, I don't know if I'd trust an LLM more than Readability - especially for longer content. Also, I wouldn't use it to scrape 90K sites. Both slow and expensive! reply ravenstine 11 hours agorootparentAlthough it works most of the time, I've found it's common for it to either pick up things that shouldn't be included or it only picks up something like the footer but not the actual body. This can be true even when, upon inspection, there's no clear reason why the body couldn't be identified. It's particularly problematic on many academic articles that are in HTML (sort of ironic). I'd also like a bit more normalization built in, even if it's turned off by default. reply thrdbndndn 17 hours agoparentprevKinda tangent, but Playwright's doc (specifically, the intro https://playwright.dev/python/docs/intro ) confuses me. It asks you to write a test and then run `pytest`, instead of just letting you to use the library directly (which exists, but is buried in the main text: https://playwright.dev/python/docs/library). I understand that using Playwright in tests is probably the most common use case (it's even in their tagline) but ultimately the introduction section of a lib should be about the lib itself, not certain scenario to use it with a 3rd-party lib B (`pytest`). Especially when it may cause side effect (I wasn't \"bitten\" by it but surely was confusing: when I was learning it before, I created test_example.py as said in a minefield folder which has batch of other test_xxxx.py files. And running `pytest` causes all of them to run, and gives confusing outputs. And it's not obvious to me at all, since I've never used pytest before and this is not a documentation about pytest, so no additional context was given.) > tagline reply simonw 17 hours agorootparentHah yeah that's confusing. https://playwright.dev/python/docs/intro is actually the documentation for pytest-playwright - their pytest plugin. https://playwright.dev/python/docs/library is the documentation for their automation library. I just filed an issue pointing out that this is confusing. https://github.com/microsoft/playwright/issues/29579 reply PaulHoule 16 hours agorootparentBack in the day I used to use HTMLUnit https://htmlunit.sourceforge.io/ to crawl Javascript-based sites from Java. I think it was originally intended for integration tests but it sure works well for webcrawlers. I just wrote a Python-based webcrawler this weekend for a small set of sites that is connected to a bookmark manager (you bookmark a page, it crawls related pages, builds database records, copies images, etc.) and had a very easy time picking out relevant links, text and images w/ CSS selectors and beautifulsoup. This time I used a database to manage the frontier because the system is interactive (you add a new link and it ought to get crawled quickly) but for a long time my habit was writing crawlers that read the frontier for pass N from a text file which is one URL per line and then write the frontier for pass N+1 to another text file because this kind of crawler is not only simple to write but it doesn't get stuck in web traps. I have a few of these systems that do very heterogenous processing of mostly scraped content and something think about setting up a celery server to break work up into tasks . reply 0xDEADFED5 15 hours agorootparentprevagreed, playwright is great. it even has device emulation profiles built in, so you can for instance use an iphone device with the right screen size/browser/metadata automatically reply thundergolfer 16 hours agoparentprevWe use shot-scraper internally to automate keeping screenshots in our documentation up-to-date. Thanks for the tool![1] Agree that Playwright is great. It's super easy to run on Modal.[2] 1. https://modal.com/docs/guide/workspaces#dashboard 2. https://modal.com/docs/examples/web-scraper#a-simple-web-scr... reply tnolet 16 hours agoparentprev100%. Playwright (which does have Python support) is completely owning this scene. The robustness is amazing. reply sam2426679 11 hours agoparentprevCan anyone recommend a good methodology for writing tests against a Playwright scraping project? I have a relatively sophisticated scraping operation going, but I haven’t found a great way to test methods that are dependent on JavaScript interaction behind a login. I’ve used Playwright’s har recording to great effect for writing tests that don’t require login, but I’ve found that har recording doesn’t get me there for post-login because the har playback keeps serving the content from pre-login (even though it includes the relevant assets from both pre and post login.) reply 3abiton 14 hours agoparentprevHow does it compare to selenium or puppeteer? reply black3r 13 hours agorootparentPlaywright is a rewrite of puppeteer by people who worked on puppeteer before, but now under Microsoft instead of Github. Not sure if it reached feature parity yet, but all the things we used to do with puppeteer work with playwright, and it seems to be more actively developed. reply sam2426679 12 hours agorootparentprevIme playwright is selenium plus some, e.g. you can inspect network activity without having a separately configured proxy. reply Oras 18 hours agoparentprev+1 for playwright. The codegen is a brilliant way to simplify scraping. reply sakisv 17 hours agoparentprevCame here to write about Playwright. I've been using it for the last ~13 months to scrape supermarket prices and it's been a great experience. reply mrtimo 17 hours agorootparentwould love to learn more about what you are doing with supermarket prices reply sakisv 11 hours agorootparentMy main drive was to document the crazy price hikes that's been going on in my home country, Greece, so I'm scraping its 3 biggest supermarkets and keep track of the prices of their products* Had a lot of fun building and automating the scraping, especially in order to get around some bot catching rules that they have. For example one of them blocks all the requests originating from non-residential IPs, so I had to use tailscale to route the scraper's traffic through my home connection and take advantage of my ISP's CGNAT. You can take a look here: https://pricewatcher.gr/en/ * I'm not doing any deduplication or price comparisons between the supermarkets, I only show historical prices of the same product, to showcase the changes. reply dommer 16 hours agorootparentprev+1 Interested in this area as well. reply samstave 16 hours agorootparentprevI too choose this guys supermaket scraper! What Ive long wanted was the the ability to map prices to SCUs by having folks simply take a pic of the UPC + price, just like gasbuddy or what not - in addition to scraping from grocery posting their coupon sheets online for scraping, in addition to people just scanning (non-PII) portions of receipts. Can you share what you've made thus far? * could it be used as an automated \"price matching\" finder? (for those companies that do a \"we price match!\" thing reply martin82 36 minutes agoprevGood beginner tutorial and some good stuff in here, but the chances of scraping any site that is behind Cloudflare or AWS WAF (which is almost all interesting sites), are basically zero. reply zffr 15 hours agoprevHere are some tips not mentioned: 1. /robots.txt can sometimes have useful info for scraping a website. It will often include links to sitemaps that let you enumerate all pages on a site. This is a useful library for fetching/parsing a sitemap (https://github.com/mediacloud/ultimate-sitemap-parser) 2. Instead of parsing HTML tags, sometimes you can extract the data you need through structured metadata. This is a useful library for extracting it into JSON (https://github.com/scrapinghub/extruct) reply evilsaloon 15 hours agoprevAlways funny seeing SaaS companies pitch their own product in blog posts. I understand it's just how marketing works, but pitching your own product as a solution to a problem (that you yourself are introducing, perhaps the first time to a novice reader) never fails to amuse me. reply 65 18 hours agoprevI'm not sure why Python web scraping is so popular compared to Node.js web scraping. npm has some very well made packages for DOM parsing, and since it's in Javascript we have more native feeling DOM features (e.g. node-html-parser using querySelector instead of select - it just feels a lot more intuitive). It's super easy to scrape with Puppeteer or just regular html parsers on a Lambda. reply danpalmer 17 hours agoparentHaving done a lot of web scraping, the thing that often matters is string processing. Javascript/Node are fairly poor at this compared to Python, and lack a lot of the standard library ergonomics that Python has developed over many years. Web scraping in Node just doesn't feel productive. I'd imagine Perl is also good for those in that camp. I've also used Ruby and again it was nice and expressive in a way that JS/Node couldn't live up to. Lastly, I've done web scraping in Swift and that felt similar to JS/Node – much more effort to do data extraction and formatting, not without benefits of course. I also suspect that DOM-like APIs are somewhat overrated here with regards to web scraping. JS/Node would only have an emulation of DOM APIs, or you're running a full web browser (which is a much bigger ask in terms of resources, deployment, performance, etc), and to be honest, lxml in Python is nice and fast. I generally found XPath much better for X(HT)ML parsing than CSS selectors, and XPath support is pretty available across a lot of different ecosystems. reply staticautomatic 16 hours agorootparentBest web scraping guy I ever met (the type you hire when no one else can figure out how) was a Perl expert. I don’t know Perl so I don’t know why, but this is very real. reply danpalmer 16 hours agorootparentYeah I'm not surprised. My previous company had scrapers for ~50 of our suppliers (so they didn't need to integrated with us), and I worked on/off on them for 7 years. It was a very different type of work to the product/infra work I spent most of my time doing. One scraper is often not hugely valuable, most companies I've seen with scrapers have many scrapers. This means that the time investment available for each one is low. Some companies outsource this, and that can work ok. Then scrapers also break. Frequently. Website redesigns, platform moves, bot protection (yes, even if you have a contract allowing you to scrape, IT and BizDev don't talk to each other), the site moving to needing JavaScript to render anything on the page... they can all cause you to go back to the drawing board. The concept of \"tech debt\" kinda goes out of the window when you rewrite the code every 6 months. Instead the value comes from how quickly you can write a scraper and get it back in production. The code can in fact be terrible because you don't really need to read it again, automated testing is often pointless because you're not going to edit the scraper without re-testing manually anyway. Instead having a library of tested utility functions, a good manual feedback loop, and quick deployments, were much more useful for us. reply thrdbndndn 17 hours agoparentprevTo me it's mainly the following three reasons, but take it with a grain of salt since my JS is not as fluent as Python. 1. the async nature of JS is surprisingly detrimental when writing scraping script. It's hard to describe, but it makes have a mental image of the whole code base or workflow harder. Writing mostly sync code and only use things like ThreadPoolExecutor (not even Threading directly) when necessary has been much easier for me to write clean, easy-to-maintain code. 2. I really don't like the syntax of loops or iterations in JS, and there are a lot of them in web scraping. 3. String processing and/or data re-shaping feels harder in JS. The built-in functions often feel unintuitive. reply Chiron1991 17 hours agorootparentI agree with anything you said, and: 4. Having the scraped data in Python-land makes it sometimes way easier to dump it into an analysis landscape, which is probably Python, too. reply spaniard89277 16 hours agorootparentprevI've had some experiences with selenium and now I'm using puppeteer, and I honestly don't see the problem with JS. It's true that I have not much experience coding but it seems to me that Pupeteer + Flask serving ML to extract data is the cake. Also, being able to play around evaluating expressions in pupeteer, etc, makes it manageable. Maybe I lack experience but I don't see JS being a barrier. I would like to know what kind of string work are you doing. I can't imagine being dependent on parsing strings and such, that looks very easy to break, even easier that css selector dance. reply giantrobot 17 hours agorootparentprev> String processing and/or data re-shaping feels harder in JS. The built-in functions often feel unintuitive. Hey don't worry there's probably a library that does it for you! It only pulls down a half gigabyte of dependencies to left-pad strings! I hate the JavaScript ecosystem so fucking much. reply macintux 17 hours agoparentprevPerhaps more of the people who need to run this kind of data scraping operation are comfortable with Python. Data scientists, operations personnel, etc. I've been using Perl and Python for 30 years, and JS for a few weeks scattered across those same years. reply aosaigh 17 hours agoparentprevBecause it’s been around longer. Beautiful Soup was first released in 2004 according to its wiki page and I’m sure there were plenty of libraries before it. reply mdaniel 6 hours agoparentprev> I'm not sure why Python web scraping is so popular compared to Node.js web scraping Take this with a grain of salt, since I am fully cognizant that I'm the outlier in most of these conversations, but Scrapy is A++ the no-kidding best framework for this activity that has been created thus far. So, if there was scrapyjs maybe I'd look into it, but there's not (that I'm aware of) so here we are. This conversation often comes up in any such \"well, I just use requests & ...\" conversation and if one is happy with main.py and a bunch of requests invocations, I'm glad for you, but I don't want to try and cobble together all the side-band stuff that Scrapy and its ecosystem provide for me in a reusable and predictable way Also, often those conversations conflate the server side language with the \"scrape using headed browser\" language which happens to be the same one. So, if one is using cheeriothen sure node can be a fine thing - if the blog post is all \"fire up puppeteer, what can go wrong?!\" then there is the road to ruin of doing battle with all kinds of detection problems since it's kind of a browser but kind of not I, under no circumstances, want the target site running their JS during my crawl runs. I fully accept responsibility for reproducing any XHR or auth or whatever to find the 3 URLs that I care about, without downloading every thumbnail and marketing JS and beacon and and and. I'm also cognizant that my traffic will thus stand out since it uniquely does not make the beacon and marketing calls, but my experience has been that I get the ban hammer less often with my target fetches than trying to pretend to be a browser with a human on the keyboard/mouse but is not reply dist-epoch 17 hours agoparentprevOne of the reasons is that after you scrape it you want to do something with the data: put it in a Postgres/SQLite, save it to disk, POST it to some webserver, extract some stats from it and write to a CSV, ... This stuff is much easier to do in Python. reply 1vuio0pswjnm7 8 hours agoprevI would prefer if people would not submit sites to HN that are using anti-scraping tactics such as DataDome that block ordinary web users making a single HTTP request using a non-popular, smaller, simpler client. One example is www.reuters.com. It makes no sense because the site works fine without Javascript but Javascript is required as a result of the use of DataDome. See below for example demonstration. For anyone who is doing the scraping that causes these websites to use hacks like DataDome: Does your scraping solution get blocked by DataDome. I suspect many will answer no, indicating to me that DataDome is not effective at anything more than blocking non-popular clients. To be more specific, there seems to be a blurring of the line between blocking non-popular clients and preventing \"scraping\". If scraping can be accomplished with the gigantic, complex popular clients, then why block the smaller, simpler non-popular clients that make a single HTTP request. To browse www.reuters.com text-only in a gigantic, complex, popular browser 1. Clear all cookies 2. Allow Javascript in Settings for the site ct.captcha-delivery.com 3. Block Javascript for the site www.reuters.com 4. Block images for the site www.reuters.com First try browsing www.reuters.com with these settings. Two cookies will be stored. One from reuters.com. This one is the DataDome cookie. And another one from www.reuters.com. This second cookie can be deleted with no effect on browsing. NB. No ad blocker is needed. Then clear the cookies, remove the above settings and try browsing www.reuters.com with Javascript enabled for all sites and again without an ad blocker. This is what DataDome and Reuters ask web users to do: \"Please enable JS and disable any ad blocker.\" Following this instruction from some anonymous web developer totally locks up the computer I am using. The user experience is unbearable. Whereas with the above settings I used for the demonstration, browsing and reading is fast. reply thijsvandien 17 hours agoprevI thought scraping is kind of dead given all the CAPTCHAs and auth walls everywhere. The article does mention proxies and rate limiting, but could anyone with (recent) practical experience elaborate on dealing with such challenges? reply caesil 17 hours agoparentNot only is scraping not dead but it has won the arms race. There are ways around every defense, and this will only accelerate as AI advances. The CAPTCHAs and walls are more of a desperate, doomed retreat. reply annowiki 17 hours agorootparentHow do you get around 403/401's from WSJ/Reuters/Axios? Because I've tried user agent manipulation and it seems like I'd have to use selenium and headless to deal with them. reply eddd-ddde 16 hours agorootparentSometimes you also need \"Accept: html\" I have noticed. reply jonatron 16 hours agorootparentprevIf curl-impersonate works, it's probably TLS fingerprinting. reply accidbuddy 17 hours agorootparentprevSome months ago, I had problems with captcha. I tried to write an application to access many drugstores and compare the price, but captcha with login system fail the mission. Do you have any piece of advice for me? reply nico 16 hours agorootparentNot sure about the currently available tools, given the break-neck speed of AI progress, but a couple of years ago I built a scraper that used a captcha-solving service, they sell something like 1000 solutions for $10, it was super cheap. The process was a bit slow because they were using humans to solve the captchas, but it worked really well reply jawerty 16 hours agorootparentprevTry 2captcha https://2captcha.com/2captcha-api reply leumon 15 hours agoparentprevIf you have a decent gpu (16gb+ vram) and are using Linux, then this tool I wrote some days ago might do the trick. (at least for googles recaptcha). Also, for now, you have to call the main.py every time you see a captcha on a site and you need the gui since I am only using vision via Screenshots, no HTML or similar. (Sorry that it's not yet that well optimized. I am currently very busy with lots of other things, but next week I should have time to improve this further. But it should still work for basic scraping.) https://github.com/notune/captcha-solver/ reply pocket_cheese 15 hours agoparentprevA few different techniques - 1. use mobile phone proxies. Because of how mobile phone networks do NAT, basically it means that thousands of people share IPs and are much less like to get blocked. 2. Reverse engineer APIs if the data you want is returned in an ajax call. 3. Use a captcha solving service to defeat captchas. There's many and they are cheap. 4. Use an actual phone or get really good at convincing the server you are a mobile phone. 5. Buy 1000s of fake emails to simulate multiple accounts. 6. Experiment. Experiment. Experiment. Get some burner accounts. Figure out if they have request per min/hour/day throttling. See what behavior triggers a cloudflare captchas. Check if different variables such as email domain, useragent, voip vs non-voip sms based 2fa. your goal is to simulate a human. So if you sequentially enumerate through every document - that might be what get's you flagged. Best of luck and happy scraping! reply nlunbeck 16 hours agoparentprevI've noticed many smaller and medium sites only use client-side CAPTCHAs/paywalls reply cnqso 17 hours agoprevAny modern web scraping set up is going to require browser agents. You will probably have to build your own tools to get anything from a major social media platform, or even NYT articles. reply mr_00ff00 16 hours agoparentMay be misunderstanding what you mean by “browser agents” but I’ve done some web scraping that had dynamic content and it was easy with a simple chrome driver / gecko driver + scraper crate in Rust reply 1-6 16 hours agoprevHow many complete guides are out there for Python Scraping? reply brianarbuckle 14 hours agoprevIt's much simpler to get the links via pandas read_html: import pandas as pd tables = pd.read_html('https://commons.wikimedia.org/wiki/List_of_dog_breeds', extract_links=\"all\") tables[-1] reply givemeethekeys 17 hours agoprevAre scrapers written on a per-website basis? Are there techniques to separate content from menus / ads / filler / additional information, etc? How do people deal with design changes - is it by rewriting the scraper whenever this happens? Thanks! reply spaniard89277 16 hours agoparentYeah. I managed to abstract a bit the structure but in the end websites change. reply staticautomatic 16 hours agoparentprevYeah it’s often gonna be a per site, lots of xpath queries, email me when it breaks kind of endeavor. reply DishyDev 14 hours agoprevI've had to do a lot of scraping recently and something that really helps is https://pypi.org/project/requests-cache/ . It's a drop in replacement for the requests library but it caches all the responses to a sqlite database. Really helps if you need to tweak your script and you're being rated limited by the sites you're scraping. reply SinjonSuarez 16 hours agoprevCheck out the cloudscraper library if are having speed/cpu issues with sites that require js/have cloudfare defending them. That plus a proxy list plus threading allows me to make 300 requests a minute across 32 different proxies. Recently implemented it for a project: https://github.com/rezaisrad/discogs/tree/main/src/managers reply fireant 3 hours agoparentI've found myself writing the same session/proxy/rate limiting/header faking management code over and over for my scrapers. I've extracted it into it's own service that runs in docker and acts as a MITM proxy between you and target. It is client language agnostic, so you can write scrapers in python, node or whatever and still have great performance. Highly recommend this approach, it allows you to separate infrastructure code, that gets highly complex as you need more requests, from actual spider/parser code that is usually pretty straightforward and project specific. https://github.com/jkelin/forward-proxy-manager reply mndgs 14 hours agoparentprevNicely written scraper, btw. Good code. reply SinjonSuarez 12 hours agorootparentappreciate that! as a few mentioned here, there’s a lot of useful scraping tools/libraries to leverage these days. headless selenium no longer seems to make sense to me for most use cases reply philippta 17 hours agoprevShameless plug: Flyscrape[0] eliminates a lot of boilerplate code that is otherwise necessary when building a scraper from scratch, while still giving you the flexibility to extract data that perfectly fit your needs. It comes as a single binary executable and runs small JavaScript files without having to deal with npm or node (or python). You can have a collection of small and isolated scraping scripts, rather than full on node (or python) projects. [0]: https://github.com/philippta/flyscrape reply simonw 15 hours agoparentDoes Flyscrape execute JavaScript that is on the page (e.g. by running a headless browser) or is it just parsing HTML and using CSS selectors to extract code from a static DOM? reply thomasisaac 17 hours agoprevWe've used ScraperAPI for a long time: https://www.scraperapi.com/ Couldn't recommend them more. reply croemer 17 hours agoparentI've used ScrapingBee which has similar pricing and has worked well, can't say which one is better: https://www.scrapingbee.com/ reply rustdeveloper 15 hours agorootparentI'm using Scraping Fish because of their pay-as-you-go style pricing as opposed to subscription with monthly scraping volume commitment. And they don't charge extra credits for JS rendering or residential proxies because the cost of each request is the same: https://scrapingfish.com reply screye 17 hours agorootparentprevWe used to be on ScraperAPI, but moved to ScrapingBee after more frequent failures from ScraperAPI. If your scraping needs have realtime requirements, then I'd recommend ScrapingBee. reply thomasisaac 16 hours agorootparentWeird, we found the exact opposite - what were you scraping? ScrapingBee really struggles on so many domains - ScraperAPI is almost as good as Brightdata when it comes to hard to beat sites. reply daolf 15 hours agorootparentHi Thomas, really sorry you had a bad experience with ScrapingBee. Would you mind sending me the account you used as I wasn't able to find anything under Thomas Isaac or Tillypa and couldn't see what was going wrong then. I'm sure your comment has nothing to do with the fact that you share the same investor as ScraperAPI but I just wanted be sure. reply DoodahMan 8 hours agorootparentany HN discount by chance? ;) i'm testing y'all out now for a time-sensitive scrape job that must be done by Mar 1st. reply bilater 15 hours agoprevI'm convinced there is a gold mine sitting right in front of us ready to be picked by someone who can intelligently combine web scraping knowledge with LLMs e.g. scrape data, feed it into LLMs do get insights in an automated fashion. I don't know exactly what the final manifestation looks like but its there and will be super obvious when someone does it. reply bfeynman 15 hours agoparentI feel that the more immediate and impactful opportunity that people are doing is instead of scraping to get/understand content. LLM agents can just interactively navigate websites and perform actions. Parsing/Scraping can be brittle with changes, but an LLM agent to perform an action can just follow steps to search, click on results, and navigate like a human would reply rashkov 12 hours agorootparentAre you aware of any projects for this? I began to build my own but quickly saw that the context window is not large enough to hold the DOM of many websites. I began to strip unnecessary things from the DOM but it became a bit of a slog. L reply generalizations 15 hours agoparentprevI tried that. Turns out that LLM-generated regex is still better (and a lot faster) than using an LLM directly. reply calf 15 hours agoprevI've been writing rudimentary Python scripts to scrape online recipe websites for my hobby cooking purposes, and I wish there was some general software that could do this more simply. One of the websites has started making their images unclickable, so measures like that make me think it might become harder to automatically fetch such content. reply dogman144 15 hours agoprevThere was a similar guide on HN titled something like \"how to scrape like the big boys\" which dug into a setup using mobile IPs, racks of burner phones, and so on. It's been lost to a bad bookmark setup of mine, and if anyone has a lead on that resource, please link, thank you and unlimited e-karma heading your way. reply recursive4 15 hours agoparenthttps://news.ycombinator.com/item?id=29117022 reply dogman144 15 hours agorootparentAmazing, this it! Sincere thanks, been looking around for this for a few years, looks like my HN search abilities needs work. reply throwaway81523 14 hours agoprevThis is basically an advertisement for the site's scraping proxy service. reply jerzyt 12 hours agoprevOf course, Wikipedia is the easiest website to scrape. The HTML is so clean an organized. I'd like to find some code to scrape Airbnb. reply anotherpaulg 14 hours agoprevI recently used Playwright for Python [0] and pypandoc [1] to build a scraper that fetches a webpage and turns the content into sane markdown so that it can be passed into an AI coding chat [2]. They are both powerful yet pragmatic dependencies to add to a project. I really like that both packages contain wheels or scriptable methods to install their underlying platform-specific binary dependencies. This means you don't need to ask end users to figure out some complex, platform-specific package manager to install playwright and pandoc. Playwright let's you scrape pages that rely on js. Pandoc is great at turning HTML into sensible markdown. For example, below is an excerpt of the openai pricing docs [3] that have been scraped to markdown [4] in this manner. [0] https://playwright.dev/python/docs/intro [1] https://github.com/JessicaTegner/pypandoc [2] https://github.com/paul-gauthier/aider [3] https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turb... [4] https://gist.githubusercontent.com/paul-gauthier/95a1434a28d... ## GPT-4 and GPT-4 Turbo GPT-4 is a large multimodal model (accepting text or image inputs and outputting text) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. GPT-4 is available in the OpenAI API to [paying customers](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4). Like `gpt-3.5-turbo`, GPT-4 is optimized for chat but works well for traditional completions tasks using the [Chat Completions API](/docs/api-reference/chat). Learn how to use GPT-4 in our [text generation guide](/docs/guides/text-generation). +-----------------+-----------------+-----------------+-----------------+ModelDescriptionContext windowTraining data+=================+=================+=================+=================+gpt| 128,000 tokensUp to Dec 2023| -4-0125-preview||| New|||||||||| **GPT-4||Turbo**\\||The latest||GPT-4 model||intended to||reduce cases of||\"laziness\"||where the model||doesn't||complete a||task. Returns a||maximum of||4,096 output||tokens. [Learn||more](ht||tps://openai.co||m/blog/new-embe||dding-models-an||d-api-updates).|+-----------------+-----------------+-----------------+-----------------+gpt-Currently128,000 tokensUp to Dec 2023| 4-turbo-previewpoints to||`gpt-4||-0125-preview`.|+-----------------+-----------------+-----------------+-----------------+ ... reply naiv 16 hours agoprevI wonder what percentage of Google's daily searches are actually coming from scrapers reply aaroninsf 15 hours agoprevAs someone who works at a non-profit which is increasingly and regularly crawled, sometimes very aggressively, PLEASE PLEASE PLEASE establish and use a consistent useragent string. This lets us load balance and steer traffic appropriately. Thank you. reply codingminds 15 hours agoparentMind sharing which one? I'm curious reply lagt_t 16 hours agoprevHow expensive are the content bundles? reply justinzollars 15 hours agoprevI've tried this for the first time recently in 10 years - it's really become a miserable chore. There are so many countermeasures deployed to web scraping. The best path forward I could imagine is utilizing LLMs, taking screenshots and having the AI tell me what it sees on the page; but even gathering links is difficult. xml site maps for the win. reply moritonal 15 hours agoparentLiterally step for step what I spent my weekend putting together. Here's the preview blog I wrote on it. https://blog.bonner.is/using-ai-to-find-fencing-courses-in-l... Only step you missed was embeddings to avoid all the privacy pages, and a cookie banner blocker (which arguably the AI could navigate if I cared). reply justinzollars 15 hours agorootparentAwesome! Things have gotten so bad this is the only alternative. I tried building a hobby search engine then quickly gave up, but did imagine how I would do the scraping! reply f311a 15 hours agoprev>BeautifulSoup > Features: Excellent HTML/XML parser, easy web scraping interface, flexible navigation and search. It does not feature any parser. It’s basically a wrapper over lxml. >lxml > Features: Very fast XML and HTML parser. It’s fast, but there are alternatives that are literally 5x faster. This article is just another rewrite of a basic introduction. It’s not a guide, since it does mot describe any issues that you face in practice. reply thrdbndndn 15 hours agoparentBeautiful Soup comes with a \"html.parser\", and by default it doesn't not use or even install lxml. reply cmdlineluser 14 hours agoparentprevI'm sorry but BeautifulSoup is not just a wrapper over lxml. lxml even has a module for using beautifulsoup's parser. > lxml can make use of BeautifulSoup as a parser backend https://lxml.de/elementsoup.html > A very nice feature of BeautifulSoup is its excellent support for encoding detection which can provide better results for real-world HTML pages that do not (correctly) declare their encoding. reply antisthenes 12 hours agoparentprevParsing HTML super-fast is very low on the list of priorities when web-scraping things. Yes, in practice. Most of the time it won't even register on the scale, compared to the time spent sending/receiving requests and data. reply konexis 14 hours agoprevGood luck bypassing akamai reply sakisv 12 hours agoparentThe way I'm bypassing it is by using tailscale to route the scraper's traffic through my home connection and take advantage my ISP's CGNAT. Works like a charm. reply hubraumhugo 17 hours agoprev [–] I got so annoyed by this kind of tedious web scraping work (maintenance, proxies, etc.) that I'm now trying to fully automate it with LLMs. AI should automate repetitive and un-creative work, and web scraping definitely fits this description. It's a boring but challenging problem. I've started using LLMs to generate web scrapers and data processing steps on the fly that adapt to website changes. Using an LLM for every data extraction, would be expensive and slow, but using LLMs to generate the scraper code and subsequently adapt it to website modifications is highly efficient. The service is using many small AI agents that basically just pick the right strategy for a specific sub-task in our workflows. In our case, an agent is a medium-sized LLM prompt that has a) context and b) a set of functions available to call. Tasks involve automatically deciding how to access a website (proxy, browser), naviage through pages, analyze network calls, and transform the data into the same structure. The main challenge: We quickly realized that doing this for a few data sources with low complexity is one thing, doing it for thousands of websites in a reliable, scalable, and cost-efficient way is a whole different beast. The integration of tightly constrained agents with traditional engineering methods effectively solved this issue. Feel free to give it a try: https://www.kadoa.com/add reply nico 16 hours agoparentKadoa looks great. For tool discovery/usage, are you using LangChain or something else? Also, do you support scraping private sites, ie. sites that require a login/password to access the data to scrape? Thank you! reply hubraumhugo 13 hours agorootparentWe found LangChain and other agentic frameworks to have too much overhead, so we built our own tailored orchestration layer. Authenticated scraping is currently in beta, could you email me your use case (see my profile)? reply kalev 13 hours agorootparentprev+1 on the question about scraping behind authentication. One huge use case we have as an ecommerce store is to crawl data from our vendors, which do not have (or incomplete) export files reply thomasisaac 13 hours agoparentprevIncredible product, will give it a spin soon. How do you do under volume? I tried it out with Google but it was quite slow. reply naiv 16 hours agoparentprev [–] Minimum extraction cost 100 credits , so only 250 pages could be parsed with the regular plan? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Learn how to create web crawlers using Python libraries like BeautifulSoup, addressing common scraping obstacles and following best practices for extensive scraping.",
      "Discover techniques for extracting data from HTML files, leveraging CSS selectors, downloading images, and managing dynamic content to enhance your scraping capabilities.",
      "Implement strategies like crawl delays, proxies, rotating user agents, and simulating natural human behavior to prevent blocks while scraping websites responsibly, along with utilizing tools such as Selenium for web automation."
    ],
    "commentSummary": [
      "The discussion encompasses optimizing web scraping workflows in Python, emphasizing the importance of separating crawling and scraping steps, and utilizing caching for efficiency.",
      "Debates on ETL vs. ELT processes in data warehousing stress the significance of testing, flexibility, and data integrity, with a focus on tools like Playwright and challenges posed by Cloudflare or AWS WAF.",
      "Comparison of programming languages for web scraping favors Python over Node.js, addressing common obstacles like 403/401 errors, captchas, and paywalls, suggesting solutions such as browser agents, captcha-solving services, and proxy management."
    ],
    "points": 352,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1708442207
  },
  {
    "id": 39442429,
    "title": "Microsoft Detects Hackers Using Its AI Tools for Cyberespionage",
    "originLink": "https://www.schneier.com/blog/archives/2024/02/microsoft-is-spying-on-users-of-its-ai-tools.html",
    "originBody": "Microsoft Is Spying on Users of Its AI Tools Microsoft announced that it caught Chinese, Russian, and Iranian hackers using its AI tools—presumably coding tools—to improve their hacking abilities. From their report: In collaboration with OpenAI, we are sharing threat intelligence showing detected state affiliated adversaries—tracked as Forest Blizzard, Emerald Sleet, Crimson Sandstorm, Charcoal Typhoon, and Salmon Typhoon—using LLMs to augment cyberoperations. The only way Microsoft or OpenAI would know this would be to spy on chatbot sessions. I’m sure the terms of service—if I bothered to read them—gives them that permission. And of course it’s no surprise that Microsoft and OpenAI (and, presumably, everyone else) are spying on our usage of AI, but this confirms it. Tags: artificial intelligence, cyberespionage, espionage, Microsoft Posted on February 20, 2024 at 7:02 AM • 15 Comments Two clicks for more privacy: The Facebook Like button will be enabled once you click here. No data is loaded from Facebook until you enable the button. Click the [i] button for more information. not connected to Facebook Two clicks for more privacy: The Tweet button will be enabled once you click here. No data is loaded from Twitter until you enable the button. Click the [i] button for more information. not connected to Twitter If you click to activate the share buttons, data will be loaded from a third party, allowing them to track your visit to schneier.com. For more details click the [i] button.",
    "commentLink": "https://news.ycombinator.com/item?id=39442429",
    "commentBody": "Microsoft is spying on users of its AI tools (schneier.com)347 points by mikece 18 hours agohidepastfavorite140 comments surrTurr 18 hours agoIf you call this \"spying\", you should read the TOS of the OpenAI API again. If you, as a \"hacker\", still use their API, it's your own fault: > OpenAI may securely retain API inputs and outputs for up to 30 days to provide the services and to identify abuse. After 30 days, API inputs and outputs are removed from our systems, unless we are legally required to retain them. You can also request zero data retention (ZDR) for eligible endpoints if you have a qualifying use-case. For details on data handling, visit our Platform Docs page. Source: https://openai.com/enterprise-privacy reply blueblimp 16 hours agoparentIt's still an exceptionally poor privacy policy compared to pre-LLM online services. Compare with the Google Docs privacy policy, for example: https://support.google.com/docs/answer/10381817?hl=en > Google respects your privacy. We access your private content only when we have your permission or are required to by law. I think it is reasonable to expect the same from LLM API providers. The fact that they all currently do mass surveillance on users is bad. reply blackoil 16 hours agorootparent> only when we have your permission or are required to by law That can be very vague. does TOS gave them the permission? Does law can eb interpreted to require them to check for CSAM and pirated material? What about access by Iran or North Korea agents? If you are pushing things to cloud, these questions will be asked. reply judge2020 15 hours agorootparentAnyone who cares will heed this language and not use them in the first place. For everyone else, Google's reputation as a \"secure\" place to host your proprietary files, docs, and emails will be flushed away if they cooperate with (foreign, mostly) law enforcement or have such a glaring security vulnerability that allows an org's files to leak from Google servers. reply binarymax 17 hours agoparentprevAzure's TOS are different. You can also opt-out of data retention: https://learn.microsoft.com/en-us/legal/cognitive-services/o... reply blackoil 17 hours agorootparentIt also lists \"30-days retention window\" for Asynchronous Abuse Monitoring. reply binarymax 17 hours agorootparentYes, but you can opt-out of the data retention and abuse monitoring if you go through a vetting process and are deemed low risk. reply ftkftk 17 hours agorootparentThat is correct. The process is manual and definitely has humans in the loop. It is also required for every individual subscription in your account. reply Spooky23 16 hours agorootparentIt’s also not failure evident. I’ve managed a very large tenant with a number of carve out T&C. I’m aware of at least a dozen times that settings reverted during upgrades or migrations. O365 is a huge service with a lot of humans and duct tape holding things together. Unless you’re auditing them, you have no idea. reply bugbuddy 16 hours agorootparentprevThat’s the catch every little thing is a subscription on their platform. reply gpjanik 17 hours agorootparentprevI'd assume Azure's TOS does not allow foreign secret services to use it for its operations. I remember TOS of iTunes mentioned you can't use it to build nuclear bombs. In that sense, there's no way that what Microsoft did was illegal. reply infecto 17 hours agoparentprevThese are not all that unique and pretty common across all cloud services, GCP, AWS, Azure included. Especially in the spectrum of their ML/AI services. I don't think these TOS are out of this world. reply gpjanik 17 hours agoprevI love how the user made \"Microsoft is spying on users of its AI tools\" from \"Microsoft found out that hostile governments of authoritarian countries use their AI tools to conduct illegal activities\" and forgot that you legit have to click \"Yes, I agree\" under the terms and conditions that state they indeed do track all this stuff. Next up, \"Major television broadcasters spy on soccer players during Champions League games\". reply JeremyNT 6 hours agoparentJust because it's obvious to us doesn't mean it's obvious to everybody. The alarmist headline is warranted, and the fact that it's common doesn't make it acceptable. I welcome more awareness of this kind of lopsided ToS that enables such user hostile actions. reply probably_satan 17 hours agoparentprevNot to mention Microsoft intentionally built their AI lab in China haha. Bananas. reply bastardoperator 14 hours agorootparentYou're referring to an office/lab they built in 1998, lol. Pretty sure they bought in on this latest round... reply hef19898 17 hours agoparentprevCan I use those tools without clicking on \"Yes\"? reply bilekas 17 hours agorootparentCan you drive your own car without a key ? You don't have a 'right' to use the services. So the answer to your question is \"It depends if the service lets you, so 9/10 cases, no. Rightly so\" Edit : I can see a lot of people don't like this. But like it or not, right or wrong, that is how it is. Expecting anything else is just fanciful. reply arp242 17 hours agorootparentInformed consent is key. Do they clearly inform you of this when using these tools or is it hidden in a dense Privacy Policy somewhere with language so vague and woolly most people hardly understand it, which you \"accepted\" when you first installed Windows? I don't have a Windows machine and didn't check, but I'm guessing it's a lot closer to the second than the first. In principle I don't think anything is \"wrong\" with keeping a record of AI chat sessions, but you do need to know about it so you can modify your behaviour as you see fit (i.e. ask different questions). If there's a camera pointed at you then some might decide that the nose is best left unpicked and those balls will have to remain unscratched. This is really the problem with a lot of these things: not that they're doing these things as such, but that it's so hidden and obfuscated that you need to spend an hour or more trying to understand it, and even then you only know what they \"may\" do, not what they \"actually\" do – a lot of these privacy policies are so vague and full of qualified language that they \"may\" do almost anything. And even in recording conversations there's nuance. Is this recorded with your full account detail, or is that information stripped? Are all conversations recorded or only some? Etc. etc. reply atq2119 15 hours agorootparentAnother important ingredient of informed consent is that there are realistic, viable alternatives. Those alternatives may have downsides, but if those downsides are so large as to become crippling, they stop being realistic and viable. That is often a crucial issue in these discussions around big tech, enshittification, and so on. reply arp242 15 hours agorootparentYes, it depends on the specifics though. \"Simply don't use any AI tools\" is a viable alternative, IMHO. But \"simply don't use Windows\" is already a lot more tricky, because so much of the world runs on it. You and I can get away with just running Linux, but we're not normal people in this regard. reply atq2119 12 hours agorootparentAgreed, though I'd add that it's quite likely that \"simply don't use AI tools\" will become less and less viable relatively quickly. reply arp242 11 hours agorootparentMeh; I think it's the reverse and that more and more people will find that automating these type of tasks is not actually a good thing because unlike automating manual labour, there is value in doing the work yourself, as that is the only way to have a profound and deep understanding of things. reply atq2119 6 hours agorootparentI think both can be true. By analogy, consider mental arithmetic vs. electronic calculators and spreadsheets. Being able to do mental arithmetic is an important skill, arguably the only way to have a profound and deep understanding of many things. At the same time, calculators and spreadsheets allow an individual to do things that just would be totally out of reach without these tools. Some people argue that generative AI has already reached this point, and I think that's plausible in a few narrow domains. It's also plausible that the domains to which it applies keep broadening. reply Geisterde 15 hours agorootparentprev>Informed consent is key sounds nice reply arp242 15 hours agorootparentIf you want to say something then say it instead of leaving weird cryptic comments. Otherwise don't post anything. reply Geisterde 11 hours agorootparentMy appologies. I think informed consent is a wonderful idea, I just am skeptical that it exists in real life. I would say if you use an electronic device today, you are being spied on (with very narrow exceptions). Informed consent to me (take the example of a phone or many apps), based on the level I have to break it down to people for them to understand, is something like \"we will see you whacking it to hentai, we will know when you take a dump, we have enough of your data to derive private information you never gave us, we have all the data necessary to totally ruin your life socially and economically, and it will all be leaked to criminals that will profit from your demise, dont worry though, we will give you a 1 year sub to mcafee antivirus as a consolation prize.\" I think we can at least agree that ^^that happens to people. So, whos going to be the first company to not veat around the bush and just admit that reality? That, is why I say informed consent sounds nice, every company that can keep a skilled contract lawyer on payroll has an incentive to downplay their spying. I dont know in what version of reality informed consent could exist, because the incentive is fairly obvious. reply barrysteve 14 hours agorootparentprevThere's nothing cryptic about his post. He'a referring to an idea that sounds nice in theory and doesn't work in practice. It's a common English phrase. And the phrase works well, the concept of informed concept sounds good in theory and never works in practice. Anyone who has observed tech companies for decades knows they don't ask for your consent before they make any consequential changes. Tech companies do whatever they want until they get sued or regulated. It's always been this way. reply Nullabillity 12 hours agorootparentRead arp's comment again. The \"consent\" under those premises isn't valid, because it was neither informed nor freely given. reply int_19h 16 hours agorootparentprevYou don't. But it means that the title of the story is correct as worded: if you do use those tools, you are getting spied on. The fact that you're warned about that somewhere in the TOS doesn't change the fact, and the story is basically correct. reply educaysean 16 hours agorootparentI think the word \"spy\" is problematic here because one might assume that the subject being spied on must be unaware of the fact, otherwise you no longer qualify as \"being spied on\". reply Geisterde 15 hours agorootparentThe government doesnt spy on its cotizens, because the citizens know they are being spied on? reply anp 15 hours agorootparentI hate that states do this, but yeah it’s called “surveillance” when legitimized by norms, statute, and precedent. reply pbhjpbhj 14 hours agorootparentIt's called surveillance _to_ legitimise it. reply OkayPhysicist 14 hours agorootparentprevThat's actually a fantastic example of the difference between actually owning something and not: I am absolutely allowed to modify my car in any way I please, including installing a ignition bypass. reply r721 17 hours agoparentprevNot even a random \"user\", but Bruce Schneier! reply tuwtuwtuwtuw 17 hours agorootparentWho also helpfully linked to his pages on Facebook and Twitter. I'm sure those never \"spy\" on their users. reply mixmastamyk 11 hours agorootparent”Yet you participate in society. I am very intelligent.”. ;-) reply nonrandomstring 17 hours agoparentprev> Next up, \"Major television broadcasters spy on soccer players during Champions League games\". I strongly agree with you that this story is a nothingburger. But as I note your reaction, I take a different interpretation. The implication that statements like \"Microsoft spies on X\" are truisms, is the news. We obviously all now accept as a given that these are tautologies. In other words \"Spies spy on X\" -> (is to say) \"Microsoft = spies\", not just in this sentence/context, but in all contexts. Terms, conditions, legality, illegality, hostility or friendliness all recede into irrelevance hereafter. reply Urgo 16 hours agoprevGoogle Bard.. Gemini makes it explicitly clear that they do this right from the homepage: \"Your conversations are processed by human reviewers to improve the technologies powering Gemini Apps. Don’t enter anything you wouldn’t want reviewed or used.\" \"How conversations improve Gemini Just by having a conversation with Gemini, you’re making Google services better, including the machine-learning models that power Gemini. As part of that improvement, trained reviewers need to process your conversations. So when using Gemini, don’t enter anything you wouldn’t want a reviewer to see or Google to use. Your Google Workspace content, like from Gmail or Drive, is not reviewed or used to improve Gemini. You can turn Gemini Apps Activity off If you don’t want future conversations reviewed or used to improve machine-learning models, turn off Gemini Apps Activity\" reply ilc 15 hours agoparentI'd love to get some clarity on this from Google on if they actually support not getting spied on or not? It is only kinda 1/2 clear here. reply VyseofArcadia 17 hours agoprevCould be shortened to just \"Microsoft Is Spying on Users\". If you use a modern MS product, you are sending them lots of data, full stop. reply berniedurfee 12 hours agoparentOr more generally, “Everyone is spying on users, that’s how the internet works.” reply Zuiii 6 hours agorootparentMore like \"Most Big-Tech and VC-funded companies spy on users. That's how exploiting customers for profit works.\" Many of the privacy apps and services I use explicitly set out to collect as little information as possible. It's sad that people have been conditioned to think spying is appropriate or that it has to be this way for the internet to \"work\". reply codeptualize 18 hours agoprevYes.. they are pretty clear that they monitor for abuse. 30 days. Not sure how this is a surprise. reply wredue 17 hours agoparentAs nothingburgers go, this is pretty tame and expected. You use a platform that entirely runs on a hosted back end, you’re going to have everything logged and saved. Compare that to, say, Nvidia, who sends every single titlebar value, as well as every single click you make to their servers (this data sends even if you opt out, as it is classified as “necessary”). Probably the most invasive spyware you have installed today. reply ambichook 11 hours agorootparentjfc, is this from geforce experience? if so i knew it was bad but this is something else entirely reply Zelphyr 18 hours agoprevI don't trust Microsoft as far as I can throw them collectively but it doesn't come as a surprise to me that they are doing this any more than it would if I were to hear that OpenAI, Google, etc... are engaging in this behavior. That's not to excuse it, however. I think it's already time they all implemented a privacy stance where they are only allowed to view chat conversations if the user gives them explicit and revokable permission. reply capital_guy 17 hours agoprevAnyone using these tools is asking for all their data to be hoovered up. Locally powered AI is the only way to use it if you care about this kind of thing. reply lenerdenator 15 hours agoprevDuh? AI is built on data. The more you spy, the more data you get to train AI, the more you train AI, the better/more valuable it gets and the more people want to use it to give you data. It's a cycle. reply bearjaws 17 hours agoprevEZ rage bait. Everyone of these platforms is scraping your inputs to train their next model. How is it not obvious they would also detect abuse? reply 1vuio0pswjnm7 10 hours agoprev\"I'm sure the terms of service-if I bothered to read them-gives them that permission. And of course it's no surprise that Microsoft and OpenAI (and, presumably, everyone else) are spying on our usage of AI, but this confirms it.\" Not sure why anyone would think that terms that allow surveillance are not functionally equivalent to evidence of surveillance. But it seems like this thinking is what lets people trust the aptly-named \"privacy policies\" used by so-called \"tech\" companies to make users more comfortable with surveillance. If a so-called \"tech\" company makes statements in a \"privacy policy\", something like, \"We do not do X\", and there is no way to verify whether they do X or not,^1 what purpose does the privacy policy serve. If in fact someone acquires evidence that the company is doing X, it is already too late. X was not prevented. Whereas, if a so-called \"tech\" company includes provisions in its terms of service that allow it to do X, then what functional difference does it make whether the company does X or not. They could do X at any time. Anyone who agrees to the terms has given them permission to do X. In general, companies do not ask for the explicit right to do X in an agreement if there is zero possibility they will do X. Why do these so-called \"tech\" companies need a separate document, a \"privacy policy\", to make users believe the company will not do X. Why not include a prohibition against X in the terms of service. To understand what the company does, read the terms of service. If the terms allow the company to do X, then assume the company does X. It is foolish to assume otherwise. 1. If there was a way to verify it, then the company would not need to make such assurances. reply _sword 17 hours agoprevThe White House’s AI regulatory order requires big cloud providers to monitor for and report foreign clients (ie Chinese) for using their services to train AI fyi. Monitoring for malicious activity isn’t nearly as invasive as what the government is requiring reply nonethewiser 16 hours agoparentSounds like the government is requiring monitoring of malicious activity. reply mistrial9 16 hours agoparentprevoddly, top ranking AI research in the world right now comes from Stanford USA+Wuhan PRC .. this is trivially verified. Intelligent commentary will need to differentiate between public cooperation layers, non-public cooperation layers, and non-public adversarial layers .. because, they are all active.. all of those. reply _sword 14 hours agorootparentThe White House doesn’t differentiate! If you’re foreign and training bigger models on US cloud infra, you’re reported https://www.whitehouse.gov/briefing-room/statements-releases... reply skybrian 15 hours agoprevOpenAI is welcome to train on the questions I’ve been asking about how to do type-level programming in TypeScript. In other circumstances, I might ask such questions in a public forum like StackOverflow, or a discussion forum on GitHub, but ChatGPT is more convenient. I suppose other people might ask more private questions, but I have trouble coming up with examples of private things that I’d want to ask an AI chatbot about. And there are plenty of things to ask about that aren’t personal. reply copperx 14 hours agoparentThat sounds like the old \"I don't do bad things therefore I don't need privacy\" pro-spying argument. reply skybrian 12 hours agorootparentI do need privacy sometimes, but not for this. Others may have different use cases. This is true of everyone. Do you need privacy all the time? Presumably you're okay with your Hacker News comments being public. reply kmeisthax 17 hours agoprevThe whole excuse OpenAI gave for going proprietary was specifically that they could spy on you and use that to reactively adjust GPT's alignment training. For what it's worth, the US military is also evaluating LLMs in the same way the Chinese and Russian hackers are. I imagine - or at least hope - they aren't using OpenAI's hosted ChatGPT and are getting copies of models they can run on SIPRNET or JWICS. reply living_room_pc 16 hours agoprevI always assumed so, and I never feed it anything sensitive. reply autoexec 16 hours agoparentI assume it's true for anyone offering AI as a service. Anytime you hand your data over to someone else they will use it for whatever benefits them. No one should be handing them sensitive data. reply lm28469 16 hours agoprevWhat's new? The vast majority of online companies are glorified data miners reply cooper_ganglia 17 hours agoprev> I’m sure the terms of service—if I bothered to read them—gives them that permission. I can’t tell if this article is parody or not, and that’s concerning, haha reply sandworm101 18 hours agoprev>> The only way Microsoft or OpenAI would know this would be to spy on chatbot sessions. I’m sure the terms of service—if I bothered to read them—gives them that permission. And of course it’s no surprise that Microsoft and OpenAI (and, presumably, everyone else) are spying on our usage of AI, but this confirms it. What? The chatbot is not your therapist. The chatbot is Microsoft. They are the same entity. Everything shared with the chatbot is shared with microsoft, just as every google search is shared with google. Related legal determination re chatbots: https://www.washingtonpost.com/travel/2024/02/18/air-canada-... reply BandButcher 17 hours agoprevImo bad story piece to cover up the recent Microsoft hacks done by \"state actors\". This has nothing to do with MS spying on user ai tool usage. Admit it MS, you got hacked, service accounts and executive emails were leaked, along with who knows what else, and you \"caught them\" after damage was done. reply orev 16 hours agoprevI don’t think this is a surprise to anyone here (I just assume anything done on any of these services is being “used to improve it” i.e. “spying”). But if Schneier is talking about it, maybe it will get more attention. reply kelnos 16 hours agoprevThe title/headline here is a bit flamebaity. Of course Microsoft is going to retain copies of AI chatbot sessions for a bit, and look for signs of abuse. Even not having read their ToS, it's pretty safe to assume that's going to happen. I just don't see the harm. There are plenty of examples of privacy overreach on the internet, and Microsoft is guilty of some of them, but this particular thing seems fine to me. And in this case, I think MS is doing the right thing! They shouldn't allow people to use these tools if they're going to use them to help them build malware. Certainly this sort of abuse-tracking should be narrow in scope, but it seems like that's exactly what it is, at least in this instance. If you don't want a third-party reading your interactions with an LLM, build/train your own. Eventually we may decide (legally, hopefully) that our interactions with AI tools should have strong privacy protections, but that has not happened yet, and companies are going to apply their usual privacy rules to these things: that is, not much privacy granted, at all. I generally have a lot of respect for Schneier, but this sort of breathless, sensational \"reporting\" isn't doing privacy advocacy any favors. reply bastard_op 16 hours agoprevDo they have a product that does NOT spy on users at all? Pretty much everything they do is either cloud connected inherently giving them visibility into what you do, or they feed constant telemetry back of what you do in one form or another in the os, office apps, whatever you happen to use they make, on whatever platform you use it on, including non-windows systems. Really do you expect anything more of Microsoft at this point in history? reply nox101 17 hours agoprevCurious what the difference is between that and a search engine. reply 23B1 18 hours agoprevYou should know that if you're interacting with an AI tool through various social channels – facebook, twitter, instagram, texting – those chats are not only being actively read by the employees of that company, but anyone who is responsible for the 'marketing' of that company, e.g. outside vendors. Certainly these firms are bound by contract, but the stuff I've seen people ask these chatbots – health information, sexual information, highly personal stuff you might only share with your shrink... well, it's being read by the 22 year old marketing intern as well. Unencrypted, in the clear, copy & paste'able, and attributable to your username/ID reply bilekas 17 hours agoprevI do not like M$' behavior in general but this is almost certainly not spying. The OP seems to think that because the users region was recorded its spying on sessions. With a self righteous \"I didn't read the TOS but This just confirms it\". Microsoft would obviously be recording region metrics, maybe even keyword/request trends. None of this information is identifiable and makes complete sense to be monitoring for improvement on a new product area. reply api 18 hours agoprevI've seen numerous stories recently that boil down to people being surprised that cloud hosted products \"spy\" on you. If you're sending the data to someone else's computer and it's not encrypted, you're.... sending the data to someone else's computer. Do people not understand this? reply __MatrixMan__ 17 hours agoparentMy wife teaches English. She uses Google docs revision history to spy on her students writing (and occasionally to recover it in the event of a disaster). So if it's plagiarized, she can see the moment where they pasted the plagiarized version. They apparently have no idea. So yeah, I think that people legit don't understand. The gap between tech literate and hacker is problematically large. reply api 17 hours agorootparentI increasingly feel like a member of some kind of high priest or scribe class in a feudal society, and I don't like it. I don't like where this is going. It'll either end in a dark cyberpunk total surveillance and mass manipulation state or a kind of Butlerian Jihad where people reject networked cybernetic systems en masse. Maybe the first happens followed by the second. reply __MatrixMan__ 17 hours agorootparentAgreed. I think we need to be a bit more conscious of whose agenda we're willing to further with our work. I'd rather retire of unremarkable means in a functioning society than be wealthy in a dystopia of my own making. reply atum47 16 hours agoprevNot to be an as*ole but is anyone surprised by this? reply interludead 16 hours agoprevThere is no such a thing as privacy nowadays reply jsnell 18 hours agoprevOriginal discussion: https://news.ycombinator.com/item?id=39368859 reply RIMR 14 hours agoprevFrom the comments: >OpenAI are “spying” on ChatGPT sessions the same way that email providers “spy” on your email to filter spam. This sounds like a variation on “Google is reading your email”. Yeah, but Google is actually reading your emails. They build a profile of everything you buy from emailed receipts, and use that data to target ads at you. It's definitely not about improving the user experience, like a Spam filter accomplishes... reply add-sub-mul-div 17 hours agoprevWhy waste time on performative outrage about how services that require a login have our data and use it in ways we'll never know? You have to accept it as a given and move on in order to start asking the right questions. Like, are we letting ourselves be herded towards a future where it's normalized that all search must be done by authenticated users, and \"classic search\" is effectively deprecated ten years from now? reply racl101 15 hours agoprevSSSHHHHHHHHHHHHHHOCKER. reply wut-wut 11 hours agoprevWell Duh. reply drivingmenuts 13 hours agoprevIf it’s running on their serve and it’s part of the license, why shouldn’t they? A real competitor or wanna-be should have the sense not to do that, so it seems justified that they would keep an eye for how their product is used. reply tzm 14 hours agoprevspying === user engagement? reply dev1ycan 15 hours agoprevMicrosoft is very literally an arm of the US government, I think people should already realize that. Google is terrible, but Google refused (they say so we don't know how true this is), to help the US government on spying, and Microsoft stepped in. Bill Gates gets to sit with Xi Jinping and get called a \"friend\" and MS has exceptions in China unlike other companies. Microsoft wants to buy ActivisionBlizzard, gets brought to the senate on the antitrust case, and the senators start barking against Sony instead, lol. Microsoft is very literally all the US accusses Chinese companies of being for the Chinese government, now, so far it doesn't seem like MS has done that much with our data, but who knows what the future holds. reply coldtea 15 hours agoparent>Google is terrible, but Google refused Isn't \"we refused\" what they would have said they did even if they haven't refused? reply dev1ycan 14 hours agorootparentYes, I know, that's why I stated I can't confirm if that's really the truth. Again I'll cite the whole ordeal with Apple claiming they are the privacy company and how they \"refused\" the government access to a persons iphone back in the day as an example, I wonder how they refused when the patriot act is a thing? they refused the government? lol reply canadiantim 15 hours agoprevHow much do people trust that VSCode isn't spyware? reply throwaway22032 16 hours agoprevWhen you send data to someone else they have that data, act accordingly. Anything else is just stupid. Didn't we all learn on the playground that a secret isn't secret any more once you tell a friend? reply numair 17 hours agoprevWhile we are on this topic — can someone, anyone, tell me how to disable the AI autocompletion suggestions in Outlook for iOS? Getting ready to dump Microsoft and Exchange to get away from that utter mess. Perhaps some of the people who are commenting here don’t realize that there isn’t much of a way to opt out of these data “analysis” activities. reply sammyteee 18 hours agoprevIn other news, water is wet. All joking aside, and even as the article points out re: the T+Cs, is anybody surprised? reply michaelt 18 hours agoparentMany cloud providers maintain the pretence that their employees don't have access to customer data. For example, they like to maintain that if you run an amazon competitor hosted on AWS, amazon insiders can't look at sales numbers in your hosted database. Or if you run a google competitor with e-mail hosted on gmail, that google insiders can't view your e-mails. And so on. Personally I've always found these promises rather hard to believe, but a lot of people have a great deal of trust in them. reply nullindividual 16 hours agorootparentI worked for a large cloud provider and that is correct, employees did not have access to customer data. The encryption key in this product is split between a SQL database and the front end servers with blob data being stored outside of SQL (customer data). There was no way for a single employee to get access to both portions of the key to decrypt customer data. Using AWS when you're a competitor of Amazon seems like poor business decision making. reply marcosdumay 15 hours agorootparentWho isn't a competitor of Amazon? Or, better, who isn't a competitor and is also not at risk of becoming one? reply weare138 15 hours agoprevIn collaboration with OpenAI, we are sharing threat intelligence showing detected state affiliated adversaries—tracked as Forest Blizzard, Emerald Sleet, Crimson Sandstorm, Charcoal Typhoon, and Salmon Typhoon Does anyone else think this new APT naming scheme is terrible or is it just me? reply gigel82 16 hours agoprevI mean, probably. But there's no smoking gun here, just wild speculation. Someone should dig deeper and ask Microsoft / OpenAI to disclose exactly how they obtained the \"threat intelligence\". Here's the full report for reference, but they don't answer how the data was collected or correlated to the specific \"threat actors\": https://www.microsoft.com/en-us/security/blog/2024/02/14/sta... reply fnordpiglet 17 hours agoprevA more precise yet general headline: “Microsoft is spying on users” It’s nearly impossible to install their products without opting into layers of spyware and adware cloaked in dark patterns and requiring extraordinary measures to back out of hidden spyware installs done without permission. Microsoft has always been a fairly malignant business, built in shenanigans of various sorts, and as the market perfects new shenanigan vectors they’re right there pushing the envelope. reply giancarlostoro 15 hours agoparentI've posted it before, but when I saw that Microsoft Defender sends your files to be inspected, and has no audit history of what goes from your computer to their servers, I installed Linux. reply jxramos 13 hours agorootparentDo you have a blogpost or something of the like to read up on the analysis backing that conclusion. I’m very curious here. reply tredre3 13 hours agorootparentIn Windows' \"Virus and threat protection\" settings there is the following checkbox (defaults to enabled): Automatic sample submission Send sample files to Microsoft to help protect you and others from potential threats. We'll prompt you if the file we need is likely to contain personal information. Regarding that last sentence, it is not documented what they consider \"likely to contain personal information\". reply Vetch 16 hours agoparentprevIn this specific instance, I think it is necessary to make a distinction because given the direction things are headed, spied upon is tautological in the context of AI. AI-as-a-service requires sending private details to gain utility and there is a real risk this is the only kind of AI that will be allowed to exist. GPT4 was most probably used given they stated this action was taken in collaboration with OpenAI and so the emphasis on Microsoft was likely used as engagement bait by the blog author. In truth, Microsoft probably put this out to advertise how they take steps to keep AI safe. Unfortunately, this will also be ammunition for those who seek to ban opensource AI, you can only do this kind of thing with Monitored AIs. Even if you're not an adversarial government leaning on it as a hacking enhancement, it's only a matter of time till governments worldwide demand certain controversial conversational topics be reported to law enforcement. I suspect the Knight Paladins of Anthropic would be more than eager to further this greater cause for the Safety of mankind. From the start, since all but a few services are using your data to improve their models there is already no notion of privacy. The ethical ones will eventually (if not already) report suspicious activity (as determined by law) to law enforcement and the less ethical ones will report all activity to advertising and insurance agencies. Some already lobby to ban opensource AI because enhancing the learning rate and reducing the friction of gaining new information for humanity without controlling oversight will also enhance hackers or other bad actors ability to access sanctioned knowledge. They consider this a heresy and deem humanity at large incapable of responsibly handling such increases in cognitive ability. Only a few Adept can be trusted with administering AI. Truly, spying is among the more trivial concerns for the future of computing, given AI's compute heaviness and the amount of centralizing control it engenders by default. reply miohtama 15 hours agorootparent> In truth, Microsoft probably put this out to advertise how they take steps to keep AI safe. Unfortunately, this will also be ammunition for those who seek to ban opensource AI, you can only do this kind of thing with Monitored AIs. This is exactly how the OpenAI blog post reads between the lines: “We work with the government to make the world safe. Or else.” https://openai.com/blog/disrupting-malicious-uses-of-ai-by-s... The actual “bad” tasks “bad guys” performed were no different that what one can accomplish with Google searches. But no one talks about stopping terrorists to use Google. reply sonicanatidae 15 hours agoparentprevHow generous of you to assume that one could opt-out of all of it, regardless of the pattern. reply m463 13 hours agoparentprevThe darkest pattern is that employers use microsoft software as your tools for employment. This seems to violate all kinds of boundaries between your personal and professional life. Most people as they mature learn how important boundaries are to health and well-being, as a child or teen growing up, as a partner, as a parent, and with friends and community. People don't need this mess too. There is no clear way to opt out of this, to create and maintain a boundary and to say no. reply vosper 16 hours agoparentprevWhat are some examples outside of the OpenAI one in Schneier's blog post? reply illusive4080 16 hours agorootparentWindows 11 reply vosper 14 hours agorootparentWhat are the “layers of spyware and adware cloaked in dark patterns and requiring extraordinary measures to back out of hidden spyware installs done without permission” in Windows 11? reply fnordpiglet 11 hours agorootparentThis does a fairly detailed analysis of the various vectors and wraps up patching in an enormous script plus related tools. https://simeononsecurity.com/github/optimizing-and-hardening... There’s also a lot of extensive details of the dark patterns in the installers where no means yes with the privacy preserving option hidden behind rolled up options etc. Even then you need to do other patches to avoid telemetry and data collection. Even then you end up with ads surfaced in the start menu, etc. reply jwitthuhn 13 hours agorootparentprevEverything you type in the start menu gets sent to microsoft by default and this can only be changed with Group Policy, so anyone on home edition is stuck with that. reply vosper 12 hours agorootparentThanks for giving me an answer, all the other posts in this chain just seem to be people naming Microsoft products (I get it, people love bashing Microsoft...) reply CoastalCoder 16 hours agorootparentprevAnd Windows 10, IIRC. reply fnordpiglet 16 hours agorootparentprevOffice 365 reply Zenul_Abidin 16 hours agorootparentOutlook reply throwanem 16 hours agorootparentVS Code. reply enopod_ 16 hours agorootparentEdge reply fnordpiglet 15 hours agorootparentXBox reply rubicon33 14 hours agorootparentNotepad reply chihuahua 7 hours agorootparentdonkey.bas https://en.wikipedia.org/wiki/DONKEY.BAS reply iAMkenough 8 hours agorootparentprevPaint reply sleepybrett 15 hours agorootparentprevgithub, azure... reply vosper 14 hours agorootparentprevWhat are the “layers of spyware and adware cloaked in dark patterns and requiring extraordinary measures to back out of hidden spyware installs done without permission” in Office 365? reply fnordpiglet 14 hours agorootparentSpecifically you can’t back out for office 365. That part was windows 11. The rest of their products offer no option to bypass the spyware and adware. reply 2OEH8eoCRo0 18 hours agoprevNo shit. The internet is a network of other people's computers. Was the internet designed to be private? reply bilvar 17 hours agoparentThe neighbourhood is a network of other people's homes. Let's allow everybody in ours then. reply 2OEH8eoCRo0 17 hours agorootparentWhat does the public street represent in your analogy? reply probably_satan 17 hours agoprevWhy did Microsoft build their AI lab in China, then? Sounds like Microsoft sold the US citizens out to China and are now trying to play the victim card while adding Russia to the blame. Say what you want about conspiracy theories but China and Bill Gates seem to be afraid of women to the point where they have chosen to try to eradicate them from society. Maybe the rumors are true about Bill Gates' attempt to make little girls infertil via vaccination. reply DinoCoder99 18 hours agoprevHow do they determine access is state-affiliated? Secondly why is Microsoft kowtowing to jingoist bullshit? reply rhdunn 17 hours agoparentYou can geolocate a user/request based on the IP address. This is common in servers. It allows a server to know which country a request is coming from. I suspect that this is what OpenAI are doing, as Bing Translate and Google Translate do similar things. reply BlueTemplar 17 hours agoparentprev2nd : Because Microsoft is still part of PRISM ? https://en.wikipedia.org/wiki/PRISM P.S.: As a reminder, metadata is typically more informative than data. reply poszlem 18 hours agoparentprevThe illusion of a borderless world has faded. The comfortable notion of \"global citizenship\" was a fleeting artifact of American dominance. As new realities emerge, the vital concept of state security demands far greater vigilance than in decades past. This is not jingoism, it's a clear-eyed assessment of our world. In this new reality, we can no longer afford the naivete of sharing our most valuable assets with nations who oppose us. reply int_19h 16 hours agorootparentYet corporations still embrace this illusion when it comes to, say, operating in those nations. It's a borderless world for them, just not for us. reply DinoCoder99 7 hours agorootparentprevWhy are you conflating the concept of \"nation\" with that of a \"state\"? These are two words with different meanings. This just strikes me as meaningless friction to cause domestic arms production. reply 15457345234 15 hours agorootparentprevWarmongers will never cease to monger war, I guess. What you've written is massively jingoistic; yes certain forces seem oddly desperate to talk a war into existence - I guess all those solar panels coming out of CN are disrupting certain money flows on a permanent-looking basis - but the absence of any actual, idk, military buildup will be harder to explain away. Unless you just deepfake the war. Which is probably what will happen. reply cynicalsecurity 16 hours agoprevFinally Microsoft is doing some good things. Spying on Russian, Iranian etc spies is what you are supposed to do. reply bee_rider 17 hours agoprevI could see somebody being confused and not understanding that, say, a copilot tool in a local IDE, or some of their AI in the start menu stuff, is actually using online services. But it is pretty obvious that when you write something into a website that’s… going on the internet, right? reply phillipcarter 15 hours agoprevThis is a weird post. Yes, they monitor for abuse of their TOS. No surprise there. But also, this is really the only way to actually improve the underlying tech. How do you know patterns of abuse? Normal use? Weird but non-abusive use? By collecting data on that usage and feeding it back into development. reply yjftsjthsd-h 14 hours agoparent> But also, this is really the only way to actually improve the underlying tech. How do you know patterns of abuse? Normal use? Weird but non-abusive use? By collecting data on that usage and feeding it back into development. You can collect that data internally or let users opt in; spying on users is absolutely not the only way to improve. reply phillipcarter 12 hours agorootparentIt's not spying on users. And no, it's really the only way to do it effectively. When you make it opt in or internal only, you get far too much selection bias to make the product good. That's just how it works with this tech right now. reply notavalleyman 16 hours agoprev [–] There are real spies in this story, who are committing truly evil acts in the world, and using technology to further their aims of spreading malware and destroying international space conventions. And yet the poster finds Microsoft the bad guys in this story? This is a story about our international enemies using advanced western tech to develop malware, with which to attack us. It's right to be outraged, but not at Microsoft reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Chinese, Russian, and Iranian hackers are leveraging Microsoft's AI tools to enhance their hacking skills, raising concerns about potential privacy breaches.",
      "This discovery suggests that Microsoft and other AI firms might inadvertently be monitoring user activities with AI tools.",
      "The utilization of AI for cyberespionage poses a notable threat, prompting discussions on privacy issues and surveillance measures."
    ],
    "commentSummary": [
      "The focus is on the diverse privacy policies and data practices of tech giants such as Microsoft, OpenAI, and Google, highlighting worries about data handling, monitoring, and accessibility.",
      "The debate underscores the significance of obtaining informed consent, the impact of surveillance on AI training, and the possible abuse of user information.",
      "Users show doubt regarding privacy policies, advocating for transparency and discussing the ethical implications of data collection and surveillance in today's digital era."
    ],
    "points": 347,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1708442902
  },
  {
    "id": 39439655,
    "title": "Creating 3D Disney World Map with Rust and WebAssembly",
    "originLink": "https://mary.codes/blog/programming/translating_openstreetmaps_to_HTML5_canvas_rust_wasm/",
    "originBody": "Translating OpenStreetMap data to HTML5 Canvas with Rust and WebAssembly Mary Knize Feb 19th, 2024 13 min read Programming I'm working on a revamp of an old project of mine called Line Buddy (github). It used a now-deprecated API library called themeparks (github) and A-Frame to visually represent the wait times in the Disney World theme parks in 3D. The original project used OpenStreetMap screenshots as the base, with columns representing the wait times. (They're all zero now since this version of the API no longer works.) My plan is to use OpenStreetMap data to create a simplified version of the map. Eventually, I'll create the map in 3D. As a proof of concept, however, I'm going to draw to an HTML5 canvas first. I want to make sure that I'm able to get the data that I need, process it, and use it to create my own maps. TABLE OF CONTENTS Setting up the project Creating a new project with wasm-pack Building and testing WASM code Getting map coordinates Calling the Overpass API Writing the Rust code Drawing on the canvas Processing ways and relations Drawing all of the other map areas The finished map SETTING UP THE PROJECT I'm not going to use any sort of JavaScript library for this first experiment. I set up a basic HTML page inside of a directory, and I'm adding CSS and JavaScript directly to this page. A very simple setup because all I need the JavaScript to do is call the WASM code and draw to a canvas. OSM Mapsbody { margin: 0; padding: 0; height: 100%; width: 100%; }Most of the heavy lifting for the OSM data processing will be taken care of by Rust. While I haven't performed any benchmarks about whether Rust or JavaScript processing is faster, I decided to use Rust and WebAssembly because there will be a lot of coordinates to process, and Rust generally performs faster at large amounts of data processing. CREATING A NEW PROJECT WITH WASM-PACK I decided to use wasm-pack to create a Rust library that will then be used by my client-side JavaScript. While I already had wasm-pack installed, I re-ran the installer because my installed version of wasm-pack was extremely out of date. Creating a new wasm-pack project is easy. Instead of my normal workflow of using cargo new process-maps to create my new Rust project, I used wasm-pack new process-maps. This creates a new Rust project that is optimized for creating WASM files. I created this project inside the same directory as my HTML file. This is because I want to be able to easily call the WASM code from my HTML file. BUILDING AND TESTING WASM CODE When I open up the lib.rs file generated by the wasm-pack tool, I see that it already contains some minimal code that will display an alert whenever the greet() function is called from JavaScript. There's a line-by-line explanation of this file at https://rustwasm.github.io/docs/wasm-pack/tutorials/npm-browser-packages/template-deep-dive/src-lib-rs.html. mod utils; use wasm_bindgen::prelude::*; #[wasm_bindgen] extern \"C\" { fn alert(s: &str); } #[wasm_bindgen] pub fn greet() { alert(\"Hello, process-maps!\"); } I am going to compile this code and import it into my HTML file. wasm-pack build --target web --dev This creates a development build of my Rust code. Running the command without --dev will compile a production release, which is slower but optimized. I'm targeting \"web\" since I'm not using any sort of bundler or webpack in my project. Under the canvas tag in my HTML file, I'm going to import the WASM code and call the greet function.import init, { greet } from './process-maps/pkg/process_maps.js'; async function run() { await init(); greet(); } run(); When I open my HTML file in a browser, I see an alert that says \"Hello, process-maps!\" This means that my WASM code is working and I can start writing the code to process the OSM data. GETTING MAP COORDINATES Before I can write any Rust code, I need to get the coordinates for a location. In this case, I want to get the coordinates for the different Disney World parks. Because I want to pick a very specific area of the map, I will navigate to openstreetmap.org and search for the locations I want to map. Then, I'll click the \"Export\" button. In the Export sidebar, I'll click \"Manually select a different area\". Once I have the area cropped, I'm going to add the coordinates to my code. Since I'm going to be creating maps of all the Disney parks, I'm using query parameters to differentiate between which park map to draw. const queryString = window.location.search; const searchParams = new URLSearchParams(queryString); let park = searchParams.get('park'); let n, w, s, e; switch (park) { case 'epcot': n = 28.3768; w = -81.5553; s = 28.3661; e = -81.5425; break; case 'hollywood_studios': case 'hs': n = 28.3625; w = -81.5641; s = 28.3523; e = -81.5561; break; case 'animal_kingdom': case 'ak': n = 28.3692; w = -81.5984; s = 28.3524; e = -81.5831; break; case 'magic_kingdom': case 'mk': default: n = 28.42266; w = -81.58586; s = 28.41604; e = -81.57600; break; } CALLING THE OVERPASS API OpenStreetMap's public API is called the Overpass API. It has an interesting query language that I'm not totally sure I understand, but I was able to create some queries that return the data I need. Below, I have created query strings for all of the map items I need: walkways, gardens, water, and buildings. Below that, I'm fetching the data for walkways. // The bounding box is described with the south, west, north, and east coordinates, in that order. let bbox = `${s},${w},${n},${e}`; // Queries for each map type. let buildings = `way[building][!name];foreach{(._;>;);out;}`; let named_buildings = `way[building][name];foreach{(._;>;);out;}`; let walkways = `way[highway];foreach{(._;>;);out;}`; let trees = `node[natural=tree];foreach{(._;>;);out;}`; let gardens = `(way[leisure=garden];way[landuse=forest];way[landuse=meadow];);foreach{(._;>;);out;}`; let water = `relation[natural=water];foreach{(._;>;);out;}`; // Sets the timeout and bounding box. let query = `[timeout:90][bbox:${bbox}];`; let url = `https://overpass-api.de/api/interpreter?data=${query}`; function getWalkways(url) { fetch(`${url}${walkways};out;`).then(response => { return response.text(); }).then(data => { console.log(data); }); } The response in the console should look somewhat like this, a long XML document with coordinates. I'm going to send these coordinates to my Rust code, which will parse the XML file and return coordinates that I'll then be able to map to the canvas. WRITING THE RUST CODE To parse the XML file, I'm using a crate called osm-xml. It abstracts away the hard parts of parsing the OSM data. I tried doing it without this crate and trust me, it's very difficult. I'm adding osm-xml and js-sys to my Cargo.toml. The crate js-sys provides interop data structures for JavaScript. [dependencies] wasm-bindgen = \"0.2.84\" console_error_panic_hook = { version = \"0.1.7\", optional = true } osm-xml = \"0.6.2\" js-sys = \"0.3.67\" OSM data can be thought of as three different types. There's nodes, which are discrete coordinates. The query for trees searches for nodes. Ways are polygons that are created from nodes, so processing a way requires processing the information for the way, and then the nodes. Buildings, walkways, and gardens are ways. Finally, there are relations, which are made of multiple ways. Water is a relation because there can be islands within the water. Any time there might need to be holes cut within a larger polygon, it will be a relation. Since nodes are the easiest to parse, I'm starting with processing the nodes first, then the ways, then relations. First, I'm going to turn my \"greet\" function into a log function. This way, I'll be able to log output to the JS console. #[wasm_bindgen] extern \"C\" { #[wasm_bindgen(js_namespace = console)] fn log(s: &str); } Next, I'm passing the string provided by the API (in the fetch call I'm returning response.text()) to my WASM-bound function. #[wasm_bindgen] pub fn process_nodes(text: String) -> js_sys::Array { // Parses the string into the OSM data structure. let doc = osm::OSM::parse(text.as_bytes()).unwrap(); // Bounds are the min and max latitudes and longitudes of the map. let bounds = doc.bounds.unwrap(); // New JS array that will be returned. let arr = js_sys::Array::new(); // Iterate over the nodes found in the document. // Each node should be the location of a tree. for (_id, node) in doc.nodes.iter() { // The nodes will be processed and pushed to the JS array, // but for now, just logging out the data. log(&format!(\"{} {}\"), node.lat, node.lon); } arr } Now I have to call this function from JavaScript. I'll create a getTrees function that fetches data from the trees endpoint, then call the WASM process_nodes function. // First, update the import at the top of the script. import init, {process_nodes} from './process-maps/pkg/process_maps.js'; // Add getTrees to the bottom of the script. function getTrees(url) { fetch(`${url}${trees};out;`).then(response => { return response.text(); }).then(data => { process_nodes(data, width, height); }); } getTrees(url); After rebuilding the WASM code and refreshing the page, I see lists of coordinates in my developer console. I can't just use raw coordinates to draw to a canvas, however. I need to convert the geographical coordinates to xy coordinates that will map to the size of my canvas. First, I need a function that will take a single coordinate value and map it to the range of 0 to the width or height of the canvas. // start1 and stop1 are the min and max map bounds that were unwrapped earlier. // start2 and stop2 are the min and max values of the canvas. fn map_points(value: f64, start1: f64, stop1: f64, start2: f64, stop2: f64) -> f64 { ((value - start1) / (stop1 - start1) * (stop2 - start2) + start2).floor() } Next, I need to create the x and y coordinates and wrap them in an array for export to my JavaScript. fn process_points(node: &osm::Node, bounds: &osm::Bounds, width: f64, height: f64) -> js_sys::Array { let y = map_points(node.lat, bounds.minlat, bounds.maxlat, 0.0, width); let x = map_points(node.lon, bounds.minlon, bounds.maxlon, 0.0, height); let point = js_sys::Array::new(); // Numbers being sent to JavaScript must be transformed into a JsValue. point.push(&JsValue::from_f64(x)); point.push(&JsValue::from_f64(y)); point } Finally, I can add a call to process_points from my process_nodes function. #[wasm_bindgen] pub fn process_nodes(text: String, width: f64, height: f64) -> js_sys::Array { let doc = osm::OSM::parse(text.as_bytes()).unwrap(); let bounds = doc.bounds.unwrap(); let arr = js_sys::Array::new(); for (_id, node) in doc.nodes.iter() { arr.push(&process_points(node, &bounds, width, height)); } arr } Now, calling this function from my JavaScript file should return an array of points. Since I'm actually returning an array, I can stop calling log from the Rust script, and I can call console.log from JavaScript. function getTrees(url) { fetch(`${url}${trees};out;`).then(response => { return response.text(); }).then(data => { let p = process_nodes(data, width, height); console.log(p); }); } getTrees(url); This returns a large array of arrays with length 2. Each point corresponds to a point on the canvas. DRAWING ON THE CANVAS Now that there are point coordinates, they can be drawn to the canvas itself. First, I need to prepare the canvas for drawing. let canvas = document.getElementById('canvas'); let ratio = (Math.abs(w) - Math.abs(e)) / (n - s); let width = 1800; let height = width * ratio; canvas.width = height; canvas.height = width; let ctx = canvas.getContext('2d'); ctx.translate(0, width); ctx.scale(1, -1); ctx.fillStyle = '#ffffff'; ctx.fillRect(0, 0, height, width); I had to do some rotating and flipping of the canvas to get the map to draw in the correct direction. Perhaps this is a problem with my coordinate mapping, or some other issue, but this seems to fix it. Next, I have to get the processed tree coordinates, and once those are available, draw the trees to the canvas. const tree_data = getTrees(url); // Returns the processed coordinates. // Waits for all functions to resolve. // Right now, it's just the getTrees function. Promise.all([tree_data]).then(values => { const [trees] = values; for (let tree of trees) { ctx.beginPath(); ctx.arc(tree[0], tree[1], 3, 0, 2 * Math.PI); ctx.fillStyle = 'rgb(40,107,83)'; ctx.fill(); ctx.closePath(); } }); // Updated this function to return the fetched and processed data. function getTrees(url) { return fetch(`${url}${trees};out;`).then(response => { return response.text(); }).then(data => { return process_nodes(data, width, height); }); } All the trees should now be drawn to the canvas. PROCESSING WAYS AND RELATIONS To draw any other items to the map, I need to add the ability to process OpenStreetMap ways and relations to my Rust code. This is my initial code to extract ways from the OSM data. It's nearly identical to the process_nodes function, but I'll need to add another processing step. #[wasm_bindgen] pub fn process_ways(text: String, width: f64, height: f64) -> js_sys::Array { let doc = osm::OSM::parse(text.as_bytes()).unwrap(); let bounds = doc.bounds.unwrap(); let arr = js_sys::Array::new(); for (_id, way) in doc.ways.iter() { log(&format!(\"{:?}\", way)); } arr } Then, I import this function into my JavaScript code and call it. import init, {process_nodes, process_ways} from './process-maps/pkg/process_maps.js'; const walkway_data = getWalkways(url); function getWalkways(url) { return fetch(`${url}${walkways};out;`).then(response => { return response.text(); }).then(data => { return process_ways(data, width, height); }); } Logging this out will return a \"Way\" object, which contains multiple nodes. Now, I need to write an additional function that will process each of the nodes in the way and create canvas coordinates. fn process_coords(doc: &osm::OSM, way: &osm::Way, bounds: &osm::Bounds, width: f64, height: f64) -> js_sys::Array { // Create an empty array to return the coordinates. let coords = js_sys::Array::new(); // Iterate over each of the nodes in the way. for node in way.nodes.iter() { // Finds the node by its ID number. let n = &doc.resolve_reference(&node); // Matches the node by reference and then generates coordinates. match n { osm::Reference::Node(node) => { let point = process_points(node, &bounds, width, height); coords.push(&point); }, _ => {} } } coords } I need to call this function from process_ways. #[wasm_bindgen] pub fn process_ways(text: String, width: f64, height: f64) -> js_sys::Array { let doc = osm::OSM::parse(text.as_bytes()).unwrap(); let bounds = doc.bounds.unwrap(); let arr = js_sys::Array::new(); for (_id, way) in doc.ways.iter() { arr.push(&process_coords(&doc, way, &bounds, width, height)); } arr } Logging the result out from JavaScript will return an array of all the walkways, broken down into coordinate pairs. Processing the relations (waterways, in the case of this map) requires a further breakdown step. Relations need to be mapped to ways, which are then mapped to nodes. When I process the ways, I'm only going to push the coordinates for the \"outer\" way to my array. The \"inner\" ways are cutout areas of the larger polygon. For my purposes, I'm going to draw the water first, then the land and everything else, so I really don't need to worry about the inner polygons. This cuts down on extra processing and drawing time. #[wasm_bindgen] pub fn process_relations(text: String, width: f64, height: f64) -> js_sys::Array { let doc = osm::OSM::parse(text.as_bytes()).unwrap(); let bounds = doc.bounds.unwrap(); let arr = js_sys::Array::new(); for (_id, relation) in doc.relations.iter() { for member in relation.members.iter() { match member { // Matching ways within the relation. osm::Member::Way(way, t) => { let w = &doc.resolve_reference(&way); match w { osm::Reference::Way(way) => { // Only processing coordinates for the \"outer\" way. if t == \"outer\" {arr.push(&process_coords(&doc, way, &bounds, width, height)); } }, _ => {} } }, _ => {} } } } arr } DRAWING ALL OF THE OTHER MAP AREAS Since the rest of the map is constructed with filled polygons, I created a function to draw each polygon for the various map types. Each type of map item will have a different color, and walkways will only have a stroke and no fill. Below is all the code that fetches the OSM data for each landmark, calls the WASM functions to process the data, then draws the coordinates to the canvas. I've left out the Overpass API code, which can be referenced above at calling the Overpass API. const url = `https://overpass-api.de/api/interpreter?data=${query}`; const water_data = getWater(url); const garden_data = getGardens(url); const walkway_data = getWalkways(url); const tree_data = getTrees(url); const building_data = getBuildings(url); const nbuilding_data = getNamedBuildings(url); Promise.all([water_data, garden_data, walkway_data, tree_data, building_data, nbuilding_data]).then(values => { const [water, gardens, walkways, trees, buildings, named_buildings] = values; drawPolygons(water, 'rgb(83,156,156)', null); drawPolygons(gardens, 'rgb(136,172,140)', null); drawPolygons(walkways, null, 'rgb(0,0,0)'); for (let tree of trees) { ctx.beginPath(); ctx.arc(tree[0], tree[1], 3, 0, 2 * Math.PI); ctx.fillStyle = 'rgb(40,107,83)'; ctx.fill(); ctx.closePath(); } drawPolygons(buildings, 'rgb(98,90,87)', null); drawPolygons(named_buildings, 'rgb(220,177,102)', null); }); function drawPolygons(p, fill, stroke) { for (let polygon of p) { ctx.beginPath(); for (let point of polygon) { ctx.lineTo(point[0], point[1]); } if (fill) { ctx.fillStyle = fill; ctx.fill(); } if (stroke) { ctx.strokeStyle = stroke; ctx.stroke(); } ctx.closePath(); } } function getWater(url) { return fetch(`${url}${water};out;`).then(response => { return response.text(); }).then(data => { return process_relations(data, width, height); }); } function getWalkways(url) { return fetch(`${url}${walkways};out;`).then(response => { return response.text(); }).then(data => { return process_ways(data, width, height); }); } function getBuildings(url) { return fetch(`${url}${buildings};out;`).then(response => { return response.text(); }).then(data => { return process_ways(data, width, height); }); } function getNamedBuildings(url) { return fetch(`${url}${named_buildings};out;`).then(response => { return response.text(); }).then(data => { return process_ways(data, width, height); }); } function getGardens(url) { return fetch(`${url}${gardens};out;`).then(response => { return response.text(); }).then(data => { return process_ways(data, width, height); }); } function getTrees(url) { return fetch(`${url}${trees};out;`).then(response => { return response.text(); }).then(data => { return process_nodes(data, width, height); }); } THE FINISHED MAP I've benchmarked the map's loading time, and it takes about 5.2 seconds on average when the Rust code is compiled in development mode, and about 2.3 seconds in production mode. In an earlier iteration I had each map endpoint individually queried, processed, and drawn before querying the next endpoint, which was much slower. Now, the map is at the mercy of the slowest query (around 2.1 seconds). A way to save more time would be to save the coordinates to a file instead of querying the API, but any changes wouldn't be reflected in the map. Here's a finished screenshot of the Magic Kingdom's map. I've added a working demo here. If you want to check out other parks besides the Magic Kingdom, add the query strings \"?park=epcot\", \"?park=hs\", or \"?park=ak\" to the end of the URL. This is part one of a much larger project, so while it feels a bit janky now, everything will be cleaned up in the future. The main goal here was to prove that it is possible to use Rust and WASM to process these coordinates to make my own maps.",
    "commentLink": "https://news.ycombinator.com/item?id=39439655",
    "commentBody": "Translating OpenStreetMap data to HTML5 Canvas with Rust and WebAssembly (mary.codes)342 points by todsacerdoti 23 hours agohidepastfavorite33 comments simpaticoder 21 hours agoWhat a clear, step-by-step write-up of getting this POC (proof-of-concept) code running. She has picked as her audience experienced programmers, and one gets the sense that the write-up is as much for herself as for us. Which is great. We get to live vicariously through her and solve a small-ish (but non-trivial) problem with Rust, WASM and an OSM api with a cool visual payoff at the end. It's easy to let yourself not be impressed by straight-forward writing to a clear audience, because it looks so simple. But it's absolutely not simple and I'm glad to have found this blog and author. Thanks again, HN! reply mxfh 21 hours agoprevThe direct plotting from geographic coordinates (e.g., WGS84) can lead to visual distortions, especially for parks located further from the equator, where circles will appear flattened. A practical solution would be to reproject the data to the Mercator projection or the appropriate local UTM Zone, ensuring accurate distances in cartesian units with no visible distortions of angles. Alternatively, a quite straightforward method would involve scaling the longitudinal axis by the cosine of the latitude (cos φ [1]) at the center latitude of the area of interest. This adjustment provides a good approximation for small extents, like theme parks, without delving deeply into map projections. [1] https://en.wikipedia.org/wiki/Longitude#Length_of_a_degree_o... reply jillesvangurp 18 hours agoparentShould not be an issue here as this is based on openstreetmap, which uses wgs84 coordinates everywhere. Changing the projection won't make that much of a difference for small features like parks or buildings. You are right that this is an issue if you start plotting e.g. circles using a naive algorithm as the x and y scales in degrees are simply not the same in meters. Which is an issue if you want to draw circles and rectangles on a map. The solution to that is to do it in meters and then translate the points from some origin using e.g. the Haversine distance. A related issue you have there is that distance algorithms aren't accurate either but it's close enough as an approximation over small distances. I have some algorithms for that in my jillesvangurp/geogeometry library. I also have some UTM coordinate conversion algorithms in there that I recently added. reply mxfh 15 hours agorootparentThe distortion is still there, even for small features as seen in the OP examples of circular features of the park. This due to the changing length of longitudinal unit at a given latitude. Historically this is the reason web mapping picked up Mercator in the first place, since it preserves angles and relatives sizes (conformal) at city scales, which is the primary use case of those. WGS84 is a CRS first, not meant to be used as cartographic projection for a known local scope. (for some Plate carée is more of a plot than a projection) I’m not talking about converting between local CRS for surveying an local plate consistency. You just want the display to look not visibly distorted in a way that’s not intentional. Florida is still quite forgiving here. Eurodisney not so much. https://commons.wikimedia.org/wiki/File:Plate_Carr%C3%A9e_wi... reply wodenokoto 20 hours agoparentprevI thought all maps were drawn using web Mercator and that things were simply not large enough to be really distorted. Does gmaps and company do projected rendering? reply modeless 14 hours agorootparentGoogle Maps on web uses perspective projection, unless WebGL is disabled in your browser. Zoom out and see! reply sebstefan 18 hours agorootparentprevThey do, if you zoom out they're a mercator that wraps around, but when they give you latitude and longitude instead of x and y, you're getting the real sphere stuff reply samstave 18 hours agorootparentprevI used to know a lot more about rendering than I do today, but (forgive me if this sounds stupid) - I wonder if you could add to a gps coordinate area (say 1 sq. km. to also include that patche's NORMAL from the surface of the globe that you're projecting your map to, and have the 1km tile's image adjusted based on a orthographic view from that tile's NORMAL, so effectively - you're projecting each 1k square more directly from areal imagery? Or is this already considered in modern projection methods? reply lstodd 19 hours agoparentprevI once experimented with something like this for OpenXcom is to first rotate the geographic coordinate system so that local equator/zero meridian intersection is at the center of the area of interest, then apply merkator or whatever other projection you like. Otherwise it's a sea of hacks which doesn't get rid of latitude-induced distortions anyway. After that being very annoyed with lat/long stuff I moved to N-vector representation. https://en.wikipedia.org/wiki/N-vector reply maxerickson 21 hours agoprevThe mentioned Overpass API uses sets as a sort of fundamental concept, so the code way[building][!name];foreach{(._;>;);out;} can be replaced with way[building][!name];(._;>;);out; The \"(._;>;);\" stanza is sort of dense and seems opaque without knowing what it's doing, but it's straightforward, returning the union \"();\" of the previous result \"._;\" and the children of the previous result \">;\". The change I propose above adjusts the \">;\" to operate on all of the \"way[building][!name];\" results together, rather than each element one at the time. reply enigmo 19 hours agoparentthis is a common enough need that out geom does a similar operation in fewer characters reply maxerickson 16 hours agorootparentIt doesn't do exactly the same thing, it inserts the lat/long into the parent instead of returning the nodes. Often doesn't matter, but it does if you are using a library that is expecting osm xml. reply emporas 17 hours agoprevI was looking at that project [1] trying to understand what this article explains in a more simple way. There is some non-trivial complexity of how to put together wasm-pack, web-sys, js-sys and wasm-bindgen functions to work well together with javascript for a web page or a browser extension. Nice article! [1] https://github.com/drakerossman/hackernews-userscript reply gerogerke 19 hours agoprevFor anyone interested, I have a similar blog-post [0], in which I explore rendering OSM data using WebGL. [0]: https://gero.dev/blog/webgl-render-osm-streets reply wvh 19 hours agoprevThis is a cool project; it has just enough meat on its bones to learn a bit about WASM and mapping data, and an attainable goal with a clear, visual result after some effort. I just spent half an hour to try to get this to work myself to see how far WASM has come. Not sure I'll be doing something with this specifically, but good to keep in the back of my head for future challenges along the same vein. reply dakial1 22 hours agoprevAuthor could use that queue time and some ML to forecast waiting times trends and create a real time Next-Best-Ride (NBR) model and plot those in the map. I'd find that useful the next time I'm pushing my 2 kids around that place... reply HumblyTossed 14 hours agoprevIn case you're hungry after reading this, she has a post on how to make creamy grits: https://mary.codes/blog/recipes/the_secret_to_the_creamiest_... I love grits, but have never tried baking soda (or even heard of it). Gonna have to try this. reply naitgacem 21 hours agoprevI have tried to use Canvas on Android to draw some custom view (chart). I still have a PTSD from that experience. I have an exceptionally great amount of respect for people who can do this sort of stuff. reply liftm 18 hours agoparentWell, if you're playing with wasm like this post, you could sidestep canvas and render the image on the wasm side. c.f. https://github.com/plotters-rs/plotters-wasm-demo reply ativzzz 15 hours agoparentprevI did something similar but used SVG instead - it didn't feel that bad reply beeboobaa 20 hours agoprev> While I haven't performed any benchmarks about whether Rust or JavaScript processing is faster, I decided to use Rust and WebAssembly because there will be a lot of coordinates to process, and Rust generally performs faster at large amounts of data processing. Is this really true for Rust compiled to WebAssembly in the browser? reply afavour 19 hours agoparentIt’s an interesting question. When it comes to processing large amounts of data I’d actually be more concerned about JavaScript’s memory usage than strict processing performance. Certainly you could use JS smartly with typed arrays and such to minimise memory impact but you'd basically be manually creating a lot of the convenience that Rust gives you out of the box. reply grovesNL 19 hours agoparentprevIt depends on the use case (e.g., how much you're interacting with web APIs) but I'd generally expect this to be the case for data processing at least. During compilation from Rust to WebAssembly you have the opportunity to apply ahead-of-time optimizations that would be too expensive or impossible to apply to JavaScript during execution, so this can improve performance pretty significantly over JavaScript. reply davidmurdoch 20 hours agoparentprevDepends on the data types and if the WASM code can make use of SIMD. reply butz 17 hours agoprevHere is an idea: use Mapsforge binary format for map data. Map sizes are quite small, especially useful for not to huge areas. Implementing HTTP Range requests might be an interesting exercise in reducing bandwidth usage. reply coder543 12 hours agoparentAn existing solution: > PMTiles readers use HTTP Range Requests to fetch only the relevant tile or metadata inside a PMTiles archive on-demand. https://docs.protomaps.com/pmtiles/ reply Solvency 19 hours agoprevIf they have a public API why did she have to crop and manually export stuff? What is she exporting, data? If so what is the API doing? reply Ajedi32 19 hours agoparentLooks like she wasn't actually exporting anything, just using the export feature as a convenient way to get the coordinates of the bounding box she wanted to search in. reply NoPedantsThanks 17 hours agoprevWasn't there a post on here recently about OSM going to vector-based tiles? I did a quick search and it seems like this is already a widely-offered option. Can anyone jog our memory on what the change is? reply mikeocool 11 hours agoparentThe OSM Foundation is building out a stack to generate 'minutely' vector tiles -- meaning the if you edit the map, it'll be reflected in the vector tiles within a few minutes. Most vector tile sets today generated from OSM are hours behind the current state of the map data in the best case, much more commonly they are months behind. reply maxerickson 11 hours agoparentprevOSM foundation is going to host vector tiles and build a stack to keep them up to date with ongoing editing. You are probably talking about https://news.ycombinator.com/item?id=39339182 reply NoPedantsThanks 4 hours agoparentprevThanks for the replies. Sounds like a nice addition. reply londons_explore 14 hours agoprev [–] So the data seems to come from the overpass API as XML, gets a bit of transformation in rust, passed back to javascript for drawing... To me, that seems like the wrong design for performance... Drawing you probably want to use webgl shaders for so you can do zooming and panning at 60 fps... And downloading the data as XML seems like a bad idea if you're drawing city-sized areas because the XML will be huge and contain plenty more detail than you need. Instead you need a server-side component to pack simplified and quantized polygon data for the necessary scale into a binary datastructure for sending to the client. And thats exactly how bing/google maps/apple maps work... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mary Knize is enhancing her Line Buddy project by leveraging OpenStreetMap data and utilizing Rust with WebAssembly to develop a 3D map of Disney World theme parks.",
      "The project demonstrates fetching, processing, and presenting map data with Rust and JavaScript to efficiently handle elements like buildings, walkways, and water bodies, optimizing loading times while offering park selection.",
      "There are upcoming plans for additional improvements to advance the project's functionality."
    ],
    "commentSummary": [
      "The post explores translating OpenStreetMap data to HTML5 Canvas with Rust and WebAssembly, emphasizing visual distortions in mapping and advocating for the Mercator projection for accuracy.",
      "It discusses the choice between JavaScript and Rust for data processing and the potential for vector-based tiles in OSM, suggesting WebGL for drawing to enhance performance instead of transferring data between Rust and JavaScript.",
      "The use of various projections for precise mapping is also covered in the discussion."
    ],
    "points": 342,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1708424881
  },
  {
    "id": 39444282,
    "title": "Picat's Versatile Planning Capabilities Spotlighted",
    "originLink": "https://www.hillelwayne.com/post/picat/",
    "originBody": "Planner programming blows my mind Contact via Email Open Github account in new tab Posted on Feb 20, 2024 Picat is a research language intended to combine logic programming, imperative programming, and constraint solving. I originally learned it to help with vacation scheduling but soon discovered its planner module, which is one of the most fascinating programming models I’ve ever seen. First, a brief explanation of logic programming (LP). In imperative and functional programming, we take inputs and write algorithms that produce outputs. In LP and constraint solving, we instead provide a set of equations and find assignments that satisfy those relationships. For example: main => Arr = [a, b, c, a], X = a, member(X, Arr), member(Y, Arr), X != Y, println([X, Y]). Non-function identifiers that start with lowercase letters are “atoms”, or unique tokens. Identifiers that start with uppercase letters are variables. So [a, b, c, a] is a list of four atoms, while Arr and X are variables. So Member(X, Arr) returns true as you’d expect. The interesting thing is Member(Y, Arr). Y wasn’t defined yet! So Picat finds a value for Y that makes the equation true. Y could be any of a, b, or c. Then the line after that makes it impossible for Y to be a, so this prints either ab or ac. Picat can even handle expressions like member(a, Z), instantiating Z as a list! Planning pushes this all one step further: instead of finding variable assignments that satisfy equations, we find variable mutations that reach a certain end state. And this opens up some really cool possibilities. To showcase this, we’ll use Picat to solve a pathing problem. The problem We place a marker on the grid, starting at the origin (0, 0), and pick another coordinate as the goal. At each step we can move one step in any cardinal direction, but cannot go off the boundaries of the grid. The program is successful when the marker is at the goal coordinate. As a small example: +---+|G|O+---+ One solution would be to move to (1, 0) and then to (1, 1). To solve this with planning, we need to provide three things: A starting state Start, which contains both the origin and goal coordinates. A set of action functions that represent state transitions. In Picat these functions must all be named action and take four parameters: a current state, a next state, an action name, and a cost. We’ll see that all below. A function named final(S) that determines if S is a final state. Once we define all of these, we can call the builtin best_plan(Start, Plan) which will assign Plan to the shortest sequence of steps needed to reach a final state.1 Our first implementation import planner. import util. main => Origin = {0, 0} , Goal = {2, 2} , Start = {Origin, Goal} , best_plan(Start, Path) , println(Plan) . Explanation Since final takes just one argument, we’ll need to store both the current position and the goal into said argument. Picat has great pattern matching so we can just write it like this: final({Pos, Goal}) => Pos = Goal. Explanation Finally, we need to define the actions which the planner can take. We only need one action here. action(From, To, Action, Cost) ?=> From = {{Fx, Fy}, Goal} , Dir = [{-1, 0}, {1, 0}, {0, -1}, {0, 1}] , member({Dx, Dy}, Dir) % (a) , Tx = Fx + Dx , Ty = Fy + Dy , member(Tx, 0..10) % (b) , member(Ty, 0..10) % (b) , To = {{Tx, Ty}, Goal} , Action = {move, T[1]} , Cost = 1 . Explanation And that’s it, we’re done with the program. Here’s the output: > picat planner1.pi [{move,{1,0}},{move,{2,0}},{move,{2,1}},{move,{2,2}}] That’s a little tough to read, so I had Picat output structured data that I could process into a picture. main => Origin = {0, 0} , Goal = {2, 2} , Start = {Origin, Goal} , best_plan(Start, Path) - , println(Plan) + , printf(\"Origin: %w\", Origin) + , printf(\"Goal: %w\", Goal) + , printf(\"Bounds: {10, 10}\") + , printf(\"Path: \") + , println(join([to_string(A[2]): A in Plan], \", \")) . I used a Raku script to visualize it.2 Here’s what we now get: > raku format_path.raku -bf planner1.pi +-----+||G| •|O••+-----+ To show that the planner can route around an “obstacle”, I’ll add a rule that the state cannot be a certain value: , Tx = Fx + Dx , Ty = Fy + Dy + , {Tx, Ty} != {2, 1} +-----+||•G| •|O•+-----+ Let’s comment that out for now, leaving this as our current version of the code: Code Adding multiple goals Next I’ll add multiple goals. In order to succeed, the planner needs to reach every single goal in order. We start with one change to main: main => Origin = {0, 0} - , Goal = {2, 2} + , Goal = [{2, 2}, {3, 4}] Goal now represents a “queue” of goals to reach, in order. Then we add a new action which removes a goal from our queue once we’ve reached it. action(From, To, Action, Cost) ?=> From = {Pos, Goal} , Goal = [Pos|Rest] , To = {Pos, Rest} , Action = {mark, From[1]} , Cost = 1 . Explanation Since we’re now destructively removing goals from our list when we reach them, final needs to be adjusted: final({Pos, Goal}) => - Pos = Goal. + Goal = []. And that’s it. We didn’t even have to update our first action! +-----+G| •| G•| •|O••+-----+ Code Cost minimization Going through the goals in order doesn’t always lead to the shortest total path. main => Origin = {0, 0} - , Goal = [{2, 2}, {3, 4}] + , Goal = [{9, 2}, {0, 4}, {9, 6}, {0, 9}] +----------+ |G|•|•|•••••••••G|•| |G•••••••••| |•|•••••••••G|•| |O•••••••••| +----------+ What if we didn’t care about the order of the goals and just wanted to find the shortest path? Then we only need to change two lines: action(From, T, Action, Cost) ?=> From = {Pos, Goal} - , Goal = [Pos|Rest] - , T = {Pos, Rest} + , member(Pos, Goal) + , T = {Pos, delete(Goal, Pos)} , Action = {mark, From[1]} , Cost = 1 . Now the planner can delete any goal it’s passing over regardless of where it is in the Goal list. So Picat can “choose” which goal it moves to next so as to minimize the overall path length. +----------+ |G•••••••••| |• •| |• •| |• G| |• •| |G •| |• •| |• G| |•|O+----------+ Final code: Code Other variations Picat supports a lot more variations on planning: best_plan(S, Limit, Plan) caps the maximum cost at Limit— good for failing early. For each best_plan, there’s a best_plan_nondet that finds every possible best plan. sequence(P, Action) restricts the possible actions based on the current partial plan, so we can add restrictions like “you have to move twice before you turn”. The coolest thing to me is that the planning integrates with all the other Picat features. I whipped up a quick demo that combines planning and constraint solving. The partition problem is an NP-complete problem where you partition a list of numbers into two equal sums. This program takes a list of numbers and finds the sublist with the largest possible equal partitioning:3 Code Removed: [5,17] Final: [32,122,77,86,59,47,154,141,172,49,62,99,109,30,977] 32+99+977=1108 122+77+86+59+47+154+141+172+49+62+109+30=1108 This is all so mindblowing to me. It’s almost like a metaconstraint solver, allowing me to express constraints on the valid constraints. Should I use Picat? Depends? I would not recommend using Picat in production. It’s a research language and doesn’t have a lot of affordances, like good documentation or clear error messages. Here’s what you get when there’s no plan that solves the problem: *** error(failed,main/0) But hey it runs on Windows, which is better than 99% of research languages. Picat seems more useful as a “toolkit” language, one you learn to solve a specific class of computational problems, and where you’re not expecting to maintain or share the code afterwards. But it’s really good in that niche! There’s a handful of problems I struggled to do with regular programming languages and constraint solvers. Picat solves a lot of them quite elegantly. Appendix: Other planning languages While originally pioneered for robotics and AI, “planning” is most-often used for video game AIs, where it’s called “Goal Oriented Action Planning” (GOAP). Usually it’s built as libraries on top of other languages, or implemented as a custom search strategy. You can read more about GOAP here. There is also PDDL, a planning description language that independent planners take as input, in the same way that DIMACS is a description format for SAT. Thanks to Predrag Gruevski for feedback. I first shared my thoughts on Picat on my newsletter. I write new newsletter posts weekly. If you want just any plan, regardless of how long it is, you can call plan(Start, Plan) instead. [return] I put a commented version of the formatter up on my Patreon. I’ll inline it in this post in like a week or so, gotta leave something for the Patreon [return] I also wrote a more convoluted plan that finds the most elements you can remove without getting you a valid partition. That’s also on the Patreon and will also be inlined here in a week. [return] Categories: Programming Tags: Picat, Raku, Constraint Solving, Niche Language Demos",
    "commentLink": "https://news.ycombinator.com/item?id=39444282",
    "commentBody": "Planner programming blows my mind (hillelwayne.com)293 points by todsacerdoti 15 hours agohidepastfavorite34 comments sterlind 10 hours agoI've actually used Picat's planning mode at work! I prototyped a system to orchestrate maintenance on fleets of devices. The idea was that, rather than telling the system how to do it (e.g. workflows to roll out an update), you'd tell the system what you wanted (e.g. up to date machines), what actions were available (e.g. pull a machine from rotation, apply an update), and what constraints to obey (e.g. X of Y machines must be online, don't work in more than two regions simultaneously.) I modeled a few scenarios like that in Picat and had it generate optimal plans. It worked swimmingly for pet problems, but predictably fell over scaling to cattle sizes. Planning is EXPTIME after all (e.g. Towers of Hanoi).* Picat does have an escape hatch - you can define heuristics - so I built a random forest of state predicates and trained a naive Bayes classifier to predict fruitful paths. But even with that, and symmetry breaking constraints, and even some hierarchical planning, I couldn't make it work without too much handholding. It's still AI winter for the classic GOFAI problem domains, apparently. :/ * maybe not, actually, if you reformulate the planning problem as returning a polynomial-time generator of a potentially exponentially long plan reply 7thaccount 8 hours agoparentThere are plenty of commercial solvers out there that beat the pants off the open source options in terms of performance and in terms of depleting one's wallet :) CPLEX, Xpress, GUROBI, and Hexaly all come to mind. Hexaly is really good for scheduling problems and things like vehicle routing. You typically access these via an API they offer you for the popular industry languages. This approach seems to make a lot more sense to me than having a dedicated solver language that isn't as good for all the general purpose stuff. Calling GUROBI from Python is a breeze as is all the standard stuff in Python. Mosek is a lot cheaper than GUROBI, but both of it's APIs are extremely low level and the performance isn't as good as GUROBI either. reply shoo 8 hours agorootparentYup. & when you get a model that works matched up with an industrial scale decision problem that's valuable to solve, arguably it's only of academic interest if you can solve it \"optimally\". The problem is only a simplified model of reality anyway -- it's often better to get a quick close-enough approximate solution to a problem that's a good approximation of the situation than an exact optimal solution to a simpler problem that's a poorer approximation. If you're lucky enough to get a problem that's basically stable over time, where the problem structure doesn't change, then maybe you can get improved solutions rapidly at industrial scale replacing use of a black-box MIP solver like Gurobi/CPLEX with a decomposition that exploits the problem structure, where sub-problems can be solved by some specialized graph algorithm or heuristic or brute force (if they all have bounded size), and the general purpose MIP/LP solver can be left with the job of figuring out how to deal with the shared resources and constraints that bind the subproblems together. The downside to a highly specialised custom solver is that it usually isn't flexible to changing requirements (unless you get very lucky) -- a slight change in business rule can break the problem structure that underpins the entire solution approach. reply polivier 7 hours agorootparentprev> There are plenty of commercial solvers out there that beat the pants off the open source options in terms of performance and in terms of depleting one's wallet :) While this is generally true, there are some exceptions. I recently compared the performance of CP-SAT vs CPLEX for a problem (linear constraints and objective). For large instances where proving optimality in a reasonable time was out of the question, CP-SAT had much faster convergence to near-optimal solutions than CPLEX when the time limit was small enough (~30s to a few minutes). This is with the CPLEX solver tuned towards improving the upper bound as much as possible (it was a minimization problem). reply whatever1 7 hours agorootparentIf feasibility is your goal then cp/sat solvers/heuristics should be your tool of choice. I you have optimality requirements (aka from the feasible solutions find the absolutely best) then optimization is the way to go reply polivier 2 hours agorootparentI think that you've misunderstood what I said. For large instances of this specific problem (and when the time limit is too short to allow either CP-SAT or CPLEX to prove optimality) the best integer feasible solution found by CP-SAT is generally of better quality (w.r.t. the objective value) than the best integer feasible solution found by CPLEX. Furthermore, in some cases, CP-SAT can offer a certificate of optimality faster than CPLEX. reply sterlind 4 hours agorootparentprevcan't you just use the strong duality theorem to reframe an integral optimization problem as a system of integer inequalities? I thought you usually don't do that because the satisfaction problem is harder in practice. reply sterlind 4 hours agorootparentprevGurobi is a MIP solver right, not a planner? I use Gurobi at work for a certain kind of bi-level programming and it's amazing, like literally ~500x faster than CBC. Picat's planner is more like a Prolog flavor of PDDL (e.g. fast-downward and its ilk.) reply TimTheTinker 6 hours agorootparentprevAre the commercial offerings you mentioned better than TimeFold? [0] (formerly known as OptaPlanner before the main developers forked it) TimeFold's heuristics-based approach makes fast solutions to even highly-complex scenarios within the reach of anyone who can write Java or Python expressions that evaluate to true when constraints are satisfied. [0] https://timefold.ai/ reply zokier 1 hour agorootparentHuh, didn't know Optaplanner was forked. Bit sad, you would have thought it was good fit for IBM to all the enterprisey business automation stuff. Does Optaplanner have anyone left working on IBM(/RH) side? Anyways, I wish the best for Timefold team, hopefully they can find success independently. With all the money sloshing around for all sorts of dumb AI projects, I think Timefold definitely deserves a portion too. reply Aeolun 1 hour agorootparentprevI found Optaplanner to be nice, but supremely unergonomic to use. I could replicate their example problems and not much else. > All OptaPlanner features are part of Community Edition, except for Multithreaded Solving Of course, because why not. If you can take features away from an existing thing to make more money. reply boxed 55 minutes agorootparentprevI just hate it when you go to the pricing page and theres NO PRICING. None. reply Aeolun 1 hour agorootparentprevIf any of these had reasonable pricing I’d be happy to pay, but if the first price you see is ‘contact us’ you can be certain it’s too much for hobby use… reply eigenvalue 6 hours agorootparentprevWhat about this one: https://www.cvxpy.org/ If you can convert your problem into a convex one (which I believe is often possible if you’re clever about how you express it), that would seem to be a pretty good option, no? reply mlsu 1 hour agorootparentCVXpy is a frontend that transforms problems into a form that different solvers can understand. It does make it very easy to format problems in Python but you still need a solver like Gurobi on the backend. You can use a variety of solvers on the same problem though, which is nice: https://www.cvxpy.org/tutorial/advanced/index.html#choosing-... My understanding is that Gurobi the best -- but also the most expensive. reply pea 14 minutes agoparentprevWhy don't high quality open-source solvers exist? The SOTA in other numerical computing is often open-source. I've always wondered why optimization was different. reply polivier 10 hours agoparentprevIt is possible that something like CP-SAT (https://developers.google.com/optimization/cp) would have scaled well in your case. This solver easily handles an absurd amount of variables and constraints, and has excellent heuristics built-in. reply OJFord 8 hours agoparentprevSo it's not actually different (in result/performance) from the depth-first search you'd get naïvely mashing some keys dimly recalling prolog? reply nickpsecurity 8 hours agoparentprev\"Planning is EXPTIME after all (e.g. Towers of Hanoi).*\" The old planners would include meta-rules, or heuristics, that decided which rules to apply. That would cut the search space. Some split the problem into different representations with specialized, automated solvers. Jahob Analysis System and Cyc come to mind. Far as for real-world use, the neatest design I remember from classic A.I. was the Procedural Reasoning System. I've always wanted to see a version of it rebuilt with modern methods supplementing its weaknesses. Just for kicks and to see what it could do. https://en.wikipedia.org/wiki/Procedural_reasoning_system reply polivier 10 hours agoprevFor those interested, HN user hakank (Hakan Kjellerstrand), who is a very active member of the constraint programming community (among others), has a ton of Picat resources and examples on his website: http://www.hakank.org/picat/ reply asciimike 3 hours agoprevPleasantly surprised to see Predrag show up as a reviewer, but at the same time not at all surprised: - The [Firebase technical screen](https://startupandrew.com/posts/how-firebase-interviewed-sof...) would have been much easier with something like this, as it was Just Another Optimization Problem™. Part of me wants to try it again with Picat! - He's doing other very interesting things with programming languages, e.g.: https://github.com/obi1kenobi/trustfall reply pcthrowaway 8 hours agoprevMy first thought was, this looks like a type system, except you need to solve it yourself. In Typesecript, naively: const main = ([a_, b_, c_]: [a, b, c, a]) => { type SomeTuple = [a, b, c, a]; const X: a = a_; const Y: Exclude // =console.log(X, Y); } Except nothing solves this because a, b, and c can all be the same. After trying to express this correctly, I ended up with something that appears useable (but that still uses assertions and doesn't really express the type of Y correctly). type Narrowable = stringnumberbigintboolean; /* Express the type of a value in a tuple that is not the type of the second parameter For example: - ValOfTupleExceptFor -> 6 - ValOfTupleExceptFor -> 1 */ type ValOfTupleExceptFor = Tup extends [infer First, ...(infer Rest extends Narrowable[])] ? First extends Val ? Rest extends [] ? never : ValOfTupleExceptFor : First : never; const NO_SOLUTION: unique symbol = Symbol('NO_SOLUTION') const getValOfTupleExcluding = (tup: readonly Narrowable[], val: (typeof tup)[number]): ValOfTupleExceptFor => { const [first, ...rest] = tup; if (!first) { return NO_SOLUTION as never; } if (first === val) { return getValOfTupleExcluding(rest, val); } return first as ValOfTupleExceptFor; } const main = ([a_, b_, c_]: [a, b, c, a]) => { const someTuple = [a_, b_, c_, a_] as const; const X: a = a_; // This still resolves to type 'never' const Y: ValOfTupleExceptFor = getValOfTupleExcluding(someTuple, a_); console.log(X, Y); } Which really highlights how powerful the 'planner' style program is in terms of simplicity and conciseness. I guess Typescript isn't even powerful enough to express this kind of constraint. edit: TS playground link with some experiments if anyone's interested: http://tinyurl.com/3p2pzdtn reply crabmusket 4 hours agoprevNice to see GOAP being referenced again. It was the secret sauce that made F.E.A.R.'s enemies so fun. And Jeff Orkin's paper on how it works is very readable and entertaining. reply cwillu 1 hour agoprevReally hating the trend of putting half of the document behind series of “click here to unhide the content” dropdowns. reply rad_gruchalski 10 hours agoprevLooks Prolog-ish. Interesting, thanks for sharing. reply hwayne 9 hours agoparentAFAICT Picat is a direct descendant of B-Prolog and shares a lot of idioms (like tabling) with it. https://en.wikipedia.org/wiki/B-Prolog reply OhMeadhbh 6 hours agorootparentTHX for the reference. I used Prolog a fair amount in the 90s, but didn't know about B-Prolog. Now I do. And I came here to make @rad_gruchalski's comment. So... thx for making that comment so I don't have to. reply fumeux_fume 6 hours agoprevIs this related to Answer Set Programming? Seems like there’s an overlap. reply sterlind 4 hours agoparentthey're syntactically related in that both come from the Prolog world, and you can indeed use ASP to do planning (there's examples of Towers of Hanoi and Blockworld in the ASP guidebook), and you can incorporate heuristics into ASP. ...but actually they're really different! ASP isn't Turing-complete - it's a lot more like an SMT solver. crucially, there's a grounding stage, where the set of every expressible term in the model (its Herbrand universe) is explicitly written down. so if you have e.g. `f(a;b). g(X,Y) :- f(X),f(Y).` then it will write out every expansion of g during pre-processing. this makes ASP very powerful, and very fast even at complex problems, but it dooms the solver if the universe is large. in contrast, Picat is basically a souped up Prolog. it's a full programming language, and it doesn't require grounding so infinite state spaces are okay. it leverages its tabling mechanism to memo-ize predicates evaluation, and it automatically manages the time/space tradeoff with search, which is nifty. but at the end of the day it's brute force, not deep witchcraft like Z3. reply fumeux_fume 2 hours agorootparentI see, thanks for your explanation! reply bloaf 8 hours agoprevI'm a little confused about how planning is different from vector reachability, which, from what I understand, has Ackermann complexity rather than EXPTIME. Can anyone help me out with the constraints on \"planning\" that allow it to be solved in a sane amount of time? https://www.quantamagazine.org/an-easy-sounding-problem-yiel... reply eru 6 hours agoparent> [...] from what I understand, has Ackermann complexity rather than EXPTIME. Really bad worst case times aren't necessarily bad in practice, if most instances you actually encounter can be solved quickly (especially if you are happy to be satisfied with worse than proven-optimal solutions.) Compare how Hindley-Milner type inference, which forms the basis of Rust's or Haskell's type systems, is double-exponential in the worst case (or something like that), but typically fast in practice. reply nickpsecurity 7 hours agoparentprevWe normally pick up stuff like that in papers, blog posts, etc. The only heuristics book I remember was How to Solve It. I also found a survey paper on heuristics. Here they are in case they help: https://www.amazon.com/How-Solve-Heuristics-Zbigniew-Michale... https://www.jsoftware.us/vol7/jsw0709-23.pdf reply Guthur 1 hour agoprev [–] I've actually been using Prolog professionally including some CLPFD, and I love it. I want it everywhere. Or more precisely i want a logical core with emphasis on purity and push imperative action to the edges. It so sad that as an industry we seemed locked into really bad tools. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Picat is a unique programming language blending logic, imperative programming, and constraint solving.",
      "The author delves into Picat's planning concept, showcases solving pathing problems, and explores different planning techniques in Picat.",
      "While Picat has limitations for production, it excels in solving specific computational problems; GOAP and PDDL are briefly mentioned as alternative planning languages."
    ],
    "commentSummary": [
      "The post explores the application of planner programming for fleet maintenance systems, addressing scaling challenges and optimization solutions.",
      "It compares specialized and general solvers, highlighting CP-SAT as an alternative to CPLEX in specific scenarios.",
      "The discussion delves into various optimization tools like OptaPlanner, open-source solvers, and the role of planning in artificial intelligence, mentioning Picat and different programming languages."
    ],
    "points": 293,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1708451105
  },
  {
    "id": 39449424,
    "title": "If Architects Worked Like Coders: Unrealistic Demands (1995)",
    "originLink": "http://www.gksoft.com/a/fun/architects.html",
    "originBody": "If Architects had to work like Programmers Dear Mr. Architect! Please design and build me a house. I am not quite sure of what I need, so you should use your discretion. My house should have between two and forty-five bedrooms. Just make sure the plans are such that the bedrooms can be easily added or deleted. When you bring the blueprints to me, I will make the final decision of what I want. Also, bring me the cost breakdown for each configuration so that I can arbitrarily pick one. Keep in mind that the house I ultimately choose must cost less than the one I am currently living in. Make sure, however, that you correct all the deficiencies that exist in my current house (the floor of my kitchen vibrates when I walk across it, and the walls don't have nearly enough insulation in them). As you design, also keep in mind that I want to keep yearly maintenance costs as low as possible. This should mean the incorporation of extra-cost features like aluminium, vinyl, or composite siding. (If you choose not to specify aluminium, be prepared to explain your decision in detail.) Please take care that modern design practices and the latest materials are used in construction of the house, as I want it to be a showplace for the most up-to-date ideas and methods. Be alerted, however, that kitchen should be designed to accommodate, among other things, my 1952 Gibson refrigerator. To insure that you are building the correct house for our entire family, make certain that you contact each of our children, and also our in-laws. My mother-in-law will have very strong feelings about how the house should be designed, since she visits us at least once a year. Make sure that you weigh all of these options carefully and come to the right decision. I, however, retain the right to overrule any choices that you make. Please don't bother me with small details right now. Your job is to develop the overall plans for the house: get the big picture. At this time, for example, it is not appropriate to be choosing the color of the carpet. However, keep in mind that my wife likes blue. Also, do not worry at this time about acquiring the resources to build the house itself. Your first priority is to develop detailed plans and specifications. Once I approve these plans, however, I would expect the house to be under roof within 48 hours. While you are designing this house specifically for me, keep in mind that sooner or later I will have to sell it to someone else. It therefore should have appeal to a wide variety of potential buyers. Please make sure before you finalize the plans that there is a consensus of the population in my area that they like the features this house has. Please prepare a complete set of blueprints. It is not necessary at this time to do the real design, since they will be used only for construction bids. Be advised, however, that you will be held accountable for any increase of construction costs as a result of later design changes. You must be thrilled to be working on an interesting project as this! To be able to use the latest techniques and materials and to be given such freedom in your designs is something that can't happen very often. Contact me as soon as possible with your complete ideas and plans. P.S.: My wife has just told me that she disagrees with many of the instructions I've given you in this letter. As architect, it is your responsibility to resolve these differences. I have tried in the past and have been unable to accomplish this. If you can't handle this responsibility, I will have to find another architect. P.P.S.: Perhaps what I need is not a house at all, but a travel trailer. Please advise me as soon as possible if this is the case. Autor: unbekannt Quelle: \"http://www.mal.com/~jreid/stories/architects\" toHTML: 1995-07-22 - Gunnar Anzingerletzte Änderung: 1997-06-11 -- Gunnar Anzinger",
    "commentLink": "https://news.ycombinator.com/item?id=39449424",
    "commentBody": "If architects had to work like programmers (1995) (gksoft.com)289 points by cebert 7 hours agohidepastfavorite103 comments teeray 5 hours agoAlso, I don’t care how you accomplish this work, but you must atomize your anticipated work into individual bite-sized and estimated tasks. Your estimates need not be accurate, but you will be held accountable if you run over your estimates and be met with suspicion if your estimates are arbitrarily deemed too high. You will be left to execute these tasks as you see fit, but you must report progress on them daily in a one-hour meeting along with every other architect working on entirely unrelated work. You may be asked to repeat this same oral update in other meetings. These meetings may be time consuming, but you will still be expected to meet your time estimates. During this project to design my house, there may be periods of time when I may require you to assist with architectural emergencies, such as stabilizing the Leaning Tower of Pisa. These emergencies supersede your work and may come at any hour of the day or night, but should not impact your time estimates. reply happymellon 3 hours agoparentYour pencil to draw the diagrams will be managed by a completely different person, you are not allowed to sharpen it yourself. However that person may not be available as they will be sharpening everyone else's pencils, as well as adjusting desks. They are all from Blue Pants, so won't work the same time shifts as you reply archerx 34 minutes agorootparentMy last job was almost this. My question is how does it get to that point? reply pyrale 5 minutes agorootparentMy best explanation is that some people believe that project management can be completely oblivious to the nature of the work to be done, and still be useful. To them, maximizing their ability to make choices about the way work will be done means they have more ability to take the good decisions that will lead the project to a successful end. reply sage76 18 minutes agoparentprev> you must atomize your anticipated work into individual bite-sized and estimated tasks Based on a 2 line description from a product manager, so you actually have no clue on the scope of the task until you start working on it. This is how at my last company, my team got every estimate wrong and engineers got fired. Yes the salt is real. reply vrosas 5 hours agoparentprevYour firm’s partners have hired BCG to streamline the home building process. Your home and every other one in the neighborhood that your coworkers have been designing will be left to rot and most of you will be let go. Some of you will be reassigned to design horse stables and indoor pools for a more profitable market segment. reply reactordev 4 hours agorootparentYour neighborhood was sold to an HOA. All houses must conform to new rules, reporting, and ever increasing fees until all revenue is squeezed from the premises. Violators will be fined Fibonacci derived value amounts or forfeit their property to the HOA. If you are still around by this time. Your equity is worthless. reply RCitronsBroker 3 hours agorootparentFibonacci derived values is wayyy too un-arbitrary reply qprofyeh 2 hours agoparentprevThere isn’t a more accurate description of “corporate agile” than exactly this. It’s a joke, and the joke’s on us. reply ip26 4 hours agoparentprevIt's suddenly not ridiculous at all if you map PM to general contractor and SWE to tradespeople. reply fatherzine 5 hours agoparentprevnext [–]the building must make $10b/year in revenue, or else you are all fired. reply blkhawk 2 hours agorootparent.... more than last year or you are all fired. reply mgoetzke 3 hours agoparentprevThe last part is what constantly destroys me :) reply ZaoLahma 3 hours agorootparentFor extra amusement, make it so that you have responsibility without authority. I.e. you know what is broken and roughly how to fix it and in everyone's view it absolutely is your responsibility to \"git'erdone\", but you can't since you either don't have access or resources to do what needs doing and you won't get the access or the resources due to office politics or some other equally unfixable issue. That's a fun little stress-biscuit to eat. reply adrianmsmith 1 hour agorootparentOr you literally aren't allowed to fix it, while it also being your responsibility to fix it. For example you see are responsible for fixing problem X, so you go to do it, and are told that X isn't on the planning board this month (because problem X hadn't occurred yet when planning happened at the start of the month) therefore you aren't allowed to work on it. But in other meetings, it's your fault it's not fixed, X is your job after all. reply dxxth 1 hour agorootparentprevThis is very real in the context of many locked down, tightly-regulated industries such as DoD manufacturing gigs with poorly-managed webs of IT and Engineering infrastructure. Data governance and security, change and configuration management, are all cross functional teams with their own political priorities, cultures, budgets, and requirements. Sometimes even separate tools anf platforms with zero integration or worse, financial redundancy. It is pain to work in such an environment. reply whstl 1 hour agorootparentprev> For extra amusement, make it so that you have responsibility without authority. Ah! The good old recipe for burnout. reply oivey 5 hours agoprevThis is really some next level victimhood. Building houses often does involve dealing with whiny homeowners/builders with bad taste and no clue who want everything but also don’t want to pay. That’s the job. Good software engineers know working with stakeholders and users is also the job. I’ll add another shocker: sometimes blueprints are poorly specified and/or incorrect. And yet, people build the houses. That’s the job! reply twelvechairs 4 hours agoparentYes. Conversely if programmers had to work like Architects they'd be paid a fraction, not get promoted to run serious projects until they are 50+, work copious unpaid overtime, not be allowed to work from home, be held legally liable for their work, spend most of their days focussing on compliance rather than outcomes, be more client focussed than they have ever considered regardless of above, etc. reply throwitaway222 4 hours agorootparentSome architects get hired to do a 50 unit apartment complex for 200k that only takes them a few months. The owners of those businesses farm it out to a junior to do the entire thing, then look over it quickly. Said junior usually only does this for a maximum of 10 years and graduates to owning their own business. However 85% of them graduate to running a drafter business which just draws plans. reply darkwater 12 minutes agorootparentBUT if there is a serious issue in the houses built by following the project you had signed, well, you will go to jail (or pay the fine). I still have to see a software engineer held PERSONALLY accountable, in front of the law, for a PII leak, for example. reply freddie_mercury 3 hours agorootparentprevThere is not enough industry growth for every junior to own their own business in 10 years. reply whatshisface 3 hours agorootparentThere is if the average size of said business is between 2 and 1. reply wwilim 50 minutes agorootparentprevAnd be used as factotums if the investors ever notice they're good at getting things done. The amount of times my dad was asked to drive 5 hours somewhere to deliver a giant stack of paperwork to a local administration office to secure a building permit... reply oivey 4 hours agorootparentprevYes, and, if they wanted to just build to a spec like a construction worker, they’d get to look forward to getting paid even worse still, physically demanding labor, working in the hot, working in the cold, shitting in boiling portapotties, and shitting in frozen portapotties. reply watwut 1 hour agorootparentprevProgrammers are rarely promoted to run serious projects. Many work a lot of unpaid overtime. Many of use spend our days focusing on compliance rather then outcomes. reply magicalhippo 3 hours agoparentprev> I’ll add another shocker: sometimes blueprints are poorly specified and/or incorrect. A family member bought a plot, some blueprints for a two-story house from an architect firm and hired someone to build it. Well into the construction, the builders asked if he would like them to raise the roof of the building by one meter, as it would be a negligible increase in cost. He was going to reject the offer but changed his mind and accepted. Once completed he realized that if he had rejected the offer, he wouldn't be able to use the second floor at all. The house had a Gable-style roof[1], and the stairs a U shape along one of the outer walls. Had he not raised the roof, one couldn't have walked up the stairs due to the roof being so low along the outer walls. Even after raising the roof, tall people would still need to tilt their heads going up. [1]: https://en.wikipedia.org/wiki/Gable_roof reply dasil003 3 hours agorootparentI love that they just asked it but they didn't explain why. reply magicalhippo 2 hours agorootparentYeah I've always assumed they figured this wouldn't work, but ya never know... The guy did get the drawings independently checked BTW, they were indeed drawn with the roof too low. reply whstl 1 hour agorootparentMaybe the builder assumed they noticed too, and if they had said no the builder would try to explain. I only ever did one big renovation and built one house, but I've seen builders fixing mistakes from engineers and architects sooo many times. reply yen223 4 hours agoparentprevThe charitable take is that this is a way to tell architects what life as a programmer is like. (I 100% believe that if an architect read this article, their takeaway would be that programmers have it too easy) reply t_luke 2 hours agoparentprevIf Programmers Had to Work Like Architects - you aren't allowed to do any programming yourself, you just write a specification - the majority of the people doing the programming are incapable of reading the specification - many of those who can will deliberately ignore it to save money - nevertheless, it's your fault if it's realised incorrectly reply carlmr 2 hours agorootparentThis sounds a lot like normal software architect work at big corp. You write the spec, it gets sent off somewhere cheap, the people there lied on their CV or got their degree from a diploma mill, they will ignore your spec and make all the tests green instead of checking if they do something, and then you need to take responsibility. reply baud147258 1 hour agorootparentprev> If Programmers Had to Work Like Architects According to my brother who work in construction, architects are often clueless on how to build stuff and existing material limitation, especially with the money he's given. reply blauditore 54 minutes agorootparentI get the same impression: There are many head-in-the-cloud architects who see themselves as artists. The equivalent totally exists in the software world; it's people who want to be pure \"software architects\", designing what others should implement. In my experience, this dictating mindset never works - designers (technical or not) should evolve their ideas with the implementors/builders, otherwise such disconnects happen. reply timeon 1 hour agoparentprevStill there is one difference. Architect needs to design not only within client's and technical constraints but also within legal regulations. reply blauditore 1 hour agorootparent...and so do programmers, increasingly so nowadays (at least it's very tangible in Europe). reply whstl 58 minutes agorootparentAlso true for a while for some industries like finance. reply ozim 3 hours agoparentprevNot really. Most of developers jobs are “here is Jira ticket build what is written there”. You get product owners, business analysts, scrum masters that should take away 90% of BS. But still it is not the case and a lot of those business roles seem like they are just useless and I would do much better job directly talking to the customer. reply rurban 2 hours agorootparentYou are a bit naive here. Those type customers are extremely common, and there are specialized jobs to deal with such customers. In SW they are the PM's. Without a good PM to shield you from the customers insanities you are lost. In architecture or other engineering professions it's the same. As architect you have a special man to deal with politicians and special clients, and in many other professions even with the press, who has no idea either. Never talk directly to the customer. Or if so, you are not allowed to make promises. reply runtu 1 hour agorootparentYou shouldn't be talking to the customer to hear what monstrosity of an application they think they want months from now, or even what \"features\" they think they want added. You should talk to the customer to understand the problems they have, and work with them to figure out the smallest thing that might help them solve their biggest problem. Build and deliver that small thing fast, then iterate. reply cortesoft 4 hours agoprevThis sounds like a perfect example of a \"cocktail party idea\", with programmers thinking they know how other fields operate. I am sure an architect could write a similar post about programmers, with just as many false assumptions and misunderstandings about what the job actually takes. https://danluu.com/cocktail-ideas/ reply endofreach 4 hours agoparentThat would actually be a very, very interesting read. reply krisoft 40 minutes agorootparentI don't have an article for you, but this is a topic I discussed with a friend who works as an architect of buildings. His main points were: - programming is way easier because you get instantaneous feedback[1]. When he has an idea it will take sometimes tens of years for it to be realised. (if ever) - the rules they operate under are not deterministic. They might design a building which is totally fine under one interpretation of the regulations and fails under an other. Sometimes what actually changed is not even the interpretation of the rules, but things outside of their control such as the political favours of the investors behind the building. If plan reviewers want to find some problem with your thing they will. - builders replace materials and techniques often, sometimes even without discussing it with the architects. In programming you don't have to worry that your compiler cheapens out and replaces the doubles you declared with single floats. - With programming if you don't like what you made you just rewrite it[1]. With his line of work once you know know that something is wrong it is way too late to change anything. Heaps of money has been spent and years are passed. Because of reputational and liability reasons this leads to a mindset where you are unlikely to accept that there was ever anything wrong with your idea thus architects become solidly set in their thinking. - Everything he thinks is mediated through layers and layers of other people. If something goes wrong he can always blame the builder, or the owner or the occupier. This leads to even less honest self-reflection. 1: A common theme in all of this is that what he has experience with is very small scale coding. He knows that a compiler provides instant feedback on syntax errors and he thinks that is all there to software development. We who work in the industry know that feedback loops are not always that fast in Software Engineering , but those parts of the work were not experienced by him. reply AdamN 19 minutes agorootparentInteresting feedback. As you rightly point out, in major tech projects most of these advantages to software don't hold up (determinism, rapid feedback loops, etc...). So I guess everybody is squeezed no matter where they are :-) reply YetAnotherNick 4 hours agoparentprev> I am sure an architect could write a similar post about programmers Yes and I would like it. reply eastbound 3 hours agorootparentYes. Try justifying jobs paid 700€/day every day for years. reply psychoslave 10 minutes agoprevRelated links: https://www.reddit.com/r/programming/comments/5z1vh/if_archi... https://www.reddit.com/r/programming/comments/9hwmh/if_archi... https://www.linkedin.com/posts/namanaggarwal_if-architects-h... reply kmoser 5 hours agoprevAlso, I am a visual and tactile person so I would need you to build a mock-up of this house in advance, preferably 1:1 scale, and fully functional so I can see how it will function. If I am dissatisfied with any aspect of the house, I expect you to rebuild it from scratch, only faster since you have already had practice building it once before, so how hard could it be to build it again while incorporating my changes? reply Groxx 4 hours agoparentWhat is this, a house for ants? How am I expected to live in this? Scale it up! It's already built, so I expect it on the lot tomorrow. reply 6510 4 hours agoparentprevOdd that no one mentioned my favorite? The house must be rebuild in the same location while I'm using it and the transition to the new house must be seamless. The garage must be rebuild with the car in it, the kitchen floor and counter top must be replaced while the dishwasher and the oven are running, I must be able to shower and stay in my tub during the bath room replacement, you must rebuild the bedroom discreetly while I'm having sex, toilets must be rebuild while in use. reply Groxx 3 hours agorootparent\"hot-swappable toilets\" was not a mental picture I expected to have today reply ZaoLahma 1 hour agorootparentLet's hope they at least allow you to wait for a flush to pipe so you won't have to carry over the user's transaction. reply timeon 54 minutes agorootparentprevNot sure what your point is. It is called reconstruction. Also architects often need to adjust design while building is under construction. Deploying code is not comparable to what you have described. reply jabits 3 hours agorootparentprevHilarious reply rovek 4 hours agoprevFun but I prefer the previous version about a bridge building team https://www.stilldrinking.org/programming-sucks - Second section reply ako 2 hours agoprevIt’s wrong to think programming is similar to construction. Creating new products consists of a product design phase and a product manufacturing phase. The first part finishes with a precise and tested design (so probably needs (digital) prototypes that can be tested), manufacturing just creates those design with as accurate as possible. Architects and software engineers are both part of the product design phase, software engineers deliver the first version of a product that can be tested (that can include multiple iteration, and new versions of an already finished version of the product). Product manufacturing in IT mostly consists of getting a copy of the product to the end user, either by creating an actual copy, or by allowing all users to have access to the final version created by product design. Software engineering is part of design, you’re part of getting the requirements and design final, don’t expect just to manufacture according to finalized designs. Architect simply look at the bigger picture design, components and interfaces, whereas engineers have a smaller focus. Architect are usually just engineers with more experience so they have more experience with the bigger picture design. reply osigurdson 16 minutes agoparent>> Architect are usually just engineers with more experience I don't think that is true at all. These are distinct fields. reply MASNeo 4 hours agoprevFurther, please ensure strict privacy of anyone entering the house but at the same time allow for good communication among everyone. Also, make sure only authorized people can enter or see what’s happening inside and keep everyone very safe from fires, physical harm or other people. Unfortunately, the safety must be accomplished without additional cost or restrictions in use. reply lr4444lr 53 minutes agoprevOnce \"Agile\" methodology effectively won in this industry, any hope of actually bringing software development into an \"engineering\" discipline was dead. I'm not saying there aren't positives to doing things that way, but it's got a lot to do with why the satire of this article rings true. reply isoprophlex 1 hour agoprev\"If you tell me how much time a task will take, please don't specify the time in hours or days, but use these made up 'points' that mean different things to different people\" Also, many commenters saying this is in bad taste, badmouthing architects or assuming a victimized 'hurr programming is so hard' stance. I read it differently, as a critique of the software industry itself, about how we're utterly unable to get our clients to understand the realities of our work. Noone in their right mind would ask for a house with 2-to-42 bedrooms. Yet the average IT worker somehow accepts this as normal in their software work. It would behoove us to get our clients to understand this, and not delegate that task to the scrumlords... who generally only make things more confusing and complex. reply tdudhhu 1 hour agoprevIf you read the work of Christopher Alexander you will notice he is proposing a much leaner method for building. For example: start by marking the position of the front door on the property and lay out the living room with sticks and wire so you can decide where you want to look at while sitting on your couch. And I believe he was right. Building lean can be done but does not fit in the 'architectual way'. reply doubloon 5 hours agoprevmost of any job is listening to people. doesnt matter if you are a president or a janitor. people want things but they dont know exactly how or why, most of all they just want to feel that you have listened to them about their problem. and then done your best to help them. reply jq-r 50 minutes agoparentWhile this sounds nice in a vacuum, I don't think that the reality for many people over here. Its usually: \"our product needs X, get it done\". You can listen to that manager for hours or days, use your psychiatrist hat to extract more useful info, renegotiate requrements etc, but that doesn't save you from probably weeks or months of just hard work. I would understand if one is a contact person for some account. And then you listen to your client as best you can, write up a doc/ticket/whatever for someone else to deal with that. A janitor who listens most of the time isn't really a good janitor so this is more much less universal than it sounds. reply bruce511 5 hours agoparentprevThis 100%. We've discovered, especially in small customers, it's worth finding out if the payer, and user, are the same person. In some places the new house is being built for folk who are happy with the old house. And if the users don't want to change they can sabotage the project away. It's worth finding out, before any money is spent, just how devoted to thd project the owner is. As in, if push comes to shove, who gets fired, the architect or the monther-in-law? reply waynesonfire 5 hours agoparentprevyes, i need to feel heard. thanks for the specs and see you at tomorrows standup! reply hooby 3 hours agoprevThat's a very funny take - but I wonder how architects feel about it, because I'm pretty sure that there is similar stuff happening when designing houses... The biggest difference though probably is, that the architect who creates the blue prints - will not be involved in the actual construction work. Therefore the blue-print (which the customer signed off on) has to be ground truth. reply brudgers 3 hours agoparentIt’s similar, because clients are clients. But with a good chance of not being paid. And with direct personal liability that cannot be shielded with a corporate entity; and professional license requirements; and building codes enforced by governments… And of course if you fuck up, people may die. The article captures another aspect of architecture as well: ordinary people assume living in a house and using buildings gives them informed opinions on architectural matters. But that’s just people. reply moooo99 2 hours agorootparentBut isn’t the risk you’re describing mostly tied to the actual engineer implementing or adjusting the design the architects come up with? No clue how that works in the US, but here most of the calculations are being done by an engineer with a different education than an architect. reply elteto 5 hours agoprevOh so this guy has a spec? Must be nice. reply rurban 2 hours agoprevkommt hin. I worked both as professional architect and programmer. Thanksfully, I could tame my SW clients, but heard enough stories of totally incapable PM's who accept such clients. But we also had similar \"Bauherren\", esp. in politics. German politicians are famous for demanding the \"eierlegendewollmilchsau\" in construction. I even had specialized jobs to deal with such clients, such as e.g. in stage design. There was a whole SW VR project to be able to show the client his absurdities beforehand. reply andy_ppp 2 hours agoprevWell, knowing an architect quite well they have exactly the same issues as we do of poor specifications that change, quite often due to unforeseen things on site. While a single person might be smart, people in general are idiots. reply robertlagrant 1 hour agoprev> At this time, for example, it is not appropriate to be choosing the color of the carpet. However, keep in mind that my wife likes blue. This made me chuckle. Captures the mentality perfectly. reply MeImCounting 3 hours agoprevHaving worked in construction/contracting I can say that architects usually get badmouthed on the jobsite for being totally ignorant of the actual intricacies of construction and finishing. Obviously this isnt always true but it still made me smile to see something that could be interpreted as a bit of goodhearted architect bashing on HN tonight. reply croes 4 hours agoprevArchitects can't simply release a patch or update after they delivered, so programmers have it better unless you program for NASA or medical devices. Same with any craftsmanship. If they botch something they have to start all over again, sometimes they can't. Compared to that being a programmer is easy. reply osigurdson 12 minutes agoparent>> Architects can't simply release a patch or update after they delivered Some changes are trivial, others are like moving to a new standard for railroad widths. reply fendy3002 3 hours agoparentprevcounterpoint: because it can be patched / updated, client will want it patched / updated. The worse thing on programming, it's hard to estimate a task because it's very dependent on the software architecture. If we're talking about building, you'll know the estimates of adding a sink on a specific room / floor vs adding another floor on the roof. Both need to be reviewed by the blueprints and whether the foundation support another floor if we're talking the later. In software it should be similar, however most of the time management isn't aware of the software architecture and the challenge to make the change. Adding a button to change some value may take either hours to days depending on the architecture, same with adding a sink may take days to months depending on the plumbing blueprint. Which is why in software, sometimes management ask you to add 50 floors to an existing 100 floors building and simultaneously change all the electricity placements on all floors, in under 3 months. reply bentobean 1 hour agoprevThis, but each sentence as a separate Slack DM with its own corresponding “Whack Whack” sound. reply NoPicklez 3 hours agoprevWhat is this post trying to achieve. To me these \"what it's like in comparison to x\" post just try to exaggerate and overblow the Programmers life whilst downplaying an Architects. Almost all of these points just illustrate that every project based profession has similar pain points. It's also ignorant to assume you know what it's like to be an architect or what another profession is like for that matter unless you have worked in both fields. reply demondemidi 5 hours agoprevThe person who wrote this doesn't know any building architects who serve the wealthy. Rich people who want custom homes often want to design it themselves, and then get extremely annoyed when confronted with the reality of basic design principles, usability, materials, structural integrity, etc. And then like to change their plans last minute once they actually start to see it framed in (assuming they don't freak out because they've never seen framing before and don't realize it isn't done yet). Or perhaps one of their rich friends made a glib comment or a jab while being shown the foundation and now the customer wants both their kids to have their own recital hall, because one isn't enough. Another stellar example: someone wanted a garage put above their kitchen because they wanted to park their ferrari next to their bedroom on the second floor. Damn the exhaust fumes. reply angarg12 4 hours agoparent> someone wanted a garage put above their kitchen because they wanted to park their ferrari next to their bedroom on the second floor. You mean this guy? https://youtu.be/Us8mDKUaX2M?si=qnrckuGLyWzoFNAe&t=1515 reply maximus-decimus 1 hour agorootparentI love how he's saying it's convenient because that gives you 2 parking spots... as if they can't afford to have room for a second parking space that doesn't block the car you put on the second floor while there's a tonne of empty space just in front of the garage door lol. reply defrost 4 hours agorootparentprevAt the time, aspirationally, maybe. The next morning, after they sobered up, more like this guy: https://youtu.be/fqgrOl1q9p8?t=5 reply rurban 2 hours agoparentprevAn architect friend of mine had a special job for special clients, who turned out to be Saudi wives. They had good ideas, but their ideas changed every month with the newest update of their architectural magazine. So she went down to Riyadh or Mecca every other month to explain them the details of the design involved. The crazy thing is, that there were a couple of wives involved, not just one. And a couple of new high gloss architectural magazines. A lot of models had to be built. But this is common. Hitler as another rich nightmare customer was famous for adoring Speer's models, and changing his mind constantly. He always knew better. reply aorloff 5 hours agoprevWe are not far off from the point where these kinds of constraint sets can produce designs. JitX is doing this for circuit boards. Houses are probably simpler, although people are more complex about their home. reply spintin 2 hours agoprevThe biggest thing missing from this is you have to build a brick house with a hammer and nails or a wooden house with mortar. The premise of micro-services where lost in complexity, the whole point always was \"you should be able to select the tools you like\". reply AndyPa32 1 hour agoparentAs an architect and tech consultant I mostly advise against going all in on microservices. And when I do recommend them, tooling is never part of the argument. Organizational and team structures are. reply swozey 4 hours agoprevIf I had to spend the amount of time and consideration on every merge to main that an architect probably has to spend on every minor change that might lead to a safety or regulatory or, whatever else you can come up with had to, I'm sure I'd get absolutely no work done and be a ball of stress. I'm incredibly grateful that I work on cattle not pets, or humans. reply rightbyte 1 hour agoprevI used to laught at these joke chain mails. E.g. the \"If programmer made cars\" chain mail. But the sad reality is that programming mispractices have spread like a plague to other engineering discliplines. Even total BS like agile have been forced on other poor engineers in totally different sectors. So programming didn't evolve into an engineering discipline, we brought everyone else down with us! The royal we, as in The Man, by the way. I did nothing wrong. Leave me alone. :) reply mcapodici 3 hours agoprevSounds like an episode of grand designs. reply MountainMan1312 7 hours agoprevAs a handyman who does small repairs and remodeling, this is exactly how it is. reply vrosas 5 hours agoparentYeah I’m on the client side of this now with a new home, trying to vaguely describe a vision to contractors who have great questions about problems and decisions I hadn’t even thought about. I’m sorry for myself. reply raldi 4 hours agoprevAlso I expect you to mow the lawn every two weeks and replace roof shingles whenever necessary. reply andrewstuart 3 hours agoprevIt's a false analogy. Programming has a lot more in common with portrait painting or sculpting than building something physical like a house. It is very hard and expensive to change physical things, software is relatively easily comparatively to change. Because software is changeable and pliable, it is practical to not make all decisions in advance. reply issafram 4 hours agoprevHe was being too nice reply Semiapies 3 hours agoprevAlso, I want it to do everything that this really cool house I saw does. But, I only want to pay for a day or two of labor in order to accomplish every feature that massive contruction project had, plus this one key feature that that house didn't have. What? You can't do that? My husband/ son/ nephew/ gardener knows how to draw with pencils, I can have them do it if you won't see reason! reply dkarl 5 hours agoprevIf? reply jillesvangurp 3 hours agoprevA common misconception with software architecture and software design is assuming that this is somehow separate from your program and something you create before you start programming. In most projects, this is limited to some preliminary scribbling on a whiteboard and some random musings in a wiki or issue tracker. But it used to be that people were very serious about waterfall and wasted lots of time on this. But if you think about it, an architectural drawing for a building is essentially an executable recipe for building a house that the construction team uses to build it. I.e. it's actually very similar to a program. It tells the team of builders all they need to know to build the building. In the software world, the blue prints actually are executable. That's the whole point. The job of programming is essentially just creating a very detailed blueprint in some language. The compiler/interpreter then generates the executable from this blue print. The only difference here is that we replaced the team of construction workers with another program the construction process is automated and does not involve any people. This wasn't always the case, the word compiling refers to people stacking together punch cards in the right order. The first computers were humans flipping switches, doing calculations, and messing around with cables, punch cards and what not. The word debugging refers to removing actual live (or dead) bugs from circuitry. Writing a program used to mean creating a plan to task these people to do all these things. Having a plan for that is kind of crucial. Exactly like having an architecture blue print for a building is important. That's what a program is: a blueprint for creating/generating instructions that a computer can work with. Ada Lovelace, widely recognized as the first programmer, never even had access to a computer. She was designing software for a machine that did not yet exist. But she was a programmer and not an architect. Having a separate program for producing the program just isn't a thing (well except for meta programming of course). Just like having a blue print for a blue print for a bridge or a building is not really a thing. There's just the blueprint. And just like programmers have to do a lot of problem solving while they create their blue prints (programs), architects have to do a lot of problem solving while they are creating their blueprints. It's this problem solving that make their jobs hard and unpredictable. Once you have the blueprint, things get relatively straightforward and predictable. But before that, buildings are just as risky and unpredictable as software programs are. You have to deal with requirements, budgets, regulations, flaky customers, etc. This can get really complicated and risky. That's why large engineering projects run over budget so often. And ironically, agile engineering is actually a thing too now. E.g. SpaceX iterates on their rockets rather than designing them years in advance: real world engineering is learning from software engineering. reply LAC-Tech 3 hours agoprevI love it. Any more of these? I love 80s and 90s programming humour. Real programmers don't eat quiche, story of mel, etc. reply petre 3 hours agoprevIf Architects had to work like this, you'd likely earn a reputation as a cheapskate and get the \"sorry but we're busy with other projects\" line, or get the design for a barn from some desperate junior architect. reply ninetyninenine 5 hours agoprevThere is merit in the opposite concept where programmers are working like architects/(other engineers outside of programming). The problem is nobody cares about that style because although people die when a building falls, nobody dies when a program crashes. At least most of the time, nobody dies. reply Ygg2 5 hours agoprev [–] Honestly not complicated enough. PPPS. House will might be sent to the moon or Mariana trench. Make sure it can sustain these pressures. PPPPS. This goes for trailer as well. Additionally we need a plan to deploy it on a neutron star. reply kmoser 5 hours agoparent [–] Assume it will be a spherical house in a vacuum. Except when it isn't. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The letter presents architects with conflicting and unrealistic demands for designing and constructing a house, such as vague specifications, budget limitations, conflicting preferences, and a tight deadline.",
      "Architects are advised to accommodate various family members' opinions, follow contemporary design and materials, and appeal to potential future buyers.",
      "The letter concludes with a warning of seeking a different architect and hints at a travel trailer as a more fitting alternative."
    ],
    "commentSummary": [
      "The article and comments highlight the challenges and similarities in project management, client communication, liability, and adaptability between architects and software engineers.",
      "It discusses the importance of collaboration, understanding client expectations, and navigating regulatory challenges in both fields.",
      "The conversation emphasizes effective communication, collaboration, and problem-solving crucial for managing complexities in construction and software development projects while mentioning the implications of project management decisions and potential consequences of mistakes in each industry."
    ],
    "points": 289,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1708481487
  },
  {
    "id": 39439771,
    "title": "Transform your neighborhood with AI on Google Street View",
    "originLink": "https://googlemapsmania.blogspot.com/2024/02/ai-your-home-on-street-view.html",
    "originBody": "Monday, February 19, 2024 AI Your Home on Street View Have you ever wanted to radically alter the ambiance of your neighborhood? Perhaps you've always dreamed of turning your sleepy suburban road into a bustling inner-city street. Or maybe you've always wanted to dig up your nearby traffic heavy roads and replace them with green fields and trees. Well now you can - at least virtually. Panoramai is a new fun tool which allows you to grab Google Maps Street View panoramas from any location in the world and change their appearance based on your own AI prompts. For example the animated GIF above shows my childhood home re-imagined as a Vincent van Gogh painting, as a sc-fi landscape, a post-zombie apocalypse and under 3 feet of water. You can also change the appearance of your home on Street View using the Netherlands Board of Tourism's Dutch Cycling Lifestyle map. It is a matter of great sadness to the Dutch people that people in the rest of the world are not able to live in cycle-friendly environments. Therefore the Netherlands Board of Tourism decided to help the great car-worshiping unwashed picture the beauty of a car free environment. Enter your address into the Dutch Cycling Lifestyle and you can see how your street might look without that noisy road and those dirty cars. Like Panoramia Dutch Cycling Lifestyle uses an AI to alter the Google Maps Street View image of your street - only in this case to make it a little more Dutch. The result is an imagined view of your road, looking a little greener and probably a lot more attractive. Wait! There is even more fun to be had with Google Maps Street View. Thanks to Street Galleries, you can also create your own virtual outdoors art gallery with Street View. Street Galleries is a Google Arts & Culture project which allows you to decorate cities around the world on Google Maps Street View with works of art from some of the world's leading museums. You can choose from one of ten locations in a number of the world's major cities. Once you have chosen a location, you can begin adding paintings to the Street View of that location. Pick a painting from the Google Arts & Culture digital collections and you can hang it anywhere within your Street View panorama. You can move the painting around in the Street View, allowing you to hang the picture on a building, on the road or even just leave it hanging in mid-air. Hat-tip: Webcurios Posted by Keir Clarke at 3:33 AM Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest Labels: AI, Street View No comments: Post a Comment Newer Post Older Post Home Subscribe to: Post Comments (Atom) Blog Archive ▼ 2024 (52) ▼ February (19) The World's First OpenStreetMap The Chain Restaurants of America AI Your Home on Street View The Right-Wing Terrorism Map The Best Price Comparison Maps Alien Arrivals Nosedive in 2023! The Sad State of Local News 2023 OpenStreetMap Edits in Real Time The Classic Map Arcade The Most Controversial Interactive Map Mappy Races Where to Watch April's Solar Eclipse A Year of CO2 Mapping the Spread of War in the Middle East The 12 Best Daily Map Games Using AI to Detect Oil Spills Car Commutes are Getting Longer Mapping Gaza's Destroyed Buildings Mapping Oil and Gas Emissions ► January (33) ► 2023 (316) ► December (24) ► November (28) ► October (28) ► September (26) ► August (28) ► July (26) ► June (28) ► May (27) ► April (24) ► March (28) ► February (24) ► January (25) ► 2022 (303) ► December (22) ► November (29) ► October (26) ► September (25) ► August (25) ► July (26) ► June (28) ► May (23) ► April (26) ► March (25) ► February (21) ► January (27) ► 2021 (445) ► December (20) ► November (27) ► October (26) ► September (36) ► August (44) ► July (42) ► June (44) ► May (43) ► April (39) ► March (46) ► February (38) ► January (40) ► 2020 (596) ► December (44) ► November (43) ► October (51) ► September (54) ► August (46) ► July (54) ► June (52) ► May (47) ► April (48) ► March (56) ► February (47) ► January (54) ► 2019 (689) ► December (43) ► November (44) ► October (62) ► September (54) ► August (61) ► July (61) ► June (57) ► May (53) ► April (51) ► March (68) ► February (66) ► January (69) ► 2018 (721) ► December (44) ► November (55) ► October (66) ► September (66) ► August (61) ► July (55) ► June (59) ► May (64) ► April (63) ► March (68) ► February (63) ► January (57) ► 2017 (600) ► December (43) ► November (54) ► October (62) ► September (57) ► August (54) ► July (62) ► June (57) ► May (49) ► April (47) ► March (43) ► February (34) ► January (38) ► 2016 (759) ► December (33) ► November (42) ► October (58) ► September (59) ► August (67) ► July (73) ► June (65) ► May (65) ► April (69) ► March (81) ► February (71) ► January (76) ► 2015 (977) ► December (58) ► November (68) ► October (72) ► September (81) ► August (69) ► July (94) ► June (86) ► May (79) ► April (93) ► March (102) ► February (85) ► January (90) ► 2014 (1005) ► December (77) ► November (82) ► October (83) ► September (86) ► August (80) ► July (92) ► June (82) ► May (90) ► April (97) ► March (83) ► February (73) ► January (80) ► 2013 (1000) ► December (71) ► November (79) ► October (89) ► September (94) ► August (99) ► July (81) ► June (70) ► May (101) ► April (77) ► March (78) ► February (78) ► January (83) ► 2012 (914) ► December (70) ► November (74) ► October (80) ► September (62) ► August (78) ► July (82) ► June (84) ► May (78) ► April (80) ► March (78) ► February (71) ► January (77) ► 2011 (994) ► December (64) ► November (75) ► October (90) ► September (80) ► August (93) ► July (83) ► June (92) ► May (90) ► April (96) ► March (91) ► February (58) ► January (82) ► 2010 (1206) ► December (80) ► November (84) ► October (82) ► September (94) ► August (115) ► July (115) ► June (129) ► May (113) ► April (100) ► March (104) ► February (95) ► January (95) ► 2009 (1077) ► December (90) ► November (96) ► October (94) ► September (98) ► August (94) ► July (106) ► June (90) ► May (74) ► April (94) ► March (91) ► February (75) ► January (75) ► 2008 (380) ► December (67) ► November (81) ► October (70) ► September (65) ► August (57) ► July (16) ► June (7) ► May (3) ► April (6) ► March (5) ► February (1) ► January (2) ► 2007 (16) ► December (1) ► November (1) ► July (2) ► June (1) ► May (3) ► April (2) ► February (4) ► January (2) ► 2006 (20) ► December (5) ► November (2) ► October (2) ► September (2) ► August (1) ► July (2) ► June (1) ► May (2) ► April (2) ► March (1) ► 2005 (6) ► November (1) ► September (1) ► July (2) ► May (1) ► April (1) Map Blogs Cartonerd weeklyOSM Google Geo Developers Blog Google Earth Blog Google+ Awesome Inc. theme. Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=39439771",
    "commentBody": "AI your home on street view (googlemapsmania.blogspot.com)243 points by chippy 23 hours agohidepastfavorite60 comments matsemann 22 hours agoI was gonna say something along that this would be cool to use to imagine streets built differently. Like with less traffic, cycle lanes, street vendors etc., and then they link to a Dutch website already doing something similar. Very cool. Things like these could be useful in helping to push decision makers and the public to see new opportunities. Right now when something is being built, it's always a optimistic 3d render on a sunny day with people laughing being shown to sway the public in favor of the project. Letting us \"normal people\" fight back against certain projects or suggest our own without needing to have professional architects draw a concept could be nice. reply Vinnl 14 hours agoparentLink for those coming to the comments first: https://dutchcyclinglifestyle.com/ reply loceng 18 hours agoparentprevRequired inclusion like this in publicly accessible Google Streetview interface would be ideal as well - otherwise there are some tricks that are used sometimes to make mammoth buildings look much smaller in perspective due to the angles (etc) they use and print on their promotional material in order to get less resistance from the public. reply sandworm101 15 hours agorootparentArchitectural rendering are all about tricks. Look closely and you will see the silliness. The human figures used in indoor areas are tiny, to make the interior of a building look large and spacious. The the people on the sidewalks of exterior renderings are huge, to make the building look small and innocuous. reply pxmpxm 15 hours agorootparentThere was a fun thing in London couple years back where for sale apartments would be staged with scaled down furniture for the same effect. reply sandworm101 14 hours agorootparentThere was an airline ad a couple years ago (Singapore??) that made a great joke of this. Airline ads are always full of tiny women in order to make the seats and windows look bigger than reality. The last shot of the commercial was of a stewardess reclining literally inside the window sill, a person who would have been maybe 18\" tall, or a window that was 5' tall. reply asdaq1312512 21 hours agoparentprevAre there already tools out there? Let's collect some. https://radwege-check.de Lets you compare bicycle lane designs, and how safe both cyclists and motorists feel. reply jozzhart 18 hours agorootparenthttps://www.betastreets.co.uk/ is a street design tool, uses basic AI for object removal from uploaded images reply CalRobert 19 hours agorootparentprevBetterstreets.ai made a splash when Dall-e was new, not sure what they're doing now. reply Tempest1981 19 hours agoprev> beauty of a car free environment. ... you can see how your street might look without that noisy road and those dirty cars. Beautiful. But I see that picture-perfect pathway, and wonder the cost and time to maintain the vegetation. Bicycling through California suburbia, I see mostly dead lawns, ever since the droughts began. Few people (or cities) have time/money/motivation to create beautiful gardens. So I'm imagining a dusty gravel pathway instead. reply not2b 12 hours agoparentWe took out our front lawn and replaced it with a drought tolerant garden with redwood chips, succulents and other drought tolerant plants. It looks much better than it used to look, and we got part of it paid for by money from the Santa Clara County water district (though we paid most of the cost). reply bombcar 8 hours agorootparentThey need to be maintained also; five or ten years in with \"minimal maintenance\" and the redwoods have weeds growing through them. Maintenance is hard. reply not2b 8 hours agorootparentIt's considerably less work than maintaining the grass lawn that it replaced, which needed to be mowed and weeded. Also, you use quotes here, but I'm not sure who you're quoting, it wasn't me. reply Foreignborn 19 hours agoparentprevWhile nothing is truly maintenance free, methods like hardscaping, xeriscaping and even permaculture planting methods like STUN (okay a harder sell but still) are easily doable. In the longer term, the emergent benefit is that bicycle friendly infrastructure incentivizes density which saves on money and maintenance. reply tokai 17 hours agoparentprevWhy do you assume that maintaining some bushes should be significant over maintaining a road? reply persolb 11 hours agorootparentSeems reasonable to me. I cut the simple bushes in front of my house back from the path much much more often than the township does anything with the road. Manpower wise, the bushes along the path probably take as much average labor as the road. Maybe material/gas to resurface the road every 10 years would make the road cost more per year. reply bombcar 8 hours agorootparentThere is a bike trail near our house, and the plowing and mowing and tree clearing along it takes way more man-hours than the once-a-decade resurfacing. Not sure what the comparative costs are, and a road would have similar maintenance if it was that close to trees (a bike path is just a narrow road). reply NegativeK 19 hours agoparentprevIf it's publicly maintained, instead of bushes and grass, it could be well designed xeriscaping with local plants and gravel/rock. That's basically what the front of our house is, with very minor drip irrigation (that I wish we would get rid of.) reply QuercusMax 13 hours agoparentprevLawns are incredibly stupid in California. There are so many more appropriate ways to cover ground. There are definitely choices better than a dusty gravel pathway. reply lIIllIIllIIllII 10 hours agoparentprevwhat you probably need is just more vegetation, and vegetation that is naturally suitable for the climate/microclimate, that will grow... naturally and without the need for upkeep. Like, if nature itself needs upkeep, something other than lack of upkeep is the issue. reply ssl-3 9 hours agorootparentIt still needs manual upkeep. Nature has a way of planting things that don't even necessarily belong on the same continent in spaces where they are undesirable. See, for example: Kudzu. We introduced it intentionally, and nature's own mechanisms have it literally growing beyond control -- blanketing (and typically killing) anything in its path in the US South, including mature trees. reply samstave 18 hours agoparentprevSoon: \"Enhance and beautify your commute with Vision Pro, now with automatic urban blight removal - gone are the days of seeing strip malls, now strip parks. Homeless encampments? Adult recreation areas! Litter? Flowers!\" -- But in serious, this will be amazing once Civil Engineers, Urban Planners, Landscape architects etc - can use this, but have it also calc \"cost of options\" for each - and have it do Environmental Impact analysis based on the plan, AND Title 24 implications... which are always fun to deal with especially in cities such as San Francisco. (Title 24 are the codes for efficiency, safety etc for new builds basically - but they can be a pain in the ass to navigate when planning in urban areas in California.. Environmental Impact studies cost a boatload and are usually lamented about for things like \"Protect the Frogs of Marin\" (which affected a lot of building aspirations especially in wealthy areas, such as Marin - but ARE very important when determining if a new development on top of some natural habit is going to F-up the surrounding ecosystem over the next N years) reply DoneWithAllThat 18 hours agoparentprevThere is no drought. A few very small sections of California are categorized as “abnormally dry”. That’s all. reply bayindirh 22 hours agoprevLooks like it's hugged to hiatus in a different way. The inrush traffic created considerable cost for them, so they have turned image generation off. reply vmax1 22 hours agoparentWe're working on putting it back online - should be up in a few minutes! reply vmax1 22 hours agorootparentIt's back up now reply elliottcarlson 22 hours agorootparentLittle feedback on the site; the site is very hard to use on my current laptop at 1920x1080 - overflow is hidden so I can't scroll to the button to perform the generation (and after disabling overflow hidden in dev tools I can see that's because the page is designed to be a very static height). Great concept though! reply vmax1 21 hours agorootparentThanks for the feedback :) Btw the code is open source and available here: https://github.com/vidalmaxime/streetview-diffusion reply Anotheroneagain 20 hours agorootparentCould you make it a bit more verbose? You really should provide some kind of feedback about what's going on. Am I waiting for something? Or is it broken? No information there. reply toyg 21 hours agorootparentprevHow long is the generation going to take ? It's been spinning for some time here (Firefox)... reply goda90 20 hours agorootparentDid you ever find out? I'm several minutes in and about to bail. reply posterguy 19 hours agorootparentopen the console and count the errors for yourself reply OccamsMirror 20 hours agorootparentprevDown again? reply readingnews 22 hours agoparentprevI wonder if this is: AI use in the real world is still early on, and so it is expensive. OR Whoa, slashdot (I guess I'm old, perhaps reddit) effect and we can not afford the bandwidth, quick turn it off! OR Same as above but with CPU costs. I am curious to know the limiting factor. reply bayindirh 22 hours agorootparentIt's \"An AI worthy GPU is scarce and its TDP is 450+ watts, so it's doubly expensive, turn it off!\". CPU's are way cheaper when compared to GPUs, and image bandwidth is at most \"mneh!\" in Slashdot or in The Register terms. tips hat Reported from a data center warmed by GPUs. reply dakial1 22 hours agoparentprevThey could do like everyone else and add ads or ask for donations reply chippy 15 hours agorootparentindeed, the dev is asking for donations (buy me a coffee link on top right) https://www.buymeacoffee.com/aurelien3 reply dudefeliciano 19 hours agoprevI would have though the service is suspended due to image generation being too expensive, but it looks like google maps quota has been exceeded reply edwinjm 22 hours agoprevDirect link to the site: https://dutchcyclinglifestyle.com/ reply wildrhythms 21 hours agoparentThe site is: https://www.panoramai.xyz/ The site you posted is mentioned in the article but only as a related project. reply yanslookup 20 hours agorootparentIs that the right site? Is it supposed to do something? I type a location in the search bar and then it just sits there saying waiting for location... reply robertlagrant 9 hours agoprev> It is a matter of great sadness to the Dutch people that people in the rest of the world are not able to live in cycle-friendly environments. Just flatten the land everywhere to make it as easy to implement as it was in the Netherlands, and hey presto! reply IshKebab 17 minutes agoparentDefinitely part of the problem but also definitely not the full story. E.g. in Britain plenty of people cycle despite the terrible infrastructure and hills. Besides, now we have electric bikes and e-scooters so hills shouldn't really be a barrier. reply soneca 19 hours agoprevCould the link be changed to the original site: https://www.panoramai.xyz/ ? Just too many ads in the OP reply amelius 16 hours agoprevI want an AI that better interpolates between different viewpoints in Street View. Because right now I often get lost when going from point to point. reply bmacho 21 hours agoprevRemove the people from your street, and it will look more livable. Sad but it's true, most people like a certain amount of people density / space, and more people than that is uncomfortable. reply teitoklien 18 hours agoparentIdk, high density housing is perfectly fine and healthy for people to socialize feel a sense of community. These days humans blame their environment too much, and themselves too little. Maybe a lot of americans have forgotten what it means to be a community and being neighbours. Its pretty fun and bustling to live in a place full of life and people, provided all members respect each other, do not make too much noise, are polite, etc. Its not high density that is the problem, its the norm in China, India, South East Asia, etc, they live perfectly happy lives, rank higher in community bonding, socialising, etc. It used to be true in America too (level of socialisation), maybe that needs to brought back again, instead of complaining about high density housing. reply alistairSH 19 hours agoparentprevMaybe in the densest cities. Out here in suburbia? Empty streets have a very dead feeling. This is especially true in office parks that are completely empty outside work hours. reply konschubert 21 hours agoparentprev??? A big empty road with cars and no people doesn’t seem very inviting to me. reply hardcopy 19 hours agoparentprevRemove the cars* from your street people != cars reply CalRobert 19 hours agoparentprevI love the street near me full of kids playing and riding bikes. reply pkamb 18 hours agoparentprevIt’s always the cars. reply RankingMember 18 hours agoprevdang, already hugged to death reply whyenot 12 hours agoprev> AI your home... \"AI\" is now a verb? reply fatkam 23 hours agoprev [7 more] [flagged] dang 22 hours agoparent [–] Could you please not post unsubstantive comments to HN? You've been doing this quite a bit, already, and we're trying for something different here. If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site more to heart, we'd be grateful. reply fatkam 22 hours agorootparent [6 more] [flagged] dang 22 hours agorootparentI'm not just talking about the GP comment but the dozens of other comments you've posted to HN so far with this account, many of which have been low-quality. reply KronisLV 21 hours agorootparentThe parent comment that you're replying to reads as a bit... mean in tone? So, to balance it out: your moderation work and also patience is appreciated. reply fatkam 16 hours agorootparentnext [2 more] [flagged] KronisLV 16 hours agorootparentNot really, I wouldn't endorse the disposition on anyone's part. I'm glad that for the most part HN is moderated in a way that encourages civil discourse, even in disagreement. Not making fun of each other, not ad hominems, not swearing or even trying to make people bad, like other sites on the Internet sometimes do. I've said plenty of immature or \"cringe\" things in my past and probably will in the future, so for what it's worth, I appreciate gentle nudges in the direction of being better and nicer to those around me. reply hunter2_ 21 hours agorootparentprev> not sure why it is such a mystery feature from people on this site What would be substantive would be to teach us that this can be done, rather than point out (based on dubious evidence, AFAICT) that we don't know yet as a means of one-upmanship. I don't believe you're trying to be mean, but I suspect that a vast majority of people would feel otherwise which is worth avoiding if you're able. BTW, lack of \"reply\" option is probably due to the delay feature (the duration varies based on certain criteria) and can be bypassed by clicking the timestamp, which loads the comment in its own page, on which there is no such delay. reply dakial1 22 hours agorootparentprev [–] That is a better comment Fatkam. But one question, your house is blurred because you (or someone) asked Google to do so? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Panoramia is a new tool enabling users to modify their neighborhood on Google Maps Street View through AI suggestions.",
      "Dutch Cycling Lifestyle tool converts streets into bike-friendly settings, enhancing the urban environment for cyclists.",
      "Street Galleries lets users adorn cities on Street View with artwork from prestigious museums."
    ],
    "commentSummary": [
      "The blog post examines AI's role in redesigning Google Street View, sustainable landscaping in California, urban planning hurdles, website usability, and AI interpolation challenges.",
      "Community members evaluate the significance of greenery, dense housing, and community cohesion, emphasizing adherence to forum rules and respectful interactions.",
      "Commenters praise the site's civil atmosphere, moderation, and offer suggestions for enhancements and resolution of technical glitches."
    ],
    "points": 243,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1708426081
  },
  {
    "id": 39440503,
    "title": "Hetzner's Diverse Server Options and Services",
    "originLink": "https://robot.hetzner.com/order/index/culture/en_GB",
    "originBody": "ORDERING Language: German English Country: Germany Choose countryPlease select your country to determine the VAT due. In this way, all prices can be shown individually for your country. Australia Greece Russian Federation Austria Hungary Singapore Belarus Ireland Slovakia Belgium Italy Slovenia Bulgaria Latvia South Africa Croatia Lithuania Spain Cyprus Luxembourg Sweden Czech Republic Malta Switzerland Denmark Netherlands Ukraine Estonia Norway United Kingdom Finland Poland United States France Portugal All Others Germany RomaniaThank you for your interest in the products of Hetzner Online. If you are not a Hetzner client yet and want to take over a server or Storage Box from another Hetzner client, please use the server transfer site or the Storage Box transfer site. To view the current order status have a look at our Order processing status site. Shopping cart We offer the following products: Top Offers Dedicated Server AX102 AMD Ryzen 9 7950X3D 16-Core \"Raphael\" (Zen 4) 128 GB DDR5 ECC RAM 2 x 1.92 TB NVMe SSD Datacenter Edition (Gen 4, Software RAID 1) 1 Gbit/s bandwidth more information... Price (monthly): from € 123.76 Setup (once): € 46.41 Dedicated Server EX101 Intel® Core™ i9-13900 24 Core \"Raptor Lake-S\" 64 GB DDR5 ECC RAM 2 x 1.92 TB NVMe SSD Datacenter Edition (Gen4, Software RAID 1) 1 Gbit/s bandwidth more information... Price (monthly): from € 99.96 Setup (once): € 46.41 Dedicated GPU-Server GEX44 Intel® Core™ i5-13500 14 Core \"Raptor Lake-S\" 64 GB DDR4 RAM 2 x 1.92 TB NVMe SSD Datacenter Edition (Gen 3, Software RAID 1) Nvidia RTX 4000 SFF Ada Generation 1 Gbit/s bandwidth Price (monthly): € 218.96 Setup (once): € 94.01 Dedicated Servers Server Auction Dedicated Servers AX (AMD) Dedicated Servers EX (Intel Raptor Lake) Dedicated Servers EX (Intel Sapphire Rapids SP) Dedicated GPU-Servers Dedicated Servers SX Dell PowerEdge™ Colocation Colocation Storage Boxes Storage Boxes BX Domain Administration Domain Administration",
    "commentLink": "https://news.ycombinator.com/item?id=39440503",
    "commentBody": "Hetzner GPU Server (hetzner.com)239 points by matteocontrini 21 hours agohidepastfavorite146 comments Hetzner_OL 11 minutes agoHi there, Thanks for helping to spread the news about our new GPU server, the GEX44. Please keep in mind that it’s not like we don’t want you to open an account – it’s more about safety & security. In general, we take extra precautions when it comes to verifying accounts and accepting new customers, as in these cases the potential of abuse of our ToC are higher. As a reliable hosting partner, we always take the time to listen to our customers and will do our best to find a solution for each case. So feel free to reach out to us if you need further assistance. --Katie reply aborsy 20 hours agoprevThe price of a brand new Dell precision PC with a core i9 13900 CPU (24 cores), 64 GB RAM and 2X2TB NVMe is probably around 2.5k$. It costs tens of Euros per month in electricity to run this PC. The hardware is good to go for some 3 years, before running into various issues (newer CPUs becoming much more power efficient, firmware updates getting less and less frequent etc). Consider also the cost of components failure, time wasted in dealing with hardware, etc. On hetzner it all costs less than 100$ per month. Doesn’t this imply that it’s better to rent than to buy? reply wongarsu 19 hours agoparentHetzner is not only able to get better volume discounts, they also spend a lot of engineering time bringing down costs, and have decades of experience doing so. They offer some Dell Servers for those that really want them, but most of their servers have a custom mix of consumer hardware, server hardware and in-house hardware (for example they use their own racking system), optimized to minimize lifetime cost in a datacenter. For example most servers use datacenter SSDs, but consumer CPUs. reply dist-epoch 16 hours agorootparentHetzner is buying ASUS consumer AM5 motherboards, but with the whole CPU + memory block rotated so that the memory sticks are horizontal instead of vertical for better airflow: https://www.youtube.com/watch?v=V2P8mjWRqpk reply justinclift 14 hours agorootparentThey also buy (at least) ASRock Rack motherboards as well. Saying that from looking at the dmidecode output from some of our dedicated server currently with them: Handle 0x0002, DMI type 2, 15 bytes Base Board Information Manufacturer: ASRockRack Product Name: B565D4-V1L reply cobertos 16 hours agorootparentprevDo you have more details on their own racking system? I couldn't find anything on this. reply wongarsu 15 hours agorootparentThe best resource I know is a tour of one of their data centers. Around minute 6 you get some nice views on their current-gen racks, and later in the video you see the back side, followed by the assembly of the servers. https://www.youtube.com/watch?v=5eo8nz_niiM reply brudgers 18 hours agoparentprevDoesn’t this imply that it’s better to rent than to buy? TANSTAAFL 1. Physical hardware is a useful abstraction. 2. At the end of a three year cycle, you can sell something you own. (This has associated costs of course). 3. Rents can go up. Terms and Conditions can change. Credit cards can expire or be cancelled. In other words, renting introduces a significant dependency. 4. You are your own most important customer. Statistically, you are not The Clouds's most important customer. When The Clouds has a fault, it gets resolved based on its business model. Engineering decisions are specific to a specific problem and all of them come with tradeoffs. reply chrisweekly 14 hours agorootparent> TANSTAAFL \"There ain't no such thing as a free lunch\" (I had to look it up, figure I'll save someone else yhe trouble) reply echelon 14 hours agorootparentIt used to be moderately common acronym back in the 90's and 00's. It felt like a blast from the past seeing it crop up again. reply nortonham 14 hours agorootparentprevthanks, I had no idea what that meant either reply k8sToGo 16 hours agorootparentprevYou forgot that Hetzner also has spare parts. reply brudgers 14 hours agorootparentAnd Dells have warranties. And both offer service agreements. And Hetzner is a straight line expense while a Dell might be a depreciating asset. And either both or neither may offer discounts off retail. And so on. Because that’s the nature of actual decisions versus online arguments. reply Dalewyn 17 hours agorootparentprev>2. At the end of a three year cycle, you can sell something you own. (This has associated costs of course). This is huge and often overlooked or undervalued. Renting means you have nothing when the contract ends, buying means you have something for your money spent. reply cduzz 15 hours agorootparentA 3 year depreciation cycle may be reasonable for some spaces but not others. I'm not going to lose sleep over not getting firmware updates; when they've got all the bugs on my stuff ironed out I don't want more firmware updates. I haven't done the spreadsheet work to identify the cross-over point for where old systems become uneconomical but when I've done it in the past for my heavily clustered workloads it's typically better to buy an off-lease server that's 3 years old and run it for another 8 years, than it is to forklift all my infrastructure every 3 years chasing the newest generation thing. Certainly the cloud's got some advantages, but running equipment until it's really old is also pretty good too. I'd bet that AWS doesn't throw older systems out just because they're old. reply Slartie 11 hours agorootparent> A 3 year depreciation cycle may be reasonable for some spaces but not others. Exactly. I am running a server at Hetzner which has a CPU that was discontinued about 7 years ago. I don't know the exact age of the machine because it was already used when I got it 4 years ago, but based on the CPU availability it's at least 7 or 8 years old, potentially even older. Nothing on that machine has failed in the last 4 years except for HDDs (the spinning platter type), which are immediately swapped when broken, RAID rebuilds, everything's fine. 3 years is no time for hardware nowadays. It can live much longer, especially if storage is solid-state. And the performance improvements often aren't substantial enough to warrant a swap within anything shorter than 5 or 6 years. reply CapeTheory 14 hours agorootparentprevDo you happen to have a preferred source for these 3-year old off-lease servers? reply tiffanyh 17 hours agoparentprevYes. This is what you expect prices to be when a vendor is buying in bulk. Cloud vendors should be like \"Costco\" (buy servers in bulk and can pass along those savings & this is what Hetzner does) The trick AWS (and other cloud vendors have done), is charge \"Uber Eats\" pricing for Costco items. reply fragmede 15 hours agorootparentThat's a really clever analogy, but is it really a trick? UberEats delivers the food to my door but if I go to Costco I have to make my dinner myself. Sometimes life happens and UberEats isn't just a convenience thing, it's the only way I'll manage to feed me and my crew. I could run a pubsub queuing system and a database myself, but if I don't have to do that, it frees me up to focus on the tasks I'm really trying to accomplish instead. reply sph 10 hours agorootparentThat's fine, but I think it is a bit reckless to tell everybody that grocery shopping is useless and we might as well order from Uber Eats three times a day. Which is basically what happens with everyone terribly afraid of managing VPS that these days it's only AWS, Azure and GCP. First the excuse is \"it's too small to bother doing it myself\", then it becomes \"it's too large to run it ourselves\". Some can afford 3x daily Uber Eats, but we should stop discouraging home cooking. It is not that scary. reply schroeding 19 hours agoparentprevIf you pay European (German) energy prices, this very well may be the case, in my experience. For some Hetzner servers (especially from their \"Server Auctions\"), for domestic customers the cost of electricity alone, disregarding the hardware cost, would sometimes be higher than the rent Hetzner wants. reply throwaway11460 20 hours agoparentprevYes, the volume discounts and economies of scale they can get (while you can't) are insane. reply singhrac 18 hours agorootparentOne of the simplest is that data center energy pricing is just simply different from residential energy, because they often get wholesale (plus small markup) rather than residential rates. reply formerly_proven 18 hours agorootparentThe majority of residential rates (about 40 ct/kWh) are taxes and levies, not the actual price of the electricity. That's why industrial rates can be less than half of residential rates. reply throwaway11460 18 hours agorootparentCan you be more specific? What kind of taxes? VAT is paid by the end consumer so you're not getting out of that one. Are there special consumption taxes on electricity in Germany? In my experience the largest difference is distribution cost. The mandated distribution monopoly charges a lot (regulated price) to small customers. reply Propelloni 17 hours agorootparentprevYou need to compare prices again. Residential electricity costs are significantly below your quoted rate for quite some time now. I've just checked and the lowest rate I saw was 21 ct/kWh. Most vendors are somewhere in the 25 to 30 ct/kWh bracket. reply EVa5I7bHFq9mnYK 15 hours agorootparentLarge part of their operation is in Helsinki, where electricity prices are a fraction of Germany prices. reply callalex 15 hours agorootparentprevThere’s no point in having this discussion, it is extremely specific to different locations. reply lelandbatey 16 hours agorootparentprevAverage USA energy prices are (according the Bureau of Labor Statistics) ~$0.18/kWh at the moment, with some folks at $0.40+ (San Francisco, San Diego, Hawaii) while others are as low as $0.13/kWh (Seattle, Saint Lewis) https://www.bls.gov/regions/midwest/data/averageenergyprices... reply formerly_proven 16 hours agorootparentprev> The average electricity price for households fell by almost 8 percent at the beginning of 2024 compared to the annual average for 2023 and now amounts to an average of 42.22 ct/kWh (2023: 45.73 ct/kWh; base price included pro rata for a consumption of 3,500 kWh/a ). https://www.bdew.de/service/daten-und-grafiken/bdew-strompre... reply noAnswer 14 hours agorootparentI'm at a small local energy provider. I have their most expensive 100% renewable tariff. Even after the invasion the price only went up to 28,36 €ct/kWh. Yet, my parents pay above 45 to RWE. (That's what cheap nuclear power stations get you I guess.) My father finally changed providers but thanks to the super fair contract has to wait now over a year for it to happen. And thanks to the \"Strompreisbremse\" I have to subsidise those ass companies. sorry for being of topic reply singhrac 15 hours agorootparentprevI think \"taxes and levies\" is a bit true, but also you pay for distribution. If your power line goes down in your neighborhood, you basically just grumble but it gets fixed. In wholesale connections you pay for the distribution power line (+ maintenance) basically up front, but it doesn't get put in the final wholesale bill. This isn't to say your local utility isn't wasteful with its ratepayer money; it totally can be. There isn't enough pressure to lower rates. It's just worth saying that these specific \"taxes\" do have an intended destination, not just general govt. reply mratsim 10 hours agorootparentOr for some there is a national base price. reply xhkkffbf 17 hours agorootparentprevIn the winter time, I get free heat from my server. In the summer, I have to pay to run an AC to get it out of the envelope. reply teaearlgraycold 16 hours agorootparentGotta run some ducting for that server! reply rstuart4133 7 hours agoparentprevOn purchase price alone you are usually better off not renting. Given a server generally comes with 5 years warranty, you are about 50% better off on those figures. However, the killer isn't the purchase price. You have to put it somewhere with a lot of bandwidth, UPS, generators and cooling. To use your Hetzner example, you can't get co-lo for less than $119. That's 14U, which is more than you need for one server of course, but then there are usually charges on top of that. So yes, for small installations it's hard to beat renting. You need to be using several racks for the equation to tilt towards purchase. reply treffer 13 hours agoparentprevLast time I checked the GCP prices vs. hetzner servers there was a factor of 10 in monthly cost. So I cant say this is a general rule of thumb. It is true though that hetzner can provide superb performance per $. Potentially below what you can do (unless you buy racks of hardware). Also keep in mind that hetzner is mostly a beowulf cluster. See e.g. this press picture from their newsroom page: https://cdn.hetzner.com/assets/Uploads/IMG-0546-91.jpg They do have dell servers, too. But don't expect to run on one of these nodes unless the machine type has Dell / PowerEdge in their name. This means the management capabilities are minimal (but usually enough). A fully licensed management card can do way more. Their renting pricing is ridiculously cheap though. Hardware failure is more common than on cloud providers (see cluster hardware), support has been helpful in the past. Overall IMHO a good choice if you are price sensitive. But even then consider development vs. deployment vs. running (server) costs. Servers might make up less of the total cost of a service than you expect. reply rthnbgrredf 13 hours agorootparentTo be fair, companies that merely launch VMs on GCP constitute a small fraction. GCP truly excels when you leverage its object storage, BigQuery, managed postgres database (which starts at 7$/month), and serverless solutions for cloud-native applications. Our company operates around 500 services, with billing per second, and a significant number of them are scaled down to zero when not in use. If you need a GPU for a batch processing task involving 10,000 images, you can simply activate a VM equipped with a high-performance GPU for an hour, pay for that duration, and then shut it down. At Hetzner, you're required to pay for a whole month upfront, regardless of whether you need the GPU for just an hour each day. Therefore, I'd argue that if you require continuous, raw computing power, Hetzner is indeed cost-effective. However, the cost-effectiveness elsewhere really hinges on your usage patterns. reply qeternity 10 hours agorootparent> To be fair, companies that merely launch VMs on GCP constitute a small fraction. I think you would be surprised. reply whizzter 14 hours agoparentprevIt seems a bit like car leasing, providers gets a bit of discounts for their volume purchases and customers pays a premium (that is still affordable) compared to just taking the entire deprecation cost themselves. The provider then still has something worth a fair bit of cash that can still generate income (through resale). So if you look at a 2 year horizon and want to keep cutting edge (or prefer operating expenditures to capital expenditures) then it's cheaper whereas looking at a 5 year horizon the capital expenditure will pay off. Hetzner is very bare-bones compared to regular clouds (a friend who was after performance but wasn't prepared for this ran into some issues when a disk died), so you need some procedures for backup/replication in place but if your procedures are in place then you can save big compared to the regular clouds. reply chpatrick 14 hours agorootparentThere's definitely a tradeoff when it comes to maintenance effort. You need to make sure your RAID and monitoring works and if a disk fails you need to message support to replace it. In general I've had a great experience with them. reply elric 19 hours agoparentprevThey build their own servers using some custom components. Economies of scale probably don't really factor into the pricing of their GPU stuff (unless I'm seriously underestimating their GPU customer base), but they do factor into the peripheral stuff like PSUs, fans, hard drives, etc. reply jsheard 19 hours agorootparentThere's some videos out there showing off their custom motherboards, it's an odd mixture of standard-ish consumer ATX but stripped down to the absolute bare essentials and with some components rearranged for better airflow in racks chassis they also build themselves: https://youtube.com/watch?v=V2P8mjWRqpk Another video showing the custom racks and other infrastructure: https://www.youtube.com/watch?v=5eo8nz_niiM reply YetAnotherNick 18 hours agoparentprevIt's definitely weird that cloud costs which is supposed to be commoditized is differing by 2 orders of magnitude. Hetzner/OVH seemed to have solved the hard part of making the service cheaper but just can't solve the easier task of making the platform usable. Why do I need to upload documents for verification. Why can't I try server for 2 hours right after signing up for the service. reply mike_hearn 18 hours agorootparentThose are part of making it cheaper. ID verification is an anti-fraud measure and ensures that bans stick, which in turn means they don't have to invest so heavily in heuristic anti-fraud and \"spam filtering\" type work. A delay on provisioning means they are provisioning hardware JIT in some cases, meaning they need less idle float capacity, which in turn drives down costs. reply Scotrix 18 hours agorootparentprevHave been in deep contact with data centre and rental server offerings in the past, there is a crazy large amount of fraud going on, e.g. 2 hours for free means 2 hours free resource to DDOS. reply YetAnotherNick 18 hours agorootparentI don't want it free. I want to put $2 on the account. Try the server and decide after using the server. reply albert180 16 hours agorootparentAnd they want a customer who will rent the dedicated box for a long time and not wipe it after every 2$/2h Trialcustomer reply Scotrix 17 hours agorootparentprevsame problem, credit card fraud/chargebacks happening anyway, that’s why you have an intense verification process to reduce the fraud going on. reply throwaway81523 14 hours agorootparentprevYou can do that with Hetzner virtual (cloud) servers, just not with dedicated servers. reply 15457345234 10 hours agorootparentprevYou're basically describing the internet-scumbag use pattern - bounce around providers and servers using nothing for more than a few hours at a time and making tiny charges to (other people's) credit cards that won't trip fraud controls because they look like verification charges. There are very very many reasons to prohibit exactly that type of usage pattern. reply EVa5I7bHFq9mnYK 15 hours agorootparentprevSure you can. I rented a server and after two weeks decided I don't like it, and got refunded all the money, including the setup fee. reply delfinom 16 hours agorootparentprevYou can thank crypto for ruining the internet for us. reply petercooper 19 hours agoprevI've just ordered one to ditz around with. We're early in the process of deploying our own internal server for Mixtral work, but it'll be interesting to see how this performs (in raw terms, almost certainly worse, since we can roll out a 4090 without getting into trouble – but Hetzner has a better connection and handles the maintenance, so..) Order has been accepted but not deployed yet. I'll update when I have some initial inference numbers. reply petercooper 15 hours agoparentI got it after a few hours. You need to install drivers/CUDA yourself, but all very straightforward. Unfortunately due to having 20GB of VRAM, I'm limited to mixtral:8x7b-instruct-v0.1-q2_K but it runs fine, generating at about 40 tokens/s (65 tok/s for eval). As per official specs, it's running maxed out at 70W (being an SFF card). (I've now tried running the Q4 mixtral which is 26GB. 18GB is on GPU, 8GB through CPU. Gets about 11 tok/s.) reply syntaxing 5 hours agoparentprevI never deployed it professionally but wouldn’t getting a Mac Studio be a better run for your money if you’re only inferencing? reply SushiHippie 20 hours agoprevFWIW, on the server auction, you could get GPU servers since a while: https://www.hetzner.com/sb/#additional=GPU Looks like currently there are only GTX 1080s on the server auction, I don't know if they normally have other GPUs available as well. reply alberth 20 hours agoparentNvidia RTX 4000 SFF Ada From the OP link, the GPU-Server GEX44 server line includes RTX 4000. And the server only cost €184/month. reply SushiHippie 18 hours agorootparentI meant if they have other GPUs normally available in the server auction. As far as I understand the servers in the server auction are mostly custom built ones for customers and once they stop renting them they land on the server auction for others to rent. The one with the RTX 4000 seems to be a standardized line of products. reply dizhn 17 hours agorootparentThey briefly had GPU servers before. I believe they stopped when crypto messed up the gpu prices and supply. That's why you're seeing them in the auction now. But they usually go quick. This might change now since they have a brand new offering. The old GPUs would have very litle memory by the way. Not ideal for AI. reply albert180 16 hours agorootparentThey stopped because they had a high percentage of abuse with Morons mining crypto reply dmaa 14 hours agoprevIt looks like a good value for money, but immediately after registering, I get this reply \"After reviewing your updated customer information, we have decided to deactivate your account because of some concerns we have regarding this information. Therefore, we have cancelled all your existing products and orders with us.\" Someone has a similar experience? I've tried it twice, filled everything truthfuly, valid credit cards etc. and no still the same. reply breakingcups 14 hours agoparentThere's a manual verification process if you contact support. Obnoxious that it's needed, but works for many people. reply lobito14 3 hours agoparentprevHetzner has one of the worst customer support I've ever seen. Get really, really, faraway from this company. reply stanislavb 14 hours agoparentprevI haven’t faced this issue myself, however, lots of people from India and some other countries have this experience. I’d assume it’s something to do with collecting recurring CC payments from Indian cards. That’s inherently difficult. reply dmaa 14 hours agorootparentThat would also be my guess, but I live in western Europe. reply belk 13 hours agoparentprevI got this when I first registered, contacted support and it was lifted in a day or two. reply binarymax 17 hours agoprevThe GPU product page: https://www.nvidia.com/en-us/design-visualization/rtx-4000-s... 20GB RAM, can fit 13B param models, and maybe some quantized larger models. reply ecmascript 20 hours agoprevNot really related to their GPU offering but Hetzner is such a great company. I have a dedicated server I rented where I host all of my side projects. For the same price as one shitty VPS I get 8 cores, 64GB ram and a disk of 240GB or something like that. Just incredible, response times are incredibly low and there is no cap on data. The few times I have contacted support they've been great. The only times I have experienced downtime is when I have stopped or rebooted the servers. Can't recommend enough, especially if you are inside the EU and care about data privacy. The fact that they're a european company is of great value to me. If you are new on server provisioning and want to learn how to setup a new server, I wrote a small guide for how to do it for node-projects (works for pretty much any web app if you replace the node.js stuff): https://deployjs.com/ (hosted on that server) reply happylion0801 19 hours agoparentSigned up from an Asian country and during signup they asked me to upload a passport copy page. I thought that was weird (I mean no other cloud provider asked for it but okay). I uploaded anyway because I thought it’s in EU so they at least have better privacy laws so they may end up deleting it later from their systems but I immediately got rejected. Not sure why no reason specified. So now they have my passport copy page without me having an account. Oh well, seems like either they have an overly aggressive spam filter that gets false positives or they don’t really like accepting signups from Asia reply lofties 16 hours agorootparentHad the same experience. Contacted support for manual verification, was online the next day with Hetzner. Machine has been running without interruption for a year now. reply deltaknight 16 hours agorootparentprevI had a similar experience very recently (albeit within Europe). Did you try their manual verification process? It was available to me after two failed automatic attempts, and it basically had me send an email to Hetzner support asking if they needed any more information. I got approved manually the very next morning. Maybe not the most customer-friendly experience, but I figure half an hour of signup weirdness is okay for a solid after-signup service in the long term, especially as that half an hour is likely to deter a decent chunk of fraud. reply EVa5I7bHFq9mnYK 15 hours agorootparentprevSame story from the US. I think they mostly worry that you are trying to avoid paying VAT. reply lakomen 16 hours agorootparentprevHetzner is pretty hostile towards foreigners. And don't you dare suggest in their forum that foreign tech is better than German, instaban. Fuck Hetzner reply lukan 15 hours agorootparentI can see a possible scenario, in where rather your general attitude might have been the problem. reply Shorel 13 hours agorootparentprevI'm from South America, got a server with them, no issues so far. reply EVa5I7bHFq9mnYK 15 hours agorootparentprevWhat German tech - AMD, Intel or Nvidia? reply callalex 15 hours agorootparentPlease don’t feed the trolls. reply pmontra 19 hours agoparentprevWhich server do you have on Hetzner that I missed? Dedicated servers start at 37.30 Euro per month now there, the AX-41 couch is pretty good and has no setup fee. On the other side I'm paying OVH about 12 Euro per month for two small VPSes and they are even too much for me, performance wise. reply e12e 17 hours agorootparentHave a look at the server auction? Tbh, I would limit my dedicated server rentals to server/ddr ram setups (which is a little more expensive). But if you just need some cpu (to transcode some video, crack some passwords, what-have-you, run a game/videochat server) - consumer cpu models give you unmetered gigabit or 30tb/month 10gbps. So can definitely be worth it vs a VPS. reply ecmascript 2 hours agorootparentprevI have a dedicated server that I got on their auction page for 30 euro per month. That gives me 64GB of ram, 8 cores and some disk space. Sure, you can get a cheaper VPS, but given a VPS with the approx same perf it would cost much more. reply gokhan 20 hours agoparentprevHad a server with them for more than 10 years. Had they eliminated the setup fee, they would get more of my money for sure for small projects. reply sshagent 20 hours agorootparentThey often have a couple of servers with no setup. reply ecmascript 20 hours agorootparentprevThe setup fee is a one-time fee though, so if you have a server for many years that won't hurt so much. I pay around €30/month for the server I am renting but I know it will last a long time even if I get spikes on any on my many projects. For a 8 core, 16GB droplet/vps at DigitalOcean it will cost you ~$96/month. The performance is better at Hetzner for about 1/3 the cost. reply gokhan 20 hours agorootparentYes. I'm talking about occasional needs for a small server for a month or two. reply gowthamgts12 20 hours agorootparentWhy not go with hertzner cloud? No setup fees and you'll be billed by the usage. Still cheaper than other cloud providers. reply throwaway11460 19 hours agorootparentThe VM performance tax is huge. Also, no GPU and limited/paid traffic (large limit though). reply ecmascript 20 hours agorootparentprevThey have VPS cloud offerings similar to DigitalOcean as well, with no setup fees. reply e12e 17 hours agorootparentprevUse the server auction? No setup fee then. reply Citizen_Lame 20 hours agorootparentprevnext [2 more] [flagged] ecmascript 20 hours agorootparentI don't work at Hetzner, I live in Sweden. I'm just a happy customer giving praise for a company that really doesn't get so much attention. With all big american companies seemingly getting so much attention nowadays, I am glad for the European alternatives that we have where you don't have to ship your customers data to the three letter agencies. reply thatwasunusual 18 hours agoparentprevDitto. Been using them since 2012 or something, and have never had a problem (that wasn't my own fault). reply Citizen_Lame 20 hours agoparentprevAh yes, they are great if you messing about. Not so great if you have crucial business server with them. reply elric 19 hours agorootparentCare to elaborate on that? I've been using their services for some 20 years at various scales, and the only complaints I've had are that their shared mailservers occasionally get blacklisted. That, and the UI of their KonsoleH could do with a bit of a refresh. reply rapsey 20 hours agorootparentprevWe've been running stuff on their platform for years with very little problems. Why not? reply withinboredom 19 hours agorootparentprevI haven't run into a single issue for years, what do you mean by that? reply ecmascript 20 hours agorootparentprevwhy not? reply kristianp 13 hours agoprevIt's running an RTX 4000 SFF Ada, 20GB GDDR6. Interesting that they've gone with a single-slot, low power card. Doesn't pull enough power to mess with their cooling/power design, I guess. Not a lot of graphics RAM to do much with LLMs. That card is about $1500: https://www.amazon.com/dp/B0C2KMXQYG reply nubinetwork 14 hours agoprevAfter the exchange rate, that's $270 CAD a month... after a few months, you might as well just buy a raptorlake and run it locally. reply throwaway81523 14 hours agoparentIt's a GPU system, not just raptor lake. reply zymhan 14 hours agoparentprevWelcome to Cloud Computing. reply ThinkBeat 13 hours agoprevI need some help: I have no idea where the NVIDIA RTX 4000 SFF belongs in the hierarchy of speed and suitability for \"AI\" computing. How does this compare to a NVIDIA A800 40G? (if that is still considered good, I can't keep up) To me the RTX part of the name sounds more on the consumer side but I am probably wrong about that as well. reply greatNespresso 20 hours agoprevIs Nvidia RTX 4000 solid for training? Alternatively this card seems listed for around $1800 so at $180/month on Hetzner, it may be even cheaper to buy it, depending on the expected usage. reply kookamamie 20 hours agoparentNo, not a great GPU for training. Consider something like A5000, A6000 or their Ada variants - if you want to go full \"data-center\", A40, L40, L40S are solid picks for small-scale training. reply brucethemoose2 16 hours agoparentprevAre you going to be training constantly? TBH renting a big GPU (A100 or better) and getting the run over with quick is usually best. VRAM is everything. reply Aerroon 20 hours agoparentprevWhy does this GPU cost so much? It seems to be like an RTX 3060 Ti/3070 with more VRAM and less power draw. Does that really justify the price tag? reply petercooper 19 hours agorootparentNVIDIA's licensing. Note that the 3060/3070 is a \"GeForce\" whereas a 4000 is a \"Quadro\". Then look at the GeForce driver EULA: https://www.nvidia.com/content/DriverDownloads/licence.php?l... > No Datacenter Deployment. The SOFTWARE is not licensed for datacenter deployment, except that blockchain processing in a datacenter is permitted. I could never figure out why they allowed blockchain processing, given how wasteful much of it was and how it impacted card supply for years, but there you go. reply pmontra 19 hours agorootparentBecause they made a lot of money selling to miners at prices that maybe they couldn't ask to gamers? reply 8organicbits 18 hours agorootparentWouldn't the GeForce license then say \"no block chain\" and a different NVIDIA brand (BlockForce?) would allow it, letting NVDA charge more for large scale mining? Otherwise it looks like gamers and miners are paying the same prices. reply bbatsell 18 hours agorootparentThey do. But since most miners would just ignore the license, they implemented LHR (low hashrate) beginning on GeForce 3000-series that detects the types of ops used for mining and artificially slows the GPU to about 25% of its actual speed. reply pmontra 17 hours agorootparentThey did that but with mixed success. One random link from a Google search: https://old.reddit.com/r/EtherMining/comments/t4mwxj/workaro... reply latchkey 16 hours agorootparentTheir source code got leaked and it was reverse engineered to defeat it. That said, it was all kind of late in the game, GPU mining died not too long afterwards. reply sodality2 18 hours agorootparentprevI bet that now the AI demand spike will be the new crypto demand spike that NVDA latches onto. reply Aerroon 18 hours agorootparentprevThis seems insane to me. It makes me even more confused that AMD/Intel aren't trying their hardest to go after AI compute. Nvidia is basically leaving the door open. But I suppose that the idea could be to protect their gaming market in a way. If datacenters were buying up gaming GPUs then that could cause shortages for regular customers. reply latchkey 16 hours agorootparentAMD is trying their hardest. reply kookamamie 20 hours agorootparentprevBecause it's \"enterprise\". reply HPsquared 19 hours agorootparentprevLike Apple, they use RAM for market segmentation. reply stefan_ 19 hours agorootparentprevDatacenter customers don't have options, gamers do. reply rmbyrro 20 hours agoparentprevYou have to discount the price of a similar server without the GPU, tough. But even so, it's still quite expensive. I have the impression Nvidia charges more from data center customers. So, for cards available to retail consumers, it's not a fair cost comparison. With only 20GB, RTX 4000 has limited use... I was expecting they'd be offering something with at least 60GB at this point. reply Ayesh 20 hours agoparentprevIt's with ample RAM and the latest Intel CPU. Assuming the whole thing draws 100W, that's also including 72kWh of electricity. reply bluedino 20 hours agorootparentI'm not sure what the exact 'rule' is here, but you wouldn't use a contraction in this sentence. It is, with ample RAM and the latest CPU. reply sjsdaiuasgdia 19 hours agorootparentFrom https://www.yourdictionary.com/articles/contractions-correct... - Contractions can be used in any position in a sentence; however, homophone contractions such as \"it's\" and \"they're\" sound better when followed by another word or phrase. The reason is that the sounds of \"its\" and \"it's\" and \"they're\" and \"their\" are so similar that they can be confusing unless they are used with the context of an additional word. reply 0xcde4c3db 14 hours agorootparentI think it's less to do with homophone confusion and more to do with usage patterns relating the contracted part to a phrase rather than a single word [1] [2]. By the logic of the homophone explanation, the following exchange should be much more acceptable than a sentence ending with \"it's\" or \"they're\", but to me it seems equally strange: \"Are you waiting for someone?\" \"Yes, I'm.\" [1] https://www.youtube.com/watch?v=CkZyZFa5qO0 [2] https://en.wikipedia.org/wiki/Clitic reply zimpenfish 18 hours agorootparentprevTo be fair, the contracted version doesn't read nearly as nicely. Q. Is it good for training? A. It's with ample RAM and the latest Intel CPU. vs Q. Is it good for training? A. It is with ample RAM and the latest Intel CPU. reply bluedino 17 hours agorootparentI think they are agreeing with it not reading as nicely. But you still need the comma. Otherwise it sounds like: \"It is with great pleasure...\" reply zimpenfish 6 minutes agorootparentI don't think you necessarily need the comma but it would depend on the reader, I think. I didn't need the comma to read it correctly but I can see how you might mentally insert a pause between \"it is\" and \"with\" that makes it read weirdly. Probably better to reorganise it as \"With ample ram [...], it is.\" gregoriol 19 hours agoprevIt's a good price at ~0.30$/hour comparing to Paperspace's RTX 4000 with 30GB RAM at ~0.56$/hour. Are there many other providers for such environments? reply pacohernandezg 16 hours agoparentvast.ai, runpod.io, tensordock.com, ... I have just ordered a GEX44 from Hetzner. :-) reply pinetroey 14 hours agoprevA few months ago I tried to register an account, but I was denied. I don't know why, never got a response... I went with ovh. reply V__ 20 hours agoprevNote: For now for existing Hetzner customers only. reply yread 13 hours agoprevIt's a pity you can't commit to 3 year rental for a discount reply christkv 20 hours agoprevHow is the gpu for inference or tuning ? reply renewiltord 13 hours agoprevThey had a strange setup process. I had to scan my passport and stuff. However, I did have a VPS with them that had uptime greater than a decade and that I killed only because the underlying hardware was going away. That was some 7 years ago or so. reply ramesh31 17 hours agoprevDoes every European tech company have some kind of committe that ensures their customer facing website looks like a decrepit ASP page from 2004? I mean what explains this? It's a billion dollar corporation and I am having serious reservations about whether I would even enter my credit card info on this site. reply Shorel 10 hours agoparentI was going to seriously disagree with you, and then I looked at their source code. It's awful. I would still buy from them because their reputation trumps any concerns. reply system2 14 hours agoparentprevI see the same thing on HN posts. 1999 vibes everywhere. reply lakomen 16 hours agoparentprevIt's true. And it breaks on mobile when you use the menu. reply delfinom 16 hours agoparentprevHetzner's site looks fine to me. Highly readable and to the point unlike modern web shit where you have to scroll through 5 miles of marketing animatinos. reply rsynnott 15 hours agoparentprevI mean, see also AWS; for a very long time their console looked like something from the dark ages. I think they have largely spruced it up now, though I'm not sure it helped _usability_ too much, and may have actually made it worse. reply hipadev23 16 hours agoprev [–] Just make sure any workloads you place on Hetzner are treated as spot instances and outputs are immediately sent off network. The reliability of their systems and customer support is very low. Hetzner is a 5-star or 1-star company. Are you willing to risk your business on the flip of a coin? My own experience [2] was definitely a 1-star. [1] https://www.trustpilot.com/review/hetzner.com [2] https://news.ycombinator.com/item?id=39075608 reply gizmo 15 hours agoparentReliability and customer support are unbelievably good. 99.995% uptime for the past 10 years. Hetzner will happily customize your servers however you like, and at their low prices you can have hot standbys for everything and still save money. reply whizzter 14 hours agorootparentMy friend went onto Hetzner for the performance/price, unfortunately the machines drive died within 2 months iirc. Seems like if you're buying on auction you'll get a wiped drive and if you're unlucky it's been written to a lot already, don't remember the specifics but he really wasn't a happy customer afterwards. I think the last sentence you have there is pertient though, maybe don't go for Hetzner unless you can also afford to have a hot-spare (but for the performance/price you probably can compared to any public cloud). In my friends case he was boot-strapping but needed the CPU so he was really penny pinching. reply tomschwiha 15 hours agoparentprevI've been with Hetzner for around 10 years and didn't had a single larger incident so far. Support also answered promptly - didn't had the need to request a lot support so far. Running 2-3 servers and a few cloud instances. reply dr_faustus 14 hours agoparentprev [–] Trustpilot is a scam which encourages bad reviews to force companies to buy their paid plan to “manage” those bad reviews and to feed it good reviews. reply hipadev23 12 hours agorootparent [–] Hetzner is also a fraudulent operation with zero support in my personal experience. But as you can see in this thread alone, any negative feedback is magically silenced. It sucks. Maybe we go back to webhostingtalk? reply 15457345234 8 hours agorootparent [–] Negative feedback is silenced? Dude the characteristic feature of Hetzner threads on HN is a bunch of people in the comments moaning about having to scan their passport or repeating the 'they just turn your servers off with no warning' line. In fact it's so prevalent and consistent - at least here - that to my eye it looks to me like black PR. Or that they're really good at rooting out internet scumbags and that the internet scumbags are unhappy about it. reply hipadev23 8 hours agorootparent [–] I run a legitimate non-sleazy business in the US doing mid 7 figures a year. It’s not a “line”, it was the reality of an attempt to do business with Hetzner. I’ve never had any serious issues with other vps, colo, dedicated, or cloud providers in 25 years as an employee of many companies or as a business owner: only Hetzner. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The website provides various server options like dedicated servers and GPU servers, with prices depending on server type and specifications.",
      "Customers can transfer servers from other Hetzner clients and access colocation services and domain administration.",
      "VAT calculation and pricing in local currency are available based on the customer's selected country."
    ],
    "commentSummary": [
      "The focus is on Hetzner's new GPU server, comparing cost-effectiveness of renting vs buying hardware, electricity pricing, and server longevity.",
      "Discussions include signup processes, VPS cloud offerings, GPU limitations, pricing, and overall quality of cloud computing services, with user experiences included.",
      "Users share opinions on fraud concerns, ID verification, GPU availability, and customer support, comparing Hetzner with providers like DigitalOcean and AMD, while also addressing data privacy, NVIDIA pricing, and the impact of mining on GPU availability."
    ],
    "points": 239,
    "commentCount": 146,
    "retryCount": 0,
    "time": 1708432756
  },
  {
    "id": 39443283,
    "title": "Valve Open Sources Steam Audio SDK 4.5.2 Under Apache 2.0",
    "originLink": "https://www.phoronix.com/news/Steam-Audio-SDK-Fully-Open",
    "originBody": "Valve Makes All Steam Audio SDK Source Code Available Under Apache 2.0 License Written by Michael Larabel in Valve on 20 February 2024 at 12:00 AM EST. 11 Comments With Valve's release today of the Steam Audio SDK 4.5.2 they have made the software development kit fully open-source under an Apache 2.0 license. Steam Audio 4.5.2 may not sound exciting in the context of a version number but as described in the release announcement is now \"the first open source release of the Steam Audio SDK source code.\" The rest of this work in this Steam Audio SDK release amounts to bug fixes and other standard changes. In a SteamCommunity.com announcement posted today entitled \"Steam Audio Open Source Release\", it notes: \"The entire Steam Audio codebase, including both the SDK and all plugins, is now released under the Apache 2.0 license. This allows developers to use Steam Audio in commercial products, and to modify or redistribute it under their own licensing terms without having to include source code. We welcome contributions from developers who would like to fix bugs or add features to Steam Audio.\" Kudos to Valve on this fully open-source Steam Audio SDK, plug-ins included. Just the latest great open-source/Linux work from Valve. Steam Audio is described by Valve as an immersive audio solution for games and VR. Plug-ins exist for Unity, Unreal Engine, and other game engines. This 3D sound API is promoted as a full-featured audio solution with real-time sound propagation, VR integration, great 3D audio capabilities, and more. More background information on Steam Audio via the project site. 11 Comments",
    "commentLink": "https://news.ycombinator.com/item?id=39443283",
    "commentBody": "Valve Makes All Steam Audio SDK Source Code Available Under Apache 2.0 License (phoronix.com)236 points by LinuxBender 17 hours agohidepastfavorite61 comments jsheard 17 hours agoThis is the Impulsonic middleware they acquired back in 2017. They quickly made it free in binary form, but third party uptake seems to have been practically non-existent, probably in large part because they only made it available on PC and Android but not any of the consoles or iOS. They also promised Wwise integration but that never materialized and doesn't appear to be anywhere in the open source release. reply sunshowers 17 hours agoparentInterestingly, the repo's readme says it works on Win/Mac/Linux, Android and iOS. Wonder if they had builds going but never released them to others. https://github.com/ValveSoftware/steam-audio reply jsheard 17 hours agorootparentIt always supported Win/Mac/Linux (that's what I meant by PC) but iOS seems to have landed on GitHub a couple of months ago, and they neglected to update the website. https://github.com/ValveSoftware/steam-audio/commit/b86a4284... Still no consoles or Wwise out of the box, but I suppose developers now have the option to do that plumbing themselves if they think it's worth it. reply spondyl 14 hours agorootparent> It always supported Win/Mac/Linux (that's what I meant by PC) I would guess that for a lot of people, PC is read as \"Windows\". Probably in part due to the marketing campaigns of the mid-2000s that would compare \"PC vs Mac\" which is of course dumb but even in reading that myself, my brain swapped out PC for Windows reply taeric 13 hours agorootparentFrom Valve, specifically, they have gone a long way to making it so that PC reads as \"Steam.\" Has some shortcomings, I think, but largely works. Does leave Mac out a bit. Such that I think you have a point. Not sure how much the market for this software makes that distinction, though. reply cassianoleal 11 hours agorootparentA lot of games on Steam work natively on mac. With the game porting toolkit, it's likely that the vast majority of them work - though maybe not all too well. reply bugbuddy 16 hours agoparentprevI bet they did not release the source code immediately because they wanted to make sure there was no patent infringement hiding anywhere in it. reply henriquecm8 15 hours agoprevI am starting to learn gamedev, in specific I would like to make a stealth game. Ever since I first read about steam audio, I've wondered if it would be possible to use the sound propagation to determine if a npc has heard a sound. There are probably better ways to do this. And not always the most realistic approach is the best approach. But I've always found that some games are very inconsistent about how npcs hear you. Sometimes you are in a closed room and make very little sound, and they hear you from outside, other times you are in an open place and they don't hear anything. It seems like some games only check the distance to sound origin, without taking occlusion/attenuation into consideration. reply duped 13 hours agoparentI'm no expert but one issue is that \"correct\" sound propagation needs to handle frequency dependent refraction (why sound is muffled coming from the other room, in additional to leakage through the wall). One total hack I would imagine would work alright is to render the scene twice, once displayed to the player and the other where the only light sources are the player's sound and walls/windows/etc just have some opacity changed. If the light around the enemy is above a threshold the player is detected. You wouldn't need to render the whole scene, just the area immediately around the player, and avoid doing it if there are no enemies. Another hack that would be more involved would be to compute a kind of graph for the whole level and when a player makes noise, you propagate it through the graph using precomputed weights. You'd have to handcraft a graph for every level. But I could imagine this getting the best results in terms of performance and gameplay, since you could tune it per scene. reply sclangdon 15 hours agoparentprevMost games don't have the time budget for that. Visuals take precedence, and it's not easy to get to 60fps as it is, especially if you're doing a lot of other processing. And stealth games especially will probably rely a lot on shadows and other visual things, which make rendering more expensive. reply henriquecm8 14 hours agorootparentThat's true, I just like when stealth games go beyond just player visibility, some of my inspirations do that: - Thief Gold, sound was important, each surface would make a certain amount of noise, and you could damp the noise by covering the ground in moss, or reduce it by walking slower. - Splinter Cell Chaos Theory: which had a sound-meter with 2 indicators, one for the noise you are making and the background noise. I specially like this system because of how it allows you to make much more well-informed decisions. - Sniper Elite: you can cover the sound of your rifle with loud background noise in some maps, they are regular, and they have cue moments before the loudest part. reply jvanderbot 14 hours agorootparentNot PvE, but Hunt Showdowns use of sound is worth mentioning. Like you mentioned, every surface has a sound and level, animals react to your sight and sound and will make their own noises, weather and env effects cover or uncover sounds, you can use items or gunfire or anything else to decoy. Really great. All a pvp game so you're trying to sneak up on actual people reply kevinh 15 hours agoparentprevI'm not sure if anything has it built in, but Thief: The Dark Project (in 1998) had sound propagation. There's some discussion about it here: https://www.ttlg.com/forums/showthread.php?t=151206. I wonder if you could query Unity's navmesh to calculate the distance, but that wouldn't work with, say, someone on the second floor hearing someone outside through the window. reply lytedev 14 hours agorootparentWouldn't you have an alternate navmesh for sounds with the appropriate \"cost\" heuristics based on how well sounds propagates through the mesh's material and across media? reply jjmarr 13 hours agorootparentprevThis sounds like a great use of hardware accelerated raytracing. The problem itself is very similar. Sound attenuation can be modelled as transparency. reply ssfrr 13 hours agoparentprevWhat you probably want is to place a virtual listener/receiver where the NPC is, so the system would use the same rendering that it does for the player. Then you put an envelope detector on the signal and you have an approximation of how much sound is reaching that NPC. You could also render the player sound separately from environmental sounds. So the NPC only hears the player if their sound exceeds the environmental noise. You could get real fancy and do some directional processing on the NPC so they’ll go in the apparent direction. It’s probably overkill though. You could probably do a much cheaper approximation and it would feel about the same to the player. It would be fun to test it out and see though! reply arafalov 13 hours agoparentprevI am - very slowly - learning Unreal and they do seem to have such functionality. E.g. https://dev.epicgames.com/community/learning/tutorials/5Ed/u... Since they have a full sound engine built-in as well, I suspect they deal with attenuation, et al correctly. reply justin66 17 hours agoprevA couple of impressions from someone whose knowledge of this is limited entirely to the project landing page: https://valvesoftware.github.io/steam-audio/ First, one of the little screenshots references Sansar, so it must be good! Second, I sense that whatever software is responsible for the screenshot in the background would flunk an Old Man Murray Start to Crate test: https://www.oldmanmurray.com/features/39.html reply tapoxi 16 hours agoparentSince they hired Erik and Chet it's been an inside joke at Valve to fail the crate test as soon as possible. (Erik left and returned, Chet has a YouTube channel now.) reply justin66 15 hours agorootparentAwesome. reply jorvi 17 hours agoprevI wish they developed this further as a cross-platform competitor to Windows Sonic / Dolby Headphones / DTS Unbound. It’s quite nice to got quasi-surround from a 5/7.1 source with just some headphones and HRTF magic, and its one of the biggest things I miss with gaming on Linux. reply viraptor 14 hours agoparentYou can enable that manually as long as the app exposes enough outputs. https://kaeru.my/notes/pipewire-surround-headphones reply torginus 10 hours agoprevWhat I've always found weird in gamedev is that everyone under the sun builds their own graphics engine - I don't think there's even popular middleware for that. But almost nobody builds their audio engine - to the point that basically there's no good open source audio engine in existence. I wonder why is that - while the requisite knowledge is not as readily available as it is for graphics, and this might be hubris speaking, but it can't be that much more difficult. reply dark__paladin 16 hours agoprevQuestion from someone who is pretty far separated from game dev: Is this now an open source alternative to fmod and wwise? reply jsheard 16 hours agoparentSteam Audio is only concerned with spatial audio rendering, which is a subset of what Fmod and Wwise do. Any time you hear dynamically mixed music in a game, for example, that was probably arranged using Fmod or Wwise. reply on_the_train 16 hours agorootparentWhat in particular feature enables or facilitates dynamic music? Is that not just a different music file? reply jsheard 16 hours agorootparentThey provide the editing tools for a musician to build up the dynamic composition and state machine that drives it, plus the runtime component which executes all that inside the game. It's basically a whole DAW specialized for games: https://www.youtube.com/watch?v=7A1HMOsD2eU reply on_the_train 15 hours agorootparentI see, thanks reply jestarray 15 hours agoprevHow does this compare to fmod? reply diimdeep 15 hours agoprevThat's good, since similar project from [1] Google as often happens is dead. [1] https://github.com/resonance-audio/resonance-audio reply ramesh31 17 hours agoprev [–] Seems like they're giving up (on HMD hardware). Index will probably be the last headset Valve makes. It was a fun idea. But with the Vision Pro showing that even Apple can't make the \"smartphone strapped to your face\" version of VR work, it's becoming pretty obvious that it's a technological dead end for anyone but niche gamers. reply dr_kretyn 17 hours agoparentI'm clearly just a casual passer-by. Any chance you could elaborate the connection between opening an audio SDK and giving up on hardware? To me that feels like a big leap. reply hgs3 16 hours agoparentprev> it's becoming pretty obvious that it's a technological dead end for anyone but niche gamers. Even if that's the case, it's OK to have a niche product. Gaming is a multi-billion dollar industry so the market is worth pursuing. reply webkike 17 hours agoparentprevGiving up on hardware? The steam deck OLED was released last November reply taeric 16 hours agoparentprevWith how well the steam deck was received, I'd expect them to lean more into it? Is it better in press than it was in actual sales? And I know I have tentative plans for the next VR thing they try. I have the PSVR2 and it is really good at the games it is for. To the point that I'd expect next generation efforts to be amazing. reply 0cf8612b2e1e 14 hours agorootparentThe problem is that Valve’s profit margin on the store is very healthy. Low production runs of a hardware device are just not making them a lot of money comparatively. reply pennomi 12 hours agorootparentThat being said, Steam Deck probably sells more games than, for example, the Steam Controller did. Probably sells more games than their VR headset did too. reply taeric 14 hours agorootparentprevFair. That said, I get the impression they still do a lot because they want to. Not necessarily because they have to. reply ffsm8 12 hours agorootparentValve is a privately held company. It's not forced to maximize quarterly returns for shareholders, it just does whatever Gabe wants. reply ranger207 16 hours agoparentprevI have a copy of every piece of Valve hardware except for a Steam Machine, and if there's anything that that's taught me, it's that Valve cannot make hardware v2 in anything. I snapped up the transparent case OLED Steam Deck immediately because I do not expect there to be a Steam Deck 2 reply mtsr 16 hours agorootparentI’ve understood it as: Valve isn’t really in the hardware or console business. They release these fairly open devices in the hopes they get copied by others. Either to create a new market for Steam or to ensure an upcoming market has at least some Steam compatible devices. reply themaninthedark 15 hours agorootparentprevValve knows how to count to 2, it just takes a long time. Now I would never expect a v3 of anything... reply BudaDude 17 hours agoparentprevThats a pretty hot take. It's way too early to say the Vision Pro is a flop. Remember, the first Apple Watch received mixed reviews. Yet, after several updates and feature additions, it became one of the most, if not the most, popular wearables. History often repeats itself with Apple products initially deemed failures. To illustrate, consider the initial reception of the iPod, as discussed in this link: https://forums.macrumors.com/threads/apples-new-thing-ipod.5... One more thing, Valve hasn't exited the hardware market. The Steam Deck is a clear indication of their ongoing commitment. reply DiabloD3 4 hours agorootparentBut.... the Watch was a failure and failed to find its market fit. It never took off with biohackers, it isn't useful for interacting with products or services or any communications service, and it can't even really tell the time unless you charge it every other day. The Steam Deck otoh became an absolute hit on day one and completely sold out in short order. It sold waaaayyy better than Valve ever thought it would. reply jwells89 16 hours agorootparentprevYeah Vision Pro strikes me as typical Apple first gen more than anything. It might in reality be a flop, but I’d wait a few hardware generations before declaring as such with any level of certainty. reply taeric 16 hours agorootparentprevVision Pro is going to be an interesting watch. I doubt it is bad gear, but it is hard to really understand the market Apple is chasing. In particular, the consumer VR market is dominated by gamers to an absurd degree. Trying to pull casuals into it feels way too strained. Similarly, professional use doesn't seem to be a strong value add, either. Will be interesting to see where things land. reply J_Shelby_J 16 hours agorootparentVision Pro is to meta quest 3 what Starbucks is to local coffee shops. Meaning, quest 3 is amazing, and Vision Pro is the best thing to happen to AR. I hope steam doesn’t pull out. Zuck can preach about openness but the platform is still antagonistic to pc users. reply taeric 14 hours agorootparentI'm still rather bullish against AR. The passthrough of the PSVR2 is nice, to get your controllers in hand and room setup. Otherwise, I don't see the value add, just yet. Heck, the best games make use of the headphones so well that I feel like I'm already in another room. reply TravelPiglet 16 hours agorootparentprevApple Watch is a flop. It's not delivered on anything other than being a good fitness tracker. reply prewett 16 hours agorootparentI would dearly love to have a flop like the Apple Watch... If a product with tens of billions in yearly revenue is a flop, well, sign me up! reply FirmwareBurner 15 hours agorootparent>If a product with tens of billions in yearly revenue is a flop, well, sign me up! Windows 11 also sold incredibly well yet I still consider it a flop. reply smoldesu 15 hours agorootparentprevAt this point, Apple could release a line of $99 drink cozies and sell out 5 minutes after they announce it. The fact that people buy what Apple sells isn't really an indication of whether the product is good or not. reply TillE 14 hours agorootparentIt's been selling well for almost 9 years now! You can call it many things, but it is clearly not a \"flop\". reply smoldesu 14 hours agorootparentI didn't call it a \"flop\", I said it's status as a flop doesn't correlate to how good a product is. The Butterfly Keyboard never flopped, but it also never got a legitimate competitor. People bought them because they were forced to, same as they were with Airpods and arguably the Apple Watch. I feel the same way about a lot of Apple products. The Magic Trackpad would sell out instantly if it got a new model with USB-C - but Apple knows they can ship more Lightning cables if they avoid it. It's part of the sinister math that goes into making you and I rely on Apple's constant... ahem, Innovation. reply zlsa 11 hours agorootparentThe Magic Trackpad comes with a USB-C (computer) to Lightning (Magic Trackpad) cable. https://www.apple.com/shop/product/MK2D3AM/A/magic-trackpad-... (I agree with you, for what it's worth - Apple is weirdly slow to update some of their products; AirPods Max stand out here as missing USB-C and the lossless audio of AirPods Pro.) reply smoldesu 10 hours agorootparentYou're right, and it doesn't break compatibility with USB (curse the idea of MFi on PC) which is the important part. Still, it makes my relatively-new Magic Trackpad 2 feel old and alien. Whatever reason there was for Lightning is lost me when I can't juice my peripherals up with the same cable that charges my Macbook. reply iamthepieman 13 hours agorootparentprevI take it you mean flop differently than a successful product from a business and profit perspective. Like a comedian can flop around on stage in a hilarious bit of slapstick humor which you might call a successful flop. But the two hour comedy special the slapstick is part of might be panned by critics and audiences and that would be a flop of an entirely different sort. reply WheatMillington 15 hours agorootparentprevOver 50 million units sold and the most popular wearable device ever made. Yeah, total flop. reply smoldesu 16 hours agorootparentprev> History often repeats itself with Apple products initially deemed failures. Maybe, but that never saved the Lisa, the Newton or the Butterfly keyboard. Sometimes, Apple takes a big risk that's far too early, expensive or simply doesn't pay off. I don't think it's a particularly hot take to say that VR/AR content creation has been stagnant for a decade. Valve couldn't save it by moving mountains, Meta barely made it mass-marketable with a barebones MVP, and Apple is... gentrifying the higher-end. It's not so much an insult towards Apple as it is to the field and it's market fit. Nobody has had an \"iPhone moment\" yet, not even Apple. reply HeWhoLurksLate 15 hours agorootparentIn the case of the butterfly keyboard, market fit is a very different thing from engineering failure reply cma 17 hours agoparentprev [–] Vision pro is laptop chips and silicon backplane microdisplays, a bit of a move away from the smartphone on your face approach components wise, though I'm sure some of the sensors may be shared. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Valve has launched the Steam Audio SDK 4.5.2 as fully open-source under the Apache 2.0 license, allowing commercial use and modifications.",
      "The release comprises bug fixes and standard updates, offering an immersive audio solution for games and VR, compatible with different game engines.",
      "Developers are encouraged by Valve to contribute to further improving the software."
    ],
    "commentSummary": [
      "Valve has released their Steam Audio SDK source code under the Apache 2.0 license, enhancing accessibility for developers across various platforms.",
      "Discussions revolve around sound propagation importance in stealth games, suggesting methods for distance calculation and sound attenuation modeling, and comparing audio engines like Steam Audio, Fmod, and Wwise.",
      "Users' opinions on Valve's hardware ventures, such as the Steam Deck and Apple's Vision Pro VR headset, vary, with some skeptical and others optimistic, focusing on Apple's use of Lightning cables and challenges within the VR/AR market."
    ],
    "points": 236,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1708446567
  }
]
