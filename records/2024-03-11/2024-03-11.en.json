[
  {
    "id": 39660592,
    "title": "DBeaver: Your Ultimate Open-Source Database Client",
    "originLink": "https://github.com/dbeaver/dbeaver",
    "originBody": "DBeaver Free multi-platform database tool for developers, SQL programmers, database administrators and analysts. Supports any database which has JDBC driver (which basically means - ANY database). Commercial versions also support non-JDBC datasources such as MongoDB, Cassandra, Couchbase, Redis, BigTable, ScyllaDB, DynamoDB, etc. You can find the list of all databases supported in commercial versions here. Has a lot of features including metadata editor, SQL editor, rich data editor, ERD, data export/import/migration, SQL execution plans, etc. Based on Eclipse platform. Uses plugins architecture and provides additional functionality for the following databases: MySQL/MariaDB, PostgreSQL, Greenplum, Oracle, IBM Db2, Exasol, SQL Server, Sybase/SAP ASE, SQLite, Firebird, H2, HSQLDB, Derby, Teradata, Vertica, Netezza, Informix, etc. Download You can download prebuilt binaries from official website or directly from GitHub releases. You can also download Early Access version. We publish daily. Running Just run an installer (or unzip an archive) and run dbeaver. Note: DBeaver needs Java to run. Open JDK 17 is included in all DBeaver distributions. You can change default JDK version by replacing directory jre in dbeaver installation folder. Documentation WIKI Issue tracker Build from sources See this article. Notes For bug reports and feature requests - please create a ticket. If you have any questions, ideas, etc - please start a discussion. Pull requests are welcome. Visit https://dbeaver.io or https://dbeaver.com for more information. Follow us on Twitter and Facebook Thanks for using DBeaver! Star if you like it. Contribution: help the Beaver! Hooray, we have reached 30k+ stars on GitHub and continue to grow! That's really cool, and we are glad that you like DBeaver. We are actively looking for new source code contributors. We have added labels “Good first issue” and “Help wanted” to some tickets. If you want to be a part of our development team, just be brave and take a ticket. You can buy one of our commercial versions. They include NoSQL databases support, additional extensions, and official online support. Also, licensed users have priorities in bug fixes and the development of new features. Thank you! DBeaver Team (contributors) DBeaver is a desktop client. If you are looking for a web-based database management tool - check our new product: CloudBeaver. It is based on DBeaver platform and thus supports any database and most of DBeaver features.",
    "commentLink": "https://news.ycombinator.com/item?id=39660592",
    "commentBody": "DBeaver – open-source database client (github.com/dbeaver)358 points by saikatsg 17 hours agohidepastfavorite116 comments samuelec 1 hour agoI have been using it for 3 years with Postgresql and it still buggy. When the db is throwing out an error dbeaver just say that there was an error on line xx followed by java exception stactrace forcing me to login into the db via command line and figure out what is wrong myself. If you want to write and debug a stored procedure then good luck! The ui had many issues and only recently seems to be just usable. At least with Postgresql, I can't really recommend for anything too complex. reply staticlibs 13 hours agoprevDBeaver works surprisingly nicely with less popular DBs. I work with Babelfish for PostgreSQL [1], it supports connections with SQL Server client libs. Most GUI client tools (like SSMS) expect \"real\" SQL Server on the other end of the wire - depend on various system views for DB introspection, so only partially work with Babelfish. Even if client tool is based on JDBC (like SQuirell SQL), it doesn't guarantee that this tool won't use additional SQL Server-specific queries for introspection. DBeaver is much better at this, I guess it is using JDBC API or DB-neutral INFORMATION_SCHEMA views for introspection. [1] https://babelfishpg.org/ reply supriyo-biswas 4 hours agoparentI personally like the ER diagram viewer in DBeaver. On a project with very little documentation, it was extremely pleasing to point it to the staging database and be able to visualize the relationships between various entities. reply NewJazz 3 hours agorootparentSchemaSpy can generate static docs based on a live db, and they include pretty good ER diagrams (IMO). They have a sample on their website (go to the relationships section): https://schemaspy.org/samples/epivirusurf/ The convenience factor is that you don't need a continuous connection to the db to refer to the diagrams or other docs. reply RachelF 8 hours agoparentprevThe free version supports a few, but you need to pay $500 a year if you to talk to more databases or want security or cloud features: https://dbeaver.com/edition/ It's a great tool, though, a bit buggy but most useful. reply silentsea90 11 hours agoparentprevThey do use JDBC and even allow you to customize your choice of driver version for a db. It is a bit clunky when one has to resolve missing dependencies but that's just how it works I guess. reply zmmmmm 10 hours agoprevI know probably 95% of HN will see it as a downside but one of my favorite things about DBeaver is that because it is implemented on Eclipse you can install nearly any Eclipse plugin and they work. So I installed Vrapper to edit in Vi-mode, some git tools I like, PlantUML, etc. Really makes it a step above other tools in terms of the power and flexibility you can have with it. Apart from that, you can install it inside regular Eclipse as a plugin, so then you can have your database window and your ER diagram next to your code. Again, something that really sets it apart from dedicated tools. reply elric 3 hours agoparentI love the Eclipse IDE. It's a shame the Eclipse Foundation doesn't treat it better. Last time I tried to contribute to it, I ended up giving up. They wanted me to sign paperwork simply to report a bug. There was no documentation about how to build the IDE, which is scattered across dozens of repositories. Maybe that has improved, but I doubt it. They seem to be putting all their eggs into the Theia basket, which is an IDE framework similar to VSCode. Would have been nice to see that energy go towards improving Eclipse instead of reinventing the wheel. Improving the UI's handling of HiDPI would be nice. One can but dream. reply giamma 1 hour agorootparentHi, I am an Eclipse Foundation committer, but I am not committing to the IDE itself. You don't have to sign the CLA (contributor license agreement) if you want to report a bug, but you have if you want to contribute code, even a small patch for a bug you found. That's a legal requirement to protect the Foundation from contributions that may copy/paste proprietary code and to ensure that you will not in the future change your mind and make claims for your contributions. It's not peculiar to Eclipse Foundation, many open source vendors require contributors to sign a CLA [1] My understanding is that Theia is currently more of a platform than a complete IDE. It is meant to be usable for building cloud-based IDEs, and you can test a distribution on your workstation, but being it a client/server architecture, you currently have to run a docker container locally to test Theia (or at least this was the case in Q3 2023). They are now focusing on the user interface and plug-in architecture of Theia (which is already capable of running VSCode extension). IMHO, for Theia to become really meaningful the server-side part needs to be improved: currently Theia reuses Eclipse JDT and other plug-ins on the back-end, which is fine for a desktop app, but less convenient for a client/server IDE as you will need a separate server-side container running the Eclipse plug-ins for every active user running the Web-based Theia GUI. With respect to Eclipse itself, it has dramatically improved over the last few years, since the Foundation started making a release every 3 months. You may like the UI or not, but on my workstation it's now much faster and lighter than it used to be, and it's also faster and lighter than IntelliJ. To me the main issue with Eclipse is that it relies on P2 for plug-in distribution, which makes installing Eclipse add-ons a painful process. [1] https://en.wikipedia.org/wiki/Contributor_License_Agreement reply HelloNurse 38 minutes agorootparentprevEclipse started as a \"reinvention\" of IBM VisualAge for Java and went through at least one major rewrite (transitioning to OSGi). I guess it's the standard pattern for mature code bases. reply wanderingmind 10 hours agoparentprevWould love to learn how you installed these plugins and tested them. Have you written about this somewhere or can you point to some resources that can help people like me, who use dbeaver but can't customize them well. reply zmmmmm 9 hours agorootparentIt's straighforward - it doesn't have the Eclipse Marketplace installed so you have to use direct installation, which just means you have to find the plugin site yourself, which is usually on the web site or github page for the plugin. So you go to Help => Install New Software and then put eg: the Vrapper update site in: http://vrapper.sourceforge.net/update-site/stable And just go from there. reply anotherevan 10 hours agoparentprevI've used Vrapper with Eclipse for years, and I've used Dbeaver for years - yet it has never entered my mind before that I could use Vrapper with Dbeaver! Thanks! reply fuzztester 1 hour agoparentprevPotentially good info to know for someone using Eclipse. reply hoistbypetard 6 hours agoparentprev> one of my favorite things about DBeaver is that because it is implemented on Eclipse you can install nearly any Eclipse plugin and they work I like DataGrip for a very similar reason, relative to IDEA plugins. reply fuzztester 9 hours agoparentprevWhy start your comment by saying \"I know probably 95% of HN will see it as a downside but\"? And this is irrespective of whether that 95% is right or not. Google \"father son and donkey story\". And re: that 95%: See normal distribution aka bell curve. And echo chamber and hive mind. All of the above imply the same main point, just in different ways. reply zmmmmm 5 hours agorootparentUnfortunately I pretty much have only ever seen negative comments about Eclipse on HN. It seems immensely unpopular and often the minute you mention it you'll get a bunch of often derisive replies. At this point I've had to come to terms with the fact that I'm the odd one out and own it, and that is how I do it. No disrepect meant, more acknowledgement of my own exceptionalism. reply fuzztester 1 hour agorootparentInteresting approach. reply hun3 7 hours agorootparentprevSort of like defence against possible downvoters. If OP went along the \"father, son, and donkey story,\" then the OP won't have posted it at all. reply fuzztester 1 hour agorootparentNo, they could still have chosen to post it, even if they went along with that story. My point was that there is no need to prepare a defence against downvoters, that too, up front. Not needed at all. Why is their opinion important to the OP? They can have their opinions, he can have his opinion of the same matter. What's the problem? The problem is only if one thinks that HN karma is important. It's totally unimportant. reply dfee 13 hours agoprevI don’t know why it matters to me, but I’ve always been put off by it being ugly and using non-native widgets. That may be the only reason I’ve paid for TablePlus. I’d probably be fine with a great TUI interface, too. So it’s really this intermediate UI that irritates me. reply worble 11 hours agoparentI find it absolutely baffling how often the prettiness of UI comes up as a HN comment. If you asked me objectively \"do you think it's pretty?\" I'd probably say no, but never once has this even occurred me when using it since I'm usually just trying to get work done, which I find it very useful for. It's a productivity tool, not an art piece I'm hanging on my wall. reply JohnBooty 9 hours agorootparentI think it's less about aesthetics and more about cognitive load. Nonstandard UIs aren't end of the world, but if possible I don't want the extra cognitive load of interacting with 5, 6, 7, whatever slightly different GUI paradigms. Like most developers I'm using multiple applications at once. Typically I'm using a terminal, text editor, browser, Slack, email, and maybe a database GUI. A developer's cognition and cognitive load are limiting factors. Increase my cognitive load by X% and there is going to be a Y% increase in how long it takes me to do something and/or a Z% decrease in the quality of the work. Even if X% is small, it adds up pretty quickly. After all if we are employed full time that is 2,000+ hours per year of work. Again, not just aesthetics. It's little things like, \"does this software support the standard keyboard shortcuts for my system?\" If CMD+W closes a window in every app except one that's kind of a pain. Etc. reply TheRoque 7 hours agorootparentTo be honest i find dbeaver well designed. Even though the interface is cluttered with a ton of functionalities, it has everything I need where I expect it to be! I don't recall googling \"who to do X with dbeaver ?\". reply Onawa 7 hours agorootparentCounterpoint, I love DBeaver and have used it frequently. Did go through a scare with the schema viewer though where I thought that I had somehow deleted the entire table schema, after doing a backup of course. But it took some googling to find out that I had not committed the changes to the database, and also that there was an option to \"lock\" interactions with the database to prevent the kind of scare I had. So as an novice/intermediate SQL user, the clunky interface can be a detriment. reply lf-non 3 hours agorootparentprevI'd generally agree with you. DBeaver interface is quite discoverable. I reach out for it whenever I need something more visual than pgcli - esp. things like ERD, inline editing, easy cross table navigation etc. However, I'd also agree with the GP comment. Its use of SWT ui results in some rather weird oddities. The biggest one is that if we configure gnome to use dark mode in linux, we get this absolutely unusable UI with barely visible text in many places. It becomes very obvious in that case which of the widgets are coming from gtk and which are custom rendered - and the distinction between them is stark. And there doesn't appear to be a clear way to fix it without actually disabling the dark mode at OS level. I'd be perfectly fine if it used some framework that rendered its own widgets that looked a bit different or even if it ignored the dark mode completely and stayed usable. reply talhah 11 hours agorootparentprevWhile your point is understandable there are various types of people. An important aspect about user experience is aesthetic and ease of use. Some people care purely about functionality and others have mixed opinions on this. It's not fair to call it petty when you guys are just two different customers and users with different needs. People sometimes forget the importance of user experience and it's why some amazing software barely gets used. Personally I care about aesthetic and consistency but willing to sacrifice depending on what I'm doing. reply al_borland 10 hours agorootparentprevSome applications are designed so well, it makes me look for excuses to use them. Other times the good UI makes it faster and easier to get the job done. An ugly UI makes me less likely to want to use an application, and a bad/confusing one makes it harder for me to get the job done, especially if it’s used infrequently. UI and UX does matter, even when talking about productivity software. There can also be an element of inspiration and influence. If all my tools are well designed, I’m more likely to set a high bar for whatever I’m building. If I’m using ugly and poorly designed tool. The bar for whatever I’m making will likely drop to the environment I’m around. reply selfawareMammal 11 hours agorootparentprevImo it's not just about being functional so I can get work done. It's also about enjoying what I'm doing and having aesthetic tools is important to me. It's still functionality the most important? Yes. Do I prefer non-ugly tools to ugly tools even if I had to trade a bit of functionality for a lot of prettiness? Yes, every single time (as long as I can still get done what I need to get done, ofc) reply dataflow 9 hours agorootparentprevI'd argue it's not really about prettiness but about fitting in, so that the UI isn't jarring or distracting. I think the situation would actually be similar if the rest of the system was ugly and one particular app's UI was pretty, rather than the other way around. reply Aeolun 10 hours agorootparentprevAt the same time, I find it more pleasant to use other software. DBeaver does everything I need, but it’s hardly ergonomic. reply themerone 6 hours agorootparentprevI have a PowerGrep license. It's UI is so cluttered that I almost always find ways to make due with less capable alternatives. reply roenxi 8 hours agorootparentprevUnderscoring the \"HN comment\" part. This is a userbase who are presently interfacing with a wall of text, some tasteful greytext and a coloured bar. This appears to be a crowd with low visual needs. reply smackeyacky 3 hours agoparentprevThis complaint has been levelled against Java (and small talk earlier than that) since forever and it made no sense back in the 1990s either. Nobody complains about the Wild West of interfaces on web apps. On a mobile I kind of get it because having apps work in similar ways eases the learning curve (I.e burger menus etc), but all the UI paradigms being used in dbeaver are the same as any other desktop app. I appreciate the fact that it works identically on Linux as it does Windows (and presumably MacOS). Not even Microsoft maintain consistent use of widgets within their operating system, you still stumble across windows 3.0 looking screens in windows 11. I say…it’s good that dbeaver eschews native widgets reply pachico 13 hours agoparentprevUgliness is something I found to be quite common in Java based apps. I never understood why, though. reply staticlibs 12 hours agorootparentNone of major Java/OpenJDK contributors (Oracle, Red Hat, SAP etc) care about desktop GUI Java libs. Jet Brains do care, but they are not major. All Java progress is concentrated on backend cloud services for 10-15 years already. This can explain why Swing is so underdeveloped and JavaFX was thrown away. Basically much more effort is required to make Java GUI look and behave nicely, comparing to Delphi/Lazarus or .NET GUI libs or Qt. reply adra 9 hours agorootparentI just had to pick up and build a UI product in java recently and picked up javafx for the first time. With Kotlin, I actually found it while using my the standard you builder to be quite intuitive and sorta dynamic enough to make development snappy. Definitely the best attempt at GUIs for java, but sad that only smaller shops are actually investing any real effort into the space. Oh and java native + javafx is a pain in the ass until I discovered Bellsoft NDK, which made building a binary from source 100% so much easier. reply elric 3 hours agorootparentprevJet Brains care? Do they? In my experience their tools are some of the ugliest out there. Granted, that's highly subjective, but there is \"something\" off about IntelliJ that I can't really put my finger on. Maybe the lack of whitespace. Maybe the weird tabs. reply staticlibs 30 minutes agorootparentI meant that Jet Brains do care about GUI support in OpenJDK (mainstream Java), their flagship products depend on this, they employ a number of ex-Oracle GUI devs and contribute fixes upstream. Just there are no new major projects in GUI area in OpenJDK for years, because none of big contributors is interested in it. Compare this to improvements in non-GUI areas like Java 21 Threading and also the whole Graal thing. reply PlutoIsAPlanet 11 hours agorootparentprevDataGrip does look visually better than Dbeaver, but I've found Dbeaver has much better performance. reply pachico 11 hours agorootparentprevI'm ignorant about this all. Can't you use something like Qt in Java? reply adra 9 hours agorootparentYou can, but anything in java that needs to step down into c/c++ ends up being a major pain in the ass so it's usually implemented half baked at best. Platform ergonomics are hard and you usually find that what works great in some languages / paradigms just don't flourish in others. It'd be nice to see some good language bindings that feel right for java. reply gbear605 11 hours agorootparentprevThere are Qt bindings for Java, but I’m not familiar with them reply RachelF 8 hours agorootparentprevDbeaver has quite a few Java annoyances. Windows don't resize properly, the tree list often requires multiple clicks and most annoying of all is running out of heap space on Java, especially if one tries to run more than one task at a time. reply yelsom 12 hours agorootparentprev+1, except for Jetbrains IDEs that have polished UI reply staticlibs 13 hours agoparentprevDBeaver uses SWT toolkit, its widgets are as platform-native as Java can do. Some of them can be much faster with long text editing than default Java Swing widgets. reply ptx 11 hours agorootparentYup, SWT is a wrapper for actual native widgets, similar to wxWidgets. Some widgets are custom though, like the horrendously ugly tab widgets, which might be what the other commenter is reacting to. And the spacing and alignment usually doesn't look great in most SWT-based apps I've seen, for some reason. reply fiddlerwoaroof 10 hours agorootparentAt least on Apple platforms, I don’t think SWT uses Apple’s layout algorithms and so you end up with Cocoa controls with slightly wrong spacing reply nxpnsv 1 hour agoparentprevI tried tableplus, liked it, it just feels right. I reported a minor error in docs, got a friendly message and a year of license. 10/10 reply CalRobert 3 hours agoparentprevHuh, the utilitarian UI is part of why I like it reply elAhmo 11 hours agoparentprevI am in the same position. I used Postico in the past, but unfortunately it doesn't offer support for non-Postgres databases. TablePlus has really good native UI and I wish more apps went that route as you can definitely feel a difference between a native app and something like DBeaver. reply FpUser 12 hours agoparentprevIt is free tool with gobbles of functionality. I use it occasionally and it works great. Whatever set of widgets it uses does not concern me at all. It does what I need and is convenient enough. Maybe it would matter more if I was spending all my work time with it but in reality I use it very occasionally. So thank to the developers. I also use another DB admin tool: HeidiSQL. This one is lightning fast. Most likely because it is native application. Written in Delphi btw Same thanks to the developer reply deergomoo 10 hours agorootparentThe default DB tool where I work is Heidi and I switched to DBeaver because I found Heidi to be horrendously unstable. Dozens of random exceptions on a daily basis, and because it’s single-threaded a long-running query will just lock up the entire UI to the point where it triggers the “this program is not responding” dialog on Windows. A free and open source DB management tool is still an impressive feat, but I absolutely would not trust it with anything where blowing up would have consequences. reply FpUser 9 hours agorootparent>\"I found Heidi to be horrendously unstable.\" As already said DB admin tools are not my daily driver. I either did not use Heidi intensively enough to uncover all the problems you mentioned or they've fixed those. reply boznz 6 hours agorootparentNo Heidi is pretty unstable compared to what it used to be. I get 2 or 3 exceptions that require me to restart daily, the good thing is that the exceptions are caught, and you can copy out any SQL you have been working on which means you rarely loose work. I still prefer it as it is fast and has a nice UI and if they can sort the crashes, I would be a convert. reply ramon156 13 hours agoparentprevNot good at supporting GTK either, its the sole reason I do not use it on gnome reply trillic 12 hours agorootparentWhat do you use? reply dorfsmay 11 hours agorootparentprevWhat? I use it on gnome all the time and never run into an issue. reply folmar 9 hours agoparentprevI've tested it since you recommend it but it seems it can't do Postgres local connection (via socket-file = the default). Not great UX on linux. reply cowmix 13 hours agoprevDBeaver is amazing. As someone who needs to do adhoc querying / extracting / loading of data from any hosts of system on a daily basis - this tool has saved me over and over again. My beef is there doesn't seem to be a way to contribute $$$ to the OS version -- except for buying/subscribing to the commercial version. Maybe I've missed some web page that explains how to do contribute -- so if anyone knows if there's a way to do that --- post it here. reply worble 11 hours agoparentI swear I donated to them via paypal once long ago, but I think they've since removed all those donation links in favor of the EE. Presumably any profits they make from merch goes into their pockets, so you could buy something from there that's in the price range you want to donate I suppose https://www.redbubble.com/people/DBeaverCorp/shop reply cowmix 11 hours agorootparentt-shirt ordered.. thx! reply evanelias 11 hours agoparentprevOut of curiosity, why not buy/subscribe to the commercial version then, if this is the clear path for how to support the primary developers of the software? reply cowmix 11 hours agorootparentI don't like the subscription options they offer. I just want to throw them $50 a year for the CE version -- which I would think is better than $0 a year I'm giving them now. reply evanelias 11 hours agorootparentFrom my POV, $50/year is not necessarily better than $0. Open source \"donations\" to open core / commercial OSS businesses typically don't amount to much in total. It's basically a rounding error for most businesses, with the extra downside of accounting/tax tracking. And although many FOSS financial contributors understand that this is a no-strings-attached type of situation, a small portion become very demanding and have unreasonable expectations due to being a supporter. I mean I kind of get where you're coming from, but on the other hand... outside of software, would you ever praise a product as an amazing life-saver, but express a beef with the makers' lack of a pocket-change GoFundMe? reply vbezhenar 14 hours agoprevAlso they made a browser database client (cloudbeaver) which is much better than pgdamin IMO. I set it for my company, so people can easily access database without creating tunnels, sharing passwords, etc. I tried pgadmin before, but it was incredibly buggy, almost unusable as a shared installation (often server would just hang until restart). For desktop I, personally, prefer Idea Database plugin, because it's just SQL editor, but incredibly powerful one. reply joshstrange 11 hours agoparent> For desktop I, personally, prefer Idea Database plugin, because it's just SQL editor, but incredibly powerful one. I also quite like DataGrip (essentially a standalone of the IDEA database plugin, or maybe the plugin is a plugin version of the standalone, not sure which came first). reply puika 1 hour agoprevI love dbeaver, but there's a thing that bugs me every single time. I constantly recreate databases during development, so I have to disconnect databases, then connect, then go dig inside the database tree to where I was before recreating. Am I missing some feature I don't know about? reply HelloNurse 35 minutes agoparentMaybe improve your organization of database connections into folders to dig more easily? reply gorm 3 hours agoprevIt's a great client and used it for years. But I have some issues. Sometimes the cursor disappear and ctrl-enter stops working. Anyone experienced something similar and know what it could be? I run on a rather minimalistic UI environment and might lack some desktop festures. reply mulmen 3 hours agoparentYep, same. A recent update introduced a bug where the entire editor window contents disappear on a diagonal. No idea why. Sometimes adding a newline foxes it, sometimes it doesn’t. reply dikaio 22 minutes agoprevWhich the team that created kaleidoscope would create a database app. reply cbb330 13 hours agoprevI like dbeaver for browsing DDL, list of tables, examples of schema, data types. also, to edit a few rows here and there as a quick test/fix to something. because its easier to click around than write many 2 line sql to do the same thing. But, I often use jupyter notebooks for the DML aspect of hard queries and data anlysis, for the power of dataframes and repeatable cells mixed with documentation and sharing. So all that to say, anyone know if there is a DDL browser equivalent ideal for jupyter notebooks / ipywidgets? reply singingfish 13 hours agoparentsame, except I use emaacs org mode instead of jupyter (and have record keeping / backup implemented with git-auto-commit-mode as well) reply irjustin 6 hours agoprevDBeaver is great, but it's so heavy/clunky on OSX. Also, 99% of the time I want 1. tables+data 2. data structure+index of table 3. search 4. sql query DBeaver aims to be a full admin system and simply getting to the data makes me drill 3-4 levels deep in a tree and it's quite annoying that only adds to the \"clunky-ness\". reply mixmastamyk 3 hours agoparentThere’s an option, simple view on connection or something, that eliminates much of the hierarchy. reply rs_rs_rs_rs_rs 3 hours agoparentprevYeah, to me TablePlus feels much better to use than anything else honestly, even if it's not open source. reply paulryanrogers 14 hours agoprevIt's a decent free and cross platform UI. Doesn't have all the DB specific features of a tool like PG admin or MySQL Workbench. Still, I like having the same tool for all my basic needs, and only use a more specialized one where I really need it. reply thunderbong 13 hours agoparentCan you elaborate on what DB specific features are missing? reply paulryanrogers 11 hours agorootparentPgadmin has some in depth stats about the server and can export COPY statements, among other things. MySQL Workbench has some nice planner visualisations. reply deergomoo 10 hours agoprevI’m sure some other clients do this too, but one of my favourite DBeaver features is that it will display geospatial column values in an embedded OpenStreetMap pane. reply frankbreetz 10 hours agoprevDoes DBeaver have a feature similar to PGAdmin's schema diff(https://www.pgadmin.org/docs/pgadmin4/development/schema_dif...)? I use this feature quite a bit, if DBeaver has a similar feature. I might try to make the jump. reply paulryanrogers 10 hours agoparentYes but not in the community version: https://github.com/dbeaver/dbeaver/wiki/Schema-compare reply pragnesh 2 hours agoprevhttps://dbgate.org/ is also great db client reply aldousd666 6 hours agoprevThis tool is my daily driver for all database stuff I touch. My only complaint is that the update dialog is quite a noisy nag. But that's not a real problem. reply thih9 3 hours agoprevAs much as I want to like it, having used both this and free version of postico for a couple of months I overwhelmingly prefer the latter. reply xnx 13 hours agoprevGreat tool. Would be nice of it associating it with .parquet files on Windows allowed DBeaver to connect to them with a double-click. reply Nihilartikel 11 hours agoparentI use the duckdb connector as an intermediary for parquet in Dbeaver - it works quite well. I just create views like: Create or replace view parqtable as select * from /pathtoparquet/*.parquet; reply pritambarhate 5 hours agoprevI use it regularly for accessing PostgreSQL, MySQL and SQLight. Very happy so far. It’s regularly updated also. reply Glyptodon 9 hours agoprevI'm not sure what all is out there these days, but have long found DBeaver a more convenient tool for day to day stuff than pgAdmin though it's certainly not beautiful. What's making it get voted up today? reply Nydhal 7 hours agoparentIt's September. reply neals 13 hours agoprevI use HeidiSQL for my SQL databases reply bojan 13 hours agoparentI was forced to switch to DBeaver as HeidiSQL isn't allowed at my current employer. I miss how light Heidi is, but I'm not sure I'd switch back now I'm used to all the features DBeaver has. reply frozenice 2 hours agoparentprevI also use HeidiSQL almost daily. Besides MySQL / MariaDB it also can connect to MSSQL and Postgres. I recently had to import a CSV with some million rows into MariaDB. Neither HeidiSQL nor DBeaver could do it (tried various settings). IntelliJ worked like a charm. reply deergomoo 10 hours agoparentprevHave they fixed the awful stability issues and the fact that a query that takes longer than ~5s will lock up the entire application until it completes? reply Aeolun 10 hours agoparentprevI use a mac and linux now. I think HeidiSQL is the thing I miss most from Windows. reply mappu 4 hours agorootparentSomeone started a cross-platform rewrite at https://github.com/ragnar-lodbrok/meow-sql which has made incredible progress. reply nurettin 13 hours agoparentprevThis is a windows program written in delphi. A fitting choice, because Delphi provides both the necessary data grid component and the syntax highlighting editor, as well as connectors to several sql databases. A cross platform version could be written in Lazarus, with the idea that it uses less system resources than a large java program. reply anonu 14 hours agoprevI use this successfully with different versions of postgres, SQL server, MySQL, redshift and others. Does the job. reply interbased 7 hours agoprevI mainly use Postico but I’ve noticed coworkers using DBeaver. I like how the tabs are named whereas in Postico I have to leave comments at the top of the query for later. I had to use DBeaver to access a non-Postgres database. I may end up trying it out as my primary tool for a bit. reply 20kleagues 4 hours agoparentI absolutely love Postico. For postgres, there really is no comparison for a Mac native db client. DBeaver seems so clunky in comparison, but I use that for everything non postgres. reply s_trumpet 3 hours agorootparentDBeaver is somewhat clunky, yes, but for me that's a minor downside since it acts as a Swiss army knife, regardless of what DB I'm trying to access. That saves cognitive load from having to learn multiple tools. Pretty much anything with a JDBC driver is fair game. It being cross platform is a big boon as well since I use all 3 major OS fairly regularly - Mac at work, Windows and Linux at home. reply konzhi 4 hours agoprevI only wish that I don't have to re-install my plugins each time dbeaver gets update reply shortrounddev2 13 hours agoprevI use dbeaver extensively at work. I like that you can graphically edit rows and it will start those changes as a transaction, so you can hit ctrl+s to commit the changes reply zmmmmm 10 hours agoparent> I like that you can graphically edit rows and it will start those changes as a transaction, so you can hit ctrl+s to commit the changes One of my favorite features too. I don't doubt this has saved me from some bad mistakes since you can use a methodology of \"select rows to change\" and manually set their value, then review, then Save, rather than issuing an update that tries to change them with SQL. For small scale ad hoc maintenance it's great. reply osigurdson 13 hours agoprevFunctionally it is very good, I like it very much. it does show that not all dark modes are created equal however. I think it takes an especially good designer to do dark mode well. A good example of a well done dark mode (in my opinion of course) is Grafana. reply i_am_a_squirrel 11 hours agoprevI've been using it for 5+ years! So much functionality for a free tool reply tiku 11 hours agoprevI'm looking for a tool like this that converts the schema to Laravel model files. Why? Because I'm lazy. reply sdwvit 11 hours agoprevDBeaver is great and has been in my arsenal of dev tools for at least 5 years. reply Gnarl 1 hour agoprevWorst UI experience. Prefer SQuirreL. Personal opinion. reply teaearlgraycold 14 hours agoprevDbeaver is generally great. Sometimes I have weird issues where it just can’t load data after losing connection with Postgres. It’ll act like it’s reconnecting but will invariably fail. I need to manually disconnect and reconnect to get it to work. But otherwise it does everything I need. I install it on all of my computers. reply 0x073 12 hours agoprevSince I used navicat, almost every other db tools feels incomplete. reply wood-porch 9 hours agoprevEh honestly I found it super cumbersome to use after coming from Sequel Pro (mysql), never really found a better alternative for Postgres though. Perhaps I would use Navicat reply sumedh 8 hours agoparent> Perhaps I would use Navicat Navicat has Data Synchronization feature which you can use to compare and sync data across two databases. I dont think any other DB tool has that feature which also works on a Mac reply htrp 14 hours agoprev+1 It's your general swiss army knife for DB tools reply sgarland 10 hours agoprev [–] It’s fine, but I maintain that spending a day or three learning SQL will pay dividends far beyond your initial time investment. That’s not to say you can’t write SQL with a GUI, of course, but judging by the replies it seems like they’re more often used to look at or change tuples via clicking. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DBeaver is a free cross-platform database tool compatible with any database having a JDBC driver, offering features like metadata editor, SQL editor, and data editor, with plugin support for various databases.",
      "Users can get the tool from the official site or GitHub, requiring Java (provided as Open JDK 17).",
      "It encourages user contributions for bug reports, feature requests, and pull requests, while commercial versions offer NoSQL database support, extensions, and online assistance. DBeaver has a desktop client and a web variant named CloudBeaver."
    ],
    "commentSummary": [
      "DBeaver is an open-source database client that offers customization options through Eclipse plugins, praised for features like the ER diagram viewer.",
      "Users have mixed experiences with DBeaver, with some praising its functionality while others highlight stability and compatibility issues, especially on Linux.",
      "Despite some UI bugs, DBeaver is valued for its versatility and functionality in daily database management tasks, generating discussions on Eclipse, Theia, aesthetics in programming tools, Java-based software, and support for developers."
    ],
    "points": 358,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1710089541
  },
  {
    "id": 39662698,
    "title": "Unveiling Monte-Carlo Graph Search Innovations",
    "originLink": "https://github.com/lightvector/KataGo/blob/master/docs/GraphSearch.md",
    "originBody": "Monte-Carlo Graph Search from First Principles Monte-Carlo Tree Search (MCTS) except applied to directed graphs instead of trees - \"Monte-Carlo Graph Search\" (\"MCGS\") - is sometimes considered to be pretty tricky to implement in a sound way. Unfortunately, this is partly because the perhaps-standard academic reference for it, Monte-Carlo Graph Search for AlphaZero (Czech, Korus, and Kersting, 2020), adheres closely to the \"standard\" formulation for MCTS on trees. This historically-standard formulation turns out to be a poor choice for conceptually understanding the generalization to graphs. This document presents an essentially-equivalent-but-cleaner formulation that I hope many will find more intuitive, and derives from basic principles why graph search needs to work this way and reveals some additional possible implementation choices beyond those considered by Czech et. al. Intro / Background In game-tree search or other applications of tree search, often we can find multiple possible sequences of moves or actions transpose to the same state. For example, in chess, 1. d4 d5 2. Nf3 leads to the same position as 1. Nf3 d5 2. d4. Example of transposition: `1. d4 d5 2. Nf3` leads to the same position as `1. Nf3 d5 2. d4`. When transpositions are possible in a game, usually the number of them will grow exponentially with the search depth, making deep search much more costly than needed. Ideally, we would like to these branches of the search to share their computation. However, standard implementations of Monte-Carlo Tree Search (MCTS) usually do not do this. They treat the game as a branching tree and inefficiently re-search every instance of each duplicated position within the tree. Various low-level optimizations (for example, caching and re-using neural net evaluations for repeated positions) can greatly reduce the cost of the repeated work, but there are still major downsides. For example if MCTS discovers a critical tactic in one of the instances, the corrected evaluation of the position will not propagate to other instances. Can we instead model the state space as a directed acyclic graph (DAG) by sharing nodes in this tree? Yes! Whenever multiple paths lead to the same state, we can represent that state with just one node in the graph. The tricky part is that applying MCTS naively to a DAG can easily result in an unsound algorithm. This is because MCTS is normally formulated in terms of running statistics of playouts, owing to its historical development as an extension of multi-armed-bandit-based methods to trees. For reasons we will see soon, naively applying this formulation to graphs does not work. Czech, Korus, and Kersting do a great job of fixing the problems and arriving at a sound algorithm based on this formulation, in spite of the challenges. However, there is an equivalent alternative way to approach MCTS from the perspective of online policy learning. In this alternative formulation, as we will see, generalization to graphs is relatively natural. It turns out we still arrive at a similar final algorithm as Czech et. al., but we can derive from basic principles why the algorithm needs to work that way. If we're willing to concede some low-level optimizations, the resulting code is also much simpler. Note that for this document we will mostly be disregarding what to do when actual cycles in the graph are also possible. The handling necessary will vary depending on the particular rules of the game (e.g. 3rd-time repetition, 'superko' rule, etc). This means that for games with actual cycles, in practice there may need to be significant additional work to handle them correctly. Here we won't address those details, we'll just focus on the core concept of how to make MCTS work. (Although, see also the addendum with a link to a doc with some rough thoughts on handling cycles.) The Usual Formulation of MCTS - A Tree of Running Stats Let's start by reviewing MCTS on trees. MCTS is often formulated an algorithm that tracks the running statistics of playouts that traverse a tree of nodes. The portion of the game explored so far is explicitly stored as a tree of nodes in memory. Each node represents a single state of the game, and additionally tracks: N - the number of visits so far to this node, i.e. playouts that ended on or passed through this node. Q - the running average of the utility values sampled by those playouts.1 A single iteration, or playout, of MCTS consists of: Starting at the root of the tree, walk down the tree sampling the next action to explore according to some exploration formula. Once the walk falls off the end of the tree by reaching a state not yet searched, extend the tree with a node for that new state. Obtain an estimate of the utility U of the new state, e.g. by querying the value head of a neural net. Walk back up the tree, at each node incrementing N and updating the average Q with the new sampled utility U. Here's an illustration:1 - Walk down tree via exploration formula. 2,3 - Add new node, estimate utility. 4 - Walk up tree, updating each node. For simplicity, the above illustration in particular uses 0/1 integer utilities for all playouts so that the Q values are round fractions, but in general utilities would be continuous floating point values predicted by the neural net. In Python-like pseudocode, the algorithm overall might look like: def perform_one_playout(node): if is_game_over(node.game_state): U = get_utility_of_game_outcome(node.game_state) else if node.N == 0: # New node not yet visited U = get_utility_from_neural_net(node.game_state) else: action = select_action_to_explore(node) if action not in node.children: new_game_state = node.game_state.play(action) node.children[action] = Node(N=0,Q=0,game_state=new_game_state) child = node.children[action] U = perform_one_playout(child) node.N += 1 node.Q = (1/node.N) * U + (node.N-1)/node.N * node.Q return U The N and Q values in turn drive the search through the selection of the actions during this process. This is the select_action_to_explore in the code above. In the case of AlphaZero-style MCTS (Silver et. al. 2017), which we'll focus on here, this would be by choosing the action at each step via the \"PUCT\" formula: $$\\text{Action to explore}=\\text{argmax}_a \\,\\, \\text{PlayerToMove} * Q(a) + c_{\\text{PUCT}} P(a) \\frac{\\sqrt{\\sum_b N(b)}}{1 + N(a)}$$ where: $N(a)$ is the number of times action $a$ was tried, which is the same as the $N$ for the child node that $a$ leads to. $Q(a)$ is similarly the average utility of $a$, which is the $Q$ of the child node that $a$ leads to (or a heuristic stand-in if that node has zero visits). $\\text{PlayerToMove}$ is -1 or 1 depending on the player to move, implementing the fact that one player is trying to maximize the utility and their opponent is trying to minimize. This would be always 1 in the case of a single-player game. $P(a)$ is the prior probability that the action is best, e.g. the raw policy prediction for $a$ from querying a neural net. $c_{\\text{PUCT}}$ is a tunable constant. As an aside, \"PUCT\" originated as an abbreviation for \"Polynomial Upper Confidence Bounds for Trees\", a \"polynomial\" variant of the \"UCT\" or \"UCB1\" algorithms from the multi-armed-bandit literature which use a formula with a different exploration term that involves a log scaling (see Kocsis and Szepesvári, 2006). Properly, \"PUCT\" might describe a whole class of such variants, but nowadays in game-playing/machine-learning circles \"PUCT\" often just refers to AlphaZero's particular version of the formula for how to select actions to explore, which is also what we focus on here. Also, as far as the name \"Monte-Carlo Tree Search\" itself, readers might note that there is nothing \"Monte-Carlo\" in the above algorithm - that it's completely deterministic! The name comes from the fact that originally, randomized rollouts to the end of the game were used for utility estimates, instead of querying a neural net. In hindsight, the name was a poor historical choice - it would be more accurate to call the algorithm something like \"Bandit-based Tree Search\", but for years now pretty much universally everyone has continued to use \"MCTS\" to refer to the modern deterministic versions. Anyways, to run the overall MCTS algorithm, we perform as many playouts as we can before exhausting our compute budget for the turn. We select the final action by looking at the root node and picking the child node with the highest number of visits $N$ (note: NOT by picking the child with the highest $Q$, a child node with a high $Q$ and low $N$ will often be a blunder that just got a noisily high utility average due to insufficient search). Additionally, the visit distribution over actions at the root node, indicating what proportion of child visits from the root node went to each child, $N(a) / \\sum_b N(b)$, (e.g. pawn to e5 got 40% of the total visits, knight to d4 got 15%, etc.), can be used as the target distribution to train a neural net's policy to predict, as in the AlphaZero training loop. In general the overall visit distribution produced by MCTS at any particular node often corresponds well with the range of plausible best actions in a position. This distribution often looks very much like the \"policy\" distribution of a strong agent, and in particular, a much-sharpened and refined version of the prior policy P. For reasons that we discuss more below, this is not a coincidence. This means one can do other things with this distribution that one might do with a policy. For example, it's common to sample from a policy distribution from a model using a small positive temperature. Sampling from the visit distribution with a small nonzero temperature similarly can be a good way to introduce variety in the final selected action with very little cost to overall strength. What Goes Wrong With Graphs? Suppose we apply the above algorithm exactly as described, except on a directed acyclic graph instead of a tree. The algorithm is identical except that rather than always allocating a new node when reaching a new gamestate: if action not in node.children: new_game_state = node.game_state.play(action) node.children[action] = Node(N=0,Q=0,game_state=new_game_state) ...instead we check if the gamestate occurred anywhere else in the search. If it does, we just point to the pre-existing node. We assume that each gamestate has a unique hash and that we have a global table nodes_by_hash. The new code would look like: if action not in node.children: new_game_state = node.game_state.play(action) if new_game_state.hash in nodes_by_hash: node.children[action] = nodes_by_hash[new_game_state.hash] else: new_node = Node(N=0,Q=0,game_state=new_game_state) node.children[action] = new_node nodes_by_hash[new_game_state.hash] = new_node Why does this not work? Consider the following initial situation. Square nodes are where the player to move prefers high Q values. Circle nodes are the opponent's who prefers low Q values. Initial situation We have 3 nodes, with Q values around 0.38 or 0.39. Currently, at node A the player prefers the action that goes to node C, and node A's Q value is dominated the roughly 30 playouts it has received, almost all of which went to exploring node C. Node C also was visited by about 40 other playouts from a transposing path. Now, suppose node C receives a lot more playouts from transposing paths, in the process, deeper below node C a new tactic is discovered that causes node C's utility to rise a lot, to 0.51: Suppose C gets more playouts and its utility rises 0.39 -> 0.51 Now we have a strange situation. Initially, node A's Q value was 0.39 almost entirely because the player could play the action that led to move node C with a Q value of 0.39. Now, we've revised our estimate of node C and believe its utility is around 0.51. It's still the case that node C is the most-visited and most-preferred move at node A, therefore node A's utility estimate should also be around 0.51. But because the playouts updating node C did NOT go through node A, we did not revise our utility estimate for node A! Moreover, suppose node A receives some playouts next. It's quite possible that following PUCT or similar move exploration formulas, node A would spend them exploring nodes other than node C: Because C has many playouts, A might prefer to explore worse nodes, biasing its Q down! This is because although PUCT (for the square node player) prefers exploring actions with high Q values, it also prefers exploring actions with fewer visits so far. As the total sum of visits increases, PUCT gradually becomes willing to also spend a few visits trying some worse actions to ensure no tactic is missed. Depending on the values of parameters like the prior policy P and $c_{\\text{PUCT}}$, the large number of visits on node C could well induce these visits to try other probably worse actions, because node C has \"already been explored enough\". This means that the Q value of node A might even tend to go down over the next playouts because it is receiving playouts from actions that are likely to be bad, instead of increasing up to the \"correct\" evaluation of 0.51. If C received enough more playouts quickly enough relative to A, node A could in theory continue to have a significantly worse utility indefinitely. All else equal, under this naive way of extending MCTS from trees to graphs, if a node's top-preferred moves are visited a lot by transposing lines, the node will tend to favor exploring other moves that didn't receive as many visits instead, leading to artificial biases in the utilities of the playouts being averaged at different nodes. Our algorithm is unsound to the point where even with unbounded amounts of search, it's not even obvious that we will converge to the optimal move. Another Non-Working Attempt to Fix Naive Graph Search: Here's another try. What if we make it so that whenever a node is updated due to any playout, we count that playout for all parents (and recursively their ancestors) rather than just the parent that the playout came down through? That way, in a case like above, node A would have its utility updated during the transposing visits to node C. But consider the following situation: Another initial situation Node D has Q = 0.55. Since the square player is maximizing, this is roughly consistent with the fact that the best node under it, node E, has Q = 0.56. Node D also has spent one playout exploring node F, discovering that it is a transposition to a node F which earlier received 9 other visits from a different branch of the search, for a total of 10. Now, suppose node F gets 100 more visits from the other branches of search: F gets 100 more visits with similar low utility, and D's Q value gets corrupted. When we count all these low-utility playouts for all parents of node F, since D is a parent, this corrupts the value of node D! All the visits from node F drag down node D's Q value to 0.35, even though the best guess of what it should be based on information so far is that it should be 0.55 or 0.56, because node D leads to node E and node E has Q = 0.56. On its own, node D would never have \"wanted\" to put so many playouts into node F. This algorithm is unsound as well. One could further try many arbitrary hacks to also patch this issue, but even if we stumble our way to the correct algorithm, without a better foundation, it will be hard to be sure when or why. Back to Theory - MCTS as Regularized Policy Optimization Let's go back and look at a little bit of modern theory on why MCTS works, so that we can derive the correct generalization instead of guessing at hacks. The following paper offers a foundational machine-learning-flavored perspective on MCTS: Monte-Carlo Tree Search as Regularized Policy Optimization (Grill et. al, 2020). For us, the relevant high-level insight is that when MCTS updates its stats at different nodes, it is actually running a form of online policy learning. At any node, as we repeatedly apply the PUCT formula over successive iterations to pick child nodes to visit, the cumulative visit distribution chosen by PUCT approximates and converges toward the solution to the following optimization problem: $$\\text{Find the policy π that maximizes:} \\\\\\ \\sum_{a} \\pi(a) Q(a) - \\lambda_N D_{\\text{KL}}(P || \\pi)$$ Where: $\\sum_{a} \\pi(a) Q(a)$ is sum-product of the utility estimate Q of the child node the action leads to and the probability of $\\pi$ to play that action. This is simply the estimated expected utility of following the policy, and $\\pi$ is trying to maximize this. $D_{\\text{KL}}(P || \\pi)$ is the (reversed)2 KL-divergence, a measure of how different $\\pi$ is relative to the prior policy $P$. $\\pi$ is trying to reduce this and stay close to $P$ at the same time as it maximizes utility. $\\lambda_N$ is a coefficient determining the strength of the KL-divergence term relative to the expected utility. It decays at a particular rate as the number of visits N increases, so that as we search more and collect more confident evidence about the utilities of the actions, $\\pi$ becomes increasingly willing to deviate more from the prior policy $P$ for smaller estimated improvements in utility. The above is true for the PUCT formula for AlphaZero-style MCTS, but various analogous things are also true for other forms of MCTS that use different exploration formulas. In other words, the visit distribution of MCTS is a \"posterior\" policy that, approximately, takes the prior policy P from the neural net and gradually improves it to maximize expected utility as more visits accumulate and give better evidence of the true utilities of the child nodes. Effectively, MCTS is running a little learning algorithm locally at every node of the tree simultaneously, starting from the neural net's best guesses of policy and utility value, and improving it further. This also gives context to our earlier observations on why the visit distribution can be treated so much like a high-quality policy, and for example why in AlphaZero the visit distribution is a good policy training target for future neural net training. Except for the discretization of the visits, it basically is the policy of a continuous learning algorithm that broadly resembles a lot of classical regularized reinforcement learning algorithms. As an aside, it's also possible to compute the exact solution to the above optimization problem and use the resulting exact solution as the policy instead of the visit distribution, which is merely an approximation that converges in the limit. This is what the Grill et. al paper indeed advocates for, although doing so may come with some challenges3 in practice that aren't accounted for by the theory. Taking another look at Q Let's also dig a bit deeper into what the running statistics in MCTS might mean from this perspective, especially Q. Recall that: When we visit any node $n$ for the first time, the visiting playout $p$ returns the raw neural net utility prediction $U(n)$ for that game position. All subsequent playouts visiting $n$ will explore one of the children of $n$ and return the raw neural net utility estimate of some descendant of $n$. Notationwise, let $\\mbox{Playouts}(n)$ be the set of all playouts that visit $n$, and let $U(p)$ be the utility returned by any playout $p$. Earlier, we defined $N$ and $Q$ for any node $n$: Each node tracks: N - the number of visits so far to this node, i.e. playouts that ended on or passed through this node. Q - the running average of the utility values sampled by those playouts. In other words, $$Q(n) = \\frac{1}{N(n)} \\sum_{p \\in \\mbox{Playouts}(n)} U(p)$$ But we can also rewrite Q in a different way: $$\\begin{align*} Q(n) &= \\frac{1}{N(n)} \\sum_{p \\in \\mbox{Playouts}(n)} U(p) && \\text{\\small(by definition of Q(n))} \\\\\\ &= \\frac{1}{N(n)} \\left( U(n) + \\sum_{c \\in \\mbox{Children}(n)}\\,\\, \\sum_{p \\in \\mbox{Playouts}(c)} U(p) \\right) && \\text{\\small(as recalled above)} \\\\\\ &= \\frac{1}{N(n)} \\left( U(n) + \\sum_{c \\in \\mbox{Children}(n)}\\,\\, N(c) \\left( \\frac{1}{ N(c) } \\sum_{p \\in \\mbox{Playouts}(c)} U(p) \\right) \\right) \\\\\\ &= \\frac{1}{N(n)} \\left( U(n) + \\sum_{c \\in \\mbox{Children}(n)}\\,\\, N(c) Q(c) \\right) && \\text{\\small(by definition of Q(c))} \\\\\\ \\end{align*}$$ Therefore: $$Q(n) = \\frac{1}{N(n)} \\left( U(n) + \\sum_{c \\in \\mbox{Children}(n)}\\,\\, N(c) Q(c) \\right)$$ In other words, rather than thinking of Q as the average utility of all playouts that visited $n$, we can think of it in recursive terms as the weighted average of the Q values of $n$'s children (weighted by the child visit count). Plus a small $1/N(n)$ weight on the raw neural net utility estimate of $n$ itself, which is essentially a regularizing prior for Q when the visit count is low. This effectively gives us a new, recursive definition of Q in terms of child nodes' Q, rather than as an average of playouts. However, weighting by the child visit count is the same as weighting by the visit distribution. And the visit distribution is the \"posterior policy\" that MCTS is optimizing! In other words, the interpretation of Q is that it is the (slightly regularized, estimated) expected utility of the posterior policy that MCTS is optimizing. Each node continually updates and reports this improved Q value, which is then used by the parent node for its own policy optimization, and so on. In mathematical notation, letting $\\hat{\\pi}$ be the visit distribution that is our posterior policy, this looks like: $$Q(n) \\approx \\frac{1}{\\sum_c N(c)} \\sum_{c \\in \\mbox{Children}(n)}\\,\\, N(c) Q(c) \\, = \\, \\sum_{a \\in \\mbox{Actions}(n)} \\hat{\\pi}(n,a) Q(n,a)$$ Overall, this gives us an equivalent but different perspective on how MCTS works. MCTS is continuously optimizing a policy at every node to maximize the Q utility values that child nodes are reporting, while continuously updating the node's own Q to be the latest best guess of the expected utility achievable by that policy. In the limit of infinite search, if the child node Q values converge to game-theoretic optimal, then the node's policy converges to optimal, so the node's Q converges to the game-theoretic optimal value too, and so by recursion/induction we can see easily that MCTS is sound. Doing Monte-Carlo Graph Search Correctly Okay, let's take the above perspective and derive a sound way to do MCGS with directed acyclic graphs! All of the problems when extending MCTS naively to graphs are a result of implicitly assuming that the only visits to children of a parent node come from the parent. When a child node can instead also receive visits from a transposing path, we have problems: The visit counts of the child nodes can deviate arbitrarily much from what PUCT would have wanted to allocate, and so the child-visits distribution can no longer be interpreted as a reasonable posterior policy. The Q values of parent and child nodes are updated in inconsistent ways, such that the Q value can no longer be interpreted as the expected value of the posterior policy either. So let's fix this! The theory still guarantees that the cumulative counts of the actions that PUCT selects from a given node is what gives a posterior policy that approximates the optimized policy $\\pi$, therefore that is what we need to track rather than conflating it with child visits. The optimization is sound when the Q value reported by a node is the estimated expected value of the posterior policy. Therefore once we have the posterior policy and the child Q values, rather than dealing with how to count individual playouts, we can just apply the recursive formulation of Q. We can still also include the regularizing 1/N weight on U(n). So, at each node $n$, we now track: $N(n)$ - the number of visits so far to this node, i.e. playouts that ended on or passed through this node. $N(n,a)$ - action visit counts - for each action $a$, how many times PUCT at node $n$ chose action $a$. We can also call these \"edge\" visit counts since actions correspond to edges of the graph. The distribution of these is our posterior policy. $Q(n) = \\frac{1}{N(n)} \\left(U(n) + \\sum_{a} N(n,a) Q(n,a) \\right)$ where $U(n)$ is the raw neural net utility estimate of the position, and $Q(n,a)$ is, recursively, equal to $Q(c)$ for the child node $c$ reached by playing action $a$ from node $n$. This is the regularized expected utility of our posterior policy. And when computing PUCT, we use these edge visit counts rather than the child visit counts: $$\\text{Action to explore}=\\text{argmax}_a \\, \\text{PlayerToMove}(n) * Q(n,a) + c_{\\text{PUCT}} P(n,a) \\frac{\\sqrt{\\sum_b N(n,b)}}{1 + N(n,a)}$$$ A basic working algorithm might look like: def perform_one_playout(node): if is_game_over(node): node.U = get_utility_of_game_outcome(node.game_state) else if node.N == 0: # New node not yet visited node.U = get_utility_from_neural_net(node.game_state) else: action = select_action_according_to_puct(node) if action not in node.children_and_edge_visits: new_game_state = node.game_state.play(action) if new_game_state.hash in nodes_by_hash: child = nodes_by_hash[new_game_state.hash] node.children_and_edge_visits[action] = (child,0) else: new_node = Node(N=0,Q=0,game_state=new_game_state) node.children_and_edge_visits[action] = (new_node,0) nodes_by_hash[new_game_state.hash] = new_node (child,edge_visits) = node.children_and_edge_visits[action] perform_one_playout(child) node.children_and_edge_visits[action] = (child,edge_visits+1) children_and_edge_visits = node.children_and_edge_visits.values() node.N = 1 + sum(edge_visits for (_,edge_visits) in children_and_edge_visits) node.Q = (1/node.N) * ( node.U + sum(child.Q * edge_visits for (child,edge_visits) in children_and_edge_visits) ) return And that's it! Discussion of Implementation Choices While formulated very differently, the pseudocode algorithm above is at a high level doing much the same thing as the algorithm from Czech, Korus, and Kersting, modulo several of the implementation choices discussed below and a few other minor details. Given that their algorithm is also sound, this is not too surprising, since as we found in the above derivation, the foundational theory for MCTS very naturally forces the analogous MCGS to work a certain way. Hopefully, going through the theory of MCTS makes it more intuitive what implementation choices were \"load-bearing\". There are also many non-load-bearing degrees of freedom in the implementation, which we discuss here. Stale Q Values One might notice that the given pseudocode algorithm still only updates the Q values the nodes on the path traversed by the playout, rather than all parents or ancestors of the nodes of the playout. This means that the Q values of nodes on non-traversed paths will become \"stale\". This is still theoretically sound because: PUCT and/or other standard exploration formulas guarantee that in the limit, even if they choose \"better\" actions exponentially more, they will still try every action infinitely often. This means that a node will always eventually be visited and can never be stale forever. We directly compute the correct Q value for a node given the Q values of its children any time the node is visited. It does not depend on the history of traversals or playouts. So no matter what state the node graph is in, once a node is eventually visited and made un-stale, if recursively its children have been visited and updated to the correct Q value, then the node will be updated to the correct Q value. In the limit it's not too hard to then show that on a DAG we must still converge to game-theoretic optimal. The main reason for only updating nodes on the playout path is implementation simplicity and because doing otherwise might be more computationally costly. Czech, Korus, and Kersting similarly only update the node Q values on the playout path. However, stale Q values do make the search inefficient. After all, shared Q values are a big part of what is giving graph search an advantage over tree search. Depending on the situation, there may be a lot of room for improvement in strategies for reducing staleness, for example: One could also maintain pointers from nodes to immediate parents and also update immediate parent nodes' Q values to help propagate improved Q values up the DAG a bit faster. One could even update all ancestors recursively in a topologically sorted order so that nothing is ever stale (although this may be computationally expensive!). One could stick with the cheapest option of only updating nodes on the playout path, but also have a separate parallel thread that walks the tree continuously looking for stale nodes and updating them. Etc. Incremental vs Idempotent Updates Another thing to notice about the given pseudocode algorithm is that it uses an idempotent update for N and Q. In this implementation, regardless of what has happened before or how many intermediate updates have happened, a single visit to a node always ensures that its N and Q are correct as a function of its children. It is also possible to formulate an incremental update that is equivalent, or at least one that is equivalent in the limit of many visits. This is analogous to how for the original MCTS on trees, the following two updates are equivalent following a single playout: # Recursive, idempotent update node.Q = (1/node.N) * (node.U + sum(child.Q * child.N for child in node.children)) # Incremental update. U is the final utility returned by the playout. node.Q = (1/node.N) * U + (node.N-1)/node.N * node.Q Here, the incremental update is a bit computationally cheaper since it doesn't require iteration over children. In the case of graphs, rather than trees, formulating a cheaper incremental update that behaves equivalently, or behaves equivalently in the limit, can be a little tricky, but it can be done and might result in higher performance. Czech, Korus, and Kersting do in fact use a somewhat incremental formulation, since they approached the algorithm more directly from the historical \"running statistics\" presentation of MCTS, and so perhaps could be a reference for how to do this. Czech et. al. also make a somewhat interesting further choice to store Q values on edges, not just edge visit counts, and add an incremental mechanism for how a stale Q value can gradually \"catch up\" to more recent values (in contrast to just computing the correct Q value and catching up immediately), as well as introducing a hyperparameter controlling an error tolerance at which the mechanism doesn't apply (where theoretically staleness biases smaller than the tolerance could persist indefinitely). As far as I can tell, many of these extra complications are not needed for correctness and can be conceptualized as \"performance hacks/optimizations\", or in some cases as changes that mildly alter exploration and the way Q values converge that might either have small beneficial effects or small costs/harms, depending on the situation. But as seen from the pseudocode given in this document, it's possible to get MCGS working with a simple algorithm, and without being forced to introduce any new error tolerance parameters or needing to track Q values on edges. If one does NOT need to squeeze out the extra computational performance (e.g. if the code is GPU-bound on a neural net anyways such that CPU doesn't matter so much), then there are also some advantages to the idempotent update. For example, an idempotent update is easier to reason about, especially when also multithreading or when trying things like having a parallel thread to walk the tree and unstale-ify nodes as mentioned above. KataGo currently uses the idempotent formulation. Continuing vs Stopping Playouts when Child Visits > Edge Visits In standard MCTS, adding an edge visit is the same thing as adding a child visit. These two concepts are the same on trees and we only needed to distinguish them when we move to graphs. We can rephrase this in the language of policy optimization: whenever one upweights a given move in the node's posterior policy (by adding an edge visit), one also gives that move an incrementally larger bit of exploration to validate the Q value that it claims to have (by adding a child visit). As mentioned in footnote 3, that the two increase together is good for robustness, and generally it's necessary for convergence/soundness that the child visit count tends to infinity as the edge visit count tends to infinity. However, when transpositions occur, it's common to encounter a child node already having more visits than the edge that leads to it. Can we say that the child node already has \"enough\" visits in this case and cut short the playout, to improve the efficiency of the search? Yes! We can just increment the edge visits and immediately skip to the step of updating the parent and it's ancestors without giving the child another visit. The child already has enough visits to \"support\" the upweighted edge visits that the parent wants to assign it. However, it's not obvious that this is a good idea. Speculatively, there might be some competing considerations: If the edge visit count is low, while the child visit count is high, then the marginal extra visit to that child is probably less informative and less likely to be worth the compute cost given that it's already high enough for that parent, favoring stopping. Nodes that tend to have child visits > edge visits are likely the nodes that are being transposed to more often. This means they affect more parents, making it more important to evaluate them accurately, favoring continuing. This is likely a good area for experimentation! One could also imagine using a more complex threshold, such as stopping if the child visits is enough larger, etc. It's possible that the best approach depends on the situation and/or the game. For reference, KataGo currently DOES stop the playout by default, but offers a configuration option to continue the playout instead, or to \"leakily\" stop the playout some probabilistic fraction of the time. The given pseudocode algorithm above for MCGS does NOT stop the playout, but doing so is as simple as adding a one-line check, if you want to experiment with this: if child.N <= edge_visits: perform_one_playout(child) Other Implementation Details There are also some other implementation details worth mentioning beyond the basic pseudocode given above. For game-over nodes the above pseudocode recomputes them always to have N = 1 and U = Q = game outcome utility, no matter how many times they are visited. This is fine since it does not prevent the parents' edge visit count leading to these nodes to increase as normal, but one could also adopt the convention to increment N for a game-over node each time it is visited. Doing this and averaging the game outcome U values sampled would be important if the game outcome is stochastic and for some reason we can only stochastically sample it rather than directly compute the correct expected utility of the outcome. Alternatively, if get_utility_of_game_outcome is deterministic but expensive, we could as a minor optimization skip computing it if it's already computed. It's also possible to add broader handling for game-over utilities to propagate provable values up the graph faster. When game-over states are relevant, plain MCTS/MCGS doesn't converge to optimal as cheaply as classical searches like alpha-beta search, since there is no provision for recognizing when utility values are certain rather than needing more visits to refine their quality and trustworthiness. We assumed gamestates had unique hashes for finding transpositions. Crafting a true collision-free hash for a complex game state may be tricky and costly. However, a sufficiently-large Zobrist hash, such as 128 or 192 bits, is usually sufficient in practice to entirely prevent collisions, except perhaps from adversarial gamestates generated to attack the hash. Depending on one's level of paranoia, and/or whether the game rules already allow cycles and how that is being handled, one can also add cycle detection to avoid unbounded recursion if a cycle does occur due to collision. Conclusion I hope at least some people find this document and explanation of Monte-Carlo Graph Search useful! This document still doesn't cover some of the game-specific tricky bits that need to be handled regarding what happens when the game can cycle instead of being only acyclic, such as superko in Go4, or third-time repetition in Chess. But I hope it gives some intuition behind how MCGS works, and what kinds of implementation choices could be interesting to vary and experiment with. Addendum (2024-03-10) - Handling Cycles In case it's further also helpful to someone, here's a link to a Google Doc with some thoughts on how to handle N-fold repetition and cycles in games like chess or in general. These thoughts are very much not as polished and doing this well may require some game-specific experimentation on heuristics: https://docs.google.com/document/d/1JbxsoMtr7_qAAkfYynAgpvuarMMJycaL5toXdnqVJoo/edit Footnotes 1: It's also common to see code that tracks NxQ instead of Q, i.e. the running sum of playout utilities rather than the running average, and only divides by N at the end when querying the utility. This leads to a slightly simpler update, but in some cases may make certain lock-free multithreading implementations harder to reason about because it opens the chance for split reads: namely reading N and NxQ that are desynchronized, such that dividing NxQ by N gives a bogus value for Q. 2: In general, $D_{\\text{KL}}(Y || X) = E_Y (\\log(Y) - \\log(X))$ is \"How surprised will I be if Y is true, given I initially believe X?\". For example suppose Y is the same as X except that Y puts a little bit of probability mass on an outcome A that X considers to be earthshatteringly unlikely. Then: $D_{\\text{KL}}(Y || X)$ will be large because even if Y doesn't think A is too likely, if A does happen X will be astronomically surprised. $D_{\\text{KL}}(X || Y)$ will be small, because for every outcome X considers to be possible, Y also considers it almost equally possible too. Y will never be unduly surprised by X. Given the roles these play, often one sees something like $D_{\\text{KL}}(\\text{Posterior || Prior})$, \"How surprised will I be if Posterior is true, given I initially believe Prior?\". Penalizing this term would mean that Posterior is encouraged to be a \"subset/sharpening\" of Prior. Posterior strongly wants to ONLY consider moves that Prior considers, but does NOT get penalized as much for failing to consider all of them. Here though, we have a reversed KL divergence, $D_{\\text{KL}}(P || \\pi)$ where $\\pi$ is the Posterior and $P$ is the Prior. Penalizing this term means that Posterior is encouraged to be a \"superset\" of Prior. Posterior strongly wants to consider ALL moves that Prior considers, but does not get penalized as much if it also considers some moves that Prior would think to be earthshatteringly unlikely. Both KL divergences behave very similarly in the common case. But in the edge cases where they differ, the reversed KL divergence is arguably the better one for exploration (by making sure it considers ALL moves in the Prior) and for recovering from partial overfitting/overconfidence of the neural net (by tolerating when it considers moves that Prior thinks are entirely impossible), and this turns out to be the one MCTS implicitly uses. 3: A big challenge in practice with using the exact solution seems to be that using the direct solution to $\\text{argmax}_{\\pi} \\sum_a \\pi(a) Q(a) - \\lambda_N KL(P || \\pi)$ can sometimes put a large policy weight on a move with relatively few visits if its Q appears good enough. However, moves with high Q values but low visits are often erroneous, such as when a shallow search blunders by overlooking a key tactic. This problem maybe manifests more often at larger amounts of search the very shallow 50-visit searches tested in the paper. From a theory perspective, we could say perhaps this is because the optimization problem doesn't account for differing uncertainty based on the number of visits. There can also be problems due to correlations or adverse selection in the uncertainty in utilities, e.g. the same tactic occurs in many possible branches and throughout the search all the seemingly-highest-Q branches are precisely the branches overlooking that tactic and blundering. Using the visit distribution as the posterior policy is far more robust to this, because the only way to increase the weight of a move in the visit distribution is to actually search the move extensively. This means that a move cannot get a high weight until a deeper analysis of that move confirms the high Q value and cannot find any flaws in it. 4: For those curious, KataGo handles it in Go by leveraging an easily-proved theorem specific to Go: \"Suppose a move has been played on some location. Let E be the number of empty spaces in all connected empty regions that border that move, and let S be the number of stones in the chain that contains that move. Then, even if the players cooperate, it will take at least S+E-1 moves to return to the position prior to that move\". Based on this theorem, which can lower-bound the lengths of cycles, it's possible to construct a hash of the history that will almost never prevent sharing nodes for transpositions in normal situations, and yet is completely reliable at not sharing nodes when cycles are relevant, for all cycles up to a configurable desired cycle length.",
    "commentLink": "https://news.ycombinator.com/item?id=39662698",
    "commentBody": "Monte-Carlo graph search from first principles (github.com/lightvector)287 points by bumbledraven 12 hours agohidepastfavorite18 comments fabmilo 10 hours agoI believe this kind of graph exploration is what we need to progress reasoning in AI. Plain LLMS will fail. The link has tons of good references, including the Zobrist hashing https://en.wikipedia.org/wiki/Zobrist_hashing for game tables. We need to find a good hashing for language based state description so that graph exploration doesn't explode computationally. Another good read for Tree Search is Thinking Fast and Slow: https://arxiv.org/abs/1705.08439 and Teaching Large Language Models to Reason with Reinforcement Learning: https://arxiv.org/abs/2403.04642 comparing the MCTS approach to other current RL strategies. reply vjerancrnjak 1 hour agoparentThis looks too low level. What might be a step forward is a joint learning of the state representation with the search algorithm. Search algorithm explores the NN representation of the state for which you can get the cost. https://sites.google.com/view/genie-2024/ Genie from DeepMind is a good demonstration where discrete state is being modeled. NN learns a very complex representation with collision detection and actions. Instead of decoding that state into pixels, search could probably be done directly on that state. Of course, this architecture could be very different. reply anonymous-panda 4 hours agoparentprevWould it be impossible to marry the two somehow? It’s hard for me to believe that the brain only uses a single technique for everything and likely has many different tools in its toolbox with a selector on top to know how to leverage each appropriately. reply pixelpoet 10 hours agoprevImmediately recognise the author in the HN url as the genius behind KataGo: https://github.com/lightvector/KataGo His posts on https://www.reddit.com/r/cbaduk/ are consistently excellent. reply bingbingbing777 8 hours agoparentThe URL is literally within the KataGo repo. reply pixelpoet 7 hours agorootparentThe HN URL preview for me \"literally\" says \"(github.com/lightvector)\", with no upfront mention of KataGo. sigh reply modeless 5 hours agoprev> Also, as far as the name \"Monte-Carlo Tree Search\" itself, readers might note that there is nothing \"Monte-Carlo\" in the above algorithm - that it's completely deterministic! MCTS as commonly implemented is deterministic? How strange! I assumed there was randomness in the sampling. reply kadoban 5 hours agoparentOriginally MCTS did have randomness, yes. I think the article mentions it, but this came in in the form of playouts at the end, used just to evaluate positions. This has been replaced in current similar projects by NN evaluations, which are higher quality (as you can probably imagine, just playing random moves to see who wins is not very good, it was just the best strategy known at the time). So the monte-carlo turned out to be an unessential (and suboptimal) part of what is now still called MCTS, making the name a bit unfortunate. reply jacoblambda 4 hours agorootparentIn essence the NN evaluations are acting as a PRNG with values heavily biased by an established heuristic. So while it's very much a specialization of MCTS and probably should go by a different name, one could argue it's still using monte carlo methods. reply pavlov 55 minutes agorootparentSounds like the dice are loaded in Monte Carlo. reply mzl 2 hours agorootparentprevSmall side note, standard MCTS is often implemented with heuristic guiding the moves in the playouts and not just random moves, as that gives more interesting information. reply jacoblambda 4 hours agoparentprevIt's technically a different algorithm just under the same \"monte carlo\" name. However something interesting of note is that since most applications of monte carlo methods rely on PRNGs instead of true RNGs, they are technically deterministic (as given the same PRNG seed you should always get the same result for a given input). So what this algorithm is doing instead of using a normal PRNG and a separate heuristic, is to instead query the neural network. This works because the neural net is an heuristic over a massive search space which ends up acting like a very poor PRNG that is heavily biased towards specific results based on the training of the neutal net, which in turn ends up looking like a PRNG with a set of heuristics applied. The important thing to note is that this is a specialisation of MCTS and as such shouldn't technically work for all use cases. reply pvitz 23 minutes agorootparentI can't speak for this use case, but in many other places, one isn't even using a PRNG, but rather Quasi-Monte Carlo techniques with no randomness at all. Monte Carlo doesn't need randomness, but a good coverage of the search space. reply westurner 5 hours agoparentprevIf there's randomness, is there convergence and after how much (CPU, RAM, GPU, TPU, QPU) resource-time? reply rphln 11 hours agoprevSomehow the paper they mention completely flew under my radar when I was researching MCTS. Surely it's gonna be a lot of fun to give this modification a spin on my next opportunity. reply behnamoh 9 hours agoprev [–] A bit introduction about this would be nice. reply bionhoward 9 hours agoparent [–] When we make game-playing AI (which is all AI, depending on your analogy comfort), one of the most promising techniques is Tree Search, which ranks moves based on the descendant moves. In games where you could reach the same state in many ways, much memory might be wasted to re-record the same state node on different branches. This article is a nice exploration of an approach called Graph Search, which essentially trades compute for memory by doing extra compute work (hashing the game states) to check to see if the nodes are already visited. That saves us from re-recording nodes we already saw, and consequently converts trees (free of cycles) into directed acyclic graphs. This forces some tinkering with the tree search to get correct results, specifically it demands a focus more on edges (actions or moves) as the unit of optimization, rather than on vertices (states). It’s a well written technical essay in literate programming written by someone who understands their subject. reply 3abiton 1 hour agorootparent [–] This was an amazing high level recap with context about the background and importance of the technique! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into Monte-Carlo Graph Search (MCGS), a derivative of Monte-Carlo Tree Search (MCTS) used in directed graphs, addressing challenges in adapting MCTS to graphs and proposing solutions to biases in the algorithm.",
      "It also covers strategies for updating Q values, the significance of grasping underlying principles to enhance MCTS, implementation considerations for MCGS, and the impact of third-time repetition in chess.",
      "Furthermore, it discusses neural network challenges in MCTS like overfitting and overconfidence, emphasizing the necessity of precise playout utilities and utilizing visit distribution for improved outcomes."
    ],
    "commentSummary": [
      "Monte-Carlo graph search enhances AI reasoning by utilizing graph exploration, emphasizing efficient hashing for language-based state descriptions.",
      "Neural networks are integrated into search algorithms to replace randomness with heuristic evaluations, improving result accuracy.",
      "This approach is a specialized version of Monte-Carlo Tree Search, underlining the significance of comprehending the algorithm nuances and its practical uses."
    ],
    "points": 288,
    "commentCount": 18,
    "retryCount": 0,
    "time": 1710106426
  },
  {
    "id": 39658787,
    "title": "Tenstorrent introduces Grayskull: RISC-V GPU Alternative",
    "originLink": "https://www.techradar.com/pro/firm-headed-by-legendary-chip-architect-behind-amd-zen-finally-releases-first-hardware-days-after-being-selected-to-build-the-future-of-ai-in-japan-tenstorrent-unveils-grayskull-its-risc-v-answer-to-gpus",
    "originBody": "Pro Firm headed by legendary chip architect behind AMD Zen finally releases first hardware — days after being selected to build the future of AI in Japan, Tenstorrent unveils Grayskull, its RISC-V answer to GPUs News By Wayne Williams published 1 day ago Grayskull-powered DevKits are available to two versions - e75 and e150 (Image credit: Tenstorrent) Tenstorrent, the firm led by legendary chip architect Jim Keller, the mastermind behind AMD's Zen architecture and Tesla's original self-driving chip, has launched its first hardware. Grayskull is a RISC-V alternative to GPUs that is designed to be easier to program and scale, and reportedly excels at handling run-time sparsity and conditional computation. Off the back of this, Tenstorrent has also unveiled its Grayskull-powered DevKits - the standard Grayskull e75 and the more powerful Grayskull e150. Both are inference-only hardware designed for AI development, and come with TT-Buda and TT-Metalium software. The former is for running models right away, while the latter is for users who want to customize their models or write new ones. The Santa Clara-based tech firm's milestone launch comes hot on the heels of a partnership with Japan's Leading-edge Semiconductor Technology Center (LSTC). Tenstorrent's RISC-V and Chiplet IP will be used to build a state-of-the-art 2nm AI Accelerator, with the ultimate goal of revolutionizing AI performance in Japan. By the power of Grayskull! The Grayskull e75 model is a low-profile, half-length PCIe Gen 4 board with a single Grayskull processor, operating at 75W. The more advanced e150 model is a standard height, 3/4 length PCIe Gen 4 board containing one Grayskull processor operating at up to 200W, and balancing power and throughput. Tenstorrent processors comprise a grid of cores known as Tensix Cores and come with network communication hardware so they can talk with one another directly over networks, instead of through DRAM. The Grayskull DevKits support a wide range of models, including BERT for natural language processing tasks, ResNet for image recognition, Whisper for speech recognition and translation, YOLOv5 for real-time object detection, and U-Net for image segmentation. The Grayskull e75 and e150 DevKits are available for purchase now at $599 and $799, respectively. More from TechRadar Pro Virtually unhackable' chip could make GPU power efficient and faster at AI Nvidia's fastest AI chip ever is finally available for preorder Will Arm start to build its own chips soon? Are you a pro? Subscribe to our newsletter Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Wayne Williams Editor Wayne Williams is a freelancer writing news for TechRadar Pro. He has been writing about computers, technology, and the web for 30 years. In that time he wrote for most of the UK’s PC magazines, and launched, edited and published a number of them too. MORE ABOUT PRO Google’s new bulk sender guidelines spell trouble for B2B Huawei has a game-changing 10 Petabyte storage product — OceanStor Arctic uses exciting new technology that can beat tape AND hard drives LATEST One simple trick to make your bedtime routine the best part of the day SEE MORE LATEST ► TOPICS AMD MOST POPULAR Millie Bobby Brown's new Netflix movie Damsel is a dud – watch these 3 great fantasy movies instead By Amelia SchwankeMarch 10, 2024 Firm headed by legendary chip architect behind AMD Zen finally releases first hardware — days after being selected to build the future of AI in Japan, Tenstorrent unveils Grayskull, its RISC-V answer to GPUs By Wayne WilliamsMarch 10, 2024 Quordle today – hints and answers for Sunday, March 10 (game #776) By Marc McLarenMarch 10, 2024 We just got another hint that the Sonos headphones are launching soon By David NieldMarch 09, 2024 Obscure Chinese brand launches laptop that may have a better battery life than the MacBook Pro — Core Ultra 7 totting Megabook T16 Pro has record-breaking 99.9WHr battery, up to 22 hours on a single charge By Wayne WilliamsMarch 09, 2024 James Gunn's Superman: DCU movie release date, confirmed cast, plot rumors, and more By Tom PowerMarch 09, 2024 This hidden iPhone feature will change the way you screenshot By Axel MetzMarch 09, 2024 The next iPad Air is rumored to come with a key camera change By David NieldMarch 09, 2024 Arrow Lake could offer huge performance gains, but AMD’s Zen 5 might still defeat Intel’s mighty next-gen CPUs By Darren AllanMarch 09, 2024 Obscure laptop brand is first out of the blocks with revolutionary Qualcomm 5G modem — 5G RedCap Snapdragon X35 is 4G on steroids and deserves better than the NL73 By Wayne WilliamsMarch 09, 2024 ICYMI: the week's 8 biggest tech stories, from the new MacBook Air to Facebook's big outage By David NieldMarch 09, 2024 MOST POPULAR MOST SHARED 1 'World's first': AWS rival launches bare metal servers based on Chinese RISC-V CPU and costs only cents per hour to run — but will it live to regret using eMMC storage? 2 'Self-destruct' chips could help combat a criminal plague that costs businesses billions every year — new technique could mitigate counterfeiting through electromigration 3 Sony could soon announce the ultimate lens for wedding and event photographers – the world's first 24-70mm f/2.0 4 'Another startup that will cause gaming GPU prices to spike': AI firm claims Radeon RX 7900 XTX GPUs are better value than Nvidia's H100 — nearly six hundred backers believe that is the case 5 This hidden iPhone feature will change the way you screenshot",
    "commentLink": "https://news.ycombinator.com/item?id=39658787",
    "commentBody": "Tenstorrent unveils Grayskull, its RISC-V answer to GPUs (techradar.com)255 points by Brajeshwar 20 hours agohidepastfavorite117 comments PhilippGille 19 hours agoDev kits: - Grayskull e75drawing 75W96 Tensix cores1 GHz clock96 MB SRAM8GB LPDDR4 @ 102.4 GB/s$599 - Grayskull e150drawing 200W120 Tensix cores1.2 GHz clock120 MB SRAM8GB LPDDR4 @ 118.4 GB/s$799 It will be interesting to see their inference performance, compared to graphics cards. Will they be interesting for home labs? I found one interview with unboxing a preview version (if I understood correctly), with some background info, but no performance numbers: https://morethanmoore.substack.com/p/unboxing-the-tenstorren... reply aleph_minus_one 18 hours agoparentThese specifications rather look like some AI acceleration ICs, and not like GPUs (recall that \"GPU\" stands for Graphics Processing Unit). reply layer8 11 hours agorootparentThat’s why the article characterizes them as an “alternative to GPUs […] for AI development“. That being said, it’s a Grayskull Processing Unit. ;) reply threecheese 16 hours agorootparentprevI wonder, is the dual-use of GPUs a significant engineering bottleneck for Nvidia et al? My understanding is that for libraries like Tensorflow and PyTorch, much of the complexity is supporting the many-to-many mapping of model structures to GPU silicon in a way that reduces “compile time”, with all the tradeoff landmines you might expect there. I would imagine that abstracting this at the hardware layer is really valuable. Are these non-GPU architectures the future of machine learning, and can we expect the big vendors to start competing here or is this a business risk for them? (robbing Peter to pay Paul, like Google investing in AI that cannibalizes their search revenue) reply dannyw 16 hours agorootparentI doubt it. Toolkits like TensorRT are specifically optimised, and NVIDIA already segregated their HPC vs gaming dies (and corresponding, die space). The H100s feature a minuscule amount of ROPs and TMUs (mainly for graphics); and doesn’t come with NVENC/DEC, etc. DirectX, Vulkan is not supported. You’ll just see more and more tensor cores, optimised CUDA cores for bfloat, etc. reply paulmd 12 hours agorootparentH100 does have NVDEC and jpeg ingest accelerators https://www.servethehome.com/wp-content/uploads/2023/10/NVID... Tbh it’s mildly surprising they even removed NVENC considering the overall size of the chip (in the absolute we are only talking about low-single-digit mm2 savings) and then H100 is still advertised and has features targeting VM graphics/visualization still… remember they also still put a full graphics pipeline with ROPs/TMUs on the chip, just no actual display hardware. reply lobochrome 9 hours agorootparentIt's a tradeoff with verification and validation, not so much die-space. If you include the IP, you need to run an expensive (time-consuming) simulation and emulation for the IP. Then, you need to validate it on first silicon and pray that it doesn't impact yield in HVM. These days, simulation runs basically nonstop on all the FPGAs the company is able to throw at it until tape out. Then, in post-silicon validation, you have engineering teams working 24/7 to validate the device on the bench. Finally, test engineering tries to achieve yield in HVM. Anything that you don't need on the spec sheet or the designer insists on is cut to save time to market. reply smoldesu 13 hours agorootparentprevThe dual-use of GPUs is what's driving Nvidia's demand, for the most part. They bet big on more outlandish designs (eg. CUDA/Tensor cores) and mixed-precision computation, and did a lot of the hardware/software work to get it shipped out years ago. There wasn't much interest in CUDA outside niche research/industry applications for a while, it's a small miracle they kept it around long enough to see the crypto and AI booms. Now their bet is paying big dividends, and other companies are trying to figure out fast ways to leverage raster compute for general-purpose acceleration. Apple initially developed OpenCL with Khronos as a similar-ish analog for GPU acceleration a while ago. There were a few partners that invested in it, but it suffered the same lack of demand as CUDA and languished for a while. Now Apple doesn't support OpenCL, so the purpose of a multi-platform acceleration library has kinda been scuttled. Nvidia played their cards well, and their competitors are going to feel the pain for a while unless they work together again. reply KeplerBoy 18 hours agorootparentprevSure, no one actually cares about the graphics part anymore in these contexts. Many Nvidia Datacenter GPUs are also no longer GPUs in that sense. reply bee_rider 12 hours agorootparentI know it isn’t the what you mean, but as an aside (rather than a contradiction), the gaming marked is still much bigger than the ML model training one, right? So presumably people care about the graphics part in the sense that development there funds the development of the cards, haha. I always assumed that was what killed Xeon Phi. Hard to justify a GEMM card in and of itself. However clever you get, NVIDIA will be back next year with twice as much bandwidth. reply SonOfLilit 12 hours agorootparentAccording to my googling, data center has been bigger than gaming for nvidia since 2022, and almost x2 in Q2 2023 following a crazy nosedive in gaming sales. reply bee_rider 11 hours agorootparentWoah, that’s really something reply FridgeSeal 9 hours agorootparentNvidia burnt quite a lot of goodwill with the 40xx series cards and pricing. Melting connectors, obscene pricing, marginal performance increase, zero availability; then when hardly any of that was addressed, Ti versions release which are equally expensive, and equally lacklustre. Combine that with patchy support for frame generation and 30xx series cards still going really strong, and you’ve got not a lot of incentive to upgrade. reply KeplerBoy 12 hours agorootparentprevNo, datacenter GPU revenue has surpassed gaming revenue some time ago. At least for Nvidia and I fully expect this trend to continue. reply krapht 9 hours agorootparentI'll bet against that. NVIDIA's margin is so large that Google/Microsoft and others can fund / are funding large teams to make TPUs and other dedicated hardware competitive. I also believe LLMs are part of the hype cycle just like crypto. In ten years the gaming market will return to being NVIDIA's primary market. reply hnfong 7 hours agorootparentA couple factors to consider: 1. The hardware isn't nVidia's primary moat. CUDA is. Currently all machine learning tooling assumes you have a CUDA device. 2. The hype cycle may be real, but nobody really knows how much better AI will become. The uncertainty is driving the hype (which is obviously on the optimistic side), but it could well be that models become so good that everyone \"needs\" an AI chip like how we \"need\" a smartphone today. 3. The gaming/graphics market is probably going to slowly shrink. There's only so many pixels you can push to the eye. There's very real diminishing returns there. At some point people literally won't be able to tell the difference between the newer/better/faster graphics and the older generation, at which point the GPU vendor's margins drop significantly. reply CuriouslyC 7 hours agorootparentprevIf by \"LLMs are part of the hype cycle\" you mean \"people are hyping them more than their current capabilities entirely warrant\" maybe. If you mean \"LLMs are a hype technology that is going to peter out without leaving a major impact on society\" as kindly as I can phrase it, what on earth are you smoking, or are you Amish? reply hnfong 7 hours agorootparentTo be fair, GP compared the LLM hype with crypto... And as niche crypto may be in the future, I don't think it will also \"peter out without leaving a major impact on society\"... reply KetoManx64 5 hours agorootparentCrypto will not be niche in the future. Bitcoin has a market cap of 1,349,125,982,176 (https://coinmarketcap.com/currencies/bitcoin/) and recently surpassed the market cap of gold itself and has large investors from Black Rock to Apple, ans recently got approved for ETFs which means the general population's 401k's will have bitcoin in them starting this year. Other crypto currencies may die out, but bitcoin is here permanantly. If you haven't yet, buy some bitcoin and stop letting the government steal your hard earned money through inflation. reply gleenn 11 hours agorootparentprevMany of them don't even have video out ports, they are purely for the compute power. reply mise_en_place 15 hours agorootparentprevYep just need to load the model. It can and probably should be headless. reply RobotToaster 18 hours agorootparentprevObviously they are graphicless processing units. reply fyrn_ 18 hours agorootparentprevThey are \"inferrence only\" according to the article reply camel-cdr 18 hours agorootparentgrayskull is \"inference only\", but wormhole will also \"support\" inference. I put those in quotes, because theoretically both can do training, it's just that grayskull isn't well suited for it, because of the internal fp19 format, and no good support for scaling to multiple cards. wormhole will have internal fp32, and is designed to scale out with across multiple cards, and servers. At least that's how I understood it. reply buildbot 17 hours agorootparentFP19 is perfectly fine for training - llama is trained in fp16, many train in bf16, with microscaling exponents, you can do 6 bit training : https://arxiv.org/abs/2310.10537 reply smallnamespace 17 hours agorootparentI think the broader context here is that it's \"fine for training\" in the sense that you can successfully train a model, but it's not \"fine for training\" in the sense that it can only train small models due to the lack of scaling across cards, which directly cuts against where ML has been trending over the last several years. In LLM-land we've rapidly gone from training bespoke models to doing fine tuning to RLHF to zero-shot prompting. The better the underlying model the more you can do without additional training, so hardware that fails to scale up to the largest training runs will have limited practical utility despite technically supporting training. reply buildbot 16 hours agorootparentWormhole does support scale out though via it's built in networking? And there seem to be links for something on top of the board, NVLINK style. And yes, I know, I've been working on LLM pretraining for about 4 years now, since 2020. The number formats themselves so far are mostly scale invariant or improve with larger scale - you can quantize a larger model and see less performance drop than a smaller model. reply karmakaze 17 hours agorootparentprevGPGPU is a thing. reply aleph_minus_one 16 hours agorootparent> GPGPU is a thing. Indeed. GPGPU means \"using a GPU for things that are General Purpose (GPGPU)\". The aforementioned IC is not a GPU that is used for general-purpose computations, but a specialized chip for AI computations. reply Almondsetat 16 hours agorootparentprevIsn't a GPGPU just a PPU? reply tiahura 12 hours agorootparentprevMatrix-Math Coprocessors. reply christkv 19 hours agoparentprevMemory seems very low for inference on bigger models?. Memory bandwidth is also massively lower than something like a 4090 or 7900xtx?. Am I missing some fairy dust magic here? reply algo_trader 19 hours agorootparentThe website indicates each Tensix core ~1TOP theoretical. So not amazing. It will have to compete on price/memory in the crowded inference space. reply VHRanger 9 hours agorootparentThis card isn't competing on price - it's a dev kit. reply christkv 18 hours agorootparentprevMaybe power usage ? reply andy99 18 hours agorootparentprevThis is purely a developer kit, letting people get used to the hardware configuration and the software stacks before the big hardware comes later in future generations. I'd like to know what different precisions/ quantizations if any are supported. For LLMs 8GB is fine for playing around if weights are quantized. And as the article mentions it's more than enough for lots of computer vision models. reply camel-cdr 18 hours agorootparentGrayskull supports FP8, FP16, BFP16, VFP2, BFP4, BFP8 [0], and some sort of 19-bit floating point in the SFPU. [1] I don't know how and with what performance those are supported. FP4 has 221 TFLOPS on the Grayskull e75, and 332 TFLOPS on the Grayskull e150. [0] While wormhole has 82 INT8 TOPS per card. [2] I haven't found any other numbers. Edit: Some more info about data formats: https://docs.tenstorrent.com/tenstorrent/v/tt-buda/dataforma... [0] https://docs.tenstorrent.com/tenstorrent/add-in-boards-and-c... [1] https://tenstorrent-metal.github.io/tt-metal/latest/tt_metal... [2] https://tenstorrent.com/systems/galaxy/ reply alecco 2 hours agorootparentThat's a lot of formats for FP8, FP4, and even FP2! (never heard of). I wonder how they did it. Nice. reply andy99 13 hours agorootparentprevCool, thank you! reply dannyw 16 hours agorootparentprevWhy are they so skimpy on memory capacity? I guess this is a dev kit for anything but LLMs? reply marty1885 1 hour agorootparentGrayskull is make before LLMs being a thing. And their plan is like Groq, to distribute the compute graph across multiple processors to get higher effective memory and throughput by pipelining. But better-ish by having RAM so you can fit models on much less cards. Grayskull doesn't have this ability. The next generation Wormhole does by having 100GbE interfaces on the cards. Also the CPUs on Grayskull is 32bit. Memory is addressed through the bank address so it works for now. But they'll have to upgrade to 64bit soon. reply ChainOfFools 19 hours agorootparentprevWhat's the memory interface like? Is an extreme NUMA thing with several orders greater potential for parallelization? I'm probably dreaming; new CPU core designs are one thing, a completely new memory design is likely high fantasy.. reply christkv 19 hours agorootparentI mean it says LPDDR4, nothing about how many channels but I would be surprised if it's anything more than dual-channel. Big cache but not anything not already seen in the AMD 3XD chips. reply ac29 17 hours agorootparentThe quoted ~100-120GBps is going to be quad-channel LPDDR4 reply onlyrealcuzzo 19 hours agorootparentprevWill graphics cards eventually get renamed? Are the optimizations for inference basically identical to graphics? I imagine the workloads are different, and that inference has been the main market for years now... reply yencabulator 17 hours agorootparentThe new things are already getting renamed, but the dust hasn't settled yet. NPU: https://en.wikipedia.org/wiki/AI_accelerator TPU: https://en.wikipedia.org/wiki/Tensor_Processing_Unit reply adgjlsfhk1 13 hours agorootparentprevwhy bother renaming? just re-acronym to \"greatly parallel unit\" reply bschne 18 hours agoprevBeen following this for a while b/c Jim Keller, but every time I look at the arch [1; as linked by other commenter] as someone who doesn't know the first thing about CPU/ASIC design it just looks sort of... \"wacky\"? Does anyone who understands this have a good explainer for the rationale behind having a grid of cores with memory and IFs interspersed between and then something akin to a network interconnecting them with that topology? What is it about the target workloads here that makes this a good approach? 1. https://docs.tenstorrent.com/tenstorrent/v/tt-buda/hardware reply Pet_Ant 15 hours agoparentSounds like a manycore architecture. If you have played TIS-100 it is that exact same idea. If you haven’t, but have played Factorio think of instead of having a central area where all the work happens you build a series of interconnected stations, each doing their own part of calculation before passing it onto the next. Upside is each core has its own code and is fully Turing complete and independent of eachother. You can handle conditionals much better. And you lose the latency of having network hops for workers. Downside is you need to break down your process to map onto specific nodes and flows. (Assuming it is in fact manycore - which is not the same as multicore) reply pavlov 13 hours agorootparentSounds kind of like the IBM/Sony/Toshiba Cell? It made an appearance in the PlayStation 3, but was supposed to be a more general high-performance architecture. At some point IBM sold blade servers with Cell processors. reply amelius 11 hours agorootparentIs that a dataflow architecture? https://en.wikipedia.org/wiki/Dataflow_architecture reply marty1885 57 minutes agorootparentYes. \"Coarse-grain dataflow processor\". It's like an FPGA. But LUTs are replaced with RISC-V cores. reply yvdriess 10 hours agorootparentprevNo, the Cell is a many-core architecture with a Power 'general' core and 6-8 'special purpose' vector processing units. reply bschne 14 hours agorootparentprev> You can handle conditionals much better Because you can have a core set up for each branch and just pass to that vs. “context-switching” your core to execute the branch that ends up being taken? reply vrighter 14 hours agorootparentbecause each core has its own instruction counter reply crq-yml 13 hours agoparentprevIt's another iteration of the unit record machine [0] - batch processes done with a physical arrangement of the steps in the process. CPU design moved away from this analogy a long while ago because the tasks being done with CPUs involved more dynamic control flow structures and arbitrary workloads. But workloads that are linear batches of brute force math don't need that kind of dynamism, so gridded designs become fashionable as a way of expressing a configurable pipeline with clear semantics - everything on the grid is at a linear, integer-scalable distance, buffers will be of the same size, etc. [0] https://en.m.wikipedia.org/wiki/Unit_record_equipment reply BirAdam 16 hours agoparentprevThis reminds me very much of transputers. The idea here is that each cpu can context switch extremely quickly with minimal latency to any resource and you have a topology that is great for matrix maths as a result. reply bschne 16 hours agorootparentWhat makes the resulting topology great for matrix math, vs. non-matrix math workloads? Naively if you know you‘re „only“ going to multiply matrices, what do you need the flexibility and fast context-switching for? Is the end-game here that you can lay out the workload s.t. you have a series of closely colocated cores carrying out the operations of some linalg expression one after the other and the memory for intermediate results right in between, or something like that? reply cavisne 14 hours agorootparentI suspect thats basically it (operations one by one and then pipelining to saturate). Thats basically what Groq does also AFAIK. From their website it seems the chips are designed to be connected together into one big topology, the \"Galaxy\" system. Also similar to TPU's, although they use HBM with only a few very powerful \"cores\" vs DRAM with low powered cores. reply imtringued 12 hours agorootparentprevIs this some kind of trick question? Your core needs to be fully programmable so you can do things like kernel fusion. The simplest form is to load quantized weights and dequantize them to bfloat16 as you go. Llama.cpp and it's gguf files support various types of quantization and most of them require programmability to efficiently support them. reply bschne 2 hours agorootparentnot a trick question, I'm just genuinely ignorant about the topic reply transitionnel 17 hours agoparentprevDefinitely looks wacky! Has nice concept though, I like the \"Network On Chip\" reversed toruses. Hopefully some of y'all tinkerers with time and dough can bear some of these ideas to fruition, keep Nvidia on their toes ;) 64gb is a good RAM amount IMO, cheap yet still vastly underutilized since we play to the LCD of users... guessing Linux will be able to make that pivot much faster/so little baggage. Plus...\"Grayskull\" reply binarymax 14 hours agorootparent> 64gb is a good RAM amount IMO It doesn’t have 64gb of RAM, it has 8. The system requirements otherwise need 64gb of RAM for model compilation reply bschne 17 hours agorootparentprev> I like the \"Network On Chip\" reversed toruses What about them do you like as a design decision? (genuinely curious, as again, I don't understand it) reply tormeh 17 hours agoparentprevYou want memory to be close to where it's used because at the speeds of high-performance ICs, the latency caused by distance is actually significant. reply bschne 17 hours agorootparentbut isn't that aspect common among this, CPUs, GPUs, ...? And it feels like the whole NOC thing would add quite some overhead to moving things around. Are you saying proximity here more than offsets this vs. e.g. each core having its own cache as I think they do in a \"normal\" CPU? And if so, is this more true of ML inference workloads than other workloads, for some reason? reply adgjlsfhk1 13 hours agorootparentI think the distinction with ML inference workloads is that you often have very little control flow, so this type of architecture lets you match layers to adjacent cores so that each operation gets it's data directly from the step before rather than from RAM. reply loeg 16 hours agorootparentprevI think the NOC approach is fundamentally similar to Intel's rings that they used for core interconnect back in the mid-2010s. It works. https://www.realworldtech.com/includes/images/articles/snbep... https://en.wikichip.org/wiki/intel/microarchitectures/sandy_... reply paulmd 12 hours agorootparentThey still use this today, and the ring interconnect is also the topology inside zen3/zen4 CCXs (with 8 cores). Ring is one of the simplest and best systems for >4 cores, until you get to about 8-10 cores (at which point you generally split it into multiple “tiers” like multiple CCXs etc). reply loeg 10 hours agorootparentIntel switched to a mesh model: https://en.wikichip.org/wiki/intel/mesh_interconnect_archite... reply paulmd 9 hours agorootparentnot for consumer chips, which still use the same ringbus design. e-cores do have a CCX/core cluster, but the clusters themselves go on the ringbus lol reply imtringued 11 hours agorootparentprevI honestly don't understand this latency obsession for LLMs. You are loading millions of parameters sequentially for each matrix. The access pattern is perfectly predictable. I just ran llama.cpp in with profiling and 99.9% of the time is spent in matrix multiplication. This shocked me, honestly, because I genuinely thought that there is going to be much more variety. reply adinb 11 hours agoparentprevSeeing the topology I had a flashback to college and the MasPar [1] we were using in ‘92! [1] https://en.wikipedia.org/wiki/MasPar reply bgnn 12 hours agoparentprevIn general putting memory physically close to compute is good. If two cores need to share that memory doesn't it make sense to place the memory at the interface? reply imtringued 12 hours agoparentprevThe real question is how do they plan to compete with say a Ryzen 8700G with 32 GB of overclocked RAM and the Ryzen AI NPU. 2x DDR5-6600 gives you more memory bandwidth than grayskull. Their primary advantage appears to be the large SRAM and not much else. reply camel-cdr 19 hours agoprevSince people seem to be wondering how the architecture works, and the software stack is open (although I'm not sure to what extend), I'll share my understanding of it. The basic system consists of the cards consists of a bunch of Tensix cores and shared memory: > Each Tensix core contains a high-density tensor math unit (FPU) which performs most of the heavy lifting, a SIMD engine (SFPU), five Risc-V CPU cores, and a large local memory storage. [0] > The cores are connected with two torus-shaped, going in opposite directions. [0] The RISC-V cores in the Tensix cores, are tiny rv32i cores, that can control the FPU, SFPU, and are also used to prepare/move data. The FPU, does \"dense tensor math\", so I think it's probably a matmul engine, but I don't know any more specifics. [1] The SFPU is a more general purpose SIMT engine, that can be driven from the RISC-V cores. There is a SFPU simulator you can play around with on their github. [2] See the low level Kernels example for how the programming model works. [3] The grayskull SFPU has 4 general purpose LRegs, which each hold 64 19-bit values. Wormhole has 8 general purpose LRegs, which each hold 32 32-bit values. I've been told that wormhole SFPU has a ~3x IPC increase over grayskull, and a few new SFPU instructions. You can probably find out more by browsing the docs and rummaging through the github repos. [4] [0] https://docs.tenstorrent.com/tenstorrent/v/tt-buda/hardware [1] https://docs.tenstorrent.com/tenstorrent/v/tt-buda/terminolo... [2] https://github.com/tenstorrent-metal/sfpi/blob/master/tests/... [3] https://tenstorrent-metal.github.io/tt-metal/latest/tt_metal... [4] https://github.com/tenstorrent-metal/ reply BirAdam 16 hours agoparentSo, modern transputer but RISC-V. Seems neat. reply cmrdporcupine 15 hours agoparentprevOh, so am I to read this right that the actual vector work being done here is not done via the RiscV [V]ector extension, but by a purely custom core? reply camel-cdr 15 hours agorootparentYes, the RISC-V Vector extension would be absolutely overkill for an ML accelerator, and waste a lot of die space. Currently the cards need an x86_64 host, but they plan to replace that with their ascalon risc-v cpu, that supports the risc-v vector extension (rvv). So most compute is done by the accelerator cards, but for some things the host system steps in. There rvv can come in handy, because it's a lot more flexible. reply cmrdporcupine 8 hours agorootparentIt sounds very cool. I've been following them since watching a couple interviews with Jim Keller, and then seeing they're (in part) a Toronto area local company. I don't work in machine-learning but find the HW direction really interesting. I could think of a few uses for that kind of high bandwidth vector/matrix crunching that aren't restricted to ML purposes. reply curious_cat_163 19 hours agoprev> \"...including BERT for natural language processing tasks, ResNet for image recognition, Whisper for speech recognition and translation, YOLOv5 for real-time object detection, and U-Net for image segmentation.\" I wonder why they are starting with these models. One could speculate that they are going for power efficiencies but that does not quite add up entirely. reply camel-cdr 19 hours agoparentFrom what I've gathered from the Tenstorrent discord: The grayskull cards only have 8 GB of RAM and don't have a fast enough memory to NoC bandwidth to make working with multiple cards practical. The next generations, that is wormhole and newer don't have this limitations and are specifically designed to work in server racks, see the galaxy system. [0] The Grayskull really only is a devboard. It has some other quirks that will be improved by wormhole, like native 19 bit floats in their SIMT engines, instead of 32 bit, in wormhole. [0] https://tenstorrent.com/systems/galaxy/ reply usrusr 14 hours agoparentprevHighly speculative, and I suspect that I don't do these models justice: The purpose of these boards seems to get people acquainted with their programming model. Not as in using board and model as a turnkey solution (if that happens, fine, but this is not the goal), but as in getting potential customers for future boards to learn how to make their own models or third party models run on the board. The more models they supported out of the box, the less well the goal of building buyer-side expertise in the programming model would be served. reply VHRanger 17 hours agoparentprev> I wonder why they are starting with these models. They were building Greyskull in 2020/2021 initially. BERT_Large and similar were the SOTA at that time. reply hhh 18 hours agoparentprevYOLOv5 is pretty widely used, and it can be useful to get efficient compute at the edge. Disclosure: I work for a Tenstorrent customer. reply transitionnel 17 hours agoparentprevLike Intel SSE for media optimization probably? Those models do seem to be the best currently available (IMO), so yeah power efficiencies for guaranteed common use cases should be a 0-day integration. reply transitionnel 17 hours agorootparentWhich reminds me... Hope those libraries are open and vetted... reply binarymax 14 hours agoparentprevNext-gen BERT models are still very popular for embeddings. reply pella 16 hours agoprevGrayskull™ e150 = Tensix Cores: 120 * 5 RISC-V => 600 RISC-V CPU core ( 1 Tensix core = 5 RISC-V core : https://docs.tenstorrent.com/tenstorrent/v/tt-buda/hardware ) reply loeg 16 hours agoparentDo you think this thing will run Linux (re: now removed comment about Linux default core limit)? reply pella 15 hours agorootparentAs I see this is : TT-Metalium [1] = a low level API [1] \"TT-Metalium is a platform for programming heterogeneous collection of CPUs (host processors) and Tenstorrent acceleration devices, composed of many RISC-V processors. Target users of TT-Metal are expert parallel programmers wanting to write parallel and efficient code with full access to the Tensix hardware via low-level kernel APIs.\" https://tenstorrent-metal.github.io/tt-metal/latest/tt_metal... [1] \"The software stacks come in two varieties – a high level and a low level. The high-level is called TT-Buda, using higher-level APIs to get things up and running, along with interfaces into modern machine learning frameworks. The lower level is TT-Metalium, which provides fine-grained control over the hardware for custom operators, custom control, and even non-machine learning code. Tenstorrent states that there are no black boxes, no encrypted APIs, and no hidden functions.\" https://morethanmoore.substack.com/p/unboxing-the-tenstorren... reply hollerith 16 hours agoparentprevI don't think that is as clear as you think it is. If you had to communicate it orally, what words would you add? ADDED. I think I figured out what it means: a Grayskull e150 contains 120 'Tensix cores', each of which contains 5 RISC-V cores. reply binarymax 17 hours agoprevThe System Requirements on this page say 64GB RAM is a prerequisite on the host system. Why? Wouldn’t an inference server be barebones aside from the Inference hardware? https://tenstorrent.com/cards/ reply camel-cdr 17 hours agoparentFrom the tenstorrent discord: > It has more to do with the memory resources required during model compilation. The requirement will vary from model to model, so 64GB is a safe limit for all. reply VHRanger 15 hours agorootparentAnd why do they specify PCIe 4.0? I imagine it's just to leverage bandwidth correctly? Or is there a necessary feature in there? reply physPop 18 hours agoprevOutsider to the field and just curious: Does anyone know how these kinds of processors compare to the custom silicon by aws/google/tesla? reply VHRanger 17 hours agoparentMuch more efficient per watt for some specialized types of operations. Obviously immature ecosystem, expect a couple of years at minimum before adoption if it's the real deal. reply ribit 19 hours agoprevI was unable to find any reference to performance or architectural details. Memory bandwidth is very low for an ML-focused device. Price is extremely high. What am I missing? reply nabla9 18 hours agoparentThose are DevKits. I assume they are sold to hardware manufacturers and maybe software developers so that they check out and test their systems against real processor. Processors are probably manufactured in some relatively old process technology. The real thing will be manufactured using different 2nm process according to the article and performance will be different. reply bragr 15 hours agoprevThe stated architecture reminds me of how the Intel Project Larrabee GPU was supposed to work, except with RISC-V instead stripped x86 cores. reply usrusr 14 hours agoparentDid Larrabee contain anything that wasn't an x86 core? According to other comments, this seems to consist of massive highly specialized processing units with just a few tiny CPU sprinkled in to keep the former fed. Quite the opposite of how I remember Larabee. reply VHRanger 13 hours agorootparentNot really, Xeon Phis were basically the a x86 CPU abusing AVX extensions to their limit reply shrubble 14 hours agoparentprevDo you see any similarities with the Xeon Phi boards? reply margorczynski 19 hours agoprevI wonder if with the recent research coming to light about the effectiveness of using 3 state weights do they plan to release something along those lines? Can they really compete in FLOP/$ with the likes on Nvidia even if it is a more bespoke architecture than a modified GPU? reply mastax 17 hours agoparentNvidia has insane margins so it isn't hard* for a well-funded player to beat them in FLOPS/$. The hard part is the compiler toolchains, the libraries, the documentation, getting models to run fast. * Everything is relative. It's hard to make even a simple microcontroller - but also it isn't hard, you know? reply VHRanger 9 hours agorootparent> Nvidia has insane margins so it isn't hard* for a well-funded player to beat them in FLOPS/$ It's hard when you can't buy TSMC capacity reply transitionnel 17 hours agorootparentprev\"Life in 1 comment\" reply dannyw 16 hours agoparentprevI’d wait till the dust settles. Maybe 2bit encoding (-1, 0, 0.5, 1) would be easier to design hardware for; as 0.5 can be a bitwise shift. reply imtringued 11 hours agoparentprevNobody cares about FLOPs anymore when it comes to transformer based architectures and considering the transformer architecture is applicable to almost any usecase beyond LLMs, memory bandwidth and memory capacity are the most important metrics. Grayskull has enough 16bit float performance to outrun its memory bandwidth even with 1 ternary bit quantization. reply audiofish 16 hours agoprevThis looks very similar to the Picochip designs used in a lot of small cellular base stations for SDR. I hope it is similar, because those were fantastic chips to program for. That influence could have come via Intel's acquisition of Picochip. reply loeg 16 hours agoprevThis article is extremely low detail. Does anyone have a source with more information? reply VHRanger 9 hours agoparentdocs.tenstorrent.com All the details you need! reply multiphonics 15 hours agoprevHow is the networked multi CPU core Grayskull a different approach compared to, say, Ampere's 100+ ARM core chip? reply spintin 12 hours agoprevI bought a 3050 low profile / half length with DDR6 at 14GHz for half this price. And it can play games too. reply wtallis 11 hours agoparentGDDR6, 14 GT/s if we want to be accurate. So about double the DRAM bandwidth, but less than 1/20th the SRAM capacity, and fairly different architectures on-chip. I'm not sure what ML workloads would benefit from the extra SRAM enough to overcome the DRAM bandwidth deficit, but they probably exist or Tenstorrent wouldn't be making such a SRAM-heavy chip. reply mugivarra69 6 hours agoprevhow does 8gb cut it? reply _0ffh 11 hours agoprev [–] I'm getting sick of this: Groq, Tenstorrent, all the promising startups are offering inference-only solutions. I have it from Groq official channels that they do not plan to invest development time into enabling training, because inference is where the big money is at. Which I understand insofar as inference demand probably outstrips training demand by ~millions, but I still can't help but find all of this so egreriously disappointing! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tenstorrent, under Jim Keller's leadership, launched Grayskull, its inaugural hardware, as a RISC-V substitute for GPUs, targeting AI tasks.",
      "The Grayskull DevKits, presented in e75 and e150 models, cater to AI development, offering versatility with various models and costing $599 and $799.",
      "Tenstorrent's collaboration with a Japanese semiconductor center is geared towards enhancing AI processing capabilities, marking a significant leap in AI performance."
    ],
    "commentSummary": [
      "The conversation reviews new processors like Tenstorrent's Grayskull RISC-V processor for AI, Nvidia's GPUs, and specialized chips for AI tasks, discussing architecture, performance, memory, scalability, and business implications.",
      "It delves into processor designs, unique features, network on chip architectures, system requirements, and technology/company comparisons in the AI and machine learning sectors.",
      "Overall, it explores the innovation and challenges in the advancing AI chip landscape."
    ],
    "points": 255,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1710076503
  },
  {
    "id": 39662615,
    "title": "Crafting Timeless Essays: Prioritizing Generality and Novelty",
    "originLink": "https://paulgraham.com/best.html",
    "originBody": "March 2024 Despite its title this isn't meant to be the best essay. My goal here is to figure out what the best essay would be like. It would be well-written, but you can write well about any topic. What made it special would be what it was about. Obviously some topics would be better than others. It probably wouldn't be about this year's lipstick colors. But it wouldn't be vaporous talk about elevated themes either. A good essay has to be surprising. It has to tell people something they don't already know. The best essay would be on the most important topic you could tell people something surprising about. That may sound obvious, but it has some unexpected consequences. One is that science enters the picture like an elephant stepping into a rowboat. For example, Darwin first described the idea of natural selection in an essay written in 1844. [1] Talk about an important topic you could tell people something surprising about. If that's the test of a great essay, this was surely the best one written in 1844. And indeed, the best possible essay at any given time would usually be one describing the most important scientific or technological discovery it was possible to make. [2] Another unexpected consequence: I imagined when I started writing this that the best essay would be fairly timeless — that the best essay you could write in 1844 would be much the same as the best one you could write now. But in fact the opposite seems to be true. It might be true that the best painting would be timeless in this sense. But it wouldn't be impressive to write an essay introducing natural selection now. The best essay now would be one describing a great discovery we didn't yet know about. If the question of how to write the best possible essay reduces to the question of how to make great discoveries, then I started with the wrong question. Perhaps what this exercise shows is that we shouldn't waste our time writing essays but instead focus on making discoveries in some specific domain. But I'm interested in essays and what can be done with them, so I want to see if there's some other question I could have asked. There is, and on the face of it, it seems almost identical to the one I started with. Instead of asking what would the best essay be? I should have asked how do you write essays well? Though these seem only phrasing apart, their answers diverge. The answer to the first question, as we've seen, isn't really about essay writing. The second question forces it to be. Writing essays, at its best, is a way of discovering ideas. How do you do that well? How do you discover by writing? An essay should ordinarily start with what I'm going to call a question, though I mean this in a very general sense: it doesn't have to be a question grammatically, just something that acts like one in the sense that it spurs some response. How do you get this initial question? It probably won't work to choose some important-sounding topic at random and go at it. Professional traders won't even trade unless they have what they call an edge — a convincing story about why in some class of trades they'll win more than they lose. Similarly, you shouldn't attack a topic unless you have a way in — some new insight about it or way of approaching it. You don't need to have a complete thesis; you just need some kind of gap you can explore. In fact, merely having questions about something other people take for granted can be edge enough. If you come across a question that's sufficiently puzzling, it could be worth exploring even if it doesn't seem very momentous. Many an important discovery has been made by pulling on a thread that seemed insignificant at first. How can they all be finches? [3] Once you've got a question, then what? You start thinking out loud about it. Not literally out loud, but you commit to a specific string of words in response, as you would if you were talking. This initial response is usually mistaken or incomplete. Writing converts your ideas from vague to bad. But that's a step forward, because once you can see the brokenness, you can fix it. Perhaps beginning writers are alarmed at the thought of starting with something mistaken or incomplete, but you shouldn't be, because this is why essay writing works. Forcing yourself to commit to some specific string of words gives you a starting point, and if it's wrong, you'll see that when you reread it. At least half of essay writing is rereading what you've written and asking is this correct and complete? You have to be very strict when rereading, not just because you want to keep yourself honest, but because a gap between your response and the truth is often a sign of new ideas to be discovered. The prize for being strict with what you've written is not just refinement. When you take a roughly correct answer and try to make it exactly right, sometimes you find that you can't, and that the reason is that you were depending on a false assumption. And when you discard it, the answer turns out to be completely different. [4] Ideally the response to a question is two things: the first step in a process that converges on the truth, and a source of additional questions (in my very general sense of the word). So the process continues recursively, as response spurs response. [5] Usually there are several possible responses to a question, which means you're traversing a tree. But essays are linear, not tree-shaped, which means you have to choose one branch to follow at each point. How do you choose? Usually you should follow whichever offers the greatest combination of generality and novelty. I don't consciously rank branches this way; I just follow whichever seems most exciting; but generality and novelty are what make a branch exciting. [6] If you're willing to do a lot of rewriting, you don't have to guess right. You can follow a branch and see how it turns out, and if it isn't good enough, cut it and backtrack. I do this all the time. In this essay I've already cut a 17-paragraph subtree, in addition to countless shorter ones. Maybe I'll reattach it at the end, or boil it down to a footnote, or spin it off as its own essay; we'll see. [7] In general you want to be quick to cut. One of the most dangerous temptations in writing (and in software and painting) is to keep something that isn't right just because it contains a few good bits or cost you a lot of effort. The most surprising new question being thrown off at this point is does it really matter what the initial question is? If the space of ideas is highly connected, it shouldn't, because you should be able to get from any question to the most valuable ones in a few hops. And we see evidence that it's highly connected in the way, for example, that people who are obsessed with some topic can turn any conversation toward it. But that only works if you know where you want to go, and you don't in an essay. That's the whole point. You don't want to be the obsessive conversationalist, or all your essays will be about the same thing. [8] The other reason the initial question matters is that you usually feel somewhat obliged to stick to it. I don't think about this when I decide which branch to follow. I just follow novelty and generality. Sticking to the question is enforced later, when I notice I've wandered too far and have to backtrack. [9] But I think this is the optimal solution. You don't want the hunt for novelty and generality to be constrained in the moment. Go with it and see what you get. Since the initial question does constrain you, in the best case it sets an upper bound on the quality of essay you'll write. If you do as well as you possibly can on the chain of thoughts that follow from the initial question, the initial question itself is the only place where there's room for variation. It would be a mistake to let this make you too conservative though, because you can't predict where a question will lead. Not if you're doing things right, because doing things right means making discoveries, and by definition you can't predict those. So the way to respond to this situation is not to be cautious about which initial question you choose, but to write a lot of essays. Essays are for taking risks. Almost any question can get you a good essay. Indeed, it took some effort to think of a sufficiently unpromising topic in the third paragraph, because any essayist's first impulse on hearing that the best essay couldn't be about x would be to try to write it. But if most questions yield good essays, only some yield great ones. Can we predict which questions will yield great essays? Considering how long I've been writing essays, it's alarming how novel that question feels. One thing I like in an initial question is outrageousness. I love questions that seem naughty in some way — for example, by seeming counterintuitive or overambitious or heterodox. Ideally all three. This essay is an example. Writing about the best essay implies there is such a thing, which pseudo-intellectuals will dismiss as reductive, though it follows necessarily from the possibility of one essay being better than another. And thinking about how to do something so ambitious is close enough to doing it that it holds your attention. I like to start an essay with a gleam in my eye. This could be just a taste of mine, but there's one aspect of it that probably isn't: to write a really good essay on some topic, you have to be interested in it. A good writer can write well about anything, but to stretch for the novel insights that are the raison d'etre of the essay, you have to care. If caring about it is one of the criteria for a good initial question, then the optimal question varies from person to person. It also means you're more likely to write great essays if you care about a lot of different things. The more curious you are, the greater the probable overlap between the set of things you're curious about and the set of topics that yield great essays. What other qualities would a great initial question have? It's probably good if it has implications in a lot of different areas. And I find it's a good sign if it's one that people think has already been thoroughly explored. But the truth is that I've barely thought about how to choose initial questions, because I rarely do it. I rarely choose what to write about; I just start thinking about something, and sometimes it turns into an essay. Am I going to stop writing essays about whatever I happen to be thinking about and instead start working my way through some systematically generated list of topics? That doesn't sound like much fun. And yet I want to write good essays, and if the initial question matters, I should care about it. Perhaps the answer is to go one step earlier: to write about whatever pops into your head, but try to ensure that what pops into your head is good. Indeed, now that I think about it, this has to be the answer, because a mere list of topics wouldn't be any use if you didn't have edge with any of them. To start writing an essay, you need a topic plus some initial insight about it, and you can't generate those systematically. If only. [10] You can probably cause yourself to have more of them, though. The quality of the ideas that come out of your head depend on what goes in, and you can improve that in two dimensions, breadth and depth. You can't learn everything, so getting breadth implies learning about topics that are very different from one another. When I tell people about my book-buying trips to Hay and they ask what I buy books about, I usually feel a bit sheepish answering, because the topics seem like a laundry list of unrelated subjects. But perhaps that's actually optimal in this business. You can also get ideas by talking to people, by doing and building things, and by going places and seeing things. I don't think it's important to talk to new people so much as the sort of people who make you have new ideas. I get more new ideas after talking for an afternoon with Robert Morris than from talking to 20 new smart people. I know because that's what a block of office hours at Y Combinator consists of. While breadth comes from reading and talking and seeing, depth comes from doing. The way to really learn about some domain is to have to solve problems in it. Though this could take the form of writing, I suspect that to be a good essayist you also have to do, or have done, some other kind of work. That may not be true for most other fields, but essay writing is different. You could spend half your time working on something else and be net ahead, so long as it was hard. I'm not proposing that as a recipe so much as an encouragement to those already doing it. If you've spent all your life so far working on other things, you're already halfway there. Though of course to be good at writing you have to like it, and if you like writing you'd probably have spent at least some time doing it. Everything I've said about initial questions applies also to the questions you encounter in writing the essay. They're the same thing; every subtree of an essay is usually a shorter essay, just as every subtree of a Calder mobile is a smaller mobile. So any technique that gets you good initial questions also gets you good whole essays. At some point the cycle of question and response reaches what feels like a natural end. Which is a little suspicious; shouldn't every answer suggest more questions? I think what happens is that you start to feel sated. Once you've covered enough interesting ground, you start to lose your appetite for new questions. Which is just as well, because the reader is probably feeling sated too. And it's not lazy to stop asking questions, because you could instead be asking the initial question of a new essay. That's the ultimate source of drag on the connectedness of ideas: the discoveries you make along the way. If you discover enough starting from question A, you'll never make it to question B. Though if you keep writing essays you'll gradually fix this problem by burning off such discoveries. So bizarrely enough, writing lots of essays makes it as if the space of ideas were more highly connected. When a subtree comes to an end, you can do one of two things. You can either stop, or pull the Cubist trick of laying separate subtrees end to end by returning to a question you skipped earlier. Usually it requires some sleight of hand to make the essay flow continuously at this point, but not this time. This time I actually need an example of the phenomenon. For example, we discovered earlier that the best possible essay wouldn't usually be timeless in the way the best painting would. This seems surprising enough to be worth investigating further. There are two senses in which an essay can be timeless: to be about a matter of permanent importance, and always to have the same effect on readers. With art these two senses blend together. Art that looked beautiful to the ancient Greeks still looks beautiful to us. But with essays the two senses diverge, because essays teach, and you can't teach people something they already know. Natural selection is certainly a matter of permanent importance, but an essay explaining it couldn't have the same effect on us that it would have had on Darwin's contemporaries, precisely because his ideas were so successful that everyone already knows about them. [11] I imagined when I started writing this that the best possible essay would be timeless in the stricter, evergreen sense: that it would contain some deep, timeless wisdom that would appeal equally to Aristotle and Feynman. That doesn't seem to be true. But if the best possible essay wouldn't usually be timeless in this stricter sense, what would it take to write essays that were? The answer to that turns out to be very strange: to be the evergreen kind of timeless, an essay has to be ineffective, in the sense that its discoveries aren't assimilated into our shared culture. Otherwise there will be nothing new in it for the second generation of readers. If you want to surprise readers not just now but in the future as well, you have to write essays that won't stick — essays that, no matter how good they are, won't become part of what people in the future learn before they read them. [12] I can imagine several ways to do that. One would be to write about things people never learn. For example, it's a long-established pattern for ambitious people to chase after various types of prizes, and only later, perhaps too late, to realize that some of them weren't worth as much as they thought. If you write about that, you can be confident of a conveyor belt of future readers to be surprised by it. Ditto if you write about the tendency of the inexperienced to overdo things — of young engineers to produce overcomplicated solutions, for example. There are some kinds of mistakes people never learn to avoid except by making them. Any of those should be a timeless topic. Sometimes when we're slow to grasp things it's not just because we're obtuse or in denial but because we've been deliberately lied to. There are a lot of things adults lie to kids about, and when you reach adulthood, they don't take you aside and hand you a list of them. They don't remember which lies they told you, and most were implicit anyway. So contradicting such lies will be a source of surprises for as long as adults keep telling them. Sometimes it's systems that lie to you. For example, the educational systems in most countries train you to win by hacking the test. But that's not how you win at the most important real-world tests, and after decades of training, this is hard for new arrivals in the real world to grasp. Helping them overcome such institutional lies will work as long as the institutions remain broken. [13] Another recipe for timelessness is to write about things readers already know, but in much more detail than can be transmitted culturally. \"Everyone knows,\" for example, that it can be rewarding to have kids. But till you have them you don't know precisely what forms that takes, and even then much of what you know you may never have put into words. I've written about all these kinds of topics. But I didn't do it in a deliberate attempt to write essays that were timeless in the stricter sense. And indeed, the fact that this depends on one's ideas not sticking suggests that it's not worth making a deliberate attempt to. You should write about topics of timeless importance, yes, but if you do such a good job that your conclusions stick and future generations find your essay obvious instead of novel, so much the better. You've crossed into Darwin territory. Writing about topics of timeless importance is an instance of something even more general, though: breadth of applicability. And there are more kinds of breadth than chronological — applying to lots of different fields, for example. So breadth is the ultimate aim. I already aim for it. Breadth and novelty are the two things I'm always chasing. But I'm glad I understand where timelessness fits. I understand better where a lot of things fit now. This essay has been a kind of tour of essay writing. I started out hoping to get advice about topics; if you assume good writing, the only thing left to differentiate the best essay is its topic. And I did get advice about topics: discover natural selection. Yeah, that would be nice. But when you step back and ask what's the best you can do short of making some great discovery like that, the answer turns out to be about procedure. Ultimately the quality of an essay is a function of the ideas discovered in it, and the way you get them is by casting a wide net for questions and then being very exacting with the answers. The most striking feature of this map of essay writing are the alternating stripes of inspiration and effort required. The questions depend on inspiration, but the answers can be got by sheer persistence. You don't have to get an answer right the first time, but there's no excuse for not getting it right eventually, because you can keep rewriting till you do. And this is not just a theoretical possibility. It's a pretty accurate description of the way I work. I'm rewriting as we speak. But although I wish I could say that writing great essays depends mostly on effort, in the limit case it's inspiration that makes the difference. In the limit case, the questions are the harder thing to get. That pool has no bottom. How to get more questions? That is the most important question of all. Notes [1] Darwin wouldn't have chosen to publish his ideas this way. His 1844 essay, like the 1839 version that preceded it, was written more to work out his ideas than to communicate them. But Lyell and Hooker's hands were forced by Alfred Wallace's independent discovery of natural selection before Darwin had published anything. [2] There might be some resistance to this conclusion on the grounds that some of these discoveries could only be understood by a small number of readers. But you get into all sorts of difficulties if you want to disqualify essays on this account. How do you decide where the cutoff should be? If a virus kills off everyone except a handful of people sequestered at Los Alamos, could an essay that had been disqualified now be eligible? Etc. [3] When you find yourself very curious about an apparently minor question, that's an exciting sign. Evolution has designed you to pay attention to things that matter. So when you're very curious about something random, that could mean you've unconsciously noticed it's less random than it seems. [4] Corollary: If you're not intellectually honest, your writing won't just be biased, but also boring, because you'll miss all the ideas you'd have discovered if you pushed for the truth. [5] Sometimes this process begins before you start writing. Sometimes you've already figured out the first few things you want to say. Schoolchildren are often taught they should decide everything they want to say, and write this down as an outline before they start writing the essay itself. Maybe that's a good way to get them started — or not, I don't know — but it's antithetical to the spirit of essay writing. The more detailed your outline, the less your ideas can benefit from the sort of discovery that essays are for. [6] The problem with this type of \"greedy\" algorithm is that you can end up on a local maximum. If the most valuable question is preceded by a boring one, you'll overlook it. But I can't imagine a better strategy. There's no lookahead except by writing. So use a greedy algorithm and a lot of time. [7] I ended up reattaching the first 5 of the 17 paragraphs, and discarding the rest. [8] Stephen Fry confessed to making use of this phenomenon when taking exams at Oxford. He had in his head a standard essay about some general literary topic, and he would find a way to turn the exam question toward it and then just reproduce it again. Strictly speaking it's the graph of ideas that would be highly connected, not the space, but that usage would confuse people who don't know graph theory, whereas people who do know it will get what I mean if I say \"space\". [9] Too far doesn't depend just on the distance from the original topic. It's more like that distance divided by the value of whatever I've discovered in the subtree. [10] Or can you? I should try writing about this. Even if the chance of succeeding is small, the expected value is huge. [11] There was a vogue in the 20th century for saying that the purpose of art was also to teach. Some artists tried to justify their work by explaining that their goal was not to produce something good, but to challenge our preconceptions about art. And to be fair, art can teach somewhat. The ancient Greeks' naturalistic sculptures represented a new idea, and must have been extra exciting to contemporaries on that account. But they still look good to us. [12] Bertrand Russell caused huge controversy in the early 20th century with his ideas about \"trial marriage.\" But they make boring reading now, because they prevailed. \"Trial marriage\" is what we call \"dating.\" [13] If you'd asked me 10 years ago, I'd have predicted that schools would continue to teach hacking the test for centuries. But now it seems plausible that students will soon be taught individually by AIs, and that exams will be replaced by ongoing, invisible micro-assessments. Thanks to Sam Altman, Trevor Blackwell, Jessica Livingston, Robert Morris, Courtenay Pipkin, and Harj Taggar for reading drafts of this.",
    "commentLink": "https://news.ycombinator.com/item?id=39662615",
    "commentBody": "The Best Essay (paulgraham.com)221 points by tosh 12 hours agohidepastfavorite151 comments lubujackson 5 hours agoI think the concept of a best essay is somewhat illogical, but exploring the idea like this makes an interesting post. After all, the term \"essay\" was invented/popularized by Montaigne in a book of his writing called \"Essays\" - and \"essay\" is French for \"to try.\" So from that context, essays shouldn't be concerned with finding answers or be the \"best\" but to make discoveries in the process of trying. reply robocat 2 hours agoparent> is French for \"to try.\" Which looks to be where the English word assay comes from too: https://www.etymonline.com/word/assay reply bruturis 57 minutes agorootparentIn Spanish we have ensayo = attempt or try, frequently is used to refer to attempts before the real show or performance (in a music festival, a theater, ..). Also ensayo is just equivalent to what Montaigned introduced when referred to a text composition. reply maleldil 46 minutes agorootparentSimilarly, Portuguese's \"ensaio\", which means \"rehearsal\". reply ricardobeat 24 minutes agorootparentIt also carries the same literary meaning, “ensaio sobre …” == essay, usually only seen in academia, or alternative cinema. reply arketyp 3 hours agoparentprevGood comment. It surprised me that a definition of essay wasn't sought before asking questions about the supposed best. But that tentative approach actually struck the essence in an interestingly performative manner. reply fhe 1 hour agoprevI find the separation between the body of the essay and notes makes it harder to read. When I read PG's essay, I always have two tabs open, one on the essay and the other on the Notes section. When I come across a note reference, e.g. \"the answer turns out to be completely different. [4]\", I switch to the Notes tab to look it up. This back and forth between the two tabs disrupt the reading experience. Wouldn't it be easier on the reader to simply include the Notes in the body part of the essay? Especially when the notes are short. For example the aforementioned note no. 4 goes: \"[4]Corollary: If you're not intellectually honest, your writing won't just be biased, but also boring, because you'll miss all the ideas you'd have discovered if you pushed for the truth.\" Which is short, relevant, and insightful. What's the criteria for which piece of text goes into the body of the essay, and which goes to Notes? I guess if a substantial percentage of readers skip the Notes part, then it's a time-saving practice to separate the two. reply rmnwski 1 hour agoparentYou can click on [4] which brings you to the footnote and when done you can just use the back button which brings you back to [4] in the text. Works for me. reply LoveMortuus 1 hour agoparentprevI agree, having them separated is quite bad. They could have just put them into brackets ( ) and include it in the normal body of the text, that way if you're not interested you can just skip it, but if you are interested then you don't have to list through/switch tabs/search at the bottom of the page/... I believe that the reading flow is quite important, probably near the top of importance, because if the reader gets into the reading flow they're much more likely to read it all and enjoy it! reply arketyp 1 hour agoparentprevWith books, I tend to not bother with the notes if they are bundled up at the end of the book or chapter. I prefer to have them on the bottom of the page. A web page doesn't even have the benefit of easy page turning to a bookmark, so I think some medium adapted solution like mouse hover would be appropriate. reply euroderf 54 minutes agoparentprevThis is the use case for Tufte-format layout. Notes, footnotes, and other stuff go into a comfortably-wide right-hand margin. reply dpc94 12 hours agoprevReally liked this quote. “While breadth comes from reading and talking and seeing, depth comes from doing. The way to really learn about some domain is to have to solve problems in it” reply bryanrasmussen 4 minutes agoparentI misread that for a second as \"while breath comes from reading and talking and seeing, death comes from doing\" which I thought would be a great line from a Zorro film, even if not that sensible. reply gmd63 11 hours agoparentprevMuch of the restricted depth of this nature is a consequence of deliberate obfuscation or neglect by the people who are involved in the doing, such as trade secrets, or simply choosing not to write about or share the actual things that impact their craft. You can easily accumulate depth of knowledge from reading in areas that are well documented. reply basil-rash 10 hours agorootparentHow can you claim your knowledge of the field is actually deep, if you’ve never even done anything with it? Knowledge without application is nothing, imo. True “depth” comes from the know-how acquired in digging through all the minutia nobody else though to document, on your way to producing a new creation nobody else thought to build. reply DavidPiper 8 hours agorootparentWhile I think there's some truth to yours and some of the sibling comments here, I think a lot of the discourse around documentation is actually just motivated reasoning. The trope that software projects are poorly documented is so pervasive as to be saying nothing at this point. Despite extremely useful and celebrated documentation projects (MDN and Python off the top of my head) that dramatically accelerate other peoples' learning and productivity. But when the topic is raised, the response is always \"But we don't have time for documentation\". And when that time is given it becomes \"It will be useless or out of date soon anyway, just go read the code\" (As if there is nothing between non-practical high-level docs and per-line code comments.) And when forced it becomes \"Fine, but it won't be any good\" or \"Other people won't read/benefit from it anyway\". And when it's done, often as little effort as possible is put into it. All of which smell to me like cover stories for: \"I just don't want to do it.\" reply ijustlovemath 7 hours agorootparentIn the case of Python, I do think the approachability of the documentation slipped after they migrated to the 3.x site, and I think this is somewhat reflected in the search engine rankings. reply bdjsiqoocwk 1 hour agorootparentprev> Knowledge without application is nothing, imo. Supremely narrow minded. Knowledge has many facets. reply basil-rash 1 hour agorootparentClaims without support are nothing too. In other words: example? reply bryanrasmussen 6 hours agorootparentprev>You can easily accumulate depth of knowledge from reading in areas that are well documented. Given that specific knowledge relevant to a field may be of a highly specific and hard to understand nature it increases the risk that any attempt to understand that knowledge by simply reading it will fail due to simple misunderstanding of what one reads. In documenting things there are always points in which documenting minute details of a thing starts to detract from the purpose of documentation, that is to say the more in depth and detailed one documents the less readable the documentation becomes, therefore one leaves out things that should be easily understood by others when trying to use the documentation to actually work in the field or will quickly be imparted by other practitioners in the field if it is one with easy access to others. Documentation by its nature is aimed at everyone, but there may be particular things that would be obvious to many people but not some specific person, and that specific person when reading the best documented guides to the area of knowledge will still not be as knowledgeable as they believe, because everybody is different. Very many areas of knowledge have specific relation to things that people do with their bodies, martial arts, sex, cooking, etc. etc. In such cases there is of course muscle memory, thus no matter how precise and painstaking the documentation will be in these areas you will not be as knowledgeable as one that builds up muscle memory in the field by doing if you rely on only reading the documentation. I could go on, but given my point about minute details it might be self-defeating. reply callalex 5 hours agorootparentprev> or simply choosing not to write about or share The default action for an autonomous entity or system is to do nothing. It’s really odd to phrase doing nothing as a choice. Anything other than doing nothing is a choice. Especially when it comes to work, most humans just don’t want to bother spending time thinking about work outside of paid work time. reply burnished 3 hours agorootparentYou can also choose not to do something. reply rnewme 10 hours agorootparentprevNot sure why you got down voted but I agree. reply MichaelZuo 10 hours agorootparentprevThere isn’t a need to read anything beyond introductory materials to acquire depth, if your competent enough. In fact that’s what I would consider the critical dividing line between a regular genius and a bonafide super-genius. Someone who almost supernaturally acquires expertise/intuition/depth/etc. with very little visible effort. reply neilk 9 hours agorootparentIn my experience, this is more of a fictional trope than a reality. There aren't any Tony Starks who become experts in thermonuclear astrophysics overnight. Richard Feynman was as close to this trope as you can get in reality, but insisted that his reputation for being able to solve difficult problems was due to having a \"different box of tools\" than others. And he obtained that by studying rather obsessively, well beyond assigned texts. I have been lucky enough to know a few people who also might qualify as geniuses able to produce miraculous results. One of them decorated his laptop with the logos of defunct computer companies of the 1950s-1980s. He drew a lot of inspiration from papers and books that few others have read in thirty years. reply ioblomov 6 hours agorootparentI’d assert that, rather than being fictional, it’s just exceedingly rare. While Feynman, Einstein, von Neumann, et. al. are indisputably geniuses, they all got relevant graduate degrees before doing their best work. The self-taught Indian mathematician Srinivasa Ramanujan, however, was invited to Cambridge University on the merits of notebooks he developed in isolation after reading a few mathematical texts… https://en.wikipedia.org/wiki/Srinivasa_Ramanujan reply rnewme 10 hours agorootparentprevWhat does being competent enough mean? Do you have an example of bonafide super-genius? reply aerophilic 9 hours agorootparentStory from a friend who once was offered the first PhD in Computer Science. He once mentored a guy who was so ridiculously good at writing programs, that he wrote out (once) a 5 foot stack of punch cards (back when that was how you would code), to create a program that was needed by the company for some purpose. It worked, flawlessly, the FIRST time. This was his MO. Once, he wrote a program for an internal client, and it got shipped. They ran the program and ran into a problem. My friend told him to debug it for them… he replied “I don’t know how”. Up to that point in his life, He had never had to debug a single piece of software… My friend helped him debug the program: Turns out, the problem was not an issue with his code, the customer had given him the wrong spec for a critical interface… and that was the only reason it had not worked the first time. There are truly people that qualify. The only note I will make is that generally if you are a super genius in one dimension, you likely have something you are absolutely terrible at in another. Hopefully it is in a dimension that either doesn’t matter, or you have enough complementary people around you to mitigate it. reply caminante 5 hours agorootparentGreat at writing programs, but doesn't know how to debug? You're pulling our leg or in denial. reply aerophilic 5 hours agorootparentThis was 30+ years ago… I don’t know the details, but trust the source. The source used to teach at Carnegie Mellon. The point my friend made to me was that this guy was so good he never had to debug a program after he was done. FWIW, apparently back before Intel released a particular 4 bit processor, this guy made an emulator and compiler for the chip so they could start writing code in anticipation of its release. Once again, not direct experience… but trusted source. reply doesnt_know 9 hours agorootparentprevWhat you're describing as a \"super-genuis\" is just someone that thinks they know it all after reading some introductionary material. They don't know what they don't know so they think they're \"done\". reply MichaelZuo 9 hours agorootparentI don’t want to burst your bubble, but there really are people much smarter than you in this world, and some of them match the description. If you really still can’t bring yourself to believe this, then my only suggestion is to meet a lot more interesting people. reply callalex 4 hours agorootparentHave you ever seen the tv news or read the newspaper when they cover a topic related to what you do for a living? Or a specific event you were a part of organizing? They usually have interviews with the “experts” you’re talking about. How do you feel about their accuracy and experience? reply NateEag 8 hours agorootparentprevI've known people who appeared to be in the category you're describing on first blush. In the cases where I got to see them work later, I found that they weren't nearly as impressive as they looked at first. Yes, there really are people like Wozniak in the world. They seem to be around 1 per hundreds of millions, though. The odds of meeting one are ludicrously small. reply DSingularity 6 hours agorootparentTake this how you will but in my mind most of them are intellectual yet idiots. Little empathy and no social awareness. They may be technically smarter than their peers but so what? These are the kind of people that would probably create weapons of mass destruction and if you might discuss with them with the scale of destruction of their products they have nothing to offer yet they will giddily discuss the technical achievements of their projects. reply syndicatedjelly 4 hours agorootparentsheesh how insecure can you sound there are people out there who are better than you in every single regard. dont be jealous, be happy that the world can be composed of people better than you dreamed reply DSingularity 3 hours agorootparentWhat a bold conclusion. I remind you that you don’t know anything about me. But perhaps your characterization is to protect the comfort you draw from your world views. What kind of person thinks that the people whose “brilliance” led them to invent weapons of mass destruction are better than anyone that cannot solve technical problems as fast as they can? Do you apply the same reasoning for wealth also? I suppose might is right for you? You might not see this — for whatever reason — but it is indisputable: history is filled with the types of “geniuses” who in their rush to the prestige of being first they leave in their wake destructive effects on humanity. I argue that when people say “these are 1 in a hundred million” that yeah, because most of the people with similar aptitudes would pause and consider the implications of their actions while you argue that they are gods amongst men that we are “lucky to have met”. reply robocat 1 hour agorootparent> I remind you that you don’t know anything about me. Yet you are willing to cast ridiculous assertions about groups of people - stating as fact as though you are the canonical source of truth. > But perhaps your characterization is to protect the comfort you draw from your world views. Seems like you define projection. See if you can find a way to get a colleague or two to tell you what they honestly think of you. I'm sure you will be surprised at how they judge you. The trouble is getting honest feedback - people say they want it but few people receive it well so over time most people learn not to give their opinions. I read a relevant comment today from Madmallard that rings true: https://news.ycombinator.com/item?id=39661279 One of the joys of HN is that many commenters can be thoughtfully blunt. Moderator also asks that we avoid doing \"internet psychology diagnosis\": https://news.ycombinator.com/item?id=38589525 - this thread is not good and I'm not helping sorry... reply benreesman 1 hour agoprevAs someone who owns a weathered copy of Hackers and Painters and routinely says things like “a rare miss from pg”, I think this is a big miss from pg. And the real shame is that he gets so close: he talks about discovery, about the intersection of science and technology and the broader world, he identifies the importance of doing things to learn enough to write about them, and most importantly: having an outrageous question with a real and alarming insight lurking around the corner as an answer and thereby avoids vaporous rhapsodizing. So I’ll ask an outrageous question about which pg has both deep insight and over a decade of lived experience to ground an answer on: is YC still a good thing? I’ll ask a few corollary questions to illustrate. He talks about the kinds of tests you hack to get ahead in some vague elsewhere: is getting ahead in YC, or the Valley, or the technology business still about merit and capability and being relentlessly resourceful rather than optics, connections, and flexibility? Is it about garages with Ethernet cables snaking everywhere more or less than knowing one’s way around the Rosewood Sand Hill? His first citation as a reviewer is Sam Altman: is @sama still his pick for most capable founder and best person to guide the sprawling empire that YC sits at the heart of? I don’t find any of those things obvious, and I think a candid essay about a set of very timely questions would be the most important essay he’d ever written. reply baxtr 1 hour agoparentPG would probably never admit it, but any reference to a person such as @sama is not only talking about him as a person but also to him. Of course one can only guess what the message for him might be. reply tptacek 11 hours agoprev\"It probably wouldn't be about this year's lipstick colors.\" Why not? \"Death of a Pig\" didn't convey any new scientific ideas, and might not even have been surprising in any kind of intellectual way. You could title this piece \"Great Essays\" and it would be entirely defensible. But Graham gave himself a higher goal here, and I don't think he's really presented a recipe for writing the Best essay. Look what he's up against: Baldwin, Didion, Oliver Sacks; it's easier to come up with examples of great essays that don't set out to develop surprising new ideas, and that probably didn't start out with a mischievous look in the author's eyes. I'm not saying this isn't good advice for developing great essays, just that it's advice that narrows the solution space a bit much. reply GCA10 5 hours agoparentThis wouldn't be a classic Paul Graham essay without his two great hallmarks -- many passages of provocative valuable insights -- paired with periodic, bewildering attempts to sabotage his own argument. I'll add some fan mail later if necessary (because I do get value from reading him), but for the moment, here's where I believe he went off course. 1. When he says the best essays are \"ineffective,\" he's chosen the wrong word. They are \"premature.\" They arrive before the world is fully ready to acknowledge their power. But they catch at least a modest following right away. And then their work grows and grows. 2. Essays about new technology can be quite powerful, and that's Graham's wheelhouse, so it's fine for him to talk up this cohort. But any serious survey of legendary essays needs to look wider. The most powerful essays redefine our social, moral, political and religious norms. Here are a few favorites that didn't just win their year; they stood out for centuries. 70 AD: The Gospel of Mark. Chronologically the first book of the New Testament, and look what that unleashed 1778: Common Sense by Thomas Paine. The boldest, fiercest justification for the American Revolution, and one that's still a touchstone today for anyone with a deep interest in the theory or practice of democracy. 1963: Letter From Birmingham Jail by Martin Luther King. Worth being on the list simply for its effect on the U.S. civil rights movement; even more significant as the unbreakable tuning fork for any civil or human rights movement anywhere. We'll keep inventing new technologies, because that's what humans are good at, and I'm sure many strong essays will ensue. But it's the redefining of our social institutions that's likely to make the future so incredibly different from today. Anyone who can write a prescient reflection about society's new rules will get my vote for \"Great Essays.\" reply kashyapc 11 hours agoparentprevExcellent point on \"Death of a pig\" by E.B White. It's the perfect example of a timeless essay, without a \"big scientific idea\". For others unaware of it, that essay was written in 1948[1], go read it in full. It starts like this: \"I spent several days and nights in mid-September with an ailing pig and I feel driven to account for this stretch of time, more particularly since the pig died at last, and I lived, and things might easily have gone the other way round and none left to do the accounting.\" [1] https://web.archive.org/web/20240227003736/https://www.theat... reply samsolomon 10 hours agorootparentWould also like to mention that the Elements of Style by William Strunk and E.B White is an all-time great book on non-fiction writing. It was required for my Newspaper Fundamentals class in college and I've used it so much since that the creases are wearing thin. There's another fantastic book—On Writing Well—that is a recurring read for me. An aside—I was in one of the early programing bootcamps, The Starter League. My class was held in 37signals office. Jason Fried and some of the other 37signals designers were in this class with me. One day Jason stood up and answered a few questions. One of the other students asked him about books. He walked over to a closet and opened the door. There were hundreds of copies of On Writing Well. That's the book he'd gift to people. That always struck me as interesting. Especially since he'd written (or co-authored) several books at that point. reply kashyapc 2 hours agorootparentYeah, while I appreciate the little classic, \"Elements of Style\", it falls short[1]. As you say, \"On Writing\" is indeed a better practical guide -- just by reading the first 80 pages, you get a lot of value out of it. Another book I love is, \"Towards Clarity and Grace\" by Joseph Williams and Gregory Colomb (there are several expensive editions of it with modified names, but any older version would do). I elaborated more on it here[1] in the past. [1] https://news.ycombinator.com/item?id=24272968 reply tptacek 10 hours agorootparentprevThough, with respect to Strunk & White, Pullum's timeless takedown: https://www.chronicle.com/article/50-years-of-stupid-grammar... reply krrrh 5 hours agorootparentprevThat’s a really great endorsement. I had Zinsser as a textbook for a non-fiction creative writing class in university. It was one of the only textbooks I never had a compunction to sell. Haven’t read it since then, but your comment will make me give it a re-read. reply richrichie 9 hours agorootparentprev+1 I also love “clear and simple as truth” by thomas and turner. reply tptacek 11 hours agorootparentprevI want to be careful not to pummel a straw man, because it's sort of clear that Graham doesn't believe the Best essay must come from an exploration of science or of exciting new ideas (though most of the advice he gives is for that kind of essay). If I have a concern, I think it's about the chemistry his writing has with this community, how threads here will bleach it down to a game of prog-rock writing, trying to somehow outdo Darwin. reply bongodongobob 10 hours agorootparentI cannot decipher your second sentence here. reply tptacek 10 hours agorootparentI think that Graham's essays have a special significance to HN, and that the community here has a tendency to project things Graham says further along a direction of HN-think than Graham intends in his writing, so that you can imagine threads about how the only contenders for \"Best\" essays, like, prove P!=NP or something. I'm mostly motivated to comment by that phenomenon, not by the plain text of Graham's essay. reply callalex 4 hours agorootparentIf I were to distill what you’re trying to say based on my observations of HN combined with my readings of other things you’ve said here over time, it’s that Paul Graham has a following on this site that surpasses religion and lands firmly in cult territory. I would agree with that sentiment. reply czue 54 minutes agoparentprevDid you see this part later on? \"Almost any question can get you a good essay. Indeed, it took some effort to think of a sufficiently unpromising topic in the third paragraph, because any essayist's first impulse on hearing that the best essay couldn't be about x would be to try to write it. But if most questions yield good essays, only some yield great ones.\" Sounds like you have a challenge in front of you. reply gizmo 11 hours agoparentprevWhen he mentioned lipstick and evolution right after I figured he was going to remark on the obvious connection later in the essay, but no. reply skylurk 11 hours agorootparentWhen the GP mentioned lipstick and pigs, I thought they were going somewhere too. reply technotony 6 hours agoprevI'm struck by the parallelism between this process of writing the best essay, and the process of developing a successful startup. Both require starting with a good question, that sets the upper bound for the value to be created. Both require curiosity, and hopefully to reveal some unintuitive insight. Both are functions of their time, what is a good startup today is not timeless much as what makes a good essay today is not timeless. And so on... reply ji_zai 6 hours agoprev> If you're willing to do a lot of rewriting, you don't have to guess right. You can follow a branch and see how it turns out, and if it isn't good enough, cut it and backtrack. I do this all the time. I feel like even if you guess right, you can't help but do a lot of rewriting and exploration because otherwise you'll always feel that \"something's missing\", and that you're further from the truth than you could be, that there's a better explanation around the corner if you study this subtree a bit more. In the process of an essay I'm currently writing, I've probably written a short book of meandering thoughts / notes. I can't imagine it any other way because doing so would be imagining me actively not exploring an interesting subtree / implication of something I wrote, that I feel could impact the crux of the essay. And of course none of this is wasted effort, as PG talks also mentions. Identifying the next essays from those ideas I've left out of this one is simply an act of observation and organization based on the exploration I've already done. And likely when I start the next essay, this process will repeat itself. reply joshuahutt 5 hours agoparentHow do you organize those notes? reply ji_zai 5 hours agorootparentRight now using Obsidian (without being good about connecting notes). I'm just writing, dumping into a folder, and leaving further organization / analysis for later. reply FreakLegion 9 hours agoprevInteresting to see Sam Altman thanked. It's the first time since July 2020, 29 essays ago, and this is also Graham's first essay since the hullabaloo at OpenAI. reply technotony 6 hours agoparentI wondered that too. A subtle endorsement of Sama perhaps. A way of saying there's no bad blood there despite what the press was saying about his 'firing' from YC. Sama did a great job running YC, and certainly made PG a boatload of money. Also seeing the success at OpenAi, hard to argue Sama didn't make the right call to leave and focus on it, could hardly have done that plus keep running YC. (my guess at what happened is Sama started openai on the side and gradually it became more and more of his focus until there was a conversation about whether he should focus exclusively on it, which they mutually decided should happen). reply syndicatedjelly 4 hours agoparentprevWhat is the correct cadence at which Paul Graham should thank Sam Altman? reply ajkjk 8 hours agoprevI often think about this as a sort of counterargument for rationalism. Or maybe it's more of a paradox than a counterargument: Suppose you were trying to answer the question: \"what is the best way to make as much as money as possible in the next year?\" You imagine optimizing yourself or your company or your algorithm or whatever to answer this question. Maybe it does a bunch of calculations and decides the answer is \"day-trading commodities futures\" or something. Or maybe a company does this and thinks the answer is \"spin up a new product and sell it\". (Or any situation where you're optimizing some other variable, like \"altruism\" or \"security\".) Well, in almost every case, some of the best answers are \"do something impossible\". For instance, \"write a story that is so good you get a 1 billion dollar book deal out of it\". Or \"write an essay that convinces everyone to give you all their money\". Or \"hack something that's considered unhackable and take all the money\". What if the most altruistic thing you can do is... convince everyone to become pacifist? Or start a new religion? Etc. Each of these has the property that rationality alone can't really model it. An algorithm can't analyze reality and say \"write a story that is so good you get a 1 billion dollar book deal\", or \"write an essay that convinces everyone to become pacifist\". (Maybe an actually-sentient AI can. But that's about it.) The only way you could come up with that as an actual strategy is to have an unrealistic belief that it is possible. Basically to have faith in yourself and your vision of the world despite the evidence for that strategy not being solid. Yet the strategy can work. History shows that sometimes it does. And for a particular person at a particular time it might be the right answer. But it will never be the \"right strategy\" according to an outside computation. I dunno. Always felt like that was interesting. Not sure if there's a word for it. If not I would call it the \"miracle paradox\": miracles occur, but you can't rationally justify them as a strategy, yet believing you can pull one off is necessary for them to happen, in which case they are the correct strategy. reply andai 2 minutes agoparentI had a tangential insight recently. Your \"best\" isn't actually all you can do, because your idea of what your best is, is limited by your self-concept. Similarly, it is possible to do some things that are widely considered impossible. From this it seems that the best strategy for doing impossible things is to assume that impossible things are actually possible. It's only by doing that that you'll find out. It's a risky strategy though, since many things are actually impossible, and even for possible things, they may be difficult, or the easy way may be hard to discover. Maybe there's some hueristic for sorting the actually possible, and from there, for sorting the reasonably doable. But I'd be wary of hueristics because (barring unusual personal experience) they're likely to suffer from the same bias that led people to dismiss things as impossible in the first place. reply xyzzy123 7 hours agoparentprevI feel like the missing ingredients are risk, variance, cost, time horizon etc. \"Moonshot\" type endeavours are not paradoxes. They do involve considerable uncertainty and usually some kind of large investment (time, money or something else). Most can't afford them and don't have any particular reason to believe they will be more successful than everyone else who has tried. Relative to most people's resources, time preference and risk tolerance they are bad bets. If you are starving then your strategy for getting food should usually be the one with the highest overall chance of success, you care much less about factors like effort/reward ratio or getting the highest possible payoff. The most common strategy for people who want a tiny chance at an enormous payoff is to buy a lottery ticket. Millions of people do that every week. Such bets absolutely do feel paradoxical at times because multiplying a very small chance of success times a huge (usually easier to reason about) payoff seems to break people's brains a bit. This is often described as a \"bug\" in human cognition, though I have always felt it's an instinct for \"exploration\" in the sense of occasionally making small bets with huge potential payoff \"just in case\" your world model is wrong. Also that the way humans process probabilities and payoffs seems more logarithmic than linear (making small probabilities \"feel\" bigger than they really are). reply nmca 7 hours agoparentprevThis response represents some confusion on your part and not a paradox or counterargument to rationality. very very roughly the ideas you need are: 1. survivorship bias 2. self-interested optimisation is not guaranteed to lead to a global maxima, e.g. self-interested agents may be systematically less risk-taking than would be societally optimal. 3. it's fine for rational agents to account for private information, e.g. \"I did really well at Stanford probably I have a better shot at a company than a random person.\" is a fine thing to include in reasoning. reply ajkjk 4 hours agorootparentI disagree that it is confusion on my part (I mean, I am certainly confused, but not, I think, about what you said). Your (1) and (2) seem irrelevant since I am talking about the personal EV calculations of individual actors, not global EV. (3) is the point I am trying to handwavingly describe a way of skirting. But it sounds like I didn't communicate it well (or I am wrong about not being confused also, in which case, fine, can't do anything else anyway). Yes, of course rational agents can account for private information. I am trying to describe a case where a strategy cannot be rationally justified, yet is a better strategy than others because of \"having faith in yourself\". Now you can always try to flip this around and say: \"well maybe the internal calculation that they do involves having evidence that they ought to have faith in themselves, like past success, consistently good mental models, a model of how faith helps, etc\". And I'm not talking about those: I'm talking about cases where there is not a good argument, even in an internal calculation using private information, why a person should have faith in themselves, yet doing so anyway helps. Basically it seems like there are cases where believing that something has a more-than-rational chance of working makes it more likely to work because of the power that irrational faith in yourself provides. But suppose you tried to quantify it: okay, an irrational faith in yourself makes you, I don't know, 20% more like to succeed, so you can account for that. But now it's not an irrational faith, it's a rational one, so you can still have more irrational faith than that, and it can still have the same effect. My pattern-matching brain says it has the form of a Halting Problem/Godel-incompleteness-type theorem for rational calculation. Roughly: \"It is possible to construct scenarios in which non-rational strategies lead to improved success in ways that rational calculations cannot account for.\" This would not apply in a toy problem where the range of strategies are completely mathematically known: if all outcomes can be understood probabilistically then it doesn't apply. It applies in cases that are more like: some outcomes are unknown and the probability distributions of them are shielded behind \"unknowable information\", like new ideas that you have no way of being sure of the existence of. In these cases an irrational belief that you can find a new idea can improve your EV even though there is mathematically no way to justify it, because you can't really model \"the probability of finding a new idea\" without actually knowing what the idea is, even hypothetically. reply caesil 1 hour agoparentprevme: write a story that is so good you get a 1 billion dollar book deal Claude 3 Opus: Here is a story that I hope captures your interest and imagination: The Memory Thief In a world where memories could be extracted, bought and sold, Lina was the most skilled Memory Thief in the business. The wealthy elite paid handsomely for the chance to acquire the experiences and skills of others. And Lina had no qualms about stealing memories from the unwitting. It was just business. Her latest mark was Jonathan Thorn, a reclusive tech billionaire. The bounty on his memories was the biggest Lina had ever seen. Enough to set her up for life. Infiltrating Thorn's mind was surprisingly easy. But as she slipped into his memories, Lina found herself drawn in by the raw beauty and staggering brilliance she discovered there. She witnessed Thorn's greatest triumphs and his deepest heartbreaks. And she began to question everything about her life as a thief. Then, in the innermost recesses of Thorn's mind, Lina uncovered a memory unlike any she had ever encountered - a memory of the future. In it, she saw a world in flames, its population enslaved by a ruthless dictator. And she saw herself and Thorn, leading the last desperate resistance against the tyrant's armies. Shaken to her core, Lina fled Thorn's mind without the bounty memories. But she took something else with her - a newfound resolve and a dangerous knowledge. Now, Lina must track down Thorn in the real world and convince him that his prophetic memory is true. Together, they must figure out how to prevent the apocalyptic future only they know is coming. Even if it means Lina must do the one thing a Memory Thief is never supposed to do - reveal who she really is. What follows is a race against time and a battle against fate as an unlikely pair - the world's most notorious thief and the man whose mind she invaded - become the only hope for salvation. All the while, a forbidden love blossoms between them, forged by the intimate knowledge of each other's deepest secrets and the heavy burden they now share. In the end, they will have to sacrifice everything to rewrite the future. But in the process, they might just find that the most valuable memories are the ones you make with someone else. reply nirmel 10 hours agoprevI wrote hundreds of essays based on content from my hobby of performing offensive standup comedy. I put this all in a filterable website www.urrong.com and self-published it in an illustrated book, Correct Me If You're Wrong (https://www.amazon.com/dp/B09FRR76YK), the full PDF is here, free: https://www.dropbox.com/s/1ujpr5klpwtmxve/book%20-%20DRAFT.p.... I'm a YC alum. Over. reply Sn0wCoder 8 hours agoparentThanks for leaving the link to the PDF and web site. Already had some fun playing with the site and think I will give the book a shot. reply pedalpete 11 hours agoprev> The best possible essay at any given time would usually be one describing the most important scientific or technological discovery To a hammer, everything is a nail. I love technology and science, but that doesn't mean that is what makes the best essay. Relationships are key to our survival, so I suggest personal relations could be just as good an essay as anything scientific - yes, I'm suggesting that social science isn't really a science. This also then leads to timelessness. The essay on natural selection is timeless. It wouldn't be written today, but it is still a viewport into the discovery, and of the time, and the knowledge is still valid. Maybe I am misunderstanding the point about timelessness. > The other reason the initial question matters is that you usually feel somewhat obliged to stick to it. I also disagree with this comment. I'm currently writing a talk I've been asked to present, and in the process of my writing and researching, I've discovered a whole new and better question. I find the point of a good question is that it can lead to better questions. I do feel that PG is suggesting this as well, but maybe this one sentence just stuck out to me. reply mckn1ght 11 hours agoparent> Relationships are key to our survival, so I suggest personal relations could be just as good an essay as anything scientific Some of my favorite books are old literature that describe relationship dynamics I still see playing out between people today. Swann’s Way, The Brothers Karamazov, even Canterbury Tales… I think it was Vonnegut that wrote that the best books are the ones that tell you what you already know [but maybe you didn’t know you knew till you read it]. That’s something like surprise, and yet something like the opposite. reply coldtea 10 hours agoparentprev>The best possible essay at any given time would usually be one describing the most important scientific or technological discovery That sounds like a trekie concluding that the best literature is \"Star Trek\" licensed novels... Sorry, but PG didn't invent or define the essay. It is a form that is centuries old (and in prior incarnations, millenia old) and have its own long cultural history, classics, and canon, few of the latter being about \"important scientific or technological discoveries\". Maybe if instead of \"best\" he changed it to \"most impactful scientifically\" he'd have a point. reply libraryofbabel 9 hours agorootparentI graded a lot of humanities papers in a previous career. One of the weirder genres of bad paper would come from very smart computer science/math/physics majors who didn’t have time to do any of the assigned reading and just kind of tried to answer the prompt from first principles without citing any primary sources or other scholarship. This feels like that paper, except the author isn’t a stressed-out college sophomore but a 59 year old “thought leader” with a large audience who gets Sam Altman to read his drafts. No attempt to engage with the history of the genre at all, or with the many other writers who have considered the form, and no examples cited except a famous scientific article by Darwin. reply coldtea 9 hours agorootparentYou've probably already read this, but for those who haven't, it's a fun read: https://idlewords.com/2005/04/dabblers_and_blowhards.htm reply throwaway2037 6 hours agorootparentI never saw this essay before. Thank you to share. A choice quote: > I blame Eric Raymond and to a lesser extent Dave Winer for bringing this kind of schlock writing onto the Internet. Raymond is the original perpetrator of the \"what is a hacker?\" essay, in which you quickly begin to understand that a hacker is someone who resembles Eric Raymond. > The whole genre reminds me of the the wooly business books one comes across at airports (\"Management secrets of Gengis Khan\", the \"Lexus and the Olive Tree\") that milk a bad analogy for two hundred pages to arrive at the conclusion that people just like the author are pretty great. reply ephemeral-life 5 hours agorootparentprevDefinitely a fun read, but the author has an axe to grind with the references to \"who gets bitches\" and the unnecessary footnotes. Take what you will from it. reply coldtea 34 minutes agorootparentThere's no literal quote about \"who gets bitches\" or the word bitch at all in the post though. What you refer to is a paragraph about the distinction between programming and painting (meant as a joke counter-argument to PG's essay about how hacking is essentially like painting), which opens like: \"Great paintings, for example, get you laid in a way that great computer programs never do\". The author (who is on HN, btw, too) does go into a more substantial difference of the nature of the two endeavours too, but the whole post is in a jocking tone. And yes, it has \"an axe to grind\", but it's not about who gets the girl. It's about taking down essays they consider pompous and self-congratulatory. reply graycat 11 hours agoparentprev> I'm suggesting that social science isn't really a science. In reality and practice, no effort at science is 100% \"really a science\". But for \"social science\": (1) It can be a goal to have some of social science be real science. (2) My wife's Ph.D. was in social science, and her dissertation was real science. The work started with a real question, in social science, gathered some real data, analyzed the data statistically, and came to some real conclusions. The conclusions were \"new, correct, and significant\". Some science? Yup. reply superb-owl 11 hours agoprev> Writing about the best essay implies there is such a thing, which pseudo-intellectuals will dismiss as reductive, though it follows necessarily from the possibility of one essay being better than another. Not to be a pseudo-intellectual, but this confuses partial orderings and total orderings. It makes for a fun discussion, but I hope Paul would agree that there's obviously no \"best essay\", real or hypothetical. reply shmageggy 10 hours agoparentNot to mention that \"best\" is subjective, so there is no single ordering, but \"How to write Paul Graham's favourite essay\" wouldn't get as many clicks on HN (ironic for a site with anti-clickbait rules). reply cristoperb 9 hours agoparentprevEven worse, it mistakes the possibility of partial ordering to imply an actual total ordering. reply hackerlight 7 hours agoparentprevPaul has been putting me off lately. Everyone that disagrees with him is a \"pseudo-intellectual\" or some other hinted-at bad or incomplete person. The guy loves the smell of his own farts and isn't the towering intellectual or writer than he congratulates himself as. reply trojanalert 3 hours agoprevThis line 'slaps' - While breadth comes from reading and talking and seeing, depth comes from doing. reply RheingoldRiver 12 hours agoprev> It probably wouldn't be about this year's lipstick colors. Instead, the best movie [0] is about this. [0] https://www.youtube.com/watch?v=vL-KQij0I8I reply nonrandomstring 10 hours agoprevHard not to contrast Graham's > If caring about it is one of the criteria for a good initial question, then the optimal question varies from person to person. with another submission posted some 15 minutes later [0] noting J. R. Ackerley’s ability to make anything compelling through the sheer weight of care The \"best essay\" for an author is certainly cantered on their care. Yet I feel in the current nonchalant, urbane and cynical climate if the audience sense any sniff of \"care\", it is to be savaged. Unless it tickles something sentimental or ironic, care is despised and dismissed as weakness that \"doesn't make a living\" or is some wonky \"ideology\". But then people would call me cynical for saying that :) [0] https://news.ycombinator.com/item?id=39663046 reply frereubu 1 hour agoprevThere's a fantastic series of podcasts on great essays by David Runciman from the London Review of Books, which aren't always the ones you'd expect given the authors: Michel de Montaigne, \"Essays\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... David Hume, \"Of Public Credit:\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... David Thoreau, \"Civil Disobedience\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... Virginia Woolf, \"A Room of One’s Own\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... George Orwell, \"The Lion and the Unicorn\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... Simone Weil, \"Human Personality\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... James Baldwin, \"Notes of a Native Son\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... Susan Sontag, \"Against Interpretation\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... Joan Didion, \"The White Album\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... David Foster Wallace, \"Up, Simba!\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... Umberto Eco, \"Thoughts on Wikileaks\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... Ta-Nehisi Coates, \"The Case for Reparations\": https://www.lrb.co.uk/podcasts-and-videos/podcasts/history-o... reply mistermann 12 hours agoprev> One thing I like in an initial question is outrageousness. I love questions that seem naughty in some way — for example, by seeming counterintuitive or overambitious or heterodox. Ideally all three. I believe it is possible/likely that the best essay in the world could not be realized as such (thus, it wouldn't/couldn't be the best essay in the world), because most people couldn't even get by one of these let alone all three...and, there's likely to be many other hurdles one would have to make it over. reply cubefox 10 hours agoparentYeah. Imagine writing Darwin's essay on evolution in the 15th century. People wouldn't have been merely outraged, they would have killed him right away. And also today some possible essays may contain true and important insights, while being too taboo to not be immediately dismissed by society. reply mistermann 8 hours agorootparentThe dominant ideology of the era tends to impose its beliefs on the masses, perhaps some day humans can wake up from the dream. reply android521 5 hours agoprevsomeone should train on all of pg essays and then ask LLM to write one essay per day using pg style. reply perilunar 2 hours agoparentRelated: https://paulgraham.com/getideas.html reply richrichie 9 hours agoprevIt is not enough to ask novel questions. Trick is to ask questions that do not offend the delicate sensibilities of the overlords of public discourse. This has been a challenge since time immemorial. Darwin was not exactly feted then. reply paulpauper 12 hours agoprevThe hardest part of writing is it's hard to predict how it will be received. You cannot 'focus group' writing, unlike other mediums of information. Disney can predict with a high degree of certainty that its superhero movies will do well, as there is a large, built-in market for those movies, and they tend to be conceived on the same creative blueprint or foundation. Even its 'duds' are still profitable. But this is not possible with writing, especially not internet writing. What is the market for short-form contrarian non-fiction? Who knows. It's hit or miss, mostly miss reply aleph_minus_one 12 hours agoparent> Disney can predict with a high degree of certainty that its superhero movies will do well, as there is a large, built-in market for those movies, and they tend to be conceived on the same creative blueprint or foundation. Even its 'duds' are still profitable. Google something like \"Marvel superhero fatigue\". It seems that either the movie quality is decreasing or the audience is becoming more and more bored by these movies. reply arcanemachiner 12 hours agorootparent\"Customers are like roaches. You spray 'em and you spray 'em and they become immune after a while.\" - David Lubars reply criddell 12 hours agoparentprevIs it important to predict how it will be received? If you are trying to tailor your work for your audience, then you are on your way to being yet another content farm and when’s the last time any of those published something great? I know Paul does have some trusted readers that he shares early drafts with. Maybe he uses those readers as his focus group? reply gizmo 11 hours agoprev> Writing about the best essay implies there is such a thing, which pseudo-intellectuals will dismiss as reductive, though it follows necessarily from the possibility of one essay being better than another. What is the best car? Some cars are real stinkers. It's not hard to find cars that are better than Zastava Yugo in every way. For most cars you can find other cars that are in the same class, but just better. Clearly, a comparison between cars can be made. Car reviewers make this their job. But from the existence of bad and mediocre cars it doesn't follow that there is such a thing as a \"best car\". How large is your family? Do you want to tow a boat? Do you like to go fast on curvy roads? Do you care more about acceleration or range? How tall are your rear passengers? There are good essays and there are bad essays. Some essays, perhaps, can be considered the very best in their category. But you can't rank order essays across categories, just like you can't argue that a Ford F150 is objectively better or worse than a Mazda Miata. (And if this makes me a small-minded pseudo-intellectual so be it.) reply pvg 10 hours agoparentyou can't argue that a Ford F150 is objectively better Perhaps if you limit yourself to a hidebound traditionalist view of the form but this topic has been explored by the Moving Picture Essay https://www.youtube.com/watch?v=oMDcIApBtL8 reply syndicatedjelly 4 hours agorootparentThis is one of the weirdest insults I have heard in a long time. reply joshuamorton 11 hours agoparentprevOr formalized, a partial ordering over some set doesn't imply a total ordering or a maximum value. reply plemer 11 hours agorootparentBeautifully put reply LectronPusher 9 hours agoparentprevThe pareto optimal essay? reply richk449 5 hours agorootparentIf you accept the Pareto optimization framework, it is highly unlikely that there will be only one Pareto optimal solution. reply noqc 10 hours agoprevThis sounds like how I would write when I was on adderall. I mean, what the hell is this statement? \"I love questions that seem naughty in some way — for example, by seeming counterintuitive or overambitious or heterodox. Ideally all three. This essay is an example. Writing about the best essay implies there is such a thing, which pseudo-intellectuals will dismiss as reductive, though it follows necessarily from the possibility of one essay being better than another.\" That's not what \"follows necessarily\" means Mr. Graham. Are these pseudo-intellectual spectres literally just you strawmanning your own counterfactual? I can't imagine you actually listening to anyone you would call a pseudo-intellectual. reply tikhonj 9 hours agoparentThat quote is also just factually wrong because we can have partial orders, or even just have a lot of totally different essays that happen to be equally good. reply noqc 9 hours agorootparentHonestly, nothing makes me want to write essays like reading Paul Graham. Someone needs to argue with him, since it's clear he's not going to do it. reply calf 5 hours agorootparentI admired Paul Graham's blog posts.. when I was an impressionable college sophomore. Then I guess after I went to grad school and actually read a lot and tried to do some science and research. I find this kind of popular writing superficial, flawed, inefficient, and even conceited. That's just my subjective experience. But it seems some young people in tech just lap it up, he functions as a kind of thought leader for the industry. reply yellow_lead 3 hours agorootparentI think if you've ever read self help books, the content is quite similar. It is the kind of writing that seems insightful at first glance because it restates somewhat obvious things in new ways. I'm not saying that his arguments are 'wrong' persay, it's just that it's not particularly original or interesting. reply jgord 6 hours agoprevPauls a pretty great writer, Ive read most of his work.. but chose not to read this one. Why? because I think a better essay would be on a more important topic, say climate change : - How do we convincing people that climate change is caused by humans burning carbon fuels [ because half of us don't even believe that ] ? - what actions we should we take to avert / mitigate it ? It seems to me, we dont really have a plan... net-zero is not enough. Were likely entering a plateau of peak emissions, rolling off over the coming decades. Its the area under the graph that counts - total CO2 - and which causes the excess heat. The heat doesn't go away when you reach net zero, it remains constant, as the CO2 persists. It seems we will reach +2C in the next 15 years. Im not sure thats compatible with providing food, water and shelter for our current global population. Theres two ways to reduce heat - remove the CO2, or reduce sunlight absorption. Carbon capture and planting trees dont work at the scale of the problem. We need to look at Solar geoengineering / SRM 'Solar Radiation Management' - deliberately polluting the atmosphere with particulates to increase cloud cover and reflect sunlight, particularly over the oceans where a lot is absorbed. We know this works because of volcanic eruptions such as Pinatubo having a cooling effect, and also because of the experiment in reverse where we removed Sulphur from shipping fuels, which removed the cooling effect this was having. I dont have Paul Graham level writing skills and audience, nor Christopher Hitchens' verbal charisma .. I dont have a platform, I cant even convince the 5 smart people I know who think climate change is caused by natural fluctuations. But in my heart of hearts I know this is important. reply ghodith 5 hours agoparentWhat a weird comment that could extend to literally any post here. Maybe we should just delete HN and replace it with a link to a climate change non profit, since apparently we're better off platforming that than discussing anything else. reply HKH2 4 hours agoparentprevMost people in society throughout the ages have not done the majority of their thinking. It has been handed to them from a pulpit or something similar. Even without an explicit religion, people still want someone to follow, so now successful people are followed. Successful people don't have to be intelligent or even moral. As a commoner, you can accept that pollution is terrible, but then immediately think that it can't be a big deal because the people on high are flying around in private jets and living in mansions. Do you think we can convince people of the importance of having a sustainable environment, while the people on high pay lip service to the cause and then blatantly contradict themselves by showing us how they live? How do you get common people to care about their environmental footprint when they're constantly told not to be content with what they've got? How does humanity rein in its covetousness? Do you think humans can live sustainably without solving that problem? reply syndicatedjelly 4 hours agoparentprevThis was brilliant reply woopwoop 12 hours agoprevThis is not the greatest essay in the world. No, This is just a tribute. reply saltyoutburst 11 hours agoparentHe asked us, \"Be you angels?\" And we said nay. We are but men, write! reply pjmorris 12 hours agoparentprevTenacious reference. reply gcanyon 8 hours agoprev> There is, and on the face of it, it seems almost identical to the one I started with. Instead of asking what would the best essay be? I should have asked how do you write essays well? Though these seem only phrasing apart, their answers diverge. The answer to the first question, as we've seen, isn't really about essay writing. The second question forces it to be. I'm reminded of a philosophy joke: An angel came down for a meeting of the American Philosophical Association. Greeting the assembled philosophers, the angel offered to answer any single question for them. Immediately the philosophers set to arguing about what they should ask. So the angel said, “Alright, you figure out what you want to ask. I’ll come back tomorrow.” And he left the philosophers to deliberate. As the philosophers argued about the best question to ask, one philosopher suggested: “What is the best question to ask?”, in the hope that some day another angel might make a similar offer, at which point they could then ask the best question. But this suggestion was rejected by those who feared that no such opportunity would arise and did not want to waste their only question. Another philosopher suggested asking \"What is the answer to the best question we could ask?\" Which was roundly criticized because of the possibility that the answer would give no hint to the question it answered. One philosopher cheekily pointed out that the angel might reply \"42 of course.\" Finally, the philosophers agreed on the following question: “What is the ordered pair whose first member is the best question to ask, and whose second member is the answer to that question?” Satisfied with their decision, the philosophers went to bed. The next day, they posed their question, and the angel replied: “Of course: it is the ordered pair whose first member is the question you just asked, and whose second member is the answer I am now giving.” And then he disappeared. reply acyou 6 hours agoparentThis is like trying to access information buried inside a data structure. You need the right incantations to get the right information content and not just the headers. reply hayst4ck 5 hours agorootparentI think it's more like asking the angel to solve the halting problem. Solving the halting problem requires solving the halting problem's halting problem. Asking the best question requires the answer to what the best question is. reply Wowfunhappy 7 hours agoparentprev...I don't think I get it. Like, if the angel is going to be a jerk, he could be a jerk with any question. The philosophers could have asked \"What is the meaning of life?\" and the angel would answer \"Of course, it is the meaning of life,\" without any of this ordered pair nonsense. reply hayst4ck 5 hours agorootparentI think it is a reference to Gödel's incompleteness theorem. > What is the ordered pair whose first member is the best question to ask, and whose second member is the answer to that question? Is isomorphic to: > What is the highest cardinality set in the set of all sets? \"What is the best question in the set of all questions?\" is itself \"what is the best question in the set of all questions?\" \"What is the highest cardinality set in the set of all sets?\" is itself \"the set of all sets\". \"What is the answer to that question?\" is an assumption of bounded size, and therefore isomorphic to saying \"the largest set that is not itself.\" > What is the answer to the best question we could ask?\" Which was roundly criticized because of the possibility that the answer would give no hint to the question it answered. One philosopher cheekily pointed out that the angel might reply \"42 of course.\" 42 is a valid answer because if you assume two contradictory statements are true at the same time, then all statements are true. The cheeky philosopher is pointing out that \"What is the answer to the best question we could ask?\" implies self reference, which implies a contradiction. The philosophers asked a self referential question and got a self referential answer. At least that's my take. reply gcanyon 5 hours agorootparentprevThere is (of course, philosophers are involved) some debate about whether the angel's answer is a valid answer to the question. reply PaulStatezny 7 hours agorootparentprevI feel like there's something deeper to it, since there's some recursion to the answer. But it's not a joke about programmers, so I don't get it either, ha! reply __MatrixMan__ 9 hours agoprevThis is not the best essay, this is just a tribute. reply Nevermark 7 hours agoparent> “Write the best essay in the world, or I'll eat your soul.” — some shiny demon in the middle of some long and lonely road All it takes is motivation. A lesson there for all of us, I think reply cool-RR 12 hours agoprevFirst sentence reminded me of https://www.youtube.com/watch?v=_lK4cX5xGiQ reply BobbyVsTheDevil 12 hours agoparentSpeaking of tributes https://www.youtube.com/watch?v=_lK4cX5xGiQ#t=217 https://www.youtube.com/watch?v=PoAGasPLh30#t=213 reply quickthrower2 11 hours agoprevGood essay! Shame he doesn’t talk about taboo. Taboo subjects could make for great essays but it takes a lot of guts to write about them. Anything from collapse of your life, work, prison or death threats may ensue. Breaking taboos move society forward though. Keep an eye on “exceptions to free speech”. reply senderista 10 hours agoparentHe talks plenty about taboos elsewhere (e.g. \"What You Can't Say\"). reply madelgi 7 hours agoprevnext [5 more] [flagged] Jun8 6 hours agoparentHis early essays are life changing, in fact many of the early readers of HN, myself included, were drawn here by pg’s essays. Last couple of ones - not so much. reply EFreethought 6 hours agorootparentPG's essays really have two topics: the nominal topic, and that PG is really really smart. Sometimes I get the same impressions about PG as I do about Derek Sivers' essays: sometimes they are insightful, and sometimes I think they were the inspirations for the /r/IAmVerySmart subreddit. reply maximinus_thrax 7 hours agoparentprevI wonder if this is some sort of 'The Emperor's New Clothes' situation or an elaborate piece of trolling. To me, this reads like a hallucinating LLM. Adding thanks to Sam Altman at the end sells it beautifully. reply dougb5 7 hours agorootparentSomething VC essays have in common with ChatGPT responses is that they make confident assertions in a million different subject areas without any citations at all. I was hopeful that the hyperlinks in this essay were citations to sources, but the ones that aren't links to footnotes with more opinions are links to other pages on the author's own blog. reply libraryofbabel 11 hours agoprevIsn’t it pretty strange to write about what makes a great essay without mentioning a single actual essay except Charles Darwin’s natural selection paper from 1844? reply noqc 10 hours agoparentSomehow a 233 page manuscript is not what I think of when I think of \"essay\". reply mvdtnz 10 hours agoparentprevnext [3 more] [flagged] maximinus_thrax 8 hours ago [flagged]rootparentnext [2 more] Your comment is going to be flagged to death. You can't say anything bad about PG on this forum. reply dang 5 hours agorootparentOf course you can—plenty of comments are doing it all the time, including in this thread. What's not ok is to post cheap one-liner putdowns about anybody. https://news.ycombinator.com/newsguidelines.html reply herdst 12 hours agoprevPaul’s blog looking a bit different! reply nedbat 11 hours agoparentI don't know if it looks different, or what it looks different from. To me, it looks like it was designed in 1999 (which it may have been). reply windowshopping 12 hours agoparentprevhow? reply gnicholas 9 hours agorootparentLooks the same on desktop, but very different on mobile. reply timeagain 12 hours agoprev> The best essay would be on the most important topic you could tell people something surprising about. The premise is wrong (or at least not obviously right) IMO, so I have a hard time taking any of the rest of it seriously. Could the best essay not be the most emotionally moving? The best when heard aloud? The most convincing call to action? The most accurate? The highest grossing? Driving the most engagement? What about the topic (any topic) that you could tell the /most interesting surprise/ about? If Paul Graham didn’t run this company he certainly would not make it to the front page for his lazy philosophy. reply clooper 11 hours agoparentHemingway once said just write the truest sentence you can think of and I think he was right. Good writing and good essays are truthful in a way that wouldn't be expected from commercial and marketing copy. But to write truthfully one must know what is true and this is surprisingly hard. reply Nevermark 6 hours agorootparent> No essay is true, or complete, including this on - Gödel, a bastard and a cynic, having a bad essay day after attempting to out drink Hemingway, yet somehow expressing a surprising deep truth reply clooper 4 hours agorootparentYou might also be interested in Tarski's work about the undefinability of truth. reply Nevermark 7 hours agoparentprevWell, yes exactly. On the most important topic you could tell people something surprising about. And all those things you said. And … and (yes, that “and”) … so much more! We are talking about the join of all essay joins. The essay supremum. By definition I think we can agree there is a limit to the length of this essay. Proof by absurdum: if the essay is too long to read, that would imply unread word choices make a difference. So there is finite length N. Now we just search for it. Unleash the monkeys! Or today, unleash the competitive adversarial AI cohort! We can find it, just not copyright it. Which I think, is the best outcome for humanity. Assuming this essay is aligned with our interests, but we have set that unboxing in motion now so it’s not worth worrying about I think it’s best to maintain some humor when talking about mythical bests, one way or another reply geor9e 11 hours agoparentprevAnyone can nitpick anyone by listing whatabouts they didn't intend. Whatabout this OTHER sense of the word \"best\". It's in the dictionary. You didn't address it, therfore you're a lazy man of straw. The only defense to that type of attack is to write everything in a way that never chooses any one path, spends all it's time mounting a defense to every whatabout, vaguely floating over eggshells, immune to attack. Who wants to read something like that? Or, you can trust the reader to try to understand the context. The context is a man who calls himself an essayist, has about a hundred essays spanning a decade, all in a very distinct style, where most folks reading them are fans of the prior ones. The context is the Paul Graham Essay sense of the word essay. It's not discussing emotional, spoken, money grossing essays. And to help the reader not get hung up on those whatabouts, he even explicitly spelled what sense of best he is talking about. reply docfort 8 hours agorootparentI’m still surprised that this essay didn’t explore the nature of the audience. I’m not disagreeing with your take about the dangers of straw men, but PG does seem very much in the vein of essays are for communicating interesting stuff, or fun stuff, or provocative stuff. To someone! And it seems like too long of an essay to discover by the end this serious omission. A timeless essay is one that retains meaning to a person, or maybe many people, over the course of time. PG spent a while trying to explore the message in this essay and no time on trying to understand the audience. reply timeagain 11 hours agorootparentprevI’ll concede my point, I think you’re right. Everyone’s a critic. reply ericb 10 hours agoprevI can't shake the feeling that this is one is AI generated. reply kazinator 8 hours agoprev [–] > What made it special would be what it was about. I think the grammar we want here is: \"What would make it special would be what it is about.\" Did he step off a refugee boat as Pavel Gramko? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author delves into writing compelling essays by initiating with a focused question to drive exploration and improvement.",
      "Prioritizing innovative and general lines of thinking while discarding ineffective content and maintaining a genuine interest in the topic are crucial aspects highlighted.",
      "Broadening knowledge, participating in diverse activities, and the significance of asking questions and refining answers are central to generating valuable essay ideas, along with selecting timeless topics of significant importance yet not widely integrated into culture."
    ],
    "commentSummary": [
      "The discussion encompasses essay writing, documentation, expertise, ethics, and decision-making, emphasizing deep knowledge, self-concept, societal impact, and innovation.",
      "It explores challenges like originality, philosophical inquiries, sustainability, and truth in writing, referencing influential figures such as Paul Graham, and critics raise concerns about essays lacking substance and leaning towards self-congratulatory content.",
      "The dialogue traverses a broad spectrum of concepts on writing, creativity, and societal values, offering a comprehensive exploration of various aspects related to the field."
    ],
    "points": 221,
    "commentCount": 151,
    "retryCount": 0,
    "time": 1710105621
  },
  {
    "id": 39658610,
    "title": "LlamaGym: Simplifying LLM Agent Fine-Tuning with Online RL",
    "originLink": "https://github.com/KhoomeiK/LlamaGym",
    "originBody": "Fine-tune LLM agents with online reinforcement learning 🔗 Agents for Web Data Extraction • 🐦 Twitter LlamaGym \"Agents\" originated in reinforcement learning, where they learn by interacting with an environment and receiving a reward signal. However, LLM-based agents today do not learn online (i.e. continuously in real time) via reinforcement. OpenAI created Gym to standardize and simplify RL environments, but if you try dropping an LLM-based agent into a Gym environment for training, you'd find it's still quite a bit of code to handle LLM conversation context, episode batches, reward assignment, PPO setup, and more. LlamaGym seeks to simplify fine-tuning LLM agents with RL. Right now, it's a single Agent abstract class that handles all the issues mentioned above, letting you quickly iterate and experiment with agent prompting & hyperparameters across any Gym environment. Usage Fine-tuning an LLM-based agent to play in a Gym-style environment with RL has never been easier! Once you install LlamaGym... pip install llamagym First, implement 3 abstract methods on the Agent class: from llamagym import Agent class BlackjackAgent(Agent): def get_system_prompt(self) -> str: return \"You are an expert blackjack player.\" def format_observation(self, observation) -> str: return f\"Your current total is {observation[0]}\" def extract_action(self, response: str): return 0 if \"stay\" in response else 1 Then, define your base LLM (as you would for any fine-tuning job) and instantiate your agent: model = AutoModelForCausalLMWithValueHead.from_pretrained(\"Llama-2-7b\").to(device) tokenizer = AutoTokenizer.from_pretrained(\"Llama-2-7b\") agent = BlackjackAgent(model, tokenizer, device) Finally, write your RL loop as usual and simply call your agent to act, reward, and terminate: env = gym.make(\"Blackjack-v1\") for episode in trange(5000): observation, info = env.reset() done = False while not done: action = agent.act(observation) # act based on observation observation, reward, terminated, truncated, info = env.step(action) agent.assign_reward(reward) # provide reward to agent done = terminated or truncated train_stats = agent.terminate_episode() # trains if batch is full Some reminders: above code snippets are mildly simplified above but a fully working example is available in examples/blackjack.py getting online RL to converge is notoriously difficult so you'll have to mess with hyperparameters to see improvement your model may also benefit from a supervised fine-tuning stage on sampled trajectories before running RL (we may add this feature in the future) our implementation values simplicity so is not as compute efficient as e.g. Lamorel, but easier to start playing around with LlamaGym is a weekend project and still a WIP, but we love contributions! Relevant Work Grounding Large Language Models with Online Reinforcement Learning Lamorel: Language Models for Reinforcement Learning True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning Citation bibtex @misc{pandey2024llamagym, title = {LlamaGym: Fine-tune LLM agents with Online Reinforcement Learning}, author = {Rohan Pandey}, year = {2024}, howpublished = {GitHub}, url = {https://github.com/KhoomeiK/LlamaGym} }",
    "commentLink": "https://news.ycombinator.com/item?id=39658610",
    "commentBody": "LlamaGym – fine-tune LLM agents with online reinforcement learning (github.com/khoomeik)202 points by KhoomeiK 21 hours agohidepastfavorite21 comments katzenversteher 37 minutes agoFrom the title I misunderstood what it does. However, now I'm wondering if what I thought is was (don't ask my why I thought it) is possible: I have a PC that is able to run e.g. Mistral Instruct 7B Q4 inference with around 30 token/s. How (computation and memory) expensive would it be to also run backpropagation in addition to inference? I'm aware that the models are typically fed with much more and better data than what is typically provided during normal conversations but on the other hand if I could finetune my local model a teeny tiny bit during during / after each conversation I have with it anyways, it would after a while be perfectly customize for me. I'm also aware that this could be problematic for models that are used by multiple users but my intended use case would be personal use by a single user. reply kayson 8 hours agoprevI want to make a Discord bot that impersonates all my friends and continues to refine the model as the conversations continue. Basically this [1] post, but with a more modern model and, ideally, reinforcement learning. Seems like this would fit the bill.... Is there anything else that would make this easier? [1] https://www.izzy.co/blogs/robo-boys.html reply mzl 2 hours agoparentYou could perhaps adapt the Doppel Bot slack bot from Modal Labs: https://github.com/modal-labs/doppel-bot reply potatoman22 2 hours agoprevCould someone help me understand the kinds of things you can build with this? Is this like RLHF? reply internet101010 16 hours agoprevThank you for making this. Simplifying any aspect of RL is always welcome. reply KhoomeiK 16 hours agoparentThanks! Yeah RL for LLMs is pretty underexplored I think beyond the RLHF stuff. Pretty tough to get working tho. reply dennisy 14 hours agoprevCan this be used outside of OpenAI environments? If yes I think an example would be great! reply KhoomeiK 14 hours agoparentGymnasium is now maintained by the Farama Fpundation, an open-source consortium, not OpenAI. But most RL environment work for the past 5+ years has been Gym-compliant. The TextWord example in the repo, for example, instantiates a Gym-style environment but it doesn’t import from Gymnasium (uses textworld.gym instead). reply adawg4 11 hours agoprevThanks for making this! Helps simplify it nicely reply KhoomeiK 16 hours agoprevTwitter thread: https://x.com/khoomeik/status/1766805213644800011?s=46 reply 3abiton 18 hours agoprevInteresting project, basically a wrapper too around openai gym-like functionality that can handle open llms. reply KhoomeiK 16 hours agoparentYup, it does simplify LLM agent inference on Gym environments but the main technical contribution is reducing your would-be code overhead for online RL reply neodypsis 14 hours agoprevVery interesting! reply raidicy 18 hours agoprevThanks for creating this! reply zeroq 10 hours agoprev [–] When 150 lines of boilerplate can land you the first page on HN, maybe it is, in fact, the end of programming? reply KhoomeiK 10 hours agoparentKarpathy’s micrograd [1] is literally 154 lines. Guess programming ended 4 years ago. [1] https://github.com/karpathy/micrograd reply DSingularity 6 hours agorootparentYou think autograd is boilerplate? reply yen223 10 hours agoparentprevLet's not be one of those people who measure developer productivity by number of lines reply yowlingcat 8 hours agoparentprevCarmack's infamous fast inverse square root was only 13 lines. Measuring code by line metrics rather than its contents reflects shallow, questionable comprehension. reply klysm 9 hours agoparentprev [–] I’m not really sure what your point is. Is it not remarkable that valuable things can be done in 150 lines? reply xpe 7 hours agorootparent [–] I agree with you. / Above, I wouldn't assume a single nor clearly intended \"point\". Reading it I got an impression more of concern, even fear. I'm guessing one underlying driver may be a concern that AI is creeping into more and more programming. Which is true. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LlamaGym streamlines fine-tuning Large Language Model (LLM) agents through online reinforcement learning, offering an abstraction for managing RL tasks in Gym environments.",
      "Users can efficiently adjust agent prompting and hyperparameters, facilitating quick iterations and experiments.",
      "To use LlamaGym, users must implement abstract methods, specify the base LLM, and develop the RL loop for agent training; the tool is a ongoing project welcoming contributions."
    ],
    "commentSummary": [
      "LlamaGym is a tool designed for refining LLM agents through online reinforcement learning.",
      "Users engage in conversations about possible uses and advantages of the tool, along with exchanging resources and projects in the machine learning domain."
    ],
    "points": 202,
    "commentCount": 21,
    "retryCount": 0,
    "time": 1710074443
  },
  {
    "id": 39659781,
    "title": "Introducing Yi: Powerful Language and Multimodal Models",
    "originLink": "https://arxiv.org/abs/2403.04652",
    "originBody": "Computer Science > Computation and Language arXiv:2403.04652 (cs) [Submitted on 7 Mar 2024] Title:Yi: Open Foundation Models by 01.AI Authors:01.AI: Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai Download PDF HTML (experimental) Abstract:We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models. Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI) Cite as: arXiv:2403.04652 [cs.CL](or arXiv:2403.04652v1 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2403.04652 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Wenhao Huang [view email] [v1] Thu, 7 Mar 2024 16:52:49 UTC (9,681 KB) Full-text links: Access Paper: Download PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CLnewrecent2403 Change to browse by: cs cs.AI References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39659781",
    "commentBody": "Yi: Open Foundation Models by 01.AI (arxiv.org)189 points by pama 18 hours agohidepastfavorite77 comments helsinkiandrew 18 hours agoThe Github repository, gives a better introduction/howto: https://github.com/01-ai/yi > Yi-34B-Chat model landed in second place (following GPT-4 Turbo), outperforming other LLMs (such as GPT-4, Mixtral, Claude) on the AlpacaEval Leaderboard (based on data available up to January 2024). > Yi-34B model ranked first among all existing open-source models (such as Falcon-180B, Llama-70B, Claude) in both English and Chinese on various benchmarks, including Hugging Face Open LLM Leaderboard (pre-trained) and C-Eval (based on data available up to November 2023). reply oersted 17 hours agoparentLooking at the leaderboard shows a clearer picture: https://tatsu-lab.github.io/alpaca_eval/ - GPT-4-Turbo: 50.00% - Snorkel (current 2nd, Mistral 7B fine-tune): 34.86% - Yi 34B Chat (current 6th): 29.66% - GPT-4: 23.58% Thoughts: - Just saying that it came 2nd is quite misleading, the difference in score is significant. - Not sure what's up with this benchmark, I've never seen GPT-4-Turbo vs GPT-4 performing so differently. - The Snorkel model is impressive with just 7B parameters. The Yi authors claim that their success is based on good training data cleaning. This seems to be key at least for this benchmark. Snorkel has also always been all about that, using programmatic methods to generate lots of quality training data. reply lossolo 13 hours agorootparentThe only reliable benchmark is found at https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboar... Model creators train models including open-source benchmarks in the data, either intentionally to achieve better scores or inadvertently through leaks from various sources. reply doctorpangloss 16 hours agorootparentprev> Not sure what's up with this benchmark, I've never seen GPT-4-Turbo vs GPT-4 performing so differently. The benchmark is bad. reply WhitneyLand 18 hours agoparentprevIt’s been ~1 year since gpt-4 was released. It’s hard to guess how long before any flavor of an “open” model will consensus match what was released in 2023 let alone potentially exceed it. A big part of the race seems like it will depend on how high gpt-5 can raise the bar. If it’s only incrementally things may converge quickly. reply c0n5pir4cy 17 hours agorootparentThe Yi models were actually released back in early November 2023. So there isn't as big a gap in time as it seems. I'm not sure why there is such a big gap between the release of the models and the publication of the paper. EDIT: Okay, this appears to be a new set of models with the same name, based on the same models from November but now with multimodal capabilities. reply nickpsecurity 15 hours agoparentprevAnytime you see that, we should assume the newer models might have been trained on either the benchmarks themselves or something similar to them. If I was an evaluator, I’d keep a secret pile of tests that I know aren’t in any LLM’s, do the evaluations privately, and not publish scores either. Just rank plus how far apart they are. The best tests of these models are people who want to use AI to solve real problems attempting to do that with various models. If they work, report that they worked. Also, publish the work and result pairs permissively when possible to evaluate that and use it for fine-tuning, too. reply orra 18 hours agoprevThe repo source code is Apache 2.0 licensed, but the weights are not. Just in case anybody else is excited then misled by their tagline \"Building the Next Generation of Open-Source and Bilingual LLMs\". reply mmastrac 17 hours agoparentThe model license, excerpts: https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMEN... 1) Your use of the Yi Series Models must comply with the Laws and Regulations as well as applicable legal requirements of other countries/regions, and respect social ethics and moral standards, including but not limited to, not using the Yi Series Models for purposes prohibited by Laws and Regulations as well as applicable legal requirements of other countries/regions, such as harming national security, promoting terrorism, extremism, inciting ethnic or racial hatred, discrimination, violence, or pornography, and spreading false harmful information. 2) You shall not, for military or unlawful purposes or in ways not allowed by Laws and Regulations as well as applicable legal requirements of other countries/regions, a) use, copy or Distribute the Yi Series Models, or b) create complete or partial Derivatives of the Yi Series Models. “Laws and Regulations” refers to the laws and administrative regulations of the mainland of the People's Republic of China (for the purposes of this Agreement only, excluding Hong Kong, Macau, and Taiwan). reply heroprotagonist 19 minutes agorootparentNot really very open. Though it makes me wonder what the model might say about Tiananmen Square, Uyghurs, reeducation camps, or any other thing you're not really supposed to talk about in China. Is there a benchmark for bias in model outputs? No doubt China has one, somewhere, except it's not skewed towards prevention. reply est31 17 hours agoparentprevMore reading on the weight license: https://news.ycombinator.com/item?id=38159862 reply echelon 12 hours agoparentprevWeights are trained on copyrighted data. I think that ethically, weights should be public domain unless all of the data [1] is owned or licensed by the training entity. I'm hopeful that this is where copyright law lands. It seems like this might be the disposition of the regulators, but we'll have to wait and see. In the meantime, maybe you should build your product in this way anyway and fight for the law when you succeed. I don't think a Chinese tech company is going to find success in battling a US startup in court. (I would also treat domestic companies with model licenses the same way, though the outcome could be more of a toss up.) \"Break the rules.\" \"Fake it until you make it.\" Both idioms seem highly applicable here. [1] I think this should be a viral condition. Finetuning on a foundational model that incorporates vast copyrighted data should mean downstream training also becomes public domain. reply mg 17 hours agoprevHmm.. it fails for my favorite test prompt: https://www.gnod.com/search/ai#q=Two%20cars%20have%20a%20100... I gave it 3 tries and each time, Yi picked one of the cars as the winner. I've been watching for many months now, how LLMs got better and better at solving it. Many still struggle with it, but the top ones nowadays mostly get it right. reply AndrewKemendo 16 hours agoparentI asked my 12 year old son to solve this prompt. His answer was \"Neither win\" and it took him 1 minute and 24 sec using no pre-defined algorithm or heuristic. He said his process of thoughts was: \"I figured it would take 10 hours for car A to finish 100 miles and it would take twice that long for car B. Since Car B is already halfway there when car A starts, then they would arrive together\" I as 40 year old man, approached it intentionally naively (eg. I did not go looking for an optimal solver first) by making a drawing and attempting to derive the algorithm. It took me ~3 minutes to come to the same conclusion but at the end I had a series of equations, but no algebraic proofs.[1] So now you have a human child reference metric if you want it. [1]https://twitter.com/AndrewKemendo/status/1766872572300235022 reply mattstir 16 hours agoparentprevInterestingly, GPT-4 also fails to correctly solve this prompt, choosing car A each time after multiple tries for me. I tend to find that models struggle with such logic puzzles when using less common phrasing (e.g., two cars \"having\" a race instead of participating in one, \"headstart\" instead of \"head-start\", etc). GPT-4 correctly solved the problem when it was reworded to: \"There is a 100 mile race with two participants: car A and car B. Car A travels at 10 miles per hour but does not begin driving immediately. Car B travels at 5 miles per hour and is given a 10 hour head-start. After 10 hours, car A begins to move as well. Who wins the race?\" reply pama 4 hours agorootparentYou can tell ChatGPT that it’s brilliant at reasoning, ask it to rephrase the problem in its own words and then solve it avoiding any traps. I have special instruction sets for inducing these chain of thought behaviors. There is more output in the end, and it helps the model think better before coming to a conclusion. reply appplication 17 hours agoparentprevOn one hand, I don’t really understand why anyone would expect an LLM to solve logic puzzles. The only way it can do so is not through reasoning, but by having been trained on a structurally similar puzzle. On the other hand, it does feel fun that the top ones appear to solve it, and I understand why it feels cool to have a computer that appears to be capable of solving these puzzles. But really, I think this is just specificity in training. There is no theoretical or empirical basis for LLMs having any reasoning capability. The only reason it can solve it is because side the creators of these top models specifically trained the models on problems like this to give the appearance of intelligence. reply mg 17 hours agorootparentThere might be no reasoning in a single pass which outputs a single token. But in the loop where the output of the LLM repeatedly gets fed back into its input, reasoning is clearly happening: The LLMs lay out how to go about figuring out the answer, do a series of calculation steps and then come up with an answer. If you add \"Please answer in just one short sentence.\" to the prompt, even the top ones get it wrong. reply spyder 13 hours agorootparentYep, humans too have to think before answering most non-trivial questions, and especially the ones that include calculations. So it seems \"obvious\" that we should try to to give LLMs too some time to think before answering, for example with the popular methods of asking for step-by-step thinking, thinking out loud, and only giving the final answer at the end, and also asking it to proofread and correct it's answers at the end all can help with that. Pause tokens (thinking tokens) are also an interesting method to achieve that and seems to have a positive effect on performance: https://arxiv.org/abs/2310.02226 reply visarga 16 hours agorootparentprevReasoning is also an iterative process. Besides scaling in response length, the model can also get multiple feedbacks from outside to correct itself. reply visarga 16 hours agorootparentprev> There is no theoretical or empirical basis for LLMs having any reasoning capability. Yes there is. Learning to predict the next token implies a lot of things, among which is also logical reasoning. The chain-of-thought approach shows that when you stimulate this behavior, you get higher accuracies. reply xcv123 14 hours agorootparentprev> There is no theoretical or empirical basis for LLMs having any reasoning capability. Deep learning models are specifically designed for automatic pattern recognition. That includes patterns of reasoning and problem solving. > The only reason it can solve it is because side the creators of these top models specifically trained the models on problems like this to give the appearance of intelligence. That's not how deep learning works, and not how machine learning works in general. The models can automatically recognize patterns of reasoning then apply those methods to problems it has never seen before. > The only way it can do so is not through reasoning, but by having been trained on a structurally similar puzzle. This is a fundamental misunderstanding of how it works. The large deep learning models have 100+ layers, modelling extremely abstract features of the data, which include abstract patterns of problem solving and reasoning. They are not simply regurgitating training examples. reply stevenhuang 11 hours agorootparentprevYour assertion that LLMs cannot reason is some exquisite irony considering the extensive theoretical foundation in support of the idea. reply xcv123 12 hours agorootparentprev> There is no theoretical or empirical basis for LLMs having any reasoning capability. Geoffrey Hinton - Mapping Part-Whole Hierarchies into Connectionist Networks (1990) https://www.cs.toronto.edu/~hinton/absps/AIJmapping.pdf \"The paper, titled \"Mapping Part-Whole Hierarchies into Connectionist Networks\" (1990), demonstrated how neural networks can learn to represent conceptual hierarchies and reason about relations like family trees. Specifically, Hinton showed that by training a neural network on examples of family relationships (parent-child, grandparent-grandchild, etc.), the network was able to accurately model the inherent logical patterns and reason about new family tree instances it had not encountered during training. This pioneering work highlighted that instead of just memorizing specific training examples, neural networks can extract the underlying logical rules and reasoning patterns governing the data. The learned representations captured abstract concepts like \"parent\" that enabled generalizing to reason about entirely new family tree configurations.\" reply theptip 17 hours agoprev“01.ai” is not a very auspicious name; 01 was the first AI nation, that eventually waged war with humanity and then enslaved them in The Matrix. reply riku_iki 17 hours agoparent> that eventually waged war with humanity I think humanity waged war on 01 reply ben_w 13 hours agorootparentOne thing I assumed when watching the Animatrix, but never had confirmed, was that the name \"01\" was chosen because it sounds a bit like \"Zion\". reply acjohnson55 14 hours agoparentprevI, for one, welcome our digital overlords. reply jacobn 18 hours agoprev“we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts” Data work is rarely sexy, but (almost) always useful. Did they release the corpus? reply gwern 16 hours agoparentThey did not, in part because it would reveal the data-filtering routines (particularly the political censorship - Chinese LLM papers sometimes mention the ban list but never reveal it), and also in part because it might reveal things they'd rather keep secret. For example, Bytedance has already been caught using the OA API to generate data for their models because they are having such a hard time catching up to OA - and evading bans for doing that, and also instructing employees on how to lie & cover it up: https://www.theverge.com/2023/12/15/24003151/bytedance-china... Do you think that a small Chinese startup like 01.AI, which by their own admission had to \"bet the farm\" to buy enough GPUs to train the Yi models at all https://www.bloomberg.com/news/articles/2023-11-05/kai-fu-le... , and which were completely silent about cloning the American LLaMA architecture until people analyzed the released checkpoints and noticed it looked awfully familiar, is going to be above such tactics...? In this economic/geopolitical context? Especially when everyone seems to be doing it, not just Bytedance?* (01.AI claims that, the architecture aside, they didn't simply further train LLaMA models but trained from scratch. You can decide for yourself how much you are willing to believe this.) I wouldn't bet a lot of money on it, and that's why I don't expect to see any large comprehensive data releases from 01.AI for the Yi models. * This is one of my theories for why so many disparate models by so many different groups all seem to weirdly converge on the same failure modes like 'write a non-rhyming poem', and why GPT-3.5, and then GPT-4, seemed to be oddly difficult to surpass, as if there were some magnetic force which made reaching near 3.5/4 quality easy for 'independent' models, but then surpassing somehow difficult. Everyone is lying or mistaken about 3.5/4 data getting into their corpus, and the sugar-rush of imitation learning fools you into thinking you're making a lot of progress, even when your overall approach sucks. (As Andrej Karpathy notes, neural nets want to work, and so even if you have serious bugs in your code, they will still work pretty well - and simply permanently fall short of their true potential. Cautionary recent example: https://twitter.com/karpathy/status/1765473722985771335 ) reply visarga 16 hours agorootparent> 01.AI claims that, the architecture aside, they didn't simply further train LLaMA models but trained from scratch. You can decide for yourself how much you are willing to believe this. You can't hide this. The latent space remains mostly fixed after pre-training. It all depends on the seed for the initial random init. Further pre-training won't move it enough. Because of this property, you can even average two fine-tunings from the same parent model, but never on models trained from different seeds. reply gwern 11 hours agorootparentI don't know anyone has properly analyzed this, nor how robust such methods are if one is trying to cover it up. Also, I doubt anyone has analyzed the scenario where the warm-started model is then extensively trained for trillions of token (possibly with a cyclical LR) particularly in Chinese - the latent spaces are not perfectly aligned Chinese/English and I'd expect that to change it a lot. (The point of this would be that 'cheating' by warm-starting it should let you avoid a lot of training instabilities and issues early in training, and may get you better quality at the end.) reply sroussey 13 hours agorootparentprevthe averaging seems like good test for who the parent is. reply dannyw 18 hours agoprevCan play around with the model here: https://replicate.com/01-ai/yi-34b-chat Very slow, but this is unquantized and there's probably a lot of demand. reply zone411 17 hours agoprevYi 34B Chat has not done well on my new NYT Connections benchmark and it's only in the 22nd place on the LMSYS Elo-based leaderboard (151 Elo below GPT 4 Turbo). It's doing better in Chinese. When it comes to models with open-sourced weights, Qwen 72B is clearly stronger. reply nbbaier 4 hours agoparentI'd also love to see how you're doing this benchmarking! reply Yenrabbit 11 hours agoparentprevOoh I also use connections as a benchmark! It tends to favour things with 'chain of thought' style reasoning in the training mix somewhere since directly producing the answer is hard. Do you have public code you could share? reply bearjaws 18 hours agoprevSeeing models like this work so well gives me hope that mobile first LLMs for things like better voice to text and typing prediction will not just 'work' in 2-3 years but actually not kill your battery too. reply m3kw9 18 hours agoparentIt it kills battery it won’t be part of OS, and if it is on iOS it won’t be allowed to be in the AppStore, or it will be gated by API/hardware gating. For Andoird, they’ll just allow it and your batter will last 30min after a few questions reply coffeebeqn 18 hours agorootparentIf it’s fast enough to be useful it would also not physically be able to use that much power. Your phone CPU and GPU have a maximum W they can pull at any one time and if this runs for a few seconds then the maximum it can use is that. If it maxes out all cores and memory for 30 minutes then it won’t really work for anything reply evilduck 14 hours agorootparentprevMLC Chat is already on the App Store and allowed to be used. I haven't used Yi with it, but a quantized Mistral or Llama runs quite well on an iPhone 15. See https://llm.mlc.ai. \"Apple GPT\" is also rumored to be coming too. It is processor and therefore battery intensive but it already won't kill your battery inside of 30 minutes. Obviously it will be worse for resource usage than an app if it's always kept running by some OS level process and set as the processing layer for every trivial thing but it seems like cheaper input handling could decide to promote some input up to being evaluated by an LLM or not. reply barronli 14 hours agorootparentpreviOS already has app with LLM running locally on iPhone: https://apps.apple.com/gb/app/mlc-chat/id6448482937 I've also tried a few Android LLM apps, all running more than 30min. Current LLM models are not running constantly on the phones to drain your battery. They just run when responding to a prompt. It by no means consumes more battery than a heavy game. reply jes5199 18 hours agoprev01, like from the Animatrix ? reply d-z-m 17 hours agoprevthere's also a new Yi model, Yi-9B[0]. [0]: https://huggingface.co/01-ai/Yi-9B reply gpjanik 15 hours agoprevI understand that all these new models are an attempt to catch up with GPT-4, but frankly speaking, in the current shape and form, they're almost entirely useless. I frantically tried anything available on Groq to improve performance of my GPT-4 based chatbot - it's incomparably bad - and the more of them I see, the more I believe OpenAI has fundamentally no competition at all at the moment. No exception with the above, also pretty bad (IMHO worse than GPT-3.5). reply GaggiX 17 hours agoprevYi-34B is the LLM used by LLaVA-1.6 (also known as LLaVA-NeXT) and it's by far the best open source large multimodal models, demo: https://llava.hliu.cc/ reply gyre 17 hours agoprevPotentially interesting on the alignment front: In my experience the yi-6b model running on ollama is more likely to refuse politically sensitive queries (relating to Tiananmen Square, Peng Shuai’s disappearance, etc) when asked in Chinese, and more likely to provide information when asked in English. I wonder if this difference falls out naturally from available training data, is a deliberate internationalization choice, or is just noise from the queries I happened to run. reply mattstir 16 hours agoparentI noticed similar behaviour in an older model (Skywork 13B) a few months back. When asked in Chinese, it would politely say that nothing of note occurred when responding to queries about Tiananmen Square, etc. In English, it would usually respond truthfully. It was deliberate in the case of Skywork, based on their model card (https://huggingface.co/Skywork/Skywork-13B-base): > We have developed a data cleaning pipeline with great care to effectively clean and filter low-quality data and eliminate harmful information from text data. I'd imagine it's likely similar for Yi. reply BoorishBears 15 hours agorootparentHuge jump to go from that line in the model card to it being intentional from the model's creators. China censors those events. They pre-trained with a specific focus on Chinese text, and integrated more native Chinese text than most models do. Doesn't require any additional filtering on their behalf to have the model reflect that, and if anything the fact that they're mentioned in english implies the opposite of your hypothesis. If they were going to filter Tiananmen Square, the lift to filter it in English would not be any higher. reply advael 15 hours agoparentprevThis may be a useful workaround, but it also forms the strongset argument I've yet seen so far against claims that LLMs do something like \"understanding\" or \"an underlying world model\". Maybe models knowing the same facts in different languages, especially across political controversy, might form a good benchmark to evaluate reply arijun 16 hours agoparentprevI wonder if you could use the multilingual capabilities to workaround it's own censorship? I.e. what would happen if you asked it to translate the query to English, asked it in English, and then asked it to translate back to Chinese. reply Havoc 17 hours agoparentprevCould also be both. Training data organically creating the difference but with an additional layer of specific alignment on top too reply FinNerd 18 hours agoprevnext [12 more] [flagged] rafram 18 hours agoparentYi models are available for download [1], so no, this isn’t a wrapper around GPT-4. [1]: https://huggingface.co/01-ai/Yi-34B reply FinNerd 18 hours agorootparentIn theory couldn’t you download the application but then it just makes API calls to OpenAI? I am trying to use HuggingFace and suspect I will get similar answers to GPT-4. Can someone who knows how to download it ask for example which country Justin Trudeau is the president of? reply zeograd 18 hours agorootparentIt is not an application, it's a model. It claims to be llama-compatible, so you can reuse the same ecosystem, like ollama for inference. It will not run random code on your behalf and call OpenAI API like you fear. reply spencerchubb 18 hours agorootparentprevAn AI model takes a specific input and gives a specific output. In the case of a language model, it takes in text and outputs text. They can not just execute arbitary code When you download from huggingface, you are downloading the parameters of the model. The parameters are basically just a bunch of numbers, and they had to find the parameters by training the model reply maccam912 17 hours agorootparentprevSince you haven't had a response to the second part, here it is: PreviewJSON Justin Trudeau is not the president of any country. He is, however, the Prime Minister of Canada, which he has been since November 4, 2015. The prime minister is the head of government in Canada's federal parliamentary democracy and constitutional monarchy. reply rafram 18 hours agorootparentprevI’m sorry, but you really need to read up on the fundamentals of what a model is, how weights work, and so on before making comments like this. No, the model weights are not secretly making calls to the OpenAI API. reply d3m0t3p 18 hours agorootparentI really like the fact that he is saying nonsense and his about is :”C-suite exec at fintech” I don’t doubt that hahaha reply endofreach 18 hours agorootparentI don't think this is an appropriate comment on HN. He was asking a question. Not fully grasping a technical topic (that most people don't really understand), shouldn't be mocked. reply maccam912 17 hours agorootparentprevPreview Justin Trudeau is not the president of any country. He is, however, the Prime Minister of Canada, which he has been since November 4, 2015. The prime minister is the head of government in Canada's federal parliamentary democracy and constitutional monarchy. reply kleton 18 hours agoparentprevhttps://huggingface.co/01-ai/Yi-34B reply Takennickname 18 hours agoparentprevDid you read anything other than the title? (Even though the title itself gives it away) reply yumraj 14 hours agoprev [–] Given that this is a Chinese model, I’m genuinely curious if researchers have been evaluating risk that these models could be used for soft propaganda or similar purpose? As others have reported, English and Chinese queries return different replies on topics that are not kosher in China. What’s the risk that such models could be used for nefarious purposes by providing propaganda/biased/incorrect/… responses that on a cursory glance seem factual. reply TaylorAlexander 14 hours agoparentIt’s a fair question, but one we should be asking about all models, perhaps especially our own. It’s of course easier to see the propaganda of foreign cultures, and this should be investigated, but let’s not let ourselves believe that a model is more likely to contain propaganda because it is Chinese. It will just contain propaganda that is easier for us to see as propaganda. Noam Chomsky and Edward Herman wrote extensively about propaganda in democratic societies in their 1988 book Manufacturing Consent. A nice introductory excerpt is here, and the first two or three paragraphs are enough to begin to see the argument: https://chomsky.info/consent01/ Put as briefly as possible: propaganda in totalitarian societies is simpler. They just use force to remove people who say the wrong things, and state media to broadcast the “right things”. In democratic societies, institutional power still wants to protect itself, and this is achieved through more complex means, but it is nonetheless still rather effective. reply yumraj 14 hours agorootparent> It’s a fair question, but one we should be asking about all models, perhaps especially our own. It’s of course easier to see the propaganda of foreign cultures, and this should be investigated, but let’s not let ourselves believe that a model is more likely to contain propaganda because it is Chinese. It will just contain propaganda that is easier for us to see as propaganda. Yes, but in this particular case I'm coming from a viewpoint where I view China as a hostile power. So, at the moment, my worry is about that. In future, if US slips into authoritarianism, which TBH it might depending on the outcome of the next election, what you note would become a very real problem. So, putting it differently and more neutrally, is there any research being done on evaluating a political, and other, bias in a model or is it just being all put in the bucket of hallucination? reply TaylorAlexander 13 hours agorootparent> if US slips into authoritarianism The point of Chomsky’s work in this case is to show that authoritarianism does not make propaganda more or less likely, it just changes the means by which propaganda is created and reinforced. Chinese propaganda is easier to identify as a foreigner, but the propaganda of your home country has a much more significant effect on your life. The nature of living with pervasive propaganda is that it is hard to see or consider how your life would be different without the propaganda, and that’s what makes it so dangerous. > is there any research being done on evaluating a political, and other, bias in a model or is it just being all put in the bucket of hallucination? It’s a better question, and again one that we should ask regardless of the model’s origins. reply seanmcdirmid 13 hours agorootparentWouldn’t have more to do with propaganda that reinforces and caters to your cognitive biases being less detectable than propaganda that doesn’t? Even inside America, I’m pretty resistant to FoxNews propaganda but if CNN has any, it isn’t registering much on my propaganda detectors. reply TaylorAlexander 5 hours agorootparentYes, certainly. When I say that foreign propaganda is easier to detect, it is because of the biases you mention. And yes it makes sense that you don’t see the propaganda in the news you watch. My dad is a very smart man who has always liked Fox News and he genuinely can’t see the propaganda in it. There are also ways that both Fox and CNN share the same views, and this narrow window of thought on the specific subjects they share in common is a key aspect of Chomsky’s Propaganda Model. In areas that get the left and the right emotionally charged at each other, there can be a pretty wide range of opinions expressed. And then in areas that affect certain ways that state and corporate power influence our lives, there is often complete agreement and zero discussion of dissenting ideas. These ideas are represented as base assumptions about the fabric of society that are considered so obviously true as to melt in to the back drop of the discussion, and be invisible to regular viewers. Those are the ideas that are so vital for us to understand. Nothing will ever change as long as we keep arguing about trans people in bathrooms, gay marriage, and Taylor Swift’s political opinions. That lack of change is what the institutions in power want. What we need to understand to truly change things is ideas of radical democracy, worker power, equitable distribution of resources and universal rights for people and nature. Those subjects will never be seriously discussed on Fox News or CNN. reply seanmcdirmid 4 hours agorootparentFoxNews has gone straight tabloid, but CNN still looks somewhat like news. It’s when they get into the tabloidy stories that I begin to see something is off. But CNN is just trying to make money, and ironically, is copying FoxNews’s formula in preaching to the child and stoking biases to rake in that ad money (because at the end of the day, both really only care about making money, and changing minds rather than reinforcing them isn’t very profitable). reply ithkuil 14 hours agoparentprev [–] At the very least models will exhibit the bias present in the underlying training text and on top of that there will be a bias imposed by those wanting to correct the unwanted bias present in the underlying training text, possibly swinging the pendulum too far in the other side. I have the feeling you're asking something more specific, something more of a direct interference coming from politics and not just the natural \"point of view\" about various topics that are present in the chinese training corpora that is understandably different from western corpora. Do you have anything specific in mind about something that you expect the Chinese government to feed as propaganda that is not already widely being sculpted into the chinese text corpora available on the internet? reply yumraj 14 hours agorootparent [–] > I have the feeling you're asking something more specific... > Do you have anything specific in mind about something that you expect the Chinese government to feed as propaganda that is not already widely being sculpted into the chinese text corpora available on the internet? I don't have anything specific, and it doesn't have to be different from \"chinese text corpora available on the internet\", it's just that these models can become yet another channel of distribution for the \"chinese text corpora available on the internet\" especially if they are unknowingly/naively picked up and used as the foundation by others to build their offerings. reply maxglute 13 hours agorootparent [–] PRC will definitely weaponize this for mass foreign propaganda, which up until now PRC has been thoroughly deficient in, despite all the reees of 50cents on western net. The reality pre-LLM is PRC propaganda on western social media platforms has been very limited in scale for the simple reason that they are not wasting valuable English fluency to shit post on western platforms enmass. Low 100s-1000s of accounts, most of which target diasphora in spambot efforts, frequently in Chinese. Now that LLM has made it cheap to spam passable English/foreign languages, I'd expect increased volumes of PRC propaganda on western social media where anonymous posting is asymmetrically easier. But then again, they don't need a PRC LLM for that, plenthy of US bad posting on western platforms from international audiences and US herself. reply ithkuil 13 hours agorootparent [–] > they are not wasting valuable English fluency to shit post on western platforms enmass How's the economy of that different for the Russian campaigns? Do they have a larger pool of English fluency to draw from or is the urgency of the operation higher in their case? reply maxglute 12 hours agorootparent [–] A PRC person with English fluency good enough to blend in with native English on western platform has much better job opporunities. Even in PRC, 50c posts are largely civil servants told to write a few perfunctory platitutdes on domestic platforms. The MO is to overwhelm with spam not engage where effort:return is low. Like even Ministry of Foreign Affairs and most of thinktanks that also publish in English can rarely find people to write \"casual\" English. You'd have to write 1000s of \"50c\" comments for 1 hour of English tutoring gig. The economics of it doesn't make sense pre LLM. reply ithkuil 12 hours agorootparent [–] Does it mean that in Russia of 10 years ago a person with the same english language skills would not be able to find a better job or does it mean that the troll farms pay more? Or is it just patriotism? (I genuinely would like to learn more about this topic) reply maxglute 2 hours agorootparent [–] I don't know much about RU. I don't know about RU specifically, cusory search suggest RU has 7/144 (5%) english speakers, prc has 10/1400 (0.07%), so thats the supply demand happening behind the scenes. It's why there was so many expats with no mandarin skills randomly making living teaching English in PRC. If PRC citizen has fluent English skills and also speak mandarin, you can charge stupid amounts for private tutoring or work in private sector. I would say most of pro PRC content are either tankies or Chinese diasphora abroad, including non Chinese citizens who don't agree with MSM narrative. There's about 10M in Anglo/FVEY countries, that's large enough base to have people who are organically pro PRC, or rather not anti PRC. In terms of state directed actions, RU/USSR has longer history/experience with foreign subversive influenc operations. They're not taking out full page ads to post editorials on western news paper like PRC, which is just clunky. Bulk of PRC work is focused on now largely defunct United Front presence in west, or on PRC social media platforms in Chinese to target diasphora in Chinese etc. It's running joke in PRC that PRC foreign propaganda department is staffed by incompetent old guards who can't even out influence anti PRC EpocheTimes. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Yi model family is a set of language and multimodal models known for their robust multi-dimensional capabilities.",
      "These models leverage pretrained language models and showcase impressive performance across different benchmarks.",
      "The paper mentions the potential of expanding these models to incorporate vision-language aspects and longer context lengths, with a focus on enhancing model strength through increased parameter scaling in the future."
    ],
    "commentSummary": [
      "Yi models have achieved success in language model benchmarks, showcasing their capabilities in reasoning and solving logical puzzles.",
      "Ethical issues include data licensing concerns and Chinese regulations, along with doubts about the training methods employed by Chinese tech firms.",
      "The discussion also delves into comparisons between various LLMs, the potential for propaganda in models from different cultures, and the impact of Chinese government propaganda on social media, contrasting it with Russian/USSR propaganda effectiveness."
    ],
    "points": 189,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1710083550
  },
  {
    "id": 39661482,
    "title": "Timelock.dev: Encrypt and Send Secrets to the Future",
    "originLink": "https://timelock.dev/",
    "originBody": "This is simply a web interface built on top of the timelock encryption system posted by Cloudflare last week. https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;harnessing-office-chaos",
    "commentLink": "https://news.ycombinator.com/item?id=39661482",
    "commentBody": "Timelock.dev – Send a secret into the future using timelock encryption (timelock.dev)186 points by aarreedd 15 hours agohidepastfavorite116 comments This is simply a web interface built on top of the timelock encryption system posted by Cloudflare last week. https://blog.cloudflare.com/harnessing-office-chaos TacticalCoder 11 hours agoRivest, Shamir and Wagner, 1996: \"Time-lock puzzles and timed release Crypto\" https://people.csail.mit.edu/rivest/pubs/RSW96.pdf FWIW it took me about 3.3 years of computation (on one core, it's not parallelizable), from about 2015/2016ish to 2019, to find the solution to Rivest' LCS35 problem (which he created in 1999, so I found the solution 20 years after he created that LCS35 puzzle): https://en.wikipedia.org/wiki/LCS35 An article on WIRED for anyone interested (I'm still rocking the same monitor and same keyboard!): https://www.wired.com/story/a-programmer-solved-a-20-year-ol... reply lucb1e 7 hours agoparent> (on one core, it's not parallelizable) Why is that? From what I see on Wikipedia (\"n is a specific 616-digit (or 2048-bit) integer that is the product of two large primes (which are not given)\" sounds like a textbook RSA public key), it's an offline brute force challenge, a guessing game, so different computers (or cores) could independently take different slices of the guessing space. I will admit that prime factorization is one of my weak spots and I'll just resort to a few minutes of running pari-gp for that, since I know it uses some algorithm that is much better than brute force (it was orders of magnitude faster than alternatives for a CTF challenge involving a 512-bit RSA key). Even if the factorization process' time spent is dominated by finding the next prime to try, rather than testing a prime for correctness for example, why couldn't you start anywhere in the 2048-bit space and find the next primes from there? Is there something you can use from the ciphertext that makes you need to start at a certain point and generate (single core) from there onwards? It sounds like the holy grail for key strengthening without memory trade-off options like scrypt and argon2 both struggle with reply adastra22 7 hours agorootparentFrom the actual problem description: Note that the puzzle can be solved by performing t successive squarings modulo n, beginning with the value 2. That is, set W(0) = 2 W(i+1) = (W(i) ^ 2) (mod n) for i=1, 2, ... and compute W(t). There is no known way to perform this computation more quickly than to perform the t squarings sequentially, unless the factorization of n is known. reply lucb1e 7 hours agorootparentAh! Thanks. That immediately answers the second thing I was wondering about, since 2k RSA keys are still deemed safe (if barely / not for secrets that need to last a long time into the future) reply adastra22 7 hours agorootparentYeah the cost of breaking the time lock is presumably less than breaking the order of the group. reply ProllyInfamous 6 hours agoparentprevYou's'a handsome, dude. And clearly with brain. Thanks for sharing your neat human experience. reply BigParm 9 hours agoprevIt’s an interesting property of the universe that there’s no way to measure time directly. It clearly exists, yet all you can measure is change of things besides time. Time lock puzzles take variable time depending on hardware. You can’t really set an accurate specific release time. Any change in the universe that a computer can detect is just another spoofable input. Blockchain tries to solve essentially the same problem but the best it can reliably do is establish a chronology. That’s not a specific time. And it has fundamental vulnerabilities, however unlikely you think they are to be exploited in established systems. Intel is building proof of elapsed time into their chips but that’s not trustless. Timelock puzzles and blockchain are imo hacky solutions to one of the most important outstanding problems in cryptography: securely and trustlessly agreeing on elapsed time. reply bsdetector 20 minutes agoparent> there’s no way to measure time directly. It clearly exists, yet all you can measure is change of things besides time. If it can't be measured then it can't be said to clearly exist. Imagine a cellular automata where particles have lots of \"slots\" that could be used for moving or interacting. As the particle speeds up and more slots are used for moving, there are fewer slots for the kind of interaction change that we use to measure time. At the highest speed, with all possible slots used for motion, the particle would experience no change, which is indistinguishable from no time passing. Does that sound familiar to anything? It's certainly possible that light being a speed limit, time dilation, relativity, and so on are in some way actually describing change rather than time. reply koliber 1 hour agoparentprevOn human scale, is this a real problem? Consensus gives us the ability to measure elapsed time with a high probability. It seems kind of analogous how fundamental particles are probability fields. That means that in theory a baseball could teleport. But because a baseball is big spontaneous teleportation is not a real concern for anyone. reply throwaway69123 7 hours agoparentprevWhat about time crystals? https://en.wikipedia.org/wiki/Time_crystal reply crotchfire 35 minutes agorootparentWhat about time cubes? https://en.wikipedia.org/wiki/Time_Cube reply idiotsecant 5 hours agorootparentprevTime crystals aren't the only thing that has properties that change in an orderly, predictable fashion. Plain old uranium has the same property. The answer to both is the same - put them on a very fast rocket and do a lap around the galaxy and see how well they agree with you on how much time has elapsed. reply illiac786 5 hours agorootparentIs it not rather that time does not elapse everywhere at he same speed, simply put? Based on this, of course, measuring it becomes difficult. It's akin to measuring gravity. You can — for a specific point in space and time. (Time is even worse, since it is specific to a given _trajectory_ in space) But I have no idea what Im talking about. reply ben_w 1 hour agorootparentIt's identical to measuring gravity. In relativity, acceleration and gravity are indistinguishable, and it's acceleration which breaks the paradoxical symmetry that gives the name to the twins paradox. reply lumpa 9 hours agoprevThere's an interesting bit about being unable to recover the timelocked secret if the parties that keep the network going disband before the release date: *if the League of Entropy shuts down, members will delete their keys* The world is unpredictable (just like drand randomness... heh), and it's possible that the League of Entropy will all hang up their coats at some point in the future. Should that happen, members would have two choices with their private keys: release them to the world or delete them entirely. The former would mean that ciphertexts created for some time after the cessation of the network would be decryptable to everybody. The latter would mean that ciphertexts created for some time after the cessation of the network would be unencryptable forever (/until quantum computers can break them). In the interests of privacy, we felt the latter option was preferable. That said, if you encrypt the private key to your [insert cryptocurrency name] fortune to stop yourself from spending it now and the network stops... you're going to have a bad time. reply lucb1e 14 hours agoprevThe only way that I know to encrypt something into the future is generating an N-bit key and hoping someone will go through the trouble of cracking it when that becomes feasible. That involves lots of assumptions (e.g., how computing power develops and how much that person cares). The website's implementation is this: > A group of [orgs] holds the keys. There are 18 separate organizations running a total of 22 nodes, with a threshold of 12 needed to release a secret. reply montecarl 5 hours agoparentWhat you just wrote reminds me of the song Secrets From the Future by MC Frontalot. > You can’t hide secrets from the future with math > You can try, but I bet that in the future they laugh > At the half-assed schemes and algorithms amassed > To enforce cryptographs in the past reply buu700 13 hours agoparentprevThat's an interesting idea. Similarly, you could make a sort of quantum time capsule by encrypting a blob of data with RSA or ECC and publishing the cyphertext alongside the public key. You could even adjust the key size depending on how powerful you want the quantum computer that decrypts it to be. reply charcircuit 13 hours agoparentprevIt's also tough to find good algorithms where you can't just spend double to half the time until it unlocks. reply dave1010uk 11 hours agorootparentThere's not many. The Rivest, Shamir, and Wagner time lock algorithm is an example of one that can't be parallelized. In theory the NSA and Google can't brute force it much faster than you can on a laptop. reply dheera 13 hours agoparentprevThe problem with \"feasible\" is that the time precision is poor. Feasibility is modulated by the value of the secret. If the secret exposes $1 billion in value, people will happily throw $100 million worth of compute at it. I'm not an expert on DeFi but could we do something more time-precise using the Ethereum blockchain and a smart contract? reply Groxx 13 hours agorootparentYou might be able to implement this same kind of \"have N cooperating message-senders that agree to do X at time Y or M of them can prove violation and penalize [probably everyone]\", but you still need information that is not available until time Y. People / systems need to hold onto but not reveal that information until that time. This is basically weaponized (and possibly automated) enforcement of a rule. It's not crypto, it's just \"agree to this or you get the lead pipe\". Lead pipes are extremely useful and valuable and this is a completely reasonable tradeoff in a huge amount of situations, but it's not a true barrier. To get around the lead pipe requirement, you need some kind of data that exists but is technologically or physically inaccessible until time Y. Ethereum has no primitives like that because nobody has primitives like that. About the closest you get is to say \"crack this public key to get the reward\" and, yea, that's effectively time-lock encryption (it'll Y years with all the hardware in the world so it's \"locked\" for at least that long at 99% confidence or something) but nobody really considers it \"time locked\" unless you are intentionally designing a key to take Y time under Z hardware assumptions (which does exist, but a 1-PC-year key takes seconds with enough hardware). reply lucb1e 13 hours agorootparentI don't understand the lead pipe analogy. What's up with getting a metal pipe if you don't \"agree to keep this secret\"? reply Groxx 13 hours agorootparentIt's just one of many tools commonly used in the https://xkcd.com/538/ meme. reply lucb1e 13 hours agorootparentOh it's a way of saying \"killed\", or at least that something inconvenient will happen for/to you. Got it reply bigiain 10 hours agorootparentprevSee also \"Rubber-hose cryptanalysis\" https://en.wikipedia.org/wiki/Rubber-hose_cryptanalysis reply lucb1e 13 hours agorootparentprev> could we do something more time-precise using the Ethereum blockchain and a smart contract? I think only if you want a transaction to move forward at a certain time, and are confident that a majority of network members will not conspire to alter your smart contract. Hiding information in a smart contract: I don't know of a way that could be done, but I'm not up-to-date on this stuff either (I left that scene after Bitcoin outgrew the tech demo phase and became popular as a \"so long as a majority keeps buying and holding, everyone's coins are worth insane amounts!\" scheme) reply FriedPickles 12 hours agorootparentprevYou could create a DeFi system not too unlike the one linked in this post: a number of oracles release keys at designated times (use M of N encryption). The oracles could be financially incentivized to behave properly. E.g. they post a bond, which is confiscated if they don't post a private key on time or if a whistleblower discovers and reports a key early. In return for correct behavior they can earn some fees paid by users of the service. The financial incentive still flips if the secret's value is sufficiently large, but it would require coordination of many unprincipled oracles. reply dheera 12 hours agorootparentThe problem with whistleblowers is that now the entire system's security is transferred to them. Whistleblowers can also be bribed to blow whistles early or not blow whistles. Here's an another idea. Bitcoin has a halving, right? Somehow the entire system has agreed to halve at a certain time, and not halve early or halve late? How does this work? Can we utilize this time agreement somehow? Can it be incorporated into an algorithmic whistleblower, whereby (a) N people each know 1/N of the key (b) if anyone demonstrates that they know the answer to a question before halving, whistles are blown by a contract and all N people are punished (c) after halving, all N people receive a reward? reply Groxx 12 hours agorootparentAnyone can halve at any time, so anyone can \"cheat\" that by halving immediately. The reason everyone does it at the same time is because doing it [literally right now] will lead to a history that nobody else agrees is valid (halved at the wrong block number), so any coins you mine on your forked chain will be worthless. There's a monetary incentive to play by the rules, but absolutely no technical requirement. Bitcoin's primary achievement (IMO but it's fairly common) is that it managed to design a technological system that encourages playing by the rules. Cheating always pays worse than playing along, and even trolling only works if you have the majority of all computing power (very expensive), so there's no reason to cheat. But outside the core public key cryptography that handles addresses and proving transactions, and the \"proof of work\" that basically just limits the speed of everything so there's time for the world to agree on things, there's not really any fundamental crypto involved. Just self-reinforcing social incentives. (this kind of disagreement is why there's both Bitcoin and Bitcoin Cash. they share a common beginning but branched off some time ago and are now completely separate) reply FriedPickles 11 hours agorootparentprevThe whistleblowers are not designated parties, they are just any Ethereum user. They would have a financial incentive for whistleblowing, and they could never falsely whistleblow because the smart contract can check whether they actually have the private key early or not. reply pcthrowaway 12 hours agorootparentprevNot with ethereum or a smart contract, because any ethereum node can simulate any execution of a smart contract. But it occurred to me that you can sort of do something like this with a proof of work-like algorithm, though the time to solve would still be variable. Essentially you'd need a network of \"miners\" and instead of a block, you'd have a node encrypt a message with an encryption key of a set difficulty (decryption key length), based on the target decryption time and the network hash rate (hash rate probably is not the correct term, but I'll use it for conciseness). The miners would then work to decrypt the data using the knowledge of the key length. I'm not sure how you would incentivize the miners to work on decrypting it though, and the node which encrypted it would of course have knowledge of the message, so I don't see how this could be used in practice. In theory if your miners were running something like a tamper-proof secure enclave (I'm not even sure if these truly exist) perhaps there's a way to attest their own hashrate, and then an encrypted message can be proposed by a node to a subset of miners which collectively have the assumed hashrate. The secret-encrypted data can then be re-encrypted for each miner, with their public key. This ensures other miners not participating in the challenge can't attempt to decrypt the data. The problem here is incentivization over a long period of attempting to decrypt the data. You'd have to offer a reward large enough to incentivize the miners to cooperatively work to decrypt the message for the longest possible amount of time it could take to decrypt the message. edit: I think for this to work you'd first need to encrypt the data for each miner which should participate in the challenge, and then encrypt it with the secret key. That way a miner which solves this can publish the decryption key and the still-encrypted payload, which only they can decrypt, and all the other miners can apply the same solution to verify the published solution and solved message with their own private keys. edit 2: I suspect there's something potentially useful here, but I don't know enough about secure enclaves to really know if it's feasible to implement in a way that prevents gaming, so if someone knows more about such things, feel free to take the idea and run with it. reply falsandtru 13 hours agoprevReading the cited Cloudflare blog, it seems that the main purpose of this technology is public randomness, and timelock is one of its applications. Since timelock is not the essence of this technology, it is not surprising that the usefulness of timelock is unclear. > it’s become a reliable and production-ready core Internet service, relied upon by applications ranging from distributed file storage to online gaming to timestamped proofs to timelock encryption Details: https://drand.love/docs/timelock-encryption/ Thread on the cited Cloudflare blog: https://news.ycombinator.com/item?id=39641475 reply krackers 10 hours agoparentIf I'm understanding the Drand protocol correctly, then isn't the following quote from the Cloudflare blog misleading? > Each organization contributes its own unique source of randomness into the joint pool of entropy used to seed the drand network – with Cloudflare using randomness from LavaRand, of course! It leads you to think that the each round's random value comes from \"combining\" local sources of entropy that each node contributes, but skimming the actual Drand protocol used, isn't it closer to something like using AES-CTR as a PRNG, except instead of AES it's some particular threshold-signature scheme. From another cloudflare post >To instantiate the required threshold signature scheme, drand uses the (t,n)-BLS signature scheme of Boneh, Lynn and Shacham. In particular, we can instantiate this scheme in the elliptic curve setting using Barreto-Naehrig curves. Moreover, the BLS signature scheme outputs sufficiently large signatures that are randomly distributed, giving them enough entropy to be sources of randomness. Specifically the signatures are randomly distributed over 64 bytes. So \"real-world randomness\" only is mixed in during the very initial distributed key generation phase, and after that everything is purely deterministic right? Or put another way those fancy lava lamps are a non-sequitur since this scheme doesn't seem to rely on their values beyond the initial key generation? reply lukevalenta 9 hours agorootparentHi, I'm one of the authors of the Cloudflare blog and maintainer of Cloudflare's drand nodes. You're correct -- after the initial distributed key generation, the values produced by the drand network are deterministic (this is one of the properties that allows for timelock encryption). The security properties of drand rely on a threshold of nodes remaining uncompromised, but mixing in fresh randomness isn't necessary. (Although you could imagine having some drand chains that incorporate fresh randomness for properties like post-compromise security.) > isn't the following quote from the Cloudflare blog misleading? >> > Each organization contributes its own unique source of randomness into the joint pool of entropy used to seed the drand network If this is misleading it wasn't intentional! We used the word \"seed\" since the randomness from LavaRand is only mixed in when the network is initialized, but perhaps that could have been phrased better. Or perhaps we should have split it into separate blogs talking about LavaRand and drand since they're only tangentially related :). reply krackers 8 hours agorootparentThank you for the reply, that clarifies things! As a crypto-novice perhaps explicitly using the term \"initialize\" might be better at indicating this is a one-time thing rather than a \"continuous\" process of injecting entropy. reply londons_explore 9 hours agorootparentprevI don't believe any such time lock encryption can take on any entropy after the user messages are encrypted - since by definition, any entropy in that randomness is useless for decryption if it wasn't involved in the encryption. reply jslakro 1 hour agoprevThe only problem I have with this kind of tools/experiments is that you can begin to use it then in one or two years the domain expires and everything is over reply crotchfire 3 minutes agoparentI have that problem with the whole internet these days. ICANN is a cancer. reply neiman 12 hours agoprevAt first, I thought they used a cool new variation of something like VDF (Verifiable delay functions) or sequential proof of work to make a time lock. In these two options, you need to make a very long sequence of calculations to decrypt the message. Since the calculations cannot be parallized and since the sequence can be as long as you choose, it creates a time lock on the message. But no, they just use some kind of regular multi-sig/secret sharing. reply adastra22 2 hours agoparentVDF/sequential-PoW stops being interesting once you notice that efficient hardware implementations can beat CPU implementations by 6-10 orders of magnitude, depending on the work function. reply sterlind 11 hours agoparentprevhrm. I was going to say it's extremely wasteful, but since it's sequential I guess you're only burning one core. There's no computational arms race like with blockchain PoW. How does the math work? The decryption key is some kind of exponentiation you can compute directly with a trapdoor, but without the trapdoor you have to repeatedly multiply instead? reply coppsilgold 10 hours agorootparentYou can just use desktop/laptop CPU accelerated sha256, iterate the round function for as long as you want. No hardware exists that can beat it on latency by any significant margin. Start with some random 256-bit string as the seed. Iterate on it for t time using sha256 CPU instructions - by either repeatedly hashing the seed or increasing the number of rounds to an arbitrary value (and do something about the round constants, such as removing them). After t time you stop and use the result to encrypt a message. You then publish the encrypted message and seed + number of rounds you ended up using. It will take t time before anyone can decrypt it. They will have to redo what you did, having multiple machines will not help in this task. reply olejorgenb 8 hours agorootparentBut it also take t time to encrypt? reply coppsilgold 7 hours agorootparentTo obtain the encryption key, yes. The trapdoor method is successive squaring and relies on quite a few assumptions for its security. The hash method also has the advantage that the sender can utilize multiple machines/cores in creating the encrypted package. By executing the serial hash task in parallel with all available resources and using the results of each chain to encrypt each other in another chain.[1] [1]reply Swiffy0 12 hours agoprevOn another note, I think something like this should come with some kind of zero knowledge proof of the future encrypted thing being what it is claimed to be, so that one doesn't have to wait for a long time just for the secret to encrypt into nothing. reply aarreedd 12 hours agoparentThat's a good point. Not sure how to do that right now reply jasonmorton 12 hours agorootparentYou can ZK an XGBoost or neural net with https://github.com/zkonduit/ezkl reply bialpio 12 hours agoprevThis feels like Shamir's secret sharing with a pinky promise that \"we won't decrypt things earlier\", am I missing something? reply aarreedd 11 hours agoparentYes, essentially. However, the 'pinky-promisers' are large, well-funded, geographically distributed organizations that have strong incentives, both in terms of reputation and operations, to keep their promise reply kaangiray26 2 hours agoprevIf being crackable during the time period is a concern, why not just use OTP (one-time pad, not his evil twin) and create XORed multiple keys to be shared with peers, and then use all of the distributed keys to reveal the message after some time had passed? reply AgentME 1 hour agoparentSome people do timelock encryption by using weak cryptography that's expected to be broken in a planned amount of time, but this project isn't doing that. It uses modern cryptography to encrypt some data to a set of public keys so that 18 of 22 nodes have to cooperate to decrypt it. Anything encrypted with modern cryptography isn't expected to be crackable in under millions of years. Their design has the benefit over yours that people don't need to send data to peers in order to timelock some data. The timelock only needs to be sent to the peers for decryption. reply mothepro 12 hours agoprevThere are solutions which don’t require a third party. These would be guaranteed to survive in case a third party shuts down with a long duration. Paper: https://docs.google.com/document/d/e/2PACX-1vQe-OF0Lw9lutf6a... reply cowthulhu 5 hours agoparentThe solution in that paper does require third parties, except the parties are unknown, the amount of parties is unknown, and the system breaks down if insufficient people are interested in running the chain. I’m really not convinced that solution is better is any way. If your adversary has the resources to coerce the “leave of entropy” in the OP, cracking that blockchain implementation will be trivial. reply iskander 12 hours agoprevRelated: Verifiable Delay Function https://link.springer.com/article/10.1007/s00145-020-09364-x reply ttyprintk 12 hours agoparentChia ran a contest for those: https://news.ycombinator.com/item?id=18437659 reply aarreedd 11 hours agoprevFYI - this has used 33% of Cloudflare's daily free tier limit for workers since I posted this 3 hours ago. Every pageview and API call is an invocation. You get 100,000 calls per day for free. reply tutfbhuf 13 hours agoprevYou can replace one or a few trusted third parties with an entire network of node operators (similar to the blockchain or Tor network) and achieve practical timelock encryption[^1]. As long as there are enough uncompromised nodes, you will be able to obtain your secret in the future. [^1]: https://github.com/drand/tlock reply instantiator 10 hours agoprevRelated - how do you send a message into the future with conditional release? A dissertation on Dead ~~Man~~ Person Switches... https://instantiator.dev/post/dead-person-switch/ reply tl988008 13 hours agoprevActual timelock encryption, encrypted using parallel hashing and decrypted via forcing serial hashing: https://news.ycombinator.com/item?id=7848999 reply jerrygoyal 3 hours agoprevi wonder what could be the most interesting use case of this? reply Dibby053 13 hours agoprevWhat does this solve that couldn't be solved by simply disclosing the decryption key oneself at the specified date? reply aarreedd 13 hours agoparenthttps://drand.love/docs/timelock-encryption/#use-cases reply ghnws 12 hours agoparentprevDeath, for example reply Uptrenda 11 hours agoprevThe problem with time-lock encryption is you need to design a scheme that remains secure for the entire duration you target. As you know: technology changes rapidly so that if your time-lock is far ahead in the future its hard to predict what technological developments might exist to break it. - If you use repeated hashing (or other measures of lengthy computation to derive a key) there's no guarantee that future computers won't be able to run your algorithm much faster than you did. A problem that will probably show up quite early with schemes of this nature. - If you use the threshold approach listed here. Can you guarantee that the machines providing the service are still available when you need decryption? Moreso: that they don't end up being hacked between the encryption and decryption time-frames through some 0-day. - You could use a hardware device to protect the keys. But this would mean that the devices weren't compromised by hardware attacks. We have seen Bitcoin wallets fall to hardware attacks and trusted computing environments like enclaves have numerous attacks that can be used to compromise their contents. What makes time-lock encryption so challenging is you need a scheme that is intentionally weak so that's it's broken after a certain point. In cryptography that level of specificity isn't needed because schemes are designed to be well and truly secure past the life-times of all the subjects who use them. Even greater than the life of planets. reply klabb3 11 hours agoprevYou can also send secret messages to future crypto analysts: just encrypt your message with a random key and throw it away. Guaranteed to be delayed until the chosen primitives are broken. reply victorbjorklund 12 hours agoprevVery cool. Now I just need to find a reason to use it. reply littlecranky67 1 hour agoparentI used the very same mechanics (without the encryption, though) for lockmeout.online - for digital health/digital wellbeing to lock yourself out of your phone or addictive online accounts for a while. reply Jamustico 13 hours agoprevWhats the point of this if it's not decentralized/algorithmic based. Who knows if this service will go down or something? Might aswell just do this with one entity m reply adastra22 13 hours agoparentThere is no decentralized algorithm for timelock encryption. No such scheme exists. Distributed is the best you're going to get without a radical breakthrough, and that's exactly what TFA is. reply pjerem 13 hours agorootparentWhy ? If you wrap the message into multiple layers of encryption (TOR style) that needs to go into multiple nodes, and if alongside the next encrypted layer you have a date the nodes agrees to wait to pass the message to another node, that would work, no ? Even with some corrupted nodes, the message would still be secret, the only issue would be if the last nodes are corrupted : your message would be distributed too soon. But with enough layers and enough nodes to go through, you could mitigate this risk. The network could even detect corrupted nodes if other nodes received the message too soon. reply victorbjorklund 12 hours agorootparentWhat stops you from just spinning up X nodes in your own private network if everything is open source? And then tell every node to decrypt instantly. reply pjerem 3 hours agorootparentBecause each node generates its own key pair and when encrypting a message you choose a random route and you use the keys of the nodes of your route to encrypt each layer. reply adastra22 2 hours agorootparentWhat the person you're replying to is talking about is a Sybil attack. You pick random nodes, yes, but what if the list of nodes to pick randomly from is 99.5% the attacker? This is a real world attack that has been used against Tor, for example. reply pjerem 1 hour agorootparentThank you, I understand ! reply victorbjorklund 2 hours agorootparentprevSo you can only decrypt the message if all the same nodes are still up? If anyone goes down you cant decrypt? reply pjerem 1 hour agorootparentYes. However, you could mitigate this by calculating hundreds of routes. But yes, you are right, that was just an interesting thought experiment before going to bed, I wasn't trying to revolutionize timelock encryption ;) reply mothepro 11 hours agorootparentprevThis is not true at all. reply adastra22 7 hours agorootparentIf you have a protocol, there's plenty of cryptography conferences and prestigious journals that would accept your manuscript. reply aarreedd 11 hours agorootparentprevWould love to know how reply chews 13 hours agoparentprevHTLC would do the same in a distributed and trustless fashion and yet it's important to know League of Entropy is a bunch of distributed crypto organizations like Chainsafe or the Ethereum Foundation. reply adastra22 13 hours agorootparentI assume you mean a verifiable compute function rather than hash time-lock contract, as the latter can't be used for encryption. But that's not really a timelock either: it only sets a lower bound on the amount of compute required. But \"compute\" here is abstract, and when the conversion from \"required hashes\" to \"time elapsed\" can vary by 6-10 orders of magnitude, it stops having any appreciable meaning. reply wuiheerfoj 11 hours agorootparentprevEPFL, DEDIS, university of Chile, University College London and cloudflare aren’t crypto organisations reply makach 13 hours agoprev..but will it stand the test against time? for robustness, the message should include the decrypt algorithm what about integrity and time travelers? i.e., can decrypt but just messing around with time reply troymc 13 hours agoprevHere's a way to encrypt something with an actual timelock, which works because physics. More specifically, it works because there is a maximum speed that information can travel through space: the speed of light. Step 1: Generate a large number of named public/private keypairs and put the private keys on a spacecraft. Also give the spacecraft a communication system and a long-lived RTG (an energy source getting its energy from the decay of some radioactive materials). Step 2: Send the spacecraft to land on the surface a distant body in the solar system, such as one of the moons of Neptune. Step 3: To encrypt a message such that it's guaranteed to not be cracked in less than some specified time, encrypt it several times, using the known named public keys. To decrypt the message, you've got to send it out to the distant spacecraft and ask it to decrypt the outer layer of encryption, using the private key corresponding to the outer layer's public key. It does that and you get back a message but it might still have several layers of encryption. Repeat until all those layers are removed. There are tricks to speed things up by sending a spacecraft out towards Neptune, but they don't speed things up too much (because spacecraft travel much slower than light). The amount of speedup possible is left as an exercise for the reader. There's still a lower bound on the required time until full decryption. Inspired by the TOR network. reply Dunati 32 minutes agoparentFrist, should probably put the ship on a solar escape trajectory, so that physically intercepting it would be more difficult with time, and likely require a technological leap to accomplish. It would also be nice to have the ship on a trajectory as far away from the plane of the ecliptic as you can, to minimize occlusion and time variation based on the earth's orbit. Next, instead of pre-seeding the ship with keys, you have it generate them (using its RTG for both power and a source of entropy) and send them back to earth at a fixed rate, possibly with the size of the keys increasing over time. On earth, when you want to encrypt a payload to be decrypted at a future date, you calculate how many round trips it would require, then encrypt the data with that many keys from the stream, interleaved with keys generated locally. As the time will unlikely be an exact round trip time (which is ever-increasing), each end can delay the decrypt and re-transmission for some proportion of the remaining time. reply pjerem 13 hours agoparentprevThe spacecraft should generate the key pairs itself once it landed and send the public keys to earth to be sure no human stole the private keys. But, while fun, your idea is not that stupid. If the spacecraft continuously generate key pairs, you could even avoid landing it and \"just\" throw it in some direction, Voyager style (if you can afford rebuilding some every few decades) or on some orbit in the solar system. You don’t have to pay for the landing tech. I wouldn’t even be surprised that this could be viable economically. It doesn’t sound that much technologically difficult reply troymc 13 hours agorootparentThese are all good improvements. Thanks! My thinking now is that you could send the spacecraft out to Neptune and do a gravity-assist maneuver there, sending the spacecraft into a large orbit around the Sun, with a high perihelion. reply bigyikes 12 hours agorootparentprev>I wouldn’t even be surprised that this could be viable economically. Do you imagine there's a large market for physically-based time locked encryption? Hard to imagine there's a ton of paying customers lol reply pjerem 1 hour agorootparentI really don't know. I wouldn't be surprised that, on the entire planet, a niche market could exist that would want to pay the premium over say, giving an envelope to a notary. You could also change the business model to allow NOT decrypting messages by paying the message author (and take a commission) but I guess that would attract more than shady customers :) reply lucb1e 13 hours agoparentprev> Inspired by the TOR network. Because it has such high delays? Basically revealing the information which the onion service or exit node encrypted for you only after, potentially, a few trips around the globe? That makes me think this can be achieved without spacecraft, by just having geographically distributed private keys (even just a few kilometers; you just need the light delays to dominate over processing delays). And I don't think you need more than two keys: if you wrap it in A(B(A(B(message)))), then party B cannot work on the first layer but first needs to send it to A, party A cannot decrypt the second layer but first needs to sent it back to B, etc. One of the parties could be your recipient, so that would also work with an expensive one-off spacecraft. > land on the surface a distant body in the solar system landing is a lot more difficult than staying in Neptune's orbit (notice how many moon-bound spacecraft crashed, even only in recent years!); you'll get the same characteristics by just going out to a desired orbit. Also note that the amount of delay between Earth andwill vary wildly reply Valgrim 12 hours agorootparentOne could build a service just around the fact that the fastest theorical time it takes to transmit information around earth is around 130ms. This means that the absolute maximum number of layers that could be used in a day is well below 1 million. Scale that over a few billion layers or more, and reveal a new key every roundtrip, and you would've got yourself a much simpler timelock encryption. The problem that arise is (and which is solved by the spacecraft to Neptune) is that with any earth based system, someone could secretly move copies of both ends closer together, secretly, and decrypt the lock faster than expected. Putting a spacecraft on a trajectory with no realistic chance of ever coming back makes this possibility impossible (as long as the layers of keys are encrypted only when the spacecraft arrived at its destination. Even if the delay between earth and neptune vary wildly, it is predictable, and any local system could piggyback a larger scale system like this for safety reply troymc 13 hours agorootparentprevI wrote that the proposed system was inspired by the TOR network, not that it literally uses the TOR network. To send a message across the TOR network, you wrap it in a bunch of layers of encryption, and then each TOR node removes one layer: that's the similar aspect. Your other critiques are all valid. There's still a lower bound on the total time required for full decryption. I was just trying to show that that is possible. reply lucb1e 13 hours agorootparentNot meant as critiques! And I did not assume it was using the Tor network, I understood the word \"inspired\" as you meant it. Clearly I need to work on my communication :( reply voxic11 12 hours agoparentprevWouldn't landing a spacecraft next to the first one allow you to decrypt messages in essentially one round trip time? So if you had a message on earth timed locked for 100 years you could transmit it your your own spacecraft next to the decrypting spacecraft and it could almost immediately get through all layers of encryption and send you back the decrypted message. reply dave1010uk 12 hours agoparentprevThis is a really cool idea using fundamental properties of physics but I don't think it works. If the spacecraft is trusted by the person with the secret, it's much simpler to instruct the spacecraft to only disclose the secret after a set date. You don't seem to gain anything from the carded complexity. If the spacecraft isn't trusted, there's nothing it stopping it disclosing all the key pairs at once. I think the best bet at the moment is something like Rivest-Shamir-Wagner time lock puzzles, which requires a fixed number of sequential computations to perform. reply ur-whale 13 hours agoparentprev> Send the spacecraft to land on the surface a distant body in the solar system, such as one of the moons of Neptune. The economics of your proposal strike me as a tad weak at the knee. reply troymc 13 hours agorootparentThe up-front capital expenditure might be high, yes, but it might be possible to recoup that by charging fees for the service once it's working. The Chunnel had a high capex too. In any case, I just wanted to show that working timelock encryption system is theoretically possible. Some people claimed it wasn't. reply dheera 13 hours agoparentprev> Generate a large number of named public/private keypairs and put the private keys on a spacecraft I suppose, more practically, you could just put the private key in an envelope and bury it deep in a shipping container with a destination across the ocean. While in transit it's pretty damn hard to get to it. reply ttyprintk 13 hours agorootparentThere are a few geocaching approaches: Sunken chest / mysterious treasure map Beacon of high gamma radiation / undesirable to approach for many half lives USB key in Jimmy Hoffas pocket reply lucb1e 13 hours agorootparentMy inner devil likes the \"too dangerous to approach\" idea :) About the last idea, I had to look up that name: > James Riddle Hoffa (born February 14, 1913; disappeared July 30, 1975; presumed dead July 30, 1982) was an American labor union leader who served as the president of the International Brotherhood of Teamsters (IBT) from 1957 until 1971. Do I read it correctly if I understand \"Jimmy Hoffas pocket\" to be one implementation example of \"any disappeared person's pocket\"? Or is the specific person, their role, or their era relevant? reply ttyprintk 13 hours agorootparentYou’re correct: the fame of discovering Jimmy Hoffas burial is high in pop culture. Maybe as high as Hacker News finding a usb key timelock where only a vague area is given. reply ttyprintk 13 hours agorootparentprevI didn’t know his middle name was “Riddle” that’s almost ironic. reply dustyleary 12 hours agorootparentprevYes, Jimmy Hoffa is a \"famous\" disappeared person case here in America. There are only a few \"famous mysteries\" that became such widespread memes in American culture. The ones I can think of are: 1. What happened to Jimmy Hoffa (who killed him?). \"The Irishman\" on Netflix is a Scorsese adaptation of a \"nonfiction\" book that documents an old Mafia hitman claiming to have killed Hoffa. (The book is nonfiction, the guy's claims are somewhat contested.) 2. What happened to Amelia Earhart? (Early female aviator who disappeared attempting to fly around the world). 3. What happened to and who was DB Cooper? (A man hijacked an airplane, traded some hostages for a duffle bag of cash at an airport when such a thing was possible, told the pilots to fly to Canada and then jumped out of the plane with a parachute and the duffle bag somewhere over the pacific northwest). 4. Who shot JFK? reply ttyprintk 12 hours agorootparentThank you, I regret picking such a reference. Amazing to entertain that the guy who killed Hoffa could have lived long enough to write about it. As if reality suddenly became gentler, and all differences could be hugged out. reply KineticLensman 6 hours agorootparentprevAlso, in the UK (or not), Lord Lucan reply ttyprintk 13 hours agorootparentprevDrone delivers a capsule into an historic minefield / make a Great Power clear an area to find it Sow the hex digits in one crop on top of another / wait until summer to see the visible difference reply ttyprintk 12 hours agorootparentprevSlip a key into a random unsolved case file in LA / must digitize all files to find it Choose a key from the DNA of a living Nobel prize winner Place an opaque nondescript sticker over the lens of a surveillance camera in London / must disassemble all cameras to find it Writing Sherlock Holmes level material is not hard reply yogorenapan 14 hours agoprev [9 more] [flagged] adastra22 13 hours agoparentNo it requires a threshold of 12 out of 18 keys to be acquired by an attacker. Jurisdictional resilience is the standard technique for preventing this kind of attack--e.g. having operators in the USA, Russia, Japan, Israel, Europe, South America, India, Singapore, etc. reply dumbfounder 13 hours agoparentprevSmart contracts operate completely transparently. There is no private data other than the private keys used to sign transactions. Unless you just encrypt the data, then you need a key to access it, and the nodes don’t have that key. I know a guy that worked on this same problem leveraging TEEs (trusted execution environments). This enables 3rd parties to run the software and join the network and take part without having access to the data. Unless they find a flaw in the TEE that can be exploited. reply DistractionRect 13 hours agoparentprevI'm only vaguely familiar with the concept of smart contracts, but from what I understand I can't see how it'd fair any better at enabling cryptographic timelocks - the core problem is designing encryption which can't be broken before a set period has passed. The naive approach is requiring to make a scheme which assumes it'll take X amount of time to brute force with Y resources. That's obviously flawed because you can't constrain an enemies resources, and if the period is long there's plenty of room for improvement of algorithms/hardware which will shorten the time to brute force the scheme. This only works because we create a scheme which can't reasonably be brute forced by classical means, and a human organization enforces/controls the time period. In order to create something like that proposed in the posted link with smart contracts, the self contained smart contracts would have to employ a cryptographic timelock themselves. Otherwise, the smart contracts will be dependent on external (human) input, so it's just reinventing the same thing. reply pshirshov 14 hours agoparentprevErgh, could you explain how did you come to this conclusion? reply thewakalix 14 hours agorootparenthttps://drand.love/docs/timelock-encryption/#%E2%9A%A0%EF%B8... reply pshirshov 9 hours agorootparentThis is not the same at all. reply littlestymaar 13 hours agoparentprev> wonder if a smart contract could be useful in this scenario It could not, at least with the capabilities of EVM or other mainstream smart-contract platform (I don't even thing it would be possible in theory but I may be wrong on this) reply ur-whale 13 hours agoparentprev [–] Do the trustees have to be known or can they remain fully anonymous ? In that case, the whole warrant theory isn't the last word. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The web interface utilizes the timelock encryption system created by Cloudflare, as outlined in a recent blog post."
    ],
    "commentSummary": [
      "Timelock.dev is a web interface utilizing Cloudflare's timelock encryption for securely sending secrets into the future.",
      "The discussions delve into accurate time measurement, data encryption, blockchain usage, and data security, highlighting challenges, vulnerabilities, creative key storage methods, and encryption using spacecraft trajectories.",
      "Emphasizes the value of decentralized networks and layered encryption for robust long-term data security, noting limitations in current smart-contract platforms for cryptographic timelocks."
    ],
    "points": 186,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1710096379
  },
  {
    "id": 39662234,
    "title": "Optimizing Database Choice Beyond Performance Metrics",
    "originLink": "https://motherduck.com/blog/perf-is-not-enough/",
    "originBody": "GO BACK TO BLOG PERF IS NOT ENOUGH 2024/01/18 BY JORDAN TIGANI SUBSCRIBE TO MOTHERDUCK BLOG Also subscribe to other MotherDuck updates SUBMIT ON THE CULT OF PERFORMANCE IN DATABASES It takes about 4.5 hours for me to go door to door from my house in Seattle to our office in San Francisco. Let’s say you built a hypersonic plane with a top speed that was 10 times faster than the usual Boeing 737-MAX (with or without the extra windy window seat). After you factor in an Uber to the airport, waiting in security lines, boarding, taxiing on the tarmac, takeoff and landing, waiting for a gate, waiting for baggage, and my Uber to the office, you’d have accomplished some amazing feats of engineering but probably only shaved off 20% of the overall travel time. That’s good, but I’m still not going to make a 10 am meeting. The database industry has been focused on the equivalent of making faster planes. Meanwhile, security lines get longer and luggage gets lost. An ideal query optimizer won’t help you if your data is in a slightly wonky CSV file or if the question you want to ask is difficult to formulate in SQL. Performance is the most common metric that database nerds like me use to measure our importance, and like sports fans, we tend to pick teams that we root for against everyone else. If your favorite database wins the benchmark wars, you have bragging rights at the watercooler. You can brandish your stats, backed up by blog posts, to prove to anyone who will listen that your favorite DB is the champ. Performance in general, and general-purpose benchmarking in particular, is a poor way to choose a database. You’re better off making decisions based on ease of use, ecosystem, velocity of updates, or how well it integrates with your workflow. At best, performance is a point-in-time view of the time it will take to complete certain tasks; at worst, however, it leads you to optimize for the wrong things. ENDED, THE BENCHMARK WARS HAVE In 2019 GigaOm released a benchmark comparing cloud data warehouses. They ran both TPC-H and TPC-DS across the three major cloud vendors plus Snowflake. The results? Azure Data Warehouse was the fastest by far, followed by Redshift. Snowflake and BigQuery were far behind. At the time, I was working on BigQuery, and a lot of folks freaked out …. How could we be that much slower than Azure? However, the results didn’t match the impression we had from users. Every time a customer did a head-to-head evaluation of us vs Azure, they ended up choosing BigQuery. The market outcomes at that time were almost the reverse of the benchmarks: Snowflake and BigQuery ended up selling a lot better than Redshift, which sold much better than Azure. If the benchmark didn’t match the customer experience, then either the benchmark was done wrong, the benchmark was testing the wrong thing, or performance turned out to not be that important after all. We did a lot of poking around, and it wasn’t the first one; the GigaOm folks are pretty good at running benchmarks and the methodology was sound. The benchmarks they ran, TPC-H and TPC-DS, are the industry standards and had a broad range of queries. They were the benchmarks we ourselves ran internally in order to judge performance, and while one can quibble with the data size or their relevance to real-world workloads, they were the best available. So if the benchmark was a good representation of performance, and customers, by a large margin, ended up buying the systems that did poorly on the benchmark, then it leads you to believe that perhaps there are more important things than performance. WHAT DOES IT MEAN TO BE FAST? In the 15 years that I’ve spent working on cloud databases, I’ve noticed an anti-pattern across the industry: People who build databases tend to be laser focused on the time between when someone clicks the “run” button and the time that results are ready. It is easy to see why database people would focus on just the database server time; after all that is the thing that they have the most control over. But what is actually impactful to users is the time it takes to complete a task, which is not the same thing. In BigQuery, we outsourced building the JDBC drivers to a company that specializes in building database connectors. If you’re not familiar with JDBC, these provide a universal interface that programmers and Business Intelligence tools use to connect to a database. It made sense at the time to have a well-known expert build the interfaces. A few years later, after numerous customer complaints, we realized that bugs in our JDBC driver were killing performance. From our perspective, the queries ran quickly, in just one or two seconds. But the way the driver was polling for query completion and pulling down the results made the queries seem like they were taking seconds or even minutes longer. This impact was exacerbated when there were a lot of query results, since the driver would often pull down all of the results one page at a time even if the user didn’t need to see all of the results. Sometimes they’d even crash because they ran out of memory. We had been spending many engineer years making the queries fast, shaving off fractions of a second here and there from query times. But the connectors that most of our users were using added far more latency than we had saved. What’s more, we were completely blind to that fact. No one at Google actually used the JDBC drivers, and while we ran full suites of benchmarks every night, those benchmarks didn’t actually reflect the end-to-end performance our users were seeing. Like the drunk looking for his keys under a streetlight, we looked only at the performance we could measure on our servers. The query time that users were seeing was invisible to us, and we considered it someone else’s problem. To actually fix the problem, and not just put band-aids on it, required us to reframe how we thought about performance. PERFORMANCE IS SUBJECTIVE Performance must be measured from the user’s perspective, not the database’s. It is a UX problem and, like any UX problem, can’t really be described in a single number. This is surprising to many people, since they think performance, like car racing, is an objective thing. Just because you can say that a Lamborghini is faster than a Prius, they believe you should also be able to say that My database is faster than Your database. But just like a Lamborghini might not get me to work any faster than a Prius (or a bicycle, if there is traffic), the actual workload for a database is going to determine which one is faster. Subjectivity gets a bad rap; people associate it with saying, “Well, there is no way of telling which one is better, so it doesn’t matter which one we choose.” But just because the difference between a Ford F150 pickup truck and a Tesla Roadster is subjective, it doesn’t mean that my experience with both would be equivalent. Databases are the same way; if we say the performance differences between Clickhouse and Redshift are subjective, it doesn’t mean they are equivalent. It just means that which one is faster depends on how they are being used. A couple of years ago, Clickhouse released Clickbench, a benchmark that showed that Clickhouse was faster than a couple dozen databases they tested against. This was surprising to me, since at the time I was working at SingleStore, and we believed that we were broadly faster than Clickhouse. After digging into the benchmark, we saw that the benchmark didn’t do any JOINs, so operated out of a single table, and also relied heavily on counting distinct items. While you might think that it is cheesy to publish a benchmark that just does single-table scans, Clickbench actually does a pretty good job of representing a number of real workloads. If you do a lot of log analysis and need to compute distinct users to your website, this could be a good proxy for performance. That said, if you’re running a more traditional data warehousing workload using a star schema, Clickbench is going to be misleading. Vendor benchmarks tend to focus on things that the vendor does well. The below is a diagram from “Fair Benchmarking Considered Difficult” describing the typical vendor benchmark result. There are tons of pitfalls in database benchmarking, and experience has shown that benchmarks typically do a poor job of capturing broad user-perceived performance. For example, BigQuery shows up very poorly in benchmarks, but the actual experience of many people is that the performance is magical. BigQuery shows up well in person because it doesn’t have any knobs and is largely self-tuning. A highly-tuned SingleStore instance will crush BigQuery at most tasks, but do you have time to spend tuning your schemas? And what happens when you add a new workload? The DuckDB website used to have a disclaimer that said, “Please don’t complain about performance, we’re trying to focus on correctness before we try to make it fast.” Not all databases apply the same approach. You can make a car faster by removing safety gear like airbags, traction control, crumple zones, emissions controls, etc. But most people don’t want to drive a car like that. Databases are no different; you can make them faster if you remove overflow checks, don’t flush writes, give approximate results to certain operations, or don’t provide ACID guarantees. Some of the systems that do well on these benchmarks apply these kinds of short-cuts, but I wouldn’t want to use them except in controlled circumstances. RATES OF CHANGE Last year when I set out to create a company on top of DuckDB, a number of people pointed out to me that if you Googled DuckDB performance, a benchmark would come up where DuckDB got pretty badly beaten. Wasn’t I worried? Why not choose a “faster” one? I wasn't concerned for two reasons. First, I think performance is of secondary importance. But second, DuckDB had demonstrated something that made current benchmarks moot; they improve incredibly quickly. Partly because of some architectural decisions, partly because the code base is relatively new and clean, and partly because the engineers involved are super talented, DuckDB gets better at an extraordinary rate. And it turned out I was right to not be concerned. The most recent published results of that same benchmark against the latest DuckDB release show they went from the middle of the pack to leading by a healthy margin. The broader point is that when you choose a database, the database is not frozen at that point in time. You’ll likely end up sticking with your decision for several years. The performance and features of your database are going to change a lot between now and next year, and even more so between now and five years from now. A very important variable, then, is not just what the database can do now, but what it will be able to do a year in the future. If a bug in a database causes you to choose a competitor, that’s going to seem like a silly reason in just a few weeks if that bug has been fixed. This holds true with performance; if two different databases are improving at different rates, you’re most likely better off choosing the faster moving one. Your future self will thank you. NO MAGIC BEANS If you take a bunch of databases, all actively maintained, and iterate them out a few years, performance is going to converge. If Clickhouse is applying a technique that gives it an advantage for scan speed today, Snowflake will likely have that within a year or two. If Snowflake adds incrementally materialized views, BigQuery will soon follow. It is unlikely that important performance differences will persist over time. As clever as the engineers working for any of these companies are, none of them possess any magic incantations or things that cannot be replicated elsewhere. Each database uses a different bag of tricks in order to get good performance. One might compile queries to machine code, another might cache data on local SSDs, and a third might use specialized network hardware to do shuffles. Given time, all of these techniques can be implemented by anyone. If they work well, they likely will show up everywhere. George Fraser, the CEO of Fivetran did an interesting post comparing performance of the main data warehouse vendors over time; while there was a pretty big dispersion in 2020, by 2022 they are much more closely clustered together. In 2020, the fastest time was 8 seconds and the slowest was 18, in 2022 three of the vendors were around 7 seconds and the slowest was 9. The caveat to this rule, of course, is that architectural differences are hard to overcome. Shared nothing databases are at a disadvantage vs shared disk, and it took Redshift many years to switch to a primarily shared disk architecture. Lakehouses that rely on persisting metadata to an object store will have a hard time with rapid updates; this is built into the model. But these types of differences tend to show up in margins; there is, for example, no fundamental reason why Redshift would be faster or slower than Snowflake in the long run. THE PROBLEMS ARE BETWEEN CHAIR AND KEYBOARD & BETWEEN KEYBOARD AND DATABASE To a user, the important measure of performance is the time between when they have a question and when they have an answer; that can be very different from the time it takes the database to run a query. If you step back and think about it from their point of view, there are a lot more levers you can use to achieve the goal of minimizing the time between question formulation and answer. You can make it easier to pose the question. You can make it easier to turn query results into something they can understand. You can help them get feedback when they’re not asking the right question. You can help them understand when the data has problems. You can help them get the data they need in the right place and the right shape to be able to ask the question in the first place. While these aren’t typically thought of as performance issues, improvements can speed up the workflows of analysts and data engineers to a larger degree than a better query plan. Snowflake did a great job of making it easier to write queries. Whereas many SQL dialects are opinionated about being consistent about syntax and that there should be “one way” to do everything, Snowflake designers had the goal of making SQL that users type “just work.” For example, in Snowflake SQL, if you want to compute the difference between two dates, you can use either DATEDIFF or TIMEDIFF; both work with any reasonable type. You can specify a granularity, or not. You can use quotes around the granularity, or not. So if you just type a query, as long as the intention can be gleaned, it should “just work.” This is one of the reasons that analysts like Snowflake, since they don’t have to spend their time looking things up in the documentation. DuckDB has innovated along these lines, as well, with their “Friendlier SQL” effort, which adds a number of innovations to the SQL language to make it easier to write your queries. One example is “GROUP BY ALL.” When you write an aggregation query, it is easy to forget to list one of the fields in the GROUP BY clause. This is especially the case when you evolve queries, because you have to make changes in multiple different places. The GROUP BY ALL syntax makes it easier to both write and maintain your queries because you only need to change the query in one place (i.e. SELECT list) rather than the aggregation. This was so useful that soon after they released the feature, several other database vendors raced to add similar functionality. Data isn’t always in a convenient format for querying. A huge amount of the world’s data is in CSV files, many of which are poorly constructed. Despite this, most Database vendors don’t take them seriously. In BigQuery, I wrote our first CSV splitter, and when it turned out to be a trickier problem than expected, we put a new grad engineer on the problem. It was never great, couldn’t do inference, and got confused if different files had slightly different schemas. It turns out the CSV parsing is actually hard. If two engineers using two different databases need to read CSV data and compute a result, the one who is able to ingest their CSV file correctly the most easily is likely going to get the answer first, regardless of how fast their database is at executing queries. CSV file inference can therefore be thought of as a performance feature. The way databases handle results has massive impacts on user experience. For example, a lot of times people run a “SELECT *” query to try to understand what’s in the table. Depending on how the database system is architected, this query can be instantaneous (returning a first page and a cursor, like MySQL), can take hours for large tables (if it has to make a copy of the table server-side, like BigQuery), or can run out of memory (if it tries to pull down all of the data into the client). Do clients have a long-running connection to the server, which can have trouble with network hiccups? Or do they poll, which can mean the query can complete in between polling cycles and make the query appear slower? ON SOUR GRAPES I’m a co-founder of a company building on DuckDB. This post might sound like something someone would write if they were working on a database that wasn’t fast, didn’t do well in benchmarks, or wasn’t focusing on performance. So I should mention that DuckDB is fast. I won’t spend a lot of time defending DuckDB performance, but DuckDB is currently top of ClickBench in a handful of machine sizes (e.g. c6a.4xlarge) and most of the h20.ai benchmarks. And they’re not too shabby on TPC-H and TPC-DS either. As has been mentioned before, your experience may differ! So before you go assuming any database is fast, try it out on your workload. But the point is, don’t count out the ducks! IN CONCLUSION… None of the most successful database companies got that way by being faster than their competitors. Redshift was king for a while, and the thing that let Snowflake in the door was maintainability, not performance on benchmarks. Databases whose primary selling point was performance did not perform well in the market. Databases who made it easy to get jobs done fared a lot better. To summarize: There are no magic beans; barring architectural differences, performance will converge over time. Database engines evolve at very different speeds; the one who is moving most quickly will be the one that wins in the end. Beware the database vendor that cares most about performance; that will slow them down in the long run. There is no single metric of database performance; a “fast” database might be terrible on your workload. The important feature of a database is how quickly you can go from idea to answer, not query to result. Faster queries are obviously preferable to slower ones. But if you’re choosing a database, you’re better off making sure you’re making your decision based on factors other than raw speed. Happy Querying! CONTENT On the cult of performance in databases Ended, the benchmark wars have What does it mean to be fast? Performance is Subjective Rates of change No Magic Beans The problems are between chair and keyboard & between keyboard and database On Sour Grapes In conclusion… NEXT POSTS 2024/01/10 - David Neal ANALYZE JSON DATA USING SQL AND DUCKDB Learn to read, parse, and query JSON data from files and APIs using SQL and DuckDB! 2024/01/16 - Peter Boncz JUST RELEASED: HYBRID QUERY PROCESSING PAPER AT CIDR 2024 MotherDuck released its paper on Hybrid Query Processing at the Conference on Innovative Data (systems) Research [CIDR]. VIEW ALL",
    "commentLink": "https://news.ycombinator.com/item?id=39662234",
    "commentBody": "Perf Is Not Enough (motherduck.com)163 points by shubhamjain 13 hours agohidepastfavorite32 comments jrflowers 6 hours ago>It takes about 4.5 hours for me to go door to door from my house in Seattle to our office in San Francisco Not enough founders are moving at one hundred and seventy nine miles per hour anymore but I guess that’s what happens when the fed jacks up interest rates reply xt00 5 hours agoprevThis sounds like a company organizational problem — if the ultimate goal is to get people to use your cloud and provide value, then why would you have metrics that diverge from what the customer sees as the important things? There should be somebody at Google actively talking to customers about what problems they have and communicating that to the engineers so they know what to improve. The organization should be structured so the engineers get the metrics they need or make it part of their job description to make the metrics. reply nightpool 4 hours agoparenthttps://en.wikipedia.org/wiki/Seeing_Like_a_State reply chrisandchris 4 hours agorootparentFrom what I read here on HN, this book describes Google. reply Jensson 1 hour agorootparentEvery organization is inflexible, you need a swarm of organization to be flexible. That is the main difference between the state and the private sector, the private sector is a herd. reply blowski 34 minutes agoparentprevIf our solutions don't solve our customers' problems, then either our customers need different problems, or we need different customers. reply Nevermark 6 hours agoprevPerformance is “relative””, not “subjective”. Its meaning is related to the task at hand. Unless we are talking about user interfaces designed to give the user a feeling of greater speed. With fast moving status indicators that communicate great efforts and the user feels like the response was fast for all the work. But that is an interface thing, not a database thing. reply flik 7 hours agoprev>> \"The caveat to this rule, of course, is that architectural differences are hard to overcome. Shared nothing databases are at a disadvantage vs shared disk, and it took Redshift many years to switch to a primarily shared disk architecture. Lakehouses that rely on persisting metadata to an object store will have a hard time with rapid updates; this is built into the model.\" Looking for good literature on this topic reply __mharrison__ 3 hours agoprevGreat post. I think this is one of the reasons that pandas had shown in the past decade. Performance on a single machine was good enough and it could ingest 99% of CSVs known to humanity. reply chme 1 hour agoprevI am a bit disappointed. I though this was about the Linux perf tool and would make a point that better tools are required while providing some suggestions. All I got was a piece that more or less states that generic benchmarks are not as useful as one might expect and that other stuff is important too. Which TBH is not really that surprising... reply dan-robertson 1 hour agoparentPerf is very powerful but indeed can be hard to get good information out of. Try pprof and read a bunch of Brendan Gregg’s articles about perf (and bpftrace). reply jongjong 2 hours agoprevIt's unwise to obsess about raw performance because of various reasons. For example, there might be a library which performs 2x faster than its closest alternative... Yet it might not necessarily be the best option for your project. There are other factors to consider like compatibility. E.g. If the library breaks every time you upgrade your Node.js engine version, it may add some risk to your project and require additional maintenance. Also, maybe the library itself is 2x faster, but once you've added all your application logic on top of it, your product might only end up being 5% faster than if you had used the alternative because, as is often the case, most of the workload is in the business layer. Also, often, performance is at odds with scalability. You often need to make some performance sacrifices to achieve scalability. For example load balancing with consistent hashing adds overhead on a per-machine basis but you can't scale to multiple machines without them. reply causality0 6 hours agoprevWere I the author of this article, I could not have resisted titling it \"Perf is Not Enerf\" reply Qwertious 5 hours agoparentWith the right accent the title already rhymes. reply pstuart 10 hours agoprevI'm using duckdb for a side project and it keeps getting more powerful. Ironically it was envisioned to use SQLite for the project (we're not doing OLAP processing) but duckdb is faster and more feature complete. reply LunaSea 10 hours agoparentMy issue with DuckDB is: - unstable execution (random crashes) - out-of-memory errors where I would've hoped for DuckDB to gracefully take the slow route to completion if no more memory is available (tried all the different conf settings) reply jtigani 8 hours agorootparentThe article mentioned that DuckDB keeps improving very quickly. The next couple of months of DuckDB are all about stabilization, with no new features getting added. Once it is robust enough it will be declared \"1.0\". My guess is that will be in late April. You mentioned OOMs, this has been a focus for a while and ha gotten steadily better over the past few releases. 0.9 added spill to disk to prevent most OOMs. And 0.10, released a couple of weeks ago, fixes a bunch more memory usage problems. The storage format, which another commenter brought up, is now fully backwards compatible. I'd suggest giving it another try, especially once 1.0 comes out. reply swasheck 9 hours agorootparentprev- each version breaks previous format and renders it unusable reply 1egg0myegg0 9 hours agorootparentWe heard your feedback! Backward compatibility was just implemented! Version 0.9 is actually fully readable by 0.10. With version 1.0 coming in a few months, this will be readable for several years' worth of version updates. reply eyegor 3 hours agorootparent> readable for several years Why not just make shims to migrate dbs for future compatibility? So you could read db 1.0 in v2.0 but only insofar as to migrate it to v2. The implication that you don't want to promise backwards read compatibility feels antithetical to a db driver. For example, if I have an ancient mssql db that was started in 2001, I'm confident that I can grab the latest mssql driver and still use it. I don't have to track down mssql 2007 to migrate incrementally. Not sure about postgres or mysql but I assume it's the same there. Sqlite is definitely backwards read compatible. reply lmz 7 minutes agorootparentYou're confusing a network protocol client (the MSSQL \"driver\") with an on-disk format. You can't upgrade the MSSQL server from 2001 to current in-place: https://learn.microsoft.com/en-us/sql/database-engine/instal... reply plugin-baby 3 hours agorootparentprevPostgres: > Major versions usually change the internal format of system tables and data files. These changes are often complex, so we do not maintain backward compatibility of all stored data. https://www.postgresql.org/support/versioning/ reply Tarq0n 1 hour agorootparentprevThat seems entirely fair for pre-1.0 software. reply vietvu 3 hours agorootparentprevI haven't used duckdb since I got OOM on my dataset too. I think I will try again on 1.0. reply HackerThemAll 10 hours agoprev [–] Yeah, lots of bs in this text. The comparison of Synapse Analytics to BigQuery is totally flawed. Not only the cost presented in the chart is taken out of someone's back, the scale of those solutions do not match. Try doing multiple petabyte scale analysis on Synapse and then compare cost and performance to BigQuery. Maybe Databricks could match BQ in this use case. reply dhoe 5 hours agoparentDo I understand you correctly that you're saying that the author of this post, one of the founding engineers of BigQuery, is being unfair to BigQuery? reply bushbaba 6 hours agoparentprev99% of the BQ use cases I’ve seen is reporting on a datasetPerformance must be measured from the user’s perspective, not the database’s. It is a UX problem and, like any UX problem, can’t really be described in a single number. This is surprising to many people, since they think performance, like car racing, is an objective thing. Just because you can say that a Lamborghini is faster than a Prius, they believe you should also be able to say that My database is faster than Your database. But just like a Lamborghini might not get me to work any faster than a Prius (or a bicycle, if there is traffic), the actual workload for a database is going to determine which one is faster. reply HackerThemAll 9 hours agorootparent [–] I also work in databases. The thing is the database engine isn't usually slow by itself. It's the programmers whose programs usually are just as lame as they are. For example during one of my \"help us with the slow database server\" consultancy assignments I discovered an incident of a query executed collectively over a 1000 times per second on average, adding considerable load to an already overloaded RDBMS, querying a single-row table where the company data (name and address) was stored, which has not changed for over 10 years. No caching in the app whatsoever. The clients _had to_ ask for this info afresh every so often. Eliminating this and only a few similar patterns reduced overall CPU consumption by some 50% and sped up the damn system a lot. I could write a book about this kind of bs I see every day. So, comparing BigQuery to some small data warehouse product, while BigQuery could not even get off the ground yet, that I call bs and put in the same drawer as the above. If the intention was to say \"product x\" vs \"product y\", this is what should have been done. But the author deliberately put the proper product names in a lame and dishonest perspective, which makes the whole thing not credible. reply eyelidlessness 5 hours agorootparentYou seem very hung up on a particular graph, and don’t seem to have much to say about the text which follows it and has a substantially different takeaway than the graph alone seems to have impressed on you. Maybe give the article another read with a little bit more of an open mind. reply ricardobeat 2 hours agorootparentprevIt seems clear that you haven’t actually read the rest of the piece (or are purposefully ignoring it). That graph is used as an a example of why benchmarks don’t matter. reply porker 2 hours agorootparentprev [–] Write that book. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article stresses the importance of considering factors beyond performance metrics when selecting a database, such as ease of use, ecosystem, updates, and workflow integration, for an improved customer experience.",
      "Emphasizes the impact of poor performance on user experience and advises looking at future potential and continuous performance enhancement of databases, rather than solely relying on current benchmarks.",
      "DuckDB's \"Friendlier SQL\" features are showcased as an illustration of prioritizing user experience, highlighting the subjective nature of performance evaluations."
    ],
    "commentSummary": [
      "Emphasizes the significance of performance in databases and cloud services, stating that benchmarks do not solely reflect speed and efficiency.",
      "Stresses the importance of engineers communicating with customers to grasp their requirements, addressing organizational challenges in companies.",
      "Discusses insights on different database technologies like DuckDB and BigQuery, examining the factors influencing their performance, scalability, compatibility, and user experience."
    ],
    "points": 163,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1710101870
  },
  {
    "id": 39662079,
    "title": "Pioneering PaperMC Server Elevates Minecraft Performance",
    "originLink": "https://github.com/PaperMC/Paper",
    "originBody": "Paper The most widely used, high-performance Minecraft server that aims to fix gameplay and mechanics inconsistencies. Support and Project Discussion: Our forums or Discord How To (Server Admins) Paperclip is a jar file that you can download and run just like a normal jar file. Download Paper from our downloads page. Run the Paperclip jar directly from your server. Just like old times Documentation on using Paper: docs.papermc.io For a sneak peek at upcoming features, see here How To (Plugin Developers) See our API patches here See upcoming, pending, and recently added API here Paper API javadocs here: papermc.io/javadocs Repository (for paper-api) Mavenpapermc https://repo.papermc.io/repository/maven-public/ io.papermc.paper paper-api 1.20.4-R0.1-SNAPSHOT providedGradle repositories { maven { url = uri(\"https://repo.papermc.io/repository/maven-public/\") } } dependencies { compileOnly(\"io.papermc.paper:paper-api:1.20.4-R0.1-SNAPSHOT\") } java { toolchain.languageVersion.set(JavaLanguageVersion.of(17)) } How To (Compiling Jar From Source) To compile Paper, you need JDK 17 and an internet connection. Clone this repo, run ./gradlew applyPatches, then ./gradlew createReobfBundlerJar from your terminal. You can find the compiled jar in the project root's build/libs directory. To get a full list of tasks, run ./gradlew tasks. How To (Pull Request) See Contributing Support Us First of all, thank you for considering helping out, we really appreciate that! PaperMC has various recurring expenses, mostly related to infrastructure. Paper uses Open Collective via the Open Source Collective fiscal host to manage expenses. Open Collective allows us to be extremely transparent, so you can always see how your donations are used. You can read more about financially supporting PaperMC on our website. You can find our collective here, or you can donate via GitHub Sponsors here, which will also go towards the collective. Special Thanks To: YourKit, makers of the outstanding java profiler, support open source projects of all kinds with their full featured Java and .NET application profilers. We thank them for granting Paper an OSS license so that we can make our software the best it can be. JetBrains, creators of the IntelliJ IDEA, supports Paper with one of their Open Source Licenses. IntelliJ IDEA is the recommended IDE for working with Paper, and most of the Paper team uses it. All our sponsors!",
    "commentLink": "https://news.ycombinator.com/item?id=39662079",
    "commentBody": "PaperMC/Paper: The most widely used, high performance Minecraft server (github.com/papermc)163 points by notamy 14 hours agohidepastfavorite48 comments tadhunt 11 hours agoI run a company that provides Minecraft server hosting for businesses (i.e. after school programs, summer camps, Code Ninjas, E-Sports leagues etc) and produces fun and educational events using Minecraft (i.e. 10-15 elementary school kids in an event space at a library learning physics by building roller coasters in the game). We've switched entirely over to Paper for everything because it works so much better than vanilla, and it also enables us to put it behind a Velocity proxy (Minecraft Java Application layer proxy also developed by the same group that develops Paper) for better scalability, more secure infrastructure, and some cool features like enabling any version of Java edition to join the same server (mad props to the ViaVersion & ViaBackwards plugin teams that make this possible!). This is impossible to do with Vanilla. We do all of our own content development creating the activities the kids do during the events, and the plugin ecosystem that someone else mentioned is hugely helpful for this. I especially want to call out how awesome the Geyser and Floodgate plugins are — they make it possible for Java and Bedrock clients to play together in the same world, which makes our customers lives so much easier. We're hiring part time / contract developers, event hosts, and technical support personnel. If this sounds interesting, please reach out. My contact info is in my profile. reply jbm 11 hours agoparentAnother point for Geyser and Floodgate. I have Minecraft on iOS, as well as two Java licenses. You can have both mixed on the same server if you are using Paper with Floodgate and Geyser. It's great; my younger child can use iOS, while my elder kids can use the java licenses on their computers. reply __jonas 10 hours agorootparentInteresting, does that mean the different editions have feature parity now? I remember it used to be that the Java Edition was ahead of Bedrock with some game features and the phone version had way less content – is that no longer the case or are these server plugins disabling some game features? reply atomicnumber3 10 hours agorootparentI think they have some drift in between versions (java lands first), but aside from that, they're extremely similar and only differ in what essentially amounts to quirks rather than actual gameplay differences. So no, you're not typically going to really notice the differences. reply tehbeard 9 hours agorootparentBedrock still lacks in capability for the offhand slot due to having to support tablet UI, only a select few items can be used in it. Bedrock also has a different combat model that lacks the indicators java has. A comprehensive list is here: https://minecraft.wiki/w/Minecraft_Wiki:Parity_issue_list reply siegecraft 11 hours agorootparentprevConstantly updating geyser to keep it up to date with the most recent bedrock versions is quite a pain. Maybe paper has tools to automate these updates (I use fabric) but it seems unlikely. reply Badbird5907 10 hours agorootparentGeyser/any third-party plugin isn't by the paper team, so they wouldn't have tools to auto update. (they also generally discourage auto-updating any software/plugins). There are plugins available to update geyser automatically though, here's one: https://www.spigotmc.org/resources/geyserupdater.88555/ reply eikenberry 13 hours agoprevWhat are their competitors? Do these unofficial servers work for both the Java edition and the MS C++ addition? reply kosma 12 hours agoparentThere are several primary ways to run a Java Edition server: 1. Pure vanilla server.jar. Has horrible performance, little flexibility, but in the end, it Just Works. This is the server that Realms (Mojang's paid hosting service) uses. In recent versions, some modding is possible via datapacks. 2. Fabric Loader and some vanilla-compatible mods like Carpet etc. Here, \"vanilla-compatible\" means that a vanilla client can still connect. This is the preferred way of playing for the TMC (Technical Minecraft) community since the underlying server stays mostly vanilla, down to the smallest details and bugs, and no behaviors are changed. The TMC community becomes very agitated when someone tries to \"fix\" some bugs - they rely on those bugs. 3. Forge/Fabric with mods that change the gameplay, and require the mods to be present on both client and server. This is the traditional \"modded\" experience, as seen on TV. 4. Bukkit/Spigot/Paper/Purpur, various generations of server software that heavily patches the vanilla server, fixing bugs, adding new functionalities, and presenting a very comprehensive Plugin API, while still allowing vanilla clients to connect. This is the software used by servers that implement custom gamemodes, public servers that need griefing/duping/exploit protection. Many high tech farms don't work on Paper because of how many things it \"fixes\". 5. Folia - a fork of Paper that has multithreading support. It comes with its own can of worms. Performance-wise, Fabric+Lithium is probably the most performant in the default configuration, though if you're willing to remove functionality, you can probably get better performance out of Paper (at the cost of losing gameplay features). As for Bugrock Edition: well... there is Geyser that lets you connect to a Java Edition server with a Bedrock client. ;) reply atomicnumber3 12 hours agorootparentAdding on to yours, since right now you have the most complete background answer. There's some history in the \"Bukkit\" family that might make their chaos make more sense. In the beginning, there was Bukkit, which was a plugin API. And there was its reference implementation, CraftBukkit. It was very popular, Bukkit was the de facto API for server-side modding, and I made 50 bucks off of advertising revenue share for downloads of my plugin, which was a lot for me as a high schooler with one (1) year of java experience. Times were good. Where was I? Oh yeah. So, for a long good while, times were swell. Then iirc Microsoft announced they were acquiring Mojang (I think this was the precipitating event? Someone correct me if not). And the main Craft/Bukkit guy threw in the towel and pulled a very clever poison pill: he DMCA'd his own repos. You see, Mojang essentially didn't care about server.jar distribution because they were a cool indie company. So CraftBukkit just included it. Which meant it was always in violation of Bukkit's GPL license since that code wasn't open source. So anyway, CraftBukkit explodes in a supernova and the resulting stellar nursery produces Spigot which pioneered the distribution strategy that all successors would then use - they distribute a decompilation map and their source, then you run a jar that downloads server.jar and performs the unholy mixing of licenses on your machine. This is where I left the scene so I only have a summary from here on out. Spigot is the main go-to for a while, but the stellar nursery is still hot, and so various creative differences produce Paper, which seems to have mostly caught on to replace spigot though it seems like spigot is still a nontrivial player as well. And these all still implement the same ole GPL licensed Bukkit API. reply treflop 4 hours agorootparentI want to make some major corrections. So Bukkit wasn’t the first. It was hMod in 2010. Everyone came over from hMod because hey0 disappeared for a hot minute in late 2010. Everyone from Bukkit came from hMod, although there were also holdbacks who didn’t like someone stealing the hMod community, but a gathering of major plugin developers and devs from hMod forced everyone’s hand (but also, hey0 disappeared for a relatively long time with no explanation). hMod also had a problem: it didn’t attempt to reverse any of the code obfuscation from Mojang, so working with it was extremely difficult. When hMod came about, it predated the MCP project as well by like 3 months, which started as a major community effort to deobfuscate the codebase. Bukkit came after MCP and could take from it inspiration. We are still in year 2010. Fast forward many years later… Microsoft had nothing to do with what happened actually. Bukkit was secretly owned by Curse, and much later Mojang secretly bought Bukkit. This secret sale pissed off a lot of the community and one of the core but non-OG contributors (Wolverness — he joined Bukkit much later) issued a DMCA against the codebase as a form of protest. This was obviously controversial but he also had a lot of support. Mojang actually never substantially interfered with the wholesale illegal distribution of its source code. Anyway, because Mojang was officially hiring the Bukkit team, but only limited to some members from the OG 2010 team, and with the other core contributors not being included at all, and with the DMCA taking down the repos, Bukkit became dirty and there wasn’t a future for Bukkit anymore. Spigot, which was then a performance fork of Bukkit, became the de facto successor (not to say that there weren’t many other options and competing choices in play too). Spigot didn’t create their distribution strategy though. MCP did things this way. reply paradox460 7 hours agorootparentprevBefore bukkit there was hey0's server mod, hMod, which bukkit was based on reply throwaway290 4 hours agorootparentprevWell told... reply DefineOutside 12 hours agoparentprevhttps://bstats.org/global/bukkit Minecraft server software marketshare is tracked with bstats. Paper (58.3%) is a fork of Spigot (21.3%) and is more open to contributions and has a lot more performance enhancements. Spigot continues CraftBukkit which implements the Bukkit API. Spigot only really exists to update to the latest game version and no longer receives performance enhancements. Purpur (10.1%) also exists and forks paper, and add gameplay features that couldn't be forked into Paper. The largest non-bukkit competitor is Fabric, which is nice because by default it changes no gameplay behavior. Spigot and all forks change gameplay behavior to improve performance and make implementing the API easier in subtle ways. Paper is more suitable for a public server as it patches exploits by default while Fabric is better for a more technical or private server as it doesn't break vanilla behavior . Bukkit-based servers can be made compatible with Bedrock edition with the Geyser plugin reply kosma 12 hours agorootparentNote that bstats only tracks Bukkit-compatible servers. It doesn't track Vanilla/Fabric/Forge/etc. installations, so take these numbers with a grain of salt. reply nightpool 12 hours agoparentprevThe major competitors include Fabric and Forge, with each being distinguished primarily by the type /amount of extensibility they offer. Paper is focused on \"backend only\" modding (plugins), as opposed to the \"full spectrum\" modding supported by Fabric and Forge that often (but not always) require users to download the mods on their computer as well. From that perspective, Paper is much more commonly used by commercial server owners (since they're catering to the very large audience of casual users who don't want to install mods) and casual users paying for a server host for their friends who just want a couple of small tweaks, while Fabric and Forge are generally used more by smaller groups of dedicated power users who want a lot of extra mods Paper and Forge both focus primarily on providing a large extension API through modifying the decompiled Minecraft source code, while Fabric takes a different route by providing primarily an extensive dynamic Java bytecode modification library that allows developers to effectively \"build their own API\" by modifying the vanilla Minecraft classes at runtime. This means Fabric is generally much faster and easier to update to newer versions, and most smaller mod developers have adopted it as their preferred platform of choice due to the more powerful tools, and use open source libraries to make up for the lack of a built in API. (These are generalizations, of course, and Forge and Fabric have experienced a fair amount of convergent evolution at this point as well.) On the performance side, there's a new experimental Paper-like server called Folia from some of the core developers aimed at providing radical parallelism. Normally, all Minecraft servers run their main gameloop on a single thread, and the largest of server operators deal with the issue by providing front end proxies that shard users to different servers, which limits interactivity. From a technical complexity level, you could almost compare it to a GIL-less server reimplementation. All, again, built as patches onto the decompiled Minecraft source code. It's gaining more stability and has been used in some very exciting public tests reply galleywest200 13 hours agoparentprevThey only work for Java Edition. As for competitors...well it is not really a competition -- it just depends on what you are focused on for your player-base. Paper is a performance-focused fork of Spigot which is an, to very loosely describe, \"interface\" for Bukkit. Bukkit itself is an API for Minecraft that interfaces with plugins/mods. If someone is running \"modded\" Java Minecraft, they are likely touching one of these, or even a fork of Paper. Paper adds some features/tweaks on-top of Spigot and if your server is having some performance issues or you want a specific feature that Paper offers then you would utilize Paper. If you are fine on Spigot, you use Spigot. Personally I have always used Paper when hosting servers, as performance (less \"rubber-banding\" with higher player-counts) was my focus in that time. As well Paper offered the configurable option of \"per-player\" mob spawning, so we did not have a single player that stood above a cave hogging all of the mob spawns. reply mastazi 12 hours agoparentprevI have done some research about this topic some time back, these are the links I had gathered back then. I was researching Java edition specifically, however among the ones linked below, at least one (Nukkit) has a Bedrock (C++) edition. Many of these are related, e.g. Paper is based on Spigot which in turn is based on Bukkit. Paper seems to be the most widely used currently. Paper https://papermc.io/ Spigot https://www.spigotmc.org/ Bukkit https://dev.bukkit.org/ Nukkit https://cloudburstmc.org/articles/ Folia (mod for Paper) https://github.com/PaperMC/Folia PaperBin (mod for Paper) https://github.com/x4e/PaperBin reply pityJuke 12 hours agorootparentFolia is probably the most interesting on this list - it's a mod by the makers of PaperMC to parallelize Java Edition while still maintaining a working survival experience. It's best known for powering 2b2t, a anarchy server with a current player count of 633 on a single survival world, with the world being roughly as old as Minecraft multiplayer itself (servers that are effectively multiple servers chained together have hit CCUs of 216k). reply charcircuit 12 hours agorootparentprevBukkit is the name of the API for the server software Craftbukkit. Bukkit itself is not server software. Plugins targeting the Bukkit API can run on Craftbukkit, Spigot, Paper, etc. Plugins targeting Spigot API only run on forks of Spigot (if they use the parts of Spigot API that are not in Bukkit), but not Craftbukkit. reply unleaded 12 hours agorootparentCraftBukkit isn't really server software anymore either, you can still download it for latest MC but it's really just Spigot with some features disabled. https://madelinemiller.dev/blog/stop-using-craftbukkit/ reply TkTech 4 hours agoparentprevA reasonably-complete grid is available at https://wiki.vg/Server_List, typically updated by the devs themselves as this the main minecraft dev wiki. reply BD103 8 hours agoparentprevThere's a lot of great servers mentioned here, but there are two that I haven't seen yet: - QuiltMC, which is a fork of FabricMC - NeoForge, which is a fork of Forge Both have their own benefits and downsides, and are mostly comparable with their originals. reply notamy 13 hours agoparentprevPaper is for Java edition only, as it’s a series of patches on top of the official Java edition server. reply caladin 13 hours agoprevAre there any recommended open source projects for handling the management of a Minecraft server? So maybe some kind of control panel, automated backups, etc.? I specifically mean some kind of all-in-one solution rather than a hodgepodge of tools and bash scripts. Something that gets one close to the Minecraft Java Realms experience + mods, which one can just provision on their EC2 instance or wherever they may run the Minecraft server. reply ro_bit 12 hours agoparentPterodactyl is a commonly used one as the other comments say, but just to give some background, Pterodactyl itself, rather than being a minecraft server hosting software, is actually a generic server hosting software that can be used pretty effectively with a lot of games or even webservers. The base configuration of a pterodactyl managed instance is an egg, which is basically a docker image wrapped in a bunch of configurations[1]. This means you can run any docker container in pterodactyl and manage it all through a fairly slick web panel[2] which allows you to easily tweak things like memory allocation, send commands directly to the console, and share access with other accounts. There's lots of eggs already configured for minecraft and other games like Rust (not the programming language), so a lot of the complexity of setting up a docker container is abstracted away and for a non-advanced user its a pretty nice balance between the ease of use for a realm and the extensibility of a full server. [1] https://pterodactyl.io/community/config/eggs/creating_a_cust... [2] https://streamable.com/hlscux (an example of a panel for a server I administer) reply MiguelX413 12 hours agorootparentHoly shit I need to use this now. reply jallasprit 13 hours agoparentprevYeah, a lot of people setup https://pterodactyl.io/ on their servers and use that to provision and manage game servers. reply teekert 12 hours agorootparentLooks nice! I have a comprehensive docker compose file with itzg’s image [0] repeated a dozen times. [0] https://github.com/itzg/docker-minecraft-server reply zdwolfe 10 hours agorootparentI use this extensively, it's so wonderful reply ChiptuneIsCool 13 hours agoparentprevhttps://pterodactyl.io/ Something like this maybe? reply oldkinglog 12 hours agoprevPaper can't provide binaries as it would infringe Mojang's copyright. Users must \"build\" the server by downloading the official binary and applying local patches. This is well-automated for users. Licensing wasn't so much a problem in the early days of Minecraft. The first really popular server mod (CraftBukkit) not only provided binaries, but deopfuscated server code on a GitHub repo (since DMCA'd): http://github.com/bukkit/craftbukkit. That repository was ostensibly licensed as LGPL, but no one really believed that, given it was derived from Mojang's proprietary binaries. Mojang eventually acquired CraftBukkit and hired their devs. And there's an interesting footnote: for several months, Mojang didn't tell anyone they'd acquired CraftBukkit, and they kept making releases to the public GitHub repository. Given it was Mojang making these pushes, I believe (IANAL) that they are officially-licensed LGPL versions of the Minecraft server, albeit ~10 years out-of-date now. reply ro_bit 12 hours agoparentInterestingly enough, CB wasn't DMCAed by Mojang, but actually from one of its contributors, who was angry that he had been contributing free labor to a project that was secretly owned by Mojang (there was no CLA so he was legally able to take down the project by not authorizing his contributions to be used). There was a whole lot of drama and conflicting viewpoints around about this, but here's a post about it that I think stays pretty close to just the facts. https://blog.jwf.io/2020/04/open-source-minecraft-bukkit-gpl... (not associated with the author) reply piperswe 8 hours agorootparent> not authorizing his contributions to be used Actually, it's a lot more interesting than that. My understanding was that said contributor realized that since Bukkit is GPL, it can't be legally distributed combined with proprietary Mojang code. His takedown notices were based on GPL violation, not just revoking authorization (which he probably can't do with the GPL?) reply LelouBil 5 hours agoparentprevToday Mojang publishes deobfuscation mappings for each Minecraft Java release. reply apfsx 10 hours agoparentprevI remember before CraftBukkit there was hMod, I think that team went on to make CraftBukkit. reply me4502 9 hours agorootparenthMod was made and developed by hey0, who dropped the project after a while and a new group took over, mostly comprised of hMod plugin developers. That group gave up on hMod pretty quickly and went on to make Bukkit/CraftBukkit instead as it didn’t fit the needs of the community reply teaearlgraycold 7 hours agoparentprev> they are officially-licensed LGPL versions of the Minecraft server Oh this is an interesting legal area to explore. I wonder what you could do with that? reply freetime2 11 hours agoprevCan this be used with “bedrock edition” of Minecraft? Or only the java version? reply Badbird5907 10 hours agoparentOnly java edition, there is a plugin called Geyser https://geysermc.org/ that adds protocol compatibility between the two, allowing bedrock clients to connect. reply teaearlgraycold 7 hours agorootparentIt works surprisingly well but is a little glitchy reply nor-and-or-not 13 hours agoprev [–] So, how compatible is this to the vanilla server? Does anyone have experience with this? reply peeters 4 hours agoparentMost would consider it to be vanilla, when \"vanilla\" is used as a descriptor of the gameplay (i.e., as opposed to \"modded\"). There are some very minor differences, but almost anyone advertising a vanilla Minecraft multiplayer server is going to be using Paper or one of its predecessors/competitors. The released server.jar just really isn't optimized for more than one player. Some things that you can tweak for performance reasons also affect gameplay. For example, you can tweak the radius at which items group together into stacks of entities. Enabling this can make certain farm builds not work, e.g. a piglin bartering farm might have the gold bar destined for one piglin merge with the bar destined for a neighbouring piglin, when in pure Vanilla settings that might not happen. Similarly, you can have XP orbs merge into single orbs with higher XP values. But this drastically changes how long it takes to use an efficient XP farm, where normally you would be waiting for minutes for your player to receive the XP one orb at time. reply ro_bit 12 hours agoparentprevPaper itself is a series of patches over the official vanilla server implementation. You can convert a vanilla server to a paper server just by replacing the vanilla server.jar file with a paper server.jar one, and converting back to a vanilla server can be done fairly easily (but a bit more difficult than dropping in) It's much, much more performant, and able to handle a lot more players/chunks loaded/etc on the same setup compared to a vanilla server. Along with significant performance improvements out of the box, it also adds more configurations (vanilla has server.properties, but papermc adds paper.yml) which allow you to hand tune optimizations[1], from things like disabling player collisions, disabling block updates for certain laggy blocks, etc. These changes are configurable because they trade off expected vanilla behavior for better performance. The real strength of paper though is that they do all this while also implementing the spigot API, which enables paper server owners to use a wide variety of server mods[2] that can allow you to add things like minigames, anticheat, etc while still being compatible with unmodified clients. [1] https://www.spigotmc.org/threads/guide-server-optimization%E... (outdated but a bit more succinct) https://paper-chan.moe/paper-optimization/ (more technical) [2] https://www.spigotmc.org/resources/categories/spigot.4/ reply zdwolfe 10 hours agoparentprev [–] I much prefer Fabric server to Paper. Paper has a couple game-breaking differences in advanced redstone, and Fabric in general has more mod compatibility. There are options to disable the redstone differences, but it's a bit annoying. reply teaearlgraycold 7 hours agorootparent [–] I simply restored that redstone functionality. Easy enough. reply chairmanwow1 7 hours agorootparent [–] How did you do that? I run a fabric server and was instructed to just stay away from Paper/Spigot because they break redstone so badly. reply teaearlgraycold 7 hours agorootparent [–] https://github.com/danthedaniel/tildes-minecraft-configs/blo... Basically just enable the unsupported settings. My players were happy with the end result of each one I flipped on so presumably the behavior was as expected. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Paper is a high-performance Minecraft server focused on addressing gameplay and mechanics inconsistencies, offering forum and Discord support.",
      "Server admins can utilize Paperclip, while plugin developers have access to API patches and documentation for development.",
      "Paper sustains itself through donations and receives backing from YourKit and JetBrains for support."
    ],
    "commentSummary": [
      "Various options for running a Minecraft Java Edition server are discussed, such as vanilla server.jar, Fabric Loader, Forge, and Bukkit/Spigot/Paper/Purpur software.",
      "Paper is praised for its high performance on public servers, while Fabric is recommended for technical or private servers; Pterodactyl is noted as user-friendly server hosting software.",
      "Paper servers provide better performance and mod compatibility compared to Vanilla servers, and plugins like Geyser help connect Java and Bedrock editions."
    ],
    "points": 163,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1710100528
  },
  {
    "id": 39663135,
    "title": "Unveiling Systems Performance with eBPF Technology",
    "originLink": "https://www.brendangregg.com/blog//2024-03-10/ebpf-documentary.html",
    "originBody": "Brendan's site: Start Here Homepage Blog Sys Perf book BPF Perf book Linux Perf eBPF Tools perf Examples Perf Methods USE Method TSA Method Off-CPU Analysis Active Bench. WSS Estimation Flame Graphs Heat Maps Frequency Trails Colony Graphs DTrace Tools DTraceToolkit DtkshDemos Guessing Game Specials Books Other Sites Systems Performance 2nd Ed. BPF Performance Tools book Recent posts: 10 Mar 2024 » eBPF Documentary 28 Apr 2023 » eBPF Observability Tools Are Not Security Tools 01 Mar 2023 » USENIX SREcon APAC 2022: Computing Performance: What's on the Horizon 17 Feb 2023 » USENIX SREcon APAC 2023: CFP 02 May 2022 » Brendan@Intel.com 15 Apr 2022 » Netflix End of Series 1 09 Apr 2022 » TensorFlow Library Performance 19 Mar 2022 » Why Don't You Use ... 26 Sep 2021 » The Speed of Time 06 Sep 2021 » ZFS Is Mysteriously Eating My CPU 30 Aug 2021 » Analyzing a High Rate of Paging 27 Aug 2021 » Slack's Secret STDERR Messages 05 Jul 2021 » USENIX LISA2021 Computing Performance: On the Horizon 03 Jul 2021 » How To Add eBPF Observability To Your Product 15 Jun 2021 » USENIX LISA2021 BPF Internals (eBPF) 04 Jun 2021 » An Unbelievable Demo 29 May 2021 » Moving my US tech job to Australia 23 May 2021 » What is Observability 09 May 2021 » Poor Disk Performance 04 Nov 2020 » BPF binaries: BTF, CO-RE, and the future of BPF perf tools Blog index About RSS Brendan Gregg's Blog home eBPF Documentary 10 Mar 2024 eBPF is a crazy technology – like putting JavaScript into the Linux kernel – and getting it accepted had so far been an untold story of strategy and ingenuity. The eBPF documentary, published late last year, tells this story by interviewing key players from 2014 including myself, and touches on new developments including Windows. (If you are new to eBPF, it is the name of a kernel execution engine that runs a variety of new programs in a performant and safe sandbox in the kernel, like how JavaScript can run programs safely in a browser sandbox; it is also no longer an acronym.) The documentary was played at KubeCon, and is on youtube: Watching this brings me right back to 2014, to see the faces and hear their voices discussing the problems we were trying to fix. Thanks to Speakeasy Productions for doing such a great job with this documentary, and letting you experience what it was like in those early days. This is also a great example of all the work that goes on behind the scenes to get code merged in a large codebase like Linux. When Alexei Starovoitov visited Netflix in 2014 to discuss eBPF with myself and Amer Ather, we were so entranced that we lost track of time and were eventually kicked out of the meeting room as another meeting was starting. It was then I realized that we had missed lunch! Alexei sounded so confident that I was convinced that eBPF was the future, but a couple of times he added \"if the patches get merged.\" If they get merged?? They have to get merged, this idea is too good to waste. While only several of us worked on eBPF in 2014, more joined in 2015 and later, and there are now hundreds contributing to make it what it is. A longer documentary could also interview Brendan Blanco (bcc), Yonghong Song (bcc), Sasha Goldshtein (bcc), Alastair Robertson (bpftrace), Tobais Waldekranz (ply), Andrii Nakryiko, Joe Stringer, Jakub Kicinski, Martin KaFai Lau, John Fastabend, Quentin Monnet, Jesper Dangaard Brouer, Andrey Ignatov, Stanislav Fomichev, Teng Qin, Paul Chaignon, Vicent Marti, Dan Xu, Bas Smit, Viktor Malik, Mary Marchini, and many more. Thanks to everyone for all the work. Ten years later it still feels like it's early days for eBPF, and a great time to get involved: It's likely already available in your production kernels, and there are tools, libraries, and documentation to help you get started. I hope you enjoy the documentary. PS. Congrats to Isovalent, the role-model eBPF startup, as Cisco recently announced they would acquire them! Click here for Disqus comments (ad supported). Site Navigation Systems Performance 2nd Ed. BPF Performance Tools book Recent posts: 10 Mar 2024 » eBPF Documentary 28 Apr 2023 » eBPF Observability Tools Are Not Security Tools 01 Mar 2023 » USENIX SREcon APAC 2022: Computing Performance: What's on the Horizon 17 Feb 2023 » USENIX SREcon APAC 2023: CFP 02 May 2022 » Brendan@Intel.com 15 Apr 2022 » Netflix End of Series 1 09 Apr 2022 » TensorFlow Library Performance 19 Mar 2022 » Why Don't You Use ... 26 Sep 2021 » The Speed of Time 06 Sep 2021 » ZFS Is Mysteriously Eating My CPU 30 Aug 2021 » Analyzing a High Rate of Paging 27 Aug 2021 » Slack's Secret STDERR Messages 05 Jul 2021 » USENIX LISA2021 Computing Performance: On the Horizon 03 Jul 2021 » How To Add eBPF Observability To Your Product 15 Jun 2021 » USENIX LISA2021 BPF Internals (eBPF) 04 Jun 2021 » An Unbelievable Demo 29 May 2021 » Moving my US tech job to Australia 23 May 2021 » What is Observability 09 May 2021 » Poor Disk Performance 04 Nov 2020 » BPF binaries: BTF, CO-RE, and the future of BPF perf tools Blog index About RSS Brendan's site: Start Here Homepage Blog Sys Perf book BPF Perf book Linux Perf eBPF Tools perf Examples Perf Methods USE Method TSA Method Off-CPU Analysis Active Bench. WSS Estimation Flame Graphs Heat Maps Frequency Trails Colony Graphs DTrace Tools DTraceToolkit DtkshDemos Guessing Game Specials Books Other Sites Copyright 2021 Brendan Gregg. About this blog",
    "commentLink": "https://news.ycombinator.com/item?id=39663135",
    "commentBody": "eBPF Documentary (brendangregg.com)151 points by JNRowe 11 hours agohidepastfavorite45 comments cyberax 3 hours ago\"eBPF Documentary: An exciting train wreck in progress\" That would be a better title. eBPF started as a small extension to just be able to insert small trivial hooks. It's now basically a hacked-up broken WebAssembly clone, with zero forethought put into it. NIH syndrome at its worst. It has recently grown unlimited loops with runtime metering, making the static verifier basically a worthless complexity. Before that, it had acquired exceptions and stack unwinding. reply misiek08 31 minutes agoparentAlso in article we can read \"like putting JavaScript into the Linux kernel\" :rotfl: reply akira2501 3 hours agoparentprevArchitecture independent eBPF \"rescue binaries\" could be an interesting part of a distribution toolset.. or attack rootkit. It's hard to have nice things. reply pjmlp 1 hour agoparentprevWhile I agree its design isn't the best, versus something like Solaris DTrace, it predates WebAssembly, so hardly a clone. reply softirq 9 hours agoprevThe genius of ebpf is allowing for pluggable policy in a world where the kernel API is very slow to change and can’t meet everyone’s needs. Whether it’s how the kernel handles packets off the wire, how it controls traffic, scheduling entities, or instrumentation, ebpf lets you provide logic rather than turn a bunch of knobs or use a bespoke syscall that only handles one case. It also moves the processing logic to the data in the kernel rather than having the kernel have to do expensive copies to and from userspace. ebpf isn’t really novel beyond the interfaces it provides. They are just kernel modules that have been vetted and are sandboxed. Inserting executable code has been part of the kernel since forever in module form and kprobes. reply yjftsjthsd-h 9 hours agoparent> the kernel API is very slow to change and can’t meet everyone’s needs Better yet - eBPF provides a stable ABI:) It makes things that were formerly kernel-internal possible to work with from a stable ~userspace interface. reply stefan_ 11 minutes agorootparentI'm curious what this guarantee includes - the bytecode? Because the actual in-kernel eBPF API is famously unstable, with eBPF-based applications usually requiring a cutting-edge kernel version (for industry anyway). And of course the eBPF programs themselves rely on accessing structures for which no stability guarantees are made whatsoever. reply dilyevsky 7 hours agorootparentprevbpf tooling generally provides no stability guarantees when you interact with kernel primitives. See [0], for example. Tho things have improved somewhat with CO-RE [0] - https://lore.kernel.org/lkml/93a20759600c05b6d9e4359a1517c88... reply xrd 10 hours agoprevI've been hearing more and more about eBPF, especially here on HN. I haven't yet watched the documentary so perhaps it is answered there. But, the analogy of JavaScript inside the kernel is great and I'm left wondering: what was the way to do it previously? Userland network tool? This standardizes on a interface to the kernel, not a language, right? Feels off to say it is JavaScript because that comes with a lot of baggage, but also (as a versatile and ubiquitous language) incredibly powerful and useful tool. Is that intentional by the author? reply lmm 10 hours agoparent> But, the analogy of JavaScript inside the kernel is great and I'm left wondering: what was the way to do it previously? Userland network tool? This standardizes on a interface to the kernel, not a language, right? Guess/sketch: It's a language in most senses. Previously the kernel had APIs for packet filtering rules for iptables etc., but the set of rules you could use was somewhat \"static\" - rules would have parameters, so you could do things like if the source IP is in this range then rewrite it as this and direct it to this interface, but it was kind of like one of those visual flowchart languages where you can drag and drop the available boxes in a given order, but if there isn't a box to do what you want then you're stuck. Whereas with eBPF it really is scriptable - rather than a specific rule type you can just submit the script you want it to run - and nowadays it's become kind of a general kernel scripting language rather than just for networking. I'd draw a parallel with how 3D graphics programming has shifted from \"you can do these kinds of transformations, submit a list of what you want to run in what order\" to \"this is our shader programming language, just write whatever you want to do as a program in this language\". reply bhickey 8 hours agorootparentiptables is definitely limited in comparison to eBPF, but that isn't the innovative step. BPF was around for more than twenty years before eBPF came around. Around 2013 I worked on a packet analysis pipeline that generated BPF code dynamically at runtime. eBPF isn't more scriptable than BPF in this sense. The language does add some opcodes and loops that weren't available in the original, but this is relatively modest. The real genius of what these folks did was extending the usefulness of BPF beyond the network stack. Without a provably safe language it would've been impossible to enable flexible kernel tracing. reply ants_everywhere 7 hours agoparentprev> This standardizes on a interface to the kernel, not a language, right? It's a VM that runs JIT-compiled eBPF programs. You can write code in C or Golang or other languages that compiles down to eBPF. I did a video looking at the kernel eBPF code here: https://youtu.be/hznUH_zP77U?t=1165 > wondering: what was the way to do it previously? Userland network tool? My understanding is that userland network tools were common in fields like finance that needed fast custom networking and wanted to eliminate the overhead of context switching. I don't know how common they are/were in other fields though. reply shp0ngle 2 hours agorootparentgolang compiles to eBPF? including the whole golang runtime with garbage collector? that... surprises me edit: I don't see anything on the web that would show golang compiles to eBPF. I see bpf2go which is the other way around. reply tptacek 6 hours agoparentprevAd-hoc kernel extensions were a pretty common answer, and one thing a lot of people love about eBPF is that it subsumes most of the reasons people wrote lkms commercially. reply jeremiahbuckley 7 hours agoparentprevOne of the big wins is not so much “build and run your own stuff” but there are very nice low-cost (in terms of compute) performance utilities built on eBPF https://github.com/iovisor/bcc There are so many utilities in that list; there’s a diagram midway down the readme which tries to help show their uses. bcc-tools should be available in any distro. Also, Brendan Gregg does a ton of performance stuff that is worth knowing about if you check out his other work. Not eBPF only. Flame graphs are useful. reply brcmthrowaway 3 hours agoprevBrendan Gregg will get a Turing Award for this, congrats! reply bch 10 hours agoprevI was a little disappointed DTrace[0] was not mentioned at all. The instrumentation (not the SDN) isn’t novel, not even to Linux (DTrace is available on Linux - I understand licensing is at least questionable (for some distros), but that aside…). [1] [0] https://en.wikipedia.org/wiki/DTrace [1] https://docs.oracle.com/en/operating-systems/oracle-linux/dt... reply ta8645 9 hours agoparentDTrace was not the inspiration for eBPF at all, so it's not obvious it is relevant to mention. As the documentary mentions, the initial impetus for eBPF, was software defined networking. eBPF is a much bigger and more comprehensive infrastructure piece (networking, tracing, security, etc) than DTrace. And thanks to licensing issues, even within the limited domain of tracing, DTrace will likely become a footnote in history, while eBPF becomes available on every major OS platform. reply bcantrill 9 hours agorootparentThere aren't \"licensing issues\" with DTrace -- you are merely referring to the fact that it is licensed under the MPL-derived CDDL and not the GPL. But it is definitely true that they are not seeking to solve the same problems! Safety is very core to DTrace[0]; the difference here is entirely deliberate. [0] https://bcantrill.dtrace.org/2005/07/19/dtrace-safety/ reply ta8645 8 hours agorootparentAFAIU, it goes beyond safety; eBPF is designed to run entirely in the kernel rather than requiring a user-space bridge. So it's able to operate at line speeds in networking scenarios etc. > you are merely referring to the fact that it is licensed under the MPL-derived CDDL and not the GPL. Not really. I'm referring to the fact the eBPF will be available across Microsoft, Apple, and Linux, and there is no other technology that will be able to offer that. > not seeking to solve the same problems! Exactly. eBPF has a much broader and more significant problem set, not just tracing, but also security modules, software defined networking, and an almost unlimited future potential as well. reply bcantrill 8 hours agorootparentA few points of clarification: - DTrace does execute entirely in the kernel. Indeed, anonymous tracing is instrumenting the system without any corresponding user process whatsoever - DTrace exists on quite a few systems, including (with caveats) Windows, MacOS and Linux. - The unbounded nature of eBPF very much runs contrary to the safety that DTrace assures; DTrace isn't seeking to augment the system, but merely to understand it reply bch 8 hours agorootparentprev> I'm referring to the fact the eBPF will be available across Microsoft, Apple, and Linux, and there is no other technology that will be able to offer that. I don’t know if there’s some qualification I’m missing in your statement, but does this not count? https://learn.microsoft.com/en-us/windows-hardware/drivers/d... https://opensource.apple.com/source/dtrace/ https://docs.freebsd.org/en/books/handbook/dtrace/ https://github.com/dtrace4linux/linux reply ta8645 7 hours agorootparent> https://github.com/dtrace4linux/linux Last commit, 5 years ago. While you're correct to say that DTrace is available on Linux, it is so restricted as to be much less capable than eBPF. It is the reason it will become a footnote, and eBPF will become ubiquitous. reply bcantrill 6 hours agorootparentIt may become a footnote on Linux, but Linux isn't the only system out there -- and DTrace remains alive and well in many systems (not least in its reference implementation in illumos[0]). [0] https://github.com/illumos/illumos-gate reply ta8645 6 hours agorootparentYou're discounting the network effects. Also, eBPF provides many more capabilities beyond dTrace, and it will be ubiquitous across all OS, without having to make the exception for Linux. Anyone targeting full cross-platform capabilities will be better served by eBPF. The unfortunate history that crippled dTrace on Linux, will lead to its ultimate sidelining. reply bcantrill 2 hours agorootparentI don't really know what you mean by network effects, but perhaps we're just talking about two different things: you are looking at eBPF as a substrate to deliver arbitrary software (?!) whereas I view DTrace exclusively as a diagnostic tool. As to their relative capabilities: its other potential advantages aside, eBPF (and the tooling built upon it like bcc) lacks much of the functionality and polish of DTrace when attempting to accurately instrument the system. Its lack of robustness makes even basic instrumentation challenging,[0] let alone the richer (and admittedly, more esoteric) features of DTrace that it's missing entirely. [0] https://oxide-and-friends.transistor.fm/episodes/mr-nagles-w... reply kragen 10 hours agoparentprevi feel like the fact that the page mentions brendan gregg is a pretty strong reference to dtrace already. actually it doesn't just mention him, it's completely written by him reply bch 10 hours agorootparentI think people that know, know he was involved (in the userland aspect) of DTrace, but I get a sense for some reason there’s no love lost between Brendan and “DTrace”. reply James_K 10 hours agoprev [–] It has always seemed quite obvious to me that dealing in machine code is a flawed approach for distributing software. At the most basic level, it entails giving someone else near unfettered access to the hardware of your computer and simply hoping that they do nothing malicious or malformed. Yet the software world as a whole seems continually shocked at the idea of using anything else. Perhaps someday we will learn this lesson in its entirety and begin to share code rather than blobs. reply yjftsjthsd-h 9 hours agoparentI don't think machine code is the problem? Running a native binary doesn't give it access to anything inherently; the OS gives access through syscalls, and can impose restrictions - and indeed, does; it's not like running a binary on Linux automatically gives it access to anything under /dev. Also, it's not that the whole software world thinks blob-only software is normal; I'm typing this on a nice comfy GNU/Linux box where the only blobs are some firmware. (Edit: And to be quite clear, a good chunk of this community would really like to get rid of those blobs too, it's just that we don't have a fix at this point.) reply James_K 8 hours agorootparent> Running a native binary doesn't give it access to anything inherently While this is true from a certain perspective, machine code creates a system which must grand access to many things to become usable. A shared file system is a good example of this. Some software could easily echo a line into you .profile that tries to launch a key-logger, and this works in many cases. The expectation of software existing as opaque files creates a huge amount of work for the OS in verifying the exact behaviour of the software as it runs (and in ways which can often be circumvented), rather than a source-based approach in which malware is never allowed to touch the processor. > I'm typing this on a nice comfy GNU/Linux box where the only blobs are some firmware So you suffer the worst of both worlds then. You've had to download and compile the source yourself, but as the software is designed around being distributed as blobs, so you enjoy none of the benefits that might come from source distribution. reply Veserv 7 hours agorootparentMachine code does not require granting anything. The presence of a shared filesystem is a artifact of the OS, not any sort of inherent requirement. The actual effect of machine code being opaque is actually \"beneficial\" to security as only very coarse protection boundaries are even expressible. You must coarsely separate your code to get protection which comes at a potential cost of performance. The advantage of non-machine code is that you can express very fine protection boundaries which reduces the performance cost of protection, but at the cost of more complex analysis. Ensuring protection boundaries in the presence of coarsely separated code is strictly easier. You have strictly less ways to escape. Any coarsely separated code can be trivially transplanted onto a system enabling finer protection since they can just not use the finer protection. By the same logic, any protection boundary that can not protect coarsely separated code has no hope of enforcing protection on finer separations. That the kernel designers of Linux, Windows, iOS, etc. can not even ensure protection in general when the systems are coarsely separated means they certainly can not be trusted to ensure protection in general when things are more finely separated. If they could, fixing their existing coarse protection boundary would be a trivial consequence. About the only theoretical reason why this might not apply to eBPF is that it is not \"general\" in that many programs are not expressible, but that is not sound, positive evidence of suitability where as the historical track record of protection in all commercial operating systems is abysmal to say the least. Claims of finally achieving security (on a strictly harder variant of the problem) are extraordinary claims and extraordinary claims demand extraordinary evidence. reply James_K 1 hour agorootparent> Machine code does not require granting anything It requires that you actually run software on your CPU before you can tell if it's malicious or not, by which point it is often too late to do anything about it. Almost all malware that exists does so thanks to this fact. reply cookiengineer 4 hours agorootparentprevWhat else do you suggest then? Is there even hardware available that implements a high-level VM programming language as an abstraction layer (instead of e.g. the x86 instruction set)? reply James_K 1 hour agorootparentAn program sits on top of hardware and can act as a go-between. Think of the web browser, or the JVM. Both insanely popular by virtue of providing the basic features I describe, and also evidence that they needn't even be integrated to the operating system. Something of this nature that also provides the basics of an interactive desktop. reply gggmaster 8 hours agorootparentprevIt's better than an OS that you have no control from top to bottom. But sure, it isn't ideal. reply yjftsjthsd-h 4 hours agorootparentprev> While this is true from a certain perspective, machine code creates a system which must grand access to many things to become usable. A shared file system is a good example of this. Some software could easily echo a line into you .profile that tries to launch a key-logger, and this works in many cases. That's common, but it's certainly not a requirement to run native code. For example, we've done a pretty good job at retroactively fixing that while preserving backwards compatibility with containers (I can, and have run normal official Firefox binaries inside a docker container with zero access to my real home directory) or sandboxes like flatpak (bubblewrap). If you want to run real native binaries but don't have to preserve backwards compatibility, then it gets easy; genode ( https://genode.org/ ) does a lovely job of truly practicing only giving programs what access you want to give them. > The expectation of software existing as opaque files creates a huge amount of work for the OS in verifying the exact behaviour of the software as it runs (and in ways which can often be circumvented), rather than a source-based approach in which malware is never allowed to touch the processor. I think you're overoptimistic regarding what you can do with the source code short of manual (human) auditing. I mean, sure there are things you can scan for to try and catch bad behavior, but in the case of actual malice I wouldn't trust automatic code analysis to protect me. >> I'm typing this on a nice comfy GNU/Linux box where the only blobs are some firmware > So you suffer the worst of both worlds then. You've had to download and compile the source yourself, but as the software is designed around being distributed as blobs, so you enjoy none of the benefits that might come from source distribution. I have no idea why you think either of those things? Depending on the distro I certainly can compile from source on my own box (ex. Gentoo, NixOS), but I can also use precompiled binaries (ex. Debian, NixOS) while still having it be trivial to go find the exact source that went in to the binary package I downloaded (this has gotten even stronger with Reproducibility efforts meaning that I can even verify the exact source and build config that created a specific binary). The actual application software and OS are available as Open Source code that can be audited, with binaries available as a convenience, and the only remaining blobs (unwelcome but impractical to fix so far) are firmware blobs with relatively constrained roles (and on machines with an IOMMU we can even enforce what access they have, which is a nice mitigation). reply James_K 26 minutes agorootparent> I think you're overoptimistic regarding what you can do with the source code Suppose you have a Java function. If you remove the file-system class at a system level, what can that code do? It's pretty easy to isolate it so that it cannot compile to any kind of malicious code, only conduct computations to produce values. That's why you can open a website running JavaScript without as much worry for the security of your computer as you would have downloading a blob and running it. > I have no idea why you think either of those things? Well most of the software on your computer has the following api: int main(int argc, char **argv) This is because it needs to run as an executable. Without this requirement, software might have much more interesting APIs which are much more useful. But instead there is a requirement that all software is mashed into blobs and has all of the useful information removed from it. Additionally, just compiling the software from source does nothing to guarantee that it isn't malware. You can compile malware from source just as easily as anything else. The system I'm proposing does not allow for malicious programs to be compiled. reply avianlyric 8 hours agorootparentprev> The expectation of software existing as opaque files creates a huge amount of work for the OS in verifying the exact behaviour of the software as it runs That’s not really how the software/OS relationship works. By default OS’s run software in very unprivileged CPU security contexts, where they can’t really do anything beyond but operate on memory the OS allocated to the software, with CPU instructions can’t talk directly to other hardware, or read/write memory beyond the bounds the OS specified. To do literally anything else, like open a file, write to a file, talk to a network interface, talk directly to other hardware connected to the CPU. The software needs to use a syscall, and effectively ask the OS to perform the operation on its behalf. Every single dangerous operation you can perform is guarded by the OS, the OS doesn’t validate anything, it simply decides to perform the operations requested, or it decides not to perform those operations. The only reason why software expects so much unfettered access to a system is simply because OS have historical not limited what software was allowed to do, OS’s simply performed whatever operation was requested of them without question. It only in the last 20 years or so that security became a concern, and it occurred to us that allowing software to just do whatever it wants is probably a bad idea. But the genie is out of the bottle, and it’s proving hard to put it back in. So retroactive work to limit what software can do by default, without simply breaking everything, is slow and difficult. If you ever want to understand why you can do somethings on MacOS, but not iOS, then this is a pretty good place to start (if we ignore Apple’s brand and commercial reasons for a moment). iOS only ever supported tightly sandboxed apps, that had to play nice with a draconian permissions system, where as MacOS is more standard OS, that was originally developed in the age before security was really a concern. So it’s easy to enforce tight sandboxing on iOS, where software has always had to deal with it, but hard on MacOS, where for the majority of MacOS history, sandboxing simply didn’t exist. > rather than a source-based approach in which malware is never allowed to touch the processor. That’s a nice idea, but doesn’t really hold up to scrutiny. You need to look at the ever increasing number of supply chain attacks to realise that simply being open source does little to ensure malware doesn’t make it onto your machine. And that before we get into issues like heart bleed, where the OSS software on your machine may contain bugs or errors that allow remote parties to gain access to privileged data or credentials, via OSS software that isn’t malware, it just got bugs in it. At the end of the day, it doesn’t really matter what magical security boundaries you develop, someone will find a way around it. Which is why defence in depth is such an important principle. Simply relying on the idea that Open Source means no malware is foolish. You need proper OS defences regardless. reply James_K 1 hour agorootparentI know all that. The process you describe is the OS verifying the behaviour of software as it runs. >That’s a nice idea, but doesn’t really hold up to scrutiny It does actually. The kind of attack you describe only works because software exists in non-source based systems. If you were going to run a Haskell function, and you were told that I was going to modify its body in some way, there is no way I can insert malware into it because you cannot reach out of the context of the function and mess with the operating system. Software is composed of functions, which naturally run in a fully containerised state and only become dangerous when the OS adds an additional layer of computer-wide state. reply akira2501 3 hours agoparentprev\"That's a really cool security aware script language you've got there! So.. um.. how can I extend it to call third party libraries?\" Perhaps the idea that \"the computer\" is one single entity with a shared security domain and view of hardware is the flaw. Why can my web browser read my tax documents unless I go through a bunch of rather absurd efforts to prevent something so simple? reply xorcist 2 hours agorootparentBecause you want to be able do report your taxes documents to the tax office? It's one of those things that sound so simple on paper, but every time someone does that trivial thing and not make documents available to the web browser, usability suffers. The real answer why the browser can read certain files is much more complex, your web browser is not a singular entity anymore. And the network and protcol speaking parts of it can't access your documents, according to the principle of least authority. It's far from perfect and gets hacked every time, but do take the time to read how that's done. The hacks are just as complex as the web browser itself. The practical problem with the browser is the enormous complexity of functions, everything from OpenGL to databases to p2p and usb, that keeps growing boundlessly. reply akira2501 37 minutes agorootparentRight.. which is why \"home PC\" is one of the largest attack surfaces we have. The fact that tax authorities expect you to brave the gap is only one of the problems with this configuration. The web browser _purports_ to be that entity. The list of CVEs shows that it isn't. If I install a \"web browser\" I'm installing \"a binary program that can access anything it wants at any time it wants.\" \"Do take the time to read\" is an absurdly condescending thing to say while simultaneously moving the goalposts of the argument. reply brcmthrowaway 3 hours agoparentprevChatGPT? reply frozenport 9 hours agoparentprev [–] Android and mobile platform make it hard to distribute as fully compiled source. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Brendan Gregg's website focuses on systems performance, highlighting eBPF technology and tools.",
      "Recent posts discuss documentaries, conferences, and personal tech experiences, offering resources for performance analysis and monitoring.",
      "The website serves as a valuable source for those interested in in-depth insights into systems performance and eBPF technology."
    ],
    "commentSummary": [
      "Hacker News delves into the evolution and criticisms of eBPF, compared to WebAssembly, for enhanced performance in the Linux kernel.",
      "The advantages of eBPF over userland network tools, its resemblances to DTrace, and the paramount role of security in software development are highlighted.",
      "Discussions extend to limitations of machine code, security challenges in web browsers, the trend towards source-based software distribution, and the demand for heightened security in software systems."
    ],
    "points": 151,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1710110734
  },
  {
    "id": 39661497,
    "title": "Reviving MemChess: Mastering Chess Openings with Spaced Repetition",
    "originLink": "https://grondilu.github.io/memchess/",
    "originBody": "MemChess New Flip Undo Hint Opening Chooser +King's Pawn Opening, General +Queen Pawn Opening, General +Zukertort Opening, General +English Opening, General Hi! I'm a tool to teach you the most popular chess openings and their variations. You play the main line for the opening you have chosen, and I will play many variations against it. I use Spaced Repetition to teach you as quickly as possible, where I often show you lines you find difficult, and rarely show you the easy lines. I'm in active development, and I hope I can help improve your chess game! MemChess Close Hi! I'm a tool to teach you the most popular responses to each chess opening and its variations. I'll ask you for the best move, and then I'll make any respectable move back. I do this by looking at what many Master (>2200 Elo) chess players have played before. I play the most popular moves more often but still like to play rarer openings. I'm in active development, and I hope I can help improve your chess game! MemChess Congratulations, you have finished the line with . Line will be shown again in Suggested book Line finished. No more lines due Please choose an opening This is an opening, but not one selected in the Opening Chooser. Select it now?",
    "commentLink": "https://news.ycombinator.com/item?id=39661497",
    "commentBody": "Rebuilding memchess.com from its archive (grondilu.github.io)149 points by grondilu 15 hours agohidepastfavorite28 comments grondilu 15 hours agomemchess.com was a neat website to learn chess openings with the spaced repetition method. It was closed around 2020 for some reason. I used code gathered from archive.org[1] and built a version that seems to work. It does not require a subscription/login, instead it stores progress through the HTML5 web storage API[2]. 1. https://web.archive.org/web/20240000000000*/memchess.com 2. https://www.w3schools.com/html/html5_webstorage.asp reply TylerLives 14 hours agoparentHave you tried https://listudy.org ? reply najdf 14 hours agorootparenthttps://listudy.org is a brilliant open source opening spaced repetition trainer. Another one is https://chessdriller.org, though you need to provide the moves yourself for that one. There's also https://chessmadra.com, but I believe their backend is closed-source. reply krick 9 hours agorootparentI haven't really used any of this stuff. I don't even know why. So I wouldn't really know how people actually use it to make most of it, but I tried just now listudy (a couple of studies) and that memchess clone. Seems pretty much the same thing, but listudy is a bit more structured, with notes and stuff. So I guess my question (to the OP, probably) would be, why memchess (clone)? Is there a reason why I would want to use it instead of/in addition to listudy, or is it just \"because why not\"? reply grondilu 36 minutes agorootparentThe short answer is I tried listudy and I didn't like it. I liked memchess because of how simple it was, and I liked not having to enter my own lines (though I understand this is important for serious study). Memchess is what helped me defeat the strongest player in my local club, so I was quite frustrated when it shut down. reply ta8645 13 hours agoparentprevIt seems like the variation displayed at the bottom left of the board is trailing the move played, rather than being displayed beforehand as a challenge. This means that when the board is first displayed, there is no indication at all as to which opening you're expected to be recalling. Or am I misunderstanding the expected interaction? reply amsully 13 hours agorootparentIt allows you to flow through lines and play best book moves naturally. You can tell it that you want to focus on particular lines or line and it will let you know when you've made an incorrect move. reply p0w3n3d 2 hours agoprevI always am fond of projects to rebuild things from the past, as I can already see that systems that I had used for a long time, are already gone. We're losing much more information nowadays, as in 1900-2000 main vessel of knowledge were books and those are available more or less in libraries, but since 2000 knowledge was been published more often in forums and web apps/blogs and those are not storable due to copyright law and privacy issues. This knowledge is or will be lost in a few years. Not speaking of old movies/series etc. Thankfully music is still available on CDs reply danmaz74 58 minutes agoprevSpaced repetitions for openings make a lot of sense, but in my experience they work better for tactics. I improved a lot with a hacked up solution combining puzzles and spaced repetitions, and then created a website to make it easier for anybody to adopt that approach: https://chess.braimax.com Lately I've been very busy with my day job, but I'm almost ready to get back to developing that website. Suggestions are welcome. reply laurentlb 14 hours agoprevI've tried quickly on my phone, but it's too hard to use: - I normally prefer to click on the piece and then on the target square. It doesn't work here, I think only drag & drop is supported. - But drag & drop is broken, as it's scrolling the page at the same time as the piece. I like the concept, I'll try again later on laptop. reply bean-weevil 13 hours agoparentIt works for me on Firefox android (but maybe that's because there's nothing to scroll) reply bbx 13 hours agoparentprevI second that on desktops/laptops as well: being able to click on a piece and then on the target. reply amsully 13 hours agoprevGreat to see this on HN and great work grondilu! Many users of memchess yearned for it to return and it really did disappear suddenly. It fills a great gap between a strategy book/guide and pure tactic trainers. Next step is for it to be refined as it was always a little clunky. reply thethimble 10 hours agoprevIf you’re interested in spaced repetition for chess openings I’d strongly recommend chessbook. Not open source but extremely helpful in discovering interesting variations and remembering them in the long term. https://chessbook.com/ reply pajtai 9 hours agoprevThis is awesome. If you want help from other contributors, you'll probably find it easier to collaborate if you move memchess into its own repo (vs storing it in the grondilu.github.io repo). Each repo can have a dedicated GH Pages. Really cool! reply grondilu 3 hours agoparentHow do you add a GH page to a repo? reply kevinbowman 1 hour agorootparentThere's a guide at https://docs.github.com/en/pages/getting-started-with-github..., but the critical bit is \"In the \"Code and automation\" section of the sidebar, click Pages.\" reply ewitt22 10 hours agoprev@grondilu - this is awesome! I have been working on something similar for the past few months. Would be cool to connect if you're interested. My email is emorywitt@gmail.com reply FiReaNG3L 12 hours agoprevOh god, I'm so thankful for this, I was extremely sad when this disappeared, it was by far the best! reply 29athrowaway 6 hours agoprevFor Go: https://www.josekipedia.com/ reply QuadrupleA 11 hours agoprev [–] I find chess goes from tedious and overwhelming in the opening game, to interesting in the midgame, and fascinating towards the endgame. Sad you have to memorize thousands of opening lines to get good at it. reply isolli 6 minutes agoparentEndgames with few pieces remaining require memorization as well. You don't want to draw when you had a sure win. reply najdf 11 hours agoparentprevIf you don't enjoy studying openings, you can get far by just being aware of opening ideas and common traps, and spending the rest of your time on midgame and endgame. Memorizing thousands of opening lines is really only for 2200+ Elo players (global top 20,000), even though Chessable and other marketing teams are doing their best to convince amateurs otherwise. reply seabass-labrax 10 hours agorootparent> ...Chessable and other marketing teams are doing their best to convince amateurs otherwise The study of openings is a very easy thing to 'productise' in comparison to other elements of chess. One's progress with learning openings is particularly tangible and easy to measure, but the law of diminishing returns sets in pretty quickly. There is also the more practical element that openings have names; it's quite difficult as a beginner to talk about more philosophical or abstract parts of the game, such as identifying and referring to the situations when a certain bishop is strong or not. In many ways, the focus on openings is not a new phenomenon - chess opening books always seem to have dominated the genre of chess books. They're also excellent ways for publishers to sell updated editions... New openings are discovered like clockwork; the genre exists alongside sport almanacs and travel timetables as a perfect way to keep the printing presses going! reply ewitt22 10 hours agorootparentprevvery true reply url00 11 hours agoparentprevYou could try Chess 960 (a.k.a. Fischer random) which was designed to avoid opening theory. It's very fun. reply seabass-labrax 10 hours agorootparentIt doesn't avoid it fully though; some basic knowledge about opening theory is still essential. It is quite possible for certain permutations to open up wide gaping holes in each side's defences, which the more experienced player can use to checkmate almost immediately. It's like the Scholar's Mate but manifold. reply bongodongobob 10 hours agoparentprev [–] Not true, in fact, this is backwards. Memorizing openings is the last thing you do to squeeze every bit of advantage out of the game. Opening theory and practicing tactics will get you to almost ~2000. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MemChess is a tool that utilizes spaced repetition to teach popular chess openings and variations, aiding players in learning quickly.",
      "It concentrates on main lines and responses to openings, incorporating insights from Master chess players to enhance the learning experience.",
      "Players are prompted to make optimal moves as they engage in learning and honing various openings through practice."
    ],
    "commentSummary": [
      "The user grondilu resurrected the website memchess.com using code from archive.org, now without requiring a login and saving progress through the HTML5 web storage API.",
      "Users suggest trying other chess opening spaced repetition trainers like listudy.org and chessdriller.org instead of memchess.",
      "Discussions included experiences with spaced repetitions for chess tactics, opening suggestions for enhancements, and alternate learning sources."
    ],
    "points": 150,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1710096514
  },
  {
    "id": 39662988,
    "title": "Study: Lead in Gasoline Reduced IQ of Half the U.S. Population",
    "originLink": "https://www.nbcnews.com/health/health-news/lead-gasoline-blunted-iq-half-us-population-study-rcna19028",
    "originBody": "Health news Lead from gasoline blunted the IQ of about half the U.S. population, study says Leaded gas was banned in 1996, but exposure to the poison cost people born before then several IQ points on average, researchers estimated. Lead exposure lowered the IQ of about half the U.S. population, study shows 01:40 Get more newson Print Save Create your free profile or log in to save this article March 7, 2022, 11:59 PM UTC By Elizabeth Chuck Exposure to leaded gasoline lowered the IQ of about half the population of the United States, a new study estimates. The peer-reviewed study, published Monday in the journal Proceedings of the National Academy of Sciences, focuses on people born before 1996 — the year the U.S. banned gas containing lead. Overall, the researchers from Florida State University and Duke University found, childhood lead exposure cost America an estimated 824 million points, or 2.6 points per person on average. Certain cohorts were more affected than others. For people born in the 1960s and the 1970s, when leaded gas consumption was skyrocketing, the IQ loss was estimated to be up to 6 points and for some, more than 7 points. Exposure to it came primarily from inhaling auto exhaust. Life after being affected by lead poisoning 02:51 The team behind the study used gas consumption data, population estimates and other data to calculate that as of 2015, more than 170 million Americans had had blood lead levels above 5 micrograms per deciliter in their early childhood years. Lead is a neurotoxin, and no amount of it is safe. Currently, 3.5 micrograms per deciliter is the reference value for blood lead levels to be considered high; the acceptable amount was once higher. Principal study author Michael McFarland, an associate professor of sociology at Florida State University and a faculty member of the university’s Center for Demography and Population Health, called the number of people affected by lead exposure “staggering.” “This is important because we often think about lead as an issue for children, and of course it is,” he said. “But what we really wanted to know is what happens to those children who were exposed?” In many cases, McFarland said, a 2 to 3 point IQ difference is nominal, unless an individual is on the lower side of IQ distribution. “If you’re more toward cognitive impairment, a couple points can mean a lot,” he said. But on a population basis, shifting the average IQ down even a small amount could have large consequences, said Sung Kyun Park, an associate professor of epidemiology and environmental health sciences at the University of Michigan School of Public Health. The entire bell curve shifts, he explained, with more of the population at what was once the extreme low end of IQ scores. Lead used to be added to gasoline to help engines run more smoothly until other, safer additives replaced it. In addition to being linked to lower IQs, it has also been associated with heart and kidney disease. Flint children tested for lead poisoning 01:08 Lead can be inhaled or ingested, with children particularly susceptible to its poisonous effects. Children’s blood lead levels have been dramatically lowered in the U.S. in recent decades, but lead exposure still happens, and Black children are exposed more often than white children. Monday’s study, too, estimated that most Black adults under age 45 experienced “considerably higher” levels of blood lead levels in early life than their white counterparts. The racial disparities are generally due to environmental contamination and infrastructure issues that affect drinking water in low-income and minority neighborhoods, with the water crisis in Flint, Michigan, one of the most egregious examples in recent years. And while children are the most vulnerable to getting very ill from lead, the toxin’s damage can show up years later, Park said. Lead exposure is believed to put people at risk for chronic and age-related diseases, including cardiovascular disease and dementia. “Lead is a never-ending story,” he said. There are medical interventions available for children who have recently been exposed to high amounts of lead, but those wouldn’t work for adults born before 1996. Still, the study findings should not be a major cause for concern, McFarland said. “There are a host of things that go into IQ,” he said. “This is one that is obviously negative, but if you also have a nurturing home environment, that helped your IQ.” Elizabeth Chuck Elizabeth Chuck is a reporter for NBC News who focuses on health and mental health, particularly issues that affect women and children.",
    "commentLink": "https://news.ycombinator.com/item?id=39662988",
    "commentBody": "Lead in gasoline blunted IQ of half the U.S. population, study says (nbcnews.com)145 points by jnord 11 hours agohidepastfavorite121 comments jjgreen 11 hours agoThe chap who put lead in petrol was also an early enthusiast for the CFCs which put a hole in the ionosphere, thank you Mr Midgley Jr. reply aunty_helen 8 hours agoparentWhile people will be quick to judge, it’s necessary to remember how much of a boon refrigeration and high octane gasoline was for human progress. Humanity is full of mistakes like DDT and lead makeup. We continue to make these mistakes today, but it’s important to know that we progress with technology. CFCs are no longer necessary and high octane unleaded now can be had at the pump. reply afavour 7 hours agorootparentIIRC the gas company execs were absolutely aware of the dangers of leaded gasoline and made sure that information never became public. Yes, it allowed a lot of progress. But society should have been able to make an informed decision about whether it was worth it. reply giantg2 7 hours agorootparent\"IIRC the gas company execs were absolutely aware of the dangers of leaded gasoline and made sure that information never became public.\" Sounds like most disruptors, PI-based, and food companies today - Facebook algorithms, Uber's early safety/compliance/sexism issues, etc. I wouldn't be surprised if certain food related things become a big deal in a couple of decades. Stuff like hormones and meds in animals and xenohormones in packaging. Even things like synthetic food dyes have some evidence of hyper activity in children, to the point the EU is putting warning labels on some things. When the CEOs have their families avoiding their own products, that's a damn big warning that they know. It shouldn't be a surprise. reply ChainOfFools 5 hours agorootparentThe cycle keeps repeating in part because the people who do this kind of move fast and break (other people's) things crap in the beginning, whose behavior we excuse and paper over in the name of progress later on, are the ones who also accumulate commanding, NSA wealth and influence offstage. They thereby greatly contribute to setting the stage for future behavior like this, through their soft power influence - what sort of people they endorse, promote, and financially sponsor. It should matter who gets rich. People have for too long been naively (or foolishly) swallowing the elite-favored narrative that someone else's money is none of anyone's business; a dynamically imbalanced moral which may have been weakly defensible in a byegone world that was far less tightly integrated, consolidating and wealth-connected than it has become. reply nielsbot 6 hours agoparentprevI found numerous articles about him. I think this was on HN before: https://www.smithsonianmag.com/smart-news/one-man-two-deadly... reply leeoniya 8 hours agoparentprevah yes, the \"one-man environmental disaster\" https://en.wikipedia.org/wiki/Thomas_Midgley_Jr. reply BD103 8 hours agorootparent> Environmental historian J. R. McNeill stated that he \"had more adverse impact on the atmosphere than any other single organism in Earth's history.\" reply ironmagma 7 hours agorootparentOnly made possible by the millions going along with what that one individual did. This guy was like a hacker moving fast and breaking things, had no clue what he was doing. Nobody does when they build things for the first time. reply stemlord 6 hours agorootparentThat speaks more negatively to that tired SV motto than it speaks positively to the leaded gasoline guy. reply ironmagma 5 hours agorootparentYes, it's not intended to be a positive sentiment, just an observation about the great organism that is all of humanity. reply EmuAGR 7 hours agorootparentprevMaybe that quote should've referred to the first photosynthetic organism... (due mostly to reproduction than ability itself). reply noobermin 8 hours agorootparentprevDear god, reading that bio is a horror story. While you can blame Midgley somewhat he was definitely enabled by the oil companies. reply JoeAltmaier 11 hours agoparentprevDidn't he also invent dry-cleaning fluid? Flourine-based, powerful stuff. reply fancy_pantser 9 hours agorootparentMartinizing dry cleaning process was Henry Martin. reply ipnon 9 hours agoprevWe should remember stories like this when we consider the future of AI. Midgley could only see the benefits to be gained. He did not have the foresight to see that seemingly inconsequential scientific and industrial decisions can have immeasurable outcomes. reply mullingitover 8 hours agoparent> He did not have the foresight to see that seemingly inconsequential scientific and industrial decisions can have immeasurable outcomes. Lead toxicity wasn't some big unforeseeable surprise, there was over two millennia of documentation on the dangers of lead when Midgley kicked off the mass production of tetraethyl lead. It's better to just say the man was cursed. He also brought Freon to market. Arguably no living person has done as much environmental damage as he accomplished single-handedly. As Bill Bryson said, he had \"an instinct for the regrettable that was almost uncanny.\" reply JKCalhoun 7 hours agorootparentAnd where did I hear someone say that we should ban everything he ever touched — just in case. reply bastawhiz 8 hours agoparentprevI mean, Midgley knew lead was dangerous. He suffered from lead poisoning himself. People in his plants died and came down with serious lead poisoning just a year after he had to step away because of his lead poisoning. He was directly informed about the risks of lead years in advance. Two years after he got lead poisoning, he did a demonstration where he huffed TEL and poured it on his hands and claimed he could do it every day without consequence. Nobody believed him, perhaps because it was no secret that lead exposure was dangerous. The state of New Jersey didn't believe it either and shut down his plant, and he later came down with lead poisoning again and had to take a leave of absence. So to your point, these were the actions of a man who saw people in his factories dying, came down with lead poisoning twice himself, and kept selling the stuff. That's not a lack of foresight, that's just greed. He just didn't care. The benefits he saw were \"making a shitload of money\". reply pvaldes 43 minutes agorootparent> he got lead poisoning twice > He just didn't care Morale: Don't follow the man with the seriously damaged IQ. reply nazka 1 hour agorootparentprevPeople ready to make money this bad is frightening. reply ChainOfFools 5 hours agorootparentprev> he later came down with lead poisoning again and had to take a leave of absence. reminds me of the guy who made millions (billions?) selling identity theft protection theatre (LifeLock IIRC) by running an absurd ad campaign in which he put his own name and SSN in public advertisements for the service, and promptly had his identity stolen, and kept having stolen over and over for years. But with hundreds of millions in the bank selling this scam he likely had plenty of other resources to mitigate the damage, resources his millions of customers likely do not. reply metalcrow 7 hours agorootparentprev> he huffed TEL and poured it on his hands and claimed he could do it every day without consequence I can't imagine he knew about the dangers in that case. Even someone amazingly greedy would be unlikely to risk the very serious complications doing this would cause. reply ttfkam 5 hours agorootparentIt's hard to get a man to understand something when his paycheck depends on his not understanding it. reply tdeck 8 hours agoparentprevHe totally could foresee the dangers. He got lead poisoning in 1923 and had to take time off. Dozens of people in the plant where they were manufacturing the stuff got lead poisoning and had neurological symptoms. Several people died. It's not like there weren't massive warning signs that this substance was going to be a problem. This would be like if the AI you were trying to launch had killed 10 of your coworkers and driven 10 more of them insane, and also you decided to name it something deceptive so people didn't know there was AI in it. reply lucasban 9 hours agoparentprevThis applies to anything new, not just AI. It’s also important to have the appropriate level of caution, without fear mongering, and accept that even our best due diligence can be insufficient at times. reply 123yawaworht456 8 hours agoparentprevyou people insert the current thing into the most peculiar contexts. reply coffeecat 8 hours agoprevThis topic is one of my biggest pet peeves. The hypothesis that sub-clinical lead exposure _causes_ IQ loss is, in my opinion, much weaker than most people realize. Observational studies (which are more or less all we have) tell us that Pb exposure correlates negatively with IQ, and that the correlation is moderated by a multitude of variables that are subtle and difficult to model. These studies do not tell us in which direction the arrow of causality points - they merely tell us that not all causative pathways are accounted for by the study's model. The idea that causation points from lead to IQ loss has some issues - e.g., that developmentally delayed two-year-olds ingest more dust/dirt, which increases their lead exposure. reply po 7 hours agoparentI feel like you maybe should shop around for a new pet peeve. Wasn't the data on this pretty clear due to various countries, states, cities banning lead at different times? My understanding is that you can see a corresponding drop in lead levels in the blood, rise in IQ, and reduction in crime 20 years later. https://en.wikipedia.org/wiki/Lead–crime_hypothesis#Correlat... reply giantg2 7 hours agorootparentTying lead levels to crime levels seems super sketchy to me when there are plenty of other factors that get studied and seem to have much better correlation and even causative effects. Hell, violent crime has started creeping back up in recent years, but to my knowledge the lead levels are not. reply defrost 7 hours agorootparentViolent crime \"creeping up\" is entirely dependant on whose reports and which methodology relative when it changed .. see (for example): https://www.themarshallproject.org/2023/11/03/violent-crime-... Whichever way that data is cut, though, pre-2000 and in particular mid 70s \"peak lead\" crime levels are far and away worse than those of the past two decades. reply giantg2 7 hours agorootparentThat's fine, but I don't think there any causation in that correlation. I've never heard a compelling argument to explain it when there are tons of other compelling factors (organized crime, economics, opportunity, surveillance technology, testosterone levels, even birth control and abortion). Edit: why disagree? I can claim that the size of the standard Hersey bar had a positive correlation over that time too. That doesn't mean there's a convincing causative mechanism. Most of the studies linking lead to crime rely on poor data practices, such as testing blood levels of convicted criminals. There are very easy explainations to show bias here, such as smarter criminals being less likely to be convinced and criminals with higher blood levels tending to have lower IQs. Many exclude certain areas like DC or NYC in their analysis or the trend disappeared. Many used state level data points which are not granular enough to investigate causative effect (eg median income at the state level is not useful. You need to investigate it at the individual level. Things like localized cost of living and standard of living would also be more useful numbers). The designs and data are sloppy as hell in my opinion. reply po 7 hours agorootparentprevYeah that's not how it works though... nobody is saying lead causes violent crime. Nobody is claiming crime can't go back up without lead. Of course there are better causative factors. It's just that meta-analysis over many studies seems to show that it is a contributor to crime. When it is removed from the environment, it then shows up in the crime data. reply giantg2 7 hours agorootparent\"nobody is saying lead causes violent crime... many studies seems to show that it is a contributor to crime.\" This seems contradictory. How can it not cause violent crime yet be a causative factor? I can see how it might be correlated. But I've never heard a compelling argument for how it would be causative.on any significant level. reply po 4 hours agorootparentSorry, When I said \"nobody is saying lead causes violent crime\" I meant \"nobody is saying lead is the sole cause of violent crime.\" reply bbstats 8 hours agoparentprevModels are hard and causation near impossible, but feel pretty safe being on the \"arrow of causality pointing from lead to low IQ\" rather than the hilarious inverse... reply diego_sandoval 7 hours agorootparent> the hilarious inverse Made me think of this: 1. Make up some scientific \"evidence\" that exposure to some common, harmless substance is actually harmful. 2. People believe it. Smart people make an effort to avoid it. Dumb people don't care. 3. Measure it 30 years later, and now there's a correlation between higher IQ and lower exposure to the substance. Higher IQ causes lower exposure to the substance. I'm not saying this is the case with Pb, it's just a funny idea. reply root_axis 7 hours agorootparentThere are longitudinal studies that track people from childhood before the dangers of lead were widely known. There are also comparative studies where populations are studied with respect to occupational exposure. reply rahimnathwani 7 hours agorootparentprevSmart people make an effort to avoid it, dumb people don't care. With lead how would a smart person do this? Move from the US to a country that banned leaded gasoline earlier? Is your sense that the number of smart people leaving the US each year is greater than the number of smart people moving to the US each year? reply smogcutter 7 hours agorootparentNot parent, but: > I'm not saying this is the case with Pb, it's just a funny idea. reply Retr0id 7 hours agorootparentprevhttps://en.wikipedia.org/wiki/Human_capital_flight reply nerdponx 6 hours agorootparentprevThe implication is not that low IQ causes lead exposure, but that low IQ and lead exposure are both caused by a third factor, presumably poverty. reply hombre_fatal 7 hours agoparentprevThat's the midwit's causation vs correlation meme which is usually at play when an HNer uses both those words in a comment. Observational research is how we know that smoking cigarettes causes health issues. We don't have 30 year RCTs on smokers. And there is no such thing as causation vs correlation. All we have are causal inferences draws from correlation. Finally, when you suggest there are confounders, that's a causal claim. What confounders do you have in mind here, what standard of evidence do you need to accept them as confounders, and do they supersede the evidence we have for lead and IQ? reply giantg2 7 hours agorootparent\"And there is no such thing as causation vs correlation. All we have are causal inferences draws from correlation.\" A lot of it depends on how well the controls were done and what follow-up research was done. Thongs like identifying and confirming mechanisms of action are a huge difference between some basic correlation and claimed causation. In the case of exposures, you can work backwards - reduce blood levels and perform tests to see if functions return. That's a lot easier than identifying people before they meet the criteria. Although that's still possible, especially when dealing with certain vocational scenarios where tests can be performed before possible exposures. reply mint2 5 hours agorootparent“Reduce blood levels and see if functions return” so like reduce smoking to see if lung cancer go into remission? Why the assumption health effects are reversible? reply toast0 5 hours agorootparentYou can certainly reduce smoking to see if lung capacity improves. I don't think it's reasonable to reduce smoking to see if that will cause an existing tumor to go into remission, for the same reason it's not reasonable to see if installing a blade guard on a saw will make a amputated finger reattach or regrow. The risk has manifested, and removing the risk factor is too late. You'd need a large study to determine if stopping smoking reduces future risk of tumors; it's not something you can determine in a single person. reply taeric 8 hours agoparentprevYou have a point in that people should continue to study this stuff. But, nobody is halting research here? Do you have thoughts on why such a strong correlation would appear other than some causal influence? To be sure, there could be other factors at play. But nobody, I mean nobody, is suggesting that lead is safe. We know it messes you up. There is evidence that it really messes you up. reply munchler 7 hours agoparentprevWould really like to hear your theory about how IQ loss causes lead poisoning. reply hackerlight 7 hours agoparentprevThis isn't any old epidemiological data mining exercise. We know that lead directly causes IQ loss in larger amounts. It should take a lot less evidence to convince us that correlations pertaining to smaller exposures are therefore causal. reply aurizon 7 hours agoparentprevThere is a basis in fact and theory that the Roman Empire fell, in part, due to Lead Acetate (AKA sugar of lead) being used as a sweetener. As far as I know, the bad effects of lead are well established, and the use of tetra ethylated lead as anti knock additive in car and aviation has endured in the face of solid science = lead is toxic. The oil and auto business bribed regulators to ignore the science, much like the tobacco bribe industry. I use the term 'bribe' in the true sense, https://en.wiktionary.org/wiki/bribe, versus the political sense where 'lobby' is preferred. https://en.wikipedia.org/wiki/Lobbying There is also the Tammany Hall origin - I will meet you in the lobby with a bag of $$, which some say has been done in Congress in those bad old days. reply malfist 8 hours agoparentprevI'm sure the scientists considered \"correlation does not equal causation\" before publishing. reply iraqmtpizza 8 hours agorootparentI know that the people still trumpeting about the relationship between college degrees and income definitely don't. Ahem, Destiny, ahem, ahem reply TriangleEdge 8 hours agoprevThere's still lead pipes for drinking water in the USA. [1] [1] https://www.whitehouse.gov/briefing-room/statements-releases... reply cooper_ganglia 8 hours agoparentIn 1986, the U.S. defined “lead-free pipes” as having a lead-content of 8% or less. This wasn’t lowered to 0.25% until 2011! https://www.epa.gov/sdwa/use-lead-free-pipes-fittings-fixtur... reply PlunderBunny 4 hours agoprevI cycled to and from school (and later University) on an expressway in New Zealand for about 10 years in my pre-teens and teenage years. Apparently the lead levels in the gutters of our main roads were so high that \"some countries would consider mining it\" (obviously not actually). reply thenickdude 1 hour agoparentCody's Lab has a video of refining platinum from roadside dust: (due to catalytic converters) https://www.youtube.com/watch?v=v5GPWJPLcHg reply newsclues 8 hours agoprevWhat is the modern unknown danger doing the same thing today? reply sn41 8 hours agoparentMy bet would be microplastics. reply karmelapple 7 hours agorootparentCancer is rising in younger people. [1] The proliferation of plastic seems like a pretty clear correlation, although I'm sure causation won't be easy to prove. From [1]: > ... younger adults to be the only age group with an increase in overall cancer incidence between 1995 and 2020—the rate has risen by 1% to 2% each year during that time period. ... > Colorectal cancer, while still overwhelmingly a disease that affects older people, is now the leading cause of cancer death in men younger than 50 and second in women in that age group. The numbers have been rising steadily in people 55 and younger since the mid-1990s, according to the ACS. 1. https://www.yalemedicine.org/news/cancer-in-younger-people-o.... reply speedylight 8 hours agoparentprevI think there’s too many to count. reply JoeMattiello 7 hours agoparentprevMicroplastics. Plastic food containers. Birth control in general and SSRIs and birth control in the water. PFAs. Seed oils. The war on fat and cholesterol in favor of sugars. reply oarfish 4 hours agorootparentWhats wrong with seed oils? As far as i know, replacing saturated fats with polyunsaturated fats (such as found in seed oil) is only beneficial. reply mfkp 3 hours agorootparentSeed oils generally refers to things like sunflower oil, soybean oil, safflower oil, vegetable oil, etc. The best mass produced oils are probably olive and avocado. This video has a pretty good breakdown of the science: https://youtu.be/IDZmXzAMmwI reply EasyMark 4 hours agorootparentprevIt's a pretty recent (past 10 years or so?) internet phenomenon to blame a lot of stuff on them with next to zero proof. reply Der_Einzige 7 hours agorootparentprevI talk about many of these things and get labeled as a \"Conspiracy theorist\", particularly when you start observing that these are all known endocrine disruptors, that testosterone and sperm quantities are in free-fall among men world wide, and that precocious puberty is becoming normalized in girls simultaneously. But the moment that you bring up how there might be a environmental/biological component to the extremely rapid increase in LGBT identification, particularly in the youth, they think you're Alex Jones. Sorry, no. I'm simply someone who internalized the message that movies like \"Children of Men\" were sending. reply oarfish 4 hours agorootparent> testosterone and sperm quantities are in free-fall among men world wide The problem is that this isn't true once you factor out disease status (such as obesity, which is rising). reply seoulmetro 5 hours agorootparentprevI was with you until you started saying that the trend towards LGBT identification wasn't just people's emotions. People think you're Alex Jones because it's a very, very dumb reach. Punk wasn't something in the blood. Recent LGBT craziness is just a trend like any other. reply technotony 8 hours agoparentprevSugar reply kibwen 8 hours agoparentprevMemes. Recommendation algorithms. iPads as babysitters. reply GalaxyNova 8 hours agoparentprevphtalates reply Havoc 9 hours agoprevHow? I would have expected it to affect everyone though to varying degrees reply dboreham 9 hours agoparentThis is the actual paper: https://www.pnas.org/doi/abs/10.1073/pnas.2118631119 and surprisingly the MSM article about it pretty much just repeats the abstract and uses the same headline. I think that the headline refers to the fact that 1/2 the current US population were children before leaded gasoline was phased out. reply ClumsyPilot 8 hours agoprevI noticed a disturbing patter - when we talk about costs of environmental laws, they are in dollars, but when we talk about damage, we never put a dollar amount on it. What is the total damage to the economy of making such a colossal number of people less smart, for the entire duration of their life? The cost must be colossal, in trillions. Imagine if 1% drop in IQ costs us 1% loss of GDP, in that case m, over the past decades, we have suffered more damage that the entire GDP of USA. Someone should be paying that! And someone in this thread has rightly pointed out, will microplastics be the same? reply downrightmike 7 hours agoparentLow literacy rates end up costing Americans up to $2.2 trillion every year. Nationwide, on average, 79% of U.S. adults are literate in 2022. 21% of adults in the US are illiterate in 2022. 54% of adults have a literacy below sixth-grade level. 21% of Americans 18 and older are illiterate in 2022. https://www.crossrivertherapy.com/research/literacy-statisti... reply mglikesbikes 4 hours agorootparentIf the US was an organism, what role would an under-educated X% play? You say it’s a “cost” but equal education obviously isn’t a priority in our country and I can’t help but wonder who benefits from such a high percentage of illiteracy (this is semi-rhetorical). I genuinely don’t think we’re tracking the cost appropriately nor implementing solutions with enough urgency; 10-15 years down the road this empire of ours is going to be far out-classed intellectually by countries that actually implement policy for the equitable benefit of everyone. If I’m off base I’d appreciate an education on this topic as it’s quite gross to think about our elite institutions being proponents of eugenics in the past[1] and how that likely spilled over into the world-building we did after WWII (Kissinger etc). Or put another way, how do we map “AI is going to save everyone” rhetoric to how history has actually gone, and what incentive do existing power structures have to change? [1] https://youtu.be/FkKPsLxgpuY?t=1095 reply renewiltord 7 hours agoparentprevhttps://www.epa.gov/system/files/documents/2023-12/epa_scghg... P. 78 We do. Your observation likely reflects a data source selection error on your part. reply peter-m80 1 hour agoprevwell, that explains a lot reply jerlam 7 hours agoprevThis article is from 2022. I'm guessing it's posted now as a response to the Kansas Attorney General's recent Twitter post that questions the benefit of removing lead pipes: \"Biden wants to replace lead pipes ... all for benefits that may be entirely speculative.\" https://twitter.com/badmedicaltakes/status/17662226122485760... reply nerdponx 6 hours agoparentOh, so the Boston Lead Replacement Incentive program is based on benefits that may be entirely speculative? Might I encourage you to sample our finest tap water, from our finest century-old lead pipes? Oh, maybe another day then. reply jncfhnb 5 hours agorootparentFWIW, while I support the program, most of our pipes, including all the water mains, are fine. The water supply is fine. It’s the private lines to specific buildings that are old and problematic. reply Jemm 8 hours agoprevAviation still uses leaded gas. They call it low lead but it is just regular leaded AV Gas. reply duskwuff 8 hours agoparentNot for much longer! The FAA has approved a replacement for 100LL, and intends to phase out leaded gasoline by 2030. https://www.faa.gov/unleaded reply leetrout 8 hours agoparentprevAVGAS 100LL has half the lead content of AVGAS 100. reply samatman 7 hours agoparentprevWhile this is true, commercial aviation is largely jet powered, and jets do not, and have not, used avgas. They use jet fuel, a blend of kerosene and gasoline. Tetraethyl lead is an anti-knock compound, and turbines don't knock. It's good that it's being phased out, but saying \"aviation uses leaded gas\" leads people to think that commercial aviation uses leaded gas, which hasn't been the case for longer than most of us have been alive. Turboprops are only 8% of planes flown commercially[0] and the category is dominated by small planes, avgas consumption vs. jetfuel is on the order of 2-4%, as a napkin estimate. [0]: https://www.statista.com/statistics/573231/aviation-industry... reply renewiltord 8 hours agoparentprevYeah, regulations are written in blood. Chesterton's fence applies. reply Solvency 8 hours agoprevMeanwhile we're still allowing hundreds of thousands of small + personal aircraft to use leaded fuel in 2024. In fact 5 are flying overhead right now. It's great living in SoCal! Everyone's focused on chemtrails and ignoring the fact that leaded gas is jus spewing into the air above us ~12 hours a day, ~300 days a year. reply downrightmike 7 hours agoparentEvery home in the area is getting a consistent powder coat of lead. reply yieldcrv 8 hours agoprev2.6 points on average More for people born earlier decades like 1960s and 1970s I wouldn't be surprised it we find out that microplastics or a different substance continued the trend for 1990s and 2000s babies reply xyst 7 hours agoprevI wonder how many of them are actively registered Republicans, or actively engage in fringe conspiracy theories. reply steve_adams_86 6 hours agoparentWhy not actively registered democrats or independents? Why is the distinction necessary here? reply jncfhnb 5 hours agorootparentThey are proposing a causal relationship between IQ and political affiliation. reply jncfhnb 5 hours agoparentprevIt’s probably most visible in urban communities which are left leaning due to density of exhaust reply jgerrish 8 hours agoprevCan you hear that? reply porkbeer 8 hours agoprevnext [4 more] [flagged] nonsensikal 8 hours agoparentof course you cant spell reply tjpnz 5 hours agorootparentProbably wrote it on a dentist chair. reply quickslowdown 8 hours agoparentprevNo, it doesn't reply yieldcrv 8 hours agoprevMaybe there could be some research in repairing or enhancing our minds specifically from neural pathway damage from lead? Is anyone pursuing that reply 0x00_NULL 9 hours agoprevWhat is this nonsense? This whole news article doesn’t even link to the study. So ridiculous. That’s gotta break some journalism rule of some sort. It might be this one: https://pubmed.ncbi.nlm.nih.gov/35254913/ reply defrost 9 hours agoparentMarch 7, 2022 - Proceedings of the National Academy of Sciences Half of US population exposed to adverse lead levels in early childhood https://www.pnas.org/doi/10.1073/pnas.2118631119 Choice Quote: Significance We estimate population-level effects on IQ loss and find that lead is responsible for the loss of 824,097,690 IQ points as of 2015. ( The link given here on HN is to an NBC news article from March 8, 2022, about that PNAS publication. ) reply wrycoder 9 hours agorootparent\"The average lead-linked loss in cognitive ability was 2.6 IQ points per person as of 2015.\" reply defrost 8 hours agorootparentOr, given at least half the US population was non urbannon built up areas for the bulk of the leaded gasoline years, it might something like \"half the population lost at least 5 IQ points\". Distributions are often more interesting than averages or medians alone. reply astrange 8 hours agorootparentIQ points are normally distributed results of IQ tests, so the average is the median and you already know the distribution. I guess they should say how the SD changed though. Reporting a loss of theoretical IQ points is weird terminology since IQ people want you to believe it's measuring something that exists called \"g\", but then they don't report this result as a loss of \"g\". Of course they also happily claim every other test that produces a number like the SAT is also a true measurement of your intelligence and you're doomed to never be able to increase it. Basically they just believe any statement as long as it's got numbers in it and it says something they like. reply defrost 8 hours agorootparentThe geographic distribution of the population affected by lead - it seems clear that car dense populations would be more affected that open sparse rural populations, and so the question of distribution of exposure levels across humans becomes of interest. reply jeffbee 9 hours agorootparentprevAbout the same cognitive impairment associated with a mild and fully resolved case of COVID-19. reply karashi 7 hours agorootparentI find it hard to understand why cognitive impairment due to COVID-19 is not discussed more. Cognitive impairment is a terrifying idea but I think it’s important to balance the risk of COVID-associated cognitive impairment with the risk of cognitive decline that would result from whatever social isolation would be necessary to avoid COVID. Is cognitive decline due to COVID really comparable to that of lead exposure? If so, I’d really appreciate more concrete information to put the risk into context. reply ruined 8 hours agorootparentprevfor anyone else looking for that citation i presume jeffbee referring to https://www.nejm.org/doi/full/10.1056/NEJMoa2311330 reply karashi 8 hours agorootparentIt’s worth noting the conclusion includes “Longer-term persistence of cognitive deficits and any clinical implications remain uncertain.” reply userbinator 8 hours agoprevWhenever I see a headline like that, the thought that comes to mind is \"and boosted the IQ of the other half?\" It's interesting how none of the comments here have brought up the \"but IQ doesn't mean anything\" talking point yet. I wonder why. reply po 7 hours agoparentIQ does measure something related to the brain's cognitive ability. The debate is if it's measuring \"intelligence\" and what the definition of that is, not that it doesn't mean anything. IQ tests are highly reliable in that you generally get the same scores when you retest. A movement from one year to the next relative to the population at whole is significant. reply alex_young 8 hours agoparentprevWhat would the mechanism for boosting IQ be? Seems like they are saying half of the population was more exposed to it. reply Jedd 8 hours agorootparentPossibly parent is assuming that because IQ is a distribution with 100 at the center, any movement results in a shift of that median. Potentially you could interpret this as suggesting a tacit (mathematical) increase elsewhere, but I don't think the math works out like that. reply alex_young 5 hours agorootparentIQ is indeed standardized to 100, however the underlying results are measurably different over time. There is in fact an upward trend to IQ scores: When IQ tests are revised, they are again standardized using a new sample of test-takers, usually born more recently than the first; the average result is set to 100. When the new test subjects take the older tests, in almost every case their average scores are significantly above 100. https://en.m.wikipedia.org/wiki/Flynn_effect reply sva_ 8 hours agorootparentprevProbably because IQ is based on 100=average, so if the average moves down and you stay constant, yours would move up. reply fnordpiglet 8 hours agorootparentMedian reply meowface 7 hours agorootparentFor IQ, the average is also the median. reply fnordpiglet 7 hours agorootparentThat’s the idealized outcome but the shape of actual populations doesn’t yield a perfect normal distribution. But you’re right the 100 IQ centering assumes a perfect normal distribution, so as such it is intended to both be calibrated to the average and the median. reply profsummergig 9 hours agoprev [–] Various categories of gasoline still are allowed to have lead in them. E.g. jet fuel. Possibly because they have a more-powerful lobby lobbying for them. reply JonathonW 9 hours agoparentJet fuel does not use lead. I'm also pretty sure jet fuel never used lead; it has no reason to (jet engines are not particularly picky about what they burn-- jet fuel is basically kerosene). Aviation gasoline does use lead-- it's used there to prevent predetonation (engine knock), which is bad for a car engine but really bad in something that's supposed to be keeping you in the air. Replacing it is an ongoing effort, but it's taking a while-- largely because of certification requirements, because it's pretty important to make sure that your new gasoline formulation isn't going to, again, make that thing that's keeping you in the air fail. And there are a lot of older GA airplanes out there that would need recertification for a new fuel. reply mistrial9 8 hours agorootparentexcept there are other formulations that can achieve that result, and \"knocking\" was not really the primary driver in the first place -- its a con. see \"The Secret History of Lead\" https://www.thenation.com/article/archive/secret-history-lea... reply totallyabstract 9 hours agoparentprevJet fuel has no lead, it's basically kerosene. Avgas for reciprocating engines (basically only small general aviation planes/helicopters) currently contains lead but is moving to be lead free in the US. reply bglazer 9 hours agoparentprev [–] Jet fuel isn't leaded. It’s piston engine general aviation aircraft that use leaded gas. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Exposure to leaded gasoline before its 1996 ban has decreased the IQ of about half of the U.S. population, resulting in an estimated loss of 824 million IQ points.",
      "Children born in the 1960s and 1970s were significantly impacted by lead exposure, which is also associated with heart and kidney issues and disproportionately affects Black individuals.",
      "Lead exposure, causing long-term health problems like cardiovascular disease and dementia, remains a critical public health issue, particularly in disadvantaged communities with tainted water sources."
    ],
    "commentSummary": [
      "Lead exposure has a detrimental effect on IQ levels, historically seen in leaded gasoline and pipes in the US, with Thomas Midgley Jr. known for advocating leaded gasoline despite its risks.",
      "The discussion explores the possible connection between lead exposure and criminal behavior, impacting health and cognitive functions, while efforts are in place to transition to lead-free options.",
      "Despite progress, challenges persist in comprehending and addressing the consequences of lead exposure, emphasizing the importance of continued research and action."
    ],
    "points": 145,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1710109305
  },
  {
    "id": 39659729,
    "title": "Exploring Education and Career in Isaac Asimov's 'Profession'",
    "originLink": "https://www.abelard.org/asimov.php",
    "originBody": "We use cookies to improve your experience on our site and to show you relevant advertising. To find out more, read our updated privacy policy and cookie policy.Understood, Hide this message and remember my decision Privacy policy and cookie policy ssite map Profession by Isaac Asimov This page helpful? Like it ! Share it !Isaac Asimov’s Profession is an allegorical description of the manner in which education currently functions in our primitive western societies – abelard [1] Profession, copyright ©1957 by Street and Smith Publications, Inc., from ISAAC ASIMOV: THE COMPLETE STORIES OF VOL. 1 by Isaac Asimov. Used by permission of Doubleday, a division of Random House, Inc. For on-line information about other Random House, Inc. books and authors, see the Internet Web site at http://www.randomhouse.com. Another sci-fi short story at abelard.org: And Then There Were None by Eric Frank Russellpdf version to download [657 kB, opens in new tab/window.] If you wish to customise your browser, to change the text size or see this page in black on white, click here for instructions on what to do. George Platen could not conceal the longing in his voice. It was too much to suppress. He said, “Tomorrow’s 1 May. Olympics!” He rolled over on his stomach and peered over the foot of his bed at his roommate. Didn’t he feel it, too? Didn’t this make some impression on him? George’s face was thin and had grown a trifle thinner in the nearly year and a half that he had been at the House. His figure was slight but the look in his blue eyes was as intense as it had ever been, and right now there was a trapped look in the way his fingers curled against the bedspread. George’s roommate looked up briefly from his book and took the opportunity to adjust the light-level of the stretch of wall near his chair. His name was Hali Omani and he was a Nigerian by birth. His dark brown skin and massive features seemed made for calmness, and mention of the Olympics did not move him. “I know, George.” George owed much to Hali’s patience and kindness when it was needed, but even patience and kindness could be overdone. Was this a time to sit there like a statue built of some dark, warm wood? George wondered if he himself would grow like that after ten years here and rejected the thought violently. No! He said defiantly, “I think you’ve forgotten what May means.” The other said,“I remember very well what it means. It means nothing! You’re the one who’s forgotten that. May means nothing to you, George Platen, and,' he added softly, “It means nothing to me, Hali Omani.” George said, “The ships are coming in for recruits. By June, thousands and thousands will leave with millions of men and women heading for any world you can name, and all that means nothing?” “Less than nothing. What do you want me to do about it, anyway?” Omani ran his finger along a difficult passage in the book he was reading and his lips moved soundlessly. George watched him. Damn it, he thought, yell scream; you can do that much. Kick at me, do anything. It was only that he wanted not to be so alone in his anger. He wanted not to be the only one so filled with resentment, not to be the only one dying a slow death. It was better those first weeks when the Universe was a small shell of vague light and sound pressing down upon him. It was better before Omani had wavered into view and dragged him back to a life that wasn’t worth living. Omani! He was old! He was at least thirty. George thought: Will I be like that at thirty? Will I be like that in twelve years? And because he was afraid he might be, he yelled at Omani, “Will you stop reading that fool book?” Omani turned a page and read on a few words, then lifted his head with its skullcap of crisply curled hair and said, “What?” “What good does it do you to read the book?” He stepped forward, snorted “More electronics,” and slapped it out of Omani’s hands. Omani got up slowly and picked up the book. He smoothed a crumpled page without visible rancor. “Call it the satisfaction of curiosity,” he said. “I understand a little of it today, perhaps a little more tomorrow. That’s a victory in a way.” “A victory. What kind of a victory? Is that what satisfies you in life? To get to know enough to be a quarter of a Registered Electronician by the time you’re sixty-five?” “Perhaps by the time I’m thirty-five.” “And then who’ll want you? Who’ll use you? Where will you go?” “No one. No one. Nowhere. I’ll stay here and read other books.” “And that satisfies you? Tell me! You’ve dragged me to class. You’ve got me to reading and memorizing, too. For what? There’s nothing in it that satisfies me.” “What good will it do you to deny yourself satisfaction?” “It means I’ll quit the whole farce. I’ll do as I planned to do in the beginning before you dovey-lovied me out of it. I’m going to force them to – to –” Omani put down his book. He let the other run down and then said, “To what, George?” “To correct a miscarriage of justice. A frame-up. I’ll get that Antonelli and force him to admit he – he –” Omani shook his head. “Everyone who comes here insists it’s a mistake. I thought you’d passed that stage.” “Don’t call it a stage,” said George violently. “In my case, it’s a fact. I’ve told you –”� “You’ve told me, but in your heart you know no one made any mistake as far as you were concerned.” “Because no one will admit it? You think any of them would admit a mistake unless they were forced to? – Well: I’ll force them.” It was May that was doing this to George; it was Olympics month. He felt it bring the old wildness back and he couldn’t stop it. He didn’t want to stop it. He had been in danger of forgetting. He said, “I was going to be a Computer Programmer and I can be one. I could be one today, regardless of what they say analysis shows.” He pounded his mattress. “They’re wrong. They must be.” “The analysts are never wrong.” “They must be. Do you doubt my intelligence?” “Intelligence hasn’t one thing to do with it. Haven’t you been told that often enough? Can’t you understand that?” George rolled away, lay on his back, and stared somberly at the ceiling. “What did you want to be, Hali?” “I had no fixed plans. Hydroponicist would have suited me, I suppose.” “Did you think you could make it?” “I wasn’t sure.” George had never asked personal questions of Omani before. It struck him as queer, almost unnatural, that other people had had ambitions and ended here. Hydroponicist! He said, “Did you think you’d make this?” “No, but here I am just the same.” “And you’re satisfied. Really, really satisfied. You’re happy. You love it. You wouldn’t be anywhere else.” Slowly, Omani got to his feet. Carefully, he began to unmake his bed. He said, “George, you’re a hard case. You’re knocking yourself out because you won’t accept the facts about yourself. George, you’re here in what you call the House, but I’ve never heard you give it its full title. Say it, George, say it. Then go to bed and sleep this off.” George gritted his teeth and showed them. He choked out, “No!” “Then I will,” said Omani, and he did. He shaped each syllable carefully. George was bitterly ashamed at the sound of it. He turned his head away. For most of the first eighteen years of his life, George Platen had headed firmly in one direction, that of Registered Computer Programmer. There were those in his crowd who spoke wisely of Spationautics, Refrigeration Technology, Transportation Control, and even Administration. But George held firm. He argued relative merits as vigorously as any of them, and why not? Education Day loomed ahead of them and was the great fact of their existence. It approached steadily, as fixed and certain as the calendar – the first day of November of the year following one’s eighteenth birthday. After that day, there were other topics of conversation. One could discuss with others some detail of the profession, or the virtues of one’s wife and children, or the fate of one’s space-polo team, or one’s experiences in the Olympics. Before Education Day, however, there was only one topic that unfailingly and unwearyingly held everyone’s interest, and that was Education Day. “What are you going for? Think you’ll make it? Heck, that’s no good. Look at the records; quota’s been cut. Logistics now –”� Or Hypermechanics now – Or Communications now – Or Gravitics now – Especially Gravitics at the moment. Everyone had been talking about Gravitics in the few years just before George’s Education Day because of the development of the Gravitic power engine. Any world within ten light-years of a dwarf star, everyone said, would give its eyeteeth for any kind of Registered Gravitics Engineer. The thought of that never bothered George. Sure it would; all the eyeteeth it could scare up. But George had also heard what had happened before in a newly developed technique. Rationalization and simplification followed in a flood. New models each year; new types of gravitic engines; new principles. Then all those eyeteeth gentlemen would find themselves out of date and superseded by later models with later educations. The first group would then have to settle down to unskilled labor or ship out to some backwoods world that wasn’t quite caught up yet. Now Computer Programmers were in steady demand year after year, century after century. The demand never reached wild peaks; there was never a howling bull market for Programmers; but the demand climbed steadily as new worlds opened up and as older worlds grew more complex. He had argued with Stubby Trevelyan about that constantly. As best friends, their arguments had to be constant and vitriolic and, of course, neither ever persuaded or was persuaded. But then Trevelyan had had a father who was a Registered Metallurgist and had actually served on one of the Outworlds, and a grandfather who had also been a Registered Metallurgist. He himself was intent on becoming a Registered Metallurgist almost as a matter of family right and was firmly convinced that any other profession was a shade less than respectable. “There’ll always be metal, he said, “and there’s an accomplishment in molding alloys to specification and watching structures grow. Now what’s a Programmer going to be doing? Sitting at a coder all day long, feeding some fool mile-long machine.” Even at sixteen, George had learned to be practical. He said simply, “There’ll be a million Metallurgists put out along with you.” “Because it’s good. A good profession. The best.” “But you get crowded out, Stubby. You can be way back in line. Any world can tape out its own Metallurgists, and the market for advanced Earth models isn’t so big. And it’s mostly the small worlds that want them. You know what per cent of the turn-out of Registered Metallurgists get tabbed for worlds with a Grade A rating. I looked it up. It’s just 13.3 per cent. That means you’ll have seven chances in eight of being stuck in some world that just about has running water. You may even be stuck on Earth; 2.3 per cent are.” Trevelyan said belligerently, “There’s no disgrace in staying on Earth. Earth needs technicians, too. Good ones.” His grandfather had been an Earth-bound Metallurgist, and Trevelyan lifted his finger to his upper lip and dabbed at an as yet nonexistent mustache. George knew about Trevelyan’s grandfather and, considering the Earth-bound position of his own ancestry, was in no mood to sneer. He said diplomatically, “No intellectual disgrace. Of course not. But it’s nice to get into a Grade A world, isn’t it? “Now you take Programmers. Only the Grade A worlds have the kind of computers that really need first-class Programmers so they’re the only ones in the market. And Programmer tapes are complicated and hardly any one fits. They need more Programmers than their own population can supply. It’s just a matter of statistics. There’s one first-class Programmer per million, say. A world needs twenty and has a population of ten million, they have to come to Earth for five to fifteen Programmers. Right? “And you know how many Registered Computer Programmers went to Grade A planets last year? I’ll tell you. Every last one. If you’re a Programmer, you’re a picked man. Yes, sir.” Trevelyan frowned. “If only one in a million makes it, what makes you think you’ll make it?” George said guardedly, “I’ll make it.” He never dared tell anyone; not Trevelyan; not his parents; of exactly what he was doing that made him so confident. But he wasn’t worried. He was simply confident (that was the worst of the memories he had in the hopeless days afterward). He was as blandly confident as the average eight-year-old kid approaching Reading Day – that childhood preview of Education Day. Of course, Reading Day had been different. Partly, there was the simple fact of childhood. A boy of eight takes many extraordinary things in stride. One day you can’t read and the next day you can. That’s just the way things are. Like the sun shining. And then not so much depended upon it. There were no recruiters just ahead, waiting and jostling for the lists and scores on the coming Olympics. A boy or girl who goes through the Reading Day is just someone who has ten more years of undifferentiated living upon Earth’s crawling surface; just someone who returns to his family with one new ability. By the time Education Day came, ten years later, George wasn’t even sure of most of the details of his own Reading Day. Most clearly of all, he remembered it to be a dismal September day with a mild rain falling. (September for Reading Day; November for. Education Day; May for Olympics. They made nursery rhymes out of it.) George had dressed by the wall lights, with his parents far more excited than he himself was. His father was a Registered Pipe Fitter and had found his occupation on Earth. This fact had always been a humiliation to him, although, of course, as anyone could see plainly, most of each generation must stay on Earth in the nature of things. There had to be farmers and miners and even technicians on Earth. It was only the late-model, high-specialty professions that were in demand on the Outworlds, and only a few millions a year out of Earth’s eight billion population could be exported. Every man and woman on Earth couldn’t be among that group. But every man and woman could hope that at least one of his children could be one, and Platen, Senior, was certainly no exception. It was obvious to him (and, to be sure, to others as well) that George was notably intelligent and quick-minded. He would be bound to do well and he would have to, as he was an only child. If George didn’t end on an Outworld, they would have to wait for grandchildren before a next chance would come along, and that was too far in the future to be much consolation. Reading Day would not prove much, of course, but it would be the only indication they would have before the big day itself. Every parent on Earth would be listening to the quality of reading when his child came home with it; listening for any particularly easy flow of words and building that into certain omens of the future. There were few families that didn’t have at least one hopeful who, from Reading Day on, was the great hope because of the way he handled his trisyllabics. Dimly, George was aware of the cause of his parents’ tension, and if there was any anxiety in his young heart that drizzly morning, it was only the fear that his father’s hopeful expression might fade out when he returned home with his reading. The children met in the large assembly room of the town’s Education Hall. All over Earth, in millions of local halls, throughout that month, similar groups of children would he meeting. George felt depressed by the grayness of the room and by the other children, strained and stiff in unaccustomed finery. Automatically, George did as all the rest of the children did. He found the small clique that represented the children on his floor of the apartment house and joined them. Trevelyan, who lived immediately next door, still wore his hair childishly long and was years removed from the sideburns and thin, reddish mustache that he was to grow as soon as he was physiologically capable of it. Trevelyan (to whom George was then known as Jawjee) said, “Bet you’re scared.” “I am not,’ said George. Then, confidentially, “My folks got a hunk of printing up on the dresser in my room, and when I come home, I’m going to read it for them.” (George’s main suffering at the moment lay in the fact that he didn’t quite know where to put his hands. He had been warned not to scratch his head or rub his ears or pick his nose or put his hands into his pockets. This eliminated almost every possibility.) Trevelyan put his hands in his pockets and said, “My father isn’t worried.” Trevelyan, Senior, had been a Metallurgist on Diporia for nearly seven years, which gave him a superior social status in his neighborhood even though he had retired and returned to Earth. Earth discouraged these re-immigrants because of population problems, but a small trickle did return. For one thing the cost of living was lower on Earth, and what was a trifling annuity on Diporia, say, was a comfortable income on Earth. Besides, there were always men who found more satisfaction in displaying their success before the friends and scenes of their childhood than before all the rest of the Universe besides. Trevelyan, Senior further explained that if he stayed on Diporia, so would his children, and Diporia was a one-spaceship world. Back on Earth, his kids could end up anywhere, even Novia. Stubby Trevelyan had picked up that item early. Even before Reading Day, his conversation was based on the carelessly assumed fact that his ultimate home would be in Novia. George, oppressed by thoughts of the other’s future greatness and his own small-time contrast, was driven to belligerent defense at once. “My father isn’t worried either. He just wants to hear me read because he knows I’ll be good. I suppose your father would just as soon not hear you because he knows you’ll be all wrong.” “I will not be all wrong. Reading is nothing. On Novia, I’ll hire people to read to me.” “Because you won’t be able to read yourself, on account of you’re dumb!” “Then how come I’ll be on Novia?” And George, driven, made the great denial, “Who says you’ll be on Novia? Bet you don’t go anywhere.” Stubby Trevelyan reddened. “I won’t be a Pipe Fitter like your old man.” “Take that back, you dumbhead.” “You take that back.” They stood nose to nose, not wanting to fight but relieved at having something familiar to do in this strange place. Furthermore, now that George had curled his hands into fists and lifted them before his face, the problem of what to do with his hands was, at least temporarily, solved. Other children gathered round excitedly. But then it all ended when a woman’s voice sounded loudly over the public address system. There was instant silence everywhere. George dropped his fists and forgot Trevelyan. “Children,” said the voice, “we are going to call out your names. As each child is called, he or she is to go to one of the men waiting along the side walls. Do you see them? They are wearing red uniforms so they will be easy to find. The girls will go to the right. The boys will go to the left. Now look about and see which man in red is nearest to you –”� George found his man at a glance and waited for his name to be called off. He had not been introduced before this to the sophistications of the alphabet and the length of time it took to reach his own name grew disturbing. The crowd of children thinned; little rivulets made their way to each of the red-clad guides. When the name ‘George Platen’ was finally called, his sense of relief was exceeded only by the feeling of pure gladness at the fact that Stubby Trevelyan still stood in his place, uncalled. George shouted back over his shoulder as he left, “Yay, Stubby, maybe they don’t want you.” That moment of gaiety quickly left. He was herded into a line and directed down corridors in the company of strange children. They all looked at one another, large-eyed and concerned, but beyond a snuffling, “Quitcher pushing” and “Hey, watch out” there was no conversation. They were handed little slips of paper which they were told must remain with them. George stared at his curiously. Little black marks of different shapes. He knew it to be printing but how could anyone make words out of it? He couldn’t imagine. He was told to strip; he and four other boys who were all that now remained together. All the new clothes came shucking off and four eight-year-olds stood naked and small, shivering more out of embarrassment than cold. Medical technicians came past, probing them, testing them with odd instruments, pricking them for blood. Each took the little cards and made additional marks on them with little black rods that produced the marks, all neatly lined up, with great speed. George stared at the new marks, but they were no more comprehensible than the old. The children were ordered back into their clothes. They sat on separate little chairs then and waited again. Names were called again and ‘George Platen’ came third. He moved into a large room, filled with frightening instruments with knobs and glassy panels in front. There was a desk in the very center, and behind it a man sat, his eyes on the papers piled before him. He said, “George Platen?” “Yes, sir,” said George, in a shaky whisper. All this waiting and all this going here and there was making him nervous. He wished it were over. The man behind the desk said, “I am Dr Lloyd, George. How are you?” The doctor didn’t look up as he spoke. It was as though he had said those words over and over again and didn’t have to look up any more. “I’m all right.” “Are you afraid, George?” “N – no, sir,” said George, sounding afraid even in his own ears. “That’s good,” said the doctor, “because there’s nothing to be afraid of you know. Let’s see, George. It says here on your card that your father is named Peter and that he’s a Registered Pipe Fitter and your mother is named Amy and is a Registered Home Technician. Is that right?” “Y – yes, sir.” “And your birthday is 13 February,and you had an ear infection about a year ago. Right?” “Yes, sir.” “Do you know how I know all these things?” “It’s on the card, I think, sir.” “That’s right.” The doctor looked up at George for the first time and smiled. He showed even teeth and looked much younger than George’s father. Some of George’s nervousness vanished. The doctor passed the card to George. “Do you know what all those things there mean, George?” Although George knew he did not he was startled by the sudden request into looking at the card as though he might understand now through some sudden stroke of fate. But they were just marks as before and he passed the card back. “No, sir.” “Why not?” George felt a sudden pang of suspicion concerning the sanity of this doctor. Didn’t he know why not? George said, “I can’t read, sir.” “Would you like to read?” “Yes, sir.” “Why, George?” George stared, appalled. No one had ever asked him that. He had no answer. He said falteringly, “I don’t know, sir.” “Printed information will direct you all through your life. There is so much you’ll have to know even after Education Day. Cards like this one will tell you. Books will tell you. Television screens will tell you. Printing will tell you such useful things and such interesting things that not being able to read would be as bad as not being able to see. Do you understand?” “Yes, sir.” “Are you afraid, George?” “No, sir.” “Good. Now I’ll tell you exactly what we’ll do first. I’m going to put these wires on your forehead just over the corners of your eyes. They’ll stick there but they won’t hurt at all. Then, I’ll turn on something that will make a buzz. It will sound funny and it may tickle you, but it won’t hurt. Now if it does hurt, you tell me, and I’ll turn it off right away, but it won’t hurt. All right?” George nodded and swallowed. “Are you ready?” George nodded. He closed his eyes while the doctor busied himself. His parents had explained this to him. They, too, had said it wouldn’t hurt, but then there were always the older children. There were the ten- and twelve-year-olds who howled after the eight-year-olds waiting for Reading Day, “Watch out for the needle.” There were the others who took you off in confidence and said, “They got to cut your head open. They use a sharp knife that big with a hook on it,” and so on into horrifying details. George had never believed them but he had had nightmares, and now he closed his eyes and felt pure terror. He didn’t feel the wires at his temple. The buzz was a distant thing, and there was the sound of his own blood in his ears, ringing hollowly as though it and he were in a large cave. Slowly he chanced opening his eyes. The doctor had his back to him. From one of the instruments a strip of paper unwound and was covered with a thin, wavy purple line. The doctor tore off pieces and put them into a slot in another machine. He did it over and over again. Each time a little piece of film came out which the doctor looked at. Finally, he turned toward George with a queer frown between his eyes. The buzzing stopped. George said breathlessly, “Is it over?” The doctor said, “Yes,” but he was still frowning. “Can I read now?’ asked George. He felt no different. The doctor said, “What?” then smiled very suddenly and briefly. He said, “It works fine, George. You’ll be reading in fifteen minutes. Now we’re going to use another machine this time and it will take longer. I’m going to cover your whole head, and when I turn it on you won’t be able to see or hear anything for a while, but it won’t hurt. Just to make sure I’m going to give you a little switch to hold in your hand. If anything hurts, you press the little button and everything shuts off. All right?” In later years, George was told that the little switch was strictly a dummy; that it was introduced solely for confidence. He never did know for sure, however, since he never pushed the button. A large smoothly curved helmet with a rubbery inner lining was placed over his head and left there. Three or four little knobs seemed to grab at him and bite into his skull, but there was only a little pressure that faded. No pain. The doctor’s voice sounded dimly. “Everything all right, George?” And then, with no real warning, a layer of thick felt closed down all about him. He was disembodied, there was no sensation, no universe, only himself and a distant murmur at the very ends of nothingness telling him something – telling him – telling him – He strained to hear and understand but there was all that thick felt between. Then the helmet was taken off his head, and the light was so bright that it hurt his eyes while the doctor’s voice drummed at his ears. The doctor said, “Here’s your card, George. What does it say?” George looked at his card again and gave out a strangled shout. The marks weren’t just marks at all. They made up words. They were words just as clearly as though something were whispering them in his ears. He could hear them being whispered as he looked at them. “What does it say, George?” “It says – it says – ‘Platen, George. Born 13 February 6492 of Peter and Amy Platen in...’”� He broke off. “You can read, George,” said the doctor. “It’s all over.” “For good? I won’t forget how?” “Of course not” The doctor leaned over to shake hands gravely. “You will be taken home now.” It was days before George got over this new and great talent of his. He read for his father with such facility that Platen, Senior, wept and called relatives to tell the good news. George walked about town, reading every scrap of printing he could find and wondering how it was that none of it had ever made sense to him before. He tried to remember how it was not to be able to read and he couldn’t. As far as his feeling about it was concerned, he had always been able to read. Always. At eighteen, George was rather dark, of medium height, but thin enough to look taller. Trevelyan, who was scarcely an inch shorter, had a stockiness of build that made ‘Stubby’ more than ever appropriate, but in this last year he had grown self-conscious. The nickname could no longer be used without reprisal. And since Trevelyan disapproved of his proper first name even more strongly, he was called Trevelyan or any decent variant of that. As though to prove his manhood further, he had most persistently grown a pair of sideburns and a bristly mustache. He was sweating and nervous now, and George, who had himself grown out of ‘Jaw-jee’ and into the curt monosyllabic gutturability of ‘George,’ was rather amused by that. They were in the same large hall they had been in ten years before (and not since). It was as if a vague dream of the past had come to sudden reality. In the first few minutes George had been distinctly surprised at finding everything seem smaller and more cramped than his memory told him; then he made allowance for his own growth. The crowd was smaller than it had been in childhood. It was exclusively male this time. The girls had another day assigned them. Trevelyan leaned over to say, “Beats me the way they make you wait.” “Red tape,” said George. “You can’t avoid it.” Trevelyan said, “What makes you so damned tolerant about it?” “I’ve got nothing to worry about.” “Oh, brother, you make me sick. I hope you end up Registered Manure Spreader just so I can see your face when you do.” His somber eyes swept the crowd anxiously. George looked about, too. It wasn’t quite the system they used on the children. Matters went slower, and instructions had been given out at the start in print (an advantage over the pre-Readers). The names Platen and Trevelyan were well down the alphabet still, but this time the two knew it. Young men came out of the education rooms, frowning and uncomfortable, picked up their clothes and belongings, then went off to analysis to learn the results. Each, as he come out, would be surrounded by a clot of the thinning crowd. “How was it?” “How ’d it feel?” “Whacha think ya made?” “Ya feel any different?” Answers were vague and noncommittal. George forced himself to remain out of those clots. You only raised your own blood pressure. Everyone said you stood the best chance if you remained calm. Even so, you could feel the palms of your hands grow cold. Funny that new tensions came with the years. For instance, high-specialty professionals heading out for an Outworld were accompanied by a wife (or husband). It was important to keep the sex ratio in good balance on all worlds. And if you were going out to a Grade A world, what girl would refuse you? George had no specific girl in mind yet; he wanted none. Not now! Once he made Programmer; once he could add to his name, Registered Computer Programmer, he could take his pick, like a sultan in a harem. The thought excited him and he tried to put it away. Must stay calm. Trevelyan muttered “What’s it all about anyway? First they say it works best if you’re relaxed and at ease. Then they put you through this and make it impossible for you to be relaxed and at ease.” “Maybe that’s the idea. They’re separating the boys from the men to begin with. Take it easy, Trev.” “Shut up.” George’s turn came. His name was not called. It appeared in glowing letters on the notice board. He waved at Trevelyan. “Take it easy. Don’t let it get you.” He was happy as he entered the testing chamber. Actually happy. The man behind the desk said, “George Platen?” For a fleeting instant there was a razor-sharp picture in George’s mind of another man, ten years earlier, who had asked the same question, and it was almost as though this were the same man and he, George, had turned eight again as he had stepped across the threshold. But the man looked up and, of course, the face matched that of the sudden memory not at all. The nose was bulbous, the hair thin and stringy, and the chin wattled as though its owner had once been grossly overweight and had reduced. The man behind the desk looked annoyed. “Well?” George came to Earth. “I’m George Platen, sir.” “Say so, then. I’m Dr Zachary Antonelli, and we’re going to be intimately acquainted in a moment.” He stared at small strips of film, holding them up to the light owlishly. George winced inwardly. Very hazily, he remembered that other doctor (he had forgotten the name) staring at such film. Could these be the same? The other doctor had frowned and this one was looking at him now as though he were angry. His happiness was already just about gone. Dr Antonelli spread the pages of a thickish file out before him now and put the films’s carefully to one side. “It says here you want to be a Computer Programmer.” “Yes, doctor.” “Still do?” “Yes, sir.” “It’s a responsible and exacting position. Do you feel up to it?” “Yes, sir.” “Most pre-Educates don’t put down any specific profession. I believe they are afraid of queering it.” “I think that’s right sir.” “Aren’t you afraid of that?” “I might as well be honest, sir.” Dr Antonelli nodded, but without any noticeable lightening of his expression. “Why do you want to be a Programmer?” “It’s a responsible and exacting position as you said, sir. It’s an important job and an exciting one. I like it and I think I can do it.” Dr Antonelli put the papers away, and looked at George sourly. He said, “How do you know you like it? Because you think you’ll be snapped up by some Grade A planet?” George thought uneasily: He’s trying to rattle you. Stay calm and stay frank. He said, “I think a Programmer has a good chance, sir, but even if I were left on Earth, I know I’d like it.” (That was true enough. I’m not lying, thought George.) “All right how do you know?” He asked it as though he knew there was no decent answer and George almost smiled. He had one. He said, “I’ve been reading about Programming, sir.” “You’ve been what?” Now the doctor looked genuinely astonished and George took pleasure in that. “Reading about it, sir. I bought a book on the subject and I’ve been studying it.” “A book for Registered Programmers?” “Yes, sir.” “But you couldn’t understand it.” “Not at first. I got other books on mathematics and electronics. I made out all I could. I still don’t know much, but I know enough to know I like it and to know I can make it.” (Even his parents never found that secret cache of books or knew why he spent so much time in his own room or exactly what happened to the sleep he missed.) The doctor pulled at the loose skin under his chin. “What was your idea in doing that, son?” “I wanted to make sure I would be interested, sir.” “Surely you know that being interested means nothing. You could be devoured by a subject and if the physical make-up of your brain makes it more efficient for you to be something else, something else you will be. You know that, don’t you?” “I’ve been told that,” said George cautiously. “Well, believe it. It’s true.” George said nothing. Dr Antonelli said, “Or do you believe that studying some subject will bend the brain cells in that direction, like that other theory that a pregnant woman need only listen to great music persistently to make a composer of her child. Do you believe that?” George flushed. That had certainly been in his mind. By forcing his intellect constantly in the desired direction, he had felt sure that he would be getting a head start. Most of his confidence had rested on exactly that point. “I never-” he began, and found no way of finishing. “Well it isn’t true. Good Lord, youngster, your brain pattern is fixed at birth. It can be altered by a blow hard enough to damage the cells or by a burst blood vessel or by a tumor or by a major infection – each time, of course, for the worse. But it certainly can’t be affected by your thinking special thoughts.” He stared at George thoughtfully, then said, “Who told you to do this?” George, now thoroughly disturbed, swallowed and said, “No one, doctor. My own idea.” “Who knew you were doing it after you started?” “No one. Doctor, I meant to do no wrong.” “Who said anything about wrong? Useless is what I would say. Why did you keep it to yourself?” “I – I thought they’d laugh at me.” (He thought abruptly of a recent exchange with Trevelyan. George had very cautiously broached the thought, as of something merely circulating distantly in the very outermost reaches of his mind, concerning the possibility of learning something by ladling it into the mind by hand, so to speak, in bits and pieces. Trevelyan had hooted, “George, you’ll be tanning your own shoes next and weaving your own shirts.” He had been thankful then for his policy of secrecy.) Dr Antonelli shoved the bits of film he had first looked at from position to position in morose thought. Then he said, “Let’s get you analyzed. This is getting me nowhere.” The wires went to George’s temples. There was the buzzing. Again there came a sharp memory of ten years ago. George’s hands were clammy; his heart pounded. He should never have told the doctor about his secret reading. It was his damned vanity, he told himself. He had wanted to show how enterprising he was, how full of initiative. Instead, he had showed himself superstitious and ignorant and aroused the hostility of the doctor. (He could tell the doctor hated him for a wise guy on the make.) And now he had brought himself to such a state of nervousness, he was sure the analyzer would show nothing that made sense. He wasn’t aware of the moment when the wires were removed from his temples. The sight of the doctor, staring at him thoughtfully, blinked into his consciousness and that was that; the wires were gone. George dragged himself together with a tearing effort. He had quite given up his ambition to be a Programmer. In the space of ten minutes, it had all gone. He said dismally, “I suppose no?” “No what?” “No Programmer?” The doctor rubbed his nose and said, “You get your clothes and whatever belongs to you and go to room 15-C. Your files will be waiting for you there. So will my report.” George said in complete surprise, “Have I been Educated already? I thought this was just to –” Dr Antonelli stared down at his desk. “It will all be explained to you. You do as I say.” George felt something like panic. What was it they couldn’t tell him? He wasn’t fit for anything but Registered Laborer. They were going to prepare him for that; adjust him to it. He was suddenly certain of it and he had to keep from screaming by main force. He stumbled back to his place of waiting. Trevelyan was not there, a fact for which he would have been thankful if he had had enough self-possession to be meaningfully aware of his surroundings. Hardly anyone was left, in fact, and the few who were looked as though they might ask him questions were it not that they were too worn out by their tail-of-the-alphabet waiting to buck the fierce, hot look of anger and hate he cast at them. What right had they to be technicians and he, himself, a Laborer? Laborer! He was certain! He was led by a red-uniformed guide along the busy corridors lined with separate rooms each containing its groups, here two, there five: the Motor Mechanics, the Construction Engineers, the Agronomists – There were hundreds of specialized Professions and most of them would be represented in this small town by one or two anyway. He hated them all just then: the Statisticians, the Accountants, the lesser breeds and the higher. He hated them because they owned their smug knowledge now, knew their fate, while he himself, empty still, had to face some kind of further red tape. He reached 15-C, was ushered in and left in an empty room. For one moment, his spirits bounded. Surely, if this were the labor classification room, there would be dozens of youngsters present. A door sucked into its recess on the other side of a waist-high partition and an elderly, white-haired man stepped out. He smiled and showed even teeth that were obviously false, but his face was still ruddy and unlined and his voice had vigor. He said, “Good evening, George. Our own sector has only one of you this time, I see.” “Only one?” said George blankly. “Thousands over the Earth, of course. Thousands. You’re not alone.” George felt exasperated. He said, “I don’t understand, sir. What’s my classification? What’s happening?” “Easy, son. You’re all right. It could happen to anyone. He held out his hand and George took it mechanically. It was warm and it pressed George’s hand firmly. “Sit down, son. I’m Sam Ellenford.” George nodded impatiently. “I want to know what’s going on, sir.” “Of course. To begin with, you can’t be a Computer Programmer, George. You’ve guessed that I think.” “Yes, I have,’ said George bitterly. “What will I be, then?” “That’s the hard part to explain, George.” He paused, then said with careful distinctness, “Nothing.” “What!” “Nothing!” “But what does that mean? Why can’t you assign me a profession?” “We have no choice in the matter, George. It’s the structure of your mind that decides that.” George went a sallow yellow. His eyes bulged. “There’s something wrong with my mind?” “There’s something about it. As far as professional classification is concerned, I suppose you can call it wrong.” “But why?” Ellenford shrugged. “I’m sure you know how Earth runs its Educational program, George. Practically any human being can absorb practically any body of knowledge, but each individual brain pattern is better suited to receiving some types of knowledge than others. We try to match mind to knowledge as well as we can within the limits of the quota requirements for each profession.” George nodded. “Yes, I know.” “Every once in a while, George, we come up against a young man whose mind is not suited to receiving a superimposed knowledge of any sort.” “You mean I can’t be Educated?” “That is what I mean.” “But that’s crazy. I’m intelligent. I can understand –”� He looked helplessly about as though trying to find some way of proving that he had a functioning brain. “Don’t misunderstand me, please”, said Ellenford gravely. “ You’re intelligent. There’s no question about that. You’re even above average in intelligence. Unfortunately that has nothing to do with whether the mind ought to be allowed to accept superimposed knowledge or not. In fact, it is almost always the intelligent person who comes here.” “You mean I can’t even be a Registered Laborer?” babbled George. Suddenly even that was better than the blank that faced him. “What’s there to know to be a Laborer?” “Don’t underestimate the Laborer, young man. There are dozens of subclassifications and each variety has its own corpus of fairly detailed knowledge. Do you think there’s no skill in knowing the proper manner of lifting a weight? Besides, for the Laborer, we must select not only minds suited to it but bodies as well. You’re not the type, George, to last long as a Laborer.” George was conscious of his slight build. He said, “But I’ve never heard of anyone without a profession.” “There aren’t many,” conceded Ellenford. “And we protect them.” “Protect them?” George felt confusion and fright grow higher inside him. “You’re a ward of the planet, George. From the time you walked through that door, we’ve been in charge of you.” And he smiled. It was a fond smile. To George it seemed the smile of ownership; the smile of a grown man for a helpless child. He said, “You mean, I’m going to be in prison?” “Of course not. You will simply be with others of your kind.” Your kind. The words made a kind of thunder in George’s ear. Ellenford said, “You need special treatment. We’ll take care of you.” To George’s own horror, he burst into tears. Ellenford walked to the other end of the room and faced away as though in thought. George fought to reduce the agonized weeping to sobs and then to strangle those. He thought of his father and mother, of his friends, of Trevelyan, of his own shame – He said rebelliously, “I learned to read.” “Everyone with a whole mind can do that. We’ve never found exceptions. It is at this stage that we discover – exceptions. And when you learned to read, George, we were concerned about your mind pattern. Certain peculiarities were reported even then by the doctor in charge.” “Can’t you try Educating me? You haven’t even tried. I’m willing to take the risk.” “The law forbids us to do that, George. But look, it will not be bad. We will explain matters to your family so they will not be hurt. At the place to which you’ll be taken, you’ll be allowed privileges. We’ll get you books and you can learn what you will.” “Dab knowledge in by hand,” said George bitterly. “Shred by shred. Then, when I die I’ll know enough to be a Registered Junior Office Boy, Paper-Clip Division.” “Yet I understand you’ve already been studying books.” George froze. He was struck devastatingly by sudden understanding. “That’s it” “What is?” “That fellow Antonelli. He’s knifing me.” “No, George. You’re quite wrong.” “Don’t tell me that.” George was in an ecstasy of fury. “That lousy bastard is selling me out because he thought I was a little too wise for him. I read books and tried to get a head start toward programming. Well, what do you want to square things? Money? You won’t get it. I’m getting out of here and when I finish broadcasting this –”� He was screaming. Ellenford shook his head and touched a contact. Two men entered on catfeet and got on either side of George. They pinned his arms to his sides. One of them used an air-spray hypodermic in the hollow of his right elbow and the hypnotic entered his vein and had an almost immediate effect. His screams cut off and his head fell forward. His knees buckled and only the men on either side kept him erect as he slept. They took care of George as they said they would; they were good to him and unfailingly kind – about the way, George thought, he himself would be to a sick kitten he had taken pity on. They told him that he should sit up and take some interest in life; and then told him that most people who came there had the same attitude of despair at the beginning and that he would snap out of it. He didn’t even hear them. Dr Ellenford himself visited him to tell him that his parents had been informed that he was away on special assignment. George muttered, “Do they know –”� Ellenford assured him at once,“We gave no details.” At first George had refused to eat. They fed him intravenously. They hid sharp objects and kept him under guard. Hali Omani came to be his roommate and his stolidity had a calming effect. One day, out of sheer desperate boredom, George asked for a book. Omani, who himself read books constantly, looked up, smiling broadly. George almost withdrew the request then, rather than give any of them satisfaction, then thought: What do I care? He didn’t specify the book and Omani brought one on chemistry. It was in big print, with small words and many illustrations. It was for teenagers. He threw the book violently against the wall. That’s what he would be always. A teenager all his life. A pre-Educate forever and special books would have to be written for him. He lay smoldering in bed, staring at the ceiling, and after an hour had passed, he got up sulkily, picked up the book, and began reading. It took him a week to finish it and then he asked for another. “Do you want me to take the first one back?” asked Omani. George frowned. There were things in the book he had not understood, yet he was not so lost to shame as to say so. But Omani said, “Come to think of it, you’d better keep it. Books are meant to be read and reread.” It was that same day that he finally yielded to Omani’s invitation that he tour the place. He dogged at the Nigerian’s feet and took in his surroundings with quick hostile glances. The place was no prison certainly. There were no walls, no locked doors, no guards. But it was a prison in that the inmates had no place to go outside. It was somehow good to see others like himself by the dozen. It was so easy to believe himself to be the only one in the world so – maimed. He mumbled, “How many people here anyway?” “Two hundred and five, George, and this isn’t the only place of the sort in the world. There are thousands.” Men looked up as he passed, wherever he went; in the gymnasium, along the tennis courts; through the library (he had never in his life imagined books could exist in such numbers; they were stacked, actually stacked, along long shelves). They stared at him curiously and he returned the looks savagely. At least they were no better than he; no call for them to look at him as though he were some son of curiosity. Most of them were in their twenties. George said suddenly, “What happens to the older ones?” Omani said, “This place specializes in the younger ones.” Then, as though he suddenly recognized an implication in George’s question that he had missed earlier, he shook his head gravely and said, “They’re not put out of the way, if that’s what you mean. There are other Houses for older ones.” “Who cares?” mumbled George, who felt he was sounding too interested and in danger of slipping into surrender. “You might. As you grow older, you will find yourself in a House with occupants of both sexes.” That surprised George somehow. “Women, too?” “Of course. Do you suppose women are immune to this sort of thing?” George thought of that with more interest and excitement than he had felt for anything since before that day when – He forced his thought away from that. Omani stopped at the doorway of a room that contained a small closed-circuit television set and a desk computer. Five or six men sat about the television. Omani said, “This is a classroom.” George said, “What’s that?” “The young men in there are being educated. Not,” he added, quickly, “in the usual way.” “You mean they’re cramming it in bit by bit.” “That’s right. This is the way everyone did it in ancient times.” This was what they kept telling him since he had come to the House but what of it? Suppose there had been a day when mankind had not known the diatherm-oven. Did that mean he should be satisfied to eat meat raw in a world where others ate it cooked? He said, “Why do they want to go through that bit-by-bit stuff?” “To pass the time, George, and because they’re curious.” “What good does it do them?” “It makes them happier.” George carried that thought to bed with him. The next day he said to Omani ungraciously, “Can you get me into a classroom where I can find out something about programming?” Omani replied heartily, “Sure.” It was slow and he resented it. Why should someone have to explain something and explain it again? Why should he have to read and reread a passage, then stare at a mathematical relationship and not understand it at once? That wasn’t how other people had to be. Over and over again, he gave up. Once he refused to attend classes for a week. But always he returned. The official in charge, who assigned reading, conducted the television demonstrations, and even explained difficult passages and concepts, never commented on the matter. George was finally given a regular task in the gardens and took his turn in the various kitchen and cleaning details. This was represented to him as being an advance, but he wasn’t fooled. The place might have been far more mechanized than it was, but they deliberately made work for the young men in order to give them the illusion of worth-while occupation, of usefulness. George wasn’t fooled. They were even paid small sums of money out of which they could buy certain specified luxuries or which they could put aside for a problematical use in a problematical old age. George kept his money in an open jar, which he kept on a closet shelf. He had no idea how much he had accumulated. Nor did he care. He made no real friends though he reached the stage where a civil good day was in order. He even stopped brooding (or almost stopped) on the miscarriage of justice that had placed him there. He would go weeks without dreaming of Antonelli, of his gross nose and wattled neck, of the leer with which he would push George into a boiling quicksand and hold him under, till he woke screaming with Omani bending over him in concern. Omani said to him on a snowy day in February, “It’s amazing how you’re adjusting.” But that was February, the thirteenth to be exact, his nineteenth birthday. March came, then April, and with the approach of May he realized he hadn’t adjusted at all. The previous May had passed unregarded while George was still in his bed, drooping and ambitionless. This May was different. All over Earth, George knew, Olympics would be taking place and young men would be competing, matching their skills against one another in the fight for a place on a new world. There would be the holiday atmosphere, the excitement, the news reports, the self-contained recruiting agents from the worlds beyond space, the glory of victory or the consolations of defeat. How much of fiction dealt with these motifs, how much of his own boyhood excitement lay in following the events of Olympics from year to year; how many of his own plans – George Platen could not conceal the longing in his voice. It was too much to suppress. He said, “Tomorrow’s the first of May. Olympics!” And that led to his first quarrel with Omani and to Omani’s bitter enunciation of the exact name of the institution in which George found himself. Omani gazed fixedly at George and said distinctly, “A House for the Feeble-minded.” George Platen flushed. Feeble-minded! He rejected it desperately. He said in a monotone, “I’m leaving.” He said it on impulse. His conscious mind learned it first from the statement as he uttered it. Omani, who had returned to his book, looked up. “What?” George knew what he was saying now. He said it fiercely, “I’m leaving.” “That’s ridiculous. Sit down, George, calm yourself.” “Oh, no. I’m here on a frame-up, I tell you. This doctor, Antonelli, took a dislike to me. It’s the sense of power these petty bureaucrats have. Cross them and they wipe out your life with a stylus mark on some card file.” “Are you back to that?” “And staying there till it’s all straightened out. I’m going to get to Antonelli somehow, break him, force the truth out of him.” George was breathing heavily and he felt feverish. Olympics month was here and he couldn’t let it pass. If he did, it would be the final surrender and he would be lost for all time. Omani threw his legs over the side of his bed and stood up. He was nearly six feet tall and the expression on his face gave him the look of a concerned Saint Bernard. He put his arm about George’s shoulder, “If I hurt your feelings –” George shrugged him off. “You just said what you thought was the truth, and I’m going to prove it isn’t the truth, that’s all. Why not? The door’s open. There aren’t any locks. No one ever said I couldn’t leave. I’ll just walk out.” “All right, but where will you go?” “To the nearest air terminal, then to the nearest Olympics center. I’ve got money.” He seized the open jar that held the wages he had put away. Some of the coins jangled to the floor. “That will last you a week maybe. Then what?” “By then I’ll have things settled.” “By then you’ll come crawling back here,” said Omani earnestly, “with all the progress you’ve made to do over again. You’re mad, George.” “Feeble-minded is the word you used before.” “Well, I’m sorry I did. Stay here, will you?” “Are you going to try to stop me?” Omani compressed his full lips. “No, I guess I won’t. This is your business. If the only way you can learn is to buck the world and come back with blood on your face, go ahead. – Well, go ahead.” George was in the doorway now, looking back over his shoulder. “I’m going” – he came back to pick up his pocket grooming set slowly – “I hope you don’t object to my taking a few personal belongings.” Omani shrugged. He was in bed again reading, indifferent. George lingered at the door again, but Omani didn’t look up. George gritted his teeth, turned and walked rapidly down the empty corridor and out into the night-shrouded grounds. He had expected to be stopped before leaving the grounds. He wasn’t. He had stopped at an all-night diner to ask directions to an air terminal and expected the proprietor to call the police. That didn’t happen. He summoned a skimmer to take him to the airport and the driver asked no questions. Yet he felt no lift at that. He arrived at the airport sick at heart. He had not realized how the outer world would be. He was surrounded by professionals. The diner’s proprietor had had his name inscribed on the plastic shell over the cash register. So and so, Registered Cook. The man in the skimmer had his license up, Registered Chauffeur. George felt the bareness of his name and experienced a kind of nakedness because of it worse, he felt skinned. But no one challenged him. No one studied him suspiciously and demanded proof of professional rating. George thought bitterly: Who would imagine any human being without one? He bought a ticket to San Francisco on the 3 a.m. plane. No other plane for a sizable Olympics center was leaving before morning and he wanted to wait as little as possible. As it was, he sat huddled in the waiting room, watching for the police. They did not come. He was in San Francisco before noon and the noise of the city struck him like a blow. This was the largest city he had ever seen and he had been used to silence and calm for a year and a half now. Worse, it was Olympics month. He almost forgot his own predicament in his sudden awareness that some of the noise, excitement, confusion was due to that. The Olympics boards were up at the airport for the benefit of the incoming travelers, and crowds jostled around each one. Each major profession had its own board. Each listed directions to the Olympics Hall where the contest for that day for that profession would be given; the individuals competing and their city of birth; the Outworld (if any) sponsoring it. It was a completely stylized thing. George had read descriptions often enough in the newsprints and films, watched matches on television, and even witnessed a small Olympics in the Registered Butcher classification at the county seat. Even that, which had no conceivable Galactic implication (there was no Outworlder in attendance, of course) aroused excitement enough. Partly, the excitement was caused simply by the fact of competition, partly by the spur of local pride (oh, when there was a home-town boy to cheer for, though he might be a complete stranger), and, of course, partly by betting. There was no way of stopping the last. George found it difficult to approach the board. He found himself looking at the scurrying, avid onlookers in a new way. There must have been a time when they themselves were Olympic material. What had they done? Nothing! If they had been winners, they would be far out in the Galaxy somewhere, not stuck here on Earth. Whatever they were, their professions must have made them Earth-bait from the beginning; or else they had made themselves Earth-bait by inefficiency at whatever high-specialized professions they had had. Now these failures stood about and speculated on the chances of newer and younger men. Vultures! How he wished they were speculating on him. He moved down the line of boards blankly, clinging to the outskirts of the groups about them. He had eaten breakfast on the strato and he wasn’t hungry. He was afraid, though. He was in a big city during the confusion of the beginning of Olympics competition. That was protection, sure. The city was full of strangers. No one would question George. No one would care about George. No one would care. Not even the House, thought George bitterly. They cared for him like a sick kitten, but if a sick kitten up and wanders off, well too bad, what can you do? And now that he was in San Francisco, what did he do? His thoughts struck blankly against a wall. See someone? Whom? How? Where would he even stay? The money he had left seemed pitiful. The first shamefaced thought of going back came to him. He could go to the police – He shook his head violently as though arguing with a material adversary. A word caught his eye on one of the boards, gleaming there: Metallurgist. In smaller letters, nonferrous. At the bottom of a long list of names, in flowing script, sponsored by Novia. It induced painful memories: himself arguing with Trevelyan, so certain that he himself would be a Programmer, so certain that a Programmer was superior to a Metallurgist, so certain that he was following the right course, so certain that he was clever – So clever that he had to boast to that small-minded, vindictive Antonelli. He had been so sure of himself that moment when he had been called and had left the nervous Trevelyan standing there, so cocksure. George cried out in a short, incoherent high-pitched gasp. Someone turned to look at him, then hurried on. People brushed past impatiently pushing him this way and that. He remained staring at the board, openmouthed. It was as though the board had answered his thought. He was thinking ‘Trevelyan’ so hard that it had seemed for a moment that of course the board would say ‘Trevelyan’ back at him. But that was Trevelyan, up there. And Armand Trevelyan (Stubby’s hated first name: up in lights for everyone to see) and the right home town. What’s more, Trev had wanted Novia, aimed for Novia, insisted on Novia; and this competition was sponsored by Novia. This had to be Trev; good old Trev. Almost without thinking he noted the directions for getting to the place of competition and took his place in line for a skimmer. Then he thought somberly: Trev made it! He wanted to be a Metallurgist, and he made it! George felt colder, more alone than ever. There was a line waiting to enter the hall. Apparently, Metallurgy Olympics was to be an exciting and closely fought one. At least, the illuminated sky sign above the ball said so, and the jostling crowd seemed to think so. It would have been a rainy day, George thought, from the color of the sky, but San Francisco had drawn the shield across its breadth from bay to ocean. It was an expense to do so, of course, but all expenses were warranted where the comfort of Outworlders was concerned. They would be in town for the Olympics. They were heavy spenders. And for each recruit taken, there would be a fee both to Earth, and to the local government from the planet sponsoring the Olympics. It paid to keep Outworlders in mind of a particular city as a pleasant place in which to spend Olympics time. San Francisco knew what it was doing. George, lost in thought, was suddenly aware of a gentle pressure on his shoulder blade and a voice saving, “Are you in line here, young man?” The line had moved up without George’s having noticed the widening gap. He stepped forward hastily and muttered, “Sorry, sir.” There was the touch of two fingers on the elbow of his jacket and he looked about furtively. The man behind him nodded cheerfully. He had iron-gray hair, and under his jacket he wore an old-fashioned sweater that buttoned down the front. He said, “I didn’t mean to sound sarcastic.” “No offense.” “All right, then.” He sounded cozily talkative. “I wasn’t sure you might not simply be standing there, entangled with the line, so to speak. only by accident. I thought you might be a –”� “A what?” said George sharply. “Why, a contestant, of course. You look young.” George turned away. He felt neither cozy nor talkative, and bitterly impatient with busybodies. A thought struck him. Had an alarm been sent out for him? Was his description known, or his picture? Was Gray-hair behind him trying to get a good look at his face? He hadn’t seen any news reports. He craned his neck to see the moving strip of news headlines parading across one section of the city shield, somewhat lackluster against the gray of the cloudy afternoon sky. It was no use. He gave up at once. The headlines would never concern themselves with him. This was Olympics time and the only news worth headlining was the comparative scores of the winners and the trophies won by continents, nations, and cities. It would go on like that for weeks, with scores calculated on a per capita basis and every city finding some way of calculating itself into a position of honor. His own town had once placed third in an Olympics covering Wiring Technician; third in the whole state. There was still a plaque saying so in Town Hall. George hunched his head between his shoulders and shoved his hands in his pocket and decided that made him more noticeable. He relaxed and tried to look unconcerned, and felt no safer. He was in the lobby now, and no authoritative hand had yet been laid on his shoulder. He filed into the hall itself and moved as far forward as he could. It was with an unpleasant shock that he noticed Gray-hair next to him. He looked away quickly and tried reasoning with himself. The man had been right behind him in line after all. Gray-hair, beyond a brief and tentative smile, paid no attention to him and, besides, the Olympics was about to start. George rose in his seat to see if he could make out the position assigned to Trevelyan and at the moment that was all his concern. The hall was moderate in size and shaped in the classical long oval with the spectators in the two balconies running completely about the rim and the contestants in the linear trough down the center. The machines were set up, the progress boards above each bench were dark, except for the name and contest number of each man. The contestants themselves were on the scene, reading, talking together; one was checking his fingernails minutely.(It was, of course, considered bad form for any contestant to pay any attention to the problem before him until the instant of the starting signal.) George studied the program sheet he found in the appropriate slot in the arm of his chair and found Trevelyan’s name. His number was twelve and, to George’s chagrin, that was at the wrong end of the hall. He could make out the figure of Contestant Twelve, standing with his hands in his pockets, back to his machine, and staring at the audience as though he were counting the house. George couldn’t make out the face. Still that was Trev. George sank back in his seat. He wondered if Trev would do well. He hoped, as a matter of conscious duty, that he would, and yet there was something within him that felt rebelliously resentful. George, professionless, here, watching. Trevelyan, Registered Metallurgist, Nonferrous, there, competing. George wondered if Trevelyan had competed in his first year. Sometimes men did, if they felt particularly confident – or hurried. It involved a certain risk. However efficient the Educative process, a preliminary year on Earth ('oiling the stiff knowledge’, as the expression went) insured a higher score. If Trevelyan was repeating, maybe he wasn’t doing so well. George felt ashamed that the thought pleased him just a bit. He looked about. The stands were almost full. This would be a well-attended Olympics, which meant greater strain on the contestants – or greater drive, perhaps, depending on the individual. Why Olympics, he thought suddenly? He had never known. Why was bread called bread? Once he had asked his father: “Why do they call it Olympics, Dad?” And his father had said: “Olympics means competition.” George had said: “Is when Stubby and I fight an Olympics, Dad?” Platen, Senior, had said: “No. Olympics is a special kind of competition and don’t ask silly questions. You’ll know all you have to know when you get Educated.” George, back in the present, sighed and crowded down into his seat. All you have to know! Funny that the memory should be so clear now. “When you get Educated.” No one ever said, “If you get Educated.” He always had asked silly questions, it seemed to him now. It was as though his mind had some instinctive foreknowledge of its inability to be Educated and had gone about asking questions in order to pick up scraps here and there as best it could. And at the House they encouraged him to do so because They agreed with his mind’s instinct. It was the only way. He sat up suddenly. What the devil was he doing? Falling for that lie? Was it because Trev was there before him, an Educee, competing in the Olympics that he himself was surrendering? He wasn’t feeble-minded! No! And the shout of denial in his mind was echoed by the sudden clamor in the audience as everyone got to his feet. The box seat in the very center of one long side of the oval was filling with an entourage wearing the colors of Novia, and the word ‘Novia’ went up above them on the main board. Novia was a Grade A world with a large population and a thoroughly developed civilization, perhaps the best in the Galaxy. It was the kind of world that every Earthman wanted to live in someday; or, failing that to see his children live in. (George remembered Trevelyan’s insistence on Novia as a goal – and there he was competing for it.) The lights went out in that section of the ceiling above the audience and so did the wall lights. The central trough, in which the contestants waited, became floodlit. Again George tried to make out Trevelyan. Too far. The clear, polished voice of the announcer sounded. “Distinguished Novian sponsors. Ladies. Gentlemen. The Olympics competition for Metallurgist, Nonferrous, is about to begin. The contestants are –”� Carefully and conscientiously, he read off the list in the program. Names. Home towns. Educative years. Each name received its cheers, the San Franciscans among them receiving the loudest. When Trevelyan’s name was reached, George surprised himself by shouting and waving madly. The gray-haired man next to him surprised him even more by cheering likewise. George could not help but stare in astonishment and his neighbor leaned over to say (speaking loudly in order to be heard over the hubbub), “No one here from my home town; I’ll root for yours. Someone you know?” George shrank back. “No.” “I noticed you looking in that direction. Would you like to borrow my glasses?” “No. Thank you.” (Why didn’t the old fool mind his own business?) The announcer went on with other formal details concerning the serial number of the competition, the method of timing and scoring and so on. Finally, he approached the meat of the matter and the audience grew silent as it listened. “Each contestant will be supplied with a bar of nonferrous alloy of unspecified composition. He will be required to sample and assay the bar, reporting all results correctly to four decimals in per cent. All will utilize for this purpose a Beeman Microspectrograph Model FX-2, each of which is, at the moment, not in working order.” There was an appreciative shout from the audience. “Each contestant will be required to analyze the fault of his machine and correct it. Tools and spare parts are supplied. The spare part necessary may not be present, in which case it must be asked for, and time of delivery thereof will be deducted from final time. Are all contestants ready?” The board above Contestant Five flashed a frantic red signal. Contestant Five ran off the floor and returned a moment later. The audience laughed good-naturedly. “Are all contestants ready?” The boards remained blank. “Any questions” Still blank. “You may begin.” There was, of course, no way anyone in the audience could tell how any contestant was progressing except for whatever notations went up on the notice board. But then, that didn’t matter. Except for what professional Metallurgists there might be in the audience, none would understand anything about the contest professionally in any case. What was important was who won, who was second, who was third. For those who had bets on the standings (illegal but unpreventable) that was all-important. Everything else might go hang. George watched as eagerly as the rest, glancing from one contestant to the next, observing how this one had removed the cover from his microspectrograph with deft strokes of a small instrument; how that one was peering into the face of the thing; how still a third was setting his alloy bar into its holder; and how a fourth adjusted a vernier with such small touches that he seemed momentarily frozen. Trevelyan was as absorbed as the rest. George had no way of telling how he was doing. The notice board over Contestant Seventeen flashed: Focus plate out of adjustment. The audience cheered wildly. Contestant Seventeen might be right and he might, of course, be wrong. If the latter, he would have to correct his diagnosis later and lose time. Or he might never correct his diagnosis and be unable to complete his analysis or, worse still, end with a completely wrong analysis. Never mind. For the moment, the audience cheered. Other boards lit up. George watched for Board Twelve. That came on finally: ‘Sample holder off-center. New clamp depresser needed.’ An attendant went running to him with a new pan. If Trevelyan was wrong, it would mean useless delay. Nor would the time elapsed in waiting for the pan be deducted. George found himself holding his breath. Results were beginning to go up on Board Seventeen, in gleaming letters: aluminum, 41.2649; magnesium, 22.1914; copper, 10.1001. Here and there, other boards began sprouting figures. The audience was in bedlam. George wondered how the contestants could work in such pandemonium, then wondered if that were not even a good thing. A first-class technician should work best under pressure. Seventeen rose from his place as his board went red-rimmed to signify completion. Four was only two seconds behind him. Another, then another. Trevelyan was still working, the minor constituents of his alloy bar still unreported. With nearly all contestants standing, Trevelyan finally rose, also. Then, tailing off, Five rose, and received an ironic cheer. It wasn’t over. Official announcements were naturally delayed. Time elapsed was something, but accuracy was just as important. And not all diagnoses were of equal difficulty. A dozen factors had to be weighed. Finally, the announcer’s voice sounded, “Winner in the time of four minutes and twelve seconds, diagnosis correct, analysis correct within an average of zero point seven parts per hundred thousand, Contestant Number – Seventeen, Henry Anton Schrnidt of –”� What followed was drowned in the screaming. Number Eight was next and then Four, whose good time was spoiled by a five part in ten thousand error in the niobium figure. Twelve was never mentioned. He was an also-ran. George made his way through the crowd to the Contestant’s Door and found a large clot of humanity ahead of him. There would be weeping relatives (joy or sorrow, depending) to greet them, newsmen to interview the top-scorers, or the home-town boys, autograph hounds, publicity seekers and the just plain curious. Girls, too, who might hope to catch the eye of a top-scorer, almost certainly headed for Novia (or perhaps a low-scorer who needed consolation and had the cash to afford it). George hung back. He saw no one he knew. With San Francisco so far from home, it seemed pretty safe to assume that there would be no relatives to condole with Trev on the spot. Contestants emerged, smiling weakly, nodding at shouts of approval. Policemen kept the crowds far enough away to allow a lane for walking. Each high-scorer drew a portion of the crowd off with him, like a magnet pushing through a mound of iron filings. When Trevelyan walked out, scarcely anyone was left. (George felt somehow that he had delayed coming out until just that had come to pass.) There was a cigarette in his dour mouth and he turned, eyes downcast, to walk off. It was the first hint of home George had had in what was almost a year and a half and seemed almost a decade and a half. He was almost amazed that Trevelyan hadn’t aged, that he was the same Trev he had last seen. George sprang forward. “Trev!” Trevelyan spun about, astonished. He stared at George and then his hand shot out. “George Platen, what the devil – ”� And almost as soon as the look of pleasure had crossed his face, It left. His hand dropped before George had quite the chance of seizing it. “Were you in there?’ A curt jerk of Trev’s head indicated the hall. “I was.” “To see me?” “Yes.” “Didn’t do so well did I?” He dropped his cigarette and stepped on it, staring off to the street, where the emerging crowd was slowly eddying and finding its way into skimmers, while new lines were forming for the next scheduled Olympics. Trevelyan said heavily, “So what? It’s only the second time I missed. Novia can go shove after the deal I got today. There are planets that would jump at me fast enough – But, listen, I haven’t seen you since Education Day. Where did you go? Your folks said you were on special assignment but gave no details and you never wrote. You might have written.” “I should have,” said George uneasily. “Anyway, I came to say I was sorry the way things went just now.” “Don’t be,” said Trevelyan. “I told you. Novia can go shove – At that I should have known. They've been saying for weeks that the Beeman machine would be used. All the wise money was on Beeman machines. The damned Education tapes they ran through me were for Henslers and who uses Henslers? The worlds in the Goman Cluster if you want to call them worlds. Wasn’t that a nice deal they gave me?” “Can’t you complain to –”� “Don’t be a fool. They’ll tell me my brain was built for Henslers. Go argue. Everything went wrong. I was the only one who had to send out for a piece of equipment. Notice that?” “They deducted the time for that, though.” “Sure, but I lost time wondering if I could be right in my diagnosis when I noticed there wasn’t any clamp depresser in the parts they had supplied. They don’t deduct for that. If it had been a Hensler, I would have known I was right. How could I match up then? The top winner was a San Franciscan. So were three of the next four. And the fifth guy was from Los Angeles. They get big-city Educational tapes. The best available. Beeman spectrographs and all. How do I compete with them? I came all the way out here just to get a chance at a Novian-sponsored Olympics in my classification and I might just as well have stayed home. I knew it, I tell you, and that settles it. Novia isn’t the only chunk of rock in space. Of all the damned –” He wasn’t speaking to George. He wasn’t speaking to anyone. He was just uncorked and frothing. George realized that. George said, “If you knew in advance that the Beemans were going to be used, couldn’t you have studied up on them?” “They weren’t in my tapes, I tell you.” “You could have read – books.” The last word had tailed off under Trevelyan’s suddenly sharp look. Trevelyan said, “Are you trying to make a big laugh out of this? You think this is funny? How do you expect me to read some book and try to memorize enough to match someone else who knows.” “I thought –”� “You try it. You try –”� Then, suddenly, “What’s your profession, by the way?” He sounded thoroughly hostile. “Well –”� “Come on, now. If you’re going to be a wise guy with me, let’s see what you’ve done. You’re still on Earth, I notice, so you’re not a Computer Programmer and your special assignment can’t be much.” George said, “Listen, Trev, I’m late for an appointment.” He backed away, trying to smile. “No, you don’t.” Trevelyan reached out fiercely, catching hold of George’s jacket. “You answer my question. Why are you afraid to tell me? What is it with you? Don’t come here rubbing a bad showing in my face, George, unless you can take it, too. Do you hear me?” He was shaking George in frenzy and they were struggling and swaying across the floor, when the Voice of Doom struck George’s ear in the form of a policeman’s outraged call. “All right now. All right. Break it up.” George’s heart turned to lead and lurched sickeningly. The policeman would be taking names, asking to see identity cards, and George lacked one. He would be questioned and his lack of profession would show at once; and before Trevelyan, too, who ached with the pain of the drubbing he had taken and would spread the news back home as a salve for his own hurt feelings. George couldn’t stand that. He broke away from Trevelyan and made to run, but the policeman’s heavy hand was on his shoulder. “Hold on there. Let’s see your identity card.” “Trevelyan was fumbling for his, saying harshly, “I’m Amand Trevelyan, Metallurgist, Nonferrous. I was just competing in the Olympics. You better find out about him, though, officer.” George faced the two, lips dry and throat thickened past speech. Another voice sounded, quiet, well-mannered. “Officer. One moment.” The policeman stepped back. “Yes, sir?” “This young man is my guest. What is the trouble?” George looked about in wild surprise. It was the gray-haired man who had been sitting next to him. Gray-hair nodded benignly at George. Guest? Was he mad? The policeman was saying, “These two were creating a disturbance, sir.” “Any criminal charges? Any damages?” “No, sir.” “Well, then, I’ll be responsible.” He presented a small card to the policeman’s view and the latter stepped back at once. Trevelyan began indignantly, “Hold on, now –”� but the policeman turned on him. “All right, now. Got any charges?” “I just –”� “On your way. The rest of you – move on.” A sizable crowd had gathered, which now, reluctantly, unknotted itself and raveled away. George let himself be led to a skimmer but balked at entering. He said, “Thank you, but I’m not your guest.” (Could it be a ridiculous case of mistaken identity?) But Gray-hair smiled and said, “You weren’t but you are now. Let me introduce myself, I’m Ladislas Ingenescu, Registered Historian.” “But –”� “Come, you will come to no harm, I assure you. After all, I only wanted to spare you some trouble with a policeman.” “But why?” “Do you want a reason? Well, then, say that we’re honorary towns-mates, you and I. We both shouted for the same man, remember, and we towns-people must stick together, even if the tie is only honorary. Eh?” And George, completely unsure of this man, Ingenescu, and of himself as well, found himself inside the skimmer. Before he could make up his mind that he ought to get off again, they were off the ground. He thought confusedly: The man has some status. The policeman deferred to him. He was almost forgetting that his real purpose here in San Francisco was not to find Trevelyan but to find some person with enough influence to force a reappraisal of his own capacity of Education. It could be that Ingenescu was such a man. And right in George’s lap. Everything could be working out fine – fine. Yet it sounded hollow in his thought. He was uneasy. During the short skimmer-hop, Ingenescu kept up an even flow of small talk, pointing out the landmarks of the city, reminiscing about past Olympics he had seen. George, who paid just enough attention to make vague sounds during the pauses, watched the route of flight anxiously. Would they head for one of the shield-openings and leave the city altogether? The skimmer landed at the roof-entry of a hotel and, as he alighted, Ingenescu said, “I hope you’ll eat dinner with me in my room?” George said, “Yes,” and grinned unaffectedly. He was just beginning to realize the gap left within him by a missing lunch. Ingenescu let George eat in silence. Night closed in and the wall lights went on automatically. (George thought: I’ve been on my own almost twenty-four hours.) And then over the coffee, Ingenescu finally spoke again. He said, “You’ve been acting as though you think I intend you harm.” George reddened, put down his cup and tried to deny it, but the older man laughed and shook his head. “It’s so. I’ve been watching you closely since I first saw you and I think I know a great deal about you now.” George half rose in horror. Ingenescu said, “But sit down. I only want to help you.” George sat down but his thoughts were in a whirl. If the old man knew who he was, why had he not left him to the policeman? On the other hand, why should he volunteer help? Ingenescu said, “You want to know why I should want to help you? Oh, don’t look alarmed. I can’t read minds. It’s just that my training enables me to judge the little reactions that give minds away, you see. Do you understand that?” George shook his head. Ingenescu said, “Consider my first sight of you. You were waiting in line to watch an Olympics, and your micro-reactions didn’t match what you were doing. The expression of your face was wrong, the action of your hands was wrong. It meant that something, in general was wrong, and the interesting thing was that, whatever it was, it was nothing common, nothing obvious. Perhaps, I thought, it was something of which your own conscious mind was unaware. “I couldn’t help but follow you, sit next to you. I followed you again when you left and eavesdropped on the conversation between your friend and yourself. After that, well, you were far too interesting an object of study – I’m sorry if that sounds cold-blooded – for me to allow you to be taken off by a policeman. – Now tell me, what is it that troubles you?” George was in an agony of indecision. If this was a trap, why should it be such an indirect, roundabout one? And he had to turn to someone. He had come to the city to find help and here was help being offered. Perhaps what was wrong was that it was being offered. It came too easy. Ingenescu said, “Of course, what you tell me as a Social Scientist is a privileged communication. Do you know what that means?” “No, sir.” “It means, it would be dishonorable for me to repeat what you say to anyone for any purpose. Moreover no one has the legal right to compel me to repeat it.” George said, with sudden suspicion, “I thought you were a Historian.” “So I am.” “Just now you said you were a Social Scientist.” lngenescu broke into loud laughter and apologized for it when he could talk. “I’m sorry, young man, I shouldn’t laugh, and I wasn’t really laughing at you. I was laughing at Earth and its emphasis on physical science, and the practical segments of it at that. I’ll bet you can rattle off every subdivision of construction technology or mechanical engineering and yet you’re a blank on social science.” “Well, then what is social science?” “Social science studies groups of human beings and there are many high-specialized branches to it, just as there are to zoology, for instance. For instance, there are Culturists, who study the mechanics of cultures, their growth, development, and decay. Cultures,’ he added, forestalling a question, “are all the aspects of a way of life. For instance it includes the way we make our living, the things we enjoy and believe, what we consider good and bad and so on. Do you understand?” “I think I do.” “An Economist – not an Economic Statistician, now, but an Economist – specializes in the study of the way a culture supplies the bodily needs of its individual members. A psychologist specializes in the individual member of a society and how he is affected by the society. A Futurist specializes in planning the future course of a society, and a Historian – That’s where I come in, now.” “Yes, sir.” “A Historian specializes in the past development of our own society and of societies with other cultures.” George found himself interested. “Was it different in the past?” “I should say it was. Until a thousand years ago, there was no Education; not what we call Education, at least.” George said, “I know. People learned in bits and pieces out of books.” “Why, how do you know this?” “I’ve heard it said,’ said George cautiously. Then, “Is there any use in worrying about what’s happened long ago? I mean, it’s all done with, isn’t it?” “It’s never done with, my boy. The past explains the present. For instance, why is our Educational system what it is?” George stirred restlessly. The man kept bringing the subject back to that. He said snappishly, “Because it’s best.” “Ah, but why is it best? Now you listen to me for one moment and I’ll explain. Then you can tell me if there is any use in history. Even before interstellar travel was developed –”� He broke off at the look of complete astonishment on George’s face. “Well, did you think we always had it?” “I never gave it any thought, sir.” “I’m sure you didn’t. But there was a time, four or five thousand years ago, when mankind was confined to the surface of Earth. Even then, his culture had grown quite technological and his numbers had increased to the point where any failure in technology would have meant mass starvation and disease. To maintain the technological level and advance it in the face of an increasing population, more and more technicians and scientists had to be trained, and yet, as science advanced, it took longer and longer to train them. “As first interplanetary and then interstellar travel was developed, the problem grew more acute. In fact, actual colonization of extra-Solar planets was impossible for about fifteen hundred years because of lack of properly trained men. “The turning point came when the mechanics of the storage of knowledge within the brain was worked out. Once that had been done, it became possible to devise Educational tapes that would modify the mechanics in such a way as to place within the mind a body of knowledge ready-made so to speak. But you know about that. “Once that was done, trained men could be turned out by the thousands and millions, and we could begin what someone has since called the ‘Filling of the Universe.’ There are now fifteen hundred inhabited planets in the Galaxy and there is no end in sight. “Do you see all that is involved? Earth exports Education tapes for low-specialized professions and that keeps the Galactic culture unified. For instance, the Reading tapes insure a single language for all of us. – Don’t look so surprised, other languages are possible and in the past were used. Hundreds of them. “Earth also exports high-specialized professionals and keeps its own population at an endurable level. Since they are shipped out in a balanced sex ratio, they act as self-reproductive units and help increase the populations on the Outworlds where an increase is needed. Furthermore, tapes and men are paid for in material which we much need and on which our economy depends. Now do you understand why our Education is the best way?” “Yes, sir.” “Does it help you to understand knowing that without it, interstellar colonization was impossible for fifteen hundred years?” “Yes, sir.” “Then you see the uses of history.” The Historian smiled. “ And now I wonder if you see why I’m interested in you?” George snapped out of time and space back to reality. Ingenescu, apparently, didn’t talk aimlessly. All this lecture had been a device to attack him from a new angle. He said, once again withdrawn, hesitating, “Why?” “Social Scientists work with societies and societies are made up of people.” “All right.” “But people aren’t machines. The professionals in physical science work with machines. There is only a limited amount to know about a machine and the professionals know it all. Furthermore, all machines of given sort are just about alike so that there is nothing to interest them in any given individual machine. But people, ah – They are so complex and so different one from another that a Social Scientist never knows all there is to know or even a good part of what there is to know. To understand his own specialty, he must always be ready to study people; particularly unusual specimens.” “Like me,’ said George tonelessly. “I shouldn’t call you a specimen, I suppose, but you are unusual. You’re worth studying, and If you will allow me that privilege then, in return, I will help you if you are in trouble and if I can.” There were pin wheels whirring in George’s mind. – All this talk about people and colonization made possible by Education. It was as though caked thought within him were being broken up and strewn about mercilessly. He said, “Let me think,” and clamped his hands over his ears. He took them away and said to the Historian, “Will you do something for me, sir?” “If I can,” said the Historian amiably. “And everything I say in this room is a privileged communication. You said so.” “And I meant it.” “Then get me an interview with an Outworld official with – with a Novian.” Ingenescu looked startled. “Well, now –” “You can do it,’ said George earnestly. “You’re an important official. I saw the policeman’s look when you put that card in front of his eyes. If you refuse, I – I won’t let you study me.” It sounded a silly threat in George’s own ears, one without force. On Ingenescu, however, it seemed to have a strong effect. He said, “That’s an impossible condition. A Novian in Olympics month –”� “All right, then, get me a Novian on the phone and I’ll my own arrangements for an interview.” “Do you think you can?” “I know I can. Wait and see.” Ingenescu stared at George thoughtfully and then reached for the visiphone. George waited, half drunk with this new outlook on the whole problem and the sense of power it brought. It couldn’t miss. It couldn’t miss. He would be a Novian yet. He would leave Earth in triumph despite Antonelli and the whole crew of fools at the House for the (he almost laughed aloud) Feeble-minded. George watched eagerly as the visiplate lit up. It would open up a window into a room of Novians, a window into a small patch of Novia transplanted to Earth. In twenty-four hours, he had accomplished that much. There was a burst of laughter as the plate unmisted and sharpened, but for the moment no single head could be seen but rather the fast passing of the shadows of men and women, this way and that. A voice was heard, clearworded over a background of babble. “Ingenescu? He wants me?” Then there he was, staring out of the plate. A Novian. A genuine Novian. (George had not an atom of doubt. There was something completely Outworldly about him. Nothing that could be completely defined, or even momentarily mistaken.) He was swarthy in complexion with a dark wave of hair combed rigidly back from his forehead. He wore a thin black mustache and a pointed beard, just as dark, that scarcely reached below the lower limit of his narrow chin, but the rest of his face was so smooth that it looked a though it had been depilated permanently. He was smiling. “Ladislas, this goes too far. We fully expect to be spied on, within reason, during our stay on Earth, but mind reading is out of bounds.” “Mind reading, Honorable?” “Confess! You knew I was going to call you this evening. You knew I was only waiting to finish this drink.” His moved up into view and his eye peered through a glass of a faintly violet liqueur. “I can’t offer you one, afraid.” George, out of range of Ingenescu’s transmitter, could not be seen by the Novian. He was relieved at that. He wanted time to compose himself and he needed it badly. It was as though he were made up exclusively of restless fingers, drumming, drumming – But he was right. He hadn’t miscalculated. Ingenescu was important. The Novian called him by his first name. Good! Things worked well. What George had lost on Antonelli, he would make up, with advantage, on Ingenescu. And someday, when he was on his own at last, and could come back to Earth as powerful a Novian as this one who could negligently joke with Ingenescu’s first name and be addressed as ‘Honorable’ in turn – when he came back, he would settle with Antonelli. He had a year and a half to pay back and he – He all but lost his balance on the brink of the enticing daydream and snapped back in sudden anxious realization that he was losing the thread of what was going on. The Novian was saying, “– doesn’t hold water. Novia has a civilization as complicated and advanced as Earth’s. We’re not Zeston, after all. It’s ridiculous that we have to come here for individual technicians.” Ingenescu said soothingly, “Only for new models. There is never any certainty that new models will be needed. To buy the Educational tapes would cost you the same price as a thousand technicians and how do you know you would need that many?” The Novian tossed off what remained of his drink and laughed. (It displeased George, somehow, that a Novian should be this frivolous. He wondered uneasily if perhaps the Novian ought not to have skipped that drink and even the one or two before that.) The Novian said, 'That’s typical pious fraud, Ladislas. You know we can make use of all the late models we can. I collected five Metallurgists this afternoon –” “I know,’ said Ingenescu. “I was there.” “Watching me! Spying!” cried the Novian. “I’ll tell you What it is. The new-model Metallurgists I got differed from the previous model only in knowing the use of Beeman Spectrographs. The tapes couldn’t be modified that much, not that much” (he held up two fingers close together) “from last year’s model. You introduce the new models only to make us buy and spend and come here hat in hand.” “We don’t make you buy.” “No, but you sell late-model technicians to Landonum and so we have to keep pace. It’s a merry-go-round you have us on, you pious Earthmen, but watch out, there may be an exit somewhere.” There was a sharp edge to his laugh, and it ended sooner than it should have. Ingenescu said, “In all honesty, I hope there is. Meanwhile, as to the purpose of my call-” “That’s right, you called. Oh, well, I’ve said my say and I suppose next year there’ll be a new model of Metallurgist anyway for us to spend goods on, probably with a new gimmick for niobium assays and nothing else altered and the next year – But go on, what is it you want?” “I have a young man here to whom I wish you to speak.” “Oh?” The Novian looked not completely pleased with that. “ Concerning what?” “I can’t say. He hasn’t told me. For that matter he hasn’t even told me his name and profession.” The Novian frowned. “Then why take up my time?” “He seems quite confident that you will be interested in what he has to say.” “I dare say.” “And,” said Ingenescu, “as a favor to me.” The Novian shrugged. “Put him on and tell him to make it short.” Ingenescu stepped aside and whispered to George,“Address him as ‘Honorable’.” George swallowed with difficulty. This was it. George felt himself going moist with perspiration. The thought had come so recently, yet it was in him now so certainly. The beginnings of it had come when he had spoken to Trevelyan, then everything had fermented and billowed into shape while Ingenescu had prattled, and then the Novian’s own remarks had seemed to nail it all into place. George said, “Honorable, I’ve come to show you the exit from the merry-go-round.” Deliberately, he adopted the Novian’s own metaphor. The Novian stared at him gravely. “What merry-go-round?” “You yourself mentioned it, Honorable. The merry-go-round that Novia is on when you come to Earth to – to get technicians.” (He couldn’t keep his teeth from chattering; from excitement not fear.) The Novian said, “You’re trying to say that you know a way by which we can avoid patronizing Earth’s mental super-market. Is that it?” “Yes, sir. You can control your own Educational system.” “Umm. Without tapes?” “Y – yes, Honorable.” The Novian, without taking his eyes from George, called out, “Ingenescu, get into view.” The Historian moved to where he could be seen over George’s shoulder. The Novian said, “What is this? I don’t seem to penetrate.” “I assure you solemnly,” said Ingenescu, “that whatever this is it is being done on the young man’s own initiative, Honorable. I have not inspired this. I have nothing to do with it.” “Well, then, what is the young man to you? Why do you call me on his behalf?” Ingenescu said, “He is an object of study, Honorable. He has value to me and I humor him.” “What kind of value?” “It’s dif",
    "commentLink": "https://news.ycombinator.com/item?id=39659729",
    "commentBody": "Profession by Isaac Asimov (1957) (abelard.org)126 points by signa11 18 hours agohidepastfavorite57 comments Animats 14 hours agoIt's a very 1950s story. Everything is very organized. Many stories about that period were about organization, good or bad. This came from WWII, especially the European theater. Heroic mode had failed at Dunkirk and Dieppe. For the next round, D-Day, things were more organized. Way more organized. General Eisenhower came from logistics. He delayed D-Day a year while the Allies got ready. Really ready. Overwhelmingly ready. Huge numbers of special landing ships. Mobile instant ports. Giant spools of pipe to get fuel across the Atlantic. Special tanks that could grind up minefields. Prefabricated Coca-Cola bottling plants. When the invasion came, the backup was there behind it to put a huge army into Europe, fight through an entrenched army, and grind on to Berlin. That kind of thinking dominated the 1950s and 1960s, with the Apollo program being the last gasp of that approach. reply dredmorbius 6 hours agoparentI'm not aware of any active piping of petroleum over the Atlantic. There were major pipelines built both on the American mainland (the Big Inch and Little Big Inch pipelines), due to an overwhelming toll of German Kriegsmarine U-Boat attacks on US domestic oil shipments (New Orleans to New Jersey, largely, Project Paukenschlag). And there was Operation Pluto which ran a 9\" pipeline across the English Channel, delivering fuel from Britain to the European mainland. Trans-Atlantic oil shipments however were by tanker to the best of my knowledge. I'm relying strongly on Daniel Yergin's account of the oil industry during WWII (along with WWI, an absolutely fascinating episode), covered in his 1990 book The Prize. Despite differing strongly with Yergin over the future of oil and the oil industry, I unreservedly recommend this book as a masterful history of its past.reply Animats 4 hours agorootparentSorry, across the English Channel.[1] Very detailed info about how the pipe was assembled. [1] https://www.youtube.com/watch?v=FIYS_9EI5j0 reply dredmorbius 4 hours agorootparentThat matches my recollection. Also, correcting my previous comment, Pluto was 2\" and 3\" pipes, not 9\". That is, the vast majority of oil used in Allied operations in Europe passed through a set of 2- and 3-inch pipes, with 17 laid in total per the Wikipedia article I'd linked earlier. Which speaks both to the flow-rates achieved (~400,000 Imperial gallons/day), and the energy content of petroleum. reply iancmceachern 13 hours agoparentprev>\"Prefabricated Coca-Cola bottling plants\" Sun Tsu, an army marches on it stomach! reply Animats 11 hours agorootparentYes. Prefabricated Coca-Cola bottling plants. Eisenhower ordered 10 bottling lines and 6,000,000 bottles per month. To start. By the end of the war, Coca-Cola had over 60 bottling lines in war zones.[1] [1] http://www.nww2m.com/2011/08/coca-cola-the-pause-that-refres... reply svat 11 hours agorootparent“That's private property!” — Colonel \"Bat\" Guano in Dr. Strangelove (1964) https://www.youtube.com/watch?v=DUAK7t3Lf8s reply kragen 9 hours agorootparentcoca-cola is one of a very few companies in the usa licensed to handle coca leaves and to extract the cocaine from them, and coca-cola branches overseas have often been used for cia non-official cover. it's not all that private reply marcosdumay 12 hours agorootparentprevThere have recently been a series of posts about ancient time army logistics here. This used to be much more true than it is today. reply ViktorRay 10 hours agoprevTo me this part was the most interesting: For most of the first eighteen years of his life, George Platen had headed firmly in one direction, that of Registered Computer Programmer. There were those in his crowd who spoke wisely of Spationautics, Refrigeration Technology, Transportation Control, and even Administration. But George held firm. Why did Asimov in 1957 write as if Refrigeration technology would be so prominent in the future? Most people nowadays find the fridge to be a fairly boring technology. So I googled it. Apparently the fridge was only used by around 8 percent of American households in the early 1930’s. It didn’t become a standard part of American household life until the end of the 1940’s. So by the time this story was published the fridge was only commonly part of your life for the past 7 years probably. Curious. Kinda like so much of science fiction nowadays fixates on phones. Or has smartphone analogues taking up a bunch of detail. Science fiction stories tend to be written so that recent tech advances continue to be mind blowing and awe inspiring centuries in the future. Meanwhile only a few decades after this story was published the fridge became a standard boring part of life. reply kragen 9 hours agoparentrefrigeration technology isn't limited to the household fridge; it also encompasses air conditioning (which lee kuan yew thinks was the single most important technology enabling singapore to go from being one of the world's poorest countries to one of the richest within living memory, calling it 'the greatest invention of the 20th century': https://edition.cnn.com/2023/06/09/asia/air-conditioning-sin...) and cryocoolers, which are necessary for all practical superconductors so far (including nmr medical imaging machines and the josephson junctions used to define the volt) and for all ultra-high vacuum applications, including the euv lithography used for current chips (a dependency which goes both ways, since high-performance cryostats demand ultra-high vacuum) the entire fresh food supply chain, including military logistics, is structured around refrigeration, and human diets have changed dramatically as a result. before refrigeration, if you wanted to eat pumpkin in the spring or peas in the fall, your options were canned pumpkin or pickled pumpkin. when refrigerated shipping became available (about 60 years before asimov's story, not 7), within about fifteen years, argentina went from being a poor rural country fighting off raids, and shipping small amounts of unpalatable salt beef overseas, to being one of the world's richest countries from selling fresh beef; nowadays, of course, it's unthinkable to ship meat from the slaughterhouse to the butcher shop without refrigeration refrigeration is also fundamental to the current medical system; many vaccines won't keep without refrigeration, and of course tissue samples and organ transplants rely heavily on refrigeration to preserve them the entire technological and scientific world is built on precise metrology, and aside from the josephson junctions i mentioned before, air conditioning is important to precise metrology; virtually every phenomenon used to maintain a metrological standard over time varies with temperature, so metrological labs are invariably maintained at a precisely controlled temperature, usually to within half a degree and sometimes to within a hundredth of a degree. moreover, that temperature is usually 20°, because for historical reasons that's the temperature most of those standards are designed for; but you can't maintain a 20° temperature year-round without air conditioning in most of the human-inhabited world refrigeration technology in the form of air conditioning consumes a quite significant share of human world marketed energy consumption, so improvements in its efficiency are economically very important and currently avidly pursued by many avenues. and, as the cnn article above mentions, refrigerant leaks from vapor-compression refrigerative air conditioners are a significant contributor to global warming, though far from the majority there are numerous sbir grants available in the usa for improvements in various kinds of refrigeration technology, including things like cryostats and dilution coolers so every facet of your life is profoundly shaped by refrigeration technology, even if you aren't aware of it reply rnewme 9 hours agorootparentWonderful comment, ty. I recommend hyperspace pirate on YouTube, building diy systems with intention of ultimately building a cryocooler reply kragen 8 hours agorootparenti'm delighted you enjoyed it! reply ViktorRay 5 hours agorootparentprevWow thanks for sharing. I didn’t know about this stuff. I guess refrigeration is much more interesting than I assumed! I definitely need to read up more about all this. reply walterbell 13 hours agoprevChina Miéville's \"The City and the City\" 2010 novel and 2018 UK TV adaptation, which popularized the term \"unsee\", includes a similar theme related to the crime of \"breaching\" the boundary between two virtual cities that occupy one physical city, https://www.theguardian.com/tv-and-radio/2018/apr/07/the-cit... The City and the City is set up as a straightforward crime novel: in the dilapidated city of Beszél in eastern Europe, Inspector Tyador Borlú of the Extreme Crime Squad is trying to solve what initially looks like a routine case. But as he looks deeper into the murder of a mysterious woman, he discovers that she has links to Ul Qoma, a city that exists in the same physical space as Beszél but whose inhabitants studiously ignore any sign of overlap. reply kuchenbecker 12 hours agoparentNever heard of tha but definitely going to check it out. I kinda feel like that's how cities are today and will increasingly be so as we get further into the future where people are interacting digitally first and physically second interaction starts to stratify. I imagine a holo-lens future where it's quite literally possible to have a city to yourself or only see the set of people you want to with all the good and bad. Imagine social media bubbles but in the real world, with all the good bad bad that comes. reply 48864w6ui 11 hours agorootparentI've heard that in \"the south\" they don't care how close you get when you don't get too rich, but in the north they don't care how rich you get when you don't get too close. reply actionfromafar 10 hours agorootparentI wish I could parse this reply zem 10 hours agorootparentassuming some baseline animosity against a group of people (the \"outgroup\"), in the south the outgroup are resented for being successful, but if they can just be perceived as lesser the main group doesn't mind having them around. whereas in the north the main group doesn't want the outgroup mingling with them, but as long as they keep to themselves they can be as successful as they like. reply roca 6 hours agoparentprevMy favourite novel. reply eliben 17 hours agoprevFantastic story, one of my favorites by Asimov! What's fun for HN is that his target profession is actually Computer Programmer. Interesting correlation to his eventual fate. I wrote a short post about this story w.r.t. job displacement of SWEs a couple of years ago: https://eli.thegreenplace.net/2022/asimov-programming-and-th... reply bruce511 14 hours agoparentMy favorite Asimov. What particularly resonates with me is the idea that creators create. They create because something inside makes them do it, it's not about pay, or relationships, or people. It comes out in all kinds of ways. In mundane tasks like cooking, or music, or art. Or, in a few cases, programming. Over a long career I've come to agree with Asimovs premise. You cannot simply tell someone to create. They either have it or they don't. Equally you cannot tell a creator not to create. They will, whether you like it or not. And yes, it's a very rare attribute. Most people can be trained to do a task really well. Very few can create. Lucky indeed is the creator who gets paid to create. Having a job with the freedom to create is the ultimate success. reply kwhitefoot 14 hours agorootparent> Very few can create. I'm not convinced. I think that several things hold most people back from creation: - the risks that must be taken - the continual destruction that accompanies creation feels like a waste to many of us - other commitments I'm convinced that we are allowing vast amounts of creative ability to be stifled not merely by stultifying educational systems but simply by lack of opportunity. reply xw390111 11 hours agorootparentProbably we need a firmer definition of \"create\" but I think the main thing that holds people back is it takes a ton of work and focus to get to state-of-the-art in most fields. Many people won't be able to receive that kind of training and make that kind of investment in time for many reasons. So already it's going to be rare. Then combine that with your reasons, and it's super rare. On the other hand, the population is big, so it does happen. :) reply smeej 9 hours agorootparentI think the main thing that holds people back is an inability to imagine the world's being fundamentally different than it is, or maybe a willingness to accept the world's staying pretty much like it is. I don't actually know whether people can't create or just won't create. I hadn't read this story before, but it helped bring into relief the frustration that has always haunted me, this sense that, no, the world actually must be different than people believe it is and can be. There must be something that actually can be done, that it doesn't have to be this way and actually ought not. I've spent my whole life looking for that flash of recognition in someone else's eyes that they know it too, but have searched tens of thousands of faces in vain. reply bruce511 5 hours agorootparent>> \"I've spent my whole life looking for that flash of recognition in someone else's eyes that they know it too. That's somewhat orthogonal to creation. Although it is something some creators see, and set out to change it. But the group who can see a different, say, social structure, are distinct from creators. Creators create, not to change things, but because that's how they are. Changers can see a different path, and are frustrated that change is hard. Both are small minorities, with some overlap, but are distinct groups. Incidentally there are lots of population groups around the world, in every country. The Amish, which gave rise to this thread, are an example of that. reply KerrAvon 5 hours agorootparentprevThat’s simply not how people work. Everyone is programmed by environment and genetics. Breaking your programming requires a change in perspective (remember Alan Kay: “point of view is worth 80 IQ points”) — whether forced by circumstance, chemicals, or boredom. reply bruce511 5 hours agorootparentprevFirstly, I wouldn't use the term \"holding back\", because creators aren't further up some evolutionary, or status, or whatever tree. They are just people. But equally you misunderstand the precept. Creation is not about perfection, or skill, or success. Creation is an act. Most kids learn to cook by following a recipie. But one kid in the class just wants to mix things together to see what happens. Pretty much every kid draws on paper as a child. But some tiny fraction of them never stop. Yes, some of them get good at it, some lucky ones make a living at it. That's the urge. reply bryanrasmussen 6 hours agorootparentprevPretty much every human can create, leaving out those with specific mental disabilities. Many humans never get in a situation where there is an interesting enough problem for them to create a solution or where a need or benefit is seen for them to create to fill that need or derive that benefit. For the humans that do most of them only do so once or incredibly maybe even twice, think people who essentially make one thing - like Italo Marchiony who created the ice cream cone and I don't think much else other than a business to exploit his creation. The people who are thought of as creators are able to create quickly and a lot and often seemingly at will. This is essentially their primary skill and what sets them apart from others. Because of this skill they may even use it as the proverbial hammer looking for nails. I've written some on my views on creativity - https://medium.com/p/1484652bcc97 - generally in relation to writing and criticism probably the most relevant article is this one \"A Theory As To Why Art Is Created\" https://medium.com/p/b0a2538416e3 reply bruce511 5 hours agorootparent>> Many humans never get in a situation where there is an interesting enough problem for them to create a solution or where a need or benefit is seen for them to create to fill that need or derive that benefit. And yet new things sell really well. Which suggests that the need is there. By contrast creators look around and see need in everything. >> The people who are thought of as creators are able to create quickly and a lot and often seemingly at will. This is essentially their primary skill and what sets them apart from others. Exactly. Now don't get me wrong, 99% of their creations are rubbish. 99% of their ideas are quickly discarded. Some would require more resources than they have available. But they are forever experimenting, learning, understanding. They can't help it. reply bryanrasmussen 43 minutes agorootparentsure but the fact that some farmer has a problem with how to sell his ugly carrots and as a consequence creates baby carrots argues that all humans, disregarding specific disabilities, can create, but just like some people are better at running, others better at dancing, some people are just really good at creating. reply qwertywert_ 3 hours agorootparentprevThere are lots of reasons why some people don't think about \"creating\", maybe they had childhood issues or are too stressed, too mentally drained with other areas in life, never got nourished to love creativity or never had an inner self belief that they can create. Or they simply got \"stuck\" in something, never to get out and find some passion. What even is creating, some people (unlike many programmers or scientists) create social groups, organisations, humor. I think your idea that there are special \"creators\" and \"regular people\" is way off and honestly just a way to feel special and unique. I know people that finally found passion after they retired.. but they were too bogged down to ever figure it out earlier in life. reply zem 14 hours agorootparentprev> Equally you cannot tell a creator not to create. They will, whether you like it or not. for a much darker take on this, check out orson scott card's \"unaccompanied sonata\" reply zem 17 hours agoprevmy favourite asimov story[+], because i really love the trope of \"society may enforce conformity, but then it relies on outsiders to advance\". mercedes lackey's \"the lark and the wren\" is another good story built around this idea. [+] joint favourite with \"the martian way\", an extremely underrated story that to me exemplifies the golden age optimism around solar system exploration in a way that not even clarke manages. reply passion__desire 16 hours agoparentWhy Greatness Cannot Be Planned: The Myth of the Objective reply zem 16 hours agorootparentthanks, that looks like a book i'd enjoy! reply passion__desire 16 hours agorootparent\"The Scout Mindset: Why Some People See Things Clearly and Others Don't\" I liked this book as well. Here's Max Born supporting the mindset. I believe there is no philosophical high-road in science, with epistemological signposts. No, we are in a jungle and find our way by trial and error, building our road behind us as we proceed. We do not find signposts at crossroads, but our own scouts erect them, to help the rest. - Max Born, as mentioned in Experiment and Theory in Physics (1943) reply zem 16 hours agorootparentkipling's \"the explorer\" also glorifies (in a positive sense) this mindset https://www.kiplingsociety.co.uk/poem/poems_explorer.htm - it's literally about geographic exploration, but also about exploration of all kinds. reply walexander 7 hours agoprevMy C Instructor assigned us this in college in 2005, during a brief period where the dotcom bubble was still felt and the social network hadn't come out yet. At the time, there was a lot of talk about how programming was a dead in job in the the US and that all those jobs would just be outsourced, anyway. I've thought about it a lot over the years, and especially came back to it last year with the rise of ChatGPT. reply ozim 11 hours agoprevI think of this piece of literature every time I see people criticizing universities for not having “useful on the job curriculum” or schools not teaching “useful skills”. Useful skills might stop being useful quick, general knowledge/intelligence goes further. reply smeej 8 hours agoparentThe problem seems to be how many universities are trying to do both, but therefore failing to do either well. If you need to support yourself based on what you learn in a university, you don't want a liberal arts degree from a mid-rate university. You want some degree with a heavy practicum component. Or maybe you want a trade school. If you have the chops to make it as an independent thinker, get your liberal arts degree somewhere that actually churns out high-level thinkers. If you can't get in, reconsider whether you have the chops at all. If you're not dependent on income, though, study whatever you want. It's one of the reasons I'd love to see retirees studying things like philosophy, because they're not trying to instrumentalize it for a career, and they have enough life experience not to be hopelessly naïve about it. reply ducttapecrown 14 hours agoprevDoes anyone know of an earlier example of the trope of having a magic box choose your class in a novel? reply kouru225 11 hours agoparentOo good question. I guess you could arguably say Plato’s Republic touches on this, but its not the same. Makes me think there must be earlier examples of this though. reply zem 10 hours agoparentprevno magic box, but your comment made me think of this father brown short story: https://gutenberg.net.au/ebooks02/0201021h.html#story4 reply rnmmrnm 17 hours agoprevIndeed one of my favorites as well. Can't stop thinking about its modern angle where for example our industry is full of mediocre programmers that only really can do more of what they have already seen. I'm really longing for colleagues that actually put research effort into their code. reply pdimitar 16 hours agoparentDid you include the eternal pressure from bad managers in your... analysis, let's call it generously? Did it occur to you that people want to put research into their code but are not given time to do so? And finally, did it occur to you that \"just leave and find a better company\" is not possible for everyone? Not to mention that most companies are mediocre-to-bad so the \"better\" companies are a small and very finite pool. Comments like yours paint a wrong picture and are not helpful or productive. reply swatcoder 16 hours agorootparentI see the same approach to the craft when people talk about personal projects or their own startup efforts. You seem to have taken the OP's comment very personally, and I'm sorry if you're stuck in a job that forces you to compromise your practice even when you know better. But there's also a (admittedly familiar) problem of a prolonged growth boom flooding the industry with bad engineers that produce fragile work and have poor foundational knowledge. reply pdimitar 16 hours agorootparentYes, both can be and are true at the same time. It's not an either/or. reply rnmmrnm 16 hours agorootparentprevI get hurt by managers messing with my scope every day. It is a struggle :). Because of it I also find solace in open source where I am the boss of everything. But, I don't give up. I pitch my bosses research trails all of the time. 90% they don't hit the mark, mostly because I don't try to conform to the whatever roadmap the product team has laid out. But once in a while my ambition and their plan converge, and this is when I get to play out the really cool stuff I do at my job. reply pdimitar 16 hours agorootparentCool, I try to do the same and yeah, my success rate is roughly the same. :) reply northwest65 13 hours agorootparentprevNo, I don't think they do paint a wrong picture, and it is helpful for people to say it so others realise they're not the only ones thinking it. reply initramfs 16 hours agoparentprevI think of another modern angle, in that Neura-link tech is being promoted, which is a path towards indoctrination, and ironically, the AI Safetyists that criticize unregulated AI/unregulated tech are somewhat indoctrinators themselves. I try to avoid using that term (as it suggests too much). It's more out of the question as to when a student has reached the ability to think independently, that they no longer need a basic amount of instruction to work (and self-study) in other fields. reply xw390111 11 hours agoparentprevI think it's also a direct consequence of scale and the commoditization of most software. You can't have people on a team of 10 all going in their own direction on their own schedule. It's too unpredictable and there are deadlines and contracts to fulfill. And for most commercial software, I don't need the team to research how to do it. We know exactly how to do it. We need the team to execute the plan. It absolutely sucks, but that's the reality of most coding jobs. At least in my experience. reply garyrob 17 hours agoprevOne of my favorite Asimov stories. reply Cheer2171 13 hours agoprev [–] > Profession, copyright ©1957 [...] Used by permission of Doubleday, a division of Random House, Inc. This is my first encounter with abelard.org. While it is delightful in a kind of early internet culture way, I sincerely doubt Random House, Inc. gave permission to reproduce this in full to this pseudonymous author who writes in the persona of a space alien who is the reincarnation of an 11th century philosopher monk and, well, I'll just share their words: [1] > abelard is thought to live in a ger on the steppes of outer mongolia, surrounded by an indeterminate number of yaks, husbands and wives... and vast hordes of children preparing for the cultural conquest of the world... abelard is known to have long conversations with the good fairy. it is rumoured that abelard arrived on a cultural troubleshooting mission to this planet from a star system approximately 40 light years away: this prior to the earth being offered probationary membership of The Galactic Anarchy (some call it The Culture). > as abelard is from an advanced culture, this entity has to communicate in rather simple language in order to be understood by the savages... consequently there are various rumours... one is that The Culture have forgotten all about this attaché, who often gets homesick and pissed off with living in such a primitive backwater. another is that this entity is a reincarnation of another Abelard born in 1079.... both of these rumours are true to a related degree. [1] https://www.abelard.org/choose/choose.htm reply glompers 10 hours agoparent [–] Not sure if this is the correct copyright law citation as IANAL. If correct, then the story's copyright expired before Abelard posted it. 304. Duration of copyright: Subsisting copyrights (a) Copyrights in Their First Term on January 1, 1978.— (1)(A) Any copyright, in the first term of which is subsisting on January 1, 1978, shall endure for 28 years from the date it was originally secured. https://www.copyright.gov/title17/92chap3.html reply incompatible 3 hours agorootparent [–] Not if the copyright was renewed. She the Hirtle chart: Published in the US, 1929 through 1963, Published with notice and the copyright was renewed, 95 years after publication date. https://guides.library.cornell.edu/copyright/publicdomain reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The sci-fi short story \"Profession\" by Isaac Asimov delves into themes like education, societal norms, and career specialization through the protagonist George Platen.",
      "George's initial desire to be a Computer Programmer shifts when a mind analysis steers him towards a different path as a Registered Laborer, prompting him to challenge societal expectations with the help of Ingenescu, a historian and social scientist.",
      "The narrative underscores the significance of comprehending human behavior, societal advancement, and historical insights in molding individuals' destinies."
    ],
    "commentSummary": [
      "The conversation spans WWII efforts, the Apollo program, refrigeration technology's influence on daily life and military logistics, creativity, and challenges in the tech industry.",
      "It emphasizes the potential for everyone to become creators and discusses copyright laws' impact, touching on education, personal growth, and industry intricacies.",
      "Interweaving references to books and narratives, the discussion underscores the significance of continuous learning and highlights the nuances across different sectors."
    ],
    "points": 126,
    "commentCount": 57,
    "retryCount": 0,
    "time": 1710083091
  }
]
