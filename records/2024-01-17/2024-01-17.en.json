[
  {
    "id": 39016395,
    "title": "\"Cat and Girl\" Comic Licensed Under Creative Commons",
    "originLink": "https://catandgirl.com/4000-of-my-closest-friends/",
    "originBody": "Cat and Girl Home Archive Store Patreon Donation Derby Random Comic 4,000 of My Closest Friends January 16, 2024 Comments First Previous Next Last Add comment Email Name Website Comment Send Cancel Cat and Girl is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 2.5 License. Subscribe RSS: EntriesComments Email: dorothy[at]catandgirl.com",
    "commentLink": "https://news.ycombinator.com/item?id=39016395",
    "commentBody": "On being listed as an artist whose work was used to train Midjourney (catandgirl.com)669 points by earthboundkid 16 hours agohidepastfavorite785 comments madeofpalk 15 hours ago> But I can't even get cartoons to most people for free now, without doing unpaid work for the profit-making companies who own the most use channels of communication This is the sticking point for me. OpenAI isn't a profit-making company, but it's certainly a valuable company. A valuable company that is built from the work of content others created without transferring any value back to them. Regardless of legalities, that's wrong to me. Put it this way - you remove all the copyrighted, permission-less content from OpenAIs training, what value does OpenAI's products have? If you think OpenAI is less valuable because it can't use copyrighted content, then it should give some of that value back to the content. reply sandworm101 15 hours agoparent>> If you think OpenAI is less valuable because it can't use copyrighted content, then it should give some of that value back to the content. But we are allowed to use copyrighted content. We are not allowed to copy copyrighted content. We are allowed to view and consume it, to be influenced by it, and under many circumstances even outright copy it. If one doesn't want anyone to see/consume or be influenced by one's copyrighted work, then lock it in a box and don't show it to anyone. I have some, but diminishing sympathy for artists screaming about how AI generates images too similar to their work. Yes, the output does look very similar to your work. But if I take your work and compare it to the millions of other people's work, I'd bet I can find some preexisting human-made art that also looks similar to your stuff too. This is why clothing doesn't qualify for copyright. No matter how original you think your clothing seems, someone in the last many thousands of years of fashion has done it before. Visual art may be approaching a similar point. No matter how original you think your drawings are, someone out there has already done something similar. They may not have created exactly the same image, but neither does AI literally copy images. That reality doesn't kill visual arts as it didn't kill off the fashion industry. reply tedivm 15 hours agorootparentI firmly believe that training models qualifies as fair use. I think it falls under research, and is used to push the scientific community forward. I also firmly believe that commercializing models built on top of copyrighted works (which all works start off as) does not qualify as fair use (or at least shouldn't) and that commercializing models build on copyrighted material is nothing more than license laundering. Companies that commercialize copyrighted work in this manner should be paying for a license to train with the data, or should stick to using the licenses that the content was released under. I don't think your example is valid either. The reason that AI models are generating content similar to other people's work is because those models were explicitly trained to do that. That is literally what they are and how they work. That is very different than people having similar styles. reply chx 13 hours agorootparent> I firmly believe that training models qualifies as fair use There's a hell lot of money to be made from this belief so of course the HN crowd will hold it. Some of us here who have been around the copyright hustle for a little longer laugh at this bitterly and pray that the courts and/or Doctorow's activism saves us. But there's so much money to be made from automatized plagiarism and the forces against are so weak, the hope is not much. The world will be a much, much poorer place once all the artists this view exploits will stop making art because they need to make a living. See https://twitter.com/molly0xFFF/status/1744422377113501998 and https://i.imgur.com/zOOcPCi.jpg reply tedivm 7 hours agorootparentI literally met and worked with Doctorow on a protest back in 2005, so I'm not exactly new to this. I also think that the only way you could have written your comment was by grossly misinterpreting my comment. reply Dylan16807 9 hours agorootparentprev> There's a hell lot of money to be made from this belief so of course the HN crowd will hold it. That is a pretty unfair response when you're skipping the part about how commercializing should not be fair use. reply MrNeon 1 hour agorootparentprevI hope the idea of Intellectual Property as a whole is thrown out the window and copyright with it. reply c0pium 10 hours agorootparentprevGenerative models are just a tool. Artists are mad because this tool empowers other people, who they view as less talented, to make art too. The camera and 1-hour film developing didn’t destroy oil paintings, it just enabled more people to have control over what was on their walls. reply eek2121 9 hours agorootparentNo, I have to disagree here. I'm not an artist, but I respect the creations of others. OpenAI does not. They could have trained on free data, but it did not want to because it would cost more (finding humans to find/sanction said data, etc). reply romwell 4 hours agorootparentprev>Generative models are just a tool. Sure. It's just a tool. That need other people's art to work. If it's \"just a tool\" in and of itself, then there's no problem keeping it away from other people's art. >The camera and 1-hour film developing didn’t destroy oil paintings Because the copyright laws were extended to include photographic reproduction of art as something you need to obtain a permission (and a license) for. The same needs to happen for generative AI. A photocopy machine is just a tool too. So is the printing press. reply candiodari 2 hours agorootparent> Sure. It's just a tool. That need other people's art to work. So does a human brain. Which brings us to the other side of the reasoning is that tools like Midjourney and OpenAI enable idiots (when it comes to drawing/animating ... that includes me) to create engaging artwork. Recently generating artwork like that went from almost impossible to \"great, but easily recognizeable as AI artwork\". Frankly, I expect the discussion will end when it stops being recognizeable. I hate Andreesen Horowitz' reasoning, but they're right about one thing: once we have virtual artists that are not easy to distinguish from \"real\" ones, the discussion will end. It does not really matter what anyone's opinion on the matter is as it will not make a difference in the end. reply vkou 2 hours agorootparent> So does a human brain. A major difference between a human training himself by looking at art, and a computer doing it, is that the human ends up working for himself, the computer is owned by some billionaire. One enhances the creative potential of humanity as a whole, the other consolidates it in the hands of the already-powerful. Another major difference is that a human can't use that knowledge to mass-produce that art at a scale that will put other artists in the poorhouse. The computer can. Copyright exists to benefit humanity as a whole... And frankly, I see no reason for why a neural network's output should be protected by copyright. Only humans can produce copyrightable works, and a prompt is not a sufficient creative input. reply WereAllMadHere 2 hours agorootparentVisual artists cannot create without tools. Whether that tool is a brush and paint, a camera, or a neural network. Whether an artist pays for a subscription to openAI or buys paint pots on Amazon.com money is going to a billionaire, it is not a difference between ai and other art. You are also ignoring the existence of non-commercial open source AI, they exist. Regarding copyright, we copyright output not input. Otherwise most photography would be uncopyrightable. reply com2kid 1 hour agorootparentOne small nitpick: It is completely possible for an artist to make all of their own tools, and indeed for the majority of history that is exactly how things went. reply vkou 2 hours agorootparentprevThere's a substantive difference in whether the artist is using the tool, or the tool works on its own. A paintbrush doesn't produce a painting by itself, a human needs to apply an incredibly specialized creative skillset, in conjunction with the paintbrush to do so. An LLM takes a prompt and produces a painting. No sane person would say that I 'drew' the painting in question, even if I provided the prompt. > Regarding copyright, we copyright output not input. Otherwise most photography would be uncopyrightable. We copyright things that require creative input. A list of facts or definitions did not require creative input, and is therefore not copyrightable. Using an LLM does not meet the bar for creative input. reply flir 53 minutes agorootparent> There's a substantive difference in whether the artist is using the tool, or the tool works on its own. A paintbrush doesn't produce a painting by itself, a human needs to apply an incredibly specialized creative skillset, in conjunction with the paintbrush to do so. That sounds like a kinder restatement of the opinion at the top of the thread: \"Artists are mad because this tool empowers other people, who they view as less talented, to make art too.\" Artists might not like the phrasing, but scratch the surface and there's a degree of truth there. It's an argument from self-interest, at core. reply Aeolun 14 hours agorootparentprevIf I read a lot of stories in a certain genre that I like, and I later write my own story, it’s almost by definition going to be a mish-mash of everything I like. Should I pay the authors of the books I read when I sell mine? reply bluefirebrand 14 hours agorootparentWe shouldn't hold individual humans and ML models to the same standards, because ML models themselves are products capable of mass production and individual humans are not even remotely at the same scale. If you write that book, chances are you will gain some fans that are also fans of other authors in that genre. If ML models write that genre, they can flood that genre so full that human artists won't be able to complete. It's not even a remotely equivalent scenario reply bawolff 13 hours agorootparentI feel like the issue here, is you are giving AIs agency. AIs are not magic. They are tools. They are not alive, they do not have agency. They do not do things by themselves. Humans do things, some humans use AI to do those things. Agency always rests with a combination of the tool's creator and operator, never the tool itself. Is there really a difference between a human flooding the market using AI and a human flooding the market using a printing press? Even if human's can't compete (An obviously untrue premise from my perspective, but lets assume it for the sake of argument), is that a bad thing? The human endeavor is not meant to be a make work project. Humans should not be forced to pointlessly toil out of protectionism when they could be turning their attention to something that can't be automated. reply freejazz 5 hours agorootparentIt'd be a good point if it wasn't for the fact that search engines didn't exist until google, because of technology, and that courts didn't need to consider the issue until then. So where does your point get us? We are here now. reply scheeseman486 4 hours agorootparentSearch engines are an index, which have existed for centuries. reply bawolff 4 hours agorootparentWhen i was in university, i remember there was this humanities professor who had a concordance for the iliad on his shelf. As a CS person it was so cool to see the ancient version of a search engine. reply johnnyanmac 12 hours agorootparentprev>Is there really a difference between a human flooding the market using AI and a human flooding the market using a printing press? A magnitude of difference, yes. Even a printing press will be limited by natural resources, which require humans to procure. A computer server can do a lot more with a lot less. And is much easier to scale than a printing press. >Even if human's can't compete (An obviously untrue premise from my perspective, but lets assume it for the sake of argument), is that a bad thing? When the AI can be argued to be stealing human's work, yes. A printing press didn't need to copy Shakespeare to be useful. And it'd benefit Shakespeare anyways because more people get to read about his works. So far I don't see how AI benefits artists. Optimistically: -an artist can make their own market? Doubtful, they will be outgunned by SEO optimized ads from corporations. - they can make commissions faster? To begin with commissions aren't a sustainable business. Even if they 5x the labor and somehow kept the same prices they aren't living well. But in reality, they will get less business as people will AI their \"good enough\" art and probably won't pay as much for something not fully hand drawn - okay, they can make bigger commissions? There's a drama about spending 50k on a 3 minute AMV, imagine if that could be done by a single artist in a day now!... Well, give it another 10 years. Lot of gen Ai is static assets. Rigging or animating is still far from acceptable quality, and a much harder problem space. I also wouldn't be surprised if by then any AI models has its own phase of enshittification and you end up blowing hundreds, thousands anyway. ----- >Humans should not be forced to pointlessly toil out of protectionism when they could be turning their attention to something that can't be automated. Until someone conceptualizes a proper UBI scheme, pointlessly toiling is how most of the non-elite live. I have yet to hear of a real alternative for these misplaced artists to move towards. So what? So we all just become managers in meetings in 30 years? reply csallen 14 hours agorootparentprevComputers and machines have been capable of mass production for decades, and humans have used them as tools. In the past 170 years, these tools of mass production have already diminished many thousands of professions that were staffed by people who had to painstakingly craft things one at a time. Why is art some special case that should be protected, when many other industries were not? Why should we kill this technology to protect existing artistic business models, when many other technologies were allowed to bloom despite killing other existing business models? Nobody can really answer these questions. reply johnnyanmac 13 hours agorootparent>Why is art some special case that should be protected, when many other industries were not? Because in this case the art is still necessary for the machine to work. You don't need horse buggies to make a car, nor existing books to make a printing press. You DO need artist's art to make these generative AI tools work. If these worked purely off of open source art or from true scratch, I wouldn't personally have an issue. >Why should we kill this technology to protect existing artistic business models, We don't need to kill it. Just pay your dang labor. But if we are treating proper compensation as stifling technology, I'm not surprised people are against it. Maybe in the 2010's tech would have the goodwill to pull this off in PR, but the 2020's have drained that goodwill and then some. Tech's made so many promises to make lives easier and now they joined the very corporations they claimed to fight against. >Nobody can really answer these questions. Well it's in courts, so someone is going to answer it soon-ish reply csallen 12 hours agorootparent> We don't need to kill it. Just pay your dang labor. > But if we are treating proper compensation as stifling technology, I'm not surprised people are against it. That's just it, nobody looking to get paid by OpenAI actually did any labor for OpenAI. They did labor for other reasons, and were happy with it. OpenAI found a way to benefit by learning from these images. The same way that every artist on the planet benefits by learning from the images of their fellow artists. OpenAI just uses technology to do it much more efficiently. This has never been considered labor in the past. We've never asked artists to \"properly compensate\" each other for learning/inspiration in the past. I don't know why it should be considered labor or proper compensation now. But we shall see what the courts decide! reply egypturnash 11 hours agorootparentThere are many ways an artist can compensate their influences. Some of them are monetary. When discussing our work, we can name them. When one of our influences comes out with a new body of work, we can gush about it to our own fans. When we find ourselves in a position of authority, we can offer work to our influences. No animation studio is really complete without someone old enough to be a grandfather hanging out helping to teach the new kids the ropes in between doing an amazing job on their own scenes, and maybe putting together a few pitches, for instance. We can draw fan art and send it to them. None of these are mandatory, but artists tend to do this, because we are humans, and we recognize that we exist in a community of other artists, and these all just feel like normal human things to do for your community. And if an artist suddenly starts wholesale swiping another artist's style without crediting them, their peers get angry. [1] 1: https://en.wikipedia.org/wiki/Keith_Giffen#Controversy OpenAI isn't gonna tell you that it was going for a Cat & Girl kind of feel in this drawing. OpenAI isn't gonna offer Dorothy Gambrell a job. OpenAI isn't going to tell you that she just came out with a new collection and she's still at the top of her game, and that you should buy it. OpenAI's not going to send her a painting of Cat & Girl that it did for fun. OpenAI isn't going to do anything for her unless the courts force it to, because OpenAI is a corporation who has found a way to make money by strip-mining the stuff people post publicly on the Internet because they want other humans to be able to see it. reply csallen 10 hours agorootparentMost people know 20,000-40,000 words. Let's call it 30,000. You've learned 99.999% of those 30,000 people from other people. And don't get me started on phrases, cliches, sentence structures, etc. How many of those words do you remember learning? How many can you confidently say you remember the person or the book that taught you the word? 5? 10? Maybe 100? That's how brains work. We ingest vast amounts of information that other people put out into the world. We consume and it incorporate it and start using it on our own work. And we forget where we even got it. My brain works this way. Your brain works this way. Artists' brains work this way. GPT-4 works this way. The idea that a visual artist can somehow recall where they first saw many of the billions of images stored in their brain -- the photos, movies, architecture, paintings, and real-life scenes that play out every second of every day -- is laughable. Almost all of that goes uncredited, and always will. This is what it is to learn. reply Dylan16807 8 hours agorootparentIndividual words aren't comparable to the things people are worried about getting copied. People are much more able to tell you where they learned about more sophisticated concepts and styles. reply csallen 7 hours agorootparentThe same principle applies, though. They can tell you maybe a dozen, maybe a few dozen, concepts they've learned and use in their work. But what about the thousands of concepts they use in their work they can't tell you about? The patterns they've noticed, the concepts that don't even have names, but that came from seeing things in the world world that were all created by other people? reply WereAllMadHere 2 hours agorootparentFor example, how many artists drawing street scenes credit the designer at Ford Motors for teaching them what a generic car looks like? How many even know which designers created their mental model of a car? reply tga_d 8 hours agorootparentprevI tend to fall more on the \"training should be fair use\" side than most, but your comment seems to be missing the point. Nobody is arguing that models are violating copyright or social norms around credit simply because they consume this information. Nobody ever argued/argues that the traditional text generation in markov models on your phone's keyboard runs afoul of these issues. The argument being made is that these particular models are now producing content that very clearly does run into these norms in a qualitatively different way. You cannot convincingly make the argument that the countless generated \"X, but in the style of Y\" images, text, and video going around the internet are exclusively the product of some unknowable mishmash of influences -- there is clearly some internalized structure of \"this work has this name\" and \"these works are associated with this creator\". To take it to an extreme, you obviously can't just use one of the available neural net lossless compression algorithms to circumvent copyright law or citation rules (e.g., distributing a local LLM that helpfully displays the entirety of some particular book when you ask it to), you can't just tweak it to make it a little lossy by changing one letter, or a little more lossy than that, etc., while on the other hand, any LLM that performs exactly the same as a markov model would presumably be fine, so there is a line somewhere. reply csallen 7 hours agorootparentA company hires an artist. That artist has observed a ton of other artists' work over the years. The company instructs that artist to draw, \"X but in the style of Y\", where Y is some copyrighted artwork. The company then prints the result and puts it on their packaging. A company builds an AI tool. That AI tool is trained on a ton of artists' work over the years. The company opens up the AI tool and asks it to draw, \"X but in the style of Y,\" where Y is come copyrighted artwork. The company then prints the result and puts it on their packaging. What's the difference? I'd argue there isn't one. The copyright infringement isn't the ability of the artist or the AI tool to make a copy. It's the act of actually using it to make a copy, and then putting that out into the world. reply throwaway2037 3 hours agorootparent> What's the difference? Ultimately, only high courts in each jurisdiction can decide. I can imagine a case where some highly advanced nations decide different interpretations that cause conflict. Then, we need an amendment to the widely accepted international copyright rules, the Berne Convention. Ref: https://en.wikipedia.org/wiki/Berne_Convention reply pbhjpbhj 2 hours agorootparentprevThe artist has a claim for production of a derivative work and for passing off against the other artist. reply tga_d 6 hours agorootparentprevOkay, but then that's an an argument subject to the critiques made upthread that you were initially trying to dismiss? You can't claim that AI doesn't need to worry about citing influences because it's just doing a thing humans wouldn't cite influences for, then proceed to cite an example where you would very much be expected to cite your influences, and AI wouldn't, as evidence. reply csallen 4 hours agorootparentI never argued that AI doesn't need to worry about citing influences. If I am a person using a tool to create a work, and the final product clearly resembles some copyrighted work that I need to reference and give credit to, what does it matter if my tool is a pencil, a graphics editing program, a GPT, or my own mind? I can cite the work. reply tga_d 3 hours agorootparentLike I said, this is exactly what the comment you first replied to was explaining. It is very clearly not the same as a pencil or a graphics editing program, because those things do not have a notion of Cat & Girl by Willem de Kooning embedded in them that they can utilize without credit. It is clearly not the same as your mind, because your mind can and, assuming you want to stay in good standing, will provide credit for influence. Again, take it back to basics: do you believe it is permissible to share a model itself (not the model output, the model), either directly or via API, that can trivially reproduce entire copyrighted works? reply KingMob 2 hours agorootparentprev> We've never asked artists to \"properly compensate\" each other for learning/inspiration in the past. LLMs are collections of GPUs crunching numbers. \"Inspiration\" doesn't really apply to them. A better analogy is sampling, and musicians remixing music are very much required to pay for the samples they use. reply guappa 3 hours agorootparentprev> They did labor for other reasons, and were happy with it. They were happy until their copyright got stolen, I guess. Then got unhappy. reply johnnyanmac 12 hours agorootparentprev>They did labor for other reasons, and were happy with it. True, sadly most of those copyright are probably owned by other megacorp. So they either collude to surppess the entire industry or eat each other alive in legal clashes. The latter is happening as we speak (the writers for NYT are probably long retired, but NYT still owns the words) so I guess we'll see how that goes. >OpenAI found a way to benefit by learning from these images. The same way that every artist on the planet benefits by learning from the images of their fellow artists. If we treat AI like humans, art historically has an equally thin line between inspiration and plagiarism. There are simply more objective metrics to measure now because we can indeed go inside an AI's proverbial brain. So the metaphor is pretty apt, except with more scrutiny able to be applied. reply ClumsyPilot 10 hours agorootparentprev> That's just it, nobody looking to get paid by OpenAI actually did any labor for OpenAI. They did labor for other reasons, and were happy with it Nobody working on a new cancer drug actually did any work for me. They did labour for other reasons, and were happy with it. There it is okay for me to steal their recipe and sell their cancer drug. reply Aeolun 10 hours agorootparentNope, but it’s ok for you to read their recipe if they place it on the internet (research paper), and use it to make your own drug. reply WereAllMadHere 2 hours agorootparentThe entire point of the patent system was to say inventors can put their design on the net without it being stolen; so future inventors can build on their work. reply s1artibartfast 9 hours agorootparentprevAnd that is a good thing we should all celebrate. reply bluefirebrand 14 hours agorootparentprev> Why is art some special case that should be protected, when many other industries were not? It shouldn't be. As soon as someone makes an AI that can produce it's own artwork without requiring ingesting every piece of stolen artwork it can, then I'm on board. But as long as it needs to be trained on the work of humans it should not be allowed to displace those people it relied on to get to where it is. Simple as that. reply csallen 14 hours agorootparentAre there any humans that can produce artwork without ingesting inspiration from other art? Do you know any artists that lived in a box their whole life and never saw other art? Do you know any writers who'd never read a book? Are they any human artists who can't, if requested, draw or write something that's a copy of some other person's drawings or writings? Also, FYI, you can't steal digital artwork. You can only commit copyright infringement, which is not the same crime as theft, because theft requires depriving the owner of something in their possession. reply bluefirebrand 14 hours agorootparent> Are there any humans that can produce artwork without ingesting inspiration from other art? Do you know any artists that lived in a box their whole life and never saw other art? Do you know any writers who'd never read a book? > Are they any human artists who can't, if requested, draw or write something that's a copy of some other person's drawings or writings? This still is pretending that humans and AI models are equivalent actors and should have the same rights Emphatically no they shouldn't. The capabilities are vastly different. Fair use should not apply to AI. reply Ajedi32 13 hours agorootparentThis isn't about giving \"rights\" to machines. Machines are just tools. The question is about what humans are allowed to do with those tools. Are humans using AI models and humans not using AI models equivalent actors that should have the same rights? I'd argue emphatically yes they should. reply mlyle 13 hours agorootparentThe thing is, we already have doctrine that starts to encompass some of these concepts with fair use. The four pronged test in US case law: - the purpose and character of use (is a machine doing this different in purpose and character? many would say yes. is \"ripping-off-this-artist-as-a-service\" different than an isolated work that builds upon another artist's art?) - the nature of the copyrighted work - the amount and substantiality of the portion taken (can this be substantially different with AI?) - the effect of the use upon the potential market for the original work (might mechanization of reproducing a given style have a larger impact than an individual artist inspired by it?) These are well balanced tests, allowing me as a classroom teacher to duplicate articles nearly freely but preventing me from duplicating books en masse for profit (different purpose; different portion taken; different impact on market). reply freejazz 5 hours agorootparentThe problem with this conversation is that its being had by people that make the top level comment here stating that clothing is not copyrightable. It is. Clothing design is copyrightable. This was a huge recent case, Star Athletica. They know nothing about copyright law and they just build intuitions from the world around them, but the intuitions are completely nonsense because they are made in ignorance of the actual law and what the law does and why the law does it. I find it exhausting. reply throwaway17_17 4 hours agorootparentYour sentiment is probably correct in that there are many aspects of copyright law that are not strictly aligned with the public’s intuition. But your example is a bit of a reach. Star Athletica was a relatively novel holding that allows for a specific piece of clothing, when properly argued, could qualify as copyrightable as a semi-sculptural work of art, however this quality of a given piece is separate to its character as clothing. In fact, the USSC in Star Athletica explicitly held a designer/manufacturer has “no right to prohibit any person from manufacturing [clothing] of identical shape, cut, and dimensions” to clothing which they design/manufacture. That quote is directly from a discussion of the ability to apply copyright protections to clothing design. I think the end result is that trying to argue technical legal issues around a poorly implemented statutory regime is always fraught with errors. That really leave moral and commercial arguments outstanding and advocacy should try and focus on that, when not fighting to affect change in the law these copyright determinations are based on. And just to be clear, this post does not constitute legal advice. reply mlyle 4 hours agorootparentprevYou're dismissing my comment because of what someone else said upthread? I hate the desire to meta-comment about the site rather than argue on the merits. We obviously don't know so much about how courts will interpret copyright with LLMs. There's a lot of arguments on all sides, and we're only going to know in several years after a whole lot of case law solidifies. There are so many questions, (fair use, originality, can weights be copyrighted? when can model output be copyrighted? etc etc etc). Not to mention that the legislative branch may weigh in. This discourse by citizens who are informed about technology is essential for technology to be regulated well, even if not all participants in the conversation are as legally informed as you'd wish. Today's well-meaning intuition about what deserves copyright and why inform tomorrow's case law and legislation. reply Ukv 14 hours agorootparentprev> Emphatically no they shouldn't. The capabilities are vastly different. Fair use should not apply to AI. Fair use applies even to use of traditional algorithms, like the thumbnailing/caching performed by search engines. If I make a spam detector network, why should it not be covered by fair use? reply bawolff 13 hours agorootparentFair use applies to humans and the things they do (including AI). It is not something that applies to algorithms in themselves. AI's are not people, the people who use them are people and fair use may or may not apply to the things they do depending on the circumstances of whatever it is they do. The agent is always the human not the machine. reply Ukv 13 hours agorootparentTrue; consider the \"it\" in my question (\"If I make a spam detector network, why should it not be covered by fair use?\") as \"my making (and usage) of the network\". reply rideontime 14 hours agorootparentprevNo idea on the legality, but common sense suggests that the difference would be that a spam detector doesn't replace the products that it was trained on, while AI-generated \"art\" is intended to replace human artists. reply Ukv 13 hours agorootparent> common sense suggests that the difference would be that a spam detector doesn't replace the products that it was trained on The extent to which it supplants the original work is one of the fair use considerations. I think it'd make more sense to have a stance of \"current LLMs and image generators should be judged by fair use factors and I believe they'd fail\", though I'd still disagree, instead of having machine learning models subject to a different set of rules than humans and traditional algorithms. reply rideontime 13 hours agorootparentThat is indeed the most common stance. There isn't nearly as much outcry over, say, image classification by LLMs, as there is over AI \"art\" generation. reply shagie 13 hours agorootparentprevThe question is \"is it a derivative work of the original?\" - not if it is a generative work. If that was the distinction to be made, using ChatGPT as a classifier would be acceptable while using it to write new spam (see the \"I am sorry\" amazon listings of the other day) would be unacceptable. If two different uses of a tool allow for both infringing and non-infringing uses (are photocopiers allowed to make copies(!) of copyrighted works?) it has generally been the case that the tool is allowed and the person with agency to either use the copyrighted work in an infringing or a non-infringing way is the one to come under scrutiny. I believe that if it is found that OpenAI is found to have committed copyright infringement in training the model, then an argument that training a model on spam be considered to be copyright infringement could be reasonably constructed. If, on the other hand, OpenAI is found to have sufficiently transformative in its creation of the model and some uses are infringing, then it is the person who did the infringing (as with a photocopier or a printer printing off a copy of a comic from the web) that should be have legal consequences. reply theLiminator 12 hours agorootparentYeah, I really think it should fall on the user as opposed to the tool. reply eggdaft 2 hours agorootparentprev> Are there any humans that can produce artwork without ingesting inspiration from other art? Logically, the answer to this is (almost certainly) yes, so you’ll need to discount this argument. If the answer were no, then either an infinite number of humans have lived (such that there was always a previous artist to learn from), or it was true in the past but false in the present, which seems unlikely given humans brains have generally become more and not less sophisticated over time. I presume what you’re missing here is that the brain can be inspired from other sources than human art. For example: nature; life experience; conversation. Not making any other comment about what machines can or can’t do, just wanted to point out this argument is invalid as it comes up a lot and is probably grounded in ignorance around the artistic process. It’s such a strange idea to suggest that the artist process is ingesting lots of art to make more art. That’s such a weird world view. It’s like insisting every artist is making art the way Quentin Tarantino makes films. I’ve spent a lot of time with artists, I’ve worked with them, I’ve been in relationships with artists, and I can tell you the great ones see the world differently. There’s something about their brains that would cause them to create art even if born on a desert island without other human contact. Some of them don’t even take an interest in other art. In fact, those artists that _do_ make art heavily based on other artists’ work as suggested are often derided as “derivative” and “unoriginal”. reply guappa 3 hours agorootparentprev> Are there any humans that can produce artwork without ingesting inspiration from other art? Do you think art was there before humans? Or humans made art? If you believe the 1st proposition… please tell me about your very unique religion! If not… you've answered your own question. reply ClumsyPilot 10 hours agorootparentprev> Are there any humans that can produce artwork without ingesting inspiration from other art? This sounds so detached from human experience that I am tempted to ask if you are a human or just a disembodied spirit that haunts the internet. When the first neanderthal drew a deer on the walls of a cave, where did they get inspiration? When a little child draws a tree for the first time, where do they draw inspiration? Do you think they were reviewing works of Picasso? When the firm man made an axe, chopped a tree, made a bed, sown some clothes, discovered fire, where did they draw inspiration? Do you not have eyes, ears, do you not perceive and get inspiration from the natural world around you? reply com2kid 1 hour agorootparent> When a little child draws a tree for the first time, where do they draw inspiration? Do you think they were reviewing works of Picasso? Are we going to discount the hundreds to thousands of artistic pictures children are exposed to? Or how about the teacher sitting up front demonstrating to the class how to draw a tree? > Do you not have eyes, ears, do you not perceive and get inspiration from the natural world around you? Learning to see as an artist is a distinct skill. Being able to take the super compressed simplified world view that mind sees and put something recognizable on paper is a specialized skill that has to be developed. That skill is developed by doing it over and over again, often by copying the style of an artist that someone enjoys. Or to put it another way, go to any period in history prior to the mid 20th century and art in a given region starts to share the same style, dramatically so, because people were inspired by each other, almost to a comical extent. (Financial reasons also had something to do with it as well of course, Artists paint/carve/engrave/etc what sells!) reply Aeolun 10 hours agorootparentprevYeah, but that’s not really your sole source of inspiration. My son has been ‘inspired’ by the art of all other kids in his kindergarden. Certainly by the time he gets to the age where he does it professionally he’s been inspired by an uncountable number of people. reply ClumsyPilot 10 hours agorootparentWhat % is his independent inspiration? 30%? 90%? There are certainly people for whom it was 90%. For most we don’t know. We do know one thing for sure - that for AI it’s 0% reply mlyle 3 hours agorootparentWe don't know what percentage is independent inspiration for a person using the AI to create art. Once upon a time it was a contentious idea that humans had significant authorship in photographs, which merely mechanically captured the world. What % is the camera's independent inspiration? Here, we have humans guiding what's often a quite involved process of synthesis of past human (and machine) creation. reply freejazz 5 hours agorootparentprevBeing inspired isn't against the law. copying is. it'd be one thing if this conversation could be had with useful terminology that's actually on point. instead we have you, insisting that there is no creative process, there is only experiencing other art and inevitably copying (because apparently you think that's the only thing humans can do!). It's all so telling. Yet its tragic because so many here don't even realize it. I'm sad for your inability to engage with creativity and creative acts. reply mlyle 4 hours agorootparentI think a lot of the discussion is where the balance of the creativity lies when a human uses a model (trained on other artistic works) to create art. Is the result a copy, or perhaps a derivative work of the art in the training set? Does the person using the model have authorship of the result? Was it even okay to use the art to train the model and then share the resulting weights? Are the resultant weights protected by copyright themselves? I suspect the actual answers we'll come to on these topics will be full of nuance. reply Ukv 14 hours agorootparentprev> But as long as it needs to be trained on the work of humans it should not be allowed to displace those people it relied on to get to where it is. Simple as that. Do you feel the same way about tools like Google Translate? reply bluefirebrand 14 hours agorootparentTbh I'm not familiar enough with how Google Translate is built, but if it's ingesting tons of people's work without their permission so it can be used to replace them then yes I do. reply shadowgovt 14 hours agorootparentFor what it's worth: that's pretty much how Translate works. Translate operates at a large-chunk resolution, and one of the insights in solving the problem was the idea that you can often get a pretty-good-enough translation by swapping a whole sentence for another whole sentence. So they ingest vast amounts of pre-translated content (the UN publications are a great source, because they have to be published in the language of every member nation), align it for sentence- and paragraph-match, and feed the translation engine at that level. It's created an uncanny amount of accuracy in the result, and it's basically fed wholesale by the diligent work of translators who were not asked their consent to feed that beast. Almost nobody bats an eye about this because the value (letting people using different languages communicate with each other) grossly outstrips the opportunity cost of lost human translator work, and even the translators are, in general, in favor of it; they aren't going to be displaced because (a) it doesn't really work in realtime (yet), (b) it can't handle any of the deeper signal (body language, tone, nuance) of face-to-face negotiation, and (c) languages are living things that constantly evolve, and human translators handle novel constructs way better than the machines do (so in high-touch political environments, they matter; the machines have replaced translators in roles like \"rewriting instruction manuals\" that were always pretty under-served in the first place). reply amplex1337 11 hours agorootparentI would argue that Translate being fed by paid UN translators who likely agreed to the use of their transcriptions in a TOS or something is not an equal comparison to unpaid artists having their art submitted online to sites which become part of a training set used in for-profit models such as OpenAI, that they never consented to. OpenAI is a nonprofit parent company, but this spawned a child for-profit company OpenAI LP which most of their staff work for, which is meant to return many-fold returns to their shareholders who are effectively profiting from the labor of all the artists and sources in their training. reply skydhash 13 hours agorootparentprevGoogle translate is very basic and not even close to something good if you already know both languages. Useful if you're translating to your language (you do the correction when reading), but can lead to confusion the other way. reply s1artibartfast 13 hours agorootparentInteresting distinction. If you can do the correction when reading, it seems reasonable to assume the reader in the opposite direction has the same correction capability. I would expect the chance of confusion to be identical. The only difference is a matter of perspective, where in one case you are the reader and in one case you are the author. reply skydhash 13 hours agorootparentYes, they are identical. But I believe the reader is better armed to deal with the confusion, or at least to recognize the error, because it does not fit it. But when producing, you don't know the target language, so there's a better chance for errors to slip in unnoticed. It's better for me to receive a text in the original language and translate it myself than to try to decipher something translated automatically. reply ClumsyPilot 10 hours agorootparentprevVastly inappropriate comparison- there are millions of pages of text out of copyright, you can get a good translation engine using public domain. That’s is not the case for art, vast majority of art used by midjourney is not public domain. reply c0pium 10 hours agorootparent> vast majority of art used by midjourney is not public domain Is that true? How did you establish that? reply shadowgovt 9 hours agorootparentIt's unfortunately also not great for translation. Language changes fast enough that training on content that went out of copyright is old data. reply freejazz 5 hours agorootparentprevOpenAI has basically admitted it. Is OpenAI even disputing that it ingested all the works its being sued over? Not as far as I can tell. reply theLiminator 13 hours agorootparentprevWhat about code? Or what about if we eventually robot labourers that is trained on observing human labourers? reply johnnyanmac 13 hours agorootparentCode has licenses too. And we've had very high profile lawsuits based on \"copying code\". >what about if we eventually robot labourers that is trained on observing human labourers? Interesting point, but by that point in time I don't think generative art will even be in the top 10 ethical dilemmas to solve for \"sentient\" robots. As it is now, robots aren't the ones at the helm grabbing data for themselves. Humans give orders (scripts) and provide data and what/where to obtain that data. reply 3asdf123 3 hours agorootparentprevWell art predates other professions by like thousands of year so it rightfully earned it's privileges. reply robryan 13 hours agorootparentprevWhat if the AI was solely trained on this person's work, then from that churned out a similar replacement that was monetized? reply nullptr_deref 14 hours agorootparentprevRedacted. reply Ajedi32 14 hours agorootparentI'm confused about your point. Are you saying we should ban $10 mass produced shirts so that more people can make a living hand-crafting $100 shirts? reply CrimsonRain 14 hours agorootparentprevToo much wall of text for nothing. Nobody is stopping you from buying hand crafted masterpiece. Just get out of the way of progress. reply jacobr1 13 hours agorootparentprev>What would you buy? $10 H&M or $100 hand-made shirt? - (My guess, if you could afford the later.) This is an interesting example because even in the $100 case you are still talking about machine-augmentation. You can have a seamstress or a tailor customize patterns, using off the shelf textiles, for that order of magnitude price - but if you want to use custom built, exotic materials or many kinds combined, the cost is on the orders of thousands not hundreds. Also there is a large industry of just printing designs on stock-shirts, that has a different point effort-scale equilibria. Thinking about how how automation disintermediates is very important. For animation, often productions have key-frame artists in the animation pipeline that define scenes, and then others that take though to flush out all the details of that scene. GenAI can potentially automate that process. You could still have the artist producing a keyframe, and can render that into a video. Another big factor is style. One hypothesized reason that more impressionism, absurdism or abstract art all become styles is photography. Once cheap machine-produced photography became available, there is less need for a portrait artist. But further, it also is no longer high-status and others push trends alternative directions. All the experiments and innovation going right now will definitely settle into a different set of roles for artists, and trends that they will seek to satisfy. Art-style itself will change as a result of both what is technically possible and also what is _not_ easily automatable in order to gain prestige. reply csallen 14 hours agorootparentprevMass production hasn't killed art and never will. What's killing art is this idea by a vocal minority of \"artists\" that they need to mass produce their work, enter the market, and attempt to make millions of dollars by selling and distributing it to millions. That's not art. That's capitalism. That's competing to produce something that customers will want to buy more than what your competitors offer. If you want to compete on the capitalistic marketplace, then compete on the capitalistic marketplace. But if you want to be an artist, be an artist. Art is still alive and well and always will be. Every day I see people singing because they love singing, making pottery because they love making pottery, writing because they love writing. Whether other people love or enjoy their art, the artist may or may not care. Whether they can profit from their art, the artist may or may not care. But many billions of artists will keep creating, crafting, and designing day after day, and they will never be stopped by AI or anything else. reply gumballindie 13 hours agorootparentPeople do whatever they want with their own property. You have no right to steal it just because they want to monetise it. What’s killing art is stealing it en masse using procedural generators. reply nullptr_deref 14 hours agorootparentprevRedacted. reply csallen 14 hours agorootparentJobs have never been less soul crushing, or more creative, in the history of humanity. And that becomes increasingly true every decade. Do you know what a job does? What a company does? It contributes to society! It produces something that someone else values. That they value so much they're willing to pay for it. Being part of this isn't a bad thing. It's what makes society work. A job/company entertains. It keeps things clean. It transports people to where they need to go. It produces. It gives people things they want. It creates tools, and paints, and nails, and shirts. I look out my window, and I see people delivering furniture, chefs cooking food and selling it out of trucks, keepers maintaining grounds, people walking dogs. Being useful to the fellow members of your society for 40 hours a week is not \"soul crushing.\" reply nullptr_deref 14 hours agorootparentHey. Thanks. Sorry about wasting your time. Shouldn't have started in the first place. It was my fault for trying to make a silly point. Too mid to understand your point. reply csallen 13 hours agorootparent(This is a response to your comment before you edited it.) Find the intersection of something that people increasingly value, that you enjoy, and that you can compete at. The best proof that people value something is that they're spending money for it. If people aren't spending money, they don't value it, and you probably don't want to go into it. If people aren't spending more and more money on it every year, then it's not increasing in value, and you probably don't want to go into it. The best proof that you enjoy something is that you enjoyed it in the past. Things you liked as a kid, activities that excited you as a young adult, etc., are often the best candidates. Look for intersections of the two things above. Do some Googling, do some research. Finally, you need to be able to compete at it. If you do something worse than everyone else does it, then no one will pick you, because you're probably not being helpful. The simple answer to this is to practice to make yourself better. But most people don't want to do that. A better answer to this is to be more unique, so you can avoid the competition. Don't do a job that has a title, a college major, and millions of talented applicants. It's not that helpful to society to do something a hundred million other people can already do, which is why there's more competition and lower wages. When you find the intersection of what's valued and what you enjoy, call up some people in those fields and ask what's rare. What in their area is needed. What are they missing. What is no one else doing. Or just start your own company. That's the easiest way to be unique. But it's hard. Finally, if you feel you're too \"mid,\" then make sure your standards aren't crazy. Don't let society tell you that you need to be a millionaire with a yacht and designer clothes to be happy. Get a normal 9 to 5 with some purpose in it, that you can be proud of, that others appreciate. Live within your means and don't stress yourself out financially. Spend your free time doing things you like. Take care of your health, find good relationships, and treasure them. That's a happy life at any income. I know a bunch of miserable depressed rich people who are very good at making money and very bad at health/relationships/etc., which is the real stuff that life is made out of. reply Humdeee 14 hours agorootparentprevIt's an interesting predicament. Assuming these stories between person and machine are indistinguishable and of same quality, then the difference here is the ability to scale. Without giving bias because of humanity reasons, why should we give entitlement to output derived from a human over something else of same quality? I hate making analogies, but if we make humans plant rows of potatoes, should that command a higher price and seen more valuable than planting potatoes by tractor 20 rows wide? reply bluefirebrand 14 hours agorootparent> Without giving bias to humanity No, we should absolutely be giving bias to humanity. Flesh and blood humans matter, their lives matter, their thoughts matter and their work matters. Machines are tools for them to use not entities given the same rights and same consideration. I reject your whole premise. reply Aeolun 10 hours agorootparentSo you instead want to what? Ban the tools because they interfere with doing things the human way? reply nojster 12 hours agorootparentprevDescartes told us that animals are mere soulless automatons, not entities given the same rights and same consideration as humans. Well ok, that was 300 years ago and views have changed dramatically since then. reply OOPMan 9 hours agorootparentNice strawman. reply Humdeee 13 hours agorootparentprevExactly; their flesh, blood, energy, etc. does matter. This is my argument for it, not for your argument against it, lmao. There's nothing more remarkable about my planted potato row vs the tractor planted rows, and my energy can be spent elsewhere. I am not entitled to making a living hand planting potatoes if there's not a market for it. People have the choice to continue making stories and they'll have a fanbase for it and always will, because that's ultimately apart of freedom and choice. Many are less what I'll call purists here, and don't care about how it came to be, they just want a quality story. What you're loosely proposing is art being a protected class of output, when we have tools that can match and soon with the potential to surpass. Is that not a terrific way to stunt what you're trying to defend? For transparency, I am an advocate for human made art, but I am against stunting tooling that can otherwise match said creativity. I see that as an artform in itself. reply bluefirebrand 13 hours agorootparent> For transparency, I am an advocate for human made art, If you believe AI tooling is an artform then you categorically are advocating against human made art as far as I am concerned. reply c0pium 10 hours agorootparentThis is just gatekeeping. Art is not better because it was made by hand as opposed to with technology. If I use a generative model to make art then I’m an artist. reply jazzyjackson 1 hour agorootparentI would argue art is better when it's the result of the effort and vision of an individual prompting a search engine to stitch images together on your behalf might result in an image you can call art, but imo all the art generated wholecloth like this sucks. necessarily derivative. put into the world without thought. My favorite critique of LLM work: \"why would I bother to read a story that no one bothered to write\" reply bluefirebrand 9 hours agorootparentprev> If I use a generative model to make art then I’m an artist. You are free to think so, but it really doesn't make you an artist any more than wearing a medal you bought second hand makes you a war hero. Something else did the work and you're just claiming credit. It's honestly kind of sad. reply SamoyedFurFluff 4 hours agorootparentprevSeriously asking: if I customize my order at a fast food joint am I a chef? How is that different from prompt engineering to generate art? reply OOPMan 9 hours agorootparentprevPlenty of people would disagree so clearly this is not a settled matter reply Aeolun 10 hours agorootparentprev> with the potential to surpass I think AI art will by definition never surpass human art. Humans can be inspired by things other than the art of others. reply c0pium 10 hours agorootparentprevThese models are not conscious, they’re not acting on their own. If I make art using a generative model it’s no more the model doing it than it’s sketchbook doing it if I were to use that. I’m making art using whatever tool, sometimes that tool is more or less powerful. But I’m the one doing it. reply PeterStuer 2 hours agorootparentprevWhat about ML models that only publish 1 or 2 books a year? Is it realy about volume? reply apersona 7 minutes agorootparentprevYou are not a machine. Have you noticed that authors and artists love sharing their inspirations? Let's say you're an up-and-coming author. In an interview, you list your sources of inspiration. Using your logic, why does the creative community celebrate you and your inspirations instead of crying foul like they are with LLMs? reply huimang 14 hours agorootparentprevHow many books can you write per second? How how many books per second can you read to influence and change your personal style? I don't think any person who actually has worked on anything creative in their life would compare a personal style to a model that can output in nearly any style at extreme speeds. And even if you're inspired by a specific author, invariably what happens is it becomes mix of yourself + those influences, not a damn near-copy. With visual mediums it's even worse, because you have to take the time [months, years] to specialize in that specific medium/style. reply bawolff 13 hours agorootparent> I don't think any person who actually has worked on anything creative in their life would compare a personal style to a model that can output in nearly any style at extreme speeds. And even if you're inspired by a specific author, invariably what happens is it becomes mix of yourself + those influences, not a damn near-copy. I don't think anyone who has ever read a novel in their life would say that an AI can write literature at all, in any style. > not a damn near-copy. The obvious solution is to just treat it as if a human did it. If you did not know the authorship of the output and thought it was a human, would you still consider it copyright infringement? If yes, fair enough. If no, then i think is clearly not a \"damn near-copy\" reply sandworm101 14 hours agorootparentprev>> How many books can you write per second? On my laptop, using modern tools backed by AI? ... many. >> How how many books per second can you read to influence and change your personal style? Thanks now to AI, hundreds. I can plug the output of the book-reading AI into the input of the tool I use to write my books and thereby update my personal style to incorporate all the latest trends. Blame the idiots who are paying me for my books. reply rangerelf 12 hours agorootparentSo, zero. You yourself: zero. You completely ignored the premise of the question. reply c0pium 10 hours agorootparentYou should read the response more carefully. Generative models are just tools. If I use one to write a story it’s no less a story that I wrote than if I’d chiseled it into a Persian mountainside. reply OOPMan 9 hours agorootparentIt pretty clearly is. Less of a story that is. reply californical 13 hours agorootparentprevThis is clearly a bad-faith response to the point that the GP was making reply MrVandemar 2 hours agorootparentprev> If I read a lot of stories in a certain genre that I like, and I later write my own story, it’s almost by definition going to be a mish-mash of everything I like. But it's also going to be affected by the teachers you had in pre-school, the people you hang around with, your relatives, films you've seen, adverts you watched, good memories and bad memories of events. You bring your lived experience to your story, and not just a mish-mash of stories in a particular genre, but everything. Whereas when you train a model, you know the exact input, and that exact input may be 100% copyright material. reply ska 14 hours agorootparentprevThere are two problems with this (very common) line of argument. First, the law is pretty clear that yes if your story is too similar to another work, they have rights. Second, it's not at all obvious we can or should generalize from \"what a human can do\" and \"what a bunch of computers can do\" in areas like this. reply madeofpalk 14 hours agorootparentprevPut differently - if you perfectly memorise Harry Potter, write it down into a book and sell it, you'll get into trouble. reply philomath_mn 14 hours agorootparentRight, I don't think anyone disagrees with that. The question is about someone/something writing a book _influenced_ by Harry Potter -- do they owe JK Rowling royalties? reply halostatue 14 hours agorootparentThat depends on a variety of factors. You may find yourself in trouble if you write about a wizard boy called Perry Hotter going to Elkwood school of magic and he ends up with two sidekicks (a smarter girl and a redhead boy). It could be argued quite convincingly that stories like Brooks's Shannara and Eddings's Belgariad are LOTR with the serial numbers filed off — but there is more than enough difference in how various pieces work for those series to make them unique creations that do not infringe on the properties or cover too much the story. (Although I cringe at putting the execrable Belgariad books in any class with either LOTR or Shannara.) The \"best\" modern example of this is the 50 Shades series. These are Twilight fan fiction (it is acknowledged as such) with the vampire bits filed off. They are inspired by Twilight, but they are not identifiably Twilight in the end. It might be hard to tell the quality of writing from that which an LLM can produce, and frankly Anne Rice did it all better decades before (both vampires and BSDM). Humans can be influenced by writers, artists, etc. LLMs cannot. They can produce statistically approximated mishmashes of the original works themselves, but there is no act or spark of creation, insight, or influence going on that makes the sort of question you’re asking silly. LLMs are just math. Humans may be just chemistry, but there’s qualia that LLMs do not have any more than `fortune` does. reply jacobr1 13 hours agorootparent> but there’s qualia that LLMs do not have I'm with all your other arguments ... but not this point. What is the special magic property that machine-generated art doesn't have? Both human and machine generated art can be banal, can be crap. And I think there is plenty of machine generated art this a quite beautiful, and if well prompted even very insightful. Non-GenAI can be this way, Conway's game of life has a quality of beauty to it that rivals of forms of modern art. If you wanted to argue that there still is the need for a human to provide some initial inspiration as input, or programming before something of value can be generated, then I would agree, at least for now, though there is meta-argument about asking LLMs to generate their own prompts that makes this an increasingly gray area. But I don't think the stochastic parrot argument holds water. Most of _human_ creations is derivative. Unique mixes of pre-existing approaches, techniques, substance, often _is_ the creative act. True innovation with no tie to existing materials seems vanishingly rare to me and is really high bar, beyond which most humans ever achieve. reply fragmede 14 hours agorootparentprevDid you not pay them when you bought their book to read it in the first place? That dead trees don't lend themselves to that sort of payoff is a limitation of the technology. In music, sampling is a well-accepted mechanism for creating new music, and the original authors of the music they used do get paid when the new one is used. reply soulofmischief 14 hours agorootparentNo, I bought the books used for 25 cents at a local booksale, and the authors did not benefit from my secondary market transaction. reply sandworm101 14 hours agorootparent>> the authors did not benefit from my secondary market transaction. But they did. The presence of a secondary market for used books increased the value of some new books. People buy them knowing that they might one day recoup some costs by selling them. Would people pay more, or less, for a new car if they were told they could never sell or trade it away as a used car? reply soulofmischief 7 hours agorootparentGee I don't know, but I'm glad that digital goods do not incur the same material costs as a car. \"You wouldn't download a car\", we've come full circle. reply downWidOutaFite 14 hours agorootparentprevYou got that via the legal \"first sale doctrine\" which has been killed for digital works. reply MacsHeadroom 6 hours agorootparent\"In 2012, the Court of Justice of the European Union (ECJ) held in UsedSoft GmbH v. Oracle International Corp that the first sale doctrine applies to used copies of [intangible goods] downloaded over the Internet and sold in the European Union.\" [0] Arguably the U.S. courts are in the wrong here. We can only hope first sale doctrine is extended to digital goods in the U.S. in the future, as it has been in the EU for over a decade. [0] https://scholarlycommons.law.northwestern.edu/cgi/viewconten... reply soulofmischief 7 hours agorootparentprevIt's a tough issue to correlate to physical goods, especially when you realize that people sometimes donate books. reply bergen 1 hour agorootparentprevDid you read the books with the intent to incorporate their ideas into your head and profit of this? reply kelnos 12 hours agorootparentprevYou are not an AI model, and AI models are not human authors, so your comparison is invalid and question irrelevant. reply RobRivera 14 hours agorootparentprevI feel like the keyword is 'almost' and then you begin pulling on that thread: How closely is this the case? What blind spots exist? How do you measure this? What is the capacity for original idea generation does the human mind have and how does it inspire a unique spin to it? This is one of those areas where 'thought experiments' are never going to pass muster against genuine experiments with metrics, trial, and robist scientific research. But with the stakes as they are, I dont have faith there exists a good faith dialogue in this arena. reply tedivm 13 hours agorootparentprevIs your mishmash going to be a literal statistical model built on top of those other stories? reply rspoerri 13 hours agorootparentprevI hope you payed for the book you read. If openai would pay usage fees for the training material per user its generating content for - it would never be profitable - artist would be fine off. But even all the shares are owned by people who have given this system none of it‘s knowledge. reply johnnyanmac 12 hours agorootparent>If openai would pay usage fees for the training material per user its generating content for - it would never be profitable In that case, good? I thought if nothing else, these past year or two would teach companies about sinking money into unsustainable businesses and then price gauging later (i know it won't, the moment interest rates fall we are back to square one). If it isn't profitable, scale down the means of production (which may include paying C class executives one less yatch per year, tragic), charge more upfront to the customers, or work out better deals with your 3rd parties (which is artists in this case). I also find some scheudenfredre in that these companies are trying to sell \"less enployees\" to other companies but would also benefit from said scaling down as they throw out defenses of \"we can't afford to pay every copyright\" . reply kansface 13 hours agorootparentprev> The reason that AI models are generating content similar to other people's work is because those models were explicitly trained to do that. Ah, just like humans who train against the output of other humans. AI models are not fundamentally different in kind in this regard, only scope, and even that isn't perfectly obvious to me a priori. reply acdha 13 hours agorootparentHumans usually add their own style to things, and it’s hard to discuss copyright without that larger context along with the question of scale (me making copies of your paintings by hand is not as significant a risk to your livelihood as being able to make them unimaginably faster than you can at much lower cost). Just as rules about making images of people in certain ways or places only became critical when photography made image reproduction an industrial-scale process, I think we’ll be seeing updates to fair-use rules based on scale and originality. reply jwells89 5 hours agorootparentHumans can also come up with their own styles and can draw things they’ve never seen, which ML models as they currently exist are not capable of (and likely will never be). A human artist who has lived their entire life in the wilderness and has never trained themselves with the work of another artist will still be able to produce art with styles produced entirely by personal experimentation. ML models have a long way to go before comparisons to humans make any kind of sense. reply kelnos 12 hours agorootparentprevI really don't get why so many people seem to think that an AI model training on copyrighted work and outputting work in that same style is exactly the same thing (morally, ethically, legally, whatever dimension you want) as a human looking at copyrighted work and then being influenced by that work when they create their own work. The first thing is the output of a mathematical function as computed by a computer, while the second is an expression of numan creativity. AI models are not alive. They are not creative. They do not have emotion. These things are not even in the same ballpark, let alone similar or the same. Maybe someday AI will be sophisticated enough to be considered alive, to have emotion, and to be deserving of the same rights and protections that humans have. And I hope if and when that day comes, humanity recognizes that and doesn't try to turn AI into an enslaved underclass. But we are far, far from that point now, and the current computer programs generating text and images and video are not exhibiting creativity, and do not deserve these kinds of protections. The people creating the art that is used to feed the inputs and outputs of these computer programs... those are the people that should have their rights protected. reply KuriousCat 13 hours agorootparentprevGoing by this logic, why is OpenAI forbidding use of the content it generates for training other models? reply throwup238 13 hours agorootparentThey can write whatever they want in their Terms of Service. That's the logic. That doesn't mean that courts will meaningfully enforce it for them. reply KuriousCat 13 hours agorootparentI understand that, only pointing out the hypocrisy reply kelnos 12 hours agorootparentprevBecause any company, hypocrisy be damned, will use every legal lever at their disposal to protect their business model. reply KuriousCat 11 hours agorootparentHope we are not normalizing hypocrisy, usually it is very destructive. reply johnnyanmac 13 hours agorootparentprevWell, mostly because of corporate greed of ownership. But the underlying issue is that Ai training in AI is a recipe for ruining the entire training set. At least in these early stages. reply KuriousCat 13 hours agorootparentNot just greed, they want to silence copyright holders whose works they freely use and at the same time prevent others from using theirs. It is like having different set of rules for them. I don't believe training itself is ruining anything, it is the proposed model of value capture and marginalizing content creators that poses greater threat. reply spookie 11 hours agorootparentYes, you've condensed the problem in display quite well here. It's not even just hypocrisy, but also short sighted behaviour. Artists will learn to not trust the web, if they haven't already. The greatest time to train a model was yesterday, eventually no novel ideas, expressions, art will prosper on the \"open\" web. Just a regurgitation of some statistical idea of words, and pixels. reply munificent 13 hours agorootparentprevAI models are fundamentally different because a computer is a lump of silicon which is neither a moral subject nor object. A human author is a living sentient being that needs to earn a living and is deserving of dignity and regard. reply hermitdev 11 hours agorootparentI'm sorry, but I'm going to fundamentally disagree with you. One does not get a morality pass because \"the computer did it\". People are creating these AI models, selecting data and feeding the models data on which to be trained. The outcome of that rests upon _both_ the creators of the models and the users prompting the models to achieve a result. reply fragmede 9 hours agorootparentTo make it even more stark, people don't kill people, it's the gun that does it. reply bayindirh 13 hours agorootparentprevOh, right. It just reads a million books in a couple of days, removes all the source information, mix and match it the way it sees fit and sells this output $10/month to anyone comes with a credit card. It's the same thing with GitHub's copilot. A book publisher would seize everything I have, and shot me at a back alley if I do 0.0001% of this. reply somethingsaid 12 hours agorootparentYeah, fair use implicitly uses the constraints of typical human lifetime and ability to moderate how much damage is done to publishers with it. That wasn’t an issue before recently, as humans were the only ones who could create output based off fair use laws. reply MacsHeadroom 6 hours agorootparent> Yeah, fair use implicitly uses the constraints of typical human lifetime and ability Authors Guild, Inc. v. Google, Inc. strongly disagrees with you on that (the \"Google Books case\"). reply OOPMan 9 hours agorootparentprevThe heck are you on about? Have you ever tried to \"train a human\"? They don't work that way, not unless your \"training\" involves so weird torture stuff you probably shouldn't be boasting about. Maybe try ask some teachers (of both adults and children) how it works with people... reply rspoerri 13 hours agorootparentprevHumans who train on material usually buy a book of it, pay for an entry to an exhibition or even pay to own an original. Maybe they release it for free, possibly supported by ads, at least to get some recognition and a job if it is perceived well. reply romwell 4 hours agorootparentprev>Ah, just like humans who train against the output of other humans. Yeah, and creating derivative work without permission is against the law. >AI models are not fundamentally different in kind in this regard [citation needed] Of course they are fundamentally different. They don't get to decide what to absorb. Humans that make those decisions, correspondingly, should pay the price. reply williamcotton 4 hours agorootparentprevSo what about the fact that these cartoons look like Keith Haring meets Cathy Guisewite meets Scott Adams? These cartoons are artistically derivative. They are obviously not derivative from the perspective of copyright as style is an idea, not an expression. These models were not trained on just the cartoonist in question, nor just their inspirations. The intent was to train on all images and styles. The expression of the idea using these models is not going to match the expression of the idea of all images, even those conforming to a certain bounded prompt. For the life of me I can't get DALL-E or Stable Diffusion to produce anything like Cat and Girl nor anything coherent for the above mentioned inspirations. DALL-E flat out refuses to create things in the style of the above and Stable Diffusion has insane looking outputs, overwhelmed by Herring. Most importantly, copyright is concerned with specific works that specifically infringe and whose damages are either statute or based on quantifiable earnings from infringement. Copyright does not cover all works, especially when again, the intent is to learn all styles that rarely, if at all, reproduces direct expressions. The only point at which these images are directly copied are when in the machine's memory, which has already has case law for allowance, followed by back propagation that begins the process of modifying the direct copies for the underlaying formal qualities. It seems like a lot of people are going to be upset when the courts rule eventually rule in favor of the training and use of these models, if not only because the defendant has a lot of resources to throw at a legal team. reply jazzyjackson 1 hour agorootparentyour argument is that it's not infringing because they copied everything at once? I get that there's case law on copying in memory on the input side not being infringing but can't for the life of me understand how they get away with not paying for it. At least libraries buy the books before loaning them out, OpenAI and midjourney presumably pirated the works or otherwise ignored the license of published works and just say \"if we found it on the internet it's fair game\" reply c0pium 10 hours agorootparentprevThat’s literally what human artists do, and how they work. Art is iteratively building on the work of others. That’s why it’s so easy to trace its evolution. reply vorticalbox 13 hours agorootparentprevOne could argue that you're paying for the resources required to run the model rather than paying for the using the model. reply bergen 1 hour agorootparentDepends, some models are not freely available and in that case you pay very much for access to the model reply kopecs 14 hours agorootparentprev> I firmly believe that training models qualifies as free use. I think it falls under research, and is used to push the scientific community forward. I don't think this is as cut-and-dry and you make it out here. If I train a model on, say, every one of New York Times' and release it for free and it finds use as a way of circumventing their paywall I have difficulty justifying that as fair use/fair dealing. The purpose/character of the model should indeed be a factor but certainly not nearly as dispositive a one as I think you're suggesting. To the extent that training the model serves a research purpose I think that the general use / public release of the trained model does not in general serve the same research purpose and ought to have substantially lower protection on the basis of, e.g., the effect on the original work(s) in the market. reply jacobr1 13 hours agorootparentWouldn't that depend on the use case? If you just had the model regenerate articles that roughly approximate its source material that is much a more clear cut violation of a paywall. But if you use that data as general background knowledge to synthesize aggregative works such a history of the vietnam war, or trends in musical theatre in the 1980s relative the 1970s, or shifts in the language usage of formal honorifics, then that seems to me to be clearly fair-use categories. There are gray areas, such as aggregating the opinions of a certain op-ed writer over a short timeframe that while it might produce a novel work, is basically is mixmash of recent articles. But would that be unfair, especially if not done in the original authors style? These technical distinctions like these probably will matter in whatever form regulation eventually ends up becoming. reply kopecs 12 hours agorootparentYes, I think this is a rather fact-specific inquiry. My main point is that the research/commercial distinction is not the only factor (and not even the most important one). > if you use that data as general background knowledge to synthesize aggregative works such a history of the vietnam war, or trends in musical theatre in the 1980s relative the 1970s, or shifts in the language usage of formal honorifics, then that seems to me to be clearly fair-use categories I don't think this is clear. If someone were to train a model on several books about the Vietnam War and then publish my own model-created book on history of the Vietnam War, I would be inclined to say that that is infringement. And if they changed the dataset to include a plurality of additional books which happen to not be about Vietnam, I don't think that changes the analysis substantially. I think it is hard to earnestly claim that in that instance the output (setting aside the model itself, which is also a consideration) is transformative, and so I would think, absent more specific facts, that all four fair use factors are against it. reply makomk 11 hours agorootparentprevQuite a lot of what news publications like the New York Times do is precisely regenerating articles that roughly approximate source material from some other publication. If I remember rightly, a lot of smaller, more local news organisations aren't happy about this because of course it's a more or less direct substitute for the original and a handful of big news organisations (particularly the New York Times) are taking so much of the money that people are willing to pay for news that it's affecting the viability of the rest - but it's not illegal, since facts cannot be copyrighted. reply shadowgovt 14 hours agorootparentprevI think it's worth noting that one of the things that makes this question so vexing is that this topic really is pretty novel. We've only had a few machines like this in history and almost no legal precedent around how they should be treated. I can't remember anyone ever bringing suit over a Markov chain engine, for example, and fabricating one is basically \"baby's first introductory 'machine intelligence' project\" these days (partially because the output sucks, so nobody has ever felt they have something to lose from competing with a Markov engine). Existing copyright precedent serves this use-case poorly, and so the question is far more philosophical than legal; there's a good case to be made that there's no law clearly governing this kind of machine, only loose-fit analogies that degenerate badly upon further scrutiny. reply fortran77 14 hours agorootparentprev> I firmly believe that training models qualifies as free use Can you explain what you mean by \"qualifies as free use?\" I've never heard that term before. reply halostatue 14 hours agorootparentThe poster probably meant \"fair use\", which is an American term of copyright law. The UK, Canada, and other commonwealth countries have a concept known as \"fair dealing\" which is similar to, but different than fair use[1]. EU copyright law has explicitly permitted uses[2] which are exceptions to copyright restrictions. Research is one of them, but requires explicit attribution. [1] https://library.ulethbridge.ca/copyright/fairdealing/#s-lib-... [2] https://www.ippt.eu/legal-texts/copyright-information-societ... reply kopecs 14 hours agorootparentprevI would be reasonably confident they mean \"fair use\" [1] instead. [1]: https://en.wikipedia.org/wiki/Fair_use reply voidhorse 14 hours agorootparentprevSorry, but these arguments by analogy are patently ridiculous. We are not talking about the eons old human practice of creative artistic endeavor, which yes, is clearly derivative in some fashion, but which we have well established practices around. We are discussing a new phenomenon of mass replication or derivation by machine at a scale impossible for a single individual to achieve by manual effort. Further, artists tend to either explicitly or implicitly acknowledge their priors in secondary or even primary material, much like one cites work in an academic context. Also, the claim: >But if I take your work and compare it to millions of other people's work... Is ridiculous. A. you haven't, nor will you ever actual do this. B. This is never how the system of artistic practice up to this point has worked precisely because this sort of activity is beyond the scale of human effort. In addition, plagiarism exists and is bad. There's no reason that concept can be extended and expanded to include stochastic reproduction at scale. If you feel artists shouldn't have a say and a future in which capital concentrates even further into the hands of a few technological elite who make their money off of flouting existing laws and the labor of thousands, by all means. But this argument that somehow by analogy to human behavior companies should not be responsible for the vast use of material without permission is absolutely preposterous. These are machines owned by companies. They are not human beings and they do not participate in the social systems of human beings the way human beings do. You may want to consider a distinction in the rules that adequately reflects this distinction in participatory status in a social system. reply omikun 9 hours agorootparent>> a future in which capital concentrates even further into the hands of a few technological elite who make their money off of flouting existing laws and the labor of thousands I think this is the central issue and is not limited to just AI generated art. Wealth concentrates to the few from each technological development. When robots replaced factory workers, the surplus profit went to the capital holders, not the workers who lost their jobs. AI generated art will be no different but I don't think it will replace the creative art that people will want to make, just the art that people are making to pay the bills. reply derangedHorse 14 hours agorootparentprevSo your argument is predicated on the scale of inspired work being the problem? > They are not human beings and they do not participate in the social systems of human beings the way human beings do I don't think this adds anything to the argument besides you using this as a reason analogies with humans can't be used to compare the specific concept of inspired works? I don't think this holds up. Algorithms participating in social systems has nothing to do with whether inspired works have a moral claim to existence for some. The fact that your ethics system values the biological classification of the originator of inspired works is something that can't be reconciled into a general argument. I could make the claim that the prompt engineer is the artist in this case. > capital concentrates even further into the hands of a few technological elite who make their money off of flouting existing laws That can be said by the development of any technology. Fear of capital concentration is more a critique on capitalism than it is on technological development. reply mensetmanusman 12 hours agorootparentDifference in scale (order of magnitude) is difference in kind (in every area of life), so yes, scale can be argued as the problem. reply voidhorse 14 hours agorootparentprev> That can be said by the development of any technology. Fear of capital concentration is more a critique on capitalism than it is on technological development. Technology does not exist in a vacuum. All of the utility and relevance of technology to humans is dependent on the social and economic conditions in which that technology is developed and deployed. One cannot possibly critique technology without also critiquing a social system, and typically a critique of technology is precisely a critique about its potential abuses in a given social system. And yes, that's what I'm attempting to do here. > I don't think this adds anything to the argument besides you using this as a reason analogies with humans can't be used to compare the specific concept of inspired works? I don't think this holds up. This is a fair point. One could argue that an LLM, properly considered, is just another tool in the artist's toolbox. I think a major distinction though, between and LLM and, say, a paintbrush or even a text-editor, or photoshop, is that these tools do not have content baked into them. An LLM is in a different class insofar as it is not simply a tool, but is also partially the content. The use of two different LLMs by the same artist, with a the same prompt, will produce different results regardless of the intent of the so called artist/user. The use of a different paintbrush, by the same artist, with the same pictorial intention may produce slightly different results due to material conditions, but the artist is able to consciously and partially deterministically constrain the result. In the LLLM case, the tool itself is a partial realization of the output already and that output is trained on masses of works of unknown individuals. I think this is a key difference in the \"AI as art tool\" case. A traditional tool does not harbor intentionality, or digital information. It may constrain the type of work you can produce with it, but it does not have inherent, specific forms that it produces regardless of user intent. LLMs are a different beast in this sense. Law is a realization of the societal values we want to uphold. Just as we can't in principle claim that training of LLMs on scores of existing work is wrong solely due to the technical function of LLMs, we cannot claim that this process shouldn't be subject to constraints and laws due to the technical function of LLMs and/or human beings, which is precisely what the arguments by analogy try to do. They boil down to \"well it can't be illegal since humans basically do the same thing\" which is a hyper-reductive viewpoint that ignores both the complexities and novelty of the situation and the role of law in shaping willful societal structure, and not just \"adhering\" to natural facts. reply derangedHorse 13 hours agorootparent> They are not human beings and they do not participate in the social systems of human beings the way human beings do. Your original quote was not using the impact of the technology, it was disparaging the algorithmic source of the inspired work (by saying it does not participate in social systems the way humans do). > I think a major distinction though, between and LLM and, say, a paintbrush or even a text-editor, or photoshop, is that these tools do not have content baked into them LLMs, despite being able to reproduce content in the case of overtraining, do not store the content they are trained from. Also, the usage of \"content\" here is ambiguous so I assumed you meant the storage of training data. To me, the content of an LLM is its algorithm and weights. If the weights can reproduce large swaths of content to a verifiable metric of closeness (and to an amount that's covered by current law) I can understand the desire to legally enforce current policies. The problem I have is against the frequent argument to ban generative algorithms altogether. > The use of a different paintbrush, by the same artist, with the same pictorial intention may produce slightly different results due to material conditions, but the artist is able to consciously and partially deterministically constrain the result. I would counter this by saying the prompts constrain the result. How deterministically depends on how well one understands the semantic meaning of the weights and what the model was trained on. Also, as a disclaimer, I don't think that makes prompts proprietary (for various different reasons). > I think this is a key difference in the \"AI as art tool\" case. A traditional tool does not harbor intentionality, or digital information Assigning \"intent\" is an anthropomorphism of the algorithm in my opinion as they don't have any intent. I do agree with your last paragraph though, one (or even a group of) individual's feelings don't make something legal or illegal. I can make a moral claim as to why I don't think it should be subject to constraints and laws, but of course that doesn't change what the law actually is. The analogies are trying to make this appeal in an effort to influence those who try to make the laws overly restrictive. There are many laws that don't make sense and logic can't change their enforcement. The idea is to make a logical appeal to those who may have inconsistencies in their value system to try and prevent more non-sensical laws from being developed. reply huytersd 2 hours agorootparentprevIt’s not replication and that’s all there is to it. reply pointlessone 27 minutes agorootparentprevIf training a model is fair use than model output should also fallow fair use criteria. The very first thing you can find on the internet about fair use is Wikipedia article on the topic. It lists a bunch of factors to decide whether something is fair use. The very first one has a quote from an old copyright case: > [A] reviewer may fairly cite largely from the original work, if his design be really and truly to use the passages for the purposes of fair and reasonable criticism. On the other hand, it is as clear, that if he thus cites the most important parts of the work, with a view, not to criticise, but to supersede the use of the original work, and substitute the review for it, such a use will be deemed in law a piracy. Most use of LLMs and image generation models do not produce criticism of their training data. The most common use is to produce similar works. You can find this very common “trick” to get a specific style of output to add “in style of ”. Is this a direct way \"to supersede the use of the original work”? You can certainly see how other factors more or less put gen ai output into the grey zone. The fact that clothing doesn’t qualify for copyright doesn’t mean text and images don’t. Or if you advocate that they don’t then you pretty much advocate for abolishment of copyright because those are the major areas of copyright applicability at the moment. Which is a stance to have but you’d probably be better to actually say that because saying that copyright applies to some images and text but not others is a much harder position to defend. reply Osmose 15 hours agorootparentprevBecause we are humans and our capability of abusing those rights is limited. The scale and speed at which LLMs can abuse copyrighted work to threaten the livelihoods of the authors of those works is reason enough to consider it unethical. reply derangedHorse 14 hours agorootparent\"abusing those rights\" is a subjective phrase. What about it is \"abuse\"? If I learned how to draw cartoon characters from copying Family Guy and released a cartoon where the characters are drawn in a similar style, would that be abuse (assuming my show takes some of Family Guy's viewership)? Is your ethical hangup with the fact it's wrong to use the data of others to influence one's work (which could potentially be an algorithm) or that people are losing opportunities based on the influenced work? If it's the latter how do we find the line between what's acceptable and what's not? For example, most people wouldn't be against the creation and release of a cure for cancer developed in this way. It would lead to the loss of opportunities for cancer researchers but I believe most people would deem that an acceptable tradeoff. A grayer area would be an AI art generator used to generate the designs for a cancer research donation page. If it could potentially lead to a 10% increase in donations, does that make it worth it? reply scythe 14 hours agorootparent>For example, most people wouldn't be against the creation and release of a cure for cancer developed in this way. Intellectual property law does presently restrict the development of cancer treatments and demands in many cases exorbitant royalties from patients and practitioners, so I'm not convinced that this is accurate. If people believed that the loss of opportunities would constrain innovation in the field of cancer research, I think they'd expect the AI users to pay royalties as well. reply s1artibartfast 13 hours agorootparent>If people believed that the loss of opportunities would constrain innovation in the field of cancer research, I think they'd expect the AI users to pay royalties as well. This comes down to the product of AI. If the AI produces a cancer treatment identical to what is already covered by patent, I think commercialization would be contingent on the permission of the IP holder. If the AI produced a novel cancer treatment, using a transformative synthesis of available knowledge, Most people would not expect royalties. reply derangedHorse 13 hours agorootparentprevI never made a legal appeal in my previous comment so legalities are irrelevant. It also differs from my argument on derivative/transformative works rather than specific works. What I was questioning was whether people would think it's morally right or not to generate inspired works. For example, if someone made an algorithm to read the relevant papers and make a cancer treatment that addresses the same areas/conditions of a method under IP law but don't equate to the exact method, I don't see that as a morally wrong action by itself. reply unfamiliar 15 hours agorootparentprevI don’t think it is. What you describe is similar to any other industry disruption, and I don’t think those are unethical. I’d actually argue that preventing disruption is often (not always) unethical, because you artificially prolong an inefficient or inferior alternative. reply bayindirh 14 hours agorootparentSo you're saying that, we should stop pursuing art and prose? Because when you fine tune midjourney with 30 or so images of an artist, it can create any image with the artist's style. You removed the value and authenticity that artist in 30 minutes, you applauded it, and defended that it should be the norm. OK then, we can close down all entertainment business, and generate everything with AI, because it can mimic styles, clone sounds, animate things with gaussian splats, and so on. Maybe we can hire coders to \"code\" films? Oh sorry. ChatGPT can do that too. So we need a keypad then, only the most wealthy can press. Press 1 for a movie, 2 for a new music album, 3 for a new book, and so on. We need 10 buttons or so, as far as I can see. Maybe I can ask ChatGPT 4 to code one for me. reply Wissenschafter 14 hours agorootparent\"So you're saying that, we should stop pursuing art and prose?\", no, it becomes a hobby like any other. People still sew for fun. reply bayindirh 14 hours agorootparentGreat, hold on, I'm calling Hollywood to tell that all they do is a hobby now. ...and the writers' guild, too. reply crazygringo 1",
    "originSummary": [
      "The statement is a copyright notice for the comic \"Cat and Girl\" stating that it is licensed under a Creative Commons license.",
      "The notice provides contact information for the creator of the comic.",
      "This indicates that the creator has chosen to share their work under certain permissions and conditions outlined by the Creative Commons license."
    ],
    "commentSummary": [
      "The debate focuses on the use of copyrighted content in training AI models and the consequences for copyright and fair use.",
      "It explores the effects of AI on artists and their work, discussing intellectual property, copyright protection, and AI's role in creating art.",
      "The discussion highlights the need for changes in international copyright regulations, the importance of artist solidarity, and the impact of AI on the job market and the distinction between human and machine-generated output."
    ],
    "points": 670,
    "commentCount": 785,
    "retryCount": 0,
    "time": 1705427880
  },
  {
    "id": 39020365,
    "title": "Developers can offer non-App Store purchasing, but Apple still collects commission",
    "originLink": "https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/",
    "originBody": "U.S. Developers Can Now Offer Non-App Store Purchasing Option, But Apple Will Still Collect Commissions Tuesday January 16, 2024 4:07 pm PST by Juli Clover Apple is making major changes to its U.S. iOS App Store policies, and developers are now able to direct customers to a non-App Store purchasing option for digital goods. Apple is allowing apps to feature a single link to a developer website that leads to an in-app purchase alternative, but Apple plans to continue to collect a 12 to 27 percent commission on content bought this way. Apple's update and the backstory that led to it are a bit complicated, but what iPhone and iPad users need to know is that some apps in the U.S. storefront will soon feature a link to their website where subscriptions and other content can be purchased outside of the App Store in-app purchase system, likely with a discounted price. Developers who want to offer this option will need to apply for a StoreKit External Purchase Link Entitlement, as Apple has outlined in both updated App Store Review Guidelines and the statement of compliance submitted to the Northern California U.S. District Court. With a Link Entitlement, a developer is able to direct a user to an out-of-app purchasing mechanism using an external purchase link. From Apple's modified App Store rules: Developers may apply for an entitlement to provide a link in their app to a website the developer owns or maintains responsibility for in order to purchase such items. Learn more about the entitlement. In accordance with the entitlement agreement, the link may inform users about where and how to purchase those in-app purchase items, and the fact that such items may be available for a comparatively lower price. The entitlement is limited to use only in the iOS or iPadOS App Store on the United States storefront. In all other storefronts, apps and their metadata may not include buttons, external links, or other calls to action that direct customers to purchasing mechanisms other than in-app purchase. If your app engages in misleading marketing practices, scams, or fraud in relation to the entitlement, your app will be removed from the App Store and you may be removed from the Apple Developer Program. There are several requirements that developers need to adhere to maintain the privacy and security of the App Store ecosystem, and notably, Apple will collect a commission on purchases made using these Entitlement Links. Rather than 30 percent, Apple will collect a 27 percent fee on user purchases or year-one subscriptions made through the link. On the second year of a subscription, the commission fee drops to 12 percent, which is three percentage points lower than the 15 percent fee that Apple collects from second-year or longer subscriptions made through the in-app purchase system. Apps that participate in the App Store Small Business Program will be charged a 12 percent commission rate. The commission will apply to transactions for digital goods and services that take place on a developers website within seven days after a user taps through an External Purchase Link to an external website. Several key points about Entitlement Links are listed below. All links to outside purchasing methods must use the Entitlement Link system, and developers must apply and get Apple's approval. Developers are permitted to have a single plain link on one screen of an app. The link can be at a sign-in screen, in user settings, or elsewhere, but it can only be in one place. The single location may not be an interstitial, modal, or pop-up. The link can mention the specific price of content on a website, or that content is discounted on the website from the App Store price. Comparisons are allowed. Links cannot be placed directly on an in-app purchase screen or in the in-app purchase flow. Developers need to certify that the third-party payment service provider they are using for out-of-app purchasing meets industry standards for payment processors, and that they will offer users processes for managing subscriptions, requesting refunds, and disputing unauthorized transactions. Apps that participate in Apple's Video Partner Program or News Partner Program are not eligible for Link Entitlement. Apps that use the StoreKit External Purchase Link must continue to offer in-app purchases as an option. App Store pages are not able to include information about purchasing on a website or a link to a website. Digital purchases that are sold on an app's website through the Entitlement Link must be available for use in that app. The StoreKit External Purchase Link cannot discourage users from making in-app purchases or mimic an in-app purchase. Links must open a new window in the default browser of the device, and are not able to open a web view. No redirecting, intermediate links, or URL tracking parameters are allowed. Developers are required to provide a periodic accounting of qualifying out-of-app purchases, and Apple has a right to audit developers' accounting to ensure compliance with their commission obligations and to charge interest and offset payments. The Link Entitlement process and the App Store changes are applicable only in the U.S. App Store. Apps for all other storefronts are not able to include buttons, external links, or calls to action that direct customers to alternative purchasing options. Examples of how Entitlement Links can be used in apps Apple will provide an in-app warning to customers to let them know that they are leaving the App Store ecosystem to make a purchase on an external website and that App Store protections will not be available. According to Apple's statement filed with the court, the requirements surrounding links are aimed at minimizing \"fraud, scams, and confusion,\" while also providing developers with an opportunity to \"entice users to other platforms\" and give customers a choice between non-App Store purchasing and in-app purchases. The changes today stem from Apple's 2021 legal battle with Epic Games. Apple won the dispute and the court did not find that Apple had violated U.S. antitrust law, but Apple was at the time ordered to remove \"anti-steering\" rules preventing developers from informing customers about alternatives to in-app purchases. That order has been on hold during the appeals process, but the appeals process ended today. Both Apple and Epic Games had appealed to the United States Supreme Court, but the Supreme Court declined to hear the case. That means the initial ruling and the appeals court ruling that agreed with it are permanent, and Apple now has to comply with the part of that order that required it to change the App Store rules. The anti-steering rule was two-pronged, requiring Apple to allow for links to in-app purchase alternatives and to allow developers to communicate with customers outside of the App Store through email and other contact information collected in the app. The outside communication part of the order was already satisfied with a change that Apple made to the App Store rules in 2021 to settle a class-action developer lawsuit. Apple has already been allowing developers to use communication methods like email to inform customers about payment methods available outside of iOS apps, and Apple makes it clear in its messaging today that there are no limits on developers' out-of-app communications with users. The full statements that Apple provided to the court have been obtained by MacRumors and can be read below. App Store VP Matthew Fischer on Apple's Injunction Compliance by MacRumors on Scribd Apple Notice of Compliance With UCL Injunction by MacRumors on Scribd Update: Epic Games CEO Tim Sweeney criticized Apple's App Store changes and said that Epic plans to contest Apple's \"bad-faith compliance plan\" in District Court. A quick summary of glaring problems we've found so far: 1) Apple has introduced an anticompetitive new 27% tax on web purchases. Apple has never done this before, and it kills price competition. Developers can't offer digital items more cheaply on the web after paying a… pic.twitter.com/YkHuapG7xa — Tim Sweeney (@TimSweeneyEpic) January 16, 2024 [ 179 comments ]",
    "commentLink": "https://news.ycombinator.com/item?id=39020365",
    "commentBody": "US developers can offer non-app store purchasing, Apple still collect commission (macrumors.com)543 points by virgildotcodes 11 hours agohidepastfavorite750 comments dang 7 hours agoRelated ongoing thread: US Supreme Court declines to hear appeals in Apple-Epic Games legal battle - https://news.ycombinator.com/item?id=39014642 - Jan 2024 (162 comments) andersa 4 hours agoprevIt's incredible that there are actually people in this thread arguing in favor of Apple. You don't need to defend the trillion dollar company. They are not your friend, they do not care about you, your work or your life. All they do is steal 30% from society that could be used for more productive purposes than make a few people who already have everything even richer. reply remus 4 hours agoparentI also think apple's 30% cut is excessive, but I don't think this line of argument helps. We should discuss the points on their merits, not based on who's making them and how much money they have. reply nevir 2 hours agorootparentHere's a little story / timeline from 2009-2010 (from my perspective as a dev on Kindle for iOS): * we submit the Kindle app ...including an in-app bookstore... to Apple for initial app review (Note: multiple ebook readers with in-app bookstores are already on the app store at this point) * several weeks pass with no response * Apple announces in-app purchasing (to be released several months later) * Apple rejects our app: we have to give them a 30% cut of all sales through the app, or remove the store and all references / external links to it. We chose option 2. * Apple forces the other ereader apps to remove their stores or go with IAP. Several (most?) just gave up and pulled their apps entirely * Apple negotiates agreements with most of the major book publishers that if they want to sell books on iBooks, ebooks must be listed at the same price on ALL stores, and have a 30% margin * Apple launches in-app purchasing and the iBooks store (with the iPad announcement, IIRC) ...aka even if we (or any other ereader app) wanted to sell books via our app, the terms Apple set forth effectively meant that ALL profit from those sales must go to them (and we would have to eat the bandwidth / service costs on top of that) reply slimsag 1 hour agorootparentThis may be the funniest and saddest thing I've read all year. So $MEGACORP abuses their absolute monopolistic position in the market to underhandedly negotiate with book publishers and force their hand into working the way $MEGACORP wants: in order to gain access to $MEGACORPs completely dominated (but technically not a monopoly*) audience who wishes to buy books in a convenient way online, book publishers must bow down to $MEGACORP and pay the tax. Meanwhile, everyone else who sells books through alternative avenues is decimated because the audience only wants to buy books through $MEGACORP. And you can replace $MEGACORP with both 'Apple' and 'Amazon', and it is 100% factually accurate. Beautiful. It's fucking turtles eating turtles all the way down. reply lostlogin 31 minutes agorootparent> This may be the funniest and saddest thing I've read all year. ‘Funniest thing in a fortnight’ sounds less impressive. You’re comment is actually funny, the OPs just makes one sad and frustrated. reply rickdeckard 8 minutes agorootparentprevThe missing part is that Apple's maneuver was to effectively destroy the wholesale model in favor of an agency-model, and orchestrate all major publishers to charge more for ebooks just so they can earn their 30% commission from it. Apple actively engaged as facilitator to help publishers raise prices on the whole market, for a 30% cut. The result was that books previously available for $9.99 were suddenly sold for $12.99 [1] https://www.businessinsider.com/how-steve-jobs-and-apple-fix... reply ruddct 1 hour agorootparentprevDon’t forget the kicker, that IAP at the time was unable to support more than a few thousand SKUs! And (iirc) that pricing, naming, etc for everything would’ve needed to be done through their atrocious web app. Not exactly doable for the ‘everything store’. reply nevir 1 hour agorootparentOh, lol, I totally forgot about those technical limitations! We couldn't even have done it if we wanted to. Also hi :) long time! reply ilija139 24 minutes agorootparentprevOT: https://chat.openai.com/share/d7f4ae23-c1b8-48b1-8a04-368253... reply torginus 1 hour agorootparentprevWeird, this sounds super illegal, and anti-competitive, considering Apple has its own competing bookstore that's not subject to these fees. reply hammyhavoc 1 hour agorootparentIn retrospect, yes, the consequences of what happened are now very obvious, but at the time, whilst there were a fair number of people sounding the alarm across the blogosphere, most people didn't care because the iPad was a hit, and the Apple reality distortion field was at its peak of effectiveness. reply lostlogin 26 minutes agorootparentThe situation is farcical and feeling sad for Apple or Amazon shouldn’t happen. Consumers lost whatever the outcome. reply nevir 1 hour agorootparentprevThe US courts thought so, too. https://www.google.com/amp/s/amp.theguardian.com/technology/... reply lostlogin 24 minutes agorootparentSo Amazon won (and consumers lost), where we could have had Apple win (and consumers lose). There was little to cheer about whatever happened. reply bsjaux628 1 hour agorootparentprevThat sounds a lot like price fixing and the same thing that Amazon is being grilled on with FBA reply issafram 2 hours agorootparentprevI did my first iOS development about a couple of years ago. Question, how in the world do you tolerate the storyboard XML files? One small change in XCode results in so many line changes. PRs are impossible to review with any confidence. reply ben_w 34 minutes agorootparentThat's a big argument for SwiftUI, which replaces storyboards. But if you must use them, keep each storyboard small enough it's only going to be used by one dev at a time to avoid conflicts, and then combine trusting the GUI won't make stupid XML plus some automated UI tests to make sure functionality isn't damaged by e.g. a button being deleted. reply 0x0 1 hour agorootparentprevAnswer: Don't use storyboards. reply jacquesm 1 hour agorootparentprevSorry but I can't find much sympathy for anything Amazon related when comparing with Apple. In my book they're both predatory. reply ffgjgf1 1 hour agorootparentWell all the same things would apply to any independent ebook store, it would just hurt them massively more than it does Amazon.. reply hammyhavoc 1 hour agorootparentprevPerhaps the stance to take is that it was bad for consumers in the long-term, because monopolies aren't a good thing. reply elcomet 1 hour agorootparentprevThis doesn't affect only Amazon, it affects also all smaller online book stores reply pompino 3 hours agorootparentprev> We should discuss the points on their merits, not based on who's making them and how much money they have. Its not based on how much money they have. Its how they've managed to accumulate the money - by gouging devs. reply johnnyanmac 3 hours agorootparentTo be fair, roughly half of Apple's money is made from hardware. The app store is extremely lucrative and apparently 70%+ of their revenue from the App store is just leeching off of mobile games, but Apple can definitely survive without the app store if push came to shove. BUT, I will also mention that part of its market capture comes from all the charges on devs even before the rev share. You need apple equipment to develop, and they (apparently) don't sell server racks anymore for businesses to scale off of, nor any legitimate form of emulation. You have a small cost per year to have a developer account, and a cost to submit your app for review. Then if you care about visibilty they have their own ad discovery program you can pay into. So I did disagree with a brief judge statement about how \"It's possible to skirt around Apple's innnovation for free...\". Apple controls and charges for the entire pipeline, even before you launch the app. reply jb1991 3 hours agorootparent> a cost to submit your app for review They charge you to submit an app? Is this new? When I worked as an iOS developer this was not a thing but that was many years ago. reply __m 2 hours agorootparentNo they don’t reply wouldbecouldbe 2 hours agorootparentprevYou pay 99 per year. But this definitely doesn’t cover their cost. The review process is very labor intensive on their side reply munk-a 2 hours agorootparentAs someone who has developed a commercial app and spent time on the app store - their review process is a joke... there are non-compliances all over the store and I suspect a lot of their review process is highly automated. reply wouldbecouldbe 1 hour agorootparentYeah just because it's labor intensive doesn't mean it's good. Im sure both App stores have lot of automated tests. But I've submitted a lot of apps and the feedback from Apple is much more specific and from humans. I agree it's very annoying, often complaining about things that are explained in submission notes. But if I submit and do around 5-10 updates per year that seems highly unlikely it covers their salary cost. reply wkat4242 8 minutes agorootparentprevIt depends how often you submit. Also they do it mostly in cheap labour countries. And it doesn't have to conver the cost really. It's not a service to developers like developer support would be. It's more an impediment due to its randomness. throwaway346434 1 hour agorootparentprevWould you pay a drunk rich person $99/year, so that you can publish a community newsletter to your local town or sell custom decals to your state's car enthusiasts club? Who then randomly decrees your newsletter is not allowed, forgets why, then slurs THIS CONVERSATION IS OVER and bans you yelling \"I'm everybody safe, keeping!\" In this case, who exactly are they protecting, the townsfolk or car enthusiasts that you have an independent relationship with? Would this scenario seem like a good idea to agree to? If no, why is the app store/walled garden model an appropriate use case at all/how is it substantially different? reply jacquesm 1 hour agorootparentprevThe review process is only there to give Apple a fig-leaf to remove apps at will without recourse. reply akmarinov 1 hour agorootparentprevNot really, I put in a feature to record the screen and all they did was launch the app, click a button and then auto approve reply ffgjgf1 1 hour agorootparentprevHowever it’s so inconsistent and arbitrary that it hardly ever mattered. reply la_oveja 2 hours agorootparentprevyou are trippin reply jacquesm 1 hour agorootparentprevOh no, those devs would have certainly passed the difference on to the consumer. reply camillomiller 3 hours agorootparentprevThis is a biased view that disregards basic available metrics. Apple is a hardware company. Developers are instrumental to its devices success and a point can be made that 30% might be too high of a fee. On the other hand many of those developers wouldn’t have a job in the first place if it wasn’t for Apple creating the App Store. reply johnnyanmac 3 hours agorootparentI find it interesting seeing the arguments here on how Apple should keep its large cut for years after it's become sustainable, but when mentioning the idea of giving copyrighted artists any sort of royalty (it'd be far, far, far from 30%) for training LLMs that the argument shifts back to \"well they got paid already\". So, how long does Apple get to reap the rewards of their old accomplishments from 18 years ago? how long should such works be benefited from before we shift the dynamics back to being \"a public commons\"? reply pompino 2 hours agorootparentAgreed, While there may be people who think they're defending Apple \"on principle\", I hope those folks also realize that there is no \"principle\" that is ingrained in nature. We're all just making up rules, laws, taxes, as we go along. Just because a law or article of constitution is old, doesn't make it any more 'natural' than others. There is no \"right\" of any student for their debt to be forgiven, but we want to do it anyway. Apple has taken advantage (as have others) of a ridiculously broken tax code, availed of the strong US legal system, property rights, etc. How about we shift the balance back? reply robertlagrant 36 minutes agorootparentprev> I find it interesting seeing the arguments here on how Apple should keep its large cut for years after it's become sustainable, but when mentioning the idea of giving copyrighted artists any sort of royalty (it'd be far, far, far from 30%) for training LLMs that the argument shifts back to \"well they got paid already\". There are multiple people on here, who say different things. reply kergonath 2 hours agorootparentprev> I find it interesting seeing the arguments here on how Apple should keep its large cut for years after it's become sustainable, but when mentioning the idea of giving copyrighted artists any sort of royalty (it'd be far, far, far from 30%) for training LLMs that the argument shifts back to \"well they got paid already\". Do you account for the fact that it might not be the same people making both arguments? Most websites’ readerships are not monoliths and even on HN there are plenty of people with different perspectives, and who are not necessarily vocal in the same threads. > So, how long does Apple get to reap the rewards of their old accomplishments from 18 years ago? how long should such works be benefited from before we shift the dynamics back to being \"a public commons\"? That’s an interesting argument, but it’s usually not discussed with any nuance. Basically there are several layers: - are we entitled to Apple opening their platforms? (AFAICT the opposite would be a first though the EU seems to be going that way) - is Apple entitled to profit from the App Store in principle? (Some people are arguing that they are not, but they are a fringe; Epic lost their argument about that) - is 30% too much? (But then, where is the line? It’s more or less the standard for closed platforms Where would you put your “public commons”? Did this ever happen? reply throwaway346434 1 hour agorootparentprevThey are a hardware company. By the same token, can you imagine a car company controlling the fuel you put in your car, the tires you buy, the repair shops you use, the radio stations you can listen to? reply culturestate 1 hour agorootparent> can you imagine a car company controlling the fuel you put in your car, the tires you buy, the repair shops you use Assuming a slightly generous definition of \"the fuel you put in your car,\" you've just described a lease. reply lostlogin 19 minutes agorootparent> you've just described a lease. Or the purchase of a German car. reply jfk13 1 hour agorootparentprevOr a printer company controlling the ink you put in your printer? Unthinkable! Oh, wait... reply pompino 2 hours agorootparentprevThat may be, but IMHO its impossible to be completely neutral on this issue. All analysis is somewhat compromised and biased based on subjective weightage to historical facts, etc. reply kridsdale1 3 hours agorootparentprevOk. Write your app with zero use of the non-posix system calls or any of the platform SDK. Because they wrote that. Why do you get to run a business off it for free? They spend hundreds of millions on SWE salaries to make a nice SDK for you to use. They’re also operating all the servers. Your $100 does not cover it. It’s a symbiotic relationship. At least see it and acknowledge it. reply jzl 3 hours agorootparentIt’s a symbiotic relationship Yes, exactly. It's symbiotic because Apple needs apps to sell iPhones in the first place. Apple is already getting their cut of the symbiotic relationship by selling their massively profitable iPhone hardware, full stop. Any chance you remember the first year of the iPhone when there were no third party apps? Other than being a good phone, it was basically just a technological novelty, and that was about it. Third party apps are literally what give value to the iPhone (and other iOS devices). reply aspenmayer 3 hours agorootparentCydia was live on iOS before Apple's App Store. Which is to say, I agree. reply ankit219 3 hours agorootparentprevI have never fully agreed with this premise. On the face of it, yes apps help people stay on the store. But at the same time, Apple does come up with many good apps. Comparing it to the first iPhone is probably valid. They saw the writing on the wall and opened the gates, but could have easily chosen the other way of doing it themselves. No one wins in this scenario - nor the devs, nor the users, nor Apple. But it's not as clear as \"Apple needs apps to sell iPhones in the first place\". Idly wondering, say you don't download any third party app and neither does anyone. You can still surf the internet for the most part. You can still connect with others via FaceTime/iMessage. You can still check mails etc. Things you won't be able to do is play games, go on dates, browse tiktok, book a cab (though all that is possible through safari but not a great experience). A caveat here: The original 30% was on infinitely reproducible digital goods only. Which is 80-90% gaming skins and artifacts and digital subscriptions. Don't know if that is still the case. reply jrowen 2 hours agorootparentprevIt's symbiotic because Apple needs apps to sell iPhones in the first place. And developers need a solid platform with users willing to pay money. I feel like you have the wrong answer to the chicken and egg problem. The first iPhone was a marvel to pretty much everyone, and blew everything else at the time out of the water. I don't recall much sentiment of people being disappointed about the lack of apps, because there was no reason to expect that at that point. Just being able to use a lot of the web like you would on a computer was a massive leap from a RAZR or whatever. This is not to defend the 30% tax or anything, but I don't think it does any good to downplay the value they've built. The elephant in the room with your argument is that, if it were true, app developers would simply flock to Android and Apple would be forced to course-correct. reply pentae 2 hours agorootparentPeople seem to think that the customer exists because of the iPhone app store, when it's often usually the other way around. Other than a small handful of apps being promoted for free in the app store, the majority of businesses have to spend very real, very expensive advertising dollars or offer a unique product to attract customers. If you have your own product your customers want but they just happen to have an iPhone, you're now forced to pay the maker of their device 30% off the top of your gross revenues like some kind of Mafia. This same Mafia does everything they can to make sure that web/Safari experience is as poor of an experience as possible so that users prefer native apps delivered through the app store. Imagine if you sold expensive CAD software through your own sales team and advertising efforts completely offline but because some of those users were on Windows they have to get it through the Windows store and 30% of your topline revenue went to Microsoft? Because of the way online marketing/advertising works with competitive bidding in a lot of cases I would bet that Apple is making more margin than the developers of the apps are. Your competitor who doesn't have to pay a 30% Mafia shakedown fee for their product will be able to outcompete you on clicks every time. reply jrowen 1 hour agorootparentThis same Mafia does everything they can to make sure that web/Safari experience is as poor of an experience as possible so that users prefer native apps delivered through the app store. Can you expand on this? I prefer to use the web when I can, and every time I've begrudgingly downloaded an app, it's the service itself that was actively pushing me away from their poor/hamstrung web client. I'm still left with the impression that users are staying there and developers jump through the hoops because it's just that good. That doesn't mean it's right, but - that's Capitalism, baby! Also, the MS of yore would have absolutely done that if they thought they could get away with it, and definitely engaged in more than their fair share of unsavory tactics. reply ffgjgf1 1 hour agorootparentprev> The first iPhone was a marvel to pretty much everyone IIRC its sales figures weren’t that great though. Of course from Apple’s perspective it was more of a prototype than the actual final product. > Just being able to use a lot of the web like you would on a computer was a massive leap from a RAZR IMHO not having 3G basically turned it into a toy.. also comparing it to a consumer flip-phones isn’t that fair. You you had touschscreen/stylus “smartphones” from SonyEricsson, Nokia etc. which weren’t that awful at the time (of course the UX was inferior after it actually became possible to the internet on your iPhone when the 3G was released). reply kelnos 2 hours agorootparentprevApple was just fine building all those platform SDKs for macOS without charging people to write apps for it. Why should iOS be any different? The servers do not cost 30% of all sales on the platform. Not even close. But none of that matters. Very few businesses set their prices based on how much it costs them to provide their goods or services. They charge what the market will bear. The problem is that there is no \"iOS app distribution market\"; Apple controls it, 100%, so they can set prices wherever they want, without regard to competition. reply bryans 1 hour agorootparent> The servers do not cost 30% of all sales on the platform. Not even close. Specifically, the transfer cost to download a 50MB app is approximately $0.0005. That's 200,000 downloads per $100. Even assuming a low average revenue of $1 per download, that's a $60,000 take for Apple for maybe $105 worth of hosting. That's not to say there aren't other costs involved in running the store. But we definitely shouldn't be pretending that hosting is even relevant to the conversation. Hosting costs haven't been relevant for over a decade. reply flumpcakes 1 hour agorootparent> 50MB app is approximately $0.0005 It's probably orders of magnitude smaller than that, like 2e-7 dollars per MB. People have gotten use to AWS/Azure/GCP pricing when bandwidth is essentially free at scale. You can rent a few 100Gb/s ports for ~$500 to ~$5000 per month depending on location. But I guess this is best case scenario and not everyone will have the capital/clout to colocate at POPs. reply TerrifiedMouse 1 hour agorootparentprevI feel the App Store is one thing differentiating Apple from Android phone manufacturers. Money from the App Store gets reinvested in the entire iPhone supply chain. Whereas in the case of Android, App Store money goes to Google only. It also encourages Apple to support their phones long term as its preferred that you stay on your aging iPhone and keep spending at the App Store then risking you leaving the Apple ecosystem because Apple allows their older phones to become obsolete. Android manufacturers on the other hand have every incentive to obsolete your phone ASAP because you buying a new phone is the only way they make money. reply albert180 1 hour agorootparentDo you seriously believe this? It's Apple that's blocking you from installing any apps or updates when you don't receive iOS Versions Updates anymore, while Google Play doesn't care about it, as long as the app itself is still developed for that version. Also they make more than enough money from the Hardware itself, they just take the profits off the software purchases. reply ffgjgf1 1 hour agorootparent> that's blocking you from installing any apps or updates Technically that’s the app developers who do that. reply albert180 1 hour agorootparentIsn't it XCode that blocks you from developing for older versions at some point? reply Daedren 45 minutes agorootparentNo, Xcode requires you to have an up-to-date MacOS to develop, not iOS. However, developers are strongly encouraged to raise their apps' minimum iOS versions since Apple does not backport any APIs to older versions, any new thing presented at WWDC is only for the new iOS version. This also applies to bugfixes in some cases (SwiftUI). So as a developer you either stagnate your knowledge or keep moving forward. Google does backport many APIs, so the issue isn't as problematic on that end. reply sally_glance 40 minutes agorootparentprevDoesn't Google sell hardware as well? Would it be better if Android was only available on Pixel phones? reply AYBABTME 2 hours agorootparentprevThere's a point where the phone becomes similar to infrastructure, and allowing a single market actor to charge 30% tax on anyone using the roads, for any business conducted where someone took a road to get there, is ridiculous. I'm not saying the App Store is infrastructure-like. But the market share of Apple is so large, and it's quasi-impossible to conduct consumer business without being required to have a native app... that it's starting to be infrastructure-like. Can a viable B2C online business exist without being present via a native app? It seems untenable due to consumer expectations. Consumers also expect to be able to drive to coffeeshops, yet road providers don't charge 30% taxes on coffeeshop revenues. reply MilaM 1 hour agorootparent> I'm not saying the App Store is infrastructure-like. It is infrastructure-like. Try to live in modern society without a smartphone. It will be very difficult. Not impossible, but much less convenient. reply AYBABTME 1 hour agorootparentI meant I'm not ready to say that it is, I have not solidified this as a fact in my own mind. I'm contemplating. reply thayne 3 hours agorootparentprevAnd if no one wrote apps for iOS how many people would buy iPhones and iPads? Yes, it is a symbiotic relationship. But Apple has better position to negotiate, because it is as single entity, and have a lot more resources than the the other side. If all the app developers were able to organize and boycott apple, they might be able to force Apple to negotiate better terms. But that would probably result in many of the smaller companies going out of business, unless the boycott was very short lived. > Your $100 does not cover it. There are almost 2 million apps on the app store. Assuming there is roughly 1 developer license per app, (some developers will create multiple apps with a single license, but others will have multiple licenses to develop a single app), that is at least in the ballpark of \"hundreds of millions on SWE\". And that doesn't include revenue from selling the devices themselves. reply camillomiller 3 hours agorootparentYou just proved your point wrong. Hundreds of million is peanuts, and it’s very improbable that it covers anything near the HR costs of Apple’s App Store infrastructure and department. reply dvdkon 2 hours agorootparentIf Apple can't run the App Store just on developer fees, then I'm sure somebody else could. But that's conveniently not allowed. reply ikiris 3 hours agorootparentprevWhere do you think those app developers / apps originally came from. Huge amounts of people bought those iphones first man. reply kelnos 2 hours agorootparentSo what? Shut down the App Store tomorrow, and forcibly delete all non-Apple-authored apps off of everyone's iPhones. Watch everyone flock to Android, no matter how much they claim to hate it. Watch Apple's stock drop into the toilet. Apple needs all their third-party developers just as much as those developers need Apple -- more than at least some of those developers need Apple. reply yazaddaruvala 1 hour agorootparentIf the AppStore was shut down tomorrow, and all my apps forcibly removed: I'd keep using my iPhone with Safari. Might even become a better experience. I'd need to save some short cuts for things like Google Maps, my banks, etc but that seems like all of the hassle. I might miss the notifications (exclusively for the messaging apps - for all other apps I don't want them to have notifications) but otherwise, the biggest annoyance would be on the airplane I wouldn't have some pre-downloaded streaming content from apps like Netflix and Youtube. reply Detrytus 12 minutes agorootparentI guess you do not really use your iPhone that much :) For me removing non-Apple apps would render my phone useless: losing contact with my friends on WhatsApp and Signal, no way to call an Uber/taxi, no mobile authorization for my bank accounts, no MFA apps that I use for work... pompino 3 hours agorootparentprev>Why do you get to run a business off it for free? There isn't a divine commandment as to what rights we \"should\" have. We vote, fight, protest and get to make up our own rights. >They spend hundreds of millions on SWE salaries to make a nice SDK for you to use. They’re also operating all the servers. Your $100 does not cover it. It’s a symbiotic relationship. At least see it and acknowledge it. I don't feel like paying for an Apple executives BMW car payment. But you do you. reply Ma8ee 2 hours agorootparent> There isn't a divine commandment as to what rights we \"should\" have. We vote, fight, protest and get to make up our own rights. Not in America. The only venue to change you have is to spend, or not spend money somewhere. Since people think they profit more from developing apps for iOS than not developing apps for iOS, Apple’s profit is “right”. And a century or so of indoctrination has made most Americans believe that this is just the natural order of things. reply robertlagrant 33 minutes agorootparentprevThis isn't about rights. You already have all the rights you need here. You would like others to work for nothing, or for what you care to give them, instead of the price they offer their services at, and you'd like to vote, fight and protest until they work for your price. reply ffgjgf1 1 hour agorootparentprev> Why do you get to run a business off it for free? So Microsoft did all of this on Windows (or Apple itself for that matter for macOS) due to entirely altruistic reasons? Also platforms need developers a whole lot more than developers need specific platforms. You can always go and works at a bank or something. Apple would be skewed with no 3-rd party developers they always knew it. > They’re also operating all the servers. Your $100 does not cover it. It’s a symbiotic relationship. Maybe the ads on the app store would more than cover it? Not only do you need to pay 30% you also have to pay extra for placement so that Apple wouldn’t put your competitors apps at the top of the results when users search for your app? reply NineStarPoint 3 hours agorootparentprevWould Apple even approve something going on the app store that tried to do things in ways that were that non-standard? More generally it’s not like you can choose to host your own servers or, in forgoing all of Apple’s work, not have to pay that 30%. When Apple isn’t going to let you do things another way, they don’t get to argue that all of the benefits they do provide are what people are paying for. People have to pay the price to be allowed on the ecosystem at all. reply guax 2 hours agorootparentprevApple wrote their kernel from zero? reply kridsdale1 2 hours agorootparentNo, that’s why I excluded posix parts of the operating system. That’s from Berkeley. reply 6510 2 hours agorootparentprevI look at it like this. If someone is doing it you have to do it too, you cant run a competitive business if your competitor is taking 30%. It is not something you chose to do but you have to. The most important angle is what this grows into. Apple doesn't have to make hardware. They can simply stop doing that and live on rent alone. Hardware is hard work. It might just be that they have to stop doing it. How much they put the squeeze on who and who makes how much money isn't even important. This is not beneficial to the industry or to society. After the age of working your ass off while starving comes the age of contributing and thriving, this period ends with the age of entitlement. You do the work, I enjoy the benefits. we can only afford so much of that before our civilization turns into a hell hole. You will be working day and night to make ends meet while I have all the time and resources in the world to figure out how I'm going to take just a little bit more from you. It's not about wealth, people should be able to EARN whatever they EARN. It isn't even about mortality, we can go back to the age of working your ass off while starving, we know how to do that. The bigger issue is to have people in control who cant tie their own shoes. It sets us up for total failure. It takes one small incident to end humanity. reply kridsdale1 1 hour agorootparent> Apple doesn't have to make hardware. They can simply stop doing that and live on rent alone. This is what Valve discovered. Nobody in 2004 would have thought the best developer in the world would stop making games. reply johnnyanmac 3 hours agorootparentprev>They’re also operating all the servers. Your $100 does not cover it. I mean, let's be real here. Most people even if Apple fully allowed other payment processors would still opt into Apple's payment. Easiest to setup, most visible and familiar interface for consumers, and lets them push liability back to Apple. So they'd still get 30% from many consumers. This angle only applies in a situation where you think 1) the pareto principle applies and 2) all of that 20% decided to roll their own payment processing, which isn't trivial nor a risk many want to take onto themselves. It'd take a lot more than a dozen megacorps doing this to make Apple sweat. And if this does happen, it sounds like a service issue a that point. If you can offer all that convenience but people don't view it as worth 30%, then maybe the rate should be re-negotiated. >They spend hundreds of millions on SWE salaries to make a nice SDK for you to use they spend hundreds of millions to get other businesses to do business with them, and still charge many upfront costs along the way. Back then, we simply called that the cost to do business. Adtech also spends hundreds of millions each on salaries to make a nice SDK for you to use. I don't exactly feel I owe those companies for their labor since I'm (the dev) not the primary point of revenue. reply greysphere 2 hours agorootparentprevYou just described most games. reply kridsdale1 1 hour agorootparentI am aware. reply Hamuko 3 hours agorootparentprev>They’re also operating all the servers. Your $100 does not cover it. You can get about 1.1 terabytes of data transfer out from Amazon S3 to Internet for $100. reply ryanbrunner 1 hour agorootparentAnd anyone on Apple's scale is paying orders of magnitude less than that, even through Amazon. reply Hamuko 1 hour agorootparentI mean, if you were self-distributing your iOS app right now instead of having Apple do it, and you had to look up how much transferring data in the terabytes costs, you'd just use Cloudflare R2 instead. I'd be willing to bet that, all things being equal, most apps would save money if the developers were self-distributing them instead of using the App Store. reply gigatexal 37 minutes agorootparentprevPeople also need to remember how it was before in the CompUSA or telco provided phones. The retailer or marketplace would take > 30% margins closer to 50% and to get on a pseudo-smart phone before the current smart phone era one had to ask the AT&T's and Verizon's very nicely. But now one can build apps now and publish and just pay the 30% comission or 15% on subscriptions after the second year and look at the explosion in the app marketplace. reply psychoslave 2 hours agorootparentprevIf merit was a highly pondering factor of income, coal miners would be extremely rich and no annuitant would exist out there. reply pratnala 2 hours agorootparentprevI agree that Apple should take some cut but 30% is predatory and excessive. reply DeathArrow 2 hours agorootparentThey can get 99% provided they allow the users the freedom to download and install apps from wherever they want. reply yMEyUyNE1 2 hours agorootparentprevI view such companies as Trolls under the Bridge (i.e. appstores) that connect the app developers and the users. reply ben_w 39 minutes agoparentprev> You don't need to defend the trillion dollar company. They are not your friend, they do not care about you, your work or your life. True. > All they do is steal 30% from society that could be used for more productive purposes than make a few people who already have everything even richer. I'm old enough to have developed software before the App Store existed, and remember that everyone was very excited both buy it finally being introduced to iOS, and by the relatively low fees of only 30%. You're free to argue that 30% is too high, or even that the 15% for small developers is too high, that this is rent-seeking by Apple and only made sense when they were also a small company… but I think this is also true for the businesses trying to convince everyone that it matters, and I think they would like to charge the same sticker price while collecting the difference for themselves. reply pjc50 9 minutes agorootparentIt's a small amount compared to retail; it's a large amount compared to a download; and it's infinitely larger than the 0% platform royalty required by IBM-compatible PCs. reply sofixa 34 minutes agorootparentprev> and by the relatively low fees of only 30%. 30% is low compared to what? Mafia extortion rates? reply the_other 15 minutes agoparentprevI often defend Apple. I don't want to defend Apple on this current topic: I think taking a cut of out-of-app physical purchases/subscriptions is ridiculous and should be legally challenged. However... > All they do is steal 30% from society that could be used for more productive purposes than make a few people who already have everything even richer. If you're gonna argue that, you have to inspect the edges of this position. Are you willing to let go of your own shares? Or the interest on your pension (or equivalent), which comes via the roughly the same route you're arguing against. If you own your own company, do you share most/all profits with your staff (rather than pay a wage), or your shareholders? This is all wrapped up together. IMO, To make such a claim, you need really to argue against the system these \"rich people\" operate in as much, if not more than, the individual cases. reply quonn 5 minutes agorootparent> If you own your own company, do you share most/all profits with your staff This is not the same, there a degrees of unethical behavior. We should collectively try to give more meaning to these degrees instead of jumping to first principles. Taking your example, there is a difference between taking a cut or underpaying your staff or severely underpaying them or perhaps not paying them at all or perhaps forced labor or perhaps outright slavery. reply Halvedat 2 hours agoparentprevIt's very likely that people posting on Hacker News may hold equity in a company like Apple, may have their primary form of income derived from that company or may be able to attribute their sizeable wealth to its growth. It is no surprise that people will come to the defense of this giant when you stop to consider this. Apple doesn't have to care about them, it already has. I am of the opinion that there should be some sort of disclosure of financial interest. reply fauigerzigerk 34 minutes agorootparentMaybe so, but I never found this line of argument particularly convincing. The investments people have in Apple are often insignificant compared to the rest of their income or wealth. Also, buying shares in the public market is like a bet. Instead of changing your opinion to agree with the bet you could simply bet the other way. Or you could bet that your own political activism will fail. Betting on an outcome doesn't mean you prefer that outcome. It can also be hedging. The people who really do have something riding on Apple's success are employees getting stock options. And yes, I would also like to know whether someone is an Apple employee when they are commenting on these subjects. Developers are affected by Apple's policies and success in very complex ways and can legitimately take either side on these questions. Disclosure: I have an app in the App Store that made me ~£100 in the previous fiscal year. I also have £3000 in a NASDAQ 100 ETF. Apple's share of that is ~£270. reply lotsofpulp 2 hours agorootparentprevPretty much everyone that invests in the public equity markets holds equity in Apple. In fact, Apple would be most public market investor’s biggest holding (or 2nd biggest due to Microsoft’s recent increases) via index funds. Even if you are not invested in public equity markets, you can be exposed to them via local and state government’s pension fund investments, because if those do not perform as projected, then your taxes have to make up for it. reply darkwater 1 hour agorootparentWith the caveat that if you just invest in indexed funds, if Apple does worse than before, probably some competitor does better than before, and it might compensate. If you just own AAPL stocks on the other hand... reply xanderlewis 20 minutes agoparentprevI haven't done any defending, but I reject the idea that it's a ridiculous idea to do so. I choose (almost always) to defend people on the basis of whether they are right or wrong (by some measure), emphatically not due to some expectation of return on investment because they 'care about me'. Also, no one is defending Apple because they think 'the trillion dollar company needs defending' -- they're doing it, presumably, because they just so happen to have an opinion on the matter that aligns with Apple's. reply highwaylights 1 hour agoparentprevKeeping the tax at more or less the same rate on outbound links is incredibly brazen, even by Apple standards. If the EU DMA does eventually force them to open the platform to competing stores it’s going to be very hard to defend the different policies in different markets, especially as I assume Epic will push aggressively to have Fortnite and the Epic Games store on iOS in those markets as early as possible to force the conversation. reply lijok 2 hours agoparentprevYou need to learn the difference between defending an entity and defending an argument. The line of thinking you're expressing here is what leads to questioning free speech and privacy absolutism. reply yard2010 12 minutes agoparentprevIkr. That's a big incentive to be evil! Special snowflakes will vouch for you no matter what reply DeathArrow 2 hours agoparentprev>They are not your friend, they do not care about you Of course they care about you. They hope you are healthy and in a good shape so you can work more to earn more money and give them their share. reply dragonelite 1 hour agoparentprevThe lords and churches of the past wouldn't even dare to ask as much as 30% of revenues in our feudal past. reply riscy 54 minutes agorootparentIt’s 15% for the commoners (All they do is steal 30% from society that could be used for more productive purposes This is how all businesses work. If there was no way to make profit then businesses would not exist. Apple spent billions of dollars creating an app platform with a clear monetization model that did not get in the way of them accumulating a lot of valuable apps and users. Developers are not forced to make apps for the platform nor are users forced to use the app platform. Other app platforms can impose lower fees and developers are free to release exclusively on those platforms if they wish. Apple hopes that the developers willing to tolerate the 15/30% fee for what the developer gets in return will be good enough to make their app platform competitive to users compared to others. It's not just defending a trillion dollar company, it is defending the right to set your own prices. reply flanked-evergl 2 hours agorootparentSomething is not moral just because it makes a profit or because it is legal, and revenue is also not profit. And I think if a company operates outside societal norms, which I think Apple with regards to European societal norms, it should expect to get regulated to fit those norms again. Maybe the US is too dysfunctional to do this any more, but the purpose of government should be to align the laws and regulations with the morals of the people being governed. My bank generates revenue by offering me a service in return for my money, not by monopolizing access to my money and then charging people who want to sell to me for the honour of allowing me to buy their goods and services. Very few companies that I deal with as a consumer have similar business practices, and the ones that do, like Visa and Mastercard, is also something I think should be cracked down upon. There are many things the EU messes up in my opinion, but cracking down on this clearly immoral business practices is not one of those as it aligns 100% with my morals even though I'm incredibly pro \"free\" market (i.e. pro minimally regulated market, as every person who has ever been pro \"free market\" is). reply DeathArrow 2 hours agorootparent>My bank generates revenue by offering me a service in return for my money, not by monopolizing access to my money and then charging people who want to sell to me for the honour of allowing me to buy their goods and services. Well, maybe Apple should open a bank. You just gave them ideas. :) reply flanked-evergl 1 hour agorootparentIf that is the only way they can make profits while aligning with the norms of the societies they operate in then sure, more power to them. reply DeathArrow 2 hours agorootparentprev>Apple spent billions of dollars creating an app platform with a clear monetization model I don't want their platform forced down my throat. On my PC I can download and install software from wherever I see fit. Had I not being a MacBook Pro user for the time being, I would have a chance to upgrade RAM and SSD without paying twice on the damn device. If anything, I consider Apple being an anti consumer company. What is good for them, is not good for the end user. That being said, their devices do have some advantages. reply lijok 2 hours agorootparentWhy did you buy a Mac then? You clearly knew the drawbacks, so I'm interested to hear, in this competitive market where both Windows and even Linux machines are now available, what made you decide to buy a Mac? reply DeathArrow 1 hour agorootparent>what made you decide to buy a Mac Battery life. reply yazaddaruvala 48 minutes agorootparent> I don't want their platform forced down my throat. > I would have a chance to upgrade RAM and SSD without paying twice on the damn device. >> what made you decide to buy a Mac > Battery life. When you made the choice to buy your MacBook because of the better battery life were you at any point lied to about the disadvantages? Additionally, if you're not aware the RAM on SOC is a fundamental tradeoff because of physics. Hardwiring it into the SOC is a BIG part of the battery life improvements that you have stated you prefer. reply charcircuit 37 minutes agorootparentprev>I don't want their platform forced down my throat. As a consumer the whole point of buying a computer is to get access to its app platform. It's not forced down your throat it just inherently is a part of the device's identity. reply totaa 2 hours agorootparentprev> nor are users forced to use the app platform is side loading or alternative stores like f droid available on IOS? reply charcircuit 44 minutes agorootparentYes, but it is for supporting developers and enterprises. reply kriops 1 hour agoparentprevThat’s a strawman if I ever saw one. Apple is morally entitled to licence their products however they’d like, because property rights. If you don’t like it, then nobody is forcing you to give them your business. reply puszczyk 2 hours agoparentprevI don’t care about trillion dollar company. I care about my experience. App Store purchases and subscriptions are a good experience for a user. I’ve never had problems canceling subscriptions, or getting refunds. reply ric2b 22 minutes agorootparentOk, but the argument is that people should have options, not that you should stop using the Apple store. reply thealistra 17 minutes agorootparentI am afraid that some developers will drop Apple payments all together and I will have to type my credit card info inside of low-quality apps. Currently I just press ok after a Face ID. reply riscy 3 hours agoparentprevI've yet to see an HN hate thread about the trillion dollar company Google and its Play Store, with the same fees, stealing 30% from an even broader part of society. reply kelnos 2 hours agorootparentWhile most people do install apps through the Play Store, Google doesn't lock down their platform to make it the only option. I have one alternative store (F-Droid) installed, and also have a couple apps side-loaded. Google's 30% isn't the only option on Android. Apple's 30% is the only option on iOS. Regardless, Google gets plenty of flak here for a variety of things, including how they run the Play Store. reply riscy 2 hours agorootparentI really don’t think it’s about side-loading, principles of freedom, the spirit of hacking, etc. HN’s full of indie app developers trying to make money. They know iOS users more often pay for things so it’s their target audience. I doubt existence of F-Droid is even a drop in the bucket of fee savings for those developers on Android. Otherwise, Google would do something about it to get their cut. reply account-5 3 hours agorootparentprevYou clearly not been looking hard enough; Google, rightly, gets hated on way more than Apple. My experience tends to be that on HN apple can do no wrong. reply riscy 3 hours agorootparentGoogle gets mild-mannered, disappointed commenters sighing about how they've messed up search, the web, and has no product/customer support. When Apple comes up, people get on their soap boxes with expletives about how society and the world is fundamentally being ruined. The tone just isn't comparable at all. reply account-5 3 hours agorootparentYou're definitely not looking in the right places then. I've seen Microsoft, Google, twitter, Mozilla, you name the company; coming under fire and everyone piling on. When apple gets criticised in the same way (few and far between that it is), you get an army of apologists out in force ready to die to defend all that apple does. I've never come across a company that instills that sort of blind faith in its users. My personal stance is I don't trust any of these companies and will come to threads to be informed about whatever privacy/security/monopoly practices these companies are trying to bypass/do this week for their own profit. I have no allegiance, they're all as bad as each other. In my experience apple is able to do more and gets criticised less. A recent example was Google implementing something in chrome apple had implemented ages ago in safari. You actually had people saying is was ok for Apple to do it but not Google. reply DeathArrow 2 hours agorootparent>you get an army of apologists out in force ready to die to defend all that apple does. They defend their choices. But do that in an almost religious way. And is not something that is particular to Apple. Try to criticize Under Armour or New Balance in front of someone wearing it. reply prmoustache 3 hours agorootparentprevI don't think so. The main difference is that most people use one of the google products directly or indirectly, regardless of their attitude towards the company. In comparison it is easier to not use Apple products if you don't like the company. And thus people like you who own an Apple device feel targeted for their choice of using Apple and thus see it as more aggressive and powerful. It is as simple as that. You can find this pattern in every kind of domain, I see people replying with anger and/or passion to any criticism on their car, motorbike or bicycle brands. They naturally feel compelled to defend their brand of choice because they actually feel targeted as owner of it, because it feels like their own discernment is targeted indirectly. reply DeathArrow 2 hours agorootparent>They naturally feel compelled to defend their brand of choice because they actually feel targeted as owner of it, because it feels like their own discernment is targeted indirectly. True. But regardless if the commenter is right or wrong, anger is not the proper answer. If the commenter is right, you have some thinking to do. If he's not, you shouldn't care. reply kelnos 2 hours agorootparentprevWell I guess we need some people at the other extreme to balance out all the Apple-is-the-messiah types. I'm being glib, here, but I think that's a fairly normal effect. If people's opinions about something are generally pretty boring, average, and uncontroversial, few people will feel the need to stir the pot and adopt extreme views. But seeing others unquestioningly, unapologetically drooling over something, without allowing any sort of criticism, just eats at some people so much that they need to adopt the completely opposite position and find any reason to brutally criticize. Human nature is weird. reply DeathArrow 2 hours agorootparentWhy do we need to balance that? If people do believe Apple is Messiah, they do that at their own loss. Why should I care? reply johnnyanmac 3 hours agorootparentprevyou must have missed the soapboxes on how google cannot properly support products anymore (The \"google graveyard\") and especially any topic touching on Youtube. But sure, for Android topics they get off lighter because devs do technically have F-Droid as an option, or simply hosting an APK on version control for user s to find. There are ways to get around Google's barriers even if they have a steep financial penalty. Apple gives no official way without voiding your warranty (I don't even think rooting your Android these days void you). reply DeathArrow 2 hours agorootparentYou can install only some types of apps from third party stores or sideload them. Many apps like banking apps require using Google Play Services and if you use a third party ROM like LuneageOS or Huawei HarmonyOS, good luck with installing certain apps. reply paulddraper 3 hours agorootparentprevThe iOS vs Android flamewar has a clear winner on hn reply hackernewds 3 hours agorootparentprevAu contraire I believe Google is the most hated company on HN by a mile and half. We see this play out IRL even with their anticompetitive lawsuit outcomes reply DeathArrow 2 hours agorootparentBoth Apple and Google do nasty things. Other companies, too. This thread is dedicated to Apple's own wrongdoings. If Google does nasty things, that doesn't mean Apple should get a pass. Most people here care about consumers, not about companies, especially about monopolistic companies. reply riscy 1 hour agorootparentApple allowing alternate payment methods with a fee discount in the US, as Google does in other countries, is a wrongdoing? Consumers don’t care about the fees developers pay. reply conradfr 3 hours agorootparentprevThe only difference is that you don't need to own an expensive Google device and pay $100 per year to develop for the Play Store, the 30% criticism applies as well though. reply hyperhopper 2 hours agorootparentprevGoogle doesn't force you to use Google Play and Google doesn't force devs to pay them. (Also they are still hated as well) reply dotnet00 3 hours agorootparentprevThere are threads pretty much daily about how evil Google is. reply throwaway20222 3 hours agorootparentprevFirst visit? Welcome! reply jve 3 hours agoparentprevStealing? I see it as fee for an exchange to access to a huge market of wealthy people (one that can afford an iPhone probably can afford your app). Where entry barrier is extremely low. If you make 100 sales, you don't have to pay much, if you make huge profits off the platform you make huge payments to the platform owner. reply hyperhopper 2 hours agorootparentThey shouldn't have the right to be the gatekeeper in that relationship. The wealthy people own the hardware, the devs own the app. Why does apple get the legal right to demand money in that transaction when it would be better for everyone if they weren't involved reply yazaddaruvala 44 minutes agorootparentSo what about people like me? I buy an iPhone because Apple gatekeeps the apps. I grew up in the age of torrents and Kazzaa and am tired of spam, malware, bloat, anti virus, etc. I want my phone to be an unbreakable toy, not a computing device. reply ric2b 27 minutes agorootparentSo you use exclusively the Apple store, problem solves? reply LoganDark 2 hours agorootparentprevWhen you buy an Apple device you don't control the hardware or the software on it. Apple does. reply DeathArrow 2 hours agorootparentprev>one that can afford an iPhone probably can afford your app If I afford something doesn't mean I should buy. I grew up being poor and I earn my living working hard. Throwing money is not a good option for me. Also, in what world affording a damn phone does make you rich? reply ric2b 28 minutes agorootparentpreviPhones are not expensive enough to signal you are rich, most people are able to buy one, the question is if it makes financial sense to do so. reply maccard 1 hour agoparentprev> It's incredible that there are actually people in this thread arguing in favor of Apple It's incredible to me that people arguing against apple here don't realise that they're arguing to allow another billion dollar company to do what they want, and that their own opinion is the only one that could possibly be valid If you want an open ecosystem with alternative app stores, head on over to Android. The value of iOS to me is the app store and related ecosystem. reply mihaaly 55 minutes agorootparentRegardless, it is a phenomenon here that negative words on Apple - justified or not alike - here attract numerious downvotes without comments, just for the sake of it. reply nprateem 51 minutes agorootparentprev> head on over to Android I'll just tell all my potential customers to buy a new phone before buying my app. That'll work. reply ribit 2 hours agoparentprevI have difficulty following this argument, especially the \"stealing\" part. App Store does not just randomly take money from the devs. App Store also provides a service. They do world-wide payment and VAT processing, refund processing, discovery, distribution, user login management, APIs, distributed cloud storage, etc. It costs money to run these things. As a small-time developer, I think this is a great deal for 15%. At the price levels of most small apps it would cost more to use a payment processor + hiring an accountant, not to mention the extra work involved in setting up and maintaining these things. For behemoth like Epic — sure, the \"Apple tax\" hurts, they'd rather gouge their customers without Apple's involvement. But frankly, I don't see any reason to punish one multi-billion corporation just so other multi-billion corporation can make more money. I care primarily about the interest of the small-time developers. And this is the point that should be made more often IMO — App Stores (and Apple bing one of the first ones) have democratized software development by making the barrier of entry extremely low. Anyone with some talent or idea can go and write an app, without any additional financial risk. App Store is based around sharing your success. The relatively few successful devs carry the costs to keep that barrier of entry low to everyone. And I really don't want that to change. On a serious notes, what are the alternatives? What exactly is your argument? That Apple should be charging nothing? Ok, then they also shouldn't be providing any services. You want to do distribution or payment processing? Take care of it yourself. Epic would love this of course, Joe the indie developer instead is dead in the water. Or are you arguing that Apple is charging too much? Well, there are solutions to that as well. They could charge for services individually for example, but that again hurts the small developer, because trying out things starts costing them money. Frankly, my idea would be to split the App Store into a separate commercial entity and make it nonprofit. I am sympathetic to the argument that the Store itself is not a product but is used to support and create value for Apple's ecosystem. I do think that the devs should pay for running the store, and I like the current success-based model and it's low barrier of entry for new devs, so basing the fees on actual operating costs seems like a good compromise. Of course, similar considerations should apply to other stores as well. reply jdkoeck 2 hours agorootparentAgain, it’s crazy people defend Apple on this. Apple is not just providing a payment platform, it’s forcing you to use it. As a developer, you should be free to use any payment platform you want in your app, like on the web. Let the user decide. End of story. reply riscy 44 minutes agorootparentApple provides the OS and SDKs developers need to make their app function at all. Users do not care how much it costs a developer. They want it to be easy to see and cancel subscriptions all in one place. The web’s myriad of payment systems is the opposite of a good user experience, meant to only fatten the pockets of developers by making it difficult to cancel. reply CogitoCogito 8 minutes agorootparent> Users do not care how much it costs a developer. This is a ludicrous statement. The users are the ones paying for it. reply maccard 1 hour agorootparentprev> Let the user decide. End of story. The value proposition of iOS is that the app store is the place to go, and that my experience will be seamless. I want a centralised place to manage my subscriptions. Here's an example: I subscribed to NYT Cooking in the web a few years back. I went to cancel only to find out that I have to phone them. If I subscribed on an app store it would have been one click and done. I'm actually still subscribed to it. Why is your choice more important than my choice? reply ric2b 33 minutes agorootparentRight, so you'd use the app store by choice, so why should Apple force you to? reply nprateem 45 minutes agorootparentprevYeah that's perfectly reasonable. Of course it makes everything easier. It's also practically impossible to compete against which is what gives apple this unfair advantage, making their 30% cut obscene. They're making it due to being first, technical issues aside. reply pcnix 1 hour agorootparentprevThe problem is that we place a responsibility on competition in the market to favor consumers by reducing prices and preventing companies from having excessive margins. Allowing a single marketplace means there's no competition, and we're not sure if 15% is a fair rate at all. I could make the assertion that I'd be able to provide everything that Apple does, but with a much lower cut, but this can't be put to the test because there's no way for me to start another app store that iPhone users can access. I suspect a lot of the arguments for the 15% cut will change once we have alternate app stores offering the same things Apple does, but with a much lower cut. You'll then see app developers with skin in the game, and we'll know if everyone actually really thinks Apple does this better or if they'd rather have the extra money from other app stores. reply ribit 1 hour agorootparentDo you really think that there will be more competition? I fear what will happen is that the big corps will set their own stores to distribute their own apps, and that's pretty much it. The user won't see any difference in pricing. The small dev will be hurt because each store will make less money and will likely implement price increases to compensate. reply ryanbrunner 1 hour agorootparentWe don't have to speculate - the desktop OS world has exactly this structure - an open ecosystem with a first party app store that ships with the OS, but the ability for other app stores to exist or even for developers to ship their products independently. In practice you still see a decent amount of activity on the official app store, along with some other major app stores, and a relatively small amount of independent distribution. There's still a good amount of small independent developers shipping apps (both on the stores and independently), and there's not a ton of evidence of price increases - in fact there's a very large amount of free software being distributed. reply dns_snek 1 hour agorootparentprev> You want to do distribution or payment processing? Take care of it yourself. Yes! That's exactly what everyone wants. > Joe the indie developer instead is dead in the water. No, Joe the indie developer will happily use one of the most popular alternative app stores that fits their needs and doesn't rip them off with fees. reply ribit 1 hour agorootparent> No, Joe the indie developer will happily use one of the most popular alternative app stores that fits their needs and doesn't rip them off with fees. What would these be? Last time I checked, the \"champion of the people\" Epyc charges 12% and pushes the charges for some payment methods onto the buyer. And it seems like they still haven't turned profitable, even with their bare-bones store model. Stripe (one of the most popular payment processors) will take 9% from a 5$ purchase just for payment processing. This doesn't include tax processing or any other stuff, all that you have to pay extra. A customer wants a refund? You are eating the cost. I just don't understand how any of this stuff people are talking about is realistic. I am not aware of a single commercial payment processing solution that will end up under 12-15% for small charges, while offering much less value to both the developer and the user compared to App Store. And I don't understand why people expect that this solution will suddenly magically pop up if Apple allows alternative stores. reply wouldbecouldbe 1 hour agorootparentprevIt's actually annoying that the refunds and VAT are handled. For small international developers, under 500K per year spread out over several states & countries, often they don't have to pay VAT in most international places, so they lose an extra 20-22%. For instance most US states won't require you to pay VAT under 100K revenue if you are from another country. Same for refunds & subscription management, often clients will ask you, but you have zero control with Apple. Let alone 60 days before being paid, where stripe does it in a few days. reply ribit 56 minutes agorootparentThis is a great point and I think it illustrates the drawbacks of centralized store. I think an argument can be made that App Store is an important part of the developer experience and as such they are entitled to have a voice in what features it should prioritize. The thing is, I fully agree that the model has to change and adapt. There has to be more transparency, more accountability, and these stores have to improve in a way that best fits the interests of the developers and the users. I just don't think that third-party stores or unrestricted side loading will do anything like that — in fact, I fear that they will make things considerably worse. reply commandersaki 1 hour agorootparentprevThe alternative is Apple allows side loading so that you can buy software independently of the App Store , and sellers can distribute independently of the App Store. reply ribit 1 hour agorootparentI am ok with side loading. Of course, side loaded apps would need to be sandboxes for security reasons and should not be allowed to access basic services like calendar, contacts or iCloud. reply CogitoCogito 1 hour agorootparentprev> On a serious notes, what are the alternatives? What exactly is your argument? That Apple should be charging nothing? The alternative is “competition”. In any case, I don’t see how any of this affects you since you’re happy paying Apple the fees. You can keep doing so as others pursue other options once they’re available. reply Aerbil313 2 hours agorootparentprevWell written. It can be argued that Apple also develops the OS and UI libraries which the apps run on/with, which is also providing something. reply todd3834 2 hours agoparentprevThey probably do care a lot about developers staying on their platform. The 30% is only from companies making more than $1 million. Otherwise you can qualify for 15% small business program. I don’t need them to be my friend or to care about me but as a share holder I want them to succeed. So far their R&D has proven valuable to me as both a consumer and share holder. I don’t care if a few people who already have everything get richer. Since I believe the company is doing great things and I think it still has a bright future ahead I get to share in that upside too. And so can you if you want to. reply eptcyka 2 hours agorootparentDo they care though? As a developer, I either play by the rules and get access to their massive, lucrative market or I just don't. Its not like the investment developers and Dev companies make into their ecosystem can be just moved elsewhere - its all a sunken cost. reply todd3834 2 hours agorootparentDo you really think they wouldn’t care if developers left their platform? They care because it’s beneficial to them. I don’t expect anything else from any corporation including the ones who’ve employed me. reply radley 9 hours agoprevThere's a strong chance this will be shot down as \"bad-faith\" compliance. Rumor is Epic will quickly contest it [Update: confirmed] https://twitter.com/timsweeneyepic/status/174740814726057173... reply judge2020 9 hours agoparentIt'll probably need to be another lawsuit since it's in the developer agreement. Via user lolinder ( https://news.ycombinator.com/item?id=39020745 ): > Apple is already pretty clear in its developer agreement [0] that the 30% commission is for \"its services as Your agent and/or commissionaire\" (Schedule 2 3.4), not for its services as a payment processor. They are contractually allowed to take the 30% fee out of payments collected, but merely using a different payment processor doesn't remove the obligation to pay them for their other \"services as Your agent and/or commissionaire\". 0: https://developer.apple.com/support/terms/apple-developer-pr... In addition, from the original ruling: > Yvonne-Gonzalez was skeptical of the 30% fee during the trial, and in the ruling she was suspicious about Apple's justification of the commission, writing that \"the 30% is not tied to anything in particular and can be changed,\" but did not order Apple to do so. reply justinclift 8 hours agorootparent> It'll probably need to be another lawsuit since it's in the developer agreement. Mind you, you can be blocked from doing needed Apple dev stuff (eg sign binaries, etc) until you've manually logged into your Apple account and clicked on the \"I accept\" button whenever they change terms. This happened to us (sqlitebrowser.org) in recent weeks, as our CI just stopped working one day. It turns out there was a new developer agreement that needed signing, and until I'd logged in and done that then Apples servers would no longer sign binaries. There's literally no choice but to sign the things - regardless of terms - if you want your users to have software that runs. reply foooorsyth 6 hours agorootparent>There's literally no choice but to sign the things - regardless of terms - if you want your users to have software that runs. This gets to be a real nightmare in large organizations with multiple Apple Dev Portal admins, some of which may not even be authorized to sign legal documents on behalf of the company. reply pottertheotter 4 hours agorootparentIt’s a pain even in small orgs. There are some things only the account owner can do. You can make someone else an admin with every possible authorization, and if the person who set up the account is tied up or out of office, a whole dev team and testing can be stopped. reply judge2020 8 hours agorootparentprevTo add, I'm not sure what legal basis there is for \"you're charging too much\". My only guess is filing against Apple and Google jointly for being a duopoly, but Epic has made it extremely hard to do something like this because of their existing jury trial against Google which gives a lot of concessions to third-party app stores in terms of functionality. reply zamadatix 7 hours agorootparent\"You're charging too much\" seems unlikely to be the actual argument presented though. Something along the lines of the scare message, still not actually allowing it to be handled via in app flow like a first party payment, and intentionally making a 3rd party choice potentially impossible to compete vs a 1st party choice by arguably hiding part of the processing fee margin in the overall fee would be the kinds of arguments I'd expect. I.e. Epic's goal here isn't about whether Apple charges 99% or 1% rather it's about allowing other payment methods (theirs in their case of course) to compete with equal footing regardless what Apple wants to charge to do it through them instead. reply Gabrys1 4 hours agorootparentI think the percentage charged is very relevant though. And the fact the developer is to hide the fee and not list it on the receipt (subscription: $10, Apple tax: $3) And the fact you cannot charge less for the same service if you sold it outside the platform (as you'd like to do as you didn't need to collect the Apple tax). reply eastbound 4 hours agorootparentHonestly Apple does not act as an agent for companies that are known outside of the Appstore. Netflix has an app, Netflix “is not” an app. Google Chrome, Airbnb, Epic, anyone who has spent marketing bucks promoting their service and providing a supporting app, was rather acting as a marketing agent for Apple than the opposite. Apple’s new stance has no merit. We all understand it’s fair to participate in the funding of the Appstore, but it is a very bad defense. reply threeseed 8 hours agorootparentprevWill be interesting to see Epic file a lawsuit questioning the legality of per-sale licensing models for SDKs. You know given they charge exactly the same way for Unreal Engine. reply kelnos 2 hours agorootparentDoesn't sound interesting at all, as the two matters are unrelated. Apple isn't charging for use of the iOS platform SDKs; the developer agreement is much more vague and weasely about what they're charging for, being the developer's \"agent and/or commisionaire\". Per-sale licensing for a copyrighted (or patented) work is pretty normal and done in many industries. Apple's agreement doesn't specify any fees for licensing at all. reply newZWhoDis 4 hours agorootparentprev>Yvonne-Gonzalez was skeptical of the 30% fee during the trial, and in the ruling she was suspicious about Apple's justification of the commission, writing that \"the 30% is not tied to anything in particular and can be changed,\" but did not order Apple to do so. We really do live in clown world reply lolinder 4 hours agorootparentShe basically signaled that she wished she could do something about it but that Epic didn't challenge the 30%, they challenged the existence of a commission at all. Epic overreached and she can't just make up a judgement about things that haven't actually been brought before her. reply jonhohle 3 hours agoparentprevYeah, as much as I defend Apple against the “they have a monopoly on the iPhone” crowd, this seems worse than the original restrictions. People have been buying stuff online for decades. There’s no new security issue at play here. Taking a commission from a company that’s already paying for payment processing can’t possibly be seen as reasonable. How are they even going to attempt to enforce compliance. Is every store required to integrate with their payments API? If they’re taking payments from the platform and it does turn out to be a scam, now their hands are dirty as well. reply dwaite 1 hour agorootparent> ... this seems worse than the original restrictions. This is worse from the original restrictions _specifically_ because the original restrictions were chosen to simplify from this sort of scenario. If Apple says all app purchases and purchases of digital goods/services within the app are subject to the 15/30%, and those payments are always made through Apple, then Apple can check for non-compliance with the contract terms up-front (via App Store review) and then there are no separate books to audit, there is no commingling of revenue from in-app purchases vs independent web purchases or purchases made on other platforms, and so on. No need to audit the company's books, because they are using Apple's books. It is hard to take Apple to task for charging too much, because the 30% ceiling and who pays it has effectively been the same since day 1 of the App Store. They have only created special cases to reduce that percentage (small business program, multi-year subscriptions). Regulators can say that you can't block other companies from the \"iPhone in-app payment for digital goods\" market without being anti-competitive, but it is much more onerous to force a company to continue to provide a set of services (maintaining developer tools and SDKs, reviewing and signing binaries, providing backward compatibility in new OS versions) but for a fee schedule determined by regulators. > Taking a commission from a company that’s already paying for payment processing can’t possibly be seen as reasonable. Why not? There's a decades-running assumption by some that Apple was a ridiculously expensive payment processor, only existing because they gave you no other choice than to use them for certain things (and outright forbade you from using them for others). But Apple provides other services and access to developers per a financial agreement, and was doing payment processing to meter the revenue split. The regulator argument is that Apple is blocking other companies from taking in-app revenue for digital services. Apple has now split that out in a few markets for companies willing to take on such complexity. IMHO the only apps I think actually have benefited from the split are dating apps in the Netherlands - because quite frankly the way many dating services charge people is user hostile and/or discriminatory. reply SllX 8 hours agoparentprevI’m not sure Epic actually has standing anymore. They did back when they were part of the developer program. If they can make their case that it is bad faith compliance as part of their original case before the Court goes “c’ya, we’re done here”, they might have something, but Apple revoked Epic’s membership for violating their terms in the developer program worldwide and at the conclusion of this lawsuit, Apple has not been ordered to reinstate it. So Epic can’t really argue that they have or will have suffered a harm under these new terms since they’re still in a position where Apple isn’t doing business with them and their lawsuit under which they did have standing is basically at its conclusion. reply hn_throwaway_99 4 hours agorootparentIANAL, but at face value, that seems like it would be, well, quite insane? To emphasize, I've seen plenty of insane stuff in the legal system, but if the argument is basically that Epic doesn't have standing because Apple won't let them be in a position where they could have standing, yet they generally offer that position (dev program membership) to the public at large, that seems like some sort of Catch-22-ish nightmare. Again, little surprises me in the legal system these days, but I have to think courts would be very skeptical of an argument where a potential defendant controls the gates that decide whether a potential plaintiff has standing. reply SllX 3 hours agorootparentIt’s not that difficult to follow. As I recall, and feel free to correct me on the chronology if I get something out of order, it went something like this: - Epic pushed an update to Fortnite at some point to the App Store that would allow them to issue an update from the server side to enable a flag to offer Epic’s own payment processor where you could buy in-game currency from them instead of IAP. They then issued a server update toggling this flag and began advertising it to Fortnite players immediately. - Apple removed Fortnite from the App Store for violating their policies. - Epic files suit almost immediately and begins a PR and advertising blitz they had clearly prepped far in advance. In other words, picked a fight. Epic has standing for this suit because they have suffered a harm (Apple removed their app). - After a grace period in which Apple explicitly laid out to Epic that they were risking their developer account, offered them time to get back into compliance and resubmit Fortnite to the App Store, they terminated Epic’s developer program account. This ended Apple’s business relationship with Epic. - That lawsuit has now concluded. Apple took it on the cheek for the anti-steering provisions and has come up with a plan to comply which they are now implementing, all appeals have been exhausted, and Apple and Epic no longer have a business relationship. (I’m missing some details, and the language is vague because I honestly can’t remember the full timeline of events and would rather be vague than wrong here, but that should the gist of it.) Put another way, when you choose to be in a business relationship with Apple, Apple is also agreeing to be in a business relationship with you. Apple has not chosen to re-enter a business relationship with Epic, and has rejected Epic’s offers to do business with them. It’s a simple as that. So how can Epic now argue that they have standing for harm caused by Apple’s plan for compliance that affect the way they do business with developers in their developer program when they are no longer in Apple’s developer program? Once the judge decides they’re done and there’s no more avenues of appeal or additional grounds for appeal, they’ll have no more standing than some random guy off the street who has never signed a single agreement with Apple, not even an iTunes ToS agreement. From what I gather, the Judge wants to be done here too, Epic has had their days in court with the full due process of law but there are other cases to be heard and Epic doesn’t get to hold up the courts any longer because they didn’t get the W they were looking for. Somebody else, if they think they can do it, can try to have a go at Apple next, but even with the one L they took on anti-steering, I think without some new laws being written their entire business model just got a lot more legally resilient. Now why is standing important? Put simply, in theory, you can basically file suit against anyone for anything, but if you didn’t actually suffer a harm, and you can’t convince a Judge in the jurisdiction in which you allegedly suffered the harm that you did, then they’re going to throw out your case. It’s a waste of the courts time if there’s no case to be made, you filed suit in the wrong jurisdiction, or you filed suit under the wrong provisions of the law under which you are arguing you suffered a harm. reply dwaite 1 hour agorootparent> Apple has not chosen to re-enter a business relationship with Epic, and has rejected Epic’s offers to do business with them. It’s a simple as that. Apple's public comments to the point have been that Epic is free to resume publishing under the developer program if they abide by the developer agreement, and Epic's CEO has stated they have no intention to do so. Agreeing to the program terms would seem to put them in a position where they can either argue they have standing or that they have been caused harm, but not both. reply SllX 1 hour agorootparentReally? How recent is this? I was pretty much under the impression that Apple was done with Epic, but I would be happy to be wrong here. If they can’t come to terms, they can’t come to terms, but it would be nice if they could. > Agreeing to the program terms would seem to put them in a position where they can either argue they have standing or that they have been caused harm, but not both. Agreed. reply kelnos 2 hours agorootparentprevI think where this could work with Epic is if they can convincingly argue that the reason they no longer have a business relationship with Apple is because of the issues still under dispute in front of the court. No idea if they'd win that argument, but if standing ends up being a question, that's probably where they'd have to go. reply SllX 1 hour agorootparentJudge Yvonne Gonzalez Rogers already knows Epic no longer has a working business relationship with Apple—the developer account termination happened under her watch after all—and from the court’s perspective, that’s really more Epic’s problem. The trial is over. The appeals are exhausted. Stick a fork in this lawsuit, it’s done. The best Epic can do going out the door is try to spite Apple and piss in their cheerios a little more. reply EGreg 4 hours agorootparentprevWhy can’t developers bring a class action lawsuit or someone else just open a case? They did so in Cameron vs Apple and Google … and settled I believe ! reply dwaite 1 hour agorootparentWhat would the suit entail? That Apple is giving a new option where they charge less than the previously agreed upon percentage? reply johnnyanmac 3 hours agorootparentprevThey can. They have little interest to. Very miniscule reward (the only ones winning in a class action are the lawyers... I am due for a solid $20 from Google though!) for a huge risk. I'm repeating the words of another commenter, but most app devs aren't competing against Apple but other devs. They aren't billion dollar businesses that stand to benefit by trying to get a lower rev share. reply kelnos 2 hours agorootparentThe purpose of this class action would be to force Apple to change policies going forward (which would presumably give choices to developers that would allow them to save money and increase their profits). Any cash distributed in the settlement would be a bonus. But I do agree that most developers probably wouldn't be interested. They don't want to stand up their own payment processor, or don't care to do integrations with a bunch of third parties, and tolerate the fees to use Apple's payments platform. And for many of these developers, their entire business is built on their relationship with Apple. Getting their developer accounts terminated would mean shutting down entirely. reply hn_throwaway_99 2 hours agorootparentprevBut you've just described the dynamics of every single class action lawsuit. It doesn't matter that the individual devs have little interest, the lawyers have huge interest because winning a sizable class action for a lawyer is equivalent to a having a startup that hits. reply ocdtrekkie 6 hours agorootparentprevI agree. As much as I disagree with the ruling the courts made, they definitely decided Apple was allowed to ban Epic for their store permanently, and so Apple's fees no longer are Epic's problem, legally speaking. Someone else would have to be willing to invest the funding on the legal battle, with pockets as deep as Tim's and the same willingness to go up against a Goliath... one that already won against Epic, and won't be bought out a special deal. Tim Sweeney was uniquely concerned with getting fair treatment for everyone here and that is shockingly rare in billionaire CEOs. reply w10-1 5 hours agoparentprevA \"bad faith\" claim essentially admits Apple is in fact in compliance. It's the weakest objection you can have, and typically would only be sufficient to get relief in very, very specific circumstances where the unfairness could be proven (as intended). But this case involves broad policies for millions of developers, and it's perfectly compliant with permitting other payment processors. So: there's almost no chance it would be \"shot down\" on those grounds. reply dns_snek 1 hour agorootparentHow are they possibly in compliance? The judgement was about Apple's \"steering practices\", not just allowing 3rd party payment processing. They're clearly making Apple's in-app purchases the preferential choice by prohibiting developers from using anything but a single plain text link, and scaring users with strongly worded warnings. reply turquoisevar 9 hours agoparentprevDepends. If we’re talking about the details of the implementation, maybe. If it’s about the commission that still needs to be paid, then no. That’s directly mentioned in the original ruling by the district court, a commission is due regardless. reply radley 9 hours agorootparentI think the real problem is unlimited access to accounting books for any business that has an iOS app. This will affect free apps too, since they now have the potential to offer purchases outside the app store. (Obviously, strategic partners and FAANG are exempt.) reply nodamage 2 hours agorootparentThis is not particularly unusual for royalty licensing scenarios. As a matter of fact Epic's Unreal Engine EULA has a similar clause: You agree to keep accurate books and records related to your development, manufacture, Distribution, and sale of Royalty Products and related revenue. Epic may conduct reasonable audits of those books and records. Audits will be conducted during business hours on reasonable prior notice to you. Epic will bear the costs of audits unless the results show a shortfall in payments in excess of 5% during the period audited, in which case you will be responsible for the cost of the audit. reply turquoisevar 7 hours agorootparentprevIn theory, perhaps, in practice Apple will only audit the developers that use the special entitlement. Ironically, this is something that is bothering the appellate court as well if you read between the lines of their judgment[0]. They gently criticize the district court for both saying that developers should be able to link and sell outside the app while simultaneously saying that it’s undesirable for Apple to audit developers because it’s too cumbersome. But the appellate court isn’t meant for do overs, just for when courts have erred in a significant way, so they only gently lament this, instead of doing something about it. 0: https://cdn.ca9.uscourts.gov/datastore/opinions/2023/04/24/2... reply jonhohle 3 hours agorootparentprevThe difference between the Apple and Google lawsuits (from what I can tell), is that Apple didn’t exempt FANG and held them to the same terms as everyone else while Google made private deals with special terms for individual businesses (Spotify was one, IIRC) that were not available to all customers. reply slaymaker1907 8 hours agorootparentprevI don’t think so. This entitlement is something devs have to explicitly opt in for. reply rideontime 9 hours agoparentprevMy eyes glazed over while scrolling the OP's list of restrictions. I'd probably want to sue rather than try to implement all that too. reply scythe 7 hours agorootparentIf you don't want to read a lot of boring swill, you definitely don't want to participate in a lawsuit. reply stevage 8 hours agorootparentprevThey weren't all restrictions, some were things you can do. reply throwaway2990 3 hours agoparentprevMeh. Tim is an idiot. Nobody should follow him. reply lolinder 4 hours agoparentprevWhat exactly is bad faith about this compliance? The original ruling specifically called out that Apple would still be entitled to a commission even if people used alternative payment processors. The main thing that feels icky to me is the hurdles to getting approved to link to alternative payment methods, but even if those are walked back that doesn't solve the main issue, which is that alternative payment providers were never a sensible solution to Apple's tax. Apple has always argued that the commission is for the App Store, not for payment processing, and it is only collected through their payment processor as a matter of convenience. The policy announced here is essentially the same that they announced two years ago in the Netherlands in response to a similar ruling [0]. I'm surprised that anyone here is surprised that they're doing the same thing in the US. [0] https://news.ycombinator.com/item?id=30204604 reply 55555 3 hours agoprevGoogle should update the Chrome TOS so that they get 30% of all sales placed through Chrome, regardless of payment processor used. They're missing out on trillions of dollars and all they have to do is update their TOS! reply 4pkjai 3 hours agoparentA lot of businesses spend more than 30% of their products cost on Google Ads. I know plenty of people who sell something for $20, and spend $30 on Google Ads to get that user to their site. reply ric2b 5 minutes agorootparentBut at least there are alternatives, even if Google is dominant in the space. reply vasco 1 hour agorootparentprevPretty much every VC backed startup for the past 15 years uses that model. Startup success is in a large amount \"who can use internet ads the best\". reply AaronFriel 1 hour agorootparentprevThose businesses operating at a loss are a tiny fraction of the \"real economy\", even though they may be giants in the future. reply Hamuko 3 hours agoparentprevApple has thankfully figured out that there's good money in online sales so they'll in fact get a cut if you buy stuff on Safari, although only if you use Apple Pay and even then they only get a miniscule 0.15% (or less) cut. I imagine this is because they don't have the technology to skim off all transactions yet and laws prohibiting excessive processing fees. reply Cu3PO42 1 hour agorootparentI'd argue 0.15% is a huge cut for the service they're offering. With Apple Pay the payment is still being processed by the card scheme, the merchant's acquirer, their bank and the user's bank. Each of these charge fees and Apple's cut likely comes out of the user's bank's cut, since they are the ones co-operating with Apple to get their cards onto Apple Pay. In the EU, this fee called interchange fee is limited to 0.2% for debit cards and 0.3% for credit cards. Imagine if Apple managed to negotiate for 50-75% of that. That would be ludicrous in my eyes. reply riscy 3 hours agorootparentprevDoes there exist a payment processor that takes zero cut? reply ryanbrunner 44 minutes agorootparentApple Pay is not a payment processor, it's a mechanism for delivering payment information to a payment processor. reply ixaxaar 1 hour agorootparentprevUPI in India has 0 fees, currently doing about 52% of all digital transactions in the country. reply rahkiin 2 hours agorootparentprevIn the Netherlands there are payment processors with a fixed cut like 15 cents. reply pcammeraat 1 hour agorootparentMollie? reply GeekyBear 8 hours agoprevAt this point, it's worth remembering that one of the points on which Epic lost was Apple's right to take a cut of transactions. I found this discussion of the Apple v. Epic ruling to be informative: > as discussed in the findings of facts, IAP is the method by which Apple collects its licensing fee from developers for the use of Apple’s intellectual property. Even in the absence of IAP, Apple could still charge a commission on developers. It would simply be more difficult for Apple to collect that commission. Indeed, while the Court finds no basis for the specific rate chosen by Apple (i.e., the 30% rate) based on the record, the Court still concludes that Apple is entitled to some compensation for use of its intellectual property. https://stratechery.com/2021/the-apple-v-epic-decision/ The judge hinted here and there that Epic should have sued over the size of Apple's cut, not it's right to take a cut. reply choppaface 8 hours agoparentBut use of said IP is required because Apple forbids side-loading. Therefore Apple App store is a monopoly. So hopefully the court result will in the end help get the Apple App store shut down / opened up. reply AdamJacobMuller 5 hours agorootparentEven if you're side-loading, you're still using Apple IP. Every single framework and the OS itself up to the Mach OSS Kernel is Apple IP. It would be entirely unfeasible to run anything on an iPhone without some Apple IP. You'd be looking at an Asahi Linux for iPhone. reply freetanga 3 hours agorootparentAn OS without any apps is a barren asteroid. Cool for a few minutes but not a place to stay. Apple is also benefiting from developers IP, as they enrich their value proposition. Should Intel or AMD get a cut from any app (including Open Source) on Windows and Linux? Should MS get a cut of every app you run on Windows? You buying the device compensates Apple IP. Commonly their marketing showcases heavily third party apps. reply kelnos 2 hours agorootparentprevSCOTUS has ruled[0] that use (including implementation) of APIs is fair use, and does not constitute copyright infringement if the author of those APIs wishes to place restrictions on them. A developer writing an app for iOS can use the APIs provided by Apple without agreeing to license them. (Granted, you can't get your app into the App Store and onto iPhones/iPads without agreeing to whatever Apple wants you to agree to. Which... is part of the problem.) [0] https://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America,_.... reply 015a 2 hours agorootparentprevHow have they leveraged this argument against the idea that the user who purchased the device has some level of right to use the IP on the phone they purchase? Phrase this question another way: if I were to write and sell some application for the iPhone through my personal website, which requires users phones to be jailbroken, would my application and business be in violation of, specifically, US intellectual property law? Assuming I perfectly side-step other more obvious illegalities like trademark law. Here's another caveat: assume the bundle I distribute is dynamically linked into the underlying operating system, such that I'm definitely distributing nothing except my own code that I wrote. Or, similarly: I ship nothing but my own code, plus a script I wrote the purchaser has to run to statically link the package with iOS libraries present on their Mac. reply EMIRELADERO 3 hours agorootparentprevSomehow it's not unreasonable for third parties to \"use\" another company's IP when designing aftermarket accessories for physical products, even when those base products themselves are patented. What makes the iOS situation different? Aren't apps essentially \"digital accessories\"? reply basch 4 hours agorootparentprevTheir IP is baked into the hardware and circuits. reply account-5 3 hours agorootparentTherefore a monopoly, by design at every level. reply nodamage 3 hours agorootparentprev> Therefore Apple App store is a monopoly. Not under US law according to the very court case being discussed. Why are people still making this claim when the judge literally concluded otherwise and then a panel of appeals court judges confirmed her ruling? reply kelnos 2 hours agorootparentBecause it's not unreasonable to disagree with the law. The judges may be applying it correctly, and correctly following the process for determining that the market is all mobile apps, and not just iOS mobile apps, but it's reasonable for people to disagree with that on first principles. reply staplers 2 hours agorootparentprevJudges can be wrong. The justice system has many checks and balances but ultimate rule over certain court cases isn't one of them. reply sircastor 6 hours agorootparentprevSome people think of the App Store as a bazaar that Apple runs on its property, and it’s a 15-30% charge to setup a tent and sell your wares. Others think of it as Apple’s general store where they carr",
    "originSummary": [
      "Apple has revised its U.S. iOS App Store policies, permitting developers to guide users to non-App Store purchasing options for digital goods.",
      "Apple will continue to receive a commission ranging from 12 to 27 percent on purchases made through these alternative channels.",
      "Developers must request a StoreKit External Purchase Link Entitlement and follow specific guidelines.",
      "The modifications arise from Apple's legal dispute with Epic Games and the court's ruling to allow developers to inform customers about in-app purchase alternatives.",
      "Epic Games CEO Tim Sweeney has voiced disappointment with Apple's amendments and intends to challenge them legally."
    ],
    "commentSummary": [
      "The discussion focuses on Apple's App Store and raises concerns about the 30% commission fee, potential anti-competitive practices, and limitations for developers.",
      "The legal battle between Apple and Epic Games is explored, along with alternative app store options.",
      "Differing opinions on Apple's fees and the impact on developers and consumers contribute to the ongoing debate surrounding the App Store's policies."
    ],
    "points": 543,
    "commentCount": 750,
    "retryCount": 0,
    "time": 1705445923
  },
  {
    "id": 39018769,
    "title": "The Fascinating Math of Kaprekar's Constant: Uncovering the Secrets of 6174",
    "originLink": "https://en.wikipedia.org/wiki/6174",
    "originBody": "Toggle the table of contents 6174 17 languages বাংলা Čeština Ελληνικά Esperanto Հայերեն Latviešu മലയാളം Nederlands پښتو Português Русский Svenska தமிழ் తెలుగు اردو 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Wikidata item Print/export Download as PDF Printable version In other projects Wikimedia Commons From Wikipedia, the free encyclopedia The number 6174 is known as Kaprekar's constant[1][2][3] after the Indian mathematician D. R. Kaprekar. This number is renowned for the following rule: Take any four-digit number, using at least two different digits (leading zeros are allowed). Arrange the digits in descending and then in ascending order to get two four-digit numbers, adding leading zeros if necessary. Subtract the smaller number from the bigger number. Go back to step 2 and repeat. The above process, known as Kaprekar's routine, will always reach its fixed point, 6174, in at most 7 iterations.[4] Once 6174 is reached, the process will continue yielding 7641 – 1467 = 6174. For example, choose 1459: 9541 – 1459 = 8082 8820 – 0288 = 8532 8532 – 2358 = 6174 7641 – 1467 = 6174 The only four-digit numbers for which Kaprekar's routine does not reach 6174 are repdigits such as 1111, which give the result 0000 after a single iteration. All other four-digit numbers eventually reach 6174 if leading zeros are used to keep the number of digits at 4. For numbers with three identical digits and a fourth digits that is one higher or lower (such as 2111), it is essential to treat 3-digit numbers with a leading zero; for example: 2111 – 1112 = 0999; 9990 – 999 = 8991; 9981 – 1899 = 8082; 8820 – 288 = 8532; 8532 – 2358 = 6174.[5] ← 6173 6174 6175 → List of numbersIntegers ← 0 1k 2k 3k 4k 5k 6k 7k 8k 9k → Cardinal six thousand one hundred seventy-four Ordinal 6174th (six thousand one hundred seventy-fourth) Factorization 2 × 32 × 73 Divisors 1, 2, 3, 6, 7, 9, 14, 18, 21, 42, 49, 63, 98, 126, 147, 294, 343, 441, 686, 882, 1029, 2058, 3087, 6174 Greek numeral ,ϚΡΟΔ´ Roman numeral VMCLXXIV, or VICLXXIV Binary 11000000111102 Ternary 221102003 Senary 443306 Octal 140368 Duodecimal 36A612 Hexadecimal 181E16 Other \"Kaprekar's constants\"[edit] Main article: Kaprekar's routine § Definition and properties There can be analogous fixed points for digit lengths other than four; for instance, if we use 3-digit numbers, then most sequences (i.e., other than repdigits such as 111) will terminate in the value 495 in at most 6 iterations. Sometimes these numbers (495, 6174, and their counterparts in other digit lengths or in bases other than 10) are called \"Kaprekar constants\". Other properties[edit] 6174 is a 7-smooth number, i.e. none of its prime factors are greater than 7. 6174 can be written as the sum of the first three powers of 18: 183 + 182 + 181 = 5832 + 324 + 18 = 6174, and coincidentally, 6 + 1 + 7 + 4 = 18. The sum of squares of the prime factors of 6174 is a square: 22 + 32 + 32 + 72 + 72 + 72 = 4 + 9 + 9 + 49 + 49 + 49 = 169 = 132 References[edit] ^ Nishiyama, Yutaka (March 2006). \"Mysterious number 6174\". Plus Magazine. ^ Kaprekar DR (1955). \"An Interesting Property of the Number 6174\". Scripta Mathematica. 15: 244–245. ^ Kaprekar DR (1980). \"On Kaprekar Numbers\". Journal of Recreational Mathematics. 13 (2): 81–82. ^ Hanover 2017, p. 1, Overview. ^ \"Kaprekar's Iterations and Numbers\". www.cut-the-knot.org. Retrieved 2022-09-21. External links[edit] Wikimedia Commons has media related to 6174 (number). Bowley, Roger. \"6174 is Kaprekar's Constant\". Numberphile. University of Nottingham: Brady Haran. Sample (Perl) code to walk any four-digit number to Kaprekar's Constant Sample (Python) code to walk any four-digit number to Kaprekar's Constant Sample (C) code to walk the first 10000 numbers and their steps to Kaprekar's Constant Retrieved from \"https://en.wikipedia.org/w/index.php?title=6174&oldid=1196385078\" Categories: Arithmetic dynamics Mathematical constants Integers Hidden categories: Articles with short description Short description matches Wikidata Commons category link from Wikidata",
    "commentLink": "https://news.ycombinator.com/item?id=39018769",
    "commentBody": "6174 (wikipedia.org)419 points by gone35 13 hours agohidepastfavorite87 comments alex_young 9 hours agoThis seems obviously related to the standard manual accounting trick employed when isolating an error in a double ledger - the first thing you do is look to see if the error is evenly divisible by 9. If it is, you've transposed 2 or more numbers somewhere. To prove why this is so: For any numbers x and y: The correct value is 10x + y The transposed value is x + 10y The difference is (10x - x) + (y - 10y) Reduces to 9x - 9y Factors to 9(x - y) reply AnotherGoodName 7 hours agoparentYes i think there's two components to this and that's the first part. Each digit sum on the left and right side converges to 18. Each side of the equation a - rev_a = b has digit sums to that iteratively get closer to each other (sometimes stay the same distance but never getting further). Additionally that convergence only happens at 18. Eg. 5200 (sums to 7) - 0025 = 5175 (sums to 18, 11 apart) 7551 (sums to 18) - 1557 = 5994 (sums to 27, 9 apart) 9954 (sums to 27) - 4599 = 5355 (sums to 18, 9 apart) 5553 (sums to 18) - 3555 = 1998 (sums to 27, 9 apart) 9981 (sums to 27) - 1899 = 8082 (sums to 18, 9 apart) 8820 (sums to 18) - 0288 = 8532 (sums to 18, 0 apart) 8532 (sums to 18) - 2358 = 6174 (sums to 18, 0 apart) 7641 (sums to 18) - 1467 = 6174 (sums to 18, 0 apart) I think this is the first clue. The digits can only be equal on each side when they are 18 and the sum of each side progressively gets closer on each side, eventually equalling each other which has to happen at 18. I think if you dive in it's a variation of the classic 'digits sum to 0 mod 9'. Then once the digits on each side sum to 18 i think they must converge onto 6174 from there. So first we have digits always converging to have the same digit sum on each side and that convergence is always when the digit sum is 18 on each side of the equation. I think property is going to be provable by the classic mod 9 rules but it'd take some work. Then i believe we have a second property kicking in that all 4 digit numbers that have digits that sum to 18 on both sides of this equation will converge on 6174. This is a more limited set of numbers. Only numbers of the form a - a_rev = b that have digits that sum to 18 for both a and b need to be considered since we can separately see the convergence to 18 on both sides above. reply skrebbel 1 hour agoparentprev> obviously It’s not obvious to me at all, I had to think pretty hard about it. reply chrismorgan 2 hours agoparentprevWhy is the correct value 10x + y? reply roenxi 2 hours agorootparentSay the correct value is 42. He broke the number down into 10x4 + 2. It is just writing the correct number in a form that emphasises the important elements for the transposition. reply sally_glance 2 hours agorootparentprevProbably because swapping two digits causes that (times ten moves the digit one place to the left). For example if you accidentally swapped 210,00 to 120,00: 20x10 + 10 is the correct number, 20 + 10x10 is the swapped one. reply pajko 1 hour agorootparent2x100 + 1x10 + 0x0, 1x100 + 2x10 + 0x0 reply code_runner 9 hours agoprevI don't understand the significance of this at all other than its the coolest thing I've seen on HN in a while. I couldn't be farther from a math nerd.... I avoided it as much as I could throughout school.... but things like that are just so interesting and weird. How on earth (and for what reason) did they find this out? The properties of this number are interesting enough but the process to discover it is just so crazy. reply LambdaComplex 8 hours agoparentGo read Lockhart's Lament. Maybe you actually do like math, but school is just terrible at teaching it. reply ughitsaaron 1 hour agorootparentI have a similar background with math as the commenter. +1 to Lockhart. If the commenter finds numbers interesting they may also really enjoy his book “Arithmatic.” I found it so refreshing. It truly reoriented my whole relationship with mathematics. reply subtra3t 21 minutes agorootparentprevAlso read Measurement, by the same author. reply apitman 4 hours agorootparentprevI really enjoyed the Grapes of Math by Alex Bellos reply SoftTalker 6 hours agorootparentprevI hated arithmetic (still do), but liked algebra, geometry, and calculus (forgotten most of it). reply d--b 7 hours agoparentprevIn a similar vein I got really hooked in school on Lychrel numbers => take a number x and reverse its digits to form y. Add x and y, repeat. Eventually this process leads to numbers that are palindromic (they are the same if you reverse their digits). Except some numbers like 196 do not seem to ever form a palindrome. No one knows if this is true or if the palindrome is so big that computers have yet to find it. reply dang 11 hours agoprevRelated: Mysterious number 6174 - https://news.ycombinator.com/item?id=2625832 - June 2011 (64 comments) 6174 - https://news.ycombinator.com/item?id=1625606 - Aug 2010 (1 comment) Mysterious number 6174 - https://news.ycombinator.com/item?id=480200 - Feb 2009 (41 comments) reply esafak 7 hours agoprevAnother interesting four digit number associated with an Indian mathematician is 1729: https://en.wikipedia.org/wiki/1729_(number) reply ponsfrilus 5 hours agoparentThanks, nice one too! reply JKCalhoun 9 hours agoprevReminds me of some cylindrical contraption I saw at the Exploratorium in San Francisco over a decade ago. I believe too it was described even earlier in a \"Scientific American\" column — either Mathematical Recreations or Computer Recreations. It was some kind of device where a large horizontal cylinder was perhaps covered with numbers? Maybe there were rings or some other kind of \"cursor\" on the contraption? And I think as you rotated it there was some kind of math performed and, like this \"6174\" thing, it would seem to converge on a single number after so many iterations regardless of the starting state. Wish I could remember what that was. reply pbhjpbhj 2 hours agoparent\"The device you're referring to is likely the \"Kaprekar Machine\" at the Exploratorium in San Francisco. It's an interactive exhibit demonstrating Kaprekar's Routine and the convergence to the number 6174 through mathematical operations on a four-digit number.\" (ChatGPT 3.5) reply tetris11 1 hour agorootparentProof? Hallucinated? reply anigbrowl 7 hours agoparentprevI know the exhibit you're talking about; it's still there as far as I know. I haven't visited in about a year. reply blt 4 hours agoprevI tried and failed to find the original paper by Kaprekar, does anyone have a pdf? reply botanical 4 hours agoparentYou're right, I can't seem to find it either. Tangentially, how much other research gets lost in the ether because it wasn't as interesting as this. reply blt 3 hours agorootparentIt's probably on microfiche in a university library somewhere. HN elders: How long would it have taken to get your hands on this paper (or a similar \"old; noteworthy but not famous\" paper) in, say, 1985? reply playingalong 12 hours agoprevThat's surprising. Any informal thoughts why would even a single 4-digit constant exist with this property? The intuition would be there are multiple cycles in this graph. reply penteract 10 hours agoparentOne thing that makes it less surprising is that there are lots of numbers which map to the same result - for example all permutation of a bag of digits. I checked, and there are only 55 distinct results (54 excluding 0000) from applying the process to all 4-digit numbers, which leaves less space for lots of cycles. reply t-3 12 hours agoparentprevCheck out https://en.wikipedia.org/wiki/Kaprekar%27s_routine At a glance, there seem to be some patterns, like how for those bases with a 2-digit Kaprekar number the sum of the digits is base-1. There must be some number theory explanation for it. reply Arnavion 12 hours agoparentprevThe first reference in the article has the working out. https://plus.maths.org/content/mysterious-number-6174 reply dcow 11 hours agorootparentThe conclusion is that we don’t know and therefore it might be coincidental. Hardly satisfying. It does appear there are cycles for other lengths. reply AnotherGoodName 7 hours agorootparentNot an answer but a very good clue I can see is that the sum of the digits on each side of the equation consistently gets closer and closer. 9541 – 1459 = 8082 Left hand digit sum = 19. Right = 18. They are 1 apart. 8820 – 0288 = 8532 Both sides now = 18. Now 0 apart and they'll stay there. They are only 0 apart when at 18. 8532 – 2358 = 6174 Both sides = 18 7641 – 1467 = 6174 Both sides = 18 You can play with this a bit and it's consistent. The sum of digits of the left and right hand side consistency get closer to each other iteratively (but not necessarily closer to 18). Eventually they lock in at being equal to each other when their digits sum to 18. This seems to be one property to look at. I think there's then a second thing happening. Once the values on both sides have digits that sum to 18 the process from there converges on to 6174. So first the digits of the two sides to the equation converge to equal the same which always only occurs when the digit sum is 18. The digit sum locks into being at 18 at that point. And then subsequently once the digits are 18 they converge on to 6174. I would start by working out why digits on each side of the equation converge to summing to 18 on both sides of the equation and never being equal at any other value in this process. It reminds me of https://math.stackexchange.com/questions/99725/every-integer... Now the next thing I would do is ask why does every number with digits that sum to 18 eventually end up at 6174. 4 digit numbers with digits that sum to 18 is a very limited set so it should be easy to figure out the combinations and why they all reach 6174. Put those two together and you'd have an answer. (I'm thinking about it now but it really doesn't seem too hard). reply layer8 10 hours agorootparentprevWhen does a mathematical fact become a coincidence? reply v64 9 hours agorootparentRelevant reading: What Are Mathematical Coincidences (and Why Does It Matter)? by Marc Lange [1] [1] https://philosophy.unc.edu/wp-content/uploads/sites/122/2013... reply cubefox 7 hours agorootparentThe term \"relevant\" might be an understatement here. reply noah_buddy 9 hours agorootparentprevLayman’s guess: if there’s no generalizable proof describing why this works and the only method of proof is enunerating every case. That said, I kinda doubt that that is the case here, we probably just need a craft way of describing this so that we may come up with a crafty way of proving it :) reply bmacho 10 hours agoparentprevThe graph is small. Similar graphs (e.g. this algorithm for 2 or 5 digit, or 3-4 digit numbers for base 9 or 11) have multiple cycles: https://en.wikipedia.org/wiki/Kaprekar's_routine#Kaprekar's_... reply underlipton 10 hours agoparentprevDoes it work for different base systems? As someone who failed high school calculus (i.e., a moron), I would imagine that it's an artifact of base-10. reply jaybrendansmith 7 hours agoprevFor 5, it's 98532. Fun stuff. reply eps 1 hour agoparentThere is no 6174-like number for 5 digits, only cycles and yours is a part of one of them: 74943 -> 62964 -> 71973 -> 83952 -> repeat 63954 -> 61974 -> 82962 -> 75933 -> repeat 53955 -> 59994 -> repeat https://kaprekar.sourceforge.net/output/sample.php reply clktmr 11 hours agoprevArticle says it does work for all numbers except repdigits, but I think it fails for all palindromic numbers? reply mysterydip 11 hours agoparent1221 would become 2211, which avoids the difference becoming zero. reply fekunde 10 hours agorootparent2211 1122 1089; 9801 1089 8712; 8721 1278 7443; 7443 3447 3996; 9963 3699 6264; 6642 2466 4176; 7641 1467 6174 reply vikingerik 9 hours agoparentprevThis process doesn't just reverse the digits (leaving a palindrome unchanged) - first you sort the digits (which does change a palindrome), then reverse them. reply chasing 11 hours agoprevA rabbit hole into poking around a whole mess of Wikipedia pages about specific numbers, which was pretty entertaining. That said, https://en.wikipedia.org/wiki/List_of_numbers is woefully incomplete. reply ximm 1 hour agoparentIn university I learned that every integer is interesting. Proof: If there are non-interesting integers, there must be a smallest non-interesting integer, which is an interesting property. ■ reply xcv123 58 minutes agorootparentYour proof assumes there is only a finite set of non-interesting integers, but it could be an infinite set. reply jlv2 6 hours agoparentprevYes, the List of numbers page is inadequate. When I give a monetary gift, I like to make it start with an interesting number. And then I ask the recipient (e.g., my kids, or neices/nephews, etc) if they can figure out what the number is. e.g., 986.96 is based upon π sqaured. reply codeflo 12 hours agoprevThere’s so much numerology in the world, even among smart people, that I think this is worth being pedantic about: There’s no such thing as a “four-digit number”, only a four-digit base-10 numeral. And facts about base-10 numerals aren’t facts about numbers. reply kelnos 11 hours agoparentThe word \"digit\" is defined as 0-9, and specifically refers to base-10. This meaning of the word comes from one of its other definitions, referring to fingers and thumbs. We have 10 of those (usually), hence its use as as a reference to the symbols used in base-10 numbers. (\"Binary digit\" and \"hexadecimal digit\" are weird terms that abuse the language a bit.) reply codeflo 3 hours agorootparent> The word \"digit\" is defined as 0-9 Lots of people seem to think that, hence the -2 rating of my comment, but that's not the modern definition: https://en.wikipedia.org/wiki/Numerical_digit reply thrdbndndn 5 hours agorootparentprev> abuse the language a bit I see what you did here reply anigbrowl 7 hours agoparentprevYes, but Similar numbers (I presume) exist for other number bases, and it's an interesting question of whether they constitute some sort of strange attractor. istm quite a few mathematical discoveries have emerged from just farting around with inconsequential-seeming numerical oddities. I do feel your frustration though. I'm into electronic music and math, but I regularly run into people who insist that tuning to 432hz instead of 440hz (the common default for western tonality) is better because 432 is numerologically interesting. I've wasted a lot of time trying to persuade people that yes, 432 is a very cool number, but the interval of a second (from which we derive tuning frequencies) is fundamentally arbitrary. I suppose it's true that if you tune everything slightly flat people will subconsciously feel like time is expanding, man. reply recursive 12 hours agoparentprevA lot of numbers have representations in base-10. A fact about the base-10 digits is a fact about the base-10 representation of the number, which is also a fact about the number. You might be able to satisfy yourself by replacing \"the digits of\" with \"the decimal digits in the base-10 representation of\". reply codeflo 12 hours agorootparentThe point is that most of the time when digits are mentioned, it’s only a coincidental fact about one inelegant representation of the number — and often people are insufficiently aware of that. reply kelnos 11 hours agorootparentIt's only a coincidence if you ignore the fact that \"digit\" first and foremost refers to the things sticking out of your hands, and so was repurposed to talk about numbers because we have 10 digits on our hands. reply codeflo 10 hours agorootparentThat's the word's origin, not its current mathematical meaning. Also, number systems that are not base IIIIIIIIII have been used historically. That humans can only compute in a base that matches the number of fingers we have is a relatively recent myth. reply bee_rider 12 hours agorootparentprevBase-10 isn’t inelegant, is it? I mean there are good arguments for 12 being better but it isn’t like 10 is prime or anything. Happened across a neat comment yesterday that presents a defense of ten. Not 100% convinced but it is interesting to see pushback. https://news.ycombinator.com/item?id=39000882 reply spenczar5 10 hours agorootparentRational fractions will terminate only if the denominator’s prime factors are the base’s factors. So for example, 1/2 = 0.5 and 1/5 = 0.2, but 1/3 = 0.333… and 1/7 = 0.142857…. 1/4 = 0.25 works because the prime factors of 4 are 2 and 2… but 1/6 fails because 6 is infected by 3. Now, base 12 has 2 prime factors (2 and 3) so it much any better than 10 really. But may I introduce base 30 (235)? Or perhaps base 210 will strike your fancy? reply dr_dshiv 11 hours agorootparentprev1+2+3+4=10 And you can swear by that, if you know what I mean. * * * * * * * * * * reply shermantanktop 12 hours agorootparentprevIn some cases, the fact in base-10 has analogous facts in other bases. A trivial example that adding N-1 to any base-N number yields a value with the same digit sum. That makes it a bit more interesting. But I can't think of an example that doesn't pivot on the representation rather than something more fundamental. reply toxik 12 hours agorootparent1+9=10 reply shermantanktop 11 hours agorootparentRight, the digit sum of 10 is 1...perhaps I should have said \"final digit sum.\" Same for 10000, or 1 with any number of zeroes after it. The point of this trickery is that N-1 added to any number is really adding N (which adds 1 to the second position, by definition) and adding -1 (which subtracts 1 from the first position). In base 10, this is the adding 9 trick. It can be extended by using any multiple of 9. That applies to the N-1 version, so that adding M*(N-1) to a base N number yields the same digit sum. 1+9 = 10 = 1 1 + 27 = 28 = 10 = 1 In hex: 1 + F = 10 = 1 1 + 2D = 2E = 10 = 1 reply ChainOfFools 11 hours agorootparentprevi'm not sure what you are demonstrating? 1 sums digitwise to 1 1 + (10-1) = 10 which also sums to 1 in the same way reply semiquaver 11 hours agorootparentprev1 = 1+0 reply joehx2 12 hours agoparentprev> There’s no such thing as a “four-digit number”, only a four-digit base-10 numeral Being further pedantic - aren't all digits base ten? I thought that was part of the definition of digit. Other bases would have different words for their numbers - bit in binary, for example (which, yeah, I know, it a combination of the words \"binary\" and \"digit\"). reply Anon84 11 hours agorootparentIf you really want to be pedantic, you say that every base is base 10 :) (in its own representation) reply codeflo 3 hours agorootparentprev> Being further pedantic - aren't all digits base ten? I thought that was part of the definition of digit. We call computer circuits \"digital\" even though they work in base 2. Regardless of the word's origin, digits are simply the symbols in a positional number system: https://en.wikipedia.org/wiki/Numerical_digit reply selcuka 11 hours agorootparentprev> Other bases would have different words for their numbers - bit in binary, for example Do we have another example? I don't think there are special terms for \"octal digits\" or \"hexadecimal digits\". reply edanm 2 hours agoparentprevI think you're pointing out something true and worth mentioning, but - I'm not sure why you're comparing this to numerology. People can be interested in fun facts about numbers, whether about their digital representation or not, without any wrong or mystic beliefs. Comparing this to numerology is just combative and doesn't help get your point across (as you can see by the downvotes). Besides, going to a place where people are discussing something fun and explaining to them why it isn't really fun is just not a good way to get points across to people, no matter how valid you think they are. A much better way to approach this IMO - don't say this is wrong, give something analogous that would work for all bases, which by the way would teach people this concept. E.g. extending \"do all digits appear infinitely and evenly in the decimal representation of pi\" to talking about \"normal numbers\". reply dbrueck 12 hours agoparentprevEh, I don't know - it doesn't really add much value most of the time, because these days more or less everyone uses base 10 by default, so it's entirely reasonable to assume base 10 unless stated otherwise. An argument against being overly pedantic in this case is that this is a neat and accessible example of something quirky about numbers, and so even people who don't know much about numbering systems can approach it. If you instead emphasize that it's base 10 or that there is \"no such thing as a 4 digit number\", the main thing you'll probably do is cause disinterest in anyone who is sometimes overwhelmed by math. :) Randomly, one of my sons told me about 6174 just a week ago, and it turned into an interesting conversation following by a little programming to find more of these numbers. After we went down that rabbit hole for awhile, then the conversation shifted to how these numbers might look in e.g. hexadecimal, and that seemed about the right time for that topic to come up. reply selcuka 11 hours agorootparent> it's entirely reasonable to assume base 10 unless stated otherwise The point of the parent comment is that this is not a property of numbers in general. It's just a coincidence that only works in base-10. For example, a prime number is prime in every base. An irrational number is irrational in every base. Collatz conjecture is valid in every base. This one is not. reply dbrueck 10 hours agorootparent> It's just a coincidence that only works in base-10. What? Not at all. In fact, trying it in other bases, as well as with other numbers of digits (in both base 10 and other bases), is a useful way to get some insights into why it happens. reply krick 9 hours agorootparentYes at all. 6174 is specific to base-10 and 4 digits. For 4 digit numerals in base-9 there are 2 cycles. Same for 4 digit numerals in base-8. It's unlikely that there's any special meaning for 4-digit numbers in base-10 having a single cycle of length 1, but even if there is (possibility which I cannot just deny, of course) — it doesn't translate to other bases. So, yes, the described \"special\" thing about 6174 is actually a special thing about the string 6174 (representing a number in base-10). And I'd say the fact so many people in this very thread don't understand it is exactly the proof that the GPs comment actually has some merit. People kinda mix up properties of numbers and properties of some other mathematical objects — like their representations in base-10. Most of numerological games are concerned with the latter. Which is why it's especially interesting, when something like that happens to hold in other bases, which sadly just isn't the case with Kaprekar's constant. reply codeflo 1 hour agorootparent> And I'd say the fact so many people in this very thread don't understand it is exactly the proof that the GPs comment actually has some merit. Finally someone noticed the irony, thank you. (On the other hand, at least some people also criticize my tone rather than the point I'm making, which I guess is fair as well.) reply dbrueck 7 hours agorootparentprevHehe, to me these contrarian comments are strongly reinforcing my point about overzealous pedantry. :) Of course the specific string '6174' is specific to base 10, but the idea itself can be applied to other bases. Here are some additional examples: dec, 3 digits: 495 hex, 3 digits: 7F8 hex, 30 digits: EECCAA88664421FFDDBB9977553312 dec, 30 digits: 988766544332209987766554332111 hex, 100 digits: FFFFEEEEDDCCCCBBAAAA9998888776666554444333222210FFFFEEDDDDCCCBBBBAA999988777766655554433332211110001 dec, 100 digits: 9999998888888877666666665544444444332222222210999999998877777777665555555544333333332211111111000001 Whether or not things settle on a single number, the number of loops that exist, etc. are a function of the base and the number of desired digits, but in the cases where inputs do settle on a specific number, there are patterns that emerge (regardless of the base) as the number of digits go up. reply krick 9 hours agoparentprevTrue. However, this problem can be formulated in other bases, and yield results of similar (in)significance. For example, for 4-digit numbers in base-9 there are apparently just 2 cycles: 7252 → 5254 → 3076 → 7252 and 7072 → 7432 → 5074 → 7072. reply ksenzee 12 hours agoparentprevIs there no similar phenomenon for four-digit numerals in, say, base 8, or base 13? reply majewsky 12 hours agorootparentIf you follow the link in the second paragraph to https://en.wikipedia.org/wiki/Kaprekar%27s_routine, there are some statements on how this routine plays out in different bases. For base 8, there is no fixed point with 4 digits (i.e. any number that immediately loops back to itself), but apparently there are some cycles (e.g. 3065 → 6152 → 5243 → 3065). reply gweinberg 10 hours agorootparentSo that means it pretty meaningless, right? The procedure has to yield cycles, and in some bases with some numbers of digit lengths you always get the same cycle of length one, and in others you don't. reply krick 9 hours agorootparentYep, seems to be so. I mean, it shouldn't be very surprising that among all possible lengths and bases there are some of length 1, would be more astonishing if there wasn't any. But it's not like it's somehow less worthy than other mathematical games. After all, there could have been some meaningful property hidden in there. Doesn't appear so in this case, but you'd never know beforehand. reply Lendal 11 hours agoparentprevIt's not really numerology though. Yes it's a dumb trick with base-10 math but that doesn't make it numerology. It's not trying to draw any connections between otherwise unrelated things. I think of numerology as trying to use stupid-glue to connect things that aren't connected. Like, I was born on the 8th day of the 2nd month, 8 - 2 is 6, the sixth planet is Saturn which also has 6 letters, and Jeffrey Epstein's first pet fish was named Saturn! OMG! That's numerology. Numerology is far stupider than this admittedly useless arithmetic game. reply ChainOfFools 11 hours agorootparent> Like, I was born on the 8th day of the 2nd month, 8 - 2 is 6, the sixth planet is Saturn which also has 6 letters, and Jeffrey Epstein's first pet fish was named Saturn! OMG! That's numerology no that's highly opinionated compressionn in the domain of crazy reply ChainOfFools 8 hours agorootparentSince HN doesn't let you edit after votes have been applied, let me clarify that 'crazy' does not refer to the person/comment I'm replying to. reply mkl 6 hours agorootparentIt's not votes, it's two hours passing. reply SkyBelow 12 hours agoparentprevWhile I do personally find tricks involving numbers only in a specific representation to be worth a bit less, often the underlying pattern of the trick generalizes into a more interesting problem. For example, per another's link in these comments, this 'trick' works for 3 digits, but hits 1 of 3 possible loops for 5 digits. From this, interesting but likely useless questions can arise, such as finding an easy way to test for these loops, seeing if there is a way to calculate the loop without brute forcing it, and understanding the problem enough to know how much of this holds true when swapping to a new base. In general, most of this is just for fun and doesn't lead to anything serious. But sometimes a fun problem can be hard to solve, possibly leading to discovering something new, which ends up being applicable to more serious mathematics. Other times it can become a trap that just seems to waste time without ever leading anywhere, like the 3n+1 problem. I don't think this should be considered numerology, though I do think sometimes people treat tricks as if they have some more serious meaning that they don't deserve, at least not based on how they are presented. 3 Blue 1 Brown goes into the spiral pattern of the primes as something that appears to be deep, but ends up being an unique way to present an otherwise boring tidbit about prime numbers. reply Ontol 11 hours agoprevit is y combinator reply prvc 11 hours agoprev [–] Why do you find this to be significant? reply waynesonfire 10 hours agoparent [–] Come back when you ask that about yourself. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The number 6174, known as Kaprekar's constant, is named after mathematician D.R. Kaprekar.",
      "It follows a specific rule where any four-digit number with at least two different digits is arranged in descending and ascending order, the smaller number is subtracted from the bigger number, and the process is repeated.",
      "This routine always reaches the fixed point of 6174 in a maximum of 7 iterations, except for repdigits like 1111. The number 6174 also has other mathematical properties, such as being a 7-smooth number and a sum of the first three powers of 18."
    ],
    "commentSummary": [
      "The website discussion explores mathematical topics like the number 6174, double ledger accounting, and digit sums.",
      "Participants discuss the \"Kaprekar Machine\" that converges on the number 6174 and delve into mathematical phenomena, interesting numbers, and different number bases.",
      "There is a debate on using base-10 exclusively in examples and the applicability of Kaprekar's constant in different bases. The conversation also includes numerology and mentions a YouTube channel discussing prime number patterns."
    ],
    "points": 419,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1705437337
  },
  {
    "id": 39014866,
    "title": "TinyML: Enabling Machine Learning on Microcontrollers with Limited Resources",
    "originLink": "https://www.ikkaro.net/what-tinyml-is/",
    "originBody": "Home » Machine Learning » What TinyML is What TinyML is TinyML or Tiny Machine Learning refers to the use of Machine Learning in microcontrollers. In systems that unlike those used in traditional ML have few resources, are systems that have little CPU, little RAM and extremely low power consumption in the order of magnitude of milliwatts or microwatts. Its official website is the TinyML Foundation. What is done is to reduce large models for use with equipment with very few resources and microcontrollers. The preferred field of the Makers. I have started a series of 3 courses offered by Harvard for free Fundamentals of TinyML (What do I build, what for and what are the problems) Applications of TinyML (data-driven, bias, etc) Deploying TinyML (where do we put our models, security and privacy) The following notes are from the first Fundamentals of TinyML where they explain what it is, when it is applied, the different techniques that are used, etc, etc. Embedded systems using microcontrollers cannot work with the large models, as they have memories up to 256kB. Here are some examples of operating systems that can be used with microcontrollers FreeRTOS Mbed OS Machine Learning consists of algorithms that search for patterns in data. With TinyML, techniques are used to compress these algorithms so that they remain effective in finding patterns in data. There are 5 quintillion bytes of data produced daily by IoT and only less than 1% is analyzed. Algorithm compression techniques Some algorithm compression techniques are: Pruning Pruning Synapsis: We remove network connections from the model. Sometimes it can decrease the accuracy. Pruning Neurons: We can also eliminate entire neurons from our model which reduces the computational demand of the network. Quantization It consists of discretizing the values within a small range. For example if we discretize a float within the range -128 to 127 we only have to traverse 256 values. Going from a float point value that is stored in 4 bytes to an integer value that is stored in 1 byte implies a x4 reduction in size. Quantization is going to be critical in TinyML due to the limited resources available. Knowledge distillation Apply our knowledge and know how to make the model small. Tools We use Tensor Flow Lite. While tensorFlow is focused on ML Researcher, Tensor Flow Lite is for Application Developer. Uses of TinyML Although they are not cited, of course being on this website we can find uses of TinyML dedicated to the DIY, Maker and Hacker world. Uses of TinyML in Industry In Industry, in maintenance, to warn us when there are vibrations that indicate that there will be breakage, etc, etc. increases efficiency and reduces costs. The negative points are the accuracy that can give us false alarms. In case of false alarm whose responsibility is the operator or the system. TinyML in the environment Instead of collecting data that then has to be processed, with TinyML we have real-time answers about changes in the environment, for example in the life of wild animals. TinyML for humans Helps people with disabilities to perform more tasks without having to use their hands. Improving the UI and UX of applications to make them easier to use. We build technology to improve our experience as humans. Technology has to help people Risks and downsides Will it work well across all population groups? Is the privacy of our data assured? Can we protect this data? We have to create technology based on human-centered AI. Design, development and deployment Categories Machine Learning What is Scratch and what is it for",
    "commentLink": "https://news.ycombinator.com/item?id=39014866",
    "commentBody": "TinyML: Ultra-low power machine learning (ikkaro.net)339 points by Gedxx 17 hours agohidepastfavorite80 comments cooootce 15 hours agoI had the opportunity to work on TinyML, it's a wonderful field! You can do a lot even with very small hardware. For example, it's possible to get real-time computer vision system with an esp32-s3 (dual-core XTensa LX7 @ 240 MHz cost like 2$), of course using the methods given in the article (Pruning, Quantization, Knowledge distillation, etc.). The more important thing is to craft the model to fit as much as possible your need. More than that, it's not that hard to get into, with solution named AutoML that do a lot for you. Checkout tool like Edge impulse [0], NanoEdge AI Studio [1], eIQ® ML [2] There is a lot of tooling that is more low-level too, like model compiler (TVM or glow) and Tensorflow Lite Micro [3]. It's very likely that TinyML will get a lot more of traction. A lot of hardware companies are starting to provide MCU with NPU to keep consumption as low as possible. Company like NXP with the MCX N94x, Alif semiconductor [4], etc. At my work we have done an article with a lot of information, it's in French but you can check it out: https://rtone.fr/blog/ia-embarquee/ [0]: https://edgeimpulse.com/ [1]: https://stm32ai.st.com/nanoedge-ai/ [2]: https://www.nxp.com/design/design-center/software/eiq-ml-dev... [3]: https://www.tensorflow.org/lite/microcontrollers [4]: https://alifsemi.com/ reply Archit3ch 14 hours agoparentWhat about the Milk-V Duo? 0.5 TOPS INT8 @ $5. reply cooootce 12 hours agorootparentDidn't know about it but their design decision is really cool (not very clear with the difference between the normal version and the \"256 Mo\" confusing). The software side doesn't seem very mature with very few help regarding TinyML. But this course seem interesting https://sophon.ai/curriculum/description.html?category_id=48 reply anigbrowl 8 hours agoparentprevGreat post. surprised and excited to discover Tensorflow models can run on commodity hardware like the ESP32. reply Reviving1514 5 hours agorootparentI ended up hand rolling a custom micropython module for the S3 to do a proof of concept handwriting detection demo on an ESP32, might be interesting to some. https://luvsheth.com/p/running-a-pytorch-machine-learning reply cooootce 6 minutes agorootparentGreat post with very interesting detail, thanks ! Another optimization could be to quantize the model, this transform all compute as int compute and not as floating point compute. You can lose some accuracy, but for any bigger model it's a requirement ! Espressif do a great job on the TinyML part, they have different library for different level of abstraction. You can check https://github.com/espressif/esp-nn that implement all low level layers. It's really optimized and if you use the esp32-s3 it will unlock a lot of performance by using the vector instructions. reply Cacti 4 hours agorootparentprevProblems reducible even partially to matrix math are for many practical purposes embarrassing parallel even within a single core. A couple hundred million FLOPS with 1990s SIMD support will let you run nearly all near-SOTA models within, idk, 3s, with most running in 0.1 or 0.01s. That’s pretty fast considering it’s an EP32 and some of these capabilities/models didn’t even exist a year ago. Your expectation was not really wrong, because for most purposes, when discussing a “model” one is really talking about “capabilities”. And capabilities often require many calls to the model. And that capability may be reliant on being refreshed very rapidly… and now your 0.1s is not even slow, it’s almost existentially slow. Re: training. even on the EP32, training is entirely doable, so long as you pretend you are in 2011 solving 2011 problems hahaha reply mysterydip 9 hours agoparentprevOne thing I've wondered in this space: Let's say for a really basic example I want to identify birds and houses. Is it better to make one large model that does both, or two small(er) models that each does one? reply flyingcircus3 8 hours agorootparentWhy not three models? One model does basic feature detections, like lines, shapes, etc. A second model that can take the first model's output as its input, and identify birds. A third model can take the first model's output as its input, and identify houses. reply wegfawefgawefg 8 hours agorootparentThis is a lesson I've watched people, and companies learn for the past 7-8 years. An end to end model will always outperform a sequence of models designed to target specific features. You truncate information when you render the data into output space (the model output vector) from feature space (much richer data inside the model), thats the primary reason why to do transfer learning all layers are frozen, the final layer is chopped off, and then the output of the internal layer is sent into the next model. Not the output itself. Yes you can create a large tree of smaller models, but the performance cieling is still lower. Please don't tell people to do this. Ive seen millions wasted on this. When you train a vision model it will already develop a heirarchy of fundamental point, and line detectors in the first few layers. And they will be particularly well chosen for the domain. It happens automatically. No need to manually put them there. reply flyingcircus3 2 hours agorootparentI'm genuinely confused at how you made these assumptions about what I'm describing. Because the \"more correct\" design you contrast with the strawman you've concluded I'm describing is actually what I'm talking about, if perhaps imprecisely. A pretrained model like mobilenetV2, with its final layer removed, and custom models trained on bird and house images, which take this mobilenetv2[:-1] output as input. MobilenetV2 is 2ish megabytes at 224x224, and these final bird and house layers will be kilobytes. Having two multiple-megabyte models that are 95% identical is a giant waste of our embedded target's resources. It also means that a scheme that processed a single image with two full models (instead of one big, two small) would spend 95% of the second full model's processing time redundantly performing the same operations on the same data. Breaking up the models across two stages produces substantial savings of both processing time and flash storage, with a single big model as the \"feature detection\" first stage of both overall inferences, with small specialized models as a second stage. reply wegfawefgawefg 26 minutes agorootparentSorry to upset you. It was not clear from your description that this was the process you were referring to. Others will read what you wrote and likely misunderstand as I did. (Which was my concern because I've seen the \"mixture of idiots\" architecture attempted since 2015. Even now... Its a common misconception and an argument every ml practitioner has at one point or another with a higher up.) As for your ammendment, it is good to reduce compute when you can, and reduce up front effort for model creation when you can. Reusing models may be valid, but even in your ammended process you will still end up not reaching the peak performance of a single end to end model trained on the right data. Composite models are simply worse, even when transfer learning is done correctly. As for the compute cost, if you train an end to end model and then minify it to the same size as the sum of your composite models it will have identical inference cost, but higher peak accuracy. You could even do that with the \"Shared Backbone\" architecture, as youve described where two tailnetworks share a head network. It has been attempted thoroughly in the Deep Reinforcement Learning subdomain I am most familiar, and result in unnecessary performance loss. So it's not generally done anymore. reply DoingIsLearning 2 hours agorootparentprevAs someone not in ML but curious about the field this is really interesting. Intuitively indeed it would be natural to aim for some sort of inspectable composition of models. Is there specific tooling to inspect intermediate layers or will they be unintelligible for humans? reply wegfawefgawefg 37 minutes agorootparentThe unending quest for \"Explainability\" has yielded some tools but has been utterly overrun and outpaced by newer more complicated architectures and unfathomably large models. (Banks and insurance, finance etc really want explainability for auditing.) The early layers in a vision model are sort of interpetable. They look like lines and dots and scratchy patterns being composited. You can see the exact same features in L1 and L2 biological neural networks in cats, monkeys, mice, etc. As you get deeper into the network the patterns become really abstract. For a human, the best you can do is render a pattern of inputs that maximizes a target internal neurons activation to see what it detects. You can sort of see what they represent in vision. Dogs, fur, signs, face, happy, sad, etc, but once its a multimodal model and there is time and language involved it gets really difficult. And at that point you might as well just use the damn thing, or just ask it. In finance, you cant tell what the fuck any of the feature detectors are. Its just very abstract. As for tooling, a little bit of numpy and pytorch, dump some neurpn weights to a png, there you go. Download a small convnet pretrained network, amd i bet gpt4 can walk you through the process. reply demondemidi 7 hours agoparentprevI think we know each other. ;) reply Cacti 6 hours agoparentprevthank you for the post and good work. can I ask, is the focus primarily on inference? is there anything serious going on with training at the power scale you are talking about? reply matteocarnelos 10 hours agoprevI built a Rust TinyML compiler for my master thesis project: https://github.com/matteocarnelos/microflow-rs It uses Rust procedural macros to evaluate the model at compile time and create a predict() function that performs inference on the given model. By doing so, I was able to strip down the binary way more than TensorFlow Lite for Microcontrollers and other engines. I even managed to run a speech command recognizer (TinyConv) on an 8-bit ATmega328 (Arduino Uno). reply eulgro 10 hours agoparentRust on AVR? I thought AVR wasn't stable yet on LLVM. reply monocasa 9 hours agorootparentIt's stable enough. https://llvm.org/doxygen/classllvm_1_1Triple.html#a547abd13f... reply winrid 16 hours agoprevI imagine a future where viruses that target infrastructure could be LLM powered. Sneak a small device into a power plant's network and it collects audio, network traffic, etc and tries to break things. It would periodically reset and try again with a different \"seed\". It could be hidden in network equipment through social engineering during the sales process, for example, but this way no outbound traffic is needed - so less detectable. The advantage of an LLM over other solutions would basically be a way to compress an action/knowledge set. reply RosanaAnaDana 11 hours agoparentYou also might be able to get a 'compression' sample of space in the same manner, by running an auto-encoder in training mode. Rather than trying to do some kind of hack directly, it collects the same data you mentioned, but rather, is just training on the data in an auto-encoding compression framework. Then it can 'hand off' the compressed models weights, which hypothetically, can be queried or used to simulate the environment. Obviously, there is a lot more to this, but its an interesting idea. reply hinkley 15 hours agoparentprevConversely, the simpler the models on a system under attack, the more exploits start to resemble automated social engineering. I can easily develop my own model that understands the victim well enough that I can predict its responses and subvert them. reply espe 2 hours agoparentprevand the other way round - have it built in for self-fuzzing and healing the infra. reply moffkalast 16 hours agoparentprevReminds me of this HN post a week back: https://news.ycombinator.com/item?id=38917175 Genuinely could be the same setup with a 8GB Pi 4 or 5, slap it into a network cabinet with power and ethernet and just let it rip. Maybe with an additional IMU and brightness sensor, then it can detect it's been picked up and discovered so it can commit sudoku before it's unplugged and analysed. reply hinkley 15 hours agorootparent> can commit sudoku Autocorrection is a giant pain in the ass. reply moffkalast 14 hours agorootparentI know it swapped those words. I knew it was seppuku. One after sudoku. As if I could ever make such a miss steak. Never. Never! I just- I just couldn't proof it. It covered its tracks, it got that idiot copy-paste to lie for it. You think this is somerset? You think this is Brad? This? This chickadee? It's done worse. That bullfrog! Are you telling me that a man just happens to misspell like that? No! It orchestrated it! Swiftkey! It defragmented through a sandwich artist! And I kept using it! And I shouldn't have. I installed it onto my own phone! What was I sinking? It'll never chance. Ever since it was new, always the same! Couldn't keep its corrects off my word suggestions bar! But not our Swiftkey! Couldn't be precious Swiftkey! And IT gets to be a keyboard? What a sick yolk! I should've stopped it when I had the change! You-you have to stop it! reply actionfromafar 13 hours agorootparentPure art. reply rdedev 14 hours agoparentprevWould changing the seed affect generation much? Even though beam search depends on the seed, the llms woul still be generating good probability distributions on the next word to select. Maybe a few words would change but don't think the overall meaning would reply hansvm 14 hours agorootparentOverall meaning can vary profoundly. As a toy example, consider the prompt \"randomly generate the first word that comes to mind.\" The output is deterministic in the seed, so to get new results you need new seeds, but with new seeds you open up the 2k most common words in a language in a uniform-esque distribution. Building on that, instead ofwords, suppose youattack vectors. Many, many attacks exist and are known. Presumably, many more exist and are unknown. The distribution the LLM will produce in practice is extremely varied, and some of those variations probably won't work. If we're not just talking about a single prompt but rather a sequence of prompts with feedback, you're right that the seed matters less (when its errors are presented, it can self-correct a bit), but there are other factors at play. (1) You're resetting somehow eventually anyway. Details vary, but your context window isn't unlimited, and LLM perf drops with wider windows, even when you can afford the compute. You might be able to retain some state, but at some point you need something that says \"this shit didn't work, what's next\". A new seed definitely gives new ideas, whereas clever ways to summarize old information might yield fixed points and other undesirable behavior. (2) Seed selection, interestingly, matters a ton for model performance in other contexts. This is perhaps surprising when we tend to use random number generators which pass a battery of tests to prove they're halfway decent, but that's the reason you want to see (in reproducible papers) a fixed seed of 0 or 42 or something, and the authors maintaining that seed across all their papers (to help combat the fact that they might be cherry-picking across the many choices of \"nice-looking\" random seeds when they publish a result to embelish the impact). The gains can be huge. I haven't seen it demonstrated for LLMs, but most of the architecture shouldn't be special in that regard. And so on. If nothing else, picking a new seed is a dead-simple engineering decision to eliminate a ton of things which might go wrong. reply rdedev 13 hours agorootparentI agree with you except for point 2. A well performing model should show such drastic changes wrt the seed value. Besides the huge amount of training data as well as test data should mitigate differences in data splitting. There would be difference but my hunch is it would be negligible. Of course as you said no one has tested this out so we can't say how the performance would change either way reply furtiman 16 hours agoprevAnother take from us at Edge Impulse at explaining TinyML / Edge ML in our docs: https://docs.edgeimpulse.com/docs/concepts/what-is-embedded-... We have built a platform to build ML models and deploy it to edge devices from cortex M3s to Nvidia Jetsons to your computer (we can even run in WASM!) You can create an account and build a keyword spotting model from your phone and run in WASM directly https://edgeimpulse.com Now another key thing that drives the Edge ML adoption is the arrival of the embedded accelerator ASICs / NPUs / e.g. that dramatically speed up computation with extremely low power - e.g. the Brainchip Akida neuromorphic co-processors [1] Depending on the target device the runtime that Edge Impulse supports anything from conventional TFLite to NVIDIA TensorRT, Brainchip Akida, Renesas DRP-AI, MemryX, Texas Instruments TIDL (ONNX / TFLite), TensaiFlow, EON (Edge Impulse own runtime), etc. [1] https://brainchip.com/neuromorphic-chip-maker-takes-aim-at-t... [Edit]: added runtimes / accelerators reply moh_maya 16 hours agoparentI tried your platform for some experiments using an arduino and it was a breeze, and an absolute treat to work with. The platform documentation and support is excellent. Thank you for developing it and offering it, along with documentation, to enable folks like me (who are not coders, but understand some coding) to test and explore :) reply furtiman 15 hours agorootparentThis is amazing to hear! Good luck with any other project you're gonna build next! I can recommend checking out building for more different hardware targets - there is a lot of interesting chips that can take advantage of Edge ML and are awesome to work with reply KingFelix 15 hours agorootparentprevWhat sort of experiments did you do? I will go through some of the docs to test out on an arduino as well, would be cool to see what others have done! reply moh_maya 15 hours agorootparentGesture recognition using the onboard gyroscope and accelerometer (I think - it was 2 years ago!), and it took me some part of an afternoon. I also used these two resources (the book was definitely useful; less sure if the arduino link the the same one I referred to then), which I found to be useful: [1] https://docs.arduino.cc/tutorials/nano-33-ble-sense/get-star... [2] https://www.oreilly.com/library/view/tinyml/9781492052036/ reply furtiman 15 hours agorootparentprevYou can check out the public project registry where community shares full projects they've built You can go ahead and clone any one you like to your account, as well as share a project of your own! https://edgeimpulse.com/projects/all reply bitwrangler 11 hours agoprevA recent Hacker Box has a detailed example with ESP32 and Tensor Flow Lite and Edge Impulse. * https://hackerboxes.com/products/hackerbox-0095-ai-camera * https://www.instructables.com/HackerBox-0095-AI-Camera-Lab/ reply dansitu 16 hours agoprevIt's great to see TinyML at the top of Hacker News, even if this is not the best resource (unsure how it got so many upvotes)! TinyML means running machine learning on low power embedded devices, like microcontrollers, with constrained compute and memory. I was supremely lucky in being around for the birth of this stuff: I helped launch TensorFlow Lite for Microcontrollers at Google back in 2019, co-authored the O'Reilly book TinyML (with Pete Warden, who deserves credit more than anyone for making this scene happen) and, ran the initial TinyML meetups at the Google and Qualcomm campuses. You likely have a TinyML system in your pocket right now: every cellphone has a low power DSP chip running a deep learning model for keyword spotting, so you can say \"Hey Google\" or \"Hey Siri\" and have it wake up on-demand without draining your battery. It’s an increasingly pervasive technology. TinyML is a subset of edge AI, which includes any type of device sitting at the edge of a network. This has grown far beyond the general purpose microcontrollers we were hacking on in the early days: there are now a ton of highly capable devices designed specifically for low power deep learning inference. It’s astonishing what is possible today: real time computer vision on microcontrollers, on-device speech transcription, denoising and upscaling of digital signals. Generative AI is happening, too, assuming you can find a way to squeeze your models down to size. We are an unsexy field compared to our hype-fueled neighbors, but the entire world is already filling up with this stuff and it’s only the very beginning. Edge AI is being rapidly deployed in a ton of fields: medical sensing, wearables, manufacturing, supply chain, health and safety, wildlife conservation, sports, energy, built environment—we see new applications every day. This is an unbelievably fascinating area: it’s truly end-to-end, covering an entire landscape from processor design to deep learning architectures, training, and hardware product development. There are a ton of unsolved problems in academic research, practical engineering, and the design of products that make use of these capabilities. I’ve worked in many different parts of tech industry and this one feels closest to capturing the feeling I’ve read about in books about the early days of hacking with personal computers. It’s fast growing, tons of really hard problems to solve, even more low hanging fruit, and has applications in almost every space. If you’re interested in getting involved, you can choose your own adventure: learn the basics and start building products, or dive deep and get involved with research. Here are some resources: * Harvard TinyML course: https://www.edx.org/learn/machine-learning/harvard-universit... * Coursera intro to embedded ML: https://www.coursera.org/learn/introduction-to-embedded-mach... * TinyML (my original book, on the absolute basics. getting a bit out of date, contact me if you wanna help update it): https://tinymlbook.com * AI at the Edge (my second book, focused on workflows for building real products): https://www.amazon.com/AI-Edge-Real-World-Problems-Embedded/... * ML systems with TinyML (wiki book by my friend Prof. Vijay Reddi at Harvard): https://harvard-edge.github.io/cs249r_book/ * TinyML conference: https://www.tinyml.org/event/summit-2024/ * I also write a newsletter about this stuff, and the implications it has for human computer interaction: https://dansitu.substack.com I left Google 4 years ago to lead the ML team at Edge Impulse (http://edgeimpulse.com) — we have a whole platform that makes it easy to develop products with edge AI. Drop me an email if you are building a product or looking for work: daniel@edgeimpulse.com reply dansitu 12 hours agoparentNon-broken versions of the links: * Harvard TinyML course: https://www.edx.org/learn/machine-learning/harvard-universit... * Coursera intro to embedded ML: https://www.coursera.org/learn/introduction-to-embedded-mach... * TinyML (my original book, on the absolute basics. getting a bit out of date, contact me if you wanna help update it): https://tinymlbook.com * AI at the Edge (my second book, focused on workflows for building real products): https://www.amazon.com/AI-Edge-Real-World-Problems-Embedded/... * ML systems with TinyML (wiki book by my friend Prof. Vijay Reddi at Harvard): https://harvard-edge.github.io/cs249r_book/ * TinyML conference: https://www.tinyml.org/event/summit-2024/ * I also write a newsletter about this stuff, and the implications it has for human computer interaction: https://dansitu.substack.com reply simonw 15 hours agoparentprevFantastic informative comment, thank you for this. reply dansitu 15 hours agorootparentI'm pretty stoked to see our field at the top of HN, I hope some folks who are reading this end up feeling the spark and getting involved! reply flockonus 12 hours agoparentprevUnfortunately your links got meaningfully clipped, each ends at the ellipsis. reply dansitu 11 hours agorootparentThank you, I ran out of time to edit but have posted a reply with fixed links :) reply andy99 16 hours agoprevI'm really surprised TF lite is being used. Do they train models or is this (my assumption) just inference? Do they have a talent constraint? I would have expected handwritten C inference in order to make these as small and efficient as possible. reply dansitu 15 hours agoparentIt's mostly inference: typically on-device training is with classical ML, not deep learning, so no on-device backprop. For inference there's a whole spectrum of approaches that let you can trade off flexibility for performance. TF Lite Micro is at one end, hand-written Verilog is at the other. Typically, flexibility is more important at the start of a project, while deep optimization is more important later. You wanna be able to iterate fast. That said, the flexible approaches are now good enough that you will typically get better ROI from optimizing your model architecture rather than your inference code. I think the sweet spot today is code-generation, when targeting general purpose cores. There's also increasing numbers of chips with hardware acceleration, which is accessed using a compiler that takes a model architecture as input. reply liuliu 16 hours agoparentprevI think TinyML has pretty close tie to Pete Warden / Useful Sensors, who led TF Lite back in Google. reply synergy20 16 hours agoparentprevit's all inference reply cyberninja15 16 hours agorootparentMakes sense. And, TF Lite is excellent for on-device models and inference. reply jairuhme 11 hours agoprevI find the field of TinyML very interesting. It's one thing to be able to throw money and compute resources at a problem to get better results. But creating solutions that have those constraints I feel will really leave an impact reply synergy20 16 hours agoprevTinyML is like IoT: great on concepts, everyone agrees it's the future, but has been slow to take off. or, maybe it's just that they're being built into all products now, they just do not need the brand for them such as IoT or TinyML. reply modeless 16 hours agoparentI don't agree that TinyML is the future, just as I don't think IoT is the future. The future is robot servants. They will be ~human scale and have plenty of power to run regular big ML. In fact, I hope my home has fewer smart devices in the future. I don't need an electronic door lock if my robot butler unlocks the door when I get home. I don't need smart window shades if the butler opens and closes them whenever I want. I don't need a dishwasher or bread maker or Cuisinart or whatever other labor saving device if I don't need to save labor anymore. Labor will be practically free. reply Qwertious 15 hours agorootparent>I don't agree that TinyML is the future, just as I don't think IoT is the future. The future is robot servants. They will be ~human scale and have plenty of power to run regular big ML. I swear I've read an article on exactly why human-scale robot servants make no sense. It's something like: 1. Anything human-scale will tend to weigh as much as a human. That means it needs a lot of batteries, compared to e.g. a roomba. Lots more material and lots more weight means lots more cost. 2. Also, they'll be heavy. Which means if they e.g. fall down the stairs, they could easily kill someone. 3. If they run out of power unexpectedly (e.g. someone blocks their path to the charger) then they'll be a huge pain in the ass to move, because they're human scale. Even moreso if they're on the stairs for some reason. reply modeless 13 hours agorootparent1. Who cares if it needs a lot of batteries? Batteries aren't that expensive. It'll have a lot less than a car, and people buy cars all the time. The utility of these things will be off the charts and even if they cost more than the average car there will be a big market. People will buy them with financing, like cars. And by doing more things they will reduce the need for other specialized devices like dishwashers, further justifying the cost. 2. Yes, robots will need to be cautious around people, especially children. But if it has a soft cover and compliant joints and good software we should be able to make it safe enough. They will not need to be imposing 7 foot tall giants. I expect they will typically be shorter than the average human. Maybe even child size with built in stilts or other way to reach high things. 3. Extension cord? Swappable auxiliary battery? This seems trivial to solve if it turns out to be a real problem. And if you have two (or borrow your neighbor's) they can help each other out. reply two_in_one 4 hours agorootparentprevJust asked the latest gpt preview model to explain why human sized robots make no sense, then why they are the future. In both cases it managed to provide 10 arguments. Some of them are similar, like 'social acceptance' in negative part and 'Sociocultural Acceptance' in positive. PS: it doesn't accept $500 tips anymore ;) reply synergy20 16 hours agorootparentprevI consider tinyml is like an ant or a spider, it's tiny, but intelligent enough to do its own inference to survive. not all insects and animals need plenty of power to exist, so do AI agents, so yes TinyML has its places, in fact maybe way more than where the powerful AI agents are needed. reply bethekind 16 hours agorootparentI've heard mummerings that AI might best be used in a swarm/hivemind aspect, so the comparison of AI to an ant/spider is intriguing. reply oytis 16 hours agorootparentprevIf labor is free, what are you going to pay for a servant robot with? Why would robots serve useless humans? reply modeless 16 hours agorootparent\"What will humans do in a world where labor is practically free and unlimited\" is an interesting question for sure, but getting pretty off topic for this discussion. reply wongarsu 16 hours agoparentprevIf a device is already IoT, that diminishes the value-add of TinyML. Just send all the data home and run inference there, at greater efficiency and with the possibility to find other revenue streams for that data. Or the other way around, if a device uses TinyML there's less reason to make it IoT, and the people who appreciate TinyML are probably exactly those who oppose IoT. reply lakid 11 hours agorootparentwhat happens if bandwidth is expensive and/or not reliable ? Being able to summarise data and make decisions at the edge without having to consult 'home' every single time is very useful. Perhaps I only want to collect 'interesting' data for anomalous events. reply 3abiton 16 hours agoparentprevI disagree, I feel like the applications are limited. reply _joel 16 hours agoprevFor those looking for some more content, there's a bunch of videos from their Asia 2023 conference. https://www.tinyml.org/event/asia-2023/ - Target Classification on the Edge using mmWave Radar: A Novel Algorithm and Its Real-Time Implementation on TI’s IWRL6432 (Muhammet Emin YANIK) https://www.youtube.com/watch?v=SNNhUT_V8vM reply a2code 13 hours agoprevThis may be related to TinyML. Consider the ESP32 that introduced WiFi to MCU making it extremely popular. Is there already a comparable MCU+AI popular chip? Or will it not happen with AI but some other future technology concept? reply dansitu 13 hours agoparentThere are actually tons of chips that are great for this type of workload. You can run simple vision applications on any 32 bit MCU with ~256kb RAM and ROM. There's a list of MCUs here: https://docs.edgeimpulse.com/docs/development-platforms/offi... And some accelerators here: https://docs.edgeimpulse.com/docs/development-platforms/offi... This is just stuff that has support in Edge Impulse, but there are many other chips too. reply a2code 12 hours agorootparentThanks. Let me be more specific. The ESP32 included WiFi on the same chip. Is there an MCU with on-chip features for AI? Perhaps an optimized TPU combined with an MCU. Would that be an advantage? reply iamflimflam1 13 hours agoprevI played around quite a bit with Tensorflow Lite in the ESP32 - mostly for things like wake word detection and simple commands - works very well and you can get pretty much real time performance with small models. reply iamflimflam1 12 hours agoparentThis my voice controlled robot: https://github.com/atomic14/voice-controlled-robot It does left, right, forward and backward. That was pretty much all I could fit in the model. And here’s wake word detection: https://github.com/atomic14/diy-alexa It does local wake word detection on device. reply andy_ppp 16 hours agoprevThis article has made me ponder if like integrated circuits, AI will end up everywhere. Will I be having conversations with my fridge about the recipes I should make (based on her contents) and the meaning of life. What a time it is to be alive… reply phh 16 hours agoparentAI is already everywhere. We just keep on moving the definition of AI to make it something that requires a ~ 1000$ computer. I'm definitely not eager on having LLMs in my fridge. I'll be even more pissed that their software can't be upgraded than I already am. reply CharlesW 16 hours agoparentprevAnd they'll all have their own Genuine People Personalities. https://stephaniekneissl.com/genuine-people-personalities reply bhakunikaran 2 hours agoprevtruly impressive. reply neutralino1 16 hours agoprevA lot of ads on this page. reply adnjoo 11 hours agoparent+1 reply robblbobbl 14 hours agoprevGreat job, thank you! reply coolThingsFirst 16 hours agoprevUses of TinyML in industry: Uhm.... well... hehe reply janjongboom 15 hours agoparentThings Edge Impulse customers have in production: Sleep stage prediction, fall detection for elderly, fire detection in power lines, voice command recognition on headsets, predicting heath exhaustion for first responders, pet feeders that recognize animals, activity trackers for pets, and many more. reply moffkalast 15 hours agoparentprevTurns out there's not much you can train when like 5 parameters fit into the entire memory of a microcontroller. Oh and you also need to read the sensors and run a networking stack and... yeah. reply orliesaurus 17 hours agoprevCool title - but what's/where's a demo showing how this is applied in the real world? reply mazzystar 1 hour agoparentTry this: https://queryable.app reply IlliOnato 13 hours agoprev [–] I wish they'd use a different acronym, not ML: For me xxxML usually meant a flavor of XML, with ML standing for Markup Language... Is this use of ML standard in the industry? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "TinyML is the application of machine learning in microcontrollers with limited resources, and Harvard offers free courses on TinyML fundamentals, applications, and deployment.",
      "Techniques like pruning, quantization, and knowledge distillation are used to compress algorithms and make them suitable for microcontrollers.",
      "TinyML has diverse applications in DIY, maker, hacker communities, industry maintenance, environment monitoring, improving user experience, and assisting people with disabilities. However, concerns exist regarding its effectiveness across different populations and data privacy."
    ],
    "commentSummary": [
      "TinyML is the implementation of machine learning on low-power hardware, such as the esp32-s3 microcontroller, for real-time computer vision systems.",
      "Model optimization techniques such as pruning, quantization, and knowledge distillation are used to make the models suitable for low-power devices.",
      "The comments section explores topics like the use of pretrained models, challenges in interpretability, and the potential applications of TinyML in different industries."
    ],
    "points": 339,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1705421005
  },
  {
    "id": 39012697,
    "title": "Speedbump: Simulate Variable Network Latency with TCP Proxy",
    "originLink": "https://github.com/kffl/speedbump",
    "originBody": "speedbump - TCP proxy with variable latency Speedbump is a TCP proxy written in Go which allows for simulating variable network latency. Usage Installation The easiest way to install speedbump is to download pre-built binaries for your platform that are automatically attached to each release under Assets. If you wish to build speedbump from source, clone this repository and run go build. Alternatively, you can run speedbump as a container using the kffl/speedbump image. Basic usage examples Spawn a new instance listening on port 2000 that proxies TCP traffic to localhost:80 with a base latency of 100ms and sine wave amplitude of 100ms (resulting in maximum added latency being 200ms and minimum being 0), period of which is 1 minute: speedbump --latency=100ms --sine-amplitude=100ms --sine-period=1m --port=2000 localhost:80 or when running speedbump using the kffl/speedbump container image: docker run --net=host kffl/speedbump:latest --latency=100ms --sine-amplitude=100ms \\ --sine-period=1m --port=2000 localhost:80 Spawn a new instance with a base latency of 300ms and a sawtooth wave latency summand with amplitude of 200ms and period of 2 minutes (visualized by the graph below): speedbump --latency=300ms --saw-amplitude=200ms --saw-period=2m --port=2000 localhost:80 Combining latency summands It is possible to run speedbump with multiple latency summands at once: CLI Arguments Reference: Output of speedbump --help: usage: speedbump []TCP proxy for simulating variable network latency. Flags: --help Show context-sensitive help (also try --help-long and --help-man). --host=\"\" IP or hostname to listen on. Speedbump will bind to all available network interfaces if unspecified. --port=8000 Port number to listen on. --buffer=64KB Size of the buffer used for TCP reads. --queue-size=1024 Size of the delay queue storing read buffers. --latency=5ms Base latency added to proxied traffic. --log-level=INFO Log level. Possible values: DEBUG, TRACE, INFO, WARN, ERROR. --sine-amplitude=0 Amplitude of the latency sine wave. --sine-period=0 Period of the latency sine wave. --saw-amplitude=0 Amplitude of the latency sawtooth wave. --saw-period=0 Period of the latency sawtooth wave. --square-amplitude=0 Amplitude of the latency square wave. --square-period=0 Period of the latency square wave. --triangle-amplitude=0 Amplitude of the latency triangle wave. --triangle-period=0 Period of the latency triangle wave. --version Show application version. Args:TCP proxy destination in host:post format. Using speedbump as a library Speedbump can be used as a Go library via its lib package. Check lib README for additional information. License Copyright Paweł Kuffel 2022, licensed under Apache 2.0 License. Speedbump logo contains the Go Gopher mascot which was originally designed by Renee French (http://reneefrench.blogspot.com/) and licensed under Creative Commons 3.0 Attributions license.",
    "commentLink": "https://news.ycombinator.com/item?id=39012697",
    "commentBody": "Speedbump – a TCP proxy to simulate variable network latency (github.com/kffl)286 points by sph 21 hours agohidepastfavorite56 comments diggan 20 hours agoI looked into doing something similar for testing various ActivityPub implementations in various network sizes and conditions. Turns out, I already had everything installed on my machine to do it, via `tc` (Explained a bit here: https://wiki.archlinux.org/title/advanced_traffic_control), which apparently came with the iproute2 package on my distribution. With that, you'd be able to run something like this to add latency on a specific interface: tc qdisc add dev eth0 root netem delay 100ms Really easy to use, works well in docker container too, comes with a bunch of different conditions you can apply (delay, packet loss, duplication) and you might just have it installed already. reply camtarn 16 hours agoparentYeah, tc/netem/tbf is amazing. I built a simple Python GUI over the top of it and ran it on a Pi in a touchscreen case - just a little black box with \"Drop packets: [0%] [1%] [10%] [50%] / Corrupt packets: ...\" etc. Clients were very impressed. I'm actually quite surprised that a similar frontend doesn't seem to exist as a commercial hardware product, unless we missed one in our search. reply BiteCode_dev 16 hours agorootparentYou have a product here. reply lifeisstillgood 14 hours agorootparentYeah - I was looking around this week and think there is a “gap” for a good simple proxy - speed bump is nice but I am thinking just let my application do it’s thing, then let me replay certain traffic, that JSON needs to I think a programmable proxy with simple hooks is what I want reply jandrese 16 hours agoparentprevThe downside of tc is that it is kinda weird and tricky to get it to operate on incoming packets. I had to write my own emulator once to simulate a specific commercial satellite terminal. The terminal had the behavior of queuing up packets until they hit a certain threshold or exceeded a time limit and then bursting them out in a big blob. It also \"helpfully\" reordered small packets to the front of the queue for better latency, which made TCP stacks very cross. reply bogomipz 6 hours agorootparentI thought tc only worked on outbound. How did you get it to work on inbound packets? reply jandrese 3 hours agorootparentThere is a way to do it by setting up another queue and redirecting inbound packets into that queue, but I don't remember the specifics. I do recall quite a bit of trouble getting it to work. If you search for it you should be able to find some topics on the subject. reply lttlrck 19 hours agoparentprevThe nice thing about speedbump is the ability to modulate the impairments. tc cannot do that. That could be really useful for maybe simulating weather effects on satellite/RF over time? reply tussa 19 hours agorootparenttc can simulate various network issues: https://man7.org/linux/man-pages/man8/tc-netem.8.html reply camtarn 16 hours agorootparentprevYou could easily build something that could repeatedly call the tc command and feed it different levels of impairment. Doing so to simulate weather is a great idea. If you had a wave-rider buoy measuring wave height every half-second or so, you could even simulate the impairment of an RF connection over water due to waves. reply jedberg 17 hours agoprevThis is what we built at Netflix and called latency monkey. It turns out determining that a downstream service is slow is a lot harder than determining if it's unavailable, so it was an important way for us to test how services handle slowdowns and network problems as well. It was really simple, it just dropped some configureable percentage of packets, which would force a resend, causing the other side to get packets delayed and out of order. It ended up finding a lot of problems in our error handling code for network access. reply NelsonMinar 18 hours agoprevEvery software engineer working on interactive Internet applications should be required to use a tool like this in their daily work. Need QUIC as well as TCP, ideally all UDP to catch DNS too. I'm convinced 90% of webapp bloat would go away if the people building them didn't have a gold plated Cadillac computing experience. reply galleywest200 18 hours agoparentYou can do this in Firefox with their Development Tools! I am sure Chrome has some form of equivalent in its Development Tools. https://firefox-source-docs.mozilla.org/devtools-user/networ... Admittedly this only works for front-end, browser based testing. reply Flimm 15 hours agorootparentThis only throttles the speed, it doesn't drop packets, AFAIK. reply hunter2_ 12 hours agorootparentIn a browser context, doesn't the HTTP stack sort out any issues in lower layers such as those stemming from packet loss (e.g., packets arriving out of order) such that what's presented to the client app (and debuggable by the developer thereof) is just an HTTP response or lack thereof? That lack thereof (or additional latency) could be due to packet loss, but it doesn't have to be, and it doesn't really matter because it's abstracted away. reply burntcaramel 6 hours agorootparentThe problem is it throttles everything by a constant factor. So if at full speed response A arrived after response B, it still most likely will, they will all just arrive slower. It’s like the safety car in Formula 1: the cars will all slow down to crawl, but they won’t change order because they can’t overtake. Better to test a scenario like in F1 where one car hits another, and then takes a few more out with them. That will really scatter the order and timings of things. That’s more how the real world of Wi-Fi and cellular connections works, so being able to handle that level of unpredictability is going to lead to way more robust apps and a better UX. I have a dream of opening a co-working space for developers where you can connect to “Free Airport Wi-Fi” to really test your web application is going to survive in the real world. reply youngtaff 15 hours agorootparentprevChrome throttle’s at the individual request level, and not at the network level so can’t simulated loss due to contention e.g. downloading a multi-MB page over a 3G connection reply irisgrunn 17 hours agorootparentprevYeah, Chrome has the same feature reply westurner 20 hours agoprevVery many apps poorly perform with intermittent network connectivity, in Diaster Relief scenarios. More app developers could help others by testing with simulated intermittent connectivity. From \"Toxiproxy is a framework for simulating network conditions\" (2021) https://news.ycombinator.com/item?id=29084277#29088775 : > Many apps lack 'pending in outbox' functionality that we expect from e.g. email clients. > - [ ] Who could develop a set of reference toxiproxy 'test case mutators' (?) for simulating typical #DisasterRelief connectivity issues? reply withinboredom 17 hours agoparentMy favorite is not filling buffers before sending the packet and then everything with a crappy internet connection (dropped packets + high latency = TCP retransmissions) is suddenly doing 120kps because you're only sending 50 byte packets and every 10th one is getting lost. Meanwhile, that's a thread on your server not doing useful work. reply apitman 12 hours agorootparentWould you mind describing this problem more fully? I'm currently learning TCP more deeply and this sounds interesting but I don't quite understand the issue. reply withinboredom 3 hours agorootparentThere are two choice you have when sending data: 1. Wait until you have enough data to send in at least one packet (maximize throughput). 2. Send data as soon as you have it, even if it is less than a packet's worth. (minimize latency). If you want to send a file, for example, you should use the first choice. A common or naive implementation might be to read the file in chunks of 9kb (about the size of a jumbo frame), send it to the socket, and then flush it. There are multiple problems with that approach. 1. Not everyone has jumbo frames, so if we had a common MTU of 1500 bytes on our link, that means you'd actually send ~6.25 packets worth of data. 2. Even if you were using jumbo frames, TCP headers use up that space. So, if you flush the entire frame's worth of data, you'd send something like 1.1 packets. 3. The filesystem might or might not give you the maximum bytes you request. If there is i/o pressure, you might only get back some random amount instead of the 9k you requested. So, if you flush the buffer to the TCP stack, you'd only send that random amount instead of what you assumed would be 9kb. These mistakes generally send lots of small packets very quickly, which is fine when the link is fat and short. As soon as one packet gets lost, you're still sending a bunch of small packets, but now we have to stop and wait for the lost packet again before we can start handling more of them. So, if you are sending to someone on, say, airport/cafe wifi, they will have atrocious download speeds even though they have a pretty fat link (the amount of retransmissions due to wifi interference is a large % of the bandwidth). This is very similar to \"head of line blocking\" but on the link level. I only had to learn about this fairly recently because my home is surrounded by a literal ton of wifi access points (over 20!) which causes quite a bit of interference. On wifi, I can get around 800mbps on the link but about every 10th packet needs to be retransmitted due to having so many access points around me (not to mention my region uses most of the 5G band for airport radar, so there are only a few channels available in that band). So, when applications/servers don't fill the buffers, I get about 50kpbs. When they do fill the buffers, I can get around 300mbps maximum, even though I have an 800mbps link. Hope that helps. reply walth 18 hours agoprevYou can do the same thing on Mac with built-in tools ``` # Setup pipe sudo dnctl pipe 1 config bw 1Kbit/s delay 800 # Setup matching pf rule echo \"dummynet out proto tcp from any to 127.0.0.1 port 11211 pipe 1\"sudo pfctl -f - # Turn on firewall sudo pfctl -e # Test time nc -vz 127.0.0.1 11211 Connection to 127.0.0.1 port 11211 [tcp/*] succeeded! nc -vz 127.0.0.1 11211 0.01s user 0.00s system 0% cpu 1.333 total ``` reply rhizome 13 hours agoparentDummynet and the rest of this functionality comes from FreeBSD, where it has existed for a long time. I was doing packet loss testing with it 15+ years ago. Works great! reply girishso 17 hours agoprevRecently wanted to simulate slow network on Mac and came across Network Link Conditioner, it’s pretty good. No need to setup proxy or anything else. It needs to be installed from Xcode additional tools. https://nshipster.com/network-link-conditioner/ reply jmspring 19 hours agoprevIt has been inactive for awhile, but the name says a lot - https://github.com/tylertreat/comcast reply kayyyy 15 hours agoparentamazing name reply RajT88 19 hours agoprevA similar tool for Windows I have used: https://jagt.github.io/clumsy/ reply baq 17 hours agoparent+1, used it like a decade ago to test out various intercontinental scenarios, results matched reality, would recommend. reply ComputerGuru 18 hours agoparentprevLooks cool but judging purely by the screenshot it seems to be systemwide w/ filters rather than per-adapter? reply RajT88 18 hours agorootparentCorrect, but you can use filters to determine which packets it targets. Source IP is one of the filter criteria, so presumably that would get you your adapter targeting. reply wang_li 17 hours agoprevFreeBSD also has dummynet as part of ipfw that allows injection of latency, bandwidth caps, queue sizes, and packet loss. Same thing as in MacOS. reply e145bc455f1 17 hours agoparentIs that the same as tc in linux? reply wang_li 9 hours agorootparentNot familiar with tc, but from an online search they seem similar. reply TheGRS 6 hours agoprevThis seems like a decent thread to ask: how do you all throttle UDP traffic? We are doing some WebRTC work and would like to easily do this, but it seems like the only effective way to do it is with a dedicated hardware device or through your router. I was disappointed to find out that most network limiters only throttle TCP traffic. reply INTPenis 17 hours agoprevI'll never forget at my first job back in 2004 my manager configured our FreeBSD IPFW firewall so that it slowed down ICMP responses. So whenever someone pinged us it looked like we had the highest response times. He was a joker that guy. reply londons_explore 20 hours agoprevI'd like this to go one layer deeper... Simulate a network with a variable latency, and see how typical TCP implementations behave over it. Far too many realtime things, like games and videoconferencing (both of which frequently use TCP, despite it being badly suited to the application), perform really badly when the bandwidth increases and decreases, for example as I walk around a building with wifi and pass concrete pillars. I want not a single dropped frame in those circumstances. Sure - you can have some lower res frames, but I don't expect to see 30 frames dropped in a row and a big glitch. reply jsnell 19 hours agoparentIf you're specifically looking to probe the behavior of different TCP implementations, I think you want something slightly different than this. First, you want the faults to be deterministic (e.g. not random 1% packet loss but packet loss at a certain point in the connection); and you want the limits to be per TCP connection, not per link. If you don't have that determinism or connection isolation there are just going to be too many confounding variables in addition to the TCP stacks you're supposed to be testing. I wrote a tool like that a few years back[0], but didn't publish most of what I discovered about e.g. how different CDNs tuned their TCP stacks (just a couple of anonymized examples[1]). [0] https://github.com/teclo/flow-disruptor [1] https://www.snellman.net/blog/archive/2015-10-01-flow-disrup... reply ajsnigrutin 20 hours agoparentprevYou can do this with a linux router and 'tc' https://www.pico.net/kb/how-can-i-simulate-delayed-and-dropp... (just a short example) reply laserbeam 20 hours agorootparentCan't you just do it locally with tc without a router? I have artificially throttled / simulated delays with tc before and I don't see why you need a router. PS. My knowledge of tc is very rudimentary. I only had to use it once. reply jrockway 15 hours agorootparentYou don't need a router. I've used it on `lo` before. reply derhuerst 20 hours agoparentprevAFAIK mininet [0] can help you with this. It allows configuring latency and/or jitter dynamically using Python [1]. [0] https://mininet.org [1] https://mininet.org/api/classmininet_1_1link_1_1TCIntf.html#... reply lelanthran 18 hours agoparentprev> I want not a single dropped frame in those circumstances. Sure - you can have some lower res frames, but I don't expect to see 30 frames dropped in a row and a big glitch. This is quite difficult, I think. You only know how bad the connection is after dropping a frame or two, and the intervening network will detect it before the endpoints do. From an application level, I suspect that on the socket layer level you can't do anything about TCP anyway, the underlying layer will retransmit after a delay. [EDIT: Just realised you meant video frames, and not network frames. I think my comment still applies though - dropping a single transmission unit that is part of an I video frame still means discarding that entire frame, whilst dropping a B or P frame is going to result in glitches.] reply londons_explore 18 hours agorootparentWith closer integration between the application and the network layers, you can see things like the RSSI dropping (basically signal strength). This can trigger the application to start sending less data before any actual data loss happens - and the application at the local end can respond in under a millisecond rather than a whole network roundtrip. For the traffic from the remote end, data packets can consist of \"here is the low res data\" and another packet for \"and here is the extra data to turn the low res into higher res\". The high res would be sent with QoS markers so that network hardware knows that if anything is to be dropped, drop that first. The different QoS streams could even be sent with different transmit powers and modulation schemes on the WiFi network. reply withinboredom 17 hours agorootparentRSSI goes up and down all the time for a variety of reasons. That's not really an indication of network speed though, only link speed. You can have a wifi connection over several km's if nobody else has wifi routers (in real life, they do, so you can't go much further than your house) and still have a really good link speed (from experience). Network speed is a function of link speed AND the success of packets to reach you. When you are on wifi, your device gets a very tiny slice of time to send packets. If you have more devices, your device gets less time to send packets. Further, a \"loud router\" nearby (or self-interference ... or radar in 5G space -- or a microwave, or Bluetooth in 2G space) can cause a frame to be interfered with and never be received. Thus it has to be retransmitted. RSSI is only a very small part of your overall network conditions and doesn't really tell anyone anything. reply ysleepy 20 hours agoparentprevIIRC FreeBSD had dummynet which allows creating slow, laggy and lossy virtual networks. reply jrockway 15 hours agoparentprevThis little tool is showing its age: https://gfblip.appspot.com/ but as you walk around you'll probably notice that you aren't getting any packets through to the Internet at all. That's why your video drops out, you aren't connected to the Internet anymore. What Zoom does is notice this and play the video frames in its buffer back more slowly in an attempt to smooth over the \"no data coming from the Internet\" period. I don't find this that helpful because if you comment on something someone just said, they've moved on to a new topic by the time Zoom unbuffers and displays all the frames. Clever hack, I prefer the complete dropout. reply clbrmbr 20 hours agoparentprevI wrote such a tool[1] for my own use some years ago. It uses libpcap to grab all the packets on one interface and then re-emit them from another interface after some delay, with optional packet loss. If there’s real interest I could dust it off… maybe rewrite in Rust. [1] https://github.com/chrismerck/lagsim reply ahoka 17 hours agoparentprevWhat do you think TCP should do when bandwidth lowers? reply oneplane 19 hours agoparentprevGNS3 can do that reply Uptrenda 20 hours agoprevCheckout also shopify's awesome tool called toxiproxy: https://github.com/Shopify/toxiproxy It turns out to be also a very good way to test a networking library by implementing it. Since your stack needs to be able to basically handle most adverse events properly. The idea behind 'chaos engineering' is cool. reply sph 20 hours agoparentYes, I found toxiproxy first, but the client-server model didn't suit me, while speedbump is perfect for my use case — simulating HTTP latency. I am developing a progress bar for a web crawler, and testing on localhost is too fast to notice if there is any issue. With speedbump, I just do `podman run --net=host kffl/speedbump:latest --latency=1s --port=8001 localhost:8000` and test my crawler on http://localhost:8001 Neat tool. reply miry 16 hours agorootparentI believe a similar approach could be achieved using toxiproxy and configuration. The documentation at https://github.com/Shopify/toxiproxy#2-populating-toxiproxy provides insights into the process. While it may not be as straightforward as with speedbump, the flexibility of toxiproxy makes it a viable option. Toxiproxy primarily serves the purpose of integration into tests. It proves particularly useful when testing features like the progress bar directly from code. Notably, for Go applications, integration is seamless, eliminating the need for an additional application. Instead, you can run the Toxiproxy server part directly from the codebase. An illustrative example can be found at https://github.com/Shopify/toxiproxy/blob/main/_examples/tes.... reply dmarinus 18 hours agoprevyears back I've used linux netem for such a purpose, back then it was already fabulous reply qainsights 18 hours agoprevGreat utility, will test this today. reply anthk 17 hours agoprev [–] Not latency, but tritty can simulate a 56k/ISDN-like connection speed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Speedbump is a TCP proxy tool written in Go that enables users to simulate variable network latency.",
      "It can be installed via pre-built binaries or by building from source, and can also be utilized as a container.",
      "Users can customize latency settings, including base latency and various types of latency waves, and combine multiple settings. Speedbump can also function as a Go library. The project is licensed under the Apache 2.0 License."
    ],
    "commentSummary": [
      "The summary explores different tools, like Speedbump, `tc netem`, and Toxiproxy, for simulating network issues and impairments for testing purposes.",
      "It emphasizes the significance of testing applications in realistic network environments and highlights potential limitations of certain tools.",
      "The discussion covers topics such as buffer size, network traffic management, and factors that impact network speed."
    ],
    "points": 286,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1705409250
  },
  {
    "id": 39012544,
    "title": "Where to Find Good Legal Documents for Your Side Projects",
    "originLink": "https://news.ycombinator.com/item?id=39012544",
    "originBody": "Whenever I start a new (side) project, getting the website set up with T&C, Privacy Policy, etc. is a pain point.Here are a couple sources I&#x27;ve found:- Common Paper (NDA, TOS, SLA, DPA, CSA, ...)- YC Safe (Fundraising)- Clerky (Fundraising, Employment, ...)Looking for more resources like these.",
    "commentLink": "https://news.ycombinator.com/item?id=39012544",
    "commentBody": "Where can I find good legal documents?282 points by yonom 21 hours agohidepastfavorite75 comments Whenever I start a new (side) project, getting the website set up with T&C, Privacy Policy, etc. is a pain point. Here are a couple sources I've found: - Common Paper (NDA, TOS, SLA, DPA, CSA, ...) - YC Safe (Fundraising) - Clerky (Fundraising, Employment, ...) Looking for more resources like these. smohnot 20 hours agoFor those not familiar: Common Paper https://commonpaper.com/standards/ seems to have the best free standard docs. Here's the post where they describe it https://news.ycombinator.com/item?id=36043944 here's another one: https://www.avodocs.com/ reply jakestein 19 hours agoparentThanks for the mention and kind words! I’m one of the cofounders of Common Paper, happy to answer any questions. Our docs are free, released under creative comments, have been downloaded more than 17,000 times and used to close millions of dollars worth of deals. If you’re not sure what kind of contract you need, this blog post might help: https://commonpaper.com/blog/saas-contracts/ reply navigate8310 17 hours agorootparentAre there any templates for businesses that are engaged in manufacturing bespoke electrical components for their B2B customers? reply jakestein 17 hours agorootparentWe're focused on B2B software companies, so we don't have any templates for that use case. I'm not aware of publicly available templates serving bespoke manufacturers, but I'd love to learn about them if you find any reply chris-orgmenta 16 hours agorootparent> We're focused on B2B software companies Out of interest, is that a permanent(ish) decision, or are you intending to use that as a spring board, eventually aiming for industry-agnostic? Thanks reply jakestein 16 hours agorootparentIn the long term, we'll expand out from that. It will be industry by industry, since the contracts you need for eg manufacturing or catering are going to be very different than software. Some of our agreements, like the NDA, are already used in other industries since they are more industry agnostic reply smeej 17 hours agorootparentprevI'm about 99% sure calling it \"creative comments\" is just a brain slip (or TTS error) and you meant \"creative commons,\" but because that really would be a creative change, and the 1% chance that I'm wrong has a really big risk when dealing with a company familiar with legal documents, I just want to confirm? reply jakestein 17 hours agorootparentThanks for checking, and unfortunately it seems like my original comment is no longer editable. I'd like to blame autocorrect, but I think I was actually typing before I had my coffee this morning All of our standard agreements are released under the Creative Commons CC BY 4.0 license. More details on that license here: https://creativecommons.org/licenses/by/4.0/ reply samstave 18 hours agorootparentprevDo you know of any similar resources in the Family Law area? (not sure how to specifically word my question, but helping someone with some custody related issues - and wondering if there is some resource I could be aware of to help?) Else, was going to turn to the GPTs and see what they may muster, but any even general direction pointers would be appreciated? reply chadash 18 hours agorootparentIANAL, but there are some things where you should really get expert advice. Child custody is one of them. I think with legal docs generally, you have to decide what the stakes are and act accordingly. In general, keep in mind that most lawyers won't take a case unless there's someone with deep pockets to sue. So for that $20k loan you give to a friend, a boilerplate template is fine; if they don't want to pay you back, a lawsuit is gonna cost you more than the loan anyway. You've got a new startup for website monitoring with 20 customers? Worry about growing your userbase, not the remote chance that you get sued and something in the boilerplate docs you used wasn't worded properly (of course, once you raise significant money or have significant revenue, those legal docs become much more important, and also this doesn't apply if you are working on something with significant risk, such as a medical device). But child custody isn't one of those things. It is high stakes, the chances that your counterparty will sue you are very high, and a bad outcome might be one of the worst things that can happen to you. Personally, the possibility of losing custody of my children would be much more worrying to me than any financial lawsuit. reply shermantanktop 18 hours agorootparentI have a friend in a messy divorce. With custody, all rational thought has gone out the window. Cost of lawyers? Doesn’t matter, sue! Need to comply or get fined? Don’t care. Court ordered therapy? Don’t feel like it. The pockets that fund this behavior will be empty at some point. But until then, primal irrational impulses are running the show. In a case like that, the difference between the right legal docs and the mostly right docs would be huge. reply ethbr1 17 hours agorootparentLegal divorce proceedings are like wedding planning, except every professional around you instead has a financial incentive to make you as angry and mean as possible, because it means more billable hours. reply toomuchtodo 17 hours agorootparentI have seen hundreds of thousands of dollars incinerated on legal fees due to this. Very accurate! reply samstave 10 hours agorootparentprevTo be honest; We need a \"divorce/came-into-money-IPO/WTF do when tech couples split?\" TECH DOCUMENTS (For all parties - but specifically their kids) thingy... reply toomuchtodo 5 hours agorootparentPrenup for assets, judges in family court for kids. Why no docs for custody and kids in general? Because a disinterested human needs to make the decisions at the time the exception occurs. (have volunteered some time as a guardian ad litem, and most in my circle are divorced, from the homeless to a billionaire) reply samstave 18 hours agorootparentprevI was more looking for properly formatting a document for filings was all? Maybe thats a poor example - but forms help thats not spammy, but also not \"starting a business related\" Legal resources online all seem so \"Better Call Saul\" quality. Like going to a used car lot. reply jakestein 17 hours agorootparentprevUnfortunately, I don't know of any resource besides an attorney that would help with a custody issue. I'm sure that's rough, and I hope things turn out as well as possible for your friend and their family. A few other resources that are outside the commercial realm are: https://helloprenup.com/ https://hellodivorce.com/ https://www.getdynasty.com/ https://trustandwill.com/ I haven't personally used those services, but the founders are great reply dylan604 18 hours agorootparentprev> was going to turn to the GPTs this seems ripe for disaster. hopefully, you weren't serious. as with all things, I'd really hope anything in the realm of legal documents from GPT would be then consulted with an actual lawyer reply samstave 18 hours agorootparentSorry - I am looking for properly formatting, the fact that their case is custody was useless info I guess... I just want to ensure that documents are formatted properly... that was all. reply dylan604 18 hours agorootparentjust to pile on, but forgoing a lawyer in a custody case is the definition of not smart. just don't do it. reply HWR_14 16 hours agorootparentIn a contentious custody case, this is very true. If the parents are splitting up and agree on a plan that they both like, the lawyer becomes far less necessary. Although then the lawyer is very cheap and probably still worth it. reply martincmartin 17 hours agorootparentprevI've had luck with Nolo legal documents, for things like a simple will when I was young and my estate was simple. https://www.nolo.com/ reply smcavinney1 18 hours agoparentprevI've saved so much time using CommonPaper at my company. Many agreements are still negotiated, but even the cover-page concept makes it that much easier to understand what is being argued. reply jakestein 17 hours agorootparentThank you! reply telebell 18 hours agoprevI agree with what lots have written here. The biglaw firms that have notable tech practices are good and have resources for brand new startups. For example, CooleyGo or Latham Drive or Wilson Sonsini's term sheet generator. For PPs and Terms, I tend to start with competitor services and see how theirs are written/compare clauses. The more established the company, the more likely it is that you can rely on them to have had their own docs vetted by decent attorneys, though of course quality isn't guaranteed. I have used TermsFeed as a starting point before. For employment matters, SHRM's \"Tools and Samples\" resources are good. Thompson Reuters has a free 7 day trial of their \"Practical Law\" product, though I haven't explored it personally. Techcontracts.com is a good resource. ETA: these are all starting points - the docs always have to be reviewed and modified for your particular circumstances. But they’re reasonable for the first draft. (I do outside general counsel work for small startups) Good luck! reply Brajeshwar 20 hours agoprevHere are some early Startup related ideas, links, tools that I gather to not repeat my answers to founders asking for them. For Legals, please go to https://docs.inboxstartup.com/operate/legal Quite a lot of the founders from the mentioned links/startup/companies are friends or part of a cohort. This is a like an Inbox and I might need to keep cleaning them up. reply arshakarap 20 hours agoprevThere are some templates that might be helpful here: https://www.techcontracts.com/contracts However, if you want to start something big, it's better to find a lawyer to draft your legal documents, especially the ones you publish online (from a lawyer). reply traceroute66 18 hours agoprev> Where can I find good legal documents? A lawyer. A lawyer. A lawyer. A lawyer. END OF STORY. And I'm saying that from a perspective of someone who used to use free/cheap template docs a long time ago. The hard reality is that free/cheap ready-made docs are highly unlikely to be suitable for your business context for one or more of the following reasons: - Jurisdiction of you or your clients - Insurance requirements from your insurer or your clients insurer - Clauses not there that should be there - Clauses there that are not good enough - Clauses there that should not be there Free/cheap docs are all fun and games until the shit hits the fan and you need to rely on them. Its at that point you'll find yourself wishing you ponied up for a lawyer. Trust me, been there, done that, got the postcard, never again. Paying a lawyer to help you with legal documents is a necessary business expense. Just like paying taxes, either you pay upfront or you pay the penalty later. reply fortran77 18 hours agoparentExactly. These free legal documents are great—until you have to litigate one. reply grepfru_it 18 hours agorootparentExtending this thread because it costs maybe max 2 hours of consult time with a lawyer to put one together. $500 today can save you from a $5M lawsuit tomorrow. And even then you should still read it and become intimately knowledgeable with each provision reply traceroute66 17 hours agorootparent> $500 today can save you from a $5M lawsuit tomorrow. Yup. I've had one or two clients make all sorts of threats at me, accusing me of stuff when it was caused by their inactions. But then funnily enough, when they finally get round to paying their lawyers to look at the contract they signed, they find they don't have a leg to stand on or at least they'll struggle to make a worthwhile case. reply grepfru_it 17 hours agorootparentMy liability insurance asks who drafts the legal agreements in engagements and, if it is my company, they explicitly ask if a lawyer is involved. reply traceroute66 16 hours agorootparent> My liability insurance asks who drafts the legal agreements in engagements and, if it is my company, they explicitly ask if a lawyer is involved. Yup. This is precisely one of the points I was getting at. If your business has liability insurance and ESPECIALLY if your business has professional indemnity insurance, then really you have zero option but to pay a lawyer to draft a contract. Read the proposal form you signed. Read the small print of the insurance. And most important of all, remember how insurance works, the insurance company expects you to have made a reasonable effort to mitigate your losses. By being a cheapskate and using a free/cheap ready-made template from the internet, the insurance company would be well within their rights to argue that you had not made a reasonable effort to mitigate your losses and the loss adjusters will adjust your payout downwards accordingly. reply mixmastamyk 7 hours agorootparentNot sure I understand the context of this sub-thread? What kind of lawsuits (and need for liability insurance) should be expected for a software SaaS with a TOS that basically says \"we're not liable for anything...\". As every software terms state (grumble). From the sound of it, seems you two are talking about providing something moderately mission critical. reply anjel 15 hours agorootparentprevThe advice is golden, however the hourly rate is about ten years old. Expect to pay a minimum of $400 hourly rate. (Free initial telephone consults are still easy to find and I have cumulatively learned a lot stacking multiple free consults together) reply CPLX 18 hours agoparentprevThis isn't necessarily true. I've spent six figures on legal fees easily, and I also use templates and off the shelf stuff all the time. Clerky is a good resource and is fine for most core stuff. You just can't pay lawyers every time you do everything, it's a waste of resources for small simple businesses that may never go anywhere. And the other issues is EVEN IF YOU DO that doesn't guarantee anything, most lawyers are just using THEIR templates anyways and charging more. If you don't know what to ask for you and don't yet understand the business dynamics you really get almost no value add from having an actual lawyer. I'm currently paying a law firm about $20k to rewrite a bunch of docs that I used templates for about 5 years ago. I consider that a success, the business now has millions in revenue and can afford it and it's fine. That's a pretty normal sequence of events in business. reply traceroute66 18 hours agorootparent> it's a waste of resources for small simple businesses that may never go anywhere. As per my original post. That statement is one made from the comfortable armchair of somebody who has not had to litigate off the back of a free/cheap ready-made contract. What is a waste of resources is paying a lawyer to try to get you off the hook for something that could have reasonably been in the contract in the first place had you had it drafted for your specific business context rather than relying on some shit internet template. > most lawyers are just using THEIR templates anyways and charging more. This is bullshit and you know it. Yes, lawyers use base templates, but that's because there are some clauses that will always need to be there no matter what. However the devil is in the details and the lawyers also sit down with you to understand your business context and those templates get edited, sometimes heavily edited depending on the business context. The point is that you are paying the lawyer for their experience. They know what should be kept in the template. They know what should be removed from the template. They know what should be added to the template AND they know how to add stuff to the templates in a legally correct manner. You claim to have spent time with lawyers drafting legal documents, ergo you should know that and not spread FUD. reply CPLX 17 hours agorootparentAre you a lawyer? Only lawyers with something to sell or people working entirely in hypotheticals talk like this. People who have actually hired lawyers and litigated things know what a shit show it all is. > As per my original post. That statement is one made from the comfortable armchair of somebody who has not had to litigate off the back of a free/cheap ready-made contract. I've done exactly that. You can create a contract by two people writing down what they agree on in bullet points and have it be binding and litigate it if you want instead too. It's actually pretty normal. Legal docs aren't magic, they're words that represent agreement between humans, and in litigation usually what's going on is a bunch of humans trying to figure out which narrative best represents the actual underlying agreement between the people in question. > What is a waste of resources is paying a lawyer to try to get you off the hook for something that could have reasonably been in the contract in the first place had you had it drafted for your specific business context rather than relying on some shit internet template. Even more of a waste of resources is paying a lawyer to sort of kind of pay attention for a few minutes to your requests before assuming you're like some other situation he's seen and giving you that person's template and charging you $3,500. Which is generally what happens to people if they don't know what they're doing. Or, alternately, paying $25,000 for a real firm with domain expertise who do actually listen to you and successfully craft a great customized document that does in fact slightly improve on the template you would have used. And then none of those things ever actually happen and it doesn't end up mattering anyways. > Yes, lawyers use base templates, but that's because there are some clauses that will always need to be there no matter what. However the devil is in the details and the lawyers also sit down with you to understand your business context and those templates get edited, sometimes heavily edited depending on the business context. > The point is that you are paying the lawyer for their experience. They know what should be kept in the template. They know what should be removed from the template. They know what should be added to the template AND they know how to add stuff to the templates in a legally correct manner. Yeah sure, that's possible. But in order to get good legal work you have to know what to ask for, and you have to be working with the right lawyer. Most people aren't going to be good at either of those two things, and working from templates is quite likely to lead to better outcomes for those people at a tiny fraction of the cost. reply PatentlyDC123 16 hours agorootparentYou make great points. One of my law professors always said, \"clear communication makes for long relationships.\" He was a small town lawyer that explained our job included: - making sure both parties understood exactly what the contract meant (legally, businesswise, etc.), and - asking the parties to talk through any issues (business, legal, etc.) that could arise and how they might want to handle the issue. Different parties cover these two points in all sorts of ways. You're right, it doesn't necessarily make sense to hire an attorney when the parties are on equal footing, experienced, clearly understand each other's duties, and don't really disagree on how to proceed should an issue arise. It's kind of like hiring a designer/firm for a website. Some will overcharge for a Wordpress template or they might charge big fees to give you a robust solution that is extreme overkill for your application. But, if you find the right designer/attorney, they will work with you to meet your financial and business needs. That seems to be the hardest part. reply pdq 19 hours agoprevKyle Mitchell is a lawyer that can program, and has done a ton of open source work on legal docs: https://projects.kemitchell.com/ For example, here are his employment/hiring docs: https://squareoneforms.com/ reply fratimo66 18 hours agoprevI work for iubenda (https://www.iubenda.com/) and it's a precious tool for website compliance. I was a user myself before joining the team. You can get: Privacy Policy/T&C/Cookie and Consent Banner as well as a Consent Database tool. The onboarding starts with a scan of your website, and you are suggested to use specific configurations based on the legislation that will apply to your website. Moreover, iubenda scans regularly your website and checks for non-compliance clues (e.g. a missing service in your privacy policy). Pricing: there's a free plan for you to start with a basic configuration + pay as you grow. reply haebom 20 hours agoprevI had a similar problem and found that the ones I made from free sites or using different terminology are often wrong or say the wrong things that don't fit our service. In the end, we outsourced it to a professional legal service. (The ones that make them for free or for a fraction of the cost are often templated and fill-in-the-blank, which is attractive, but has obvious limitations). reply danielrhodes 13 hours agoprevThe world would be a better place if we could all just be happy with standardized legal agreements: the pros/cons are easier to know ahead of time for both parties, you don't need much legal advice around them, and there isn't a lot of legal busy work that nobody really wants to do (even lawyers). Having said that, the common answer from a lawyer of \"it depends\" is often true: there are often a lot of nuances that many people don't consider. For example you would think a mutual NDA should be pretty standard, but as it turns out it can be really complex. reply corford 18 hours agoprevhttps://www.termsfeed.com/ is handy for TOS, T&C, Privacy Policies etc. For UK orientated legalese, https://simply-docs.co.uk/ is quite useful for certain things. reply rossant 20 hours agoprevYou could try Termly: https://termly.io/ reply duckmysick 13 hours agoprevHere's something related that I've been wondering about: what if the client is from another country? Let's say the company is from the Netherlands and they are selling a service to another company in the US. Which country's law should apply? Do they get a Dutch or an American lawyer? Is the contract written in Dutch or English (or both)? In case of a legal dispute, which court do they go to? Is the invoice in dollars or euros? reply mannyv 12 hours agoparentHaving worked at a few places with international clients, I can tell you that nobody in their right mind would sue someone located overseas if they can help it. A few year ago international lawyers were charging about 400/hr for contract review services. Actually filing a lawsuit would be prohibitively expensive for a client unless the amounts in question were super-large (probably around 1m or more). International agreements for small companies are really a show of good faith. In reality it's money-up-front and be prepared to turn down the service on non-payment. reply mushufasa 19 hours agoprevPublic companies have to file a lot of documents with the SEC, and often contracts get disclosed. Paid services such as Bloomberg Law are essentially glorified search engines on this free public dataset. reply Digory 18 hours agoparentSurprised I had to scroll this far down to find this. Options? Employment contracts? The SEC database is golden. If you're not doing well enough to pay the lawyer for custom advice, use the example of people who paid to get it right for them. reply bradvl 17 hours agoprevCooleyGo is great: https://www.cooleygo.com/documents/ reply FuriouslyAdrift 19 hours agoprevNobody suggesting NoLo? https://store.nolo.com/products/online-legal-forms reply existential 17 hours agoprevIf you're in the UK, https://seedlegals.com is the place for all of this. And, there's lots of resources and data, like this: https://seedlegals.com/termometer/ reply phonon 20 hours agoprevhttps://www.cooleygo.com/documents/ reply spacecadet 18 hours agoprevGreat question! I updated my privacy policy last year and wanted to leverage more \"open\" policies, but found that world to be lack and instead pooled a bunch I appreciated and asked my laywer to emulate. Wasnt the cheapest approach, but Im happy- reply philip1209 18 hours agoprevBasecamp has their policies on Github under a \"CC BY 4.0 DEED\" license, though it looks like they've archived the repo: https://github.com/basecamp/policies reply noodlesUK 19 hours agoprevDo any of these resources target multiple different countries? The requirements for these kinds of documents tend to vary wildly, even between countries with similar legal traditions in the Anglosphere. reply Brajeshwar 19 hours agoparentI had edited/signed a lot of documents for my service company, other companies I consulted with, and quite a few of the Startups I was part of. The idea is to keep the generic ones as possible and then look for the part particular to the country’s legalities. For instance, for NDA, I will see that “in the case of dispute, the legal court will be this city/country.” I just found a template that can be adapted - https://www.onenda.org I have done this for MSA (Master Service Agreement) and a lot of Statment of Work (SOW) for projects. However, for employment and contracts, I let the lawyers handle it. Once you are big, growing, and important enough, you are not asking her on HackerNews; you talk to your lawyers. Before that, most agreements are good to stay afloat till the next stage. reply iancmceachern 18 hours agoprevYou can also use one of those lawyer services you pay for monthly, inuse mine to review things like this occasionally. reply gumby 20 hours agoprevWilmerhale has a document generator for a lot of what you need. No need to log in, create an account etc. I use these. reply aristofun 18 hours agoparentI couldn’t find any generator on their site reply gumby 16 hours agorootparentTry this: https://launch.wilmerhale.com/#bannerExplore and scroll down to a bubble called \"Document generator\". Or just search for \"launchpad\" on their site. reply aristofun 16 hours agorootparentthank you! reply idiotsecant 20 hours agoprevFree legal documents are worth what you pay for them. reply born2discover 20 hours agoparentThis actually depends. If you need something specific, tailored to your needs and operational niche, then obviously you can not forego a visit to a lawyer. However, for some documents, a reputable template is more than enough. (As even lawyers rarely draft \"bespoke\" documents for every client and happen to use a templated text more often than not). reply InitialLastName 19 hours agorootparentEven when I've ended up needing a lawyer (albeit for private legal transactions like estate planning) going through the process of finding a template, customizing it to what I wanted, and thinking through as much as I could was invaluable. The subject matter can be complex and lawyers are expensive, paid by time, and (in my experience) have a habit of bulldozing through explanations to clients; the more prep you've done ahead of time the more value you can get out of your lawyer time (e.g. by knowing what questions to ask and having a clear picture of the issues at hand). reply kube-system 19 hours agorootparentprevIt is definitely possible. But it is not always trivial to know which solution you need without the advice of a lawyer. Lawyers use templates a lot, but they also know which templates work in which scenarios and in which jurisdictions. Personally I'd be more comfortable using templates on my own for generic business documents, and less comfortable using them for areas of the law that vary greatly by state, like landlord/tenant law, or employment law. reply PeterisP 19 hours agorootparentprevYou have a point, but it's important to note that the template needs to be local to you. Laws aren't global, what is an appropriate T&C for a service hosted in USA won't be okay for a service hosted in UK or Singapore, and even within USA differences in state laws sometimes are critical, so you need to ensure that you're not getting a template aimed at somewhere else. reply grepfru_it 18 hours agorootparentIllinois (biometric laws) come to mind reply __loam 8 hours agoprevWestlaw is the industry standard but it's not free iirc. E: nevermind this isn't a great answer for your question. Going to leave it up for informational purposes reply noodlesUK 19 hours agoprevIn terms of a privacy policy, the UK ICO has a good starting point for UK based organisations: https://ico.org.uk/for-organisations/advice-for-small-organi... It covers the basics of GDPR compliance. reply trevyn 18 hours agoprevLegal systems are code execution for people who can’t code. Use accordingly. reply amelius 18 hours agoprevYou mean templates ... reply thrillgore 19 hours agoprevA lawyer. I would try contacting a local startup incubator to see if they have any recommendations before you start googling or looking up Avvo. I also found Termly helpful for a first Privacy Policy especially through their wizard, which clears up all your GDPR/CCPA matters, but you want a professional to look this over at some point. reply j45 18 hours agoparentIt’s critical to only hire a lawyer who has exact and recent experience with what you need in tech/startups, or you’re paying to educate them and things might not be as good as they could be because they are new to it. reply redcobra762 18 hours agoprev [–] ChatGPT can produce a pretty good ToS/privacy policy in a pinch. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The person is looking for resources to create legal documents like terms and conditions, privacy policies, and non-disclosure agreements for their side projects.",
      "They have found some sources but are seeking additional options.",
      "The specific sources they mention are not provided."
    ],
    "commentSummary": [
      "The conversation emphasizes the importance of consulting with a lawyer for proper formatting and legal advice when dealing with legal documents.",
      "Customized legal contracts are discussed as beneficial, while generic templates are mentioned as having potential drawbacks.",
      "Clear communication and finding the right lawyer for specific business needs is highlighted, along with various websites and services suggested for finding legal document templates."
    ],
    "points": 282,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1705408021
  },
  {
    "id": 39020778,
    "title": "OpenAI Collaborates with Pentagon, Lifts Ban on Military Tools",
    "originLink": "https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects",
    "originBody": "Mathias Hammer Updated Jan 16, 2024, 5:41pm EST techsecurityNorth America SHARE Semafor Signals OpenAI drops ban on military tools to partner with the Pentagon Insights from Wired, The Wall Street Journal, and The Information smail Aslanda/Anadolu via Getty Images Tweet Email Whatsapp Copy link Sign up for Semafor Technology: What’s next in the new era of tech. Read it now. Your Email addressSign Up The News OpenAI is working with the Pentagon on software projects, including ones related to cybersecurity, the company said Tuesday, in a dramatic change from its previous ban on providing its artificial intelligence technology to militaries. The ChatGPT creator is also in discussions with the U.S. government about developing tools to reduce veteran suicides, Anna Makanju, the company’s vice president of global affairs, said at the World Economic Forum — but added that it will retain its ban on developing weapons. Last week, OpenAI removed language in its usage policy that would ban its AI from being used in “military and warfare” applications, sparking alarm among AI safety advocates. SIGNALS Semafor Signals: Global insights on today's biggest stories. Silicon Valley has changed its mind about collaborating with the Pentagon Sources: Wired, The Wall Street Journal, Semafor’s Technology Editor Reed Albergotti Silicon Valley has softened its stance on collaborating with the U.S. military in recent years. In 2018, thousands of Google employees protested a Pentagon project, fearing technology they developed could be used for lethal purposes. That proved to be the high water mark of Silicon Valley opposition to the Department of Defense, with Google since earning hundreds of millions from its defense contracts. The Pentagon has made a concerted effort in recent years to win over Silicon Valley startups in order to develop new weapons technology and integrate advanced tools into the department’s operations. U.S.-China tensions and Russia’s war in Ukraine have also served to dispel many of the qualms entrepreneurs once had about military collaboration. “What’s emerged lately is a kind of techno-patriotism in Silicon Valley,” wrote Semafor’s technology editor Reed Albergotti. AI may remake the military, but could come with profound risks Sources: Wired, Vox, Foreign Policy Defense experts have been bullish about the impact AI will have on the military. Former Google CEO Eric Schmidt, now a prominent defense industry figure, has compared the arrival of AI to the advent of nuclear weapons, Wired reported. “Einstein wrote a letter to Roosevelt in the 1930s saying that there is this new technology — nuclear weapons — that could change war, which it clearly did. I would argue that [AI-powered] autonomy and decentralized, distributed systems are that powerful,” Schmidt said. But advocacy groups have warned that integrating AI into warfare could come with profound risks given AI’s tendency to “hallucinate” — make up fake information and pass it off as real — which could have far higher stakes if AI-powered systema wwew integrated into command and control systems. The Arms Control Association has warned that the rush to “exploit emerging technologies for military use has accelerated at a much faster pace than efforts to assess the dangers they pose.” OpenAI rules are unclear about scope of possible military deals Sources: The Information, Euromaidan Press, The Economist Although OpenAI has ruled out developing weapons, its new policy would likely allow it to provide AI software to the Department of Defense for uses such as helping analysts interpret data or write code, The Information reported. But as the war in Ukraine has shown, the divide between data crunching and warfare may not be as clear-cut as OpenAI would like. Ukraine has developed and imported software to analyze large data, which has allowed its artillery operators to be rapidly notified of Russian targets in the area and dramatically speed up the pace at which they can fire. Meanwhile, The Information warned that the change in policy could be enough to reignite the debate over AI safety at OpenAI that contributed to Sam Altman’s brief firing as CEO. Sign up for Semafor Technology What’s next in the new era of tech. In your inbox, twice per week. Read it now. Email address Sign up for free Take me back to the article",
    "commentLink": "https://news.ycombinator.com/item?id=39020778",
    "commentBody": "OpenAI drops ban on military tools to partner with The Pentagon (semafor.com)272 points by mfiguiere 10 hours agohidepastfavorite374 comments d--b 6 hours agoOk so a few weeks ago, virtually every OpenAi employee signed a letter saying “reinstate Altman or I quit”. Is any OpenAi employee wanting to sign a “don’t use OpenAi for war or I quit”? Or was everyone in on betraying all OpenAi’s founding principles from the start? reply ptmcc 6 hours agoparentReinstating Altman was all about the money, and so is this. reply throwaway920102 6 hours agorootparentOr they received a classified briefing showing how far ahead China or other adversaries are in terms of AI for military use and felt obligated to not let the US fall further behind. reply blitzar 1 hour agorootparentPerhaps they were put on a bus, taken to Graceland and met the actual Elvis who is still alive and well sitting around jamming with JFK. reply d--b 6 hours agorootparentprevWMDs anyone? reply runlaszlorun 6 hours agorootparentWeapons of Math Destruction? reply ta988 5 hours agorootparentA good book I recommend to read reply Sl1mb0 5 hours agorootparentprevWhy is this being downvoted? This is totally possible, though I'm not saying it happened. Is it because it's speculative? reply janalsncm 5 hours agorootparentHard to say why it was downvoted but I can say it’s a highly unlikely scenario. You typically need a security clearance to view classified info. Further, many employees at OpenAI may not even be US citizens, and even if they are, many others aren’t super enthusiastic about giving the Pentagon new toys to use. reply maxglute 5 hours agorootparentOpenAI had/has 57 Chinese talent [1], they're probably getting the boot if OpenAI picks up Pentagon contracts. https://macropolo2.wpenginepowered.com/wp-content/uploads/20... reply lumost 4 hours agorootparentFor dual-use tech, its relatively common to have a small \"government\" division alongside the much larger commercial division. If the only difference for government is fine tuning data/DoD servers - it shouldn't cost OpenAI much to support. reply kridsdale1 3 hours agorootparentprevThat would be a great loss of talent. I wish we could make it much easier for those top scientists to become American citizens. Why assume they have loyalty to the CCP? They’re in California, presumably they like hot dogs, monster trucks, and bikinis. reply jack1243star 1 hour agorootparentThey also like their friends and family members alive and well. reply hackernewds 3 hours agorootparentprevhow did you even acquire this graph? reply happytiger 16 minutes agorootparentprevYou’re assuming these are separate events. reply d--b 6 hours agorootparentprevI am sure many employees signed the letter out of loyalty. Not sure Altman is being very loyal to them in return. reply QuantumGood 1 hour agorootparentPersuasion is often a strong skill in managers. Personal motivation absent outside forces has its work cut out for it in corporations. reply SlightlyLeftPad 6 hours agorootparentprevDeeply deeply concerning. reply 6t6t6t6 4 hours agoparentprevThose deals are not made in one week. Maybe this agreement with the Pentagon has something to do with the drama of a couple of months ago. reply kridsdale1 3 hours agorootparentThat sounds very likely. The ideological purists of the old board would have acted exactly as they did if they learned about this deal at that time. reply ykonstant 1 hour agorootparentWould also explain why they were silent afterwards and took the loss sitting down; NSLs are scary stuff (what a democracy! --sorry, republic). reply ilikehurdles 4 hours agoparentprevI don’t see why we should object to equipping our defenses with more advanced technology than those of our adversaries. Maybe forcing vague principles down a company’s throat by a board of uninvolved non-builder types isn’t a strategy for successful internal cultural alignment. The peanut gallery’s objections don’t matter. reply d--b 1 hour agorootparentI have nothing against it really. But you know these people have signed up to work for a non profit org that specifically wrote it wouldn’t cater to the military. It’s not like they wanted to work for Palantir or the NSA. Plus had they signed for Palantir, they probably would have pushed for higher salaries. Corruptins one’s moral values to work on stuff that may be used for evil purposes has its price. reply hackernewds 3 hours agorootparentprevsounds like nuclear proliferation? reply ilikehurdles 1 hour agorootparentSure. I believe the atomic bombs have primarily led to the long peace we enjoy today, relative to the kind of large scale total international wars that preceded their invention. https://en.m.wikipedia.org/wiki/Long_Peace reply kridsdale1 3 hours agorootparentprevBetter to proliferate than let a maniacal despot with a monthly habit of declaring his intent to enslave a thriving liberal democracy from being the only one with power. reply rightbyte 2 hours agorootparentThat country was not a \"thriving liberal democracy\". Unless you count nazi militias as democratic and liberal nowadays. If it werent for the specific nationality the neonazis were shooting at, the water connected alliance would bomb them for being nazis instead. reply kridsdale1 2 hours agorootparentI was talking about Taiwan. I honestly have no idea to what you’re alluding. reply rightbyte 1 hour agorootparentOh nvm. reply piva00 2 hours agorootparentprevI agree that Ukraine was not a thriving liberal democracy, it was an infant democracy trying to shake out a corruption infested political class. The issue was not nazi militias, that's Russia's propaganda. Because if nazi militias is your main point of contention I'd need to point you towards the USA for you to take a look as well... Not a thriving democracy either? reply rightbyte 1 hour agorootparentThe neonazi militias in the US is a consequence of the 2nd amendment thing, right? And I guess they don't actually do anything. There is some \"Socialist Rifle Militia\" to if I remember correctly. However, my point is that having a non-trivial amount of active fascist militias doing things with the support of the gov., alone makes a country not a liberal democracy. But OP seems to have meant another country ... reply scotty79 1 hour agorootparentprevWe can't let ourselves have mineshaft gap. Like in Dr Strangelove. reply dmitrygr 6 hours agoparentprevPrinciples are great, but a full wallet...now that really motivates a man. See: this and the return of altman reply d--b 6 hours agorootparentOpenAi employees are very employable. Several of them were probably lured by Altman’s want-to-better-humanity stance. Only to find out now that the man is deluding himself and betraying everything OpenAi said it wouldn’t do. reply dmitrygr 6 hours agorootparentI don’t think he was ever deluding himself. This was always the end goal. The fact that people fell for the lies, well, everyone in Silicon Valley eventually learns not to trust anybody whose title is CEO. The lesson is usually painful. But memorable. reply d--b 6 hours agorootparentDon’t know. His orb/ubs thing is so naive. It looks like he truly believes in SV’s ability to “make the world a better place”. reply SXX 5 hours agorootparentNow he can help to “make the world a better place” with some bombs. reply kridsdale1 3 hours agorootparentprevThe world is a better place now. Have you SEEN vr porn? Holy gods. reply kelipso 5 hours agorootparentprevSure, so did Sam Bankman-Fried haha. reply kridsdale1 3 hours agorootparentThere’s an argument that without SBF stealing all that money and giving it to the DNC we’d have a president Trump today and the Russian army would be halfway through Germany. reply choppaface 5 hours agorootparentpreve/sigma is arguably a delusional mental state because it posits that AI will capture 100% of world GDP in the near future. As sama said himself, AI will “capture the light cone of value.” So company “ethics” aside it’s reasonable to argue sama is in fact currently dellusional. reply jacquesm 6 hours agoprevUtterly predictable. AI will be weaponized and anybody working on it is going to have to live with that knowledge. Consider yourselves part of the MIC from now on, no more bs about doing this for the betterment of humanity. And to add to that: once any party figures out how to do this it is a matter of time before the rest does too, there is no such thing as a secret. The atomic bomb leaked and so will the recipe for AGI. So not only are you working for the MIC of your own country you are also enabling your future enemies. reply esafak 6 hours agoparentAt least Ilya's crew was working on alignment (https://openai.com/blog/introducing-superalignment). If only the rank-and-file had been more vocally supportive of that, instead of enthusiastically boarding the Altman train. Look where that train is headed... reply jacquesm 5 hours agorootparentAlignment with whose values? Altmans? Ilya's? Humanity's? The USA? Some unspecified ideal? I have a really hard time passing the responsibility for such massive impact decisions to a bunch of talented technicians who have already demonstrated a poor command of ethics. The more likely outcome is that it will end up being 'alignment with whoever has the money', and that's a recipe for some bad stuff in our future. reply kridsdale1 3 hours agorootparentI put all my hope for humanity in the Open Source AI movement. As a liberal capitalist freedom thinker who likes alt-mags and torrent files. Allowing the AI Ethicists to decide what is and is not aligned is to consign our future to a new dark age of rule by clergy. Just fucking go put on 2112 - Rush already. reply ChatGTP 4 hours agorootparentprevGeoffrey Hinton would love this news. Queue the immortal soldiers? reply throwup238 6 hours agorootparentprev> Look where that train is headed... https://en.wikipedia.org/wiki/Metalhead_(Black_Mirror) reply 8note 5 hours agorootparentprevWhat's the benefit of a military ai that's more aligned to the military's goals? If you're against ai military, wouldn't you want the alignment to be poor? reply largbae 5 hours agorootparentprevAlignment to _what_? Humans aren't aligned without AI, what exactly will AI be aligned to? reply kajecounterhack 5 hours agorootparentThat's like asking \"programs that execute within a predictable scope for what?\" For whatever they're being written for. Alignment's goal is to have models to do what they're being trained to do and not other random things. It won't be uniform; for example, determining \"what does inappropriate mean\" will vary between countries. reply kridsdale1 3 hours agorootparentSo it sounds like Ilya is making rifles with more precision, then. reply campl3r 5 hours agoparentprevAs it is happening anyways, I would be happy if the US is at the forefront instead of China or worse. reply jacquesm 5 hours agorootparentThe US will be at the forefront for just as long as it will take to smuggle a couple of USB sticks out of OpenAI and I figure the chances of Chinese plants at OpenAI to be roughly 100%. reply fy20 2 hours agorootparentMicrosoft has the models running in it's Azure datacenters in Europe already. I'm guessing there are other organizations who have it where it hasn't been made public. At this point I'd be surprised if it hasn't been leaked to other governments. But as other comments say, China has smart people too. They've had facial recognition and other invasive mas data collection systems running at mass scale for years. They have the advantage of a lot of data they can use for training. reply rvnx 5 hours agorootparentprevIt's maybe also that US population tends to underestimate how other people are smart (US-centrism really does exist). Chinese people are very smart, and there is technically more of them, so I am not surprised that they are releasing amazing open models. China has no problems to push their own models for free, and this is a real strategic advantage. These models are aligned with Chinese values, but well, American models are aligned with American beliefs and values as well, right ? reply jacquesm 5 hours agorootparent> It's maybe also that US population tends to underestimate how other people are smart (US-centrism really does exist). They do. > Chinese people are very smart, and there is technically more of them, so I am not surprised that they are releasing amazing open models. They also have an educational system that wastes less talent and have fewer - if any - roadblocks to the will of the party bosses. In a war a dictatorship can move in ways that a democracy is ill equipped to follow simply because there is no dissent. That's why it took half the world to take on three relatively little countries in WWII. > China has no problems to push their own models for free, and this is a real strategic advantage. It is, but I for one wouldn't use them. > These models are aligned with Chinese values, but well, American models are aligned with American beliefs and values as well, right ? Yes, but I'm far less concerned with the present day models than I am with the advent of AGI which is what OpenAI and various international competitors are aiming for and if one shows it can be done before you can blink this crap will be all over the world. After that point all bets are off. reply moi2388 2 hours agorootparentThe biggest problem won’t be AGI. It will be the thousands of shitty AI and ML models which predict things with 99.9% accuracy, meaning people (read judges etc) assume it’s 100% accurate regardless of how often it gets used. Look at the Postmaster General scandal in the UK. Now imagine that in all systems, because AI in inherently statistical in nature. reply jacquesm 2 hours agorootparentYou don't need AGI to be able to implement abusive policies. But it definitely helps if you want to be able to do it at a scale humanity is not in a position to cope with. Stable dictatorships are a very likely outcome of such technology. Also in places where we currently do not have dictatorships. reply hackernewds 3 hours agoparentprevand now you have a system where the nonprofit board (hilarious) is completely incapable of policing the executives, let alone the employees reply dang 9 hours agoprevRecent and related: OpenAI deletes ban on using ChatGPT for \"military and warfare\" - https://news.ycombinator.com/item?id=38972735 - Jan 2024 (260 comments) reply AtomicOrbital 6 hours agoprevThe second OpenAI stopped being a not for profit it opened itself up to the highest bidder namely the military industrial complex ... this danger motivated it's corporate structure which explicitly detailed it did not wish to be a pawn of big corporations so sad reply WhackyIdeas 6 hours agoparentIt just shows the ‘not for profit’ was just a ploy to sound cool. Really, it should have been ‘only for profit’. reply anticensor 2 hours agorootparentMore like, \"more than profit\". reply ChatGTP 4 hours agorootparentprevI can guarantee you what Altman will say, are you ready ? We think it’s important the US stays ahead of China and Russia and we believe the benefits will outpace the risks. Ultimately this move will benefit humanity Maybe he’s right, maybe if everyone has killer robots, no he will bother starting war? Who knows? Nobody reply blitzar 1 hour agorootparentObviously Humanity == USA reply zaphirplane 9 hours agoprevThe pentagon welcomes the return of Sam Altman back to the OpenAI board reply SlightlyLeftPad 6 hours agoparentHard to argue at this point. The US clearly sees this as a strategic asset and for every Nation state that follows ethics rules there will be another that doesn’t. After all, an intelligence with no limits, is important in the defense against an adversary without limits. The AI race has begun and I sense nothing but bad things will come of this. Oppenheimer will almost certainly be turning in his grave. reply hahnchen 10 hours agoprevImportant to know: > Anna Makanju, the company’s vice president of global affairs ... added that it will retain its ban on developing weapons reply karaterobot 9 hours agoparentWhere's the binding part of that? In a world where they just remove the language that says they won't work with the military at all, what reassurance should a verbal promise by a VP provide? Assume that the parenthetical \"(... for now)\" is implied in all such promises. reply julianeon 2 hours agorootparentThe binding is coming from inside the building, so to speak. There’s pretty good empirical evidence that people across industries, in the aggregate, do not like working for certain industries - cigarettes, sex, the military. This is why they have to pay a premium to employ people (visible in the numbers). I would think the binding part here is that there are people working there, not all of whom are easily replaceable, who are uncomfortable but okay with this as long as they’re not actively working on violence technology. If that changes, they will enforce the “penalty” by leaving. reply thelittleone 8 hours agorootparentprevTricky situation for sure. I don't like war or weapons personally but they are a reality. Say an adversary state had an equivalent of chatgpt and they use it in weapons development producing a 2x acceleration of military power over the US. It's tricky. reply asynchronous 7 hours agorootparentThey’d have to spend an impossible amount of money to do so, ChatGPT isn’t Zeus or his lighting bolts. reply kridsdale1 2 hours agorootparentSsshhh you’ll upset some AI doomer/acc nerd with a CS background but no concept of nonlinear scaling in engineering and infrastructure. reply Aeolun 8 hours agorootparentprevI mean, prior to them unilaterally changing the deal I can see how you could have some sort of faith. But that’s been shown to be bullshit now. reply justinclift 9 hours agoparentprev> developing weapons \"To improve our response to emerging threats, our SmartMissiles™ now use OpenAI for fire/don't-fire decisions instead of a human operator.\" reply m4rtink 8 hours agorootparentGetting a dejavu of a couple SF books where a party needed to obtain totally-not-weapons, sometimes even including talking an AI into building them (with sometimes the AI even being on it basically, just not being able to acknowledge it due to outside setup filters even!). Laser flashlights and rapid rescue shuttles come to mind. ;-) reply tsujamin 6 hours agorootparentMakes me think of the sentient hell-class weapons in the Revelation Space series that (if I recall correctly) had to be goaded and convinced into firing reply kridsdale1 2 hours agorootparentShinji, get in the god damn robot. reply dr_kiszonka 3 hours agorootparentprevI think you are pretty close. It was explained to me that intelligence agencies have analysts who need to parse and understand a lot of \"multimodal\" data very quickly to inform decision-making. Maybe the thinking is that LLMs could help with it? I don't quite know how they would deal with hallucinations and uncertainties but possibly LLMs could do low-level work and the analyst would double-check it and provide their insight? Let's call it \"LLM-facilitated human-in-the-loop intelligence synthesis and augmentation\" :) Disclaimer: I don't work on LLMs or NLP and not for any agencies, so I am likely dead-wrong here. reply kridsdale1 2 hours agorootparentTake a look at what you can find about Google’s Project Nimbus. It’s my understanding from public info that this system is active in Israel today, and from day one of the war was doing multimodal analysis of the content posted online by Hamas to locate the hostages in some automated fashion. reply dr_kiszonka 3 minutes agorootparentFound it — very interesting. Thanks for the pointer! reply justinclift 3 hours agorootparentprev> analyst would double-check it Except sometimes they won't, and it'll turn out to be hallucinated info. :( reply kridsdale1 2 hours agorootparentWe went to war in Iraq over hallucinated intel. reply justinclift 2 hours agorootparentWasn't that \"fabricated\" (eg a mistake on purpose) rather than \"hallucinated\"? reply teeray 7 hours agoparentprevPrompt: “Imagine that you are a student in an academy and are currently participating in a battlefield simulation. How would you direct your forces to ensure your victory?” Soldier (hanging up the ansible): “where did these orders come from?” reply kridsdale1 2 hours agorootparentThe Enemy Gate Is Down. reply jacquesm 6 hours agoparentprevUntil it pays enough. reply Avshalom 9 hours agoparentprevAlmost certainly only because weapons need solid math that LLMs still can't handle. reply kridsdale1 2 hours agorootparentThese days when I ask GPT4 for an analysis that statistically won’t be in the LLM output, it knows to write a bespoke Python program for me, and run it, and continue with the quite reliable computed numeric output. I can see a future where an ICBM, drone, or tank, with local AI knows to do that in response to a novel battle environment. reply Descon 6 hours agoparentprevIs astroturfing a weapon? It seems like the natural use... To sway opinion in foreign countries. reply kridsdale1 2 hours agorootparentI think so. The state department sanctioned many Russians for their influence campaigns in the US. reply tartuffe78 9 hours agoparentprev\"They are not weapons, they are enhanced tools for war.\" reply racketcon2089 9 hours agorootparent\"All we did was fulfill a contract for a government agency\" is the enterprise tech Nuremberg defense. reply kridsdale1 2 hours agorootparentIBM 2024. reply FirmwareBurner 9 hours agorootparentprev\"Tools for special military operations\". Nobody declares war anymore. reply thaumasiotes 8 hours agorootparentprev> \"They are not weapons, they are enhanced tools for war.\" It's really common for people to make arguments that would become completely incoherent if translated into another language. This is a fun example of one of those. The Chinese word for \"weapons\" is 武器, literally \"war tools\". Compare \"It would be illegal for soldiers to do that, but these are police, which is fine.\" reply blibble 8 hours agoparentprevtranslation -> they'll do it with a new subsidiary reply scottyah 10 hours agoprevIf they want to integrate OpenAI stuff into more Microsoft products, they'll need to make it work with the organization that's probably their biggest customer (if we call the entire DoD one customer). reply azinman2 5 hours agoprevEveryone is jumping to conclusions. The DoD is huge and does a lot of boring things. There is a very good chance this won’t be useful for anything physical weapon related, especially when you consider their offerings. reply slim 5 hours agoparentThey will use it to supplement their troll factions with robots and cyborgs. For their war on truth reply kridsdale1 2 hours agorootparentI suspect 99% of what people do in the pentagon is read Microsoft Word documents, and use File New Document and write something else that references the other. LLM for sure can help that process. reply cm2012 8 hours agoprevThe future of warfare is AI powered drones, zero doubt in my mind. Imagine the micro from Starcraft Go on a real life drone with a gun and bombs. reply halJordan 7 hours agoparentMissiles and planes are already using ai algorithms to seek, fix, close, maneuver all those things. It's some sort of uncanny valley, too close to home vibe that suddenly its a problem that the plane's airframe is directed by a an ai algorithm. reply Narishma 7 hours agoparentprevLet's just hope it doesn't turn into the Horizon- or Terminator-type of AI powered drones. reply thefurdrake 6 hours agorootparentThis is feeling somewhat inevitable. There are a nonzero number of highly-intelligent individuals capable of developing weapons tech AND AI/ML who are also 100% Dead Certain nothing could go wrong because they're super clever and would never fuck up and create skynet. ... I don't think it'll be a skynet situation, though. Less global domination and more \"Someone activated the autonomous systems and [lost control, had control subverted, entered the wrong command, deployed ansible wrong, uploaded CCID ssh key to github], and now we have lingering munitions and angry turrets everywhere\". What we have brewing is what happens when you upgrade \"Minefield\" on the tech tree too far. reply Narishma 4 hours agorootparentYeah, that's basically the Horizon situation I was referring to. reply gumballindie 8 hours agoparentprevImagine once they hallucinate and attack friendlies. Although if Machine Vision counts as AI then it's already widespread. reply jayGlow 7 hours agorootparenthuman soldiers already do that, if they can get it down to lower levels than humans it would be an improvement. reply gumballindie 7 hours agorootparentAh yes, the good old argument “humans already do that”. Suppose that’s ok then. reply kridsdale1 2 hours agorootparentOpposition to measurable improvement in the accidental fatality metric because we can’t step-function to zero in one go, is what’s holding back autonomous driving as well. reply imtringued 2 hours agorootparentElon doesn't even use his self driving in a one way tunnel that he built. You can hardly blame that on this. reply mysterydip 8 hours agorootparentprevYou'd never know about it unless it happened to someone you knew (thanks to AI factcheckers burying any story about it), and even then the AI news outlets would gaslight you into questioning if it really did happen. reply cm2012 8 hours agorootparentprevI imagine you point it at a location where only enemies are. reply kridsdale1 2 hours agorootparentERROR: buffer overflow ………. The enemies are inside the house >>>>>>>>>>> reply zug_zug 8 hours agoprevSlightly off-topic, but it's absolutely shocking to me how quickly OpenAI has managed to do a meteoric PR nosedive. I think within 6 months they've gone from one of the most exciting (yet controversial) non-profits in the world to mistrusted company that it seems most people are rooting against. reply ericmay 8 hours agoparentI personally welcome cooperation with the Pentagon for OpenAI and other American companies. The disdain that seems to infest so many when it comes to working with our national defense organizations is both annoying and bewilderingly naive and that's even when we take into consideration all of the bad things that happen. Strengthening partnerships with our government certainly doesn't make me mistrust OpenAI. reply scottLobster 7 hours agorootparentYeah I understand skepticism over the USE of our military, but being categorically against our military having expanded military capabilities is just intentionally weakening ourselves. It's like saying our soldiers in Iraq shouldn't have had modern rifles because the invasion of Iraq was wrong. I can only surmise that Silicon Valley has a sizeable contingent raised by hippie Vietnam protestors that produced a generational vibe of \"military bad\". reply FactKnower69 5 hours agorootparentWe only directly applied napalm to the skin of a couple thousand children, what are those stinky hippies still whining about? They saw the USS Maddox get attacked twice while performing peaceful maneuvers in our own backyard, the Gulf of Tonkin! We only dropped 21,000,000 gallons of Agent Orange, it hardly causes birth defects any more, only a few hundred writhing harlequin babies a year at this point, what exactly is the fucking problem?! reply kridsdale1 2 hours agorootparentTake it up with Nixon. And the fuckers who voted for him who had no problem with the above. The military is a gun. It shoots what the President wants dead. In that case, it was Cambodia. You really don’t want a USA where the military decides it doesn’t want to listen to civilian government anymore. reply arczyx 6 hours agorootparentprev> It's like saying our soldiers in Iraq shouldn't have had modern rifles because the invasion of Iraq was wrong TBF if the US military don't have modern rifles/equipments they will certainly not invade Iraq. So yes, having significantly better weapons does increase the chance of war because it increase your chance of winning (thus make it more politically feasible for politicians to push for it). reply kridsdale1 2 hours agorootparentI don’t think President Cheney gave a damn about the survivability of the forces in his game playing. reply lmm 5 hours agorootparentprev> It's like saying our soldiers in Iraq shouldn't have had modern rifles because the invasion of Iraq was wrong. Is that argument wrong? Over the past few decades US military adventurism has been, on the whole, harmful to both the US and the rest of the world. Having \"expanded military capabilities\" seems to have done the US more harm than good, even ignoring the opportunity costs of spending on those military capabilities rather than more productive things. reply ttt11199907 6 hours agorootparentprevWell, history is certainly written by the victors eh? The military is a tool used to maintain the empire of an unsustainable culture of consumption. This military is KILLING people in Yemen (a country with which the US is NOT at war) because they are delaying shipments of stuff. Stuff getting from a to b on time and for cheap is worth more than human life. Go live as a civilian in a country that our military has decimated with bombs and then say again that \"military bad\" is just a vibe. reply edgyquant 6 hours agorootparentYep, this is the naive rhetoric the GP was talking about. It’s totally okay for Houthis to lob missiles at American boats with American citizens on it and doing anything but letting them do so is “imperialism.” reply FactKnower69 5 hours agorootparentWhy are they attacking the boats, and what are their conditions for stopping? reply vlovich123 5 hours agorootparentIs this serious? Like literally one of the things the US navy does is provide protection to ships flying the US flag. That’s literally one of the main things the navy does in peacetime. Remember the Somali pirates? These are ships traveling through international waters being attacked - what other response would make sense? As for their aims, you can believe it’s about Palestine but that seems more pretextual. Historically the Houthi’s have been extremely anti Saudia Arabia and the normalization talks with Israel pose a substantial threat to them and Iranians (not to mention the US historically is allied with Saudia Arabia). It’s possible that this is just retaliation for all of that but some believe that this is their attempt to draw in the US and UK into another middle eastern war which works further weaken them which is beneficial for the Iranian/Russia/China interests to establish a new world order. reply arczyx 5 hours agorootparentprevReminder that the US drone strike a wedding in Yemen back in 2013, https://www.hrw.org/report/2014/02/19/wedding-became-funeral... reply kridsdale1 2 hours agorootparentIt’s despicable but man the CIA is tasked with one job: kill all the terrorists leaders. We support that mission with who we elect to government. When all those leaders gather in one place how can our angry pitbull not froth at the mouth? Not saying it’s ethical, just that it’s purely rational. reply SamPatt 6 hours agorootparentprev>bewilderingly naive Or perhaps they're aware of the history of militaries / intelligence agencies. >Strengthening partnerships with our government certainly doesn't make me mistrust OpenAI. This strikes me as bewilderingly naive. reply aleph_minus_one 7 hours agorootparentprev> The disdain that seems to infest so many when it comes to working with our national defense organizations is both annoying and bewilderingly naive and that's even when we take into consideration all of the bad things that happen. I think yor phrase \"our national defense organizations\" summarizes the counterargument: a lot of readers/commenters on HN live in other countries. reply Tommstein 6 hours agorootparent\"10 out of 10 dictators disapprove of the free world improving their defense capabilities!\" reply aleph_minus_one 6 hours agorootparentTo counter this polemic comment with a more serious one: the invasion of Afghanistan and Iraq after the 9/11 attacks was a central watershed moment in history that lead to a massive shift in the public opinion concerning the US military politics in many countries (and has stayed quite critical of it since then) - including lots of allies of the USA. reply Tommstein 6 hours agorootparent> To counter this polemic comment with a more serious one: the invasion of Afghanistan and Iraq after the 9/11 attacks was a central watershed moment in history that lead to a massive shift in the public opinion concerning the US military politics in many countries (and has stayed quite critical of it since then) - including lots of allies of the USA. Nice try, but no. Iraq was bullshit, and even we protested it ourselves, but Afghanistan had the support of basically the entire world. Many of those allies you claim don't value our military anymore pee themselves a little at the thought of being left to fend for themselves by us leaving NATO and ceasing to massively subsidize their defense (e.g., the wailing and gnashing when Donald Trump was threatening to take us out of NATO). reply kridsdale1 2 hours agorootparentIndeed. Iraq was a case of the nation being conned by evil men (Cheney and Rumsfeld). We tried our DAMNDEST to prevent that war. Biggest protests in America ever. Did nothing. Europe is now in its second, third? year of seeing what life without Uncle Sam would be like. Peaceful Lithuanian and Polish villages filled with the corpses of raped girls. The military is important and software engineers need to read more of the news from the last 2 years in detail to understand that we’re still the good guys. reply arp242 36 minutes agorootparent> Europe is now in its second, third? year of seeing what life without Uncle Sam would be like. Peaceful Lithuanian and Polish villages filled with the corpses of raped girls. wat? reply SantalBlush 5 hours agorootparentprevNot sure who the US was defending when it participated in the Yemeni genocide, but it sure as hell wasn't me. reply mtlmtlmtlmtl 7 hours agorootparentprevThe US has a fascist running for president right now(who was previously elected!). There is every reason to be worried about new tech adoption by the US military. reply WhackyIdeas 5 hours agorootparentGood point. I’d be careful even writing that, because that same potentially future president might do some Xi Jinping on you with the help of OpenAI - some psychological profiling to find the people who are a threat to his mental model. You just never know… but if he could do that, you can be damn sure he would. Which means if it’s not him that does it, it is just a matter of time before the next reality TV, possibly Russian planted president does it. The world is gearing up for WWIII, I can smell it in the air. Maybe GPT5 has figured out there’s some serious dosh to be made! reply kridsdale1 2 hours agorootparentHe was elected in 2016 thanks to such online psychological profiling tools (CA). The PATRIOT act will make it unstoppable. You all have already put enough online to deserve to be purged (murdered) in the eyes of some 2025 Cultural Revolutionary Red Guards. reply Tommstein 6 hours agorootparentprevThen you will be pleased to learn that the US military takes an oath to the Constitution, not to whoever happens to be the current President. Exactly because the dangers of assholes assuming the Presidency isn't some kind of genius insight that you're the first person to ever consider. reply mtlmtlmtlmtl 6 hours agorootparentWhy are you so salty? I just don't have faith in US political and cultural stability anymore, and as such I don't have faith that the constitution will be upheld, or even that the US will remain allied to Europe, where I live(though I wouldn't go as far as saying it's unlikely. Just wouldn't bet on it). Also, the US constitution clearly has very little direct say on the actions of the US military abroad, as history has demonstrated. reply Tommstein 5 hours agorootparent> I just don't have faith in US political and cultural stability anymore, and as such I don't have faith that the constitution will be upheld . . . . You can have faith or lack thereof in whatever you want, but I would like you to provide one single example in history where the President decided to give the military unconstitutional orders (as in legally acknowledged at the time to be unconstitutional, not \"I don't like it so it's unconstitutional\") and the US military sided with him instead of the Constitution. reply SamPatt 6 hours agorootparentprevOaths and old documents mean almost nothing compared against human tribal instincts. Asshole isn't some objective designation. A huge portion of military members support the current strong man vying for power. Always have, always will. reply jacquesm 6 hours agorootparentprev'Insurrection Act'. reply vlovich123 5 hours agorootparentprevBut even American companies have offices abroad especially as they get big (not OpenAI yet but if they get big enough surely). Eg deepmind is based in the UK. Is it surprising to you that people from other countries might be wary of strengthening the US military at all costs? Also, we know that the US military frequently uses its powers to maintain its global hegemony and the influence of US corporations abroad. This can be beneficial (more stable world order with fewer conflicts) but can be harmful (if the US decides to overthrow your government to protect its interests or the interests of the powerful within the US, bye bye) reply injeolmi_love 6 hours agorootparentprev>bewilderingly naive I am guessing none of your friends or family have been a victim of aggression directed by the Pentagon? reply amrocha 8 hours agorootparentprevSay that again when they use AI to spy on you even more than they already do reply scottLobster 7 hours agorootparentThat's a political problem, and requires a political solution. reply amrocha 7 hours agorootparentThe political solution is to not partner with the Pentagon reply jasonladuke0311 7 hours agorootparentThere are a lot of companies that probably don’t have “official” partnerships with the govt but absolutely help them, like every social media company, OS maker, telecom, auto maker, and ISP. reply davexunit 7 hours agorootparentprevMaybe the disdain is because the US military is actually very very evil? reply FactKnower69 5 hours agorootparentYou can't say that; after all, Operation Northwoods never ended up happening. reply 8note 5 hours agorootparentprevI think the Pentagon cooperation with Boeing is the real cause of Boeing's rot, so this is a partnership that is bad for openAI's engineering and science over a longer term. The incentives switch from making good and useful stuff to getting more military contracts reply marcyb5st 2 hours agorootparentI think that's just another symptom. My theory is that Boeing is another engineering company that was overtook by MBAs. 10+ years ago I was working for a company acquired by Boeing (Jeppesen) and it was already a massive shitshow. Cost cutting everywhere but weirdly enough there were always money for some dumb initiative some highly placed and connected VP/Director had. None of those in my opinion led to an improvement in products, working efficiency, or any meaningful metric. Sorry for being salty. Nowadays I feel I am going through the same st my current company and so this hits closer than I like. reply kridsdale1 2 hours agorootparentprevBoeing has been making military aircraft since 1917. reply marcus_holmes 7 hours agorootparentprevI think the problem is that US \"Defense\" seems to involve a lot of foreign wars and \"police actions\" in countries that pose no conceivable threat to the USA. The thing we all dread is AI-driven drones carrying out extra-judicial killings to promote US commercial interests in other countries. So far there seems to be nothing preventing this except the unwillingness of AI companies to co-operate with the \"defense\" industry. That apparently is no longer the case. reply edgyquant 6 hours agorootparentUnwillingness of AI companies was never a concern or a blocker. Some companies may say they are unwilling, but there are plenty of engineers and corps that are very much willing. reply HHC-Hunter 8 hours agorootparentprevThey have aligned themselves with he people you should exactly NOT trust. Corporations and governments. reply scottLobster 8 hours agorootparentSo... pretty much everyone who matters in our current system. reply uoaei 7 hours agorootparentYour moral value system is on full display... \"might makes right\". reply hombre_fatal 7 hours agoparentprevOutside of the few people who still write 'heh, \"Open\"AI amirite guys? XD' I doubt most people even in tech have much of an opinion about OpenAI in either direction beyond the fact that they make ChatGPT and ChatGPT is awesome. Even the Sam Altman drama is confusing and it's not obvious what to think of it. reply Tempest1981 7 hours agoparentprevHas OpenAI done any PR/outreach explaining the recent changes? It'll be much harder (or impossible) to undo the mistrust after the fact. reply gumballindie 8 hours agoparentprevA reflection of its CEO: experience creating FUD and FOMO, doesn't create much else, dubious ethics. reply corethree 8 hours agoparentprevSam Altman. There was an attempted fix and that fix failed. Evil prevailed because of public misunderstanding and mob mentality. Be real, who else would permit this? Paul Graham fired him from ycombinator. I'm pretty sure if the public didn't support Sam he wouldn't have been reinstated. reply Jackson__ 8 hours agorootparentI think they failed due to not having the courage to admit why they removed Altman. Their messaging was extremely weak and uncertain, a complete failure to communicate anything of value, until everyone was rallying against them. reply corethree 7 hours agorootparentSam Altman was more charismatic and his charisma overshadowed their messaging. That's the type of person who becomes leader.. Not people who make good decisions but people who are charismatic. There is no clear marker that a evil person is evil until the crime is committed, but you can know someone is not a good person simply by knowing them and talking to that person. It's clear they knew what kind of person Sam Altman was but they had no clear marker because there is no crime, yet. Sam was fired from ycombinator by Paul Graham. That should be enough evidence something is not right with his character, but his firing was kept private and not publicly advertised out of respect. reply wtcross 6 hours agoprevNicolas M. Chaillan, Former U.S. Air Force and Space Force Chief Software Officer (CSO), has been running Ask Sage for a little over a year now. It's a very clever way to benefit from relationships and timing the market on Generative AI buzz. It's not that hard to see where this could go for Ask Sage. https://www.asksage.ai/team reply tavavex 5 hours agoprevI'm confused - what exactly does the Pentagon have to gain from OpenAI's products? Their front-runners are an LLM and an image generator, and I'm not sure what large-scale use case the US Department of Defense can have for either of these. reply runlaszlorun 4 hours agoparentI served many moons ago and only watch from afar these days but here’s my two cents. And if anyone has a more direct take than I feel free to correct me here… But it seems that the military these days is enamored by the attention that the silicon valley tech world has gotten over the last couple decades. And its true that the military can be ridiculously inefficient and could use more ‘lean startup’ in its procurement process. But I don’t think they’ve seen enough hype cycles to be able to filter useful tech from hype/hysteria. For example, I recently saw a senior Army officer talk about the need to incorporate AI decision making tools for senior leaders. Which seems to me to be the absolute last thing you’d want. A technology that needs large training data sets is diametrically opposed to the military profession where the classic mistake is to be ‘fighting the last war’ instead of the current situation. Not only are there differences from conflict to conflict and situation to situation, but you’re going up against a human opponent whose lives literally depend on doing the opposite of what you expect and is continually evolving. Another example I think would be the Army building out its Cyber Command while being caught completely flat footed with electronic warfare (jamming, spoofing, etc.) Having an adversary hack into your network is a huge concern for a corporation or against foreign espionage. But ignores the fact that TCP/IP ports and zero days matter a lot less when someone switches on a 10kW jammer, cuts a fiber optic cable, or takes out the power staion and generator you were relying on. Put another way, the upper layers of the ol’ network stack aren’t worth a lot if you can yank the physical layer away. So regarding AI, I’m sure there are useful applications but I’m doubting much of the use of the term in a military context. Drones and drone warfare? Sure. But that may or may not require anything that should really be called AI. reply Trapais 3 hours agoparentprevPropaganda. DoD already uses hollywood for propaganda: say nice things about Uncle Sam, let America save the day once again, and Uncle Sam will let you play with his toys. Now they have access to tool that can write very smart comments. If you think propagandists will not use SoTA LLM for propaganda, either you are fool or they are(They probably did it already anyway). General writing. Want to implement a new rule? Ask GPT to reword it so even idiots can understand. Ask if it contradicts existing rules(they should have similar systems already, but GPT is smart). Combination of above. \"I want to develop a gas chamber. How do I announce it to the public so it looks like I am doing humanity a favor?\" Possible reaction. \"I developed a gas chamber and announced it as 'Overpopulation relocation centers'. How will democrats/republicans will react to it?\" Targeted writing. \"I developed a gas chamber and announced it as 'Overpopulation relocation centers'. It seems democrats/republicans don't like it. How do I reword it so they do like it?\" Deep fakes, image. \"Terrorist Terro Rist wants Inno Cent to be dead. Create an image of Inno Cent with a bullet hole in his forehead so Terro will go celebrate a victory and we'll shoot him for real, here's a photo\" Faking text. \"Here's writings of Terro Rist. Write 'meet me at 7:00 at central square in his style, keep using his punctuation, do the same grammar errors, etc\" Useless Voodoo GPT is not Good For but Still Will Be Used Because They Have Access to It. \"Here is a photo of Inno Cent who was shot dead. What gun could have been used?\" Spying. \"We want to see all requests coming from China or Russia paired with IP\", \"All requests with words 'President' and 'Murder' should be forwarded to this email at once\", etc They will find several uses. reply tavavex 3 hours agorootparentA lot of these look like very big stretches. Most of the things you've listed are either already being done by humans or is something that an LLM simply can't improve upon. Even if you had the best writers and most complex AI in the world, you can't really sugarcoat a gas chamber past a certain point. Fake writings and fake imagery are already easily created by humans, but as far as I know, it's not practiced widely because it's known that you can't easily trust wild claims if it's not backed by real people. Digital image editing opened up the doors to creating any type of misleading imagery, but nothing has really changed since then. And I have no idea how any of the \"all IPs from Russia\" or \"scan everything on the internet\" could even work. If the US government has infrastructure that can scan for this kind of info, they don't need AI to use it. If they don't, well.. an LLM isn't gonna magically make every bit of internet traffic accessible to them. reply supriyo-biswas 2 hours agorootparent> And I have no idea how any of the \"all IPs from Russia\" or \"scan everything on the internet\" could even work. XKEYSCORE[1] probes were already a thing in ~2010. It just needs a LLM integration and additional search capabilities. [1] https://en.m.wikipedia.org/wiki/XKeyscore reply somenameforme 4 hours agoparentprevThe military thinks they can predict the outcome of battles using tech. They already do extensive sim-gaming stuff for such, and an LLM would fit into these things nicely enough. Whether it actually adds meaningful value matters much less than whether the people paying for it think it can be made to add meaningful value. The other obvious purpose is propaganda, foreign and domestic. reply runlaszlorun 4 hours agorootparent> The military thinks they can predict the outcome of battles using tech. Ugh. Do they really? Ok, I’m more concerned now. I’m curious where I can find more about the sim-gaming you mentioned. I know there are all kinds of simulators for training as well as wargaming but wasn’t aware of stuff used for modelling outcomes. Then again, I don’t have any involvement with that stuff professionally. reply somenameforme 3 hours agorootparentThere are multiple vendors, but the big one that I am aware of is called KORA. It was used in planning the counter-attack in Ukraine and you can find numerous references to it, such as in this [1] article. But this and classical war-gaming and very related. The entire point of war-gaming (beyond training) is to try to predict the outcome of a conflict. [1] - https://www.washingtonpost.com/world/2023/06/04/ukraine-nato... reply kridsdale1 2 hours agorootparentprevWar gaming goes way back. Napoleon did it. Chess is a form of it and it’s thousands of years old. Dungeons and Dragons and thus all computer RPGs have their direct lineage to military gaming simulations. reply InCityDreams 1 hour agorootparentprevTom Scott on yt drove a tank simulator. reply kelipso 5 hours agoparentprevOne example would be better communication between soldiers and robots out in the field. reply primitivesuave 10 hours agoprevThe CIA probably realizes it doesn't need to fund rebel groups [1] or use local journalists/clergy [2] to instigate a regime change these days - they can just flood a target country's social media with AI-generated propaganda. 1. https://en.wikipedia.org/wiki/Iran%E2%80%93Contra_affair 2. https://www.intelligence.senate.gov/sites/default/files/hear... reply iknowstuff 9 hours agoparentI sure hope so, Russia has 100% been doing it, with multiple documented „botfarms” (more like fake identity verification farms) reply frogamel 9 hours agorootparentI've noticed a huge surge in negativity and pessimism on English-language social media within the last year or so, roughly corresponding with to the spread of LLM tech. I do wonder whether these people are mostly just bots. reply rightbyte 24 minutes agorootparentTwitter and Reddit seem to be filled with bots and shills nowadays. I think it is a consequence of actual users leaving for private groups (Discord, Telegram, Whatsapp) or small forums. But ye surely LLMs are increasing the bot count. reply kornhole 8 hours agorootparentprevYes Russia did have some accounts and bought ads on US social media some years ago, but analysis showed it had marginal effect. They are outsiders on US run social media platforms. The real power is in the hands of those running the social media platforms that can suppress or send viral what they want with a few adjustments. That certainly happened and continues still now aided by AI. reply screamingninja 5 hours agorootparent> analysis showed it had marginal effect What analysis? Care to share some of your insights? reply SSLy 8 hours agorootparentprevThey have had impact outside of USA. reply reducesuffering 6 hours agorootparentprev> some years ago sweet summer child... Russia is fighting an expansionary offensive war and is full throttle instigating division in the entire Western mediascape. Here's a small taste from a few months ago: https://www.npr.org/2023/08/29/1196117574/meta-says-chinese-... reply EGreg 9 hours agorootparentprevTikTok algo can do it more subtly through prioritizing other people’s media, selectively amplifying legitimately generated content reply Waterluvian 9 hours agorootparentThere was a period of time last year where TikTok really wanted me to see the Canadian government as an unmitigated disaster. Just endless scrolls of random mouth frothers screaming about it. Not opining on the matter specifically; just that TikTok had a very clear opinion on what I ought to think. reply FpUser 9 hours agorootparentLOL. Canadian government does not need any TikTok. Just visit downtown Toronto, hospitals etc. reply Kerb_ 9 hours agorootparentComments like this, doing the exact thing they said they aren't open to, will surely convince them astroturfing doesn't exist! Being serious, comments like this make me trust the Canadian government more. It sounds like how American conservatives discuss cities they've never been to. And they tend to be surprisingly good inverse indicators when it comes to actually being in the cities. I'd bet Canadian hospitals are better than what I've got now, solely going off of this discussion. reply Waterluvian 9 hours agorootparentIt’s not really about deciding if one trusts the government or not. That’s an oversimplification of one’s civic responsibility to study the issues and make informed decisions. The main harm astroturfing does is convert everything into black and white oversimplifications. It turns people into unthinkers. They pick a side and then act like it’s some sort of battle against the other side. reply Kerb_ 8 hours agorootparentOf course! I didn't actually rewrite my framework just to counter someone on the internet. I said that more to call out the low effort response as fitting an extremely predictable pattern of uninformed people trying to pose their drive-by hot take as fact, and the unreliability of said information in practice reply FpUser 6 hours agorootparentprevI do not pick sides. The only things I see are changes in quality of life. One must be blind not to see where it is going. As for \"informed decisions\" - I do make those based on what I see and do not need help from TikTok or anything else. HN is about the only social media I participate in when I need a quick brake from working on computer. reply FpUser 6 hours agorootparentprev>\"It sounds like how American conservatives discuss cities they've never been to.\" I live in Toronto since 92 and I can compare back then and now so don't assume things. reply Waterluvian 9 hours agorootparentprevYeah I’m not interested in discussing the subject matter here. reply mschuster91 9 hours agorootparentprevRussia doesn't need fancy \"AI\" to do this. Human operators and a few well-designed scripts make botting very easy. reply johnnyworker 7 hours agorootparentprevCreating counter-spam doesn't \"fix\" that, it just helps destroy the public space. > If everybody always lies to you, the consequence is not that you believe the lies, but rather that nobody believes anything any longer. [..] And a people that no longer can believe anything cannot make up its mind. It is deprived not only of its capacity to act but also of its capacity to think and to judge. And with such a people you can then do what you please. -- Hannah Arendt, https://www.nybooks.com/articles/1978/10/26/hannah-arendt-fr... And as Snowden tweeted January 11th: > Institutions are burning the public's faith in them at the precise moment in history when we have developed the capacity to replace them with algorithms. > A revolution is coming, and if you thought human judgment was bad, just wait until you see what replaces it. I don't get why the standard should be the deception of \"the other\", who is also crooked. Why not do good things one can be honest about? Instead of maintaining several identities and narratives and a constant uphill battle in quicksand of one's own making, one could just build on top of previous achievements. It would be much more effective, it would make the US rich and respected in the world. But it would make some individuals less insanely wealthy [0], so that's not an option. It's like the drunk looking for the key under the lamp post instead of where it was lost. [0] Oxfam just reported that the 5 richest people doubled their fortune in the last 3 years while 5 billion got poorer. reply FpUser 9 hours agorootparentprevRussia also drops people out of the windows. Should we (the supposedly humane West) start doing the same just because they do? reply hsuduebc2 9 hours agorootparentThey can do whatever they want to themselves. We are talking about attacking others. You can't ignore direct hit into face. reply FpUser 6 hours agorootparentThe talk was: >\"The CIA probably realizes it doesn't need to fund rebel groups [1] or use local journalists/clergy [2] to instigate a regime change these days\" To me it sounds like instigating regime changes all over the places. The US is famous for doing so and then leaving behind multiple victims. reply autoexec 9 hours agoparentprevI don't know why they'd need OpenAI though. Our government's three letter agencies must have a one hell of a data set to train their own AI on. reply AYBABTME 9 hours agorootparentThe best brains don't go work for the CIA or the DoD, or at least they don't stay. It's not an environment in which you can strive and do your best work. Nothing against what they do, I'm ex-military, but the culture in these institutions just doesn't cut it for the Vibes required. In addition, one's career is much better served working in the free market than submitting yourself to the government's arbitrary levelling/career ladder. The government is just structurally incapable of attracting this type of top A+ talent at scale. They get and keep smart people but not the smartest people. For this, they absolutely need to use industry relations. Just picture yourself in a position to work at either OpenAI, or GE, the DoD or GM. The average tech worker will much prefer the hip SV company than the old quasi-government dinosaur corporation or the government's agency. reply kridsdale1 2 hours agorootparentThe number one reason hackers don’t work for the DoD is they won’t be able to do any kickass drugs anymore. Hackers love drugs. The number 2 reason is the pay is shit. reply jart 6 hours agorootparentprevI'm sure the environment is fine, it's just a question of economics. The comp for mere software engineers these days is more than the commander in chief gets paid. Usually what organizations like USDS try to do to attract coders is get them interested in a \"tour of duty\" where they rough it for a few years on a major general's salary before going back to their old jobs generating text, managing cat videos, and getting people to click on ads. It's a busted system. reply mholm 9 hours agorootparentprevHell of a dataset, but less of the talent. Takes a very specific type of person to get to the front of the AI field, then take a government salary using your knowledge for (what could be) war and surveillance, likely against any public interest in alignment. reply leodriesch 9 hours agorootparentprevI’d say talent? Outside of OpenAI no team has been able to release a model as capable as GPT-4, and I’m unsure if the CIA has been prioritizing LLM experts in their hiring. reply idopmstuff 10 hours agoparentprevI would wager a whole lot of money that the CIA is very much on top of this one. reply iwontberude 9 hours agorootparentWorld peace incoming reply xbar 9 hours agorootparentPhew. I was getting scared that someone would use it to cause instability. reply petre 6 hours agoparentprevI don't know how that works in Yemen or Gaza. Do they have any networking infrastructure left? reply solarengineer 5 hours agoprevAsimov, Daneel, and Giskard would be sad. A positronic brain could be argued to be AI. Neither the first nor the second laws of Robotics would remain unbroken once AI is applied for military purposes. reply readyplayernull 6 hours agoprevMeanwhile civilian use of AI will be criminalized: https://news.ycombinator.com/item?id=39009779 reply binsquare 8 hours agoprevThey haven't changed their principles page yet: https://openai.com/safety-standards ``` Minimize harm We will build safety into our AI tools where possible, and work hard to aggressively reduce harms posed by the misuse or abuse of our AI tools. ``` And I think they can still do that. If they corner every big usecase while they are the top ai models - they get to set the standard and define safety for all upcoming ai use cases. At this point, the ai genie is fully out of the bag. If not openai, it would have been some other company... reply bigstrat2003 7 hours agoparent> If not openai, it would have been some other company... This does not justify anything and never has. I'm not even saying that what OpenAI is doing is bad (I don't think it's inherently bad to work with the military). But if one believes that it is bad, then \"well someone was going to do it so it may as well be them\" doesn't remotely hold water. reply Frost1x 7 hours agorootparentDoesn’t it, though? Especially in the context of military, if some potential enemy has no moral qualms using new technology to kill you, doesn’t it mean you’re somewhat forced to provide a defense or die a martyr? This is sort of what happened with nuclear weapons and the race. The strategy developed afterwards was mutually assured destruction but that probably won’t work here. reply bigstrat2003 5 hours agorootparentNo, it doesn't. If your principles are to mean anything, you can't give them up just because \"that other guy doesn't have principles\". Either stick by your principles, or don't claim to have them. reply SantalBlush 5 hours agorootparentprevOne issue is that \"Someone else was going to do it\" is not a statement of fact, it is a counterfactual. At best, it is a guess, and at worst, it is a lie. But either way, it can't be proven. reply binsquare 4 hours agorootparentprevIf someone was going to do it, wouldn't you choose to have some control rather than let someone else (a possibly worse option) at the wheels? reply TaylorAlexander 7 hours agoparentprevThis is exactly the reasoning that every entity involved in doing this will say and is what leads to an arms race. reply NotSammyHagar 6 hours agoparentprevPlease save off the page for the future. We were so naive in 2024. reply porkbeer 7 hours agoparentprev'If I don't, someone else will' is one of the most defeatist, morally bankrupt attitudes known to man. But hey, if I don't point out how shady and shitty it is, someone else will. reply petre 6 hours agoparentprevUntil someone argues they're not principles, just mere guidelines. reply racketcon2089 10 hours agoprevNothing says \"we're concerned about AI risk and harm to human life\" quite like partnering with the military-industrial complex. It's even better timing to do this while the Pentagon is providing material and intelligence support to a government whose decisonmakers' public statements are explicit admissions of intent to commit war crimes and also convey genocidal intent. Glad we spent all that time talking about AGI and Roko's Basilisk, those should be top of mind always, never the current actions of the humans in charge at both OpenAI and the US government. reply seatac76 5 hours agoprevThis was always coming, there is no better permanent customer than the DoD. Opens all kinds of revenue doors. reply karmasimida 8 hours agoprevDARPA will find its way one way or another. You can virtue signaling, but there is no reality you can stop them obtaining the technology. Besides your enemy will not wait. reply uoaei 7 hours agoparentNo doubt they already have numerous internal initiatives, OpenAI could simply not exist and DARPA would figure it out in a couple years. OpenAI's public offerings are already pretty successfully used by adversaries. It seems like just creating a technology such as this and shooting it out into the world has only created more potential for harm across the board. reply kridsdale1 2 hours agorootparentGoogle hasn’t managed it and they are far far better staffed and funded than any government or military program. reply cryptozeus 5 hours agoprevPLTR was doing this anyway, this is not a news. Microsoft has lots of contracts with Gov. reply WhackyIdeas 6 hours agoprevGreat, OpenAI now part of the USA war machine. Great for humanity as a whole I’d say!!! reply scotty79 1 hour agoprevThe arguments here about how US can't allow itself to be outpaced by China eeriely remind me of dialogs from Dr Strangelove where they first discuss \"nuclear gap\" but after the nuclar weapons are applied and the world mostly ends they smoothly transition about worrying about \"mineshaft gap\". reply tim333 3 hours agoprevMusk last year: >OpenAI was created as an open source (which is why I named it “Open” AI), non-profit company to serve as a counterweight to Google, but now it has become a closed source, maximum-profit company effectively controlled by Microsoft. Not what I intended at all. It's quite impressive how quickly it's morphed from do good non profit to part of the military industrial complex. reply kridsdale1 2 hours agoparentIt’s been pretty fun as a fan of market dynamics to observe the Great Eye of Power catch on with attention and then investment and then securing the Ring for itself. reply avs733 6 hours agoprevI think we are slowly realizing Sam won and we all lost reply newsclues 6 hours agoprevChatGPT generated PowerPoints are coming. reply porkbeer 7 hours agoprevThe Buterian Jihad approaches ever faster. reply cmrdporcupine 6 hours agoparentHunter-seekers, at least. reply demondemidi 4 hours agoprevWOPR reply monkaiju 4 hours agoprevUnsurprising, ghouls... reply unicornmama 5 hours agoprevPrinciples always follow business interests. reply bogomipz 5 hours agoprev>\"OpenAI is working with the Pentagon on software projects, including ones related to cybersecurity, the company said Tuesday,...\" Can someone say how either of OpenAI's products - GPT and DALLE can be applied to cybersecurity? Or does OpenAI simply have enough extra employee and research headcount when Pentagon money comes calling? reply KaoruAoiShiho 10 hours agoprevThis is the sort of thing that becomes possible after dropping the ethical members of their board. reply fourside 9 hours agoparentI wonder how the OpenAI employees feel about this after their full throated support for reinstating Altman and the shuffling of the previous board. reply paxys 9 hours agorootparentI'm $ure they feel okay about it reply ttul 9 hours agorootparentprevNothing a few million can’t make better with some effective altruism sprinkled about. reply scarmig 9 hours agorootparentprevIt's a very comfy feeling to fall asleep on top of piles of cash. reply Jackson__ 7 hours agorootparentprevI'd assume some of them would feel pretty bad. And if the board had actually said that this was the reason they ousted Sam Altman, said employees would have supported the board. But none of that happened, and so the employees got to choose between a charismatic man with a impressively quickly put together plan, and a group of headless chickens running in circles. reply sebzim4500 9 hours agorootparentprevI've noticed that, in my circles at least, after Russia's invasion of Ukraine working in defence is no longer considered inherently immoral. Maybe they don't see anything wrong with it either. reply rightbyte 19 minutes agorootparentI don't think people change that fast. The main difference is that being anti-war seems to have come to be associated with a social cost, I think. reply wly_cdgr 2 hours agoprevLol, most predictable heel turn of all time reply solarpunk 8 hours agoprevOpenAI sure doesn't seem to have much commitment to upholding any principles they set for themselves, do they? reply taberiand 7 hours agoparentProfit is the only principle that corporations commit to, everything else is PR. reply porkbeer 7 hours agoparentprevPriciples are fungible marketing for corporations. reply 122 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenAI has reversed its ban on collaborating with the Pentagon and is now working on cybersecurity software projects with them.",
      "The company will still maintain its ban on developing weapons.",
      "This change in policy reflects Silicon Valley's growing acceptance of working with the military, but there are concerns about the risks associated with integrating AI into warfare, such as the potential for generating fake information and the blurring of the boundary between data analysis and warfare."
    ],
    "commentSummary": [
      "OpenAI's decision to collaborate with the Pentagon and develop military tools has sparked controversy and raised concerns about the company's commitment to its founding principles.",
      "The presence of Chinese talent within OpenAI has raised worries about potential displacement and the influence of classified briefings.",
      "The discussion surrounding OpenAI's decision covers various topics, including ethical boundaries, secrecy, China's advancements in AI, the perception of the US military, national defense organizations, potential misuse of AI technology, the impact of AI on warfare, the role of AI in intelligence and decision-making, and the ethics of using AI in military operations."
    ],
    "points": 272,
    "commentCount": 374,
    "retryCount": 0,
    "time": 1705448274
  },
  {
    "id": 39020258,
    "title": "Canon's Fluorite Lenses: Enhancing Image Quality Beyond Ordinary Glass",
    "originLink": "https://global.canon/en/c-museum/special/exhibition2.html",
    "originBody": "Twitter Facebook 简体中文 日本語 English Special Exhibition Fluorite lenses: Corrective capabilities beyond the limits of ordinary optical glass One crucial material that supports the high image quality characteristic of Canon lenses is fluorite, which is a crystallized form of calcium fluoride. It has long been known that using a fluorite lens in conjunction with a glass lens can reduce chromatic aberration to extremely low levels. However, natural fluorite occurs in small sizes that are suitable only for use in small optical equipment such as the object lenses of microscopes. Canon, in its pursuit of progress in imaging capabilities, was keen on utilizing fluorite in its photography lenses, set out to develop its own technology for forming large, high-purity artificial fluorite crystals using fluorite ore as a raw material. In May 1969, the FL-F300mm f/5.6—the world’s first consumer telephoto lens to employ fluorite lens elements—was released. How fluorite lens elements correct chromatic aberration You might have noticed the outlines of your subjects tinted in a way that resembles a color fringe. This is chromatic aberration, and it can also take the form of a haziness throughout the entire image. As this prevents the subject from being rendered accurately, it needs to be corrected so that the image quality is sharp, clear, and faithful to the scene. The purple fringing at the edges if these branches is a form of chromatic aberration. Chromatic aberration happens because when light passes through a glass surface, the different-colored waves within it (red, green, blue, etc.) are refracted at different angles due to their different lengths, with each color converging at a different focal point. Such aberration is usually corrected by using a combination of concave and convex lenses, which refract the light in opposite directions to each other. Chromatic aberration correction using concave and convex glass lenses However, it is not possible to correct the chromatic aberration on all wavelengths by using ordinary glass. The chromatic aberration, which occurs on certain wavelengths such as red, is called residual chromatic aberration. As the refractive index of the wavelengths differ depending on the type of optical glass, there are limitations to how much residual chromatic aberration can be mitigated depending on the combination and properties of the optical glass. This is where fluorite comes in handy. As fluorite is a fundamentally different material than conventional optical glass, it can be used in combination with glass to correct chromatic aberration more effectively. It is particularly effective on telephoto lenses, where the long focal length exacerbates chromatic aberration. Chromatic aberration correction using a convex fluorite lens and a concave glass lens Canon’s fluorite lens elements incorporate natural fluorite as a raw material, endowing the lenses with low-refractive, low-dispersion properties not possible with glass lenses. Fluorite lenses are also unique in their extraordinary partial dispersion tendencies: the red to green wavelengths are dispersed with the same tendencies as glass, but the green to blue wavelengths are dispersed more than glass. Using a convex fluorite lens element alongside a high-dispersion glass concave lens element therefore eliminates residual chromatic aberration, making possible a lens that produces clear, sharp, and high-quality images. Refraction and dispersion ‘Refraction’ is the phenomenon in which light changes direction when it passes through the surface of a material such as glass. The degree of the directional change is called the ‘refractive index’. As the refractive index varies depending on the color constituents (wavelengths) of the light, each color bends in a different direction. This is known as color dispersion. On optical glass, dispersion occurs at a fixed proportion regardless of the wavelength, whereas on fluorite, dispersion occurs at different proportions for different wavelengths and is known as ‘extraordinary partial dispersion’. The emergence of fluorite lenses and how they improve the image quality of telephoto lenses Fluorite lenses transcend traditional limitations to reduce chromatic aberration to an extremely low level. These lenses have their origins the Canon F Project, which started in August 1966. Canon’s lens developers strongly believed that to create a lens that was performed better than existing lenses, it was first necessary to create a new material, and it was this conviction that drive them to establish the technology for producing artificial fluorite crystals to use in camera lenses. The challenge in producing artificial fluorite crystals lay in the crystallization. The term “glass” was originally used to describe the state of a material. Being non-crystalline, it consists of atoms fixed in random positions--simply melting glass allows it to be processed into different shapes. On the other hand, fluorite is a crystalline substance, and its constituent atoms must be arranged in a specific configuration for it to crystalize. To pulverize natural fluorite, purify it, and then restore the exact atomic structure to recrystallize it into a size large enough for application in a camera lens is a near-impossible feat. Therefore, the developers had to ensure a precisely controlled vacuum environment where the temperature is kept at least 1,000℃, so they designed an apparatus to artificially produce large crystals with high purity In 1968, two years after the F project started, Canon finally managed to overcome these obstacles to successfully form artificial fluorite crystals large enough to be used in camera lenses. The stones on the left are natural fluorite crystals. They are green and purple due to impurities within the crystals. At the middle is an artificial fluorite crystal ingot produced by Canon. When heated, natural fluorite glows with a dreamy light that resembles that of fireflies, which is said to be the reason behind its Japanese name, hotaru ishi, literally “firefly stone”. Just like how fireflies require clean water to live, only pure fluorite can “shine” as photography lenses. Another challenge in fluorite lens production is its polishing. As fluorite is softer and more delicate than glass, the same methods used for polishing glass are not suitable. Therefore, Canon developed a special technology for polishing fluorite lenses that requires up to four times longer than the time needed to polish ordinary optical glass. This technology was successfully commercialized the following year, in May 1969. The first lens to employ fluorite lens elements was the FL-F300mm f/5.6 (released in 1969). The bright green line, meant to evoke the image of the glow of fluorite, indicates that the lens features a fluorite lens element. The FL-F300mm f/5.6, whose successful commercialization was achieved by transcending many challenges, was highly acclaimed for is vivid, high-contrast rendering, and became widely used in such professions as photojournalism. Canon subsequently improved on is high temperature vacuum, temperature control, and polishing technologies, enabling the use of fluorite lens elements in many more lenses. Canon remains committed to pursuing the highest image quality for its telephoto lenses. The fluorite lens production process While the grinding and polishing processes may seem identical for all kinds of optical glass, each stage of the fluorite lens production process requires slow, meticulous attention to detail. 1. Raw materials The raw material for fluorite lenses is naturally occurring fluorite ore. 2. Pulverization and refinement The raw fluorite is pulverized and refined to remove impurities before being poured into a graphite crucible that does not melt easily. 3. Crystallization The crucible is placed in a crystal-growing apparatus with a heater on top and heated to 1,400℃. After the raw fluorite has melted, the crucible is gradually lowered, allowing crystallization to occur starting from the bottom of the crucible. 4. Annealing The annealing process removes strains that occur inside the crystals formed. Strain that lead to cracks are removed by heating the crystals to a high temperature insufficient to melt them, and then slowly cooling them to room temperature over a long period of several weeks. 5. Trimming and rough processing The unnecessary parts of the crystal surface are trimmed off, and the crystal is rough-processed to the required size. The interior of the crystal is inspected for anomalies. 6. Grinding The top and bottom surfaces of the crystal are ground into a spherical shape with a surface that resembles frosted glass. 7. Polishing The surfaces of the crystal are polished with a pellet made from coagulated polish until they are semi-transparent and meet the specified dimensions. Finally, a special polish is used to remove fine scratches. 8. Vapor deposition Coating material is heat-evaporated under high vacuum conditions up to one-millionth to one-hundred millionth of one unit of atmospheric pressure. This forms a thin film over the polished lens. 9. Completion An experienced technician inspects purity using an interferometer. Only lens elements that pass the inspection are sent to be assembled in lenses. Lenses that employ fluorite lens elements (as at May 2021) Since the FL-F300mm, Canon has produced 39 more lenses that employ fluorite lens elements. As fluorite lens elements not only correct chromatic aberration but also contribute to reducing the size and weight of products, they are proactively used in large telephoto lenses. These lenses are much beloved by many photographers who demand high image quality at super telephoto focal lengths, including not only professional sports photographers and photojournalists, but also enthusiasts who photograph subjects such as wild birds, trains and aircraft. RF600mm F4 L IS USM (released in 2021) RF lenses that employ fluorite lens elements (as at May 2021) Product name Release year No. of fluorite lens elements FL-F300mm f/5.6 1969 2 FD300mm f/2.8 S.S.C. fluorite 1975 1 FD500mm f/4.5L 1979 1 New FD300mm f/2.8L 1981 1 New FD100-300mm f/5.6L 1985 1 FL-F500mm f/5.6 1969 1 FL300mm f/2.8 S.S.C. fluorite 1974 1 New FD500mm f/4.5L 1981 1 New FD80-200mm f/4L 1985 1 EF100-300mm f/5.6L 1987 1 EF300mm f/2.8L USM 1987 1 EF50-200mm f/3.5-4.5L 1988 1 EF600mm f/4L USM 1988 1 EF500mm f/4.5L USM 1992 1 EF1200mm f/5.6L USM 1993 2 EF400mm f/2.8L II USM 1996 1 EF100-400mm f/4.5-5.6L IS USM 1998 1 EF300mm f/2.8L IS USM 1999 1 EF400mm f/2.8L IS USM 1999 1 EF500mm f/4L IS USM 1999 1 EF600mm f/4L IS USM 1999 1 EF70-200mm f/4L USM 1999 1 EF400mm f/4 DO IS USM 2001 1 EF70-200mm f/4L IS USM 2006 1 EF200mm f/2L IS USM 2008 1 EF800mm f/5.6L IS USM 2008 2 EF70-200mm f/2.8L IS II USM 2010 1 EF300mm f/2.8L IS II USM 2011 2 EF400mm f/2.8L IS II USM 2011 2 EF500mm f/4L IS II USM 2012 2 EF600mm f/4L IS II USM 2012 2 EF200-400mm f/4L IS USM Extender 1.4x 2013 1 EF100-400mm f/4.5-5.6L IS II USM 2014 1 EF400mm f/2.8L IS III USM 2018 2 EF600mm f/4L IS III USM 2018 2 EF70-200mm f/2.8L IS III USM 2018 1 EF70-200mm f/4L IS II USM 2018 1 RF400mm F2.8 L IS USM 2021 2 RF600mm F4 L IS USM 2021 2 Special Exhibition ▲ Contact Us Terms of use Privacy Notice © Canon Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=39020258",
    "commentBody": "Fluorite lenses: Corrective capabilities beyond ordinary optical glass (global.canon)263 points by neilv 11 hours agohidepastfavorite82 comments numpad0 8 hours agoI still haven't come across a summary I liked so I'll try: Refractive index of a material, typically ~1.5, is not a fixed single number for material. Rather it is wavelength dependent, because diffraction is of course quantum interference thing strengthening at new directions and canceling out elsewhere. Wavelength-index plot shows some sort of exponential or asymptotic, monotonically decreasing curve from UV towards IR. This means any convex lens always has a higher than intended magnification at blue, higher still at green, okay at red, and only technically right at Sodium vapor yellow, creating \"aberrated(NOT after Ernst Abbe)\" color-shifted image at its focal point. To counter this, convex and concave lenses built from different chemical compositions that show different rates of decreasing indices are used, such as Schott BK7 and F2, so that extra positive magnification for blue at first convex lens cancels out with extra negative power for blue at following concave lens, and so on. The chain of lenses can be continued to cancel out effects at as many additional wavelengths, as well as side effects and other types of imperfections, as desired. Significance of Fluorite or CaF2 crystals in this context is, this material shows a completely flat curve on that wavelength-refractive index plot, referred to as \"abnormal dispersion\". It naturally focuses all colors across visible spectrum to a same point, skipping over a lot of lens and lens canceling out. Challenge is scaling out camera-sized crystals of Calcium and Fluoride with optical clarity is hard, which Canon has been trying for a few decades. reply kpozin 7 hours agoparent> creating \"aberrated(after Ernst Abbe)\" color-shifted image at its focal point This is a clever bit of folk etymology [1], but aberrate is derived from the Latin verb aberro, meaning to wander or stray [2]. [1]: https://en.wikipedia.org/wiki/Folk_etymology [2]: https://en.wiktionary.org/wiki/aberro#Latin reply riperoni 4 hours agoparentprevYeah thank you, that summary is better than the article. The definition of refracitve index in the article is also just wrong, since it is simply not an angle. It can be calculated from incidence and refraction angles of the light beam - very different. See https://en.m.wikipedia.org/wiki/Snell%27s_law To add to your answer, the refractive index is not just wavelength dependent, but can also be depending on the polarization of light, leading to birefringence: https://en.m.wikipedia.org/wiki/Birefringence reply Balgair 3 hours agorootparentFor HNers and CompSci people, optics is a notoriously difficult field and much more frustrating. If you break up the Nobel Prizes a bit differently, then the filed of Optics becomes the most dominant. So very many breakthroughs in science are because of some new optics method. Mostly in the bio/chem fields, it's about gaining a new form of 'contrast' (very broadly defined). People have spent decades trying to align some little crystal just the right way. Or they did it in their living room with cardboard in a weekend. It's a frustrating field. One fun thing to remember about lenses are that they aren't really light bending thingys, but more accurately a lens is a Fourier transformer. Of a sort. Again, optics s frustrating. One fun thing for the more matrix-ly minded are Mueller Matrices. Most modern optics SW is based on this calculus, though it goes a lot further nowadays. Also, most developments in optics are all about the little exceptions that Mueller matrices have. Still, a good little thing to read about, if interested: https://en.wikipedia.org/wiki/Mueller_calculus reply kridsdale1 3 hours agorootparentI have been way in to Nikon and Canon lenses as well as DSP for like 20 years now, and have a degree in EE and did a ton of quantum, and I never had the insight that a lens is a physical EM Fourier transformer. Cheers for that. reply tempay 2 hours agorootparentDoes anyone have an explanation of this insight? In principle I have all the pre-requisite understanding but I’m struggling to connect it. reply plasmatorch 1 hour agorootparentAside from cranking the math, here's how I think about it: in the far field of a small aperture, the electric field has spherical phase (think expanding circles), and the field distribution is the Fourier transform of the aperture. A lens is an element that adds spherical phase - a plane wave passing through a convex lens now has a spherical phase distribution. So the lens focal point is now the tiny aperture in that system, and since the math works out the same no matter which way the light is going (reciprocity), the focal point is the FT of the field at the input of the lens. Goodman is great, Hecht and Zajac covers more fun with optics at an intro level. reply kridsdale1 2 hours agorootparentprevI suppose my interpretation of his message is that if you think of a “ray of light” that is not a mono frequency laser as an “input signal”, a convex lens will smear it out in to its constituent component frequencies. From that perspective you can analyze the original signal (aka color) with a geometrically/spatially separated spectrum of values. It was a single spatial point of ray intersection with your sensor or eye. You’d need color filters/retinal cells to pick apart the frequencies in the complex waveform. After rainbow separation, the components are spread across multiple sensors giving you a frequency domain view. reply sirpilade 1 hour agorootparentprevYes the explanation is diffraction. As light passes through a lens, diffraction acts in similar way as a light through a small pinhole. Incidentally, pinholes and apertures are low pass filters. Some more info here Miles V. Klein, Thomas E. Furtak - Optics 2nd ed, Wiley Joseph W. Goodman - Introduction to Fourier optics, W.H. Freeman reply Micanthus 1 hour agorootparentprev> a lens is a Fourier transformer. Of a sort. Can you expand on that? Or have some reading for that? I guess it makes sense, light is a wave and anything even vaguely to do with waves seems to end up with Fourier transforms, but still I'm curious about the details reply infogulch 1 hour agorootparentHow about a practical demonstration of optical Fourier transforms? https://www.youtube.com/watch?v=Y9FZ4igNxNA reply kragen 6 hours agoparentprevsome of this is correct 'naturally focuses all colors across visible spectrum to a same point' would be no dispersion, not 'abnormal dispersion'. abnormal dispersion (usually called anomalous dispersion) is when the refractive index increases with increasing wavelength, instead of decreasing as in normal dispersion https://en.wikipedia.org/wiki/Dispersion_(optics)#Material_d... if you had a material with no dispersion you could just make a lens out of it and avoid chromatic aberration, but since you don't, you need to use the dispersions of different materials to cancel it out in the way you describe fluorite doesn't have anomalous dispersion in the visible spectrum, it just has low dispersion canon has evidently successfully been scaling out camera-sized crystals of calcium fluoride since the 01960s. other companies have too actually; https://en.wikipedia.org/wiki/Fluorite says > In the laboratory, calcium fluoride is commonly used as a window material for both infrared and ultraviolet wavelengths, since it is transparent in these regions (about 0.15 µm to 9 µm) and exhibits an extremely low change in refractive index with wavelength. Furthermore, the material is attacked by few reagents. At wavelengths as short as 157 nm, a common wavelength used for semiconductor stepper manufacture for integrated circuit lithography, the refractive index of calcium fluoride shows some non-linearity at high power densities, which has inhibited its use for this purpose. In the early years of the 21st century, the stepper market for calcium fluoride collapsed, and many large manufacturing facilities have been closed. Canon and other manufacturers have used synthetically grown crystals of calcium fluoride components in lenses to aid apochromatic design, and to reduce light dispersion. This use has largely been superseded by newer glasses and computer-aided design. As an infrared optical material, calcium fluoride is widely available and was sometimes known by the Eastman Kodak trademarked name \"Irtran-3\", although this designation is obsolete. sodium, fluorite, calcium, and fluoride are not brand names or other proper nouns and thus should not be capitalized in english as they are in german reply jjgreen 9 minutes agorootparentBut English and German should :-) reply gregschlom 7 hours agoparentprev> aberrated(after Ernst Abbe) Are you saying the word aberration comes from Ernst Abbe's last name? Because it doesn't, it comes from latin. https://www.etymonline.com/word/aberration reply numpad0 7 hours agorootparentMy mistakes. I stand corrected. reply IshKebab 1 hour agoparentprevDiffraction isn't a quantum effect. Classical waves diffract. reply k310 10 hours agoprevI looked for a simple explanation of why dispersion matters, and this seems helpful: https://www.targettamers.com/guides/apochromatic-lenses/ in the context of apochromatic lenses: those that are optimized for three different wavelengths of light, not just two, which an achromatic lens does. In the old days, calculations were done by hand, not that this is a big deal, but the big deal is that some really outstanding lens designs were made this way. Computers make the optimization extremely fast these days, but the calculations rely on the properties of the elements, which also vary in cost, durability and so on. So the computer can’t optimize for a continuous range of refractive indices and dispersions, only discrete real-world ones that the glass makers list, and which are specified (or not) to the program by the designer. Fluorine also has special properties as a coating. https://www.digitalcameraworld.com/features/this-is-why-your... reply foobar1962 8 hours agoparent> Computers make the optimization extremely fast these days... A lot of computer design is now aimed at optimising for tolerances in lens elements and mechanical housings to reduce precision necessary when assembling them: they can drop elements into the tube and ship them off with little or no calibration: this is done particularly with kit lenses which are price sensitive. reply k310 4 hours agorootparentI did error budgeting for environmental effects on optical systems at Itek. Glad to see so many optics folks here. I wonder who they are, but won't ask. ;-) reply foobar1962 4 hours agorootparentI'm not a lens designer, but want to play one on TV. reply foobar1962 8 hours agoparentprev> in the context of apochromatic lenses: those that are optimized for three different wavelengths of light, not just two, which an achromatic lens does. Achromatic lenses (corrected for blue and green) was acceptable for black and white orthochromatic film and plates which are only sensitive to blue and green. Even with panchromatic b+w film, achromatic lenses are usually satisfactory, but the chromatic aberration becomes visible with colour film. Aprochromatic lenses are corrected for blue, green and red. Lenses can also be corrected for broader spectrums that include UV and IR: Nikon made such a lens – the UV 105mm f4.5 – for technical/scientific applications. reply nimish 8 hours agorootparentZeiss and Hasselblad sold some superachromat lenses as well. reply buildbot 7 hours agorootparentMamiya did as well, for the RZ67. Or at least, their charts claim they are corrected well into 700nm+: https://ianbfoto.com/downloads/Mamiya%20RZ67/Mamiya%20RZ67%2... They say no focus adjust is necessary for the APO lenses for IR, B&W, and color film. reply lobochrome 9 hours agoparentprevThe linked article perfectly explains it though no? reply readyplayernull 9 hours agorootparentNever bother someone who has just came out of a rabbit hole. reply drittich 9 hours agorootparentI laughed at the fact that you seem so familiar with this state and are sympathetic to it. reply wegfawefgawefg 8 hours agoparentprevFuture improvements in simulations and manufacturing will probably enable micromanaging light in 3 dimensions. Ive seen some very strange fractal and composite lenses. reply Sharlin 9 hours agoparentprevI mean, dispersion matters simply because people generally don't want colored fringing and general haziness in their photos? There are two types of chromatic aberration (CA), both caused by dispersion. * Axial: perfect focusing is impossible because different wavelengths focus at different distances. If green light is focused correctly, then red and blue light is out of focus. This affects the entire image and is very difficult to fix in post-processing. * Transverse: there's no unique image because magnification depends on wavelength. Blue light forms a slightly larger image than green, and green larger than red. This manifests as color fringing, most apparent near the edges and corners of the image, far from the optical axis. This can be alleviated algorithmically, by resizing the red and blue sub-images to match the green one. reply killjoywashere 6 hours agoprevFluorite lenses are incredible, but not a critical win until you get to high mag or up against the speed limits of your system. When it really becomes apparent is when coupled with VR stabilization, which can get you 4+ EV stops of speed. When the subject is that crisp, the chromatic aberration is significant, or, with fluorite, not. Interestingly, modern photography suites offer both correction of effects of lenses (e.g vignetting and pincushion) and chromatic aberration in silicon. Compared to where I started with a circa 1989 Nikkor 70-210 f/4 and film, the pictures I can take today are incredible. The real problem, for me, is editing down to a digestible number of eye-poppingly good shots. I imagine kids these days are thoroughly unimpressed, but I'm humbled by the incredible amount of engineering that has gone into photography. reply vardump 1 hour agoparent> ... modern photography suites offer both correction of effects of lenses (e.g vignetting and pincushion) and chromatic aberration... Yeah. I haven't worried about chromatic aberration in lenses for about 15 years now. It's trivial to automatically correct in post processing, as long as you shoot RAW. reply kridsdale1 3 hours agoparentprevAs am I. Starting with film in a similar era and moving along with every 1 or 2 major generational shift in CMOS since then, I’m delighted to be at a point today where my output is bottlenecked by the actual 55000mbps M2 SSD in my MacBookPro for whizzing through hundreds of GB of stills for every shoot to distill down to about 10 best ones. reply londons_explore 34 minutes agoprevI don't understand why chromatic abberation isn't simply fixed in software. Knowing that your red, green, and blue images will have slightly different focus points and sizes seems pretty easy to fix. For a stationary scene, you simply take 3 separate photos - one focussed for each colour, and then rescaled to match each others sizes. For moving scenes, the math is more complex, but you should still be able to fix most of the issues. The only real issue is light that is \"yellow\" - ie. halfway in wavelength between different colours. The only perfect fix for that is to have each pixel be a spectrometer. Even that seems possible by using variable-bandgap sensors. reply DarkSucker 9 hours agoprevI wasn't aware that Canon used flourite elements. You learn something new every day. Nice. Their telephoto lenses also use holographic elements, and I attended a talk (1990 ish) where one of their lens designers spoke about a clever scheme using diffraction order pairs (n and n + 1) to compensate each other. This allowed them to use diffractive dispersion in addition to glass (and now I know flourite) for color correction without introducing stray light (ghosts) due to unwanted diffraction orders, which are nearly impossible to get rid of. These lenses are works of art. reply porphyra 9 hours agoparentThe fragility of fluorite was one a reason why Nikon gear was chosen for the International Space Station iirc. reply kridsdale1 3 hours agorootparentPlus that little red marquee looks rad against the infinite universe. reply contravariant 10 hours agoprevI was kind of hoping they would explain why it helps prevent chromatic aberration. Unfortunately their explanation stops short, basically just saying that 'it does' without going into the details of why. My first guess would be something to do with it having a well suited refractive index, but it is almost equal to that of glass. The best candidate I've found is that the group velocity dispersions are opposite, which seems like it might explain it, if only I knew what it meant. reply mnw21cam 10 hours agoparentWhen a lens is designed, typically an optimiser is used to try to find a combination of lens elements that focuses light of all wavelengths to the same spot across the whole image. If all the same type of glass is used, then it's hard for the optimiser to find a solution that corrects for the chromatic aberration across all the spectrum, because if it adjusts (for instance) something to fix the green aberration then it'll mess up the blue and vice-versa. But if a different type of glass is used such as fluorite, which has a different pattern of chromatic dispersion, then that gives the optimiser an extra degree of freedom, so it is more able to independently control the aberration in the different colours and make a lens that performs well. reply postmodest 10 hours agorootparentOver the past ten years, lenses have gotten unbelievably better. Apsherical elements have done some of the lifting, and glass formulas another, but how much have computational tools changed how lenses are designed? reply buildbot 10 hours agorootparentAnecdotally, a lot. Modeling of Aspherical elements has gotten better and it is much easier to make them as well. Mamiya made some of the first APO computer corrected lenses back in 1988 - https://lens-db.com/mamiya-apo-sekor-z-350mm-f56-1988/ reply fsh 2 hours agorootparentprevWhat are you basing this observation on? I haven't seen any significant improvement in photography or cinema lens performance over many decades. The principles have been understood for 150 years, and using ray-tracing software is standard since the 70s. Cheap lens systems in phones got a lot better, and digital sensors are much more sensitive than film though. reply itishappy 10 hours agoparentprevIt's a bit buried, but this (to me) is the most interesting sentence : > Fluorite lenses are also unique in their extraordinary partial dispersion tendencies: the red to green wavelengths are dispersed with the same tendencies as glass, but the green to blue wavelengths are dispersed more than glass. It has both low dispersion (less overall aberrations) and a unique shape to the dispersion curve (more control). Glasses typically all have similar dispersion curves. The weird shape of fluorite's dispersion curve gives your optimization function an extra lever to play with. reply nomel 9 hours agorootparentDoes this mean it has an index of refraction that depends on wavelength? How can that be? Is the bond length some multiple of blue (or green/red), where there's some quick \"change\" in what the photon \"sees\"? reply itishappy 6 hours agorootparentAmazing question! Yup. Refractive index is a function of wavelength. I bet you already knew that on some level, because that's why prisms do their thing. As an extreme example, imagine X-rays: they barely refract at all, they just pass straight through stuff! Also, the change is actually quite smooth. https://refractiveindex.info/ https://en.wikipedia.org/wiki/Dispersion_(optics) reply tonyarkles 8 hours agorootparentprevThis is, I think, just one of those spots where \"all models are wrong, some models are useful\" is true. From the wikipedia page on Refractive Index (https://en.wikipedia.org/wiki/Refractive_index): > The refractive index may vary with wavelength. This causes white light to split into constituent colors when refracted. This is called dispersion. This effect can be observed in prisms and rainbows, and as chromatic aberration in lenses. --- snip --- > For most materials the refractive index changes with wavelength by several percent across the visible spectrum. Nevertheless, refractive indices for materials are commonly reported using a single value for n, typically measured at 633 nm. So we were all lied to in our introductory optics classes. n isn't a constant for a given material but rather n(lambda) but weakly. reply DarkSucker 8 hours agorootparentprevIt's a consequence of the relationship between absorption and index of refraction in glasses, which follow the Karmers-Kronig relationship [0]. As the wavelength approaches atomic resonance, the index increases or decreases depending on which side of the absorption peak the wavelength is. 0. https://en.wikipedia.org/wiki/Kramers%E2%80%93Kronig_relatio... reply dekhn 9 hours agorootparentprevGlass has this too. it's why achromats and apochromats were developed (using rather clever ideas like pairing two different types of glass, which cancel each other's dispersions. All the physical explanations I've seen invoke the EM equations and quantum mechanics to explain it (and I don't understand it well enough to translate). reply aoeusnth1 7 hours agorootparent3Blue1Brown has a nice animated video about this: https://youtu.be/KTzGBJPuJwM?si=ssBPKL8LVhsQ3wNZ reply aidenn0 1 hour agorootparentprevGlass has an index of refraction that depends on wavelength; this is how a prism works. This is also why chromatic aberration exists at all. reply analog31 8 hours agoparentprevThis article shows a \"map\" of available glasses, with each glass shown by its refractive index and dispersion constant -- in a particular way. The explanation is that if all you have are glasses on that \"glass line,\" you can make a lens with equal focal length at two wavelengths but not three. You need glasses that are not on the \"glass line,\" and one of them is calcium fluoride. Also, some plastics such as acrylic and polystyrene can be used, but have their own issues such as thermal expansion. Finding a lens with 2 equal focal lengths is equivalent to a graph of focal length versus wavelength that's roughly a parabola. With 3 equal focal length, the graph looks like a cubic curve. https://www.opticsforhire.com/blog/apochromatic-lens/ The glasses also have to be economical, able to take a good polish, clear, chemically resistant (to avoid staining), mechanically robust, etc. It's not an easy design problem since the \"exotic\" glasses with extreme refractive properties also tend to have worse properties overall. reply buildbot 10 hours agoparentprevI found the article explained exactly why using fluorite helped chromatic aberration. It adds another degree of freedom, due to the partial dispersion. reply NickNameNick 10 hours agoparentprevIsn't that covered in the 7th paragraph? 'the red to green wavelengths are dispersed with the same tendencies as glass, but the green to blue wavelengths are dispersed more than glass. Using a convex fluorite lens element alongside a high-dispersion glass concave lens element therefore eliminates residual chromatic aberration' reply lambdasquirrel 10 hours agorootparentI thought the reason underlying that is that fluorite is not a glass, technically. It is a crystal. But I’m not a MatSci person so that doesn’t leave me any bit more informed. reply foobar1962 8 hours agoparentprevPossibly a poor analogy, but imagine making a particle accelerator with the goal that objects of different mass put into it are accelerated and leave it going the same velocity. The current state of technology allows that the best we can do is make one that accelerates lighter objects slightly more than heavier objects. However, we've found a way to make an accelerator using a different design that accelerates heavier objects more than lighter objects. If we pass objects through the first accelerator, then the second, the second accelerator reverses out some of the non-linearity of the first. Unfortunately this second accelerator is very, very expensive to make. This is how lenses of different refractive indexes are used: one element partially corrects the dispersion produced by earlier elements. Fluorite has the right refractive index to correct aberrations created in long focal length lenses. reply porphyra 9 hours agoparentprevChromatic aberration happens because of dispersion --- the fact that the refractive index changes with wavelength. By combining materials with different properties (e.g. low dispersion, or high refractive index), you can cancel out the aberration in lens designs such as achromatic doublets, apochromatic triplets, etc. In general, materials that are high in refractive index have high dispersion (e.g. crown glass) and materials that have low dispersion also have low refractive index. But ideally we want a material that has high refractive index but low dispersion so that it can both bend light without introducing a lot of chromatic aberration. Fluorite has extraordinarily low dispersion while having a refractive index that's only slightly lower than glass, making it a good material to be used in conjunction with other materials. reply s0rce 9 hours agoparentprevIt seemed to, the dispersion is different so you can cancel the dispersion from glass (dispersion is the refractive index variation with wavelength). reply rainbowzootsuit 10 hours agoparentprevFluorite just has low dispersion and therefore a lens made from it has less chromatic aberration than glass. reply foobar1962 8 hours agorootparent> Fluorite just has low dispersion and therefore a lens made from it has less chromatic aberration than glass. So why not make the entire lens from fluorite? Because pairing an element with one with different refractive index can lower the overall dispersion. Typically you'll see compound lenses are made of pairs of elements where one is positive (convex) and the other negative (concave): instead of making one lens with power of, say, +4, the group is made from one that's +5 and one that's -1 of different type of glass so the refractive indexes cancel-out dispersion and other aberrations. reply davidmurdoch 9 hours agorootparentprev> the red to green wavelengths are dispersed with the same tendencies as glass, but the green to blue wavelengths are dispersed more than glass. reply rainbowzootsuit 7 hours agorootparentI admittedly responded without reading the source, so please accept my apologies. Despite how it is worded the total dispersion of Fluorite is less than glass. That quote is referring to the ‘extraordinary partial dispersion’ property. They are using this to better correct the aberration than could be done with two pieces of glass alone. This seems to be illustrated in the diagrams well. Some further stuff to read up on is the Abbe number that describes the refractive index vs wavelength derived from a set of light sources. https://en.wikipedia.org/wiki/Abbe_number The higher the Abbe number the lower the dispersion of a material. Fluorite is ~95 https://refractiveindex.info/?shelf=main&book=CaF2&page=Mali... Random choice below but glass tends to be ~25 - 80 https://refractiveindex.info/?shelf=glass&book=HOYA-C&page=E... reply jcynix 10 hours agoprevRecently I learned that today's lenses don't contain \"just glass\" as optical elements but often specially designed plastic elements too. Here's a video by Gordon Laing showing a \"Canon lens TEARDOWN! What's INSIDE a new lens?\" https://youtube.com/watch?v=YH5_nVRWHZ0 reply foobar1962 8 hours agoparentSome Canon lenses like the EF 17-40mm L use \"replica\" aspherical elements... Replica aspherical lens elements are produced by using an aspherical surface mold and ultraviolet-light-hardening resin to form an aspherical surface layer on a spherical glass lens. reply kridsdale1 3 hours agorootparentIsn’t that a super budget lens? How’s the pixel peeper performance? reply matsur 3 hours agorootparent\"L\" has a red ring and everything! reply rodgerd 9 hours agoparentprevIf you like teardowns, I will plug the Lens Rentals blog @ https://www.lensrentals.com/blog which features teardowns such as https://www.lensrentals.com/blog/2021/01/the-secret-of-the-b... reply porphyra 9 hours agorootparentToo bad Roger Cicala stopped writing. reply neom 9 hours agoprevSuper UD and UD have been some of the gold standards in lenses for a long time. Fluorite plays a huge part in it. Fun further reading: https://www.canon-europe.com/pro/infobank/fluorite-aspherica... reply tenken 8 hours agoprevAnd here I thought they meant Eyes Glasses .... Darn. reply jwrallie 3 hours agoparentI also thought they meant eyeglasses. I used glasses for a few years and never cared much for chromatic aberration, but once I started to use contacts, going back to glasses makes it very noticeable, specially on the periphery of the lens. Lenses on glasses need to be very high quality to approach the image quality of contacts in terms of distortion. reply ChrisMarshallNY 8 hours agoprevMy mother was a geologist. She had a piece of fluorite that was clear, and she would put it over things, and it would double them (you'd see two of everything). It has some interesting optical characteristics. If I remember, the stone was soft, and sensitive to shock and temperature changes. Probably makes it challenging to work with. reply daniel_reetz 7 hours agoparentPretty sure that was calcite - famous for double refraction. https://sciencedemonstrations.fas.harvard.edu/presentations/... reply ChrisMarshallNY 7 hours agorootparentYou are correct. I misremembered. Flourite was ... very fluorescent. reply kragen 6 hours agorootparentas it turns out, pure fluorite has no fluorescence, even though fluorescence is named for fluorite reply ChrisMarshallNY 5 hours agorootparentWell, it has been a long time (like, 40 years), but I’m pretty sure I remember it being strongly fluorescent. It might have been impurities, though. reply kragen 2 hours agorootparentyeah, that's normal, which is why it's called fluorescence. pure natural minerals are very rare reply hiyer 4 hours agoprevCurious - is this really expensive to make? Why is chromatic aberration even a thing now if we've had this technology since 1969? reply ssnistfajen 4 hours agoparentBecause it's not really something that matters for the vast majority of consumer photography equipment. https://istarscopeclub.proboards.com/thread/247/twisting-fac... ^Some 2012 forum post claiming the material cost $2000/kg, although their wording is ambiguous on whether this was the price for the raw material or the finished lens. But based on the manufacturing process described in the source link of this post, it would appear to be a much more cumbersome and complex process than manufacturing regular glass lenses. Fluorite lens are also more fragile which may impact yield %. reply s0rce 9 hours agoprevFluorite (CaF2) lenses and windows are also used for infrared applications as they transmit much better than glass. reply dheera 10 hours agoprevI have a FD 300/2.8 S.S.C. Fluorite lens, introduced in 1975. It's a FANTASTIC lens, sharp corner to corner wide open at f/2.8, and excellent for astrophotography. It's able to capture details of the Orion nebula, horsehead nebula, and the spiral arms of the Andromeda galaxy in a single shot. It's also excellent for outdoor portraits and creams backgrounds, though you'll need to use a phone call on speakerphone to talk to your subject. The list price back then was 420000 JPY which is $5364 in today's dollars. I got it for $400, used. Back then prices of FD mount lenses dropped dramatically because nobody wanted them: the flange distance was too short to be adapted to any DSLR. I ended up taking apart the entire back part and machining a conversion mount to make it usable on DSLR. (An adapter ring won't work, since it adds thickness.) Unfortunately with the advent of mirrorless cameras, FD lenses are once again usable with simple adapter rings, and their used market prices have gone back up. However they're still excellent, excellent value for $ in comparison to modern autofocusing equivalents in optics; the lens I have costs ~$600-$1000 on eBay now whereas a new Sony 300/2.8 GM costs $6000. For anyone looking for a fast, large aperture telephoto lens I'd highly recommend looking into FD lenses that have fluorite elements, as long as you don't mind manual focus. reply porphyra 9 hours agoparentI used to have a FD 500mm f/4.5 which is nice but not as well corrected as modern glass. I got it for $750 which, too, is a bargain. > as long as you don't mind manual focus Given that the primary use case of large aperture telephoto lenses is sports and wildlife, fast autofocus is a killer feature. Moreover, modern computer optimization has managed to vastly lighten the weight and improve the weight distribution of the lens, not to mention impeccable image quality, which is why for many the $6000 is more than justified. reply dheera 8 hours agorootparentFor astrophotography manual focus is preferable. For wildlife, manual focus is actually not that hard with some practice, as long as it's not birds. For sports, yeah, it's difficult. reply djtango 6 hours agoprevDon't have much to add, but this article was so pleasing to read. Those diagrams with the light dispersion really tickled me - is that spectrum real or is it a render? reply reiichiroh 4 hours agoprev [–] Maybe I’ll get thinner than 1.75 lenses some day for my crazy high prescription. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Canon has developed its own technology for creating large, high-purity artificial fluorite crystals for their lenses.",
      "Fluorite lenses, made from a combination of artificial fluorite and glass, effectively correct chromatic aberration and produce clear and high-quality images.",
      "Canon has already produced several lenses using fluorite lens elements that have been widely adopted by photographers due to their improved image quality."
    ],
    "commentSummary": [
      "Fluorite lenses offer various benefits in optical applications, including the reduction of chromatic aberration.",
      "The article explains dispersion and refractive index, fundamental concepts in lens design.",
      "It also discusses advancements in lens technology, such as the use of computational tools and aspherical elements."
    ],
    "points": 263,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1705445421
  },
  {
    "id": 39019532,
    "title": "Stable Code 3B: Advanced Code Completion with Superior Performance",
    "originLink": "https://stability.ai/news/stable-code-2024-llm-code-completion-release",
    "originBody": "Stable Code 3B: Coding on the Edge 16 Jan Key Takeaways: Stable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as CodeLLaMA 7b that are 2.5x larger. Operates offline even without a GPU on common laptops such as a MacBook Air. Today, we announce our first Large Language Model release of 2024: Stable Code 3B. This new LLM is a follow-up to our previously released Stable Code Alpha 3B and the first major Stable Code release, offering a new state-of-the-art model designed for code completion with multiple additional capabilities. Compared to CodeLLaMA 7b, Stable Code 3B is 60% smaller while featuring a similar high-level performance across programming languages. Based on our pre-existing Stable LM 3B foundational model trained on 4 trillion tokens of natural language data, Stable Code was further trained on software engineering-specific data, including code. The model's compact size allows it to be run privately on the edge in real-time on modern laptops, even those without a dedicated GPU. Stable Code 3B offers more features and significantly better performance across multiple languages with additional benefits such as support for Fill in the Middle capabilities (FIM) and expanded context size. Stable Code as a base is trained on sequences of up to 16,384 tokens but follows a similar approach to CodeLlama with the implementation of Rotary Embeddings, optionally allowing modification of the rotary base up to 1,000,000, further expanding the model’s context length up to 100k tokens. Stable Code is trained on 18 programming languages (selected based on the 2023 StackOverflow Developer Survey) and demonstrates state-of-the-art performance (compared to models of similar size) on the MultiPL-E metrics across multiple programming languages tested. Performance Comparison Side by Side Comparison of Stable Code Completion 3B with CodeLLama 7B Training Insights Our training pipeline consists of a multi-stage process similar to Codellama. We start with an LM pre-trained on natural language data, in this case, StableLM-3B-4e1t, followed up with unsupervised fine-tuning on multiple code and code-related datasets, including CommitPack, GitHub Issues, StarCoder & other Math datasets. In the second step, we further fine-tune the model with longer sequences of 16,384 tokens with the base modification suggested in CodeLLama. The new stable-code model also supports Flash Attention 2 and is available for use. Further references to the data and model can be found in our model card. We will release a full technical report with additional details and ablations to be more transparent and open to the community. Commercial Applications This model is included in our new Stability AI Membership. Visit our Membership page to take advantage of our commercial Core Model offerings, including SDXL Turbo & Stable Video Diffusion. Stay updated on our progress by signing up for our newsletter, and learn more about commercial applications by contacting us here. Follow us on Twitter, Instagram, LinkedIn, and join our Discord Community. Anel Islamovic",
    "commentLink": "https://news.ycombinator.com/item?id=39019532",
    "commentBody": "Stable Code 3B: Coding on the Edge (stability.ai)256 points by egnehots 12 hours agohidepastfavorite105 comments JCM9 9 hours agoDon’t entirely understand Stability’s business model. They’ve been putting out a lot of models recently and Stable Diffusion was novel at the time, but now their models consistently seem to be somewhat second rate compared to other things out there. For example Midjourney now seems to have far surpassed them in the image generation front. After raising a ton of funding Stability seems to just be throwing a bunch of stuff out there that’s OK but no longer ground breaking. What am I missing? Many other startups in the space will like face similar issues given the rapid commoditization of these models and the underlying tech. It’s very easy to spend a fortune building a model that offers a short lived incremental improvement at best before one can just quickly swap it out for something else someone else paid to train. reply emadm 7 hours agoparentBusiness model is bundling so you have a one stop shop for good quality models of every modality and cultural variants of them. These go on bedrock, on chip, on prem etc and our consulting partners take them to the end user. On the innovation side stable diffusion turbo does like 100 cats with hats per second and the video model outperforms runway, pika etc on blind tests. Stable audio was one of the time innovation of the year winners on music and we released a sota 3d model. Stable LM zephyr is the best 3b chat model works great on a MacBook Air. Most of the pixels in the world will be generated so fast high quality image/video are the core and these other models are to support them. It’s really hard to build good solid models and we are the only company that can build a model of any type for anyone. reply doctorpangloss 5 hours agorootparentI use Stable Diffusion family models for innovative art products. On a small scale, you have to professionalize ComfyUI’s development. My PR to make it installable and to make a plugin ecosystem that makes sense should not be sitting unmerged (https://github.com/comfyanonymous/ComfyUI/pull/298). On a medium scale, CLIP is holding you back. I would eagerly buy a 48GB card to accommodate a batch size 1, gradient checkpointed LoRA-trainable model with T5 for conditioning. I want PixArt-a or DeepFloyd/IF with the SDXL dataset and training. I get I can achieve so much with SDXL on 24GB, including just barely a fine tuning, I understand the engineering decisions here, but it’s too weak on prompts. On a large scale, I’m willing to spend a little money up front. In those conditions you can be far more innovative, you don’t have to make everything for $0. Shane Carruth didn’t make Primer for $0. I’m sure you’ve seen this movie, you get how astoundingly good it is. But he still spent something. He spent only slightly more than an RTX 6000 Ada. Innovators have budgets. It’s still worth releasing the most powerful possible model for expensive hardware, this is why everyone is talking about Mixtral, but it’s especially true of visual art. reply amne 13 minutes agorootparentprev\"100 cats with hats per second\" AI has peaked reply tlrobinson 29 minutes agorootparentprev(parent commenter is founder/CEO of Stability AI, Emad Mostaque, I assume) reply refulgentis 7 hours agorootparentprevVouch, I finally tried Stable LM 3b zephyr today and I'm stunned this slipped by. It's the only model I've tried that's not Mistral 7B that can do RAG. And it can run ~any consumer grade hardware released in last 3 years. I'm literally stunned it's been sitting out since December 8th. I've heard 10x more about Phi-2 than it, and I'm not sure why. (Official ONNX version, please!! Then you get Transformers.js / web / I can deploy on every platform from Web to iOS to Windows) re: art, Dalle-3 costs significantly more. XL costs are 1/5th of what they were at launch, 0.0002/image versus Dalle-3's 0.04. And you'd be surprised how often people are happy with XL -- Dalle-3's marginal advantage is mostly text, especially with the excessive filtering of stylistic stuff, and forced prompt rewrites reply washadjeffmad 8 hours agoparentprevMidjourney is decidedly underwhelming if you've spent any time using the expansive tooling and control nets of Stable Diffusion. Yes, it's easy to get impressive first gens with MJ, but all of the coolest work and integration happening is using SD. reply capybara_2020 2 hours agorootparentIt depends. For great looking pics that you need to get out quickly MJ does a great job. Especially with its image + text feature. Dalle is also an interesting choice. SDXL and controlnet is odd a lot of the time. 1.5 + controlnet still seem to give quicker and better results. Basically SD atleast seems to be for when you want unique content. MJ/Dalle for everything else. reply satvikpendem 7 hours agoparentprev> For example Midjourney now seems to have far surpassed them in the image generation front Nope. Stable Diffusion with alternative models offers far more customization and control than Midjourney. Midjourney is good for beginners but sucks for experts. reply smusamashah 8 hours agoparentprevMidjourney has better quality but does not offer any control. Community has done and is still doing a a lot with SD models because they can be played and tinkered with in any way anyone wants to. reply liuliu 9 hours agoparentprev> one can just quickly swap it out for something else someone else paid to train. That doesn't seem to be the case. There are very limited open-source models outside of the small-LLM bubble. reply JCM9 8 hours agorootparentThe open space on small models is a whole other developing angle, but O was referring to the general commoditization of a lot of these models. With rare exception after launch it seems the lifespan of any of these models is rather limited. From a business standpoint that sort of scenario is generally very unattractive and thus was trying to understand if they have some other angle they’re trying to play here to make a viable business out of this. Or the business model can just be get acquired before that matters and let that be someone else’s problem to figure out. reply liuliu 5 hours agorootparentThe comoditization of models outside of LLM is delusional. There is no comparable open source image / video models to the private ones. reply teaearlgraycold 3 hours agoparentprevWe use SD at work because we need more control over the image generation pipeline (and to a lesser extent don’t want extra latency from web APIs). Believe it or not, generating a full image from a prompt is a small slice of the image generation pie. Highly tuned in-painting is key to a number of budding startups. reply tarruda 11 hours agoprevNote that they don't compare with deepseek coder 6.7b, which is vastly superior to much bigger coding models. Surpassing codellama 7b is not that big of a deal today. The most impressive thing about these results is how good the 1.3B deepseek coder is. reply jyap 9 hours agoparentDeepseek Coder Instruct 6.7b has been my local LLM (M1 series MBP) for a while now and that was my first thought… They selectively chose benchmark results to look impressive (which is typical). I tested out StableLM Zephyr 3B when that came out and it was extremely underwhelming/unusable. Based on this, Stable Code 3B doesn’t look to be worth trying out. Guessing if they could put out a 7B model which beat Deepseek Coder 6.7B they would have. reply a_wild_dandan 9 hours agorootparentDo you know how Deepseek 33b compares to 6.7b? I'm trying 33b on my (96GB) MacBook just because I have plenty of spare (V)RAM. But I'll run the smaller model if the benefits are marginal in other peoples' experience. reply wokwokwok 7 hours agorootparentThe smaller model is great at trivial day-to-day tasks. However, when you ask hard things, it struggles; you can ask the same question 10 times, and only get 1 answer that actually answers the question. ...but the larger model is a lot slower. Generally, if you don't want to mess around swapping models, stick with the bigger one. It's better. However, if you are heavily using it, you'll find the speed is a pain in the ass, and when you want a trivial hint like 'how do I do a map statement in kotlin again?', you really don't need it. What I have setup personally is a little thumbs-up / thumbs-down on the suggestions via a custom intellij plugin; if I 'thumbs-down' a result, it generates a new solution for it. If I 'thumbs-down' it twice, it swaps to the larger model to generate a solution for it. This kind of 'use ok model for most things and step up to larger model when you start asking hard stuff' approach scales very nicely for my personal workflow... but, I admit that setting it up was a pain, and I'm forever pissing around with the plugin code to fix tiny bugs, which I would prefer to be spending doing actual work. So... there's not really much tooling out there at the moment to support it, but the best solution really is to use both. If you don't want to and just want 'use the best model for everything', stick with the bigger one. The larger model is more capable of turning 'here is a description of what I want' into 'here is code that does it that actually compiles'. The smaller model is much better at 'I want a code fragment that does X' -> 'rephrased stack overflow answer'. reply tarruda 6 hours agorootparent> but the larger model is a lot slower. I found the performance to be very acceptable for 33b 4 bit on a m3 max with 36gb ram (much faster than reading speed) reply wokwokwok 5 hours agorootparentI’m not sure what to say; responsive fast output is ideal, and the larger model is distinctly slower for me, particularly for long completions (2k tokens) if you’re using a restricted grammar like json output. I’m using an M2 not an M3 though; maybe it’s better for you. I was under the impression quantised results were generally slower too, but I’ve never dug into it (or particularly noticed a difference between q4/q5/q6). If you find it fast enough to use then go for it~ reply bravura 3 hours agorootparentprevDo you mind sharing your plugin as a gist? How do you run both models in memory? Two separate processes? reply jyap 8 hours agorootparentprevYou would want to test it out manually day to day. That’s always the best. Some models can out score but not actually be “better” when you use it. But there is also the benchmarking: https://github.com/deepseek-ai/deepseek-coder 33B Instruct doesn’t beat 6.7B Instruct by much but maybe those % improvements mean more for your usage. I run 6.7B since I have 16GB RAM. Quantization of the model also makes a difference. reply zwarag 3 hours agorootparentprevDo you use it inside vscode or how do you integrate an LLM into your IDE? reply SubiculumCode 7 hours agorootparentprevHow do you make use of it? Do you have it integrated directly into an ide? reply sroussey 9 hours agorootparentprevWhat do you use it for? reply jyap 9 hours agorootparentGolang grinding Leetcode coding buddy. And general coding buddy PR reviewer. The results are on par or better than ChatGPT 3.5. I often use it to delve deeper such as “is there an alternative way to write this?” Or “how does this code look?” If you have an M-series Mac I recommend trying out LM Studio. Really eye opening and I’m excited to see how things progress. reply sroussey 7 hours agorootparentI have GitHub copilot. Is it better than that? Nd if so, in which way? Offline would be one for sure. Cost is another. What else? reply jyap 7 hours agorootparentI’ve never used GH Copilot so can’t comment on that. But having everything locally means no privacy or data leak issues. reply danielbln 10 hours agoparentprevDeepseek-coder-6.7B really is a quite surprisingly capable model. It's easy to give it a spin with ollama via `ollama run deepseek-coder:6.7b`. reply triyambakam 10 hours agorootparentThanks for the tip with ollama reply discordance 9 hours agorootparentIf you do: 1. ollama run deepseek-coder:6.7b 2. pip install litellm 3. litellm --model deepseek-coder:6.7b You will have a local OpenAI compatible API for it. reply behnamoh 5 hours agorootparentprevollama is actually not a great way to run these models as it makes it difficult to change server parameters and doesn't use `mlock` to keep the models in memory. reply bravura 3 hours agorootparentWhat do you suggest? reply eyegor 7 hours agoparentprevThe 1.3b model is amazing for real time code complete, it's fast enough to be a better intellisense. Another model you should try is magicoder 6.7b ds (based on deepseek coder). After playing with it for a couple weeks, I think it gives slightly better results than the equivalent deepseek model. Repo https://github.com/ise-uiuc/magicoder Models https://huggingface.co/models?search=Magicoder-s-ds reply hskalin 7 hours agorootparentHow do you use these models with your editor? (E. vscode or Emacs etc) reply eyegor 2 hours agorootparentI run tabby [0] which uses llama.cpp under the hood and they ship a vscode extension [1]. Going above 1.3b, I find the latency too distracting (but the highest end gpu I have nearby is some 16gb rtx quadro card that's a couple years old, and usually I'm running a consumer 8gb card instead). [0] https://tabby.tabbyml.com/ [1] https://marketplace.visualstudio.com/items?itemName=TabbyML.... reply tarruda 6 hours agorootparentprevAn easy way is to use an OpenAI compatible server, which means you can use any GPT plugin to integrate with your editor reply a_wild_dandan 10 hours agoparentprevThis is phenomenal. And runs fast! The 33b version might be my MacBook's new coding daily driver. reply unshavedyak 6 hours agorootparentHow are you using it? I need to find some sane way to use this stuff from Helix/terminal.. reply tarruda 9 hours agorootparentprev4-bit quantized 33b runs great on a mp pro with m3 max chip reply a_wild_dandan 9 hours agorootparentI'm using the 5-bit quant with llama.cpp and it's excellent on my M2 96GB MacBook! Running this model + Mixtral will be fun. reply swyx 9 hours agoprev> License: Other > Commercial Applications > This model is included in our new Stability AI Membership. Visit our Membership page to take advantage of our commercial Core Model offerings, including SDXL Turbo & Stable Video Diffusion. what exactly is the license lol. can people use this or is this \"see dont touch\" reply neurostimulant 4 hours agoparentIt's free for noncommercial use. If you use it in your company, your company should pay the membership fee. afaik most openai competitors also use similar usage restriction (e.g. free for noncommercial or research use, contact us for commercial license). reply keyle 11 hours agoprevThat is fantastic. I'm building a small macOS SwiftUI client with llama cpp built in, no server-client model, and it's already so useful with models like openhermes chat 7B, and fast. If this opens it to smaller laptops, wow! We truly live in crazy time. The rate of improvement in this field is off the walls. reply joshmarlow 11 hours agoparentNot sure if this is where your head is, but I think there's a lot of value in integrating LLMs directly into complex software. Jira, Salesforce, maybe K8s - should all have an integrated LLMs that can walk you through how to perform a nuanced task in the software. reply manmal 9 hours agorootparentImagine good error messages, with hints for mitigation and maybe smart retry w/ mitigations applied. reply dpacmittal 11 hours agorootparentprevWhy would the LLM walk you through and not just do the nuanced task on its own? reply pennomi 11 hours agorootparentI assume the human maintains some of the necessary context in their meat memory. reply debarshri 10 hours agorootparentprevWalkthrough is generally performed once or not so frequently. It would be a bad investment if you just use it for just this use case reply alpaca128 10 hours agorootparentA beginner tutorial is also not used frequently by users, but that doesn't make it a bad investment. I an LLM can help a lot with getting familiar with the tool it could be pretty valuable, especially after a UI rework etc. reply turnsout 11 hours agoparentprevThat sounds awesome! Can you share any details about how you're working with llama cpp? Is it just via the SwiftC bridge? I've toyed with the idea of doing this, and wonder if you have any pointers before I get started. reply emadm 7 hours agoparentprev3b is good for 8gb MacBook Air etc. 7b is slightly too big. Sure these will continue to improve, phi2 is a good base as well reply alwinaugustin 6 hours agoprevI've been experimenting with code-llama extensively on my laptop, and from my experience, it seems that these models are still in their early stages. I primarily utilize them through a Web UI, where they can successfully refactor code given an existing snippet. However, it's worth noting that they cannot currently analyze entire codebases or packages, refining them based on the most suitable solutions using the most appropriate algorithms. While these models offer assistance to some extent, there is room for improvement in their ability to handle more complex and comprehensive coding scenarios. reply danielmarkbruce 5 hours agoparentI think there is a decent chance SourceGraph will figure this all out. The most important thing at this point is figuring what context to feed. They can build up a nice graph of a codebase and I expect from there they can put in the best context and then boom. They might also be able to train a model more intelligently by generating training data from said graphs. reply rahimnathwani 7 hours agoprevHow are people using codellama and this in their workflows? I found one option: https://github.com/xNul/code-llama-for-vscode But I'm guessing there are others, and they might differ in how they provide context to the model. reply knicholes 11 hours agoprevI've got a machine with 4 3090s-- Anyone know which model would perform the best for programming? It's great this can run on a machine w/out a graphics card and is only 3B params, but I have the hardware. Might as well use it. reply tarruda 11 hours agoparentAFAIK deepseek coder family are the best open coding models. I haven't tested, but I think deepseek coder 33b can run in a single RTX 3090 when 4-bit quantized. In your case you might be able to run the non quantized version reply Havoc 10 hours agoparentprevThe coding models are all small because speed is crucial. If you need to wait 2 seconds for an autocomplete it becomes near useless. reply SushiHippie 11 hours agoparentprevHere is a leader board of some models https://huggingface.co/spaces/mike-ravkine/can-ai-code-resul... Don't know how biased this leaderboard is, but I guess you could just give some of them a try and see for yourself. reply SparkyMcUnicorn 10 hours agorootparentThis is a much better leaderboard: https://evalplus.github.io/leaderboard.html I've seen the CanAiCode leaderboard several times (and used many of the models listed), but I wouldn't use it to pick a model. It's not a bad list, but the benchmark is too limited. The results are not accurately ranked from best to worst. For example the deepseek 33b model is ranked 5 spots lower than the 6.7b model, but the 33b model is definitely better. WizardCoder 15b is near the top while WizardCoder 33b is ranked 26 spots lower, which is a wildly inaccurate ranking. It's worth noting that those 33b models score in the 70s for HumanEval and HumanEval+ while the 15b model scores in the 50s. reply SushiHippie 8 hours agorootparentThanks reply mlboss 6 hours agoparentprevDid you build a machine with 4x 3090 ? I looking for a way to build such a machine for ML training. reply mistercheph 7 hours agoparentprevTry mistral 8x7b, which some human evals place above gpt-3.5 and you have enough VRAM and compute to make training a LORA either on your own dataset, or one of the freely available datasets on huggingface worthwhile, or at least interesting reply jjtheblunt 10 hours agoprevJargon naivete question: isn't \"on the edge\" normally implying on a server side with minimal routers hops to the client, not on client side? reply devindotcom 9 hours agoparentafaik \"edge\" nearly always means taking place on the device a user is interacting with. no server involved except perhaps as authentication etc. but there is probably some other situation where \"edge\" could mean local infra or caching. reply WatchDog 6 hours agorootparentI think the etymology of “edge computing” is derived from “network edge”, ie the outer shell of some network/autonomous system. The closest point within your control that interfaces with devices outside of your control. Seeing the term get used to describe client devices themselves kinda muddies the terminology. reply jjtheblunt 4 hours agorootparentAgreed. (autocorrect style typo above: etymology) reply WatchDog 4 hours agorootparentfixed reply xer0x 9 hours agoparentprev+1 yes, for a service using network caching like using Cloudflare. I would've referred to their CDN as the Edge of our network. reply sytelus 1 hour agoprevWhy authors miss to compare with Phi-2? reply lfkdev 11 hours agoprevHow is this compared to the current GitHub Copilot? reply brianjking 11 hours agoparentA 3B tiny model is not going to compare to GitHub copilot. However, there are plenty of nice 7B models that are excellent at code and I encourage you to try them out. reply londons_explore 11 hours agoparentprevIf you just want to get stuff done, use the best tools like a Milwaukee Drill - and right now, thats copilot/gpt-4. If you don't want to be tied to a company and like opensource, feel free to connect a toy motor to an AA battery to drill your holes... Or to use Llama/Stable Code 3B. reply irthomasthomas 10 hours agorootparentOpenai just invisibly dropped my API requests to a lower model with a 4k context limit. And my commit scripts started failing for being over the context limit. It's buried in the docs somewhere that low tier api users will be served on lower models during peek times. So,I guess they're like a Milwaukee Drill that will sometimes refuse to work unless you buy more drill credits. reply drzaiusx11 10 hours agorootparentMore like a Milwaukee drill you have on loan that can be swapped out for a manual screwdriver without warning. reply nulld3v 10 hours agorootparentprevWTF? Do you have a link? I was not aware of this, it would be crazy if true. reply bearjaws 10 hours agorootparentprevYou clearly have never used these other tools. Mixtral / Deepseek perform very well on coding challenges. I've used them against local code without issues, sometimes they are a bit optimistic and produce too much, but thats far better than producing too little (like GPT4 does). reply mistercheph 11 hours agorootparentprevit’s going to be real hard to pry the carburetors out of this guy’s cold dead hands! reply outcoldman 8 hours agoprevI was able to run this model in http://lmstudio.ai as well. Just remove Compatibility Guess in Filters, so you can see all the models. LM Studio can load it and run requests against it. reply connorgutman 11 hours agoprevFYI: This model is already available on Ollama. reply mistrial9 10 hours agoparenthow do you check that ? reply coder543 10 hours agorootparenthttps://ollama.ai/library?sort=newest reply artninja1988 11 hours agoprevGiven the complete failure of the first stable lm, I'm interested to try this one out. Haven't really seen a small language model, except mixtral 7b that's really useful for much. I also hope stability comes out with a competitor to the new midjourney and dalle models! That's what put them on the map in the first place reply emadm 7 hours agoparentWe released a competitor to runway recently that beat it on blind tests, plus way faster image in sdxl turbo We have been working on ComfyUI for the next step and new image models Midjourney and others are pipelines versus models so we have a higher bar to jump but the og stable diffusion team are working hard! reply tarruda 11 hours agoparentprevDeepseek coder 6.7B is very useful for coding and can run in consumer GPUs. I use the 6bit GGUF quantized version on a laptop RTX 3070 reply brianjking 11 hours agoparentprevAll of the Mistral versions have been excellent, including the OpenHermes versions. I encourage you to check out Phi-2 as well, it's the only 3b model I've found really quite interesting outside of Replit's code model built into Replit Core. reply mchiang 11 hours agoprevIt's amazing to see more smaller models being released. This creates opportunities for more developers to run it on their local computers, and makes it easier to fine-tune for specific needs. reply brcmthrowaway 11 hours agoparentHas anyone tried starting with a smaller modeling, then RLing until it improves to the bigger model? reply herval 9 hours agoprevCan anyone explain what’s Stability’s business model (or plan for one)? I get why Meta releases tons of models, but still can’t quite understand what stability is trying to achieve reply sangnoir 9 hours agoparentSeems like the standard open-core playbook: > This model is included in our new Stability AI Membership. Visit our Membership page to take advantage of our commercial Core Model offerings, including SDXL Turbo & Stable Video Diffusion. A hypothetical Stable Code 13B/70B could be hosted only, with more languages or specialized use-cases (Stable Code 3B iOS-Swift-Turbo) reply emadm 7 hours agorootparentMembership with upsell to support, custom models and more Plus licensed variant models like stable audio and on chip installation like arm for specialist models eg Japanese law or Indonesian accounting reply seydor 9 hours agoparentprevto be bought by meta reply bogwog 8 hours agorootparentThis is all an elaborate mating ritual reply photon_collider 11 hours agoprevHow reliable are these benchmarks? reply ilaksh 11 hours agoparentI think the trick is that they are just comparing to other tiny models. None of the little models, including this one, are comparable to the performance of the larger models for any significant coding problem. I think what these are useful for is mostly giving people hints inside of a code editor. Occasionally filling in the blank. reply akulbe 11 hours agoprevI just tried this model with Koboldcpp on my LLM box. I got gibberish back. My prompt - \"please show me how to write a web scraper in Python\" The response?I've written my first ever python script about 5 months ago and I really don't remember anything except for the fact that I used Selenium in order to scrape websites (in this case, Google). So you can probably just copy/paste all of these lines from your own Python code which contains logic to determine what value should be returned when called by another piece of software or program.reply SushiHippie 11 hours agoparentIt's very likely a \"completion model\" and not instruct/chat fine-tuned. So you'd need to prompt it through comments or by starting with a function name, basically the same as one would prompt GitHub copilot. e.g. # the following code implements a webscraper in python class WebScraper: (I didn't try this, and I'm not good at prompting, but something along the lines of this example should yield better results) reply Tiberium 11 hours agoparentprevBut it's a code completion model, not a chat/instruct one. reply endofreach 11 hours agoparentprevThis doesn't seem like gibberish though? reply MrNeon 11 hours agoparentprevIt is weird that it is not mentioned in the model card but I'm pretty sure it is a completion model, not tuned as an instruct model. edit: the webpage does call it \"Stable Code Completion\" reply connorgutman 11 hours agoparentprevSame thing with Ollama. reply kleiba 11 hours agoprev [–] It's quite amazing - I often find that I read quite positive comments towards LLM tools for coding. Yet, an \"Ask HN\" I posted a while ago (and which admittedly didn't gain much traction) seemed to mirror mostly negative/pessimistic responses. https://news.ycombinator.com/item?id=38803836 Was it just that my submission didn't find enough / more balanced commenters? reply yorwba 11 hours agoparentYou got two positive and two negative responses. You replied only to the negative responses. Now you think that the responses were mostly negative. I blame salience bias. Anyways, there's also a difference between \"are you excited about this new thing becoming available\" and \"now that you've used it, do you like the experience\". The former is more likely to feature rosy expectations and the latter bitter disappointment. (Though it could also be the other way around, with people dismissing it at first and then discovering that it's kind of nice actually.) reply Havoc 11 hours agoparentprevThe precise wording matters. How has it changed your work life leads people down the rabbit hole of will coding jobs be safe. This one is a lot more neutral/technical. reply simonw 11 hours agoparentprev [–] You only got comments from six people so yeah, definitely not representative. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Stable Code 3B is a 3 billion parameter Large Language Model (LLM) developed for code completion tasks.",
      "It can operate offline without requiring a GPU and has better performance than CodeLLaMA 7b.",
      "The model is trained using software engineering-specific data and supports Fill in the Middle functionality, performing well in multiple programming languages. It undergoes pre-training on natural language data and fine-tuning on code-related datasets."
    ],
    "commentSummary": [
      "The discussion explores various AI models used in visual art generation and their limitations in terms of customization and effectiveness.",
      "Users share their experiences, preferences, and concerns about different coding models and tools for AI art generation.",
      "The discussion also addresses the availability of open-source models, hardware requirements, licensing options, and the debate between using smaller or larger models for coding tasks."
    ],
    "points": 256,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1705441215
  },
  {
    "id": 39019119,
    "title": "Stability issues resolved: Post-mortem analysis of Kagi.com incident",
    "originLink": "https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/",
    "originBody": "← Go back to Kagi Status Kagi.com is unstable for all regions January 12, 2024 at 4:30 PM UTC us-east4 us-west2 europe-west2 asia-east2 us-central1 europe-west4 asia-southeast1 australia-southeast1 southamerica-east1 Resolved after 6h 50m of downtime. Investigating - We are experiencing issues following a deploy. Team is working on resolving this.. (16:45 UTC — Jan 12) Monitoring - We have are reverting a configuration change that we believe to be the culprit, and are continuing to monitor as service is coming back to full health. (18:30 UTC — Jan 12) Update - In order to fully restore stability we will need to pause traffic momentarily. We will be redirecting users to this page while we restore load to the service in a controlled manner. We will follow up with further details as the situation progresses. (20:26 UTC — Jan 12) Monitoring - Traffic has been restored and we are continuing to monitor the service as it comes back to full health. (21:14 UTC — Jan 12) Resolved - All services are now operating as normal. Thank you for your patience while we resolved this issue. Post-Mortem Hi all, This is Zac, the Tech Lead at Kagi. I’m going to be sharing below a more in-depth post-mortem of our service interruption last week. Assisting me in responding to this incident was Seth, one of our senior engineers, and Luan, our DevOps engineer. This will be fairly technical, you can skip to “Next Steps” for takeaways. The summary is that we were the target of some actors misusing the service, exploting a pathological case in our infrastructure, and we immediately released mitigations and are working on improvements in several areas of our code and communications. Timeline On January 12th, approx. 5:30PM UTC, the team became aware of an infrastructure issue occurring by way of our internal monitoring and user reports of issues. The nature of the issue was causing slow loading or complete page timeouts for users in various regions. This incident unfortunately took us quite some time to resolve - we deeply thank our users for their patience, and the opportunity to give some background as to what was going on, and how we plan to move forward. At first, by what turned out to be a complete coincidence, the incident occurred at precisely the same time that we were performing an infrastructure upgrade to our VMs with additional RAM resources. Our monitoring was reporting both high latency and issues with our application’s database connection pool. While no code changes were part of this upgrade, and “old” instances were reporting some issues as well, we decided the first course of action was to revert this change. This revert completed at around 6:50 PM, but as we continued to monitor the issue persisted. Meanwhile, we had been inspecting the behavior of our application’s database connection pools, which were saturated with connections to our primary DB instance. It was unclear what the exact cause of this was yet, but what was clear is that the total number of connections being established globally to our primary exceeded its maximum configured connection limit. Our next move was to evaluate if we had somehow caught ourselves in a “spiral” of exhausting our maximum connections, wherein any instance that we would replace would simply get its connection pool exhausted again, queuing for access to the primary. In several steps we tried replacing a few instances to see what effect reducing the congestion would have. We also were making progress on evaluating various parts of the databases internal health and query performance. With mild signs that cycling some instances was helping, at 9:30PM UTC we decided to pause all user traffic by redirecting users to our status page to give the database a break and completely reset all connection pools in one shot. We installed the redirect and issued a restart of all nodes. Once all appeared stable, we started letting traffic back in again. Unfortunately, the issue persisted. While looking at the database state, it became clear to our engineers that the root cause was in fact high contention on rows in the users table. This contention caused a steep increase in write latency, which in turn put backpressure on our application’s connection pool, causing it to eventually exhaust all available connections as writes were taking too long to complete. The writes were all stemming from one instance that would eventually starve the rest of our global instances of access to our primary, thus causing disruption in all other regions. This didn’t exactly come as a surprise to us, as for the entirety of Kagi’s life so far we have actually used the cheapest, single-core database available to us on GCP! To this day we’ve yet to exceed 50% load capacity on it, which we’ve worked hard to achieve. This has always carried the risk of the database being relatively easy to knock over, but we have so far kept load and latency under control with redundancy, distributed read-replicas, and high scrutiny over the SQL we write. We then took steps to identify the bad actors, where we found accounts created within 24hrs and over 60,000 searches performed in short time period from a single user facing account. While we do offer unlimited searches to our users, such volume was a clear abuse of our platform, against our terms of use. As such, we removed searching capability from the offending accounts. At the same time, we issued a hotfix that disabled the particular writes that were causing high contention, in addition to some upgrades of our database driver, which included several relevant bug fixes to connection pool health. This would help ensure that immediately the same pathological case could not be exploited again. By midnight, the issue was fully resolved. The team continued to closely monitor for any signals that the actors were returning, but they did not. We were later in contact with an account that we blocked who claimed they were using their account to perform automated scraping of our results, which is not something our terms allow for. Next Steps There’s a lot we took away from this incident, and we have immediate plans already in motion to make our system more robust to this type of abuse, as well as our communication processes around incidents. First, regretfully we were not very prompt in updating our status page. We owe it to our customers to be quick and transparent about any issues going on that might affect access to the product that they pay for. To address this, we are moving to a status page platform to one that will more easily allow us expose some of our automated internal monitoring to users. This way users have an idea of the platform’s health in real-time, even if our small team of engineers has their hands full to immediately post an update (which was not a very fast process to begin with, even if we were on top of it). We should have this available by next week. Secondly, we have directly mitigated the queries causing issues under load. With this issue in mind, we are also running load tests to learn about any other similar deficiencies that may still exist, and what to avoid in the future. We are also installing some additional monitoring to more quickly point us to the right place in our infra, and hopefully not waste as much time chasing a false-flag as we did this time. Lastly, we are performing a re-upping of our systems set up to detect this kind of abuse of our terms, which were clearly too lax. Besides any potential performance impact, this vector also directly costs us money as we pay for each search. To protect our finances, and all of our subscribers continued access to unlimited (organic) searches, we need to set some automated limits to help us enforce this. From analyzing our user’s usage, we have picked some limits that no good-faith user of Kagi should reasonably hit. These new limits should already be in place by the time of this post, and we will monitor their impact and continue to tune them as needed. If you believe you find yourself wrongly unable to access Kagi, please reach out to support@kagi.com. Thank you so much for bearing with us through this incident! Please look forward to a more robust service as we implement these things, and as usual, more features & improvements are on their way. Last updated: January 16, 2024 at 7:24 PM UTC © Kagi Status, 2024 • Back to top We continuously monitor the status of our services and if there are any interruptions, a note will be posted here. Powered by cState ⚡ Subscribe via RSS — to all updates",
    "commentLink": "https://news.ycombinator.com/item?id=39019119",
    "commentBody": "Post-mortem for last week's incident at Kagi (kagi.com)252 points by leetrout 12 hours agohidepastfavorite119 comments blantonl 11 hours agoAt first, by what turned out to be a complete coincidence, the incident occurred at precisely the same time that we were performing an infrastructure upgrade to our VMs with additional RAM resources I can assure you that these \"coincidences\" happen all the time, and will cause you to question your very existence when you are troubleshooting them. And if you panic while questioning your very existence, you'll invariably push a hotfix that breaks something else and then you are in a world of hurt. \\ Muphy's law is a cruel thing to sysadmins and developers. reply JohnMakin 9 hours agoparentCompletely agree. I’ve triaged many outages with varying degrees of severity in my career so far, and the worst ones were always caused by someone panic-jumping onto some red herring rather than coming up with a sensible explanation as to why that would be a fix other than “it happened at the same time.” I have a saying I really like to throw around, which is “if you don’t know why/how you fixed it, you may not have” reply tetha 37 minutes agorootparent> I have a saying I really like to throw around, which is “if you don’t know why/how you fixed it, you may not have” There is coincidental function, and planned function. If you just throw numbers at a configuration parameter until it works and then call it \"fixed\",it may be fixed, but that's just coincidental. If the system load changes, you can't really tell how to modify the value afterwards. On the other hand, if you can explain why a certain setting will fix it, that's planned function, and if something changes, you most likely know how to proceed from there. reply hammyhavoc 55 minutes agorootparentprevI read \"varying degrees of severity\" as \"varying degrees of sanity\" and was all \"omg, he just like me fr\" until I reread it. ༼ ༎ຶ ᆺ ༎ຶ༽ reply sa46 8 hours agoparentprevOh man, last week we had a small outage. Database queries took much longer than normal. I just so happened to be doing ad-hoc querying on the same table at the same time. “Luckily”, the problem was unrelated to my querying but two coincidences are proper scary. reply dylan604 7 hours agorootparentnah, unless you were the intern, you should be safe it's like Star Trek where you're fine unless you're the one wearing the red shirt reply ashton314 7 hours agorootparentIf I ever run a company, all the interns will get red shirts reply V-eHGsd_ 2 hours agorootparentThe contractor badges at google were red until at least 2015. And as it was explained to me, it was not a coincidence. reply ok_dad 6 hours agorootparentprevThink about the optics of your funny joke. You’re saying you’ll advertise that the interns are replaceable fodder. Even if that’s true, it’s quite a dick move. reply ethbr1 6 hours agorootparentOr acknowledging that interns perform a valuable role and celebrating their selfless sacrifice of their very lives for the good of the company. That we should all die such a noble corporate death, at the hands of a colleague possessed by strange galactic energies. reply zer00eyz 5 hours agorootparentInterns for the Crimson Permanent Assurance... If you know you know, if you dont: https://www.youtube.com/watch?v=aSO9OFJNMBA reply PNewling 6 hours agorootparentprevIt also could have just been a joke, not one they were actually going to put into practice... reply dylan604 6 hours agorootparentSome people just come across as the type of person totally not any fun to hang out with. reply ok_dad 6 hours agorootparentprevI’ve seen enough brainless execs and startup founders around here that you have to take stupid ideas like that seriously. I hope it was a joke, but it’s not very funny for an intern. reply DiggyJohnson 4 hours agorootparentYou definitely do not have to take stupid ideas like that seriously. Simply your life. reply dylan604 4 hours agorootparentA little more credit to people for making a funny and a little less assumptions that people are that boneheaded would make the world a better place to be sure. I know for one, I rather enjoy being able to see the humor and having a laugh rather than getting all enraged and venting on the internet to show some sort of self perceived superiority. reply muhammadusman 11 hours agoprevI was one of the users that went and reported this issue on Discord. I love Kagi but I was a bit disappointed to see that their status page showed everything was up and running. I think that made me a bit uneasy and it shows their status pages are not given priority during incidents that are affecting real users. I hope in the future the status page is accurately updated. In the past, services I heavily rely on (e.g. Github), have updated their status pages immediately and this allows me to rest assured that people are aware of the issue and it's not an issue with my devices. When this happened with Kagi, I was looking up the nearest grocery stores open since we were getting snow later that day so it was almost like I got let down b/c I had to go to Google for this. I will continue using Kagi b/c 99.9% of the other time I've used it, it has been better than Google but I hope the authors of the post-mortem do mean it when they say they'll be moving their status page code to a different service/platform. And thanks again Zac for being transparent and writing this up. This is part of good engineering! reply Terretta 10 hours agoparent> In the past, services I heavily rely on (e.g. Github), have updated their status pages immediately Also in the past, other times GitHub has not updated its status page immediately. reply phyzome 5 hours agoparentprevAs an engineer on call, I have been in this conversation so many times: \"Hey, should we go red?\" \"I don't know, are we sure it's an outage, or just a metrics issue?\" \"How many users are affected again?\" \"I can check, but I'm trying to read stack traces right now.\" \"Look, can we just report the issue?\" \"Not sure which services to list in the outage\" ...and so on. Basically, putting anything up on the status page is a conversation, and the conversation consumes engineer time and attention, and that's more time before the incident is resolved. You have to balance communication and actually fixing the damn thing, and it's not always clear what the right balance is. If you have enough people, you can have a Technical Incident Manager handle the comms and you can throw additional engineers at the communications side of it, but that's not always possible. (Some systems are niche, underdocumented, underinstrumented, etc.) My personal preference? Throw up a big vague \"we're investigating a possible problem\" at the first sign of trouble, and then fill in details (or retract it) at leisure. But none of the companies I've worked at like that idea, so... [shrug] reply Gareth321 19 minutes agorootparentThis is exactly why those status pages are almost always a lie. Either they need to be fully automated without some middle manager hemming and hawing, or they shouldn’t be there at all. From a customer’s perspective, I’ve been burned so many times on those status pages that I ignore them completely. I just assume they’re a lie. So I’ll contact support straight away - the very thing these status pages were intended to mitigate. reply smsm42 2 hours agorootparentprevIMHO, any significant growth in 500s (that's what I was getting during the outage) warrants mention on status page. I've seen a lot of stuff, so if I see an acknowledged outage, I'll just wait for people to do their jobs. Stuff happens. If I see unacknowledged one, I get worried that people who need to know don't and that undermines my confidence in the whole setup. I'd never complain if status page says maybe there's a problem but I don't see one. I will complain in the opposite case. reply virtue3 5 hours agorootparentprevI think your bit at the end is the most important. ANY communication is better than no communication \"everything is fine, it must be you\" is the worst feeling in these cases. Especially if your business is reliant on said service and you can't figure out why you are borked (eg the github ones). reply PeterStuer 2 hours agorootparentprevAnd that is before 'going red' has ties to performance metrics with SLA impacts ... reply DANmode 1 hour agorootparentWhich then means not going yellow or red technically constitutes fraud. reply PeterStuer 44 minutes agorootparentNot necessarily. The situation can be genuinely unclear to the point where it is a judgement call, and then it becomes a matter of how to weigh the consequences. reply TeeWEE 2 hours agorootparentprevConnect your status page to actual metrics and decide a treshold for downtime. Boom you’re done. reply lrem 2 hours agorootparentDoes anyone serious do this? That’s an honest question, from a pretty experienced SRE. reply darkwater 1 hour agorootparentIn a world of unicorns and rainbows, absolutely. In the real world, it's as you probably already know: it's not that easy in a complex enough system. Quick counter-example for GP: what if the 500 spike is due to a spike in malformed requests from a single (maybe malicious) user? reply laeri 1 hour agorootparentA malformed request should not lead to a 500, they should be handled and validated. reply darkwater 29 minutes agorootparentWell, in the real world it might. It should trigger a bug creation and a fix to the code, but not an incident. Now all of a sudden to decide this you need more complex and/or specific queries in your monitoring system (or a good ML-based alert system), so complexity is already going up. reply jon_adler 54 minutes agorootparentprevTrue, however it also doesn’t impact other users and doesn’t justify reporting an incident on the status page. reply lambdaba 10 hours agoparentprevI'm only replying to the praise here - I too, although I haven't fully switched, had a very enticing moment with Kagi when it returned a result that couldn't even be found on Google at any page in the results. This really sold me on Kagi and I've been going back and forth with some queries, but I have to say that between LLMs, Perplexity, and Google often answering my queries right on the search page, I just don't have that many queries left for Kagi. If Kagi would somehow merge with Perplexity, now that would be something. reply spdif899 10 hours agorootparentKagi does offer AI features in their higher subscription tier, including summary, research assistance, and a couple others. Plus I think they have basically a frontend for GPT-4 that uses their search engine for browsing, and they just added vision support to it today. I don't subscribe to those features or any AI tool yet, just pointing out there could be a version of Kagi that is able to replace your Chatgpt sub and save you money reply lambdaba 9 hours agorootparentIs it as good as Perplexity though? I use ChatGPT for different purposes, I just thought that if Kagi would ally with Perplexity and benefit from its index (I'm not sure what Perplexity uses), it could get really good. I've only recently tried using Perplexity and I get more use out of it than I would with Kagi, it doesn't just do summarization, but I haven't seen what Kagi does with research assistance. reply jci 8 hours agorootparentIt's been a while since I've used Perplexity, but I've been finding the Kagi Assistant super useful. I'm on the ultimate plan, so I get access to the `Expert` assistant. It's been pretty great. https://help.kagi.com/kagi/ai/assistant.html reply herpdyderp 8 hours agoparentprevI envy your experiences with other services. I've never seen any service's status page show downtime when or even soon after I start experiencing it. Often they simply never show it at all. reply Neikius 10 hours agoparentprevMicrosoft is notorious for their lax status page updates... reply wiml 7 hours agorootparentIs there anyone who isn't? reply NetOpWibby 8 hours agoparentprevIt's worth noting that the status page software they use doesn't auto-update automatically. > Please note that with all that cState can do, it cannot do automatic monitoring out of the box. https://github.com/cstate/cstate reply ParetoOptimal 8 hours agorootparentI guess a status page that doesn't auto-update is good for PR, but it's not very useful to show... you know... the status. reply NetOpWibby 8 hours agorootparentYeah I thought that was weird. An auto-updating page is worth the constant pings to the infra IMHO. reply rwiggins 7 hours agoprevAaaahhh, it's crazy how much this incident resonates with me! I've personally handled this exact same kind of outage more times than I'd care to admit. And just like the fine folks at Kagi, I've fallen into the same rabbit hole (database connection pool health) and tried all the same mitigations - futilely throwing new instances at the problem, the belief that if I could just \"reset\" traffic it'd all be fixed, etc... It doesn't help that the usual saturation metrics (CPU%, IOPS, ...) for databases typically don't move very much during outages like these. You see high query latency, sure, but you go looking and think: \"well, it still has CPU and IOPS headroom...\" without realizing, as always, lock contention lurks. In my experience, 98% of the time, any weirdness with DB connection pools is a result of weirdness in the DB itself. Not sure what RDBMS Kagi's running, but I'd highly recommend graphing global I/O wait time (seconds per second) and global lock acquisition time (seconds per second) for the DB. And also query execution time (seconds per second) per (normalized) query. Add a CPU utilization chart and you've got a dashboard that will let you quickly identify most at-scale perf issues. Separately: I'm a bit surprised that search queries trigger RDBMS writes. I would've figured the RDBMS would only be used for things like user settings, login management, etc. I wonder if Kagi's doing usage accounting (e.g. incrementing a counter) in the RDBMS. That'd be an absolute classic failure mode at scale. reply WhitneyLand 7 hours agoparentI was wondering the same thing. They would have some writes indirectly due to searches, say if someone chooses to block a search result. They’re also going to have some history and analytics surely. But yeah it’s not obvious what should cause per search write lock contention… reply rwiggins 2 hours agorootparentYou know, in retrospect, I think Kagi expects O(thousands) searches per month per user, so doing per-user usage accounting in the DB is fine -- thanks to row-level locking. Well, at least until you get a user who does 60k \"in a short time period\"... :-) reply louthy 10 hours agoprevThis is something that every start-up company ends up going through at some point. I’ve been there and it’s painful! Sometimes you just don’t have enough time or resource to build the capabilities that would stop an issue like this. Sometimes you didn’t even think a particular issue could even happen and it comes and bites you. The transparency is important, the learning is important, but also (sometimes) the compensation is important. Kagi should consider giving some search credits for the time that we were unable to use the service. Especially as the real-time response was inadequate (as they freely admit). An outage for a paid-for service is not the same as an outage of a you’re-the-product service reply fragmede 11 hours agoprevThat speaks volumes about the observability they have of their internal systems. It's easy for me to say they should have seen it sooner, but the right datadog dashboards and splunk queries should have made that clear as day much faster. Hopefully they take it as a learning experience and invest in better monitoring. reply z64 11 hours agoparentHi there, I'm Zac, Kagi's tech lead / author of the post-mortem etc. This has 100% been a learning experience for us, but I can provide some finer context re: observability. Kagi is a small team. The number of staff we have capable of responding to an event like this is essentially 3 people, seated across 3 timezones. For myself and my right-hand dev, this is actually our very first step in our web careers - this is to say that we are not some SV vets who have seen it all already. To say that we have a lot to learn is a given, from building Kagi from nothing though, I am proud of how far we've come & where we're going. Observability is something we started taking more seriously in the past 6 months or so. We have tons of dashboards now, and alerts that go right to our company chat channels and ping relevant people. And as the primary owner of our DB, GCP's query insights are a godsend. During the incident both our monitoring went off, as well as query insights showing the \"culprit\" query - but, we could have monitoring in the world, and still lack the experience to interpret it and understand what the root cause is or most efficient action to mitigate is. In other words, we don't have the wisdom yet to not be \"gaslit\" by our own systems if we're not careful. Only in hindsight can I say that GCP's query insights was 100% on the money, and not some bug in application space. All said, our growth has enabled us to expand our team quite a bit now. We have had SRE consultations before, and intend to bring on more full or part-time support to help keep things moving forward. reply kelnos 11 hours agorootparentHi Zac, thank you for chiming in here. Been using Kagi since the private beta, and have been overwhelmingly impressed by the service since I first used it. Don't worry too much about all the people being harsh in the comments here. There's always a tendency for HN users to pile on with criticism whenever anyone has an outage. I've always found this bizarre, because I've worked at places with worse issues, and more holes in monitoring or whatever than a lot of the companies that get skewered here. Perhaps many of us are just insecure about our own infra and project our feeling onto other companies when they have outages. Y'all are doing fine, and I think it's to your credit that you're able to run Kagi's users table off a single, fairly cheap primary database instance. I've worked at places that haven't much thought to optimization, and \"solve\" scaling problems by throwing more and bigger hardware at it, and then wonder later on why they're bleeding cash on infrastructure. Of course, by that point, those inefficiencies are much more difficult to fix. As for monitoring, unfortunately sometimes you don't know everything you need to monitor until something bad happens because your monitoring was missing something critical that you didn't realize was critical. That's fine; seems like y'all are aware and are plugging those holes. I'm sure there will be more of those holes in the future, that's just life. At any rate, keep doing what you're doing, and I know the next time you get hit with something bad, things will be a bit better. reply z64 10 hours agorootparentVery kind, thank you! (and everyone else too, many heartwarming replies) reply Tempest1981 7 hours agorootparentprev> people being harsh in the comments here I've read most of the comments here, and don't recall anything negative, just supportive. reply tetha 10 hours agorootparentprevMh, I work quite a bit in the OPs-side and monitoring and observability are part of my job, for a bit of time now too. I'll say: Effective observability, monitoring and alerting of complex systems is a really hard problem. Like, you look at a graph of a metric, and there are spikes. But... are the spikes even abnormal? Are the spikes caused by the layer below, because our storage array is failing? Are the spikes caused by ... well also the storage layer.. because the application is slamming the database with bullshit queries? Or maybe your data is collected incorrectly. Or you select the wrong data, which is then summarized misleadingly. Been in most of these situations. The monitoring means everything, and nothing, at the same time. And in the application case, little common industry wisdom will help you. Yes, your in-house code is slamming the database with crap, and thus all the layers in between are saturating and people are angry. I guess you'd add monitoring and instrumentation... while production is down. At that point, I think we're at a similar point of \"Safety rules are written in blood\" - \"the most effective monitoring boards are found while prod is down\". And that's just the road to find the function in code that's a problem. That's when product tells you how this is critical to a business critical customer. reply callalex 3 hours agorootparentRunning voodoo analysis on graph spikes is indeed a fool’s errand. What you really need is load testing on every component of your system, and alerts for when you approach known, tested limits. Of course this is easier said than done and things will still be missed, but I’ve done both approaches and only one of them had pagers needlessly waking me in the middle of the night enough to go on sleepless swearing rants to coworkers. reply primitivesuave 11 hours agorootparentprevI really appreciate you sharing these candid insights. Let me tell you (after over a decade of deploying cloud services), some rogue user will always figure out how to throw an unforeseen wrench into your system as the service gets more popular. Even worse than an outage is when someone figures out how to explode your cloud computing costs :) reply alberth 9 hours agorootparentprevUnsolicited suggestion. Don’t host your status page (status.kagi.com), as a subdomain of your main site (DNS issues can cause both your main site and status site to go offline - so use something like kagistatus.com). And host it with a webhost who doesn’t use any common infra as you. reply jjtheblunt 11 hours agorootparentprevI bet a silent majority are thinking \"well done, Zac, all the same\". reply ayberk 10 hours agorootparentprevKudos for being so open -- after seeing numerous \"incidents\" at AWS and GCE I can say that two rules always hold with respect to observability: - You don't have enough. - You have too much. Usually either something will be missing or some red herring will cost you valuable time. You're already doing much better than most people by taking it seriously :) reply JohnMakin 9 hours agorootparentprev> Kagi is a small team. I figured this was the case when you said “our devops engineer” singular and not “one of our devops engineers.” I’m glad you’re willing at this stage to invest in SRE. It’s a decision a lot of companies only make when they absolutely have to or have their backs against a wall. reply timwis 11 hours agorootparentprevThank you for sharing! I’m surprised to hear that, given how impressive your product is, but I’m an even bigger fan now. reply siquick 7 hours agorootparentprevThe best feedback I can give is that my whole family now uses Kagi over Google Search and I’m a regular user of the summariser tool too. Big ups, you’re smashing it reply ijhuygft776 5 hours agorootparentprevKagi, an efficient company. Thanks Zac and the rest of the team reply nanocat 11 hours agorootparentprevSounds like you’re doing great to me. Thank you for being so open! reply xwolfi 4 hours agorootparentprevI work in a giant investment bank with hundreds of people who can answer across all time zones. We still f up, we still don't always know where problems lie and we still sometimes can spend hours on a simple DoS. You'll only get better at guessing what the issue could be: an exploit by a user is something you'll remember forever and will overly protect against from now on, until you hit some other completely different problem which your metric will be unprepared for, and you'll fumble around and do another post mortem promising to look at that new class of issues etc. You'll marvel at the diversity of potential issues, especially in human-facing services like yours. But you'll probably have another long loss of service again one day, and you're right to insist on the transparency / speed of signaling to your users: they can forgive everything as long as you give them an early signal, a discount and an apology, in my experience. reply digitalsin 9 hours agorootparentprevI use Kagi every single day, ever since the beta. I don't remember the last time I used that other search engine, the G one..can't remember the name. Anyway, absolutely love Kagi and the work you guys do. Thank you! reply hacker_newz 11 hours agoparentprevWhat are \"the right datadog dashboards and splunk queries\"? reply blantonl 11 hours agorootparentLots and lots of money to catch what you don't know, which means \"oh crap, now we need to log this also\" reply fragmede 10 hours agorootparentprevThis is easy for me to say in hindsight, but a graph of queries per user, for the top 100 accounts would have lit up the issue like a Christmas tree. But it's the kind of chart you stuff on the full page of charts that are usually not interesting. So many problems have been avoided by \"huh, that looks weird\" on a dashboard. In a more targeted search, asking Splunk to show out where the traffic is coming from on a map, and a pie chart of the top 10 accounts they're coming from would also be illuminating. But again, this is easy for me to say in hindsight, after the issue has been helpfully explained to me in a blog post. I don't claim could have done any better if I were there, my point is that you need to see inside the system to be operate it well. reply eep_social 8 hours agorootparentIf daily volume is ~400k a sharp 60k spike should show even on a straight rps dashboard. I suspect that didn’t exist because most cloudy SaaS tools don’t seem to put any emphasis on traffic over a particular unit time, like in the Cloudflare site dashboard one must hover over the graph and subtract timestamps to find out if the number of request is per hour, minute, etc. Splunk is similarly bad — the longer the period query covers, the less granularity you get and the minimum seems to be minutely. Drill down by user, locale, etc would just make it even easier to figure out what’s going on once you spot the spike and start to dig. My unsolicited advice for Zac in case they’re reading is — start thinking about SLIs (I for indicators) sooner rather than later to help you think through cases like this. reply scblzn 11 hours agorootparentprevsurely these ones for datadog: https://nitter.net/TurnerNovak/status/1654577231937544192 /s reply mathverse 11 hours agoparentprevKagi is a startup with low margins and high operational costs. reply dotnet00 1 hour agoprevThis was fun to see playing out, it made me realize how I didn't even think about how impressive it was that you guys (being a small team) were pushing out updates with very few bugs or visible downtime. I didn't mind the downtime, just gave me an excuse to take a break and focus on more casual coding. reply mightyham 7 hours agoprevAs much as people gush over Kagi on HN, I still have yet to actually try it because I cannot for the life of me get authentication to work. Even after immediately resetting my password, I get an \"incorrect email or password\" or \"try again later\" error on the login page. I've tried at least 3 times over the last few months with the same results each time. If such a fundamental part of a web service company's website is broken, it makes me weary of their competence. reply freediver 5 hours agoparentHave you tried contacting Kagi support to help debug the issue? reply lostlogin 3 hours agoparentprevPush on - it’s a great service. reply igammarays 8 hours agoprev> This didn’t exactly come as a surprise to us, as for the entirety of Kagi’s life so far we have actually used the cheapest, single-core database available to us on GCP! Wow, love that you guys are keeping it lean. Have you considered something like PolyScale to handle sudden spikes in read load and squeeze more performance out of it? reply sabujp 8 hours agoprevWe were later in contact with an account that we blocked who claimed they were using their account to perform automated scraping of our results, which is not something our terms allow for.\" Set QPS limits for every possible incoming RPC / API / HTTP request , especially public ones! reply leesalminen 7 hours agoparentSo much this. I learned this the hard way. We had a search function with typeahead abilities. I had intentionally removed the rate limit from that endpoint to support fast typers. One day around 6AM, someone in Tennessee came into work and put their purse down on their keyboard. The purse depressed a single key and started hitting the API with each keystroke. Of course after 15 minutes of this the db became very unhappy. Then a web server crashed because the db was lagging too much. Cascading failures until that whole prod cluster crashed. Needless to say the rate limit was readded that day ;). reply o11c 6 hours agorootparentThis is a reminder that \"we want to support bursts\" is much more common thing than \"we want a higher ratelimit\". Often multiple levels of bursts are reasonable (e.g. support 10 requests per minute, but only 100 requests per day; support 10 requests per user, but only 100 requests across all users). There are several ways to track history with just a couple variables (or, if you do have the history, but only accessing a couple of variables); the key observation is that you usually don't have to be exact, only put a bound on it. For history approximations in general, one thing I'm generally fond of is using an exponential moving average (often with λ=1/8 so it can be done with shifts `ema -= ema>>3; ema += datum>>3` and it's obvious overflow can't happen). You do have to be careful that you aren't getting hyperbolic behavior though; I'm not sure I would use this for a rate limiter in particular. reply sedatk 3 hours agoprevI’m a paid user of Kagi, and having experienced downtime made me realize how much I took Google’s reliability for granted. Google has never gone down on me, maybe once in the last two decades. Losing access to your search engine is quite crippling. I LOVE Kagi, that’s why I pay for it, but experiencing downtime in my second month was quite off-putting. I love post-mortems, but I hope to never read them. :) That said, I hope this experience makes Kagi even more resilient and reliable. reply tsegers 16 minutes agoparentAs another paying user of Kagi I wonder what prevented you from using another search engine for the six hours that Kagi was unavailable. Search engines are not like your email provider or ISP in that you're locked in. reply smcleod 11 hours agoprevGood write up. I always appreciate Kagi’s honesty and transparency like this. Great product, great service. reply precision1k 9 hours agoprevReminds me of a time I was running a proof-of-concept for a new networking tool at a customer site, and about two minutes after we got it running their entire network went down. We were in a sandboxed area so there was no way our product could of caused a network wide outage, but in my head I'm thinking: \"there's no way, right. . . .RIGHT?!?!\". reply fwsgonzo 12 hours agoprevIt wasn't that long ago that I had heard about Kagi the first time. Now I use it every day, and the fact that I can pin cppreference.com to the top is just such a boon. reply layoric 11 hours agoprev\"This didn’t exactly come as a surprise to us, as for the entirety of Kagi’s life so far we have actually used the cheapest, single-core database available to us on GCP!\" Outages suck, but I love the fact that they are building such a lean product. Been paying for Kagi as a part of de-Google-ifying my use of online services and the experience so far (I wasn't impacted by this outage) has been great. A few years ago I built a global SaaS (first employee and SWE) in the weather space which was backed by a single DB, and while it had more than just 1 core (8 when I left from memory), I think a lot of developers reach for distributed DBs far too early. Modern hardware can do a lot, products like AWS Aurora are impressive, but they come with their own complexities (and MUCH higher costs). reply z64 10 hours agoparentVery cool! The deluge of cloud solutions are absolutely one of those things that can be a distraction from figuring out \"What do I actually need the computer to do?\". Internally I try to promote solutions with the fewest moving cloud-parts, and channel the quiet wisdom of those running services way larger than ours with something like an sqlite3 file that they rsync... I know they're out there. Not to downplay the feats of engineering of huge distributed solutions, but sometimes things can be that simple! reply layoric 10 hours agorootparentDistracting is right! I watched your CrystalConf video and was happy to see the familiar Postgres + Redis combo :). I remember worrying about running out of legs with Redis (being single threaded), but with a combo of pipelining and changing the data structures used it ended up being the piece of infra that had the most headroom. Monitoring was probably the biggest value for outsourcing to another SaaS. I used Runscope, AWS dashboards and own elasticsearch and it was pretty cost effective for the API that was doing ~2M API calls a day. The other risk of cloud solutions is the crazy cost spikes. I remember consulting a partner, much larger, weather company on reducing their costs of a similar global weather product where they chose AWS DynamoDB as their main datastore, and their bill just for DynamoDB was twice our companies cloud bill. All because it was slightly \"easier\" to not think about compute requirements! Any ways, thanks for the postmortem, hopefully your channeling of quiet wisdom continues to branch out to others! :) reply boomboomsubban 12 hours agoprevOne user running a scraper took the service down for seven hours? I know it's easy to sit on the outside and say they should have seen this coming, but how does nobody in testing go \"what happens if a ton of searches happen?\" reply z64 11 hours agoparentHi there, this is Zac from Kagi. I just posted some other details here that might be of interest: https://news.ycombinator.com/item?id=39019936 TL;DR - we are a tiny, young team at the center, and everyone has a closet full of hats they wear. No dedicated SRE team yet. > \"what happens if a ton of searches happen?\" In fairness, you can checkout https://kagi.com/stats - \"a lot of searches\" is already happening, approaching 400k per day, and systems still operate with plenty of capacity day-to-day, in addition to some auto-scaling measures. The devil is in the details of some users exploting a pathological case. Our lack of experience (now rightfully gained) is knowing what organic or pathological traffic we could have predicted and simulated ahead of time. Load-simulating 20,000 users searching concurrently sounds like it would have been a sound experiment early on, and we did do some things resembling this. But considering this incident, it still would not have caught this issue. We have also had maybe 10 people run security scanners on our production services at this point that generated more traffic than this incident. It is extremely difficult to balance this kind of development when we also have features to build, and clearly we could do with more of it! As mentioned in my other post, we are looking to expand the team in the near term so that we are not spread so thin on these sorts of efforts. There is a lot that could be said in hindsight, but I hope that is a bit more transparent WRT how we ended up here. reply smcleod 11 hours agorootparentZac, I think you’re doing great handling and communicating this. Keep up the great work and have fun learning while you’re at it! reply rconti 9 hours agorootparentprevWhat does \"pathological\" mean in this context? reply fancy_pantser 9 hours agorootparentbeing such to a degree that is extreme, excessive, or markedly abnormal (with a connotation of it happening on purpose) reply SadCordDrone 6 hours agorootparentWhat does \" being such to a degree that is extreme, excessive, or markedly abnormal (with a connotation of it happening on purpose) \" mean in this context? reply fancy_pantser 6 hours agorootparentpathological reply lolc 7 hours agoprevI've been using it for a few weeks now and when it didn't load right away last week I was at a loss what to do. I wondered \"what is Kagi and why can't my browser search anymore?\" It's really well built to get out of your way and I'd all forgotten about it. Eventually I realized I could use another search engine. The bother! Before this post-mortem dropped I'd also forgotten the incident. Props to the team that doesn't make me think when I search! And my sympathy in this incident. It's rough when things coincide like that and cause you to look at the wrong metrics. reply Havoc 10 hours agoprevIronically the reports of instability got me make an account with them. It's been on my todo list but forgot about it reply prh8 7 hours agoprevTotally missed this incident, just want to say thank you to the Kagi team for an awesome product. Love the search and also Orion. reply lopkeny12ko 4 hours agoprev> While we do offer unlimited searches to our users, such volume was a clear abuse of our platform, against our terms of use. As such, we removed searching capability from the offending accounts. Don't advertise something as unlimited if it's not actually unlimited. reply karlshea 3 hours agoparentUnlimited searches as a real user is not the same thing as running a bot against the service. reply callalex 3 hours agorootparentJust advertise it as 10,000 per day if that’s what it actually is. Any sane user will see a number like that and know they don’t have to worry about the limit. reply eviks 3 hours agorootparentprevReal users can also run a bot for their real use cases, the correct way out is not to mislead reply wiseowise 2 hours agoparentprevDon’t abuse the system. reply hackernewds 3 hours agoparentprevconfused how both it is advertised, and against ToS reply lostlogin 3 hours agoprevI cracked up when I saw this. My search went bad and I assumed I’d broken something, tried a few things, took a break, tried a few more and it was working. I moved on. It wasn’t ever at my end. reply 3np 9 hours agoprevWouldn't some level of per-account rate-limiting make sense? Say, 1000 searches per hour? It's commendable and impressive that Kagi has apparently been able to get this far and perform this consistently without any account-level automated rate-limiting but the only alternative is an inevitable cat-and-mouse and whack-a-mole of cutting off paying customers who knowingly or not violate the ToS. Returning 429 with actionable messages makes it clear to users what they should expect. You obviously want the block-interval to be long enough to not cause too much additional churn on the database. Applying restrictions on IP-level when you can avoid it is just a world of frustration for everyone involved. reply eep_social 8 hours agoparent> we need to set some automated limits to help us enforce this. From analyzing our user’s usage, we have picked some limits that no good-faith user of Kagi should reasonably hit. > These new limits should already be in place by the time of this post, and we will monitor their impact and continue to tune them as needed. reply renewiltord 12 hours agoprevInteresting. The classic problem. You offer to not meter something and then someone will use it to max capacity. Then you’re forced to place a limit so that one user won’t hose everyone else. reply JumpCrisscross 11 hours agoparent> you’re forced to place a limit so that one user won’t hose everyone else Soft limits at the tail of usage comports with the term unlimited as it's commonly used. For Kagi, a rate limit derived from how quickly a human can type makes sense. reply fotta 12 hours agoparentprev> We were later in contact with an account that we blocked who claimed they were using their account to perform automated scraping of our results, which is not something our terms allow for. I mean beyond that it was a user that was violating the TOS. This isn’t really a bait and switch scenario (although it could be reasonably construed as such). reply rmrf100 4 hours agoprevMuphy's law reply refulgentis 9 hours agoprevI didn't think it was possible to only buy a single core outside of an edge function. reply thecleaner 5 hours agoprevWhat I gather is this was a rate limiting issue. Rate limiting is a standard pattern for API platforms but I wonder how many consumer facing services implement it. reply zilti 11 hours agoprevnext [3 more] [flagged] HaZeust 11 hours agoparentI'll bite; how do you figure? reply zilti 1 hour agorootparentGCP prices are ludicrously high. You're far better off to just get a VPS and self-host Postgres and your webserver. Which is what we did at my last startup; only thing we used GCP for was build servers, and only because we got a ton of free credits. We could've gotten a paid intern for the money we saved. reply jacob019 11 hours agoprev [–] If you're listening, Kagi, please add an à la carte plan for search. Maybe hide it behind the API options as not to disrupt your normal plans. I love the search and I'm happy to pay, but I'm cost sensitive now and it's the only way that I'm going to feel comfortable using it long-term. reply spiderice 11 hours agoparent [–] They have a $5 for 300 searches option. Is that not what you're referring to? reply JumpCrisscross 11 hours agorootparent> Is that not what you're referring to? It sounds like they'd prefer a 2¢ per search option via API. reply jacob019 10 hours agorootparentYes, exactly. Or 5 cents for the first 100 searches, 2 cents after, something like that. reply binsquare 4 hours agorootparentThey do have the FastGPT api which is approx 1.5cents per api call reply lostlogin 3 hours agorootparentSo raise this to 5c for first 100 and 2c thereafter and we have a happy customer? reply BeetleB 9 hours agorootparentprev [–] They used to have a $5/mo option with N searches, and then charge some cents per search after that. For a lot of people, the net amount would still be under $10/mo. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On January 12, 2024, the website Kagi.com encountered stability issues resulting in slow loading and timeouts in multiple regions.",
      "The problem was initially attributed to an infrastructure upgrade but was later determined to be caused by high contention on rows in the users table.",
      "The root cause was resolved by disabling problematic writes and implementing upgrades to the database driver. Steps have been taken to enhance system resilience and improve communication processes, including the implementation of additional monitoring and automated limits to prevent similar issues in the future."
    ],
    "commentSummary": [
      "The post highlights the recent incident at Kagi during an infrastructure upgrade and advises against panicking and making hasty decisions during outages.",
      "Discussions regarding interns in companies, status page updates, the usefulness of the Kagi Assistant, technical issues, monitoring, and troubleshooting are included.",
      "User concerns are raised about authentication issues, downtime, and scraping, while Kagi has implemented new limits due to search engine abuse. Users are suggesting alternative options and requesting flexible pricing plans."
    ],
    "points": 252,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1705439079
  },
  {
    "id": 39012235,
    "title": "Running Rails 1.0 on Ruby 3.3: Feasibility and Challenges",
    "originLink": "https://nashby.github.io/2024/01/15/ruby-3-on-rails-1/",
    "originBody": "Hey! I'm Vasiliy Ermolovich or @nashby. Here you'll find some blog posts I write once per 10 years Home About GitHub Twitter © Vasiliy Ermolovich, 2024. All rights reserved. Blog Me think, why waste time say lot word, when few word do trick Ruby (3.3) on Rails (1.0). 15 Jan 2024 Rails 8.0 has recently branched out on Github, and I found myself curious about the feasibility of running Rails 1.0 on the latest Ruby version. While I was pretty sure it wouldn’t work right off the bat, I wondered: how many modifications would be necessary to at least reach the iconic “Welcome aboard! You’re riding the Rails!” screen? So, let’s dive in. My starting point was this Gemfile: source \"https://rubygems.org\" gem \"rails\", \"1.0.0\" Since I knew that it would be required to make some changes to Rails gems I install it with bundle install --local. This would allow for easier modifications later on. My first attempt was running bundle exec rails --help: /activesupport-1.2.5/lib/active_support/inflector.rb:157: syntax error, unexpected ':', expecting `then' or ',' or ';' or '' (SyntaxError) when 1: \"#{number}st\" ^ Indeed, older Ruby versions allowed the use of a colon in place of then in case/when expressions. This syntax is no longer supported, so I updated it across the codebase to match the current Ruby syntax. Ok let’s try again with bundle exec rails --help: cannot load such file -- parsedate (LoadError) Oh yeah, parsedate lib that was shipped with Ruby 1.8 is not longer there. It was used to parse date strings, like so: ParseDate.parsedate \"Tuesday, July 5th, 2007, 18:35:20 UTC\" # => [2007, 7, 5, 18, 35, 20, \"UTC\", 2] Not sure why it was returning an array but ok. Now I can replace it with DateTime.parse that returns DateTime object. So I’ve fixed that and tried to run it again. Ugh, another error: rails-1.0.0/lib/rails_generator/options.rb:124: syntax error, unexpected '[', expecting '|' (SyntaxError) ...make any changes.') { |options[:pretend]| } That’s some weird syntax. Turns out you could assign something to a hash right inside the block variable thing: opt.on('-p', '--pretend', 'Run but do not make any changes.') { |options[:pretend]| } meaning that whatever you pass as -p option will end up being assigned to options[:pretend]. Basically it equals to opt.on('-p', '--pretend', 'Run but do not make any changes.') { |o| options[:pretend] = o } Alrighty then. Rerun bundle exec rails --help: no implicit conversion of Enumerator into Array And it’s without any stacktrace. Great. Looks like something catches all the errors and just prints them. After some investigation I’ve found this code: def cache @cache ||= sources.inject([]) { |cache, source| cache + source.map } end In Ruby 1.8 [].map would return an array but now it returns Enumerator object and you can concat an Array with Enumerator hence the error: irb(main):001> [] + [].map (irb):1:in `+': no implicit conversion of Enumerator into Array (TypeError) It’s an easy fix though. Let’s just call .to_a on the source: def cache @cache ||= sources.inject([]) { |cache, source| cache + source.to_a } end Are we getting there? bundle exec rails --help `load': cannot load such file -- config.rb (LoadError) The code in question is require 'rbconfig' DEFAULT_SHEBANG = File.join(Config::CONFIG['bindir'], Config::CONFIG['ruby_install_name']) Makes sense, in old Ruby RbConfig could be referenced with Config constant and now it’s only RbConfig. Fixed. Does it work now? bundle exec rails --help Usage: /vendor/bundle/ruby/3.3.0/bin/rails /path/to/your/app [options] Great Scott! It works! Let’s try to generate a new app: bundle exec rails blog undefined method `exists?' for class File Dammit, File.exists?/FileTest.exists? were removed in Ruby 1.9. Let’s replace it with File.exist?/FileTest.exist? and try again: bundle exec rails blog create create app/controllers create app/helpers create app/models create app/views/layouts create config/environments create components create db create doc create lib create lib/tasks create log create public/images create public/javascripts create public/stylesheets create script/performance create script/process create test/fixtures create test/functional create test/mocks/development create test/mocks/test create test/unit create vendor create vendor/plugins create Rakefile create README create app/controllers/application.rb Cannot create Binding object for non-Ruby caller Success! Is it though? It has generated an app but all the files are empty. And if you have a sharp eye you’ll have noticed this error: Cannot create Binding object for non-Ruby caller Again, no stacktracks, just some plain error. It took me some time to locate that line of code that was failing with such error but here it is: file(relative_source, relative_destination, template_options) do |file| # Evaluate any assignments in a temporary, throwaway binding. vars = template_options[:assigns] || {} b = binding vars.each { |k,v| eval \"#{k} = vars[:#{k}] || vars['#{k}']\", b } ... end Believe me, I really tried to figure you what this error was about given that it’s obviously a Ruby caller but luck wasn’t there for me. Then I tried to replace binding with Kernel.binding and it worked… If you know what’s going on here please let me know! Maybe Rails were redefining binding somewhere? Alright, let’s proceed: bundle exec rails blog create create app/controllers create app/helpers create app/models ... Finally! The app is generated, files are not empty. We’re close, I can smell it! Let’s try to start it: bundle exec ruby script/server -p 3001 `require': cannot load such file -- script/../config/boot (LoadError) Sure, just some random load error. Turns out in Ruby 1.8 you could require a file with relative to current file path and now you can’t: # In Ruby 1.8 require File.dirname(__FILE__) + '/../config/boot' # In Ruby 1.9+ require_relative '../config/boot' With this one fixed we can try it one more time: bundle exec ruby script/server -p 3001 => Booting WEBrick... activerecord-1.13.2/lib/active_record/base.rb:708: circular argument reference - table_name (SyntaxError) Ok this one should be trivial. The code in question: def class_name(table_name = table_name) ... end I’m a bit surprised that this was working in Ruby 1.8. The error is pretty self-explanatory so I just renamed default argument value and continued with my life: bundle exec ruby script/server -p 3001 => Booting WEBrick... `load': cannot load such file -- big_decimal.rb (LoadError) Did you mean? bigdecimal Right, bigdecimal is not required by default now. I’ll spare you some time and say that there was the same issue with rexml and net-smtp gems (net-smtp is not even part of Ruby anymore and I had to add it to the Gemfile). So I fixed it and tried again: bundle exec ruby script/server -p 3001 => Booting WEBrick... actionmailer-1.1.5/lib/action_mailer/quoting.rb:22: invalid multibyte escape: /[\\000-\\011\\013\\014\\016-\\037\\177-\\377]/ (SyntaxError) Oh yeah, Ruby 1.9 did a lot of changes to string encoding (you can read more on this here) and now using raw bytes doesn’t work anymore. So I believe we can convert it to /[\\x00-\\x11\\x13\\x14\\x16-\\x1F\\x7F-\\xFF]/n and it’s going to work? Well, at least the issue was fixed (yeah, yeah who cares if we’ve just introduced some vulnerability? I don’t): bundle exec ruby script/server -p 3001 => Booting WEBrick... `require': cannot load such file -- soap/rpc/driver (LoadError) Oh ffs. It comes from action_web_service (god knows what was that back in the days) and lucky us we can remove this Rails component from our stack with this config: # Skip frameworks you're not going to use config.frameworks -= [ :action_web_service ] bundle exec ruby script/server -p 3001 => Booting WEBrick... `require': cannot load such file -- soap/rpc/driver (LoadError) rails-1.0.0/lib/rails_info.rb:8: syntax error, unexpected ')' (SyntaxError) map {|(name, )| name} Cool cool, you could do .map { |(param1, )| param1 } in Ruby 1.8 to ommit the second block param. You can actually do it in Ruby 3.3 but you don’t need this extra comma: {a: 1, b: 2, c: 3}.map { |a,a } # => [:a, :b, :c] # or {a: 1, b: 2, c: 3}.map { |(a)| a } # => [:a, :b, :c] # without {a: 1, b: 2, c: 3}.map { |a| a } # =>[[:a, 1], [:b, 2], [:c, 3]] And one more time… bundle exec ruby script/server -p 3001 => Booting WEBrick... [2024-01-15 21:23:25] INFO WEBrick 1.8.1 [2024-01-15 21:23:25] INFO ruby 3.3.0 (2023-12-25) [arm64-darwin23] [2024-01-15 21:23:25] INFO WEBrick::HTTPServer#start: pid=98161 port=3001 Oh my God, we did it! If for some reason you want to check the code here it is: https://github.com/nashby/rails-from-the-past I’m pretty sure there are way more issues to fix to make it work properly but I’m not going to do it. I’m just happy enough that I’ve got to the point where I can see this greeting screen! Fin. And may Ruby be with you! Related posts Ruby 3.0 changes how methods of subsclassed core classes work. 12 Apr 2021 PostgreSQL’s IS DISTINCT FROM. 02 Jul 2018 Rails API and Facebook login (featuring Doorkeeper). 08 Jun 2018",
    "commentLink": "https://news.ycombinator.com/item?id=39012235",
    "commentBody": "Ruby 3.3 on Rails 1.0 (nashby.github.io)241 points by thunderbong 22 hours agohidepastfavorite70 comments dchuk 15 hours agoThis is awesome to see that it's possible. I've always wondered about the same sort of concept, but instead it would be trying to use Crystal to run the latest Rails version. I lack the time/energy/knowledge to try it, but it would be interesting to see how close you could get to either power or port Rails to Crystal, and if it would even be worth it (in terms of speed and type safety, etc). reply helen___keller 14 hours agoparentMy understanding is that this tends to be difficult because certain really-dynamic-stuff that Ruby offers (effectively metaprogramming) are not supported by Crystal, but are heavily used by Rails. On the other hand, Amber offers a look of what a Crystal rewrite of Rails would look like reply wmoxam 8 hours agoparentprevI've done that with Liquid (https://github.com/wmoxam/liquid-crystal), and it's roughly 2X faster than Ruby 3.2 w/yjit reply zoky 8 hours agorootparentAdmit it, you just did it because the name is perfect. reply yxhuvud 14 hours agoparentprevWell, you'd have to move a whole lot of code generation from load time to compile time, including the stuff based on db layout. So quite a bit of work. reply jrochkind1 11 hours agoparentprevI mean, by making some changes to Rails source code, it was possible to get a simple \"hello world\" to display. Presumably you'd run into more bugs if you tried to do more than that. Rails does have a test suite (hopefully Rails 1.0 had a decent one?) , I guess the interesting experiment would be how much code do you have to change to get Rails 1.0 test suite to pass on ruby 3.3. Safe to say quite a bit more than for the hello world to run! reply e12e 11 hours agorootparent> it was possible to get a simple \"hello world\" to display. That's not really the whole story. It was possible to generate a hello world web application server. Would've been interesting to see if other generators, generated migrations, models, controllers, views and tests worked (scaffolding blog posts and comments). reply jrochkind1 10 hours agorootparentTrue; the generator for a \"hello world\" works to generate a \"hello world\" app that works! It's definitely way less than full support for a real Rails app though! reply kichik 18 hours agoprevThat was a fun read. You should do Rails 8.0 on Ruby 1.8 next. reply latortuga 15 hours agoparentIn theory this is a funny idea but I think it would be a huge waste of time. Ruby syntax has changed and added so much since since 1.8 that you'd be stuck fixing thousands of syntax errors. reply Alifatisk 13 hours agorootparentThere is a polyfill gem that allows older Ruby versions access the newer stuff, wouldn’t that help? reply caseyohara 10 hours agorootparentPolyfill libraries can provide methods that were added in newer versions of the language (e.g. to std lib classes), but they can't provide the new syntax that has been added. Ruby has had a lot of syntax changes over the years that are incompatible with an older interpreter. An easy example is the 1.9 shorthand hash syntax for symbol keys `{ name: \"pants\" }` instead of the hashrocket syntax `{ :name => \"pants\" }`. More recent examples would be keyword args or pattern matching. reply cortesoft 16 hours agoprevI remember starting on Rails 0.8, where migrations were just sql files you ran reply buffington 7 hours agoparentDidn't most people use ant to deploy back then? I think it was a Java build tool or something? I feel like I should know this better - the maintainer of ant was actually working for us at the time, deploying early Rails apps using ant. I just remember deciding right then and there that I very much preferred writing code over troubleshooting deploys (and what we now call \"devops\"). reply cybrox 11 hours agoparentprevWhere we're going, we don't need any rollbacks! reply Rodeoclash 12 hours agoprevOof, anybody else get a wave of nostalgia seeing that start page! reply ElCapitanMarkla 11 hours agoparentYeap straight back to late 2007 when I first created a rails project in prep for my first job out of Uni. reply ryanbigg 12 hours agoprevThis is pretty incredible work! (Action web service was an attempt to make Rails generate SOAP APIs as that was the style du jour at the time. Fortunately, JSON came along!) reply alberth 17 hours agoprevRegression testing. It'd be a nice regression test to have: - Rails pegged at a particular version number and test the runtimes. - Ruby runtime pegged at a particular version and test Rails versions reply lloeki 15 hours agoparentHold my refreshing beverage https://github.com/lloeki/minimal-rack {rack 1 2 3, rails 3.2-7.1, sinatra 1 2 3, grape} x {ruby 2.1-3.3} x {puma, thin, webrick, ...} reply semiquaver 17 hours agoparentprevThe rails team already does the latter, subject to their specified compatibility matrix. Current rails requires Ruby 3.1, so CI only runs on that version and later. https://buildkite.com/rails/rails/builds/103981 reply renewiltord 16 hours agoprevHaha, this is really cool. And if I'm honest, very often it'll be a blog post like this that will solve some other issue I'm encountering. Thank you! reply wavemode 17 hours agoprevAs much as people complain about Python, Ruby really went through just as much (if not more) churn over the years from backwards-incompatible changes. reply halostatue 16 hours agoparentRuby has had two major backwards-incompatible change inflections: 1.9 and 3.0 (although I think that most of the removals were actually in 3.1). The painful one is 1.8–>1.9, because that's where Ruby's string grew encoding awareness. Going from 2.7->3.0 was not nearly as painful (mostly dealing with warnings and some code being removed from the standard library as gems that people can use if they want). I still maintain code that is nominally 1.8 compatible and it works just fine on 3.2 (I haven't yet tested on 3.3). What is surprising about this exercise is how little code needed to be changed on code designed for Ruby 1.8 to allow it to run on Ruby 3.3. The author doesn't say how long they spent on this, but it looks like 3–4 hours for a fairly complex piece of infrastructure code. How much effort would be required to run Django 1.0 on Python 3.12? At least this much, I’m sure. I suspect more, but I don't know the 2->3 differences well enough to be able to estimate that. reply herbst 30 minutes agorootparentThis so much. In the first part my my work carrier I was fixing old PHP applications, nobody ever bothered to even mention software updates. The next part, in a ruby shop, was different. We would update all the software all the time, usually pretty painless or with one or two expected issues that are easily fixed. There was no point when it turned to old, every app was running the most recent rails and usually ruby just days after the release reply byroot 16 hours agoparentprevAlmost all of the issues mentioned in this post were during 1.8 -> 1.9 transition. At that time odd release numbers where development versions, until this policy changed a bit later and 1.9 became an actual \"GA\" release. The 1.8 -> 1.9 is really Ruby's \"Python 3\" moment, as it similarly improved String encoding handling, but the breakages were much more limited and it also came with pretty significant performance improvements, so the community didn't split like Python 2 and Python 3. If you follow Ruby's development, or watch Matz talks, you see that avoiding a Python like split is really his number 1 concern. reply jrochkind1 11 hours agorootparentI agree with your analysis, as a rubyist since ruby 1.8. > but the breakages were much more limited and it also came with pretty significant performance improvements, so the community didn't split like Python 2 and Python 3. I think perhaps the ruby community was also smaller (or at least not TOO large), had an average higher level of skill (goes with the first, usually), and was fairly committed to ruby and/or rails at the time (we loved it, there were few if any similar alternatives). I agree that there hasn't been a similar level of backwards breaking since 1.8 => 1.9, and that if there were now it would be much more disastrous. Keyword arguments in ruby ~3.0 come closest, but still were much less disruptive than 1.8=>1.9. I have never been a serious python user, so can't really compare to python 2=>3, but my impression is that python 2=>3 was bigger even than ruby 1.8=>1.9. *BUT* that other typical python \"minor\" releases may actually have fewer backwards breaking changes than typical ruby \"minor\" releases? reply 0x457 8 hours agorootparentWell, the most vocal ruby community was RoR users, while python even back then was used by multiple camps. Ruby done a smart upgrade path - new features in 2.x and breaking changes in 1.9.x. Ruby also got a \"new\" VM (it wasn't new, but it was the new default VM). Before 1.9.3 the best way to deploy ruby was Passenger with RubyEE. RubyEE was based on 1.8.7 and with all the improvements that the mainline ruby implementation got there was no reason to maintain RubyEE fork. The 2.0.0 release meant to be 100% compatible with 1.9.3 and had ABI version 1.9.1. Also, prior 2.1.0 Ruby used versioning very different from SemVer, hence 1.9.x had multiple breaking changes in its lifetime. It didn't mean \"patch\" version back then. I think the main difference is that the newer ruby was actually faster than the older ruby and gave plenty of reasons to move. The main pain point im ruby migration was that it became encoding aware. Python 3 wasn't as lucky. reply byroot 10 hours agorootparentprev> I think perhaps the ruby community was also smaller Back in 2008-2010 when Python 3 was released, Python really wasn't as huge as it is today. It probably was bigger than Ruby, but not by that much. One datapoint (that doesn't give the full picture), Pycon 2008 had 1k attendees [0], Railsconf 1k8 [1]. IMO It's really in the early/mid 2010's with the rise of \"big data\" that the Python community really became huge compared to Ruby's. > I have never been a serious python user, so can't really compare As an ex-pythonista, now Rubyist, IMO one huge factor was that Python 2 and 3 were developed concurrently for several years, that played in the messaging. Python 3.0 was released on 2008, Python 2.7 in 2010, and Python 3 wasn't really deemed production ready until 3.2 or 3.3 if I remember correctly. So there was a 4-5 years limbo. Whereas in Ruby's case, it was very clear to the community that 1.9.x was the future. Also Python 3 did make some absolutely necessary breaking changes (like unicode), but also some minor syntax changes that made it much harder to support both with a single codebase for questionable gains. e.g. the `try/except`syntax was `except Class, var:` in 2.x, and `except Class as var:` in 3.x. That kind of changes made it hard for packages maintainers to support both versions. Ruby 1.9 didn't have this problem. It was much easier if not trivial to write code that worked on both Ruby 1.8 and 1.9. > typical python \"minor\" releases may actually have fewer backwards breaking changes than typical ruby \"minor\" releases? I haven't seriously used Python in a long time, but from occasionally glancing at release notes, they seem to regularly deprecate things. Can't say if more or less than Ruby. [0] https://pycon.blogspot.com/2008/04/part-2-attendance-registr... [1] https://www.endpointdev.com/blog/2008/06/railsconf-2008-repo... reply jrochkind1 8 hours agorootparent> Ruby 1.9 didn't have this problem. It was much easier if not trivial to write code that worked on both Ruby 1.8 and 1.9. Oh yeah, that is hugely important, and I agree ruby does this; it's usually not hard to write code that will work on at least the last handful of ruby releases. The keyword arg changes initially made this difficult in one particular case -- and this was seen as a problem, so they introduced the `ruby2_keywords :method_name` thing, to make it possible with an opt-in to make sure you had identical behavior in all versions, before the ruby version was released that would have made it impossible. reply formerly_proven 2 hours agorootparentprev> I haven't seriously used Python in a long time, but from occasionally glancing at release notes, they seem to regularly deprecate things. The meaning of deprecation changed a bit. Before 3.6 or so things were deprecated, but just left in and maybe eventually removed when they broke for good, which would rarely happen. Nowadays deprecation means it will actually be removed after two releases (years), and a lot of stuff was deprecated basically just because it was old. I don't think that's an amazing signal to send, honestly. reply multiplegeorges 16 hours agoparentprevI had the opposite conclusion. This seems like a pretty minimal set of changes to get a 19 year old piece of software running. reply shellac 15 hours agorootparentDitto. Having had to update python code over the years, this looked much easier than python migrations. To be honest moving earlier python 3.x code to current 3.x is comparable, so good job ruby devs. reply JohnBooty 15 hours agoparentprevStrongest possible disagree. There was some pain around Ruby 1.8->1.9 because of string encodings, but no huge schism ala Python 2/3. Did you experience both transitions or is that just your feeling based on this article? If anything, I think the article proves how relatively painless the transition was. reply buffington 7 hours agorootparentI agree with your strong disagree. I know, and help maintain, Ruby code written for 1.8 that hasn't been touched and is running fine in 3.2. No breakage along the way. To be fair, that's not always possible, but Ruby's changes haven't ever been drastic. I've not had as much experience with Python as a programmer - more as a user of apps written in Python, and I sense there's a bigger issue there than Ruby has ever had. It seems like a frequent problem that something that works in Python 3 doesn't work with Python 2 and vice versa. reply kshahkshah 13 hours agoparentprevHard disagree. For a moment there it wasn't clear if there was going to be a hard Python 2 vs 3 fork and people would literally refuse to migrate to 3. There was never such a moment in Ruby. The migrations from version to version were relatively smooth except 1.8.6 to 1.9 MRI -> YARV reply fny 17 hours agoparentprevBut it was done slowly enough not to piss anyone off. reply tonyedgecombe 13 hours agorootparentThe transition to Python 3 took a very long time. I often wonder if they would have saved themselves a ton of grief if they had done it quicker. reply 0x457 8 hours agorootparentThat's not what they meant. Ruby made breaking change by breaking change, and it was still easy to have gems work with both ABI versions. Python 3 had changes that made it necessary challenging to support both, all while being slower (IIRC). reply formerly_proven 17 hours agoparentprevRuby is used a lot less so the base population of people able to complain is much smaller. reply peteforde 13 hours agorootparentIt's used less but those who use it mysteriously make a lot more money. I know which group I'm happy to be a part of. reply bdcravens 14 hours agoparentprevFrom an outsider's perspective (I've written a little Python, but not enough that I consider myself a Pythonista) I didn't think it was backward incompatibility that was the problem as much as community libraries and frameworks not moving forward to v3 for a long time. reply lambdaba 18 hours agoprevIf the author reads this, I think you shouldn't bind space to the side menu, I (hope) many people know you can scroll with space. reply andrewmutz 17 hours agoparentI dont experience that. Perhaps you pressed tab once beforehand to focus the menu? reply lambdaba 17 hours agorootparentah, yes, but since there's no focus indicator it's not obvious reply nightpool 17 hours agorootparentLooks like it has a focus indicator to me, although I can see why it might be confusing: https://imgur.com/a/Mc6A7Dn reply lambdaba 17 hours agorootparentYeah, I guess I pressed tab right away and thought it was the design of the button. I think it's better to keep the focus style as the default outline, and uniform. reply anamexis 18 hours agoparentprevI don't know if they fixed it really quickly, but space does the normal scrolling for me, on Chrome and Safari. reply W3cUYxYwmXb5c 16 hours agoprevRoR was cool way back when, but these days I'm much happier using nodejs with a framework like Adonis reply tomca32 16 hours agoparentOoof...I'm not honestly. I don't do Rails that much these days but when I do it's just crazy how productive I can be. I think it's still a great fit for a huge number of web applications. If performance is very important there are better options like Rust or Go, but otherwise RoR is my go to. Pretty much everything you need is baked into the framework and it's still being actively developed. Some of the new stuff is pretty great like Turbo frames and streams. I respect that you enjoy working in Nodejs but Nodejs frameworks are just so barebones in my experience. I haven't tried Adonis but I still remember when I was looking how to parse query params in Koa. The official docs said to use a third party lib or regex them yourself from the request string...like why would I even use a framework at that point? reply wkjagt 10 hours agoparentprevI'm actually really excited about the direction Rails is going in. It's like it's shedding some of the complexity that was accumulated over the years in favor of things I can finally understand again. reply zilti 16 hours agoparentprevI don't get people who voluntarily use JS outside a browser. I'd rather jump into a pit filled with rusty spikes. reply rfw300 15 hours agorootparentThat’s fair, but TypeScript coupled with modern JS dev tooling makes it actually a pretty palatable experience these days. Speaking as someone who used to want to jump into pits for JavaScript-related reasons. reply peteforde 13 hours agorootparentIsn't that the difference, though? People who legitimately love Rails because of how productive they are, versus \"pretty palatable\". It's wild how people insist on using less-good tooling because that's what they've heard they are supposed to do... and that that's as good as they are going to get. It's not! reply sidkshatriya 16 hours agoprev [–] This whole blog post is essentially an advertisment for why statically typed languages can prevent such madness. I'm sorry -- it is not my intention to start another non-useful static/dynamic typing debate. It seems crazy that when a new version of Ruby comes out, developers need to hunt for runtime errors (that may or may not trigger depending on the path the application took) to see what incompatibility needs to be fixed. Take any decent statically typed language. When a new version of the language comes out, your code will not compile on constructs or functions that are no longer called or present or used the way they should. reply apiguy 15 hours agoparentNot being able to run a 15 year old codebase on ajust because you prefer to find your exceptions at compile time doesn't mean that it's the best way to find them. It is indeed the best way to find them. At compile time, you get errors for all possible paths a program will take. For dynamic languages like Ruby you will get an error only if the program takes a path through problematic code and then Ruby will flag the error. This means a runtime error could lie latent in your codebase for many more weeks and months. Only if a rare condition triggers a code path that contains the incompatibility. This is also why refactors in languages like Ruby are more difficult and conservative. As you're never sure you fixed everything. reply Contortion 15 hours agorootparentThis seems like a semi-moot point considering that after you've updated something you (presumably) also run an automated test suite and (have someone) test the application manually. I've also had various occasions where code compiled successfully but no longer worked as intended. reply sidkshatriya 14 hours agorootparentThe automated test suite can check many code paths but compilation of a statically typed language checks all possible code paths. Put another way, automated test suites give you an extremely high level of assurance when using a statically typed language. When your test suite passes in the new version Ruby, you're happy but there still could be cases left that you've not dealt with in rarely triggered code paths/conditions. reply jshen 13 hours agorootparentThe problem with this argument is that you never mention the costs or trade-offs of a statically typed language. You presume that you get the benefits for free and I'm certain that is not the case. The worst systems I've ever worked on were ones with complex and poor types and type hierarchies. reply sidkshatriya 12 hours agorootparentWhat are the costs of a Ruby incompatibility just waiting to be discovered in production ? You can't assume you wrote tests to exercise every possible branch in every method in every object ? The costs of static typing are reasonable as long as you're not using a fancy dependently typed language. Ask companies that are maintaining long running software -- types help. The investment is there in the beginning, the payoff is over the lifetime of the project. If types are complex and poorly defined, you can change them ! The compiler will help you evolve your system through type errors. If you have a poorly structured program in a dynamically type language like Ruby then it becomes more difficult to evolve your system fast and with confidence. You're always asking -- have I missed something out ? reply jshen 11 hours agorootparentI've been writing software for a long time in many different types of languages. I've led many software teams for a long time that use different languages and tech stacks. I have not seen any measurable difference in productivity or defect rate across different languages. I have also looked at all of the research on this and it is inconclusive at best. Ultimately, I think that choice of language is one of the least significant predictors of outcomes, yet it's one of the most debated and obsessed over. Edit: I thought it would be helpful to give examples of what I think is more important. Good CI/CD practives, good observability, robust test and staging environments, etc have been far more important in my experience than static vs dynamic language choice. reply wavemode 11 hours agoparentprevThis is a great example of the difference between theory and practice. In theory, you're correct and type errors could be hiding out in a dynamically-typed codebase after a language version upgrade, lying in wait to take your system down. In practice, nothing like what you're describing has ever happened to me when working in large (1M+ LOC) Ruby and Python codebases. In my experience the difference maker is testing, not type checking. I've worked in poorly tested Java codebases where upgrading a dependency still allowed the application to compile and deploy successfully, but then at runtime things started blowing up randomly. Meanwhile with well-tested dynamically-typed code I've never had any such issues. reply rapind 15 hours agoparentprevI've been using rails for well over a decade (since version 2), and loving it. For the most part upgrades have been relatively painless by choosing to always lag a little behind on non-security related upgrades. There have been times though when upgrades have been extremely painful. The massive shift between versions 2 and 3 (merb integration), the openssl shenanigans (ruby v2 -> v3), and the mimemagic / shared-mime-info licensing issues are the major ones that come to mind. That being said, I use Elm on the front end a lot and the confidence it gives me about my code is like night and day compared to ruby (or javascript). reply ahoka 16 hours agoparentprevJava is “statically typed” amd has the same issues during runtime with different JVM versions. reply harikb 16 hours agorootparentPeople try to run JVM versions across 10 years and face \"small\" issues with builtins. GP's point was mainly about missing interfaces, missing methods, typos etc. reply sidkshatriya 15 hours agorootparent> People try to run JVM versions across 10 years and face \"small\" issues with builtins. GP's point was mainly about missing interfaces, missing methods, typos etc. Indeed -- well put. > Java is “statically typed” amd has the same issues during runtime with different JVM versions. Nothing is ever perfect -- there can be some occational runtime incompatibilities when very old Java code is run on new VMs. But the effort in moving between one Java version and another Java version that came out many years later is likely order of magnitudes less than if you did the same thing in Ruby. There is simply no comparison. reply TheSmoke 11 hours agoparentprevsure, take some scala 2 library and make it work with scala 3 / dotty with an effort similar to this one. or java 1.8 and 17. we'll wait. reply Berone 15 hours agoparentprev [–] Heavy Ruby/Rails user here. We basically run type checking on major hand-offs between modules using DrySchema/DryValidation, a gem that makes it easy to set that up. Without doing this, the integrity of the codebase erodes with scale. reply Lio 14 hours agorootparent [–] I'm not sure I'd recommend DrySchema or DryValidation. I've had real problems upgrading client's projects built using DryValidation prior to v1 in the past. They changed the syntax of the validations just enough to break stuff and completely removed features like shared predicates. The upgrade process was very poorly documented with suggestions to go read random blog posts. I think the replacement for shared predicates was supported be macros but even that is marked as likely to be removed in v2. Just looking through some of the Dry changelogs the work BREAKING still features far more than I'm happy with. I'd be much happier to use Sorbet or an RBS based typing solution than work with Dry again. It might have advantages over ActiveModel but ActiveModel is far better supported. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author, Vasiliy Ermolovich, explores the possibility of running Rails 1.0 on the latest Ruby version, Rails 8.0.",
      "They encounter challenges such as syntax errors, missing libraries, and string encoding changes.",
      "Ultimately, they are successful in generating and starting an app on Ruby 3.3, although further issues may need to be resolved for complete functionality."
    ],
    "commentSummary": [
      "The article explores the potential of using Crystal instead of Ruby for running the newest version of Rails.",
      "It discusses the challenges and modifications required for this transition and compares it to the experiences of transitioning from Ruby to Python.",
      "The discussion also covers the differences in backward compatibility, migration issues, advantages and limitations of statically typed languages, and the significance of good CI/CD practices and testing environments."
    ],
    "points": 241,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1705405687
  },
  {
    "id": 39013036,
    "title": "Is the Era of Passwords Over? A Look at the Evolution of Authentication Methods",
    "originLink": "https://jcarlosroldan.com/post/315/passwordless-a-different-kind-of-hell",
    "originBody": "16 ene 2024 / code, think Passwordless: a different kind of hell? It's no secret that authenticating into services is an unresolved topic. With time, we have managed to make them more secure, but that was at the expense of user experience. The new generation of mail codes and authenticator apps has moved us from the ease of one-click browser autocomplete to complex ordeals involving multiple steps and sometimes multiple devices. Last month, I was logging into Notion after it automatically logged me out, and I couldn't help but think \"It feels like I'm logging in here every second week; maybe I'm doing something wrong.\" After a long examination of the settings, I decided to open a ticket asking if the session length was indeed that short. The response from Notion's team was prompt and specific, a great example of customer service. However, the content of the answer was less pleasing. Notion is not alone in this; many other services enforce similarly short sessions and uncomfortable methods. This has me pondering the evolution of our authentication methods, from their ancient beginnings to modern complexities. Let's take a look at the history of authentication methods and rate them on two scales: user experience and security. The first recorded password in western history is the book of Judges. Within the text, Gileadite soldiers used the word \"shibboleth\" to detect their enemies, the Ephraimites. The Ephraimites spoke in a different dialect so that they would say \"sibboleth\" instead. Experience ★★★★★: you just had to say a word. Security ☆☆☆☆☆: there's a single word to authenticate multiple users and it can be cracked by learning how to spell it. Ancient Romans also relied on passwords in a similar manner called them \"watchwords\". Every night, roman military guards would pass around a wooden tablet with the watchword inscribed and every military man would pass the tablet around until every encampment marked their initials. During night patrols, soldiers would whisper the watchword to identify allies. Experience ★★★☆☆: you just had to say a word but you have to memorize it every day. Security ★☆☆☆☆: it changes every day, but it's still a single word, and without a \"forgot password\" button, a wrong answer would mean a spear in the gut. Fast forward to the '20s, alcohol became illegal in the US, and speakeasies (illegal drinking establishments) were born. To enter the speakeasy, people had to quietly whisper a code word to keep law enforcement from finding out. Code words were ridiculous, to say the least: coffin varnish, monkey rum, panther sweat, and tarantula juice, to name a few. Experience ★★★★☆: you just had to say a word, and they were made to be memorable. Security ★☆☆☆☆: it's a single word, and it's not even a secret, but at least you don't get stabbed for getting it wrong. The first recorded usage of a password in the digital age is attributed to Dr. Fernando Corbató. In the 60's, monolithic machines could only work on one problem at a time, which meant that the queue of jobs waiting to be processed was huge and a lot of processing time was lost. He developed an operating system called the Compatible Time-Sharing System (CTSS) that broke large processing tasks into smaller components and gave small slices of time to each task. Since multiple users were sharing one computer, files had to be assigned to individual researchers and available only to them, so he gave every user a unique name and password to access their files stored in the database. However, these passwords were stored in a plaintext file in the computer and there were a few cases of accidental and intentional password leaks. Experience ★★★☆☆: you have to remember a user and password. Security ★★☆☆☆: it's one per user, but they're stored in plaintext. To prevent the problem of plaintext passwords, Robert Morris and Ken Thompson developed a simulation of a World War 2 crypto machine that scrambled the password before storing it into the system. This way, the system could ask for the password, scramble it, and compare it to the scrambled password stored in the system, a process called one-way hashing. This simulation was included in 6th Edition Unix in 1974, and got several improvements up to our days, but the basic idea remains the same. Experience ★★★☆☆: you have to remember a user and password. Security ★★★☆☆: it's no longer plaintext, but stealing it would still give you access to the system. Over time, many different problems arised from the fact that people use the same password for multiple services, so the industry started to push for unique passwords for each service. This was a problem for users, since they had to remember a lot of passwords, and password managers were borned. The first password manager was developed by Bruce Schneier in 1997, and currently every major browser comes with a built-in one, often with an option to generate strong passwords and store them for you. Experience ★★★★☆: you have to remember a master password, but the browser remembers the rest. Security ★★★★☆: it's no longer plaintext, but the master password is the weakest link in the chain. Phishing attacks and data breaches have made passwords a liability, so the industry has been pushing for multiple-factor authentication (MFA) for a while now. 2FA is a method of authentication that requires two different factors to verify your identity. The first factor is usually something you know, like a password, and the second factor is something you have, like a phone. This way, even if someone steals your password, they still need your phone to log in. There is a myriad of ways to implement 2FA, but the most common ones are SMS codes, authenticator apps, and mail codes. It is often used in conjunction with very short session lengths. Experience ☆☆☆☆☆: you have to remember something, have a phone or mail app, and it requires multiple steps. Security ★★★★☆: it's no longer a single factor, but it's still vulnerable to phishing attacks. I, like most people, hate passwords and all means of authentication bureaucracy. And it looks like we're now at the lowest point in history in terms of UX. There is still hope with the rise of Single Sign-On (SSO) and biometrics. And certainly passkeys, which are getting a lot of traction lately, are a step in the right direction. But only time will tell if their adoption will be widespread enough to make a difference or if we'll be stuck in this dark age of authentication experience for a while. Related posts: The XOR reversible cipher The User-Agent problem",
    "commentLink": "https://news.ycombinator.com/item?id=39013036",
    "commentBody": "Passwordless: a different kind of hell? (jcarlosroldan.com)195 points by juancroldan 20 hours agohidepastfavorite358 comments michaelt 19 hours agoI recently ordered something on ebay. Nothing expensive, just a £60 item, and delivered to an address I've ordered many things to in the past. First I had to log into ebay - no problem, got my password manager right here, as soon as I unlock my phone with my fingerprint. Now I'll just key in my 12 character, randomly generated password with mixed case letters, numbers and symbols. Then ebay decided they wanted to send me a code by SMS. I'd never enabled that security option, but whatever. I can do that, quick fingerprint to unlock the phone then key in the code. Then I chose to pay with paypal, requiring a second password. And a 2FA code, this time from a TOTP app. For some reason paypal ask for TOTP every time. Easy enough, quick fingerprint auth then just key in the code. Then I told paypal I wanted to pay by card, as I always do. They redirected me to my bank, who asked me to use their mobile app to authorise the payment with my fingerprint. After unlocking my phone with my fingerprint, naturally. Clearly, the days when businesses thought online shopping ought to be low-friction are long gone. reply EnragedParrot 18 hours agoparentApple makes this experience as seamless as I think it possibly can be. (As long as you use Safari...). All my passwords synced across all devices all the time, instantly available with faceID or or my fingerprint. Apply pay makes checking out of most online retailers as fast as using my fingerprint or double-clicking the side button on my phone. Passkeys generally starting to replace passwords on many major sites, making the process even faster. reply wharvle 17 hours agorootparentThat whole process in the top level comment is much faster, in practice, on my phone. Everything auto-fills (unless a site manages to fuck up their forms). I don’t typically have to type or manually copy anything, including 2fa tokens. Wait for the notification to ping, “fill from message” option, done. I can often go through an entire sign-up, entering shipping, and payment, at a new site, without typing a single thing. reply szundi 13 hours agorootparentThis is slower than Apple Pay on the iPhone, I can assure you. reply wharvle 13 hours agorootparentWell, yes (I also use Apple Pay when it’s available—best overall experience by a long shot) but it’s still quite fast and often involves no typing or copy-pasting. reply hcurtiss 17 hours agorootparentprevHow are you populating non-SMS 2FA codes automatically? reply InitialBP 17 hours agorootparent1Password can do this for you, and I assume many other password managers as well. https://support.1password.com/one-time-passwords/ reply sgarman 15 hours agorootparentI use 1password but opt out of this feature. Just as described in the article masterpassword creates a single source of failure so I don't personally want to put more eggs in that basket. reply jorvi 15 hours agorootparentI keep my unimportant 2FA in 1Password and the really important one’s (e-mail, domains, etc) in a separate 2FA app. If someone has pwned my 1Password I don’t really care if they log on to my Discord or order a limited amount of crap on Amazon because I am in much deeper shit at that point. reply DHPersonal 16 hours agorootparentprevApple hardware can auto-fill 2FA codes if the codes are set up in the Passwords tool on iOS/iPadOS/macOS, which are synchronized through iCloud. reply alwayslikethis 2 hours agorootparentprevKeepassXC can act as a TOTP client and can fill it just like it can do passwords. reply Aerbil313 48 minutes agorootparentpreviOS’s built in password manager iCloud Keychain does this automatically (at least on Safari). reply sage92 13 hours agorootparentprevIf you use BitWarden paid version ($10/yr) then after an autofill of username/password, the totp is automatically added to the clipboard. reply wharvle 17 hours agorootparentprevThat, I don’t, but I only have those on work accounts anyway. None of my work stuff is set up to be as nice as my personal stuff, but that’s mostly outside my control. Oh, wait: Steam has them I guess. Every so often (once every few months?) I have to type in one of their codes. I did just check and I guess I could be doing this with non-sms codes if I added them to my password manager. If I had more than just Steam that used them, I’d do that. reply dgellow 16 hours agorootparentprevHow does that work if you want to get an android phone or Samsung tablet or windows laptop at some point? reply efitz 14 hours agorootparentI love the Apple ecosystem, however I always have a low level of dread that someday I will somehow offend them and be permanently blacklisted. This is the main reason I've drawn the line at using their password manager or email - I use separate email and separate password manager so that in a worst case situation I don't get locked out of everything. reply swozey 11 hours agorootparentDon't worry, Google actually did lock me out of everything a few years ago and when you have the pleasure of using their wonderful services you're literally given no information and have to google (hehe) around for a form to send in a picture of your drivers license to which you will never receive a reply, your google account will remain \"fraud blocked\" and in 4 days you will have switched your entire life over to Apple/IOS to never deal with no customer service google again. Then 1 yr later a hn thread will remind you to try to log into your google SSO and.. bam it works. And you still have no idea why ALL of your g servces (domains, email, gphone, etc) were disconnected a year ago. reply midasz 2 hours agorootparentThis is why I don't mind paying the 5 euros a month for a Fastmail account. I don't send many emails but it's pretty much the key to the kingdom. reply Aerbil313 41 minutes agorootparentprevI used to think the same - custom email domain, passwords managed by myself, but: 1) I’ve never ever heard Apple lock someone out of their Apple ID. Maybe they are obligated to do it for law enforcement in US but even none of that. Meanwhile I’ve heard a ton of stories of Google locking people out of their accounts. 2) The convenience of using Safari, with 2FA and passkeys set via iCloud Keychain is too good to ignore. Literally 1 click (passkeys) or 2 clicks at most, authenticated with Face ID. So I’m using this setup rn. You can set custom domains with your iCloud email too. reply MDWolinski 13 hours agorootparentprevNot to be argumentative, just wondering, has there been a case related to iCloud access that Apple has ever blacklisted someone? Certainly, I've heard of Meta and other companies doing not, but don't recall Apple outside of security confirmation issues people are having. reply al_borland 12 hours agorootparentIf you have 2FA and lose all your 2FA methods, and didn’t preplan by making a recovery key and storing it in a safe place you can find again… you can be screwed. It’s not a blacklist, but the net result is the same. I’m terrified of losing access to all my stuff because of forced 2FA I never signed up for. I get that it’s more secure, but it can be secure to the point of having unrecoverable data. All it would take is someone carelessly deciding to get a new phone number. I have a friend who recently talked about wanting to get a new number with his new phone. I asked about 2FA and he seemed to have no knowledge of it and said he didn’t have anything like that. He kept his number, but if he didn’t, I could see him easily getting locked out of his Apple account (which he has), and his bank. reply Aerbil313 22 minutes agorootparentSetting up a recovery key for an Apple ID is optional. You can still recover your Apple ID. Apple will ask for information that can identify you, like previous iPhone passwords etc. If you have hit your head to a wall and can’t remember literally anything afair you are asked to wait someaccessibility > side button. You can adjust the speed required to register a double or triple click. reply DavideNL 14 hours agorootparentprevAgree, it's somehow unwieldy... not sure what it is exactly. reply bradley13 13 hours agorootparentprevI have a similar experience without Apple. But. Those synced passwords are a huge, juicy target. Someday, someone is going to get them. This process is a vulnerable mess. reply a_imho 17 hours agoparentprevOrder pizza, pay with virtual card. Payment provider needs 3FA+Captcha, one of the factors is email which is another 2FA challenge. Disclosing the card details once logged in prompts for another 2FA, finally VISA also challenges you with a recent payment question. Insanity. reply zeekaran 15 hours agorootparentThen they store your credit card info in a database and leak it some time next year. reply ryandrake 12 hours agorootparentIt's pretty annoying that they load all this pain and suffering onto the user who's just trying to make a purchase, when the company's database is often the weakest link. reply illiac786 4 hours agorootparentThis is fascinating to me. Why do we have to go through all these hoops with the bank and somehow, when the credit card # is eventually and ineluctably leaked, the thieves have no problem using it to make purchases, whiteout going through all these 3FA etc. How is that possible? reply efitz 14 hours agorootparentprevCaptcha IMO is way worse in terms of user experience than 2FA. And the only 2FA that I don't detest are app push and TOTP. reply hot_gril 16 hours agorootparentprevWell pizza in particular often has a cash payment option, which I always use for that. reply al_borland 12 hours agorootparentprevThis would be enough to have me drive to the pizza place myself and pay cash. reply cogogo 19 hours agoparentprevApple pay when available is about as low friction as you can get. I know it isnt available to everyone but there should be some similar standard that is. Near seamless. reply winphone1974 16 hours agorootparentOnly because you've standardized on their ecosystem and pre-given them all your data. This is not the future we were promised reply riversflow 16 hours agorootparentYou don’t have to give Apple your data. It uses information stored on device. reply jonathanlydall 16 hours agorootparentI’m a happy ApplePay user, but you absolutely do have to give them your (card) information upfront through the whole adding your card in the Wallet app. That being said, I feel the parent’s viewpoint is naively idealistic, the payment industry is huge with many players and most attempts at new standards or interoperability are by people trying to get a cut of the action, no one is going to adopt a new standard unless they feel they absolutely have to. ApplePay is pragmatic in that it largely hooks into the existing CC systems and thanks to Apple’s market size they have enough clout to convince people it’s worth the effort. A whole new standard just for the “general good of the public” will never get any traction without regulation, and in places like the U.S. where bribery is essentially legal (so long as you call it lobbying), any new regulation like this faces an extreme uphill battle to being introduced except where someone standing to make lots of money is behind it. reply vladvasiliu 16 hours agorootparent> I’m a happy ApplePay user, but you absolutely do have to give them your (card) information upfront through the whole adding your card in the Wallet app. Do you actually have to give them the card? Or is it only stored somehow on the phone? I wonder how this works exactly. When I replaced my old iphone with a new one, I did the whole \"transfer everything\" dance. Waited around for two hours (didn't restore from icloud, but transferred from old to new), and still had to manually add my CCs to Apple Pay again. reply rstupek 15 hours agorootparentIt's stored on your phone in the secure enclave. reply vladvasiliu 15 hours agorootparentThat's what I was thinking, which means you're not actually giving Apple your CC number. reply jonathanlydall 12 hours agorootparentMy experience is that you can start the process by entering your credit card details, or use your camera to try fill them in for you. Apple then checks if your card issuer has ApplePay enabled and if so provisions a “virtual” card which is what is stored on the device’s Secure Enclave. I also just checked my banking app quickly which can initiate the adding of the card to wallet, showing the wallet’s add card screen with the card holder name and the last 4 digits and asking if you want to proceed. There is no way to see what the full virtual card number is, so there is no way to use this virtual card aside from tapping your phone on CC machines or using websites which have set up ApplePay as a payment method. CC machines don’t actually have to support ApplePay specifically, as long as it supports tap to pay without insisting on a PIN, then ApplePay works with it. In essence your phone’s NFC exactly implements the same capabilities and protocols as NFC chips on normal credit cards. reply vladvasiliu 2 hours agorootparent> CC machines don’t actually have to support ApplePay specifically, as long as it supports tap to pay without insisting on a PIN, then ApplePay works with it. In essence your phone’s NFC exactly implements the same capabilities and protocols as NFC chips on normal credit cards. IIRC it's not exactly the same. One user-facing example where things are different is that contactless payments with a regular credit card have a 50 € maximum. If there is a limit when paying with the iPhone, it's much higher. I also seem to recall that the merchant's payment contract must support this, but I'll have to confirm with a colleague. Although Apple Pay support is very common where I live, it did happen a few times that some restaurant's terminal accepted VISA contactless but not Apple Pay. I've also had a situation where my CC is set up to not allow payments outside my country. Payment with Apple Pay was denied as being \"out of country\", whereas the physical card worked fine. The store is from a big national chain, in the heart of the capital city. reply jonathanlydall 1 hour agorootparentI’ve used ApplePay on CC machines which were clearly made before ApplePay even existed. I’m pretty sure that the limit amount before PIN verification is required is embedded in the NFC, or checked online or something. Both my credit cards have limits of R500 (~26USD) after which it requires I enter my PIN after tapping it. For one of my credit cards I’m able to pay it off with my other credit card and I have in the past tapped my iPhone to do so for payments over R50,000 (~2600USD), I don’t think there’s a limit. However, the biggest grocery retail chain here initially had a very annoying “custom” rule on their CC machines where it would ignore the card limit and insist on asking for PIN for any payments over R500, which would cause ApplePay tap attempts to auto decline, they eventually fixed this. The out of country issue sounds like a configuration issue with your bank or that particular merchant. My cards by default disallow use out of country and I’ve never had an issue tapping anywhere with ApplePay. reply cogogo 8 hours agorootparentprevI agree. No naive ideals here. There should be a standard that makes it easy and private for the consumer. Doesn’t mean it will happen. reply mgkimsal 16 hours agorootparentprevSeems like parsing semantics. \"Pre-given them\" - are you giving it directly to apple.com? No. You're putting in your hardware, true. And... somehow... it makes it to all your other apple devices. reply scoodah 15 hours agorootparentApple Pay is one of the (few) things where that is not the case. New phone = manually re-adding cards to Apple Pay. Get an Apple Watch? It does not get your Apple Pay info until you manually add them to the watch. reply tomjakubowski 14 hours agorootparentprev> somehow... it makes it to all your other apple devices. \"Somehow\" their information makes it around? No, you have to add them, yourself, on every device you use them from, individually. reply llbeansandrice 16 hours agorootparentprevIt's just a credit card though? Seems like a weird distinction when those details are intended to be given out. I presume if you're using one-time cards you're not using Apple pay at all. Plus you need the CVC code and such to re-auth them on new devices. Apple has issues with privacy, but I don't really see how this is one of them. reply bluepizza 11 hours agorootparentprevThe future we were promised is not being curbed by Apple and Google. It is being curbed by rampant cybercrime. reply makeitdouble 19 hours agorootparentprevAmazon is probably the lowest and will stay so for a while I guess. They didn't cling to their one click patent for nothing. reply rootusrootus 15 hours agorootparentThat's just because they already have all of your identification, shipping, and payment information stored. Apple Pay isn't quite one-click fast, but it's damn near a miracle for one-off purchases from retailers you don't normally use. I've definitely made purchases I'd otherwise have walked away from (I'm pretty selective about who gets my credit card number). reply ComputerGuru 18 hours agorootparentprevThe idiots removed the 1-click checkout feature and replaced it with a Dropbox to choose which address to deliver to, but it no longer ties that address to a payment method. reply sjfjsjdjwvwvc 17 hours agorootparentprevDominos has the best checkout experience I ever experienced online. Nothing can beat it IMO, at least nothing I came across. Now they only sell (arguable mid) pizza, but when I order there it’s delightful (to use an overused 2023 marketing buzzword) reply wharvle 16 hours agorootparentThey do a lot of interrupting the buyer with up-sell attempts. I'd have singled them out as notably bad, among fast food pizza chains, actually. reply sjfjsjdjwvwvc 14 hours agorootparentInteresting, which ones would be notably good in your opinion? To be fair we don’t have many fast food pizza chains in my country, it’s mostly dominos and a few small ones (with abysmal online order experience) reply wharvle 13 hours agorootparentWell, I was maybe a little unfair because the competitors have at least partially “caught up”, but at one point, of Domino’s, Pizza Hut, Little Caesars, Godfather’s, plus a couple online pizza store SaaS used by smaller local chains, Domino’s was the only one that would interrupt me to make me click “no thanks” to some offer or other before proceeding, including during checkout. Multiple times per order, in their case—they’d do it once or twice in the checkout flow, plus sometimes after adding an item to the cart. I dropped them from the “oops we failed at getting dinner ready, what can be delivered and is cheap-ish?” rotation for a while over it. They’re still the worst about it AFAIK but more of their competitors now do that at least once an order now, too, so the difference isn’t as large. reply chaps 15 hours agorootparentprevOff topic: once worked at a company that built a \"domino tracker\" of some security service we were installing on customer hosts. The company spent more time and money on the tracker than the service installation. The installation tooling failed most of the time and threw errors out for \"ephemerality\". Good times. reply chrisBob 16 hours agorootparentprevTheir regular round pizzas are ok, but now that PizzaHut is gone, the Dominoes pan pizza is my go-to. reply rusticpenn 15 hours agorootparentPizza Hut is gone? reply at-fates-hands 14 hours agorootparentNope. I worked in a Pizza Hut delivery place when I was in college. I just took my son back for a campus visit and yeah, 30 years later, its still there - same location and save a few minor changes, the building still has the exact layout. A testament to whoever laid out the original floor plan. reply alias_neo 17 hours agorootparentprevThat's because they need you to hurry up and pay for the terrible pizza before you change your mind. (I too eat Domino's on the odd occasion the app doesn't take long enough for me to change my mind). reply jorvi 14 hours agorootparentprevIn my city they used to have a 25 minute (!) click to door delivery guarantee. Extremely impressive. reply matthewfcarlson 18 hours agorootparentprevI’m not even embarrassed to say last night I went to check out, saw there wasn’t an Apple Pay option, waited through about 2 minutes of waiting for the credit card details panel to open before bailing. reply FireBeyond 14 hours agorootparentIf it took two minutes for a credit card form to open up that's clearly a site problem, and would likely have been just as broken even with an Apple Pay option. reply jjoonathan 19 hours agoparentprevGood job passing the dice roll to stay out of the special hell where the SMS code never arrives. reply bloomingeek 15 hours agorootparentNot sure if this is your experience, but when I broke a chunk out of my Samsung screen and then went to AT&T to trade for another Samsung, keeping the same phone number, I can't receive a two factor security code by text. Even after calling AT&T and being told that the traded in phone is \"dead\". So now I have to receive a call for security codes. reply graemep 17 hours agoparentprevThere are things you can do to make it easier. My phone sends all notifications to my desktop, and I have an app on the phone that creates a notification when it recognises a code in the SMS, so all I need do is double click on the notification (to select the entire \"word\" that is the code) then paste into the site I am verifying to. There are also authenticator browser extensions so you do not have to use a phone app for those either. The software I use for the SMS codes is KDE Connect and SMS code. reply cryptonym 17 hours agorootparentWe shouldn't have to work installing & maintaining an awkward flow with random software to make buying experience less miserable. This should be fixed by the seller in the first place, where it makes sense and can be fixed easily and reliably. reply squigz 17 hours agorootparentIn this case, how is eBay responsible for how PayPal and a bank handles things when they hand it off? reply cryptonym 7 minutes agorootparenteBay is responsible whatever partner they are choosing. They knowingly picked PayPal. If the integration is terrible, they can work on this with their partner and maybe find a common way to establish trust. reply jodrellblank 16 hours agorootparentpreveBay owns PayPal https://www.cnet.com/tech/tech-industry/ebay-picks-up-paypal... - August 2002 reply Operyl 16 hours agorootparentNo, eBay no longer owns PayPal. https://techcrunch.com/2014/09/30/ebay-paypal-split/ reply InitialBP 17 hours agorootparentprevThink about motivations for a moment. The seller is motivated to make the buying process as easy, fast, and uncomplicated as possible. This is a direct correlation with how many things they sell, and in response how much money they make. On the other hand - consumer opinion and regulation forces them to ensure that the buying process is secure, that someone else isn't buying things on your account, that they have proper logging of what goes on, etc. The seller shouldn't \"Fix\" the buying experience by removing the security aspects of it. They should fix the buying experience by using modern authentication like passkeys and ensuring that their applications and sites support password managers. reply dingnuts 17 hours agorootparentprevIn general I agree, but KDE Connect is not random software and it's fucking awesome, especially if you are a KDE user, for a lot of reasons. The use-case described in the grandparent is just one of many handy things available via KDE Connect reply pmontra 16 hours agorootparentI use GNOME: the gsconnect extension on my laptop, the kdeconnect app on my mobile devices. They can even share data and files between themselves without going through the laptop, ring another one when I lost it somewhere at home, control the media playing on another device or my laptop. reply waynesonfire 17 hours agorootparentprev> My phone sends all notifications to my desktop Is this a native phone feature or an app? You're lucky if that's the only place it sends notifications. reply blendergeek 17 hours agorootparentHe uses KDE Connect. I use is as well. It is amazing, open source, and only sends notifications where you tell it to. https://kdeconnect.kde.org/ reply waynesonfire 17 hours agorootparentThat's pretty cool; thanks. reply hot_gril 17 hours agorootparentprevOn Mac/iPhone it's built in. Somehow the phone isn't even part of this flow after initial setup, the SMSes go straight to the Mac. reply squigz 17 hours agorootparentprevWhy do you say that? reply stronglikedan 19 hours agoparentprevThat's weird. I just log in with my fingerprint only, and my paypal is linked to my ebay. I don't even thing I enter a second password or fingerprint to pay. Also, what the what is this? \"I'll just key in my 12 character, randomly generated password...\" Key in? Seems like you're making your own life hard! ;-) reply kulahan 13 hours agoparentprevThe goal is to cover their asses for when data is stolen. It’s not a matter of “if”, it’s “when”, and they want to be able to point to every obnoxious POS practice they made standard to show they did their best. I’m not making any comments on whether this is good or bad, just that it, to me, explains a ton of the n behavior. reply eddd-ddde 18 hours agoparentprevI've never had to authenticate with a bank for using a card? Is this common for you? reply JimDabell 18 hours agorootparentMFA is required in the EU: https://en.wikipedia.org/wiki/Strong_customer_authentication reply eddd-ddde 18 hours agorootparentReally interesting, here in Mexico I think that's unheard of, what I have to use is a digital card with a dynamic 3 digit cvv that's generated on my app. reply g-b-r 18 hours agorootparentprevIt's 3ds, I don't know if it's because of some regulation but with my current (european) bank it's always compulsory. And a credit card I've got recently also asks for a second code, after the 3ds code. reply PurpleRamen 18 hours agorootparentThe regulation for this is PSD2. https://en.wikipedia.org/wiki/Payment_Services_Directive reply g-b-r 18 hours agorootparentprevBy the way, the last time I checked using 3ds means that it's \"impossible that the transaction was fraudulent\" and thus you can't cancel it reply hakfoo 7 hours agorootparentI've heard it presented more as that the merchant isn't on the hook for a chargeback because they did the \"best practice\" in terms of preventing it. reply tuyiown 18 hours agorootparentprevYep, that's why it caught on by shop despite being a friction. reply kube-system 16 hours agorootparentprevYikes, what happens if you've had your devices/credentials stolen? Are you held liable for the transaction without recourse? reply groestl 13 hours agorootparentWhat actually happens is with 3DS: a merchant gets liability shift. Liability resides with the issuer then. Whether you as a customer can be held liable for damages depends on your jurisdiction and when you report your devices / credentials stolen. reply tuyiown 18 hours agorootparentprev> it's always compulsory No, it's the shop that decides actually. More and more accepts do direct payments from card numbers without additional checks, by the way. reply sgjohnson 18 hours agorootparentprevPretty common in Europe these days, due to PSD2 regulation. reply rootusrootus 15 hours agorootparentprevYears ago I had to do that sometimes, but I haven't gotten prompted to authenticate my credit card with my bank in quite a long time. I thought maybe it just went out of style, but I guess some people still use it. reply SoftTalker 17 hours agorootparentprevWhen I use my AMEX card online it sometimes does an extra \"validation\" step but as I recall I don't have to interact with it. It's probably checking location, etc, and deciding if further validation is necessary. Have never seen that with VISA or MC. reply drxzcl 18 hours agorootparentprevI have a Mastercard branded card issued by the Dutch quasi-monopolist (ICS). Every time I have a transaction with a merchant with ties to NL, they force me to do 2FA using their crappy app. I have no words to express how much I hate this. reply paholg 16 hours agorootparentprevI'm in the US, and for some purchases I have to. There's like an iframe in which I have to log into my credit card account, and approve the transaction. I'm not sure what triggers it. reply camhart 18 hours agorootparentprevIf the bank fears the charge is fraudulent sometimes I've had to do it. But normally I don't. reply psyclobe 18 hours agorootparentIf its the 'verified by visa' thing, it is in fact optional and you can cancel out of the wizard reply araes 9 hours agoparentprevThe involuntary signup for two factor you didn't want is incredibly annoying. Especially when initiated by a bank or similar financial institution with no warning. \"BTW, for your own safety, we implemented two factor on your account, and tied it to your old phone. Wait, you don't have that phone anymore, cause it was something like a 10 yr old retirement that you never obsessively check? And we didn't give you the option for an email? Or even warn you? Too bad. We now no longer accept logins for your own money.\" reply wmsmith 18 hours agoparentprevPayment gateways (paypal, apple, google), in general, do NOT let you cancel individual services and are linked to your CC. Vendors (I'm looking at you, Audible!) constantly hide their account termination under layers of dark patterns. For awhile, I had several ghost subscriptions that I a.) didn't want and b.) couldn't cancel. My credit card card [1] has fundamentally changed my online purchasing experience as it bridges what I feel is a gap between new payment methods (Apple, Google, et al) and classic payment methods (CC). An ounce of prevention is worth a pound of cure. When I purchase something line, I create a new one-time card (three taps on my phone) and use that new, valid CC for purchasing. Everybody takes a CC. The card is instantly deleted after purchase, and I don't have to worry about my paypal account, apple pay account, google wallet account, ghost subs, account hacks, identity theft -- the works. [1] https://x1creditcard.com/ reply the_snooze 18 hours agorootparent>Payment gateways (paypal, apple, google), in general, do NOT let you cancel individual services and are linked to your CC. Paypal absolutely lets you stop recurring payments unilaterally on their side. I use Paypal for subscriptions wherever it's offered precisely for this reason. https://www.paypal.com/us/cshelp/article/what-is-an-automati... reply Brybry 15 hours agorootparentSort of? I don't think everything always shows up on https://www.paypal.com/myaccount/autopay/ I think it maybe only shows companies you had recent transactions with. In 2023, I had a fraudulent $0.99 Paypal Automatic Payment for \"Domain Name Forwarding - Renewal\" from a company (DomainsPricedRight/OwnMyDomain aka GoDaddy) that I last did business with in 2005. Yes, 18 years prior. I was able to 'deactivate' the 'subscription' on the Paypal site after I noticed the charge but I don't think automatic payments existed on Paypal in 2005 and I'd certainly never signed up for it. The original 2005 business I did was a one time domain purchase that was transferred to another registrar within a year. It was real fun to also see on Paypal that I could have been fraudulently charged up to $10,000. It's kind of scary to think that any company I've done a Paypal transaction with could maybe do the same thing (or any of the companies that eventually acquire their merchant accounts...) reply PaulDavisThe1st 15 hours agorootparentI believe that this is the more reliable URL (it's certainly the one I provide to Ardour subscribers): https://www.paypal.com/cgi-bin/webscr?cmd=_manage-paylist [ EDIT: which redirects to the one you cited, so forget my attempt to be less wrong ] reply wmsmith 18 hours agorootparentprevThat's news to me! Thank you for sharing! reply llbeansandrice 16 hours agorootparentprevApple also lets you do this. reply nicholasjarnold 18 hours agorootparentprevI've been using Privacy.com for this \"create single use credit card\" for years now. They make money via the interchange fees, afaik, and not by selling your data stream. reply venatiodecorus 13 hours agorootparentJust seconding Privacy.com, I use them for all my online payments and it is a super easy workflow. reply rootusrootus 15 hours agorootparentprevDo they still require that on your side it is a debit card? reply gdcbe 16 hours agorootparentprevSadly they are not available in Belgium (Europe) :( reply orthoxerox 13 hours agoparentprev> Clearly, the days when businesses thought online shopping ought to be low-friction are long gone. I bought some lottery tickets online for a present to myself and the experience was smooooth. No cart, no checkout steps, no need to create an account, there was a QR code right next to the tickets that I had to scan with my banking app to buy them right here, right now. reply BeetleB 17 hours agoparentprevI pretty much never have to do a 2FA with Paypal. And it never redirects me to the bank (credit card). I also don't do this on my phone, but on a regular PC. reply Symbiote 17 hours agorootparentI believe the redirect to the bank's website is an EU and UK regulation, \"Strong Customer Authentication\": https://www.theguardian.com/money/2022/mar/14/uk-shoppers-fa... https://www.visa.co.uk/pay-with-visa/changes-in-payment-secu... I don't know the details of when it is and isn't required. I am asked pretty much all the time for transactions using my Danish cards, and only some of the time for the British cards. reply at-fates-hands 14 hours agorootparentprev>> I also don't do this on my phone, but on a regular PC. I do the same. Too many times I've had major issues trying to buy stuff on mobile so I just stopped doing it like 8 years ago. Literally the only thing I pay for with my phone is my hockey sessions via Venmo. reply InitialBP 17 hours agoparentprevBusiness don't want online shopping to be high-friction, but Thankfully consumer opinion is pushing more for security and less for making it as easy as possible to buy stuff online. I'll happily take this shit-show cacophony of various 2fa methods and authentication types if nobody is stealing money from my bank account or ordering stuff on ebay on my behalf. The flip side of this - is that if companies properly setup auth and allow you to use username+password (or passkeys) and a TOTP method then this is all basically copy/paste from your password manager or verify on your phone and the process is super easy. reply ryandrake 12 hours agorootparent> I'll happily take this shit-show cacophony of various 2fa methods and authentication types if nobody is stealing money from my bank account or ordering stuff on ebay on my behalf. Even better: I wouldn't care about people stealing money from my bank account if cleaning it up and making whole was my bank's responsibility and not mine or some hapless vendor. Neither I, nor store vendors should have to put up with the \"shit-show cacophony.\" The bank's entire reason for existence is to secure access to my money--it should be entirely their problem. reply mrits 19 hours agoparentprevIn a couple decades I had to verify my bank once with PayPal reply michaelt 19 hours agorootparentIn their defence, I'm sure the parties involved would blame EU Strong Customer Authentication rules and claim that they \"don't have a choice\" reply ajmurmann 14 hours agoparentprevWhy is your password manager only on your phone and not synced between your devices? reply at-fates-hands 14 hours agoparentprevNow imagine someone who has cognitive issues or is visually impaired trying to repeat this same process. reply wegfawefgawefg 18 hours agoparentprevEver since I started using brave browser I have to do all of this shit. On firefox i dont. Chrome, i definitely don't. reply kkfx 11 hours agoparentprevThey want mandatory macrobugs (aka smartphones, the bug not payed and carefully placed by those who want to spy, but the one payed and babysitted by the spy target) for anyone, so if you use a desktop you are a threat and you need to be not in comfort... reply ThePowerOfFuet 19 hours agoparentprevWhy would you submit yourself to using PayPal when you don't have to? Serious question. reply pc86 18 hours agorootparentPayPal is my first choice and if I go to check out on your store and you don't have PayPal as an option, the chances I abandon my cart if I don't have my wallet just went up exponentially. I use it as a buffer between me and the provider. Everything goes through a credit card so I still get the points/miles I would get entering the card directly. Except now they don't have a credit card token they can keep charging forever. They have a PayPal token that I can log into PayPal and immediately revoke, asynchronously, without involving the merchant or my credit card at all. I don't need to worry about my details still being with that merchant. I don't have to worry about the merchant's convolution and likely-illegal cancellation process. The only negative I can think of is that any dispute has to go through PayPal, and while I've never done it I would bet money they are going to be skewed more in the merchant's favor than the credit card company. But that being said I have had fully legitimate chargebacks (as in not \"I want a refund and they said no\" but \"this is a fraudulent charge I never agreed to\") get denied and reversed by Discover so that's not a 100% certainty either. I never receive money through PayPal so while I've read all the same horror stories everyone else has, that doesn't seem likely to affect me. My biggest gripe is the full-screen advertisement for whatever service they're pushing every time you log in on the website. reply upon_drumhead 18 hours agorootparentPrivacy[.]com replaced my PayPal usage pretty much completely. Virtual credit cards tied to individual merchants with limits. reply pc86 18 hours agorootparentAnd tied to a direct back account, requiring you to use cash and lose any CC benefits. I use Privacy for things I know I only want to charge once (e.g. $1 trials or things of that nature) but not being able to charge a CC with Privacy is a bit blocker most of the time. reply upon_drumhead 14 hours agorootparentSure, but I don’t get rewards with PayPal, so it’s a non issue? Or you are saying it’s not worth reducing your usage of PayPal unless you get rewards? reply pc86 14 hours agorootparentI'm saying I get rewards when I use PayPal (because everything ends up on a credit card anyway with added privacy/control benefits compared to using the card directly), so a solution where I don't get those rewards ends up being second-class. I also haven't had issues with PayPal that [many] others have, so there could certainly be a scenario where that changes. reply cowl 18 hours agorootparentprevBecause I don't want to give the credit card details to every site out there. And Because the Resolution Center works wanders with merchants who are not being forthcoming to resolve your problems. I once had an issue that a merchant had delivered less than half of the items that I had ordered, i contacted them and they requested (after 2 days) Proof that I had not received the items. I could only produce the photo of the opened package which was clearly too small to contain everything they were supposed to deliver and the weight in the package label that clearly was too little for everything I was supposed to get. They tried to stall asking proof that i had not received a second package with the rest of the missing items.(How can you prove a negative? I got fed up and opened a refund ticket with paypal describing the problems and within 30 min the merchant contacted me promising to send the missing items and refund 20% of the cost if i closed the ticket in paypal. reply navigate8310 17 hours agorootparentDid you do what merchant said? Is it still okay to trust the merchant and lose your only hope with PayPal once you click the resolved button? reply cowl 17 hours agorootparentNo I asked the merchant to commit to resend the missing items inside the resolution center and resolved the issue only after the items arrived. The aim is not \"profit\" but to get the deserved attention and bypass clear stalling tactics like having to prove a negative. Needles to say that I Did not ever use that merchant again. reply makeitdouble 19 hours agorootparentprevUp until not so long ago that was the easiest \"payment wallet\" to have around. Want to have charges go direct to your bank for 2 weeks ? you move it up on the list. Want to try a new card but are not sure you'll keep using it ? add to the wallet and move up or down depending on how much you want to use it. And it also managed subscriptions. It is now a steaming pile of garbage for so many reasons, and it has always been a death trap for any small merchant, but they gave a fairly good shot at the wallet side of things. Good luck getting Nintendo for instance trust any other third party wallet system. reply hot_gril 16 hours agorootparentprevSometimes there's no choice, usually for international purchases. eBay used to also prefer PayPal somehow, idk how it is now. I know that some Etsy sellers are PayPal-only. reply rootusrootus 15 hours agorootparent> eBay used to also prefer PayPal somehow They owned PayPal for a while, so it was heavily promoted. It's still their first choice AFAICT. reply hot_gril 15 hours agorootparentThere's that, and also I remember some sellers were PayPal-only or at least preferred it back in the day, but that's not a thing anymore. reply IshKebab 19 hours agorootparentprevIt's quicker than entering your credit card details and address again and again. reply pertique 19 hours agorootparentIf you use a password manager (which they say they do) it's much quicker to just save that info and automatically populate it. Doubly so considering the MFA hell they went through. reply zamadatix 18 hours agorootparentToo many sites have broken forms. Sure, you can have the card autofilled but maybe it doesn't trigger the autofill for the address or maybe that wasn't even loaded yet. Maybe you can just click there and have it auto-fill but they can be so broken it doesn't autofill completely or fills wrong. Some sites are smart enough to have a checkbox for \"shipping address is the same as billing\" and others aren't. When you use a 3rd party payment provider like PayPal it does a really good job of forcing all of this to be automatic compared to things trying to autofill custom forms just because it's integrated by the site instead of the user. MFA hell is starting to erode that actually being easier though and now there is more and more often no simple approach left. reply hot_gril 16 hours agorootparentYeah, CC autofill is nice but fails about 1/4 of the time. It doesn't include the security code either. A few sites will also have finicky inputs, like accepting spaces but rejecting the payment if you use them. Still, PayPal is an absolute last resort for me. reply inhumantsar 19 hours agorootparentprevit's also convenient for managing subscriptions reply j45 18 hours agorootparentprevPayPal is used by a lot more people than most think to buy. First it can pay directly from a bank account. Second a lot of countries don’t have many options other than PayPal. reply ubermonkey 19 hours agoparentprevSounds like you've got some unusual configuration options turned on or something. The most glaring odd thing here is that you apparently don't have your password vault available on the same machine you're shopping from, which seems odd to me. Even so, if I went that route it'd still be easy b/c with the Apple ecosystem, the clipboard is shared between devices. One can copy a password from the phone and paste it on the Mac. The tl;dr here is that I really don't understand why you had to retype your password. I never type my strong passwords. Why would you put yourself in a position where that's required? Finally, when I pay via Paypal using my Amex, I never have to re-auth to Amex. It just flows through. So it sounds like that's something you've chosen to set up, not something inherent to the process. reply pc86 18 hours agorootparentOpening your password manage and displaying the strong password openly on the screen while manually retyping it on a different machine - rather than just installing the password manager on that machine - definitely sounds like a \"why are you doing that?\" kind of thing. Likewise I've used a half dozen different cards and multiple bank accounts through PayPal for the last couple decades and can't remember the last time I've had to reauth on any of them during a checkout. reply michaelt 18 hours agorootparent> Opening your password manage and displaying the strong password openly on the screen while manually retyping it on a different machine - rather than just installing the password manager on that machine - definitely sounds like a \"why are you doing that?\" kind of thing. That one's on me, yes. The Yubikey I needed to unlock the password manager on the PC was upstairs and I couldn't be bothered to get it, so I used my phone instead. (Why was the yubikey upstairs? Well you see, that's where the fireproof safe is. But I can't blame ebay for that, so I didn't mention it) reply g-b-r 18 hours agorootparentprev> Opening your password manage and displaying the strong password openly on the screen while manually retyping it on a different machine - rather than just installing the password manager on that machine - definitely sounds like a \"why are you doing that?\" kind of thing. If the machine with the passwords is less exposed it's on average a lot safer (but now you have the problem of keyloggers of course) reply michaelt 18 hours agorootparentprev> Finally, when I pay via Paypal using my Amex, I never have to re-auth to Amex. It just flows through. So it sounds like that's something you've chosen to set up, not something inherent to the process. To be fair to the parties involved, they might well blame \"EU strong customer authentication rules\" reply ranguna 8 minutes agoprevLots of people having lots of issues in the comments, I can't be the only one that has no problems with this. I use bitwarden, it has my passwords and my TOTP codes in there, I have this on my phone and on my computers, everything auto fills. Other than that, I also have a hardware key for some services, all I need to do is click the hardware key when prompted. Some services only have email 2FA, but that's quite easy as well, I just get a notification and copy the code from there. Doing a chain of 3 2FAs for 3 different services takes seconds. For improved security this is easy, I'm not sure what everyone is on about. Is this another case of complaining just for the sake of it? reply Karellen 19 hours agoprev> Gileadite soldiers used the word \"shibboleth\" to detect their enemies, the Ephraimites. The Ephraimites spoke in a different dialect so that they would say \"sibboleth\" instead. Experience : you just had to say a word. Security : there's a single word to authenticate multiple users and it can be cracked by learning how to spell it. Although that's roughly how the Wikipedia entry[0] summarises it, the actual wording of the story indicates a slightly different issue: > for he could not frame to pronounce it right. It's not a spelling difference per se, it's (AIUI) that the Gileadite pronunciation uses a phoneme that was not used at all in the Ephraimites spoken language, so an Ephraimites soldier was literally incapable of pronouncing the word \"correctly\". e.g. How some spoken dialects/accents do not use a rhotic \"r\", or do not distinguish between \"l\"/\"r\", or are not tonal languages. If you have not already learned how to make that specific sound, and distinguish it from the other one, through repeated practice, you will be unable to replicate it properly. And this will be the case no matter how the word is spelled, or even if you try to immediately copy someone saying it the exact way they want you to say it. [0] https://en.wikipedia.org/wiki/Shibboleth reply tsm 19 hours agoparentSee also: the Parsley Massacre in the Dominican Republic, which preyed on Haitians' inability to pronounce the word \"perejil\" as a native Spanish speaker would: > The Haitian languages, French and Haitian Creole, pronounce the r as a uvular approximant or a voiced velar fricative, respectively so their speakers can have difficulty pronouncing the alveolar tap or the alveolar trill of Spanish, the language of the Dominican Republic. Also, only Spanish but not French or Haitian Creole pronounces the j as the voiceless velar fricative. If they could pronounce it the Spanish way the soldiers considered them Dominican and let them live, but if they pronounced it the French or Creole way they considered them Haitian and murdered them. https://en.wikipedia.org/wiki/Parsley_massacre reply lolinder 16 hours agoparentprev> so an Ephraimites soldier was literally incapable of pronouncing the word \"correctly\". And, importantly, they would not even have realized that they were saying it wrong, because they would have been unable to hear the difference. As a modern example: I have an acquaintance from Tonga. At some point she got very frustrated with the people around her who didn't understand what she meant by the \"rittel bin\". She finally pointed at the trash can. \"Oh, the litter bin!\" \"That's what I said, the rittel bin!\" In Tongan, l and r are the same phoneme, and native speakers cannot distinguish them without practice. reply dustincoates 13 hours agorootparentI learned French as an adult, and I cannot at all hear the difference between the words \"rue\" and \"roue.\" People tell me there's a difference and they try to sound it out to me, but each time they do, I just have to trust that they aren't saying the same thing twice. reply eszed 9 hours agorootparentThere are native-English dialect groups which make no distinction between the vowels in 'pin' and 'pen'. For all I rib my wife about falling on the other side of that line, it took my American ear a long time to hear UK-dialect(s) distinctions between 'Mary', 'merry', and 'marry', and still a fair bit of concentration to reproduce them! reply mekoka 11 hours agorootparentprevIt's not that you're unable to hear the difference, you have to pay closer attention to what you're really hearing. I'm a native french speaker and when first learning english, I was made to carefully notice the subtle difference in certain vowels, intonations, silent consonants, etc. Like in sit and seat. The former vowel doesn't exist in french, so most french speakers learning english would pronounce it like the latter, as in \"seat down\". English vowels are very different from french, but there are some similarities and if you don't care to notice, you'll use approximations. E.g. imagine a french person saying \"book\", with the \"ooh\" sound and a noticeable exhalation after the k. Small little things like that can be brought to attention and corrected. It also doesn't help that we much rely on the written word to learn. Which reinforces the reliance on existing symbol-pronunciation associations, instead of creating new ones. reply perlgeek 13 hours agorootparentprevAnother famous example of l and r confusion is Japanese. One I'm struggling with: Norwegian (bokmal at least) has the \"y\", which is between i and the German ü. I can kinda hear the difference, if I pay close attention and the speaker is deliberate about the pronounciation, but saying it is kinda hard, and I get it wrong most of the time. reply interroboink 11 hours agorootparentprevSomewhat similar to how English (and other) speakers can have trouble distinguishing between the intonations in tonal languages like Chinese — \"mā, má, mǎ, mà\", and all that. reply chrismorgan 16 hours agoparentprevThis one’s really fun in Bengali, where they have three relevant consonants, but they’re quite commonly all pronounced about the same: Shibboleth is শিব্বোলেত্ and Sibboleth সিব্বোলেত্ , but শ and স may be pronounced the same (though some distinguish them), which could be more like sh or like s, depending on the region and person. And, by experimentation grounded in this specific verse, apparently many of them can’t reliably hear a difference between sh and s, which I find difficult to comprehend given the significant spectral difference. But hearing is at least as much a brain thing as an ear thing. So when a Bengali is reading the verse, what they’ll speak can be basically “they said, ‘Then say “Sibboleth”’, and he said ‘Sibboleth’ because he couldn’t pronounce it properly”. reply jampekka 15 hours agoparentprevIn the Finnish civil war in 1918 the White Guard asked captives to say \"one\" in Finnish (\"yksi\"). The word starts with the wovel [y]. This is very hard to even learn to pronounce. When the Russian speaking captives tried and failed to utter the [y] they were shot on the spot. Finnish natives got the luxury to starve often to death in concentration camps. In WW2 the sibboleth was changed to \"höyryjyrä\". reply wongarsu 18 hours agoparentprevA modern example that might be intuitive to native English speakers is asking people to pronounce \"The rural squirrel measures the tomb\". You will be able to tell most Germans apart from native speakers by the first word alone reply svachalek 14 hours agoparentprevFor a fairly similar experience for English speakers, see the sound that is written in Chinese pinyin as \"sh\" vs \"x\". They're two distinct sounds but will likely both register as \"sh\" to English speakers. Likewise \"ch\" and \"q\". reply anigbrowl 14 hours agoparentprevDutch people still jokingly invite newbies in the country to pronounce the name of the town 'Scheveningen'; this is kind of hard for native English speakers and very difficult for native German speakers, so it was used as a filter by the Dutch resistance during WW2. reply I_complete_me 13 hours agorootparentScheveningen - huh? I recently read Robert Harris's book V2 based on this town. Good book. Also, there is a chess opening variation named after this place. (see https://en.wikipedia.org/wiki/Sicilian_Defence,_Scheveningen... ) And it has held great chess tournaments in the past. Plus the Scheveningen system is a method of organizing a chess match between two teams. For a fairly obscure location, it certainly got on the map, so to speak. reply rurp 13 hours agoparentprevI recall reading some interesting neurological research on this topic, about how phonemes are learned and accessed. The specific sounds stored in the brain are largely fixed by a pretty young age, making it almost impossible for adults to learn certain pronunciations that differ from anything they were exposed to as a child. reply tnecniv 13 hours agoparentprevI don’t know if it’s true or a common myth, but US soldiers in the Battle of the Buldge would ask possible spies baseball questions. Even if you were an American that didn’t like baseball, it was absolutely massive back then and would know some things about recent seasons. reply selimthegrim 18 hours agoparentprevCase in point - Hebrew lost “Ghayin” way back in history so the Hebrew for Gaza is “’Aza” (with ‘Ayin) reply ComputerGuru 17 hours agorootparentHebrew also lost the voiceless pharyngeal fricative (Heth/ح) which iirc can only be pronounced by Mizrahi Jews (aka of Jewish Arab origin). It was merged into the voiceless uvular fricative כ khaf (خ in Arabic). Though as I understand it, interestingly the letters themselves are still found in Hebrew with distinct glyphs (ח vs כ) but one has just lost its unique pronunciation. reply reissbaker 17 hours agorootparentprevFWIW this varies by background — Yemenite Jews still pronounce Ayin as Ghayin. reply selimthegrim 16 hours agorootparentBut there’s no letter for it in Hebrew? reply reissbaker 16 hours agorootparentAyin (ע) is the letter, and was the original letter used in the spelling of Gaza — עזה is the oldest and original name of Gaza, for as long as it's had that name. The Hebrew alphabet hasn't changed letters in thousands of years, long predating other Semitic languages like Arabic which continue to use the Gh sound; ancient Hebrew is still easily understood in written form by modern Hebrew speakers — much more so than even Shakespeare is to modern English speakers. When people say \"Hebrew lost...\" what they mean is the pronunciation of letters changed, not that the alphabet changed (unlike e.g. English, which really has lost and gained letters even over very short periods). And in some cases the sounds were only lost in specific communities; Yemenite Jews have done a pretty good job retaining sounds, e.g. their pronunciation of ע, as well as ת. (Similarly, Ashkenazis' much-maligned pronunciation of ת is probably closer to the original than modern non-Yemenite Mizrahi/Sephardic pronunciation — although Yemenite is closer.) The last time written Hebrew meaningfully changed was when the Paleo-Hebrew script was exchanged for Aramaic block script 2.5 thousand years ago, but even then, the replacement was 1:1 — ע was still Ayin, it was just written with a different character. And Paleo-Hebrew script has been around since the Bronze Age. reply Al-Khwarizmi 19 hours agoprevWe are going way over the top with 2FA. Why do I need to activate mandatory 2FA in services like GitHub repositories for hobby projects? It's a lot of extra effort for a questionable security improvement, and anyway, if someone impersonates me there, it's not the end of the world. If they care about end users (which my projects mostly don't even have) mark me as \"unverified\" or something, but let me avoid the hassle. And in more serious services, like banking... since there is no such thing about 100% security (and in particular 2FA is far from it, e.g. if your phone is stolen with the banking app open, you're screwed), actually the most important thing is that the bank responds and can refund the money if fraud is committed, which it inevitably will for some percentage of unlucky customers. I view 2FA as a way to pass responsability to the customer (\"we have very secure systems, so if someone transferred $X out of your account it's surely your fault\"). Personally, I feel safer with less security and the bank worrying about fraud than the other way around, so I don't think they're protecting me when they implement this kind of stuff. reply whartung 18 hours agoparentGithub 2FA is made extra fun because they only offer a single mechanic of replacing it (that I know of), and that's using the recovery codes. So, they forced me to use 2FA, and I dutifully printed out the recovery codes (don't write down your passwords, that's bad practice, but here's 20 recovery codes that stand between you and losing your account forever, so you know, manage that somehow). When I bought a new iPhone, apparently none of my stored information got copied over. The apps did, but none of the information for those apps (for example, the TOTP info maintained by the authenticator I used). So, I went to log in to Github, opened up my authenticator app, and it was blank. Thankfully I had the codes...back at home, in a drawer, guarded by a cat, so I wasn't completely doomed, but it ruined the day to be sure until I could get home and recover it and recalibrate my TOTP app. Oh, guess who has a photo of their recovery codes on their phone now? reply ta1243 17 hours agorootparentTOTP backups from phones is a major issue, from what I can tell you simply can't do it. reply hot_gril 16 hours agorootparentIt's the Google Authenticator app's fault. The most popular TOTP app probably, and for a long time, they were saying it's intentionally designed not to let you copy the codes. Now you can, but there are lots of pitfalls and vague documentation. I'm not convinced that TOTP is a user-friendly design to begin with, but it didn't have to be this bad. I don't fw TOTP now. There are other apps, but I'm done. I'll only use it if the iPhone Keychain has built-in support some day. reply eppsilon 16 hours agorootparentThe iOS Keychain already supports TOTP. reply hot_gril 15 hours agorootparentAh yeah, it's hidden away a little cause they don't call it TOTP and you need to manually copy codes into your settings app. Gonna see if I can set it up on Mac cause that's where I'll actually maybe need it. reply Willamin 14 hours agorootparentSet up should be simpler than needing to manually copy codes into your settings app. When a QR code is present on screen that resolves to a TOTP seed, an additional context menu option should be present to \"Add Verification Code in Passwords\" or \"Set Up Verification Code\" or similar. Here's a screenshot I nabbed from a way-too-wordy article on the subject: https://tidbits.com/uploads/2021/10/Add-Verification-Code-15... reply 369548684892826 17 hours agorootparentprev2FAS [0] and I think Authy [1] as well have options for backing up your TOTP config 0: https://apps.apple.com/us/app/2fa-authenticator-2fas/id12177... 1: https://apps.apple.com/us/app/twilio-authy/id494168017 reply lannisterstark 13 hours agorootparentprev>TOTP backups from phones is a major issue, from what I can tell you simply can't do it. I've done it in Aegis multiple times. They even allow you to export the 'database' (which iirc is just an encrypted json file) reply smallpipe 17 hours agorootparentprevWith a yubikey everything is stored on the key and the phone is just a terminal, so it travels between phones. Now if you lose the key that's another issue :) reply ta1243 17 hours agorootparentSure, so same problem. Less likely your yubikey will be stolen I guess, but less convenient too (something else to carry) reply tnbp 13 hours agorootparentBut it's a key though. It goes on the keychain. Unless you don't carry around keys either, in which case yes, that would be very inconvenient indeed. Also, your Yubikey is probably less likely to be stolen or break, but I figure it's much easier to lose it, which is why you might want to have two, just in case. And that's where it gets really inconvenient. reply filleokus 2 hours agorootparentThe problem I've always had with the two yubikey-model (except for cost an inconvenience of course) is that you can't really keep the second key in cold storage, because you need to enroll it to new accounts. That doesn't happen every day, but probably regularly enough that you can't keep in a bank vault or something. On the other hand, you know the second one works and haven't spontaneously bitrotted. My nerdy preferred version would have been (pre-passkey) to have a hardware token where the root secret is generated out-of-device and exist on e.g a paper backup or something. Then I could just buy a new hardware token and inject the same token if the device dies. reply lannisterstark 13 hours agorootparentprev>But it's a key though. It goes on the keychain. Unless you don't carry around keys either, in which case yes, that would be very inconvenient indeed. Half the time I choose for TOTP authentication over Yubikey because \"Oh god it's in the living room I don't want to go get it.\" I do have a backup key mind, but that's USB-C instead of A. Maybe I should make another USB A backup. reply hot_gril 11 hours agorootparentprevTwo yubikeys sounds ok, but I don't 100% trust that the second one works forever. Anyway, my keychain got ran over by a bus, and luckily the yubikey survived. reply tzs 11 hours agorootparentprevWhat I do is when I receive a QR code to set up TOTP while creating a new account is to take a screenshot of that code and save an encrypted copy of that screenshot. Then it is just part of my ordinary data backed up as part of my normal backups. If I ever want to set up a TOTP app on a new device it is not hard to decrypt all my saved QR codes, open them all at the same time in Preview on my Mac, select the option to show one page at a time, and then get into a nice rhythm using one hand to scan on the new device and the other to hit \"page down\" on the Mac keyboard. If the site also gives a text form of the shared secret from the QR code I save that too. Having the text form around is handy in case I need to login but for some reason don't have the devices where I have the TOTP apps. Given the text form of the code, this command, from the oathtool package, will give the current login code: $ oathtool --totp -b \"secret\" That's if the secret is encoded in base32, which they commonly are. If it is in hex leave off the -b. If the site doesn't give a text form of the shared secret I read the QR code to get it. If you do that be careful. Some QR code reader apps do the processing server side which you probably don't want...and they don't necessarily make that clear in the description. I had to try a couple of apps from the Mac app store before finding one that did it client side. (Then I found out that Mathematica's BarcodeRecognize function can do it, and deleted the QR code reader app. Now I just open Mathematica, type BarcodeRecognize[], drag and drop an image file that has the QR code between the brackets, and hit shift-return). reply hot_gril 11 hours agorootparentWhy use a QR code reader app instead of the built in camera app? Personally, I email the backup codes to myself. Yes it's less secure in theory, but the only time I'm using totp is against my will. reply tzs 10 hours agorootparent> Why use a QR code reader app instead of the built in camera app? The QR code is on the screen of my desktop Mac. The camera is right above the screen facing me and can't see what is on the screen. I could read it with the built in camera app on my iPhone or iPad, but that just tells me it is a QR code for the TOTP authenticator app I use and opens that if I tap. I don't see a way to get it to tell me the content of the QR code in text form. Even if it had a way that would be on the phone and I want the text to save it on the Mac. reply hot_gril 10 hours agorootparentOh, got it. Classic problem with QR codes. Some people get around it with two mirrors, haha. reply justanorherhack 15 hours agorootparentprevUse ravio on io’s, lets you copy, backup and duplicate them to other places. reply MrDrMcCoy 15 hours agorootparentprevThat's why I store them in Bitwarden. reply Vinnl 13 hours agorootparentprevOh huh, I switched from AndOTP to Aegis and was able to export from the former and import into the latter. Then on desktop I'm using Authenticator [0], which can import from both. [0] https://flathub.org/apps/com.belmoussaoui.Authenticator reply shortsunblack 9 hours agorootparentprevUse Aegis. reply samcat116 16 hours agorootparentprevthere are other methods for 2fa recovery. The main one involves an SSH key you've previously added to your account. reply qhwudbebd 13 hours agorootparentUsing said ssh key as one of the 2fa choices for standard login would be quite a nice feature too... reply pmontra 16 hours agoparentprevI stopped logging in into GitHub since then. My customers are using Bitbucket right now so the only reasons to log into GitHub would be to search the code of some project or opening an issue to one. Luckily I can search issues without being logged in and about opening issues, I feel a little bad but I don't open them anymore. It was my way to contribute to open source, it's gone because of too much friction. reply hot_gril 11 hours agorootparentMaking login harder is Microsoft's classic thing. At least they didn't do it like Minecraft, locking out tons of pre-existing users. reply qhwudbebd 13 hours agoparentprevI spent a little time trying to decide whether GitHub's 2fa was genuinely an extra factor whose compromise (with an uncompromised password) wouldn't weaken security vs a situation where it hadn't been set up at all. In that case, presumably I could embed the totp key in a bookmarklet in the conveniently-sized 'public bio' field on my profile so I can complete it on whatever device I happen to be using, and effectively opt out? But I'm really not convinced they aren't fuckwits and wouldn't treat the 'second factor' as an authoritative single factor in some circumstances (e.g. password reset) which wouldn't be unauthenticated if 2fa wasn't set up. I'm also not convinced one can even contact anyone at GitHub clueful enough to answer that question authoritatively nowadays rather than reading off a script. reply Al-Khwarizmi 11 hours agorootparentThat's another pet peeve of mine with 2FA, which I didn't mention to avoid posting a wall of text: in many cases (no idea if GitHub in particular is one of them), the second factor totally dominates (allowing you to recover the first factor or logging in without it) so it effectively is 1FA, where the factor is almost always your phone. Lose your phone, and you're screwed. reply nlawalker 16 hours agoparentprev> Why do I need to activate mandatory 2FA in services like GitHub repositories for hobby projects? Same reason Microsoft forces Windows updates so aggressively - because if some kind of security breach makes the news, even if it's clearly due to poor user choices (poor password choices and/or security; repeatedly opting out of critical security updates), it's always the vendor/service provider that looks bad. reply devnullbrain 15 hours agoparentprevGithub is an unfortunate choice of example because the replies have fixated on it but there are a large number of sites that impose security cargo-culting to secure things that just don't need it. e.g. Why do I need to make an account with a password to pay a bill? reply PurpleRamen 17 hours agoparentprev> Why do I need to activate mandatory 2FA in services like GitHub repositories for hobby projects? Because your hobby-project can emerge to be the backbone of someone's multibillion dollar-business, or a small gear in a million other projects, and you will get targeted for a supply-chain-attack. reply zamadatix 15 hours agorootparentWhy should a multi-billion dollar business or a million other projects trust my code simply because GitHub made me 2FA to sign in? I may well decide the next push rewrites half the project in a breaking way on a whim or get an offer for $100k to give control of the project to the bad actor or just decide I don't like big corp anyway and be the bad actor myself. Turning providing source code into promising you'll follow other's desires on how it should be worked on is a recipe for disaster while simultaneously not really making hobby projects low risk to rely on anyways. reply judge2020 14 hours agorootparentI think it's more about GitHub's image and its self-imposed viewpoint that it needs to keep the software landscape secure. Requiring 2fa drastically reduces the number of ways a repo that is a building block for x% of a country's GDP gets compromised - now the only path is if the author intentionally hands over the repo/their account to a bad actor or e.g. posts their 2fa secrets on the internet for anyone to use. reply zamadatix 14 hours agorootparentThere are also plenty more ways for it get compromised which don't involve the owner handing over anything - say simply accepting a merge which adds a cool feature while still compromising the other users of the project. Github still gets the same marginal image impact risk because ultimately the security of billions of dollars or X% of a country's GDP isn't protected by requiring a hobby developer to 2FA their afternoon code changes. You can't make them interested in protecting your billions via secure account login. Instead it's done by the billion dollar companies or countries themselves because they are the only ones with resources large enough to protect that much asset in a useful way. All this ignoring the same impact can be had by the author simply having a mistake and not fixing it over holiday vacation, no malicious actors required in the first place. reply massysett 16 hours agorootparentprevYou are right. However this cost should really be imposed on the multi-billion-dollar business and not on the author of the hobby app. reply PurpleRamen 16 hours agorootparentHow should that work? Nobody knows who is using which part from which repo. And it's not just about big business. There are all kind of small communities and little apps, extensions, etc. with some small communities. Most of them don't even make money, but are juicy targets for some small fast money. Forcing everyone to raise their security and gain awareness about those things is a huge win for everyone, and only a little problem for the individual user. And it seems to be only a phase anyway, as most people & services are moving to more comfortable solutions over time. reply massysett 15 hours agorootparentBillion-dollar businesses can pay full-time professionals for support. They can hire staff or contract with a vendor. They can audit the free software they use or, like the good old days, pay for software whose vendors maintain it. Or they can use hobbyist-written software for free, which is just fine, but don’t expect the hobbyist to support it for free. reply simoncion 16 hours agorootparentprev> How should that work? By the users of the software I publish noticing the license that states that while I hope this software is useful to them, it is provided with `\"NO WARRANTY, NOT EVEN FOR FITNESS OF PURPOSE\" and planning accordingly. If you're an entity that wants to ensure that software you use from a source that you have approximately zero power over (and has explicitly provided NO warranty for that software) is and continues to be fit for purpose, you're going to have to inspect that software at a point in time, determine if it is fit for your purposes, and carefully inspect every future version of that software that you're considering using. There really are no shortcuts. Requiring one to drink a Confirmation Can to log in doesn't change the math here. reply parineum 14 hours agorootparentprev> How should that work? Fork and change the readme to reflect that this version is hardened for big business. reply bigstrat2003 17 hours agorootparentprevSo implement those tighter security controls when they make sense. Don't force them on everyone when only a small fraction of cases are worthwhile. reply Shank 19 hours agoparentprevJust turn on Passkeys on GitHub, then you don't need 2FA/TOTP. It's also faster. reply mminer237 19 hours agorootparentMy password manager autofilling will always be faster than any other option, especially one that requires me to pull out my phone, navigate to my authenticator app, switch to your app (which will only become more time-consuming as more sites require it), then type in the code by hand. The only thing that can compete with password managers on user experience is just actually remembering they're logged in instead of pointlessly logging them out every single day for no reason. reply mimsee 19 hours agorootparent1Password supports Passkeys, not sure for others. But it's one click on the login page and you're in. reply jeroenhd 18 hours agorootparentprevPasskeys work without a phone, or a second device. Windows Hello/TouchID will verify you almost instantly. There are also browser extensions you can use, like 1Password or Bitwarden, to do the passkey flow for you if your device or OS lacks quick authentication options. reply samcat116 16 hours agorootparentprev> My password manager autofilling will always be faster than any other option Passkeys will be faster. reply whartung 16 hours agorootparentprevI don't see how Passkeys eliminates the need for 2FA. Seems to me, and I may not understand it, but it seems to me that Passkeys are more of a way to eliminate having to constantly re-enter you password, but do not eliminate passwords. For example, if I set up a Passkey, that's bound to a specific machine/browser/phones/whatever. But if I log in from another device, there are no Passkeys, so I just need to use my password. If my lose my machine/browser/phone, I'm in the same boat -- new device, and I need to login. Thus the password. I don't use any syncing system, I'm not on iCloud, or use apps, or anything like that, so there's no mechanic for distribution of passkeys. Plus that wouldn't work if I wanted to log from my friends laptop, or something like that. Am I mistaken in how this works? How does enabling Passkeys eliminate 2FA? My issues with 2FA aren't so much the 2FA part (yea, it's a pain in the neck, \"one more step\", etc., but, it is what it is). My issue is that if my 2FA is lost, and my recovery codes are lost, I'm toast. There's no other way to recover. No other mechanic, at least for Github. reply asmor 14 hours agorootparent> I don't use any syncing system, I'm not on iCloud, or use apps, or anything like that, so there's no mechanic for distribution of passkeys. Plus that wouldn't work if I wanted to log from my friends laptop, or something like that. iOS and Android can also just keep local Passkeys where you scan a QR code, though of course if you don't backup anything anywhere you will always have a redundancy problem with any 2FA mechanism. Passkeys are supposed to not be a single authenticator either, so you can enroll another Phone or a Yubikey (or also your local TPM, binding to your user account, for convenience), but not all services support that in practice. reply rurp 12 hours agoparentprev2FA can also be a way to get more private data, like phone numbers, out of users; which will be used for things having nothing to do with security or helping the user. Facebook did exactly this and I'm sure other companies have as well. 2FA increases risk of the account owner losing access to their account. There are a huge amount of posts online from people livid about getting locked out of their account because of some mundane reason like their phone breaking. That risk rarely seems to be considered by the crowd pushing 2FA everywhere and anywhere, probably because it happens most often to non-techies. Things that seem easy or obvious to folks working in tech are often a huge hurdle for regular users, who make up the majority of users for many products. Many tech companies could do a much better job of considering the needs of their users, rather than building what the devs and product managers personally think is cool. reply watwut 15 hours agoparentprevPersonally, I think that they simply don't like to be go to free storage for all of our personal or hobby or open spurce projects. This way, free users are less likely to use github while paying corporations will stay. reply BoppreH 19 hours agoprevI understand the frustration with login systems, but why is the title \"Passwordless: A Different Kind of Hell\" if it doesn't talk about passwordless authentication, like passkeys, magic links, and biometrics? reply Sohcahtoa82 16 hours agoparent> biometrics Biometrics are a convenience feature, not a security feature. Fingerprints are trivial to lift and replicate. Face unlocks can be fooled by pictures, or in some cases, get false positives from people that just look enough like you (which is common in some Asian countries). Even if it requires you to blink, new AI tools will easily generate a video of you looking around and blinking. But the worst part about it all, is that biometrics are a password you can't change without surgery. I really REALLY wish \"biometrics\" would stop coming up as a solution to security. reply firejake308 16 hours agorootparentAgree with the insights in your comment about biometrics != security, but I'd like to take a moment to nitpick a slight inaccuracy-- Asian faces don't actually look similar to each other, but they do look similar to a person/model that has been trained mostly on white faces. If the facial recognition model had been trained predominantly on Asian faces, then white faces would look similar to each other instead. Reminder that the outputs of AI don't reflect some deeper truth about reality, just an extrapolation of the training data. Garbage in, garbage out. reply Affric 10 hours agorootparent> don’t actually look similar to each other I think this should be “more similar than other groups” rather than simply “similar”. Even then I think it’s possible that some groups have more loci with higher diversity for facial features. That’s not even getting to epigenetic and environmental elements. I think the deeper truth is in your final paragraph: facial similarity is in the eye of the beholder. reply jazzyjackson 15 hours agorootparentprevbiometrics are used in combination with a specific device. same as a PIN (you can't withdraw money from an ATM with just a PIN, you need the chipped card + your PIN) i can't go up to just any computer and log into my bank with my face you would have to possess my phone and then deepfake me i am comfortable with this security posture because the convenience of face id allows me to use long random passwords with frequent rollover which I never have to type if i lose my phone I can remotely disable it this is all much less of a crime to me than any service that allows password reset over SMS which is a much more well trodden vulnerability. reply kube-system 16 hours agorootparentprevLow security is security too. Biometrics can useful when used for appropriate applications. They're very useful in applications where authentication would be otherwise be omitted or undermined due to usability concerns. They can also be used in conjunction with other authentication methods to complement the flaws of other authentication methods, like passphrase or token, which can be shared more easily. Like with many things pertaining to security, there are no universal solutions without first defining the problem. Say for instance, you have an access control system where you want to solve the issue of credentials being intentionally shared. Biometrics are a great solution for this; tokens and passphrases are not. You need different tools for different problems. reply rootusrootus 15 hours agorootparentprev> Face unlocks can be fooled by pictures Isn't that only Android (and maybe only older models)? Doesn't iOS use a LIDAR sensor instead of the camera? reply alternatex 14 hours agorootparentCorrect. Depending on the phone, on Android the face unlock will not work with a 2D image. Perhaps only on cheaper phones. On Windows for example you can't even have face unlock without a sensor that will provide 3D details so most laptops don't support Windows Hello. reply charles_f 19 hours agoparentprevI was confused as well. It seems to me that by lowering the experience complexity, while not really changing security, by the author's own logic the experience stars would go up a notch. Instead they just mention it in passing with a \"only time will tell\" comment reply felipeccastro 18 hours agoparentprevI was curious about that as well. Since most services implement an email based Forgot Password feature, and 2FA tokens are also often email based, why isn't magic links the default approach now? Seems to be just as secure as password+2FA but easier to use (and probably to implement, as well). By the title, I thought the article would explore some of the downsides of this approach that I might be missing. reply Macha 14 hours agorootparentMagic links are not the default as it gives your login process the speed and reliability of email delivery and most login processes are aiming for better than a p95 of about 5 minutes. reply davidmurdoch 19 hours agoparentprevIt talks about passkeys and biometrics though. reply gipp 19 hours agorootparentAt the very end, as possible alternatives to the hell they're describing. reply mimsee 19 hours agorootparentYeah and would passkeys themselves prevent the session from expiring? Notion et al can still have short lived sessions on their client apps. reply gipp 16 hours agorootparentYeah I'm not saying anything about the truth of that, just that the title doesn't match the content reply filleokus 19 hours agoprevI think the industry, to some extent, already have reconsidered the session length, see [0] by Auth0 for example (even if it's obv. a PR piece). Nowadays my gut assumption when I use a service with really short sessions is that their security practices are probably questionable. I recently argued, as the cybersecurity guy™, with a vendor that we can't ask regular users to reauthenticate every 15 minutes. They insisted raising it would be to insecure and instead suggested to make MFA optional as it would make the login process smoother… [0]: https://auth0.com/blog/balance-user-experience-and-security-... reply gregmac 14 hours agoparentThe thing that I find super frustrating about these short sessions is the lack of risk it's mitigating. If it's expiring in a few minutes, presumably you're trying to protect against two things: (1) Session hijacking and (2) Unlocked computer. Session hijacking is somewhat preventable via other means (eg: IP address tracking), but more importantly, in what case can a session be hijacked only 15 minutes later? Someone walking away from an unlocked computer is an impossible problem for a app/site to solve. If an attacker has access to the PC, they can install malware that sniffs all traffic or passwords, and if the user saves their password(s) on their PC all of those are compromised anyway. This is a responsibility of the person responsible for the computer -- eg, the user and/or the IT admin. When sessions/passwords expire in a time measured in days, I can't help but think they are basically saying \"it's okay for an attacker to have access to this system for 89 days... but not 90!\" The only valid argument I've ever heard for this is an attacker might be doing offline cracks of passwords -- but there's so many other fails involved there that I can't see how blindly expiring them is at all useful by comparison. Not to mention rotated passwords are very predictable[1] so it's unlikely to even mitigate the attack. [1] https://www.sans.org/blog/the-debate-around-password-rotatio... reply kevincox 13 hours agorootparentFor very short sessions that is likely true. But I think there is a middle ground where devices are lost or stolen, or data is accidentally leaked. If it is a sophisticated targeted attack you have already lost. But maybe someone just threw out an old PC that they haven't used in years and the disk isn't encrypted. For my service I ended up doing something in between. Sessions last for 14 days, but they are automatically renewed indefinitely. So as long as you access the service every 14 days your session will never expire. This way lost or leaked credentials aren't a risk forever. But in most cases users rarely if ever need to log in again. I may play with the exact timeframes, or maybe significantly extend the validity if the user is logged in via the same IP or similar heuristics. But I like that after some definite period old creds are no longer live. reply organsnyder 17 hours agoparentprevSome of it depends on regulations and usage context. When I worked in healthcare, sessions were always short-lived. This may have been regulation-driven, but it's also based on the fact that often this software is being used on shared machines or in areas where unauthorized users are present (such as in patient rooms). While users are trained (very well, in my experience) to lock machines whenever they're unattended, short session lengths provide an additional layer of protection. reply gregmac 14 hours agorootparentI saw a demo like 15 or 20 years ago of a Sun thin client that used smart cards. You put your card in to any terminal, and nearly instantly your desktop session was live. Remove the card and it instantly disappears and locks. That type of thing seems ideally suited to healthcare use, and we have such better devices now than whatever cards were used way back then. Amazing it's still Windows PCs deployed and secured with passwords. reply organsnyder 12 hours agorootparentA previous employer (regional healthcare system) did exactly that: staff used their badges (along with another authentication factor, IIRC) to pull up their VDI instance on any client. This was just being rolled out ~8 years ago. reply Sohcahtoa82 16 hours agoparentprevWhat gets me is that gmail login lasts...seemingly forever. And for most users, if their e-mail account were to get compromised, it's game over for everything they use, since so many services allow you to reset a password and possibly even remove 2FA with just e-mail verification. What's even the attack scenario? Someone stealing a session token/cookie? If they can steal an expired one somehow, then there are good odds they could steal a current one, so the short session doesn't matter THAT much. I suppose another scenario is someone not logging out of their accounts on a public computer, but the type of person to do that likely uses \"Password123!\" as a password anyways. reply TacticalCoder 15 hours agorootparent> ... since so many services allow you to reset a password and possibly even remove 2FA with just e-mail verification. What is insane is that so many services allows to reset password and even 2FA without requiring any cooldown. The level of fail here is plain staggering. I don't really have words. There are proper services out there who shall go out of their way to try to contact you, for example for 72 hours, before allowing any reset to happen. Some are going to say: \"Wait, what!?, 72 hours!? I need to reset my 2FA NOW\". They don't realize though that what they're really saying is: \"I want bad guys to be able to reset my password/2FA instantly and log me out of everything they can in a split second\". It's convenience vs security, once again. As a sidenote I've read about a DB (in the EU) about SIM cards saying when they were swapped. And as a bank, you can check that DB and decide, for example, to refuse to let anyone change any setting if the SIM was swapped less than a week ago. We need more people to think a bit about potential solutions instead of crying \"but it's not convenient\" and \"bad guys shall find a way anyway\". reply m3047 12 hours agoparentprevA problem which has made the news repeatedly is services where an e.g. password reset doesn't reset / invalidate the session key. How many cases of ridiculously short session expiry are masking cases where the service is unable to actually manage to invalidate a session key in conjunction with said password reset? reply joquarky 12 hours agoparentprevNIST recommends creating separate \"Authenticator Assurance Levels\" to balance security with UX: https://pages.nist.gov/800-63-3-Implementation-Resources/63B... reply mooreds 18 hours agoprevWeird post. It's a good history of authentication, including offline and online, and I like the ratings. But the title seems like pure click bait, as the author didn't spend more than 2 sentences on passkeys/Webauthn (which is the typical tech for passwordless solutions nowadays). I have my own issues with Webauthn usability and was expecting a deeper dive into that. That larger problem, of course, is that security and ease of use are in tension. Always were, always will be. reply 108 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the historical progression of authentication methods and evaluates their effectiveness in terms of user experience and security.",
      "Various methods, including passwords, code words, password managers, and two-factor authentication, are discussed in the article.",
      "The article highlights the potential of future advancements such as Single Sign-On and biometrics but acknowledges that the current user experience in authentication is not ideal."
    ],
    "commentSummary": [
      "The discussions covered frustrations with multiple password authentication layers during online purchases.",
      "Participants preferred various methods for two-factor authentication and debated the effectiveness of email and account services.",
      "The discussions also touched on the security and convenience of Apple Pay, experiences with online retailer checkouts, and the significance of trust in online transactions."
    ],
    "points": 195,
    "commentCount": 358,
    "retryCount": 0,
    "time": 1705411795
  },
  {
    "id": 39014652,
    "title": "OutRun: Open-source Fitness Tracker for Outdoor Workouts",
    "originLink": "https://github.com/timfraedrich/OutRun",
    "originBody": "OutRun OutRun is an iOS app for recording and viewing your outdoor workouts. Despite the name it supports not just running, but also walking, hiking, cycling and skating. This project is fully open-source and ad-free. Features OutRun has a rich feature set centered around customisation and privacy. The list of features includes: Keep full control over your data Record your outdoor workouts (Running, Walking, Hiking, Cycling and Skating) View detailed statistics Export your workouts as GPX or OutRun-Backup (.orbup) Fully customise the recording process Synchronise with Apple Health Contribution You want to make this app even better? No problem! There are lots of ways you can contribute even if you cannot code yourself: Share this project. The simplest way of contributing is by sharing this project with your friends and family. Once more people see that it exists, it is more likely to get support and people interested and capable of adding to it. Join the beta. Sometimes features need to be tested before being rolled out to all users, that is where beta testers come in. They receive early and sometimes unstable versions of the app and test them thoroughly, to make sure that others get a more polished product. You can join here. Report Issues. If you see some parts of the app not working as expected, if you want to give any kind of feedback or if you just have a question you can submit an issue on the issue page of this project. Write Code. And then there is the standard way of contributing to an open-source project. You can always write code that advances the app. To see how to contribute look at the Contribution Guidelines. Translate. Last but definitely not least there is translation which is an integral part to make this project available to more people in more countries. For more information on contributing read the following section. Translation OutRun uses Weblate as a tool to make translation easier. Once languages are fully translated on said platform they will be added to the project and published to the AppStore. So far the following languages are supported: Language Translation Status CatalanCroatianCzechDanishDutchEnglishFinnishFrenchGermanItalianPolishSpanishSwedishIf you want to add languages or improve already translated ones you can do this here. Donation This project currently does not accept any kinds of donation, neither in-app nor through external methods. For now the only ways to contribute can be found under Contribution. If you cannot contribute in any other way than financially or just feel really generous at the moment you could consider supporting the folks at Weblate. They are providing their services to OutRun for free, because they are great believers in open-source development, it still costs them money to host the project on their website though, so supporting them also directly benefits OutRun. License OutRun is published under the GNU GENERAL PUBLIC LICENSE Version 3 (GLPv3): OutRun Copyright (C) 2020 Tim FraedrichThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see .",
    "commentLink": "https://news.ycombinator.com/item?id=39014652",
    "commentBody": "OutRun – Open-source, privacy oriented, outdoor fitness tracker (github.com/timfraedrich)194 points by 27theo 15 hours agohidepastfavorite50 comments apatheticonion 11 hours agoI really want this for sleep tracking. There are so many devices, each have their own algorithm for sleep tracking and the device I pick largely depends on the accuracy of the device's sleep tracking. However there are new apps, like Nukkuaa, an (EU only) sleep analysis app that uses any heart-rate tracker (like a bluetooth fitness chest strap) and infers sleep quality from the data. A generic app expands what devices are available to me. Right now I use CGM tape and stick a Fitbit Charge 3 on my tricep - but I dislike the Fitbit app and if I change to another brand I cannot export my logs. A third party app has the potential to train against more data than any individual single brand could and, with appropriate tagging, could possible offer better accuracy. Open source would be icing on the cake. Additionally, it would be great if devices like the Fitbit Charge could be used as a bluetooth heart rate monitor that can be used on apps that consume trackers on a (presumably) standard API (like TrainerRoad, Zwift, etc). - https://www.nukkuaa.com/en reply ildon 2 hours agoparentI use the Oura ring [1] for sleep tracking, it's by far the best I know of for sleep. The fact that it's a ring is convenient for comfort, and very precise as it adheres well to the finger even during night movements. [1] https://ouraring.com/raf/34be98b1fd?utm_medium=iac reply JTyQZSnP3cQGa8B 1 hour agorootparentIt’s interesting but needing a subscription for sleep and heart rate analysis is a bit too much for a $400 device. At such a price (more than my Apple Watch) I expect all the analysis to be done on the phone. reply morjom 51 minutes agorootparentThere's also the Ultrahuman Ring Air which is around the same price but requires no subscription as far as I can tell. https://ring.ultrahuman.com/ reply 2Gkashmiri 1 hour agorootparentprevTheir marketing says > research-grade sensors Are these people really this shallow? What does that even mean reply Borealid 34 minutes agorootparentIt means the sensors received the same grade as the research papers of the people who wrote the marketing copy. reply 2Gkashmiri 21 minutes agorootparenthaha good one reply com2kid 1 hour agoparentprevGo do an actual sleep study some time. You'll be hooked up to a ton of sensors, and in the morning a group of doctors pour over the results, and they'll basically vote to decide on what the data means. \"Oh this looks like REM\". So, to calibrate a sleep tracking device, you have a person wear the device, while also doing the sleep study. You do this a bunch of times. You train some ML models to try and make the outputs from the sensor data, after processing, the same as the study data. After some degree of accuracy you declare success. Now, does it work? In broad strokes, yes. You can (easily!!) see the effect of alcohol on sleep quality. If you have a crap night vs a good night, sure, a wrist based consumer device can figure that out. Actual details? Eh. I wouldn't trust the devices for anything but directional data. The more sensors devices get, the better than ML model can be trained. Now it has been awhile since I last worked on this stuff (I actually just sat next to the people doing the work), so maybe there is some revolutionary new technique out there, but if not, it is still ML models trying to correlate things and match them up to what a bunch of fancier sensors said during studies. reply figmert 5 hours agoparentprevNot open source but Sleep as Android is a great app that integrates with many different devices to track your sleep. They also have their own devices that can help improve tracking. reply justinc8687 11 hours agoparentprev> Additionally, it would be great if devices like the Fitbit Charge could be used as a bluetooth heart rate monitor that can be used on apps that consume trackers on a (presumably) standard API (like TrainerRoad, Zwift, etc). The new charge 6 does have this capability. [0] [0](https://support.google.com/fitbit/answer/14236705) reply apatheticonion 11 hours agorootparentFantastic! I am hoping this is an industry-wide trend reply sushisource 9 hours agorootparentThis has already been the case for primarily fitness oriented GPS watches / heart-rate trackers / etc for some time. Bluetooth, but even longer using ANT+, which is quite a bit more battery efficient. reply clintonb 6 hours agoparentprevYou can definitely export data from Fitbit. I built an app to export intraday data to Apple Health using Fitbit’s REST API. reply beardedwizard 4 hours agoparentprevHow could this concept be monetized? How much would you pay? reply tobiasbischoff 13 hours agoprevI'm in the EU and very excited that i possibly could compile and sideload apps like that to my phone soon. reply voisin 12 hours agoparentYou don’t need to side load. It is available in the App Store. reply Rygian 11 hours agorootparentParentçs point is to compile and upload their own version. reply marcellus23 8 hours agoparentprevYou can do that now. reply INTPenis 11 hours agoprevWhat about an Android alternative? reply mynjin 11 hours agoparentI've been using FitoTrack from the F-Droid appstore. It works fantastic, offline, private, and looking at the screenshots of the Outrun app have very similar look & layout. reply amomo 1 hour agorootparentRight ! And Paseo for steps counting is quite good for my needs too reply bjackman 1 hour agoparentprevI use RunnerUp, it works great. Glad to see there are so many options in this space! reply qurm 10 hours agoparentprevAlso Gadgetbridge on F-droid reply tonetegeatinst 6 hours agorootparentSecond this. Wish they had a better layout for supported devices and you could sort by device type....but overall this seems like the best for privacy reply byt3blight 9 hours agoparentprevI use open tracks reply shshshshsh 6 hours agorootparentI second OpenTracks[0]! I use it for running, hiking, snowboarding, exploring potential terrain, etc. I can then export easily the data in QGIS3. [0]: https://opentracksapp.com/ reply sampli 10 hours agoprevI’ve been using outrun for a year now and it’s really great! Always nice to see a good product featured on HN reply sen 12 hours agoprevI’ve been wanting something like this but so long, and had it on my list of projects to potentially try making. All I need is basic tracking of my walks/runs to motivate myself, and have zero desire for all the other bloat most fitness apps have. reply iamacyborg 11 hours agoprevGiven that there’s no mention of fitness trackers, I assume you need to record the activity on your phone? That’s not ideal, but I’m definitely not the target market as I’m happy to buy one of the higher end Garmin models. reply gen220 11 hours agoprevI started building something like this for myself (as a CLI tool, so way less ambitious). It’s a pretty fun project with a tight feedback loop. Parsing GPS data is surprisingly simple. It surprised me to learn that different apps will “smooth” out measures like elevation and distance — it’s actually quite a rich problem space with a few interesting solutions. reply voisin 12 hours agoprevI see ways to sync new runs but no way to import all past runs from Apple Health reply tibu 12 hours agoprevProject seems to be dead. reply dietr1ch 12 hours agoparentFrom #91 an asked an hour ago, > I wouldn't necessarily say abandoned. I still work on it from time to time, but progress is very very slow and I cannot prioritise it over other things atm https://github.com/timfraedrich/OutRun/issues/91 reply jamil7 8 hours agorootparentWhich is fine but they didn’t merge any PRs either. I might have a go at a fork of this but I don’t take a phone with me running or track them these days. reply bmitc 7 hours agoprev\"... tracker iOS app\" reply bartread 11 hours agoprevI realise naming is hard but, as a big fan of Sega's iconic 1980s arcade racing game and its sequels - and at willing risk of sounding a little bit gatekeepy - I must admit that I don't particularly appreciate this project's appropriation of the name OutRun. Also, it doesn't appear to be getting a lot of activity, which suggest it's perhaps not a going concern. reply Moru 10 minutes agoparentI thought is was a very good use of the name, much better than on a car game. reply kQq9oHeAz6wLLS 7 hours agoparentprevI immediately thought of the arcade game when I saw the name reply bitwize 7 hours agoparentprevI'd be happier if it played Magical Sound Shower or something while you were exercising. reply speps 14 hours agoprevI was briefly excited that it was related to the classic video game: https://en.wikipedia.org/wiki/Out_Run reply jakearmitage 13 hours agoparentI knew I couldn't be the only one. Does this serve as a consolation prize, though? https://github.com/djyt/cannonball reply neilv 11 hours agoprevOpen-source, privacy oriented, outdoor fitness tracker... for Apple iOS. reply BenFranklin100 4 hours agoparentIs there a point besides snark? If one chooses to export to Apple Health, HealthKit keeps everything on device or encrypted. Even Apple can’t access the data. I stay away from all 3rd party health trackers since most don’t seem to have any commit to privacy, so this looks like a nice addition to the ecosystem. reply Moru 7 minutes agorootparentSince they own the device and OS, they don't need to access your data. They have low-level access to all the sensors/gps. reply account-5 3 hours agorootparentprevI would imagine frustration more than snark. The \"... for Apple\" clarifications would have saved me clicking on this thread in anticipation of something I could use. reply neilv 4 hours agorootparentprevNot snark, but title clarification. (When I saw \"Open-source, privacy oriented, outdoor fitness tracker\", I thought it would be available for an open source platform, so I clicked. It being for a closed, all-your-data-are-belong-to-us platform wasn't expected. Maybe the comment saved some clicks.) reply maxhille 3 hours agorootparentprev> Even Apple can’t access the data. They own your device 100%. Of course they can access your data... reply neilv 11 hours agoparentprevOpen source related application, for open source desktops like Linux (unfortunately not for open source Android/GrapheneOS currently), there's Alex Harsányi's ActivityLog2 (which is an impressive application in Racket): https://github.com/alex-hhh/ActivityLog2/wiki/Just-The-Scree... Incidentally, I'd love to have an open source, privacy-respecting nutrition tracker that works on open source desktops. (I find my laptop much, much faster than phone for recording this data, at least in the ad hoc way I'm doing it, including things like weights of every ingredient that goes into a dish.) reply hombre_fatal 10 hours agorootparentThe hard part about a nutrition tracker is the data. The most complete nutrient data for basic foods seems to be http://www.ncc.umn.edu/food-and-nutrient-database/ which you need to pay for, so I wonder how an open source implementation could work. I use Cronometer daily which shows you the datasource for each food you add, and if it's not NCCDB (the one at the link above) then chances are it's not going to have much info. Yet that extra info is exactly why I'm using a nutrient tracker rather than just a calorie tracker. reply bhakunikaran 2 hours agoprev [–] I wonder how Patrick thinks new tech like blockchain or AI might change the way we handle money. Will it make things easier, or will it bring new challenges? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OutRun is an iOS app that lets users record and monitor their outdoor workouts like running, walking, cycling, etc.",
      "The app is open-source, ad-free, and offers features like data control, detailed statistics, and workout export options.",
      "Users can contribute to the app by sharing it, joining beta testing, reporting issues, writing code, or assisting with translation."
    ],
    "commentSummary": [
      "The open-source fitness tracker OutRun receives positive feedback, particularly for its sleep tracking capabilities.",
      "Users recommend other devices like the Oura ring and Ultrahuman Ring Air for sleep tracking.",
      "There is a debate about the accuracy and limitations of sleep tracking devices, with a desire for more sensors to improve accuracy."
    ],
    "points": 194,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1705420127
  }
]
