[
  {
    "id": 40469592,
    "title": "Spot: New Cross-Platform, Reactive GUI Toolkit for Go Developers",
    "originLink": "https://github.com/roblillack/spot",
    "originBody": "Hi HN, I’m excited to share Spot, a simple, cross-platform, React-like GUI library for Go. It is just a few days old and has lots of missing features but I&#x27;m happy with the results so far, and looking for some design feedback.Spot is designed to be easy to use and provide a consistent API across different platforms (mainly Mac & Linux). It’s inspired by React, but written in Go, aiming to combine the best of both worlds: the easy tooling & performance of Go with a modern, reactive approach to UI development.Key features:- Cross-platform: Leveraging FLTK[1] & Cocoa[2], Spot works on Mac, Linux, and the BSDs with plans for native Windows support in the future.- Reactive UI: Adopts a React-like model for building UIs, making it intuitive for those familiar with reactive frameworks.- Traditional, native widget set: Utilizes native widgets where available to provide a more traditional look and feel.Why I built it:I was searching for a cross-platform GUI toolkit for Go that had a more traditional appearance, and none of the existing options quite met my needs. I then started playing with Gocoa and go-fltk and suddenly I worked on an experiment to see how challenging it would be to build something like React in Go, and it kinda evolved into Spot. ¯\\_(ツ)_&#x2F;¯In 2024, is there a still place for classic desktop GUIs—even with a modern spin?I’d love to hear your thoughts, feedback, and any suggestions for improvement. Also, contributions are very welcome.Thank you for checking it out![1] https:&#x2F;&#x2F;github.com&#x2F;pwiecz&#x2F;go-fltk[2] https:&#x2F;&#x2F;github.com&#x2F;roblillack&#x2F;gocoa",
    "commentLink": "https://news.ycombinator.com/item?id=40469592",
    "commentBody": "Spot – Simple, cross-platform, reactive desktop GUI toolkit for Go (github.com/roblillack)276 points by da_rob 23 hours agohidepastfavorite47 comments Hi HN, I’m excited to share Spot, a simple, cross-platform, React-like GUI library for Go. It is just a few days old and has lots of missing features but I'm happy with the results so far, and looking for some design feedback. Spot is designed to be easy to use and provide a consistent API across different platforms (mainly Mac & Linux). It’s inspired by React, but written in Go, aiming to combine the best of both worlds: the easy tooling & performance of Go with a modern, reactive approach to UI development. Key features: - Cross-platform: Leveraging FLTK[1] & Cocoa[2], Spot works on Mac, Linux, and the BSDs with plans for native Windows support in the future. - Reactive UI: Adopts a React-like model for building UIs, making it intuitive for those familiar with reactive frameworks. - Traditional, native widget set: Utilizes native widgets where available to provide a more traditional look and feel. Why I built it: I was searching for a cross-platform GUI toolkit for Go that had a more traditional appearance, and none of the existing options quite met my needs. I then started playing with Gocoa and go-fltk and suddenly I worked on an experiment to see how challenging it would be to build something like React in Go, and it kinda evolved into Spot. ¯\\_(ツ)_/¯ In 2024, is there a still place for classic desktop GUIs—even with a modern spin? I’d love to hear your thoughts, feedback, and any suggestions for improvement. Also, contributions are very welcome. Thank you for checking it out! [1] https://github.com/pwiecz/go-fltk [2] https://github.com/roblillack/gocoa iamcalledrob 8 hours agoI have been looking for something like this in Go for a while. I think there's a real opportunity for Go to provide a great developer experience for cross platform UI due to how simple the build process is. Speaking from experience, half the pain of cross platform development is managing build complexity, which Go basically eliminates. I'm curious how you'll end up solving for cross-platform layout when native controls have different intrinsic sizes per platform? This is something I haven't seen solved super well in cross platform toolkits. Wishing you luck though. reply heywire 18 hours agoprevI’ll have to give this a look! I’ve been looking for a simple way to use Go to write an internal development tool which is basically just a form with some buttons and text fields. I tried Gio, but had a hard time with wrapping my head around it. Right now I’m using wails and like it much better. This looks interesting and worth a look! reply da_rob 13 hours agoparentPerfect use-case, IMHO! Looking forward to hear your thoughts ocne you tried it. reply felipefar 14 hours agoprevI'm curious to hear your take on this: what's the advantage of following the virtual control tree approach compared to instead updating directly the controls that are displayed to the user? reply da_rob 13 hours agoparentFor complex scenarios (i.e. the user interacting with the UI while there are some long-running processes in the backround doing this, too), managing the state quickly gets unwiedly: You need to write callback code everywhere that carefully has to inspect the current status of all other activities and might then updates 10s of widgets accordingly. With the reactive approach you are able to write a single rendering function that describes the interface for any given state and the framework takes care of the rest (when to call that function, what \"input\" to give it). It's just way easier to wrap my head around this and after working with React it is hard to go back. Hence the experiment to see if something comparable could be done in Go. reply felipefar 13 hours agorootparentI'm already convinced of the benefit of declarative GUIs over imperative ones. To be clearer, what I mean by declarative GUIs is ones where I can specify that UI elements should be bound to certain values in the memory of my program, so that they don't have to be changed explicitly by imperative code. Ex.: TextBox { Text: app.currentUser.name } Instead of having to call setText on that TextBox. What I'm still missing is why it has to involve a virtual tree being maintained and synchronized with the real GUI tree. The UI engine could implement the above binding by instantiating for the user a callback trigerred when the bound property changes and changing precisely the value currently shown on the screen. Hence, the UI engine is in charge of the callbacks, which keeps the user unburdened by callbacks. reply iainmerrick 10 hours agorootparentSolidJS works pretty much as you describe, and it’s great. It feels like the best bits of React, but object lifetimes and re-rendering and the real UI behavior are all much easier to understand. (Or at least, to paraphrase Joseph Heller, “it’s intuitive once you get your head around it”) reply dweymouth 4 hours agorootparentprevThis is exactly how the data binding APIs in Fyne (another Go GUI toolkit) work. And it's also an optional feature, so if you want to handle things by registering callbacks and calling setters yourself you can do that too. reply jdthedisciple 10 hours agorootparentprevIf I'm not mistaken what you describe is pretty much exactly the approach in .NET app development (WPF, MAUI, ...) with C# for the logic and XAML for declarative UI including Binding and it works well and performantly so. reply da_rob 12 hours agorootparentprevThe virtual component tree is needed as there is no 1:1 mapping of components to widgets. Using the virtual tree, you are able to build components which dynamically change the rendered subtree based on the current state. reply lenkite 12 hours agorootparentprevThanks for making this library, but IMHO these are distinct concerns. One can leverage a state machine or an ECS library for complex state management. Only basic data binding can be a framework capability if needed. The web had a lack of options for state management and hence React coupled the two. reply KingOfCoders 15 hours agoprevI applaud your effort, but cross plattform with no Windows support? reply RobotToaster 8 hours agoparentIt looks like it just uses FLTK on windows, they just don't have native support on it. reply atlas_hugged 9 hours agoparentprevUse WSL reply jppittma 15 hours agoparentprevHere’s a nickel kid. Get yourself a better computer. reply poisonborz 1 hour agorootparentWhy is platform shaming tolerated around here reply KingOfCoders 15 hours agorootparentprevThanks, I'd wish I could get my G4 Cube back. reply allanrbo 22 hours agoprevWas looking for something like this some years back. Though I wanted Windows support too. Ended up switching to C++ to use wxWidgets, giving me small self contained binaries. reply mappu 19 hours agoparentgo-fltk does build and run on Windows, pretty well actually. For a native toolkit, I was impressed to see FLTK supports Ctrl-+ and Ctrl+- to zoom the entire application like a browser. And https://github.com/fltk-rs/fltk-theme?tab=readme-ov-file#wid... really improved my impression of how \"native\" FLTK can be made to look. On a related note, I discovered GoVCL https://z-kit.cc/en/ recently and am interested to try it out. reply da_rob 22 hours agoparentprevBig fan of WxWindows, I'm just too invested into Go these days. :) Self-contained Spot \"Hello World\" is 2.3MiB on my Mac. Not pretty, but works for me. reply ofrzeta 12 hours agorootparentSo you didn't like to continue on that? https://github.com/dontpanic92/wxGo reply da_rob 11 hours agorootparentGeez, look at the build instructions! It all went downhill when they switched from Motif to GTK. reply zerr 11 hours agoparentprevThere is wxGo, but sadly the project is not maintained. reply Heliodex 19 hours agoprevIf only I had known about this (or judging by commit history, if it existed) ~3 weeks ago. I've been saying for ages that either React ported to Go or a React-like framework for Go would be an incredible development experience, so this looks perfect (I used to be a big hater of React.js until I was enlightened by React.lua). reply rkwz 16 hours agoparentI had the same problem - really liked the way I used to do component composition in React, it was hard to go back. Eventually found a way to do something similar using just the standard Go html/template. I’ve written about the implementation here: https://www.sheshbabu.com/posts/react-like-composition-using... reply biomcgary 1 hour agorootparentThis a great write-up and similar to an approach that I stumbled upon a few years ago. Have you thought about HTMXing this approach to get away from full page renders? reply apitman 18 hours agoparentprevDon't leave us hanging! What did you use instead 3 weeks ago? reply Heliodex 18 hours agorootparentWails . I wanted to use Go but wasn't as familiar with native UI as with HTML/CSS/JS, I tried out most of the other popular Go UI frameworks too though they didn't feel as comfortable to use as Spot's React-like model. Wails is still pretty epic as well though. reply parlortricks 16 hours agorootparentoh this looks great reply getcrunk 15 hours agoprevFltk supports windows. Will you be using another solution and that’s why u don’t support windows yet? reply da_rob 14 hours agoparentGiven the right C compiler and everything, Spot should work without changes on Windows (and select the FLTK) backend. My (low prio, though) goal is to implement a Win32-based backend, though, the first step is done: https://github.com/roblillack/spot/pull/4 reply ygnasygnoimqwb 22 hours agoprev> runtime.LockOSThread() > https://github.com/roblillack/spot/blob/main/ui/init_fltk.go... Does this imply GOMAXPROCS needs to be set to at least 2? reply da_rob 22 hours agoparentNo, it just means that the main goroutine should not be moved onto another OS thread to ensure that it is always the same OS thread which will run UI updates. This is necessary for most UI libraries and in Spot's case implemented for both the FLTK and the Cocoa backend. reply scosman 22 hours agoprevCool! What does building for cross platform look like? Would love if there was a command that produced my MacOs .app and Windows exe without me having to dive into package management/containers/signing headaches per platform. reply da_rob 22 hours agoparentThe backends (FLTK/Gocoa) are using CGo, which currently does not support cross-compiling as far as I know. So you still need to have a build pipeline with multiple operating systems, signing/notarization tools, etc. :( reply mappu 19 hours agorootparentYou can cross-compile Cgo, you \"just\" need a C compiler and linker that works for the target platform. osxcross, xgo have some or maybe you can set CC=zig cc. For macOS you need signing/notarization tools either when building natively or when cross-compiling, it's not any different. `rcodesign` has made this process much easier in recent years. reply apitman 19 hours agorootparent+1 for `CC=zig cc`. Feels like magic reply scosman 21 hours agorootparentprevToo bad. I swear half the electron apps use it for ease of Xplatform build. I’d love something like this for giving my CLIs UIs. reply felipefar 14 hours agoparentprevThat'd be amazing not only for go apps but for software built on other languages as well. I'd pay for a good, straightforward build packing + crash reporting + autoupdater solution for my apps. reply jakjak123 20 hours agoprevVery cool idea! If someone implements GTK I would absolutely use it, its just fltk is no fun in Linux world reply zem 12 hours agoprevinteresting that this uses fltk; I've seen very few language bindings to it reply andrewfromx 10 hours agoprevBeen using https://fyne.io/ https://github.com/fyne-io/fyne for years but will check this out ( terminal but still great go guis: https://github.com/gizak/termui https://github.com/charmbracelet/bubbletea ) Never used but should be in this list: https://gioui.org/ https://mattn.github.io/go-gtk/ https://github.com/lxn/walk reply bsimpson 4 hours agoparentIt's strange to see \"inspired by Material Design,\" and then a bunch of controls that look nothing like Material. reply ASalazarMX 23 hours agoprevI don't know how to take this in. On one side, it's great to have a cross-platform GUI toolkit that's easy to use; on the other side, it feels like a React-like UI is contrary to the spirit of simplicity and resource efficiency of Go. I'm sure it has its perfect use cases, though. reply da_rob 22 hours agoparentWhile I agree, that the virtual widget tree has a certain overhead, the typical desktop app you would create with Spot probably does not have 1000s of components—more like ten to twenty probably. Compare that to the immediate mode GUIs that currently are en vogue, that re-render all controls all the time. Spot does not have any optimizations regarding render performance at the moment and it might never be necessary to add them. On the other hand, due to the reactive programming model, I really like how state management get a lot clearer—especially when working with multiple goroutines. reply kdma 8 hours agoprev [–] Just for curioristy why did you adopot a react-like model and not implement react-dom as projects like react native do? reply bsimpson 4 hours agoparent [–] React Native runs in JavaScript. This is in Go. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Spot is a new cross-platform GUI library for the Go programming language, inspired by React, aiming to provide a consistent API across Mac, Linux, and BSDs, with future plans for Windows support.",
      "It uses FLTK and Cocoa to offer a traditional, native widget set and adopts a reactive UI model.",
      "The creator developed Spot to address the need for a traditional-looking, cross-platform GUI toolkit in Go and is seeking design feedback and contributions."
    ],
    "commentSummary": [
      "Spot is a new cross-platform GUI toolkit for Go, inspired by React, designed to offer a consistent API across Mac, Linux, and BSDs, with future plans for Windows support.",
      "It uses FLTK and Cocoa for native widgets, aiming to merge Go's performance with a modern, reactive UI approach, and the creator is seeking community feedback and contributions.",
      "Discussions emphasize the advantages of a virtual control tree for state management and compare it with declarative GUIs, while users share experiences with other frameworks like GTK and Fyne, highlighting cross-platform build challenges and efficient state management needs."
    ],
    "points": 276,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1716578371
  },
  {
    "id": 40474712,
    "title": "Security Concerns Raised Over Non-Go Repositories in Go's Checksum Database",
    "originLink": "https://reverse.put.as/2024/05/24/abusing-go-infrastructure/",
    "originBody": "Abusing Go's infrastructure May 24, 2024 · 10 min · 1918 words I apologize if this information is already known, but I couldn’t find any references about it and I wanted to understand what was going on and share with you because I think there is some value doing it. In case this wasn’t known, I apologize to the Go team for not talking to them first and jumping the full disclosure gun (I don’t think it’s that severe). I really like Go! Thanks for all your great work. The problem Last night, I was exploring the contents of Go’s checksum database, and I noticed a curious result: sqlite> select path, count(path) from modules group by path order by count(path) desc; github.com/homebrew/homebrew-core|39438 github.com/Homebrew/homebrew-core|30896 github.com/concourse/concourse|25372 github.com/openshift/release|24065 github.com/cilium/cilium|22138 copy The homebrew/Homebrew case divergence is explained by Go’s documentation (thanks to Filippo Valsorda!): To avoid ambiguity when serving from case-insensitive file systems, the $module and $version elements are case-encoded by replacing every uppercase letter with an exclamation mark followed by the corresponding lower-case letter. This allows modules example.com/M and example.com/m to both be stored on disk, since the former is encoded as example.com/!m. Anyway, this caught my attention because Homebrew is known to use Ruby, and so I went to check the repository contents. GitHub language stats confirm it: This result seems unexpected, since there are no traces of Go and there are more than 70,000 entries in Go’s checksum database. To be sure, I cloned the repository and tried to find any Go-related files such as go.mod or Go source files; however, nothing exists. So I posted a tweet (technically a toot on Mastodon), got no replied and moved on. While continuing to explore the database, I noticed another unusual case in github.com/Edu4rdSHL/rust-headless-chrome. It’s just a fork of rust-headless-chrome, and there is nothing remarkable about the fork or the original except that they are both Rust repositories, and once again, no connection to Go. Now my curiosity is piqued and the evil mode kicks in. It feels like arbitrary data can be pushed to the checksum database without a connection to Go. Why is the data being pushed? And how is it being pushed? I go to bed thinking about this, which is the most dangerous moment for security research. As I try to fall asleep, I come up with tons of ideas, but I’m usually too tired or lazy to take notes, and so, quite frequently, I can’t remember them in the morning. But this one wasn’t forgotten! Research A new day and a curious mind demands answers. Why, how and, what if are the most dangerous questions in this field. If a Git repository has nothing to do with Go code, how does it appear in the Go checksum database? From previous documentation readings, I knew that proxy.golang.org was the default modules proxy, and sum.golang.org for the checksum database. A couple of ripgrep searches in Go source code return nothing interesting, so it was time to read Go’s documentation, which is usually quite good. Where to start? Go Modules Reference was a great candidate and I finally found the answer to my question: If the go command consults the checksum database, then the first step is to retrieve the record data through the /lookup endpoint. If the module version is not yet recorded in the log, the checksum database will try to fetch it from the origin server before replying. Okay, this was easy! If the module doesn’t exist in the checksum database (and proxy), it will be downloaded by the checksum and proxy infrastructure. One of my questions was: how did the checksum database retrieve the modules since they can be anywhere? I couldn’t find anything in the Go code that was responsible for that (which wouldn’t explain at all how Ruby and Rust code ended up in the database). So, the next logical step is easy. Can I get the Go checksum server to download arbitrary data? According to the documentation, the endpoint to try this is $base/lookup/$module@$version: Returns the log record number for the entry about $module at $version, followed by the data for the record (that is, the go.sum lines for $module at $version) and a signed, encoded tree description that contains the record. First, let’s test it with a known record to see if and how it works: $ curl https://sum.golang.org/lookup/github.com/homebrew/homebrew-core@v0.0.0-20240524162643-646fe2715a1c 26235981 github.com/homebrew/homebrew-core v0.0.0-20240524162643-646fe2715a1c h1:U32osaj3vZGypOtq7tsIHhZAYNOmiShiXJysIFGTqyM= github.com/homebrew/homebrew-core v0.0.0-20240524162643-646fe2715a1c/go.mod h1:TM9a6pxWZJZZWuMzxESXhb6yaBaH9JAKDM4wpIzJsDE= go.sum database tree 26238433 TQyXJYWJL6Z1OnKk5JXLAb9xfWrtHKjAUXKx5UQCa9Q= — sum.golang.org Az3grm+I35+HBcG+YvxlX+nzkXah3cWlBac/4EytsG24bEHFLrJNvyz5SphrKAHSS0EeDKJXpnb3cvdUtqVSiaNLVAY= copy Since the repository doesn’t seem to have any version tag the pseudo-version is used instead. Go documentation explains the logic behind pseudo-versions. The next step is to verify wheter a new Go module repository will be added to the checksum database and proxy if we call the lookup endpoint, as described. After creating a simple new Go module and uploading to my GitHub account I have tried to issue the lookup command in two different forms, one not totally according to documentation, the other one also incorrect but trying to follow documentation. Both return errors, although different. $ curl https://sum.golang.org/lookup/github.com/gdbinit/fluxmatter@latest bad request: version \"latest\" is not canonical (wanted \"\") $ curl https://sum.golang.org/lookup/github.com/gdbinit/fluxmatter@v0.0.0 not found: github.com/gdbinit/fluxmatter@v0.0.0: invalid version: unknown revision v0.0.0 copy The errors are kind of expected since I haven’t versioned the module and wasn’t using the correct pseudo-version. But we can verify if the new module was fetched as described by the documentation. The easiest way would be to generate the correct pseudo-version and make another query to the checksum database. If the module was indeed downloaded, then the entry would exist and returned as in the homebrew-core test. Another way would be to resync my copy of the checksum database and query it for my module: sqlite> select * from modules where path = 'github.com/gdbinit/fluxmatter'; github.com/gdbinit/fluxmatter|v0.0.0-20240524163826-a7e64ffd69f2|2024-05-24T16:40:51.203837Z copy Finally we can query the proxy and use the latest query to return the latest known version of a module. And then download the module zip and prove that we just stored our arbitrary data in the Go infrastructure. $ curl https://proxy.golang.org/github.com/gdbinit/fluxmatter/@latest {\"Version\":\"v0.0.0-20240524163826-a7e64ffd69f2\",\"Time\":\"2024-05-24T16:38:26Z\",\"Origin\":{\"VCS\":\"git\",\"URL\":\"https://github.com/gdbinit/fluxmatter\",\"Hash\":\"a7e64ffd69f2d0751a52736e832a8d77a21059e7\"}} $ curl -O https://proxy.golang.org/github.com/gdbinit/fluxmatter/@v/v0.0.0-20240524163826-a7e64ffd69f2.zip $ file v0.0.0-20240524163826-a7e64ffd69f2.zip v0.0.0-20240524163826-a7e64ffd69f2.zip: Zip archive data, at least v2.0 to extract $ unzip -t v0.0.0-20240524163826-a7e64ffd69f2.zip Archive: v0.0.0-20240524163826-a7e64ffd69f2.zip testing: github.com/gdbinit/fluxmatter@v0.0.0-20240524163826-a7e64ffd69f2/LICENSE OK testing: github.com/gdbinit/fluxmatter@v0.0.0-20240524163826-a7e64ffd69f2/fluxmatter.go OK testing: github.com/gdbinit/fluxmatter@v0.0.0-20240524163826-a7e64ffd69f2/go.mod OK No errors detected in compressed data of v0.0.0-20240524163826-a7e64ffd69f2.zip. copy And voilà, everything works! There’s no need to specify a version in the lookup (at least for the initial seeding); just a lookup query that contains the module path and something like a version. Next I tried to do the same with a repository that has no Go code whatsoever, to prove that everything works the same way. $ curl https://sum.golang.org/lookup/github.com/gdbinit/readmem@v0.0.0 not found: github.com/gdbinit/readmem@v0.0.0: invalid version: unknown revision v0.0.0 sqlite> select * from modules where path = 'github.com/gdbinit/readmem'; github.com/gdbinit/readmem|v0.0.0-20131006075740-407cb0a56933|2024-05-24T16:45:35.88456Z $ curl https://proxy.golang.org/github.com/gdbinit/readmem/@latest {\"Version\":\"v0.0.0-20131006075740-407cb0a56933\",\"Time\":\"2013-10-06T07:57:40Z\",\"Origin\":{\"VCS\":\"git\",\"URL\":\"https://github.com/gdbinit/readmem\",\"Hash\":\"407cb0a569336f98f3772582a31c17aa080caf66\"}} $ curl -O https://proxy.golang.org/github.com/gdbinit/readmem/@v/v0.0.0-20131006075740-407cb0a56933.zip $ file v0.0.0-20131006075740-407cb0a56933.zip v0.0.0-20131006075740-407cb0a56933.zip: Zip archive data, at least v2.0 to extract $ unzip -t v0.0.0-20131006075740-407cb0a56933.zip Archive: v0.0.0-20131006075740-407cb0a56933.zip testing: github.com/gdbinit/readmem@v0.0.0-20131006075740-407cb0a56933/Entitlements.plist OK testing: github.com/gdbinit/readmem@v0.0.0-20131006075740-407cb0a56933/README OK testing: github.com/gdbinit/readmem@v0.0.0-20131006075740-407cb0a56933/readmem.xcodeproj/project.pbxproj OK testing: github.com/gdbinit/readmem@v0.0.0-20131006075740-407cb0a56933/readmem/main.c OK No errors detected in compressed data of v0.0.0-20131006075740-407cb0a56933.zip. copy This demonstrates that it’s possible to load into Go public proxy arbitrary data. The experiment was done using GitHub but it should work with other hosting sites. One curious statistic is the amount of Go modules stored at GitHub: sqlite> select count(distinct path) from modules; 1591375 sqlite> select count(distinct path) from modules where path like 'github.com%'; 1515957 copy Around 95% of the unique paths present in sum.golang.org are hosted at GitHub. This is a raw statistic, without removing bogus data such as forks and targets that aren’t really Go code. But it still shows the magnitude of GitHub dependency in the Go ecosystem. Go authors don’t seem to be completely unaware of this kind of scenario and implemented some restrictions, which are described in File path and size constraints section. The most relevant one is: A module zip file may be at most 500 MiB in size. The total uncompressed size of its files is also limited to 500 MiB. go.mod files are limited to 16 MiB. LICENSE files are also limited to 16 MiB. These limits exist to mitigate denial of service attacks on users, proxies, and other parts of the module ecosystem. Repositories that contain more than 500 MiB of files in a module directory tree should tag module versions at commits that only include files needed to build the module’s packages; videos, models, and other large assets are usually not needed for builds. 500MiB is more than enough for considerable abuse and all the others aren’t really a problem. Abuse what? For example, it can be used to bypass destination download restrictions on developer machines and in CI/CD servers (assuming that there isn’t a private GOPROXY). Malware can simply store payloads and retrieve them from the proxy when needed. And because we don’t have restrictions regarding the source domain (as long it’s a working VCS), we can load up the payloads from anywhere and make those sources disappear, leaving only a small trace in the checksum database entry. A Denial of Service (DoS) attack on proxy.golang.org might be challenging to execute. I have shown that we can request any random Git repository (and probably any of the other supported VCS) to be downloaded by the proxy. For a possible attack, we would need to first gather as many GitHub URLs as possible and then issue as many requests as possible to the lookup API. I have no idea about the server implementation, but I would assume that something similar to a work queue is implemented, so there is a limit to the amount of parallel requests that will be processed. Bandwidth protections could also possibly be triggered from GitHub’s side. There is also a possibility of DoS-ing the storage space. I’m just guessing here :-). A command and control (C2) can also be easily implemented on top of this. It’s very easy to find the latest version of any module using the latest query, so there isn’t need to query the checksum database and find all the available versions of our payload. The payload can be a simple file or can be disguised inside the go.mod or any other Go source file for extra stealthiness. A module DGA (Domain Generation Algorithm) can be used to avoid using a single repository for the C2. My original goal was to write a sample C2 to demonstrate this, but there isn’t really much to be done here. To download commands from the C2, the implant just needs the following steps: Make a request to https://proxy.golang.org/module_path/@latest. Parse the JSON result and extract the pseudo-version (or version if used). Make another request to https://proxy.golang.org/module_path/@v/version.zip to download the zip file. Extract the zip contents and parse the commands. Quite easy in some 300 or less lines of Go code. Conclusions My questions have been answered, and now I understand how the checksum database process works. So far, it’s not a severe issue in Go infrastructure. It’s something that can be easily abused but also improved. Maybe there are (documented or not) reasons to let non-Go code be uploaded to the proxy and checksum database. Or maybe there is already someone abusing this, and we can go on a treasure hunt through the almost 1.6 million unique repositories (my up-to-date database copy contains almost 22 million entries). I still have questions why certain valid non-Go projects are in the database. Are they doing it on purpose? Why? Using Go’s transparent log as safety backup? Any hints about this? I had fun with this, and I have a bunch of ideas to further explore. I have a feeling… ;-). Have fun, fG! Next » Attacking the heart of an OpenRG modem",
    "commentLink": "https://news.ycombinator.com/item?id=40474712",
    "commentBody": "Abusing Go's Infrastructure (put.as)231 points by efge 6 hours agohidepastfavorite42 comments miki123211 4 hours agoAny online service that lets users upload material that is then publicly visible will eventually be used for command-and-control, copyright infringement and hosting CSAM. This is especially true for services that have other important uses besides file hosting and hence are hard to block. This already happened to Twitter[1], Telegram[2], and even the PGP key infrastructure[3], not to mention obvious suspects like GitHub. [1] https://pentestlab.blog/2017/09/26/command-and-control-twitt... [2] https://www.blazeinfosec.com/post/leveraging-telegram-as-a-c... [3] https://torrentfreak.com/openpgp-keyservers-now-store-irremo... reply liquidgecka 2 hours agoparentAnd Gmail and Google groups, and Google drive, and Gchat, on and on. The data you store doesn't even have to be public. With Gmail they would distribute credentials to log in and read attachments that they uploaded via imap. (I am a former Google SAD-SRE [Spam, Abuse, Delivery]) reply howenterprisey 2 hours agorootparentJust curious, \"Delivery\" doesn't seem to be the same sort of thing as \"Spam\" and \"Abuse\": why are the three grouped? reply romafirst3 46 minutes agorootparentDelivery is what happens if it’s not spam or abuse. reply fred256 1 hour agorootparentprevNo inside information, but presumably this means Delivery to other organizations, which, among other things, includes maintaining outbound IP reputation, which is closely related to Spam and Abuse. reply gloryjulio 1 hour agorootparentprevJust a side note, I found the name sad sre funny and blursed at the same time reply zepolen 1 hour agorootparentprevQuestion, how would you know without invading the user's privacy? reply kccqzy 1 hour agorootparentAn algorithm that processes private user data is by itself not invading anyone's privacy. It's clear to me that invasion of privacy only happens when humans look at private user data directly, or look at user data that's not sufficiently processed by an algorithm. Otherwise, something as simple as a spell checker would be an invasion of privacy because it literally looks at every word in an email you write. That's absurd. reply _heimdall 52 minutes agorootparentAt least in my opinion, there's a big difference with where the data lives and where the checking algorithm is run. I don't think a spell checker would fall into what I'd consider a privacy concern as long as the spell checker is running locally on my device. reply nerdponx 1 hour agoparentprevIt seems like it would be pretty easy to use PyPI for this, because packages can contain arbitrary non-Python files. And you can also do things like base 64 encoding your files in strings in Python code. reply weinzierl 48 minutes agoparentprevNot sure if it has already happened, but the not so obvious one is HuggingFace. reply palata 59 minutes agoprevThat's maybe naive, but... how is that different than just pushing files to e.g. a GitHub repository? Is it just the fact that you need to create an account for GitHub? Because I can store arbitrary data there, too. Without the 500M limit... reply 8organicbits 5 hours agoprevI know pypi has some non-python projects as well. Python needs the ability to distribute wheels, which are compiled binaries, as the user may not be able to compile library code. Lots of that code is written in C, but Golang[1] is also possible. I can't find an example, but I believe I've seen this used for distributing applications (not libraries) as well. It's kinda cool to write some app in C, upload to pypi, and then ask users to install with `pip install`. [1] https://github.com/popatam/gopy_build_wheel_example reply bee_rider 5 hours agoparentHypothetically if they did try to add some requirement to use Python, people could just comply maliciously by providing the most minimal stub of Python code, right? Linux, but ls is written in Python. So it is probably better just to not play games. reply Maxatar 3 hours agorootparentYou could embed the binary data in a Python string and then have the installer dump that string to a file. reply mort96 1 hour agoparentprevI guess that was much more useful as a use case before pip started requiring you to be in a venv/virtualenv/pipenv/pyenv/whatever to download packages reply rfoo 5 hours agoparentprevpip install cmake or even proprietary binaries, pip install nvidia-cudnn-cu12 reply IshKebab 3 hours agorootparentYeah I copied CMake's idea of using PyPI and I also use it to distribute some pure Rust CLI tools using Maturin. It works really well. Pip is... well it's about on par with most other package managers, i.e. not great, not terrible, but it has some pretty huge advantages over any other software distribution method on Linux: * Very likely to be installed already on Linux and probably Mac too. * Doesn't require root to install. You can even have isolated installs via pyenv. * I don't have to ask anyone's permission to publish a package. * I only have to make one package. If any can think of a better option I'm all ears but until then I'm fairly happy with this hack. reply Too 1 hour agorootparentSome of those arguments are becoming more and more difficult as pip and distros are pushing for use of venvs and now requires a scary --break-system-packages argument if you were to use the pre installed launcher. reply kyrra 5 hours agoprevGoogler, opinions are my own. I know nothing about this space. I would hope the Go team collaborated with GCP and Drive, as hosting malicious files is something Google has to deal with all the time. This isn't much different from other endpoints Google already allows people to put random data on. reply ithkuil 4 hours agoprevI toyed with the idea of piggybacking on (i.e. abusing) the golang proxy and sumdb to have a free transparent log of checksums of arbitrary URLs https://getsum.pub/ reply arccy 4 hours agoparentsounds convoluted. If you just want a public transparency log, the public rekor instance under the sigstore project is much more appropriate for that. https://www.sigstore.dev/ https://docs.sigstore.dev/logging/overview/ reply skybrian 2 hours agorootparentInteresting! Looks like it's being used by some npm packages [1] and soon homebrew will be using it [2]. Any other interesting usage? As a user, the npm usage doesn't seem very prominent. On an npm's web page, there's a checkmark next to the version number on the right side that I hadn't paid any attention to before, with more information at the very bottom of the page. Here's an example. [3] [1] https://blog.sigstore.dev/npm-provenance-ga/ [2] https://blog.sigstore.dev/homebrew-build-provenance/ [3] https://www.npmjs.com/package/fast-check reply mynameisvlad 1 hour agorootparentIt’s at the bottom of the page on mobile. On desktop, that’s the first thing on the right hand side of the screen IIRC. reply skybrian 40 minutes agorootparentFor me on desktop, the version seems to be the fourth thing down in the right column, under weekly downloads, and there's a checkmark. (Or maybe I'm missing something.) reply lpapez 3 hours agorootparentprevSure, but the gosum database is a critical piece of worldwide software infrastructure, so you can count on it being accesible behind many firewalls and always up. And it's completely free and anonymus. Perfect for the purpose. reply ithkuil 3 hours agorootparentprevYeah when I did that there was no public rekor instance ran by the sigstore project so I choose the only available public transparency log I could bend to my needs (x509 transparency logs were an alternative but it'd quickly hit rate limits by acme providers) reply verdverm 4 hours agoprevCUE's module system is finally rolling out, MVS likes Go's, but built on OCI infra. If you are interested in dependency management systems, here are some links - proposal: https://github.com/cue-lang/proposal/tree/main/designs/modul... - custom registry: https://cuelang.org/docs/tutorial/working-with-a-custom-modu... - road map: https://github.com/orgs/cue-lang/projects/10/views/8 - in 0.9.0-alpha-5, modules become enabled by default: https://github.com/cue-lang/cue/releases/tag/v0.9.0-alpha.5 For Go Sum, the Trillian project backs the transparency log: https://github.com/google/trillian CUE plans to piggyback on the OCI options with attestations and such reply dgellow 2 hours agoparentWhat does that have to do with the linked article? reply verdverm 2 hours agorootparentThe CUE team worked with the Go team on the module system. From these interactions, and community input, they decided against using a proxy like Go has. The \"exploit\" in the article was one of the reasons they made this decision, and chose to use OCI registries instead. The V1 proposal actually proposed using the same Go proxy servers as a stopgap, which received significant pushback from the community (I was probably the loudest voice against the idea). The Go team was supportive at the time, but this would have been exactly what OP talks about, having non-Go projects in the proxy/sumdb. So CUE's module design can be seen as an evolution on Go's, building on the good parts while addressing some of the shortcomings. Fun fact, CUE started as a fork of Go, mainly for the internal compiler tooling and packages reply erik_seaberg 30 minutes agoprevW3C laid the groundwork for everything on the Web to be heavily cacheable, so it's weird that there are so few general-purpose proxy caches. Are publishers sending short \"Cache-Control: max-age\" or \"Vary: Cookie\" responses when they didn't need to? Are too many ISPs paying for transit rather than peering? reply 9dev 3 hours agoprevOff topic: took a look at the domain, had a foreboding on the innuendo, found mostly what I expected on put.as … reply KolmogorovComp 2 hours agoparenthttps://put.as/ Mildly NSFW. reply moonlion_eth 3 hours agoparentprevI made the mistake of doing that at work reply yazzku 2 hours agoparentprevNot off-topic at all; came here just for this. reply arccy 5 hours agoprevit's a known issue https://github.com/golang/go/issues/31866 reply yjftsjthsd-h 5 hours agoparentThat fix would help with accidents, but wouldn't someone intentionally hoping doing it just add a .mod and .go file to the root? reply jerf 4 hours agorootparentHow do you \"fix\" that at all? In the end, there is no definition of \"a source control repository that is a Go module\" that is robust to this sort of \"attack\"... although calling it an \"attack\" is kind of dubious, the reasons why this is a bad thing strike me as very strained and relatively weak. Mostly it hurts Google by hosting too much stuff, but, good luck bringing them down that way. reply oooyay 3 hours agoparentprevColor me unsurprised Marwan is on this issue. He and Aaron wrote Athens, Marwan wrote (to my knowledge) the first Go download protocol implementation that Athens is based on. This issue is kind of curious because Athens already uses the go mod download -json command mentioned as a preflight check for module verification. More or less, if the repo passes the go module commands understanding of a module then Athens will serve it. In more verboten terms: - a module version, pseudo version, or +incompatible must be able to be formulated - that module (and it's dependencies) must produce a valid checksum The checksum of modules just has to do with the current .mod and all files + recursively for each dependency. So, as the author pointed out you can have lots of space for arbitrary files by design so long as you have a basic go program. reply IshKebab 2 hours agoprev [–] Maybe I'm being stupid but what exactly is the issue here? It's probably a bit wasteful of the proxy to cache non-Go repos, but even if it didn't you could make it store arbitrary data just by having it cache a Go repo surely? Sounds like a complete non-issue unless I've missed something. reply gnfargbl 1 hour agoparentI don't think you've missed anything. The news here appears to be that a unsecured public proxy is willing to proxy things and make them available to the public in an unsecured fashion. The article does make the point that some monitored networks might trust golang proxy URLs more than arbitrary web URLs and that this could be used for bypassing reputation filters etc -- but there are already several ways to do that, and this one doesn't seem particularly special. reply arandomusername 2 hours agoparentprev [–] you're right. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author found non-Go repositories in Go's checksum database, raising concerns about its integrity and security.",
      "They demonstrated the ability to load arbitrary data into the Go public proxy using GitHub repositories, highlighting potential vulnerabilities despite file size constraints.",
      "The author suggests that while the Go infrastructure is not severely vulnerable, it can be abused and improved, calling for further investigation into the inclusion of non-Go projects in the checksum database."
    ],
    "commentSummary": [
      "Online services like Twitter, Telegram, and GitHub are often exploited for malicious activities, presenting challenges due to their dual-use nature, which complicates efforts to block abuse without affecting legitimate use.",
      "Privacy concerns arise from monitoring user data to detect misuse, with discussions extending to platforms like Gmail, Google Drive, HuggingFace, and GitHub for file distribution.",
      "The CUE team collaborated with the Go team on module systems, choosing OCI registries over Go's proxy due to security concerns, and discussed the minor issue of potential misuse of Go module proxies to store arbitrary data."
    ],
    "points": 231,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1716641400
  },
  {
    "id": 40473198,
    "title": "Efficient Fine-Tuning of Mistral Models with LoRA Training Paradigm",
    "originLink": "https://github.com/mistralai/mistral-finetune",
    "originBody": "Mistral-finetune mistral-finetune is a light-weight codebase that enables memory-efficient and performant finetuning of Mistral's models. It is based on LoRA, a training paradigm where most weights are frozen and only 1-2% additional weights in the form of low-rank matrix perturbations are trained. For maximum efficiency it is recommended to use a A100 or H100 GPU. The codebase is optimized for multi-GPU-single-node training setups, but for smaller models, such as the 7B a single GPU suffices. Note The goal of this repository is to provide a simple, guided entrypoint to finetune Mistral models. As such, it is fairly opinionated (especially around data formatting) and does not aim at being exhaustive across multiple model architecture or hardware types. For more generic approaches, you can check out some other great projects like torchtune. Installation To get started with Mistral LoRA fine-tuning, follow these steps: Clone this repository: cd $HOME && git clone https://github.com/mistralai/mistral-finetune.git Install all required dependencies: cd mistral-finetune pip install -r requirements.txt Model download We recommend fine-tuning one of the official Mistral models which you can download here: Model Link Checksum 7B Base V3 7B Base 0663b293810d7571dad25dae2f2a5806 7B Instruct v3 7B Instruct v3 80b71fcb6416085bcb4efad86dfb4d52 8x7B Base V1 8x7B Base (HF link) 8x7B Instruct V1 8x7B Instruct 8e2d3930145dc43d3084396f49d38a3f 8x22 Instruct V3 8x22 Instruct 471a02a6902706a2f1e44a693813855b 8x22B Base V3 8x22B Base a2fa75117174f87d1197e3a4eb50371a Important Notice: For 8x7B Base V1 and 8x7B Instruct V1, it is necessary to use our v3 tokenizer and extend the vocabulary size to 32768 prior to fine-tuning. For detailed instructions on this process, please refer to the \"Model extension\" section. E.g., to download the 7B-base model you can run the following command: mkdir -p ~/${HOME}/mistral_models cd ${HOME} && wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar tar -xf mistral-7B-v0.3.tar -C mistral_models Make sure to modify your training script and add the path to the downloaded folder as model_id_or_path. E.g., modify example/7B.yaml to include the absolute path to $HOME/mistral_models/7B: model_id_or_path: \"/Users/johndoe/mistral_models/7B\" Prepare dataset To ensure effective training, mistral-finetune has strict requirements for how the training data has to be formatted. All data files must be stored in jsonl format files. You can build two types of data files: Pretrain: Pretrain data corresponds to plain text data stored in the \"text\" key. E.g: {\"text\": \"Text contained in document n°1\"} {\"text\": \"Text contained in document n°2\"} Instruct: Currently two different types of instruction following data are supported: Instruct: conversational data stored in the \"messages\" key in the form of a list. Each list item is a dictionary containing the \"content\" and \"role\" keys. \"role\" is a string being one of \"user\", \"assistant\" or \"system_prompt\". The loss will only be computed if \"role\" == \"assistant\". E.g.: { \"messages\": [ { \"role\": \"user\", \"content\": \"User interaction n°1 contained in document n°1\" }, { \"role\": \"assistant\", \"content\": \"Bot interaction n°1 contained in document n°1\" }, { \"role\": \"user\", \"content\": \"User interaction n°2 contained in document n°1\" }, { \"role\": \"assistant\", \"content\": \"Bot interaction n°2 contained in document n°1\" } ] } { \"messages\": [ { \"role\": \"user\", \"content\": \"User interaction n°1 contained in document n°2\" }, { \"role\": \"assistant\", \"content\": \"Bot interaction n°1 contained in document n°2\" }, { \"role\": \"user\", \"content\": \"User interaction n°2 contained in document n°2\" }, { \"role\": \"assistant\", \"content\": \"Bot interaction n°2 contained in document n°2\", \"weight\": 0, # don't train on n°2 }, { \"role\": \"user\", \"content\": \"User interaction n°3 contained in document n°2\" }, { \"role\": \"assistant\", \"content\": \"Bot interaction n°3 contained in document n°2\" } ] } Function calling: conversational data stored in the \"messages\" key in the form of a list. Each list item is a dictionary containing the \"role\" and \"content\" or \"tool_calls\" keys. \"role\" is a string being one of \"user\", \"assistant\", \"system_prompt\", or \"tool\". The loss will only be computed if \"role\" == \"assistant\". Note: In function calling the \"id\" of \"tool_calls\" and the \"tool_call_id\" are randomly generated strings of exactly 9 chars. We recommend to generate this automatically in a data preparation script as is done here. E.g.: { \"messages\": [ { \"role\": \"system\", \"content\": \"You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed\" }, { \"role\": \"user\", \"content\": \"Can you help me generate an anagram of the word \\\"listen\\\"?\" }, { \"role\": \"assistant\", \"tool_calls\": [ { \"id\": \"TX92Jm8Zi\", \"type\": \"function\", \"function\": { \"name\": \"generate_anagram\", \"arguments\": \"{\\\"word\\\": \\\"listen\\\"}\" } } ] }, { \"role\": \"tool\", \"content\": \"{\\\"anagram\\\": \\\"silent\\\"}\", \"tool_call_id\": \"TX92Jm8Zi\" }, { \"role\": \"assistant\", \"content\": \"The anagram of the word \\\"listen\\\" is \\\"silent\\\".\" }, { \"role\": \"user\", \"content\": \"That's amazing! Can you generate an anagram for the word \\\"race\\\"?\" }, { \"role\": \"assistant\", \"tool_calls\": [ { \"id\": \"3XhQnxLsT\", \"type\": \"function\", \"function\": { \"name\": \"generate_anagram\", \"arguments\": \"{\\\"word\\\": \\\"race\\\"}\" } } ] } ], \"tools\": [ { \"type\": \"function\", \"function\": { \"name\": \"generate_anagram\", \"description\": \"Generate an anagram of a given word\", \"parameters\": { \"type\": \"object\", \"properties\": { \"word\": { \"type\": \"string\", \"description\": \"The word to generate an anagram of\" } }, \"required\": [ \"word\" ] } } } ] } Verify dataset Before starting a training run you should verify that your dataset is correctly formatted and get an estimation of the training time. You can do so by using the ./utils/validate_data script. Note that this step is crucial to ensure that the data is correctly formatted. Instruction following Let's go over a simple example to train a model in instruction following: Load a chunk of Ultachat_200k Create the data folder and navigate to the folder. cd $HOME && mkdir -p data && cd $HOME/data Load the data into a Pandas Dataframe. Note: Make sure to have pandas and pyarrow installed (pip install pandas pyarrow). import pandas as pd df = pd.read_parquet('https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/resolve/main/data/test_gen-00000-of-00001-3d4cd8309148a71f.parquet') Split into train and eval df_train=df.sample(frac=0.95,random_state=200) df_eval=df.drop(df_train.index) Save data to jsonl df_train.to_json(\"ultrachat_chunk_train.jsonl\", orient=\"records\", lines=True) df_eval.to_json(\"ultrachat_chunk_eval.jsonl\", orient=\"records\", lines=True) Modify your training yaml to include the ultrachat dataset and verify the yaml Modify example/7B.yaml to include the absolute path to $HOME/data/ultrachat_chunk_train.jsonl as well as a dataset mixing weight for training and $HOME/data/ultrachat_chunk_eval.jsonl for eval, e.g. data: instruct_data: \"/Users/johndoe/data/ultrachat_chunk_train.jsonl\" eval_instruct_data: \"/Users/johndoe/data/ultrachat_chunk_eval.jsonl\" Now you can verify your training yaml to make sure the data is correctly formatted and to get an estimate of your training time. cd $HOME/mistral-finetune python -m utils.validate_data --train_yaml example/7B.yaml Upon completion you should see an error report with many of the following errors: The data in line 1412 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formated.Expected last role to be one of: [assistant] but got user The data in line 1413 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formated.Expected last role to be one of: [assistant] but got user The data in line 1414 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formated.Expected last role to be one of: [assistant] but got user The data in line 1415 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formated.Expected last role to be one of: [assistant] but got user Many conversations seem to end with the 'user' role which is unnecessary as we only train on 'assistant' messages and thus would unnecessarily process data. You can make use of ./utils/reformat_data.py to correct the data: cd $HOME/mistral-finetune python -m utils.reformat_data $HOME/data/ultrachat_chunk_train.jsonl python -m utils.reformat_data $HOME/data/ultrachat_chunk_eval.jsonl You should see that a couple of samples will be skipped. Potentially change number of training steps Upon correction of the dataset, run the script again cd $HOME/mistral-finetune python -m utils.validate_data --train_yaml example/7B.yaml You should get a summary of the data input and training parameters: Train States -------------------- { \"expected\": { \"eta\": \"00:52:44\", \"data_tokens\": 25169147, \"train_tokens\": 131072000, \"epochs\": \"5.21\", \"max_steps\": 500, \"data_tokens_per_dataset\": { \"/Users/johndoe/data/ultrachat_chunk_train.jsonl\": \"25169147.0\" }, \"train_tokens_per_dataset\": { \"/Users/johndoe/data/ultrachat_chunk_train.jsonl\": \"131072000.0\" }, \"epochs_per_dataset\": { \"/Users/johndoe/data/ultrachat_chunk_train.jsonl\": \"5.2\" } }, } Having max_steps set to 500 would lead to iterating through the dataset roughly 5 times which is reasonable, but might be a bit too much. A recommended setting is shown below which would only take 30min on a 8xH100 cluster. Function calling Next let's go over a more advanced use case to fine-tune a model on function calling. Function calling requires the data to be in the format as explained above. Let's go over an example. Load a chat-formatted version of the Glaive function calling dataset Create the data folder and navigate to the folder. cd $HOME && mkdir -p data && cd $HOME/data Load the data into a Pandas Dataframe. Note: Make sure to have pandas and pyarrow installed (pip install pandas pyarrow). import pandas as pd df = pd.read_parquet('https://huggingface.co/datasets/Locutusque/function-calling-chatml/resolve/main/data/train-00000-of-00001-f0b56c6983b4a78f.parquet') Split into train and eval df_train=df.sample(frac=0.95,random_state=200) df_eval=df.drop(df_train.index) Save data to jsonl df_train.to_json(\"glaive_train.jsonl\", orient=\"records\", lines=True) df_eval.to_json(\"glaive_eval.jsonl\", orient=\"records\", lines=True) Reformat dataset As one can see the dataset does not follow the required function calling format, so it will need to be reformatted. Among other things \"from\" should be renamed to \"user\" and superfluous \"\" characters should be removed. For this dataset you can make use of ./utils/reformat_data_glaive.py: cd $HOME/mistral-finetune python -m utils.reformat_data_glaive $HOME/data/glaive_train.jsonl python -m utils.reformat_data_glaive $HOME/data/glaive_eval.jsonl Running this command will make sure that most samples are in the correct format. Note: It is impossible to write reformatting scripts that work for all kinds of datasets. If you have datasets that don't yet follow the required format above, you will most probably have to create a reformatting script yourself (mistral-chat or chat-gpt is your best friend here!). Validate dataset You can now validate the dataset by setting data.instruct_data and data.eval_instruct_data to $HOME/data/glaive_train.jsonl and $HOME/data/glaive_eval.jsonl in example/7B.yaml respectively. The reformatted datasets still has some errors which can be removed with --create_corrected. For this, make sure to add --create_corrected as follows: cd $HOME/mistral-finetune python -m utils.validate_data --train_yaml example/7B.yaml --create_corrected Running this command will show a couple of errors and save two new datasets $HOME/data/glaive_train.jsonl.corrected and $HOME/data/glaive_eval.jsonl.corrected. Make sure to use these two dataset in example/7B.yaml and run the command again. Now the dataset should be correctly formatted! Start training Having followed the dataset verification section, we can now start training. For faster training, we recommend setting max_steps to only 300. Make sure to define run_dir to your experiment folder and optionally set wandb_project to a Weights & Biases project for logging`, e.g.: max_steps: 300 run_dir: \"/Users/johndoe/ultra_chat_test\" wandb.project: ultra_chat Optionally you can also set wandb Save the training configuration and start training! Make sure to set --nproc-per-node to the number of available GPUs. cd $HOME/mistral-finetune torchrun --nproc-per-node 8 --master_port $RANDOM -m train example/7B.yaml Training on ultra-chat should take around 30min on a 8xH100 node and the resulting weights should give an MT Bench score around 6.3. Training on glaive should take around 1h on a 8xH100 node and the resulting weights should work nicely for function calling. Customizing training configuration The example mistral-finetune/examples/7B defines reasonable parameters for learning rate, weight decay, etc... but you are advised to customize these settings for your use case. Generally, a training configuration should fill the following parameters: model_id_or_path defines the model to start training from. This can be a path to a pre-trained model or a local model directory. run_dir defines the directory where training checkpoints and metrics are stored. seq_len defines the sequence length for training. This is the maximum length of input sequences the model will process. Samples are packed to reach a length of seq_len for maximum training efficiency. batch_size defines the number of training examples used per GPU. Note: The overall effective batch_size (in tokens) across all GPUs equals num_gpus x batch_size x seq_len. max_steps defines the maximum number of training steps. This is the total number of iterations the training process will run. It can be adjusted based on the specific needs of your training scenario. Total number of tokens seen during training is max_steps x num_gpus x batch_size x seq_len. optim.lr defines the learning rate. This is the initial learning rate for the optimizer. optim.weight_decay defines weight decay. Weight decay is a regularization technique used to prevent overfitting by penalizing large weights. We recommend leaving it at 0.1. optim.pct_start defines the percentage of the total training steps used for the learning rate warm-up phase before it starts to decrease. It corresponds to pct_start of PyTorch's OneCycleLR. lora.rank defines the size of the LoRA (Low-Rank Adaptation) adapters. We recommend 64 or less, which adjusts the rank of the low-rank decomposition used in LoRA. seed defines the random seed for initialization and data shuffling/sampling. Setting a seed ensures reproducibility of results. log_freq defines the logging frequency. This specifies how often (in steps) to log training metrics. data.instruct_data is the path to the instruction data used for training. This field has to be filled with one or multiple data sources in the format as explained above. Each data source should either be a path to jsonl file of a path to a directory containing jsonl files followed by a weighting to define the importance of this dataset: :. E.g.: data.instruct_data: \"/path/to/data1.jsonl:5.,/path/to/data2.jsonl:1.,/path/to/dir_of_jsonls:1.\" data.data is an optional path to additional pretraining data in the format as explained above. Note that this field can be left blank. data.eval_instruct_data is an optional path to evaluation instruction data to run cross-validation at every eval_freq steps. Cross-validation metrics are displayed as loss and perplexity. eval_freq defines how often (in steps) to evaluate the model. This specifies the interval at which the model is evaluated on the validation set. no_eval is a flag to enable or disable intermediate evaluation. Setting it to False enables periodic evaluation during training. ckpt_freq defines how often (in steps) to save checkpoints. This specifies the interval at which the model's state is saved. ckpt_only_lora defines whether to only save the trained LoRA checkpoints or whether the trained LoRA should directly be merged into the base model and saved. Note: When setting ckpt_only_lora=False make sure that you have enough CPU and GPU memory to save the full model on a single process (this is usually only possible for the 7B model). wandb.key is used to pass your Weights & Biases (wandb) API key for logging. This allows you to log training metrics to the wandb dashboard. wandb.project defines the wandb project name. This is where the training run will be logged in the wandb interface. Inference Once your model is trained, you should try it out in inference. We recommend using mistral-inference. Make sure to have mistral_inference correctly installed: pip install mistral_inference Assuming your lora.safetensors is saved under $HOME/ultra_chat_test/checkpoints/checkpoint_000300/consolidated/lora.safetensors, you can chat with the model using mistral_inference, e.g.: mistral-chat /mnt/slow/runs/patrick/mistral-finetune/7B/ --max_tokens 256 --temperature 1.0 --instruct --lora_path $HOME/ultra_chat_test/checkpoints/checkpoint_000300/consolidated/lora.safetensors Model extension Important: Note that one can only fine-tune mistral models that are compatible with the v3 tokenizer which entails that the models have a vocabulary size of 32768 - not 32000. One can however easily extend older version of vocabulary size 32000 to have a vocabulary size of 32768 by using: python -m utils.extend_model_vocab --original_model_ckpt /folder/to/old/model --extended_model_ckpt /folder/to/extended/model Once the extension has worked, one can fine-tune using the newly created model checkpoint in /folder/to/extended/model. FAQ: What's the best practice of fine-tuning MoEs? We see a higher degree of performance variance in when fine-tuning MoE models. It's not unusual to find that fine-tuning MoEs models with different seeds can lead to a high variance in performance. We did not observe such a high variance with dense models. Therefore, we suggest running multiple instances of the same fine-tuning process on MoEs models and selecting the one that performs best. How can I determine the number of tokens used during the model training process? You can use the following script to find out: https://github.com/mistralai/mistral-finetune/blob/main/utils/validate_data.py. This script accepts a .yaml training file as input and returns the number of tokens the model is being trained on. What should I do if I encounter a CUDA out-of-memory error? One possible solution is to reduce the batch size per GPU. The batch size is equal to seq_len x batch_size. Try setting batch_size to 1 and reduce seq_len. You can define the batch_size and seq_len in the .yaml file.",
    "commentLink": "https://news.ycombinator.com/item?id=40473198",
    "commentBody": "Mistral Fine-Tune (github.com/mistralai)181 points by alexmolas 11 hours agohidepastfavorite58 comments hubraumhugo 9 hours agoIs finetuning still worth it as models advance so fast? Any real-world use cases? For example, Bloomberg trained a GPT-3.5 class LLM on their financial data last year and soon after GPT-4-8k outperformed it on nearly all finance tasks. We ended up focusing on having high-quality eval data and an architecture that makes switching to new models easy. reply probably_wrong 7 hours agoparentYes. I have non-English human data annotated in a format that was designed for a very specific health-related study. LLMs haver never seen these annotations, non-English LLMs are not a top priority for companies, and we can only use offline-first ones for data privacy reasons. In this scenario fine-tuning a general purpose LM works wonders. reply newswasboring 7 hours agorootparentWhat is your strategy for this? Do you finetune when a new flagship model is made available? You said local first, so I'm guessing you might have finetuned llama. But there are llama fine-tunes available which have better performance than the base model. How do you choose? reply probably_wrong 5 hours agorootparentOur strategy is to take a well-known, battle-tested model as a base, train it, and then hopefully one day release the fine-tuned model on HuggingFace. Other than that, fine-tunes don't really matter for us because not many people are rushing to beat the top models on (say) Georgian POS tagging or Urdu sentiment analysis. As long as the model can turn language into a reasonable vector, we're happy with it. reply sp332 9 hours agoparentprevFine tuning can be useful if you need to generate lots of output in a particular format. You can fine-tune on formatted messages, and then the model will generate that automatically. That could save a bunch of tokens explaining the output format in every prompt. reply NeutralForest 8 hours agorootparentYou can use structured generation instead of fiddling with the prompt, which is unreliable. https://github.com/outlines-dev/outlines reply codetrotter 8 hours agorootparentDoes this Python package control the LLMs using something other than text? Or is the end result still that that Python package wraps your prompt with additional text containing additional instructions that become part of the prompt itself? reply tikhonj 8 hours agorootparentLooks like it actually changes how you do token generation to conform to a given context-free grammar. It's a way to structure how you sample from the model rather than a tweak to the prompt, so it's more efficient and guarantees that the output matches the formal grammar. There's a reference to the paper that describes the method at the bottom of the README: https://arxiv.org/pdf/2307.09702 reply sp332 8 hours agorootparentprevThe output of the LLM is not just one token, but a statistical distribution across all possible output tokens. The tool you use to generate output will sample from this distribution with various techniques, and you can put constraints on it like not being too repetitive. Some of them support getting very specific about the allowed output format, e.g. https://github.com/ggerganov/llama.cpp/blob/master/grammars/... So even if the LLM says that an invalid token is the most likely next token, the tool will never select it for output. It will only sample from valid tokens. reply progbits 8 hours agorootparentprevNo it limits what tokens the LLM can output. The output is guaranteed to follow the schema. reply kgeist 8 hours agoparentprev>Is finetuning still worth it as models advance so fast? Any real-world use cases? Internal corporate data GPT4 was never exposed to? reply simonw 8 hours agorootparentI still haven't seen really convincing evidence that fine tuning is useful for the internal corporate data use-case (as opposed to RAG which seems to work really well.) reply ankit219 8 hours agorootparentprevFinetuning would never add any new knowledge. For internal corporate data, use RAG or train a model from scratch. Finetuning would not help anyone answer a question from that data. reply qeternity 6 hours agorootparentThis isn’t true at all. Fine tune with a single sample and see what happens to your model. reply ankit219 6 hours agorootparentWill share a longer post on this once I finish it. I have tested this multiple times, on bigger models, on custom smaller models, and it does not work. In a strict sense, finetuning can add new knowledge but for that you need millions of tokens and multiple runs without using LoRA or Peft. For practical purposes, it does not. reply F-Lexx 6 hours agorootparentBring on the new post and more details, this stuff is interesting. reply zwaps 5 hours agorootparentprevPEFT is not for adding knowledge, that's obvious reply Exuma 7 hours agorootparentprevHow does one do RAG? I see it mentioned like 20 times reply behnamoh 6 hours agorootparentThis article reviews some of the most advanced RAG methods: https://medium.com/@krtarunsingh/advanced-rag-techniques-unl... (not mine) reply meiraleal 6 hours agorootparentprevyou supply to the LLM the subset of data that you need for a specific prompt. That's RAG. reply hectormalot 6 hours agoparentprevMy sense is that finetuning might still have a role if your use is high-volume and with a narrow/specific goal. For example, we have GPT based summaries of our customer contact calls. It's fairly high volume (a typical bank will handle millions of calls a year). We're considering finetuning for 2 reasons: (1) the current system prompt with instructions is getting quite large* and we see more challenges for GPT to stick to the instructions. We could finetune with our historic summaries and a simplified prompt to slightly improve the performance of the summaries / lower token count (performance). The idea would be to then continue improving our system prompt again from that new starting point. Exploration still to be done though. (2) we might be able to finetune a much smaller model to do what the larger model is currently doing (cost, sustainability) * The larger instruction prompt is because there are lots of specific needs for the summaries (e.g. how to write the summary, what to exclude (health, etc.), what to include (actions taken), and we e.g. give examples of good vs bad summaries to improve performance. reply barrell 9 hours agoparentprevWhen it comes to traditional NLP related tasks, LLMs are far below dedicated NLP pipelines like POS tagging and feature tagging. However, fine tuning bridges the gap quite a bit between the two. It's a narrow domain, but so is most of programming. I think if you're just training a general purpose LLM to be more inclined towards your data -- no, fine tuning is probably not very relevant. But if you're trying to solve a very specific yet fuzzy problem, and LLMs can get you _part_ of the way there, fine tuning is likely your best bet. reply NhanH 8 hours agorootparentCan you share a bit more on which tasks are you discussing about? reply anon373839 8 hours agorootparentNot the OP, but you can take a look at https://spacy.io/usage/spacy-101 to get a sense of what traditional NLP tasks look like. These things can be done much faster than LLMs with appropriate tooling (such as spaCy) and they don’t risk hallucination. reply maaaaattttt 7 hours agorootparentI’ve tried PII anonymization with standard NLP approaches and LLMs have been (way) better at this task in my experience. reply threeseed 7 hours agorootparentprevAlso you can do it at scale with SparkNLP: https://sparknlp.org Many of the use cases I've seen for LLMs would actually be better with NLP. reply meandmycode 7 hours agorootparentWhich kinds of use cases? reply barrell 5 hours agorootparentprevIn this comment I was referring to POS tagging and feature extraction. Another use case for fine tuning we have as well is to reduce a 5-shot prompt that we have to run hundreds of times per request down to a “0-shot” (heavy emphasis on the double quotes. Run five shot on gpt-4o a couple thousand times, then fine tune on cohere’s command-r or haiku or llama3 8b or whichever small but mighty llm. You can reduce costs by 99%, or somewhere in that ballpark, without really sacrificing quality on 99% of the queries. reply uyzstvqs 5 hours agoparentprevI'd summarize it as prompting for input, finetuning for output. RAG is a far better option for making a model work with your data/information. Finetuning is better for making it output in a language, style, data format, programming language, etc. reply subroutine 9 hours agoparentprevFunction calling might be one reason. If your app has a lot of custom functions for interacting with tools, fine tuning may be preferred over using context tokens. reply AlexeyBrin 7 hours agorootparentCan you recommend a tutorial/document about fine tuning a small model for selecting the correct functions ? Would be great to have something local that can replace OpenAI functions/tools API. reply Delmololo 6 hours agoparentprevMy startup has data the Internet doesn't have. I want it to be fixing knowledge based on my data and fine-tune it to be adjusted to my use case already. I also want a.feedbackloop and can't start to send more and more context with the payload just because my feedback loop adds / finetunes data reply CuriouslyC 7 hours agoparentprevFine tuning is generally not worth it over RAG with newer models unless you have a niche data set that's very different from the pretraining data. LoRAs/control vectors are also generally better as they don't induce forgetting or hallucinations, and they're usually \"good enough\" with RAG. reply keeptrying 9 hours agoparentprevI think their training methodology was suspect. Also I think there’s a bunch of important techniques that just are t shared. reply BaculumMeumEst 9 hours agoparentprevI use a programming language that isn’t publicly available and it would be useful to have a model understand it. AI isn’t something I’m super knowledgeable about but fine-tuning sounds like a good way to accomplish that. reply Tenoke 8 hours agoparentprevQuality suffers at very large contexts, and you might want to use your large context for something else (e.g. a long current conversation or a lot of recent data). reply fulafel 8 hours agoparentprevFinance is a very mainstream, broad, english-language dominated field. Most people don't work in a field that is so generalist-LLM friendly. reply ewalk153 6 hours agoparentprevThis seems to be the narrative Microsoft was pushing at the Build conference this week. reply behnamoh 6 hours agorootparentWeird, because they make one of the best small language models (phi series) which is great for finetuning. reply stuckinhell 6 hours agoparentprevNo fine tuning seems pointless right now. you can't even fine tune the frontier models. reply anshumankmr 9 hours agoprev>ps, but for smaller models, such as the 7B a single GPU suffices If I wanna do this, what GPU would I need?? I have a 3060 TI (laptop one) and i9 with 16 gigs of ram. I don't have AWS quota, or quota in GCP either. have heard of paperspace, but I want to quickly get started with fine tuning Mistral, cause we are planning to use some of their models for an engagement I am working on. reply semi-extrinsic 8 hours agoparentUnless you have zero budget, I would strongly suggest going up to a \"gaming\" desktop computer. A gaming GPU can put out 300 W of heat no problem. If your laptop GPU did the same it would melt, it's going to be limited to around 100 W. And heat dissipation is directly proportionaly to speed. Plus in a desktop, you have the ability to upgrade to faster GPU or even go for multiple GPUs. Caveat: such machines, especially multi-GPU, are loud and produce enough heat to warm up a whole room quickly. And cloud will likely be cheaper if you believe you will not spend more than 10% of the next couple of years running the GPU at full blast. reply washadjeffmad 7 hours agorootparent>heat dissipation is directly proportionaly to speed. Power limiting Nvidia GPUs up to around 60% of max TDP can offer a substantial reduction to heat and noise with minimal performance loss, but agreed on everything else. reply rpastuszak 7 hours agorootparentprevI'm using an M1 MBA and was considering getting an eGPU instead of buying a PC, would you advise against it? What variables would you consider? reply addandsubtract 6 hours agorootparentThere are no Nvidia drivers for Mac. reply qayxc 8 hours agoparentprevCheck out this site: https://www.hardware-corner.net/llm-database/Mistral/ They list hardware requirements by model and you can select your VRAM and system memory to filter available models. reply elpocko 6 hours agorootparentUnless I'm missing something - this is for inference only, using quantized models. Fine-tuning usually has much higher requirements. reply aunty_helen 4 hours agoparentprevYou can get a hetzner gpu box for 184€ a month. My company has been fine tuning mistal and llama3 on the rtx4000 these have. It is slightly limited in ram at only 20gb but we found dropping the quantisation has helped with larger input token counts. They also now do hourly rental. reply Narciss 7 hours agoparentprevTry out openpipe. I am working with it at my current company and have seen some good results. reply Hugsun 9 hours agoprevIt will be very interesting to see which tools become the de-facto for each of the common LLM use cases. The landscape is so fractured, I feel like I haven't even heard of most tools. I encountered Microsoft's olive the other day and it was completely new to me. reply joaquincabezas 9 hours agoprevNow that many open source LLMs are already “useful” it is key to ease the development _around_ them, in particular in a way where users-developers can leverage on “private data” (really, data that is not present in the pre-training of these models). Even if in the repo it is stated that it is optimized for big models (needing A100/H100), I still feel this can help with smaller models more than large ones. We can extend “If you build it, they will come” with “If you provide the tools, the will build” reply jsemrau 9 hours agoparent> If you provide the tools, the will build Only makes sense if the incentive of learning that tech provides expectations of future benefits. reply joaquincabezas 9 hours agorootparentthere is also the joy of experimenting a “new and revolutionary” technology. Although yes, with big tech and big salaries there is an economic incentive reply skerit 9 hours agoprevThe \"weights\" bit is interesting. HuggingFace's SFTTrainer allows you to train on completions only if you want, but while that makes sense for us humans, LLMs are generally better of learning to predict the entire input. This allows you to get the best of both worlds. reply DeathArrow 10 hours agoprevCan this be optimized so training of larger variants is possible with a pair of 3090 or 4090? reply qayxc 8 hours agoparentThis would involve a lot of effort, but seems to be doable. Here's a starting point that discusses some options: https://huggingface.co/blog/trl-peft reply sciencesama 9 hours agoprev [–] How can i train my Whats app chat model ?? reply qeternity 6 hours agoparent [–] What does this mean? How can you train a model on your own WhatsApp messages? To what end? To write like you? For RAG Q&A? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mistral-finetune is a lightweight codebase designed for memory-efficient and high-performance fine-tuning of Mistral's models using the LoRA (Low-Rank Adaptation) training paradigm.",
      "It supports both multi-GPU and single-GPU setups, with optimal performance on A100 or H100 GPUs, and includes comprehensive instructions for data formatting, installation, and dataset preparation.",
      "The codebase offers tools for reformatting and validating datasets, detailed training setup and management steps, and guidelines for using mistral-inference for model interaction and troubleshooting common issues like CUDA memory errors."
    ],
    "commentSummary": [
      "The discussion focuses on the relevance and strategies of fine-tuning language models (LMs) amidst rapid advancements, emphasizing its value for niche applications but noting challenges due to model obsolescence.",
      "Retrieval-Augmented Generation (RAG) is highlighted as a practical alternative for integrating specific data, offering a different approach to fine-tuning.",
      "Recommendations include using gaming desktops over laptops for running models, considering cloud services and eGPUs, and utilizing tools like HuggingFace's SFTTrainer for optimizing model performance."
    ],
    "points": 181,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1716620987
  },
  {
    "id": 40474202,
    "title": "Google's Algorithm Update Hits Independent Publishers, Boosts Big Sites and Reddit",
    "originLink": "https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet",
    "originBody": "Google just updated its algorithm. The Internet will never be the same 9 hours ago By Thomas Germain Share Serenity Strull/BBC/Getty Images (Credit: Serenity Strull/BBC/Getty Images) Over the last two years, a series of updates to Google Search amount to a dramatic upheaval to the Internet's most powerful tool, complete with an unprecedented AI feature. Will Google save the web, or destroy it? If you've ever typed \"air purifier reviews\" into Google, you were probably looking for the kind of content you'll find on HouseFresh.com. The site was started in 2020 by Gisele Navarro and her husband, based on a decade of experience writing about indoor air quality products. They filled their basement with purifiers, running rigorous science-based tests and writing articles to help consumers sort through marketing hype. HouseFresh is an example of what has been a flourishing industry of independent publishers producing exactly the sort of original content Google says it wants to promote. And indeed, soon after the website's launch, the tech giant started showing HouseFresh at the top of search results. The website grew into a thriving business with 15 full-time employees. Navarro had big plans for the future. Then, in September 2023, Google made one in a series of major updates to the algorithm that runs its search engine. \"It decimated us,\" Navarro says. \"Suddenly the search terms that used to bring up HouseFresh were sending people to big lifestyle magazines that clearly don't even test the products. The articles are full of information that I know is wrong.\" The second Google algorithm update came in March, and it was even more punishing. HouseFresh's thousands of daily visitors dwindled to just hundreds. \"We just got absolutely crushed,\" Navarro says. Over the last few weeks, HouseFresh had to lay off most of its team. If nothing changes, she says, the website is doomed. A spokesperson for Google tells the BBC that the company only launches changes to Search after rigorous testing confirms that the shift will be helpful for users, and that the company gives website owners help, resources and opportunities for feedback on their Search rankings. Google stands firm in its position that the changes will be a benefit to the web, and changes to the Search algorithm are just the start. Last week, Google CEO Sundar Pichai stood in front of a crowd at the company's annual developer conference and announced one of the most significant moves in the search engine's history. Going forward, Pichai said, Google Search would provide its own AI-generated answers to many of your questions, a feature called \"AI Overviews\" that's already rolled out to users in the United States. \"The result is a product that does the work for you,\" Pichai said. \"Google Search is generative AI at the scale of human curiosity.\" Google's work is about to have a profound impact on what many of us see when we go online AI Overviews are just one of a slew of dramatic changes Google has made to its core product over the past two years. The company says its recent effort to revamp Search will usher in an exciting new era of technology and help solve many of the issues plaguing the web. But critics say the opposite may be true. As Google retools its algorithms and uses AI to transition from a search engine to a search and answer engine, some worry the result could be no less than an extinction-level event for the businesses that make much of your favourite content. One thing is certain: Google's work is about to have a profound impact on what many of us see when we go online. Over the last two years, updates meant to make Search more “helpful” devasted many website owners who say they follow Google’s best practices. (Source: SEMrush) (Credit: BBC) The changes came about because Google recognises the web has a problem. You've seen it yourself, if you've ever used a search engine. The Internet is dominated by a school of website building known as \"search engine optimisation\", or SEO, techniques that are meant to tune articles and web pages for better recognition from Google Search. Google even provides SEO tips, tools and advice for website owners. For millions of businesses that rely on the mechanisations of the Search machine, SEO can be an unavoidable game. The trouble is SEO can be abused. Enterprising website owners realised you can sometimes make more money by making content designed to please Google's algorithms, rather than the human beings it's ostensibly designed to serve. Google's efforts to address this issue aren't always successful. If you've ever been frustrated by what comes up when you search for something like \"Best Sneakers for Women\", you know the issue. Often, the results for popular search terms are crowded with websites that contain very little useful information, but tonnes of ads and links to retailers that earn publishers a share of profits. What's often lost is what you're probably looking for when you open up Google: information from people who are knowledgeable and passionate about their topic. Google's war on spammy Search results has ramped up. In 2022, the company issued a \"Helpful Content Update\" to its algorithm meant to weed out content created solely for the purpose of ranking higher on Search. Google issued a subsequent update in September, 2023, and a third algorithm tweak in March of this year. Google says the result is \"45% less low-quality, unoriginal content in search results\". It could be viewed as a wild success. \"Our recent updates aim to connect people with content that is helpful, satisfying and original, from a diverse range of sites across the web,\" a Google spokesperson tells the BBC. \"As we work to improve Search, we're continuing to focus on sending valuable traffic to sites and supporting a healthy, open web.\" Serenity Strull/BBC/Getty Images For better or worse, Google Search dictates the shape of the web. Some online publishers say the company wields that power irresponsibly (Credit: Serenity Strull/BBC/Getty Images) But the updates had some surprising consequences as well. For example, data from the analytics tool SEMrush suggests that the website for New York Magazine lost 32% of its Google Search traffic in the past six months, while GQ.com shrank 26%. The data indicates Urban Dictionary, a wildly popular crowdsourced dictionary of English language slang, dropped some 18 million page views, amounting to more than half its Search traffic. OprahDaily.com was down nearly 58%. (SEMrush is an industry-standard tool, but its numbers are estimates and this data only measures traffic coming from Google Search, specifically.) A New York Magazine spokesperson said these findings were incomplete and didn't reflect the company's internal analysis. Representatives for GQ, Oprah Daily and Urban Dictionary did not respond to requests for comment by the time this article was published. However, experts and more than half a dozen media executives and websites owners told the BBC the broad trends in the data are all too real. The increase in traffic Reddit is seeing is unprecedented on the Internet – Lily Ray In place of these sites, there's one platform you'll be seeing much, much more of: Reddit. According to SEMrush, Reddit saw a surge that amounted to a 126% growth in traffic from Google Search. The company is already feeling the benefit. Reddit just announced its first quarterly earnings since becoming a publicly traded company in March 2024. Its revenue totals $243m (£191m), up an eye-watering 48% from the year prior. \"The increase in traffic Reddit is seeing is unprecedented on the Internet,\" says Lily Ray, vice president of SEO strategy and research at the marketing agency Amsive, and a celebrity in the world of SEO. \"Cooking content, adult content, video games, gardening, fashion, everything is all just Reddit.\" A representative for Reddit declined to comment. Reddit isn't the only winner after Google's recent algorithm updates. SEMrush data shows that other user-generated sites such as Quora and Instagram saw similarly astronomical rises, and there were impressive spikes at LinkedIn and Wikipedia as well. In one sense, Google was just following a trend. Over the past few years, swaths of savvy internet users started adding the word \"Reddit\" to the end of their web searches in the hopes it would bring up people sharing their honest opinions, as opposed to websites trying to game Google's system. It's something Google's public liaison for search, Danny Sullivan, has noted. \"We've found that people often want to learn from others' experiences, and so we surface content from hundreds of forums and other communities across the web,\" a Google spokesperson says. \"Our agreement with Reddit absolutely did not include ranking its content higher on Search.\" But Google results are a zero-sum game. If the search engine sends traffic to one site, it has to take it from another, and the effects on the losers in this Reddit equation are just as dramatic. \"Google's just committing war on publisher websites,\" Ray says. \"It's almost as if Google designed an algorithm update to specifically go after small bloggers. I've talked to so many people who've just had everything wiped out,\" she says. A number of website owners and search experts who spoke to the BBC said there's been a general shift in Google results towards websites with big established brands, and away from small and independent sites, that seems totally disconnected from the quality of the content. The change was instant for Daniel Hart, editor-in-chief of the UK-based entertainment news site Ready Steady Cut. \"After Google's September update, our traffic halved immediately, and it's only gotten worse. We've just been blitzed by the Reddit stuff in particular, but we're also being replaced by spam websites that are stealing our content. It makes no sense,\" Hart says. In the months following, the lost income forced Ready Steady Cut to reduce its team of 20 writers and editors down to just four, Hart says. A Google spokesperson said the company's recent updates have dealt a major blow to spammy, unoriginal content, and Google keeps a close watch on evolving abusive practices that lead to low-quality information in Search. After Google's updates, the company provided tips to website owners and said there was a path to recovery. Hart says the site hired consultants and pivoted to focus on Google's recommendations, spending sleepless nights working to update the site. After almost a year, none of it helped. \"I wasted the last eight months of my life trying to follow Google's advice.\" he says. \"Google claims they want content from people with first-hand experience and helpful context, and we're a massive example of that. It's just heartbreaking.\" A few examples show how recent changes to Google Search had startling effects on a wide variety of websites. (Source: SEMrush) (Credit: BBC) But the biggest offence, according to the website owners and content creators who spoke to the BBC, is the AI generated responses. Google argues its AI overviews in search results will be a boon to websites. Liz Reid, Google's head of search, wrote in a blog post that the company's AI search results actually increase traffic that Google sends to websites. \"AI Overviews get more clicks than if the page had appeared as a traditional web listing for that query,\" Reid wrote. \"As we expand this experience, we’ll continue to focus on sending valuable traffic to publishers and creators.\" However, the company hasn't shared any of the data backing up that claim, and many website owners and industry experts worry the opposite effect is just as likely. Katie Berry, owner of the cleaning advice website Housewife How-Tos, assumes users will just end their searches if Google's AI answers questions for them. The AI search results \"answer questions superficially, and often incorrectly, so people don't visit my site,\" Berry says. According to Berry, her site's traffic fell 70% after the 2022 Google update and dropped even further after Google started testing its new AI. \"My site had more traffic in its first months of existence than it gets now, even though my rankings have not changed all that much,\" she says. Others, such as travel writer David Leiter, say the potential consequences are especially problematic because they feel Google's AI is outright stealing their content. For example, Leiter says a search for \"Best Slot Canyons Near Las Vegas\" used to bring up an article on his website, World Travel Guy. However, a search earlier this week brought up an AI-generated response at the top of the page instead. \"Google has replaced my article with the giant AI Overview box, and it spits out an answer that is mostly wrong,\" Leiter says. \"The first four places it lists are not even slot canyons. A slot canyon is a specific type of canyon with a narrow passageway, but the AI doesn't understand that. It's just listing random canyons and even a walking trail instead.\" The AI Overview did include a link to Leiter's article, but only if you took the time to click a tiny arrow at the bottom of the result. Leiter says he doesn't believe he'll get more traffic than he used to as the top search result. In either case, it's a small consolation. Leiter says Google's recent algorithm updates erased 95% of his traffic. Google acknowledges that AI tools may provide inaccurate information, but says it's constantly working to improve results. A Google spokesperson says AI Overviews are generally taken from multiple webpages, not single sources, and the responses are designed to highlight relevant links. The spokesperson says publishers can use a special tag on their webpages to control whether or not AI Overviews includes a link to their sites. However, once an AI model scrapes your content, it may be impossible to remove that data. Media executives aren't the only ones questioning Google's control of the internet. Google is simultaneously battling numerous antitrust lawsuits against different parts of the company's sprawling £1.7tn ($2.2tn) business. The company is currently awaiting a decision in a lawsuit brought by the US Department of Justice accusing Google of running an illegal monopoly in the search engine industry. If the tech giant loses the case, the penalties could range from massive fines to a forced break-up of the company. Google, which controls more than 90% of the worldwide search business, argues that the company's success stems solely from the fact that it makes superior products. A Google spokesperson says the company faces \"immense competition\" and people have many choices about how they search for information online. \"I understand that Google doesn't owe us or anyone else traffic,\" says Navarro, of HouseFresh. \"But Google controls the roads. If tomorrow they decide the roads won't go to an entire town, that town dies. It's too much power to just shrug and say, 'Oh well, it's just the free market,'\" she says. \"I might just try working in the offline world, just pack it all up and tend a shop somewhere,\" says Navarro. \"Maybe it was naive to think we could succeed just by making great original content that people want to read.\" -- For timely, trusted tech news from global correspondents to your inbox, sign up to the Tech Decoded newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.",
    "commentLink": "https://news.ycombinator.com/item?id=40474202",
    "commentBody": "Google just updated its algorithm. The Internet will never be the same (bbc.com)161 points by sonabinu 7 hours agohidepastfavorite155 comments neilv 5 hours ago> A spokesperson for Google tells the BBC that the company only launches changes to Search after rigorous testing confirms that the shift will be helpful for users, and that the company gives website owners help, resources and opportunities for feedback on their Search rankings. When I got an Android tablet, I downloaded some of those free-to-play games in which you build bases and vehicles in a multiplayer world. And then other players come along and destroy what you've built, and steal your stuff. With the game providing not-so-subtle prompts to pay for the various kinds of in-game currencies/tokens, so that you can properly defend yourself or gain advantage over others. The mechanics of the game were mostly a diversion from the real way you win, which was the real point of the game's design. Regarding Google's \"help\", they helped turn SEO into this kind of game, and even offer gaming guides themselves, because, at the end of the day, people will have to pay real money for in-game resources, if they want to compete. > The second Google algorithm update came in March, and it was even more punishing. House Fresh's thousands of daily visitors dwindled to just hundreds. \"We just got absolutely crushed,\" Navarro says. Over the last few weeks, House Fresh had to lay off most of its team. If nothing changes, she says, the website is doomed. If you want visitors to a Web site, you probably have to pay money to Google or similar big tech company. It's not right, but that's the current situation. reply p3rls 5 hours agoparentNah, bribe a NYTimes writer at the beginning of your niches trajectory into the mainstream. That's how they did it in my niche and still making a half million a year almost a decade later. For reference the shocked SEO writer who found the site, but these guys never figure out just how it was done. https://x.com/paulk139/status/1550532282288508929 Why build something that takes work at all in this environment? reply Hates_ 3 hours agorootparenthttps://threadreaderapp.com/thread/1550532282288508929.html reply throwaway211 7 hours agoprevTwo big reasons people add Reddit to search phrases is because Reddit's search isn't functional and to deprioritise Ad Junk from search. It's to fix broken search, which gets more broken - more unwanted Ad Junk. But Google's an advertising company with it's ad search device in everyone's pocket. At least it's reached sufficient post-don't-be-evil to be honestly shameless, even if it does employ people in PR with broken ethics prostituting their names for their mortgages. Getting reflective's hard when you're celebrating yourself. reply jayski 6 hours agoparentReddit is the only social network I use. They're really dropping the ball by not having a fantastic search tool. I know search is a difficult problem, but with the kind of budget and engineering power they have it has to be solvable. reply orev 3 hours agorootparentReddit benefits from having bad search because it increases engagement with the site. If you can’t find it by searching, people start a new post, even if it’s just rehashing something that’s been discussed 100s of times before. The new post brings new comments, and more page views. There’s also a user benefit as the information has less chance to become outdated (a new discussion might bring new information, avoiding the Stack Overflow problem). reply tmccrary55 5 hours agorootparentprevI've been switching to Lemmy. It's like Reddit but not owned by a public corp. Reddit still has more users but Lemmy is more like old school distributed web, not just an ad product pushed by coked out MBA accountants. reply EasyMark 2 hours agorootparentIt's reddit-like BUT it doesn't have the same mass adoption and that's why I go to reddit. I don't have time to be a saint and help their numbers, so I just go to reddit and read what I want. Really HN is the only SM platform that I actually participate both ways with. I just read thing on other SM sites to catch up with friends or topics (like reddit). Comments there are mostly a trash heap but on something you want to know about you can find a gold nugget every now and then. reply pants2 2 hours agorootparentprevReddit had a great search engine called PushShift which was maintained and run by one guy. That service doesn't work since their API changes, unfortunately. reply rjh29 5 hours agorootparentprevThere's no money in it. It's expensive in both compute and storage to provide the tool, and they want most users to just be mindlessly consuming the front page or popular subreddits. reply balder1991 3 hours agorootparentThat’s my issue with Reddit nowadays. Most subreddits content is just the same question thrown over and over, and Reddit nowadays changes the home page to be just random algorithm selected stuff. And now I don’t have a choice to use Apollo anymore. reply throwaway211 6 hours agorootparentprevBefore: They were small. It was hard. Defer to Google. Now: By having bad search, Google rewards them. reply mlrtime 4 hours agorootparentprevWhy would they, google does it for free. Spend money making google crawlers more efficient, not copying google. reply yreg 6 hours agorootparentprevFor me, search is the least of reddit's problems since it is easily worked around by using a search engine. The changes to the API pricing were the final nail in the coffin for me, but the content has been going downhill for a decade anyway. reply zarathustreal 5 hours agorootparentReddit is a cesspool, at least the standard subreddits. It’s worse than Twitter. It also has a moderation culture that kind of promotes toxic traits and petty power trips. Used to be pretty good for finding niche info, but most of it for my uses was deleted back when everyone left. Can’t imagine still using Reddit nowadays reply EasyMark 2 hours agorootparentI find decent info in plenty of subs, you just have to stay away from the front page/popular subs, Especially the comments in those items. Total garbage full of people ranting. I give it a go a couple times a year but nothing ever changes. Just people raging at each other. reply jb3689 3 hours agoparentprevIt's odd. How did search engines trick us into thinking that our home page should be a search engine? The only reason I add \"reddit\" to searches is because a search engine is my entrypoint on my browser. Even then though, I find that Google tends to pin a particular threads. The most humorous ones are when it pins to a thread with \"this has been asked already a zillion times\" with no actual meaningful content in the particular thread. Google search was never really good at finding meaningful opinions in the first place. Facts? Reference? Pretty good. Meaningful tradeoff analysis of non-academic subjects? Product rankings and reviews? Opinions? Really poor. It's odd that Google decided to cannibalize the things it was good at. Finding recipes, for example, is terrible through Google because of the emphasis on SEO at the expense of user experience. AI search results are simply Google opting to make itself less relevant. reply dmje 6 hours agoparentprevBack again to say Kagi - where you can promote Reddit results. I have this, and it means the first result I normally get back for search is a Reddit answer. reply michelb 7 hours agoprevThis is good. Because it now will make way more sense to compete with Google on searching the internet, since they screwed it up. They’re are panicking and defending their bottom-line instead of innovating and leading. I think internet search is now ripe for disruption. Still need deep pockets but it’s now increasingly easier to be much better at search than Google. reply ADeerAppeared 7 hours agoparent> Still need deep pockets but it’s now increasingly easier to be much better at search than Google. That's the big problem here. All the new search engines rely on the infrastructure of the old ones. It would take very little collusion between Google and Microsoft to crush something like Kagi. And the moment any new competitor gets big, they'll do it. What's needed is an anti-trust breakup. There's simply too big a conflict of interest between Google, the search engine product, and Google, the ad-tech giant earning money off the websites. Google is using their search engine monopoly to control the wider web, and fill it with spammy SEO garbage to juice their advertising revenues. reply ricardo81 6 hours agorootparentAgree, intervention is needed. Any new entrant sort-of has to prove itself also as crawling sites can be hard, e.g. Cloudflare which fronts an appreciable portion of the web. Also large sites like Reddit you need to crawl them at 100s of pages a second to keep up to date with content, not something they're generally going to appreciate without a recognisable benefit to them. reply ADeerAppeared 6 hours agorootparent> Any new entrant sort-of has to prove itself also as crawling sites can be hard It's very hard, getting harder as websites grow more hostile to crawlers. (With more and more putting up login walls to keep poorly written AI crawlers out.) But the real problem is that tech is culturally unused to dealing with monopolists \"on even ground\". The wider tech community operates on the implied worldview that they can take on anyone by just asking daddy A16Z for a duovigintillion dollars. For 25 nearly-continuous years, you could just outspend to get into even the most hostile markets. That economic climate is just ... gone now. No more free money. You can't just buy your way to the top anymore, the business has to financially work. And most importantly: You can't outspend the incumbents anymore, but they still can outspend you. reply Exuma 6 hours agorootparentprevHow would they crush kagi reply ADeerAppeared 6 hours agorootparentKagi relies on data from the search engine giants. Without that data, it's dead in the water. That's the dumbest part of it all; Kagi's not an entirely new search engine. It's Google's index but without the ad-tech juicing, and some clever use of a few other search indices to specialize results. (I.e. Google could unshit itself at any moment, it just doesn't want to.) reply Exuma 6 hours agorootparentHmm... really? They don't do their own crawling and indexing? How does one simply repurpose google's index... just technically or business speaking? reply ADeerAppeared 4 hours agorootparentGoogle has an API. One just pays to use that. The problem being the aforementioned conflict of interest. The moment you're a competitor to Google Search (or any of Google's other products), they can kill your API access and ruin you. reply Exuma 1 hour agorootparentDamn... that makes Kagi way less exciting. I thought they had some secret sauce. It's just a paid wrapper WTF reply viraptor 7 hours agoparentprevI do like the competition starting to happen again. A lot of the articles seem to concentrate on how Google should do better. But there are so many sites that any change they do will break someone random. With best possible checks, it's just not realistic that there will be no mistakes. We'd be better with 5 players doing things slightly differently. Nobody would get completely wiped by a single change. reply david-gpu 7 hours agoprevOver the past several months I find Google search has become completely useless to find anything other than online retailers. I thought it may have been just something about my profile, but this article suggests it may have been an update on their end. Meanwhile, Duckduckgo still provides decent results in my experience. reply rutierut 6 hours agoparentDDG was such an absolutely subpar experience for me (used it for 6-8 months) that it wouldn't surprise me if nobody would _actually_ say it's better in a blind test. They just don't like Google. Kagi on the other hand really blew me away, I have no idea how they're doing it, but I'm only using Google for one thing these days (besides G Shopping & G Maps) which is when I want to use ad spend as proxy for quality/reliability of the company (for example when I'm looking for a local moving company in my city). reply dmix 1 hour agorootparentI use both DDG and Google (via ddg shortcuts) DDG is best for literal quick searches and Google for open ended questions or complicated queries Never had issues with either really reply oidar 4 hours agorootparentprevThat is a great idea. What have you used it for? How do you get the spend data? reply Nevolihs 6 hours agorootparentprevI only use Google for local search results (news or businesses) as well. Otherwise I'm also very happy with the quality of results I get from Kagi. reply beretguy 7 hours agoparentprev> Meanwhile, Duckduckgo still provides decent results in my experience. I’ve been using ddg since around 2016, but in the past few months I realize that brave gives me better results and Kagi even still better. So I’m switching to brave/Kagi combo. Brave for when i need to find easy stuff and Kagi when really need to find something. reply NayamAmarshe 7 hours agorootparentI don't see the point in using DDG. Brave Search is a superior replacement. reply Kye 6 hours agorootparentI try to live my values, so I can't use it. It's a rare case where I really do have a choice. reply beretguy 6 hours agorootparentIf you value paying money for a good product I’d say kagi is the answer. It is definitely worth its cost if you can afford it. reply beej71 3 hours agorootparentOne metareason to pay for Kagi is to demonstrate to the business world that there's a market for paid, quality search. reply sedatk 7 hours agorootparentprevAgree on that Kagi's better. reply servus45678981 5 hours agorootparentnext [2 more] [flagged] maleldil 5 hours agorootparentThe search engine is one of my most visited websites daily, and it's the gateway to other websites. It makes perfect sense to pay for it if the quality is superior to its alternatives, which I find to be true. If you haven't already tried it, they have a decent starter plan that you can use to check if it's useful for you. reply Hikikomori 7 hours agoparentprevSwitched to DDG a few months ago and find myself having to switch to Google for many searches. Going to give kagi a try after trialing it. reply pinapplepony 7 hours agorootparentI switched to DDG a while ago because Google thought I was a bot. Capchas, every single search! It kept telling me I was wrong when I tried to solve them. I'm on a popular ISP in India, my guess is that one of their IP ranges got blacklisted somehow. Between DDG and ChatGPT, I haven't looked back. reply sersi 7 hours agorootparentprevI first switched to ddg but kept using !g because I often wasn't satisfied. Since I've switched to kagi, I barely ever fallback to google (only sometimes for very local results) reply dmix 1 hour agorootparentprevPart of the pitch for using DDG is that you can easily query Google or Google images (!g !gi etc) or tons of other search engines or just use the regular bing search for basic stuff It’s more of a privacy friendly search platform than a total replacement for Google reply wand3r 7 hours agoparentprevYeah, I remember how it felt when I first used Google. You could search something like \"xck-956-fg\" and some obscure part would show up. Obviously they have amazing technology as some of that \"magic\" is still there when you can search items by just taking a picture, so this is a choice on their part to have horrible search. I use Kagi now as I can tune my searches with rankings and the results are quite good. I am sure Google could create the best and most amazing search experience with little effort. I would happily pay $20 a month for that service. For now, Kagi and LLMs have brought some of the utility back for search and I am optimistic as the technology of AI improves that information retrieval will significantly improve as search providers will be forced to compete and the technology will evolve reply steve1977 7 hours agoparentprev> Meanwhile, Duckduckgo still provides decent results in my experience. As long as Bing is running reply dherikb 7 hours agoparentprevI have a different perception, really. They are promoting more the Reddit results in the last months and, at least for me, I'm able to find faster what I want. reply balder1991 1 hour agorootparentNow it’ll be a matter of time before Reddit gets an order of magnitude more bots and spam. reply p3rls 5 hours agoparentprevDuckduckgo is garbage for my niche (entertainment). SEO wordpressed articles up at the top of every search. Looks like it puts even more weight on backlinks in its algorithm to me and as such is even more vulnerable to gaming/timing. reply mdhen 6 hours agoparentprevKagi is miles better then google, i've been using it for nearly a year now. Worth the subscription. reply giancarlostoro 7 hours agoprevImma be real its been deteriorating for a while. Used to be an insanely powerful engine. You could find things with verbatim quotes. I have not seen this work anymore for quite some time. It all started when they started adding rules to suppress results somewhere in the late 2000s or early 2010s. Its only gotten worse and worse. Googles search engine is now just a personal collection of pre approved search results for no valid rhyme or reason. reply chrisjj 7 hours agoprev> Google's efforts to address this issue aren't always successful. > Often popular search terms are crowded with websites that contain very little useful information, but tonnes of ads Ad-seller Google prefers ad-laden sites?? Who'd have guessed! :) reply 8organicbits 6 hours agoprevI've been thinking more about what we can collectively do to fix what is now broken. One tiny part of the solution may be individual curation. To me that means the return of the blogroll. I've been exploring this, along with others, at https://alexsci.com/rss-blogroll-network/ It's at an early stage but I've found value from it already. reply nottorp 6 hours agoprevnext [–]Note that the article is crying about the poor small businesses that google killed. Not about the good old days when you could find unbiased reviews that were done not-for-profit out of someone's personal experience on a geocities page that hurt your eyes with all those blink tags.reply tsuru 7 hours agoprevMaybe they should get the Twitter treatment anytime they are mentioned: \"Google formerly DoubleClick\" reply MagnumOpus 4 hours agoparent99 out of 100 people haven‘t been on the web long enough to dislike DoubleClick as much as it deserves. (And the rest know the work arounds to Google‘s shenanigans already…) reply skilled 7 hours agoprevHonestly, bravo at BBC for whoever green-lighted this story. It puts the finger right on where it hurts for small publishers and illustrates Googles new way of thinking. I am sad but also frightened for small creators and those who simply share interesting information once in a while. Google is bulldozing these people away from anywhere near the AI answers or the ten blue links below it. The article didn’t mention it but people who got hit in September with the helpful update have still not recovered. Not a single site. This is the update that HouseFresh was hit with. It’s insane. reply qp11 6 hours agoparentSmall content creators are always going to experience randomness and chaos 'in the current system', cause the Amount of Content far exceeds the amount of Attention available. If the platforms were honest they would signal to everyone how large that gap is. And people would make a more informed call about whether to get into content creation. Instead of doing that the platforms just puts the whole onus on content creator \"quality\" - hey man if you aren't up in the ranking its your fault, maybe you are a looser, go do something more outrageous, scream a little louder, repeat yourself 24x7, spam everyone. Or buy our ads. Its a nice scam cause you get even large corps and presidents shelling out billions on ads. reply _heimdall 6 hours agoparentprevIt seems pretty clear that Google's era of search technology has aged out. SEO has been a game of cat and mouse since the web became an ad platform, it was only a matter of time before Google ran out of moves. I'd much prefer a shift back to the earlier, smaller internet approach but it was driven by the enjoyment of exploration and discovery. Today most people prefer instant gratification online and just want answers and content thrown at them faster, with little regard for quality. reply ricardo81 6 hours agoparentprevI don't think there's a new element to their thinking. They used to rely on DMOZ's human curated content and then replaced it. Knowledge panels (at least the non-business ones) are reliant on wikipedia. The 10 blue links reliant on the web. They don't mind borrowing until they can replace, and monetise what they borrowed. reply nycdatasci 7 hours agoparentprevThis was inevitable. Traditional search is an inefficient way to access information in a world with large AI models. Google is being forced to do this due to competition from openAI, meta.ai, perplexity, etc. reply caseyy 7 hours agorootparentLLMs cannot compete on truthful high-quality information from experts. Only on plausible babble. If that’s the only information you search for, then your perspective makes sense. We shouldn’t be using LLMs for information retrieval at all. They should be summarizing individual web pages at best. That would make search more efficient. But to use plausible text generator without any guaranteed veracity checks in the pipeline for information retrieval is madness. reply ricardo81 4 hours agorootparentprevIt is a bit surreal that you describe sucking up the world's information and spitting it out via a commercial entity and to you as a search result as 'inefficient'. Perhaps it's more inefficient for all those content creators to bother creating anything vs what you actually derive from the search result, especially if they're not attributed/compensated. reply hobs 7 hours agorootparentprevIt's really not, they are extremely complementary. reply _heimdall 6 hours agorootparentNot necessarily. Internet search algorithms were designed to solve the discoverability problem. LLMs are generative, they're creating content rather than finding it for you. They also serve a different business model. Sure you can have ads in LLM answers similar to Google search results, but Google is also selling clicks. LLMs are designed and used, at least today, to give you the answer so you don't need to see an original source. Beyond that, the idea of a generative algorithm being used in tandem with sponsored results sounds pretty gross in my opinion. I already don't like Google's approach to search results, but if they're also generating the responses on the fly why am I even there? Its only a matter of time before the generative algorithm is trained specifically to drive more sponsorship revenue, maybe by making up content that works better for ad bids or stuffing a reference to a sponsor into a response only because the inline ad pays or converts better. reply hobs 6 hours agorootparentSeems like the option is constantly retrain an LLM or give it enough tools to interact with new data as needed. For the foreseeable future LLM's are going to need RAG or huge context windows which are effectively equivalent to RAG because you are isolating from some giant corpus your useful information you want the machine to meaningfully interact with. reply _heimdall 5 hours agorootparentIt still seems like a different use case to me, but maybe I'm wrong there. I don't mind a search algorithm including a summary as long as its true to the original, but I really don't want any generated content that's trying to predict how someone would answer my question. If LLMs end up replacing most of the use of tools like Google Search we really have moved past the discovery problem and don't necessarily want to find or see the original (human authored) content at all. reply pknerd 1 hour agoprev> The Internet will never be the same (bbc.com) Yes, now we can rank by posting crap on Reddit [1] [1] https://imgur.com/a/iMejAF3 reply rsolva 7 hours agoprevWe need to make bookmarks a common way to access the web again! So many people use Google or other search engines to access even the most basic sites, instead of storing a bookmark or typing out the whole URL. I have used a simple bookmarking strategy in Firefox for some years now, simply storing everything and anything I suspect I might want to retrieve later. I give each bookmark a few quick keywords and store it all in one folder. No hierarchy. It's surprisingly useful and has helped me retrieve so much stuff from my vague long term memory. In addition, I flush the cache automatically every time I close the browser and curate a list of sites that are exempted from the flush. I have also turned off search words suggestions in the address bar, leaving me with a tidy and usefull experience that helps me find what I'm looking for in seconds. Only saved sites show up when I start typing. But most browsers, including Firefox, need to improve their bookmarking system. I can't add keywords to bookmarks from the mobile browser. Why not? And the bookmark UI has gotten little attention for literally decades. It would not help all smaller sites, but it would help some, if browsers would improve the storing, managing and retrieving of bookmarks. reply machiaweliczny 6 hours agoparentHow about social features like sharing bookmarks or having curated list for specific subjects When I want to share bookmark folder for other collegues I have to go trough hoops (at least on chrome) reply crucialfelix 3 hours agorootparentThat's what https://del.icio.us/ was. It was very social in the best sense of the word. reply mejutoco 6 hours agoparentprevRegarding desktop bookmarks: Adding tags on Firefox is so natural (ctrl+D and maybe tab for comma separated tags). In Chrome it is not possible without an extension. reply EasyMark 2 hours agorootparentcan't you just save them on the end of the title? doesnt it search through the titles when searching the bookmarks? It's all just a big grep through your bookmarks anyway whether it's in tags or titles reply bimodalblitz 6 hours agoparentprevI just started using BookmarkOS: https://BookmarkOS.com reply balder1991 1 hour agorootparentInteresting. I’ve been sort of using Notion but by creating pages of concepts I want to save interesting quotes, links etc. It makes searching later very easy and also helps me build my own knowledge over time. reply A_D_E_P_T 6 hours agoparentprevYeah, and, along the same lines, directories should perhaps also make a comeback -- like the old Yahoo! Directory or DMOZ. Just a carefully curated and organized list of trusted sites that are made and managed by humans and for humans. Unlike, say, DMOZ, the directory of the future should be easier to navigate and incorporate \"modern\" features -- and it should also be easily searchable. (All sites on the directory are indexed; no sites off the directory are indexed.) Searching for B2B products and services has become a lot tougher lately. A good directory could conceivably make a meaningful difference. reply cageface 5 hours agorootparentI’ve been saying for two years now that curated directories are poised to make a comeback on the web. It just needs a critical mass of curators. I think this is why those awesome-$tech lists are so popular. reply EasyMark 2 hours agorootparentI mean they might have a mini revival but i think we've gone too far to change the process of the general population. Indexes appeal to more organized people who like such things, most of us just throw everything in a pile in a well known location (google) and rifle through that. reply renegat0x0 3 hours agorootparentprevI have been collecting personal sites for some time. Format JSON files. Links have tag 'personal'. https://github.com/rumca-js/Internet-Places-Database reply JKCalhoun 6 hours agorootparentprevCould work. We have databases behind PiHole, could have something similar to filter search results. reply ricardo81 6 hours agoprevThe article starts with intimating that SEO is the problem. Perhaps to an extent it is, though I'd say with G's massive market share that optimising for one sweet spot with one search engine is really the problem. It makes SEO easier, one search engine and criteria. 20-25 years ago when there were half a dozen prominent engines you'd be ranking well in some but not others. A good argument for alternative algorithms (and search engines). Generative AI is no doubt useful for simple questions, and perhaps for people who aren't even interested in being sceptical about the answer given, but it seems at too high a cost for the visibility of the actual web and the effort put in by content creators. Obviously with the huge and unrivalled ROI G gets from ads vs any rivals, the fact they're the default on most mobiles etc means any alternatives have a huge uphill battle to get seen. At least there is Bing (and its meta variants), Mojeek (independent, a smaller index but truly independent) and Brave (claims independence but has a historical tie to scraping results) reply hollerith 6 hours agoparent>20-25 years ago when there were half a dozen prominent engines That's not how I remember it. My recollection is that for a year or 2 (roughly 1998 and 1999) Altavista was the dominant engine, which rapidly gave way to Google essentially as soon as Google became available, and since 2000 Google has been the dominant engine. In the late 1990s and early 2000s, I was getting my info on what engines people were using mostly from Usenet, Slashdot and a non-web-based forum called river.org. reply whstl 6 hours agorootparentAltavista was very popular in tech circles. But I remember \"normies\" mostly using and talking about Yahoo, Lycos, Ask Jeeves, and even lots of local, country-specific, search engines. And after that moving to Google. reply hollerith 6 hours agorootparentI noticed at the time that Yahoo's engine was very popular with the less-technical users, yes. reply MagnumOpus 4 hours agorootparentprevIn those years from 97 to 99, Altavista might have been your first stop, but you had to ask Excite, Yahoo, Jeeves and Lycos for a full picture or even to get a single good answer. As you say, once Google Beta appeared it was the first and last stop for any of its users. reply ricardo81 6 hours agorootparentprevAlltheweb was fairly prominent then. The likes of Ask Jeeves and Lycos had a few % market share. Gigablast also had its day back then. reply hollerith 6 hours agorootparentBefore I switched to Altavista, I used Lycos for most of my searches though I did occasionally use Ask Jeeves and Alltheweb just to explore the space of offerings. During those years, I also used DMOZ regularly, which is not a search engine, but rather an index of links organized much like a card catalog in a library is. reply ricardo81 6 hours agorootparentYahoo bought a lot of the alternatives out, going from memory, then eventually switched to Google results. reply marcinzm 5 hours agoprevI find Google worse every day. The latest thing I noticed is a ton of AI generated sites at the top that \"summarize\" things. Mostly the same format. Except they're usually wrong in many ways. One site kept saying Llama 3 is a Mixture of Experts model despite it not being one. reply EasyMark 2 hours agoparentthe AI sites are what got me off google, DDG, etc. Kagi seems to be a bit better and avoiding that garbage although I still see ones there from time to time. Usually a dumb af site name gives it away. reply maram 2 hours agoprev>>It's almost as if Google designed an algorithm update to specifically go after small bloggers. I've talked to so many people who've just had everything wiped out While at the same time small businesses are being killed by the lockdown reply EcommerceFlow 5 hours agoprevWhile yes it’s technically “Googles fault”, I feel like the reality is the spammers have won the SEO war. Might be as simple as that, reply arnklint 7 hours agoprevWho’s gaining traffic right now? Is it just Reddit and a few others that has caught the traffic from this implosion or is the volumes of searches shrinking due to perhaps AI? reply Aachen 6 hours agoparentSee the bar chart in the article, third image (don't know if hotlinking works): https://ichef.bbci.co.uk/images/ic/1024xn/p0j04fnt.jpg.webp Probably besides the ones gaining more traftic, you're probably right the overall click-through rate will be vastly lower than it was before quote answers appeared, and now probably ever lower with the generated text answers reply hosteur 7 hours agoprevGood news for Kagi then. reply raytopia 6 hours agoprevI wonder if web directories become more popular again if it would help any of these issues. They seem a lot better for surfing the internet (thus naturally finding new websites) as opposed to searching it. reply ungamedplayer 7 hours agoprevLive by the sword, die by the sword reply jmyeet 7 hours agoprevSo I'm normally skeptical of the anecodata you see on HN and elsewhere (\"Google is now useless, I switched to DuckDuckGo 23 years ago\") but it seems like Google has done rather big things here: 1. Greatly increased the weight of doamin authority. This... seems desperate; 2. Increased the weight of reddit. Google have probably noticed how many people add \"reddit\" or \"site:reddit.com\" to search terms. I do this too. It's true for a lot of topics with all the AI-bot and affiliate astroturfing out there, Reddit can often be the only source for genuine commentary. But Reddit has a dirty little secret: it's really easy to complete change the content and character of a subreddit. You see this on controversial topics. It only takes 100-200 people (or less with bots) to completely transform a subreddit. User generated content sites have techniques to find socket puppets, brigading and the like but it's an arms race. 80% of my Tiktok follows come from crypto scam and p0rn bots. I imagine Twitter is the same way. The bots continue to get better too. Those same algorithms can be used to keep bot activity below the threshold for detecting voting rings. So Google's increased reliance on Reddit will probably escalate the astroturfing and botting on Reddit and honestly, I don't think Reddit is up to the job of defending against this. reply nottorp 6 hours agoparent> Google is now useless, I switched to DuckDuckGo 23 years ago 7-8 years ago now, i think. Definitely years ago. Yes it started its slide towards useless back then. If you're searching for pizza near you it's probably useful still, if you're searching for say product reviews or niche technical questions it's been completely useless for a while. reply KaiMagnus 6 hours agoprevGreat article, probably confirming something a lot of people on here have already suspected. But I am honestly shocked at how dire this situation seems. > Reddit isn't the only winner after Google's recent algorithm updates. SEMrush data shows that other user-generated sites such as Quora and Instagram saw similarly astronomical rises, and there were impressive spikes at LinkedIn and Wikipedia as well. How can it be that after 20-30 years, it seems they have no way of knowing which sites produce genuine content and which do not? And what’s worse, in what I can only describe as a frantic search for a solution, they simply turn to the largest players and bump up their weights. They are killing the grassroots of the internet and when places like Reddit continue their inevitable path of enshittification the internet will be truly dead Of course the internet is resilient and it will grow again, but we need to get this monopoly back in check fast. reply predictsoft 7 hours agoprevDidn't Peter Norvig used to be director of search quality? And now Head of research? Maybe they should bring him back to increase that search quality. reply anonzzzies 7 hours agoparentShareholder ROI is all you need to know that they won’t let Norvig decide anything outside purely tech stuff. reply CaptainOfCoit 7 hours agoparentprevOr the executive/management team could start using Google search themselves so they can see how bad the results are for their users. Pretty common to dogfeed in the software business yet Google seems to ignore this approach. reply sp332 7 hours agorootparentWhat search engine do you think management uses now? reply CaptainOfCoit 7 hours agorootparentThey probably use some internal version that doesn't optimize for ad revenue but instead for user experience, as is Google tradition. reply InfiniteVortex 4 hours agorootparentThis is the correct answer LOL reply hosteur 7 hours agorootparentprevProbably Kagi. reply post_break 6 hours agoprevI’ve been using bing and ddg so much more. The bing outage actually impacted me and that took me by surprise. reply EasyMark 2 hours agoparentbrave search isn't a bad fallback reply counterpartyrsk 6 hours agoprevOr is Google prioritizing sites that drive Google ad revenue, and not ranking according to content? reply joshstrange 7 hours agoprev> A spokesperson for Google tells the BBC that the company only launches changes to Search after rigorous testing confirms that the shift will be helpful for users, and that the company gives website owners help, resources and opportunities for feedback on their Search rankings. If you believe this I've got a bridge to sell you. The idea that Google does anything other than to improve its bottom line (in the _shortest_ possible timeline) is lunacy. Just look at Google constant short-sightedness and their parasitic ad team pushing the search team (before taking it over effectively) to find ways to increase the searches (which means a worse user experience). Of course Google doesn't care about quality sites, they care about what makes them the most money, the user be damned. Anyone with an ounce of morality or care about the actual user experience was run out or relegated long ago. reply Aachen 6 hours agoparentSell me another bridge then. This is why I believe it: > Google does anything to improve its bottom line (in the _shortest_ possible timeline) If the results suck, people would find alternatives. Look at how many techies installed Chrome for their parents and grandparents when MSIE was pre-installed and this was an easier-to-use, standards-compliant, and more performant option. It further snowballed from there. It takes a few years for word to get around but they're well aware that providing useful results is the only way to stay relevant The problem lies in how Google goes about doing this. People like the (mostly) honest opinions you find on reddit. People like the answer boxes at the top and look no further. My girlfriend is one of the smartest people I know and she doesn't bother reading any further. Any time we look something up together it's a discussion point that I prefer to blindly scroll by the summary and she prefers to not click any result because the answer was already given. One of the most recent times, I could prove the quoted answer was wrong (for once) and that the results clearly gave a better picture in a few more seconds, but it doesn't kill the overall trend where people use this type of summary. Quite the opposite: a friend made a fun little game that got popular tiktok. I talk to players on discord (because of course walled garden discord is their preferred platform). Any time the topic of searching or learning something comes up, most of them demonstrate 100% of their searches start on chatgpt and usually end there, too Google knows better than I do (I'm sure they've done proper studies) what they need to do to stay relevant and their answer as of today is adding generated text output that tries to answer your query without having to even look at any results. The summary box on steroids That this kills small websites is not their problem. All of this may be at the cost of honest content online, but do a Duckduckgo search for comparison: many queries result in pagefuls of content farms that pretend to be review or explanatory websites, looking legit but when you read beyond the first three sentences it's repetitive, baseless, uninsightful, and altogether useless or unreliable at best. I wouldn't know how to distinguish content farms from legit opinion websites at an algorithmic level (perhaps an LLM can make a guess that's often correct, if we think LLM vetting a desirable future), and apparently Google doesn't either because they choose to push sites like reddit and quora instead which, being big brands, everyone already knows are legit sites. I never heard of the air purifier review site the BBC article focuses on, so I can't notice their omission in the search results. Getting content farms is noticeable though and makes me consider which alternative to Duckduckgo I should try out. This thread has been mentioning Kagi, so that'll be something I try later today. Notice how quickly I'm ready to switch away once I find there's something with better results out there In short, scrapping small or even obscure sites (that we all love, if we can find them) in favor of big names, and replacing needing to look at results yourself, is what Google's internal studies apparently show will keep Google Search relevant and I don't see any evidence to cast doubt upon that. Providing what people evidently want allows them to say what you quoted with a straight face reply joshstrange 4 hours agorootparent> If the results suck, people would find alternatives. Look at how many techies installed Chrome for their parents and grandparents when MSIE was pre-installed and this was an easier-to-use, standards-compliant, and more performant option. It further snowballed from there. It takes a few years for word to get around but they're well aware that providing useful results is the only way to stay relevant The results often do suck and people are actively finding alternatives (mostly techies). No search engine I've tried has really reached the bar where I'd push my friends/family to switch but they are getting closer and closer. MSIE had a lot of momentum and it wasn't FF that techies installed, it was Chrome, all that to say just because we haven't seen a Google replacement yet doesn't mean we won't. Furthermore people's frustration with Google search is directly what has lead to the alternatives that have been gaining popularity (again, mostly in techie groups so far). With your logic I could have said back in 2009 that \"If IE sucked people would find alternatives\", momentum is hard to fight but it can be done. > they're well aware that providing useful results is the only way to stay relevant If this post [0] is to be believed then providing useful results is (now) a side effect of wanting to drive traffic to show ads and it's completely valid to degrade the search results if it means people spend more time searching (and thus see more ads). No doubt they have to provide value still but there is nothing saying that's what they are optimizing for, if it happens as a side-effect then so be it. I really have no idea how to reply to the rest of your post which can be summed up to \"Google knows best\" which I reject completely. I currently use Kagi but I'm not over the moon with it and it has its own issues [1]. [0] https://news.ycombinator.com/item?id=40133976 [1] https://news.ycombinator.com/item?id=40011314 reply meiraleal 6 hours agoprevThat would be exactly the outcome they were expecting with this change. I disagree that Google still holds the power to change the internet tho. reply guerrilla 6 hours agoprevI don't know how people still used it for the last years. Sure, DuckDuckGo used to be a bit worse, but not that much worse and then came Brave and I feel like I don't need anything else anymore. It's like having old Google back. reply gtirloni 6 hours agoprev\"Create content for us to train our AI models while we fill the front page only with big websites that pay for our ads. Thank you!\" reply mdavid626 6 hours agoprevYeah, AI answers is exactly what we need (unamused). reply coldcode 7 hours agoprevI searched for how to use a massage ball (my back hurts) last week and got a giant page full of ads and ad-like sections for buying one with barely any articles at all. That is not a search engine; it's an ads engine. reply A_D_E_P_T 7 hours agoparentThat's by design. I'm reminded of this from a month ago: https://www.wheresyoured.at/the-men-who-killed-google/ Discussed with nearly 900 comments: https://news.ycombinator.com/item?id=40133976 reply mike_hearn 6 hours agoparentprevIt depends partly on where in the world you are and what language you use. I just tried that query in English and an answer callout was the first result, specifically highlighted. Then there is a \"people ask\" refiner widget, some videos in which the first is titled \"How to use a massage ball (tutorial)\" and then a WikiHow page. No ads. I tried again in the local language and now I see some product ad results for buying a massage ball, but then it's back to the video results and sports blogs showing how to use them. That seems OK. Probably most people searching for massage balls do want to buy one or are very likely to do so soon. I dunno, seems like they're both pretty good pages of results. reply kriops 6 hours agoparentprevKagi costs Spotify money, has zero ads, and lets you customise the results. reply mkroman 6 hours agorootparentIt also can't fetch more than 2 pages of results for either web or image search. Reverse image search just straight up doesn't work. I don't know why they have it. With searxng, searching at least works. It also has plugins that I think can do some of what Kagi is useful for (but it depends on the instance) reply ajdude 6 hours agorootparent> It also can't fetch more than 2 pages of results for either web or image search I can't speak for image searching, but I found when I get limited results from Kagi, The other search engines that produce More results just pad those results with noise. Sometimes I'll do a specific search and have no results on Kagi, so I switched to another engine justified I have thousands of unrelated results reply mkroman 5 hours agorootparent> The other search engines that produce More results just pad those results with noise. Yeah, that's been an increasing problem.. Google has been especially bad with this for a while now. When I tried to search for something where I wasn't sure exactly what I was looking for, I could only change my query so many times until it stopped being what I wanted. reply whywhywhywhy 4 hours agorootparentprevYandex is the only decent reverse image search I've found, it's about the same as Google was at it's peak. reply dmje 6 hours agorootparentprevI came here to suggest Kagi. I bloody love it. No ads, ability to remove or downgrade or promote particular sites, lenses, I love it all. Your comment - honestly, hand on heart, I can’t remember the last time I paged through beyond the first page of any search engine. I didn’t even know Kagi had this “problem” and so I guess I just don’t see it as a problem. But then I realised I’m absolutely in that bit of “must be on first page” internet lore, too… reply mkroman 5 hours agorootparent> I didn’t even know Kagi had this “problem” and so I guess I just don’t see it as a problem. Took me a while before I actually realized this limitation as well. I started noticing it when I was searching for things I didn't know exactly how to find. Kagi is great for when you know exactly what to find and have a general idea of where, but when you actually need those n-page results to help you refine your search, and you can't, it's a showstopper for me. I'm not even sure why it's a limitation. It might not be intentional? reply amelius 6 hours agorootparentprevOne thing that bothers me about Kagi: I have a large landscape-oriented screen. Why does Kagi only use the left part of it, and leave the rest blank? reply mroche 5 hours agorootparentYou can switch it to a centered layout in your account preferences in the Appearance tab. reply amelius 5 hours agorootparentI am going to try this, but I suspect this will still waste half of my screen real estate. reply aragonite 6 hours agoparentprevAt the very least Google (or any search engine for that matter) should consider implementing a feature that allows users to explicitly signal query intent. For instance, a 'noshop:' prefix could be used to signal that the user is not looking to buy anything. There are two distinct groups of users searching for terms like \"massage ball\" or (in my own case) \"managed switch\": those looking to buy the product and those who looking to learn about the technology. Serving ads to the latter group is not just irritating but also ineffective since those queries were never going to lead to purchases in the first place. reply rasz 6 hours agorootparentWhich Google user would want to pay for ads about things you cant buy ('noshop')? Oh wait, you meant Google _product_. You want Google to cater to its product needs by sacrificing own money making potential and limiting its actual users. reply aragonite 6 hours agorootparentGoogle already tries to infer whether a user is shopping or seeking information and serve results accordingly. If I search for \"managed vs unmanaged switch\" or \"massage ball usage\", I do not see any ads. All I'm saying is that Google should consider allowing users to explicitly state their intent, since some queries (\"massage ball\" and \"managed switch\") are ambiguous and so the intent cannot always be reliably inferred. Analogy: while type inference works well in many cases, there are situations where explicit typing is required because the type is ambiguous based on usage alone. reply drivebycomment 6 hours agoparentprevI just tried \"how to use a massage ball\" and I don't see any ads, and links seem to be all legit articles, with YouTube video tutorials at the top. reply shzhdbi09gv8ioi 6 hours agorootparentGoogle results are tailored on a number of factors such as location, your past searches and whatnot. The same deal with the ads. Advertiser wouldn't pay for advertising of their product in a region or to a demograph they don't care for. I'd be more surprised if you did get the same result as parent. reply drivebycomment 4 hours agorootparentThe results are not that different across locations within each country, especially location agnostic queries like the particular example. It would be rather surprising if there's huge difference between locations within US for this particular query. reply booleandilemma 6 hours agoparentprevI just did the same search and the first result was how to use a massage ball. I scrolled down a bit and saw a reddit thread titled \"using a massage ball for back pain\". The declining quality of Google's search results is greatly over exaggerated. reply jascha_eng 6 hours agorootparentI wholeheartedly agree with this. I also am using an ad blocker, but aside from that the pages that are returned work perfectly fine for me for the most part. Yes sometimes there is some useless blogs taking up top spots but then I tried the so hyped alternatives like kagi and they don't filter these out either so eh... reply abenga 5 hours agorootparentIt works perfectly fine even on the Google android app that is unaffected by uBlock. I always hear people complain about search results but I can't remember the last time I even had to scroll down when searching. I do usually mostly search for ultra-specific programming stuff though. reply datavirtue 7 hours agoparentprevGet The Back Mechanic book. reply gms7777 6 hours agorootparentI appreciate the parent commenter complaining about how they were looking for something just to get ads instead, and getting a response telling them to buy something. That said, I'll also second the back mechanic book for anyone with back pain. reply Varloom 7 hours agoparentprevI reduced my google search engine usage by 90% Just Option+Space and get the answer with ChatGPT for MacOS. No ads, no nonsense, just the answer. reply whstl 6 hours agorootparentChatGPT is also becoming a victim of SEO. Not purely at the crawling level, but also at the answer level. But as a default, it gives mostly \"SEO-optimized\" answers in a SEO-optimized format. Even simple question are answered with bullet points. Some descriptions look like advertisements and it's all very authoritative, even when it's just bullshitting/guessing and doesn't really know the answer for sure. This is what I was escaping from. EDIT: For example the last thing I asked was how Brendan O'Brien did sample replacement in drums in Blood Sugar Sex Magic. It answered something that was 90% plausible but most of the content was walls and walls of text about the equipment it assumed it was used (plus info on more equipment unrelated to sample replacement), rather than actually answering the question itself. reply threeseed 7 hours agorootparentprev> No ads, no nonsense, just the answer. 5 - 10% of the time that answer will be complete rubbish. We need something better than Google and LLMs e.g. Golden before it was acquired: https://golden.com reply user3939382 6 hours agorootparentKagi works great. reply threeseed 6 hours agorootparentGreat is a loaded term. They promote \"best headphones\" on their home page. 3rd organic link is a 404. And the rest of the links are either not timely e.g. from 2016 or niche e.g. for Telehealth. Also it is not location aware so mention of Bestbuy even when not from US. reply dmje 6 hours agorootparentprevI don’t know why this is getting downvoted. It might be unpopular to use or like ChatGPT among the users of HN but - it’s true, this is one of the quickest ways to get answers for something reply pmlnr 6 hours agorootparentAnswers, sure. Bit you'll never know if those are answers, bullshit, trolling, or straight up hallucinations. So no, it's not an option at all. reply VancouverMan 5 hours agorootparent> you'll never know if those are answers, bullshit, trolling, or straight up hallucinations That's how it has always been for purely human-generated content, too. reply dmje 4 hours agorootparentExactly. This seems to be one of the skills we need to learn. In the same way we have to use our own intelligence and skills when searching google (Is this source website reputable? What’s the context I’m searching in? Do I know this person / company? Does that result sound likely? Can I compare to other answers to see if it makes sense?) - it seems to me that using ChatGPT to do anything requires a similar set of (slightly cynical / take what you read with a pinch of salt / check your facts - but nonetheless understand that this is a useful tool) type response to it. reply adolph 5 hours agorootparentprevYou write this as if that is substantially different from common epistemology of the first page of links. I’m not an llm advocate but responses like this bring to mind the “no wireless. Less storage than a nomad. Lame” reaction to the iPod. reply dmje 4 hours agorootparentNo idea what this means reply TibbityFlanders 7 hours agoprev [–] Honestly, I use Edge Copilot for almost all my searches now. Google is no longer looking and soon the majority of people will realize that. reply Aachen 6 hours agoparentI don't understand what you mean by looking in the sentence \"Google is no longer looking\". Do you mean they're no longer crawling websites and therefore blind to new content online? reply datavirtue 7 hours agoparentprev [–] Same. This has been a serious productivity booster. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google's recent search algorithm updates have significantly impacted independent publishers, favoring larger lifestyle magazines and user-generated platforms like Reddit.",
      "The introduction of AI-generated answers aims to reduce low-quality content but has led to drastic drops in traffic and revenue for smaller sites, forcing some to downsize.",
      "Google faces multiple antitrust lawsuits, including one from the US Department of Justice, accusing it of maintaining an illegal monopoly in the search engine industry."
    ],
    "commentSummary": [
      "Google's updated search algorithm has led to significant drops in website traffic and forced changes in SEO strategies, causing potential closures for some sites.",
      "Users express dissatisfaction with Google's ad-heavy search results, often adding \"Reddit\" to queries for more relevant results, and are exploring alternatives like Kagi and DuckDuckGo.",
      "The discussion reflects a broader frustration with current search engines, highlighting a shift towards AI-driven content and the need for better search tools, despite challenges faced by new competitors."
    ],
    "points": 161,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1716635666
  },
  {
    "id": 40472374,
    "title": "Embracing Mistakes: A Call for Constructive Criticism in Software Development",
    "originLink": "https://rachelbythebay.com/w/2018/04/28/meta/",
    "originBody": "Writing Software, technology, sysadmin war stories, and more. Saturday, April 28, 2018 Optimizing your talking points Internet, we need to have a little talk. It's come to my attention that some of your nodes have a pat answer to any little bit of story telling that I arrange. There are holes in the logic and they need to be brought into the light. \"People shouldn't write crap code\" I agree. People shouldn't write crap code. However, despite our powers combined both wishing for this, it still happens. A lot. I even do it myself. I'll assume you are better than me at this, whoever you are, because you said this. Therefore, you do not write crap code. That's pretty awesome. However, you still have to work with the rest of us. We're going to do dumb things like disabling the first 100 accounts on a system instead of disabling the first 100 entries in a list on occasion. We're going to use a string as a key and then hand it an int sometimes. If your programming organization is bigger than just you, then yes, eventually you are going to cross paths with one of us, and then you have a decision to make. We'll come back to that, but first, another line from the nodes. \"You shouldn't hire bad programmers\" Wouldn't that be nice? I mean, it kind of would, since none of us would have jobs, because we all suck. Except for you, of course, since we already established that you're better than the rest of us. If a \"bad programmer\" is someone who makes mistakes sometimes, then I dare say that everyone else in the world will eventually sort into that bucket. This kind of solves the earlier problem, right? If this held up, then you might in fact be the only programmer on earth. Then you wouldn't have any teammates, and you wouldn't have to worry about crap code. Pretty solid! \"I've never seen this happen, I've never done it myself, and I'll never see this in any code I write for the rest of my life\" Well, yeah, because you are THE ONE. You're the one who climbs up onto the tower at the end of the movie and starts kicking ass on all of the exiles who only see you as \"fresh fish\". Naturally, you won't do this bad coding. But the rest of us will. \"This is so easy to test for\" Is it? Testing is as much a human thing as it is a technical thing. If your people don't buy into the notion of testing, what happens then? How about the fact that tests are also just code, and therefore tests can suck just as bad as the actual program, if not worse? Keep in mind that the rest of us peons are the ones writing both the program and the tests, so what are the odds that both will be correct at the same time? \"I hope this didn't run in prod\" Most of my stories are tales from the trenches: real stuff that's happened at one place or another. Sometimes, I've filed off the identifying marks so as not to give a giant flaming arrow pointing at a certain company, but it usually doesn't matter. It seems like a lot of these places eventually make the same mistakes anyway. Because we all suck. Because we're not you. So yes, the uid thing ran in prod... somewhere. The string-map-int stuff ran in prod.... somewhere. strftime with %G happened... somewhere. The processor bugs happened... somewhere. std::string and bad vectors happened... somewhere. Exfiltrating via SMTP and DNS happened... somewhere. IPv6 badness crossing a router happened.. somewhere. malloc(\"HTTP\") happened... somewhere. Because we are not you. Finally, I have a suggestion for you. Instead of posting all of those indirect messages which dance around your actual point, try this instead: \"I am THE ONE. You only need to know me. I am better than all of you.\" Then you can bow out of the conversation and let the rest of us figure out how to get the job done with our shitty sub-par abilities. Sorry about wasting your time. This is not for you. More writingContact / send feedback",
    "commentLink": "https://news.ycombinator.com/item?id=40472374",
    "commentBody": "Optimizing your talking points (2018) (rachelbythebay.com)161 points by compiler-guy 15 hours agohidepastfavorite65 comments mgkimsal 3 hours agoCouple of stories about 'bad code': I've often worked solo, but have been on teams where I've written code that was... not great. Worked, but... suboptimal. Various reasons, but it is what it is. To remedy that, I would try to refactor old stuff in conjunction with new work. That was often rejected. So... I'd take to documenting needed refactorings/fixes in tickets. Those would rarely ever get attention - generally deprioritized or ignored. This led to - for me - constant low-level frustration. When working solo, I can prioritize what I need to; in teams... you can't ever make a suboptimal decision because it will live forever. Or until a 'system is down' moment. Those would lead to \"post mortems\", in which I would point to tickets requesting to fix ticking time bombs months earlier, explain they were ignored by leadership, and... that came across as 'blaming' or 'antagonistic'. The 'fix' is to just never write suboptimal code, which leads to more frustration and anxiety when writing. Secondly, in 2017, I got a call to fix something I'd written in ... 2003/2004. That code was still in production (with minor patches by others along the way). It's quite humbling to have to review broken code and corners cut, and realize that you were the one responsible for it (and no one else). That (and a few other incidents) have given me a big change in perspective on writing maintainable code (and documentation, etc). reply YZF 22 minutes agoparentIt depends on the team/company. I've worked on many teams/companies where: - Developers were encouraged to put in time to maintain and refactor code. (including forcing them to take time to focus on that and do nothing else) - Engineering-centric work was prioritized over random product management asks. - Schedules were adjusted to make sure the engineering work is done properly. I'd say this was my typical experience, including S&P-500 multi-billion dollar companies, 100M-$1B companies, startups. I would also say that in all these cases there were experienced software engineers and managers that could be trusted to make reasonable tradeoffs and also pay attention to business needs. Often engineers interacted/worked directly with customers. There has to be balance and that balance is typically achieved through people that can apply a balance. Striving for \"perfect\" can lead to never ending refactoring and never shipping. Ignoring technical debt or shipping garbage can lead to the collapse of the business over time. Neither of these extremes are the right thing. Where exactly you land on that spectrum also depends on the specific product, industry, customers, business. reply thadt 3 hours agoparentprevYou either die a hero or live long enough to see your name in the decade old git blame. reply mgkimsal 3 hours agorootparent2003... svn all the way :) reply SoftTalker 2 hours agoparentprev\"Works but suboptimal\" is often an acceptable tradeoff when you have more work than you have time to implement in an optimal way. \"Perfect is the enemy of good\" and other slogans that MBAs learn in school are phrases you can use when something falls through the cracks. reply mgkimsal 2 hours agorootparentMy phrasing may have been better in some cases. \"Works but is going to cause a problem when XYZ happens\" may be more accurate. I was actually told by a PM once \"how about you just say 'I told you so' after something breaks instead of always complaining about wanting to fix stuff?\" which was truly bizarre. My foodservice days had \"if you got time to lean, you got time to clean\" drilled in to me, but it's not always the same in software. What constitutes \"clean\" may be the core issue. reply rkachowski 12 hours agoprevI agree this righteous \"one\" stuff is toxic. There is something very liberating and wholesome about a senior developer being able to discuss the mistakes and fuckups they've made. It's not just a learning opportunity, it also demonstrates a culture of openness and works against imposter syndrome. A perfectionist attitude stands in opposition to this - just try harder and stop making mistakes. There's nothing to learn, just more individual effort to apply. reply evrydayhustling 11 minutes agoparentIf you can't find things to criticize about yourself, it means you aren't improving. Sharing these critiques helps others both learn from your mistakes, and that it's ok to learn from their own. reply otherme123 3 hours agoparentprevI immediately recall at least two big fuckups that the \"one\" (two different persons) didn't acknowledge, even if that meant to lose a good client, and I would guess bad mouthing. Once I ate a small fuckup from one of my juniors (we put a wrong software version in a report), and instead of fixing it with the client by simply saying \"we had a mistake, here is the fixed report\", my superior only could think about \"could we hide this, and keep the face that we are perfect?\". Of course, this same person takes any mistake from other people as an opportunity to ask for discounts, compensations and freebies. reply tetha 10 hours agoparentprevMost of us are hopefully also improving at what they do. Our config management has a bunch of stuff written by me that's perfectly functional and works overall. But after a year or two working with the tool, I would call it pretty bad quality for various reasons. But that's fine. If I have a reason to change it, I'll clean it up. Until then, it stays around as an example to show some bad practices and why other approaches are better. reply Sammi 8 hours agoprevI regularly have codevs who say similar things to \"why don't you just not make that mistake?\" and \"why not just doinstead?\". I've taken to responding with: \"Because you probably have more IQ points than me. I have fewer IQ points than you, so therefore I must do dumber and simpler things than you.\" It's made some of their faces turn red in embarrassment, as they finally realized what unreflected belittling little dorks they were being. reply dahart 3 hours agoparentI’m a bit allergic to questions that begin “Why not just …”. With those three words, I can usually already tell what’s coming before they tell me their idea, not because the suggestion is crazy complex but because almost invariably it’s the first thing that popped into the person’s head, because it’s the most obvious thing one could do, and has already been carefully considered or already tried. The question itself isn’t bad, it’s the underlying assumptions that makes it irritating and/or insulting, the assumptions that 1) their idea is so easy, and 2) you didn’t think of this obvious & easy idea. And when I’m on the other end of this asking someone else why they didn’t do it in some way that looks obvious to me, I try hard to avoid “why not just”. Sometimes I will ask “I assume you didn’t X for some reason?” But maybe the best is to ask why nicely without making any suggestions. When the question comes because they were lacking context, this can sometimes be headed off at the pass by announcing why the most obvious things didn’t work before explaining what you did do, or by highlighting the confounding requirements or problematic inputs. If the question comes from already-committed code, the goal would be to have commit or MR code comments that prevent post-facto second-guessing. Sometimes it’s useful to accept the question without retort and just answer it directly, by explaining that their idea was tried and didn’t work, and what the reasons are, and ask if they’d like to share any other ideas, earnestly not sarcastically. :P If the suggestion really was something I didn’t think of and seems like it might solve a problem, which might be rare but does happen to me on occasion, then I do like to tell them it’s a good idea and recruit them to help me implement it. In that case, pushing back on their assumptions or tone is tempting to me, but I will try to let it roll off and just take the feedback and be momentarily embarrassed. reply slowking2 5 hours agoparentprevBarring it being a joke, the first question is unhelpful and likely a jerk move. Everyone makes mistakes sometimes. The second question seems like the type of feedback that would usually be fine. People's skills and knowledge don't always overlap. What is crazy complex for A may not be for B and what is crazy complex for B may not be for A! And that doesn't have to have anything do with A or B being smarter. A might not know SQL and B might not know pandas. But sometimes it really does make sense to move some code from SQL to pandas or vice-versa (assume for the moment that both SQL and pandas are already in the tech stack). Some people find it simple to write in object oriented style and others in a functional style. What makes more sense to do is not always obvious. So the question could be a good one. If the suggestion is bad, explain why it's bad. If the suggestion is good, maybe consider if it's worth doing at current point. If it's somewhere in the middle or there's no time, acknowledge and move on. reply viraptor 5 hours agoparentprevVery nice application of grug brain https://grugbrain.dev/ > given choice between complexity or one on one against t-rex, grug take t-rex: at least grug see t-rex reply AgentOrange1234 5 hours agorootparentThanks. That was a delightful read. reply dmichulke 7 hours agoparentprevAn alternative is to plainly agree with them: \"I know, that was dumb, wasn't it?\" or \"You're right, I probably shouldn't\"or just \"I'll think about it\" This means they can't shame / guilt you for doing something suboptimal and I believe this is often their goal - establish superiority by inducing guilt or shame. It's like opting to not play the game with them. And usually, that's the winning move. reply npsimons 1 hour agorootparent> \"I'll think about it\" Unless you're being totally earnest, this one reads like \"I'll take it under advisement\"[0] [0] - https://getyarn.io/yarn-clip/ed82da6b-db48-49aa-a105-f190e63... reply kortilla 6 hours agorootparentprevThis gives them what they are looking for though (public praise). It quickly ends the interaction but doesn’t prevent future ones. reply juliushuijnk 7 hours agoparentprevIf it is said in ignorance, there is no need to view it as an attack that you need to make them pay for with embarrassment. reply OJFord 5 hours agoparentprevThat seems really combative to me, like you're just escalating the situation. I hope I phrase it better than that (but you're presumably paraphrasing so it's a hard thing to self-assess), but I point out mistakes and suggest (what I think is) a better alternative all the time in code review, that's the point of it? If you replied to me like that I'd probably just disengage, not bother to look at your PRs any more. reply tanelpoder 8 hours agoprevI can’t find the blog/article anymore, but its message was that “when you see something suboptimal in the code, do not assume incompetence”. As people who wrote the code may have had tight deadlines, different priorities set for them and other factors that may have prevented the developers from “doing the right thing” immediately… reply pornel 8 hours agoparentEven if the code has been absolutely perfect at the time of writing, growth of the codebase, and changing requirements could make it bad. e.g. if you have 10 items to store, then a simple file may be a pragmatic choice. When it grows to 10,000 items you may need a database. But if you started with a database for just 10 items, people would complain it's overengineered. If you have 2 classes, an if/else can do, but at 20 you need some Factory pattern, which would be an architecture astronautics if done from the start. And when you try to anticipate such growth, you'll create overcomplicated code whenever you guess wrong. A continuously developed project will systematically keep outgrowing itself. reply NBJack 2 hours agorootparentOr just that errant refactoring automation that moved a proverbial (or literal) bit out of place. It's good to learn from our mistakes. But if there's one thing I have learned from working across teams and orgs, it is that basically everyone works with imperfect information. And oftentimes, you just have to make do, write the code, and try to make room to course correct in the future. reply lambdaxyzw 8 hours agoparentprevThere's also chesterton fence - maybe this dumb thing in code actually was important some time ago. Or, even worse, maybe it is still an important for a rare edge case and you just don't see how, yet. reply npsimons 1 hour agorootparent> you just don't see how Always, always assume this is the case - it might frustrate you to no end, but until you have conclusive evidence something is \"wrong\", it's best to ignore it and toodle along with whatever you're supposed to be working on (I always encounter these head scratchers when working on legacy code, my tasking being something unrelated). reply YZF 15 minutes agorootparentI would soften this to always consider that possibility. The probability that something is just wrong, or the requirements have evolved in a way that makes it wrong, is probably not that different than the probability the original author had a very strong reason for doing something a certain way and that really did survive the test of time. Considering everything preexisting to be perfect is overly conservative and almost certainly wrong in most real world software engineering scenarios. The history of software engineering is full of examples of things that got replaced with much better things because the original was just not good enough. reply kqr 4 hours agorootparentprevThe Chesterton fence in my book is only about something that is still meaningful only in non-obvious ways. In evolutionary theory I believe \"used to have a sensible explanation but no longer does due to changing circumstances\" is known as discordance. reply MrJohz 3 hours agorootparentThe fence is about all things that you might want to change, whether or not they're still useful. Both of the characters in Chesterton's parable are reformers, and the assumption is still that the goal should be reform. The question is more about the motivation for reform. The first character wishes to remove the fence because they don't see a good reason for it. The second takes a different approach: first show that it isn't necessary, then remove it. Plenty of things are made useless over time. But there are also lots of things that look useless but aren't. We don't know, a priori, which is which, hence the need to be cautious. (That said, I think there are also cases where ignoring Chesterton, removing the fence, and seeing what will happen is the best option. It just requires good planning and good testing so that you can be confident that a bull doesn't suddenly appear out of nowhere!) reply brazzy 6 hours agorootparentprevThat as well is \"not doing the right thing\", namely adding a comment that explains the need for it. reply 1penny42cents 6 hours agoparentprevfwiw I’ve written about that here: https://camhashemi.com/posts/building-through-uncertainty/ reply philbo 11 hours agoprevI've received a few nasty comments in response to blog posts, both here and on reddit. When it happens I update the post to link to the nasty comment, without judgement, just to shine a light on it. Usually nothing happens but sometimes it steers the discussion to a healthier place. reply ChrisMarshallNY 8 hours agoparentI've received quite a few. Some, probably deserved; most, likely not. In a few cases, it was -sort of- deserved, but not helpful, or directly harmful to the community. Saying the right thing, in the wrong way, is still saying the wrong thing. > When it happens I update the post to link to the nasty comment, without judgement, just to shine a light on it. Not a bad idea. Not always possible, if the comment gets dead'nd. In any case, I do not respond in kind. I'm quite capable of it (recovering troll), but I won't go there. It's not being a \"snob.\" It's just that I've learned that gasoline is an ineffective fire suppressant. If I'm wrong, I've learned to promptly admit it; in the same venue as the mistake (a pet peeve is private apologies for public attacks). There is a line though. I feel that I do really good work. I've been doing this for a long time (like, 40 years), and have learned quite a bit, in that time. I've also worked some pretty tough rooms, and for folks that wouldn't accept crap, so I have learned to habitually do decent work. My general policy is to avoid casting judgment onto others in public. It doesn't help; even if I'm right (not always the case). But if we're working together, or I am using your stuff, then it might be a different story. I have had people savagely attack me, because I won’t accept garbage. Guilty as charged, but I don’t go Torvalds on them. I just tell them that their work is not acceptable to me, in a respectful manner, if possible. Nevertheless, I have found that I can always improve, and learn new stuff; sometimes, from the most unexpected places, and being open to these lessons is basic good policy. I become right, by being wrong, and learning otherwise. \"Good judgment comes from experience. Experience comes from bad judgment.\" reply kortilla 6 hours agorootparentSaying that you won’t accept “garbage” is a pretty toxic way of phrasing things. If you say that around the people you work with (especially junior people), they have a habit of interpreting any negative feedback on code as you calling it garbage. Then they start avoiding feedback from you when possible and then everyone suffers. I’ve seen this pattern play out over and over. It makes what could be a great lead senior engineer just a lone wolf only a few people want to work with. > feel that I do really good work. I've been doing this for a long time (like, 40 years), Never substitute tenure for competence, particularly when you are convincing yourself that you do good work. One of the biggest red flags in hiring is when people defer to tenure as a reason for anything technical. It is very easy to do things wrong or poorly for a long time without even knowing it. So falling into the trap of “I’ve been doing this 40 years and people have paid me for it, so it must be good” is a death sentence. reply lelanthran 26 minutes agorootparent> One of the biggest red flags in hiring is when people defer to tenure as a reason for anything technical. You know what the great thing about years of experience is? It's usually \"I hold this $TECHNICAL opinion because it is the result of careful refinement over 20 years.\" In some cases it is \"I formed this $TECHNICAL opinion 20 years ago, and haven't come across enough evidence to change my mind\" It is VERY RARELY \"I formed this $TECHNICAL opinion 20 years ago and dismissed any evidence to the contrary over the last 20 years, while still managing to retain gainful employment\". TBH, if you are seeing people from the third group often enough to use it as a heuristic, chances are it's a poor (or poorly correlated) heuristic that you haven't yet seen for the poor quality it is. IOW, you are holding an opinion based on your experience, about others who hold an opinion based on their experience. Holding on to this opinion might even make you part of that third group I listed above. Very ironic. reply ChrisMarshallNY 5 hours agorootparentprevWell, a stint working for one of the top-quality companies in the world taught me to develop a fairly thick skin. The Japanese are not gentle, when criticizing each other, and I had the honor to be considered worthy of Japanese-level evaluation. But be that as it may, I don’t “harsh out” on people; especially in public venues. I may think \"garbage,\" but I'm much more likely to say \"this won't fit into the framework in that form.\" It’s my experience that many of today’s folks get very nasty (and personal), when confronted with even mild rebuke. I can understand why Torvalds goes nuclear, although I won’t go there, myself (I consider it unprofessional). I remember once, denying a patch (SVN), because the \"fix\" would have addressed the submitter's particular issue, but also would have broken the functionality for, literally, hundreds of others. I told them that it was a good idea, but I couldn't implement it, as provided, because of that, and suggested that we figure out some changes. The response was a long, public excoriation, complete with genealogical evaluations of my ancestry, back to the Pliocene. I decided that, even though they had a point, and we probably could have figured out how to give them what they wanted, after some give-and-take, it wasn't really possible, because of their attitude. I did end up applying part of their request; just not the part that broke it for everyone else (I did credit them in the comments). I blocked them, and we have never worked together since. I remember a post here, some time back, where a fairly talented young chap, was complaining about not being made a core Linux Kernel contributor, simply because he submitted a good PR. If we want to be above-average, then we need to be willing to put ourselves into positions, where we will get criticized; and, quite frequently, the ones doing the criticism are far from gentle. It's been my experience that folks at the top of their game, frequently fail to accomodate those that are not at their level. They aren't always right, but they are often worth listening to, anyway, and we don't do ourselves any favors, by reacting badly. There is definitely something to be said for earning our stripes. reply xandrius 5 hours agorootparentprevGot the same vibes when reading their comment, basically self identifying as \"the one\" without admitting it. The funny thing is that if someone has been doing IT for 40 years, I'd expect them to be generally aware that they might be good at big picture stuff but less so on minute things, as technology, philosophies and approaches change every few years but the general concepts stay the same. reply ChrisMarshallNY 5 hours agorootparentCool. I didn’t mean it that way, but if you wish to perceive it as such, it’s a free country. reply rulalala 9 hours agoparentprevThis is great. I wish all humans could think that way. reply avensec 4 hours agoprevI read this in 2018 and I'm glad to see it posted again. It was one of those posts that made me ask myself a question. I can't remove the absolutism or extremes, so how can I create a filter for when I hear these conversations/people? I have my model, but... What strategies do people employ here? reply sdwr 4 hours agoparentThese people are claiming emotional territory that doesn't belong to them. Usually, they have precalculated that they can \"get away\" with it, which means they see you as weak. Three options: - Give up, cede the ground, and move on with your life. The less sleep you lose over how unfair it is, the better. - Confront them head-on. They are ready for a fight, but their position is inherently unreasonable. The less you get dragged into their headspace, the more you \"win\". - Come down from above. Bring social proof that they are wrong into their space. In the OP, this would be programmers who mutually respect each other's work, and are productive without nitpicking. reply BillFranklin 11 hours agoprevI don't disagree that some engineers have bad attitudes! Everyone is capable of writing bad code, and there is something to be said for the argument that all code is bad and a liability. This article is an interesting companion to \"No more pink mustache\" [1] wherein Lyft is described as \"broken at a scale that is hard to believe\" - often the explanation for quality is the org, not the human in the chair :-) [1] https://rachelbythebay.com/w/2020/02/29/poof/ reply OliverJones 1 hour agoprevDang, Rachel, you must have had a rough day at the orifice that day. Condolences. reply npsimons 1 hour agoprevTo play Devil's advocate, I'd like to point out that many of these egotistical assholes are \"trying\" to \"help\" - by \"trying\" I mean their communication skills are poor, and by \"help\" I mean they are trying to encourage people to improve. I know, I've been one of these assholes. What I've tried to do to be less of an asshole is leverage my empathy and hone my communication skills. But I also recognize the anger and frustration with things that seem like they should be no-brainers - and recognize that what we despise in ourselves is often what we attack others for. Nobody's perfect, but there are ways people can get better. Or just take advantage of tools that can help automatically catch these sorts of errors. This is precisely why people create regression tests and set them up to run in the CI/CD, to reject changes that break them. As for being less of an asshole, no shortcuts with that - just exercise your empathy, put yourself in their shoes, take a deep breath, then go to bed. Tomorrow morning, if you still feel the need to pen a blog post, maybe write out a technical solution without assigning blame. reply h2odragon 7 hours agoprevWhen someone offers advice about \"it might have done better\"; it not always an assualt on you, an insult of your abilities, and so on. The person offering the advice may be an idjit, just at dealing with others or maybe even completely. It's OK if the some of rest of the world fails to agree with you. People voicing contrary opinions, failing to agree with you, etc; doesn't threaten you in any way. reply philk10 5 hours agoprev\"You shouldn't hire bad programmers\" - I did work at a company where the CEO did say that he wished they had programmers who didn't write bugs He did also not want me to do any testing as \"the customers were better at finding bugs than we are\" reply lelanthran 23 minutes agoparent> He did also not want me to do any testing as \"the customers were better at finding bugs than we are\" He's not wrong about that quoted bit, though :-/ Paying customers are uncannily better at finding bugs than anyone! [Note: I am not advocating for removing testing from your process.] reply surfingdino 10 hours agoprevI could not disagree with the point made, but there is a flip side to this--being able to take feedback. reply jasonlotito 8 hours agoparentMost people can take feedback and apply it. I find some people just give feedback poorly and equate that to others not taking feedback. These people usually think there preferred feedback is the best way so therefore everyone should feel the same way and if they don’t the other person needs to change. They are of course wrong. But if you tell them this, they demonstrate why I use the word “most” in the opening sentence. reply npsimons 1 hour agorootparent> I find some people just give feedback poorly and equate that to others not taking feedback. Too true! I find it helps if I think of it as an engineering problem: \"what can I say that will make these people not commit the same error again?\" A bit dehumanizing and manipulative, but can be super effective at shifting away from blaming and belittling (almost never works), and towards mentoring. reply red_admiral 10 hours agoprev(2018), but still worth reading. reply akoboldfrying 11 hours agoprev>malloc(\"HTTP\") I'm reminded of a Simpsons episode where Homer tries to assemble a barbecue by pressing a brick and a piece of pipe together. reply pavlov 11 hours agoparentBecause I am the one, my custom malloc checks a given size against the list of known constant string pointers and if it matches, it calls strdup instead. So easy to program defensively in C. reply lambdaxyzw 8 hours agorootparentI guess you could check if the pointer points inside a data section of your program, and if yes, call strlen on it. It's all so easy! reply jrockway 10 hours agoparentprev\"That's one fine looking barbeque pit.\" reply auggierose 8 hours agoprevAnd how can this be? Because he is the Kwisatz Haderach! reply cushychicken 5 hours agoparentThe Mahdi is too humble to admit he is the Mahdi! reply dclowd9901 12 hours agoprev [–] Why is this person feeding trolls? Don’t we all know people who think they’re infallible are full of shit anyway and just ignore them? reply bjackman 11 hours agoparentYou can't just divide the world into people who believe fallacies and people who don't. They are rooted in fundamental cognitive biases, everyone falls for the \"just try harder\"/\"just be better\"/\"just don't put any bugs in the code\"/\"just have excellent test coverage\" thing sometimes. Certainly all the most experienced engineers I work with, who should definitely know better, still haven't fully internalised the fact that all code is broken. reply 8n4vidtmkvmk 10 hours agorootparentThey really should. I've been programming over 20 years and don't think I've seen a program without at least a few defects. A couple have come close, those are impressive, but bugs always happen. reply gravescale 10 hours agorootparentShow me a program that has no bugs and I'll show you a program that does nothing at all! Even most Hello Worlds don't check the pipe was written to correctly. And now we get into \"well actually that's technically not a bug because it is not in the spec\" and if we're finding the need to split that hair, we can hardly be talking about some mythically perfect software. reply npsimons 1 hour agorootparent> Show me a program that has no bugs and I'll show you a program that does nothing at all! \"Every program has at least one bug and can be shortened by at least one instruction -- from which, by induction, one can deduce that every program can be reduced to one instruction which doesn't work.\" reply richardw 10 hours agorootparentprevYou’re not wrong but Knuth’s programs have very few bugs and do a shitload. “The reward for coding errors found in Knuth's TeX and Metafont programs (as distinguished from errors in Knuth's books) followed an audacious scheme inspired by the wheat and chessboard problem,[10] starting at $2.56, and doubling every year until it reached $327.68.” https://en.m.wikipedia.org/wiki/Knuth_reward_check reply epr 8 hours agorootparentprev> Even most Hello Worlds don't check the pipe was written to correctly. The word for this is not \"perfect\", but \"overengineered\" reply re-thc 9 hours agorootparentprev> Show me a program that has no bugs They're features! reply throwaway4233 9 hours agoparentprev [–] I do not believe that most of these `perfectionists` are trolls. Some have just very bad experiences either in their career or childhood that make them feel that making mistakes is not normal. I was a `perfectionist` for a while due to certain bad experiences at work, and it was only through the help of really good teammates that I was able to slowly get rid of it. And that required pointing out things like what the author has done in their blog post and once someone sees that mistakes are things that anyone could make, they get more comfortable the concept. The harder part is understanding why a perfectionist is so, and then not getting frustrated while you try to help them improve. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post addresses common criticisms in software development, including poor code quality, hiring issues, and insufficient testing.",
      "The author emphasizes that mistakes are inevitable, even for experienced developers, and highlights the collaborative nature of programming.",
      "The post concludes with a sarcastic remark, suggesting critics should declare their superiority and let others handle practical challenges."
    ],
    "commentSummary": [
      "The discussion highlights the balance between striving for perfect code and managing technical debt in team environments.",
      "Emphasizes the importance of constructive feedback, understanding the context behind coding decisions, and maintaining professionalism.",
      "Advocates for a culture of openness, continuous improvement, and empathy, acknowledging the inevitability of bugs and the impracticality of perfection."
    ],
    "points": 161,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1716606788
  },
  {
    "id": 40472146,
    "title": "JetBrains Unveils RustRover IDE with AI Assistance for Rust Development",
    "originLink": "https://www.infoworld.com/article/3715317/jetbrains-releases-rustrover-ide-for-rust-development.html",
    "originBody": "Home Software Development Rust JetBrains releases RustRover IDE for Rust development RustRover provides a built-in toolchain for testing, running, debugging, and analyzing Rust code, with optional AI assistance via plugin and subscription. By Paul Krill Editor at Large, InfoWorldMay 24, 2024 2:00 am PDT Gratisography (CC0) JetBrains has released RustRover, a dedicated IDE for the Rust programming language that combines an integrated Rust toolchain with support for AI assistance through an optional plugin and subscription. Announced May 21, RustRover is positioned to simplify the Rust coding experience while “unlocking the language’s full potential,” JetBrains said. Capabilities include real-time feedback, code suggestions, simplified toolchain management, and team collaboration. JetBrains also rolled out a new licensing model. RustRover is available through a paid commercial license or a free non-commercial license, the latter for individuals using RustRover for non-commercial purposes. Previously, JetBrains offered IntelliJ Rust, an open-source Rust plugin for IntelliJ IDEs. With RustRover the company aims to provide a dedicated product with enhanced functionality for the growing Rust developer community. JetBrains also has been previewing a multi-language editor and IDE, called JetBrains Fleet, that supports Rust development. Key features of RustRover include: Rust toolchain support including support for the Rust compiler. Version control system integration, with GitHub and Git support. Users are able to streamline collaboration and control for teams. Error detection and real-time feedback for debugging. Support for front-end technologies and databases. Permissive completion and parsing including smart code suggestions even in unconventional contexts. Unit testing integration for testing, rerunning failed tests, and resolving errors. RustRover has natural similarities to JetBrains’ other language-specific IDEs including PyCharm for Python, GoLand for Go, and RubyMine for Ruby. All support the JetBrains AI Assistant plugin, which provides AI-powered code suggestions, code explanations, and code chat through a JetBrains AI subscription. Next read this: Why companies are leaving the cloud 5 easy ways to run an LLM locally Coding with AI: Tips and best practices from developers Meet Zig: The modern alternative to C What is generative AI? Artificial intelligence that creates The best open source software of 2023 Related: Rust Integrated Development Environments Development Tools Software Development Paul Krill is an editor at large at InfoWorld, whose coverage focuses on application development. Follow Copyright © 2024 IDG Communications, Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=40472146",
    "commentBody": "JetBrains releases RustRover IDE for Rust development (infoworld.com)161 points by thunderbong 16 hours agohidepastfavorite85 comments armchairhacker 16 hours agoShould probably link to the official announcement: https://blog.jetbrains.com/rust/2024/05/21/rustrover-is-rele.... Notably, there's a \"free for non-commercial use\" license that grants access to the entire RustRover IDE. There's also controversy because JetBrains removed JavaScript/TypeScript support from RustRover right before this release, admittedly because they want to keep these features gated behind a paid subscription in IntelliJ. That wouldn't normally be an issue, except the IntelliJ Rust plugin is deprecated and doesn't have the support that RustRover does; so there's currently no single IDE that fully supports Rust + web projects, even though JetBrains could easily make one, and even though people would be willing to pay for it. The end of the blog mentions \"IntelliJ IDEA Ultimate will include support for the Rust plugin for free\" so hopefully they are addressing this; if they re-package Rust support into into IntelliJ but only as a paid plugin I think that would make most people happy. reply mastazi 15 hours agoparentI wish they had a fully modular license where you just tick boxes (each box is a language) and the price changes proportionally, or at least there should be some intermediate options between the single-language IDEs and IDEA Ultimate, something like a \"two language license\" or \"three language license\". Edit: is Fleet going to be that way perhaps? Not sure if pricing has been revealed yet. reply euW3EeBe 15 hours agorootparentI think Fleet aims to do this, its been in preview for 3 years now so I'm not sure what's left for them to actually release the thing. I'm looking forward to only having one IDE installed and loading/unloading language plugins as required, instead of having multiple heavy IDEs. On top of that, the architecture of Fleet seems more amenable to remote development, like what VSCode has with their SSH plugin. In my experience the remote development feature in its current state has been a buggy mess. reply saghm 15 hours agorootparent> I think Fleet aims to do this, its been in preview for 3 years now so I'm not sure what's left for them to actually release the thing I know this isn't _technically_ something that needs to happen before a stable release, but their Vim plugin is pretty abysmal and I'm fairly sure that I'm not the only one who that would be a deal-breaker for. reply hboon 14 hours agorootparentI find their IdeaVim plugin basic, but it complements the IDE very well. Felt it’s best compare to others like Vs Code’s. What do you not like about it? reply hwbunny 12 hours agorootparentprevWho uses vim in 2024? :D reply OJFord 8 hours agorootparentTonnes of people, and what does the year have to do with it? Both vim and neovim are actively maintained, including new feature development. reply Faaak 11 hours agorootparentprevI use vim bindings in vscode. You should try it! reply JackMorgan 7 hours agorootparentprevAnyone who wants one of the best ways to manipulate text. reply mekster 14 hours agorootparentprevWhat's the state of Fleet at this moment? I haven't been following it since I've tried a few years ago and looked it was still at its early stage then. Is JetBrains aiming to replace IntelliJ with it at some point or it would be an offering to complement or compete with the current offerings? reply yardstick 15 hours agorootparentprev> I'm looking forward to only having one IDE installed and loading/unloading language plugins as required, instead of having multiple heavy IDEs. Eclipse has been doing this for decades. But its eclipse so it’s not as polished as IntelliJ etc. reply mablopoule 9 hours agorootparentprevIn line with this idea, at least the 'datagrip' part of Jetbrain can sometime be added as a paid plugin. In my case, I've worked for a long time with the \"WebStorm + Datagrip plugin\" combo when doing fullstack JS. reply epcoa 15 hours agoparentprev> except the IntelliJ Rust plugin is deprecated This is incorrect. https://plugins.jetbrains.com/plugin/22407-rust The current Rust plugin is not deprecated. Perhaps you are confusing it with the old plugin (https://github.com/intellij-rust/intellij-rust?ref=chillfish...) They're still making releases (latest 4 days ago) and there is no mention that the plugin is deprecated. In fact: https://blog.jetbrains.com/rust/2024/05/21/rustrover-is-rele... \"IntelliJ IDEA Ultimate will include support for the Rust plugin for free. However, to use the plugin in CLion, you’ll need a commercial RustRover license or buy it separately. \" > so there's currently no single IDE that fully supports Rust + web projects Wrong. Latest IntelliJ with Rust plugin and whatever web plugins you want works fine. reply iamkonstantin 14 hours agorootparentIn addition, the Fleet editor supports this use case https://www.jetbrains.com/fleet/ reply veber-alex 13 hours agorootparentFleet uses rust-analyzer for Rust support and not the jetbrains rust plugin that is used in RustRover and Intellij. Considering the state Fleet is in you are much better off with VSCode for Rust + web dev support. reply lars_francke 12 hours agorootparentprevI can attest to that. Am using Ultimate and everything works and gets regular updates. That includes Rust and web development. I have been a happy paying customer for 10+ years. reply hwbunny 12 hours agoparentprevYou can always beta test their cutting edge releases and use those versions for free for 1 month. Don't whine about Jetbrains, they give you several options if you don't have the money to use their products. reply barkingcat 14 hours agoparentprevwhy is this an issue? if you want to use the software in this specialized combination, purchase it. if you want to use the free/community editions, go ahead. if you are a student or are using it for community/non-profit purposes JetBrains also has a pretty good licensing for students/education/non-profits. aside from depriving a software maker from charging for their labour, while making profit yourself, what does this complaint actually achieve? reply ska 14 hours agorootparentThis seems to misrepresent the parent. Wasn’t the parent claim that you can’t pay for a seamless version even if you want to? They note that if a paid plugin was to be offered it would work for most ... I don’t know if they got the facts right but didn’t seem to be complaining about lack of free. reply barkingcat 13 hours agorootparentThe parent post is mistaken there is a perfectly fine rust plugin available with the intellij idea ultimate - provided you pay for intellij ultimate (and / or get it through education/nonprofit licensing) https://news.ycombinator.com/item?id=40472463 https://plugins.jetbrains.com/plugin/22407-rust reply ska 3 hours agorootparentThat’s good to know, but doesn't change my comment substantively. reply chrisandchris 10 hours agorootparentprevI have a C# backend with VueJS SPA using this backend. I just use Rider for all my development, having the backend and UI in the same project (in a monorepo). Having two IDEs would just make things less fluent IMHO. reply calderwoodra 15 hours agoparentprevAre you mixing .rs and .js(x)/.ts(x) files in the same project? Jetbrains doesn't support multiple languages in any version of their IDE well AFAIK. I daily drive PyCharm and Webstorm in parallel. reply avodonosov 15 hours agorootparentWorked fine for me with IntelliJ Ultimate - java, js, html, sql, etc in the same project reply vbezhenar 14 hours agorootparentCan you use npm for your frontend and maven for your backend, all in the same project? I think you can actually try to do it with separate module for each \"subproject\". But when I tried it, it was kind of buggy and very fragile. So in the end it was easier to just use two separate projects. reply lolinder 14 hours agorootparentI have a personal project that uses npm for my frontend and Gradle for my backend and it just works in IntelliJ Ultimate. One module per subproject. reply steveklabnik 14 hours agorootparentprevA good example of projects that do this are things built with Tauri. I myself work on a web project with a Rust backend and a TS front end, all in the same repo. That said I use VS: Code, not Jetbrains stuff. It handles this case just fine. reply brainless 15 hours agorootparentprevI think they are rethinking this in their new project, Fleet. But VS Code is so good at this that it sets a very high bar. reply MeImCounting 14 hours agorootparentPrivacy concerns mean Ill not be using VS Code any time soon. JetBrains gets my business for that reason alone reply yett 13 hours agorootparentprevI have no problems mixing .jsx and .cs files in the same Rider project reply mlinhares 15 hours agorootparentprevIt does, IntelliJ ultimate has multiple language support for quite a while now. reply 7e 14 hours agorootparentNo C++. reply pjmlp 7 hours agorootparentYeah, contrary to the full open source Eclipse and Netbeans, JetBrains wants people to pay for InteliJ and Clion, when doing JNI development. Android Studio has C++ support, because Google lacked a CDT counterpart when they went InteliJ, and had to do an agreement with JetBrains to include the support. reply veber-alex 13 hours agoprevFYI the free version of RustRover has forced telemetry that cannot be disabled. \"It’s also important to note that if you’re using a non-commercial license, you cannot opt out of the collection of anonymous usage statistics. This is similar to our Early Access Program (where statistics is opt-out) and in compliance with our Privacy Policy\" reply leoh 10 hours agoparent/etc/hosts reply ysleepy 6 hours agoparentprevI don't think that can be GDPR compliant, personal data cannot be used as a form of or alternative to payment afaik. reply igor_akhmetov 5 hours agorootparentIt's not personal, it's anonymous. reply ysleepy 1 hour agorootparentI assume the telemetry data contains some sort of installation identifier and without hard evidence I will assume the IP address as well. reply rethab 15 hours agoprevI like the jetbrains products a lot and have been using IntelliJ as my main IDE for years. What I don’t like is their pricing with regards to multiple products. I use IntelliJ 95% of the time and pay for it. Sometimes, I’m writing some Python or C and would like to have a decent IDE as well, but then I immediately need to purchase the „all products pack“. I wish there was a cheaper option for occasional use of other IDEs. Right now I use the PyCharm community edition, but this feels a bit silly :) Hopefully Fleet will solve this. reply leoh 10 hours agoparentI’m baffled by this. Many folks spend thousands on hardware. JetBrains makes such exceptional tools, I don’t understand how a few hundred bucks for the complete product pack is unreasonable (with understandable exceptions for folks with limited income). This stuff isn’t free, it surprises me when fellow engineers are averse towards paying for great software that helps us build software. reply joshstrange 4 hours agorootparentThe price is so crazy cheap I don't get it. I pay $173/yr for the all products pack ($289 first year, $231 second year, $173 year 3+) and it's some of the best value/ROI. Even if I though VSC was on the same level (I don't) the configuration and plugin management hell alone is worth $173/yr (No, I don't want to hear about how easy you think it is). If you're using it professionally then $173/yr is nothing, hell, I'd probably pay $300/yr and not gripe because again, I get so much value out of these tools. We are talking about a tiny fraction of a percentage of the income I make while using these tools. reply SpaghettiCthulu 39 minutes agorootparentThe configuration management in JetBrains' products is far worse than in VSC. On the JetBrains side we have a configuration system based on a tree of XML files, some of which contain system-specific configuration and should not be checked in to version control (sometimes in the same file as shared configuration!). On the VSC side we have a handful of json files where system-specific configuration can be stored in the user's home directory, allowing for easy version control of shared configuration. What killer configuration management feature do JetBrains' products offer that outweighs the aforementioned flaws? reply vbezhenar 14 hours agoparentprevYou can use Python plugin for Idea. It's essentially the same as PyCharm, at least theoretically. No way to use C or C#, though. Those languages are outliers for some reason. reply justinclift 12 hours agoparentprev> then I immediately need to purchase the „all products pack“. Most languages are available in IntelliJ via Plugins rather than needing to download the separate language specific product. reply poisonborz 9 hours agoparentprevIntelliJ Ultimate has all the plugins (\"other IDEs\"). So you could purchase that one time, and use the fallback version for smaller tasks. Granted, it might not have the latest features. reply akvadrako 2 hours agorootparentIt doesn't have them all, for example C. reply jatins 14 hours agoparentprevis Fleet still being developed? I had a feeling it was dead given that Jetbrains has been pretty mum about it since the announcement couple years ago reply modernerd 14 hours agorootparentThey’re still building it. There was a new version 3 days ago. https://blog.jetbrains.com/fleet/2024/05/fleet-1-35-is-out-i... reply euW3EeBe 16 hours agoprevThe VSCode extension for Rust is quite good too, probably the best IDE like support for any language I've tried. I'm more excited for JetBrains Fleet than another standalone IDE at this point. reply landonxjames 15 hours agoparentI like rust-analyzer in VSCode, but I've found that it does seem to struggle with large projects that have multiple nested Cargo workspaces. IntelliJ with the Rust plugin has handled that (admittedly niche) case better so far. I still prefer VSCode though so I just open each workspace in an individual window and it works more or less as expected. reply banish-m4 15 hours agoparentprevI gave up on JetBrains because most of their language plugins are broken most of the time. VSCode plugins have a much larger user base and tend to work, and I don't need 9 different products. It's similar to what happened to Atom. The main downside of VSCode is Electron leaks memory like crazy and will use north of 150 GiB of RAM. reply euW3EeBe 15 hours agorootparentThat's a crazy amount of memory usage, it might be an extension doing it? VSCode has a process explorer tool you can access under `Help>Open Process Explorer`. reply banish-m4 14 hours agorootparentYep. I should've last time but I will if/when it goes mad next time. I hadn't quit the process in about 2 weeks. reply gertop 15 hours agorootparentprevI'm going to make a wild guess that you use macosx and that 150GiB isn't usage but addressed space. Macs are notorious for returning nonsense memory figures with some applications. reply banish-m4 15 hours agorootparentNo, it was using 90+ GiB of swap. There is a difference between virtual address space and RSS. reply kaba0 12 hours agorootparentprevThat has not been my experience at all. VSCode is mostly just a code editor with plugins, while Intellij is a full-blown IDE. reply brainless 15 hours agoprevI have been using RustRover on Arch Linux as my daily IDE for a few months but then I switched to Zed when they landed their Linux support. RustRover is definitely more feature complete but one of my wishes is to start following the development of a large Rust project as a I use it daily. Zed, or Lapce, or Helix or similar editors fit that very well along with being very usable. Still, I would highly recommend RustRover (or PyCharm which I have used a lot over the years). reply swlkr 15 hours agoparentI'm the same way, it's one of the reasons I use helix, even for non rust dev reply sureglymop 11 hours agoprevIt's alright. Good to have this option but I'd stick with neovim. Nothing really beats treesitter + LSP + completion + formatting and still being in vim. Just the other day I wrote a treesitter query to format and highlight JavaScript within HTML within a string in rust. And all it took was a few lines of lua and 10 minutes. reply leoh 10 hours agoparent> it’s alright Are you kidding me? Even with nixos, the loe for nvim if you want to get anywhere near parity with RustRover or any other Jetbrains product ime is formidable if not impossible. Unless I can do stuff like automatically generate all arms for match statements, refactor across files, debugger support, have ease of setup (among other things), I simply cannot accept this nvim/treesitter supremacy nonsense. Can it do all that? I mean easily? With consistent keybindings between languages I don’t have to reconfigure for each language? Without installing like 20 plugins? reply sureglymop 9 hours agorootparentYes. Yes you can do all that with neovim (with under 20 plugins). Can you do it easily? If you're starting from 0 then no, probably not. Look, I am not knocking this IDE. I use these products too for certain tasks. Maybe my comparison wasn't fair because setting up neovim is basically programming lua and it is usually heavily personalized/individualized. What I really meant is, so far I don't see a killer feature yet that would get me to move from either neovim or also other editors with rust-analyzer to this. reply geggo98 4 hours agorootparentIf you have the time, could you please post the list of plugins you are using? reply pkulak 13 hours agoprevI’m locked in to JetBrains for JVM stuff, but the language server is so good, using Neovim is an absolute pleasure for Rust. reply sli 14 hours agoprevI can't justify the cost currently but I'm really going to miss the RustRover EAP. Lovely editor. reply juliusgeo 16 hours agoprevI’ve been using the pre-release version for a bit because I am a big fan of PyCharm. I found it very good besides the profiling interface. The “jump to source” option never works for some reason. Overall, though, provides a great experience. reply vishnugupta 15 hours agoprevHa, what coincidence. I installed the EAP (Early Access Program) edition only last week as I've been learning Rust. reply astkaasa 9 hours agoprevrust r over, brilliant name reply ilovecaching 15 hours agoprev [–] Reminder that neovim is completely free, you can modify the editor using Lua, it has a renaissance of new awesome plugins, LSP support for the official Rust LSP server, and as a system developer you can run it on a target device or quickly switch between a serial console and your editor using tmux. Plus, you can easily add new LLM integrations or wrap tools in about a half an hour of hacking. My neovim setup is easily 10x more productive than RustRover. reply mekster 14 hours agoparentMy NeoVim broke so often I came to realize a product that has 50 random people working on the core and various plugins is not going to make a stable product that would work for years. After all, there is no consistent testing as a whole product but it's a combination of \"works for me\" and after some months of originally setting the editor up, things start to break apart saying something is deprecated, something is incompatible with something else and it will start asking for newer NeoVim version which the distro obviously can't keep up with. It's like you're using a nightly build of a product or worse if things can't be fixed. Maybe someone could snapshot a stable state of NeoVim + some popular plugins and release as some sort of LTS and things might change but currently you'll need to spend time fixing frequently than use it for what you need it for. reply skydhash 14 hours agorootparentAre you using a neovim distro? I think the broken things only applies for automated setups. I use Vim instead of neovim, but my plugins set is pretty minimal. Most of my config is Vim-related, then a couple of options for the plugins. I instead use the terminal for most stuff. > Maybe someone could snapshot a stable state of NeoVim + some popular plugins and release as some sort of LTS Vim Plug (and maybe other plugins managers) can handle tags, branches, and commits references for versioning. reply leoh 10 hours agorootparentIt’s all fucked ime even with nixos from the pov of “just working.” Yes, it’s fun and cool and impressive if you get things all setup and dialed in. But if you care about just dropping in and not spending tons of time configuring your tools, imo JetBrains is just the way to go. reply skydhash 9 hours agorootparentAs a pragmatic programmer, one of the rules is Know thy tools. You don't really drop in. You learn the tools, and adapt them. The first commit of my dotfiles is more than 3 years ago. And I think the actual files are older. And that was me starting from scratch after years of Linux. Even with JetBrains IDEs, I usually spend an hour configuring stuff (never bothered to save the config, as I seldom use them). I tune my setup as I go. If I'm spending hours everyday with a software, I want it suited to my workflows. reply mekster 50 minutes agorootparentPeople can spend hours to initially set their environment up but the original post was about having it broken after a period of usage which involves spending unnecessary amount of time fixing than do what they want to do. reply skydhash 5 minutes agorootparentAnd that's a solved problem. First by not having a fragile setup, then doing backups to have some kind of rollback solutions. An IDE strong point is their cohesion and product support. But it's not magic and things can go haywire. It's like people coding with C or using Arch Linux vs using Java or Debian. The latter are stabler, but they're not silver bullets. pquki4 9 hours agorootparentprev> Even with JetBrains IDEs, I usually spend an hour configuring stuff That's you, not most people. Tons of people use VSCode every day for years and their user-level config is not more than a few lines. You can say what you did helped you personally and you found it worthwhile, but that is no indication of how that would apply to others. reply pquki4 9 hours agoparentprevNeed evidence of that 10x number. The bottleneck in programming is often how fast you can think, not how fast you can type or use auto completion, and definitely not some shortcuts. They cut down time but are not magic. Not to mention that different IDEs often have the same features, you just need to find them. reply jkrubin 13 hours agoparentprevTrue, but I don’t have the time or energy anymore to spend time configuring it. Jetbrains vim plugin is largely pretty good. reply leoh 10 hours agoparentprevFolks shilling for the nvim cult completely baffle me. YES it’s cool and impressive, but it also means essentially spending hours and hours of time managing your configuration if you want anything remotely close ime to RustRover or any other JetBrains product. Spending that time can be interesting, but personally, frankly, I’d rather be writing code than fucking around with lua wrappers for vimscript. Let’s be honest about this stuff and stop putting down an incredible firm that makes incredible tools. reply pjmlp 7 hours agoparentprevAs someone old enough to remember when VI was occasionally all caps, XEmacs was cool, no thanks. reply sureglymop 10 hours agoparentprevDo you have a link to your config? reply __MatrixMan__ 14 hours agoparentprev10x is quite a claim. Anyhow helix is nice for most of the reasons that neovim is, and also because you spend more time coding and less time fiddling with your setup. reply eviks 14 hours agorootparent\"Most\" by range, not by value, since plugins is the biggest reason, and they don't exist in helix reply kaba0 12 hours agoparentprevAnyone claiming bullshit like 10x difference has absolutely no basis in reality. Hell, there wouldn’t be an order of magnitude difference between your best setup vs literally touch typing on a foreign keyboard layout. Also, neovim is just a text/code editor, not an IDE even with plugins. reply gertop 15 hours agoparentprev [–] Reminder that RustRover is also free (for non commercial use) and doesn't require the user to read adv entire manual before they can use it. reply leoh 10 hours agorootparent [–] This. But also just pay for your tools if you can. It’s the kind/empathetic thing to do, honors the hard work of others, and helps elevate the craft. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "JetBrains has introduced RustRover, a specialized Integrated Development Environment (IDE) for Rust programming, launched on May 21, 2024.",
      "RustRover offers features like an integrated Rust toolchain, real-time feedback, code suggestions, simplified toolchain management, and team collaboration.",
      "Available under both paid commercial and free non-commercial licenses, RustRover supports the Rust compiler, version control, error detection, front-end technologies, databases, unit testing, and the JetBrains AI Assistant plugin."
    ],
    "commentSummary": [
      "JetBrains has introduced RustRover, a free-for-non-commercial-use IDE for Rust, but removed JavaScript/TypeScript support, making it exclusive to the paid IntelliJ subscription.",
      "This change has sparked controversy as no single IDE now fully supports both Rust and web projects, though JetBrains plans to offer Rust plugin support in IntelliJ IDEA Ultimate for free.",
      "Users are discussing various IDEs and text editors, including Vim, Neovim, VSCode, IntelliJ, and the upcoming Fleet IDE, highlighting trade-offs between customization, convenience, cost, and licensing."
    ],
    "points": 161,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1716603358
  },
  {
    "id": 40474236,
    "title": "Google's AI Rush Compromises Search Quality, Says John Gruber",
    "originLink": "https://daringfireball.net/linked/2024/05/24/publishing-ai-slop-is-a-choice",
    "originBody": "By John Gruber Archive The Talk Show Dithering Projects Contact Colophon Feeds / Social Sponsorship WorkOS provides enterprise-grade auth that can be integrated in minutes. Publishing AI Slop Is a Choice From a New York Times story by Nico Grant, under the headline “Google’s A.I. Search Errors Cause a Furor Online”: With each mishap, tech industry insiders have criticized the company for dropping the ball. But in interviews, financial analysts said Google needed to move quickly to keep up with its rivals, even if it meant growing pains. Google “doesn’t have a choice right now,” Thomas Monteiro, a Google analyst at Investing.com, said in an interview. “Companies need to move really fast, even if that includes skipping a few steps along the way. The user experience will just have to catch up.” That quote is insane. There’s no reason Google had to enable this feature now. None. If their search monopoly has been losing share recently, it’s not because of rivals who are serving up AI-generated slop. It’s because even before this, Google’s search results quality was slipping in obvious ways. This is just making it worse. They’ve turned Google Search — the crown jewel of the company, arguably the greatest consumer product ever made — into the butt of jokes. LLM-powered search results are a bauble. The trust Google has built with users over the last 25 years is the most valuable asset the company owns. Google most certainly does have a choice, and they’ve chosen to erode that trust just so they can avoid accusations that they’re “behind”. Behind is where you want to be when those who are ahead are publishing nonsense. ★ Friday, 24 May 2024 Display Preferences Copyright © 2002–2024 The Daring Fireball Company LLC.",
    "commentLink": "https://news.ycombinator.com/item?id=40474236",
    "commentBody": "Publishing AI Slop Is a Choice (daringfireball.net)158 points by mpweiher 7 hours agohidepastfavorite61 comments skilled 7 hours agoThe AI Overviews outrage is only the beginning. It has been 11 days since they made it public in the US, and it had been in experimentation for over a year before that. Already now, a lot of users will associate Google AI Overviews with nonsense; eat rocks, cook spaghetti with gasoline. Google also showed how easily these AI Overviews can be manipulated, since they use RAG. I think the Slopception is only going to get worse. Slop in. Slop out. And also, Google Research just happened to be sitting on AGREE[0]: > a learning-based framework that enables LLMs to provide accurate citations in their responses, making them more reliable and increasing user trust Which was published yesterday. I tried to submit it[1] but it didn’t get much love. [0]: https://research.google/blog/effective-large-language-model-... [1]: https://news.ycombinator.com/item?id=40469518 reply raydev 1 hour agoparent> Already now, a lot of users will associate Google AI Overviews with nonsense A lot of tech-minded folks who actively follow this stuff on Twitter et al make this association, but the average person was already defaulting to reading the first (likely incorrect) hit on Google and moving on with their day. I'm not sure they'll notice a dip in quality until it directly contradicts something they know about confidently. reply viking123 6 hours agoparentprevIt's going to get much much worse with all types of media tbh. reply skilled 6 hours agorootparentScam callers are going to have a field day with audio for sure. Maybe I am not thinking about it right but a lot of this does feel surreal and I just can’t see the light at the end this tunnel. No regulation, rapid development, no protective measures even for the very basic attack vectors. reply fullshark 6 hours agorootparentprevMaybe people will finally be willing to pay for it. reply tomrod 6 hours agoparentprevnext [–]I've never looked something up in an encyclopedia Does that include Wikipedia? I mean, the problem with encyclopedias is that they were small (despite physically taking up entire bookcases). I didn't use them much either, even back in prehistoric times when the internet wasn't invented. They tended to disappoint by giving a crappy, superficial overview of something that was only vaguely similar to the thing you wanted to know about. But I use wikipedia instead of doing an internet search a lot of the time. Or sometimes archive.org, for things probably in books. Cut out the middleman, go directly to where you know the information is. reply cfn 5 hours agorootparentFor me, having an encyclopedia at home was a huge help with my studies and even finding out what I wanted to do/be later in life. This was 40+ years ago and I also had or had access to a lot of other books but never found them disappointing at all. reply card_zero 5 hours agorootparentHuh. Maybe you studied one of the things encyclopedias tended to concentrate on? They seemed to like particular sorts of facts. Geography. Astronomy. Chemistry, to some extent. That's how I remember it, anyway. If your yen for knowledge was in tune with what the writers thought mattered, you were in luck. Even then, there were space constraints. reply skydhash 4 hours agorootparentI don't remember their titles (they're at my parents' house), but we have several. Some generic with a few plate drawings. Some illustrated. Some focused on one subject. It was not that you learned something particular, more that you know stuff existed. I now like the fact that they, as books, are frozen in time. And that their knowledge is well packaged and consistent. Unless you're a researcher, you don't need the latest. And as a child, the clear writings and the great illustrations was key to internalize the knowledge. reply Retr0id 5 hours agorootparentprevI meant physical encyclopedias, I use Wikipedia all the time. Although I reach ~half of those wikipedia articles via a google search query. reply cess11 6 hours agoparentprevI'd like to recommend that you start using libraries. You'll find that they often contain vastly more information than you'll find through web search, and you're already paying for them. reply II2II 6 hours agoparentprev> There's no \"going back to how things were before\" for me - there was no \"before\"! (...) Life will go on, but the demise of search engines is quite a terrifying prospect for me. Speaking as someone who was born long before the public Internet: you can never go back to how things were before. Similarly, you can never really stay with the way things are now. The world changes. Even if you stick to the ways of your lifetime, or go back to the ways prior to your lifetime, the ways have change simply because the context has changed. I'm not saying that we should jump the AI bandwagon. I'm just saying that we need to recognize the world is in constant flux. reply Retr0id 5 hours agorootparentTime marches onwards and people adapt, but that doesn't mean things are getting better. reply card_zero 4 hours agorootparentprevWell, how are we going to search the fluxing internet? reply lionkor 6 hours agoparentprevI'm convinced people like you and me will keep using search engines, and building them if all good ones shut down. reply animaomnium 5 hours agorootparentThere's a big number of things you can search for, but it's a finite number. There are certainly people interested in curating resources for their specific niche in exchange for curated resources in other niches, across the entire internet. Perhaps through a social web of trust. We need a new search engine resistant to sybil attacks. One that takes the social aspect of searching—connecting you with the relevant experts—into account. reply jprete 5 hours agorootparentIt's a challenging problem. The whole of Web 2.0 is built on lowering scaling costs as far as possible. The trust problem is itself very big - if someone makes the modern equivalent of the Yahoo directory, how do they establish trust that they're only incentivized by site quality? If it's a graph of trust, how does one handle the subtle changes in baseline expectations and norms that qualify as \"trustworthy\", when moving from one part of that graph to a distant part? If such a system goes massive, how does one prevent the member nodes from being overwhelmed by requests, or even from offers to contribute by people more interested in Internet karma than giving quality contributions? It's a tough nut to crack. reply bigallen 6 hours agoparentprevThis is so interesting. I have learned more from a years’s use of chatGPT than from two decades of googling. Yeah, AI has drawbacks. But AI has never displayed an ad to me and only once displayed a cookie banner, and that’s worth quite a bit to me reply fire_lake 5 hours agorootparent> I have learned more from a years’s use of chatGPT than from two decades of googling I simply don’t believe you. The human mind can only take in information so fast. reply jprete 5 hours agorootparentprevAside from the apples-to-oranges claim that a chatbots teaches you more than a search engine, it's extremely unlikely that AI will remain ad-free. Ads seem to be at a local minimum of the information economy and I don't see how chatbots would be immune to it. reply noman-land 3 hours agorootparentprevI have gotten great use from various AI tools but don't be naive. The \"ads\" are invisibly embedded in the training data and get expressed in the subtleties of the model's output. When you control the model and there's no transparency around what's in it, you can do whatever you want with the training data. This is only the beginning. reply arnaudsm 4 hours agoprevGoogle search quality has been decreasing not because of a lack of AI, but because of AI itself. LLM spamming and a decade of SEO maxxing has made pagerank completely obsolete. No one has a personal site anymore, backlinks don't mean anything now. Maybe we need a need a new paradigm for search ranking. Voting, experts, introduce organic feedback back into the loop ? reply meiraleal 2 hours agoparentGoogle shot their own foot. Starting with discontinuing Google Reader, the biggest ecosystem of blogs at the time. Could have evolved into a moat against social media walled garden. reply beloch 5 hours agoprevGoogle thinks they're untouchable. That's why they serve up so many ads that most adblockers can't keep up. That's why they've done very little to combat bad actors who manipulate google search results. That's why they think they can toss half-baked AI out there. AI could be something that helps prevent google bombing. AI could be something that lets them serve up one or two ads per page and get a similar number of click-throughs. Google's lunch is spread out on a picnic blanket, just waiting for someone to come along and eat it. reply joe8756438 5 hours agoprevGoogle is in a tough position, but the track they’re on isn’t helping. People are already publishing massive amounts of meaningless AI generated content which is making results poor. So they short-circuit the madness by introducing their own poor result content, it’s bad but at least they control it. The big question is: can it get better fast enough to save search? I think the era of search is ending, and it doesn’t look like LLMs are a good replacement. reply Havoc 5 hours agoprevUsers have no allegiance to google and this 25 years of trust he reckons google has is ephemeral. I've stopped using google search entirely in favour of Phind and OpenAI. >There’s no reason Google had to enable this feature now. Given that Bing beat them to the punch I'd say they're already late reply llm_trw 5 hours agoparentPhind is astonishingly good for how simple it is. If google had any sense they'd just buy them and use them as their new front end. reply meiraleal 2 hours agoparentprevMy view of Google during these 25 years went from amazed to disgusted. reply insin 6 hours agoprevWe must do it, promo packets are riding on this! reply squigglydonut 6 hours agoprevI agree that trust is eroding with a certain user demographic. As time moves on, they are competing to win trust with the next generations. Does anyone remember going to a library? Used to be the place to get info. Same thing. reply transcriptase 3 hours agoprevIt feels like at some point their goal reversed. From “be the tool that shows the user what they’re looking for as accurately and efficiently as possible” to “be the tool that has them clicking between Google search results and maybe relevant pages showing Google ads” reply uptownJimmy 6 hours agoprevGoogle's market share was suffering because they have been using their dominance to force everyone to participate in the \"game the search engine\" scam that Google was selling, which exists only to cheat the Google search engine. Once enough of us realized that Google search results are usually just paid ads, what's the point anymore? It's insane, and \"AI\" isn't going to fix any of it. The rot in our tech economy is astonishing. reply sunir 6 hours agoprevIt could be that Google has a professional CEO. In many of those cases, the customer is the board of directors. AI is driving share prices. reply mediumsmart 3 hours agoprevI say the mistrust Google has built with me in 25 years can totally accommodate ai search. Bring it on. reply nox101 6 hours agoprevGoogle is fighting for it's life. I'm not saying that excuses things but it puts them in context. IIRC, 60% of google's profit is ads on search. Chat GPT like results remove much of the need for \"search\". I find myself more and more trying to find an answer on Google, not finding it, asking ChatGPT, getting an answer. And I'm NOT suggesting Google search sucks. It might but that's not my point. I recently need to find some details of hardware implementations for a feature. I used Google to find specs for this hardware. The specs were hard to read or didn't explain things as I needed. So I asked ChatGPT and it gave me what I needed. It first regurgitated info from the specs but I was able to ask it to explain pieces in more detail and it was great! At what point do I just stop using Google search to find an answer? Or course AI answers have their issues. ATM I wouldn't search for reviews of some new video game or movie on ChatGPT. I also wouldn't search for product reviews. Even though it will happily spew out recommendations they'll be old at best. Maybe that will get fixed but it will have all the same issues as search and more. People will write their SEO type techniques to try to get the AI to surface their product just like they try to get them to the top of search results. I guess we'll need a new name. ARO (AI Results Optimization). Searching for products at least I get the illusion of lots of various opinions. With ChatGPT I just get the bot's one opinion. Anyway, the point is, Google has to do something. Too late and everyone will switch. Which pushes them into too soon and hence the issues we're seeing. reply drpossum 6 hours agoparentThey're entitled to their business model. They're not entitled to it being successful. reply mezyt 6 hours agoparentprev> I recently need to find some details of hardware implementations for a feature. I used Google to find specs for this hardware. The specs were hard to read or didn't explain things as I needed. So I asked ChatGPT and it gave me what I needed. It first regurgitated info from the specs but I was able to ask it to explain pieces in more detail and it was great! At what point do I just stop using Google search to find an answer? But was it accurate ? Chat GPT just provided you with the statistically most likely specs. If the actual doc for this hardware is lacking, this is 100% hallucinations. My experience is that ChatGPT is simply bullshitting you (sometime accurately), Google is drowning the info in 20 links (where they try to make you buy \"Hardware you're searching for\"), and you have to go to DuckDuckGo to find the thing you're actually searching for. reply Nutcase5753 5 hours agoparentprevI wonder how much of that pattern (Google won't give me shit, better ask GPT) is Google's own fault. Their search engine has been degraded so much that, if their LLM results are based on its search, it won't match the specificity of GPT's results. They seem to have pushed people into ChatGPT's arms. If they had consistent search services, I think less people would flock to GPT, which would give them more room to develop their own LLMs properly instead of just rushing crap like Bard or Gemini's inconsistent stuff. The gamble to rush things means you run the risk of shooting yourself in the foot and letting the other runners catch up reply ToucanLoucan 6 hours agoparentprevI hope either the information you needed was just personal curiosity or that you verified with sources what ChatGPT was reporting. The hallucination problem is still terrible with no real solutions from OpenAI apart from \"we're working on it.\" IMHO, fully cosigning that google search has gotten worse and the AI shit is only going to make that even worse, using ChatGPT as a replacement feels crazy to me. Like yeah, Google gets it wrong sometimes, sure, but ChatGPT just makes shit up. Is a dumbass better or worse than a confident liar? Guess that's up to the user to decide. reply YetAnotherNick 3 hours agorootparentFalse information problem is terrible as well on the internet. And if I were to guess based on usage, GPT generates an order of magnitude less error than a random reddit or stackoverflow or quora answer. reply llm_trw 6 hours agorootparentprevReading posts like this I'm reminded of the teachers who told me not to cite wikipedia because it's not reliable who 20 years later are spouting whatever right wing nonsense is the flavour of the month on facebook. Close enough is good enough. It doesn't need to be perfect, it needs to just not be catastrophically wrong. reply l33tman 5 hours agorootparentBut many chatgpt hallucinations are catastrophically wrong. For example when querying for things related to advanced scientific subjects in physics and medicine for starters from my own experience. I really want to give this a pass but the worst thing is that you can't really see where it turns from correct to incorrect, like you would usually pick up with a human teacher or peer when you start to get a feeling they really don't know what they're talking about. ChatGPT will gladly keep hallucinating references and reasonings that sound superficially plausible until you look them up, because that's what it's trained to output.. If they could just find a way to cut it off when it starts to become too unsure, it would be a big improvement. reply ToucanLoucan 5 hours agorootparentprevI love that in our current moment if you will, that Luddism has been reduced to a binary, with zero nuance whatsoever. Either you are FOR every introduction of technology, every automation, every convenience, every new product regardless of it's demonstrated, documented deleterious effects, or you're an extremist boomer who refuses to learn email. There's no in-between at all. I recall a time when a popular joke amongst tech people was that a tech enthusiast was someone who had every new smart home accessory and every new gadget and used them all, and a tech engineer was someone who had nothing more advanced in his house than a laser printer and he kept a gun in the same room in case it ever made a noise he didn't recognize. I guess we're just more enthusiastic than we used to be. I like my smart switches, but I don't like the notion of all human knowledge being only accessible to me through the filter of a word generator. If that makes me a Luddite, then Luddite I am. reply telesilla 5 hours agoparentprevHonestly I see a ton more ads on YouTube than search as I use Kagi but I can't avoid YouTube. I'm sure they have other future revenue streams for ad placement. reply seankurtz 3 hours agorootparentCheck out FreeTube reply vouaobrasil 6 hours agoprev> The trust Google has built with users over the last 25 years is the most valuable asset the company owns. That gave me a laugh. I don't trust Google. Their entire purpose for many years now has been simply to enable SEO trash in such a way that users see as many ads as possible while providing a minimum amount of useful information so that users are not totally frustrated. If Google could legally sell your organs, they would. They are that nefarious, and deserve all the hate they get. AI is just the next step. Frankly, if Google went bankrupt today, I think civilization would benefit immensely. reply acheron 3 hours agoparentSeriously. Google built trust from maybe 1999 through 2008, and rapidly burnt through all that long ago. reply vouaobrasil 18 minutes agorootparentI never trusted them, ever. Large tech company = evil. reply legel 6 hours agoprev [–] I’ve been using the AI Overviews in Google Search for months now and overall find more utility in them. I’ve developed an instinct for when to ignore them. Typically it is when “edge information” is involved: facts / data that are only very sparsely published in time / web space, perhaps by only one or a few web authors, perhaps only very recently. As an AI developer for over a decade, I feel like if we’re calling this “slop”, I fear what these authors think of human error. I’m also doubtful that these people who are damning this project have ever in fact innovated or developed anything significant from scratch themselves. Makes me want to start a company called Slop. reply lubujackson 5 hours agoparentFrom my perspective, the issue isn't with AI but with UI - Google is promoting these as \"answers\", pushing them to the top of their results and giving no indication that the results might ever be inaccurate. Google has been pushing to \"answer questions\" over return web sources since at least 2010 (when their public messaging changed), so that is nothing new. But it is a seismic, human difference between aggregated results from trusted web sources and reformatted comments from Reddit posts. AI is great for targetted tasks and even for general web usage, with caveats. I would love to see something like an accuracy/trust score associated with the results or at a minimum a beta flag with a \"hide results like this\" option. These are all basic UI procedures any reasonable person would make, but Google has been riding high on hubris for a while now (zero human support, just see the Google Cloud issue on the frontpage...) and this won't change until the stock starts getting slammed, which seems imminent. reply dialup_sounds 1 hour agoparentprevIf a human offered to confidently answer every question you asked without regard to whether it is true or correct, you wouldn't say they're merely mistaken when they tell you falsehoods. You would call them a bullshitter or a charlatan. Calling it slop at least acknowledges that the AI isn't trying to lie to you. It's not merely making a error, either. It just doesn't care either way. reply throwup238 6 hours agoparentprev> Makes me want to start a company called Slop. There is precedent. You can start a modern lifestyle brand like Goop. reply rickdicker 5 hours agoparentprev [–] I don't think you ever have to have innovated or developed anything significant to take issue with a major consumer product losing some utility - I don't know if you used Twitter ten years ago, but it was pretty sad to see the site slowly downgrade year by year, every new feature making it less functional and more addictive, fueling toxicity, becoming a black hole of individuals' attention. It makes you wonder what would have happened if the people put up more of a fight whenever some new stupid feature was rolled out, or a useful feature taken away to make the whole thing just a little more like a slot machine. (Hmmm... maybe Elon Musk should buy Google, so people will finally start complaining en Masse.) As a defender of the new LLM-thingies, do you think they're doing a reasonable job of promoting AI-output literacy? I think it's their job to do so when they are the ones generating the content, whereas general media-literacy was not really their problem when Google was just a directory for the web. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "John Gruber criticizes Google for quickly integrating AI into its search engine, leading to errors and reduced search quality.",
      "He argues that Google is sacrificing long-term user trust to compete with rivals, despite having the option to focus on quality.",
      "Gruber believes Google's haste to avoid seeming \"behind\" results in the dissemination of unreliable information."
    ],
    "commentSummary": [
      "Google has publicly released AI Overviews in the US, sparking concerns about their reliability and potential for manipulation, despite efforts to improve accuracy with the AGREE framework.",
      "Users are dissatisfied with Google's ad-heavy, AI-driven search results, leading them to explore alternatives like Phind and OpenAI, and expressing a need for more trustworthy, socially-driven search solutions.",
      "The discussion highlights a broader issue of rapid, unregulated AI development, potential misinformation, and the necessity for better AI-output literacy and accountability from tech companies."
    ],
    "points": 158,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1716636352
  },
  {
    "id": 40471798,
    "title": "Dynamic Webpage Styling and Security with CSS and JavaScript",
    "originLink": "https://email.ml/",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"email.ml\",cType: 'managed',cNounce: '72901',cRay: '8897cc69ea1320a6',cHash: '0dca5447f50e2f1',cUPMDTk: \"\\/?__cf_chl_tk=sIQiKmLwKKBakEI6AKXE_EcxMB6yVipUmLYahD1VZVA-1716663942-0.0.1.1-1493\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '375000',cTplV: 5,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/?__cf_chl_f_tk=sIQiKmLwKKBakEI6AKXE_EcxMB6yVipUmLYahD1VZVA-1716663942-0.0.1.1-1493\",md: \"5J8FnZz0OzsCXZD6bIB0d7HXNW210eEy62yBuoka.5E-1716663942-1.1.1.1-u.SXV4HCb4sZ8cwj68G_mb79XKXxnc4YZKV2QQOHXb3XnM_9O4pXkYJ0L_R3iKauVH7NpuG0EtxABbys7NaIr_S6Il3wDddMombYGjBPI7YRGIUDLpTpYk4.1qp1OaGoJXFPA3r1Z.1SlVmy6BJycBURLuSjZE3wQMxTfRWahbi51CM16BqJj4ESVltnRhlcPCx.EPwxA362dApt1_C_Czbubj2cqx8FrPYSYd9QQZkuoaSzzVWabnkFYfsl_Ql8_2FJhQzzIJYpMyJ2J53_QjtWAHwetiNsksgxQHlCezRP2BXLVMqkRHb_nrn908CT7v2eUNP7IjNiWT6jvYMDlWuSxd300W2NVsgY6LzkUAhuWW6ap2oMjNnZlehNKs7yvDjswbM9gKEJ700LL55Cx7ylCeU9sFk_3O3ulLDEYj8g5yT6U6V.0_aLe8_wLmC_Ln327rxVb3SnirY9itXddFUxOLlXlwzN_tWTeqYZOa_bI0sGHRKA1kzmnyiZSFjDnaWilrjrRQHuhUuIbW89j8H9hYnFcYjkynto4IUj3JgNKsfahkMOrkKRKNKsSld5tiUPtTd1elp5fwHPx03l7KFbMeu9FxF5_tml.mFoY2dElG.zzdpXsDjZ6l75x1BRmtav62XtdyNSKrOmgbvPr2vUWaLixqOWNtWepBG7_Ga8remLfTNLXIZndKljhfP2SPc_qksV9mS5sk.ef8sF_K8sy82_GGxDgrp7Xq4uNUJR8YPF_mIV8zF7NxQp2rDzy3VUvv_VXDjAi4J6y882Lgu3JiK5HFA796T4T1Sij1D2V5Qko0HzFUVU.Ll3Us91MauvUeU8W9LR48_SfLhIa_i7eG4iR4woKlbclXFEacKmLv31_adppY4QwRwvkvL2rStdQPDYsyIfGGXTGNcTOEoC7oMyocp80MAL8drCQDyfbtaP0wrpLHBSqcN_QWbXklz_ZFOCYWIiIkC3WyJpTuqsiomupoyi6bXfz2gUaJ8xdbTHIDE0koKP7_4gDfPWeOeyvdEhptpP8nfsXj4IT6Oq9cFZoEbbIAErj89KY89_N44UdgmvS4_eFOUwV7adTABRl1FH2uUVzYjhm5_72M41C3tZGd6Tkw4509EkzN7KXPZgjzekMnIslkLqHcwpPmZQsfLpcWX8_U3pkNuvXnrK.1Cc7AzDvL6imAbhvBPcuih.cGBtgFghY7Gg_VAd2oSZ5lsNnD9gtTL0xl9utGAWHMnDCEZKYcVri_iDuVEDM6Y8lQfL1dTLb4klFhgPR8U181v8u8c_DItB8Q1BWTAlMsyhe1aVPK7ZsASST_YJ6UggWYW4GfyoYtqePPVGGPd52JnTlf.nkvODpyE34j2vmXHgmc7YY6ISh58qyOM3w1cE0HSJP1WWf9lLD.1kMCrdsN_GgyfuSbj2EISyzA\",mdrd: \"zOhdWgutzhCoQHIiRrIHms7cctHBIMkLKlJZpDPlT9Y-1716663942-1.1.1.1-tIZtEugJjKb2JmTebOyNwFzEXwwUE3inIKwOj9ScB5dhnZHIHZUuKAxPI2_RoZk6U4_qWh7tUTM3uVWxPh0JQe348GwdN6Gwcbk2jBCeM3OoKkgGwdY6ZOZuxXZ8F4AVTGF.BLxpSe0WpopbdYaIQHzc8GRJy.7KDGBf.KTV9pgQfW28u7hxzNu_kGyhyStOWf88Cl3Ddo9Wk0JQVRYqAgT9uaUKJMzAQQmrZShuyR_OXMFrX.nJ7notooZhPxtTSsvh5Sr1CNoiDPRBbe6mbM1rT1V7MVTUc2ZI7e5qVjKgP5MKdVNjvXODGamNEk3mw0BsLbm1jXx5C8ZQY_PlROhAvaQhI4qpMBCPpQXG8OK22Lg5C5k5QE9a3ig6jpU1kjj9kplnpNAp5bTODlDCY_CL6KtldYHpRRLMvhdmergIaA_z95jagP38S2ZwIclAJjynAdCHoFE1pFq4.Du2kb5kjWa2U2YDGjaTbiaRhq3XHR412DdMSk4stl6lUAjrgCPc0ga89pt1.vzmEsbNy1cAv5g0aLlmkPMN7svRFjkCOCj41bSZHd.qDN3cgLdyd6vDnvO8VHfI8hB.RJ4za7_MxZdWtlXDL5zYMcxPmF8dbGyPn8gx6RyoLHtCAXszQg9KMSLvxLXR_TSkfCtM08h6WS6Wknn2yzKz44fkq8J4QNJs5Ijo5e7CsbmAjyFLm3pvPXofzV0oW5_Aji8Zzz6AdgS0gvpjhdfygmqjmwk8hIfmlZLs8271.wTCUKwrnc0rv6wtApeqZcT6JB7xpcShpCMTKf_XKomtBv.GemSQGG6J_EgK8bjrIeQb3syF9iIWGBJScymts73Y1JCXjWD0jb70t4RZijlv8M8PoH_YpH9peqpaZbDQB_iAog1G2fvcl8TmC0Ni9Mc8qo5gmjB7uShkhTkshwCTrbpJVU7hDwN4asZrvQOaW4GZUX05eEHDsGsvGHj4H1QhZmmSsyRXGO09AKTj7pHZYnBXETlcpIBWylD7eN0Hm.IlgpJ_cIaPUs.BfAoyQhC4GZmrXTukBxYg_XyMjZQiGM03nMGAT9RCi7zdrk.ThSkBE4Sx0ietJnfN53GHQzMXdEMF4myfD8RvtpabkFZG3ALRSsLvYaptbl5PjVn29prySGDBIFKcoL7hGZeLX5Xt.azn5L6e3CgpQqpZj4R43Xn197Zn4BQXyeuosH_zSPnNVkfNU.tvkXFsbZQ7F_9B6x05VAiBY0eLlzd.pDEpoRWr6HkGEaXEhHAkYGVOWru2oUF79FSFM.8K2UunzfpdTRiADRxvRFEeSwQUJUF8lOITNI4CU6QaTsJolsBsD5rp9SjVMblH6MM2ZW3TDHUE5VMsJJdsDfXSanz4l12JbvCRcscjH5rnqdhjG9B5.Fm4A939nRd7kHDayUVpIBelz7kleRbnTBHfr2awxhuxbWVVNuASfupiGq4mSsOnKhwh3OrN30A7yUItxnlBZXUQLX1x.x57LQ8pLLOeCaqYTVgQCFgNy1LBFgNYaRzH5M7IfaxM_C7B0Sz.xJMWGDKhMyGYtzYFKSCwDseaxzHrGnA6VMw4J1p.Kop9gOIR9Ptt18KuLfujavZOmWag9TMfsIW5SthwNvNvtBqyv5hFMArb7gza3pwRHwwJYO20Wsjrai9Lo8dF3cvLqrIkKMlcavlfcOPFUzytCkY2GyzaCWGNHfnHN4.ee56T2g1XYHQsGDEnYfxiZ9yZ6nvuotbOefZ3oxreOCRowys3Y7_UuvxIEvSABLPS39DIOzTA8kTd.mwJzycVJetvYooL0L_6fy.m02A7jF2fi1.yzk_7gLoiZm_66w_xhPVyVZ7FjLbidnprVcmf8Urtm0qGza3nduyut3jqA6GbCAx3c5twYHaPsow_4dBT7zBSTqxCrY0z3wcStkEZLAMgx6ITjkpkXsR_nmRRiSCod6RpQKlCZ8QnquD0Nor8wLajip2NF4xcrrnnSss9bMoWLHR6pN20JpJdaDGAb_mIKQB_GywCKdC8LwGTK6I5f3y1cD2xmFdizlWJEbGbX37Z7_G4iiNYakavztgb9Bx6cvSexkF_S9OEMakPQFDfhT8b.2i.YORaRQEwSZwwfS6sz.XfUCLr9jNBLtgWdZ6ReAbOTn18QphiZxgjFcbXgrENJ192i8gbDMHdI5g3PA2UZUL_uFHHqfzMcIKy.Ql1MjgjsKt1xFTONAMF1tAhriBZN4gdeKB1A3rDosIjdRlfiF83O1dnmtyXueiekYluksn0eS.k0HsmsmOg44T6C7S5LMrCrXvWGHVa\",cRq: {ru: 'aHR0cHM6Ly9lbWFpbC5tbC8=',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: '/1QI/w4bWGb+/VNjxaTC2+Ui6N8iVXpC7eGZtfcEe66jfxDLTJH0zCgQVyM2IxYI3lxhVo7/a/EZi+iWgO36ESHA8ISadpXcvrTUgHqj2+LbnD9d/NQACcRYHK9n39pyJpgvDB+FY0amSoU+2zNp8FYwzuvNA3YoiO68QqvmNJf0B9TK6VhQyKSqPzjvl3/y9rpO9Pj15RJgKV0oowJ5tyeiJLRfas8yTvYCAacb3194VcvNbcxTwAuQ0LVzXM+cFfI9E2x8+6AKja+EO77Q1HyXOxoZDVzxbUEd+Jj3bTjD4KiipS0dTQZZ0TXkL+C8RK75s5vpjWiPn+BLY1iBA+WzlV9+9TZ5YvNkN5WTVBu432MGDC07m3Gg5y+3tJmq+Q3xDCEmS7kK7tI+OlNic98ZtVtWvhI2tks7CkfouI2pyZGB4VFT1EwdMUJbpV/ExjQT+hnJeeLmCBVyOmBiU6JB4jy+64m9YWb9py+V0i3kl2b083YI0uEh1k8sazgH',t: 'MTcxNjY2Mzk0Mi43MjAwMDA=',cT: Math.floor(Date.now() / 1000),m: '4AGLcXuFczIwnA/Oa6KUCx/+r52MdKkW3GgBrPlGpa8=',i1: 'Y2tgUOWdYlHie8GeBmocNw==',i2: 'Qmdjn1UsPw//fySJPA4D6Q==',zh: 'AE5f/vgP9HrslTtclL1QQ3p+h6F9Q0bh7GjIQ+kuHqs=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'zVOyI94lghRTRjOJMBC4wm1f2a48j2MVN2s+j3TUV7A=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8897cc69ea1320a6';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/?__cf_chl_rt_tk=sIQiKmLwKKBakEI6AKXE_EcxMB6yVipUmLYahD1VZVA-1716663942-0.0.1.1-1493\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=40471798",
    "commentBody": "Email.ml – Minimalist Temporary Email (email.ml)152 points by ccbikai 18 hours agohidepastfavorite74 comments Brajeshwar 14 hours agoThe problem with such services is, the moment they rear their heads just above the obscure line to even an iota of popularity, they get blocked, blacklisted and what not. For those who own their own domain for emails, there are multiple ways of doing it - aliases, temporary IDs, filters, etc. Or use such services' backbone/infrastructure that can route the emails but you own the domain. `20240523@example.com` is valid for today. reply SebastianKra 8 hours agoparentIt seems to me that fewer and fewer websites actually bother to block these domains. Even if you go through all that extra trouble to maintain a list of banned domains, the user can still just whip out a Hide-My-Email address ending in @icloud.com. reply al_borland 42 minutes agorootparentIn my experience this seems to be true. For a while it seemed throw away emails were pointless to try and they would all be blocked. But over the past couple years I’ve had a lot of success using mailinator, which is probably one of the better known services which has been around for a long time. reply iforgotmysocks 4 hours agorootparentprevI don't think sites have problem with alias emails like iCloud Hide My Email. The issue is when they are used for creating spam accounts. reply pquki4 7 hours agorootparentprevThey could end in @privaterelay.appleid.com which is... easy to block. reply SebastianKra 7 hours agorootparentOnly those emails generated by Sign In With Apple AFAIK reply evantbyrne 4 hours agorootparentprevDo any HN commenters here know of any maintained lists of throwaway email domains? reply mikae1 11 hours agoparentprev> The problem with such services is, the moment they rear their heads just above the obscure line to even an iota of popularity, they get blocked, blacklisted and what not. I never ran into that problem with https://temp-mail.org. They cycle domains constantly. reply jasode 10 hours agorootparent>I never ran into that problem with https://temp-mail.org. They cycle domains constantly. You didn't run into that problem with temp-mail.org because you happen to use them on websites that don't bother blocking disposable email addresses -- or -- they do try to block them but use simplistic domain name checks. But cycling domain names isn't enough to fool the more sophisticated disposable/throwaway email address detectors (especially the paid services) because they use extra heuristics of checking DNS MX records ip addresses. E.g. temp-mail.org assigned the following random addresses \"bob531@mcatag.com\" and \"alice8224@javnoi.com\". Both of those domains are easily detected as throwaway emails because they both share the same MX ip 24.199.65.21 : - https://verifymail.io/domain/mcatag.com - https://verifymail.io/domain/javnoi.com On the other hand, the \"@mcatag.com\", \"@javnoi.com\", etc used by temp-mail.org is good enough to get past the websites that copy&paste the code from these StackOverflow answers that just use simplistic string matching : https://stackoverflow.com/questions/10976706/how-to-block-di... It all depends on how much effort and cost the websites want to put into blocking disposable email addresses. reply ccbikai 8 hours agorootparentIf they use Cloudflare's Email Routing, the MX IP is more difficult to detect reply ronnier 4 hours agorootparentprevhttps://temp-mail.org/ Is one of the harder ones to block. The use unique domains with unique Mx records and cycle through ip addresses. All seem to be on digital ocean though reply ruffrey 3 hours agoparentprevThe temp email services which support custom domains help. For example, Mailsac.com. But it has pivoted to more focus as a dev and qa platform. reply ronnier 4 hours agoparentprevYup. I just added this domain to our disposable list for one of the worlds largest social media apps. reply Terretta 7 hours agoprev> We do not share, sell, or disclose any personal information or email content to third parties, except as required by law or as necessary to protect our rights, property, or safety. This policy claims to protect your privacy but allows them to share your data broadly, including with anyone they see fit to protect their own interests, effectively offering no real privacy guarantee. reply multjoy 7 hours agoparentThey’re not going to protect your privacy to their detriment. They’re running a business, they’re not martyrs. reply Terretta 6 hours agorootparent> They're not going to protect your privacy to their detriment. They’re running a business, they’re not martyrs. Fine, then they shouldn't profess their faith. - Home page, second line: Privacy friendly - Second site page: Privacy Claim: “At Email.ML, we value the privacy of our users and are committed to protecting their personal information.” If what they mean is security, since they are not committed to protecting privacy, they should just say security. Security can be a fine selling point without the newspeak. Translated claim in ‘oldspeak’: “At Email.ML, we claim to value your privacy, but we will share your personal information whenever it suits our interests or legally required.” reply User23 4 hours agorootparentprevThen they can't be said to value it. And judging by the number of free one year identity theft protection plans I've racked up, they don't. reply hggh 10 hours agoprevI find more convenient to generate the temporary emails right from the command line with something like this: https://github.com/sdushantha/tmpmail reply zikduruqe 7 hours agoparentNot only generate, but verifying your sign up with w3m also if needed. reply Pop_- 11 hours agoprevThe github icon on the site directs to the author’s own page, and I couldn’t find any repository for the site, which makes me curious why do they even put the github link? Just for a follow? reply esnard 4 hours agoparentIt looks like the footer is identical to the footer on the author's own personal website, so I guess they never cared to change it? reply toastal 10 hours agoparentprevMarketing & social media pervade everything now reply tamimio 4 hours agoparentprevYeah, I was going to say it’s an awesome work but I looked around trying to find the repo.. and nothing was there. What’s the point of mentioning it runs on CF when you don’t provide the repo? This is just another SaaS reply jesprenj 11 hours agoprevI recommend http://grr.la -- no login, just type in the local part of the mailbox and get access to email. reply penguin_booze 8 hours agoparentAs mentioned elsewhere in this thread, this is increasingly getting blocked. This still remains my go-to service, however. reply p4bl0 14 hours agoprevI guess using a .ml domain is okay for something that explicitly needs to work temporarily, but since the freenom scandal I wouldn't trust such domains. reply ccbikai 8 hours agoparent.ml has started requiring payment to register. If you have concerns, I can use email.beer reply jimbobthrowawy 2 hours agorootparentThat domain seems to be squatted by the same guy that made the email.ml site. Edit: didn't realize that was you. reply wwalexander 11 hours agoparentprevAny generic use of a ccTLD gives me the ick. It’s one thing to use one for your low-impact personal site, quite another to use one for your email address. reply ctippett 7 hours agoprevI had high hopes for 1Password's integration with Fastmail and their masked email feature. Unfortunately I've seldom had it work properly and 1Password never prompts me to create a masked email when creating a new account for something. reply mdaniel 2 hours agoparentMy suspicion that the 1P extension is only as bright as the markup in the page, soyou get nothing,and the extension starts to behave well. Now a reasonable person could certainly wonder why the hell they don't offer \"generate masked email\" right next to their existing \"generate password\" but I have long since given up trying to understand the mysteries of 1P product management reply _joel 6 hours agoparentprevWorks great for me on Firefox MacOS. reply ravetcofx 14 hours agoprevWhat will happen on the domain gets flagged by every provider and banned from use reply brnt 12 hours agoparentAt many places freedom tld's are already shitlisted. reply mattl 7 hours agorootparentWhat’s a freedom TLD? reply jasode 7 hours agorootparent>What’s a freedom TLD? It was probably a typo/autocorrect of \"freenom\" : https://www.freenom.com/en/freeandpaiddomains.html The significance is that the \".ml\" tld is on that list and this thread's domain is \"email.ml\". Some entities block any tld registered with freenom. reply ReliableDude 4 hours agoprevCool, but in terms of UI/UX, the 'Waiting for emails' banner animation is quite choppy and annoying because it isn't seamless and loops endlessly. reply kseistrup 11 hours agoprevIt doesn't respect ~/.signature in text-only emails: The tearline and everything under it is just put on one continuous line (together with the body text proper). reply jesprenj 10 hours agoparentWhat is ~/.signature? reply kseistrup 10 hours agorootparentIt's the “signature” (free text, but people use it for information, address, phone number, disclaimers and what not). It appears under the “tearline”, that consists of two dashes and a space (“-- ”). On unix OSes it lives as the file ~/.signature, and many email clients will insert the contents of that file under the tearline in outgoing emails. E.g.: https://www.gnu.org/software/emacs/manual/html_node/emacs/Ma... reply jackthemuss 6 hours agoprevNice. I use MailSlurp.com because it has API clients reply mattl 18 hours agoprevWhat does “AD friendly” mean? reply jowsie 13 hours agoparentSearching for '\"AD friendly\" email' brings up another temporary email service that this appears to be a clone of (https://huo.fo/), and this hackernews post with you asking what it means. Active Directory comes to mind, but I'm not really sure what that would entail. reply supriyo-biswas 13 hours agorootparentIt's more likely that whatever open source solution they are using to run this, allows you to configure ad insertion easily. reply ccbikai 8 hours agoparentprevYou only need to see an advertisement in your email once when using a certain registered service, and the advertisement will not appear in your email afterwards reply mattl 5 hours agorootparentOkay you should probably make that clearer and at the very least write “Ad” not “AD”. reply tengbretson 12 hours agoprevJust buy a random cheap domain and use cloudflare email routing. reply pquki4 7 hours agoparentMost people in this thread probably have the money and technical skills to buy a domain and set everything up, but it misses the point: this is supposed to be \"infrastructure\" that everyone can use without any hassle (and likely for free), rather than private to an individual. Of course, like others have pointed out, shared \"temp email\" services are more likely to be blocked. reply Etheryte 8 hours agoparentprevThis misses the point entirely. Most of the time when I need a temporary email address I just want to see some content that's hidden behind a sign up form. Going through the effort of buying a domain, configuring it, etc is the last thing I want to do in that situation. I want a site that will give me an address for 2 minutes so I can complete the sign up process and then go on with my day. reply technothrasher 8 hours agorootparentI don't think the suggestion was to set up your own throw-away email domain every time you need a throw-away email. Set it up once, then use it as needed. This is what I do. Sure I had to 'go through the effort' initially, but its not that much effort and I use the domain for other testing and temporary purposes as well. reply Etheryte 8 hours agorootparentIn that case, how is this any different from e.g. signing up with a new account on Gmail/Outlook/etc? Why do you need to buy the domain and jump through all the other hoops? reply jasode 8 hours agorootparent>In that case, how is this any different from e.g. signing up with a new account on Gmail/Outlook/etc? Some people like to create dozens (or hundreds) of disposable emails because they just need it for a single purpose (e.g. downloading a pdf whitepaper that's hidden behind an email sign-up). The email address is truly ephemeral and they don't want to create a whole new Gmail/Outlook inbox just for that. There's also UI/UX friction in Gmail/Outlook workflow of multiple screens to create a new inbox instead of being a convenient one-click like the disposable email services. Also, Google and Microsoft puts a limit on the max # of new email accounts. They don't explicitly publish the max number but it depends on various heuristics such as your residential ip address, country, etc. E.g. some users only got up to 4 gmail accounts tied to a phone number. Others got 10 before being cut off. There are rate limits so one can probably get more accounts if creating them is spread out over multiple days. Using one-click disposable email services or using your own domain to create unlimited email aliases lets one go beyond the limits of free Gmail/Outlook accounts. reply Etheryte 4 hours agorootparentYeah this whole concept I agree with and it's actually what I brought up earlier in the comment tree. However what I still don't get is why you would choose the domain setup over one of very many free services where you get an address with a single click. reply tengbretson 5 hours agorootparentprevGmail requires verification with a phone number, so assume all of your gmail accounts are linked reply echoangle 8 hours agorootparentprevI think the idea was to do this once and use the mail everywhere you need it, not to do the setup every time you need a throwaway email. reply 38 14 hours agoprevPlease stop using cloudflare validator. It's a piece of shit that repeatedly marks real users as fake, blocking them from using many sites reply pmdr 11 hours agoparentYep. It marks residential IPs for no good reason and for god knows how long. Cloudflare is way up on the list of companies that have broken the internet. But, hey, free stuff, right?! reply janpieterz 13 hours agoparentprevWhat would you recommend? reply 5e92cb50239222b 11 hours agorootparentMaybe hcaptcha, I've never run into captcha hell with it, unlike Cloudflare and recaptcha. If those two decide you're a bot, you're done, better go change that IP and reset cookies. reply Zuiii 13 hours agorootparentprevI'd personally recommend using something that doesn't block real people like there's no tomorrow. reply quaintdev 13 hours agoprevI don't have any need for this website at the moment but I might need it in future so I bookmark it. The title of website does not have temporary email so my bookmark does not have that info. Now when I actually need this I can't find the bookmark! Can we please have a little bit context in the title of websites that aren't popular so that it's easy to search the website when needed. reply arcastroe 8 hours agoparentCan't you add any title you want to your bookmark? reply monero-xmr 13 hours agoprevMy blocked domain list is... checks production database... 18911 domains. I shall add another now reply mrweasel 8 hours agoparentIf you need to block services like this, then you're probably part of the problem. These services only exists because companies insist on ask for email addresses for stuff that has no need for it, or which users don't trust to keep their email address safe and free of spam. reply supriyo-biswas 7 hours agorootparentBlocking spam signups and preventing free trial abuse are two very legitimate purposes of blocking disposable emails. reply jesprenj 11 hours agoparentprevWhat is your business doing so that it requires you to maintain your own blocked domain list by hand? reply navigate8310 12 hours agoparentprevWildcard any .ml for sanity reply toastal 10 hours agorootparentI feel bad for us in the Meta Lang Gang. reply TZubiri 11 hours agoprev [–] I feel that that's not what email is for. Any communication method needs a return address, otherwise spam and abuse. Email has two very distinct purposes, not 1, not 3. 1- federated pseudonymous identity protocol for internet(ip/http). 2- medium-length business time insensitive communication. That's it. This sounds like a free experimental product aimed at improving 1, but most service providers will either have to block it or face abuse from users of your domain. What OP doesn't know yet probably is that they will not be the ones under dos/spam,rather they will be used to DoS, spam, simulate traffic, scam, etc... Edit: the ToS outlaws spam, so it is possible they will cooperate with spam blacklist providers like spamhaus ( not sure how). Not sure how businesses will block the domain for acct creation, probably as a whole, Ive seen websites use a whitelist approach like gmail, or outlook. reply n4r9 10 hours agoparentAs long as service providers demand that you give them a \"valid\" email address to create an account, and as long as a significant proportion of those providers sell those addresses to other spammers, services like this will continually be created. It feels like a bit of an arms race. reply jesprenj 11 hours agoparentprev> I feel that that's not what email is for. Any communication method needs a return address, otherwise spam and abuse. I don't agree. Email addresses are sometimes used for ephemeral tasks, like buying concert tickets and items in online stores. You need an email address, so they can send you the tickets or the receipt, but then you don't need this address anymore. reply pquki4 7 hours agorootparentActually that is not true: Concerts could be rescheduled or cancelled, and emails would be the only easy to contact customers for arrangements like refunds. Similar for online shopping, and in addition, emails could be used for notifying product recalls or class action resolution. While these cases are relatively rare, it illustrates it is hard to know when an email is \"done\" -- it could be useful after a purchase itself, or even long after a transaction has finished. reply geysersam 10 hours agoparentprev [–] They seem to be using cloudflare to verify the users are human. That's something in the way of protection against automated spam. reply kevincox 6 hours agorootparent [–] This comment seems to have far too much trust in Cloudflare. A more accurate statement is probably something like \"They use Cloudflare to slightly increase the cost to spammers while only rejecting a small number of individual users.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Hacker News discussion on Email.ml, a minimalist temporary email service, underscores challenges like being blocked or blacklisted as they gain popularity.",
      "Users suggest alternatives such as personal domains with aliases and filters, and note varying effectiveness of temporary email services due to sophisticated detection methods.",
      "Concerns are raised about the transparency and trustworthiness of some services, especially those using .ml domains, and the balance between preventing spam and ensuring user accessibility is debated."
    ],
    "points": 152,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1716598322
  },
  {
    "id": 40474165,
    "title": "Lapis 1.16.0: Enhanced Lua Web Framework for OpenResty with New Features",
    "originLink": "https://leafo.net/lapis/",
    "originBody": "Lapis Lapis A web framework for Lua v1.16.0 Reference Manual Install luarocks install lapis For Lua 5.1+/LuaJIT Source on GitHub November 2nd, 2023: Lapis v1.16.0 — types.params_map, improved model:update: changelog What is it? Lapis is a framework for building web applications in Lua (or MoonScript) that primarily targets OpenResty, a high performance web platform that runs on a customized version of Nginx. Lapis can also be used in other server environments, being compatible with any modern version of Lua. Want to talk Lapis? Join our Discord Lua MoonScript local lapis = require \"lapis\" local app = lapis.Application() app:match(\"/\", function(self) return \"Hello world!\" end) return app lapis = require \"lapis\" class extends lapis.Application \"/\": => \"Hello world!\" How does it work? With OpenResty, Lua is run directly inside of the Nginx worker using LuaJIT, giving you the smallest barrier between the webserver and your code. Have a look at Web Framework Benchmarks just to see how OpenResty stacks up against other platforms. Utilizing the power of Lua coroutines, you can write clean code that looks synchronous but can achieve high throughput by automatically running asynchronously without blocking. Networking operations like database queries and HTTP requests will automatically yield to allow for handling concurrent requests, all without all that callback spaghetti seen in other asynchronous platforms. It’s fast, easy to read, and easy to write. What does it come with? Lapis includes URL routing, HTML Templating, CSRF Protection and Session support, PostgreSQL/MySQL/SQLite backed models, schema generation and migrations in addition to a collection of useful functions needed when developing a website. Lua MoonScript local lapis = require \"lapis\" local app = lapis.Application() -- Define a basic pattern that matches / app:match(\"/\", function(self) local profile_url = self:url_for(\"profile\", {name = \"leafo\"}) -- Use HTML builder syntax helper to quickly and safely write markup return self:html(function() h2(\"Welcome!\") text(\"Go to my \") a({href = profile_url}, \"profile\") end) end) -- Define a named route pattern with a variable called name app:match(\"profile\", \"/:name\", function(self) return self:html(function() div({class = \"profile\"}, \"Welcome to the profile of \" .. self.params.name) end) end) return app lapis = require \"lapis\" class extends lapis.Application -- Define a basic pattern that matches / \"/\": => profile_url = @url_for \"profile\", name: \"leafo\" -- Use HTML builder syntax helper to quickly and safely write markup @html -> h2 \"Welcome!\" text \"Go to my \" a href: profile_url, \"profile\" -- Define a named route pattern with a variable called name [profile: \"/:name\"]: => @html -> div class: \"profile\", -> text \"Welcome to the profile of \", @params.name Models Get a powerful abstraction layer over your database tables just by sub-classing Model: Lua MoonScript local Model = require(\"lapis.db.model\").Model -- Create a model, backed by the table `users` local Users = Model:extend(\"users\") -- fetch some rows from the table local elderly_users = Users:select(\"where age > ? limit 5\", 10) local random_user = Users:find(1233) -- find by primary key local lee = Users:find({ name = \"Lee\", email = \"leemiller@example.com\" }) -- create a new row and edit it local user = Users:create({ name = \"Leaf\", email = \"leaf@example.com\", age = 6 }) user:update({ age = 10 }) user:delete() import Model from require \"lapis.db.model\" -- Create a model, automatically backed by the table `users` class Users extends Model -- fetch some rows from the table elderly_users = Users\\select \"where age > ? limit 5\", 10 random_user = Users\\find 1233 -- find by primary key lee = Users\\find name: \"Lee\", email: \"leemiller@example.com\" -- create a new row and edit it user = Users\\create { name: \"Leaf\" email: \"leaf@example.com\" age: 6 } user\\update age: 10 user\\delete! Templates Write your templates either in etlua or in pure Lua/MoonScript. The Widget base class allows you to organize your templates as modules, enabling you to use inheritance and mixins to mix and match methods combined with the HTML builder syntax that lets you express HTML with the full power of the language you're already using. The HTML builder syntax makes you immune to cross site scripting attacks from user-provided input by ensuring all written content is escaped correctly. Lua MoonScript Welcome back Welcome to my siteimport Widget from require \"lapis.html\" class Index extends Widget content: => h1 class: \"header\", \"Hello\" @user_panel! div class: \"body\", -> text \"Welcome to my site!\" user_panel: => return unless @current_user div class: \"user_panel\", \"Welcome back \" .. @current_user.name Full Example Using all the provided tools we can quickly and logically construct high performance and low memory web applications. Here's a more complicated example complete with forms, CSRF protection, and various database queries. Lua MoonScript local lapis = require \"lapis\" local Model = require(\"lapis.db.model\").Model local capture_errors = require(\"lapis.application\").capture_errors local csrf = require \"lapis.csrf\" local Users = Model:extend(\"users\") local app = lapis.Application() app:before_filter(function(self) self.csrf_token = csrf.generate_token(self) end) app:get(\"list_users\", \"/users\", function(self) self.users = Users:select() -- `select` all users return { render = true } end) app:get(\"user\", \"/profile/:id\", function(self) local user = Users:find({ id = self.params.id }) if not user then return { status = 404 } end return { render = true } end) app:post(\"new_user\", \"/user/new\", capture_errors(function(self) csrf.assert_token(self) Users:create({ name = assert_error(self.params.username, \"Missing username\") }) return { redirect_to = self:url_for(\"list_users\") } end)) app:get(\"new_user\", \"/user/new\", function(self) return { render = true } end) return app lapis = require \"lapis\" import Model from require \"lapis.db.model\" import respond_to, capture_errors from require \"lapis.application\" csrf = require \"lapis.csrf\" class Users extends Model class extends lapis.Application -- Execute code before every action @before_filter => @csrf_token = csrf.generate_token @ [list_users: \"/users\"]: => users = Users\\select! -- `select` all the users -- Render HTML inline for simplicity @html -> ul -> for user in *users li -> a href: @url_for(\"user\", user.id), user.name [user: \"/profile/:id\"]: => user = Users\\find id: @params.id return status: 404 unless user @html -> h2 user.name -- Respond to different HTTP actions to do the right thing [new_user: \"/user/new\"]: respond_to { POST: capture_errors => csrf.assert_token @ Users\\create name: @params.username redirect_to: @url_for \"list_users\" GET: => @html -> form method: \"POST\", action: @url_for(\"new_user\"), -> input type: \"hidden\", name: \"csrf_token\", value: @csrf_token input type: \"text\", name: \"username\" } Where can I learn more? The Reference Manual is both a complete guide and a tutorial to using Lapis. The source of Lapis can be found on Github and issues can be reported on the issues tracker. LuaRocks.org is an open source application written in Lapis. It is the public host for all Lua Rocks and the source can be found on GitHub. Anything else I should know? You can use most existing Lua libraries with Lapis with no problems. Here are some libraries you might find useful: lapis-eswidget — Base widget class for aggregating front-end assets with esbuild lapis-console — Interactive MoonScript console for Lapis that runs inside of your browser lapis-exceptions — Exception tracking and reporting lapis-bayes — General purpose Bayes classification for Spam, Fraud, etc. web_sanitize — HTML sanitization tableshape — Robus input validation and verification magick — ImageMagick bindings cloud_storage — Support for Google Cloud Storage About Lapis would not be possible without the following projects: Lua LPeg OpenResty Lapis is licensed under the MIT license. Lapis is written by @moonscript. Reference Manual · Source on GitHub ↑ Top by leaf corcoran · licensed under MIT Thu Nov 2 12:54:59 2023 PST",
    "commentLink": "https://news.ycombinator.com/item?id=40474165",
    "commentBody": "Lapis: A Web Framework for Lua (leafo.net)136 points by thunderbong 8 hours agohidepastfavorite39 comments raytopia 6 hours agoLapis is the framework that itch.io uses. [1] It also uses MoonScript [2] as the programming language which was also written by the creator of itch.io/Lapis. 1. https://leafo.net/posts/introducing_itchio.html 2. https://moonscript.org/ reply natrys 5 hours agoparentOne thing people might need to keep in mind is that Lapis isn't particularly interested in adding features that itch.io doesn't need. Been almost a decade since websocket support was requested e.g. https://github.com/leafo/lapis/issues/233 reply leafo 52 minutes agorootparentLapis is very dependent on the server backend it is running in, generally OpenResty. Last I investigated, the ergonomics of the websockets API in OpenResty didn't really seem like a good candidate for building websocket based applications. As an example, there's no trivial way to keep track of all connected clients and broadcast a message to them without overly complicated solutions. It's not trivial to listen to events from different asynchronous sources at the same time. (probably other things too but I don't remember right now) OpenResty/Nginx is not a general purpose event loop. The fact that it's primarily an HTTP webserver is evident in the design of the interfaces that are made available. That said, there's nothing stopping you from and utilizing the `ngx` APIs directly, there are just a few considerations to be made with database connection pooling, but generally you can `require` any Lapis module and use it anywhere in Lua code. For websockets in OpenResty look here: https://github.com/openresty/lua-resty-websocket The reason the issue is still open is not because I'm not interested in adding it, but because I didn't feel Lapis could provide a useful abstraction at this time. reply CaptainOfCoit 5 hours agorootparentprevWhat in that issue says that Lapis would only implement things needed for itch.io? Seems there is a off-hand comment by a non-contributor that it seems to be like that, but hardly a confirmation it actually is like that. reply dubcanada 4 hours agorootparentprevThis is completely incorrect comment. There is no rejection of any form, it is logical that the main developer who used it to build itch.io is not going to put in the effort to do something they don't care about. But the product is open source, anyone can write that functionality, and I am sure it would be accepted. What you are complaining about is a developer who is not willing to put in free effort for a feature you care about, but are not willing to contribute to. reply johnchristopher 4 hours agorootparent> But the product is open source, anyone can write that functionality, and I am sure it would be accepted. I have often read on HN that not accepting a PR for a feature that is not tangential to the core business of the project is valid and understandable when the maintainer believes it will require too much resources (bug triage, fixes, documentation, short or long term support in the forums, etc.) from him. reply mardifoufs 2 hours agorootparentprevI don't get this sentiment. Just as the maintainer is allowed to do whatever they want with their project, random people on the internet can also warn other people about downsides of said project. Absolutely nothing GP said implied that they felt entitled to a feature. It is useful information for potential users to know how the project works and if it might not fit them well. There's no contract or agreement between the maintainer and users. Users are free to talk about the project. As long as they don't demand anything I don't see the issue at all. reply natrys 3 hours agorootparentprev> What you are complaining about is a developer who is not willing to put in free effort for a feature you care about, but are not willing to contribute to. How exactly is this complaining? I am merely making an observation, not even in the earshot of their devs. I had evaluated Lapis years ago, but quickly moved on when it didn't seem adequate. I didn't complain about that in their issue tracker then, and I am not going out of my way to bring this up now because I am holding a grudge like you are implying. Whether or not they add websocket today wouldn't matter to me in the least, I am simply indifferent to it. > it is logical that the main developer who used it to build itch.io is not going to put in the effort to do something they don't care about There are plenty of frameworks that add features well beyond what the devs need for their own products. All I am saying is it might be more logical for other people here to use those instead. Of course people can come to this conclusion on their own, but highlighting what I believe to be a key information may save that time. reply xrd 5 hours agorootparentprevWhat about SSE? If that's there, then I'm happy without websockets. reply biftek 3 hours agorootparentLooks like you can bypass the lapis renderer and use ngx.print() and ngx.header() to control the response yourself Look under skip_render here: https://leafo.net/lapis/reference/actions.html#render-option... reply orange_fritter 26 minutes agoprevI've spent probably 200 hours with MoonScript, was obsessed with it for a bit, and love it 90% of the time. The problem is that MoonScript's grammar is INCREDIBLY permissive. Common typos(often involving spaces) don't break your code, whereas in Lua or Python the mistake wouldn't compile. MoonScript is awesome but I lean on compilers pretty heavily to catch my mistakes, so I sadly and regrettably admitted that I was more productive with other toolchains. reply prezjordan 6 hours agoprevI find Lua so dang pleasing to look at reply christophilus 6 hours agoparentI agree, but I miss static types. Has anyone here ever used Nelua[0]? [0] https://nelua.io/ reply otikik 4 hours agorootparentTeal, man reply iTokio 5 hours agorootparentprevLuau from Roblox is gradually typed and has great support reply stefanos82 5 hours agorootparentprevI do and I'm quite pleased with it. reply ngrilly 5 hours agoparentprevI read too fast and thought you wrote something like \"I like Lua so dang please look at it\" :) reply CableNinja 2 hours agoparentprevYea, but who starts an array index at 1!?!? reply jecel 31 minutes agorootparentAlgol 68, APL, AWK, COBOL, Fortran, Julia, Mathematica and Wolfram Language, MATLAB and Octave, PL/I, RPG, R, Smalltalk, etc (including Lua). reply nequo 40 minutes agorootparentprevR, Matlab, and most of mathematics. reply xena 5 hours agoprevI used to use Lapis for my website, it is a lovely framework. For a long time it was what I reached for when I needed something like Rails but didn't want to deal with Rails. Been thinking about using Moonscript for some Kubernetes templates, I love its table creation shorthand. reply xrd 5 hours agoprevNot knowing much about MoonScript and only knowing a tiny bit of Lua, does anyone have a recommendation on which is \"better\" if you are starting from where I'm at with these languages? MoonScript looks class based; I avoid classes like the plague in JavaScript because of \"this\" but maybe that is the wrong starting assessment? reply elpocko 4 hours agoparentMoonScript supports classes via keyword, while in Lua you have to implement them yourself. That's about it. You can ignore them, even more so than in JS. MoonScript is dead anyway. Check out yuescript instead: https://github.com/pigpigyyy/Yuescript reply hnax 3 hours agorootparentlua-classy (https://github.com/siffiejoe/lua-classy) works like gem and is fast! reply elpocko 3 hours agorootparentIt's 10x larger than it needs to be, and seems to do a lot of unnecessary stuff. I would call this one hopelessly bloated and overengineered and recommend something like https://github.com/Yonaba/30log instead. reply thefaux 2 hours agoparentprevI would strongly recommend just learning vanilla lua. The beauty of lua is that it is lightweight and runs anywhere so it is worth learning on its own terms. Wrapper languages come and go, but lua 5.1 is still highly usable even though it hasn't changed its feature set in 15 years (and never will). reply otikik 4 hours agoparentprevStart with Lua. Moonscript produces Lua so you will have to deal with Lua if you’re ever to debug a Moonscript program. The opposite isn’t true reply _heimdall 4 hours agoparentprevI wouldn't run from classes just because JS historically has, though coming from JS I'd lean towards lua for a more familiar function calling syntax. reply pfd1986 6 hours agoprevGreat name! (Portuguese word for 'pencil') reply jszymborski 6 hours agoparentHuh, here I was just assuming it was a reference to \"lapis lazuli\" but seeing as lua is Portuguese for moon, \"pencil\" makes most sense here. Thanks! reply qsort 6 hours agoparentprevIt's Latin for \"stone\". Also used in Italian as a regional/alternative word for \"pencil\". reply wonks 4 hours agorootparentAs in \"lua rock\"? reply Lalabadie 2 hours agorootparentMoon rock, I suppose reply VWWHFSfQ 6 hours agoprev [–] We started out using lapis in nginx but unfortunately had to abandon it because it basically takes away all the power of using regular location {}. You have to mount your whole app under like location / which is lame for an nginx-native framework. It's got some nice utilities that we use, but we ended up just using regular location {} routes each with their own content_by_lua code block and none of the lapis routing/handler stuff. reply leafo 2 hours agoparentI suppose I haven't really fully considered this use case, but... There's no requirement to put your entire app in a location / {} block. You can freely use as many location blocks as you want, and those that you want to be rendered by Lapis can call serve to the app as normal. eg. location = /exact-match { content_by_lua 'require(\"lapis\").serve(\"app\")'; } location /directory-match { content_by_lua 'require(\"lapis\").serve(\"app\")'; } Keep in mind pattern matching will still happen in the app: You will need to define the routes handled by Lapis within the definition of the Lapis app. Why is it done this way? Primarily, for named routes. Typically you want to be able to generate the URL of a resource within your app's code, so by having routes defined in Lua you can easily reference those. Secondly, easy parameter parsing and the parameter validation. > It's got some nice utilities that we use, but we ended up just using regular location {} routes each with their own content_by_lua code block and none of the lapis routing/handler stuff. Although it's very possible to pick and choose what components to use, keep in mind that the `serve` function does some important work with connection pooling. If you are using any query related functionality outside of a dispatch it will open and close connections per request, which is not ideal for performance. reply xrd 4 hours agoparentprevDo you know if you can use lapis to proxy to a backend, essentially getting the benefits of nginx proxy, but perhaps allowing you to layer an authz/authen handler on top using lapis? Thanks for your comment. reply pacifika 6 hours agoparentprev [–] Why is that a problem? reply VWWHFSfQ 6 hours agorootparent [–] Because we want to use rewrites, jumps, named handlers, etc. in nginx too. But you can't do that once you delegate all your routing to Lua. At that point all nginx is doing is a socket event loop. We weren't happy to lose all the power of nginx just to use a framework. reply binary132 5 hours agorootparent [–] I would think you would treat the OpenRESTy component as just an executable binary application server just as you would with any other webservice, and put your nginx stuff in front of it, just like you would with any other webserver's front end lb. I agree that it does sound nice to handle it as another component in a regular nginx config, but it doesn't sound like a dealbreaker. Just treat the nginx stuff as incidental to the implementation and it seems like it would be no big deal. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lapis is a web framework for building applications in Lua or MoonScript, optimized for the high-performance OpenResty platform but also compatible with other server environments.",
      "Version 1.16.0 introduces `types.params_map` and improved `model:update`, enhancing its functionality.",
      "Key features include URL routing, HTML templating, CSRF protection, session support, and database models for PostgreSQL, MySQL, and SQLite, leveraging LuaJIT for efficient execution within Nginx and supporting asynchronous operations via Lua coroutines."
    ],
    "commentSummary": [
      "Lapis, a Lua web framework used by itch.io, is primarily driven by itch.io's needs, causing delays in features like websocket support due to OpenResty's API limitations.",
      "Discussions on Hacker News highlight the rights of project maintainers to reject non-core feature pull requests and advocate for transparency about project limitations.",
      "Users debate the benefits and drawbacks of Lua and its variants, with some preferring alternatives like Nelua, Teal, and Luau, but the consensus favors learning vanilla Lua for its simplicity and stability."
    ],
    "points": 136,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1716634601
  },
  {
    "id": 40472226,
    "title": "Neural Network Learns to Play NetHack, Encounters Bizarre Bug",
    "originLink": "https://twitter.com/cupiabart/status/1793930355617259811",
    "originBody": "So here&#39;s a story of, by far, the weirdest bug I&#39;ve encountered in my CS career.Along with @maciejwolczyk we&#39;ve been training a neural network that learns how to play NetHack, an old roguelike game, that looks like in the screenshot. Recenlty, something unexpected happened. pic.twitter.com/AFTgRm1gtv— Bartłomiej Cupiał (@CupiaBart) May 24, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40472226",
    "commentBody": "You are lucky, full moon tonight (twitter.com/cupiabart)134 points by ca98am79 16 hours agohidepastfavorite49 comments frogulis 15 hours agohttps://threadreaderapp.com/thread/1793930355617259811.html reply kybernetikos 10 hours agoparentThanks, this story would otherwise have been unreadable for me. Bit of a difference from the recent mastodon thread about writing a unix https://fosstodon.org/@drewdevault/112319697309218275 I wish people would stop using twitter as the prime host for these kinds of things. reply GeoAtreides 11 hours agoparentprevWhy is metadat's comment flagged and dead? It seems like a perfectly fine comment to me: \"Thanks, multi-part Twitter links are now held hostage behind a registration wall, making them unreadable and lost for most.\" reply metadat 15 hours agoparentprevThanks, multi-part Twitter links are now held hostage behind a registration wall, making them unreadable and lost for most. reply aorloff 14 hours agorootparentFediverse is now integrated with Threads, and Threads is bigger than Twitter. With any luck, Fediverse becomes the open source, Threads kills X, and news media continues to die the same slow death it would have anyway. If you haven't noticed, the kids don't read, they like to watch. reply emsign 14 hours agoparentprevThank you reply 38 14 hours agoparentprevnext [4 more] [flagged] Arrath 13 hours agorootparentNot being logged in, Twitter is hostile and/or broken and only ever displays a single tweet at once, no threads and no replies. I'll take thread reader with an ad blocker over that. reply mproud 14 hours agorootparentprevI’ll take it over X.com reply dietr1ch 12 hours agorootparentprevThey can add more crap and it'd still be way better than getting a login page for a site you no longer have an account for. reply cperciva 14 hours agoprevRelated story for people who haven't heard about it: At one point CERN had problems which seemed to be related to the phase of the moon. After much head scratching, it turned out that the problem -- with an underground particle accelerator -- was indeed related to the moon phase, since the moon's gravitational pull was deforming the path of the particle beam. reply robocat 13 hours agoparentI've heard the same story about a telephone line in Marlborough: back in the black&white analogue times. The story being that king tides were getting a connection wet near the shore and causing problems. reply arnaudsm 12 hours agoparentprevI wonder how they corrected it. By calibration, or carefully modeling the deformation according to the moon's mass? reply p4bl0 14 hours agoparentprevGenuine question: how does the moon phase influence its gravity effect? reply schrectacular 14 hours agorootparentI'm guessing it's more to do with the interaction of the gravitational vectors of sun and moon. During a half-moon it is pulling at a right angle to the sun, during a new-moon with the sun, and during a full-moon against the sun. reply p51-remorse 14 hours agorootparentprevProbably the same cause of tide supercycles - when moon and sun line up, the pull is additive. When they oppose, the moon detracts from the sun’s pull by a bit. Writing that out I’m not sure it makes sense, but someone smarter will be along to correct me shortly. reply cgriswald 14 hours agorootparentprevWhen the moon is full it is opposite the sun and reduces the effect of the sun’s gravity. When it is new it is in the same direction as the sun and increases the pull toward the sun. reply andresgottlieb 14 hours agorootparentprevI guess they probably meant moon position in orbit, in general reply emsign 14 hours agoprevPeople on X don't realize people can't read their stories if they're not logged in? I hope this cancer called X finally dies. reply hunter2_ 12 hours agoparentI want to agree, except that I believe when people authenticate to a particular site and post something, they're only really concerned with readership by other people who authenticate to that same site. Like if I have a car problem and post about it on a car forum, the ability for anonymous users to read my post is of little concern because they're not nearly as likely to reply to me as someone who's already logged in. If my goal was to spread information more widely instead of soliciting in-band participation, then in that specific scenario yes of course I need to find some other way to do that, but I don't think that's a typical goal with sites that strongly favor authentication; the goal is to get likes, replies, and so forth. reply sureglymop 11 hours agorootparentWhy shouldn't they instead just get the ability to control that? Give them options to set who can or can't see their content. reply hunter2_ 3 hours agorootparentMy guess is advertising CPI is much higher for authenticated traffic, and crippling the anonymous experience is found to maximize authenticated traffic. But now I'm speculating on why the developers of the site do what they do. In my previous comment, I was speculating on why users of the site do what they do. While the former obviously influences the latter, motives and goals are quite distinct for each. reply phkahler 10 hours agorootparentprevThat's called Facebook. reply tryauuum 9 hours agorootparentfacebook? I can't remember a facebook page which allowed me to scroll reply cgriswald 14 hours agoparentprevTiktok has stopped allowing me to view videos when not registered and logged in. I only view them when they are sent to me. I’m never going to register but they’ve made it less useful to the registered people who send me links. reply CGamesPlay 13 hours agoprevA few years ago I added these banners to my shell prompt. It shows like the MOTD when opening a terminal on a full moon, new moon, or Friday the 13th (these are the special dates used in Nethack). https://github.com/CGamesPlay/dotfiles/blob/master/files/.co... reply mmsc 15 hours agoprevCorrect me if I'm wrong, but didn't a friend of mine fisted/fstd/Timo \"solve\" this game years ago and make a bot to raep the highscore? Not saying that this isn't a cool project and information, but my understanding is that the game has bugs/\"working as intended\" which allow anyone to basically auto click to rack up thousands of points. P.S Timo if you're reading this I love you with all my collection))) <3 reply omoikane 13 hours agoparentI remember reading about a bot that completed a NetHack ascension speedrun by hacking the random number generator, but it's probably not the one you were thinking of: https://pellsson.github.io/ reply p4bl0 14 hours agoprevI submitted this link yesterday, I hesitated on the title because the nethack reference would have been a good one but decided against it to avoid spoiling the bug, but as a result it didn't get traction. Too bad for HN readers that the title which worked here is the one with the spoiler! Still a fun story though, glad it made it to the front page one way or another :). reply fsmv 15 hours agoprevBut it's literally not a bug at all it's a weird feature in the game they're doing machine learning on reply peddling-brink 15 hours agoparentThe bug is in their model and codes reaction to unexpected behavior. I don’t think the author is postulating that the game or lunar cycle has the bug. reply klyrs 15 hours agoprevEvery damned time a computer scientist approaches the problems of an unfamiliar discipline, I swear. reply dskloet 15 hours agoprevWhy spoil the punch line in the title? reply peddling-brink 14 hours agoparentIt wasn’t spoiled unless you already knew that particular fact. reply cdchn 14 hours agoprevThey mention something called Singularity which makes their environment \"one file\". That sounded kind of interesting but totally ungoogleable just based on the name. reply __lm__ 13 hours agoparentSingularity (now called Apptainer - https://apptainer.org/) is a container system generally used in HPC environments that has some nice features, like having your container as single .sif file, automatically mounting your home directory, using the same user inside and outside the container, and the ability to import from the docker registry. The approach is less about isolation and more about “packaging up” an entire environment in a easy-to-use way. reply cdchn 12 hours agorootparentAhh thats what I was hoping to see. Thanks! reply neilv 13 hours agoprev\"Full moon\" (or \"phase of moon\") used to be right up there with \"solar flares\", as a joke explanation for a computer system problem. reply robocat 13 hours agoparentWhat do you blame your heisenbugs on now? reply neilv 13 hours agorootparentLooks like we still have a few good explanations in reserve: https://gist.github.com/chuckwagoncomputing/ff980f81482b5894... reply 23B1 12 hours agorootparentprevLLM autophagy reply taneq 12 hours agorootparentprevEMI (Edit: electromagnetic interference) mostly. :D Or poor cable terminations. reply ufo 15 hours agoprevBy the way, don't forget to appreciate the full moon tonight. It is no coincidence that this got posted today. :) reply colordrops 15 hours agoprevPretty fun story, but not that mindblowing or weird. If a piece of software takes unpredictable inputs like time and date, it's a relatively obvious source of nondeterminism. It's not common in games which is why they were befuddled for so long. But their environment was hardly hermetically sealed it seems. reply cdchn 14 hours agoparentMaking the system clock always be the same is the ultimate is hermeticity. reply Steuard 15 hours agoprevI guess I'm not entirely clear on the background/context here. I gather that the tweet's author isn't a serious Nethack player himself, but he is trying to train a neural net to play the game, and his training system somehow takes a model created by someone else as a baseline and tries to fine tune it somehow? But despite Nethack being based on randomly generated dungeons, the other model gets a consistent score every time, somehow? But even though the reference system reliably gets the same score through all the randomization of the dungeon, the game's full moon mechanic somehow throws it off significantly. I feel like I mostly understand most of the pieces of this story when taken individually, but I'm having trouble assembling most of them into a coherent whole. reply EdwardDiego 14 hours agoparentTheir model is trained only on runs it has seen before which didn't include sufficient full moon runs. So its performance degraded when it encountered a sufficiently novel variant. Which is where the I part of AI always falls down, input that sufficiently differs. E.g., train facial recognition on a corpus of predominately white American faces, African Americans suffer a horribly high false positive rate when the cops use your model on surveillance footage. reply recursivecaveat 15 hours agoparentprevThe score they're reporting is almost definitely the average over a set of fixed seeds I imagine. They just didn't realize that the seed is not sufficient to establish the play experience, the system clock is a factor too. reply CJefferson 14 hours agoparentprevI don't know exactly about this case, but when training models like this I tend to fix a small set, or even just one, seeds to use as a baseline 'quality measure' -- while this has the risk of over-tuning, always measuring quality using random seeds means you can misjudge a model's quality because you get particularly lucky, or unlucky, seeds. However (and again I've hit this), sometimes you don't fix everything enough, and still have some unexpected variation, like in this case. reply rvnx 11 hours agoprev [–] TL;DR: \"NetHack is a game where real-world date and time can affect the game\" https://nethackwiki.com/wiki/Time like Animal Crossing, Pokemon, The Sims, Stardew Valley, World of Warcraft, Terraria, etc. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bartłomiej Cupiał recounted an unusual bug encountered while training a neural network to play the roguelike game NetHack.",
      "The project involved collaboration with Maciej Wolczyk, highlighting the complexities and unexpected challenges in AI training.",
      "This story underscores the unpredictable nature of developing AI systems, especially in complex environments like games."
    ],
    "commentSummary": [
      "Hacker News users express frustration with Twitter's usability, especially for multi-part threads that require logging in, and suggest alternatives like Thread Reader and Fediverse.",
      "The discussion humorously attributes computer issues to \"full moons\" or \"solar flares\" and explores technical explanations for unpredictable software behavior, such as electromagnetic interference (EMI) and poor cable terminations.",
      "The thread also covers the impact of real-world time on AI models and games like NetHack, highlighting the challenges of training AI with fixed seeds and the risks of over-tuning."
    ],
    "points": 134,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1716604556
  },
  {
    "id": 40475578,
    "title": "Google Rushes to Fix AI Search Tool After Bizarre Response Errors",
    "originLink": "https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai",
    "originBody": "Google/ Tech/ Artificial Intelligence Google scrambles to manually remove weird AI answers in search Google scrambles to manually remove weird AI answers in search / The company confirmed it is ‘taking swift action’ to remove some of the AI tool’s bizarre responses. By Kylie Robison, a senior AI reporter working with The Verge's policy and tech teams. She previously worked at Fortune Magazine and Business Insider. May 25, 2024, 12:10 AM UTC Share this story If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement. Image: Álvaro Bernis / The Verge Social media is abuzz with examples of Google’s new AI Overview product saying weird stuff, from telling users to put glue on their pizza to suggesting they eat rocks. The messy rollout means Google is racing to manually disable AI Overviews for specific searches as various memes get posted, which is why users are seeing so many of them disappear shortly after being posted to social networks. It’s an odd situation, since Google has been testing AI Overviews for a year now — the feature launched in beta in May 2023 as the Search Generative Experience — and CEO Sundar Pichai has said the company served over a billion queries in that time. But Pichai has also said that Google’s brought the cost of delivering AI answers down by 80 percent over that same time, “driven by hardware, engineering and technical breakthroughs.” It appears that kind of optimization might have happened too early, before the tech was ready. “A company once known for being at the cutting edge and shipping high-quality stuff is now known for low-quality output that’s getting meme’d,” one AI founder, who wished to remain anonymous, told The Verge. Google continues to say that its AI Overview product largely outputs “high quality information” to users. “Many of the examples we’ve seen have been uncommon queries, and we’ve also seen examples that were doctored or that we couldn’t reproduce,” Google spokesperson Meghann Farnsworth said in an email to The Verge. Farnsworth also confirmed that the company is “taking swift action” to remove AI Overviews on certain queries “where appropriate under our content policies, and using these examples to develop broader improvements to our systems, some of which have already started to roll out.” Gary Marcus, an AI expert and an emeritus professor of neural science at New York University, told The Verge that a lot of AI companies are “selling dreams” that this tech will go from 80 percent correct to 100 percent. Achieving the initial 80 percent is relatively straightforward since it involves approximating a large amount of human data, Marcus said, but the final 20 percent is extremely challenging. In fact, Marcus thinks that last 20 percent might be the hardest thing of all. “You actually need to do some reasoning to decide: is this thing plausible? Is this source legitimate? You have to do things like a human fact checker might do, that actually might require artificial general intelligence,” Marcus said. And Marcus and Meta’s AI chief Yann LeCun both agree that the large language models that power current AI systems like Google’s Gemini and OpenAI’s GPT-4 will not be what creates AGI. Look, it’s a tough spot for Google to be in. Bing went big on AI before Google did with Satya Nadella’s famous “we made them dance” quote, OpenAI is reportedly working on its own search engine, a fresh AI search startup is already worth $1 billion, and a younger generation of users who just want the best experience are switching to TikTok. The company is clearly feeling the pressure to compete, and pressure is what makes for messy AI releases. Marcus points out that in 2022, Meta released an AI system called Galactica that had to be taken down shortly after its launch because, among other things, it told people to eat glass. Sounds familiar. Google has grand plans for AI Overviews — the feature as it exists today is just a tiny slice of what the company announced last week. Multistep reasoning for complex queries, the ability to generate an AI-organized results page, video search in Google Lens — there’s a lot of ambition here. But right now, the company’s reputation hinges on just getting the basics right, and it’s not looking great. “[These models] are constitutionally incapable of doing sanity checking on their own work, and that’s what’s come to bite this industry in the behind,” Marcus said. Most Popular Most Popular EcoFlow’s $200 PowerStream is so clever, you might buy a $4,000 solar generator Samsung Galaxy Watch 7 ‘Ultra’ leaks show a supersized squircle for your wrist Google scrambles to manually remove weird AI answers in search Google promised a better search experience — now it’s telling us to put glue on our pizza Spotify is going to break every Car Thing gadget it ever sold Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=40475578",
    "commentBody": "Google scrambles to manually remove weird AI answers in search (theverge.com)131 points by rntn 3 hours agohidepastfavorite185 comments zogrodea 10 minutes agoThis approach to remove bad search suggestions manually reminded of a different approach Google once took, where they weren’t satisfied with manually tweaking search results but rather wanted to tweak the algorithm that produces these results when there were bad results. 'Around 2002, a team was testing a subset of search limited to products, called Froogle. But one problem was so glaring that the team wasn't comfortable releasing Froogle: when the query \"running shoes\" was typed in, the top result was a garden gnome sculpture that happened to be wearing sneakers. Every day engineers would try to tweak the algorithm so that it would be able to distinguish between lawn art and footwear, but the gnome kept its top position. One day, seemingly miraculously, the gnome disappeared from the results. At a meeting, no one on the team claimed credit. Then an engineer arrived late, holding an elf with running shoes. He had bought the one-of-a kind product from the vendor, and since it was no longer for sale, it was no longer in the index. \"The algorithm was now returning the right results,\" says a Google engineer. \"We didn't cheat, we didn't change anything, and we launched.\"' https://news.ycombinator.com/item?id=14009245 reply ttGpN5Nde3pK 24 minutes agoprevMy whole qualm with this AI integration into search engines: it's a search engine, not a question engine. I go to google to search the internet for something, not ask it a question. IMO, asking AI for something is a different task than searching the internet. It's sorta the same problem as if I go into a store and ask an employee where something is, and they reply with \"well what are you trying to do?\" reply notatoad 2 minutes agoparent>it's a search engine, not a question engine. for a lot of people and in a lot of use cases, it is a tool for answering questions. it generally works well for that. i get that the AI implementation sucks, but to suggest that people don't use google to find the answer to questions is absurd. that's absolutely what it's for. reply bombela 9 minutes agoparentprevI sometimes wants a search engine, sometimes a question engine. Likewise at the store. Why not have both with a way to choose which one I want on the moment? reply PillCosby 13 minutes agoparentprevLike the overly helpful person at the local hardware store. reply jerf 2 hours agoprev\"Achieving the initial 80 percent is relatively straightforward since it involves approximating a large amount of human data, Marcus said, but the final 20 percent is extremely challenging. In fact, Marcus thinks that last 20 percent might be the hardest thing of all.\" 100% completely accurate is super-AI-complete. No human can meet that goal either. No, not even you, dear person reading this. You are wrong about some basic things too. It'll vary from person to person what those are, but it is guaranteed there's something. So 100% accurate can't be the goal. Obviously the goal is to get the responses to be less obviously stupid. Which, while there are cynical money-oriented business reasons for, it is obviously also a legitimate hole in the I in AI to propose putting glue on pizza to hold the cheese on. But given my prior observations that LLMs are the current reigning world-class champions at producing good sounding text that seems to slip right past all our system 1 thinking [1], it may not be a great thing to remove the obviously stupid answers. They perform a salutatory task of educating the public about the limitations and giving them memorable hooks to remember not to trust these things. Removing them and only them could be a net negative in a way. [1]: https://thedecisionlab.com/reference-guide/philosophy/system... reply lukev 2 hours agoparentI feel like there's some semantic slippage around the meaning of the word \"accuracy\" here. I grant you, my print Encyclopedia Britannica is not 100% accurate. But the difference between it and a LLM is not just a matter of degree: there's a \"chain of custody\" to information that just isn't there with a LLM. Philosophers have a working definition of knowledge as being (at least†) \"justified true belief.\" Even if a LLM is right most of the time and yields \"true belief\", it's not justified belief and therefore cannot yield knowledge at all. Knowledge is Google's raison d'etre and they have no business using it unless they can solve or work around this problem. † Yes, I know about the Gettier problem, but is not relevant to the point I'm making here. reply jononor 1 hour agorootparentEncyclopedia Britannica is also wrong in a reproducible and fixable way. And the input queries a finite set. It's output does not change due to random or arbitrary things. It is actually possible to verify. LLMs so far seem to be entirely unverifiable. reply lukev 1 hour agorootparentThey don’t just seem it. They are by design. We talk about models “hallucinating” but that’s us bringing an external value judgement after the fact. The actual process of token generation works precisely the same. It’d be more accurate to say that models always hallucinate. reply madeofpalk 50 minutes agorootparentYes - this is what i've been saying all the time. The term 'hallucinations' is misleading because the whole point of LLMs is that they recombine all their inputs into something 'new'. They only ever hallucinate outputs - that's their whole point! reply wizzwizz4 29 minutes agorootparentInto something probable. The models that underlie these chatbots are usually overfitted, so while they usually don't repeat their training data verbatim, they can. reply somenameforme 1 hour agorootparentprevLLMs are completely deterministic even if that's kind of weird to state because they output things in terms of probabilities. But if you simply took the highest probability next word, you'd always yield the exact same output given the exact same input. Randomness is intentionally injected to make them seem less robotic through the 'temperature' parameter. Why it's not just called the rng factor is beyond me. reply dleeftink 44 minutes agorootparentMaybe some models can be deterministic at a point in time, but train it for another epoch with slight parameter changes and a revised corpus and determinism goes out the proverbial (sliding) window real quick. This is not unwanted per se, and the exact feedback loop that needs improving to better integrate new knowledge or revise knowledge artefacts incrementally/post-hoc. reply Vetch 2 minutes agorootparentIf you train it then it's no longer the same model. If I have f(x) = x + 1 and change it to f(x) = x + 1 + 1/1e9, it would not mean that `f` is not deterministic. The issue would be in whatever interface I was exposing the f's at. ta8645 1 hour agorootparentprev> LLMs so far seem to be entirely unverifiable. I don't understand this complaint. Are they any less verifiable than a human? reply threeseed 1 hour agorootparentI can ask a human to explain the steps they took to answer a question. I can ask a human a question 100 times and I don't get back 100 different answers. None of those applies to an LLM. reply ta8645 1 hour agorootparentYou can ask an LLM to explain itself.. it will give you a logical stepwise progression from your question to its answer. It will often contain a mistake, but the same is true for a human. And if your LLM is giving you 100 different answers, then it has been configured to do so. Because instead, it could be configured to never vary at all. It could be 100% reproducible if so desired. reply bluefirebrand 36 minutes agorootparent> it will give you a logical stepwise progression from your question to its answer. No, it will generate a new hallucination that might be a logical stepwise progression from the question you asked to the answer you gave, but it is not due to any actual internal reasoning being done by the LLM reply ta8645 31 minutes agorootparentSo what? You have no way to know for sure if the human you ask the same question, does either. The question that started this thread was related to verifiability. And i still think it is a spurious complaint, given that we have exactly the same limitations when dealing with any human agent. reply bluefirebrand 7 minutes agorootparent> And i still think it is a spurious complaint, given that we have exactly the same limitations when dealing with any human agent We're not talking about an LLM that is trying to do the job of a human, here We're talking about an LLM that is trying to give authoritative answers to any question typed into the Google search bar It's already well past the scale that humans could handle Talking about human shortcomings when discussing LLMs is a red herring at best, or some kind of deliberate goalpost shifting at worst wizzwizz4 27 minutes agorootparentprevThe problem of other minds is no reason to throw everything out the window. Humans are capable of being conscious of their reasoning processes; token-at-a-time predictive text models wired up as chatbots aren't capable of it. Your choice is between a possibly-mistaken, possibly-lying human, and a 100%-definitely incapable computer program. You don't know either \"for sure\", but you don't know that the external world exists \"for sure\" either. It's an insight-free observation, and shouldn't be the focus of anyone's decision-making. reply ta8645 16 minutes agorootparentYou've made some interesting points, which are debatable, for sure. But you've failed to address the question being asked about \"verifiability\". kenjackson 1 hour agorootparentprevAsk a human what the meaning of life is and how it impacts their day to day interactions. I know I can tell you an answer but I couldn’t tell you steps about how I got it. And if you asked it to me twice I’d definitely give different answers unless you told me to give the same answer. In part I’d give a different answer because if someone asks me the same question twice I assume the first answer wasn’t sufficient. reply threeseed 50 minutes agorootparentNo one is taking about existential questions about meaning of life. We are talking about basic things like whether or not to eat rocks or put glue in recipes. We can answer those questions with a chain of logic and repeatability. reply eynsham 1 hour agorootparentprev> it’s not /justified/ belief Beliefs derived from the output of LLMs that are ‘right most of the time’ pass one facially plausible precisification of ‘justification’ in that they are generated by a reliable belief-generation mechanism (see e.g. Goldman). To block this point one must engage with the post-Gettier literature at least to some extent. There is a clear difference between beliefs induced by reading the outputs of LLMs and those induced by the contents of a reference work, but it is inessential to the point and arguably muddies the water to present the distinction as difference in status as knowledge or non-knowledge. reply benrutter 2 hours agoparentprev> So 100% accurate can't be the goal. Obviously the goal is to get the responses to be less obviously stupid. I'm not sure I agree. I think you're right that 100% accuracy is potentially unfeasable as a realistic aim, but I think the question is how accurate something needs to be in order to be a useful proposition for search. AI that's as knowledgable as I am is a good achievement and helpful for a lot of use cases, but if I'm searching \"What's the capital of Mongilia\" someone with averageish knowledge taking a punt with \"Maybe Mongoliana City?\" is not helpful at all- if I can't trust AI responses to a high degree, I'd much rather just have normal search results showing me other resources I can trust. Google's bar for justifying adding AI to their search proposition isn't \"be better than asking someone on the street\", it's \"be better than searching google without any AI results\" reply smashed 2 hours agorootparentThe problem is that in all the shared examples, Google ai search does not respond with a Maybe xyz, question mark? like you did. It always answers with high confidence and can't seem to navigate any gray area where there are multiple differing opinions or opposing source of truths. reply namaria 2 hours agorootparentYeah the \"manipulating language cogently is intelligence\" premise that underlines this \"AI\" cycle is proving itself wrong in a grand way. reply skybrian 1 hour agoparentprevOr to put it another way, I think Google should have a way of saying \"yes, we know this result is wrong, but we're leaving it in because it's funny.\" There is a demand for funny results. Someone asking “how many rocks should I eat” is looking for entertainment, so you might as well give it to them. reply Szpadel 1 hour agoparentprevI think the biggest difference with human (and the most important one) is that human can tell you \"I have no idea, this isn't my field\" or \"I'm just guessing here\" but LLMs will confidently say to super stupid statement. AI doesn't know what it knows. if you only score where human provide answer, then human score would be probably in high 90s reply pankajkumar229 22 minutes agorootparentI find irony here. reply notnullorvoid 1 hour agoparentprev100% accuracy should be the goal, but the way to achieve that isn't going to from teaching an AI to construct a definitive sounding answer to 100% of questions. Teaching AI how to respond with \"I don't know\", and give confidence scores is the path to nearing 100% accuracy. reply praisewhitey 1 hour agoparentprevYou're looking at it the wrong way, the goal should be 0% inaccurate. Meaning for the 20% of things it can't answer, it shouldn't make something up. reply saagarjha 1 hour agoparentprevThankfully a billion people are not asking me for answers to things, so it's OK if I am wrong sometimes. reply fragmede 1 hour agorootparentNor am I being treated as an omniscient magic black box of knowledge. Hilariously though, polyvinyl acetate, the main ingredient in Elemers glue is used as a binding agent to keep emulsions from separating into oil and water, and is used in chewing gum, and covers citrus fruits, sweets, chocolate, and apples in a glossy finish, among other food things. reply willis936 51 minutes agoparentprevFailing loudly is an excellent feature. \"More compelling lies\" is not the answer. reply bluefirebrand 39 minutes agoparentprev> You are wrong about some basic things too Sure, but probably not \"add glue to pizza to get the cheese to stick\" wrong... reply dspillett 9 minutes agorootparentAt least it suggested non-toxic glue… That suggests some context about recipes needing to be safe is somehow present in its model. reply bluefirebrand 3 minutes agorootparentMost likely this has nothing to do with \"recipes being safe\" being in the model It seems the glue thing comes from a reddit shitpost from some time ago. There's a screenshot going around on twitter about it[0](11 years in the screenshot but no idea when it was taken) It specifically mentions \"any glue will work as long as it is non-toxic\" so best guess is that's why google output that [0]https://x.com/kurtopsahl/status/1793494822436917295?t=aBfEzD... reply tomrod 1 hour agoparentprevIf everyone can be wrong, then might the assertion that all are wrong committing this same fallacy? \"Can\" is not destiny, perhaps you have met people who are fully right about the basics but you just didn't sufficiently grok their correctness. reply SoftTalker 2 hours agoparentprev> putting glue on pizza to hold the cheese on It's actually not the dumbest idea I've heard from a real person. So no surprise it might be suggested by an AI that was trained on data from real people. reply krapp 2 hours agorootparentIt wasn't an idea, though. It was a joke someone made on Reddit. If an AI can't tell the difference, it shouldn't be responsible for posting answers as authoritative. reply dspillett 8 minutes agorootparentIt may not be a joke. Perhaps it has confused making food for eating with directions for preparing food for menu photography and other advertising. reply dgellow 2 hours agorootparentprevInsane people at Google thought it would be a good idea to let Reddit of all places drive their AI search responses reply oldgradstudent 1 hour agorootparentReddit is a magnificent source of useful knowledge. r/AskHistorians r/bikewrench To name just two. There is nothing even remotely comparable. But you need to be able to detect sarcasm and irony. reply mvdtnz 13 minutes agorootparentI have seen a tremendous amount of bad advice on bikewrench. reply blablabla123 1 hour agorootparentprev...which is sometimes incredibly hard and it might not be possible because it's such a niche topic or people might be just wrong. Just thinking about Urban Myths, Conspiracy theories etc. where even without a niche factor things may sound unbelievable but actually disproving can be effort that is out of proportion reply warkdarrior 1 hour agorootparentprevIt is certainly popular here to run your web searches against reddit. Every post about how Google Search sucks ends up with comments on appending \"site:reddit.com\" to the search terms. reply candiddevmike 1 hour agorootparentprevI wonder what the impact all of those erase tools are having on LLM training. The ones that replaced all of these highly upvoted comments with nonsense. reply SirMaster 1 hour agorootparentI'm pretty sure those \"erase\" tools are just for the front-end and reddit keeps the original stuff in the back-end. And surely the deal Google made was for the back-end source data, or probably the data that includes the original and the edit. reply mvdtnz 18 minutes agoparentprev> No, not even you, dear person reading this. You are wrong about some basic things too. It'll vary from person to person what those are, but it is guaranteed there's something. The difference is that I'm not put on the interface of a product facing hundreds of millions of users every day to feed those users incorrect information. reply jsemrau 2 hours agoparentprevThis is statistics though. Edge cases are nothing new and risk management concepts have evolved around fat tails and anomalies for decades. Therefore the statement is as naive as writing a trading agent that is 100% correct. In my opinion, this error shows lack of understanding responsible scaling architectures. If this would be their first screw up I wouldn't mind, but Google just showed us a group of diverse Nazis. If there is a need for consumer protection for online services, it is exactly stuff like this. ISO 42001 lays out in great detail that AI systems need to be tested before they are rolled out to the public. The lack of understanding of AI risk management is apparent. reply wredue 1 hour agoparentprevIf I could delivery “80% correct” software for my workplace, my day would be a whole hell of a lot easier. reply bugglebeetle 2 hours agoparentprevYes, which is why the ability to sift accurate and authoritative sources from spam, propaganda, and intentionally deceptive garbage, like advertising, and present those high-quality results to the user for review and consideration, is more important than any attempt to have an AI serve a single right answer. Google, unfortunately, abandoned this problem some time ago and is now left to serve up nonsense from the melange of low-quality noise they incentivized in pursuit of profits. If they had, instead, remained focused on the former problem, it’s actually conceivable to have an LLM work more successfully from this base of knowledge. reply verisimi 31 minutes agoparentprev100% correct, 80% correct lol. The thing is that truth/reality is not a thing that is resolvable. Not even the scientific method has this sort of expectation! You can imagine getting close to those percentages, with regards to consensus opinion. That's just a question of educating people to respond appropriately. reply mvdtnz 11 minutes agorootparentNo. Whether a person should eat a certain number of small rocks each day is not a matter of opinion, it's not a deep philosophical problem and it's not a question whose truth is not resolveable. You should not be eating rocks. reply hatenberg 42 minutes agoparentprevSo google decides shipping 80% distilled crap is good enough. Yay reply Swizec 1 hour agoparentprev> No, not even you, dear person reading this. You are wrong about some basic things too. It'll vary from person to person what those are, but it is guaranteed there's something. Kahneman has a fantastic book on this called Noise. It’s all about noise in human decision making and how to counteract it. My favorite example was how even the same expert evaluating the same fingerprints on different occasions (long enough to forget) will find different results. reply PreInternet01 2 hours agoprevIt's debatable whether Google has truly lost the plot because of the \"AI wars\", but the moment the statement \"Bing returns more sensible results than you\" becomes verifiably true, it's... cause for concern? The approach that Google appears to have taken, which is to assume that the top-ranked part of its current search index is a sensible knowledge base, may have been true some years ago, but definitely isn't now: for whatever reasons, it's now 33% spam, 33% clickbait/propaganda, with the rest being equally divided between what could be called \"truths\" and miscellaneous detritus. To me, it seems that returning to the concept that search results should at least reflect a broad consensus of what is true is a necessary first step for Google. As part of that, learning to flag obvious trolling, clickbait and bad-faith content is paramount. And then, maybe then, they can start touting their LLM benefits. But until the realities of the Internet are taken into account (i.e.: it's 80% spam!), any \"we offer automated answers!\" play is doomed. reply rurp 52 minutes agoparentNot only is the current internet 80% spam, it's rapidly approaching 99% thanks in large part to LLMs. At this point I would be shocked if Google had a solid plan for how to handle this going forward as the problem space gets more difficult. reply EasyMark 27 minutes agorootparentthat's the part that scares me. I railed on someone's comment the other day about \"indexes will come back into fashion\" but the more I think about how much garbage has increased in just the past 2 to 3 years, I think I was wrong. Indexes and forums may be the only way to have a sane net where you can find things. Perhaps communities linking together in a ring like format, a \"web ring\" of sorts. reply ysavir 22 minutes agorootparentWhat I've been wanting to see for a while now is a social-network based search engine: * No pages are indexed automatically. The only indexed pages are pages that users say are worth indexing. Probably have a browser add-on for a one button click that people can use. * You can friend/follow others * Your search results are a combination of your own indexed pages and the pages indexed by people in your network. reply reustle 45 minutes agorootparentprevhttps://en.wikipedia.org/wiki/Dead_Internet_theory reply greg_V 1 hour agoparentprevOh it gets even better. The public has been hearing about AI this and AI that for over a year, but the existing use cases and deployment was confined to some super special niches like writing or the creative industries and programming. This is the first nation-scale deployment of the technology, running on Google's biggest and most profitable market in one of the most widely used internet services, and it's a shitshow. They can try manually fine tuning it, but all of the investors who have been throwing money at AI for the past year are now learning what this tech is like in the day-to-day, beyond just speculations, and it's looking... bad. reply itronitron 1 hour agorootparentIt's especially embarrassing for Google considering they have indexed virtually all of the world's information for the last 25 years. reply PreInternet01 1 hour agorootparentprevYeah, the most likely take here is that Google's leadership truly did not recognize how utterly awful the quality of their flagship search index had become over the years. I mean, it explains a lot, but still... you're recruited using industry-leading practices out of an overflowing pool of abundant talent... and this is what you make of it? As the kids say: SMH! reply lawn 1 hour agorootparent> you're recruited using industry-leading practices out of an overflowing pool of abundant talent The ridiculous focus on on leet-code is surely industry-leading (because whatever Google does becomes industry-leading) but it sure isn't a good way to filter for competency. reply rm_-rf_slash 1 hour agorootparentprevPlenty of people have been using ChatGPT for daily tasks for almost two years now. GPT-4 isn’t perfect but is otherwise really really good, and deftly handling use cases in my industry that would be impossible without it or however many billion dollars it would take to make GPT-4. From the black Nazis to the suggestion to jump off the Golden Gate Bridge b/c depression, it’s pretty clear that this fiasco isn’t an LLM problem, it’s a Google problem. reply lupire 1 hour agorootparentBecause no one cares when ChatGPT gets things wrong. reply thesimp 2 hours agoprevI'm actually shocked that a company that has spent 25 years on finetuning search results for any random question people ask in the searchbox does not have a good, clean, dataset to train an LLM on. Maybe this is the time to get out the old Encyclopedia Britannica CD and use that for training input. reply zihotki 2 hours agoparentThey spent 10 years finetuning the search and then another 15 finetuning ads and clicks. Google's business is ads, not search. reply 39896880 57 minutes agorootparentApologies in advance for this level pedantry: Google’s business is behavioral futures, not ads. Ads are just a means to that particular end. reply rurp 43 minutes agorootparentGoogle exchanges advertisement placement for money. Ads are their business by any normal definition of that term. reply 39896880 13 minutes agorootparentGoogle’s transformation of conventional methods into means of hypercapitalist surveillance is both pervasive and insidious. The “normal definition of that term” hides this. reply ImAnAmateur 48 minutes agorootparentprevSurveillance capitalism? What are behavioral futures? reply kredd 2 hours agoparentprevIt’s a bit weird since Google is taking over the “burden of proof”-like liability. Up until now, once user clicked on a search result, they mentally judged the website’s credibility, not Google’s. Now every user will judge whether data coming from Google is reliable or not, which is a big risk to take on, in my opinion. reply shombaboor 2 hours agorootparentthey went from \"look at this dumbass on reddit\" to \"no it is I (Google) who is in fact the dumbass\". It's an interesting strategy to say the least. reply seadan83 2 hours agorootparentprevThat latter point might be illuminating for a number of additional ideas. Specifically, should people have questioned Google's credibility from the start? Ie: these are the search results, vs this is what google chose. Google did well in the old days for reasons. It beat alta vista and Yahoo by having better search results and a clean loading page. Since perhaps 08 (based on memory, that date might be off) or so, Google has dominated search, to the extent that it's not salient that search engines can be really questionable. Which is also to say, google dominated, people lost sight that searching and googling are different, that gives a lot of freedom for enshittification without people getting too upset or even quite realizing - it could be different and better reply flyingspaceship 2 hours agoparentprevGoogle doesn't look like they're fine tuning anything other than revenue reply SoftTalker 2 hours agoparentprevI am also surprised that training data are not much more curated. Encyclopedias, textbooks, reputable journals, newspapers and magazines make sense. But to throw in social media? Reddit? Seems insane. reply sgift 2 hours agorootparentEven some results from \"The Onion\" seem to be in it. Looks like Google just took every website they've ever crawled as source. reply helsinkiandrew 2 hours agorootparentprevThe problem is that for some searches and answers Reddit or other social media is fine. reply dgellow 1 hour agorootparentBut only if you do a lot of filtering when going through responses. It’s kind of simple to do as a human, we see a ridiculous joke answer or obvious astroturfing and move on, but Reddit is like >99% noise, with people upvoting obviously wrong answer because it’s funny, lots of bot content, constant astroturfing attempts. reply eldaisfish 10 minutes agorootparentprevNo, it isn't. Humans interacting with human-generated text is generally fine. You cannot unleash a machine on the mountains of text stored on reddit and magically expect it to tell fact from fiction or sarcasm from bad intent. reply typpo 2 hours agoparentprevThe problem in this case is not that it was trained on bad data. The AI summaries are just that - summaries - and there are bad results that it faithfully summarizes. This is an attempt to reduce hallucinations coming full circle. A simple summarization model was meant to reduce hallucination risk, but now it's not discerning enough to exclude untruthful results from the summary. reply PartiallyTyped 2 hours agoparentprevYou may find this illuminating. The google prior to 2019 isn’t the google of today. https://www.wheresyoured.at/the-men-who-killed-google/ Edit: there was also a discussion on HN about that article. reply hilux 1 hour agorootparentCoincidentally, I was just watching a video about how South Africa has gone downhill - and that slide was hastened by McKinsey advising the crooked \"Gupta brothers\" on how to most efficiently rip off the country. reply x0x0 2 hours agoparentprevI don't think it's true at all. Two reasons. The first, even ignoring that truth isn't necessarily widely agreed (is Donald Trump a raping fraud?), is that truth changes over time. eg is Donald Trump president? And presidents are the easiest case because we all know a fixed point in time when that is recalculated. Second, Google's entire business model is built around spending nothing on content. Building clean pristinely labeled training sets is an extremely expensive thing to do at scale. Google has been in the business of stealing other people's data. Just one small example: if you produced (very expensive at scale) clean, multiple views, well lit photographs of your products for sale they would take those photos and show them on links to other people's stores; and if you didn't like that, they would kick you out of their shopping search. etc etc. Paying to produce content upends their business model. See eg the 5-10% profit margin well run news orgs have vs the 25% tech profit margin Google has even after all the money blown on moonshots. reply mvkel 25 minutes agoprev1. Google announces something that has AI bolted on 2. A VP pontificates about how much work they did to \"get it right\" 3. A pretty easy-to-anticipate first-order issue surfaces 4. Sundar issues a statement like \"this is completely unacceptable. We will be making structural changes to ensure this never happens again.\"[0] 5. GOTO 1 [0] https://m.economictimes.com/tech/technology/sundar-pichai-ca... reply pilooch 2 hours agoprevSo Google hasn't used an LLM to generate and test weird queries ? This is not putting the bar very high for the whole industry... There'd be so much to gain from a clean deployment... Either it hard, either it is a rush. As a machine learnist, I believe it's actually impossible, by design of the autoregressive LLM. This race may we'll be partially to the bottom. reply nicklecompte 2 hours agoparentGoogle’s poor testing is hardly in doubt. But keep in mind that the whole problem is that LLMs don’t handle “unlikely” text nearly as well as “likely” text. So the near-infinite space of goofy things to search on Google is basically like panning for gold in terms of AI errors (especially if they are using a cheap LLM). And in particular LLMs are less likely to generate these goofy prompts because they wouldn’t be in the training data. reply ADeerAppeared 2 hours agoparentprev> So Google hasn't used an LLM to generate and test weird queries ? You don't even need an LLM for that. Google will almost certainly have tested. The test result is just politically-unacceptable within the company: It doesn't work, it's a architectural issue inherent to the technology, we can't fix it. Instead, they just rush to patch any specific, individual errors that show up, and claim that these errors are \"rare exceptions\" or \"never happened\". What's going on here is that Google (and most other AI firms) are just trying to gaslight the world about how error-prone AI is, because they're in too deep and can't accept the reality themselves. reply kwertyoowiyop 2 hours agorootparentDeploy the cheap offshore labor! reply cjk2 2 hours agorootparentprevThey already know it’s a shit show. They are trying to push it along until it’s someone else’s fault. reply ADeerAppeared 2 hours agorootparentI'm not convinced the executive layer is aware how dire the problem is. On one hand, their support for outsourcing programmes; \"Training Indians on how to use AI\", suggests they realize AI tooling without human cleanup is a crapshoot. On the other hand, they keep digging. This kind of gaslighting is an old and proven trick for genuinely rare problems, but it doesn't work if your issues are fairly common, as they'll get replicated before you can get a fix out. Similarly, they're gambling with immense legal risks and sacrificing core products for it. They're betting the farm on AI, it may kill the company. reply cjk2 1 hour agorootparentI think they are more than aware but will magically disappear after cashing their stock just about the point the bubble pops. Don't forget that the AI industry is almost 100% based on hype. Microsoft will be the largest victim here, their entire product portfolio being turned into a nuclear fallout zone almost overnight. Satya and friends are going to trash the whole org. I regularly speak to laypeople who assume that it's some magical thing without limits that makes their lives better. They are also 100% unaware of any applications that will actually make their lives better. End game occurs when those two disconnected thoughts connect and they become disinterested. The power users and engineers who were on it a year ago are either burned out or finding the limitations a problem as well now. There is only magical thinking, lies and hope left. Granted there are some viable applications but they are rather less overstated than anything we have no and there are even negative side effects of those (think image classification, which even if it works properly, requires human review and there are psychological and competence things problems around that too). reply sebastiansm 2 hours agoparentprevGoogle is working hard to be the next Boeing. reply CaptainOfCoit 2 hours agoparentprev> So Google hasn't used an LLM to generate and test weird queries ? What about simple manual testing? Seems to have skipped QA completely, automated or not. reply nicklecompte 2 hours agorootparentThere has been a lot of excitement recently about how using lower precision floats only slightly degrades LLM performance. I am wondering if Google took those results at face value to offer a low-cost mass-use transformer LLM, but didn’t test it since according to the benchmarks (lol) the lower precision shouldn’t matter very much. But there is a more general problem: Big Tech is high on their own supply when it comes to LLMs, and AI generally. Microsoft and Google didn’t fact-check their AI even in high-profile public demos; that strongly suggests they sincerely believed it could answer “simple” factual questions with high reliability. Another example: I don’t think Sundar Pichai was lying when he said Gemini taught itself Sanskrit, I think he was given bad info and didn’t question it because motivated reasoning gives him no incentive to be skeptical. reply flyingspaceship 1 hour agorootparentWell yeah imagine how much money there is to make in information when you can cut literally everyone else involved out, take all of the information and sell it with ads and only give people a link at the bottom, if that is even needed at all reply pilooch 2 hours agorootparentprevThe adversarial surface to the LLM remains enormous, manual cannot handle it. reply jameshart 2 hours agorootparentAsking how to prevent cheese from sliding off pizza is not an adversarial prompt. reply dlachausse 2 hours agoparentprevThey still haven’t learned from the Gemini diverse Nazis debacle. reply cdme 2 hours agoprevIt’s pre-alpha trash that’s worse than traditional search in every meaningful way. Kudos to the artist at the Verge for the accompanying image — those are fingers AI would be proud of. reply bithive123 2 hours agoprevWhy do people act like LLMs only hallucinate some of the time? reply dgellow 2 hours agoparentIt’s not hallucinations here, multiple of the ridiculous results can be directly traced to redit posts where people are joking or saying absurd things reply threeseed 1 hour agorootparentThere are examples of hallucinations as well e.g. talking about a Google AI dataset that doesn't exist and using a CSAM dataset which it doesn't. One of the researchers from Google Deepmind specifically said it was hallucinating. reply kccqzy 1 hour agoparentprevNot hallucinations but these AI answers often (always?) provide sources they link to. It's just that the source is a random Reddit or Quora post that's obviously just trolling. Then, when people post these weird AI answers on Reddit and come up with more absurd jokes, the AI then picks it up again. For example in https://www.reddit.com/r/comedyheaven/comments/1cq4ieb/food_... Google AI suggested applum and bananum as a response to food names ending with \"um\" when someone suggested uranium, Copilot AI started copied that suggestion. It's entertaining to watch. reply kwertyoowiyop 2 hours agoparentprevThe best trick the A.I. companies have pulled is getting us to refer to ‘bugs’ as ‘hallucinations.’ It sounds so much more sophisticated. reply chx 1 hour agorootparentAh, my friend it's not a bug It's a fundamental feature These LLMs can produce nothing else but since the bullshit they spew resembles an answer and sometimes accidentally collide with one, people tend to think it can give answers. But no. https://hachyderm.io/@inthehands/112006855076082650 > You might be surprised to learn that I actually think LLMs have the potential to be not only fun but genuinely useful. “Show me some bullshit that would be typical in this context” can be a genuinely helpful question to have answered, in code and in natural language — for brainstorming, for seeing common conventions in an unfamiliar context, for having something crappy to react to. > Alas, that does not remotely resemble how people are pitching this technology. reply kwertyoowiyop 1 hour agorootparentThat’s a good take. So LLMs distill human creativity as well as human knowledge, and it’s more useful when their creativity goes off the rails than when their knowledge does. reply dgellow 1 hour agorootparentprevIt’s not a trick to sound sophisticated. Hallucinations are more like a subcategory of bugs. The system is technically correctly generating, structuring, and presenting false information as fact. reply greg_V 1 hour agorootparentTechnically everything an LLM does is hallucination that happens to be on a scale between correct and non-correct. But only humans with knowledge can tell the difference, math alone can't. It's not even a bug: it's the defining feature of the technology! reply elwell 1 hour agorootparent> But only humans with knowledge can tell the difference Who says the humans (all of them) aren't hallucinating too? reply itronitron 1 hour agoparentprevit's only AI if you believe it reply notnullorvoid 1 hour agoparentprevHmm yeah I kinda like the concept that it's \"hallucinating\" 100% of the time, and it just so happens that x% of those hallucinations accurately describe the real world. reply empath75 42 minutes agorootparentThat x% is far higher than people think it is because there's a tremendous amount of information about the world that ai models need to \"understand\" that people just kind of take for granted and don't even think about. A couple of years ago, AI's routinely got \"the basics\" wrong, but now so often get most things right that people don't even think it's worth commenting on that they do. In any case, human consciousness is also a hallucination. reply dazc 3 hours agoprevManually removing rogue AI results is kind of ironic isn't it? reply JCM9 2 hours agoparentPay no attention to the army of people behind the curtain pulling levers trying to make it look like they’ve actually but a real AI. reply tflol 3 hours agoparentprevYou could almost argue these results are directly human generated. edit: And in that case, who is the arbiter of truth? reply rchaud 2 hours agoparentprevPay no attention to the Accenture contractors behind the curtain! reply nikanj 2 hours agoparentprevFor this tech cycle, AI is short for Actually Indians reply Barrin92 2 hours agoparentprevgenerative AI is essentially three day labourers from an emerging economy in a trenchcoat. From data labelling, to \"human reinforcement\", to manually cleaning up nonsensical AI results. reply seydor 2 hours agoparentprevWe need an AI for that reply water-your-self 52 minutes agoprevMost of the search results fixes are manual and are in response to publicity. You can typically find analagous problems for weeks/ quarters after things like this. reply caesil 2 hours agoprevWho knows how many of these are fake. People have been dropping inspect-element-manipulated screenshots all over twitter. https://www.nytimes.com/2024/05/24/technology/google-ai-over... > A correction was made on May 24, 2024: An earlier version of this article referred incorrectly to a Google result from the company’s new artificial-intelligence tool AI Overview. A social media commenter claimed that a result for a search on depression suggested jumping off the Golden Gate Bridge as a remedy. That result was faked, a Google spokeswoman said, and never appeared in real results. that screenshot was tweeted by @allgarbled. ten minutes before, they tweeted: >free engagement hack right now is to just inspect element on the google search AI thing and edit it to something dumb. hurry up, this deal won’t last forever reply Aloisius 3 minutes agoparentI have personally reproduced several like the interest one and the hippo eggs one, though not that one specifically. Google has started restricting AI Overviews so much now that most of the example queries on Google's Search Labs page doesn't even trigger it anymore. reply ethbr1 2 hours agoparentprevI'd say the broader issue here is a lack of transparency into results. If Google is sending bad results, who can prove that? reply realreality 2 hours agorootparentThat’s always been an issue. Years ago, researchers demonstrated in an experiment that they could swing public opinion about electoral candidates by manipulating search results. Who knows if Google took that experiment and ran with it? reply ethbr1 2 hours agorootparentI mean, that's always been the TikTok argument, to me. Widely-used platforms that can +/- 1% their algorithms to affect democracy have pretty high burdens of trust/transparency, and we're not close to that with any platform (Chinese or not) that I'm aware of. Meta's probably the closest, because of scrutiny, but afaik even their transparency isn't sufficient for realtime attestation. reply SoftTalker 3 hours agoprevTrained on Twitter and Reddit. Garbage in/Garbage out, as it has always been. reply threeseed 1 hour agoparentExcept that 90% of Reddit isn't garbage. It's really useful. Problem is Google can't tell what is garbage or not. No LLM can. reply SoftTalker 54 minutes agorootparentI'd argue it's far less than 90% but yes, there is some good information there. But weeding out the noise is what needs to happen, and (for some topics more than others) there is an awful lot of it. reply maximinus_thrax 43 minutes agorootparentprev> Except that 90% of Reddit isn't garbage. It's really useful. Citation needed. I've been a Reddit user since its inception and honestly except for niche hobby subreddits, Reddit is mostly low effort garbage, bots and rehashed content. I'd wager that mainstream subreddits are 99% garbage for training an LLM for anything other than shitposting. reply nuancebydefault 2 hours agoprevThe AI is often quoted without context. It actually answered, 'somebody has suggested adding glue...',which is different from 'add glue...'. reply nialv7 1 hour agoprevI am not surprised that AI results are bad. I know they are bad. But that doesn't concern me because I expect it to get better. What concerns me is that Google would push this trash to the front page. What are they even thinking? Who gave go ahead on this? reply ben_jones 1 hour agoparentInstitutional investors panic > board panics > executives panic > evps panic and dictates incentives to ship AI > directors, ems, and below, who actually know how shit works, take a submissive role because they have mortgages in Mountain View to pay. That’s how it happens. reply geuis 2 hours agoprevHey Google. Here's a really stupid idea. Knock it off. Your core search result product has gotten increasingly worse and less reliable over at least the last 5 years. YouTube's search results are nearly unusable. I can't imagine almost any external customer is asking for the AI bullshit thing that's just being shovelwared into everything Alphabet product now. I just noticed a couple days ago the gmail iOS app now does the same predictive completion that Copilot tries to do when I'm working. It's annoying as hell and I can't find how or if I can turn it off. Stop bullshitting around with ruining your products and get back to making money by making accessing information easier and more accurate. reply izacus 1 hour agoparentGoogle: Hey geuis, our revenue is record, our stock value is record, our metrics are all at record. The execs making decisions have just paid of millions in stock [1] making them staggeringly rich no matter what happens in the future. We can't hear your over the sound of green bills going BRRRRR. [1]: https://www.businessinsider.com/alphabet-google-executive-pa... reply internet101010 1 hour agoparentprevMost accurate description of Google I have seen. YT search is so, so bad. Three relevant results followed by twelve \"people also watched\" results then back to the good results. reply fma 33 minutes agoparentprevAlthough ChatGPT is a great product, I rely on it more and more not because it's improving, but because Google results are getting worse. Yeah I would still fact check for complex, indepth things...but for quick things where I'm knowledgeable enough I can smell the hallucinations from a mile away, ChatGPT 100%. reply odyssey7 1 hour agoprevThis is analogous to the Apple Maps launch failure. Except that Apple competes to make the best smartphone, and an iPhone was still valuable without Apple Maps. What happens to Google if it stops being able to compete in search? reply kibwen 1 hour agoparent\"Search\" isn't Google's product. Google hasn't been a search company for 20 years. \"Ads\" is Google's product. And the only way they'll go bankrupt is if 1) companies realize that advertising is pointless (I'm not holding my breath), or 2) some other company takes over from Google, which seems unlikely without government intervention (I'm not holding my breath). Google is a shit company, but they'll still be around 20 years from now, because our economy is nonsensical and irrational. reply tedunangst 17 minutes agorootparentStill need visitors to see the ads. reply r053bud 1 hour agoprevI’m curious why Sundar Pichai is still running this company? From recent videos it really seems like he has no idea what he’s talking about, and the company seems to be headed in the wrong direction. Just checked the 5 year stock graph; now I understand reply JCM9 2 hours agoprevThe fall of Google’s reputation on ML is nothing short of spectacular. They went from having a near untouchable reputation as being far ahead of any other large tech company on ML to total shambles in a year. Everything they’ve released has been a complete popcorn worthy dumpster fire from faked demos, to racist models that try and pretend white people don’t exist, to this latest nonsense telling me put glue on my pizza. What the heck happened? Or was their reputation always just more hype than substance? reply rvnx 2 hours agoparentIt could be because they actually released something. If you look back, the Google Research blog posts always have grandiose claims, but you can often never use them. reply CydeWeys 2 hours agorootparentAlphaGo, AlphaFold, and Waymo FSD are all released in the sense that you can see them actually working in the real world. Those all took much longer to put together than whatever rushed features were released to catch up with OpenAI, however. reply tadfisher 31 minutes agorootparentThey are also extremely constrained problem spaces relative to the problem space of LLMs, which is apparently \"everything imaginable\". reply ugjka 2 hours agorootparentprevresearch != product reply seydor 2 hours agoparentprevIt's not really that bad. I use gemini often and it's great. I prefer their UI reply tymscar 1 hour agorootparentWhat do you like more about their ui? reply seydor 1 hour agorootparentfaster, it has options like 'modify'. I also feel it follows my commands better, esp. when i ask to rephrase reply calebkaiser 2 hours agoparentprevThere was an interesting interview with David Luan about this recently. For context, he was a co-lead at Google Brain, early hire at OpenAI, and is now a founder at Adept: https://www.latent.space/p/adept The TL;DR on his take is that there are organizational and cultural issues that prevent Google from focusing their research efforts in the way that is necessary for what he calls \"big swings,\" like training GPT-3. In regards to your second question, Google's reputation in ML is definitely not hype. Purely on the research side, Google has been behind some of the most important papers in modern ML, particularly around language model. The original Transformers paper, BERT, lots of work around neural machine translation, all of the work that DeepMind has done post-acquisition, and the list goes on. On the applied side, they also have some of the most successful/widely-adopted ML-powered products on the market (think RankBrain/anything involving a recommendation engine, Translate, Maps, a ton of functionality in Gmail, etc). reply bbarnett 2 hours agoparentprevAt least Elmer's white glue is edible, millions of kids agree. (The logic sort of makes sense. Glue sticks things together, and some glue is edible.) reply arccy 2 hours agoparentprevresearch != product reply internet101010 1 hour agoprevThey should start with just removing reddit from the data set. reply empath75 46 minutes agoprevI love chatgpt and use it all the time and find it tremendously useful, but I never want to see AI generated content when I am not specifically looking for it. I don't want to see it in comments, I don't want to see it in search results, I don't want to see it as an illustration for an article, I _really_ don't want to see AI generated word vomit blog posts or fake \"news\" articles when I'm looking for actual information. It's not even because it's sometimes (or often) wrong or full of hallucinations. Even if it's 100% factually correct all of the time, it's _poor quality writing and art_, full of cliches and bland generalities, which even if they solve all the rest of the problems it's sort of fundamental to the architecture of transformers. You can't ever be truly creative or unique if you're predicting the _most likely_ token. reply TrianguloY 2 hours agoprevPutting glue on the pizza is (apparently) a clever way to take pictures of slices of pizza that look \"perfect\" to the camera (not for eating, obviously) [1]. I remember a couple years ago some videos of \"tricks\" showing this, plus literally screwing the pizza with screws. So, yeah, the ai did in fact autocompleted the question correctly. It was just the wrong context. Good luck trying to \"fix\" that. [1] https://shotkit.com/food-photography-secrets-revealed/ (number 2) reply namaria 2 hours agoparent\"correctly but wrong\" is just wrong.... there are no points scored for \"in a very specific context it would've made sense\" reply billyjmc 38 minutes agorootparentThis is the kind of ridiculous fumble that GOFAI (like Cyc) should be able to avoid by recognizing context. I wonder how neuro-symbolic systems are coming along, and whether they can save us from this madness. The general populace wants the kinds of things LLMs provide, but isn’t prepared to be as skeptical as is needed when reviewing the answers it generates. reply alfiedotwtf 3 hours agoprevGoogle keep making these same large and embarrassing mistakes time and time again. I think it's because their devs don't eat enough rocks every day. reply ta20240226 3 hours agoparentIs it the rocks or that the pizza that is served at Google doesn’t have enough glue? reply JCM9 2 hours agoprev“manually remove weird AI answers” is an oxymoron. Sort of like saying “deployed manual drivers to improve self driving performance” reply mensetmanusman 1 hour agoprevI have already eaten rocks and glue, the AI had won. reply Freak_NL 1 hour agoprevWhy? I wouldn't mind using a search engine where Weird Al answers my queries. reply CatWChainsaw 3 hours agoprevHow hard can it possibly be to just turn off the entire AI-generated overview functionality given that it just got introduced... reply rchaud 2 hours agoparentvery hard indeed, if you're optimizing for favourable opinions from Wall St analysts come earnings time . reply YetAnotherNick 2 hours agoparentprevIt seems to be turned off for me. And I was in beta testing for a month. Or maybe they are figuring out who is doing weird searches and turning off for them. In any case this thing is just hilarious. Just right after their AI painted historical figures as black. reply moomoo11 3 hours agoprevJust focus on making useful software to improve people’s lives. Holy fuck the last five years feel like such a waste. reply szundi 2 hours agoparentManagement realized that they are not good enough to sustain progress, so they humbly allocate resources to the next generation: AI reply cratermoon 3 hours agoparentprevThrowing good money after bad. Companies spent all that money on high end GPUs for crypto mining and that went bust, now gotta figure out something to do with the hardware to try to recoup some of the investment. Google pumped $1.5 Billion into crypto. reply illusive4080 3 hours agoparentprevBut AI will solve everything! reply rchaud 2 hours agorootparent...for our shareholders reply rvnx 2 hours agorootparentI hope AI will bring back the \"Sort by date\" button on Google Reviews, and add somewhere a Google Maps link. Who knows, maybe AI can bring back exact keyword matches, or correct basic math calculations on Google Search too. reply tymscar 1 hour agorootparentMaybe AI could bring back the pre gen AI tech scene reply moomoo11 2 hours agorootparentprevIt will cost $2 billion of nvidia chips and it won't work. reply OutOfHere 2 hours agoprevWith these dangerous answers, to the general public, Google is giving AI a very bad name, when in truth it's strictly Google that deserves the feeling. reply freitzkriesler2 2 hours agoprevI'm waiting for some clever hacker to come up some sort of logic bomb that causes the learning sets to become worthless. Something innocuous to a non ai scientist human but is otherwise fatal to the LLM data sets. reply lupire 1 hour agoprevGary Marcus, an AI expert and an emeritus professor of neural science at New York University, thinks the 80/20 rule (or 90/90 rule) is true. reply causality0 1 hour agoprevIt's very funny that Bing AI is now also telling people to eat a small rock every day, and citing pages telling people about how dumb Google AI is for telling people to eat rocks. reply chx 1 hour agoprevGoogle had the best search engine there is. Then they enshittified it for short term profit and now they panic instead of reverting course and simply laugh at AI companies. Madness. reply bitwize 50 minutes agoprevGoogle hooked Joe up to the tank and is just now realizing what they'd done and scrambling to contain the damage. With the Department of Justice breathing down their necks it's a doubly bad look for them. I'm not crying any tears for them though. reply more_corn 1 hour agoprevMaybe they could create a function that identifies satire. Which seems obvious after about five seconds of consideration. reply JSDevOps 1 hour agoprev [–] The cat is out of the bag. Keep eating rocks and sticking down your pizza toppings. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google is urgently addressing bizarre and erroneous responses from its AI search tool, AI Overview, following social media backlash over suggestions like putting glue on pizza or eating rocks.",
      "Despite extensive testing and cost reductions, the rollout has been criticized for low-quality outputs, with Google attributing most issues to uncommon queries or manipulated examples.",
      "Experts highlight the difficulty of achieving near-perfect AI responses, noting that advanced reasoning capabilities are necessary, while the pressure to compete with rivals like Bing and OpenAI has led to a problematic release, affecting Google's reputation."
    ],
    "commentSummary": [
      "Google is manually correcting AI-generated search errors, reminiscent of past algorithm tweaks, sparking debate on the role of search engines.",
      "Critics argue that Large Language Models (LLMs) generate probabilistic and often unverifiable text, leading to \"hallucinations\" and inconsistent outputs, questioning their reliability.",
      "The discussion critiques Google's shift from search quality to ad revenue, raising concerns about AI's reliability, the use of uncurated data, and the need for better risk management and adherence to standards."
    ],
    "points": 131,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1716650667
  },
  {
    "id": 40475178,
    "title": "Hikikomori: The Rising Global Challenge of Social Withdrawal",
    "originLink": "https://www.cnn.com/interactive/2024/05/world/hikikomori-asia-personal-stories-wellness/",
    "originBody": "The hikikomori in Asia who withdraw from society: A life within four walls.window.CNN = window.CNN || {}; window.CNN.adTargets = window.CNN.adTargets || {}; window.CNN.adTargets.spec = 'life_but_better_mindfulness'; window.CNN = window.CNN || {}; window.CNN.interactive = window.CNN.interactive || {}; window.CNN.omniture = { \"branding_ad_page\": \"\", \"branding_ad_zone\": [\"\", \"\", \"\"], \"branding_ad_container\": [\"\", \"\", \"\"], \"branding_ad_card\": [\"\", \"\", \"\"], \"branding_content_page\": \"\", \"branding_content_zone\": [\"\", \"\", \"\"], \"branding_content_container\": [\"\", \"\", \"\"], \"branding_content_card\": [\"\", \"\", \"\"], \"cap_author\": \"\", \"cap_content_type\": \"article\", \"cap_genre\": \"\", \"cap_franchise\": \"\", \"cap_media_type\": \"\", \"cap_show_name\": \"\", \"cap_topic\": \"\", \"friendly_name\": \"The hikikomori in Asia who withdraw from society: A life within four walls.\", \"full_gallery\": \"\", \"gallery_name\": \"\", \"gallery_slide\": \"\", \"grid_size\": \"\", \"headline\": \"The hikikomori in Asia who withdraw from society: A life within four walls.\", \"ireport_assignment\": \"\", \"publish_date\": \"2024-05-25\", \"rs_flag\": \"\", \"search_results_count\": \"\", \"search_results_page\": \"\", \"search_term\": \"\", \"section\": [\"world\", \"\", \"interactives\", \"\", \"\"], \"template_type\": \"interactive\", \"video_collection\": \"\", \"video_hpt\": \"\", \"video_opportunity\": \"\", \"video_player_type\": \"\" } window.CNN.bitlyUrl = 'https://cnn.it/4bueFfj'; var editionizeRegistry = function(registry) { return (window.location.href.indexOf(\"edition\") > -1 || window.location.href.indexOf('localhost') > -1) ? registry : registry.split('cnni').join('cnn'); } var adRegistry = 'https://cdn.cnn.com/ads/cnn_2/cnn_leaf.json';var _sf_async_config = _sf_async_config || {}; _sf_async_config.uid = 37612; _sf_async_config.domain = window.location.host; _sf_async_config.useCanonical = true; _sf_async_config.sections = 'world'; _sf_async_config.scrollElement = true; (function() { function loadChartbeat() { window._sf_endpt = (new Date()).getTime(); var e = document.createElement('script'); e.setAttribute('language', 'javascript'); e.setAttribute('type', 'text/javascript'); e.setAttribute('src', '//static.chartbeat.com/js/chartbeat.js'); document.body.appendChild(e); } var oldonload = window.onload; window.onload = (typeof window.onload != 'function') ? loadChartbeat : function() { oldonload(); loadChartbeat(); }; })();{ \"@context\": \"http://schema.org/\", \"@type\": \"NewsArticle\", \"headline\": \"The hikikomori in Asia who withdraw from society: A life within four walls.\", \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://media.cnn.com/api/v1/images/stellar/prod/e3fea23c-f5a9-4039-aa8e-00ce8c20d852-1.png?c=original\", \"height\": 360, \"width\": 640 }, \"keywords\": \"\", \"publisher\": { \"@type\": \"Organization\", \"name\": \"CNN\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://amp.cnn.com/static/cnn-publisher-image.png\", \"width\": 103, \"height\": 60 } }, \"author\": { \"@type\": \"Organization\", \"name\": \"CNN\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://amp.cnn.com/static/cnn-publisher-image.png\", \"width\": 103, \"height\": 60 } }, \"url\": \"https://www.cnn.com/interactive/2024/05/world/hikikomori-asia-personal-stories-wellness\", \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"The hikikomori in Asia who withdraw from society: A life within four walls.\" }, \"dateCreated\": \"2024-05-25\", \"dateModified\": \"2024-05-25\", \"datePublished\": \"2024-05-25\", \"description\": \"Many young people across Asia are cutting themselves off from society, locking themselves at home for months or years. Here’s why.\" } var CNNENV = ( window.location.href.indexOf(\"edition\") > 0 ) ? \"//edition.cnn.com\" : \"//www.cnn.com\"; var adType = (window.innerWidth > 700) ? 'bnr' : 'rect'; window.AdFuel.queueRegistry( editionizeRegistry(adRegistry), { sync: true, syncSlots: [], slots: [ `ad_${ adType }_atf_02`, `ad_${ adType }_atf_01`, `ad_${ adType }_btf_01`, `ad_${ adType }_btf_02` ], dispatch: true, });'SCROLL DOWN“I hid in my bed, I didn’t go outside at all...”“... I wouldn’t leave my bed for even half a step.”“To be honest, I felt like I had given up.”A shrinking life: Why some Asian youth withdraw from the worldBy Jessie Yeung, Sophie Jeong, Carlotta Dotto, Woojin Lee, Kenneth Uzquiano and Saki ToiPublished May 25, 2024(CNN) — Charlie was 15 when his life inexplicably shrank to fit within the frame of his lower bunk bed in his family\\'s cramped Hong Kong apartment.“I felt very depressed, confused, like I didn’t know what I wanted,” said Charlie, who’s now 19 and still learning how to navigate the world outside.Charlie is among millions of hikikomori, a Japanese term for people who cut themselves off from society, sometimes for months or years – often Gen Z and Millennials in the prime of their youth.The phenomenon first emerged in Asia, and is particularly well-documented in Japan – but similar stories are surfacing in other parts of the world including the United States, Spain and France.Researchers at Yale University have suggested that the rise of the internet and decline of face-to-face interaction may be driving the global spread of hikikomori. Others say the Covid-19 pandemic may have created even more recluses, as most of the world retreated indoors to stop the spread of the virus.Across much of Asia, governments and organizations are now working to help hikikomori re-enter society – a task that’s growing more urgent as many nations grapple with aging populations, a shrinking workforce, falling birth rates and disenchanted youth.It’s not clear how many hikikomori there are worldwide, but more than 1.5 million are estimated to live in Hong Kong, Japan and South Korea. Here are some of their stories.HONG KONGCharlie’s withdrawal began in his early teens, after he argued with a teacher and overheard classmates criticizing him at school.“I am sensitive to people’s words; I really care about what others say about me and how they see me.”CNN is identifying Charlie by his first name for privacy reasons.At first, Charlie said he tried to attend school once or twice a week – but by 2019, he had closed himself off entirely in his bedroom. He would stay there for four months.He said he didn’t respond to friends’ messages or confide in anyone, feeling like nobody would understand anyway. His parents occasionally urged him to go outside or attend school; mostly, they left him alone.But even then, there was little space to hide.Charlie, his parents and grandmother shared a small studio apartment.Charlie shared a bunk bed with his grandma, and spent his days burrowed under the covers.He even took his meals in bed, balancing bowls on a tray. He only got up to use the bathroom and take his dishes to the kitchen.Like many hikikomori, he would sleep all day and wake at dusk. Then at night, when his family went to bed, he spent hours scrolling on his phone.Source: CharlieChildhood friendsCharlie\\'s mom got him these stuffed animals, a koala and a pig, 10 years ago.Source: CharlieSelf-help bookCharlie said a book titled \"Workplace Survival Manual for Social Animals,\" about navigating workplace challenges and social situations, helped him.Source: CharlieA small shared spaceCharlie\\'s \\'room\\' in his family\\'s studio apartment. He slept on the bottom of the bunk bed, with his grandmother on the top bunk.Charlie said there wasn\\'t just one thing that sent him spiraling into isolation.School played a part – he felt under pressure to perform in Hong Kong\\'s demanding education system.Teachers would “berate and humiliate ‘bad’ students, even saying (things like) if you misbehave this way, you’re going to end up as a beggar,” he said. “At the time, I really believed what they said.”LISTEN TO CHARLIE(00:32)“I just hid in my bed, I didn’t go outside at all, I wouldn’t leave my bed for even half a step.”Paul Wong, an associate professor at Hong Kong University, estimates up to 50,000 hikikomori live in Hong Kong – mostly middle and high schoolers, though pre-teens have begun showing symptoms too.Many Hong Kong parents are so focused on academic performance that their children don’t do “anything besides studying,” he said.When students begin to withdraw, parents may raise their voices and use tactics like guilt tripping or other forms of punishment – which only pushes them further away, he added.During the three years Hong Kong resident Ah Mun spent in his bedroom, his parents and siblings didn\\'t know what to do, he said. At one point they cut off the internet, hoping he might go outside, but it didn’t work.Ah Mun, 30, now works at the social services center of the church that helped his recovery, which offers creative workshops and therapy cats. Source: Jessie Yeung\"After a while you start to feel that going outside is quite scary, and the longer the time goes, the harder it feels to go out,” said Ah Mun, who CNN is identifying by his nickname for privacy reasons.“By the end, I wanted to go outside, but I didn’t dare … I didn’t have the courage.”The answer came when his sister reached out to the social services division of a church that helps isolated youth, where he now works – the same one that eventually helped Charlie.Initially, Ah Mun was too afraid to meet the social workers in person; they visited his home many times over several months before he ventured out of his room. His full recovery took more than a year.“The first time I stepped back outside, it felt very novel … everywhere looked different.”That was six years ago. Now, Ah Mun helps other hikikomori escape self-imposed isolation. Though their circumstances may differ, “I think I can understand their feelings,” he said.JAPANWhen Toyoaki Yamakawa’s parents fell sick, he moved from Tokyo to his hometown Fukuoka to manage their care. As their only child, he felt a “big burden” to look after them, which also meant managing their financial affairs, he said.“Once I started living closer to my parents, I had a lot of problems… which became quite difficult for me to deal with on my own.”Over time, he withdrew and remained in his home for five years starting from age 35.At the beginning, “I locked myself in the bedroom,” Yamakawa said. \"I did not have energy to do anything, so I was mostly sleeping all day.”His wife asked him to prepare meals and take out the trash – activities that allowed him to have “a role in the house,” he said.“Thanks to this ... I believe I did not become a person who can\\'t or won\\'t do anything.”But it was gaming that finally gave him the courage to come out of isolation. Fellow players praised his abilities, boosting his self-esteem and piquing an interest in live streaming.He watched YouTube videos that inspired new hobbies. He started growing plants on his balcony and experimenting in the kitchen.“When I became interested in various things, I naturally went outside and my energy recovered,” he said.Source: Toyoaki YamakawaExperimenting with foodYamakawa enjoyed making all types of food, from dumplings and sashimi platters to homemade pizza dough.Source: Toyoaki YamakawaA green thumbHis small outdoor garden boasted flowers, herbs, fruit and even vegetables like radishes.Yamakawa’s wife, previously a homemaker, got a job to support them during his five-year hikikomori period. The hardest part was feeling powerless against Yamakawa’s depression, she told CNN, asking not to be named.“I knew his original personality, which was cheerful, fast-paced, and enthusiastic about his work, so I didn\\'t know how to deal with the change in him,” she said. “I was worried that he might really disappear.”She had struggled with her health since her 20s – at times so sick she could only lie in bed, unable to job hunt after college, she said. Her parents’ unconditional support and care at the time, and her eventual recovery, shaped her response to Yamakawa’s withdrawal.“I think I had a stronger conviction than others that even if it takes time, better times will come for my husband,” she said.Like Yamakawa, many adult hikikomori in Japan withdraw after losing their job or struggling to provide for their families, according to Teppei Sekimizu, associate professor of sociology at Meiji Gakuin University. The trend reflects broader economic problems like Japan’s rising cost of living and stagnant wages.A recent government survey found that Japan has nearly 1.5 million hikikomori – and unlike Hong Kong’s teenage hikikomori, Japan’s recluses span a much broader age range, said Sekimizu. Some elderly parents in their 80s are reportedly having to support hikikomori children in their 50s, he said.Takahiro A. Kato, an associate professor at Kyushu University, said Japanese men are particularly at risk because of the “pressure on boys to go out and work hard.”“Those who fail become ashamed and think that they are not good enough.”Yamakawa said aspects of Japanese culture contributed to his isolation, such as the belief that one should “do things yourself” and not bother others, and that disgracing your family is the ultimate shame.Yamakawa holding a sheet of paper about tiramisu, the Italian dessert. Cooking was an important hobby that aided his recovery. Source: Toyoaki Yamakawa“I probably felt the pressure that since I am the (only) son, I am the one who should take care of my parents on my own, without getting help from others,” he said.How a person’s family responds to their withdrawal is critical, said Sekimizu. If family members offer emotional support, the hikikomori might be able to rejoin society more easily.But if their family “blames the person for staying at home,” the hikikomori is likely to cut off their relationships, feeling like they are “forced to seek a place ... where they can be alone,” he said.Yamakawa said he lost some friendships during that period, and his own parents accused him of being lazy and “mentally weak.”“I knew that there was no way this life could go on, because I wasn\\'t working and I had financial problems, but I couldn\\'t bring myself to work,” he said. \"I felt anxious and bitter that I couldn\\'t do anything about it.”One thing that helped his recovery were “goal sheets” of what he wanted to achieve each week, month and year, giving his days structure and purpose.At one point he went to the store and chatted with salespeople, which finally “made me feel like a normal person again,” he said.Now 44, Yamakawa has started his own organization to help other hikikomori – and says as difficult as those years were, especially for his family, it changed him for the better.“For me, becoming hikikomori was a chance to reset my personality, my way of working, and many other things. If I hadn\\'t done that, I would have hung on to the (status quo) while accumulating stress,” he said.“I am now working freelance, and I think I have found a new way of life that suits me, so I am glad I withdrew from society.”SOUTH KOREASome hikikomori withdraw over and over. Sung O-hyun, a 32-year-old South Korean, has secluded himself about five separate times for different reasons – totaling about two to three years in isolation.His first social withdrawal came in middle school, when he didn’t leave his house for a month during a school break. At 27, he suffered a setback at work and again retreated to a safe place.“I heard negative feedback a lot, things like I’m not good at work, I make mistakes,” he said. “I was disappointed in myself a lot, became very depressed and lost confidence to work again, so I just locked myself in my room.”At the time, Sung was living with his family. He felt embarrassed to see them, so he only left his room to eat or use the bathroom when they were out of the house or sleeping.“I think I slept a lot because I could forget about painful things.”Sung O-hyun, pictured in 2020, stayed in communal “sharehouses” in both South Korea and Japan. Source: Sung O-hyunDays and nights started blurring together.“When I heard sounds of my parents going to work and people waking up, I felt a sense of shame,” he said.He said he stopped talking to his family, which caused misunderstandings between them.“Since I was alone at home, I often thought things like, ‘Everyone else seems to be doing well. Why am I like this?’ It seemed like I was the only one having a hard time like this,” he said.He looked up hikikomori on the internet and came across Japanese company K2 International, which supports young people who refuse to go to school, withdraw from society or have developmental issues.They have a communal living program, or “sharehouse,” in countries like Japan and South Korea – which he joined in 2019.At the sharehouse, home to up to nine residents, life is designed to bring recluses together, encourage social interaction and offer routine.Every morning, residents gather for a meeting to talk about their mood and share their feelings.They also eat lunch together every weekday, with residents taking turns to cook.“We just do a lot of things together, like watching movies together, eating food together, talking together, sharing difficulties with each other,” said Sung’s former housemate and friend, An Yoon-seung.After prior stays, Sung returned to the sharehouse again in 2023 after going through another seclusion. “I think we keep doing (things) together so that (we) don’t feel very isolated,” he said.Source: Not Scary CompanyA common spaceThe sharehouse has shared spaces such as living rooms and attics where residents can spend time together, enjoy entertainment and relax.Source: Not Scary CompanySharing mealsResidents cook for each other and eat together every weekday – one of the things that encourage routine, daytime activity and social interaction.The sharehouse in South Korea is now run by Not Scary Company, led by a former recluse.Statistics suggest its residents are not alone in their struggle with isolation. In 2022, 2.4% of Koreans aged 19 to 34 were reclusive, according to a survey by the Korea Institute for Health and Social Affairs. That makes up about 244,000 people across the country.Last year, the government passed an amendment making some reclusive youth eligible for financial support, including up to 650,000 won ($475) per month for living expenses, to help them “re-enter society.”Many Millennials and members of Gen Z have “perfectionistic concerns,” said Hur Ji-won, an associate professor of psychology at Korea University.People with “perfectionistic concerns” are typically sensitive to criticism, are overly self-critical and fear failure.When people with those traits try new things and fail to achieve results that meet their standards, “they get very discouraged and anxious,” Hur said.Smaller families are also contributing to the problem, said Yoon Chul-kyung, executive director of G’L Out of School Youth Research Institute.“In the past, because we had large families and many siblings, we were able to learn a lot about how to relate,” Yoon said. “As the living environment has changed, there are fewer experiences of forming communal relationships than in the past.”An, the 22-year-old sharehouse resident, had long struggled with communicating with others – to the point that it restricted almost all his outside activities, like going to the hair salon or eating at restaurants alone.He began his six-month seclusion right before he was due to enter college in 2020.“I really didn’t want to go, I didn’t want to meet people I didn’t know.”An mostly stayed at home in his room, spending time on his phone and computer, reading novels and occasionally taking classes online. These activities were the only way he could “endure ... staying idle,” he said.It was An’s sister who recommended the sharehouse, where he began living in 2020 during the pandemic. “There were many people I can honestly share my story with and who have similar concerns, so it was actually a bit comforting,” An said.Sung O-hyun and An Yoon-seung became friends after living at the sharehouse in Seoul, where residents would celebrate birthdays together. Source: Sung O-hyun and Not Scary CompanyAn says he wants to eventually get his own room in Seoul and try his hand at becoming a YouTuber.“I want to get a girlfriend. I just want to be healthy,” he said. “I just want to make a lot of good friends. That’s what I want to do.”For years, researchers thought hikikomori only happened in Japan as a “culture-bound” phenomenon. But stories of extreme withdrawal worldwide show the condition is not limited to Asia.Experts say there could be more hikikomori cases worldwide that are misdiagnosed as depression or anxiety.While many hikikomori may struggle with these disorders, social withdrawal is a specific, unique syndrome that needs special treatment, experts say.Charlie took nearly a year to recover – and by the time he fully emerged from his room in 2021, the world looked different.Hong Kong\\'s borders were still closed due to the pandemic, with restrictions on social gatherings. People could still mix in small groups, which meant Charlie could take the first steps towards recovery.He began meeting with a social worker at the church center, at first just sitting and talking under a tree near his home. Gradually, he began participating in the center’s activities - pet therapy, arts and crafts and volunteer work.He said he still feels anxious when meeting new people, and is more solitary than before, but he has also come a long way.Kato, the Japanese expert, says in time, self-isolating for weeks or months may not seem so unusual. “In the future, let\\'s say in 2050, after lots of events like pandemics and wars, it may become normal not to go outside,” he said.“I think that being a hikikomori will become a way of life in the future.”CreditsWritersJessie Yeung, Sophie JeongVisual EditorCarlotta DottoIllustratorWoojin LeeDeveloperKenneth UzquianoEditorsMark Oliver, Hilary WhitemanContributing reportersSaki Toi, Moeri KarasawaPhoto editorNoemi CassanelliAudio editorMatt DemspeyRelated linksJapan was already grappling with isolation and loneliness. The pandemic made it worseCNNSouth Korea is paying ‘lonely young people’ $500 a month to re-enter societyCNNSouth Korea’s middle aged men are dying ‘lonely deaths’CNN``© 2024 Cable News Network.A Warner Bros. Discovery Company.All Rights Reserved.CNN Sans ™ & © 2016 Cable News Network.Terms of UsePrivacy PolicyAdChoices'",
    "commentLink": "https://news.ycombinator.com/item?id=40475178",
    "commentBody": "The hikikomori in Asia: A life within four walls (cnn.com)119 points by reqo 4 hours agohidepastfavorite158 comments neonate 2 hours agohttps://web.archive.org/web/20240525142056/https://www.cnn.c... https://archive.ph/lt04t ilaksh 3 hours agoprevI think this is really a spectrum and they are focusing on some more extreme aspects of it. But it is definitely not just an Asian thing and I believe to some degree this type of social withdrawal has affected perhaps a very significant portion of our society. I have definitely been socially isolated my entire life to some degree or another. But much more so in adulthood. Again, I suggest that this is relatively common, not something that happens to only a few million people. One aspect that is being glossed over is the amount of socialization or let's call it \"pseudo-social\" activity that is happening over the internet for these people. I'm someone who generally does not have friends, leaves the apartment literally only a handful of times per month to take the garbage out and maybe buy groceries once or twice a month if I am trying to save money versus Instacart. For me it comes down to money. I have a health issue that makes me fatigued etc. and don't have money for health insurance. I don't have money to go to restaurants or otherwise waste going out. So I stay home. Because I'm always in a poor health and financial state, I feel uncomfortable trying to do any \"real\" socialization. But I have always been trying one way or another to get to a point where I have a \"real\" online business that allows me to actually thrive. Such as buying a car and a house, getting health insurance and addressing my health issues, or paying taxes. But what I have managed so far is usually just enough to scrape by. There have been some minor successes here and there but rarely have I ever felt like I had enough to truly meet my basic needs such as the health concerns or financial stability. Anyway, I think it's easy to get in a position with health and financial challenges, maybe just a series of low-paying contracts, where some degree of social isolation is just practical and realistic. reply chasil 1 hour agoparentI live in Iowa, and I had a roommate who just stopped going to work one day. I learned after talking to her that she had done this before. In talking to her, it seemed to me a mix of anxiety and depression with a focus on agoraphobia. Her family from several hundred miles came to retrieve her when they contacted me. She asked to move back in later, but I declined. I saw her start a new career some years afterwards. I don't know if the \"laying flat/tang ping\" movement in China, or the issues of the people in the article, are completely separate from this. https://en.m.wikipedia.org/wiki/Tang_ping reply wholemodern 5 minutes agorootparentthe laying flat movement has mostly to do with lack of jobs/opportunity for young people. - there's a societal trend to not hire people over the age of 30-35 in China. after months of looking for work, they've given up - there's an unofficial 70% youth unemployment rate, and with 12 million new grads each year and intense competition for government work, sometimes hundred of applicants for a single stable government spot, the new grads give up - the young generation has realized that no house/car/marriage/kid is a good way to live, and there's no pressure on them to create a life. so they lay flat. thus the abysmal marriage/child rate in China. - the new grads don't want to work in a factory, day or night shift, for $2/hour. - if the workers are in 1st tier cities, they can barely save up any money working and living there, due to the recent 50% reduction in wages and increased spending on necessities. so it's easier for them to just not work and live off of parents. reply antegamisou 2 hours agoparentprevI'm sorry for what you've been through and I hope your health gets better. However I believe you've phrased the following a bit poorly, although I'm sure unintentionally: some degree of social isolation is just practical and realistic. Unfortunately this leads to connotations that encourage people completely out of touch with the ordinary person's lifestyle (billionaires, CEOs etc.) make outrageous claims and dictating how they should navigate life with minimum wage and no insurance e.g. that CEO who said skip breakfast to save money, or HN's favorites that we should just shut up and be content with modern tech automating our creative abilities instead of assisting us with menial tasks. Instead if you had written social isolation is inevitable without controlled financial and health stability, you'd be 100% spot on. Human, social isolation and practical/realistic just don't fit together. reply FormerBandmate 1 hour agorootparent> we should just shut up and be content with modern tech automating our creative abilities instead of assisting us with menial tasks This isn’t true. Dalle and ChatGPT are assists, they don’t really replace anything and make being creative more accessible. AI also helps with tons of menial tasks like code syntax reply ecjhdnc2025 50 minutes agorootparent> Dalle and ChatGPT are assists, they don’t really replace anything and make being creative more accessible. Proponents of these tools can keep saying this, but it doesn’t make it true. Almost nothing these tools can do really helps the creation of art. reply card_zero 1 hour agorootparentprevEscaping something that's inevitable is not practical or realistic. reply Anotheroneagain 1 hour agoparentprevThose people have completely taken over the west. You suffer alone in hopeless isolation if you are not like them. It isn't that you somehow didn't succeed - the majority simply prefers to be alone. Only the fact that the majority is \"individualist\" like this makes it much less obvious. reply monero-xmr 1 hour agoparentprevThere are emotional and mental support groups, churches, community events, and all manner of things you can do for free that may help break your isolation. It takes effort to do this. I know you are struggling, but ultimately no one will help you but yourself, which is a hard pill to swallow. reply hypeatei 2 minutes agoprevI think what's missed in a lot of these discussions is your upbringing. Our society changed very quickly in the last 30 years and parents may not be providing the proper \"foundation\" for their children since they didn't grow up with all this stuff. Even if you had shitty parents before, you'd probably do alright since all of society in past times was based on in-person interactions and there wasn't endless media consumption at your fingertips. Personally, my parents were very immature, divorced, and generally didn't set me up for a healthy/balanced social life. I haven't completely given up; I work, maintain loose contact with a few friends, and basically just \"doing my time\" until I die. reply marcyb5st 3 hours agoprevFrom my personal experience (I also went through a very dark period in my life and just recently climbed out of the hole I dug myself) I guess people are realizing that working hard won't get you anything close to what previous generations had. Once that settles in it's hard to push yourself to do basically anything. Additionally, I also believe that feeling is compounded by social media where selection bias only shows you cherry picked moments where it seems other people are living the life you won't get. Finally, among the younger generations there is a lot of climate change dread going around. For me it was a combination of all these factors and to this day I can't pinpoint exactly what was the trigger, but after COVID lockdowns I simply kept social distancing. reply fxtentacle 13 minutes agoparentMy pet theory is that social media isn't the problem, the always-on surveillance created by smartphones is. When I was in university, we had nude parties. I'm pretty sure nobody would risk that nowadays. That cherry-picking you mentioned goes both for highs and lows. In this day and age, you always need to plan for your worst moment to end up in someone's picture. The ubiquity of cameras has made everyday living more risky in the sense that you're constantly at risk of losing social standing over insignificant mistakes. reply forinti 3 minutes agorootparentMy grandfather said there were such parties when he was young. That would have been 1930s-1940s. And what's even more crazy is that this was in the countryside in Portugal. reply SoftTalker 2 hours agoparentprevPeople just have no historical reference. Think times are tough now? Try the great depression. Worried about climate change? In the 1980s it was nuclear war. People living paycheck to paycheck, barely scraping by? Media glorifying the rich and famous? Nothing new. reply luzojeda 41 minutes agorootparent>Worried about climate change? In the 1980s it was nuclear war. Yet they could afford _homes_. One of the most important things any human can have. A place to feel safe, call home, not be afraid month after month that your landlord will raise rent by 50% or 100% leaving you on the streets with all the stress that comes afterwards, etc. Also we have that now as well. We are at the risk of a total war between China, Russia, USA, EU, India, etc. all with nuclear weapons. We have the worst of both worlds. reply Starlevel004 1 hour agorootparentprev> Worried about climate change? In the 1980s it was nuclear war. Nuclear war didn't happen. Climate change is happening. Pretty key difference! reply david-gpu 1 hour agorootparentIf nuclear war happened, and there was no way for us to know whether it would happen, there was nothing we could do to survive the event. While climate change is happening, there is still a lot we can do to slow it down and mitigate its effects, both individually and collectively. reply SoftTalker 56 minutes agorootparentYes, at the time it seemed like something that might happen tomorrow. You had TV movies like \"The Day After\" and constant discussion of it in school and in the media. It was a real fear. reply card_zero 0 minutes agorootparentIngmar Bergman's Winter Light (1963) has somebody in Sweden becoming depressed and withdrawn due to anxiety over China developing an atomic bomb. Then in 1982 Prince sang \"everybody's got the bomb, we could all die any day\". That's two decades of continual anxiety about sudden obliteration (or worse, ''near'' obliteration). card_zero 1 minute agorootparentprevIngmar Bergman's ''Winter Light'' (1963) has somebody in Sweden becoming depressed and withdrawn due to anxiety over China developing an atomic bomb. Then in 1982 Prince sang \"everybody's got the bomb, we could all die any day\". That's two decades of continual anxiety about sudden obliteration (or worse, ''near'' obliteration). krisoft 2 hours agorootparentprev> In the 1980s it was nuclear war As opposed to now when it is nuclear war and climate change? :) reply matthewdgreen 2 hours agorootparentIt is hard to explain to people today how scary nuclear war was during the Cold War. It wasn’t so much just that we were in real jeopardy of starting a deliberate nuclear war: it was the fact that we were heartbeats away from doing one by accident, and nearly did a couple of times [1]. It was standard for kids in elementary school to discuss the implications (at least we didn’t bother with duck and cover when I grew up, since politicians realized that was pointless.) Maybe we’re still in the same place (or will be again soon) but today’s atmosphere is nowhere near as scary. [1] https://en.m.wikipedia.org/wiki/Stanislav_Petrov reply Apocryphon 2 hours agorootparentSure, but after like only one decade of halcyon optimism 9/11 happened and thousands were killed on American soil, leading to over a decade of paranoia and political-infighting which then petered out to become… more paranoia and infighting, except even more self-directed. It seems like in America the fear is more personal than ever. reply RobotToaster 1 hour agorootparentprevHey, on the bright side the nuclear winter will cancel out global warming. reply PartiallyTyped 2 hours agorootparentprevAnd disinformation, and greater wealth inequality, and “AI taking our jobs” and all other forms of doomerism. reply ajkjk 2 hours agorootparentprevYet the new struggles are real too? explaining them away as more of the same does nothing. There are old problems and there are new problems and they're both problems. reply SoftTalker 2 hours agorootparentYes there are struggles today. But there always have been. This imaginary past where most people didn't live in fear of unexpected expenses, debt, struggling to pay all their bills, and not earning enough money didn't exist. reply dabbledash 1 hour agorootparentYes, that's true. For most of human history we feared starvation, not bankruptcy. reply Apocryphon 2 hours agorootparentprevI think at the very least there was a sense that the struggles were leading to a better future. In America, it certainly feels like we’ve regressed from the optimistic “End of History” vision of the future of the ‘90s. reply ThalesX 2 hours agorootparentprev> This imaginary past where most people didn't live in fear of unexpected expenses, debt, struggling to pay all their bills, and not earning enough money didn't exist. I call bullshit... Debt. Bills. And for that matter, money. These are things that our species lived for hundreds of thousands of years without. Furthermore, there are tribes that even to this day live without these struggles. reply lotsofpulp 2 hours agorootparent> there are tribes that even to this day live without these struggles. Instead of debt and bills, that moves the problem up to struggling to defend against tribes with better weapons who want your land/labor. Or natural disasters and no resilience due to trade network. Or dying from bacterial infections and childbirth complications. reply ThalesX 2 hours agorootparentIt's hard to assess, especially from our position, which set of challenges the human soul prefers. But there's an interesting point to be made, touched in both \"Civilized to Death: The Price of Progress\"[0] as well as \"The Dawn of Everything: A New History of Humanity\"[1] that no 'savage' has been interested in non-coercively joining an 'advanced' society, whereas there are plenty of examples of 'civilized' people choosing the 'savage' life. I don't think the choice is as clear cut as it would superficially seem. [0] https://www.amazon.com/Civilized-Death-What-Lost-Modernity/d... [1] https://www.amazon.com/-/en/David-Graeber/dp/0374157359 reply globalnode 2 hours agorootparentprevwell thats unhelpful, diminishing the concerns of people by implying: \"back in my day we all had it tougher\", just doesnt help and its just not true, back in the day it was a \"different\" tough but no less or more real than what people are experiencing now. reply robotnikman 26 minutes agorootparentprevI would say nuclear war is unfortunately back on the table again considering the current conflicts going on. reply hypeatei 22 minutes agorootparentIt always has been, regardless of the current world conflicts. There are nukes ready to be launched at moments notice all the time. reply doktrin 1 hour agorootparentprevYour observation is neither novel nor helpful. People suffering are often not suffering due to a lack of information, or because they’ve never taken middle school history. reply marcyb5st 2 hours agorootparentprevPerhaps, but back in the days you weren't exposed 24/7 to all of that (IMHO). reply silverquiet 2 hours agorootparentprevThen perhaps there is no issue to worry about here. reply itronitron 2 hours agorootparentprevuser name does not check out reply ryandrake 2 hours agorootparentprev> Worried about climate change? In the 1980s it was nuclear war. During the Cold War, they were at least telling kids (at least in the USA) that the nuclear holocaust might be avoidable \"if clear heads prevail,\" or \"if we beat the Russians,\" or \"if they back down.\" There was at least hope. With Climate Change, we've told two entire generations of kids that there is no hope, it's inevitable and irreversible, and that there is no way to avoid catastrophe. So is it any surprise they're all doomers when it comes to Climate? If you tell everyone that everything is hopeless, then don't be surprised when a few conclude that it really is hopeless. reply silverquiet 2 hours agorootparentI was told as a kid that there was hope for climate change as long as we could scale back our carbon emissions. Watching the opposite of that happen over a lifetime is what made me into a doomer. reply analog31 1 hour agorootparentThe deniers simply morphed from \"it's fake\" to \"it's unavoidable.\" reply mitthrowaway2 2 hours agorootparentprevThat's true, it's a big difference. With nuclear war, every day that the sirens don't ring is the status quo protected, doom delayed by another day. With climate change, every day that the status quo is protected is another day of accelerating doom and increasing inevitability. They are different in that way. reply PartiallyTyped 2 hours agorootparentprevThe doomerism rhetoric is coming more from one side of the political isle, the one that was in denial about climate change, generally votes against legislation that helps deal with the consequences because they’d rather their perceived enemies suffer, and them along with than do good. reply Animats 50 minutes agorootparentprevThe article reports this happening in countries that are quite comfortable at the moment and have strong safety nets. This isn't being reported from Venezuela or Ukraine or even Poland or the UK. China has something else - the \"lying flat\" movement.[1][2] This is just \"dropping out\", something China is now rich enough to allow. It's not about isolation, just not working much. [1] https://www.nytimes.com/2021/07/03/world/asia/china-slackers... [2] https://www.scmp.com/news/people-culture/trending-china/arti... reply wholemodern 31 minutes agorootparentI wouldn't call what the young people lying flat in china \"rich enough\". There are stories of people in their 20s and 30s, after college or after layoffs, that they've either got a few thousand dollars saved up or from family. They couldn't find any jobs due to ~70% youth unemployment rate. Or they don't want to work in factories that pays out $2/hour and waste their degree. So they are moving to very remote countryside and renting a room for $50/month, and spending only $.50 a day. Or live off of parents, what's also known as 啃老族 or eat the old. Remember the previous Chinese premier confirmed there are 700 million people living off less than $100/month a few years ago. so this living of standard is possible for young people. Especially now that there's widespread 50% reduction in wages/cost of living in China. reply thegrim33 2 hours agoparentprevI don't understand at all. Because you (in my opinion, erroneously) believe that you need to work some X% harder than the last generation to accomplish the same thing you're just not going to try? What about pretty much every generation prior to the last, who had to toil in hard labor most days, who often didn't have indoor plumbing, who often didn't even have electricity, who went off to war, did they have any easier time being \"successful\" than you? Did they just give up and not try? Do you honestly believe you're worse off in 2024 than they were? That you're working harder than they were? I'm not even going to touch the climate dread stuff as climate change is a topic that can not be impartially discussed anymore. reply krisoft 2 hours agorootparentI don’t think you are saying the same thing. You are saying this; > you need to work some X% harder than the last generation to accomplish the same thing And this is the contrasting bit from the comment you are responding: > working hard won't get you anything close to what previous generations had The difference is that your sentence says they have to work harder to achieve the same, while their comment says that even if they work hard they can’t achieve the same. That is two very different things. reply yoyohello13 2 hours agorootparentprevSo what’s the difference between the past and now? Maybe having corporations profiting off of people’s attention and fear is not a good thing. reply jeremyt 2 hours agorootparentSocial media. That’s the difference. reply raindeer2 1 hour agorootparentprevThe reason that ppl give up now is not that it is worse now than before, but rather the opposite. Now you can do almost nothing productive and still survive. Before, you would simply starve to death. reply tristor 1 hour agoparentprev> I guess people are realizing that working hard won't get you anything close to what previous generations had. I hear this a lot. It’s also untrue. If you are open to anecdata, it’s trivial to prove this is untrue. What people constantly miss and fail to consider is what they are working hard at. Hard work is a requirement for success and class mobility, but it is not sufficient. You must also work hard at the right things. The truest thing I have learned about life is that you need to do three things to be successful: 1. Identify places where you can add value /for people who can compensate you/. 2. Learn how to articulate the value you add. 3. Ensure you get compensation for that value added. If you do these three things, your hard work will pay off, maybe in a big way. reply softsound 55 minutes agorootparent#2-3 is where I would say the majority of people get stuck. I sure have, I volunteer to help people whenever I can but that doesn't mean I'll get paid or compensated for it. Not that I mind but I can get burnt out. #1 can really be hard when #2-3 never seems to come into the picture for long. It can make anything you do seem like a waste of time. Can you add value, sure, is it worth it? That's where it can be tough. I think a problem with a lot of this too is lack of real long term community too. But I think some people are just better at managing that naturally. reply admissionsguy 2 hours agoprevI think the main thing is the lack of on-ramps. Once you have fallen out of the social circulation, there is basically no way of going back. Unless you are able to stay within an extremely narrow range of behaviours (in terms of not being weird, basically speaking expected thinks in expected tone of voice and body language), nobody wants to associate with you. And since about the only way to learn these things is to be around people who already behave in the \"right\" way, a vicious circle arises. It has nothing to do with debt, wealth or earnings. Completely independent things. People had it worse at every time in history in almost every place. It has nothing to do with social media / internet. It just something people tend to fall into when they withdraw, and have no trouble abandoning as soon as the life outside becomes tenable again. reply nicbou 1 hour agoparentSocial capital follows the same progression as regular capital. If you already have a lot of friends and acquaintances, it's easy to make more. You either get invited to things, or you can organise something and know people will come. Loneliness is a big problem among recent immigrants. They all struggle to make friends at first, because they have no social capital to build upon. It's hard to break into established circles without being introduced by a member, and few people will show up to a stranger's party unless it's vetted by friends. There is such a thing as being socially destitute, and the recovery can be quite difficult, especially when you have other things going on in your life. reply dj_mc_merlin 2 hours agoparentprev> Unless you are able to stay within an extremely narrow range of behaviours (in terms of not being weird, basically speaking expected thinks in expected tone of voice and body language), nobody wants to associate with you. I'll agree on the lack of on-ramps but this is a pretty limiting view. There's all kinds of people, many who will share some of whatever you think your weirdness is. If you only want to associate with a certain slice of society, it is not so weird that only certain slices of society want to associate with you. reply softsound 47 minutes agoparentprevI would say it's not just behaviors but isolation causes stress on the body that increases more irrational behaviors and fears. So that can be crippling when trying to get back to a health state that can handle relationships again. On-ramps to help destress the environment would be helpful too. It's a challenge because we haven't really built many areas where people are welcome to just be, even with 3rd spaces that doesn't those people that are now rewired in their stress state. Some types of maybe community service (clean up, plantings, painting etc) or festivals events might be more helpful here as they can sometimes be lower stress, no required interactions etc. It's a tough thing especially as people have different reasons to isolate though poverty is likely one of the most major ones. reply jimbokun 2 hours agoparentprevThe evidence that it has a lot to do with social media is very strong. See Jonathan Haidt’s work on the effects of personal phones and social media on young people’s mental health. reply wholinator2 1 hour agorootparentI see it in analogy to sugar free sweeteners. There's some evidence that the physical experience of tasting sweetness is an essential phase in triggering the body's mechanisms to deal with large sugar intake. And that triggering that mechanism without providing any material for your body to consume can actually do damage to you as it searches for something to metabolize (this is just an analogy, feel free to prove me wrong). But just like that, online social interactions trigger some part of our internal mechanisms for reacting to actual community and belonging and healthy debate/conversation, but without the complete \"meal\" to digest that those things actually provide. Thereby triggering maladaptive behaviors and actually doing damage to the systems that regulate in person socializing. Probably over complicated but who among us, right? reply Apocryphon 2 hours agorootparentprevMaybe social media catalyzes the problem but the point is that it’s not the root cause, in the same way that opioids did not in of themselves cause the opioid epidemic. reply jimbokun 1 hour agorootparentYou need to provide some empirical evidence demonstrating what is the true root cause of the increase in mental health problems then. reply Apocryphon 1 hour agorootparentProbably the first step is to simply prove that mental health issues are also increasing among populations that don’t use social media. reply Anotheroneagain 1 hour agorootparentprevIron poisoning and lead deficiency. They got it the wrong way round. They incorrectly packed every major cognitive process into the neocortex, instead of assigning them each to the correct part. They got the idea of intelligence the wrong way round: The function of the neocortex is dimensionality reduction - the more powerful it is, the simpler everything is. You can't improve anything that easily. You actually broke it. reply tetris11 2 hours agoparentprevI was briefly jobless in a country with a fantastic social security net. Easily the most stressful and financially unstable moment in my life. It was only two months, but I genuinely thought I'd never recover my social standing, or confidence ever again reply wholinator2 1 hour agorootparentSorry, i cannot edit on mobile but i just realized maybe i read that wrong. Are you saying that as opposed to countries without safety nets, or instead emphasizing that even with security nets it can still be a very difficult experience? reply tetris11 39 minutes agorootparentVery much the latter. I was shuttled from incompatible job (skills and geography) to incompatible job, treated with utter contempt, and had to essentially beg for financial support to pay the rent. I'm a top 10% earner in the country, paid more than my fair share of social security for many years before that. It was a jarring experience seeing just how fine the line was between being on the right and wrong side of the system. reply wholinator2 1 hour agorootparentprevHow so, what was your experience? This is pretty diametric to most people's assumptions so I'd be interested in hearing more about how that went down reply doktrin 1 hour agorootparentPossibly due to social stigma. I don’t know that what I’m describing can be attributed to the safety net itself, but many countries with excellent social safety programs also have a low social tolerance for failure. This is not limited to using social benefits - e.g fail in business and you’re a business failure who will struggle to get any financing ever again. reply danlugo92 1 hour agorootparentSounds like people I wouldn't want to associate with then. reply lyu07282 1 hour agorootparentprevFrom a European perspective who made similar experiences: European countries all tend to be hyper neoliberal nowadays (despite Americans warped perspective) they don't want to offer social safety nets for ideological reasons but have to because of past, long since crushed leftist political movements having forced them into existence, making it very difficult to abandon them now by the ruling neoliberals. The net effect is the more slowly, erosion of social safety nets and making it as painful and unpleasant as possible to use them (as a matter of policy), as well as mass corporate media propaganda against it's recipients. It's a miserable experience by-design. reply Anotheroneagain 1 hour agoparentprevIt isn't that you're failing, they are the hikikomoris, who want to be alone, and the very fact that somebody tries to socialoze with them annoys them. It's just the norm in the west, so that it's you who stands out, and suffers alone. reply ThalesX 2 hours agoprevI've recently finished reading \"Civilized to Death\"[0] and I can't help feel there's some truth to some of the ideas. One idea that stuck with me is that shit zoos have concrete cages for the monkeys, and they're miserable in them, showing similar signs to modern humans (depression, addition, anger), whereas nice zoos try to keep the monkeys in similar environments to those that they evolved for, where the monkeys are pretty much chill. The author argues that we're constructing concrete zoos for ourselves and in the process making ourselves miserable. We're so far detached from what our bodies and minds evolved for, that it's an alien environment for our species. If this holds truth, it's really no wonder that the more we pile on and the further we stray from our true species' preferences, the more horrible we will feel, and this hikikomori is a fine illustration of that. As some comments pointed out 'what about the great depression', 'what about 'nuclear war', \"don't you like your electricity\"? These are all human patches for human made problems. I don't think the correlation between progress and wellbeing is as clear cut as some would like to see it. [0] https://www.amazon.com/Civilized-Death-What-Lost-Modernity/d... reply navane 2 hours agoparentI'm pretty sure the declining birth rate (or \"fertility\") is among the consequences of the change you are describing. The difference with past misery is the lack of stories to cover it up, or to give hope that this is temporary and it will get better. reply tetris11 2 hours agoparentprevI think even if we lived in a green paradise, there would be those who would measure themselves to others and still find themselves \"short\" to their more vocally successful peers I think inequality and toxic competition from an early age demanded by our soceity is a much bigger factor reply jwells89 1 hour agorootparentPeople feeling increasingly crushed by the daily grind to keep one’s head above water is almost certainly a bigger factor. So many people are just one unfortunate event, or even worse, one paycheck away from financial ruin with little in the way of an institutional safety net (and in the case of many, even a friends/family support network) and that takes a massive toll on one’s psyche. Speaking for myself, if all needs were guaranteed to be met I’d probably be happier living in a walkable metropolis than idyllic countryside. The part of the city that sucks isn’t the city as much as it is the rat race. reply Apocryphon 2 hours agorootparentprevI think the “by the way our planet’s dying” just adds to the framing of despair. It’s not the root cause but certainly compounds to it. Independently of this phenomenon, it almost feels like we’re reliving the ‘70s (pollution, urban decay, political breakdown). reply ThalesX 2 hours agorootparentprevDepends on the checks and balances you have in the 'society'. Are vocally succesful peers lauded? Then perhaps you could run into this situation. Are they mocked for having a big mouth? Maybe the chances are slimmer. > I think inequality and toxic competition from an early age demanded by our soceity is a much bigger factor Yeah, I tend to agree with you in that these are important factors into how things are playing out. And the scale. My God. We used to have inequality and competition between a small subset of people, now we're competing with 7+ billion. reply ItCouldBeWorse 2 hours agoprevIn my experience, life-experience increases the self-isolation. To the point that the old-folkshome are often halls of quiet, as everyone knows what horrible behavior perfectly normal people are capable and do not wish to interact. The guy who conspires against everyone at work, that manager that harvests others laurels, the longer you life, the more you understand how many will flip on you in this prisoner dilemma of a society. So they all barricade themselves in suburbia, sniper one another through HOA letters and claim to do it for the family, till its time to inherit and even the core family falls apart. Maybe some hiki is just more aware of what a lonely hellish life it is to be part of western society. And chooses to opt out. Lay flat. Assumes the party escort position. If he would at least consume drugs in there, but its just ramen and colored light. reply random9749832 1 hour agoprevWhen you are a teenager it is so easy to treat your time as if it is unlimited and start sinking 1000s of hours into some MMO or other games that before you know it you are in your 20s with no girlfriend, job, skill or self-confidence. Then you got Japanese entertainment like Hatsune Miku, idols and visual novels/anime that take advantage of lonely people with make-believe girlfriends. reply jwells89 1 hour agoparentIt may not be accurate to paint large amounts of time spent MMOs and the like as a net negative, though. Speaking personally as someone who grew up in a tiny town where there’s nothing for young people to do, WoW and the small nerdy circle of friends that came with it almost certainly kept me out of serious trouble in my teenage years and I think ultimately helped steer my trajectory in such a way that allowed for a more successful adulthood, even if it was a distraction from shorter term development. Of course this is something that will vary greatly between individuals, though. For some the depths of obsession are much more deep and destructive. reply card_zero 1 hour agoparentprevLots of visual novels/anime are about shut-ins tremulously venturing out into the world and eventually making friends, usually after a lot of anxiety and misunderstanding. I think they'd probably have an encouraging effect. I remember one where a woman confesses her condition to the person in the next apartment, and is advised to start small by visiting the convenience store. She manages it, and is incredibly proud of herself. Soon she is making lists of convenience stores, and has visited every convenience store in a five-mile radius! And now her problem is to diversify, but, you know, it's a start. reply random9749832 1 hour agorootparentHow encouraging is it really if there is lots of VNs about shut-ins? Seems like just more escapism. How many does the average person play before they start going outside? reply card_zero 51 minutes agorootparentI guess somewhere in the region of twenty, sixty-two, or maybe five? How could I possibly know the answer to that question? But it seems good if the stories you're consuming discuss your issues and inspire some cognition about your life. reply random9749832 49 minutes agorootparentIt was rhetorical. Thought I made it clear with the preceding sentences. reply card_zero 42 minutes agorootparentOK, so you were rhetorically saying \"this can't often help\", and I was rhetorically replying \"maybe it sometimes does\", but neither of us really knows, except I don't think they're bad. reply hooverd 25 minutes agoparentprevHey now, Vocaloid tuning takes effort and creativity! reply okdood64 1 hour agoparentprevWait, what's wrong with Hatsune Miku? It has relatively broad appeal; performed at Coachella. reply Apocryphon 1 hour agorootparentI think they’re describing a stereotype that’s a decade behind the times. reply RoboTeddy 3 hours agoprevIn all the cases in the article it looks like shame plays a big role. I wonder if hikikomori is caused by a loop of [adverse circumstances that cause the person to feel shame] -> withdrawal to avoid shame -> being ashamed of having withdrawn [loop] reply enceladus06 3 hours agoparentShame of educational pressure might be causing this, as mentioned in the article. But why do we as society place kids under so much stress? Let kids be kids and learn by exploring. reply elmomle 3 hours agorootparentI'd bet it's because the parents are feeling a lot of stress. Especially without a robust community support network/string external role models, children tend to inherit their parents' emotional states. reply esel2k 3 hours agorootparentprevAs a parent I also thought “I won’t repeat these patterns” but reality is, that is so hard. Often parents want the best for their children and use any possible technique to make sure they are successful. I am not giving an excuse but rather want to point to our society and our behaviour. When an expat at work asked me yesterday where to move to make sure that his 5 year old will have the best schools of the country… with such an elitist behaviour, I can only facepalm and see this is going to be much worse in the next 5-10years. reply ryandrake 2 hours agorootparentProblem is, the world is an economic slugfest today unlike it was at least when I grew up. When my High School class graduated a long time ago, most of us were competing for jobs with people in our own small town. At most, we were competing with the surrounding counties. There was university for A students, community college and/or middle class office work for B students, normal working class jobs for C students, and tougher lower-paying jobs for D students. As for university, we were competing for entrance with mostly other people in our state. Today's kids are competing with the entire world, and the middle class is disappearing. So it's much higher stakes. And it's bimodal: You're either one of the few winners and get to live a comfortable life with a professional job, or you're off to WalMart or an Amazon warehouse, or Prison. The \"kind of comfortable middle class life\" is shrinking quickly. So it's not enough to just get straight A's. You need extra credit, get a 5.0 GPA, take all the \"right\" AP classes, have the \"right\" extracurriculars, and the \"right\" community service and so on. Otherwise you risk landing on the bad side of the career bimodal distribution. reply matthewdgreen 2 hours agorootparentAnd AI, should it work out, is threatening to wipe out even the “good” side of that equation. reply lotsofpulp 2 hours agorootparentprev> Problem is, the world is an economic slugfest today unlike it was at least when I grew up It only did not feel like a slugfest for a select few in developed countries like the US/Canada/UK/Aus and maybe some other European countries. For the vast majority, hustling has always been a thing, including immigrating across oceans and leaving all of your friends and family behind. It just so happens that people in the US who used to or whose parents used to have security of shelter/healthcare/food no longer have that security. reply landedgentry 2 hours agorootparentprevProblem is, with growing inequality, the 80th-percentile school and the 20th-percentile school is vastly different, whether that is in school resources or the life trajectories of graduates. reply HPsquared 3 hours agoprevIt's definitely happening in the West too. The juice isn't worth the squeeze for a lot of people. reply OutOfHere 3 hours agoparentFrankly I think it's more sensible than putting up with an unjust system in a rat race that goes nowhere, although eventually one has to get into the work game to survive. reply motohagiography 1 hour agoprevour baseline entertainments are better than the long tail of negative interactions one persists through to get to the great ones. in doomscrolling vs. disappointment, more people are picking doomscrolling. older than the examples, but I have done this. live alone on a large rural property, walden style, tech comp no family, have online interactions for remote work, old irc channels, take some sport, fitness, and music training as kind of weekly rhythm, family lives in other time zones. it was an ideal I thought I could achieve and then have it to share with others. relationships and friendships with any personal connection or intimacy still manage to fail, lots of reasons but I'm the constant. only way to sustain anything is to keep it at a polite distance with no expectations. issue i suspect is that meaning comes from the cohering and persistence of relationships, and without that persistent mutual understanding, meaning just seeps away and leaves a flat state of inertia. no advice other than to avoid this example. I sympathize with these young people, it's as though they don't see a present or future in which there is meaning for them, or in which they are a participant, and so they are just withdrawing and waiting for the next life instead of engaging this one. it's a unique and recently invented trap, avoid it as best you can. reply MathMonkeyMan 2 hours agoprev> He dare not come in company, for fear he should be misused, disgraced, overshoot himself in gesture or speeches, or be sick; he thinks every man observes him, aims at him derides him, owes hint malice. - Hippocrates?[1] [1]: https://old.reddit.com/r/AskLiteraryStudies/comments/68zg38/... reply Ozzie_osman 2 hours agoprevI know everyone is different, and saying things like \"just get a job\" or \"just go outside\" are easy to say and very hard to do when you're stuck in that type of loop. But, I will say, things that I've found will help are having some purpose (work, taking care of someone or a pet, anything), exercise (even walking outdoors), and even just getting your biological clock where you wake up and get exposed to sunlight (vs sleeping all day and staying up all night). Getting enough activation energy to do any of those things is difficult, but I've found that if you can muster it, it can help break the cycle. reply nanomonkey 1 hour agoparentHonestly, my dog is my savior. I take her to the park every evening. This gets me out into the evening sun, walking, and around like-minded people. The fact that I can interact with a group of people without scheduling an \"event\" is great. We just show up. For me it's mostly the other dogs (puppy therapy), but it's nice to exchange a few words and talk to someone about their day while the dogs run around. The two mile walk to the lake is also key. I find a morning stretch and kettle bell routine, and an evening walk keeps me mentally and physically in tune. And it's practically free, unlike modern healthcare. reply HPsquared 2 hours agoparentprevIt's a bit like refloating a sunken ship. reply itronitron 2 hours agoparentprevYeah, it's much easier for me to leave the house when it's a situation in which I don't need to care about the potential of other people's opinions. Running quick errands, walking the dog, etc. For some reason taking out the recycling is a heavier lift for me mentally due to the risk of chaos with the bins, potential for bags breaking, bins being full, etc. even though there is less social interaction than grocery shopping. reply BaculumMeumEst 2 hours agoprevIt’s fun to watch mass media inflict so much harm on people with irresponsible reporting designed to terrify, outrage, and misinform you for profit, and then turn around and report on the fallout. Maybe fun isn’t the right word. reply getpost 27 minutes agoprevSeveral adults in my friendship circle (retired or semi-retired) have evolved to spending nearly their entire waking lives online. They're able socialize normally, but they don't make the time to do that as often as in the past. This is tantamount to hikkormoridom. One friend went to visit two other friends who live together in New Mexico. He imagined they'd be out and about doing stuff during his visit, but the hosts remained preoccupied by their online activities. The visitor could have stayed home and texted. reply GlibMonkeyDeath 31 minutes agoprevHmm, no one is talking about the enabler for this - modern wealth. In the not so distant past, refusing to get up and face the world would result in starvation. Survival required people to be more social. As a parent of two adult children who are both working, I can't imagine enabling this (even though I could.) Sure, if my kids were truly disabled that would be another story, but it seems the hikikomori are just unhappy with the world. Enabling them to spend their lives doomscrolling or playing games is actively harmful. reply AIorNot 28 minutes agoparentI think the bigger enabler is the internet with its endless source of media reply forinti 1 hour agoprevIn South America it's become common to hear about adolescents and young people (mostly men) who spend all their time on video games and neither work nor study. I imagine this to be a very different phenomenon from Japan, because the culture is so different. In South America I think it is just general disengagement and disillusion with society and work environments in general. For most people life is having a bad job that pays very little and you have to spend hours on a crowded bus to get to a pretty horrible part of town. Living in the virtual world is much more comfortable and pleasant. reply Gigachad 4 minutes agoparentIn Australia there is a pretty good welfare system, but they will basically give you a job to work while you keep applying for a real one. Usually it’s something like sorting clothes in a charity store. I imagine it helps to keep people engaged in society somewhat. reply conwy 1 hour agoprevWhy to complain? Seems like heaven to me. reply 39896880 1 hour agoprevIf this topic interests you, you might enjoy the book “Shutting Out the Sun: How Japan Created Its Own Lost Generation” by Michael Zielenziger: https://openlibrary.org/books/OL24765707M/Shutting_out_the_s... reply itronitron 2 hours agoprevThe social home is a great idea and I am glad to hear that it works. It would be interesting to compare social communication styles between people in the social home with social communication among extroverts (in another situation). I expect they are quite different but it would be good to know how they differ. reply hiAndrewQuinn 1 hour agoprevI think I came pretty close to ending up like this kind of person a few times. Each time I got out it was by basically taking a hammer to my superego and doing some shit I used to consider \"unforgivable\", like being long term unemployed, or engaging in long term substance abuse, or moving countries basically on a whim. And, if that's what it takes, then I stand by it. If your life choices are \"hikikomori\" or \"scumbag\", you'd be an idiot to not choose scumbag. Ideally you can get out of it through less destructive means, but let's not pretend like closing yourself off entirely from the world is better than having a problematic but loving relationship with it in all its colors. reply Borrible 1 hour agoprevhttps://en.wikipedia.org/wiki/The_Machine_Stops \"Imagine, if you can, a small room, hexagonal in shape, like the cell of a bee. It is lighted neither by window nor by lamp, yet it is filled with a soft radiance. There are no apertures for ventilation, yet the air is fresh. There are no musical instruments, and yet, at the moment that my meditation opens, this room is throbbing with melodious sounds. An armchair is in the centre, by its side a reading-desk - that is all the furniture. And in the armchair there sits a swaddled lump of flesh - a woman, about five feet high, with a face as white as a fungus. It is to her that the little room belongs.\" ... \"Vashanti's next move was to turn off the isolation switch, and all the accumulations of the last three minutes burst upon her. The room was filled with the noise of bells, and speaking-tubes. What was the new food like? Could she recommend it? Has she had any ideas lately? Might one tell her one's own ideas? Would she make an engagement to visit the public nurseries at an early date? - say this day month.\" reply grugagag 2 hours agoprevIm curios if population desnsity may have any corelation to this or not. Im also thinking that the culture is shunning a certain personality type or people with some mental struggles that their only way to adapt is to withdraw. reply jackcosgrove 1 hour agoparentPopulation density is absolutely the driver of all of this. It increases competition, decreases personal space, and limits access to nature. Humans are social animals, but we aren't ants. We're not supposed to live in concrete jungles. As others have said, hikikomoris are one extreme of a continuum of negative outcomes caused by modern life. A low birthrate is another, which is correlated with urban living (there are other factors but this trend is unmistakable). I believe this is a natural outcome of very high population density, a negative feedback mechanism. When life becomes unbearable because of too many people, we withdraw and don't reproduce. reply blopker 2 hours agoprev> He said he didn’t respond to friends’ messages or confide in anyone, feeling like nobody would understand anyway. I feel this. I think people would call me an introvert, but I'm probably just an over-thinker. It's casual conversation that seems to be exhausting (or uninteresting?) to me. Once I'm in a space where I can talk openly about more abstract topics I start to enjoy it. Getting there just often seems like too much work though. I tried therapy, meditation, 'wellness' apps. It all either felt too 'me' focused, or too detached. I like this site because people here seem to share what they are actually thinking, and are eloquent enough to capture interesting nuance. I don't always agree with it, but there's a level of authenticity to where I always learn something about the human condition. I wanted more of that. [This is kind of a plug, but whatever] I've spent the last few years in a deep-dive around why we seem to be collectively getting lonelier over time. I started a non-profit[0] to house this research. It's evolved into a platform where we host these support groups. Anyone can join, it's free, and as long as you stick to the community guidelines [1] anyone is welcome to join. For me, it's a place to get out of my head. To hear from real people who don't generally feel like their voice matters. I know from years in tech management that these are in fact the most interesting people to talk to. I've never really talked about Totem here because I think it might be too 'woo-woo' for this crowd, but if any of that landed for you, come check us out. If you don't like it, I'd love to know why. My personal email is in my profile. We are a non-profit, grant-funded, and open-source[2] organization. Feedback of any kind is welcome. My hope is to become something like a public utility for these spaces. We're also looking for engineers to help make an app out of this. [0]: https://www.totem.org [1]: https://www.totem.org/guidelines/ [2]: https://github.com/totem-technologies/totem-server reply SoftTalker 3 hours agoprevIt's an interesting article I don't know why it had to be presented in such a cumbersome graphical style. I tried the trick of changing \"www\" to \"lite\" in the URL but it didn't work for this one. I stopped reading about halfway through. reply sctb 2 hours agoparentThis presentation is surely intended to convey a sense of spatiality related to the idea of \"A shrinking life\". I enjoyed the graphics and photos, and I definitely got a feeling of constriction and isolation that added depth to my reading experience—YMMV. (The HN guidelines discourage complaints about these sorts of \"tangential annoyances\", by the way: https://news.ycombinator.com/newsguidelines.html) reply anovikov 3 hours agoprevIf their parents are not doing anything about it when it's their direct responsibility, how can anyone expect anyone else to fix it? If the culture allows for this, the problem is with the culture. reply HPsquared 3 hours agoparentIt's hard to change another person's mind. reply anovikov 3 hours agorootparentIn America, they'd be kicked out of home as soon as they turned 16. In Russia, they'd be just beaten up by their dads. Probably badly and regularly. They do it because they can. Key is to make it impossible and force them to take up productive activities. No one in adult age should be kept at home funded by their parents unless it's short term with definite plan to fix it, or they are seriously disabled. Of course, when it's about schoolchildren, it's different. Serious psychiatric help is needed. It's again parents' fault for not providing it. reply SoftTalker 2 hours agorootparentI don't know about Russian dads, but in America it's very likely they'd be indulged. I don't know anyone who would kick their teenage child out of the house because he or she was depressed. Especially when you see the sort of life the homeless have in most places. I'm sure it happens, but at least from my perspective it would not be common. reply ryandrake 2 hours agorootparentYea, but the acceptability of adult children living at home with their parents has really changed since when I grew up. We were pretty much all kicked out at 18 to go live on our own. That was just what we did in the USA. Some had a little financial support from parents--maybe the parents paid for car insurance or gas or something. But never the rent-free home for 10-15 years like we see today. The culture has changed massively. My old man would have never accepted me living at home after high school, but I look at my kid and think, well, she'll really struggle to afford rent, and she'll never own her own home, so I guess we have no choice. reply linearrust 1 hour agorootparent> Yea, but the acceptability of adult children living at home with their parents has really changed since when I grew up. For nearly all of american history, children ( adult or not ) stayed at home with their parents until they got married. Especially the daughters. Emily Dickinson's parents didn't kick her out when she turned 16 or 18 or 30 or 50. Emily Dickinson famously died in the same house she was born in. > That was just what we did in the USA. This is simply not true. At least not for the vast majority of american families. It is actually the opposite. Where parents wanted their kids to stay at home while changes in popular culture made kids want to venture out before they got married. > My old man would have never accepted me living at home after high school Even if that mean you sleeping in the streets? I doubt that. If what you wrote is true, we'd have a far greater homeless population than we do. The idea that americans kicked out their kids after high school is nonsense. Sure parents would want their kids to get a job, go to college, etc after high school but only a tiny fraction of parents would kick their kids to the curb once they finished high school. Especially the daughters. Just ask yourself, would you kick force your kids to live on the streets at 18? Of course not. Which parent would answer yes to that? reply ryandrake 1 hour agorootparent> Just ask yourself, would you kick force your kids to live on the streets at 18? Of course not. Which parent would answer yes to that? That was the point of my post. My parents’ generation would and mine won’t. I don’t know anyone I grew up with who would have been allowed to live in their parents’ house for a decade after graduating high school. It would be unthinkable. Times have changed and what was unacceptable in the 80s and 90s is more acceptable now. reply SoftTalker 1 hour agorootparentI'm coming up on 60 years old. Though I don't know anyone of my age who stayed at home after high school (I was a college-bound kid and all my friends were) I do not think my parents would have \"kicked me out\" at any point. I did know some kids who got jobs and moved in with friends or got their own apartments, but never heard it was because their parents kicked them out, they just wanted to get away from their childhood and live on their own. I just don't remember getting kicked out by your parents being a common thing. But maybe I was just never exposed to it. reply HPsquared 2 hours agorootparentprevThat factor is probably a big one in deciding who is homeless and who isn't. It's the main barrier, really. reply SoftTalker 2 hours agorootparentThough you could probably argue the degree to which its right or wrong, they way I look at it is: I brought this person into the world, if he isn't self-supporting at age 18 (or 22, or whatever) then why is it fair to society that I make that society's problem? He's my kid, and I should own the responsibility for that. reply linearrust 1 hour agorootparentprev> In America, they'd be kicked out of home as soon as they turned 16. This is a lie. Are you american? I've never met a single person who was kicked out at 16. This include kids who were drug addicts, high school drop outs, pregnant teens, etc. > In Russia, they'd be just beaten up by their dads. I highly doubt that. Are you russian? > No one in adult age should be kept at home funded by their parents unless it's short term with definite plan to fix it Ideally. Unless they are writing poems like emily dickinson? reply kelseyfrog 2 hours agorootparentprevForcing people to work hasn't been historically successful. That includes stacking the incentives up to work or death. A large fraction of people simply choose death because no one wants to be forced to do things. If I had to guess, you don't either. reply golergka 1 hour agorootparentFor overwhelming majority of human history people had either to work or die of starvation. There's absolutely no evidence to suggest that any significant fraction chose death. reply dymk 2 hours agorootparentprevIt sounds like you’re advocating for abusing teenagers (16 is still a child) reply jmyeet 3 hours agoprevPeople aren't stupid. What we have in the world in general (not just Asia) is a crisis in hopelessness. People are facing crippling student debt (depending on your country), one bad medical incident away from being homeless, crippling housing costs and wages that barely cover costs such that you need 1-2 \"side hustles\" just to make ends meet. It's really no wonder people are checking out. It's also no wonder that people aren't having children either. They simply can't afford to. One common counterargument to this is that consumer spending is up but that really makes my point: people are spending now instead of saving because they have no future. reply yazzku 2 hours agoparent> People are facing crippling student debt (depending on your country), one bad medical incident away from being homeless. Both of these are U.S. reply Apocryphon 2 hours agorootparentOn the other hand, housing being very expensive feels like a universal phenomenon now. reply tetris11 2 hours agorootparentPerhaps tangentially related, but today my landlord gave the final inspection of the place. He told us that it pained him that we could only live there for a year. We were his cash cow. The year before, his wife had died, and he needed more family support and his daughter wanted a place near him. He told me that only a few years ago he would have simply built her a house next to the one he was renting us, but even building costs have skyrocketed so much that the only option he had was to kick us out and let her in. He said he was sorry, and the thing is, I believed him. reply anovikov 2 hours agoparentprevBut this all is so evidently untrue! Stock of accumulated capital is growing healthily which means savings rate is healthy too, even if most of savings is just reinvestment of profits from previous ones. Consumption of almost everything is growing in real terms too in almost all countries. GDP keeps growing just about everywhere save for a few most unlucky places like UK. Even inequality is falling for the first time in decades so the poor people actually see most improvement (especially in the US). That 'hopelessness' is what people call 'vibecession'. People just taught themselves - probably by going through too much doomscrolling - that things are bad. But they aren't. We are probably living through best times in our lifetime - especially the common folk, poor and working class (some highly educated, high income occupations might be on shaky ground). MAYBE, just maybe, upper middle class is a bit fucked indeed, but this is a small group numerically and hikikomori do not come from there. I'm glad i'm not on social media but whenever i'm trying to read so-called 'news', i also get depressed for a bit. People are trying to present absolutely everything in the most negative sense possible probably because otherwise it's not clickbait-y enough... reply bluefirebrand 2 hours agorootparentGDP or savings rates don't matter to individuals who are spending all of their money on food and rent Countries and corporations reporting strong financial health does not help the people who are living paycheck to paycheck It's very out of touch to suggest otherwise reply yoyohello13 2 hours agorootparentprevMaybe GDP, consumption, and corporate profits are not correlated to human happiness. reply jmyeet 1 hour agorootparentprev> Stock of accumulated capital is growing healthily How are you defining \"accumulated capital\"? Because in wealth and income terms, inequality has only been growing [1]. Specifically, does accumulated capital include paper gains in housing values of your primary residence? If so, that's misleading. Housing is a basic need. If you buy a house for $200k and it has gone up in value to $500k, you might say you've accumulated $300k in unrealized capital gains. Thing is, what would you do if you sold it? You'd still have to buy a house somewhere. And if every other house also costs $500k, what have you really gained? You might be paying higher property taxes, higher insurance costs and possibly higher mortgage costs. > Consumption of almost everything is growing in real terms too in almost all countries. I addressed this. People aren't saving for their future. > GDP keeps growing just about everywhere And who benefits from that? Let's say your wages have gone up 25% in real terms, where is that money going? If it's just more rent to your landlord, then you'e gained nothing and someone else is simply exploiting more of the value you've created. > Even inequality is falling for the first time in decades No, it's not. See the link. > We are probably living through best times in our lifetime By pretty much any objective measure, we peaked in about 1972. Since then real wages have largely stagnated and the quality of life has gone down. [1]: https://www.pewresearch.org/social-trends/2020/01/09/trends-... reply anovikov 54 minutes agorootparentThat's the thing. Inequality quickly dropped in the last 4 years. Over 40% of inequality growth accumulated since 1979 has been cancelled out in just 4 years. And, sure enough, by every possible metric, in 50 years, quality of life has improved everywhere, for every income bracket. It's utter lunacy to suggest otherwise. With the U.S. real GDP per capita growing 2.5x since, there is simply no way for any place or any social group to feel worse, by any possible metric, even if cherrypicking. And, it's not worse than world average. Even since 1991 when situation has probably been strategically the best for America, it did not fall behind the world economically (China made a quantum leap, but Latin America and ex-Soviet sphere, has fallen way behind) As for housing, well, housing never grew in price above average inflation and it does not grow that way now, either. Inflation-adjusted square foot price in US and almost all Western countries have remained stagnant for ~50 years. In UK, yes, it is different. Australia too. If people are unhappy with economic situation today, nothing will ever be enough for them. reply im3w1l 1 hour agoprevI think it may be due to some sort of \"outcome compression\". The life of working a bottom tier-job isn't materially better than a life of not working at all. One solution I've been thinking of is that maybe there needs to be some kind of state-provided minimum life. Almost like opt in communism. If you opt in then you get an 8h/day job automatically. Doesn't matter if you don't have any skills at all. The job will be guaranteed safe and non-humiliating (no sex work and if you are a vegetarian you don't have to work as a butcher etc). In exchange you get enough food, clothes and shelter to provide for yourself and children (assuming two incomes), entertainment (exchangable for cash equivalent), pension, health insurance, upskill opportunities, and some money on top. If you have negative net worth when enterring the program, your loans will gradually decrease (this could be done as some combination of the state paying them off for you and the loan giver being stiffed). You can opt out at any time you want. reply badpun 3 minutes agoparent> I think it may be due to some sort of \"outcome compression\". The life of working a bottom tier-job isn't materially better than a life of not working at all. The Hikkikomoris in Japan are all sustained by their parents. There's actually a growing concern there about the first generation of Hikkikomoris who are getting into their 50s already and their parents are starting to die, leaving them with no life skills and no source of income. reply sirianth 16 minutes agoparentprevinto it. reply enchanted-gian 1 hour agoparentprevsomething like universal basic work? reply Apocryphon 1 hour agorootparentOr a federal jobs guarantee. reply im3w1l 1 hour agorootparentprevThat might be a more palatable name for it. But a key feature of my proposal is that you actually get the goods you need rather than a lump some of money that may or may not be enough to buy them. reply Joel_Mckay 3 hours agoprevDr. John B. Calhoun's work with \"The Beautiful Ones\" may unfortunately generalize to human civilizations: https://www.youtube.com/watch?v=iOFveSUmh9U reply causality0 3 hours agoparentIt's strange. We know that instincts, hormones, and other aspects of our biology have a tremendous influence on our thoughts and behaviors, but there doesn't seem to be a lot of effort to use that to problem-solve. There's evolutionary psychology, but given the fraction of human problems traceable to maladaption to the modern world, one would think \"evolutionary anthropo-sociology\" would a foundational pillar of science. Like, why doesn't \"humans didn't evolve to sleep in a room by themselves at the age of four\" carry as much weight as \"don't let your kids eat lead paint chips\"? reply BlarfMcFlarf 2 hours agorootparentProbably because evolutionary psychology is an incestuous pseudoscience of just-so-stories, small sample sizes, bad extrapolations, non reproducibility etc. reply throw383y8 3 hours agoprevnext [5 more] [flagged] OutOfHere 3 hours agoparentIt really isn't about smoking weed at all. reply throw383y8 3 hours agorootparentDrugs are illegal in most of Asia. Weed is popular in US. reply dymk 2 hours agorootparentAlcohol is legal pretty much everywhere, and is measurably more damaging in just about every aspect compared to weed. But we don’t say it’s a booze problem. reply CatWChainsaw 2 hours agoparentprevWorry about whether or not you'll live in a slurry dystopia or die before old age as the planet cooks. reply Hikikomori 2 hours agoprev [–] I hope they have a roof as well. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The CNN article explores the phenomenon of hikikomori, individuals who isolate themselves at home for long periods due to psychological, social, and cultural factors.",
      "Initially identified in Japan, hikikomori is now a global issue, worsened by the internet and the COVID-19 pandemic, with personal stories from Hong Kong and Japan illustrating the challenges and recovery processes.",
      "Efforts to reintegrate hikikomori are urgent due to aging populations and declining birth rates, with communal living programs in South Korea and calls for specialized treatment to address social withdrawal."
    ],
    "commentSummary": [
      "The article examines hikikomori, a condition of extreme social withdrawal, and its worsening due to financial and health issues, alongside China's \"laying flat\" movement driven by job scarcity and high living costs.",
      "It explores themes of social isolation, the impact of modern technology, societal pressures, and individualism, contrasting historical and modern existential threats like nuclear war and climate change.",
      "The narrative critiques weakened safety nets, societal norms fostering isolation, economic disparity, and the potential threat of AI on job security, while suggesting solutions for young people's disengagement."
    ],
    "points": 119,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1716646236
  },
  {
    "id": 40470134,
    "title": "Understanding ACATS: The Complexities of Brokerage Account Transfers",
    "originLink": "https://www.bitsaboutmoney.com/archive/how-acats-transfers-work/",
    "originBody": "Guys what is wrong with ACATS Patrick McKenzie (patio11) • May 24th, 2024 Many beginnings imply a contemporaneous ending. This is often bittersweet. Some personal news implies a tearful goodbye to soon-to-be-former coworkers. A new adventure of scholasticism and self-discovery means saying goodbye to your high-school friends. And a new brokerage account often implies leaving a years- (or decades-!) long relationship with a firm that stuck with you, feels a bit like a jilted lover, and by the way happens to constructively control most of your net worth. This particular beginning and ending is mediated by a complex techno-legal system called ACATS: the Automated Customer Assets Transfer System. ACATS is quite impressive, underpins a very important part of the financial system, and some of the quirks of how it operates will probably surprise you. The title of this issue is a play on an AI-generated song. Infohazard warning about which I am being absolutely serious: you probably have the experience of a song being an “earworm” that you cannot get out of your head. This song is not simply an earworm. It is auditory superstimulus, like the Dorito, carefully designed to taste like nothing in nature. Unlike the Dorito, which someone is guilty of, this song either has no author or has all the authors. I think if you say the words “my cat” to me when I am on my deathbed I will immediately hum three notes. With that very important caveat out of the way, if you want to be mimetically infected as the price of getting this reference, take a listen at Sono here. A brief digression into self-regulatory organizations Brokerages are regulated by FINRA. FINRA stands for many things, though these days FINRA might deny that it is an acronym. In previous years, though, it was definitely the Financial Industry Regulatory Authority. One reason FINRA is not an acronym, to the extent it is not an acronym, is that an unsophisticated investor might hear that and assume “Ah yes, FINRA is clearly part of the government” and FINRA will immediately swear up, down, and sideways they are not. They are just a financial regulator overseeing trillions of dollars. Self-regulatory organizations (SROs) are industry associations. There are many industry associations in the world. Some pool money to pay for a-rising-tide-lifts-all-bovines advertising. Some exist to get peers together for merriment, diversion, and some conspiracy against the public. (This is a joking reference to a famous passage from Adam Smith. On a completely unrelated note, please feel free to introduce yourself if you see me at a software conference. I’ll be doing a talk about raising prices.) SROs are the type of industry associations that partially exist as a blocking play. If we don’t get our house in order, Dangerous Professionals from the government are going to barge into our house to order it for us. That will be disruptive to providing valuable services to customers at a price they are willing to pay. FINRA regulates asset transfers between brokerages Discount brokerages are large, trustworthy, competent institutions. But there are some brokerages which are not. There are wirehouses attached to large investment banks like e.g. JP Morgan (large, trustworthy, and competent, but not a discount brokerage), there is Robinhood (a large discount brokerage), but by far the most numerous are small boutiques which keep on keeping on. Some of those boutiques have been known to be a bit grasping when assets under management attempt to walk out the door. They would refuse to let their customer leave. When told this was extremely improper, they whined and said it was really difficult to facilitate their customer leaving, and wouldn’t the customer prefer staying, and Cindy who can actually take care of this will be back in the office the first Tuesday after the waxing moon. And so FINRA listened to its members (brokerages), customers, advocates, and counterparts in government, and passed a rule. Cindy can go on vacation any time she wants, but it is the brokerage and not Cindy who is responsible for outcomes, and only one outcome is acceptable: if a customer wants to move their assets out, you must let them. The full rule is necessarily more complicated than that gloss of the intent of the rule. It’s not unknowable inside baseball; see FINRA Rule 11870. It is somewhat somnambulance inducing: When a customer whose securities account is carried by a member (the \"carrying member\") wishes to transfer securities account assets, in whole or in specifically designated part, to another member (the \"receiving member\") and gives authorized instructions to the receiving member, both members must expedite and coordinate activities with respect to the transfer.. But, by the standards of many regulations, it is short and actionable. Rule 11870 doesn’t itself establish a technical artifact but exists in tandem with one: ACATS. How does one transfer securities account assets? What is a share of stock, really? An abstracted right to ownership of a corporation? A legal contract promising the same? Some complex sociopolitical edifice where judges who are not yet born will of course automatically award surplus returns of an enterprise to an equity holder even when told not to by a nuclear-armed government? A share is all of these things. But also, in a really important way, a share is an entry in a spreadsheet. Whose spreadsheet? Everyones’ spreadsheets. Stock that you own, and you really do own it, exists as the superposition of several spreadsheets. Your spreadsheets, for example. Those matter. Spreadsheets (or databases, or blockchains, or… actually no probably not blockchains even cryptoenthusiast technologists don’t believe that will happen anymore) at your brokerage. And then, in a fascinating wrinkle that Matt Levine has covered many times, a spreadsheet at the Depository Trust Company, which keeps almost all the stocks and simultaneously has very probably never heard of you. So when you move stock between brokerages, nobody needs to print out a stock certificate and courier it across Chicago, New York, or the Pacific Ocean anymore. Thank goodness. (I have no stories, but I have friends who have stories, and the Die Hard steal-the-bearer-bonds plot didn’t come from nowhere.) You just have to coordinate updating the spreadsheets. How hard could that possibly be. ACATS is a system with technical and legal elements to it. It greatly decreases the number of moving parts required to coordinate updating spreadsheets. The pre-ACATS era meant needing to interface directly with the thousands of other brokerages in the United States. You had to care deeply about the operational differences at their firms. Sometimes your Ops and their Ops didn’t use the same version of Excel. It was anarchy. ACATS puts very diverse firms between a relatively consistent experience, while simultaneously codifying operations and reducing various forms of risk to the process. This is a very common way to create value in financial technology. What does an ACATS request actually entail? A customer selects a new brokerage and tells that brokerage they intend to move in assets. That brokerage, which very much wants to get those assets onto their own books (and spreadsheets, etc etc, as a necessary consequence), will assist them in operating ACATS on their behalf. The customer will very likely never care about nor understand a complex operational symphony happening in the background. The brokerage will likely kick off a few processes which don’t necessarily happen in Internet time and aren’t strictly coupled but might feel like they are to the customer. They will ask the customer to create a new account, which (extremely relevantly) will require the brokerage running their KYC process on the customer. They will very likely ask the customer for their last brokerage statement. And they will ask the customer to authorize them moving over the previous assets. That authorization is customarily on a very templated rather short contract / form, and the template is almost inevitably going to rhyme heavily with the template in FINRA Rule 11870. But, in one of those fascinating rabbit holes about how the world actually works, authorization does not mean performing a particular ritual on a particular written instrument. Authorization means permitting something. You can permit something with words, most typically, or even a gesture. As a very concrete consequence of this, many of those forms will be filled out not by the customer, but by the brokerage employee working on onboarding them. This is not bad and is not fraud. That feels weird to say out loud but it is extremely important: they have authorization. They are doing the thing brokerages do, taking specific authorization for a specific action from a customer and translating it into a complex series of technical and legal processes to cause the physical result in the world that the customer wants to happen. And so, the form that authorizes an ACATS request might have a signature blank at the bottom. Some of them are signed by the customer, in that the customer had that form physically presented to them and they affixed their signature with a pen. Some are signed by the customer via a solution like Docusign, which might or might not imply that they actually saw an image which physically resembles the form that gets signed. And some of them are signed on the customer’s behalf. The exact form of that might look like the ASCII characters /s/ John Q. Public. Skeptical? Those are, and these words are carefully chosen to sound very rigorous, “an electronic signature in a format recognized as valid under federal law to conduct interstate commerce.” You probably assumed there would be public key encryption involved in an electronic signature and this is allowed but not required. All of this is actually normal. And, combined with the next bit, it will give many security-minded people an aneurysm. Brokerages frequently do not verify incoming ACATS requests ACATS is a network of trusted peers who have contractual (and other) relationships with a central organizing entity. One thing peers agree to do is to act upon incoming requests very, very quickly by the standards of financial institutions. One thing they do to accomplish this is very surprising: most ACATS requests will cause the brokerage losing the assets to not verify with their customer that the request is authorized. “What.”, I hear you ask. No, this is true, and this is designed, and this is normal. It only sounds batshit insane. Let’s start with the timeline: a brokerage receiving an ACATS request must complete any investigation within three business days. FINRA doesn’t get hyperspecific on any particular thing you must or mustn’t do within those three business days, but that shot clock starts running instantly once your computer gets the message from the other computer. “Cindy didn’t check her mail because she was on vacation” is not a valid excuse. The brokerage gets only two options: validate (agree to) the request, or take exception to the request. Validation starts a second shot clock to actually complete the spreadsheet updates. It is not quite a no-takesies-backsies decision. True trapdoors are rare in finance. But reversing it is uncommon and unfun for all parties. You cannot take exception simply because you feel like it. You must communicate one of twelve enumerated reasons. The general flavor of them is “that account has no assets in it”, “that account number doesn’t correspond to an account that exists in this universe”, “the person who you claim has authorized this transfer doesn’t own that account”, etc. Questions about title, about who really owns the assets in an account, sound really simple to non-specialists who are mostly familiar with individual accounts. John owns the money in John’s accounts, right? Hah, hah, hah. The “edge cases” cover trillions of dollars. John and Mary just divorced and while the account records reflect John as sole owner, the divorce decree says Mary owns half of the account. Your blockchain disagrees with an Article III judge? Then your blockchain is wrong. Fix your blockchain. These determinations are fact-intensive and, again, are not necessarily obvious to either brokerage or even to the account owner themselves. John very likely thinks he owns his own money and may even think that in a sincere and innocent fashion. The brokerage doesn’t have actual possession of a divorce decree and very likely has no actual knowledge of a contemplated divorce. It doesn’t matter. Tick tock tick tock. FINRA doesn’t care. The orderly operation of capitalism must go on, private tragedies notwithstanding, and your brokerage must make a determination before three business days are up. Validate or take exception. Those are your only two options. Now let’s superimpose another difficult reality on this one: brokerages will, in the ordinary course of business, spend long periods of time happily having no real communication with their customers. Oh sure, their customer will receive account statements, and they might even place trades, but the last time a human talked to another human was… early in the 2010s? Ping, ping, incoming message from ACATS. John purportedly wants to move his assets. The shot clock has begun. You have three business days. Does the phone number on file from 2004 still work for John? FINRA doesn’t care. Does John still use AOL? FINRA doesn’t care. Can the United States Postal Service successfully put a piece of paper in John’s hand within three business days? FINRA doesn’t care. Will John pick up the phone for an unknown caller attempting to reach him on a matter of urgency? FINRA doesn’t care. Is John in the hospital on his deathbed? FINRA doesn’t care. Brokerages are broadly competent and they know all of this. They know they cannot, at scale, successfully verify all of the transfers for all of the customers. And so they make a business decision to not contact customers for most transfers by count and reserve extraordinary efforts for contacting only very important customers, who might be most transfers by volume of assets. The brokerage will absolutely not phrase this as “We don’t verify outgoing transfers.” They will check, and check most diligently, that the account number claimed is the account number, that the name matches the name on file, etc. And their Operations team understands that sometimes names do not match and that is OK, and sometimes it means Nope That’s A Specially Enumerated Exception Right There. Sometimes they will look at the signature card, because everyone enjoys live action roleplaying occasionally. If John cannot in 2024 reproduce his signature from 2004, I have an epic non-surprise for you: FINRA doesn’t care. But, hey, it is the culture of the United States that financial institutions and expert witnesses in court sometimes do forensic analysis. Do we believe it is possible to compare signatures and gain useful information? Do we believe in the tooth fairy? Yes in some ways and no in others. We take no important decisions premised mostly on belief in the tooth fairy. And, again, “/s/ John Q. Public” is a normal and accepted way to represent John’s consent to move assets. Small account transfers with paperwork that has no glaring errors will be approved in the ordinary course. Sometimes those transfers will be fraudulent. Brokerages defrauded in this fashion will be annoyed, but not surprised, because they are competent financial institutions. They understand that the optimal amount of fraud is not zero. So what, ultimately, is a brokerage relying upon when it sends money to /s/ John Q. Public? It is relying on chained trust in a community of practice, and on a web of contracts, and on a business decision, all at once. And that means that if a bad guy can convince any brokerage in the U.S. that it is John, the bad guy can fairly reliably cause movement of all of John’s financial assets. Recent developments in ACATS fraud You can probably guess the shape of the attack. Get a copy of John’s ID from, perhaps, a vendor specializing in “fullz” on the Dark Web. Figure out where John keeps his accounts by e.g. just guessing that it might be one of the places where 80% of Americans with assets keep their retirement accounts. Open up an app, tap tap tap, request to move “your” assets to “your” new account. And then lie about being John while telling some truths you know about John. Now, wait five to seven business days. Congrats, John’s assets now appear to be in “your” brokerage account. Your brokerage is in the business of giving you access to “your” money swiftly when you want it. Now would be a great time to wire it out, take it out on that debit card connected to the account, place a trade which successfully transfers value to a confederate’s account, etc etc. Five to seven business days is much more frequent than many Americans, even many wealthy Americans, check their brokerage accounts, and so the money may be spendable before any involved human realizes it has been taken improperly. This is, obviously, super duper illegal. But in another sense it is just business. For you, as a criminal, this is Tuesday. And for brokerages, well, capitalism hopes they catch most people trying this. Some brokerages have not successfully caught some people trying this. That is normal and expected. Some brokerages have not successfully caught a rather large number of people trying this. That was a bit concerning. To FINRA, for example, which has a podcast episode about how it coordinated an industry-wide fact finding process to issue a pair of Reg Notices to let the industry know about this new Wild West of criminality and how to deal with it. Now, the most sophisticated and competent brokerages already had large security teams working on this problem. But again, some brokerages aren’t nearly as large and well-resourced as a non-specialist might suspect. Also, how to say this delicately: competence is unevenly distributed in the world. Sometimes this is wonderful; you can pick diamonds in the rough out on the Internet, who have no institutional backing but nonetheless achieve incredible results in deep areas of human endeavor. And sometimes the odd spike is in the other direction: a regulated institution has an important function headed up by a well-credentialed, impeccably pedigreed, speaks-at-conferences, well-liked-by-colleagues-and-friends individual who capitalism should not want in the chair they currently occupy. A digression: It is considered very impolite in the U.S. professional managerial class to observe that a particular, named professional manager is incompetent at their job. An individual who makes a habit of it will be optimized out of decisionmaking processes featuring PMC members, which is… all decisionmaking processes, effectively. That deviant is ipso facto disruptive to orderly operations and also a bit of a career risk to be in the same room with. And so, even if you know someone to be incompetent, part of being an effective PMC class member in an executive position is to learn the approved euphemisms and rituals. Anyhow, FINRA issued Reg Notices after a drawn out and somewhat ponderous process, for institutional reasons. They contain some mitigation recommendations that rhyme with “If a customer signs up for an account with you and doesn’t know where their brokerage account currently is, and sequentially asks you to transfer accounts at each of the top 10 brokerages in the U.S., perhaps you might want to look into that.” When you phrase it like that, it might sound obvious. But for Seeing Like A Bank reasons, the actual screen in front of the actual operations professional who is actually making a the-shot-clock-is-ticking decision on John’s accounts might not display that “John” has recently made four ACATS requests that were each rejected for non-existence. One objective of the Reg Notices is activating a ponderous machine that will eventually get a technologist deep in the bowels in the least sexy part of a brokerage to fix that screen. Should I be terrified, Patrick? This is all normal and working as designed! Capitalism will function on Monday pretty much like it did on Friday! Your assets are safe in an eventually consistent sort of way; your brokerage will eventually come around to agreeing with your view on the matter, regardless of what their first communication says. If you get mugged in San Francisco, society expresses sympathy, kinda, but you are never going to see your wallet again. Finance. Does. Not. Work. This. Way. If your brokerage makes a mistake with your assets, and they have before and will again make many mistakes, then they will make you whole. Financial institutions have capital for a reason. There is a budget for operating losses. There is a budget for fraud losses. The aggregate expenditure of effort by society in solving this problem greatly exceeds the aggregate expenditure of effort by society in solving muggings. If your balance suddenly goes to zero in a surprising fashion, that will be very stressful for you but they are eventually good for it, with very high probability. Some people hire a lawyer to resolve this and it’s just about the easiest letter for a lawyer to write: Here’s my best understanding of what my client owns. You think they own nothing. Fix this immediately or tell me in writing why you have decided not to. Lawsuits subsequent to fraudulent transfers and the brokerage deciding that, on reflection, no, they did the right thing are extremely uncommon, both in absolute numbers and as a percentage of all fraudulent transfers. But the nuclear option exists for those very, very, very few customers who need it to compel action. Should we be satisfied with this? Probably not at the current margin. Many people who own, and depend upon, assets are not competent enough to project manage the resolution pathway here, and may (largely wrongly) assume that they have been stolen from in a durable fashion. Some might come to this (mistaken) point of view because they talked to a front line customer service representative of the brokerage who, and this is aggravating but it will happen at least once today even in a regulated institution, just makes shit up rather than reading the Emergency Escalations list printed in their cubicle. Some might come to this (mistaken) point of view because their brokerage of choice is other-than-competent at answering utterly routine inquiries and instead they get their information about capitalism from the first person who replies on Reddit, who is not necessarily the custodian of Reddit’s best answer to the question. Another fun wonky control Brokerages control many accounts worth $20,000 and some accounts worth millions or much more. Frequently, the formal text of the rules will treat those accounts equivalently. Go read the rule if you have any doubt; there is no This User Is Rich exception anywhere in it. Three business days, FINRA doesn’t care. One (optional!) control that some institutions use is called a “medallion guarantee”, and it’s a fascinating combination of a physical artifact and a contractual risk transfer. The receiving institution, who may be ultimately liable (to an action from the transferring institution, to recover the assets they already re-bought for the customer out of their risk budget) for a fraudulent transfer, can optionally require a customer to get a “medallion” issued to move the risk to another institution. Hilariously, that institution can in principle be totally uninvolved. What is a medallion? A piece of paper that has a number on it and represents a promise. In brief form, that promise is “I, a financial institution who is absolutely good for this guarantee, warrant that I know this to be John. The paper attached to this medallion is authorized by John; he told me so. And if I was wrong, and I am not wrong, I will no-muss no-fuss reimburse you up to $_______.” So John, when he tells a new company that he would like to move in about $1 million, might get asked to go get a $1 million medallion. You might think this rhymes with notary services and it rhymes with insurance. All institutions involved will claim it is absolutely not notarization (a state function delegated to private individuals, who are almost universally not good for a million dollars if they screw up) and it is absolutely not insurance (a regulated industry). Also, medallions are generally free. That surprises people, particularly people who model them as specialized insurance contracts. The thresholds at which institutions request a medallion vary based on their own policies, but you might reasonably expect $500,000 or $1 million to be important thresholds. If you have an account with a million dollars in it, anywhere, your bank very probably loves you and wants you to be happy. Want a coffee? Stop by any time, they will happily give you a coffee. Charge for the coffee? Laughable. Oh you need an admissible proof of identity for a very wonky financial industry operations issue? Happy to oblige, sir, we are here for any of your diverse financial needs. Can I get you a coffee while you wait. Yes, the bank is taking risk when issuing a medallion. But it’s a tiny, tiny, tiny risk from their perspective, which insulates the receiving company from a huge risk. The bank has many years of history over which they’ve become thoroughly convinced that John is John. The receiving institution has somebody claiming to be John who spent six minutes filling out an onboarding form in a mobile app. And so the largest firms in capitalism somewhere have a spreadsheet for how much they spent on medallions, much like they can (with difficulty) come up with a pretty exact number for how much they spent on toilet paper. Toilet paper is substantially more expensive in aggregate even though no individual square of toilet paper has ever caused a $1 million wire. And, thus, medallions. Most Americans will never see one in their lives. The typical mass affluent user is most likely to see one precisely once, right around retirement age, when e.g. moving their 401k to a new custodian. But if you’re reading Bits about Money, you are much more likely to get asked for this quaint ritual than the population is at large, and now you know why. And perhaps you won’t be as frustrated as the typical person asked for a medallion, who fumes “Why do I have to walk into a bank just to get them to write ‘Yeah that’s John’ on a piece of paper? Everyone knows I’m John. My drivers license says I’m John. I already gave that to the brokerage. I swear, the entire financial industry is staffed by incompetents.” A final ACATS story Once upon a time there was a financial technologist. He made it his routine practice to buy just a few shares of every bank he worked with. This was not to make money, it was so that he could write a letter to Investor Relations if there was ever an issue he needed to escalate out of Customer Service purgatory. Investor Relations is highly placed in the org chart of banks and does not relish telling Investors they Relate to that their princess is in another castle. Some time later, that customer caused another financial institution to ACATS out some assets, including the shares of that bank. Unfortunately, that bank had in the interim had a spot of trouble, and their stock had ended up on a \"penny stock\" list. Many large, competent financial institutions have a rule about penny stocks, and it rounds to \"absolutely not.\" And so the financial institution objected to its customer, claiming that it could not process the ACATS request, because it contained a trivial amount of equity in a bank. In a bit of potent irony, the objecting financial institution owned the bank it objected to holding equity in. Sometimes, the behavior of a financial institution in the moment looks insane. Often, if you play back history, the insanity is explicable as emerging from individually reasonable actions by several separate parties with only a partial view of the facts. And, of course, playing history forward, this was trivially resolved. Just another day at the office. The business of wallets → Want more essays in your inbox? I write about the intersection of tech and finance, approximately biweekly. It's free. Get a biweekly email Great! Check your inbox (or spam folder) for an email containing a magic link. Sorry, something went wrong. Please try again.",
    "commentLink": "https://news.ycombinator.com/item?id=40470134",
    "commentBody": "Guys what is wrong with ACATS (bitsaboutmoney.com)110 points by martinky24 22 hours agohidepastfavorite59 comments CaliforniaKarl 16 hours agoFun fact about the Medallion Signature Guarantee: It's not a seal or embossment, like it often is for a notarization. Instead, it's a stamp (like what you see on [0]). It's a big stamp, but it's just a stamp. That's OK, though, because the person making the stamp is keeping a lot of records, so they'll know what they stamped. When I first got my Treasury Direct[1] account, I couldn't verify my identity using their online process, and so I had to mail in FS Form 5444[1]. My local credit union had \"Medallion Service\" as one of the reasons for booking an appointment (separate from \"Notary\"), and they were able to take care of it. As mentioned in the article, the person at the credit union asked me for a monetary value. As it was a new Treasury Direct account, I said \"$10,000\" (the maximum amount of Series I bonds you can get per calendar year). And as implied in the article, my credit union had no problems issuing this, even though they were not involved in Treasury Direct in any way. It's interesting, all the 'little' 'home-grown' ways of identity guarantee that pop up when needed. [0]: https://www.bankofamerica.com/signature-services/medallion-s... [1]: https://www.treasurydirect.gov/forms/acctauth.pdf reply bostik 12 hours agoparentI'm not in the US, but can fully appreciate the practice. If I were to request to move funds in 6-to-7 figures range across institutions, I absolutely would want the receiving outfit require that I visit my current outfit in person first to authorise such a transfer. Making sure that such visits are smooth and efficient is good service. On a related note, when I got my mortgage in the UK, the issuing bank/lender-arm required me and my wife each to get a Proof Of Being Alive. (I'm not a UK native.) Turns out embassies issue these quite routinely, for a modest fee, and a visit that lasts at most 15 minutes. At the time I found that requirement both confusing and irritating, but in the context of the article, it makes sense. The lender was about to issue a sufficiently large loan to a person with very limited[ß] UK transaction history. On the upside, more recently that same bank/lender prodded and arranged me to get an early, admittedly pretty good fixed deal on my remortgage - two months before the central banks started to hike their rates. Without that early intervention my current rate would be at least three percentage points higher. ß: from the viewpoint of a financial institution, who likely measures customer relationship ages in decades. reply CaliforniaKarl 10 hours agorootparent> I absolutely would want the receiving outfit require that I visit my current outfit in person first to authorise such a transfer. So, about that… The Medallion Signature Guarantee is often obtained from an entity not involved in the transfer. From [0] on my post: > We do not provide medallion signature guarantees for assets you're transferring out of a Bank of America or Merrill account (guarantees are typically provided by a third party not involved in the transfer or by the firm receiving the assets). In a way, that's a good thing: By the time you've initiated this transfer, the receiving entity should have done their KYC. And the losing entity probably would not want to take the cost of the liability. Having a third party—one who has had you as a customer for some time—is a good layer of security. reply mananaysiempre 6 hours agorootparentIt’s also not unreasonable to expect that an entity you want to take assets out of will, to the first order of approximatiom, be incentivized to make any process involved in that as protracted and difficult to navigate as possible. (This is also, I expect, why the asset transfer process described in TFA is initiated from the receiving side, why mobile number portability is requested via your new mobile network, etc.) So if the recipient doesn’t want to take on the verification burden because they just met you, falling back to a third party rather than the sender makes sense. reply mizzao 3 hours agoparentprevWeirdly, on [0] it says the following: \"We do not provide medallion signature guarantees for assets you're transferring out of a Bank of America or Merrill account (guarantees are typically provided by a third party not involved in the transfer or by the firm receiving the assets).\" This seems to differ from the article and seems to be a way to make it harder to withdraw assets (why would a third party provide a medallion to insure someone else's assets?) reply FeepingCreature 3 hours agorootparentThis is already noted and explained in the article. Search for \"totally uninvolved\". reply thaumasiotes 4 hours agoparentprev> Fun fact about the Medallion Signature Guarantee: It's not a seal or embossment, like it often is for a notarization. Instead, it's a stamp (like what you see on [0]). It's a big stamp, but it's just a stamp. As far as I'm aware, a \"seal\" and a \"stamp\" are the same thing. In particular, a seal is just a device that impresses ink onto paper. How is that supposed to be different from a stamp? https://www.japanlivingguide.com/living-in-japan/culture/han... https://en.wikipedia.org/wiki/Heirloom_Seal_of_the_Realm (Depending on the documentary technology of the culture, a seal may also be used to make a physical impression in a soft substance, but obviously that is still no different from a \"stamp\".) reply jameshart 4 hours agorootparentSeal and stamp can both mean several things, but both can be applied to the process of making an ink impression on a document. Seals also can be impressions left in wax or embossed into a document itself. Stamps can also be separate paper pasted onto a document (eg a postage stamp, a visa stamp) The terminology is fuzzy. reply oasisbob 1 hour agorootparentSeals can also be used to bind multiple documents together, or to permanently keep a single document intact. I worked for a bit with a non-profit which was involved in international adoptions. Seeing documents under apostille with elaborate seals wasn't uncommon. eg, a multi-page document attached to a government certificate of authenticity, bound together by a ribbon passed through a drilled hole, with the hole filled with wax. reply thaumasiotes 4 hours agorootparentprevThis isn't really compatible with the claim above that \"it's not a seal\". reply jameshart 2 hours agorootparentRight. I was agreeing and adding context to your post, not trying to refute it. reply mattofak 2 hours agoprevI've had to get a medallion before to move a small retirement account and there were two things I very much did not like about the process: (1) Most institutions require you to have been a member for 6 months before they will grant a medallion. I was transferring money between Fidelity and Schwab. Neither of which have branch offices within an hour of where I live. And I only have a virtual bank. There was no one willing to give me a medallion because I have no local banking relationships. I had to pull a favour with work to get our controller to vouch for me at our business bank. (2) I've done domestic and international wires for much more money without needing anything like a medallion. Why can't I do something like log in to Fidelity and wire the funds across directly? (Also, have fun finding the forms required to transfer your retirement account. Fidelity at least made the process exceptionally difficult, and their customer service agents acted like they didn't know what a Simple IRA was or why one would want to transfer out. Sure there's a page about transferring, but it likes to loop you into the \"transfer funds into\" flow.) reply e63f67dd-065b 19 hours agoprevI never knew about the 3-day limit before, it makes sense that this is the reason behind unverified ACATS transfers (a problem that I've spent no small amount of time arguing about on various online forums long ago). Interestingly, Fidelity lets you put your account into \"lockdown\" mode there they'll reject all ACATS (and ACH too I believe) transfers automatically without your intervention. I've always wondered what kind of legal magic they're pulling to make this compliant with FINRA rules. What eventually convinced me over that it's not a problem is, as Patrick says, that these institutions will with high likelihood make you whole for fraudulent transfers, the same way they will for fraudulent ACH. The financial system generally works. reply vineyardmike 19 hours agoparent> these institutions will with high likelihood make you whole for fraudulent transfers, the same way they will for fraudulent ACH. The financial system generally works. This applies to so much of the financial system, that it baffles me people worry about the things they do. I have family members that are so scared of card skimming they only pay in cash, and friends who are still too scared of the internet to buy things online. reply cesarb 18 hours agorootparent> This applies to so much of the financial system, that it baffles me people worry about the things they do. That's easy to understand. For the \"financial system\", $1000 is nothing. For many people, $1000 means the difference between having something to eat or not. Even if you get your money back in the end, not having any money for several days can be a disaster; and the 1% chance that you might not get that back can be too much risk to bear. > I have family members that are so scared of card skimming they only pay in cash That's understandable. With cash, all you can lose is the cash you have in hand at that moment; if your card is skimmed and used without your permission, you can lose up to the card's limit (and perhaps even more, if the limit isn't strictly enforced). You might say that fraudulent transactions will be reversed, but there's always that 1% chance that the other side will somehow manage to convince your bank that the transactions were in fact legitimate. > and friends who are still too scared of the internet to buy things online. When you buy at a store, the goods you bought are in your physical possession at the moment of the payment. When you buy things online, the goods you bought show up days or even weeks later, if they do, and until they arrive, you have no idea whether they're actually what you bought (and that's on top of the \"card skimming\" risk). Sure, if you have enough resources, you can tolerate losing $1000 of the limit of one of your credit cards for a few days until the charge is reversed (and even, in the worst case, absorb the loss in case that charge isn't reversed), but not everyone can do that. And the perceived risk can be higher than the unknowable real risk; someone who has heard about (or experienced) enough incidents in which money was lost (temporarily or not) will understandably be more reluctant to expose themselves to these risks. reply acheong08 15 hours agorootparentprevI once had to starve for 2 days because my bank froze my card over the weekend and being in a new country, had no cash with me (the nearest exchange was too far away by foot) Nowadays I pretty much have the bare minimum in the bank and just spend cash. reply skybrian 13 hours agorootparentHaving multiple, independent payment methods, of any sort, seems like the way to go. reply miki123211 8 hours agorootparentprevFor a lot of people, not having access to anything they currently have in their bank account, which is most of their savings, is actually a quite big deal, even if they can get that money back eventually. reply chiph 5 hours agorootparentprevI regard this as dealing with differing amounts of annoyance. Fraudulent credit card charge? Pffft - it will get taken care of with maybe two phone calls. Fraudulent debit card charge that leaves me with $8.31 in the bank? Getting up there on the annoyance scale. I have cash to live on for a few days, but it will likely require several phone calls and some outstanding bill-pays may bounce[0]. Someone buying a car with my stolen identity? Hugely annoying. [0] I have a couple of monthly bills that cause a physical check to be printed and mailed. And one of the recipients only checks their mail twice a week so the float time is significant. I deal with them because I must. reply preinheimer 18 hours agoprevSo many parts of the financial system only work because there’s take-backsies. Taking back a wire transfer seems to basically involve someone at your bank phoning a friend, and it all works in the end. reply psychlops 18 hours agoparentThe more one learns about how traditional finance works, the more horrifying it becomes. reply supertrope 18 hours agorootparentOn a technical basis yes. Finance rests upon a foundation of people emailing around spreadsheets and trust. Much better than new innovative cryptocurrencies that are just a trojan, wallet address typo, or exit scam away from losing your savings. reply quesera 16 hours agorootparentFWIW, \"wallet address typo\" should never be a thing. There's a serious checksum in each of the BTC wallet formats that makes the chances of any random set of typos extremely unlikely to be valid. A mis-pasted (but valid) wallet address could happen. Not to minimize the myriad other ways to easily lose, or lose control of, cryptocurrency! reply lxgr 15 hours agorootparentBitcoin does, but Ethereum doesn’t, at least not originally. There is some way to encode a checksum in the capitalization of the letters/digits A-F, but I’m not sure how ubiquitously supported that is. It seems like a pretty bad hack in any case. reply quesera 6 hours agorootparentFascinating, I am unaware of the inner workings of Ethereum. It boggles that they could make such a massive design error, especially with the Bitcoin examples staring them in the face. reply lxgr 50 minutes agorootparentI believe the idea was that “nobody will use raw low level addresses, so we don’t need to make them usable”. But ENS is now prohibitively expensive in terms of fees and also not universally supported, so here we are. reply gopher_space 13 hours agorootparentprevOne of my early lessons was realizing the spreadsheet folks have their own backup systems and they're fairly ingenious and orthogonal to my own concerns. reply psychlops 18 hours agorootparentprevAs trusting as the traditional finance system is, it seems they still experience exit scams, wire typos, and countless other scams that lose savings. Crypto has a long way to go, but tradfi hasn't solved its own problems. Until then, I guess we'd all better hope our bank representatives have a lot of friends in the industry. reply astrange 18 hours agorootparentCrypto can't solve its main problem though; it's fundamentally based around not being able to reverse transactions, which means you can't reverse a mistaken or fraudulent transaction, which is the whole reason tradfi works. And I don't think the other stuff makes up for this, even if programmability and flash loans are interesting. (I mean interesting mostly in a bad way.) reply miki123211 8 hours agorootparentThere's nothing in crypto that inherently prevents transaction reversals. This can be implemented as a smart contract on any sufficiently advanced blockchain (definitely Ethereum, not sure about Bitcoin). The way this works is that you put the money in an escrow contract first, designating a trusted third party as an arbiter. If you don't dispute the transaction for a few weeks, the money goes through irreversibly. If you do, the contract \"locks up\", and the third party can now either decide to send the money on to its destination or return it back to you. The contract is designed in such a way that those are the only things that third party can do with your money, they have no way of taking it for themselves. Either way, they get a small cut for the trouble of adjudicating the transaction. If what you're worried about is wallet hacks, not disingenuous merchants, that's doable too. Instead of storing your money directly in your wallet, you'd have to store it in a smart contract, designed in such a way that your key is the only one that can move money out of it. However, limitations would be placed on the contract with regards to how much money can be moved daily. Such limits would likely be different for transfers to trusted escrow contracts and for irreversible transactions. If you needed to move more money, you could designate a trusted third party (let's say a traditional bank doing traditional bank things for identity verification) as being able to override these limits for you. Again, that would be their only responsibility, they would not have the right to move any of your money without your permission. This essentially relegates a bank to the role of an identity verifier who cannot take control of your money in any possible way except by colluding with a hacker who already has access to your wallet. THe reason none of this exists is ecosystems and network effects, not technical limitations. reply karpierz 3 hours agorootparentBut then I can't spend my money for several weeks - the feature that crypto can't implement is having the reversal window be wider than the lockout window. reply jasonhansel 4 hours agorootparentprevThat just shifts the problem around: if there's a bug or mistake in the smart contract itself, then you face the problem that you can't reverse that smart contract. reply waveBidder 3 hours agorootparentprevCongratulations, you've just invented a bank. reply psychlops 6 hours agorootparentprevOr, just like tradfi, point out the mistake to the counterparty. Request your crypto back and the transaction is reversed. reply miki123211 4 hours agorootparentIn tradfi, the counterparty is typically a sophisticated financial institution that either is in your jurisdiction or at least cares somewhat about not annoying your government. If it isn't, it's probably a customer of said financial institution, and thus subject to their rules. In crypto, your counterparty might be literally anybody. reply psychlops 6 hours agorootparentprevOf the many crypto problems I can think of, reversible transactions seems low on the list. Tradfi \"works\" because there is no other choice given so people make it functional and it is comfortable through familiarity. People conveniently forget the rampant scams, identity theft, and numerous types of fraud that are clearly irreversible in tradfi. Then they present it as a new problem that crypto must solve as well as being the same as the old system. reply lxgr 15 hours agorootparentprevWhat’s horrifying about being able to quickly fix a honest mistake in mutual agreement? It’s not always possible, but when it is, it’s so much more efficient than the alternatives. Trust isn’t a given and not always warranted, but when it is, it is a feature, not a bug. reply NoMoreNicksLeft 16 hours agorootparentprevI was going to make a dumb joke about how this was the plot to a horror story or movie or something. I paused for a few seconds (that's discouraged around here, so it better be a good joke)... and it started to sink in that it could well be the plot not only to a horror movie, but maybe has the potential to be a compelling one. \"You'll be made whole, eventually\" sounds great, until you think back to all those times in your life where \"in a few weeks\" would've ruined you. Even death's not out of the question if it pushes someone over the edge into suicide territory. The lunatics really do run the fucking asylum. reply taneq 8 hours agorootparentprevSo much of the world essentially runs on trust, it’s amazing that it works at all, let alone as well as it does. reply icedchai 16 hours agoprevI had to get a medallion stamp to do an ACATS transfer from broker A to B. I chose to get it at bank C, where I had my checking account, which also happened to own broker D. Of course, the medallions were handled by a sales guy (sorry, financial advisor) working for broker D who then spent an hour trying to convince me I should move my account to them instead. Eventually he finally gave in and sent off the paperwork. It's annoying the hoops you have to go through to get control of your own money. reply CaliforniaKarl 16 hours agoparentReminds me of when I wanted to close my Bank of America account: I didn't know that you needed to make an appointment for that. But indeed, it's my money, I had all the ID-verification paperwork needed, and I was stubborn enough to agree to wait for a clerk to become available. For what it's worth, you may wish to consider a credit union, as it's less likely that a credit union would also own a brokerage. reply daft_pink 15 hours agoprevThe other side of this is that I’ve absolutely had a SEC advisor tell me that I couldn’t access my money unless I kept my account with him and the bank had to verify my access to the money if I wanted to transfer it, but if I stayed I could keep it. ACATS ultimately protects the consumer from keeping their money with unscrupulous advisors. I agree with the author that it is totally terrifying, but I feel the assets should be kept with a third party or third party verification service so that the broker isn’t incentivices to “verity” details. I think medallions are a good solution to this problem and like the author of the piece says they are shockingly easy to get. I think that the advisors themselves receiving the money generally have huge relationships with the companies they work with that issue medallions and even the third party companies with relationships with the receiving advisor will generally provide one for free even if the person doesn’t have one. reply niederman 13 hours agoprev> Infohazard warning about which I am being absolutely serious Small nit: since this song isn't a fact or idea per se, it would probably be more accurate to call this a cognitohazard. reply piinbinary 3 hours agoprevIt's somewhat amusing to look back on my Computer Science education, where they taught us that database transactions are used to ensure bank balances are moved atomically between accounts reply elteto 2 hours agoparentI’m sure this is true at some level. Just not _all_ levels. At some point you just can’t connect to the db on the other side because it belongs to a different institution. reply CaliforniaKarl 18 hours agoprevHeads-Up for folks trying to get to one of the linked pages: https://news.bloomberglaw.com/mergers-and-acquisitions/matt-... You can get to the same content here: https://www.bloomberg.com/opinion/articles/2023-02-16/the-se... Which was previously posted to HN here: https://news.ycombinator.com/item?id=34824986 reply BiteCode_dev 6 hours agoprevDamn I wish I thought about that title. reply recursive 18 hours agoprevIs it just me, or is that cat song not that good? I listened to it twice, and found it completely unremarkable. A minute later, I can't remember much about it. reply patio11 18 hours agoparentDe gustibus non est disputandum; there are some things that ruin many people's lives that have no appeal to me, there are some things many people enjoy responsibly that I have learned to stop myself from using because I will not make good decisions over a period of years, and then there was that freaking cat song, which gave me the strongest \"WARNING: Your brain is not in control of your response to this song, in a way which is qualitatively different than the usual ways music is moving.\" when listening to it that I found it remarkable. reply NobodyNada 17 hours agorootparent> I think if you say the words “my cat” to me when I am on my deathbed I will immediately hum three notes. The song is currently ruining my life in a very different sense: I keep listening to the song over and over to try to guess which three notes you're talking about, but I can't for the life of me figure it out. Mind clearing that up for me? (Personally, I definitely see how the song could be perceived as infectiously catchy, but it doesn't seem to do it for me. I think the structure and rhythm are a little too irregular for it to get stuck in my head.) reply NoMoreNicksLeft 16 hours agorootparentBack in the mid-1990s, I remember a news fluff piece about a woman who claimed to have epileptic seizures in response to hearing Mary Hart's voice (some newstainment journalist/anchor, maybe Entertainment Tonight?). Doctor supposedly confirmed it via experiment (though, now I wonder if the man should've kept his license, seems unsafe). Earworms are probably the same. Specific either to unique brains, or to particular brain neuro-templates. reply mtlynch 1 hour agorootparentI thought you were confusing this with an episode of Seinfeld, when Kramer has seizures in response to Mary Hart's voice.[1] But you're right. This was a real case that Seinfeld must have been referencing.[2] The doctor, Venkat Ramani, is still active and is the Vice Chair of Neurology at New York Medical College. [3] [1] https://en.m.wikipedia.org/wiki/The_Good_Samaritan_(Seinfeld... [2] https://web.archive.org/web/20240324001523/https://www.nytim... [3] https://www.nymc.edu/faculty/faculty-profiles/by-name/ramani... reply plorkyeran 13 hours agoparentprevIt's pretty catchy, but ultimately just a very generic pop song other than the lyrics. reply mynameishere 5 hours agoparentprevWhat is this song that he mentions but never seems to name or link to? reply recursive 2 hours agorootparentBut there is a link. reply nvy 17 hours agoparentprevI thought it was pretty catchy, actually. reply throwaway290 9 hours agoprevThat suno song is not only not an earworm, I can't recall what it even sounds like after 15 minutes... reply jessekv 2 hours agoparentTry the top-played suno track, I promise that you will remember all the lyrics for more than 15 minutes. https://suno.com/song/ee467d00-5813-4a74-9792-c9ae4a09d344 reply TwoNineFive 10 hours agoprev [–] People who start sentences with \"guys\" or \"bro\". reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Patrick McKenzie discusses the complexities of transferring brokerage accounts using the Automated Customer Assets Transfer System (ACATS) and the role of self-regulatory organizations like FINRA in overseeing these processes.",
      "FINRA Rule 11870 mandates prompt asset transfers between brokerages, supported by ACATS, which standardizes the process and involves behind-the-scenes complexities like spreadsheets and the Depository Trust Company (DTC).",
      "The text highlights issues such as the risk of fraud, the role of brokerage employees in assisting with transfers, and the impact of poor customer service on clients seeking financial advice from unreliable sources."
    ],
    "commentSummary": [
      "The Medallion Signature Guarantee is a security measure in U.S. financial transactions to verify identity and authorize large fund transfers, adding an extra layer of security.",
      "Users discuss the complexities of traditional finance and cryptocurrency systems, emphasizing the need for multiple payment methods to mitigate risks like fraudulent charges.",
      "The conversation also highlights the bureaucratic challenges of managing personal finances and the importance of third-party verification in secure transactions."
    ],
    "points": 110,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1716581903
  },
  {
    "id": 40476410,
    "title": "Why 'Worse is Better' Philosophy Makes tmux a Reliable Choice for Developers",
    "originLink": "https://hiandrewquinn.github.io/til-site/posts/tmux-is-worse-is-better/",
    "originBody": "Home » Posts tmux is worse is better May 23, 2024 tmux (short for “terminal mux” (short for “multiplexer”)) is i3 for your terminal. Oh, it’s so much more than that, and I recently discovered with some joy that it is installed by default on OpenBSD, but its fundamental value add to any programmer who has to SSH into servers more than once a week is it allows you to split your screen up into multiple independent shells without needing a graphical environment at all. If you want to walk the path of true digital minimalism, vanilla Vim and tmux or its spiritual grandfather screen are all you need. So it’s always been interesting to me that a developer as seasoned as Kovid Goyal, developer of the ePub powerhouse calibre and my current teminal emulator of choice Kitty, has gone on record a number of times saying he is not a fan of tmux. He has a whole section in his FAQ about it: [T]erminal multiplexers are a bad idea, do not use them, if at all possible. kitty contains features that do all of what tmux does, but better, with the exception of remote persistence. (I don’t use remote persistence. I’m aware I’m a caveman.) And from the linked Git comment: [Terminal m]ultiplexers add unnecessary overhead, suffer from a complexity cascade, because they actually have to translate escape codes, modifying them in hackish ways to get them to work with their concepts of windows/sessions. […] Energy/performance wise they are poison, every byte has to be parsed twice, once by the middleman and once by the terminal. And they act as a drag on the ecosystem as a whole, making it very hard to get any new features. […] Terminals [themselves] are fine, certainly better than any other interface paradigm I have ever seen. I’m not here to incur Goyal’s wrath. He knows way more about this space than I do, and I’m clearly a very satisfied user of kitty at home, where I have the ability to tweak my environment to be exactly what I want it to be. Therein lies the rub. At home, I run UNIX on my metal. At work, I use Windows. And so, the multiple times a day I find myself SSHing into one of our many, many quasi-embedded Linux boxes, I find myself typing out “tmux” as the first command I run. Because it’s always going to be there, and it does the one thing I actually need it to no matter what: Let me run multiple shells at once, without SSHing in multiple times, regardless of whatever funky terminal emulator I’m actually using to get the job done. Alacritty, Windows Terminal, whatever. kitty has to run on what I’m remoting from. tmux can work on what I’m remoting into. And that makes all the difference. Worse is better is more of a product managment philosophy than a software philosophy, IMO, but it basically underlines the idea that whatever your software’s killer app is, you want to get that really right for the most common use cases, even if it means living with a suboptimal approach. I love VMs for a lot of reasons, and I even maintain a set of 3 shell scripts which turns a vanilla Ubuntu VM into my personal software development workhorse, complete with LazyVim, ripgrep, tmux, and - yes - Kitty as the default terminal. But even with that it’s rare these days that I am doing such prolonged development work that I feel it’s worth actually working in such a VM. The perils of becoming a middle manager, alas. So, for the most part, I remote in directly from Windows. Which means I’m going to use tmux on my servers. Which is why I have C-b % and C-b \" memorized and not … checks kitty docs … wait, is it really C-S-Enter? That’s so nice. Man. We are missing out. Linux Shell Openbsd Heuristics Bsd Tmux Worse-Is-Better Next » Disable your browser history to write better internal docs",
    "commentLink": "https://news.ycombinator.com/item?id=40476410",
    "commentBody": "`tmux` is worse is better (hiandrewquinn.github.io)97 points by hiAndrewQuinn 1 hour agohidepastfavorite105 comments kelnos 1 hour agoI feel like \"parsing each keystroke twice\" is pretty low on the list of things that wastes CPU cycles and drains my laptop battery. I cannot at all get worked up over this, getting to the point I'd pull out the word \"poison\" to describe it. It's nice that Kitty does most of what tmux does built in, but as the author points out, that's not really all that helpful for anyone who does any work over ssh. And to me, bundling all that into a terminal emulator is bloat. Terminals should be terminals. If I want a multiplexer on top of that, I will... use a multiplexer on top of that. I am potentially sympathetic to the idea that tmux and screen make it harder for terminal developers to add new features to their terminals. I don't really know enough about the issues involved to form an opinion. But from the perspective of a regular user, I do not want a multiplexer in my terminal app. When I need one, I prefer to reach for an app-agnostic multiplexer like tmux or screen. reply Xeamek 0 minutes agoparent>that's not really all that helpful for anyone who does any work over ssh. If you are hoping to new machines every time then sure. But lets be honest, thats a minority. I don't know kitty in specific, but wezterm ie. can connect to target via SSH, start daemon of itself there and actually work better then tmux, because it's controlled from the client (where the gui runs), but with all the tmux upsides. And yes, it's not as portable as tmux (simply because its not as popular), but again, in majority of the situations the setup required is really not that much. Especially if you would spend that time on tmux config anyway. reply fiddlerwoaroof 11 minutes agoparentprevI like how iTerm2 can act as a UI for tmux via tmux’s control protocol. We don’t need to reinvent tmux to fix some of the UX issues of tmux sessions inside terminal tabs inside terminal windows. reply rickstanley 23 minutes agoparentprevFully agreed. I myself use 2 terminals: Alacritty and WezTerm. The former for local work, and the latter for my VMs, and occasional remote server work. I couldn't explain exactly why I use separated applications, since WezTerm works well for local (which even has a menu entry for that) and remote alike, but I'm comfortable with the workflow that I have today, and for me it's enough and what matters. reply pzmarzly 35 minutes agoparentprevFrom the perspective of \"a regular user\" (which is a bad term, because of course I see myself as the norm), I don't care if multiplexer is part of terminal app or not, but I do care about it reacting to my clicks. I don't want to learn obscure multi-step keyboard shortcuts in order to open a new tab, rename a tab, resize a pane, reorder tabs, copy the scrollback buffer, etc. Sadly, that seems to leave me with only 2 options that work well over SSH: - using a terminal with client-server model and built-in multiplexer (VSCode Remote SSH extension, WezTerm) - using iTerm2 with tmux integration (apparently nothing else implements tmux control protocol https://unix.stackexchange.com/a/280829) reply Barrin92 33 minutes agorootparentyou might want to give Zellij a try. (https://zellij.dev/) Works like tmux but with the UI friendliness of more modern apps. reply bayindirh 25 minutes agoparentprev> I am potentially sympathetic to the idea that tmux and screen make it harder for terminal developers to add new features to their terminals. Both are actively maintained and can work with modern terminals, as long as the hosting terminal is standards compliant. I don't understand why people love to break standards. reply fiddlerwoaroof 8 minutes agorootparentThe cynical view is that non-standard features make your product sticky reply colordrops 39 minutes agoparentprevAgree with all your points. The main problem with Tmux that no one talks about is its arcane configuration and interaction patterns. If you set up a ton of config while gritting your teeth, it begins to work well, but the configuration semantics are a nightmare. I would love something similar to tmux that has sane configuration and theming. I switched from XMonad to i3 for the same reason. reply usrbinbash 26 minutes agorootparentSure, but how often do you need to touch that config? Since I keep my configs in a repo, I just checked the last time I changed my `.tmux.conf`. Turns out it was 2018, and I changed a grand total of 2 lines. And I use tmux daily, not just remote, but also on my local machine. So yeah, if I am using something for more than half a decade without requiring any change, then its pretty safe to say that whatever hassles the configuration of that something imposes, is completely irrelevant. reply sigil 9 minutes agorootparent> Sure, but how often do you need to touch that config? More often than I’d like! tmux config has broken backwards compatibility on me multiple times over the years. This is fine for most software — you upgrade your config once and you’re done. However, the nature of tmux is that I use it on many servers, some with tmux 1.x and some with tmux 2.x. Getting a ~/.tmux.conf from my dotfiles repo that works across both has been papercutty. (Love tmux though & can’t imagine tty life without it!) reply kergonath 14 minutes agorootparentprev> Sure, but how often do you need to touch that config? It’s not only the config, though. It’s the arcane shortcuts and the weird way it seems to handle most things (trying to get the mouse working is a bloody nightmare and it regularly borks the terminal). I have a mouse with a scrolling wheel, for example. It works fine with most programs over ssh, why is it such a pain with tmux? Such pains remain even after having felt down the rabbit hole and spent 2 weeks tweaking settings. iTerm2 is fantastic and I would be very happy to just use it and call the problem solved, but I don’t have a Mac at work. reply j2kun 1 hour agoprev> remote persistence This is the real reason I actually need tmux. I often lose connection to a server for various reasons and don't want to lose my vim session or whatever reply bayindirh 27 minutes agoparentIt's not only a safety net for losing connection. As soon as I connect to a server to do work, I automatically fire screen, and create new terminals and rename them as I work with the system. If my work is unfinished, I leave the screen as is, and disconnect. We don't touch each others' screen instances on servers, so it's a safe workspace which I can return whenever I want. reply adtac 1 hour agoparentprevtmux over mosh over wireguard is all you need reply e40 53 minutes agoparentprevI can’t live without X11 emacs, so I use Starnet’s FastX, which allows me to disconnect and reconnect either when my connection drops or when I move between office and home. I absolutely couldn’t live without it. reply bradfox2 44 minutes agoparentprevIs there anything better than tmux for this purpose? I need persistence but hate the key bindings and general interface. reply vaylian 29 minutes agorootparentSounds like you might like Zellij: https://zellij.dev/ reply schainks 37 minutes agorootparentprevByobu is equivalent, different hotkeys: https://www.byobu.org/ I have moved between tmux, byobu, and screen at various times in my career. I wouldn’t say one is better than the other, as they all do the job I need just fine (eg persistent session state on remote machines) reply alex_smart 24 minutes agorootparentprevOn mac, iterm2 has excellent tmux integration. You start tmux in control mode (tmux -CC) and it opens a new window for that session with all the multiplexing handled by the terminal instead. You get to use all of your normal terminal keyboard shortcuts and terminal interface. It is the best of both worlds. And, yes, of course it works with a remote ssh session. reply timdev2 40 minutes agorootparentprevMy use-cases are pretty basic, and my use isn't all that frequent, but I've been very happy with: https://zellij.dev/ reply pwg 23 minutes agorootparentprevDoes tmux allow redefining the key bindings? GNU screen allows for redefining (at least) its hotkey. If tmux allows the same maybe you could redo the bindings more to your liking? reply dahart 11 minutes agorootparentYes you can define your own key bindings. The default is the same “prefix key” that ‘screen’ uses - Ctrl-b. Personally I like to use Ctrl-j because it’s easier to type and has fewer conflicts with shells and other things. Most of the key bindings that follow the prefix key seem natural to me, stuff like ‘c’ to create a new tab, ‘d’ to detach from tmux, ‘n’ to go to the next tab, etc., but all of that can be customized. reply qup 15 minutes agorootparentprevyes, it does. I love tmux and hate the default hotkey reply mananaysiempre 37 minutes agorootparentprevDtach[1] does session persistence and nothing else—it leaves refreshing the screen to the program running inside it (it can send a ^L or a SIGWINCH upon reattaching but that’s it). [1] https://dtach.sourceforge.net/ reply yjftsjthsd-h 37 minutes agorootparentprevDepending on exactly what you mean by persistence, there's always mosh. reply johnchristopher 39 minutes agorootparentprevThere's still `screen` ? Also, I think you rebind tmux keys ? reply shadowgovt 37 minutes agorootparentprevI just use emacs. When my terminal drops, I just `emacsclient -t` and I'm back to work. reply pessimizer 1 hour agoparentprevI've literally had projects running from screen sessions for years. I always assumed the purpose of them was so I could turn off my computer while doing something over the network that was either long-lived, on an unstable connection, or a daemon/server of some sort. reply nephanth 36 minutes agoprevImo, the killer feature of tmux is that not only does it persist your shell, it persists your workspace. If I ssh into my server, i'm going to find my whole session as i left it, with the same tabbing / panes. This is a need kitty+screen doesn't answer. If I close kitty & reopen it, i'll have to re-split every individual pane, and reconnect each of them and reattach them to the right persistent screen pts... Infact if there was a way to handle that \"workspace-persistence\" with terminal emulator-based workspaces, I'd ditch tmux in a minute EDIT: now that i think about it, this could maybe be achieved by keeping the terminal multiplexer's \"server\", but having the terminal emulator act as a client to it reply bee_rider 44 minutes agoprev> Kovid Goyal, developer of the ePub powerhouse calibre and my current teminal emulator of choice Kitty, has gone on record a number of times saying he is not a fan of tmux. He has a whole section in his FAQ about it: >> [T]erminal multiplexers are a bad idea, do not use them, if at all possible. kitty contains features that do all of what tmux does, but better, with the exception of remote persistence. Bit funny, most of the time I’m running tmux in kitty (or ssh in kitty to a server with tmux). Remote persistence is a pretty massive deal, though. In general, I don’t like getting attached to a terminal emulator, and mostly just try to avoid using any unique features they’ve got. If a terminal emulator and a command line program are competing for doing a task, the terminal emulator in some sense has an unfairly high hurdle to pass: if I become dependent on the terminal emulator to do it, then I can’t replace the terminal emulator without also getting a new muscle memory for that task, if I ever want to switch terminal emulators. Which means it has to do that task so much better than the command line program that the delta is worth more than any possible improvements from switching terminal emulators. Kitty is a great terminal emulator but this is an absurd and basically unfair bar that it could never pass. In this case, though, it is actually worse than tmux because it can’t offer the best thing terminal multiplexers do: remote persistence, which is the killer feature. reply omoikane 22 minutes agoparentkitty seems to be a relatively more opinionated terminal compared to other terminals. The first time I heard about kitty was via a comment about not implementing Sixel support: https://github.com/kovidgoyal/kitty/issues/2511#issuecomment... \"Terminal multiplexers are a bad idea\" sounds just like Sixel being \"an inferior graphics protocol\", and Sixel is probably another example of \"worse is better\". reply bee_rider 3 minutes agorootparentI have trouble laying that at the feet of Goyal, who was not really interested in the feature, but who was willing to provide feedback and appeared willing to accept a patch. The person working on the patch decided that they didn’t think the sixel library (which they were the maintainer of, so they presumably had a well informed opinion) was up to the task. The description of sixels as inferior also came from the person working on the patch. reply neilv 1 hour agoprevBasic `screen` or `tmux` awareness (how to start it, how to create and switch between windows, how to reattach if disconnected) should be in the toolkit of anyone who finds themself needing to SSH into a server. Especially in startups, where you can't afford to be sitting atop massive too-big-to-fail enterprise bureaucracy, and need to understand the systems, and work creatively and efficiently with them. reply SoftTalker 1 hour agoparentYes very much. You don't need to be an expert. Just knowing how to detach from a running tmux and reattach later, and knowing how to create and navigate between tmux windows and panes is most of what you need to know. reply noloblo 1 hour agoparentprevi use neovim and screen as defaults and find them quite useful, but never found tmux useful, please change my mind tmux what are good starter tutorials? reply lelanthran 3 minutes agorootparent> i use neovim and screen as defaults and find them quite useful, but never found tmux useful Thank heavens there's someone else - I thought I was the only one[1]. [1] I use screen, and then use vim with multiple splits and tabs, with each buffer being a `:terminal`. Works very for me, because I get all the terminals I want, splitted and tabbed in any fashion I want, using my existing vim muscle memory to switch between them, minimise, maximise, etc. Also, takes about 1s to save the session if I ever feel the need to exit vim (I never do, though). reply SoftTalker 1 hour agorootparentprevscreen and tmux are similar. Unless you need some of the more obscure features, they mostly do the same things. Screen can attach to a serial port, not sure tmux can (but you can use 'cu' or another utility for that in a tmux window). reply devsda 37 minutes agorootparentprevIn screen, splits and windows behave more like vim buffers and splits. That works nice for an editor because I might want to look at the same file at two different locations. I find it weird to apply that concept to shells. Tmux panes and windows behave more like separate shells. It's similar to what we get with individual shell gui windows that are neatly stacked and makes it easy to follow. I don't know if there is an equivalent in screen, but I like and heavily use the zoom feature which can temporarily minimize other splits and focus on single shell. reply kzrdude 29 minutes agorootparentprevTmux does the same job as screen. I didn't want to before, but now in the current year I switched from gnu screen to tmux, keeping the key bindings the same. Ending a 15 year or so streak. The reason is that tmux supports more modern features like hyperlinks and truecolor, in the distro versions. Once settled, I don't notice a difference. reply mh-cx 42 minutes agorootparentprevAs tmux can be extended with plugins I'd recommend to check through this list and see if you find something useful: https://github.com/tmux-plugins/list As a neovim user I can also very much recommend this vim plugin to seamlessly navigate between tmux panes and neovim windows: https://github.com/christoomey/vim-tmux-navigator reply CoastalCoder 25 minutes agorootparentprevIIRC, iTerm2 has special support for tmux. E.g., tmux panes get corresponding iTerm2 panes? reply Graziano_M 53 minutes agorootparentprevTmux lets you make a pane toggle full screen. That’s reason enough for me. reply jrockway 1 hour agoprevI agree with all of this completely. I also run Windows as my host OS and do all my work in Linux. I don't use X, I ssh into the Linux VM and have Emacs alongside a bunch of shells running in tmux. It is very productive and even when I have a physical Linux machine, I just start up X, maximize a terminal, and use tmux. The author of Kitty is also right about performance. I've never looked into exactly where the problem is, but tmuxbut its fundamental value add to any programmer who has to SSH into servers more than once a week is it allows you to split your screen up into multiple independent shells without needing a graphical environment at all. I actually think that value add for SSH is that people is the ability to allow long-running jobs to survive a disconnection. It can be frustrating doing something that takes multiple hours (like a large wget download) just to have your connection break about ten minutes before it's done. Even if you never use any of the other features you immediately benefit from that. reply ehsankia 54 minutes agoparentIt just fits my mental model much better. It's close to a \"remote desktop\" where you connect to an existing \"screen\" and continue where you left off, vs each time you connect, a brand new \"screen\" being created, and it getting destroyed as soon as you disconnect. The fact that the shell dies as soon as you disconnect is far more problematic than just long running jobs. reply nephanth 47 minutes agorootparentExactly. One of my favorite things about working in tmux is that my editor session persists, whatever state my laptop is in. Out of battery? Rebooted after updating the kernel? My workspace stays the same reply tombert 46 minutes agorootparentprevYeah, I agree with that. I use tmux for pretty much everything, and I do use the splits and multiple screens when I do. In fact I do use it even when I'm working locally because I like the keyboard shortcuts and mouse support (and why I've never really cared if my text editor has a terminal built in). reply yaur 44 minutes agoparentprevDoesn’t ^z and bg solve this? reply nephanth 4 minutes agorootparentYou'd also need to disown it if you want it to persist sessions If you just bg it, it will still die when your session disconnects reply tombert 41 minutes agorootparentprevYep, if you remember to do that. What's nice about tmux is that this happens automatically. It requires zero additional mental forethought outside of typing tmux exactly once, and then all your jobs are safe, and you can still see them in the foreground. reply akira2501 5 minutes agorootparentprevSIGHUP reply duskwuff 39 minutes agorootparentprevNo, it doesn't. There's no way to reattach to a process which was backgrounded and detached in another terminal -- once the parent terminal is gone, the process is stuck in the background. Most interactive processes will terminate immediately when this happens. reply adw 42 minutes agorootparentprevNot really - often what I’m doing is running something with a conditional breakpoint which will only be hit a long way into process lifetime (potentially nondeterministically), so I really do need “a session which will persist indefinitely including across SSH sessions”. reply amouat 1 hour agoprevJust wanted to say that I appreciate how well written this is. It does make a difference when an author can clearly and succinctly state their case. reply hiAndrewQuinn 1 hour agoparentThanks! reply thih9 6 minutes agoprevWhat I like about tmux is that it in itself runs in a terminal. Its UX - like scrolling, copy pasting, pane management, etc - complements other terminal apps, resulting in a workflow that I find consistent and unobtrusive. reply KolmogorovComp 1 hour agoprevThe only annoying thing with tmux are recursive sessions. By that I mean when you run tmux on your laptop, then ssh to your server and start another tmux session there. It is suddenly confusing and you then have to escape commands depending on which session you want to interact with. I’ve been using screen at first remotely for those cases, but eventually bailed out and used my graphical multiplexer locally instead of tmux, or not using tmux remotely when already using it locally. reply mrshu 4 minutes agoparentBeing a big tmux fan, I never thought it would come to something like this, but Zellij (https://zellij.dev/) has completely solved this for me. I generally run Zellij on my laptop and tmux on remote servers -- it's been a surprisingly positive experience so far. Try it out -- your tmux muscle memory (e.g. all the hotkeys you remember) will also work out of the box. reply wging 20 minutes agoparentprevThe way I typically handle that is via native terminal-app tabbing. Tab 1: local tmux session, tab 2: remote tmux session. (My normal workflow doesn't go beyond 2 systems, 3 in an edge case.) You don't need much support from the terminal to do it this way; it works in gnome-terminal on Linux and iTerm2 on Mac, and probably also macOS's built-in native terminal. You just need an app-level shortcut to go forward/back between tabs, then tmux's shortcuts for navigation within the system. (I don't use iTerm2's native tmux integration, so any poor interactions with that when I'm on Mac aren't a concern, and when I'm on Linux gnome-terminal has just as much utility for me.) reply mat_epice 31 minutes agoparentprevI have a system where I use a tmux-server config that has a different prefix (^O), status bar color (yellow), and location (top). I use that for my outer tmux session, then start default configs on other host systems. Works pretty well, I don’t have any complaints. reply jwr 39 minutes agoparentprevI made this somewhat less confusing for myself by defining keyboard shortcuts in my Terminal app (MacOS) and using those for switching between tmux screens. E.g. command-n and command-p switch to next and previous locally, while shift-command-n and shift-command-p switch next and previous in a nested remote tmux session. reply ehsankia 53 minutes agoparentprevTo me, the only annoying thing with tmux is copy/pasting across different setups and how inconsistent / flaky / tricky it is to get working. reply tetris11 49 minutes agoparentprevThis very much so. I wish there was some kind of SSH \"passthrough\" that allowed a tmux session on remote to attach to a tmux local. reply thanatos519 51 minutes agoparentprevI use different escape keys for my local and remote sessions and after many years I don't get confused anymore. reply devsda 13 minutes agorootparentI do the same for one-off tasks. I use ^a as the escape key locally and leave the remote server(s) to use the default ^b because I might need to login to random servers with no shared home or login as a common user and they often have restrictions on adding/changing custom dot files. If it gets too annoying, the remote session is openend in a new terminal tab and I temporarily remap the escape key to ^a on remote tmux. reply chasil 1 hour agoprev>every byte has to be parsed twice, once by the middleman and once by the terminal. The ~ (tilde) codes in OpenSSH imply that it also performs minimal parsing. Particularly, \"(return)~.\" to terminate a locked session. The tmux program is invaluable to me when I have a long-running set of interactive commands, and I want them to survive a network disconnect. There may be valid criticisms of tmux, but wow is it good at this particular thing. reply syslog 19 minutes agoprev„If you want to walk the path of true digital minimalism, vanilla Vim and tmux or its spiritual grandfather screen are all you need.“ I think I‘ve been walking this path for twenty years now. reply lisper 17 minutes agoprev> tmux (short for “terminal mux” (short for “multiplexer”)) is i3 for your terminal. What a weird way to describe it. It sounds more like an alternative to gnu screen [1] to me. Comparing either of those to i3 seems like a category error to me. --- [1] https://www.gnu.org/software/screen/ reply INTPenis 39 minutes agoprevKovid may be right, but from a usability standpoint they're dead wrong. Users will gravitate towards what makes their life simpler, once it was screen and now it's tmux. Regardless of the technicality behind it. I actually gave zellij a try recently but went right back to tmux. Zellij seems like a very promising software but tmux still has a lower threshold of entry. reply sgt 37 minutes agoparentWhat's wrong with screen? I tried tmux about 15 years ago but didn't really see the major benefit. Has it changed much? reply PUSH_AX 1 hour agoprevI would be lost without tmux, if you have a regular ssh date with some machine it’s a life saver. reply Ringz 33 minutes agoprevRunning Kitty and having stable connections to all my servers means I never needed tmux. That’s the point for me. If you have a stable connection, let it run in a separate tab, window, split—whatever—until it’s finished. Digital life is complex and fragmented enough, so I avoid having to deal with another layer like tmux and call it a day. Keep it simple. reply kibwen 16 minutes agoparentBut if I hibernate my computer twice a day, once to take it to work and once to bring it home, then twice a day my SSH connection will be killed due to hibernation (or even if I didn't hibernate it, it would be killed during network disconnection), meaning this setup is only good if everything I do fits into a single eight-hour workday, or if I work exclusively over a cellular connection with transparent network migration and never put my computer to sleep. Simpler to just have a program running on the server that I can attach to. reply ProfessorZoom 1 hour agoprevbad for energy? sir i code typescript in tmux reply kshmir 1 hour agoparentYou’ll need gpu acceleration reply echelon 1 hour agoparentprevIt figures that a fancy terminal emulator developer would take issue with tmux. Tmux undoes all the fancy performance end ergonomic stuff they spent their time and energy developing. Moving stuff to the GPU, multithreading, etc. -- all just to call tmux anyway and lose 100% of those features! What we really need is a protocol for tmux (or other terminal multiplexers) to expose its internal state to native terminal emulators so that you can get the best of both worlds. That's a tremendous amount of work, though. reply dymk 51 minutes agorootparentas far as I'm aware, there is some sort of protocol for looking at tmux state. iTerm has tmux integration, and will show tabs/splits as native (sub)windows. https://iterm2.com/documentation-tmux-integration.html https://github.com/tmux/tmux/wiki/Control-Mode The downside is it doesn't work very well. Tmux tabs open as new iTerm windows instead of tabs, there are resizing bugs, etc. reply lexlash 53 minutes agorootparentprevLike `tmux -CC` and iTerm2? https://iterm2.com/documentation-tmux-integration.html reply SoftTalker 1 hour agoprevLove tmux. I don't find myself splitting one window into panes very often, but I certainly use multiple \"tabs\" (windows) in a tmux session. I keep a tmux session running on my work computer, which has logins to all my commonly needed systems. I can ssh in from home (or anywhere), attach to that tmux session, and instantly have everything I need. reply noloblo 1 hour agoparenthow do you attach to a tmux session via ssh? reply CaptainOfCoit 1 hour agorootparentOther answers tell you how to attach, but vital missing piece is that beforehand, don't \"quit\"/\"exit\" the session but \"detach\" from it instead, so you can attach later. By default the command is $PREFIX + D reply n_jd 1 hour agorootparentprevYou SSH in and run `tmux attach` reply praptak 1 hour agorootparentprevssh -t -A yourhost.com \"tmux attach -d\" reply rmdes 42 minutes agoprevI'm into byobu because it allows me to choose tmux or screen and I've gotten used to how it handles new \"windows\" in my terminal, I mostly use it server side, so I can have persistence and have each tab window in the remote terminal running exactly what I want, combined with fish shell it's just a pleasure to work reply kzrdude 22 minutes agoprevIf I use a terminal with mosh with tmux with neovim with :term how many parses is that? At least 4 or 5. It works! reply sumosudo 3 minutes agoprevHa. I've got two production sites running in tmux. Yeah I know. Crazy. Mad. Foolish. but its worked for years. Of course only two sites though, I swear. reply neilv 1 hour agoprevI once half-seriously leaned on the lightweight possibilities of reattachable SSH sessions, for a very-very simple way of running/monitoring/futzing a simple server process, when you don't need a big enterprise deployment and observability platform. https://www.neilvandyke.org/racket/rackonsole/ reply eigenvalue 50 minutes agoprevI, for one, love tmux. But I hate it with the stock settings on a fresh machine. Having to use keyboard shortcuts to switch between windows is especially annoying. I have this in an ansible playbook I run on all new machines to set tmux up nicely (I also use ohmyzsh and atuin, but that’s another story): - name: Install custom tmux configuration become: yes become_user: ubuntu git: repo: https://github.com/gpakosz/.tmux.git dest: /home/ubuntu/.tmux/ - name: Set up custom tmux configuration become: yes become_user: ubuntu shell:cd /home/ubuntu/ ln -s -f .tmux/.tmux.conf cp .tmux/.tmux.conf.local . - name: Set up further tmux customizations become: yes become_user: ubuntu blockinfile: path: \"/home/ubuntu/.tmux.conf.local\" insertafter: \"# EOF\" block:set-option -g default-shell $SHELL set -g mouse on set-option -g history-limit 25000 set -g @plugin 'tmux-plugins/tmux-resurrect' reply swapsCAPS 37 minutes agoprevheh I notice I really don’t care what any of the terminal emulators provide in terms (get it) of functionality. iTerm2 now has “AI”. What? As long as I have my good ol trusty tmux setup I’m good. Since the multiplexer _is_ the middleman it has become the perfect place to implement functionality which would otherwise be emulator specific. reply hiAndrewQuinn 31 minutes agoprevOP here: I'm realizing tmux's/screen's remote persistence is a bigger deal for a lot of commenting folks than my dinky little \"how do I put my logs on the right and my vim on the left\" use case. Color me mildly surprised, I always thought screen splitting / multiple shells in tabs was the reason we were all using these. To be unambiguous, that's not the \"worse is better\" I'm talking about. That's straightforwardly better is better. reply tedunangst 47 minutes agoprevSome time ago I reported an energy wasting bug in tmux, and surprise twist, it was not the result of parsing every byte twice. reply jonathanrmumm 1 hour agoprev [–] why not just say \"less is better?\" reply kelnos 59 minutes agoparentBecause that's not the common, generally accepted term. reply kzrdude 24 minutes agoparentprevIt doesn't convey the same idea reply jonathanrmumm 18 minutes agorootparentthen someone needs to update the wikipedia > It refers to the argument that software quality does not necessarily increase with functionality: that there is a point where less functionality (\"worse\") is a preferable option (\"better\") in terms of practicality and usability. reply alabhyajindal 19 minutes agorootparentprevWhat idea does \"worse is better\" convey to someone who hasn't heard of the phrase before? reply smcameron 55 minutes agoparentprevBecause \"Worse is Better\" entered the lexicon circa 1991. https://en.wikipedia.org/wiki/Worse_is_better reply jonathanrmumm 22 minutes agorootparentyes, why? seems like not that useful of a phrase if it needs an explanation with parenthesis in a wikipedia reply alabhyajindal 29 minutes agoparentprev [–] Exactly. \"Worse is better\" makes zero sense. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the utility of tmux, a terminal multiplexer, for managing multiple shells without a graphical environment, especially useful for programmers using SSH.",
      "Kovid Goyal, developer of the Kitty terminal emulator, criticizes tmux for its complexity and inefficiency, though the author prefers tmux for its reliability and simplicity across different systems.",
      "The piece concludes with the \"worse is better\" philosophy, stressing the importance of optimizing software for common use cases, even if it means accepting some suboptimal aspects."
    ],
    "commentSummary": [
      "The discussion focuses on the utility and efficiency of terminal multiplexers like `tmux` and `screen` for managing SSH sessions and maintaining persistent terminal states.",
      "Users debate the complexities of `tmux`, its key bindings, and alternatives such as Zellij, Byobu, and terminal emulators with tmux integration (e.g., iTerm2).",
      "The conversation highlights the challenges of maintaining configurations across different setups and balancing functionality with ease of use in terminal multiplexers."
    ],
    "points": 97,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1716657630
  },
  {
    "id": 40472161,
    "title": "Building a Simple and Reliable Job System in Clojure with core.async",
    "originLink": "https://blog.janetacarr.com/creating-a-dead-simple-asynchronous-job-system-in-clojure/",
    "originBody": "May 24, 2024 Dead simple core.async job system in Clojure In my off time (and my on time, for that matter) I've been working on this quirky thing I've called Scinamalink (sounds like 'skinamarink'). Scinamalink lets customers send Magic login links to their users with a single REST API call. If you don't know what a magic login link is, it's basically a password reset email on steroids. Quite literally a link that just authenticates the user's session, skipping the usual password reset flow. My motivations for working on Scinamalink range from having something to show off my Clojure skills on Twitch, and to see if this is worthwhile for people since services like Auth0 and Clerk support magic login links. I'm 'unbundling' as the cool kids say. An early problem I was concerned with was curtailing spam. I figure the best approach for this would be domain verification for the customer's domain. Makes sense. Time for an asynchronous job and some DNS queries. Avoiding a RabbitMQ hole The first thing someone suggested was introducing some kind of message broker, like RabbitMQ, and going from there. I said hell no. I'm trying to avoid complexity. Yet, I understand that building an async worker from scratch doesn't seem like the simplest approach. My line of thinking is this: I think infrastructure is part of your Software Architecture. Every component in an architecture adds exponentially more complexity, whether it's a software component, or another process on the same network. By using something like RabbitMQ and writing the jobs for it, I'm essentially adding two or three more components to the architecture: The RabbitMQ process, its deployment configuration, and the code to manage jobs from my main application server. Such complexity may be worth it for some developers building a solution, but, as someone who wants to get to market faster, cutting the ops work looks like the better approach. The obvious trade-off being my single-process worker system may be less reliable than RabbitMQ. Simple Yet Reliable Enough I have a single process with multiple threads, thanks to core.async. It's not lost on me that if the process fails, the jobs will be lost, so my approach to reliability starts at the data model and database. Let's take a look at the PostgreSQL data definition for a job: CREATE TYPE worker_job_state AS ENUM ('created', 'starting', 'working', 'finished', 'crashed'); CREATE TABLE IF NOT EXISTS worker_jobs ( id serial primary key, current_state worker_job_state not null default 'created', timeout timestamp default now() + interval '24 hours', attempt_count integer not null default 0, priority integer not null default 1, context jsonb not null, created_at timestamp not null default now(), updated_at timestamp not null default now() ); I originally intended for timeouts and priorities to be a thing, but they're 'reserved for future use' (waste of time). But, since each job could be different in implementation, it's necessary to store some of the context in a free-form JSON blob column. For example, when verifying domain ownership, it might be a good idea to store the customer and domain associated with the verification job. However, we can see each job shares the same states describing the job's lifecycle: created, starting, working, finished, and crashed. I think each state is self-explanatory here, and these states imply each job function is a Finite-State-Machine (FSM). All work, no play So, I need to implement a Finite-State-Machine in Clojure. If you've read any of my previous works, you know whats coming next: Functions returning functions. In short, we can implement a finite state machine by having a function represent each state. When a state needs to transition to another state, it returns that function, otherwise it returns itself. We can use the recurring lexical scope in a recursion to propel the state machine forward. It might be easier to start with the loop: ;; in scinamalink.core.worker namespace (def buffer-limit (or (:job-buffer-limit env) 2048)) (defonce queue (a/chan (warning-dropping-buffer buffer-limit \"job queue full, job dropped\"))) (defn worker \"Spins off a go-loop based worker and runs the job function pulled from channel `queue` in a core.async/thread. If that job returns a fn, puts it back on the `queue` for a worker to process. Otherwise, worker discards result and repeats.\" [queue] (a/go-loop [job (a/! queue next-state))) (catch Exception e (log/warn \"Possible job failure in worker: %s\" (.getMessage e)))) (recur (a/job [customer-id domain-id] (try (let [job-context {:job-type \"domain_verification\" :customer-id customer-id :domain-id domain-id} job (db-jobs/create-db-job ds-opts (db-jobs/next-week (Instant/now)) 0 1 job-context)] (log/info \"Domain verification job created\") #(start-job job)) (catch Exception e (log/warnf \"domain verification job failed: %s %s\" customer-id domain-id)))) from jobs.domain-verification namespace The domain verification job gets created with ->job which creates the job in the database and returns the first function to place on the queue with something like (dispatch-work queue (->job customer-id domain-id)). Since the workers are so thin themselves, jobs are responsible for everything related to its function. Each state needs to clean up after itself if something goes wrong. They also update the database with its serialized context regardless of failure. However, I'm not bound by the rigidity of the job data model though. You'll notice that job-work does most of the work for this task, yet do-job sets the state to :working in the database. I did this because I didn't want to unnecessarily write the state :working to the database each time the job attempts to make the DNS query. The worker doesn't care as long as it gets a function. Although, when the process starts and loads the jobs from the database, it will start at do-job again. Starting, Restarting, and Unstarting Processes At some point, jobs will need to be loaded from the database into the worker system whether it's from failures or restarts. This is a pretty simple process: Read from database, dispatch to the appropriate job constructor, and put the resulting jobs on the queue for the workers. (defmulti ->job-fn \"Multimethod to dispatch on job creation function\" (fn [job] (let [{:keys [current-state context]} job] (vector (csk/->kebab-case-keyword (:job-type context)) (keyword current-state))))) ;; Currently in core.worker, but should be in core.loader: (defn- start-existing-helper! \"Recursively puts each `job` fn on the port `queue`, presumably to be processed by a worker (see above).\" [queue jobs] (let [job (first jobs)] (try (a/put! queue job (fn [all-good] (if all-good (start-existing-helper! queue (rest jobs)) (log/errorf \"didn't put job onto queue, exploding: %s\" job)))) (catch Exception e (log/errorf \"didn't put job onto queue, exploding: %s\" job))))) (defn start-existing-jobs! \"Starts existing jobs from the DB\" [] (try (let [jobs (db-jobs/get-all-pending-jobs ds-opts buffer-limit)] (->> jobs (mapv ->job-fn) (start-existing-helper! queue))) (catch Exception e (log/errorf \"Unexpected error while starting existing jobs: %s\" (.getMessage e))))) In the actual job definition, we can extend ->job-fn with a dispatch values that map to our database record's context column. (defmethod worker/->job-fn [:domain-verification :created] [job] #(start-job job)) (defmethod worker/->job-fn [:domain-verification :starting] [job] #(do-job job)) (defmethod worker/->job-fn [:domain-verification :working] [job] #(job-work job)) from jobs.domain-verification namespace This start-existing-jobs! function gets called when the process starts, but we need a method to periodically load jobs while the process is running. Ideally, our job loader would be aware of each running job so that the same jobs aren't loaded over and over. (defonce registry (atom {})) ;; 1hr = 3600000 ms (defn loader \"Loads jobs from the Database into the job `w/queue`, skipping the currently running ones.\" [queue ms] (a/go-loop [to-chan (a/timeout ms)] (try (when-not (a/> jobs(filterv #(not (contains? running-jobs (:id %))))(mapv w/->job-fn))] (doseq [job stored-jobs] (w/dispatch-work queue job))) (catch Exception e (log/warn \"Loader exception while loading jobs from DB: %s\" (.getMessage e))))))) (catch Exception e (log/warnf \"Possible job failure in worker: %s\" (.getMessage e)))) (recur (a/timeout ms)))) (defn start-job-loaders! ([queue] (start-job-loaders! queue 1)) ([queue n] (doseq [_ (range n)] (loader queue 600000)))) core.loader Lucky for us, core.async violates the thread-local nature of Clojure Vars. Meaning, that I can have a Var pointing to an atom where information about all the running jobs is stored (Jobs take on responsibility of registering themselves). As we can see, a loader functions very similarly to a worker running the start-existing-jobs! functionality, filtering on what's in the registry atom upon this iteration. After each iteration, the loaders will wait using a core.async/timeout for specified amount of time. Big Picture Finally, the big picture of the sub-system emerges. Follow me on the website formerly known as Twitter dot com and around the web @janetacarr , or not. ¯\\_(ツ)_/¯ Previous issue Browse all issues",
    "commentLink": "https://news.ycombinator.com/item?id=40472161",
    "commentBody": "A simple core.async job system in Clojure (janetacarr.com)92 points by janetacarr 13 hours agohidepastfavorite15 comments lkrubner 7 hours agoI appreciate this emphasis on simplicity and I recommend it to others. I've seen trends in the other direction which I think are dangerous. I notice that as some companies adopted a microservices approach there was a tendency to allow in more technologies than necessary. \"Premature polyglot programming\" is a disease that afflicts certain startups. While any sufficiently big company will be polyglot, a small startup needs to stay focused on a limited tech stack, for as long as possible. After all, each new technology requires someone on staff who has the skill for that technology, and when your startup is small, your talent pool will also be small, and so it becomes common that you only have 1 person on the team who knows how to run some particular technology (Kafka, RabbitMQ, Redshift, DynamoDB, MongoDB, Tornado, etc). So you end up with a lot of single point of failures, where the \"single point of failure\" is the single engineer who knows how to run the technology that you've made critical to the company. Be wary. Avoid this if possible. I notice, especially, as the tech industry developed better tools for managing complex devops situations (Docker, Kubernetes, Terraform) there was a tendency from some engineers to think \"Nowadays it is easy to run 10 different technologies, therefore we should run 10 different technologies.\" Be wary of this. Janet Carr's emphasis on simplicity is something we should all imitate. reply signal11 6 hours agoparentI agree, and a lot of this applies to large companies as well. Of course larger orgs will have higher tolerance for a wider set of technologies, but this tolerance is not infinite, and choosing tech because that one team was a fan creates staffing problems over time, especially as the original devs move on. Eventually you get “estate sprawl” that’s difficult to manage. Choosing to minimise dependencies is definitely a (good) choice. The key is the right tech for the task at hand. Equally org-wide “use Oracle for everything” mandates aren’t helpful. Balancing these at a large org definitely needs good engineering leadership! reply janetacarr 6 hours agoparentprevWow, thanks for the kind words. I've been bitten by the ops hellscape of microservices and various tech many times throughout my career, and it's definitely shaped how I think about designing and building software now. reply BaculumMeumEst 5 hours agoparentprevYou can use this same line of reasoning to avoid using Clojure entirely. reply janetacarr 5 hours agorootparentYeah but that's not nearly as fun! reply BaculumMeumEst 5 hours agorootparentVery true. reply Waterluvian 4 hours agoparentprevThis is what I think about most when looking at languages. “How painful would it be if I implemented the minimum now and wanted to add more capability later?” Some languages make it very painful to not have a mostly complete design upfront and then you’re trapped adding complexity just in case. reply smrtinsert 3 hours agoparentprev100% agreed. Unfortunately overdoing it on complexity early on seems to lead to a loss of the faith in engineering decision making even after overengineering leadership is removed. reply tombert 4 hours agoprevcore.async was actually the \"killer app\" that sold Clojure to me. I've always liked Go's CSP-style concurrency, but I wanted a proper functional language to go with it. It turns out that Channels/Queues are just an insanely good abstraction for getting threads to talk to each other. The closest general-purpose library I've found that helps bring that to the rest of the world has been ZeroMQ, which is great but not as easy and nice to use as core.async. That said, I really don't think RabbitMQ is so bad. The default docker configuration for it is fine for most cases you're likely to come across for all but the biggest applications, and I do think that having the ability to have jobs restart/requeued when a worker crashes out of the box is worth the tradeoffs for slightly increased complexity. I usually just use a single docker compose and glue everything together in one big ol' YAML. Still, there's obvious value in avoiding dependencies, so it of course depends on the size and scope of your project. If you think you're going to end up distributing this over 100 nodes, something like RabbitMQ or ActiveMQ might be worth playing with, but if you're just doing relatively small (at you appear to be in this project), it's probably the correct choice to mimic whatever behavior you need with core.async. reply elwell 1 hour agoparent> core.async was actually the \"killer app\" For ClojureScript, it was React. Immutable state / functional programming fits so nicely, and hiccup syntax (JSX as simple vectors) is the perfect declarative (yet transformable) DOM structure. reply tombert 1 hour agorootparentOh absolutely, no argument here at all. I don't do a lot of frontend, but for anything that does involve frontend I kind of refuse to use anything but re-frame. It took a bit of dogma-acceptance, but once I did I really liked it for all the reasons you listed, and in particular that there's no special JSX crap, and instead just using vanilla vectors and symbols. This has the interesting advantage where you can generate your HTML logic without actually having to import the re-frame/re-agent logic, which can be useful if you want to decouple logic from rendering. I haven't done any benchmarks, but I haven't noticed re-frame being slower than vanilla React, it seems to be a comparable speed, but as stated I don't do much frontend. reply jwr 36 minutes agoprevYes! core.async is a great tool and I've used it to solve a number of problems effectively. I'm very happy with what I get: reliable, predictable systems. A real \"whoa\" moment comes when you realize you can also use core.async in ClojureScript :-) reply mark_l_watson 2 hours agoprevI appreciate Janet’s aversion to adding RabbitMQ to the system. I had to use RabbitMQ with Common Lisp a year ago and in was a pain. It looks like Clojure has better RabbitMQ client options than Common Lisp, but still, very cool to build something on core.async and keep things cleaner and simpler. reply samsquire 9 hours agoprev [–] Thank you for this post Janet. I think my technical perspectives have moved in a similar direction: keep things extremely simple with minimum moving parts. I maintained (automated upgrades) a RabbitMQ cluster and while it is powerful software it is operationally expensive. For a side project you probably just batch process in a cron. If I were to take the approach in this blog post I would want everyone on the team to be extremely familiar with the model of task running: stuck jobs, timeouts, duplicate jobs, client disconnects and retries, stuck \"poison\" jobs seem like issues you might face. reply janetacarr 6 hours agoparent [–] I'm glad you liked it! I'm using a PaaS, so I didn't want to pay the extra money for a cron job. Maybe not a wise a choice, but here we are. Totally agree about the issues I might face. I'm glad that the Clojure REPL is a thing, so I can test out all of a job's functionality before sending it off to async land. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author developed Scinamalink, a Clojure-based system for sending magic login links via a simple REST API.",
      "Instead of using complex message brokers like RabbitMQ, they implemented a core.async-based job system, utilizing PostgreSQL for job state tracking and a finite-state machine for job processing.",
      "The system emphasizes simplicity and reliability, leveraging core.async for concurrency and a JSON blob for flexible job context storage, aiming to balance ease of implementation with practical reliability."
    ],
    "commentSummary": [
      "Janet Carr's blog post advocates for a simple core.async job system in Clojure, emphasizing minimalism in tech stacks.",
      "Commenters warn against the complexity and risks of adopting multiple technologies, particularly in startups, citing issues like \"premature polyglot programming\" and single points of failure.",
      "The discussion highlights the benefits of core.async for concurrency in Clojure, suggesting it as a favorable alternative to tools like RabbitMQ for smaller projects, with a consensus on minimizing dependencies and complexity."
    ],
    "points": 92,
    "commentCount": 15,
    "retryCount": 0,
    "time": 1716603563
  }
]
