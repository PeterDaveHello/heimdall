[
  {
    "id": 40337936,
    "title": "Efficient AI Compute on NVIDIA GPUs: Introducing ThunderKittens",
    "originLink": "https://hazyresearch.stanford.edu/blog/2024-05-12-tk",
    "originBody": "May 12, 2024 · 26 min read GPUs Go Brrr Benjamin Spector, Aaryan Singhal, Simran Arora, Chris Re AI uses an awful lot of compute. In the last few years we’ve focused a great deal of our work on making AI use less compute (e.g. Based, Monarch Mixer, H3, Hyena, S4, among others) and run more efficiently on the compute that we have (e.g. FlashAttention, FlashAttention-2, FlashFFTConv). Lately, reflecting on these questions has prompted us to take a step back, and ask two questions: What does the hardware actually want? And how can we give that to it? This post is a mixture of practice and philosophy. On the practical side, we’re going to talk about what we’ve learned about making GPUs go brr -- and release an embedded DSL, ThunderKittens, that we’ve built to help us write some particularly speedy kernels (which we are also releasing). On the philosophical side, we’ll briefly talk about how what we’ve learned has changed the way we think about AI compute. What's in an H100? For this post, we’re going to focus on the NVIDIA H100 for two reasons. First, it represents an awful lot of new compute going online. Second, we think the trends it implies are going to continue in future generations, and probably from other manufacturers, too. But bear in mind (and we will repeat in case you forget) that most of this post applies in some form to other GPUs, too. Figure 1: brr Advance apologies for restating the data sheet, but the details of the hardware are important for the discussion to come. An H100 SXM GPU contains, for our purposes: 80 GB of HBM3 with 3 TB/s of bandwidth. (A bit less bandwidth in practice.) 50 MB of L2 cache with 12 TB/s of bandwidth, split across the GPU into two 25MB sections connected by a crossbar. (The crossbar sucks.) 132 streaming multiprocessors (SM’s), where each has: up to 227 KB of shared memory within a 256 KB L1 cache. (Together, these have about 33 TB/s of bandwidth.) a tensor memory accelerator (TMA) -- a new chunk of hardware in Hopper that can do asynchronous address generation and fetch memory. It also does other things like facilitate the on-chip memory network (distributed shared memory) but we’re not going to focus on this much, today. 4 quadrants, where each quadrant has: A warp scheduler 512 vector registers (each containing 32 4-byte words) A tensor core for matrix multiplies A bunch of built-in instructions like sums, multiplies, that operate in parallel on these vector registers. There’s a lot of other stuff, too (memory controllers, instruction caches, etc) but we don’t care about any of that right now. All of the compute happens in the SM’s. Most of it happens in the registers. Great, how do I make it go brr? Keep the tensor core fed. That’s it. Wait, really? Yes. That’s the game. An H100 GPU has 989 TFLOPs of half-precision matrix multiply compute, and ~60 TFLOPs of “everything else”. So, every cycle the tensor core is in use, you’re getting at least 94% utilization of the hardware. And every cycle the tensor core is not in use, you’re getting no more than 6% utilization of the hardware. Put another way: % utilization H100 = % tensor cores active cycles +/- 6%. Now it turns out that keeping the tensor core fed is easier said than done. We’ve discovered a number of quirks to the hardware that are important to keeping the matrix multiplies rolling. Much of this also applies to non-H100 GPUs, but the H100 is particularly tricky to keep fed so we focus on it here. (The RTX 4090, by comparison, is very easy to work with as illustrated in figure 2.) WGMMA instructions are necessary but also really irritating to use. Shared memory is not actually that fast and also requires great care. Address generation is expensive. Occupancy remains helpful, and registers are generally the key resource. Figure 2: NVIDIA GPUs (H100 and 4090) and their spirit animals (canadian goose and golden retriever puppy). Let’s go through each of these in order. WGMMA Instructions The H100 has a new set of instructions called “warp group matrix multiply accumulate” (wgmma.mma_async in PTX, or HGMMA/IGMMA/QGMMA/BGMMA in SASS). To understand what makes them special, we need to look briefly at how you used to have to use tensor cores. The tensor core instructions available on previous GPUs were wmma.mma.sync and mma.sync instructions. With these instructions a warp of 32 threads on a single quadrant of an SM would synchronously feed their chunk of the data into the tensor core and await the result. Only then could they move on. Not so with wgmma.mma_async instructions. Here, 128 consecutive threads -- split across all quadrants of the SM -- collaboratively synchronize, and asynchronously launch a matrix multiply directly from shared memory (and optionally also registers.) These warps can then go do other things with their registers while the matrix multiply happens, and await the result whenever they want. In our microbenchmarks, we found that these instructions are necessary to extract the full compute of the H100. Without them, the GPU seems to top out around 63% of its peak utilization; we suspect this is because the tensor cores want a deep hardware pipeline to keep them fed, even from local resources. Unfortunately, the memory layouts for these instructions are quite complicated. The unswizzled shared memory layouts suffer from very poor coalescing, and so they require substantial additional bandwidth from L2. The swizzled memory layouts are flat-out incorrectly documented, which took considerable time for us to figure out. They’re also brittle, in that they appear to only work for specific matrix shapes and do not play well with other parts of the wgmma.mma_async instructions. For example, the hardware can transpose sub-matrices on its way to the tensor cores -- but only if the layout is not swizzled. Figure 3: NVIDIA’s lies. This is an extraordinarily misleading representation of the actual 128b swizzled wgmma layout. This diagram cost us three weeks of life that we will not get back, hence the public shaming. We’ve also found that unswizzled wgmma layouts have both poor memory coalescing as well as bank conflicts. On kernels such as flash attention, TMA and the L2 cache are both fast enough so as to hide these problems reasonably well. But to make the full use of the hardware, memory request must be coalesced and bank conflicts avoided, and then controlling layouts very carefully becomes critical. Despite these pains, these instructions really are necessary to make full use of the H100. Without them, you’ve already lost 37% of the potential performance of the GPU! Shared memory Shared memory appears to have a single-access latency of around 30 cycles (this matches our observations, too). That doesn’t sound like much, but in that time the SM’s tensor cores could have done almost two full 32x32 square matrix multiplies. In previous work (like Flash Attention), we’ve focused more on the HBM-SRAM bottleneck. And indeed: this really used to be the bottleneck! But as HBM has gotten faster and the tensor cores continue to grow out of proportion with the rest of the chip, even relatively small latencies like those from shared memory have also become important to either remove or hide. Shared memory can be tricky to work with because it is “banked” into 32 separate stores of memory. If one is not careful, this can lead to something called “bank conflicts”, where the same memory bank is being asked to simultaneously provide multiple different pieces of memory. This leads to requests being serialized, and in our experience this can disproportionately slow down a kernel -- and the register layouts required by wgmma and mma instructions would naively suffer from these bank conflicts. The solution is to rearrange shared memory with various “swizzling” patterns so as to avoid these conflicts, but it is an important detail to get right. More generally, we have found it very valuable to avoid movement between registers and shared memory when possible, and otherwise to use the built-in hardware (wgmma and TMA instructions) to do data movement asynchronously when possible. Synchronous movement using the actual warps is a worst-case fallback with the greatest generality. Address Generation One interesting quirk of the H100 is that the tensor cores and memory are both fast enough that merely producing the memory addresses to fetch takes a substantial fraction of the resources of the chip. (This is even more the case when complicated interleaved or swizzling patterns are added in.) NVIDIA appears to understand this, as they have bestowed on us the Tensor Memory Accelerator (or TMA, as it likes to be called). TMA allows you to specify a multi-dimensional tensor layout in global and shared memory, tell it to asynchronously fetch a subtile of that tensor, and trip a barrier when it’s done. This saves all of the address generation costs, and additionally makes it much easier to construct pipelines. We have found TMA to be, like wgmma.mma_async, completely indispensable in achieving the full potential of the H100. (Probably moreso than wgmma, in our experience.) It saves register resources and instruction dispatches, and also has useful features such as the ability to perform reductions onto global memory asynchronously, too -- this is particularly useful in complex backwards kernels. As with wgmma, the main quirk of it is that its swizzling modes are a bit difficult to decipher without some reverse engineering, but we had substantially less pain on this point. Occupancy For those newer to CUDA, occupancy refers to the number of co-scheduled threads on the exact same execution hardware. Each cycle, the warp scheduler on that quadrant of the SM will try to issue an instruction to a warp of threads that are ready for an instruction. NVIDIA uses this model because it can enable the hardware to be more easily kept full. For example, while one warp of threads is waiting for a matrix multiply, another can receive an instruction to use the fast exponential hardware. In some ways, the H100 is less reliant on occupancy than previous generations of the hardware. The asynchronous features of the chip mean that even a single instruction stream can keep many parts of the hardware busy -- fetching memory, running matrix multiplies, doing shared memory reductions, and still simultaneously running math on the registers. But occupancy is very good at hiding both sins and sync’s. A perfectly designed pipeline might run reasonably fast even without any additional occupancy, but our observations suggest that NVIDIA really has designed their GPUs with occupancy in mind. And there are enough synchronizations -- and enough ways to make mistakes -- that finding ways to increase occupancy has, in our experience, usually yielded good returns at increasing the realized utilization of the hardware. Finally, while occupancy is merely useful on the H100, we have found it to be increasingly important on the A100 and RTX 4090, respectively, likely because they rely increasingly on synchronous instruction dispatches, relative to the H100. ThunderKittens Based on the above, we asked ourselves how we might make it easier to write the kinds of kernels we care about while still extracting the full capabilities of the hardware. Motivated by a continuing proliferation of new architectures within the lab (and the fact that Flash Attention is like 1200 lines of code), we ended up designing a DSL embedded within CUDA -- at first for our own internal use. But then we decided it was useful enough that, with love in our hearts, we cleaned it up and have released it for you. ThunderKittens is that embedded DSL. It is named ThunderKittens because we think kittens are cute, and also we think it is funny to make you type kittens:: in your code. Figure 4: A ThunderKitten. Look at her big eyes! Are you not be entranced!?!? It is meant to be as simple as possible, and contains four templated types: Register tiles -- 2D tensors on the register file. Register vectors -- 1D tensors on the register file. Shared tiles -- 2D tensors in shared memory. Shared vectors -- 1D tensors in shared memory. Tiles are parameterized by a height, width, and layout. Register vectors are parameterized by a length and a layout, and shared vectors just by a length. (They don’t generally suffer from bank conflicts.) We also give operations to manipulate them, either at the warp level or at the level of a collaborative group of warps. Examples include: Initializers -- zero out a shared vector, for example. Unary ops, like exp Binary ops, like mul Row / column ops, like a row_sum Since ThunderKittens is embedded within CUDA (contrasting libraries like Triton which we also love very much and rely on heavily), the abstractions fail gracefully. If it’s missing something, just extend it to do what you want! To show an example of these primitives in action, consider Tri’s lovely flash attention -- a beautiful algorithm, but complicated to implement in practice, even on top of NVIDIA’s wonderful Cutlass library. Here's a simple forward flash attention kernel for an RTX 4090, written in ThunderKittens. #define NUM_WORKERS 16 // This kernel uses 16 workers in parallel per block, to help issue instructions more quickly. using namespace kittens; // this kernel only handles headdim=64 for simplicity. Also n should be a multiple of 256 here. __global__ void attend_ker64(int n, const bf16* __restrict__ __q__, const bf16* __restrict__ __k__, const bf16* __restrict__ __v__, bf16* __o__) { auto warpid = kittens::warpid(); auto block_start = blockIdx.x*(n*64); const bf16 *_q = __q__ + block_start, *_k = __k__ + block_start, *_v = __v__ + block_start; bf16 *_o = __o__ + block_start; extern __shared__ alignment_dummy __shm[]; // this is the CUDA shared memory shared_allocator al((int*)&__shm[0]); // K and V live in shared memory -- this is about all that will fit. st_bf_1x4 (&k_smem)[NUM_WORKERS] = al.allocate, NUM_WORKERS>(); st_bf_1x4 (&v_smem)[NUM_WORKERS] = al.allocate, NUM_WORKERS>(); // Initialize all of the register tiles. rt_bf_1x4 q_reg, k_reg, v_reg; // v_reg need to be swapped into col_l rt_fl_1x1 att_block; rt_bf_1x1 att_block_mma; rt_fl_1x4 o_reg; rt_fl_1x1::col_vec max_vec_last, max_vec; // these are column vectors for the attention block rt_fl_1x1::col_vec norm_vec_last, norm_vec; // these are column vectors for the attention block int qo_blocks = n / (q_reg.rows*NUM_WORKERS), kv_blocks = n / (q_reg.rows*NUM_WORKERS); for(auto q_blk = 0; q_blk&v_reg_col = swap_layout_inplace(v_reg); // this is a reference and the call has invalidated v_reg mul_row(o_reg, o_reg, norm_vec_last); // normalize o_reg in advance of mma_AB'ing onto it mma_AB(o_reg, att_block_mma, v_reg_col, o_reg); // mfma onto o_reg with the local attention@V matmul. } __syncthreads(); // we need to make sure all warps are done before we can start loading the next kv chunk } store(_o + (q_blk*NUM_WORKERS + warpid)*q_reg.num_elements, o_reg, q_reg.cols); // write out o. compiler has an issue with register usage if d is made constexpr q_reg.rows :/ } } Altogether, this is about 60 lines of CUDA sitting at 75% hardware utilization -- and while it is fairly dense, most of the complexity is in the algorithm, rather than in swizzling patterns or register layouts. And what of all of the complexity of TMA, WGMMA, swizzling modes, and descriptors? Here’s a FlashAttention-2 forward pass for the H100, written with ThunderKittens. template __global__ __launch_bounds__((NUM_WORKERS)*kittens::WARP_THREADS, 2) void fwd_attend_ker_dim(int N, const CUtensorMap* tma_q, const CUtensorMap* tma_k, const CUtensorMap* tma_v, CUtensorMap* tma_o) { extern __shared__ int __shm[]; // this is the CUDA shared memory tma_swizzle_allocator al((int*)&__shm[0]); constexpr int tile_width = fwd_attend_ker_tile_dims::tile_width; // constants constexpr int qo_height = fwd_attend_ker_tile_dims::qo_height; constexpr int kv_height = fwd_attend_ker_tile_dims::kv_height; st_bf (&q_smem) [NUM_WARPGROUPS] = al.allocate, NUM_WARPGROUPS>(); st_bf (&k_smem)[2][NUM_WORKERS_KV] = al.allocate, 2, NUM_WORKERS_KV>(); st_bf (&v_smem)[2][NUM_WORKERS_KV] = al.allocate, 2, NUM_WORKERS_KV>(); int tic = 0, toc = 1; rt_fl att_block; rt_bf att_block_mma; rt_fl o_prev; col_vec> max_vec_last, max_vec; col_vec> norm_vec_last, norm_vec; int warpid = kittens::warpid(); int warpgroupid = warpid/kittens::WARPGROUP_WARPS; int kv_blocks = N / (NUM_WORKERS_KV*k_smem[0][0].rows); __shared__ uint64_t qsmem_barrier, kvsmem_barrier;//, vsmem_barrier; int q_phasebit = 0; int kv_phasebit = 0; if (threadIdx.x == 0) { tma::init_barrier, NUM_WARPGROUPS>(qsmem_barrier, 1); tma::init_barrier, NUM_WORKERS_KV*2>(kvsmem_barrier, 1); } if (warpid == 0) { for (int wg = 0; wg (*)>(q_smem); // reuse q memory warpgroup::store(o_smem[warpgroupid], o_prev); __syncthreads(); if (warpid % 4 == 0) { // store o int tile_idx = (blockIdx.y * NUM_WARPGROUPS * gridDim.x) + (blockIdx.x * NUM_WARPGROUPS) + warpgroupid; tma::store_async(tma_o, (o_smem[warpgroupid]), tile_idx); tma::store_commit_group(); } tma::store_async_wait(); } So how does it do? This kernel is just 100 lines, and it actually outperforms FlashAttention-2 on the H100 by about 30%. ThunderKittens takes care of wrapping up the layouts and instructions, and gives you a mini-pytorch to play with on the GPU. Figure 5: FA2 (via Pytorch) versus TK for a wide range of configs on the H100 SXM. We also release kernels for Based linear attention and other forthcoming architectures, too. Our Based linear attention kernel runs at 215 TFLOPs (or more than 300 TFLOPs when the recompute inherent in the algorithm is considered). And while linear attention is of course theoretically more efficient, historically, they have been dramatically less efficient on real hardware. So we feel this could open up a broad range of high-throughput applications -- more to come on this point later. Figure 6: Linear attention can be quite quick with TK! If this seems up your alley, feel free to play with it! Tiles Seem Like a Good Idea In our view, what has made ThunderKittens work well for us is that it does not try to do everything. CUDA is indeed far more expressive than ThunderKittens. ThunderKittens is small and dumb and simple. Figure 7: the main message of this blog post. But ThunderKittens has good abstractions -- small tiles -- that match where both AI and hardware are going. ThunderKittens doesn’t support any dimension less than 16. But in our view, this doesn’t really matter, since the hardware doesn’t particularly want to, either. And we ask: if your matrix multiply is smaller than 16x16, are you sure what you’re doing is AI? From a philosophical point of view, we think a frame shift is in order. A “register” certainly shouldn’t be a 32-bit word like on the CPUs of old. And a 1024-bit wide vector register, as CUDA uses, is certainly a step in the right direction. But to us a “register” is a 16x16 tile of data. We think AI wants this -- after all this time, it’s still just matrix multiplies, reductions, and reshapes. And we think the hardware wants this, too -- small matrix multiplies are just begging for hardware support beyond just the systolic mma. In fact, more broadly we believe we should really reorient our ideas of AI around what maps well onto the hardware. How big should a recurrent state be? As big can fit onto an SM. How dense should the compute be? No less so than what the hardware demands. An important future direction of this work for us is to use our learnings about the hardware to help us design the AI to match. Tiles Seem Pretty General Coming soon -- ThunderKittens on AMD hardware!",
    "commentLink": "https://news.ycombinator.com/item?id=40337936",
    "commentBody": "GPUs Go Brrr (stanford.edu)660 points by nmstoker 11 hours agohidepastfavorite112 comments Animats 10 hours ago\"And we ask: if your matrix multiply is smaller than 16x16, are you sure what you’re doing is AI? From a philosophical point of view, we think a frame shift is in order. A “register” certainly shouldn’t be a 32-bit word like on the CPUs of old. And a 1024-bit wide vector register, as CUDA uses, is certainly a step in the right direction. But to us a “register” is a 16x16 tile of data. We think AI wants this.\" The hardware needs of AI are starting to focus. GPUs, after all, were designed for an entirely different job. They're used for AI because they have good matrix multiply hardware. \"AI GPUs\" get to leave out some of the stuff in a real GPU (does an H100 even have texture fill units?). Then there's a trend towards much shorter numbers. 16 bit floating point? 8 bit? 2 bit? 1 bit? That will settle out at some point. This paper indicates that hardware that likes 16x16 tiles makes a lot of sense. It's certainly possible to build such hardware. Someone reading this is probably writing it in VHDL right now, or will be soon. Then we'll see somewhat simpler, less general, and cheaper devices that do \"AI\" operations with as little excess hardware baggage as possible. Nice. reply bcatanzaro 6 hours agoparentGPUs have evolved to be AI machines with as little baggage as possible. People have been arguing GPUs were old technology and therefore unsuited for AI since at least 2014 (when Nervana was founded), but what they perhaps didn’t expect is that the GPU would evolve so quickly to be an AI machine. reply celrod 1 hour agorootparentBill Dally from Nvidia argues that there is \"no gain in building a specialized accelerator\", in part because current overhead on top of the arithmetic is in the ballpark of 20% (16% of IMMA and 22% for HMMA units) https://www.youtube.com/watch?v=gofI47kfD28 reply AnthonyMouse 42 minutes agorootparentThere does seem to be a somewhat obvious advantage: If all it has to do is matrix multiplication and not every other thing a general purpose GPU has to be good at then it costs less to design. So now someone other than Nvidia or AMD can do it, and then very easily distinguish themselves by just sticking a ton of VRAM on it. Which is currently reserved for GPUs that are extraordinarily expensive, even though the extra VRAM doesn't cost a fraction of the price difference between those and an ordinary consumer GPU. reply bjornsing 28 minutes agorootparentExactly. And that means you not only save the 22% but also a large chunk of the Nvidia margin. reply papruapap 14 minutes agorootparentprevI really hope we see AI-PU (or with some other name, INT16PU, why not) for the consumer market sometime soon. Or been able to expand GPU memory using a pcie socket (not sure if technically possible). reply dvt 9 hours agoparentprev> Then we'll see somewhat simpler, less general, and cheaper devices that do \"AI\" operations with as little excess hardware baggage as possible. Nice. Apple has already been doing this for a few years now. The NPU is totally different from the GPU or CPU on the die itself[1]. Nvidia is likely working on this as well, but I think a device that's a gaming/entertainment/crypto/AI bundle (i.e. sticking with the video card) is probably a better business move. [1] https://github.com/hollance/neural-engine/blob/master/docs/a... reply talldayo 8 hours agorootparentThe NPUs on a lot of different systems occupy an awkward spot. For extremely small models, they're the way to go for low-power inference. But once you reach LLM or vision transformer size, it makes a lot more sense to switch to GPU shaders for that extra bit of large-model performance. For stuff like Llama and Stable Diffusion, those Neural Engines are practically wasted silicon. The biggest saving grace is projects like ONNX attempting to sew them into a unified non-15-competing-standards API, but even that won't change how underpowered they are. Nvidia escapes this by designing their GPU architecture to incorporate NPU concepts at a fundamental level. It's less redundant silicon and enables you to scale a single architecture instead of flip-flopping to whichever one is most convenient. reply nxobject 3 hours agorootparentIt's currently doable for Apple – I think their strategy is to slowly enhance iPhones, bit by bit, with special-purpose models for dealing with media like photo subject identification, OCR (in every language!), voice transcription, etc. Apple's currently learning from Microsoft's attempts to make AI stick everywhere. reply joquarky 3 hours agorootparentSoon our phones will dream beside us every night (integrating new data into our personal model while on the charger) reply yosefk 5 hours agorootparentprevFor inference, Nvidia has DLA since 2017-ish if I remember correctly, which is completely separate from the GPU. reply eru 7 hours agorootparentprevAnd Google has their TPUs. reply muyuu 2 hours agoparentprevit's going to be awkward in consumer hardware either way if you segregate AI units from the GPU, the thing is both AI and GPUs will continue to need massive amounts of matrix multiplication and as little memory latency as possible the move to have more of it wrapped in the GPU makes sense but at least in the short and medium term, most devices won't be able to justify the gargantuan silicon wafer space/die growth that this would entail - also currently Nvidia's tech is ahead and they don't make state of the art x86 or ARM CPUs for the time being I think the current paradigm makes the most sense, with small compute devices making inroads in the consumer markets as non-generalist computers - note that more AI-oriented pseudo-GPUs already exist and are successful since the earlier Nvidia Tesla lineup and then the so-called \"Nvidia Data Center GPUs\" reply rfoo 1 hour agorootparent> as little memory latency as possible Should be \"as much memory bandwidth as possible\". GPUs are designed to be (relatively) more insensitive to memory latency than CPU. reply mvkel 10 hours agoparentprevWould you say this is ultimately \"ASICs for AI\"? reply dartos 9 hours agorootparentIn the same way that CPUs are ASICs for integer operations, that makes sense to me. reply saagarjha 3 hours agorootparentMost CPUs do just fine on floating point too. reply actionfromafar 2 hours agorootparentI'm still getting used to that. reply WanderPanda 9 hours agoparentprevWait but nvidia tensor-cores are exactly the hardware that likes 16x16 tiles, no? I thought that was the whole point? The hardware is already here and I'm sceptical if there is another order of magnitude in performance to be gained from even more specialized designs. reply wtallis 8 hours agorootparentWhat's the ratio of tensor cores to regular SIMD compute (\"CUDA cores\") on NVIDIA's current chips? reply creato 5 hours agorootparentThis is in the article: if you aren't using the tensor cores, you aren't utilizing ~94% of the FLOPs available. reply wtallis 2 hours agorootparentKnowing what portion of the FLOPs are in the tensor cores isn't quite the right thing to be looking at. The key question is how much more tensor core performance can be gained by reducing or eliminating the dies area devoted to non-tensor compute and higher precision arithmetic. Most of NVIDIA's GPUs are still designed primarily for graphics: they have some fixed function units that can be deleted in an AI-only chip, and a lot of die space devoted to non-tensor compute because the tensor cores don't naturally lend themselves to graphics work (though NVIDIA has spent years coming up with ways to not leave the tensor cores dark during graphics work, most notably DLSS). So the claims that NVIDIA's GPUs are already thoroughly optimized for AI and that there's no low-hanging fruit for further specialization don't seem too plausible, unless you're only talking about the part of the datacenter lineup that has already had nearly all fixed-function graphics hardware excised. And even for Hopper and Blackwell, there's some fat to be trimmed if you can narrow your requirements. reply Sharlin 1 hour agorootparentprevOn the H100 specifically. The figure is likely different on consumer cards. reply choppaface 8 hours agoparentprev“NVidia’s LIES.. On kernels such as flash attention, TMA and the L2 cache are both fast enough so as to hide these problems reasonably well. But to make the full use of the hardware, memory request must be coalesced and bank conflicts avoided ” The depth of the competition is also starting to become apparent. There’s no way the documentation error was totally an accident. Diagrams are the easiest to steal / copy and there must have been some utility for nvidia to have left this in place. Remember when Naveen Rao’s Nervana was writing NVidia Maxwell drivers that out-performed NVidia’s own? Not every documentation mishap in a high-growth product is a competition counter-measure, but given that the researchers spent so long reverse-engineering wgmma and given the China-US political situation of the H100 in particular, it seems NVidia is up to its old tricks to protect its moat. So don’t over-study the H100 peculiarities, as “what hardware does AI want?” really encompasses the commercial situation as well. reply wiz21c 2 hours agorootparentI don't understand. If they document their stuff with errors, it will hurt users, be they chinese or US ? Or is it expected that US users will call Nvidia's to ask for the correct documentation ? reply acka 1 hour agorootparentIt could be a case of classic market segmentation. The lower tier customers get the incomplete or error-ridden documentation, and the upper tier trusted customers^W'partners' get access to the juicy stuff: complete and mostly correct documentation, including stuff intentionally left out of the lower tier package like application notes containing secret hardware handshakes to unlock hidden features, all under strict NDA of course. reply jiveturkey 6 hours agoparentprevhasn't google been building such devices for a decade now? reply yayr 2 hours agorootparentyep, and the main engineers have founded groq.com with an architecture that among others precisely solved the memory management issues reply renonce 9 hours agoprev> NVIDIA’s lies. This is an extraordinarily misleading representation of the actual 128b swizzled wgmma layout. This diagram cost us three weeks of life that we will not get back, hence the public shaming. Wondering if anyone would be surprised that a huge amount of progress in AI is on the engineering side (optimizing matmuls), and that a huge portion of the engineering is about reverse engineering NVIDIA chips reply DeathArrow 2 hours agoparentArchitecture doesn't make a difference. Big enough models trained with big enough data tend to give the same results regardless of architecture. So yes, most advances in AI are mostly due to the fact we can now multiply matrices very fast. reply elcomet 2 hours agorootparentThat's not completely true. The architecture must behave well for scaling, which is not trivial. Basic multi-layer perceptrons do not scale well for example, the gradient will vanish or explode deeper in the network. reply 3abiton 1 hour agorootparentAnd data quality. Ensuring the sourcing and quality is very important to get a good model. reply rfoo 54 minutes agorootparentprevidk, they do give the same results, but given the memory bottleneck it feels like we are at a point when architecture innovations matter again, for example check out DeepSeek V2 tech report, they modded model arch specifically for lower cost inference (by making k/v cache smaller) reply winternewt 43 minutes agoprevI believe that reducing the power consumption and increasing the speed of AI inference will be best served by switching to analog, approximate circuits. We don't need perfect floating-point multiplication and addition, we just need something that takes an two input voltages and produces an output voltage that is close enough to what multiplying the input voltages would yield. reply brap 31 minutes agoparentI don’t know why you’re being downvoted, that’s an active area of research AFAIK reply gitfan86 17 minutes agorootparentMaybe because that is a VERY different problem than the one discussed here. Building a single analog chip with 1 billion neurons would cost billions of dollars in a best case scenario. A Nvidia card with 1 billion digital neurons is in the hundreds of dollars of range. Those costs could come down eventually, but at that point CUDA may be long gone. reply diginova 3 hours agoprevWhat should I do if I want to understand such articles in complete? where to start on the roadmap? reply kolinko 2 hours agoparentThis is a good course on gpu programming. Around 4.0 lesson you’ll get the required basics: https://youtube.com/playlist?list=PLzn6LN6WhlN06hIOA_ge6Srgd... Also, write your own cuda kernel to do vector-matrix multiplication (if you use pycuda, you can focus on the kernel, and write everything else with python). Just tell chatgpt that you want to write your own implementation that multiplies a 4000-element vector by 4000x12000 matrix, and to guide you through the whole process. For renting gpus, runpods is great - right now they have everything from lower tier gpus to h100s. You can start with a lesser gpu at the beginning. reply _spl 32 minutes agoprevIt reminds me of when I first read about superscalar CPU architecture and was amazed. GPUs are really next level. reply DeathArrow 13 minutes agoprevIt would be nice if such improvements find their way in pytorch and scikit-learn. reply perfmode 10 hours agoprevThis article rekindles the joy I experienced during CS 149 Parallel Programming. reply figbert 4 hours agoparentAppreciate the recommendation, will check out the course! reply latchkey 7 hours agoprevReally impressed by the writing style of this post and very much looking forward to this on AMD MI300x. Let me know if you want some time on mine. reply globular-toast 2 hours agoparentGood writing is clear and unambiguous. With speech there is an opportunity to interrupt and ask for clarification. Writing has one chance to get the message across. A reader shouldn't have to consult knowyourmeme.com to figure out what the heck the authors are trying to say. I don't even know what the title means here. That's how far they've missed the mark. reply jsemrau 5 hours agoparentprevReally? It gives me PTSD from the Wallstreetbets days. reply forrestthewoods 4 hours agorootparentI also enjoyed the article's style. I utterly despise \"academic paper speak\". It is, imho, not the most effective style to communicate complex ideas. I find it so much easier to learn from a more casual \"blog post\" or in-person presentation over stiff, rigid academic speak. reply kaycey2022 4 hours agorootparentI find both to be useful in different stages. The casual style is very helpful when starting out. But once I have put in a few weeks or months of study in, then the rigor and preciseness of academic style is good as well. I agree with you in the sense that something has \"died\" in writings the follow academic paper speak these days. Just yesterday I saw an ancient article surfaced by Scientific American and Peter Norvig on System Analysis by Strachey. It uses quite a bit of formal language but is super approachable at the same time. That kind of skill is rarely seen these days. reply imiric 2 hours agoprevHasn't this research been done by teams building NPUs today? E.g. chips built by Groq use an architecture built specifically for AI, which is why they're able to deliver the performance they do. On the consumer side, Apple silicon is also quite capable. I'm not in this field at all, but it seems to me that using general purpose processors that communicate over (relatively) slow lanes can only get us so far. Rethinking the design at the hardware level, and eventually bringing the price down for the consumer market seems like a better long-term strategy. reply phinnaeus 9 hours agoprevFYI the caption of the \"spirit animals\" image says \"canadian goose\" instead of \"Canada Goose\". reply silisili 2 hours agoparentI've only heard people in my entire lifetime call them Canadian Geese. The only time I've ever even seen or heard of Canada Goose/Geese are people on the internet telling others they are wrong. I think it's time to just accept it as correct. reply FearNotDaniel 1 hour agorootparentAbsolutely, it's like living in London and eventually having to accept that tourists will always say \"Big Ben\" when they mean the clock tower of the Palace of Westminster, which encloses the bell whose actual name is Big Ben. The name of the tower is, de facto, Big Ben, and life gets so much easier when you drop the urge to tell people they are wrong all the time... Edit: TIL the tower was properly renamed \"Elizabeth Tower\" in 2012 [0] but I seriously doubt a single person in the last 12 years has ever used that name... [0] https://en.wikipedia.org/wiki/Big_Ben reply xarope 4 hours agoparentprevI am missing the reference to the canadian goose and the retriever puppy as spirit animals. Is that to say the H100 is an ornery thing, but the RTX4090 is friendly? reply adzm 8 hours agoparentprevLikely a regional thing; they are consistently called Canadian Geese where I grew up and where I currently live. reply downrightmike 8 hours agoparentprevDon't worry, the Geese are en route to location, resolution incoming. Stand by. reply bombcar 5 hours agoparentprevIt’s a Canada Goose from Canada. A Canadian Canada Goose, or Canadian Goose. reply gosub100 4 hours agorootparenthttps://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffal... reply bn-l 2 hours agoparentprevWho cares reply wglb 8 hours agoparentprevAn error too often made. reply fastball 9 hours agoparentprevCanadian goose seems better in [current year], to avoid confusion with the clothing brand. reply adrian_b 2 hours agoparentprevI consider bad the habit of English to use nouns also as adjectives, because it causes many ambiguities, some of which can be very annoying, even if they are a rich source of jokes and word plays. In most languages the use of a noun as an adjective is marked, by a particle or by an affix or at least by a different stress pattern (like moving the stress to the last syllable), which removes the ambiguities. So for most non-native speakers \"Canadian goose\" makes much more sense than \"Canada goose\" (which may feel like \"Canada and a goose\" or \"a goose that is also Canada\" and not like \"a goose from Canada\"). reply p0w3n3d 2 hours agorootparentalways the former noun is describing the latter. Butter fly is not a flying butter (as my children's teacher told them to make a joke about butterfly) but a fly made of butter instead. reply kitd 2 hours agorootparentprev\"Canada\" isn't being used as an adjective though. The name of the species is \"Canada Goose\", like \"Long Island Shellfish\" or \"Dublin Bay Prawns\". reply actionfromafar 2 hours agorootparentprevNow you made me think of ways to English Adjective my text for word play... make it stop. reply WanderPanda 9 hours agoprevIs this \"just\" CUTLASS in user friendly? reply roschdal 48 minutes agoprevChatGPT - the largest electricity bill in the world. reply DeathArrow 2 hours agoprevSo do their kernels and library also speed up RTX 4090? reply nmstoker 11 hours agoprevSome related material here too: https://twitter.com/bfspector/status/1789749117104894179?t=k... reply apsec112 9 hours agoprevInteresting! Would this support fp8? Does anyone know how it would compare to Triton? reply jauntywundrkind 10 hours agoprevThe ThunderKittens mascot has great kitten/Sony-Aibo vibes. Nicely generated, AI (I presume). https://github.com/HazyResearch/ThunderKittens reply layer8 10 hours agoparentIt looks off because the head isn’t centered on the neck. reply Satam 4 hours agorootparentEasy fix: https://imgur.com/a/Ahwt6tr (although not sure which one is actually better) reply john_minsk 5 hours agorootparentprevGreat attention to detail! I, like the parent, was surprised by the quality as well. However now I can't unsee it:-) reply cl3misch 2 hours agoprev> The unswizzled shared memory layouts suffer from very poor coalescing If I didn't know any better I'd consider it technobabble reply behnamoh 10 hours agoprevtangential: When @sama talks about \"Universal Basic Compute\" (UBC) as a substitute for Universal Basic Income, obviously he means GPU, right? Who's going to benefit from such policies? Only nvidia? It just seems such a dystopian future to live in: imagine you can sell your UBC to others who know better how to use it, or you can use it to mine bitcoin or whatever. But all the compute is actually created by one company. There are many reasons to hate nvidia, but honestly if this UBC policy is even remotely being considered in some circles, I'd join Linus Torvalds and say \"nvidia, fuck you\". reply jra101 10 hours agoparentYou're blaming NVIDIA for Sam Altman's dumb idea? reply WanderPanda 9 hours agorootparentOne's \"dumb idea\" is another marketers \"genius stroke\". Seems like he is playing the media puppets while he can reply behnamoh 9 hours agorootparentprevnvidia's CEO literally keeps saying \"the more you buy GPUs, the more you save\"—it's hard to believe nvidia has nothing to do with such ideas. reply coffeebeqn 9 hours agorootparentGPU CEO wants to sell more GPUs? What on earth reply WanderPanda 9 hours agorootparentprevHim saying this always puts me off. Gives hard old sales-guy vibes. I really wonder who/which demographic is influenced in nvidias favor by this rethoric. reply callalex 3 hours agoparentprevYou’re looking for logic. The only logic is “when a sucker buys WorldCoin, sama bank account go brrrr”. That’s the whole logic. reply brcmthrowaway 6 hours agoprevNVIDIA needs to be broken up reply silveraxe93 2 hours agoparentNVIDIA is so damn good at its job that it took over the market. There's no regulatory or similar barriers to entry. It's literally that they do a damn good job and the competition can't be as good. You look at that and want to take a sledgehammer to a golden goose? I don't get these people reply michaelt 0 minutes agorootparentTrue: nvidia has been consistently investing for over a decade. They saw there was nascent compute use of GPUs, using programmable shaders. They produced CUDA, made it accessible on every one of their GPUs (not just the high-markup professional products) and they put resources into it year after year after year. Not just investing in the product, also the support tools (e.g. a full graphical profiler for your kernels) and training materials (e.g. providing free cloud GPU credits for Udacity courses) and libraries and open source contributions. This is what it looks like when a company has a vision, plans beyond the next quarter, and makes long-term investments. reply latchkey 5 hours agoparentprevThe better alternative is to root for AMD and others to develop their own products so that regardless of breaking NV up or not, there are alternative solutions for people to use. They all leapfrog each other with new releases now any way. Why put all your eggs into one basket. reply simondotau 4 hours agorootparentGeorge Hotz went down the AMD rabbit hole for a while and concluded that the driver software — more precisely the firmware which runs on the cards themselves — is so badly written that there's no hope of them becoming serious contenders in AI without some major changes in AMD's priorities. reply callalex 3 hours agorootparentEgohotz is brilliant in many ways, but taking him at his word when it comes to working with others has been a mistake since at least around 2010. This is well documented. reply simondotau 1 hour agorootparentWho said anything about taking him at his word? Everything he has done regarding AMD GPUs has been in public. I'm sure there are plenty of valid criticisms one can make of his skills/strategy/attitude/approach, but accusing him of being generally untrustworthy in this endeavour is utterly nonsensical. reply latchkey 4 hours agorootparentprevI'm not defending their software. It does honestly have a ton of issues. George Hotz tried to get a consumer card to work. He also refused my public invitations to have free time on my enterprise cards, calling me an AMD shill. AMD listened and responded to him and gave him even the difficult things that he was demanding. He has the tools to make it work now and if he needs more, AMD already seems willing to give it. That is progress. To simply throw out George as the be-all and end-all of a $245B company... frankly absurd. reply creato 3 hours agorootparentThe fact that consumer and \"pro\"(?) GPUs don't use (mostly) the same software is not confidence inspiring. It means that AMD's already apparently limited capacity for software development is stretched thinner than it otherwise would be. Also, if the consumer GPUs are hopelessly broken but the enterprise GPUs are fine, that greatly limits the number of people that can contribute to making the AMD AI software ecosystem better. How much of the utility of the NVIDIA software ecosystem comes from gaming GPU owners tinkering in their free time? Or grad students doing small scale research? I think these kinds of things are a big part of why NVIDIA's software is so much better than AMD right now. reply wruza 2 hours agorootparentthat greatly limits the number of people that can contribute to making the AMD AI software ecosystem better I’d say it simply dials it down to zero. No one’s gonna buy an enterprise AMD card for playing with AI, so no one’s gonna contribute to that either. As a local AI enthusiast, this “but he used consumer card” complaint makes no sense to me. reply shmerl 3 hours agorootparentprevIndeed, AMD willing to open firmware is something Nvidia never has done. reply simondotau 44 minutes agorootparentprev> To simply throw out George as the be-all and end-all of a $245B company... frankly absurd. I didn't do that, and I don't appreciate this misreading of my post. Please don't drag me into whatever drama is/was going on between you two. The only point I was making was that George's experience with AMD products reflected poorly on AMD software engineering circa 2023. Whether George is ultimately successful in convincing AMD to publicly release what he needs is beside the point. Whether he is ultimately successful convincing their GPUs to perform his expectations is beside the point. reply huhlig 5 hours agoparentprevInto what? Where would you draw such lines? reply robocat 4 hours agorootparentInto tiles ;-p GPU compute is already broken up - there is a supply chain of other cooperating players that work together to deliver GPU compute to end users: TSMC, SK hynix, Synopsys, cloud providers (Azure/Amazon etcetera), model providers (OpenAI/Anthropic etcetera). Why single out NVidia in the chain? Plus the different critical parts of the chain are in different jurisdictions. Split up NVidia and somebody else will take over that spot in the ecosystem. This interview with Synopsys is rather enlightening: https://www.acquired.fm/episodes/the-software-behind-silicon... How does the profit currently get split between the different links? Profit is the forcing variable for market cap and profit is the indicator of advantage. Break up NVidia and where does the profit move? reply uyzstvqs 10 hours agoprevWhat is needed are true NPUs as dedicated co-processors, especially for prosumer desktop systems (devs, other professionals, gamers). GPUs work in the enterprise, but they're a hassle to use for AI on the personal computing side of the market. Especially VRAM limitations, but also the lack of a standard open API other than Vulkan (again, using video stuff for AI). reply dartos 9 hours agoparentFwiw, Vulkan isn’t specifically a graphics api and has had compute specific features for a while now. (Potentially since its inception) reply the__alchemist 8 hours agorootparentCompared to CUDA, Vulkan is... not fun to code compute in! The serialization bridge and duplicating data structures and functions between CPU and GPU is tedious. reply dartos 5 hours agorootparentI hear both CUDA and Vulkan are not fun to code in. But yeah Vulkan is famously verbose. It takes about 1000 LoC to draw a triangle reply KeplerBoy 2 hours agorootparentCUDA is very much fun to code in! Nvidia provides devs with great tools (Nsight Systems and Nsight Compute), so you know where you have to optimize. reply jokoon 9 hours agoprevthis is why people should better study neuroscience, psychology if they want to advance research in AI. also things related to graph topology in neural networks maybe, but probably not related to artificial NN. I was given this video, which I found was pretty interesting: https://www.youtube.com/watch?v=nkdZRBFtqSs (How Developers might stop worrying about AI taking software jobs and Learn to Profit from LLMs - YouTube) reply dartos 9 hours agoparentI don’t think psychology will have any bearing on AI. I doubt neuroscience will either, but I’m not as sure on that. The more impressive AI systems we have moved further away from the neuron analogy that came from perceptions. The whole “intelligence” and “neural” part of AI is a red herring imo. Really poor ambiguous word choice for a specific, technical idea. reply sva_ 9 hours agorootparent> I doubt neuroscience will either, but I’m not as sure on that The stuff on spiking networks and neuromorphic computing is definitely interesting and inspired by neuroscience, but it currently seems mostly like vaporware reply dartos 5 hours agorootparentYep, I’ve heard about spiking networks, but haven’t read into them much yet. reply nradov 9 hours agorootparentprevThe question is whether current AI technologies represent any progress towards a true human equivalent artificial general intelligence. Most likely not, but no one knows for sure. If the answer turns out to be no then real progress will likely require theoretical insights from psychology, neuroscience, and other fields. reply dartos 5 hours agorootparentFwiw, I don’t think we’re any closer to general intelligence then we were 5 years ago. Other than that, I agree, especially since you added “and other fields.” Psychology might eventually give us a useful definition of “intelligence,” so that’d be something. Obviously all research can influence other areas of research. reply fastball 9 hours agorootparentprev*perceptrons reply dartos 5 hours agorootparentDarn autocorrect. Thank you. reply actionfromafar 2 hours agorootparentHaha, I didn't get it when I read \"perceptions\". Thought ... of what? :-D reply chmod775 7 hours agoparentprevI can't seem to figure out the connection between this comment and the article at hand, except that they're both about AI. reply renewiltord 8 hours agoparentprevThere are loads of psychologists and neuroscientists today. Has any of them in the last few years produced anything advancing AI? The proof of the pudding is in the eating so if they have at a higher rate than just straight CS/Mathematics and related then there’s probably some truth to it. reply benreesman 5 hours agoprev [–] It’s stupid, we can get a credible foundation model on a billion weights. If you don’t know this you’re a novice in the field. reply spullara 5 hours agoparent [–] please publish it! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the efficiency of AI computations on NVIDIA GPUs, with a specific focus on the H100 model.",
      "It introduces ThunderKittens, an embedded DSL for creating fast kernels and emphasizes optimizing hardware utilization.",
      "ThunderKittens surpasses current kernels, catering to high throughput applications, and aligns with AI and hardware advancements."
    ],
    "commentSummary": [
      "The article covers the transformation of GPUs into specialized AI devices, incorporating NPUs for AI functions and the future possibility of AI processing units for consumers.",
      "It explores GPU optimization for AI, emphasizes memory management importance in AI hardware, and highlights how data quality and architecture affect AI model performance.",
      "Discussions include hardware design, NVIDIA's significant market presence, NVIDIA vs. AMD software for AI, challenges in CUDA and Vulkan coding, and the impact of psychology and neuroscience on advancing AI technology towards artificial general intelligence."
    ],
    "points": 661,
    "commentCount": 113,
    "retryCount": 0,
    "time": 1715551558
  },
  {
    "id": 40338275,
    "title": "Cap: Free Open-Source Alternative to Loom for Effortless Screen Sharing",
    "originLink": "https://Cap.so",
    "originBody": "Beta v0.2.3 Product Pricing Updates Help Get started for free Announcing Cap Public Beta Effortless, instant screen sharing. Cap is the open source alternative to Loom. Lightweight, powerful, and stunning. Record and share in seconds. Get started for free No credit card required. Beta v0.2.3 Cap is the open source alternative to Loom. Lightweight, powerful, and stunning. Record and share in seconds. © Cap Software, Inc. 2024. Terms of ServicePrivacy Policy Product Updates Pricing Download Open Source Join the community Help FAQs Email Support Chat Support System Status",
    "commentLink": "https://news.ycombinator.com/item?id=40338275",
    "commentBody": "I made an open-source Loom alternative (cap.so)250 points by mcilroy 10 hours agohidepastfavorite123 comments toastal 13 minutes agoIs the ‘free’ part of open source not a priority? There is an AGPL license, but it looks like future contributors will be required to use proprietary services for this helping this project with Discord & Microsoft GitHub as the only communication options. If FOSS is good enough for your project, it should be good enough for the tools used to build your community… reply mihaic 5 minutes agoparentThat seems overy ideological. While the software itself might be good enough (it often isn't, as that's why these products are being developed), hosting it is a work in itself that can be better spent on the project itself. reply ascorbic 3 hours agoprevSome feedback: - You refer to it as \"screen sharing\", but that makes me think of live sharing like Zoom etc. Wouldn't \"screen recording\" be a better description? - The \"get started for free\" text in the header CTA flows onto a second line for me - It would be good to have an embedded demo video on the landing page reply master-lincoln 12 minutes agoprevI have no idea what Loom is and you don't really explain what your product can do upfront for people who don't know Loom. I'll pass reply rrr_oh_man 8 minutes agoparentThen you might not be in the target audience, which is totally fine… reply KingOfCoders 6 hours agoprev\"This pricing will be locked in for the lifetime of your subscription.\" Highly doubtful, I have been cheated out of lifetime guarantees so many times. Latest when the company is sold. reply RamblingCTO 1 hour agoparentYou're also robbing yourself of increasing the prices to keep the lights on. Business-wise this is not a very good idea. Also as a customer I have the feeling that this is a stupid idea. If the costs increase, you have to increase prices. If I like something and want to keep using it, that's the only way. reply KingOfCoders 1 hour agorootparentAs a rambling CTO myself, I admire your nick name. reply geor9e 9 hours agoprevI love it. I forget why I quit using Loom but you're my new default now. One feature request: When I press \"Start Recording\" via the browser, I don't want it to immediately start recording that screen. Give me a 3…2…1…Go with a sound so I can minimize my browser and show what I want to record. I don't all my videos to start with me fumbling for the Start button. reply aitchnyu 5 hours agoparentWonder whats the status of AI which knows which durations can be edited out since you were fumbling with mouse movements and voice. Bonus points if it detects \"takes\" of the same scene. Around 2005, Linux users were recording TV on their PCs and detecting/skipping ads automatically and soon after, podcast apps were skipping lulls in the voice. reply mcilroy 9 hours agoparentprevThank you so much! 100%, this is a priority. Should have it done soon, as well as a \"startover\" button. reply pomtato 6 hours agorootparentHeck make it even better by making that an option, so the user can set the countdown to their preference. reply codetrotter 10 hours agoprevHow does it compare to using either of the following on macOS - OBS - The built in screen recording functionality in macOS itself - ffmpeg (see http://trac.ffmpeg.org/wiki/Capture/Desktop for details on doing screen capture with it) reply geor9e 9 hours agoparentLoom is software that, in one click, overlays your face in a bubble as you can narrate your screen recording, uploads it as it's recording, and when you press stop it copies a link to your clipboard you can share immediately with whoever you are showing. It's a very polished and complete experience. OBS is for streamers. It could be set up in a similar way if you already have a twitch channel hooked into it. It won't be as polished and easy. macOS doesn't have any upload or overlay function so it's not comparable. ffmpeg is ffmpeg reply rrr_oh_man 6 minutes agorootparent> ffmpeg is ffmpeg What an excellent comment :D reply camhart 8 hours agorootparentprevOBS works great when recording locally. Sure it works for streamers, but it also outputs to standard video formats easily. You don't need a twitch channel or anything like that. You'll need to figure out where to host the video after the fact (dropbox works well enough, as it allows the viewer to stream without downloading). reply benoliver999 1 hour agorootparentYeah I fired up OBS for the first time and it was surprisingly easy to get set up. Every time I had a 'right how do I do x?' moment I figured it out in seconds. reply cyclotron3k 6 hours agorootparentprevI used loom for the first time today, not only did it do all the things you mentioned, but it perfectly transcribed what I said, then allowed me to cut out sections of the video just by deleting words and pauses from the transcription. I thought that was pretty cool. reply nozzlegear 9 hours agorootparentprevmacOS does have overlay functionality, it’s called presenter mode or something like that. Look for it in the green camera icon that shows up in the menu bar when you’re sharing your screen. It puts your face in a bubble overlayed on top of whatever you’re sharing, as you said. You’re right it doesn’t have upload though. reply DaiPlusPlus 8 hours agorootparent> You’re right it doesn’t have upload though. That's surprising considering how Apple pushes iCloud integration... reply brailsafe 1 hour agorootparentIt may not have upload, but in any worthwhile collaboration solution you can literally just drag your screen recording into the rich text area of a comment and have it show up, maybe after converting it to a neutral codec. I did this in my last job with Jira, GitHub, and Slack, ez reply stephenr 6 hours agorootparentprevApple also pushes privacy and on-device processing; But as for the feature itself: why on earth would you want a video uploaded somewhere else before you've even had a chance to watch it back at least once? reply zztop44 4 hours agorootparentBecause you’re showing a colleague how to use a simple inventory tracker that you’ve created in Google Sheets. Or how the new multilingual Figma templates work. Or how to batch rename a folder full of files using the command line. Or whatever, really. You already know what’s in the video because you literally just recorded it. And it doesn’t need to be polished because it’s probably just going to one coworker, who will only view it once. reply j45 4 hours agorootparentprevOBS can record videos pretty similarly, and automatically save, stream, or even be compelled to upload it somewhere. I'm not sure if there's a simple way to get the link of the video hosted online back into your clipboard automatically. That is one feature Jing was superior to Loom on. reply mcilroy 10 hours agoparentprevWith any of those you need to upload the video directly into Slack, or something like YouTube, Vimeo, etc. Plausible for short videos, but can become a pain if recording for more than a few mins. If you record with Cap, we'll auto handle the segmentation of the video so it can be played back efficiently (so the whole video doesn't have to be download in one go). You can also receive comments on your Cap link, reactions, and see the analytics data. It just makes it easier. Do your recording, then receive your shareable Cap link in just a few seconds. reply codetrotter 10 hours agorootparentThat makes sense. How about if I already run a PeerTube instance. In that case I already have segmentation of videos so people can watch without downloading the whole file, and they could also comment on the video on my PeerTube instance if I allowed them to. How does Cap compare to using a PeerTube instance to host screen recordings? reply twobitshifter 9 hours agorootparentThis is like the classic Dropbox comment. It’s about ease of use and the fewest steps. reply codetrotter 9 hours agorootparentYeah, but not quite. I’m not dismissing their product. Just wondering if it would be worth running a Cap server when I already have tools to make screen recordings, and I already have a PeerTube instance where I can host videos. And actually now I am also wondering if Cap could be integrated with PeerTube so you could have which ever things Cap brings to the table aside from video hosting and have it either upload to PeerTube by API, or even having some version of Cap existing as a sort of plugin into PeerTube so that you could use it from within PeerTube reply thorncorona 7 hours agorootparentIf you already have a working workflow, this is probably overcomplicating things. I would imagine the overlap of people who would use peertube and Cap is 0. reply 2Gkashmiri 6 hours agorootparentprevYeah. It can be as simple as \"video saved. Do you want to submit to peertube?\" And done. Here is a link. reply j45 4 hours agorootparentprevIt is better than peertube because it doesn't require an extra piece of software to be learned and setup and maintained to host and deliver recorded videos. The 98% will not self host. reply sedatk 6 hours agoparentprevWindows also has built-in screen recording. reply j45 4 hours agoparentprevWay simpler and quicker than those options. Being able to reply to an email with a video quicker than typing is often a line I consider. reply blackeyeblitzar 10 hours agoparentprevOne of the benefits of Loom is the convenience of video messaging. So it’s less about screen recording in the same way as OBS but more about replacing Slack with something more efficient. I’m not sure Cap is a replacement for Loom in that sense. reply j45 4 hours agorootparentI thought so too until I used the \"video messaging\" of loom and found it to be a reorganized use of existing loom features instead of first class video messaging. Video messaging can also waste both parties times if the videos are too long compared to the message. Video messaging can be infinitely priceless where a video can explain quicker what words can't. Other platforms have been able to get a bit further, I think there is one made by vimeo or someone used for sales that is quite good at the video messaging itself. reply memset 6 hours agoprevFeedback: can there be a big “loom” on the homepage showing how it works or what differentiates it? Right now we only see tweets but nothing that gives me the “aha” moment to want to try it out. Make me remember the pain, and show me really clearly how your software will fix it right now. Screen studio does a good job of this. reply j45 4 hours agoparentAn animated gif video with a ticker on the side going through each step would be great. Also being able to launch recording it without an account would be a great way to get someone to create a video, invest enough to create an account instead of come back to it later. Veed.io seems to have used this approach to great success. reply clan 1 hour agorootparentThat is a really dark pattern which I personally hate. Sucker me in and then raise the barrier. It is on the same level as those \"create free account\" but nothing is actually free except for the account creation process. We have word for this: Demos. It could be a \"free demo\" account or an \"account free\" demo. Two different but easily understandable things which clearly communicates intent and value. Nothing Sneaky and no hiding weasels. It just triggered me that this marketing funneling newspeak actually has become commonplace and generally accepted. Change the wording to \"launch demo (recording)\" and communicate it clearly will make this grumpy fart happy. And yes: There is a subtle difference and therein lies the darkness. The video and ticker are however great ideas as-is without me complaining. reply re 8 hours agoprevI'm curious about your plans around open-source, since you're using it as a selling point. There's been a lot of discussion about projects that start as open-source but then go to a more restrictive license once other companies start offering their own hosted versions. Is that a concern for you? Do you plan to include any/all of the \"pro\" features on the roadmap in the open-source code? reply lolinder 8 hours agoparentNote that they appear to have started with AGPL, which goes a long way towards preventing the kinds of uses that prominent license switches were designed to prevent. reply singron 6 hours agorootparentAlso it looks like they don't (yet) have a copyright assignment, which would make it exceedingly difficult to relicense if they got significant external contributions. reply xyst 8 hours agoprevIf there’s one way to turn off a good portion of people from using your project, it’s by locking down the community into a discord. reply Sytten 7 hours agoparentOn the contrary everybody is on discord and not being on it will turn off the vast majority of people cause I wont open matrix just for your software. reply 2Gkashmiri 6 hours agorootparentAnd people will not use discord for the same gatekeeping reason. Like I have tried discord a few times but I just cant \"feel at home\" with it. Its like too much content at once. You can have bridge to discord from matrix so that solves one problem for now but again, discord and matrix feel like eons apart. reply shivaraj1996 5 hours agoparentprevHighly disagree! Though I would like the community to not be on discord, every other competitor has lost to discord. reply j45 4 hours agorootparentSlack? I often wonder if a mattermost community should be backstop an integrate into a slack and discord community to let people use what they want, but you have your own history owned. Discord seems nice but it also has to fit the audience. I find it convoluted for business or support type chats, but it is excellent for community building around digital experiences (like gaming) reply BadHumans 7 hours agoparentprevI wish you were right but you aren't. reply recroad 8 hours agoparentprevWhat’s wrong with discord? I’m on several servers for open source projects and it’s pretty good. reply mcilroy 8 hours agoparentprevah sorry to hear that! where would you prefer it? reply xyst 8 hours agorootparentMatrix for IM, discourse for longer discussions, … Plenty of open source alternatives. Can even bridge between matrix channels and discord reply paulgb 8 hours agorootparentI wish an open standard won, but the reality is that in 2024 even in open source, it’s going to be easier to grow a Discord community than a Matrix one because more people habitually use Discord. reply refulgentis 8 hours agorootparentprevMatrix lost, or never even won, outside some unusually neckbeard-y communities. I gotta admit I kinda enjoyed watching someone try to make normies use it. Impossible. To be fair, discord is hard. But Matrix was really impossible. reply JasonSage 9 hours agoprevHow do you quantify “lightweight”? This is used in a couple places, but without additional information it’s essentially meaningless. I’m starting to think that this is a red flag when looking at a project when there’s no further information. It’s marketing-speak masquerading as a technical attribute, but without additional information could just be 100% incorrect. It’s also overly-generic… depending on the intent of the author, lightweight could mean CPU-efficient, small binary, feature-light, and so on, but when not qualified further it seems to be implicitly all of these and more, but most likely that’s not accurate. My marketing pet-peeve aside, this looks nice and I will be trying it out. :) Congrats on your HN launch! reply mcilroy 9 hours agoparentGood point. Will definitely take another look at the copy! I'm referring to it being a small bundle size to download :) reply klabb3 7 hours agorootparentNote that hackernews is notoriously anti “marketing speech”. I don’t think there’s anything bad with elaborating but lightweight/snappy/slim is pretty much what’s important, given that there’s so much slow bloatware out there. Not all users are technical, and eg throwing out benchmarks on the front page would be complete mumbo jumbo to many users. That said, I also think the best way to communicate the intended vibe is also through screenshots (which you already have) and design. Looks awesome. reply mcilroy 10 hours agoprevhey everyone! Cap is still a little rough around the edges but we'll get there. Working as quick as possible on feedback / new features and fixes. Self hosting is coming soon, as well as the option to connect your own storage option (S3 for now). appreciate you taking the time to check it out, and happy Sunday! reply joshgermon 9 hours agoparentCongrats on the product - looks great! I'd love to hear more about your experience using Tauri for this? What are the pros/cons and do you feel like it was the right choice so far? reply j45 4 hours agoparentprevKeep it up. Shipping is the hard part. If every commenter had to admit the last time they shipped the context and perspective of the comments migth surprise you. S3 Storage sounds amazing, especially if you can edit the URL and redirect it to backblaze or other s3 compatible stores. I'll be looking forward to trying out the self-hosting to be able to host links from my own domain. reply b20000 6 hours agoprevI thought this was about the adventure game Loom but no. reply dbalatero 6 hours agoparentAsk me about LOOM reply fatcow 5 hours agorootparentYou fight like a dairy farmer. reply labster 3 hours agoparentprevThat’s the distaff counterpart. reply sedatk 6 hours agoparentprevSo, this isn’t SCUMM reply RileyJames 9 hours agoprevReasons I use Loom: - Social features, users can comment & engage with my video to provide feedback - Analytics, I get notifications when a user views a video (useful for demos/sales) - Post-capture editing - Set 1.X speed by default on videos (significantly reduces video length) I don't find the capturing element of loom very good. Their chrome integration is pretty bad, seems to not handle clicking/changing pages when capturing a tab. I recently started using Screen Studio and have really liked it. reply Flammy 6 hours agoparentI've found some of Loom's auto-descriptions (including previews in Slack) to be pretty solid use case for AI. reply mcilroy 9 hours agoparentprevNice! Some good insights thank you. reply discordance 1 hour agoprevStill can't believe Atlassian paid $1B for Loom. Best of luck with your version! reply thepaulmcbride 2 hours agoprevIf you had a lifetime subscription of around $250, I’d buy it instantly. reply martinesko36 8 hours agoprevCongrats on the launch! It looks quite slick and something I could see myself using. I'm curious, what do the costs of operation and margins look like for a service like this? reply tylerlaws0n 5 hours agoprevIsn't this just the same thing as kap getkap.co except that this cap is not free? reply jak2k 3 hours agoprevWhy just Mac and not Linux or NT? reply mavelikara 2 hours agoparentFrom https://github.com/CapSoftware/Cap: > Cap is currently in public beta, and is available for macOS and Web. Windows and Linux builds are in development. Join the Cap Discord (https://discord.gg/y8gdQ3WRN3) to help test future releases and join the community. reply 4star3star 6 hours agoprevSome feedback on your site's UX - on desktop, I don't expect hovering on a menu item to open a new context, so by habit I click on it. You're opening on hover, though, and my click is closing the context immediately. reply mcilroy 6 hours agoparentthanks for this! agreed, will fix. reply 8mobile 4 hours agoprevhi, it's open source and not free. the Lite plan allows you to do the same things as Loom it seems. I would have preferred a video showing the app. reply genpfault 9 hours agoprev> open-source Loom alternative ...ScummVM[1]? [1]: https://wiki.scummvm.org/index.php/Loom reply nevster 9 hours agoparentAsk me about Loom reply Enderboi 8 hours agorootparentThis is the comment I came here for :) reply nevster 9 hours agoparentprevThat was my immediate thought.... reply LegitShady 8 hours agoparentprevYour name will be cursed forever, son of cygna, loom child...BOBBIN THREADBARE! reply bcye 9 hours agoprevThe landing page only really highlights the video recording aspect, it would be interesting to see imo how it replaces loom in what comes after the recording (even if it's still WIP). reply mcilroy 9 hours agoparentGood point! Will work on this. After you stop recording you are instantly redirected to your shareable Cap.link. Working on some new points to the landing page now :) reply skinner927 8 hours agorootparentI have no idea what loom is, so describing Cap as an open-source loom tells me nothing. There’s likely dozens of people like me, haha. reply mcilroy 8 hours agorootparentYeah that's a good point. It's a good way for us to get started with people familiar of Loom, but it's not future proof for sure. reply arnavbathla 9 hours agoprevThanks for building this! Making this my default for demos. reply paulcole 9 hours agoprev> Lightweight, powerful, and stunning What about it is stunning to you? The Steve Jobs-ification of marketing copy has hit epidemic proportions. reply martinesko36 8 hours agoparentThis is not a nice comment, but the poster is right. You can get rid of that whole line. Keep it simple. reply stevelacy 9 hours agoprevFollowing your build statuses for a while, congrats on the launch! reply mcilroy 9 hours agoparentthank you Steve, appreciate that! reply kaiwenwang 6 hours agoprevI like your buttons! reply api_or_ipa 5 hours agoprevLove it. I hate loom. Dark patterns abound. Its time we have a better orientated replay alternative (but I’m still waiting for a widely adopted true session replay alternative) reply rajdeep_das 3 hours agoprevReally nice reply brendonjohn 4 hours agoprevPrincipal engineer for Loom here: Congrats on the beta launch. Super cool, I’m looking forward to trying it out. Welcome to the Thunderdome. reply dbish 4 hours agoparents/Loom/Atlassian reply sureIy 4 hours agoprevSide question: can anyone tell me the point of Loom? My team uses it to record a screencast and then paste the link on Slack or GitHub. I do the same but I don’t need extra software because my OS already has the functionality and both services accept uploads. reply JasonSage 4 hours agoparentThe auto-transcription into text is the big thing. I personally find that most of the time when a coworker posts a video I don’t even need to watch it to get all the information I actually needed. One could argue that what you’re doing is inconveniencing your coworkers and ensuring anybody using a screen-reader can’t actually consume your videos. reply humanlity 5 hours agoprevfrankly, if a util needs login first, it may not be worth trying reply xahrepap 6 hours agoprevSome honest feedback: describe your product/tool without using a competitor’s name. I’m on mobile and the landing page only tells me “recording software” and “Loom” before it drops into Twitter like 3rd party comments. I honestly have never used Loom. I don’t know what it is. Do you really want me to have to go look up “the competition” just to know who you are? reply memset 6 hours agoparentI disagree - the fact you don’t know what Loom is probably means you’re not in the market for this kind of software. Better, at this stage, to focus on users who will immediately get it rather than explain to folks who don’t currently have a need. reply muglug 6 hours agorootparentPeople may well use Loom-like functionality that’s integrated into other tools they use without knowing the name “Loom”. Loom had a head-start, but other companies (e.g. Vimeo) copied the feature pretty quickly. And also, more importantly, this is a “Show HN” post, so reasonable for people here to question the comparison. reply bestnameever 5 hours agorootparentLoom in the title got my attention. It is the only reason I clicked on the link. It is unfortunate this is being discussed rather than the product. reply Flam 5 hours agorootparent+1 same. If this was a generic screen recording software I’d have skipped it. reply Moru 3 hours agorootparentprevSame here. However I was very disappointed because it was nothing like what I was expecting from the name. reply j45 4 hours agorootparentprevI think that might be true for a small group of people. Calling it a replacement screen recording tool like Loom is probably ideal to hit both markets. Loom made the work around the recording step easier, something that other options seem to be adopting. Still, lots of people recorded lots of screens before Loom, kind of like loom. Camtasia, Jing, and plenty of other tools out there. Jing in particular would record and upload automatically and put the link to the video in your clipboard, years in advance of Loom registering potentially questionable patents to parts of that. Loom was ultimately acquired by Atlassian as a video capture tool for JIRA, etc an won't likely grow much feature wise for the masses. It will be great for that. I'm still a paying customer of loom. There are a lot of features missing in Loom though that is going to spark a lot of alternatives to continue from where Loom left off. If anything Loom's growth seemed to have stalled relative to how much they were acquired for. Average people are using apps like tiktok to produce videos instead. Calling it a screen recording tool is a good thing. A lot of people don't even know how easy it is to do a screen recording on a computer. reply bitcharmer 4 hours agorootparentprevLoom does have a meaning for me because I follow innovation in Java. This is not it. Would be good to have a solid description of what it is. reply avodonosov 6 hours agoparentprevYes, I thought of Java virtual threads - the Project Loom. Have never heard of the other Loom. reply gmueckl 5 hours agorootparentWhen I heard of Loom, my first thought went to that classic Lucas Arts adventure game. The name is shared by so many different things, it isn't descriptive of anything. reply blkhawk 3 hours agorootparentyes same for me - I thought it was a a reimplemention at first :) reply mcilroy 6 hours agoparentprevDefinitely agree with what you're saying. It's easier for us at the start to get some attention from Loom specific users by mentioning Loom directly, but it's not a sustainable strategy going forward. Will update the marketing copy etc over the coming weeks/months to be more about the problem we're solving specifically. reply xahrepap 6 hours agorootparentYeah. I get that. And I’m sure there’s some SEO-style positive feedback you get for it. But also being able to pitch it to me in a simple way and then using Loom as an example I think would be more helpful. “Screen recording software that emphasizes simple sharing. An open source alternative to Loom”. Still super simple but is more useful to people less exposed to your world. reply pooriar 6 hours agoparentprevI think it's really useful to describe your product as an alternative to a well known product. Most people will instantly get it, much more so than a long re-definition of the original product. reply stephenr 6 hours agorootparentApparently not that well known - I've never heard of it either. reply ezconnect 6 hours agorootparentprevI have no idea what Loom is, I also have the same issue I didn't tell me what the product is and just got confuse and didn't explore further. reply ARandomerDude 6 hours agorootparentSame. I’ve never heard of Loom and (therefore?) I’m honestly not sure how this would be different on a Mac than a QuickTime screen recording. And since the promo images are from a Mac, my first thought is Quicktime is probably safer. reply memset 6 hours agorootparentJust to address this comment chain: loom does make it easier to do screen recordings and demos, and additionally has features like timestamped comments and notes and sharing functionality. The target customer is a business who is making demo videos and needs these kinds of tracking/collaboration/commenting features. Folks interested in open source maybe want a better offering, don’t want to be locked in, want more favorable pricing, or want a self hosted version. As someone who has used loom, the value proposition is clear from the title without even going to the website. reply gmueckl 5 hours agorootparentIt wouldn't hurt if the key sentence from your first paragraph was in the top two paragraphs of the landing page. reply ec109685 8 hours agoprevInterestingly ycombinator passed on Cap due to them not having a co-founder: https://x.com/richiemcilroy/status/1788996498081951848?s=46 reply JasonSage 3 hours agoparentWhat's missing (maybe for brevity) is why it's important to have a co-founder. I find it hard to believe that they just wanted that box ticked and more that there's something else a co-founder brings to the table... so what is it? What is _actually_ missing in the formula for them? Also [0]. > \"probably not good enough / won't get in\" - you'd be surprised at how many people feel that way, whether because of impostor syndrome, lack of credentials, whatever—and often they turn out to be among the best founders. So this a terrible reason not to apply! Maybe it's not a terrible reason not to apply. /shrug 0: https://news.ycombinator.com/item?id=40091622 reply cco 6 hours agoprevThe key innovation of Loom, and of course I'm using that term loosely, was the inclusion of a human face in a circle at the bottom left of your video. We're monkeys, and monkeys like to see another monkey's face while they're hearing someone talk. It helps with communication all around, nuance, recall etc This is the same \"killer feature\" of Twitch (or more generally video game streaming). So that's all to say, I really recommend you include that in your landing page somehow. Screen sharing is frankly meh, a slide deck is fine, you can use Quicktime or similar to share. But the key is automatically overlaying a circle of your webcam on top of whatever you're recording to share. I suppose the other feature of importance is a simple URL to share it. I don't see if that's possible via Cap. How do I share these videos to my colleagues? reply j45 4 hours agoparentThe simple URL is valuable. Loom still came up short against a predecessor, Jing, which would also copy the url into your clipboard. You hit stop recording, open an email, paste, and write your message and send. No need to wait while it was uploading, etc. reply neerajdotname2 8 hours agoprev [–] We built neetoRecord (https://neeto.com/neetoRecord) which is also a Loom alternative. Full disclosure - I'm part of the team that built neetoRecord. reply xyst 8 hours agoparent [–] It’s poor taste to hijack another “show hn” post. Probably not against the rules but definitely frowned upon. Similar to proposing to significant other at a friends engagement party. reply j45 5 hours agorootparent [–] Maybe. I agree it could have been worded different including giving some constructive feedback on it.. Ppl who discover a category of software might hesitate to try it out while looking for alternatives to compare it against. It also helps the author be aware of other features that might be of interest to them. Cap.so would still remain my first option to try out. It's unique compared to another good screen capture, screenity.io, which is more locally run in the browser. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cap has launched a new version featuring updated product pricing and a free start without needing a credit card, positioning itself as an open-source option similar to Loom for seamless screen sharing and recording.",
      "Users are able to engage with the community, find answers in FAQs, and receive support via email or chat."
    ],
    "commentSummary": [
      "Users are exploring Cap.so, an open-source alternative to Loom for screen recording, analyzing its features, functionality, and user interface.",
      "Cap.so is being compared to other tools like OBS and macOS built-in options, with discussions on hosting platforms like PeerTube and integration possibilities.",
      "Feedback includes appreciation for Cap's simplicity and design, along with worries about misleading marketing and recommendations for enhancements, contributing to a constructive technical discussion."
    ],
    "points": 251,
    "commentCount": 123,
    "retryCount": 0,
    "time": 1715555348
  },
  {
    "id": 40337036,
    "title": "The Challenge of Alternative Software Implementations",
    "originLink": "https://pointersgonewild.com/2024/04/20/the-alternative-implementation-problem/",
    "originBody": "Skip to content Follow: RSS Twitter Pointers Gone Wild A blog about programming, technology and society. Home About Me GitHub Tags coding, compiler, javascript, programming, python, ruby The Alternative Implementation Problem April 20, 2024 In this post, I want to talk about a dynamic that I’ve seen play itself over and over again in the software world. In fact, I would venture a guess that this kind of situation probably happens in the hardware world as well, but I’ll speak about software systems since this is where my experience lies. This discussion is going to touch a bit on human psychology, and outline a common trap so that you can hopefully avoid getting stuck in it. Most of my career, both in academia and industry, has been spent trying to optimize dynamically-typed programming languages. During my master’s I worked on a simple optimizing JIT for MATLAB. For my PhD I worked on a JIT for JavaScript. Today I’m working on YJIT, an optimizing JIT for Ruby which has now been upstreamed into CRuby. During my PhD, while working on my own JavaScript JIT, I read many papers and blog posts about JIT compilers for other dynamic languages. I read about the design of HotSpot, Self, LuaJIT, PyPy, TruffleJS, V8, SpiderMonkey, and JavaScriptCore among others. I also had the chance to interact with and meet face to face with many of the really smart people behind these projects. One of the things that struck me is that the PyPy project was kind of stuck in a weird place. They had developed an advanced JIT compiler for Python which could produce great speedups over CPython. By all accounts many people could benefit from these performance gains, but PyPy was seeing very little use in the “real world”. One of the challenges that they faced is that Python is a moving target. New versions of CPython come out regularly, always adding many new features, and PyPy struggles to keep up, is always several Python versions behind. If you want you Python software to be PyPy-compatible, you’re much more limited in terms of which Python features you use, and most Python programmers don’t want to have to think about that. Reading about LuaJIT, I found that it was and still is highly regarded. Many people regard its creator, Mike Pall, as an incredible programmer. LuaJIT offers great performance gains over the default, interpreted Lua implementation, and has seen some decent adoption in the wild. However, I again saw that there are a number of Lua users who do not want to use LuaJIT because the Lua language keeps adding new features and LuaJIT is several versions behind. This is a bit strange considering that Lua is a language that is known for its minimalism. It seems like they could have made an effort to slow down the addition of new features and/or coordinate with Mike Pall, but this wasn’t done. Almost 4 years ago, I joined Shopify to work on Ruby. For some reason, the space of Ruby JITs has been particularly competitive, and there had been a number of projects to build Ruby JITs. The TruffleRuby JIT boasted the most impressive performance numbers, but again, had seen limited deployments. There were some practical reasons for this, the warm up time of TruffleRuby is much longer than that of CRuby, but I also saw a similar dynamic to that of PyPy and LuaJIT, where CRuby kept adding features, and TruffleRuby contributors had to work hard to try and keep up. It didn’t really matter if TruffleRuby could be quite a bit faster, because Ruby users would always view CRuby as the canonical implementation, and anything that wasn’t fully compatible wasn’t seen as worthy of consideration. Hopefully, at this point, you see where I’m going with this. What I’ve concluded, based on experience, is that positioning your project as an alternative implementation of something is a losing proposition. It doesn’t matter how smart you are. It doesn’t matter how hard you work. The problem is, when you build an alternative implementation, you’ve made yourself subject to the whims of the canonical implementation. They have control over the direction of the project, and all you can do is try to keep up. In the case of JITted implementations of traditionally interpreted languages, there’s a bit of a weird dynamic, because it’s much faster to implement new features in an interpreter. The implementers of the canonical implementation may see you as competition they are trying to outrun. You may be stuck trying to ice skate uphill. Almost 4 years ago, with support from Shopify, two dedicated colleagues and I started a project to build YJIT, yet another Ruby JIT. The difference is that we made the key choice to build YJIT not as an alternative implementation, but directly inside CRuby itself. This came with a number of design tradeoffs, but critically, YJIT could be 100% compatible with every CRuby feature from the start. YJIT is now the “official” Ruby JIT, and is deployed at Shopify, Discourse and GitHub among others. If you’ve visited github.com today, or any Shopify store, you’ve interacted with YJIT. We’ve had more success than any other Ruby JIT compiler so far, and compatibility has been key in achieving this. You may read this and think that the key lesson of this post follows the old adage that “if you can’t beat them, join them”. In some ways, I suppose it does. What I want to say is that if you start a project to try and position yourself as an alternative but better implementation of something, you are likely to find yourself stuck in a spot where you’re always playing catch up and living in the shadow of the canonical implementation. The canonical project keeps evolving, and you have no choice but to follow along with limited decisional power over where your own project is headed. That’s no fun. You may have better luck trying to join up with the canonical implementation instead. However, that’s only part of the answer. In the Ruby space, there is also Crystal, a Ruby-like language that is statically compiled with type inference. This language is intentionally not Ruby-compatible, it has chosen to diverge from Ruby, but has still seen limited success. I think this is interesting because it gives us a broader perspective. Rubyists don’t like Crystal because it’s almost-Ruby-but-not-quite. It looks like Ruby, syntactically, but has many subtle differences and is very much incompatible in practice. This just confuses people, it breaks their expectations. Crystal probably would have had better luck if it had never marketed itself as being similar to Ruby in the first place. Peter Thiel has a saying that “competition is for losers”. His main point is that you shouldn’t put yourself in a position where you’re forced to compete if you don’t have to. My advice to younger programmers would be, if you’re thinking of creating your own programming language, for example, then don’t go trying to create a subset of Python, or something superficially very close to an existing language. Do your own thing. That way, you can evolve your system at your own pace and in your own direction, without being chained by expectations that your language should have to match the performance, feature set, or library ecosystem of another implementation. I’ll finish with some caveats. What I said above applies when you have a situation where there is a canonical implementation of a language or system. It doesn’t apply in a space where you have open standards. For example, if you want to implement your own JSON parser, there is a clearly defined specification that is relatively small and doesn’t evolve very fast. This is very much something you can achieve. You also have a situation where there are multiple browser-based implementations of JavaScript. This is possible in part because there is an external standard body that governs the JS specification, and the people working on the JS standard understand that JIT-compiled implementations are critical for performance and guide the evolution of the language accordingly. They are not in the game of adding many new features as fast as possible. Share this: Twitter Reddit Facebook Like Loading... From → Programming, Programming Languages 2 Comments Mike S. permalink I think your post raises the larger issue that the industry prizes innovation over stability, and we all suffer as a result. Cryptography APIs, unfortunately, have no choice but evolve as flaws are discovered and fixed. But aside from tremendous (and impressive) speed improvements, does Ruby 3.2 or newer truly have killer features over 2.6? Do Python users absolutely need the features in the latest CPython point release over Pypy? I’m not saying it’s practical to pick an older version and standardize on it. It often isn’t. I’m just lamenting the state of the industry. Reply stuartcw permalink In a slightly different realm. In the 1990s I worked on the international version of Lotus 1-2-3 for Windows and though Lotus 1-2-3 was king on DOS, Excel had become, by then, the canonical spreadsheet on Windows. Every time a new version of Excel was released the 1-2-3 team had to add the same features to catch up in order to be able to import the Excel sheet in. For example, Excel could add a new dotted line type and immediately 1–2-3 would not be 100% compatible. Compatibility was sold as the top feature and many loyal customers were persuaded to give up Lotus Suite and move over to Microsoft. Once Microsoft had achieved dominance then there was a bit of a backlash against new features in Excel as customers were suffering from version incompatibilities within the multiple versions of Excel and other software that they had. It was obvious then, that once you were forced to play catchup to someone else the game was over. I believe that the financial functions in Excel were originally pitched to Lotus by an outside developer who had created them as a plugin. This was not taken up, so he pitched them to Microsoft who included the whole new set of functions in Excel. Lotus were now on the back foot and hired the same developer as a consultant to help them implement them in Lotus 1-2-3 for compatibility with Excel! As you say, playing catch-up is a mugs game. Reply Leave a comment The Race to Make Humans Obsolete » @Love2Code on twitter My Tweets Recent Posts The Alternative Implementation Problem The Race to Make Humans Obsolete Human-Scale vs Asymmetric Social Media Preparing UVM for 3D Graphics Software Bugs That Cause Real-World Harm Memory, Pages, mmap, and Linear Address Spaces Building a Minimalistic Virtual Machine Typed vs Untyped Virtual Machines Category Cloud Art Artificial Intelligence Artificial Life Assembly Biology Compilers Creativity D Language Electronics Future Game Development Grad studies Graphics Higgs JavaScript Microcontrollers Microprocessors Music Programming Programming Languages Psychology Rant Robotics Tachyon Technology Uncategorized UVM work x86 Zeta Archives Archives Select Month April 2024 (2) December 2023 (1) November 2023 (1) May 2023 (1) March 2023 (1) February 2023 (1) June 2022 (1) May 2022 (1) March 2022 (1) February 2022 (2) January 2022 (2) December 2021 (1) June 2021 (1) February 2021 (2) December 2020 (1) September 2020 (1) January 2020 (1) November 2019 (2) October 2019 (2) June 2019 (1) January 2019 (1) December 2018 (1) May 2018 (2) February 2018 (1) January 2018 (1) December 2017 (1) October 2017 (1) June 2017 (1) April 2017 (1) January 2017 (1) December 2016 (1) November 2016 (2) September 2016 (2) May 2016 (1) March 2016 (1) February 2016 (2) January 2016 (1) November 2015 (4) October 2015 (1) September 2015 (3) August 2015 (2) July 2015 (3) June 2015 (1) May 2015 (3) April 2015 (2) March 2015 (1) February 2015 (1) December 2014 (1) November 2014 (2) October 2014 (2) September 2014 (3) July 2014 (3) May 2014 (1) April 2014 (1) March 2014 (1) February 2014 (1) January 2014 (3) December 2013 (3) October 2013 (2) September 2013 (1) August 2013 (2) July 2013 (3) June 2013 (2) May 2013 (3) March 2013 (2) February 2013 (1) January 2013 (3) December 2012 (4) November 2012 (1) October 2012 (1) September 2012 (3) July 2012 (4) June 2012 (2) May 2012 (2) April 2012 (4) March 2012 (2) February 2012 (3) January 2012 (6) December 2011 (2) November 2011 (3) October 2011 (8) September 2011 (4) August 2011 (3) Blog Stats 729,683 hits Blog at WordPress.com. %d",
    "commentLink": "https://news.ycombinator.com/item?id=40337036",
    "commentBody": "The Alternative Implementation Problem (pointersgonewild.com)193 points by mpweiher 14 hours agohidepastfavorite39 comments miki123211 11 hours agoI'll add one more point which the OP missed, but which is also very important. If you're developing an alternative implementation, you probably have a different architecture from the canonical version, and things that are easy to do in the main implementation might be very difficult to do in yours. Let's say there's a piece of proprietary software for creating financial reports[1], which is using a weird binary format to store their documents. You want to make a free alternative, which can load and save these documents. The format is not fun to deal with, so you have a single function that loads the whole document into memory and one that dumps your data structures back to disk and entirely overwrites the file, but you operate purely on in-memory data while the software is running. What you don't know is that the proprietary software doesn't do this, it was developed in the days when users had very little RAM, so it only loads and saves the section the user is currently operating on, and knows how to modify the document in-place. Then, the proprietary software introduces a way to add attachments to their documents. People keep adding stupidly large files, recordings of investor calls, scanned PDFs with hundreds of pages and so on. THe proprietary software loads documents one section at a time, so this works just fine. You, on the other hand, always deserialize the entire document at once, which suddenly becomes a problem when the documents are bigger than the users' available RAM memory. You now basically have to re-architect the entirety of your software, to support a change that took a week of a single developer's time for the main implementation. [1] a purely hypothetical example, I've never actually worked in this space. reply gizmo686 4 hours agoparentI work in an industry that requires 2 seperate implementations of all critical software components; so I have been involved in several reimplemenations to help satisfy that. There are definitely times where I saw a feature and thought \"the original implementation was obviously doing X; and this feature does not really make sense given our architecture\". However, I have never needed to rearchitect my program to implement the feature. The only real difference is that a trivial feature for one program takes a bit of work on the other. However, I could also point to functionality that took more work to implement on the original program, that the different implementation made trivial on the second program. I have only had to do 1 major rearchitecturing, and that was on the original implementation, which had some architectural assumptions that forced an exponential blowup. I'm wasn't working on the second implementation, but they started years after us, and managed to avoid our mistake from the beginning. To take a public example, that is far more complicated than anything I have worked on, consider the problem of running Windows applications under Linux. Linux is a completly different implementation of a kernel than NT, and doesn't even attempt to be compatible. However, running Windows applications does not require a rearchitecture. Just a few relativly generic Kernel features, and a userspace compatiblity layer. Not to say that Wine does not take a lot of effort to write and maintain; but not nearly as much effort as implementing Windows itself. And, it is running on top of a platform that never aspired to be Windows compatible. Wine does, however, suffer from the problem that the article points outs, in that it is perpetually behind Windows. Wine also exemplifies a second problem, which is that bugs in the primary implementation end up being relied upon, so in order to be bug-for-bug compatible, you need to first discover what all the bugs you need to implement are. reply miki123211 34 minutes agorootparentThere are huge swaths of the Windows API that Wine straight up doesn't even try to implement, the complex APIs that a screen reader or driver would require to function being good examples. There's a good argument that implementing kernel-mode driver APIs would be a complete waste of time, after all, Wine is running on top of another operating system, which should be responsible for interfacing with hardware. However, what the Wine developers didn't foresee was the fact that some applications (mainly games) started requiring a driver to function as an anti-cheat feature. reply maayank 13 minutes agorootparentprev> I work in an industry that requires 2 seperate implementations of all critical software components Very interesting, what industry if you may discuss it a bit more? reply 4death4 9 hours agoparentprev> things that are easy to do in the main implementation might be very difficult to do in yours. The author mentions this when they talk about the ease of implementing new features in an interpreted language vs. a compiled language. reply foota 8 hours agorootparentHm, this is true, but I think it's worth pointing out that this is more general. The article was focusing on how JIT is harder than interpreting, whereas neither model in parent's post is inherently more complicated. reply pansa2 8 hours agoprev> positioning your project as an alternative implementation of something is a losing proposition > don’t go trying to create a subset of Python I agree - a project that's marketed as \"Python but with more X\" is always going to struggle to compete with the canonical implementation. (Especially if X is speed - ultimately, if you're using a dynamically-typed language you probably don't care about execution speed). However, alternative implementations are not always doomed to failure. MicroPython seems to be somewhat successful, despite supporting little more than Python 3.4 (10 years old). It's designed to run on microcontrollers so it's not competing with CPython - it's competing with other microcontroller programming environments. OTOH, I'm sure the MicroPython maintainers get a lot of feature requests for more recent Python features. I once considered building an alternative implementation of Python that's lightweight and focused on embedding in applications - again, it wouldn't be competing with CPython, but with Lua. The number one feature request seemed to be \"will it support NumPy?\" reply pfarrell 13 hours agoprevI learned something similar founding a startup. If I could do it again, I would have aggressively avoided pursuing the table stakes feature in our space. Instead we should have done the minimal amount to be comfortable that our architecture could support the enterprisey things. We should have then concentrated everything on something that would set us apart, something we could demo and get a “wow… I see where this could go” instead of something like “oh this is just a clone of x”. reply wrs 10 hours agoparentI sort of see your point, but you’re misusing the term “table stakes”, which by definition are what you have to put up to play at all. It sounds like you would implement the table stakes features, but not go farther with the “normal” extensions. The strategy being to get a callback with the exciting features, and have the table stakes features that let you avoid being vetoed for missing something essential. reply pfarrell 9 hours agorootparentYeah that’s pretty much what happened. What I was thinking was we over estimated what the table stakes were. We were building a collaborative SQL editor aka notebooks. We spent way too much time getting it working with different dbs instead of focusing on a couple and building the things that actually made us stand out. A single customer wouldn’t really care about all the dbs we could talk to that were the one she was using. reply Xcelerate 13 hours agoparentprevIf I understand you correctly, you’re saying implement the bare minimum for enterprise customers—whatever their leadership requires to do business with you—but beyond that, forget about those features and focus on something very novel that sets you apart from the competition? reply AnimalMuppet 13 hours agorootparentIf you're just like the competition, you lose, because you're more of an unknown. If you want to win business, you have to have something that your established competitors do not. (And, of course, it has to be something that your customers want, not just some random thing.) reply lelanthran 2 hours agorootparent> If you're just like the competition, you lose, because you're more of an unknown. That sword cuts both ways. The reason the competition in a field has converged on all the same features, all looking the same and acting the same is because they were all shaped by the same market forces: that's what the businesses wanted! If a potential client looks at your product and it doesn't look like a duck, doesn't talk like a duck and doesn't walk like a duck, they're going to assume that it's not a duck. I've pitched a fairly simple product (still iterating on it so not sharing details) that had some extra features (not AI) to a business, and the business eventually went with a more expensive competing product, with my contact at the place explaining \"The stakeholders feel that your product is for a slightly different use-case.\" Yes, my product did have all the features of the competition. The added feature was low-code extensibility for the backend API. The business interpreted the API-extending-mechanism as \"Something only big companies would use\", not \"Something we can ignore if we don't use\". Now, fair enough, this is absolutely an outlier - in most cases extensibility is regarded favourably. But, in this edge-case the audience came away with the primary impression of \"Great Development Tool\", not \"Great User Product\"[1]. Humans still have this notion that a thing has a primary purpose and multiple secondary purposes, and they'll absolutely go with the product that has, as it's primary purpose, satisfying their need, even if some other product's secondary purpose also satisfies their need. [1] And yes, this was a failure of the pitch. For future pitches, I'll tailor to the audience, highlighting their needs and ignoring anything that the product does which isn't in their list of needs. reply Qwertious 2 hours agorootparent>The reason the competition in a field has converged on all the same features, all looking the same and acting the same is because they were all shaped by the same market forces: that's what the businesses wanted! By this logic, we all want AI. We're screaming out for Microsoft/Amazon/Google to make all their services AI-driven. I'm sure some customers do want AI, but mostly it's their investors. reply lelanthran 2 hours agorootparent> By this logic, we all want AI. With AI, specifically, it's way too early to say that products with AI in it were shaped by market forces. It takes years for the forces of the market to have an effect on what the product looks like: 1. Purchasers have to make poor purchases, which isn't known until years later. 2. Sellers have to get feedback from rejections to determine what to refine, and how, which also takes years for most products. 3. Sellers have to run out of money when ignoring the signals, which also takes years. So, sure, maybe extra AI in $PRODUCT isn't wanted by the majority of people, but we won't actually know what forces the market for $PRODUCT is exerting until much later than 2024. EDIT: web3 was so obviously unwanted, and yet it took about 4 years for that to reflect. It is not so clear about AI, so we can expect that to take longer to establish. reply pfarrell 9 hours agorootparentprevThat’s what I was trying to express. We implemented the features that were unique and interesting too late. We should have led with them and build some compelling demos reply michaeljx 12 hours agoparentprevKind of like what the iPhone did when they launched without copy-pasting, and a very underwhelming gsm radio reply pfarrell 9 hours agorootparentYeah, that original iPhone launch keynote is something I like to watch every so often. It really emphasized we’re only doing this because we see what’s wrong with smartphones and this is our vision. They didn’t go “we made the best stylus you can have”. reply arnorhs 11 hours agoprevGreat post. Definitely some great lessons in there. It's missing a key ingredient though... I see this as any other competing alternative to any kind of product. It's like saying Amazon failed because the didn't have a brick and mortar book store like people were used to.. obviously that's not the case The reason all these jit-alts failed and were in a constant catch up, is because in practicality, most developers of x-language don't care that much about JIT. Or more importantly, they care more about language features and interoperability than they care about JIT. This is why a product that \"joins them\" rather than competing wins. Because you are not offering more stability and/or interoperability. reply awinter-py 6 hours agoprevI feel this way about all wrapper code where someone is like 'okay we need our own internal version of this API'. the reasons vary but are like 'we don't trust people to use the official API correctly' (which, fine, but yours is less standard and worse-documented). sometimes the answer is 'we need extra functionality' which, fine, but 1) don't wrap the entire API for gods sake, just add 3 functions, and 2) your codebase over time will end up 99% polyfills point being if you are not using the defaults you are inflicting great misery on whoever inherits your codebase reply tedunangst 3 hours agoprevLuaJIT is mentioned, but I think it also demonstrates that this is not always the case. Lots of people and projects also selected luajit over lua as a deliberate choice. reply vedranm 2 hours agoparentCame to the comments to post just this. LuaJIT is enthusiastically used in the TeX community and luajittex executable is readily available in parallel to luatex. reply wfleming 7 hours agoprevI think the takeaway of how to succeed with this kind of project is good, but a missing element for why so many kind of never take off that I haven’t seen mentioned yet is that a lot of times the compatability of these alternative implementations is in practice lower than claimed even for old language features. E.g. it’s very common for Ruby and Python apps to have some native C extensions somewhere in their dependencies, and AFAIK the major alternative implementations have never supported them. (They’ve tried sometimes, but it’s never really worked out for obvious technical reasons, and the alternative of expecting libraries to provide multiple implementations has a rocky history as well.) Combine that with the fact these languages are often used to make CRUD websites where I/O is likely to be a bigger performance factor than CPU and the faster alternatives look a lot less appealing. reply phkahler 11 hours agoprevAnother less popular take is that people (yes me too at times) should check their ego. It can definitely be easier to make \"your own\" parallel project vs contributing to an existing open source project, but ask yourself who you're serving. Is it the project maintainers? The project itself? The users? Your own ego? Hint: if that last suggestion makes you mad it's probably in play ;-) Adding a JIT to an existing language is a major undertaking, so the canonical implementation will have a high bar for acceptance. But you should probably still aim for that. Forking or rolling your own can provide freedom to do big things too, but that should often be regarded as temporary. If your goal is to show what you can do, it probably won't go far. If your goal is to make something better for its good, you'll learn to work within others constraints. reply Smaug123 11 hours agoparentPff, sometimes the official things are just completely cursed and you need to create your own little garden. Even if it does turn into twenty acres and a crop-rotation schedule, sometimes that's better than a volcano. reply physicsguy 59 minutes agoprevEven where the language is defined by a standards committee, you end up with similar problems. In C and C++ the number of compilers has been rapidly dwindling; Intel’s suite is now LLVM based, as is Microsoft’s. Borland and others are a thing of the past. reply apitman 13 hours agoprevThias sounds similar to the situation with web browsers. In theory they're built on open protocols. In practice whatever chrome wants will eventually be where things go. reply ikari_pl 12 hours agoparentit's like this only because nobody except me seems to prefer Firefox reply kreddor 53 minutes agorootparentOh, I use it as well, especially on Android where it has proper support for extensions. reply gary_0 11 hours agorootparentprevI don't use Firefox because it's Firefox, I use it because it's not Chrome. reply gonzo41 7 hours agorootparentI use FF because I want to browse the web without being signed in to everything. reply gary_0 7 hours agorootparentExactly: Firefox doesn't add anything I want (eg. better performance, useful features, more customizability), but it doesn't do some of the bad things Chrome does, so that's why I use it. reply chillfox 10 hours agorootparentprevI had to stop using Firefox because it kept using all the memory on my machine. So I am using Edge now. reply marssaxman 5 hours agoparentprevThat may be, but I'll only find out what that is when Firefox implements it... reply ofey404 6 hours agoprev> positioning your project as an alternative implementation of something is a losing proposition. Insightful, as I witnessed the problem caused by TiDB's compatibility with MySQL. reply SergeAx 37 minutes agoprev> If you want your Python software to be PyPy-compatible, you’re much more limited in terms of which Python features you use More so, you are also limited in terms of popular libraries versions, and it is much more cumbersome because of versioning hell itself. reply breck 12 hours agoprevHaving worked on languages and compilers for many years, I thoroughly enjoyed this post. Another way to phrase the same idea: a language is _a lot more_ than just compile speed. Compile speed is very important, in the top 10 dimensions, for sure. Especially because improving compile speed increases the developer feedback loop clock speed which helps the core team improve all the other dimensions faster. But there are still >30 other dimensions that are very important in a programming language. reply mmoskal 11 hours agoparentThe post seems to be about execution speed though. However, even there it's definitely not #1 factor as witnessed by popularity of CPython... reply lsc4719 5 hours agoprev [–] Is there any comment on Kotlin? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post delves into the hurdles encountered by alternative software implementations, focusing on optimizing dynamically-typed programming languages to exemplify the challenges.",
      "Emphasizes the significance of adhering to the canonical implementation framework, highlighting the success of YJIT in Ruby as a prime example.",
      "Explores the evolution of programming languages, the delicate balance between innovation and stability, and underscores the importance of keeping abreast of tech industry trends. Includes blog statistics with 729,683 total hits spanning from December 2011 to January 2020."
    ],
    "commentSummary": [
      "The article examines the difficulties in developing alternative software implementations with varying architectures, illustrated by running Windows apps on Linux via Wine.",
      "Emphasizes the significance of emphasizing distinctive features to differentiate from rivals, customizing products for clients, and prioritizing user needs and practicality.",
      "Highlights aspects like compatibility, open-source involvement, technological supremacy, and user inclinations in software creation."
    ],
    "points": 193,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1715543345
  },
  {
    "id": 40333471,
    "title": "Nearly Half of Master's Degrees Have Negative ROI",
    "originLink": "https://reason.com/2024/05/10/nearly-half-of-all-masters-degrees-arent-worth-getting/",
    "originBody": "College Nearly Half of All Masters Degrees Aren't Worth Getting According to new research, 23 percent of bachelor's degree programs and 43 percent of master's degree programs have a negative ROI. Emma Camp5.10.2024 3:23 PM Share on FacebookShare on XShare on RedditShare by emailPrint friendly versionCopy page URL Media Contact & Reprint Requests (Photo 32533865 © HxdbzxyDreamstime.com) Is college worth it? Well, it depends on what degree you're getting and where you're getting it, according to a new paper from the Foundation for Research on Equal Opportunity (FREOPP), an economic opportunity think tank. While more than three-quarters of all bachelor's degrees have a positive return on investment (ROI), according to the paper, master's and associate degrees are much riskier bets—with many costing students in the long run. The paper, by Senior Fellow Preston Cooper, examined data from over 50,000 degree and certificate programs at thousands of American colleges and universities. Cooper's analysis looked at how much students were earning immediately after graduation, as well as how much they were making 10 years later. The paper also took into account a student's chance of dropping out when calculating a degree program's ROI. In all, Cooper found that 31 percent of students are enrolled in a program with a negative ROI—meaning that \"the earnings benefits of the degree are unlikely to fully compensate students for the cost and risk of pursuing post-secondary education.\" However, different kinds of degrees were more likely to have a negative ROI than others. For example, 77 percent of bachelor's degrees and doctoral and professional degrees have a positive ROI. In contrast, just 57 percent of master's and associate degree programs have a positive ROI. For bachelor's degrees, fine arts, education, and biology programs had the lowest median ROI, while engineering, computer science, and nursing degrees gave students the highest long-term rewards. However, where college students were enrolled also mattered when it came to ROI. For example, an English degree from the University of Virginia has a $581,925 positive return on investment—climbing to over $600,000 when only including students who graduated on time. In contrast, students at Virginia Commonwealth University—another public university—who majored in English have a negative $30,000 ROI, with just a $3,624 benefit for those who end up graduating on time. \"When choosing a college and program of study, students should evaluate several key variables that contribute to ROI. The most important is earnings after graduation,\" Cooper writes. \"Besides starting salary, another critical factor is the institution's completion rate. While students' individual ability and motivation affects their likelihood of completion, research shows that college quality also has an impact on completion rates.\" Cooper also pointed out just how much federal dollars go toward funding low-value degree programs. He found that 29 percent of the federal funding that went to the programs he studied went to programs with a negative ROI. \"That figure includes $37 billion in Pell Grants, $47 billion in loans to undergraduates, and $39 billion in loans to graduate students,\" Cooper writes. \"Because ROI is negative for these programs, it's unlikely that most of those loan dollars will be repaid.\" This latest paper paints a detailed picture of the kinds of concerns prospective students and their families should take into account when deciding whether to enroll in college. While bachelor's degrees are still a good bet overall, students need to consider what they'll really get out of both the major they want to study and the school they've been accepted into.",
    "commentLink": "https://news.ycombinator.com/item?id=40333471",
    "commentBody": "23% of bachelor's degrees and 43% of master's degrees have a negative ROI (reason.com)179 points by delichon 22 hours agohidepastfavorite315 comments timmg 22 hours agoOne thing that I think doesn't get enough attention in these types of discussions -- not just this article about ROI, but also earnings disparities between (e.g.) genders and other groups -- is that income isn't the only measure of happiness. Many people would take less money to do a job they enjoyed more. My older sister \"gave up\" a sales \"career\" to become a fiction writer. It's enough to pay the bills, but pays less than she could have had at a corporate job. She got an English degree and couldn't be happier with her choice. My younger sister got a Theater degree and is a \"struggling actress\". Same deal: she's happy with her choice. ROI is important for some aspects of the discussion. But it is not the whole story. reply glimshe 22 hours agoparentI don't know your sister so I won't comment on her particular case. One thing I've noticed is that a lot of these \"quality of life\" career choices often involve a partner - men, more often than not - swallowing the trash in a corporate job. reply fire_lake 22 hours agorootparentSad but true. The publishing industry (think Penguin books) is full of independently wealthy white women - “This unpaid internship is a fantastic career opportunity!” reply TMWNN 17 hours agorootparentHistorically and today, a sought-after job for a young woman out of college who has majored in, say, English with wealthy parents is in publishing/media in NYC, with the parents paying the bills (publishing doesn't pay very much at the entry level). It's a genteel job that the parents can tell their friends about, and the daughter can meet suitable men; the classic marriage among these types has the husband working in finance and the wife in media. reply PartiallyTyped 19 hours agorootparentprevEdit: disregard the comment. Sadly enough, independently wealthy people are virtually the only ones who could actually publish consistently and thus possibly earn something out of the whole endeavour — under the assumption that any single book by an unknown / non-celebrity author has equal probability of being a hit. Reality seems to be that most books are failures, publishers prefer celebrities as that reduces marketing costs, and they prefer established audiences. This is a good read on the topic: https://www.elysian.press/p/no-one-buys-books reply delfinom 19 hours agorootparenthttps://slate.com/culture/2024/04/book-sales-publishing-indu... A rebuttal to that article. reply PartiallyTyped 19 hours agorootparentVery interesting, thank you for the new knowledge! reply timmg 21 hours agorootparentprevInterestingly, for my older sister, the answer is yes. For my younger sister, it is no. I agree with the point, though. It's more socially acceptable for a women to choose a lower paying job that she prefers. reply uptime 19 hours agorootparentI want to find the equivalent of HN for healthcare, where I think you’d find a ton of women MDs and highly specialized nurses who are women expressing frustration with their adjunct or artist men. This community is not the whole story. reply prirun 19 hours agorootparentI had a minor procedure a while back - enough that I had to be put under - and asked my nurse if she was happy with her job. She said yeah, she loved it, but she made more money as a barkeep! I was completely shocked, especially considering the hours nurses are expected to work. This was a real nurse, not an aide. I think healthcare is a not-so-great career anymore, again because corporate profits are all that matter. reply johnnyanmac 2 hours agorootparentOh yeah. Nurses and MD's are night and day in compensation. And I'm sure you've heard the teacher salaries as well by this point. Skilled labor does not outdo the decades of gender conformity expected from these jobs. reply IdiocyInAction 17 hours agorootparentprevGiven that women generally avoid dating lower-paid partners, there's a good chance you wouldn‘t or that those people would be outliers. reply TMWNN 17 hours agorootparentprevSuch relationships exist. But in no way are such anywhere as common as those relationships in which men earn more than women. And that disparity is primarily because, as IdiocyInAction said, women don't want to be in relationships with men who earn less, not because men don't want to be in relationships with women who earn more. reply dudul 13 hours agorootparentprevGood luck finding that. Women tend to not be interested in men who earn less than they do. I think the closest you may find are women temporarily supporting their husbands while they finish their degree or something like that, but again, the plan is for the husband to become the main earner. It is sometimes presented under the myth of \"men threatened by women who have a big career\", but I have never seen researches supporting that. Men don't care if women earn a lot, note that they don't prefer it, they literally don't care. It is not a criteria when choosing a partner. Women do care a lot though about finding a partner who earns more than they do. reply boppo1 21 hours agorootparentprevYeah, I'm a man working a low paying job that I enjoy. Almost everyone I know seems to look down on me for it. 'When will you get a real job', 'when will you start your career?' reply MaKey 20 hours agorootparentSorry to hear that. Does it affect your dating life too? reply boppo1 9 hours agorootparentWhat dating life? I was at a grocery store today and wound up flirting with a cutie, but I don't make enough money to pay for myself to go out for dinner more than once a month. The expenses a typical woman expects (from my experience) are beyond what I can afford. reply cristiancavalli 19 hours agorootparentprevGood for you! You should do what makes you happy and revel in the knowledge that you provide many different kinds of value and you yourself extract value from something you like doing in multiple ways. The way I think about it: no one else has to be me and work my job. Since im going to so the work then I will choose how and what I do so long as I can. It took time for me to build the emotional strength to do what I love and not care about what others think but working on that intentionally has brought so many unforeseen, manifold benefits that I wouldn’t change that path for any other opportunity. reply lemurien 19 hours agorootparentprevIs it in Europe? I feel people would like it if you tell that you like what you do. reply boppo1 9 hours agorootparentNo, the midwest. reply dudul 20 hours agorootparentprevIt has nothing to do with \"socially acceptable\", I think the parent was simply talking about even having the option of doing it. Very easy to be an aspiring author/painter/singer/etc when you have a partner to pick up the bills with a \"real job\". reply sshine 22 hours agorootparentprev> swallowing the trash in a corporate job My wife is an artist with a marketing degree. I love programming. Swallowing trash at work just feels less painful to me. reply throwawaysleep 21 hours agorootparentI doubt he disputes that men can love their work. It is just they have fewer options if they do not. reply benterix 19 hours agorootparentI disagree. Granted, they are less likely to earn money on OnlyFans etc., but apart from that they can do the same things: they can be artists and do low paying jobs that they like. What I agree though is that if they do chose a low-paying job, it might be more difficult for them to find a partner, which is not necessarily true for women. We are still by and large living in cultures where the male has to bring home the bacon (\"a real man knows how to take care of his family\") and it will take decades for it to change, if ever. reply Sammi 1 hour agorootparentThe phenomenon is cross cultural, meaning it's biological not cultural. Women tend to prefer men who can provide. Men tend to prefer women who are young fertile caregivers. Mythological stories across cultures reflect this general tendency. It's not something we made up recently or locally. It's a global timeless phenomenon. reply superb_dev 21 hours agorootparentprevWhat options are closed to men? reply dijit 19 hours agorootparentI think men are looked down on if they go back to study, take an unpaid internship, become house-husbands (IE; non-breadwinners), exclusively attempt a creative endeavour (high risk enjoyable pursuits are seen as self-indulgent and lazy) or if they change career later in life (after 28~). If I think the same of women, I do not believe that we would typically look down on them or consider them bad people in the same way for being somehow deficient in capability (IE; \"a loser\") or shirking responsibility if they decided to do those things. In this sense, those opportunities can be considered effectively closed to them. reply singleshot_ 18 hours agorootparentI went back to study, took on an unpaid internship, and changed careers later in life. It was awesome and it seems to make a lot of people who stuck to their first career jealous. You may look down on men such as me, but most people appear not to do so. I admit I don't know too much about the house-husband or the creative endeavor. reply dijit 18 hours agorootparentI don't look down on you, I consider myself a bit more egalitarian in my principles than that. Is your notion then that you did not face social stigma? That would be an interesting counter-anecdote to what I said, may I ask where you live? reply dudul 20 hours agorootparentprevLooking pretty and finding a sugar-mommy? reply thaumasiotes 19 hours agorootparentIt is an option; I remember an article on HEMA that mentioned a participant who was idle and supported by his wife. But it's a much more competitive option for men than it is for women. (That is, a man attempting to fill this role will face much tougher competition than a woman trying to do the same thing. It's not that the option is competitive with other options.) reply dudul 19 hours agorootparentI think it's well understood yes. When an option is only available to 1% of the population I believe it is fair to say that it's not really an option. reply uptime 19 hours agorootparentprevWow this tuned into a different narrative quickly. The above comment did not blame women for men’s plight but did not compare apples to apples. Gender pay gaps usually involve different wages for same work, not downgrades that all genders do. The “great” thing about cutthroat paring down by groups like private capital in management is all genders get downgraded at the same rate. So equity will even out in the race to the bottom, and we can enjoy more balanced stories of men contributing less to the household income. reply dijit 19 hours agorootparent> Gender pay gaps usually involve different wages for same work That's not true, the oft-touted statistics are \"77cents on the dollar\", which is taking median women's earnings when employed with median men's earnings when employed, without regard to hours worked or responsibility[0]. In fact when accounting for hours worked, years of experience, responsibility and before maternity age women typically out-earn men[1][2], though mostly in large urban environments. I don't think anyone is blaming women here, but I think we should speak with factual information, lest we continue to propagate myths or erroneous talking points. Please keep in mind that women seem to be disproportionately disadvantaged after having children, which is what the study from pew research concludes after noting the early advantage for women. [0]: https://now.org/resource/the-gender-pay-gap-myth-vs-fact/ -- \"By comparing differences in annual earnings between men and women, we find that there is about a 23 cent difference per dollar according to the Census Bureau.\" [1]: https://www.theguardian.com/money/2015/aug/29/women-in-20s-e... [2]: https://www.independent.co.uk/news/world/americas/young-wome... reply apantel 19 hours agorootparent> Please keep in mind that women seem to be disproportionately disadvantaged after having children That’s because children are a full time job — the hours, the energy required, but especially the cognitive load. If the woman takes on some or most of that responsibility, then any other job she has becomes a second job. reply belorn 10 hours agorootparentIt should be noted in this context that women are individually more likely to have children then men. Men with multiple children with multiple women do not generally spend the same amount of time as the sum of all the women. reply klyrs 19 hours agorootparentprevWell, my brother is a musician and rents a room in a large house. He doesn't have a sugar mama/papa, music pays the the bills because he's actually frugal. For decades my dad implored him to \"get a job\" but in his mid-40s he's still is still happy with his life. Some other working musicians have partners with a higher income, but many don't; it's more common for them to be happily single or coupled with other musicians. If you don't like swallowing corporate trash, don't. If you want to bring somebody along on your quest for enrichment, recognize that your choice there is yours. reply marcianx 21 hours agorootparentprevReminds me of Max Shulman's advice to aspiring authors: \"Marry money\" reply colmvp 22 hours agorootparentprevOr possibly financial assistance from the Bank of Mom and Dad. reply apantel 19 hours agorootparentAka non-state welfare. reply EVa5I7bHFq9mnYK 18 hours agorootparentprevNot necessarily a partner, parents are often where the extra income comes from. Though I suspect parents are more likely to support a grown daughter than a grown son. reply giantg2 22 hours agorootparentprevThis is my experience as well. reply barbariangrunge 19 hours agorootparentprevTo chime in, I see a lot of women supporting their men as well, probably more often tbh. But yeah, one stressed out breadwinner, one struggling x isn’t that uncommon. For some reason I’ve seen a lot of breadwinners go into the trades rather than corporate reply cableshaft 19 hours agorootparentprevYep, at least in my case. My wife is able to pursue her fiction writing career full-time right now because I'm still working a corporate job. Just before deciding she was too burnt out and had to quit she was working a 130k/year proposal management job and self-publishing a book a year on the side. Right now she's making a little over 1k/month with her fiction writing (and just recently started offering editing services on the side, which she's gotten a few customers already), so it's definitely been shock to our finances ($140k -> ~$12k), and our emergency savings have been steadily draining every month, although we have been able to tighten the belt on several things so it hasn't been as steep as it would if we had maintained the same lifestyle. But she's a lot happier, and she's very talented, and she's had multiple millions of page reads on Kindle unlimited already and her books are highly rated, so hopefully this takes off (there are writers in her genre making hundreds of thousands, including a couple writers she's friends with, maybe she'll become one herself, although I'd be happy if she can get it to the point where it's like $60k/year). I also wouldn't mind a break from my own corporate job so I can hopefully work on my own creative works fulltime as well (I code video games and design board games in my spare time), but obviously we can't both afford to do this at the same time, we'd be homeless pretty quick. But yeah, in the meantime, I'm 'swallowing the trash' in a corporate job. reply thaumasiotes 19 hours agorootparent> hopefully this takes off (there are writers in her genre making hundreds of thousands, including a couple writers she's friends with, maybe she'll become one herself, although I'd be happy if she can get it to the point where it's like $60k/year) That might be more difficult than becoming a runaway success. I don't think the distribution of author earnings peaks in the middle. reply cableshaft 17 hours agorootparentI can see that. A lot of things tend to be bimodal in their distribution. Let's hope she becomes a runaway success then :). And if not, it's not like she lost her ability to do proposal management. If it gets to that point, I'm sure she could find a well-paying job again if need be. She's got a lot more practice with those interviews than I do, in fact, since she would interview when recruiters contacted her with a potentially better-paying job even if she wasn't really looking at the moment, whereas I tend to ignore recruiters when I'm not actively looking (or at least debating if I should start looking). reply exitb 22 hours agorootparentprevHow often do you actually see people go „oh we have enough, we’re going to take lower paying jobs”? People work these jobs regardless of their partner's income. reply sojournerc 21 hours agorootparentI recently switched from a 15 year programming career to becoming an apprentice woodworker at an architectural millwork shop, and I could do it because I have a partner with income that can support us both. Might not be often, but it certainly happens reply __s 21 hours agorootparentprevMy father took a 1 dollar pay cut from 16 CAD/hour to 15 CAD/hour. Working in furniture repair he wanted to transfer from house calls (driving everyday) to in-shop repairs My mother's a secretary (they call them administrative assistants now to be correct). She could advance her career by becoming a clerk, which mostly does more of the same, only more. Instead she's happy where she's at I used to work for Microsoft. I left for a startup (peerdb) & while my pay didn't go down, that meant leaving behind >50k USD in unvested stocks & my salary growth at Microsoft was pretty fast (hired as junior in 2019, was one promo away from principal when I left in 2023) which I don't expect a startup environment to match reply vsnf 20 hours agorootparent> Junior in 2019, was one promo away from principal when I left in 2023 I'm broadly unfamiliar with Microsoft's promotion culture, but this strikes me as incredible. But in the \"something seems incorrect\" sense of the word. Were you hired at an incorrectly low level, were you a one-in-a-million talent, or is there huge title inflation at Microsoft now? reply __s 20 hours agorootparentIncorrectly low level (tho I'd like to think I'm uniquely talented too) Had ~10 years of experience working at small companies (pointerware, mldsolutions) after dropping out of college (had internship with pointeware where I correctly guessed they'd hire me if I dropped out). Interviewed at Citus a week before they announced being acquired, got shooed in. Junior pay at MS was still 2x salary increase reply makeitdouble 21 hours agorootparentprevThat choice happens all the time, it's wild to me you never had it come up irl. It's really clear when someone is invited to step up in their job and refuse because it would entice more working hours, but it happen as much when people leave full time position because it doesn't give them enough time. reply giantg2 21 hours agorootparentMany promotions are actually pay rate cuts for salaried jobs. My job wanted me to work a 13% hour increase for a 7% raise. reply HeyLaughingBoy 20 hours agorootparentYup. About two months into this job, they offered me a promotion since \"you would be perfect for this position.\" I looked at the schedules of the people who also had that job and they were booked solid all week. It was pretty obvious to me that it would be basically 40+ hours/week of meetings and tons of stress for pretty much the same pay. The bonuses were probably higher, but nothing about it was worthwhile. Yeah, I like money but you'd probably have to double my salary for me to even think about it. Had two successive managers suggest the job to me. After about a year, the newest one finally took the hint and left me alone. reply exitb 19 hours agorootparentprevProbably not that wild - we may live in different bubbles. reply resource_waste 21 hours agorootparentprevI did this when I switched from Real Engineering to Programming. Took a 40k/yr paycut, but I knew my income after the experience would be higher. reply jajko 20 hours agorootparentprevI am planning to switch to 90% allocation from next year, which effectively means cca 50 days yearly of paud vacation. We have 4 mortgages on 2 properties, by no means we have 'enough' (whatever that means). QOL is for me by far of important metric, especially with kids. reply cristiancavalli 20 hours agorootparentprevBased on what? Just your practically worthless empirical experience? reply bill_joy_fanboy 20 hours agorootparentIt's only \"practically worthless\" because it doesn't align with what you believe about the world. People only demand data and statistics and deny lived experience when the shared lived experience goes counter to what they want to believe. reply cristiancavalli 20 hours agorootparentNo I’m just asking for something more than your actually worthless empirical experience. You can claim it’s worth something but that’s just your… opinion. Get some data and put some money where your mouth is otherwise you’re just crying because you have nothing to back up your truly soporific and sophomoric claims. Also I don’t nor do I have to give any credence to your empirical experiences. I do take an interest in what you claim generally from your expierences since, given the claim is solely from your empirical experiences, it would have very little to do with the actual state of the world despite you falsely claiming otherwise — especially since you have no data to back any of your senseless claims up. Sorry your feelings don’t matter when you make unqualified statements like this. reply 000ooo000 19 hours agorootparentprevWhat are you on about? People demand data and statistics in order to verify claims. >a lot of x often involves y is a broad claim which warrants more than a few anecdotes. GP may have an unpleasant tone but they're not wrong for asking that a sweeping generalisation come with something more solid than 1 person's perspective. reply johnnyanmac 2 hours agorootparent>People demand data and statistics in order to verify claims. I don't like the \"educate yourself\" dismissal for answers like these, but the divides in income and gender isn't exactly a niche topic. If you can believe that women often won't date men who make less and that women more often become stay at home parents than men, you can derive why woman may be more likely to pursue passions that pay a pittance compared to men in a relationship. If any of those 2 points feel contentious, all I'll say is Google it. I'm not interested in digging up such common facts for someone else. reply afavour 21 hours agorootparentprev[] reply imtringued 21 hours agorootparentMen aren't supposed to make more money and the wage gap is in favour of women when you look at recent male vs female graduation rates simply because there are more educated women. reply Spivak 20 hours agorootparent> wage gap is in favour of women Hahahahaha… no. You can argue that the wage gap is an imperfect metric and that it can't take into account certain advantages but it has not ever been in favor of women. https://www.pewresearch.org/short-reads/2023/03/01/gender-pa... reply tim333 19 hours agorootparentI saw stats in the UK that for women who had not had kids yet the average pay was slightly higher in the same job than for blokes. Kids are the main thing that makes the difference. reply Spivak 17 hours agorootparentYou're talking about the adjusted wage gap, not the wage gap. The adjusted wage gap measures sexism on an individual basis — all else being equal. The wage gap measures sexism on a societal basis, this is where the \"explained\" parts fall away. If 10% of the gap is explained by women having lower paying roles then that's good. Women aren't getting paid (that much) less for the same work. But it doesn't fix the issue of women systematically working lower paying roles or roles typically held by women being seen as less valuable. The straight wage gap has never been in favor of women. \"Men and women make the same when you control for the variables that cause them to make less\" isn't as applicable when you're talking about group dynamics. reply lemper 19 hours agorootparentprevbro your comment hits a bit too close to home. my partner took her master's degree yet she couldn't find a matching job in our area. at the very least, she now has better grasp on some things that actually matter to our life and her input is really valuable for me. so all in all, it's fine I guess. reply throwawaysleep 21 hours agorootparentprevI can’t say I have heard of or know of a man who has done a “quality of life” career without either inheriting or getting rich first. reply fastaguy88 20 hours agorootparentPretty much every faculty member at any university has chosen quality of life over getting rich. Most of them did not inherit or start out rich. reply coldtea 21 hours agorootparentprevWiden your social circle reply jollofricepeas 21 hours agorootparentUnderrated comment and great advice. If all your friends and acquaintances look like you and don’t have differing perspectives, then you should totally widen your social circle. Speak less, learn more. Ask questions. Break out of the loop. Say no to the algorithm. reply johnnyanmac 2 hours agorootparent>Underrated comment and great advice. useless swipe and naive advice, especially in this day and age. I can barely make friends who share my hobbies, what hope do I have in breaking the ice with someone completely different? I'm not going to specifically crash other meetups just to \"expand my horizons\". >Speak less, learn more. yes, that's why inroverts dont make as many friends, and are instead arguing with each other on Hacker News. reply coldtea 18 hours agorootparentprevTo be a little more helpful, I'll add that people DO take “quality of life” careers \"without either inheriting or getting rich first\". Just ask your average gigging musician, writer, designer, and so on. People who run \"lifestyle\" business and refuse to grow them outside a living income. People who opted out of the rat race, from the start, or mid-career, and downsized, without being rich, to have more time with family, reduce stress, etc. reply tasty_freeze 20 hours agorootparentprevI know someone who was a great EE and quit at about 30 to become a park ranger. He was even better at that and is still doing it 30 years later and he couldn't be happier. He always had a bohemian & ecological mindset so it didn't seem like a surprise when he made the shift. reply giantg2 21 hours agorootparentprevI know of one or two supported by a high earning wife. But it seems rare. reply n8henrie 19 hours agorootparentprevIt's easy to find examples within a career though, if you find that relevant. I'm a (US) physician that chooses to work a federal government job rather than private / for-profit sector, making about half as much money in exchange for numerous QoL benefits. reply pfisch 20 hours agorootparentprevI started a video game company. We are making kingmakers now. reply johnnyanmac 2 hours agorootparentBest of luck. I'm planning on doing the same in the near-ish future, without anywhere near as much buzz and no staff. I accept that I'll probably need to step back into industry if I don't make a huge splash. It's a rough industry. reply cableshaft 19 hours agorootparentprevBest of luck with your upcoming release. It looks pretty fun. You seem to be getting some pretty decent buzz as well from a couple things I've seen elsewhere. You definitely have the hook down (modern guns versus medieval battles). I think I'd be super happy with like, 10% of your buzz for my game's release (whenever that is, I'm only working on it part time), but it's a fairly small puzzle game (and still only programmer art at the moment) without a strong hook (it's just a sequel to an old free game of mine that did well in the past), so it has nowhere near as broad of an appeal, so it's to be expected. What's your situation that allowed you to be able to go full time on the game? reply solumunus 19 hours agorootparentprevThis point would have been better made without involving gender. Who cares if it’s more often than not the man? That’s completely irrelevant. Unless your point really was about the gender, in which case you should elaborate. reply johnnyanmac 2 hours agorootparent>Who cares if it’s more often than not the man? women, to put it crudely. >in which case you should elaborate. what's to elaborate on? Women are much less likely to \"date down\" econmically than men. Hence, women are less often the \"breadwinners\" and as such may be more likely to either spend their time as a full time parent or chasing jobs for passions over compensation. These aren't novel phenomenon. reply sotix 20 hours agoparentprev> income isn't the only measure of happiness. I recently asked my Greek teacher about if 2008 was as bad in Greece as American media portrayed. She said things were definitely not good from an economic perspective but life kept going on. Even though people had no jobs, you couldn’t find an empty seat in a taverna on the weekend. She pointed out the biggest difference she notices living in the US is that people prioritize money above all else. In Greece they prioritize having a good time. It really changed how I viewed the crisis and reminded me that cultural differences bias our judgements. reply throw0101c 19 hours agorootparent> She pointed out the biggest difference she notices living in the US is that people prioritize money above all else. Something Jerry Seinfeld (yes, the comedian) recently observed: > SEINFELD: In the seventies, this is the tragic turn of American culture. And this was explained to me by Mario Joiner who cracked this puzzle that I could not figure out what the hell happened. That money became everything. What happened because it was not like that in the seventies. In the seventies, it’s how cool is your job? How cool is what you’re doing? If your job’s cooler than my job, you beat me. > BRENNAN: And no one said, how much are you making? > SEINFELD: Oh, you’re doing okay. You’re making this? Yeah. Who cares? And Mario Joiner explained this to me. He said the eighties was the first time that young guys could make a lot of money fast. > Never existed before. Rich guys were Aristotle Onassis, Andrew Carnegie, shipping, iron. You couldn’t make a lot of money fast in those days. > And it has poisoned our culture to this day. It’s poison. * https://awealthofcommonsense.com/2024/05/seinfeld-on-when-mo... reply johnnyanmac 2 hours agorootparentI figured it was the exact opposite. It was the very slow start of when life started to get really expensive. by the 00's there was no time to just go out and pursue your passions, you needed to pay rent the next month. I imagine a part of this is (among many factors) impacted the US very strongly due to the \"out of the nest\" mentality. If you were 18 and still in your parent's house, you were doing something wrong. And that mentality has only very recently started changing as university costs skyrockted (and thus commuting has risen) and rent became untenable for someone making minimum wage. reply barbariangrunge 19 hours agorootparentprevIf your good times cause a crisis with long lasting consequences, that isn’t exactly a good thing. I could list a hundred example situations but there’s no need, is there? reply throw0101c 19 hours agoparentprev> […] is that income isn't the only measure of happiness. It may not be the only measure, but it is a very important part: > One could draw a snap judgment from this analysis and conclude that money, in fact, simply buys happiness. I think that would be the wrong conclusion. Clever sociologists will always find new ways of “calculating” that marriage matters most, or social fitness explains all, or income is paramount. But the subtler truth seems to be that finances, family, and social fitness are three prongs in a happiness trinity. They rise together and fall together. Low-income Americans have seen the largest declines in marriage and experience the most loneliness. High-income Americans marry more and have not only richer investment accounts but also richer social lives. In this light, the philosophical question of what contributes most to happiness is just the beginning. The deeper question is why the trinity of happiness is so stratified by income—and whether well-being in America is in danger of becoming a luxury good. * https://archive.ph/4ofJ6 / https://www.theatlantic.com/ideas/archive/2024/04/happiness-... reply endisneigh 22 hours agoparentprevI’m very curious - is your sister single? I ask because often low wage earners are being subsidized by their partner. reply bombcar 20 hours agorootparentIt could be looked at that way, or as a single-income house where the other partner is doing some supplementary income. Even if technically true a single income household is rarely discussed as “one partner subsidizing the other” but once the other partner is earning something it’s a subsidy. reply AlecSchueler 22 hours agoparentprev> earnings disparities between (e.g.) genders and other groups When people talk about the gender wage gap they're not usually talking about the overall average, but rather doing like for like comparisons. It feels like you're suggesting your sister chose to give up sales for fiction because there were other factors at play such as quality of life, that that is a decision men would be less likely to make, save therefore her actions have tipped the scale towards \"women earn less.\" But when people are talking about the gender pay gap they usually mean within a specific field or job title. So the comparison would be made between your sister and other men in sales, or between her and other men who left that career to go into fiction writing. In most cases the woman still earns less despite doing the same work and having the same work life balance (in theory; in effect she'll almost certainly be doing more additional domestic and other unpaid labour). reply IdiocyInAction 22 hours agorootparentAt least the version I hear most often about is a gross disparity between the genders without taking anything into account. IIRC if you look into it it's mostly a maternity penalty anyway. reply MaKey 22 hours agorootparentIn the news I mostly see or hear about the non-adjusted gender pay gap (women earn 17-21% less than men). The adjusted pay gap, which takes hours worked into account, is only 1-5% [0]. [0] https://en.wikipedia.org/wiki/Gender_pay_gap reply fastball 22 hours agorootparentAnd that's across all ages, but as you go younger the wage gap actually reverses and women make more than men. reply AlecSchueler 14 hours agorootparentI think the argument is that women should be paid the same for doing the same jobs regardless of their age. reply dijit 12 hours agorootparentthe issue is that if you are 10% less experienced than someone else because you took a year out of your career (imagining two people who started working 10 years ago), it is hard. Climbing the ladder has inertia, anyone taking a sabbatical will be hugely adversely affected. We should however destigmatise men taking parental leave in order to readjust this. I will point out, however, that our society still prefers that men pay more than their share and many women have reported being uncomfortable out earning their partner. Obviously these conditions cannot coexist. reply dtech 21 hours agorootparentprevThere's one thing you're missing here, and that is as women make up a larger percentage of an occupations' workforce, the wages start to go down with it, which is what happened or is happening with e.g. doctors and teachers. reply uptime 19 hours agorootparentHow are you relating this to the thread? Maybe you mean it takes time for capital to lower everyone’s income when market competition increases, the actual details of the excess humans matter less and less each year. Artificial protection and gatekeeping by ordinance are there to protect those who got there first. Then we can blame others: women or immigrants etc, for wage depression. Unions do this less but still favor those who organized first. If capital gets to hire for less, say that. Maybe the opportunity was an increase in certain pop segments, but they dont make wages go down. The people who pay wages do. reply imtringued 21 hours agorootparentprevDo you really expect employers to use their increased bargaining power in favour of workers? Of course doubling the number of workers in an industry is going to lower wages. reply bobthepanda 22 hours agorootparentprevWhich is also why a lot of the latest policies are to provide equal maternity and paternity leave. Which is pretty good anyways, why shouldn’t fathers be able to take as much time off? reply rlue 22 hours agorootparentI strongly support (m/p)aternity leave equality, but I can think of lots of reasons new mothers should be able to take more time off. reply atmavatar 21 hours agorootparentIf your goal is gender equality, though, it needs to be equal, and we should encourage fathers to take the full time off as well. Otherwise, from an employer's perspective, women are ultimately lower-value employees, because they have an additional pool of time off they can utilize, while men do not. Balancing that out gives one less reason to favor one gender over the other when it comes to hiring and assigning a salary. reply bombcar 20 hours agorootparentOne thing is the leave shouldn’t be “all at once or lose it” but a bucket that can be drained over time. The mother can use it very early and the father later which provides the best experience for everyone. reply apantel 18 hours agorootparentprevWhat if women are higher-value stay-at-home parents than men? Then maybe the optimal equilibrium is reached by more maternity leave taken on average by women than paternity leave taken by men… which produces an inverse result in the workplace. reply AlecSchueler 14 hours agorootparentThis is contrary to the goal of equality. reply apantel 12 hours agorootparentIt all hinges on whether or not sex/gender (and everything that comes with it) makes you more likely to enjoy and/or excel in certain roles. My gut feeling is it does. If it does, then the goal of that kind of equality is a false goal. If not, then you’re right. reply bobthepanda 7 hours agorootparentIt would be best if we gave parents the choice to determine what split is best for children themselves. Right now, there isn’t really choice, because few countries have had equal leave rights, and those that do have had them for such a relatively short amount of time that we have no idea. So we haven’t been able to test any sort of hypothesis at all. All we have is culturally bound gut feelings, but if we just listened to those then we would still think that there are only four elements, that the earth is flat, etc. And in the US we don’t even have legally mandated paid maternity leave, so even that would be a start. reply belorn 20 hours agorootparentprevWhile the details differ, the social benefit are mostly similar for both maternity and paternity leave. It create a better bond between parents and child, resulting in children that grow up more healthy and productive for society. Parental bonding take times. reply AlecSchueler 14 hours agorootparentprevI guess it depends what you're reading, but the point remains that there's likely gender based pay gap within any given field/position. I'm not sure where the \"anyway\" comes from in your last statement. The reasons for the disparity don't affect the reality that it's there. reply giantg2 21 hours agorootparentprevIf you're talking about the difference at the aggregate level, it's mostly a difference in selected job types (eg STEM vs social sciences). reply AlecSchueler 13 hours agorootparentI'm talking about e.g. within STEM where a gap of both pay and position remains. reply giantg2 10 hours agorootparentI'm aware of the pipeline issues for many STEM positions. Do you have any data on the pay gap by industry for the groups within STEM? reply redeux 22 hours agorootparentprevIt’s both actually, and you can’t be sure which measure someone is talking about unless you ask them. It’s quite common to hear people suggest that the gender pay gap is due to the fields women enter, rather than a disparity between pay within the same field. As with anything, there are different ways to measure societal issues, both macro and micro. I would argue that both are valuable for understanding the context and scope of the issue. reply giantg2 22 hours agorootparentprev\"But when people are talking about the gender pay gap they usually mean within a specific field or job title.\" I'm my experience, people usually confuse the two. There's rarely anyone pulling up BLS data for a specific job. We've even had presidents use the aggregate numbers misapplied to the specific level. reply jeltz 22 hours agorootparentprevNo, that depends on the speaker. Some talk within a job but from my anecdotal experience most do not. reply dudul 20 hours agorootparentprevI've never heard of the gender pay gap meant as this very granular, targeted comparison. On the contrary, I find that most data shared about this gap are just a lazy average of all men and all women earnings without controlling for stay-at-home parents, experience, diplomas, fields, etc. reply exe34 22 hours agorootparentprev> But when people are talking about the gender pay gap they usually mean within a specific field or job title And they have to remember to avoid comparing the per hour earning at all cost. reply ThouYS 22 hours agoparentprevThat's true. I feel like the important bit is covering the essentials. 2x more is not 2x better, but you need to get above a certain threshold. Incidentally this also holds for some engineering metrics. Some error metrics stop making sense after a certain threshold. E.g. if a certain error becomes disqualifying, doubling the error changes nothing. reply galangalalgol 20 hours agorootparentSome study released years ago gave a USD value for the salary that was the happiness asymptote. Recent inflation has likely demolished that number, and I think it is an important one to know. It did not speak to the case of not needing a salary at all. The ability to be idle, or seelf direct however you want is very enticing, but I could see it going badly in a number of ways. reply PartiallyTyped 19 hours agorootparentI think I have read that, IIRC it was around 150k? reply galangalalgol 15 hours agorootparentWhen I read it it was 75, so maybe they did do the study again. reply ren_engineer 21 hours agoparentprevand I know a ton of people miserable and stressed because they are buried in college debt. The fact the government has created a system where 18 year old kids can rack up massive debts that can't be removed via bankruptcy is insane. Changing that would instantly fix the system, colleges would have to prove ROI or it would go back to how it used to be for non-practical degrees(only rich kids wasted their time and money on them) reply bachmeier 21 hours agoparentprevOr on this same point but from another angle, consider the ways you get into the higher end of the income distribution without a college degree. You either have family connections that let you take over a business, or you do something that comes with a big premium (walk on roofs, go up on ladders, move furniture, work construction). If you didn't win the birth lottery that allows you to open door #1, you're stuck with the second option. Moving furniture or going up on a ladder aren't terrible when you're fresh out of high school. They are when you're 50, and you're going to be one of those that takes Social Security at 62, possibly after multiple spells of unemployment as you approached retirement. reply silverquiet 21 hours agorootparentI didn't win the birth lottery on #2 either; I've got some physical disability, though I have still done a bit of that type of work and if you think it comes with a \"big premium\" then your experience is very different from mine. Roofing is an incredibly simple job to learn for instance, and it pays little better than minimum wage. From my actual experience (admittedly limited to the South Texas market), blue collar jobs still pay like crap. The toll taken on the body actually seems to be pretty individualized; I know some men who seem to have no problem working into their 60's and I think that keeping active within reasonable bounds actually contributed to that. reply bachmeier 21 hours agorootparentMy point is not that you can make a good income doing blue collar work (you generally can't) but rather that some people do make a reasonable income doing that type of work, and in those cases there's a good explanation. > Roofing is an incredibly simple job to learn for instance, and it pays little better than minimum wage. I can't speak to your area, but doing roofing on tall houses/buildings with a steep slope here pays much better than working on relatively flat roofs or one-story houses. reply silverquiet 20 hours agorootparentYes, my experience is generally residential, but it also tends to be that risk is just one of the costs that blue-collar workers are expected to take; there is little pay premium for it. Skill does tend to get some reward; I think I'd be inclined to go electrician if I had to. I think if one wanted to get paid the most, I'd look at plumbing because it both takes some skill (despite the \"3 rules\" joke) and is literally shitty. Personally I enjoy a bit of carpentry work now and again, but my skill is nothing compared to my brother, and he was never really able to put together a comfortable living despite his ability. Fortunately, he was able to get an engineering job (not the software kind) fairly recently and was amazed at the idea of getting paid for a day off once in awhile and having health insurance. reply bombcar 20 hours agorootparentThe key in the trades is striking out on your own as rapidly as possible; if you don’t get to that tier you’ll end up with a moderate income. reply apantel 18 hours agorootparentYes. Trades == business. If you want to make a lot of money, you have to become a business owner. reply chung8123 16 hours agoparentprevHappiness is greatly impacted by the burden of large debts that hold you down. If you are not making enough to cover your expenses you will likely not be happy, even if your job is amazing. The ROI explores that to some extent. You are right though, as long as you are happy that is what matter in life. Setting yourself up for long term happiness sometimes means forgoing some up front happiness though. reply chengiz 18 hours agoparentprevIrrelevant. The article is about ROI, not about happiness. Income isn't the only measure of, I don't know, longevity, calorie intake, hair color, number of children, etc., either, but we're not talking of any of those things. reply fastball 21 hours agoparentprevThis could go the other way though. It's not just that you spent $Y on the degree, you also spent X years on the degree, which are years you could've spent finding that enjoyable career. reply xattt 21 hours agoparentprevSensemaking is at play here too. Resilient individuals will (consciously and subconsciously) adjust and retell their narrative in a positive way. This is done in order to mitigate the distress of making choices that were less-ideal, in hindsight, but were made with the best information they had at the time. [1] https://en.wikipedia.org/wiki/Sensemaking reply kibwen 21 hours agorootparentThat doesn't make a difference here, because it cuts both ways. Someone who gave up on a lucrative career in order to pursue their dream job might retroactively rationalize their decision as the best one, but also, someone who gave up on their dream job in order to make more money might also retroactively rationalize their decision as the best one. In the meantime, there's nothing inherently wrong about choosing to make less money in order to have a job that doesn't make you miserable. Or to put it a different way: there's nothing inherently virtuous about minimizing your happiness in order to maximize your wealth. reply firesteelrain 21 hours agoparentprevYour anecdote actually makes sense because she is pursuing a career in her degree. She didn’t get a degree in Sales. reply 127 22 hours agoparentprevYes, indeed. There seems to be a gender disparity in those choices. ;) reply Tom3849 21 hours agoparentprevThere is a easy way to measure happiness. Record level of women are on antidepressants. And there is significant correlation with education level. And please do not argue \"she is happy with her choice\" and on antidepressant at the same time! reply nick__m 20 hours agorootparentThere are many reasons why one takes antidepressant, I do yet I love my job and I am happy with my choice. I take them to manage anxiety caused by my wife metastatic breast cancer (at least it's was only oligometastasis and she had a complete response to treatment that she has to continue as long as the disease doesn't come back) and parkison disease. reply Tom3849 19 hours agorootparentI hope your wife recovers well. But my argument is about broad statistics, single case does not prove anything. reply johnnyanmac 1 hour agorootparentYour premise assumes people on antidepressants are not like the GP's wife. We have no reason to assume people are less happy only on antidepressant subscriptions. It could be awareness boosts in mental health, it could be higher stressors because they have more responsibility. reply nashashmi 21 hours agoparentprevThe other part of the story is a Master's degree is a good segway to a career change. So what if the cost is too high. I need a career change. reply alistairSH 20 hours agorootparentI imagine that falls into the chunk that did have a positive ROI. My wife never completed undergrad. Worked her was to director level in the insurance industry. Hated it, got a masters, pivoted to something g she likes without any real pay cut (maybe for a year or two, but she’s now earning more than she ever would have in her first career). reply 2-3-7-43-1807 21 hours agoparentprevthis and also some people study simply out of curiosity about something. reply imtringued 21 hours agoparentprevI don't get your point. Negative ROI means you will have to earn extra money to cover the shortfall through some other method. In other words, it is a rich people's game. You either have to be rich already, or become rich afterwards. reply timmg 20 hours agorootparentGoing on vacation has a “negative roi” if you only look at it from a financial perspective. That doesn’t mean that choosing to travel is always a bad idea. reply resource_waste 21 hours agoparentprev> It's enough to pay the bills, but pays less than she could have had at a corporate job. She got an English degree and couldn't be happier with her choice. My sister says something like this. She is horribly depressed and 'loves her job' and doesnt want to be like her dad who had a great paying job and retired in his 50s. Humans are adaptable. Sales are a bit immoral, there are plenty of jobs that arent immoral. reply mkl 21 hours agoprevThe actual data [1] doesn't seem to agree with the article. There's also usually a huge range of ROI for the same subject from different universities, and it doesn't give numbers of students, so (1) I suspect small sample sizes are skewing the data, and (2) it's impossible to find the percentage of students in negative-ROI fields of study or institutions. > For bachelor's degrees, fine arts, education, and biology programs had the lowest median ROI, while engineering, computer science, and nursing degrees gave students the highest long-term rewards. That doesn't seem to match the data. Based on average (over institutions, not students) ROI from the completed-on-time column, the worst are: Drama/Theatre Arts and Stagecraft, Library and Archives Assisting, Alternative and Complementary Medicine and Medical Systems, Theological and Ministerial Studies, Movement and Mind-Body Therapies and Education, Dance, Personal and Culinary Services, Other, Pastoral Counseling and Specialized Ministries, Mason/Masonry, Zoology/Animal Biology. Not too surprising except maybe the last one. The top ROI are Biomathematics, Bioinformatics, and Computational Biology, Operations Research, Advanced/Graduate Dentistry and Oral Sciences, Mathematics and Computer Science, Systems Engineering, Naval Architecture and Marine Engineering, Computer Engineering, Nuclear Engineering Technologies/Technicians, Marine Transportation, Petroleum Engineering. Again not too surprising. Only 45/338 = 13% of fields of study have a negative average ROI graduated-on-time (17% including not-graduated-on-time). [1] https://freopp.wpengine.com/roi-undergraduate/ reply GartzenDeHaes 21 hours agoparentYou're comparing medians to means. The means are much lower due to the average being skewed upwards by the wealthy. Edit: the averages are much higher than the median of course reply mkl 21 hours agorootparentFair point. Excel's pivot tables can't easily do medians. Edit: Actually, it's pretty similar with medians (via Python now). The bottom 10 are: Drama/Theatre Arts and Stagecraft, Library and Archives Assisting, Movement and Mind-Body Therapies and Education, Dance, Alternative and Complementary Medicine and Medical Systems, Theological and Ministerial Studies, Film/Video and Photographic Arts, Personal and Culinary Services, Other, Music, Mason/Masonry The top 10 are: Advanced/Graduate Dentistry and Oral Sciences, Nuclear Engineering Technologies/Technicians, Biomathematics, Bioinformatics, and Computational Biology, Operations Research, Personal Awareness and Self-Improvement, Naval Architecture and Marine Engineering, Marine Transportation, Systems Engineering, Mining and Mineral Engineering, Computer Engineering. reply RGamma 20 hours agorootparentOne can enable the Power Pivot addin, add the data into a semantic model and define a DAX measure for the median on it. reply maxerickson 20 hours agorootparentprevDo you think that graduate dentistry is a bachelor's? reply mkl 20 hours agorootparentThe people who collected the data categorised it that way for some reason. reply maxerickson 19 hours agorootparentDo you think they included that category in the analysis of bachelor's ROI, which is what you are doing, comparing it to their results for bachelor's. Like sure, a professional degree in dentistry probably does have a better average return than a 4 year degree in engineering, but that isn't the comparison they made. reply Sakos 17 hours agoparentprevI'd also like to know what the ROI is for countries other than the US, particularly ones in Europe, where getting a bachelor degree is significantly cheaper and cost of living (in many places) hasn't risen as much as it has in the US. reply tootie 20 hours agoparentprevLook at the source. It's a Republican activist group supported by Avik Roy with multiple members of the Bush family listed as fellows. reply endisneigh 22 hours agoprevThe government should make it illegal to require degrees to apply IMHO. If an employer can’t create a sufficiently good screening process that would select those who would have degrees anyways, then maybe they aren’t necessary. They also need to stop gate keeping. Jobs like being an attorney inherently require some certification, which is fair. Some governments in the USA made it illegal to take the bar exam and become an attorney without going to law school. Same thing with doctors and Step 1/2. If someone can pass without going to school, good for them. Massachusetts in particular is in clear collusion with schools, requiring school teachers to have masters degrees in education. reply borgdefense 22 hours agoparentIt is an interesting idea. The real problem though are the ridiculous cost of state run schools. As someone who has been out of college for 20 years, I can't believe what my state school charges now. When I graduated my school was a good value compared to private options. That was the whole point of a state school. Now the same school cost 4X what it does and I don't know if I could even get in when up against so many brilliant Indian and Chinese students. I assumed when I went the goal of the institution was for a more educated population in the state. That was the point of the huge difference in price for instate tuition. Now it seems like some kind of money making racket. reply ethbr1 21 hours agorootparentAfaik, state schools have lost a lot of budget from states (in inflation-adjusted terms) compared to pre-90s. Consequently, they balance the books by taking on more foreign/out-of-state students (who they can charge much more). At the core, though, it appears to be a supply and demand problem: there's a ton of demand for people who want to go to college, colleges don't have the resources (or interest?) to expand supply, and so they raise prices because they can. reply nradov 5 hours agorootparentState schools have also been wasting a fortune on gold-plated facilities. I was just at UCSD yesterday and it was ridiculously nice. This is not a responsible use of tuition and tax dollars. We need an austerity program. If that makes the state schools less attractive to top students and professors then so be it. reply qart 22 hours agoparentprevFor some domains, like most software jobs, it might work. In India, you sometimes see children manning pharmacies. It's not because of child-labour, but because dad/mom is away and someone needs to look after the shop. India has laws against this, but so rarely enforced that in effect, one can run a pharmacy without a degree. I dread to think of how many clinics in small towns are run by people without degrees. Heck, we have even had lawyers and judges practicing without degrees. I know only about a handful of such cases because some scandals they got involved in made it into news media. But who knows how many such people are practicing even now! reply makeitdouble 21 hours agoparentprevThe natural evolution is employers pooling their recruiting criteria to not have a single company bare the burden. Then they outsource it, and you end up with private entities managing the qualification tests, and you're back to a system where you pay to get through the door. We already have that for instance with employers requiring MS certification to be sysadmin with the full salary. reply doublepg23 17 hours agorootparent> We already have that for instance with employers requiring MS certification to be sysadmin with the full salary. Hm, I’ve actually had good luck with specializing in Linux. Windows is a given, sure, but any time I’ve interviewed they glanced over my Windows experience and wanted to talk Red Hat. reply karmasimida 13 hours agoparentprevFiltering by school and degrees are indeed effective, there is strong correlation in performance. The entry to lower should be cost of education itself, the degree should be free/cheap enough for someone to obtain if they are technical wise qualified. Time spent on a subject is evidence of investment. reply tim333 19 hours agoparentprevAn alternative solution we had in the UK - I think it's still going but I'm not up to date - is the Open University. It was a government subsidised study at home program that let anyone really get a degree for modest amounts of money and that could be done part time while working. The education was actually good too. (update still going but not super cheap - about £21k for a degree. I think it used to be more subsidised) reply ungreased0675 15 hours agoparentprevI agree. So many jobs have “bachelors degree” in the requirements. Doesn’t matter what kind, just have to have one. I believe companies use this to screen out lower class applicants, because if it had to do with skills a specific degree would be required. For that reason, I think the practice should be prohibited. reply Duwensatzaj 21 hours agoparentprevWhat’s hilarious is cities like Washington DC going the other way and requiring degrees for child care while exempting current child care workers. https://dcist.com/story/22/08/18/dc-child-care-workers-colle... reply imtringued 21 hours agorootparentWow they really want to make childcare unaffordable. reply glasss 22 hours agoparentprevI can generally agree with lowering barriers of entry, even if there would be some social stigma of having a doctor who didn't go to school. My reservation would be around ethics classes, but I guess I just don't know if any of the exams for lawyers or doctors have a set of questions surrounding ethics. reply giaour 22 hours agorootparentThere's an ethics exam you generally need to pass to become a lawyer: https://www.ncbex.org/exams/mpre reply nullc 18 hours agoparentprevThe requirement for degrees is substantially due to the prohibition on IQ tests (or things which could be construed as an IQ test), the degree requirement is a workaround. reply tptacek 10 hours agorootparentThere is no such prohibition. reply nullc 7 hours agorootparentThere are a host of high caliber employment lawyers who disagree with you: The requirement to show any test which could be said to have a racially desperate impact is a business necessity is such a high and risky bar that any case brought over it is an automatic settlement. Any kind of standardized IQ test is going to run into issues because there will be enough data to show statistically that different population groups tend to score lower or higher on it on average and because its scope won't map 1:1 to the business so it can't in and of itself be a necessity. reply tptacek 6 hours agorootparentNot only is it very easy to confirm what I said with a couple Google searches, but I know of huge employers, of the sort that have enormous HR teams, that routinely IQ test applicants. With actual IQ tests. There is no prohibition on IQ testing in US employment. There are lots of reasons not to do it (I sure wouldn't consider a job that required an IQ test). But there's a mythology that IQ testing is the super-effective recruiting tool that American employers are forbidden to use. No, they don't do it because it sucks ass as a recruiting/qualification tool. I invite you to get one of those high caliber employment lawyers to post here, or to reach out, and I'll swap notes. But I don't think it's actually the case that high-powered employment lawyers believe that. That, or huge corporations with lots to lose are casually inviting employment lawsuits in order to run candidates through a test that doesn't predict anything. reply johnnyanmac 1 hour agorootparent>There are lots of reasons not to do it (I sure wouldn't consider a job that required an IQ test). But there's a mythology that IQ testing is the super-effective recruiting tool that American employers are forbidden to use. It's not illegal in the same way it's not illegal to ask the age of your candidate. It's not illegal until it is (in this case, they say their age is > 40 and now any and all actions you take are a legal landmine). The general audience aren't lawyers, so the pedant argument doesn't really mean much in most cases. >That, or huge corporations with lots to lose are casually inviting employment lawsuits in order to run candidates through a test that doesn't predict anything. Do you really think all employers are competent? or that companies have perfect oversight of every hiring manager? I've seen (US) postings saying \"men/women only\" explicitly, discouraging pregnant candidates, and posting salaries below federal minimum wage. The best \"cover\" sometimes is that the hiring audience lacks the awareness nor funds to call them out. reply nradov 5 hours agorootparentprevThe requirement for degrees is substantially due to a surplus of job applicants. Requiring a degree is one way to keep applications down to a manageable level. Employers are quick to discard that requirement when they're unable to fill critical reqs. https://www.cbsnews.com/news/college-degree-job-requirement/ reply lumb63 22 hours agoprevSomething that has bothered me for a while is the cost structure of college. At least at my school, everyone paid the same amount, plus or minus three or four thousand dollars. Meanwhile, a computer science major will earn on the order of double what a psychology major might earn, both as a starting salary, and that will compound over their careers. This is also true of loans. There is one flat interest rate for (federal) student loans. That rate does not reflect the creditworthiness of the borrower at all. It doesn’t consider their major at all. Any intelligent lender would never operate this way; who would give both a computer science major and basket weaving major the same amount of money and claim (via interest rate) that both loans are equally risky? We have the concept of prices to serve as tools to encourage or discourage people from pursuing ventures society needs (or, really, is willing to pay for) more or less. It seems to me that in the context of higher education the economics are fundamentally broken. This causes a lot of grief for a lot of people, who then claim socialized higher education is the answer. It might be better than what we have; I’m not sure. That said, I’m inclined to think the free market would solve a lot of the problems seen in the current student loan situation in the US. There would be no kids with low-value degrees in $250k of debt graduating with a job paying $30k/yr. That simply shouldn’t happen. reply sidewndr46 21 hours agoparentInterest rates not reflecting the creditworthiness of the borrower is a design feature of federal loans. The idea being the government should at least subsidize loaning money to people for education. In practice it just allows universities to raise tuition to the moon. reply ethbr1 21 hours agorootparentExactly. Equality of opportunity, to the extent achievable, is important to preserve. But it needs to be done in a way that universities don't simply absorb any funding by raising tuition. IMHO, we should start by taking the ACA approach on admin overhead. I.e. If you want federal loans/grants to be usable on tuition at your institution, here's the administrative cost (\"funds not directly spent on teaching\") cap. If you exceed that, then you must rebate students the overage. reply johnnyanmac 1 hour agorootparent>But it needs to be done in a way that universities don't simply absorb any funding by raising tuition. Again, who's going to stop them? The government? No one in power has the incentive to change the system. And they wont until it's too late. reply Fredkin 21 hours agoparentprevThe prices thing doesn't work well here because 1) the government has intervened, and 2) it's based on debt - the price of which is also controlled by the government/central bank. It's a skin-in-the-game problem. If education was at least partially funded by equity, the investor in the person receiving the education, be it a private investor, company they'll work for, or even the university itself, would have taken on the risk for failure to produce any sort of ROI. I say partially, because I believe there is some merit in government subsidy for certain fields, but more of that should go towards reducing the cost of university facilities, on-campus accommodation, labs, libraries etc. rather than student finances. reply skhunted 22 hours agoparentprevIt seems to me that in the context of higher education the economics are fundamentally broken. There are lots of examples of this type of situation in the U.S. Suburbia is subsidized through the road system. Gas taxes and vehicle registration aren’t enough to pay for the road system. K-12 education is subsidized by non parents. Police services are subsidized. There’s a ton of corporate welfare too. There are some things for which a free market solution is advisable and some things for which it not advisable. I think education falls into the “not advisable” group. reply tzs 15 hours agoparentprev> There would be no kids with low-value degrees in $250k of debt graduating with a job paying $30k/yr. Very few people incur anywhere near that much debt for an undergraduate degree. Getting into the $200k+ debt neighborhood generally happens from law school or medical school. reply xboxnolifes 3 hours agorootparentI did not end up taking these particular college offers, but ~10 years ago I had offers for ~$50,000 a year after financial aid from multiple colleges. In 2024 I wouldn't be surprised if the same offer to me would be $75,000 a year. The year after year price adjustments I've see colleges make are insane. reply nradov 5 hours agorootparentprevIt's pretty common for students to incur that much debt if they get both a Bachelor's and Master's degree from private colleges. Obviously this is not a smart financial decision, and the federal government shouldn't be encouraging it with subsidized student loans. \"More than 800 people applied this year for roughly 72 spots in the film MFA program, which can total nearly $300,000 for tuition, fees and living expenses.\" https://www.wsj.com/articles/financially-hobbled-for-life-th... reply BeFlatXIII 20 hours agoparentprevThe people entering the workforce with $250k in debt are mostly physicians. It’s mosrly people with $20–60k in debt that are stuck in underemployment (or didn’t graduate, so never got the salary bump to pay off such loans). reply coldtea 21 hours agoparentprev>Meanwhile, a computer science major will earn on the order of double what a psychology major might earn, both as a starting salary, and that will compound over their careers. So? Assuming a fair charge (not the US kind of exorbitant tuition), the costs are similar (salaries from professors, administrative stuff, buildings, libraries, labs, and so on). You're not paying to buy a career, you're paying to buy an education. >There would be no kids with low-value degrees in $250k of debt graduating with a job paying $30k/yr. That simply shouldn’t happen. Yes. Or we could forbid poor people from getting their desired degrees altogether, what you describe is the same thing but with more steps. reply ethbr1 21 hours agorootparent> Or we could forbid poor people from getting their desired degrees altogether, what you describe is the same thing but with more steps. We probably should. There's little difference between \"I graduated with an uneconomic degree, and then failed to get a job in that market and/or labored under student debt I'll never be able to pay off\" and \"They wouldn't give me money for an uneconomic degree\". If anything, the latter is kinder because it nudges the college student into either funding their own uneconomic degree up-front (if they can find the money) or making other decisions. It's always seemed insane to me that we let 18-year olds, sometimes with little financial education, make decisions that will benefit or haunt them for the rest of their life with so little guidance. (And college counselors have very conflicting incentives) reply nebula8804 20 hours agorootparentIt may be better to tackle this issue at the school level than to pick and choose what constitutes a valuable degree because that can change depending on what the market wants. What may be true at student entrance may be totally false at student graduation. Isn't this a solved problem in Europe? The state treats public colleges as a utility to invest in the population. So the schools may not be as glamorous but they are easily and cheaply available for people who want to put in the work regardless of major. reply ethbr1 17 hours agorootparentI guess the issue with college funding is: is it an entitlement or an incentive program? If it's an entitlement, we could just give people money whether or not they choose to spend it on college. If it's an incentive (buying a good outcome for society), then it seems fair that the funder has some say in what they're buying. I've heard Canada does have a serious problem with oscillating supply/demand mismatch in their medical education funding. But it feels like that could be smoothed by using look back averaging & creating a +/- % system vs a hard, \"there are no more slots\" one. reply lapcat 20 hours agorootparentprev> Assuming a fair charge (not the US kind of exorbitant tuition) The submitted article was about \"American colleges and universities\", so we have to assume the US kind of exorbitant tuition. If college students didn't have to pay exorbitant tuition and take out massive student loans, we probably wouldn't even be having this discussion about ROI. reply coldtea 18 hours agorootparent>The submitted article was about \"American colleges and universities\", so we have to assume the US kind of exorbitant tuition I know. My point was \"EVEN assuming a fair charge, the costs for different degrees would STILL be similar (classrooms built and maintained, administrative salaries, campus maintenance, libraries, teacher salaries, etc)\". So, why would you expect different degrees to charge different tuition? >If college students didn't have to pay exorbitant tuition and take out massive student loans, we probably wouldn't even be having this discussion about ROI. @lumb63 was bothered about how different degrees are paying the same - which is a question not tied to the tuition being exorbitant or not. And I think his point was to use cost of the degree to discourage people from degrees that don't make money. reply lapcat 17 hours agorootparentWhat's a \"fair\" charge? Students don't pay all of the costs to run a university even in the US, and certainly not outside the US. Students don't pay any of the costs to run a public high school, after all. But a big problem nowadays is that students are paying a greater share of the public university costs, since the government is subsidizing a smaller share of the costs. If you go back 60 years or so, it wasn't the case. That's why Boomers brag about paying for their college education by working summer jobs and such, because they could! Unfortunately, college costs have become less socialized and more privatized. Different degrees do have significantly different costs, though. To some extent, market forces govern professor salaries, and professors in different departments can make vastly different salaries. Still, universities distort market forces by bundling all of the \"goods\" into one big package. Imagine if there were separate universities for each department instead of one university covering every department. For example, there would be separate engineering and philosophy universities. The engineering university would have a much higher price, not just due to the higher costs of engineering professors and the materials required by the department but also due to demand for an engineering education. The payoff of a degree with greater economic prospects makes the students more willing to pay a higher price for the degree. (EDIT: One might argue that public universities artificially inflate philosophy professor salaries, for example, beyond what they could command without the help of the other bundled departments.) To be clear, I'm not suggesting that universities should be run according to market forces. On the contrary, I think higher education should be mostly subdizied just like high school and lower education. My point is that \"costs\" are not inherently fixed. They're subject to economics, both supply and demand, and you can't ignore demand as a crucial factor. Thus, when we think in terms of the costs and price of a university education, I think it's very fair to consider the ROI. reply coldtea 9 hours agorootparent>What's a \"fair\" charge? Students don't pay all of the costs to run a university even in the US, and certainly not outside the US Outside the US yes. In the US, with what students pay (including the parts covered by subsidies and loans), they could run 2 universities, not just one! Private colleges do exist outside the US, the students don't pay US college prices to study there, but they still manage to survive and be profitable. But the operating costs of US universities are absurdly high (because they can, not because they need to). Even the \"a-list academics\" with the huge salaries are more marketing assets to draw students in than real contributors to their education. Even without the subsidies and loans, it should not be infeasible to have a university supported by tuition feeds and still be affordable and good. And with state subsidies it is US universities are not coveted because they're so better, they're coveted because they give privileged access to the US market and in a vicious circle the US academic employments. It's where the money and prestige are at, that's their draw. But kids studing in much cheaper universities Europe, Asia, and so on, come to the US and crush it, as graduate/post-graduate students, or working in IT or STEM. They oftensmoke US-born kid who studies as undergradutes in the US. So it's not like their much cheaper education and much cheaper universities were subpar. And this means the US universities could also be much much cheaper and be good too. It's the incentives which are misaligned for that, not some impossibility... reply lapcat 8 hours agorootparent> US universities are not coveted because they're so better, they're coveted because they give privileged access to the US market and in a vicious circle the US academic employments. It's where the money and prestige are at, that's their draw. Of course. This was my point about the demand side of supply and demand. > And this means the US universities could also be much much cheaper and be good too. It's the incentives which are misaligned for that, not some impossibility... I didn't say it was impossible. Indeed I was talking about the incentives. You're almost making my point for me by mentioning that people are flocking to the US for economic opportunities, while US students are not flocking to other countries for a cheaper education. When you talk about a \"fair\" charge, that can be understood in a number of ways. In terms of market price, what's fair in one country may not be fair in another country, because the markets are different, the supply and demand is different. Whereas if you're talking about fairness in a kind of moral or societal sense, I think it's fair to ask whether the price of a social service ought to depend in some way on one's income, in such a way that the wealthy beneficiaries of the system pay more than the poor. That's a legitimate sociopolitical debate, nothing strange about it, I'd say. After all, certain forms of college financial aid have always depended on those factors. reply j-krieger 22 hours agoprevWhat RoI misses is that an available, stable income is far more important than getting paid more than it was worth to spend. A master's degree may not be an advantage in an employee's economy, but when the table turns and possible employers get hundreds of applications, you may have a better chance making it past their filter with a degree. You can currently see this being the case. Among the people I know that were laid off, ex-bootcampers and self-thought developers have worse chances getting application replies than those with degrees. reply imtringued 21 hours agoparentNegative ROI means you end up paying for the privilege of having your job vs not having the degree at all. A negative ROI bachelor means you could work as a cashier and earn more in net than someone who gets an expensive degree and a job that is not paying enough to offset the student loan. A negative return master means that you would earn more in net if you stopped studying after your bachelor. In other words, you are sacrificing your bachelor earnings edge to get a masters, which is why it is more common for a master to have a negative ROI. You're already \"rich\" after the bachelor. reply brettcvz 20 hours agoprevFor those interested in this topic, I’d strongly recommend Bryan Caplan’s “The Case Against Education”: https://press.princeton.edu/books/hardcover/9780691174655/th.... He does a much more thorough analysis of the value of a college degree than any other I’ve seen, including factoring in dropout rate, majors, meeting potential spouses, opportunity cost, university prestige, etc. The summary aligns with common sense: college degrees are typically valuable, especially in in-demand fields and from prestigious universities. But the college dropout rate is around 50%, so many people incur the cost of college with little of the benefit. If you’re a high schooler considering college, it’s worth it if you are “good at school” and so confident you can finish in four years and are going to a high quality state school or better. Otherwise, you’re better off going directly into the workforce. reply mettamage 20 hours agoparentNot just meeting potential spouses. Having studied psychology has helped my romantic relationships in odd and intangible ways. For 1, talking to women I didn't know was easier because while they couldn't relate to my programming interest, they could relate to my psychology interest. For 2, it was just easier to find someone given that you were in an environment were 80% of your students were female (me being male). But what also really helped was knowledge on anxious/avoidant attachment styles or how intuition is works (I read a lot about Kahneman's research papers - not his pop science books). By understanding how intuition works, I was able to train it through meditation (I could get the academic sources but this would become a lecture). When I got a stronger intuition I could relate better to people in general that use their intuition as their default mode (something I never did as a kid). Psychology also has helped me with some mental health issues long after I graduated from it. I was surprised because I wasn't that interested in the mental health aspect when I was studying it (I liked neuroscience and statistics). I recognize mental issues with myself early so I can start acting on it early as well. This is the tip of the iceberg. My point is: I never expected these benefits. But they are very very real. Moreover, for some these benefits do not pan out this way. A friend of my also studied psychology and would've loved to have a girlfriend at the time but it didn't work out for him. In my case, it helped that I was a bit socially bold. I was socially insecure as well but it doesn't matter that one is insecure when they are socially bold (I can say that with hindsight, haha). reply BeFlatXIII 19 hours agoparentprevYour point cannot be echoed loud enough on discussions of college prices and debt forgiveness. Instead, the too responses on such articles are always someone ranting about the lunacy of putting yourself $115k in the hole for a Bachelor+Master’s of Social Work only to manage a Home Depot followed by someone else pointing out that people graduating with that much debt are almost all physicians (plus a few lawyers). Dropouts, by definition, do not have the degree to earn that salary bump. Those who go on about rule of law and the sanctity of contracts or whatever are willfully blind to the point that forgiven or not, those loans to the dropouts aren’t getting repaid. Heck, you can even see the same thing to a lesser degree for students who do graduate but with a 2.3 GPA (especially if it’s a field that de facto requires a Master’s). reply 39896880 22 hours agoprevWhat’s the ROI when you factor in having an educated population that can tell fact from fiction, engage in critical thinking, and resist fascism? reply dijit 21 hours agoparent> What’s the ROI when you factor in having an educated population that can tell fact from fiction, engage in critical thinking, and resist fascism? That's a strong assertion, I'm sure university level education can help give tools to people to help people understand facts, think critically etc if they study the right thing; I would be really cautious of applying that universally. What I've seen mostly is a different kind of groupthink, where critical thinking and open, curious and honest discussion is not tolerated. I could be wrong and my experiences are of course anecdotal. I get much more honest and genuinely curious discussion on HackerNews than from University graduates. Yes, a better educated population is considered a net benefit for society, which was the impetus for mandatory education, evidently society considers that there are diminishing returns and those happen at around age 16 for British citizens. I would like to understand who decided that age and why. reply markleeville 8 hours agorootparent> What I've seen mostly is a different kind of groupthink, where critical thinking and open, curious and honest discussion is not tolerated. In my experience it really hasn't been hard to find a home for every position at a reasonably-sized school: fraternities and clubs are all over the spectrum (often actively endorsed and sponsored by the school). Even at UC Berkeley, one of the largest organizations here is the Berkeley College Republicans — and last year I attended a Turning Point USA event officially hosted by San Francisco State University perfectly fine (even with it being the \"leftist\" school that established Ethnic Studies). A generalization that is worded like a sci-fi blockbuster logline doesn't seem to square with critical discussion — if anything it comes across as a redigestion of online forum posts and second-hand accounts. It's hard to say any university of a reasonable size has one monolithic culture, even within the same department. reply lapcat 21 hours agorootparentprev> I get much more honest and genuinely curious discussion on HackerNews than from University graduates. Are HN commenters not mainly university graduates? My impression is that HN commenters are disproportionately university graduates compared to other social networks. reply johnnyanmac 1 hour agorootparentYes, but it's still a selection bias of commenters in a specific field, who care enough about an old school forum to comment in a community. I can't personally imagine most of my college peers caring enough to post anything longer than a tweet. Not even because they are dumb nor lack critical thinking. They simply care a lot less about online communities compared to physical relationships, or are focused on their careers, etc. Commenters on any community are subject to the 1% rule; there are a LOT more lurkers than commenters on any given platform. reply dijit 21 hours agorootparentprevcorrelation != causation, I would be surprised if it wasn't a large mix of autodidacts, university graduates, bootcampers and people who fell in (helping out on some tech stuff at work or had to learn something to get a business bootstrapped) but became more and more curious over time. One of the virtues of the industry being extremely immature is that the gates are not fully established, so you are more likely to have a strong mix of paths to reach senior. Especially amongst devops/infra, \"full stack\" developers or founders. Which seems to be the usual haunt on here. reply lapcat 21 hours agorootparent> I would be surprised if it wasn't a large mix of autodidacts, university graduates, bootcampers Speaking of the tech industry, autodidacts and bootcampers are not mutually exclusive with university graduates. For example, I taught myself computer programming, but I also have a Masters degree in philosophy. In any case, what exactly is your experience in discussion with university graduates exclusively, that you can compare it with Hacker News discussion? By the way, I suspect that my esteem for HN discussion is a lot lower than yours. reply graemep 21 hours agorootparentI studied a bit of programming in other degrees but am mostly self taught. I have a masters in financial economics and another in business. reply dijit 21 hours agorootparentprev> Speaking of the tech industry, autodidacts are bootcampers are not mutually exclusive with university graduates. Definitely true, but it's largely semantics at this point, the original discussion was regarding the notion that university prepares those who attend to fight fascism by being immune to propaganda or disinformation. > what exactly is your experience in discussion with university graduates exclusively, that you can compare it with Hacker News discussion? This will be a controversial point because I will be accused of having \"a side\" (because this is the world we have become now it seems). However what I have witnessed and interacted with is that students largely convert to the university prevailing ideology and will follow it as dogma. This can be for social pressure reasons, or I would venture that it could even be part of how the environment has to be for undergrads (IE: ingestion techniques largely focus on regurgitation, not innovation or deep thought, only surface level if anything). Thus when people I interact with talk to me, they can hold opposing ideas in their head simultaniously without being able to reason about why they hold that belief. For an extremely prescient example, Swedish university students will march for Palestine against Israel using flags made in China (where there is a huge active genoice against the Uyghur Muslim population). Additionally they will loudly support Palestine while simultaneously supporting gay and trans rights- which is clearly a huge conflict. Sweden as a country itself is already pro-Palestine, being one of the first countries to be so (first country in Europe to recognise it as a state for example). Similarly the issues of Feminism have come to Sweden, when Sweden does the most (by far) to promote equality of gender, womens rights and the right to bodily autonomy -- but when questioned on what rights they will be unable to come up with anything other than really high level buzzwords like Patriarchy, the plight of the Swedish Feminist typically lives on the subjugation of women in other countries (as a fallback). It's ok to have these beliefs, of course it is, so please don't misunderstand me. It just becomes clear when discussing things that people who espouse these views from university and believe in them fervently (to the point where they cannot even engage in conversation without thinking that you are a Nazi or a Misogynist or a Zionist or some \"other\" group label) -- clearly have only surface level knowledge of the issue. It's pure ignorance and profound belief that propels the thought because of this they can't reason it or think critically about it, so they immediately reject a conversation. If they are forced into a conversation they become quickly defensive and frustrated as they lack the base understanding of the issue so they can't reason nuance or reason why situations might be different between seemingly similar issues. Similarly those who study economics can only think of the world as money generating and revenue and begin to lack basic empathy of the human condition. -- It's like they've had critical thinking and nuance stripped. (this is a huge generalisation of course). > I suspect that my esteem for HN discussion is a lot lower than yours. Probably not, I also don't regard HN discussion as the highest form of conversation, but it's far-and-away from the worst. University students are not equal, many many university students have just figured out that they need to regurgitate what they hear to get by: and they do so, without understanding deeply or thinking critically. reply lapcat 20 hours agorootparent> the original discussion was regarding the notion that university prepares those who attend to fight fascism by being immune to propaganda or disinformation. I'm neither defending nor disputing that notion. I'm just questioning the assumption that HN commenters are more enlightened as a group than university graduates. > what I have witnessed and interacted with Those are two different things. Passively watching the news, for example, doesn't count as having a discussion with university graduates; not to mention that the news isn't a representative sample. I was asking about your experience with discussions. And it feels like you're cherry-picking very specific issues, when of course university graduates have discussions about an extremely wide variety of issues, just as Hacker News commenters do. > students largely convert to the university prevailing ideology and will follow it as dogma What exactly do you think is the university prevailing ideology? Note that university administrators have mostly been vehemently, sometimes violently opposed to student protesters. In any case, nobody arrives at university as a blank slate. Students bring their preexisting views with them. Moreover, the student population is hardly uniform. You'll always find a significant number of conservative Christian students, for example, at any large university in the US. Somehow they escaped indoctrination (at university). > Similarly those who study economics can only think of the world as money generating and revenue and begin to lack basic empathy of the human condition. -- It's like they've had critical thinking and nuance stripped. (this is a huge generalisation of course). This is a huge stereotype of course. But you also have to ask yourself which students are attracted to the study of economics in the first place. Your major and coursework is mostly voluntarily chosen at college, which raises more questions about the \"indoctrination\" theory in general. reply dachworker 21 hours agoparentprevI don't think being more educated makes you more resistant to fascism. I wouldn't be surprised if there was a correlation in the other direction, because after all education at a higher level requires a huge degree of conscientiousness, and although typically being conscientious is a good trait, it's exactly the mechanism that fascists abuse to their advantage. reply Fredkin 21 hours agoparentprev1) You're assuming the educational institutions themselves are engines of anti-fascism. But plenty of very fascist ideas have come out of universities. 2) You're ignoring communism and the far-left. Plenty of disastrous ideas are produced in a vacuum by intellectuals at universities. To give just one example: Khieu Samphan, was educated at the Sorbonne. His ideas lead to the deaths of a quarter of Cambodia's population. 3) Even if we give you 1 and 2, where do you stop? How much money should be poured into education (esp if it's the taxpayer)? Resisting fascism seems like one of those infinite 'our work is never done' things that have unbounded demands. reply PartiallyTyped 19 hours agorootparentI will only speak regarding 3; I'd argue that the fraction of income that goes to administration should be regulated and capped to a fraction of the whole. Ultimately, most administration has little to do with education, and lots to do with expanding and increasing its own income. On the other hand, giving money to institutions like NASA, CERN, INRIA and the likes is overall beneficial to society and its out of such institutions that we can make progress that benefits the overall good. It's been established that companies exist only for the purpose of enriching the stock holders (according to CEOs that follow Welch's paradigm), therefore companies don't generally have any incentive to push for innovation and publish their findings unless that — in one way or the other — benefits them. reply BeFlatXIII 19 hours agoparentprevWhy can’t those skills be taught in K–12? None of them sound advanced enough to require university education. reply PartiallyTyped 19 hours agorootparentI don't think it's a matter of complexity as much as it is a matter of development. Highschoolers have an under-developed prefrontal cortex — this region of the brain fully forms by 25. > This brain region is involved in a wide range of higher-order cognitive functions, including speech formation (Broca's area), gaze (frontal eye fields), working memory (dorsolateral prefrontal cortex), and risk processing (e.g. ventromedial prefrontal cortex). The basic activity of this brain region is considered to be orchestration of thoughts and actions in accordance with internal goals.[3] Many authors have indicated an integral link between a person's will to live, personality, and the functions of the prefrontal cortex.[4] > This brain region has been implicated in executive functions, such as planning, decision making, working memory, personality expression, moderating social behavior and controlling certain aspects of speech and language.[5][6][7][8] Executive function relates to abilities to differentiate among conflicting thoughts, determine good and bad, better and best, same and different, future consequences of current activities, working toward a defined goal, prediction of outcomes, expectation based on actions, and social \"control\" (the ability to suppress urges that, if not suppressed, could lead to socially unacceptable outcomes). Source: https://en.wikipedia.org/wiki/Prefrontal_cortex reply glasss 22 hours agoparentprevSome economists insist that everyone and every organization should only be prioritizing efficiency of gathering capital. reply lupusreal 16 hours agoparentprevSeriously compare these two sales pitches from the perspective of a teenager who's nervous about starting out life on their own: \"Take out a student loan so you can get a degree and get a good job that will pay off the loan and get you secure.\" vs \"Take out a student loan, even though you have a 50% chance of dropping out and being left a renter working at McDonalds and a mountain of student debt... so that you'll know how to fight fascism.\" Obviously the financial ROI is what is most important here to the people who have a greatest stake in matter. Getting people into financial trouble to prepare them to fight any sort of political extremism is probably completely counterproductive anyway; if somebody is trapped in debt and looking for solutions they're more likely to fall in with rising populists figures. reply willcipriano 22 hours agoparentprevHow can you tell fact from fiction without having a teaching assistant a few years older than tell you what the facts are? reply TMWNN 17 hours agoparentprevThe German university system was probably the finest in the world before WW2. German university students were at the forefront of the Nazi takeover. reply malfist 22 hours agoparentprev> having an educated population that can tell fact from fiction, engage in critical thinking, and resist fascism? That's contrary to Reason's (the publisher) goals reply vundercind 20 hours agorootparentI don’t think HN is as aware that Reason is basically a joke of a publication that’s so consistently and provably wrong you’d think that was their goal (and, in fact, in a way it is) the way some online spaces are. It’s not a source familiar to this crowd. reply ungreased0675 15 hours agorootparentThat’s a textbook ad homenim attack. Do you have an issue with the article specifically? The methodology? reply vundercind 12 hours agorootparentIt’s basic media literacy. Even if Sean Hannity says the sky’s blue, I’m not gonna cite him. Again: HN’s not very familiar with this source, as other crowds would be. reply nickpsecurity 13 hours agoparentprevToday, are doing the opposite of your stated goals. They’re emphasizing specific beliefs (eg Cultural Marxism or feminism), dogmatism over discovery by evidence, feelings over facts for topics like gender, and canceling those who dissent. It’s best to avoid co",
    "originSummary": [
      "Research by FREOPP reveals that around 50% of master's degree programs yield a negative return on investment (ROI), contrasting with the generally positive ROI of bachelor's degrees.",
      "When selecting a degree program, factors such as post-graduation earning potential and institutional completion rates should be carefully weighed due to the higher risk associated with master's and associate degrees.",
      "It is crucial for prospective students to assess the prospective benefits of their selected major and educational institution to make informed decisions before committing to college."
    ],
    "commentSummary": [
      "The discussion explores ROI in higher education, gender pay gaps, societal influences on career choices, challenges in creative careers, and education's impact on critical thinking skills.",
      "It addresses the high cost of college, student debt, and the broader value of education beyond monetary returns.",
      "Emphasizes education's role in countering extremism, fostering diverse perspectives, and advocates for critical thinking, empathy, and open-mindedness in both education and society."
    ],
    "points": 179,
    "commentCount": 315,
    "retryCount": 0,
    "time": 1715512100
  },
  {
    "id": 40335209,
    "title": "Brain Harnesses Quantum Effects, Confirms Study",
    "originLink": "http://backreaction.blogspot.com/2024/05/brain-really-uses-quantum-effects-new.html",
    "originBody": "Pages Home Talk To A Scientist Comment Rules About Sunday, May 12, 2024 Brain Really Uses Quantum Effects, New Study Finds When Roger Penrose originally came out with the idea that the human brain uses quantum effects in microtubules and that was the origin of consciousness, many thought the idea was a little crazy. According to a new study, it turns out that Penrose was actually right… about the microtubules anyways. Let’s have a look. Posted by Sabine Hossenfelder at 11:00 AM Labels: Quantum foundations, Video Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest No comments: Post a Comment COMMENTS ON THIS BLOG ARE PERMANENTLY CLOSED. You can join the discussion on Patreon. Note: Only a member of this blog may post a comment. Older Post Home Subscribe to: Post Comments (Atom) Support me on Patreon Buy my book (paid link) Follow me on Twitter Tweets by @skdh More about me Sabine on Twitter Sabine on YouTube Sabine on Facebook Sabine's Homepage Sabine on Google Scholar Sabine's ArXiv Papers Search this blog Follow me on facebook Labels Physics (714) Video (470) Science and Society (249) Papers (220) Astrophysics (195) Quantum Gravity (169) Random Thoughts (163) Particle Physics (131) Books (119) Academia (115) Cosmology (109) Science (108) Philosophy (90) Travel (79) Distraction (70) Blog (67) Quantum foundations (63) Sociology of Science (57) Photo (56) Rant (49) History of Science (48) Physicists (47) Dear Dr B (27) Politics (24) Psychology (14) Sociology (9) Blog Archive ▼ 2024 (123) ▼ May 2024 (10) Brain Really Uses Quantum Effects, New Study Finds CERN Looks for Origins of Quantum Randomness Scientific Progress is Slowing Down. But Why? The era of nuclear precision has begun NASA named an asteroid after me How does gravity escape a black hole? Why Flat Earthers Scare Me Bird Flu Spreads to Dairy Cattle, Farm Worker Infe... Many Worlds Theory Gets Even More Worlds Fusion in Space: New Plasma Thruster Tested Succes... ► Apr 2024 (29) ► Mar 2024 (29) ► Feb 2024 (29) ► Jan 2024 (26) ► 2023 (116) ► Dec 2023 (24) ► Nov 2023 (9) ► Oct 2023 (8) ► Sep 2023 (8) ► Aug 2023 (7) ► Jul 2023 (10) ► Jun 2023 (8) ► May 2023 (9) ► Apr 2023 (9) ► Mar 2023 (9) ► Feb 2023 (8) ► Jan 2023 (7) ► 2022 (74) ► Dec 2022 (10) ► Nov 2022 (9) ► Oct 2022 (9) ► Sep 2022 (6) ► Aug 2022 (5) ► Jul 2022 (5) ► Jun 2022 (4) ► May 2022 (5) ► Apr 2022 (5) ► Mar 2022 (5) ► Feb 2022 (5) ► Jan 2022 (6) ► 2021 (63) ► Dec 2021 (5) ► Nov 2021 (5) ► Oct 2021 (6) ► Sep 2021 (5) ► Aug 2021 (4) ► Jul 2021 (5) ► Jun 2021 (4) ► May 2021 (7) ► Apr 2021 (6) ► Mar 2021 (6) ► Feb 2021 (6) ► Jan 2021 (4) ► 2020 (84) ► Dec 2020 (6) ► Nov 2020 (6) ► Oct 2020 (10) ► Sep 2020 (5) ► Aug 2020 (7) ► Jul 2020 (5) ► Jun 2020 (6) ► May 2020 (8) ► Apr 2020 (10) ► Mar 2020 (7) ► Feb 2020 (9) ► Jan 2020 (5) ► 2019 (119) ► Dec 2019 (8) ► Nov 2019 (7) ► Oct 2019 (6) ► Sep 2019 (8) ► Aug 2019 (9) ► Jul 2019 (10) ► Jun 2019 (14) ► May 2019 (13) ► Apr 2019 (9) ► Mar 2019 (10) ► Feb 2019 (13) ► Jan 2019 (12) ► 2018 (93) ► Dec 2018 (8) ► Nov 2018 (12) ► Oct 2018 (9) ► Sep 2018 (7) ► Aug 2018 (7) ► Jul 2018 (6) ► Jun 2018 (10) ► May 2018 (7) ► Apr 2018 (8) ► Mar 2018 (8) ► Feb 2018 (4) ► Jan 2018 (7) ► 2017 (73) ► Dec 2017 (4) ► Nov 2017 (7) ► Oct 2017 (8) ► Sep 2017 (5) ► Aug 2017 (5) ► Jul 2017 (4) ► Jun 2017 (6) ► May 2017 (8) ► Apr 2017 (7) ► Mar 2017 (6) ► Feb 2017 (6) ► Jan 2017 (7) ► 2016 (76) ► Dec 2016 (7) ► Nov 2016 (6) ► Oct 2016 (5) ► Sep 2016 (5) ► Aug 2016 (7) ► Jul 2016 (4) ► Jun 2016 (5) ► May 2016 (7) ► Apr 2016 (6) ► Mar 2016 (7) ► Feb 2016 (8) ► Jan 2016 (9) ► 2015 (90) ► Dec 2015 (11) ► Nov 2015 (9) ► Oct 2015 (8) ► Sep 2015 (8) ► Aug 2015 (10) ► Jul 2015 (5) ► Jun 2015 (7) ► May 2015 (5) ► Apr 2015 (8) ► Mar 2015 (6) ► Feb 2015 (5) ► Jan 2015 (8) ► 2014 (80) ► Dec 2014 (7) ► Nov 2014 (7) ► Oct 2014 (7) ► Sep 2014 (5) ► Aug 2014 (8) ► Jul 2014 (5) ► Jun 2014 (7) ► May 2014 (7) ► Apr 2014 (7) ► Mar 2014 (3) ► Feb 2014 (7) ► Jan 2014 (10) ► 2013 (96) ► Dec 2013 (8) ► Nov 2013 (6) ► Oct 2013 (10) ► Sep 2013 (9) ► Aug 2013 (9) ► Jul 2013 (9) ► Jun 2013 (7) ► May 2013 (9) ► Apr 2013 (8) ► Mar 2013 (6) ► Feb 2013 (7) ► Jan 2013 (8) ► 2012 (126) ► Dec 2012 (8) ► Nov 2012 (8) ► Oct 2012 (8) ► Sep 2012 (9) ► Aug 2012 (11) ► Jul 2012 (11) ► Jun 2012 (10) ► May 2012 (13) ► Apr 2012 (15) ► Mar 2012 (11) ► Feb 2012 (12) ► Jan 2012 (10) ► 2011 (123) ► Dec 2011 (30) ► Nov 2011 (11) ► Oct 2011 (8) ► Sep 2011 (9) ► Aug 2011 (9) ► Jul 2011 (10) ► Jun 2011 (9) ► May 2011 (7) ► Apr 2011 (6) ► Mar 2011 (7) ► Feb 2011 (8) ► Jan 2011 (9) ► 2010 (139) ► Dec 2010 (8) ► Nov 2010 (9) ► Oct 2010 (9) ► Sep 2010 (8) ► Aug 2010 (9) ► Jul 2010 (12) ► Jun 2010 (13) ► May 2010 (13) ► Apr 2010 (15) ► Mar 2010 (15) ► Feb 2010 (14) ► Jan 2010 (14) ► 2009 (167) ► Dec 2009 (16) ► Nov 2009 (11) ► Oct 2009 (10) ► Sep 2009 (12) ► Aug 2009 (11) ► Jul 2009 (15) ► Jun 2009 (16) ► May 2009 (13) ► Apr 2009 (15) ► Mar 2009 (15) ► Feb 2009 (16) ► Jan 2009 (17) ► 2008 (267) ► Dec 2008 (37) ► Nov 2008 (19) ► Oct 2008 (24) ► Sep 2008 (20) ► Aug 2008 (21) ► Jul 2008 (24) ► Jun 2008 (23) ► May 2008 (21) ► Apr 2008 (20) ► Mar 2008 (18) ► Feb 2008 (18) ► Jan 2008 (22) ► 2007 (311) ► Dec 2007 (36) ► Nov 2007 (23) ► Oct 2007 (24) ► Sep 2007 (19) ► Aug 2007 (24) ► Jul 2007 (24) ► Jun 2007 (19) ► May 2007 (24) ► Apr 2007 (24) ► Mar 2007 (30) ► Feb 2007 (31) ► Jan 2007 (33) ► 2006 (152) ► Dec 2006 (27) ► Nov 2006 (19) ► Oct 2006 (14) ► Sep 2006 (13) ► Aug 2006 (13) ► Jul 2006 (17) ► Jun 2006 (17) ► May 2006 (10) ► Apr 2006 (11) ► Mar 2006 (10) ► Feb 2006 (1) Visitor Count Statcounter Feeds Blogger Atom Feed Blogger Comment Feed Blogger RSS Feed Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=40335209",
    "commentBody": "Brain Uses Quantum Effects, New Study Finds [video] (backreaction.blogspot.com)165 points by lisper 18 hours agohidepastfavorite140 comments alevskaya 14 hours agoThis referenced paper seems like primarily a theoretical modelling paper (almost all of its figures are simulations?) that contains as far as I can read 3 (!) actual experimental measurements in bulk on a fluorospectrophotometer. The claim is that the observed increased fluorescent quantum yield (QY) of microtubules over tubulin can be explained by the ideas in their simulations. It's hard to buy that their proposed stories are the simplest explanation for these few measurements. Much more boring phenomena can influence QY. e.g. simply occluding fluorophores from the bulk solvent can have a huge influence on QY and spectra. (I used to design biological fluorescent reporter reagents...) reply QuantumG 12 hours agoparentOr to put it more clearly: bullshit Every article on Roger Penrose's nonsense is just like this. It's stupid pseudoscience meant to make dumb people feel better about being meatbags. reply berkeleynerd 56 minutes agorootparentI think free will is a bit like a sphere. There is no true sphere in nature, only a more or less accurate approximation. Likewise for free will, there is no such thing as true free will (tm) but there are better and worse approximations. reply kunley 4 hours agorootparentprevI honestly think, everyone who uses the term \"meatbag\" should see a therapist sometimes. There is so much negativity in this term reply CalRobert 1 hour agorootparentI generally dislike the word as well, _however_ it sorta makes sense in this context since it makes sense to think about the composition of our brains (and bodies). We are, of course, meat. reply naasking 11 hours agorootparentprevThe paper is not connected to Penrose. reply CooCooCaCha 12 hours agorootparentprevYeah I put this in the same bucket as religion and freewill. These are things people cling onto because we so desperately want to feel special and magical. It's similar to learning that the universe doesn't literally revolve around us. reply umvi 11 hours agorootparent> free will Assuming free will doesn't exist, how would a world where free will does exist be any different? As far as I can tell, our current world is indistinguishable from a world with free will, therefore our world is equivalent to one with free will. reply dgoodell 10 hours agorootparentI think you need to define free will before deciding whether or not it exists. People that argue about it are mostly just using different definitions for it. Actually, the whole thing seems to be just a semantic argument, there's no science or anything involved. IMO, I think free will is just a lie we tell ourselves. The inner workings of our brains are mostly hidden from us. When asked why we did something we just make up some bullshit that's not unlike what ChatGPT does when it helpfully fabricates an answer for us. reply throwawayqqq11 2 hours agorootparentThe dedinition should distinguish free will vs free agency. We have free agency to do what is possible for us, like ultimately committing suicide to restrict that freedom, but we dont have the freedom to choose how we choose because this will never be fully transparent to us, even though we can approximate it with mindfullness. So, to answer OPs question how the world would look like with \"free will\", i imagine a world without all those cognitive biases like this entire tribalistic political bullshit we are seeing around the world with identity infested right (and lesser, left) wing movements as a reaction to ever more complex global problems, so quite a contrast. reply GoblinSlayer 58 minutes agorootparentprevIf free will existed and I told you to give me all your money, you would be able to refuse, otherwise you wouldn't. reply bufferoverflow 2 hours agorootparentprev> As far as I can tell, our current world is indistinguishable from a world with free will As far as I can tell, everything in our world is governed by the laws of physics, and free will is an illusion. We \"make choices\", but only because physics moves our brain from one state to the next, and your \"choices\" only have one option - the one that follows physics. (this assumes there's no true randomness in the universe, as it requires a source of true randomness, so infinite regress) reply dsego 2 hours agorootparent> We \"make choices\", but only because physics moves our brain from one state to the next Is it deterministic, who makes the choices for us, why does physics move our brain, what's the cause? reply zelphirkalt 2 hours agorootparentprevBut even with true randomness, it would not suddenly turn will into free will. It would turn it into chaotic/unpredictable will maybe, governed by random events, not free. reply antiquark 11 hours agorootparentprevSimilarily, you could use the example of two people, Alice and Bob. Alice has free will, but Bob doesn't. How would their behavior differ? I've never seen an answer to this question, which hints that the question itself is ill-formed. reply danyala 11 minutes agorootparentBob would become non-deterministic. It’s a bit like when we add async to an old JS project. Once you await, you can’t stop! Now, you’re looking sideways. Try looking down at a bunch of little Bobs. That’s where it gets interesting in my opinion. reply galaxyLogic 2 hours agorootparentprevI think Free Will basically means that nobody else can force us to will, to desire, to like what we like. A non-free-will would be something where somebody else controls my mind. Like you could imagine hearing thoughts in your head which you think are thought by someone else, like in schizophrenia I guess. You can force me to do something but you cannot force me to will/want/like anything. reply CooCooCaCha 11 hours agorootparentprevNo, your response is not scientific. The default, rational assumption is that the brain works through normal physical processes. In order for free will to exist there would need to be some force outside of normal physics that influences the physical world. No evidence for this force of nature exists. And, at a high level, this is no different from other quackery like psychics, witchcraft and other similar nonsense where people insist that their minds can influence the physical world beyond normal physics. reply mkleczek 4 hours agorootparent> In order for free will to exist there would need to be some force outside of normal physics that influences the physical world. I guess this could be something analogous to dark matter... It is as much physics as any other unexplainable \"fundamental\" concept. reply alserio 9 hours agorootparentprevIf there are processes outside of physics that influence the physical world, aren't they inside physics? reply antiquark 11 hours agorootparentprev> In order for free will to exist there would need to be some force outside of normal physics Is this an axiom? reply CooCooCaCha 10 hours agorootparentNo it’s just logical. If the brain is just normal physics then your behavior is dictated by the laws of physics. This is not what most people consider free will. Therefore, you would need something outside of physics for there to be free will. Like a spirit. reply zztop44 9 hours agorootparentI think your mistake is to take “free will” to be something that exists or could exist (ie: in the same realm as physical forces). Which is, of course, just as “outside of physics” as the concept of a spirit or God. “Free will” is a cultural-linguistic construct. How could it be anything else? The very concepts of discrete individuals, let alone agency, is effectively arbitrary as far as physical forces are concerned. The “things” that free will concern are already many, many layers of abstractions (and cultural-linguistic construction) away from the individual deterministic interactions of what we currently understand to be physics. And, like all abstractions, these are leaky and imprecise. So trying to model or analyse their behavior in terms of deterministic physical interactions has always seemed misguided to me. reply Zecc 0 minutes agorootparentI equate it with saying water doesn't exist because it's all just hydrogen and oxygen. Wait, I meant sub-atomic particles. galaxyLogic 2 hours agorootparentprevLet's also think about Quantum Mechanics. It says the world is non-deterministic. A photon basically has FREE WILL to decide which slit it goes through in the double-slit experiment. The Laws of Physics cannot dictate whether Schrodinger's cat dies because an atomic reaction happens, only that there is a specific probability for that happening. So if the world is not deterministic neither are human brains. Physics cannot dictate what the brains think, it can only dictate what the probabilities for some neuron pathways opening or closing are. reply caf 1 hour agorootparentA better example than the double-slit experiment might be: you observe a single free neutron. Exactly when will it decay? reply GoblinSlayer 1 hour agorootparentprevMost people consider free will as an ability to make one's own decisions not coerced by someone else. It doesn't conflict with laws of physics. And even if they believed otherwise, it wouldn't mean much, as it won't be the first wrong belief about reality. reply PreachSoup 6 hours agorootparentprevI think it's just most people don't really understand physics. If they get the concept of emergent effects, complexity and how free will vs no free will are basically indistinguishable to the humans, I doubt they would still agree with the same conclusion. reply antiquark 10 hours agorootparentprevYou're ignoring the whole Compatibilism argument of free will. https://plato.stanford.edu/entries/compatibilism/ reply CooCooCaCha 8 hours agorootparentThat’s operating at a higher level. I’m talking about base level fundamental reality. I fully acknowledge that we feel like we have free will and that we should act like we have it. But it’s still an illusion. reply ttctciyf 15 hours agoprevThe paper itself[0] is a little (hah!) over my head, but this[1] tickled me: > I’ve heard more than one person say that what a pity that Penrose fell for this crazy Hameroff person. But, well, I’ve met both Penrose and Hameroff and they’re both crazy of course, but neither of them is stupid. 0: https://pubs.acs.org/doi/10.1021/acs.jpcb.3c07936 1: https://www.youtube.com/watch?v=R6G1D2UQ3gg&t=181s reply Animats 14 hours agoparentThe important thing in that paper is not \"quantum\", but the claim that internal data transmission by light within neural systems is a thing. The paper cites this result on ultra-weak light emissions from neurons.[1] The paper is behind Elsevier's paywall. So the paper then takes data transmission by light as a given, and goes on to hypothesize structures that direct light as fiber optics do, and from there goes on to \"ultrafast\" data transmission. Comments from biochemistry people would help here. It's not at all clear what's an actual result and what's hand-waving. However, it is clear that this is an area where experimentation is possible. Further work should move this out of the range of speculation and either confirm or dismiss it. [1] https://www.sciencedirect.com/science/article/abs/pii/S10111... reply Horffupolde 12 hours agoprevGiven that the brain is a physical object subject to all quantum effects, wouldn’t the novelty be that it doesn't use quantum effects? That it does sounds obvious. reply wk_end 12 hours agoparentWords are intended to cut up the world - this is how they get their meaning. If you’ve taken these words to apply to everything in the in universe, you’ve rendered them meaningless, which is not the intention. Projectiles are subject to quantum effects but you can model their behaviour classically perfectly well. A charitable reading would be something like “to model the operational behaviour of the brain, classical mechanics is insufficient”. reply gfodor 12 hours agorootparentThis is the kindest response I’ve ever seen to someone suffering from whatever it is that causes these kinds of comments on this specific website. reply dj_mc_merlin 12 hours agorootparent\"Whatever it is\" == the same neurophysical/genetic factors that cause high development of \"STEM\" intelligence is associated with inability to process social cues in a dynamic fashion, somewhat related to high function autism. A great preference for clear classification that is not self-contradicting. reply whoknowsidont 2 hours agorootparentThere are plenty of spaces that have the type of people the thread is describing and yet almost none of them have the type of behavior that the audience of this website loves to exhibit. reply newzisforsukas 1 hour agorootparent> audience Wouldn't users be more accurate? reply notfed 11 hours agorootparentprevA charitable reading of what? I don't think that's the claim here, is it? reply epgui 10 hours agorootparentprevI’m sorry, but your projectile example is just terrible. We’ve known for decades that the proper functioning of many (most?) enzymes depend on quantum effects. Your body would just break down otherwise. reply alevskaya 11 hours agoparentprevQuantum mechanics is needed to explain any microscopic phenomena in chemistry and biology - that is not at all in dispute. The odd set of claims is that somehow biology has 1) figured out how to preserve long-range entanglement and coherent states at 300K in a solvated environment when we struggle to do so in cold vacuum for quantum computing and 2) somehow still manages to selectively couple this to the -known- neuronal computational processes that are experimentally proven to be essential to thought and consciousness. This more or less amounts to assertions that \"biology is magic\" without any substantive experimental evidence over the last thirty years that any of the above is actually happening. That's why most biophysicists and neuroscientists don't take it at all seriously. reply Jevon23 38 minutes agorootparentBiology isn’t magic, but it does do a heck of a lot of amazing things that we don’t understand yet. We haven’t even been able to reproduce abiogenesis. reply aatd86 10 hours agorootparentprevYeah, a bit like when I try to lift my leg, I actually think about how to activate my neurons so that muscle fibers contract one by one... That's definitely not what happens. That at some level we have quantum phenomenon doesn't mean that everything occurs at the quantum level. Seems that even Nature uses abstractions. reply layer8 12 hours agoparentprev“Quantum effects” usually refers to coherent states of superposition. In an environment like the brain that is suffused with photons (it’s warm), if nothing else, superpositions decohere virtually instantly. It’s therefore implausible that quantum effects could play any appreciable computational role in the brain. reply tensor 12 hours agoparentprevWhen these claims come up they don't mean that they are using quantum effects to do normal atomic and molecular things, but rather that somehow quantum effects are used in the process of \"cognition\" thus allowing us to believe that we are in fact more than biological machines and leave room for various magical properties we like to think we have (souls, consciousness, free will, etc). While quantum effects have been found to aid in photosynthesis, interesting uses in cognition or otherwise are in fact extremely rare in biology. I believe photosynthesis is one of the few documented examples. Also, despite the popularity of the quantum brain idea, no one has been able to show definitive evidence of it for decades now. TLDR yes it would be incredibly novel if this claim were proven to be true. reply galaxyLogic 1 hour agorootparent\"Soul\" I think is basically the memories in our brain. And memories are data, like a picture of bits some of which are on and some of which are off. Is there a picture or just random noise? If there is picture, there is soul. Another way of thinking about it is that a picture is made up dots which are NOT lit up. So our memories, our soul is is not \"physical\". It does not have mass, because whether a dot is lit up or not does not change its weight. Memory is not made up of particles but by the information encoded by their positions. reply nsenifty 12 hours agoparentprevPerhaps the title should have been \"Brain uses Quantum effects in a useful/controllable way\". Sabine explains it well in the video. To use quantum effects for computation, you'd need very controlled conditions and it was thought be to not possible in the brain. reply yayr 2 hours agorootparenthttps://www.youtube.com/watch?v=R6G1D2UQ3gg here is the link reply supportengineer 3 hours agoparentprevNot to mention our eyes. Can our eyes decode quantum properties as well as the usual visual wavelengths? reply thsksbd 12 hours agoparentprevits worse than that. A physical system can go far without QM -> see a system subject to Newton laws. A chemical system is necessarily QM. Chemistry is either purely empirical, or quantum. reply epgui 10 hours agorootparentAnything that needs enzymes to function would break down without quantum effects. reply thsksbd 7 hours agorootparentWhat do you mean by quantum effects? Tunneling? If so, that's too restrictive. The interaction between two He atoms needs QM (dispersion forces). Heck, even two water molecules interacting is a QM even if you decide to classify the polar attraction as purely classical. You'd still be omitting a lot of the interaction energy. I see no way I can simulate catalyzed (enzyme) breaking of a chemical bond without: 1. Direct or indirect use of at least DFT 2. An empirical solution (curve fitting to data) reply amelius 12 hours agorootparentprevWithout QM, electrons would just drop into the nucleus. So good luck building anything without QM. reply thsksbd 11 hours agorootparentWell duh, my point is that Newtonian physics is a subset of QM, but doesn't need to be. In fact it's an important result in QM that QM becomes Newtonian in the limit. Chemistry, on the other hand, is QM. All of it. You can empirically get around it like alchemists did for a while. But it sucks. This is partially why chemists accepted the existence of the atom far (far) earlier than physicists who stuck with continuum theories - their empirical methods were banging against the wall. reply usgroup 13 hours agoprevI think -- roughly speaking -- Penrose argues that human capabilities such as transductive reasoning are clearly not computable, therefore falsifying the idea that the mind reduces to an algorithm. He then goes on to propose how nonetheless what the mind does might be physically grounded, even if not in purely computational machinery. reply rowanG077 12 hours agoparentThat doesn't make sense to me at all. If something is physically grounded and can be achieved then shouldn't it, by definition, be computable. reply denton-scratch 13 minutes agorootparent> shouldn't it, by definition, be computable By which definition? Why does anything that is \"physically grounded\" and \"achievable\" be computable? Meanwhile, I don't know what \"physically grounded\" and \"achievable\" mean in this context. reply lambdaone 2 hours agorootparentprevI think the interesting question here is whether Penrose is claiming that the things of which the brain is capable (most notably the production of consciousness) are inherently non-computable by _any_ kind of artificial device, which is effectively a form of vitalism, or whether he is claiming that they _might_ be computable, but only with a quantum computer. reply usgroup 2 hours agorootparentprevIn your statement, I'm not sure by which definition physically grounded things are a sufficient condition for being computable, but I think Penrose depends on the Turing notion of computability and the hardness of the halting problem. We can of course move goal posts, redefine computability however we like, to get whatever conclusion we care for, but I think that Penrose effort is intellectually honest. reply User23 11 hours agorootparentprevNo. Why should it be? reply naasking 11 hours agorootparentIf it's physically grounded then it would be computable by some machine, just not necessarily a Turing machine. For instance, a hypercomputer can solve the Halting problem for Turing machines. We just have to be clear about the kind of machine on which a problem is computable. reply mb64 11 hours agorootparentIt depends on if the universe is fundamentally deterministic or not. Right now we don't have any way to see beyond the apparent randomness in quantum mechanics and we probably never will. This part of the universe might be completely non-computable to us, it's just random. reply naasking 11 hours agorootparentRandomness doesn't have any real impact on whether a problem is computable. Just model the distribution of the random variable. Non-deterministic Turing machines are a thing. reply Delk 38 minutes agorootparentYou probably mean probabilistic Turing machines. Non-deterministic automata in general don't involve randomness, and the results of their computation are considered to be deterministic despite the name. And of course all TMs are just theoretical models. Non-deterministic Turing machine equivalents in particular don't physically exist and may be physically impossible. But of course if there is indeed true randomness in nature that needs to be modelled, that same randomness can be used a source of true randomness for computation, and you can then build computation that does have stochastically determined results. reply mb64 10 hours agorootparentprevThat's true. I suppose it comes down to what exactly is meant by \"is computable\". I don't think it was defined well enough, which I suppose is to be expected when discussing these topics that involve lots of hand-waving. I interpreted it to mean that we can predict the outcome of it, you interpret it to mean that we can model it. Either way, maybe it doesn't matter since the original proposal is that consciousness is a subjective experience and there isn't an obvious way to define how to programmatically create it i.e. compute it. reply User23 9 hours agorootparentprev> If it's physically grounded then it would be computable by some machine, just not necessarily a Turing machine. What makes you think that? The overwhelming majority of functions aren’t computable. The evidence suggests, if anything, that the universe is uncomputable and at best some isolatable well-measured specific phenomena can have approximations computed to some arbitrary but inexact precision. reply naasking 7 hours agorootparent> What makes you think that? Because if it's physically grounded then you can build a physical analogue of it. > overwhelming majority of functions aren’t computable By Turing machines, not by any other type of machine. As I said, if hypercomputers exist then they can solve the Halting problem, which Turing machines cannot. reply lambdaone 2 hours agorootparentConsider the set of functions that that map the reals onto other reals. Almost all of these functions are truly random, with no way of expressing them that does not require the storage of an infinite number of infinite strings. Not only is there no practical way of creating such a thing, most formulations of physics preclude any possibility of making one by placing finite limits of the amount of space or time accessible to us. (Not to mention that almost all reals are [Turing] uncomputable in their own right, but that's a more complex thing to demonstrate.) reply mb64 12 hours agoparentprevThat wasn't the impression I got from Sabine's video. It's consciousness itself (the subjective experience) that isn't computable (AI will probably never be conscious). reply coppsilgold 14 hours agoprevWould it be surprising if evolution managed to exploit quantum mechanics for function? It did exploit everything else. There is also a theory that quantum mechanics plays a role in olfaction[1]. [1]reply dekhn 13 hours agoparentThere are several biological processes for which we're fairly certain that some quantum mechanical process is exploited. Strictly speaking, everything about biology is ultimately explained by quantum chemistry and physics, but not in a particularly exciting way. However, I don't think anybody has found any concrete evidence that any process in biology exploits entanglement. Instead, the examples we've seen so far are mostly around tunnelling, and coherency. People argue a lot about the \"meaning of quantum\" but nearly all the arguing is about the behavior of entanglement and wavefunction collapse. Tunnelling and coherency are pretty banal QM phenomena at this point. As for Sabine... I don't find her popular science vides about biology to be particularly enlightening or accurate. reply bullfightonmars 12 hours agoparentprevNot at all! There is evidence of quantum effects in the light harvesting complexes in chlorophyll that operate at ambient temperatures. This appears to be a key element to the efficiency of guiding photons to the photosystem. reply greyface- 13 hours agoparentprevIt would be surprising if it didn't. Why would an evolutionary process avoid searching through mechanisms that involve quantum effects? reply novaRom 1 hour agorootparentQuantum effects are everywhere: https://en.wikipedia.org/wiki/Quantum_biology reply moralestapia 13 hours agoparentprevIt plays a role in photosynthesis, I don't have the reference at hand (I'm on my phone), but that one is 100% verified. reply dekhn 13 hours agorootparentYou can start at https://newscenter.lbl.gov/2023/06/14/photosynthesis-key-to-... and https://www2.lbl.gov/Science-Articles/Archive/PBD-quantum-se... and use the names in those articles to pull up the relevant literature. reply lll-o-lll 12 hours agoparentprevDidn’t Thunderf00t perform an experiment to prove that heavy water tastes sweet due to a quantum effect? reply krzat 1 hour agoprevWe have billions of neurons but they create one coherent experience, it would be pretty elegant if consciousness was involved here. Basically: what is conscious is quantum, what is unconscious is classical. Vast majority of brain activity is unconscious, what surfaces as experience is a global state of the entire system. reply unsupp0rted 35 minutes agoparentThat’s much cleaner than reality ever tends to be reply piloto_ciego 5 hours agoprevPay attention to the people with the most dogmatic views in this thread about determinism and free will. It’s extremely interesting. Typically (and strangely) I find the most dogmatic folks are mostly determinists. I think probably because it’s more “science-y” than a philosophy that has to say “I don’t know.” Even in this thread you’ll note that there’s a very vocal group of people who would happily shout down any idea that there’s any sort of free will in the universe and that the universe is anything other than a cold mechanistic computation that was determined billions of years ago. They’ll shit talk Penrose, or shout about pseudoscience, and very few of them will critically engage with the topic in good faith. They’ve already made up their minds. Not only that but some of them (troublingly) will even talk how we’re not even conscious at all. As if we are all just meat computers or something. Now, I don’t know if we’re conscious or not and I don’t know if the world is on rails either, but it certainly feels like I have free will and it certainly feels like I’m responsible for my actions and it qualitatively feels like I’m conscious. Obviously, not all of my choices are made freely, but some of the ones that matter to me at least seem to be made freely. But a lot of people seem absolutely sure that we’re not making anny choices and we have no free will. It’s weird to me, because literally every day I get increasing evidence suggesting I have free will within some limits and the universe is unfathomable and unpredictable. Not to be an ass, but if I can be honest I kinda pity the folks that don’t have that experience. I reckon if you’ve never made a decision that really mattered it would be easy to feel like it all wasn’t your choice. Maybe that’s too harsh, but anti-free will and determinism are just as untestable as any other philosophy I suppose, but I do wonder what kind of life can lead to people thinking that they’re not making any conscious decisions. reply Nikretep 3 hours agoparentI’m a determinist and I certainly feel like I’m in control of my thoughts and actions. That doesn’t mean I actually am. I think that we as humans are just hardwired to feel like we have free will, even if we believe otherwise on an intellectual level. reply galaxyLogic 1 hour agorootparentI don't know how you define \"determinism\" but perhaps in the sense that future is determined by the past, right? The initial state of a system and the forces acting on it determine its state at some future time. Right? But universe doesn't work tat way. Universe is non-deterministic because quantum effects happen randomly. They cannot be predicted, only their probabilities can. reply denton-scratch 3 minutes agorootparent> Universe is non-deterministic because quantum effects happen randomly. One of Penrose's foundational arguments is that the random element of quantum collapse is deeply unsatisfactory; he thinks it's a defect in the theory that must eventually lead to its replacement by a better theory. reply rcxdude 38 minutes agorootparentprevThat does depend on your interpretation of quantum mechanics: the wave-function is still entirely deterministic. Non-determinism is only induced by 'collapse', but it's not mandatory for this to ever happen: the many-worlds interpretation is basically the result of saying it doesn't, and is completely deterministic. (In fact 'collapse' interpretations are not particularly popular theoretically, because it's actually a blurry line of decoherance: it's just generally convenient for interpreting the outcome of a calculation, and you can assume it's a close enough approximation to experiment once your system has interacted with your detectors). reply thsksbd 12 hours agoprevI'm always amused by the lack of creativity of biologists. It seems every decade or so some closely held dogma of biological systems is proven wrong after they mock physicists, computer scientists or mathematicians who first suggest it. reply zone411 11 hours agoprevWhy is the link to this blog spam instead of to the paper or a better article? Hossenfelder lacks qualifications in neuroscience and is often confidently inaccurate. reply nutrie 15 hours agoprevEvery time I listen to Sir Roger speak, I wonder how much he keeps to himself. He is such a gem. reply nomemory 14 hours agoparentI suspect a lot. He has unorthodox views on lots of topics. For one he doesn't seem to believe in the heat death of the Universe. He has some interesting theories in regards to gravitational waves and so on. reply aspenmayer 13 hours agorootparent> he doesn't seem to believe in the heat death of the Universe I’m not sure I believe that the heat death of the universe really is the end of all things forever and for all time either, after reading Asimov’s The Last Question and a particularly fantastic manga oneshot adaptation by manga artist Ryul. If you haven’t read it, I can’t recommend it more highly, and the manga version is a nice addition to the canon. I found a version narrated by the man himself on the Internet Archive, and I also found a fully voice acted audio version from the Drabblecast, also linked below. https://en.wikipedia.org/wiki/The_Last_Question https://www.isfdb.org/cgi-bin/title.cgi?41230 Original publication: https://archive.org/stream/Science_Fiction_Quarterly_New_Ser... Manga version: https://imgur.com/a/9KWrH https://www.mangaupdates.com/series/oo67iat/the-last-questio... https://mangadex.org/title/f1e5d886-cc50-4cbf-ac8b-6914d9343... https://archive.org/details/manga_The_Last_Question Audiobook versions: https://archive.org/details/IsaacAsimovAudioBookCollection/1... https://www.drabblecast.org/2011/03/11/drabblecast-200-the-l... reply dumbo-octopus 12 hours agorootparentOne interesting angle I've heard proposed with regard to this is that even post-\"heat death\", there is no guarantee that local pockets of low entropy will not exist. Indeed - quite the opposite is true: let's say being in a maximum-entropy state requires all particles to assume a fully random distribution (otherwise a potential gradient would exist somewhere, a form of order, and the entropy would not be maximized). Now, take a description of an NxNxN chunk of universal \"stuff\" (energy, matter, etc). Similar to how we can find every string of digits in Pi if we look long enough (each given substring is equal to the search string with probability 1/10^N, N is length of string), we should be able to find every possible chunk in the maximum-entropy universe, with the probability for any given chunk matching being 1/M^N^3, (M is the universal \"base\", how many options a given location has for what it can be in the encoding we are using). Long story short, assuming an infinite universe post-\"heat death\", we'd expect to be able to find every possible arrangement of particles represented at some location, even very complex ones such as the entire universal state we observe today. reply rcxdude 36 minutes agorootparentThese future states may as well be parallel universes, however: there's no real way for them to interact with each other across the vast time and space of maximal entropy. reply aspenmayer 12 hours agorootparentprevI've heard these theoretical ideas advanced also, perhaps in writings by Richard Dawkins but I'm not sure; iirc the idea is loosely related to the concept of quantum foam but I may be mistaken. > Quantum foam or spacetime foam is a theoretical quantum fluctuation of spacetime on very small scales due to quantum mechanics. The theory predicts that at these small scales, particles of matter and antimatter are constantly created and destroyed. These subatomic objects are called virtual particles. The idea was devised by John Wheeler in 1955. https://en.wikipedia.org/wiki/Quantum_foam https://www.vice.com/en/article/j5yngp/the-universe-is-made-... reply _Microft 14 hours agoprev… but so does anything that performs photosynthesis? reply p0w3n3d 14 hours agoparentThere is a lot of intelligence in plants. For example crown shyness, navigating to water, trees informing each other that a giraffe is eating their leaves, etc. reply ben_w 13 hours agorootparentNone of that is what I'd call \"a lot\" of intelligence, not even in aggregate. (Evolution itself can be described as a form of intelligence, but evolution is as different from individuals as humanity collectively is from a single thought). reply AnotherGoodName 14 hours agoprevQuantum effects allow for randomness which is nice since it gives some sense of not being a machine entirely on rails. That alone alone doesn't give determinism though. For that we still need to have some influence on the outcomes of quantum events. If there's a mechanism by which we can will for some quantum outcomes to be more likely than others that'd be nice. It'd fit with intuition that we aren't just observers of a series of events but actual participants that determine the outcome. reply GrantMoyer 14 hours agoparentI find the notion that free will requires a non-determistic system strange. Consider for the sake of argument that you love apples and hate oranges. Then given a free choice between an apple and an orange, you'll always pick the apple. The system is determisitic, and your free will is what makes it deterministic. On the other hand, if you were required to choose an apple or an orange based on a coin flip, you wouldn't call that free will. reply matt-attack 14 hours agorootparentThere’s an excellent 2hr or so podcast episode on free will that has drastically changed my mind on the notion of determinism. The main argument is that the notion of “choice” is in fact an illusion and when you dive into what you’re actually doing when you “choose” you realize that it’s really just determinism under the hood. In other words a deterministic universe is actually completely consistent with our experience once you realize that the mechanism of us “choosing” anything is really an illusion. Highly recommend it. https://podcasts.apple.com/us/podcast/making-sense-with-sam-... reply teew 13 hours agorootparentprevFor some reason, most opinions on this topic that one reads on forums with of technically inclined people are non-compatibilist (the view that causal determinism and free will are mutually exclusive) while a good number of people that think a lot about will (i.e. philosophers) are compatibilists... Note though that in metaphysics/theory of mind determinism is defined as the state at a given moment being necessitated from the state at a previous moment. I think one could critique your argument by saying that you're just pushing back the question of determinism by one level (i.e. \"what's responsible for your preference of apples in the first place?\"). The fact that you always choose the same way can then be taken to be a proof of determinism instead. A compatibilist line of argument for your position might go something like this: What we consider a free will would hardly be met by a will completely detached from any deterministic constraints whatsoever. If a necessary condition for free will was that it is free from any external conditions, what would there even be for it to 'choose', and on what basis could its choices be made? Only if your mind knows of apples and oranges (objects subject to deterministic systems) and can interact with them (is at least partly part of the same system) can it make a meaningful choice between them. (Again, this view is based on the assumption that determinism exists and that free will is possible.) reply numpad0 11 hours agorootparentSometimes I wonder if those non-tech philosophers are actually smart or they're just free-running their hallucinations with entire System 1 and circular reasoning detection turned off, just because doing so allows them to generate more plausible text faster for stronger in-group approvals... Isn't the concept of free will somewhat of a mysticist mental pleaser that it'll be the thing that save us in the end in doomsday scenarios? If we accept that the world is deterministic and so are our minds and behaviors, that will be quite depressing, and if we assume free will and our souls are real, that means decisions we make comes from trekky super-reality and therefore potentially infallible, which happy. > Note though that in metaphysics/theory of mind determinism is defined as the state at a given moment being necessitated from the state at a previous moment. I think one could critique your argument by saying that you're just pushing back the question of determinism by one level I think this is just tangential to free will. If a state_old -> state_new transition() was deterministic code, but code involved RNG sampling, it can be considered both deterministic and not. It cannot be ruled one way or another here. > What we consider a free will would hardly be met by a will completely detached from any deterministic constraints whatsoever. If a necessary condition for free will was that it is free from any external conditions, what would there even be for it to 'choose', and on what basis could its choices be made? This part looks like a strawman sandcastle made up to overload opponents. A lot has to be defined in your favor for that argument to work. What's wrong with rolling a dice(assuming it still works)? Is randomness make a choice laughable meaningless non-choice? I'm starting to understand why \"technically inclined people are non-compatibilist\", everything is just way too under-defined that people are barely on same pages. reply jl6 14 hours agorootparentprevSomeone who hates oranges might still, on rare occasion, pick one. Maybe they’re having a bad day, misunderstood the instruction, or wanted to turn their life around starting now. We might make decisions with high likelihood but it’s a qualitative leap to go from 99.999999% to 100%. reply Retric 13 hours agorootparentThere’s always more context to a choice than a simple question, but we aren’t limited to vague questions. What you ate for breakfast January 3, 2024 could be what you always eat with those memories, that body, those resources etc. So saying yes once could very well mean you would say yes 100% in exactly that context without meaning you would always say yes in similar but not identical situations. On the other hand if it’s random in identical contexts then it’s just random not free will. reply ben_w 14 hours agorootparentprevIf there's a reason for it, that's still deterministic; if there is no reason for it, that's still random. reply MeImCounting 12 hours agorootparentprevThe question is rather what circumstances gave rise to the system that would make a given choice. Thats all \"free will\" is: the current state of a system that gives rise to a specific choice. Free will as its commonly understood is an entirely religious concept and has little/no utility in explaining behavior. It is, IMO entirely unrelated to consciousness and a distracting subject that traps people into circular reasoning. reply raindeer2 14 hours agorootparentprevIndeed. The problem is that the word \"free\" has different meanings when talking about free will and freedom in other contexts. What you talk about is value freedom, while ppl denying the existence of free will refer to \"physcial\" freedom. Few ppl notice the distinction though which makes the debate somewhat strange. https://arxiv.org/abs/2111.08435 reply teew 13 hours agorootparentOne thing that's really strange about this article is that it presents compatibilism and incompatibilism as having a different concept of 'free will' – compatibilism sporting an everyday sense of free and incompatibilism roughly a more scientific one. The article assumes incompatibilism to be correct on those grounds and goes from there. Coming from the philosophical literature, this is simply not the case. If both sides assume the same definition of free will, e.g. as \"the agent could have chosen differently\", they still have a genuine disagreement... reply raindeer2 51 minutes agorootparentWell there are many versions of compatibilism I guess, but just reading the Wikipedia article on compatibilism I don't think most compatibilists think freedom relies on whether or not causal determinism holds. Please tell me if i'm wrong. Defining free will: Compatibilists often define an instance of \"free will\" as one in which the agent had the freedom to act according to their own motivation. That is, the agent was not coerced or restrained. Arthur Schopenhauer famously said: \"Man can do what he wills but he cannot will what he wills.\"[14] In other words, although an agent may often be free to act according to a motive, the nature of that motive is determined. This definition of free will does not rely on the truth or falsity of causal determinism.[2] This view also makes free will close to autonomy, the ability to live according to one's own rules, as opposed to being submitted to external domination. https://en.wikipedia.org/wiki/Compatibilism reply Animats 14 hours agoparentprevThermal noise alone is enough to yield randomness. reply layer8 11 hours agoparentprevInstead it’s a machine on a roulette table? There is no third state between determinism and randomness. Either something has a cause, or it doesn’t, and hence is random (or a combination of both). > actual participants that determine the outcome. Nobody disputes that we are (well, almost nobody). But we as participants are ourselves an outcome of processes that are somehow determined, and our decisions have causes. And assuming that they’re instead random doesn’t improve the situation. reply AnotherGoodName 8 hours agorootparentThere's four states. Randomness vs deterministic. And then within randomness the possibility to influence outcomes vs none. A purely deterministic mind is a movie. You watch it. There's no way to influence the outcome or it wouldn't be deterministic. You're just a long for the ride. A non deterministic mind could be nothing more than randomness with no ability to influence outcomes still which makes it little different to a movie. Again you're just a long for the ride even if the ride has random events. But non deterministic and the ability to actually influence outcomes? That's the intuition I suspect most people have for free will right there. reply dumbo-octopus 12 hours agoparentprevFor a detailed investigation of this topic by an renowned quantum computer scientist: https://arxiv.org/abs/1306.0159. reply moralestapia 13 hours agoparentprev\"Randomness\" is not a correct interpretation of quantum mechanics, all you can say is that, beyond some limit, uncertainty in measurements kicks in. A deterministic process being behind it cannot be ruled out (yet?). reply heisenzombie 13 hours agorootparentHmm, we have actually ruled out quite a lot of that sort of thing... For example we're pretty sure there's nothing like a \"local hidden variable\", e.g. a little deterministic process in an electron determining if it's going to be spin up or spin down when measured. See: Bell's theorem. reply moralestapia 13 hours agorootparentOh, I may have missed that. So, can a deterministic process be ruled out? reply rcxdude 32 minutes agorootparentNo. It rules out hidden variables. The underlying theory is still deterministic: the wavefunction is entirely deterministic. The randomness is only introduced in the interpretation, and for example the many-worlds interpretation is still entirely deterministic, even if it appears random to an observer inside the wavefunction. reply raindeer2 45 minutes agorootparentprevIt cannot, even if ppl think that this is what Bell's theorem says. But the determinism must be quite strange: https://www.youtube.com/watch?v=ytyjgIyegDI reply Fourier864 12 hours agorootparentprevAn experiment that proved that there are no local variables which secretly determine the outcome of a quantum measurement won the Nobel Prize in 2022: https://en.m.wikipedia.org/wiki/Bell%27s_theorem It appears to be completely random. reply moralestapia 11 hours agorootparentNot an expert in physics but nowhere in there says any of the statements you make (well, except for the Nobel Prize). reply Fourier864 9 hours agorootparentSo the article says \"quantum mechanics is incompatible with local hidden-variable theories\". Hidden variable theory is what you were describing initially. It basically says that there must secretly be deterministic rules to quantum mechanics, and that the randomness we observe is just due to our ignorance of some \"hidden variables\". Einstein was a fan if this theory because he believed \"God doesn't play dice\". John Stewart Bell discovered that quantum mechanics would actually behave a little bit differently if there were secretly variables out there, and proposed experiments that would show if there were hidden variables or not. The 2022 nobel prize was awarded because these experiments were finally completed, and they showed conclusively that there were no hidden variables in quantum mechanics. reply moralestapia 8 hours agorootparent>quantum mechanics is incompatible with local hidden-variable theories >local That's an important modifier. >The 2022 nobel prize was awarded because these experiments were finally completed, and they showed conclusively that there were no hidden variables in quantum mechanics. Nah, from the same article: \"The exact nature of the assumptions required to prove a Bell-type constraint on correlations has been debated by physicists and by philosophers. While the significance of Bell's theorem is not in doubt, its full implications for the interpretation of quantum mechanics remain unresolved.\" reply canjobear 11 hours agoprevIf the brain can get any extra fitness from exploiting quantum effects, it will. But the reason people are interested in this stuff seems to be that they think there's a connection between the quantum nature of the brain and consciousness. I don't see how the argument is any more than this: 1. Quantum effects seem mysterious. 2. Consciousness seems mysterious. 3. Therefore, quantum effects cause/are consciousness. ...which seems pretty weak. reply cat_plus_plus 12 hours agoprevSo there is a little quantum soul which is the consciousness of the brain. How does the soul get it's consciousness? I think the best attitude is that a. We have no clue what causes consciousness and b. it's probably much more widespread that we previously assumed. We assume that other humans, cats and sufficiently advanced AI are conscious based on observing behaviors similar to our own. But why should self awareness be tied to specific behavior? Why not a star self aware of being a star, without having any of the same senses, drives and capabilities as a human? At best we can assume is that some other animals/things might be self aware of similar occurrences as we are. As for quantum effects, if brain is able to leverage them in computation, that's an obvious evolutionary advantage. One obvious person of a brain is to simulate many possible potential actions and their real world outcomes. If it's able to use quantum states to model multiple scenarios simultaneously, that's quicker/more detailed modeling with less energy use. But as far as quantum effects causing consciousness, how could we possibly tell and what useful insights have we gained by making this assumption? reply chefkd 11 hours agoprevIs this just huma brains or does it apply to something like beavers? reply wrp 11 hours agoprevFor a popular survey of the relevant neurophysics, try Stairway to the Mind by Alwyn Scott (1995). He argues it is very unlikely that quantum effects are relevant, and is skeptical of Penrose. reply I_am_tiberius 12 hours agoprevThe following is most likely nonsense, but I always had the intuition that there is a hidden connection between mother and child that remains after the child's birth. reply TheLoafOfBread 12 hours agoprevSo there is a chance to abuse a cockroach and turn it into quantum computer for breaking encryption on the budget. reply havaloc 13 hours agoprevStar Trek had it right, living organisms beam at a \"quantum resolution\". reply Hugsun 14 hours agoprevThe first thought that came into my head reading this title was: \"Ah, so they turned off the quantum effects for someone and they became dumb\". I guess that someone was me. reply parpfish 13 hours agoprevthis paper is going to unleash a mighty wave of woo-woo reply Animats 11 hours agoparentMost useful comment so far. reply parpfish 8 hours agorootparentThanks. I put a lot of thought into it, so it’s nice to feel appreciated. reply api 14 hours agoprevI’ve speculated for many years that biology is harnessing quantum computing in some way and that this is one of the simplest explanations for how the brain can do what it does so quickly on so little power. It only consumes around 40 watts and does things an entire AWS data center can’t do. Quantum compute could be being leveraged to do things like accelerating search for gradient descent. This would allow very rapid learning in polynomial time. There are also quantum algorithms that can more or less serialize parallel search, etc. The objection is usually that biology is hot and noisy and wet and that this is a poor environment for QC. This assumes that it’s doing QC in a remotely similar way to how the quantum computers we are trying to build would do it. Billions of years of evolution might have found ways to harness quantum phenomena for information processing that are quite radically different. It’s all analog for starters, so nothing like a “qubit” or rigidly defined circuits. Maybe some biochemical reactions are structured so as to invoke and amplify useful quantum effects that allow things to happen informatically that would be much slower or more costly without those effects. Without understanding what’s happening this would just look like noise and luck to us. I do know that there are “unreasonably effective” enzymes and reactions involved in things like DNA repair, though it’s been a while since I read about this. I think there are cases where repair complexes find DNA errors more effectively than can easily be explained by basic chemistry and chance, and we are not sure exactly what’s going on. Maybe there’s something involving quantum information processing happening somewhere. reply 613style 14 hours agoparentThis feels a lot like a standard line of muddy thinking we see in youtube videos about consciousness (for example): \"we don't understand brains, and we don't understand quantum mechanics, so they're probably related.\" It's easy to speculate, but it's not easy to find any evidence at all to back up those guesses. It's still not clear that this has anything to do with consciousness or information processing or AWS datacenters. reply throwaway11460 13 hours agorootparentThere's evidence that biology takes advantage of quantum effects on all levels - all the way from individual chemical bonds and interactions of molecules (quantum biochemistry) up to cellular and multi-cellular. Mostly because there's no way it could work on such small scales and be so energy efficient if it didn't. So one thing is certain - the brain does use quantum mechanics just like the rest of the body, because otherwise it wouldn't be possible to have so much done inside such a small volume, with such small amounts of energy. Of course this question is actually meant to be \"is brain a quantum computer?\" and we don't have any idea. reply 613style 10 hours agorootparent> otherwise it wouldn't be possible to have so much done inside such a small volume, with such small amounts of energy That's interesting. Could you share your source? reply throwaway11460 10 hours agorootparentMeh, down voters have no idea what they're talking/reading about and yet they down vote. Not everything about quantum mechanics is voodoo. Sorry, not in the mood to talk about this here any more. The Wikipedia page is a good start, it has some relevant references to research articles in the Enzyme catalysis and Energy transfer sections: https://en.m.wikipedia.org/wiki/Quantum_biology Quantum effects are also used inside cells to convert chemical energy to motion. It wouldn't be possible otherwise at that nanoscale. reply victorbstan 12 hours agoprevCould’ve just asked me. reply ioblomov 14 hours agoprevGiven that it took probabilistic models to create the fluency of generative AI, it would only make sense that brains exploit quantum effects. reply AndrewKemendo 14 hours agoprevIf you take the results of this work and also recent work in understanding what’s called the “Null space” of preparatory neuronal activity prior to action [1] then it paints a fairly clear picture that the null space might be interacting with this micro tubular architecture, and perhaps the activation function for skeletal muscle act is this wave collapse, informing the null space function evaluation [1] https://www.nature.com/articles/s41583-024-00796-z.epdf?shar... reply icf80 14 hours agoprev [–] TEXT 24: This individual soul is unbreakable and insoluble, and can be neither burned nor dried. He is everlasting, present everywhere, unchangeable, immovable and eternally the same. https://vedabase.io/en/library/bg/2/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A recent study suggests that the human brain might utilize quantum effects in microtubules, aligning with Roger Penrose's initial hypothesis.",
      "The blog post delves into this finding, referencing quantum foundations and random thoughts in physics, with closed comments directing readers to discuss on Patreon.",
      "Sabine Hossenfelder, the author, promotes her book and social media profiles on the blog for further engagement."
    ],
    "commentSummary": [
      "Scientists and philosophers are debating the role of quantum effects in the brain and biological processes, discussing free will, determinism, and consciousness.",
      "Some believe free will might be an illusion governed by physical laws, while others propose that quantum effects are significant in cognition.",
      "The discussion involves Bell's theorem, hidden variables in quantum mechanics, and the brain's potential as a quantum computer, highlighting the intricate and controversial link between quantum mechanics and consciousness."
    ],
    "points": 165,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1715527643
  },
  {
    "id": 40336609,
    "title": "Mastering the Art of Corner-Cutting in Software Development",
    "originLink": "https://blog.ometer.com/2016/05/04/professional-corner-cutting/",
    "originBody": "Havoc's Blog Archives this blog contains blog posts Professional corner-cutting by havoc Steve Jobs famously cared about the unseen backs of cabinets. Antique furniture built with hand tools isn’t like that at all. Cabinetmakers made each part to the tolerance that mattered. The invisible parts were left rough, with plane and saw marks, to save time. The visible parts, however, were cleaned up and polished. Some surfaces were made precisely straight and square, for structural reasons; while nonstructural surfaces were only straight enough to look good to the eye. Think about an apprentice in an old cabinet shop. An apprentice painstakingly smoothing an invisible surface would be yelled at for wasting time. An apprentice failing to smooth a visible surface would be yelled at for producing crappy work. To become a professional, the apprentice learned to work efficiently but still do a good job. Crucially, a “good job” was defined in terms of customer concerns. [1] Cabinetmakers were focused on what their customers cared about. Customers wanted the furniture to look good, and they wanted it to be structurally sound. They didn’t care about invisible tool marks, and didn’t want to pay extra to have those removed. Software remains a craft rather than a science, relying on the experience of the craftsperson. Like cabinetmakers, we proceed one step at a time, making judgments about what’s important and what isn’t at each step. A professional developer does thorough work when it matters, and cuts irrelevant corners that aren’t worth wasting time on. Extremely productive developers don’t have supernatural coding skills; their secret is to write only the code that matters. How can we do a better job cutting corners? I think we can learn a lot from people building tables and dressers. 1. Own the implementation decisions It is irresponsible to ask the customer (or manager, or other not-doing-the-work stakeholder) to tell you how to make technical tradeoffs. Cabinetmakers didn’t ask their customers how flat a tenon had to be and this is not the customer’s problem. The customer wants us to do it properly but not wastefully. It is our decision how to go about this, and if we get it wrong it’s our fault. On software teams, there’s often a developer trying to push these decisions up to management or onto the customer, because they don’t want to “get in trouble” later. Perhaps they complain to management about “technical debt” and being “given time to work on it.” This is a sign that we aren’t owning our decisions. If the technical debt is a problem, 1) we shouldn’t have put it in there, and 2) we should include it in our estimates and address it. A cabinetmaker would not ask the customer to put “make tenons straight” on the sprint. Nobody cares. Technical debt is our problem; that’s the job. If you don’t own your technical decisions, you can never get them right, because nobody else knows how to make them. Insist on making them. And yes, that means getting them wrong is your fault. It may mean giving people bad news about how long things will take. It may mean you get yelled at sometimes. 2. Understand the customer’s needs and preferences Because we must make tradeoffs and not push choices onto the customer, we have to understand what matters and what doesn’t. It’s easier to establish this in the world of furniture (“doesn’t break when you sit on it,” “looks nice”). In software, we have to know what job our software will do for the customer. This is where we should be getting customer input (though watching what they do may be more valuable than asking them what they think), and reaching a consensus with our management team or client. We should not ask customers for more precision than they can give us (a symptom of this is to badger customers or managers for detailed “requirements,” then complain endlessly about “changing requirements”). Our job involves converting vague needs into concrete software — if we’re lucky, we have the help of a skilled product designer, or guidance from a management team that’s able to be somewhat precise, but if not we have to do it ourselves. Accept the job and learn to do it. It’s unprofessional to be the kind of developer who doesn’t care about user experience, doesn’t care about business context, or “just wants to be told the requirements.” It’s impossible to work efficiently or to do a good job without understanding the context. A professional developer can take a desired UX and work out the technical steps to get there as efficiently as possible. And they do get there; they don’t build something odd that doesn’t meet the need, or something slapdash that doesn’t work. 3. Don’t be lazy Corner-cutting should be a deliberate decision; “this truly doesn’t matter.” It should not be because it’s 5pm and we’re going home. When we find ourselves asking “do I really have to redo this…” then we need to redo it. Cutting corners should feel like you have a clear focus and you’re skipping tasks that don’t matter for that focus. Cutting corners should not feel like you’re doing poor-quality work. To push back on an unrealistic schedule, work to narrow the scope or weaken the requirements. Let’s say you’re making some kitchen cabinets. You could make them all with hand tools, no metal connectors and no plywood. They would be gorgeous and cost about $150,000. When the customer says that’s too much time and too expensive, you could make them the usual modern way with machines, screws and plywood; which is a sound approach, though a little uglier. This is like offering to build a web app that’s not quite as slick and beautiful — something a little more off-the-shelf. That’s all fine. What’s not fine: delivering either of those choices unfinished and broken. “Oh, I forgot the cabinet doors.” “Sorry these things aren’t painted!” To cut scope, we should do something defined (such as leave out a feature or refinement), rather than something undefined (like skipping testing). Professionals are doing it for others All of this sounds hard, and it is. As in Amy Hoy’s description of these students learning a craft, at first we may fight it and focus on our own needs and emotions. Professional software developers are performing a service for others. That’s the difference between a professional and a hobbyist or an artist. To perform a service for others, we have to know what others need, and apply our expertise to meet those needs. [1] Furniture made by machine doesn’t have the same ability to flex tolerances to save time. For the most part, with woodworking machines you get what you get; the machine doesn’t know whether your surface will be visible, or how flat it has to be. It makes one kind of surface and that’s it. Some parts of machine-made furniture aren’t as good as a handmade joint or surface could be, while other parts are far more precise than necessary. Check out this discussion of how to cut a dado joint by hand, which mentions several ways to save time on the back, non-show side of the piece. My Twitter account is @havocp. Interested in becoming a better software developer? Sign up for my email list and I'll let you know when I write something new. Name Email * Published: May 4, 2016 Filed Under: Uncategorized 8 Responses to “Professional corner-cutting” rakslice says: May 8, 2016 at 12:06 pm How does this work when you’re working on something that’s too much work to be a single person project? Reply SteveOC says: May 10, 2016 at 5:40 am Small caveat …. The cabinetmaker generally knows in advance what the cabinet is likely going to be used for, and therefore what the customer is most likely to care about. A cabinetmaker, and his customer, both know when the job is finished. The software maker doesn’t always have the same luxury, since his “cabinet” … if its any good … will be cloned and copied, and used in contexts that are almost unimaginable at design time. Sometimes anyway. Its a balancing act between sticking to what is necessary, whilst being careful not to make assumptions about what may be necessary vs what may not. Cleaning up the rough edges on the “hidden” components is more an insurance policy against these assumptions than a mad stab at perfection for perfection’s sake. Great article .. thought provoking, thanks 🙂 Reply Charles Zigmund says: May 10, 2016 at 10:24 am Putting a software system into production without thoroughly testing it is mostly unheard of (except for one mad manager I once had). But not submitting it to users — who have been uninvolved with the project — is routine. Why does the designer think s(he) can anticipate the needs or irritations of the users perfecttly or even effectively? The hundreds of examples of bad design I find everywhere, in product, software, furniture, building and auto design, are due to this oversight. Every design should always be tested exhaustively on users before being put into production. Reply Professional Development – 2016 – Week 19 – Geoff Mazeroff says: May 16, 2016 at 12:14 am […] Professional Corner-cutting (via Cameron Presley) […] Reply Professional Development – 5/09/16 – 5/15/16 – The Software Mentor says: May 16, 2016 at 10:10 am […] Professional corner-cutting […] Reply Bruce Boyes says: May 21, 2016 at 9:59 am Great post and interesting comments! I got here following an old post of yours: Making Hardware Just Work, from 2003. I’m working on a presentation about writing reliable and robust I/O drivers for Arduino. Happy to share that draft document with any who have interest. Reply GTK 3.20 breakage in Debian, GDM3, Wayland and Technical Debt.Experiences in the community says: May 23, 2016 at 12:07 pm […] I was reading http://lwn.net/Articles/686502/ as well as http://blog.ometer.com/2016/05/04/professional-corner-cutting/ . Both of them make for some pretty interesting reading. Thoughts […] Reply Go Ahead, Cut a Few Cornerssays: May 26, 2016 at 5:12 am […] Actually, by doing a lot less than your best work at all times you’ll probably do a whole lot better by your client, writes software architect Havoc Pennington on his blog. […] Reply Leave a Comment Name: Required Email: Required, not published Homepage: Comment: Notify me of follow-up comments by email. Notify me of new posts by email. « Previous Post Next Post » Subscribe All Tech Elsewhere @havocp on twitter about email linkedin © Havoc Pennington. Powered by WordPress and Manifest",
    "commentLink": "https://news.ycombinator.com/item?id=40336609",
    "commentBody": "Professional Corner-Cutting (2016) (ometer.com)160 points by MississippiGary 15 hours agohidepastfavorite115 comments dkarl 13 hours agoThe hook of the article leaves unanswered the question of why Steve Jobs made a big deal out of the backs of cabinets. The answer is that Steve Jobs knew how to sell luxury items to people who identified themselves with the quality of their work, whether they were accomplished creative professionals or cube-dwelling (now coworking) \"knowledge workers\" or MBAs who take sommelier classes or aspiring writers grinding on their novels at Starbucks. It was partly about the experience of using an Apple product, but it was just as much, if not more, about vicariously identifying with the standards of craftsmanship. If it was only about the experience, and the craftsmanship was a means to the experience, then the backs of the cabinets would not have mattered. They mattered because the craftsmanship was not just a means but was fetishized for its own sake. reply gofreddygo 12 hours agoparentArticle is spot about its topic - tradeoffs. The professionalism is in picking the right corners to cut. Steve's marketing was IMO a whole load of bollocks, like all other marketing. it gets attention now when Apple is a trillion $$$ company, and superficial minds wanting a simple answer attribute it to the most visible thing - Apple's marketing. no one bought Apple products for Apple's marketing or advertising. Its for the weirdos. Same theme as other Apple marketing e.g. the bicycle for the mind metaphor, the 1984 ad, etc. This article is good and its not about the backs of cabinets. reply plufz 10 hours agorootparent> no one bought Apple products for Apple's marketing or advertising. That statement would require quite a source. Of course a lot of people bought Apple products because of the ads, like for any company that is a serious advertiser. reply gofreddygo 5 hours agorootparentApple had been doing great ads since 1977 [1] Apple revenue not so great till 2000s [2] What changed ? [1]: https://www.macworld.com/article/670956/the-14-best-apple-ad... [2]: https://www.statista.com/chart/4574/apples-revenue-since-197... reply rob74 1 minute agorootparentI think the revenue chart is a bit misleading. Apple was very successful with the Apple II and the early Macintosh models (otherwise they would have gone the way of Commodore and Atari), it's just that the market was much smaller in the 1980s - also, I doubt that the graph is inflation adjusted. plufz 3 hours agorootparentprevYou usually need multiple things combined for a successful business. A good product, a good brand, good ads, pr, etc. reply noisy_boy 4 hours agorootparentprevSocial media and it's amplification of the desperation to show off. Apple products are cool/expensive and I am cool/above average/have buying power/ to have them while I sip my pumpkin latte at Starbucks. They are also high quality machines but I don't think for everyone that buys them, the motivation is solely the software/hardware stack. reply empiko 12 hours agoparentprevIt is not about the backs of cabinets. It is about telling your customers how big of a deal the backs of cabinets really are. reply nebula8804 11 hours agorootparentThe back of Apple cabinets are are actually pretty good even in the Tim Cook era. [1]: https://www.cultofmac.com/320883/why-samsungs-design-sucks-i... [2]: https://ioshacker.com/iphone/image-compares-ugly-samsung-bea... reply makeitdouble 3 hours agorootparentOn the first, having the off-center hole helped with thicker plugs. The iPhone had less of these issues as cables were thinner (and more expensive) in general, while micro USB had a flurry of cheap cables in the while that could be much thicker. Apple never really cared for ports practicality, we've had the same discussion with the finewoven cases that wouldn't allow for regular thickness usb-c cables. Other makers made more of an effort on that front. The second would be a better point if iPhone's repairability was on par with Samsung's. Granted Samsung also became worse with time, but it was in no small part because of what the market leader could get away with. That said, I think your general point stabds: Apple cares about the back of the drawer. But not in the way I personally wish they cared. reply bee_rider 10 hours agorootparentprevThe first is not really the back of the cabinet, it is more like the design on the front. The second… more like the back of the cabinet maybe. Although the better style there might also be somewhat functional; the internals are clearly quite different, one could imagine that the nice packaging in the Apple case might be a result of only wanting people to use approved parts. reply nebula8804 5 hours agorootparent>The first is not really the back of the cabinet, it is more like the design on the front. The metaphor seemed to have gone over your head. >The second… more like the back of the cabinet maybe. Although the better style there might also be somewhat functional; the internals are clearly quite different, one could imagine that the nice packaging in the Apple case might be a result of only wanting people to use approved parts. This philosophy applies to their entire product line and has essentially been the case since Jobs came back. Hell looking at the first 1984 Mac this philosophy was there. reply thfuran 8 hours agorootparentprevI don't understand why someone would even want the ports to all be on the center line. reply golergka 6 hours agorootparentIt's aesthetically pleasing. reply vundercind 6 hours agorootparentHelps you find ports by feel, too. reply kqr 5 hours agoparentprevCutting the right corners is known as job smarts in the research on expertise, and it's one of the markers of expertise. Perhaps Steve Jobs wasn't an expert in engineering so he didn't know which corners to cut and therefore avoided cutting any corners out of fear of cutting the wrong one? reply makeitdouble 11 hours agoparentprevIt's in the link on the relevant bit [0]. TLDR: his dad built stuff, including as a hobby, and he liked overengineering. He liked he dad very much and also got on board with overengineering. [0] http://thenextweb.com/apple/2011/10/24/steve-jobs-obsession-... reply taneq 6 hours agorootparentI’ve always found the term “overengineering” as used to mean “unnecessarily overspecc’d” a little odd. Engineering is all about appropriately selecting and sizing components. Overspeccing something is underengineering it, and usually happens because you don’t have time to accurately size things so you throw the kitchen sink at it. reply makeitdouble 3 hours agorootparentGood point, overspeccing was the appropriate wording. reply justin_oaks 13 hours agoprev> If the technical debt is a problem, 1) we shouldn’t have put it in there, and 2) we should include it in our estimates and address it. Yes. Don't tell your boss that you'll need to take time to address technical debt. The boss will always say \"No don't do that, just add the feature\". Sometimes they add \"We'll fix that later.\" Later never happens and eliminating related technical debt is part of implementing the feature. Don't ask your boss about this stuff. It's part of your job and it's something you understand that your boss probably doesn't. reply noisy_boy 4 hours agoparent> Sometimes they add \"We'll fix that later.\" The answer to that answer is more questions: - how will we fix it, properly, later when we can't fix it now? Won't there be more features to be done later? - what if we fix it later and it creates its own issues and then someone will ask why did we touch it when it was working? If it breaks now, we can fix those breakages because we are already touching it. - if later, as part of which release? Don't actually ask these questions - they will piss-off your boss even more. reply strken 10 hours agoparentprevThe name \"tech debt\" has always been a bit of a misnomer. Financial debt does come with interest, but it's structured, proportionate, and you can just go pay it off with sufficient money. Tech debt is unpredictable, needs a lot of context to comprehend, needs even more to fix, is subject to the mythical man month, and taints everything else it touches in your product. It can be worth accepting such a structural flaw into your product, for a time, but it's certainly not like asking for an extension of credit. It's a struggle to come up with a better name, though. reply superice 10 hours agorootparentTech debt is more like being in debt to violent mobsters, who will show up at your house unannounced at the most inconvenient of times, during which they will hit you with a wrench until you pay whatever is in your wallet right now. They will return soon enough, because whatever payment you just made on the spot is never quite enough to settle the debt. reply delusional 4 hours agorootparentprevIt's an incredible share that we've accepted the \"technical debt\" metaphor so deeply into our vocabulary that we can't discuss the practical problem without devolving into interpreting the metaphor. The practical definition is clear. It's a lack of maintenence of core abstraction. Leftovers from different designs, deeply embedded into our solution. Sometimes we plan for it to live in a corner somewhere, but more often we discover the failure of some abstraction all too late. It doesn't work like debt at all. We didn't sign a termsheet, and we didn't calculate the impact. We need to move beyond this hopeless metaphor such that we can actually discuss what software needs, because right we're staring at a field emptied of nitrogen by years of farming and saying it has a \"resource debt\" reply lloeki 4 hours agorootparent> We didn't sign a termsheet, and we didn't calculate the impact. Which is very much like going to the bank and trusting it that it's going to be ok without looking at numbers; or going to a mob shark for a loan, but it's actually Darth Vader and he'll make sure to alter the deal any way he sees fit. Evaluating what tech debt is going to cost (through coupling and accidental complexity) is definitely part of the job. That responsible engineers yelling it's going to cost a lot are then ignored is part of the systemic cultural problem. reply YZF 12 hours agoparentprevThis is not the right answer for a real business. Sometimes we'll fix that later is perfectly acceptable. If that was good enough for Facebook it can certainly be good enough for others. reply justin_oaks 4 hours agorootparent\"We'll fix that later\" is at best a hopeful fantasy, and at worst a lie intended to manipulate. It doesn't happen. We need to be clear here. If what needs to be fixed is something that will inevitably cause serious problems down the road, then it should be fixed immediately. If it may cause minor problems then maybe you should never fix it. And there's a lot in between. The biggest problem is that bosses aren't in a good position to understand the code you're looking at. So if you're a professional, you'll make the call as to whether or not some tech debt is worth fixing now or not. The boss's job is to communicate the urgency and importance of your task, how it fits in with the company's goals, etc. Your job is to carry out the task with a reasonably balanced ratio of quality to time. reply vineyardmike 1 hour agorootparent> We need to be clear here. If what needs to be fixed is something that will inevitably cause serious problems down the road, then it should be fixed immediately. If it may cause minor problems then maybe you should never fix it. And there's a lot in between. I feel like a lot of conversations around technical debt miss this nuance. There are plenty of systems I’ve worked on that have been either (1) running for years without issue or (2) deprecated and deleted without ever addressing most of the “debt” issues/tickets that my team and I created in our tracking systems. I think a lot of engineers take pride in their craft (which is good). Until you learn that a cabinet with a rough back is still a great cabinet, you’re doomed to feel uneasy about leaving something “unfinished”. So many people feel they left some “hack” in the code they want to fix. Which is different than a bug. There are endless improvements possible. YAGNI. reply mook 10 hours agorootparentprevAnd sometimes — quite a lot of the time, in fact — never fixing it is the way to go. It sucks, your users will keep complaining about it, and it'll nag you at the back of your mind forever. But if fixing things properly means somebody else ships first and you have no users and you get cancelled, that bug still wouldn't have been fixed. But sometimes your product is so buggy nobody wants to use it. It's all trade-offs, just like… well, engineering. reply speed_spread 12 hours agorootparentprevIts up to the engineers to factor in maintenance work as part of their estimates. Business people assume perfect software and react badly when confronted with the messiness of software development. Also, Facebook being a real business is questionable. reply YZF 11 hours agorootparentMeta's market cap is 1.2T, it has $60B cash, revenue 143B a quarter... Not a real business? I mean maybe the original team in FB should have made sure they didn't accumulate technical debt and FB having failed would be a better outcome for shareholders. Engineers need to give the business the options. Not unilaterally decide that they're not going to accumulate technical debt. That's my point. And trust me, business people don't assume perfect software (at least if they've been in business for any amount of time). [EDIT: What's maybe different about] business people is they a) look at things as a negotiation. b) want to get the max value for their investment c) likely have seen a lot of engineering projects from the outside. Engineering needs to work with the business not go and decide what's right for the business. This doesn't mean you don't have ethical responsibilities as a software engineer in many situations but there are also many situations where the business needs to call the right balance between engineering effort the things like quality or feature sets. reply Aeolun 10 hours agorootparentI don’t think biz is in any position to decide that. They can tell us when something needs to be delivered by, and we can decide which corners need to be cut to achieve that. reply bee_rider 10 hours agorootparentprevThe IEEE code of ethics https://www.ieee.org/about/corporate/governance/p7-8.html Would seem to preclude working on a site, like Facebook > 1. to hold paramount the safety, health, and welfare of the public, to strive to comply with ethical design and sustainable development practices, to protect the privacy of others, and to disclose promptly factors that might endanger the public or the environment; Given that sites like Facebook have been used to spread medical conspiracy theories (health of the public), the whole business model is privacy violations, and the site was used to enable the Rohingya massacre (safety/welfare of the public), https://www.pbs.org/newshour/amp/world/amnesty-report-finds-... But I mean an ethical engineer wouldn’t get in the way of a place like Facebook because they wouldn’t work there in the first place. reply YZF 4 hours agorootparentI'm not an expert in ethics but to me this stance goes too far. The Internet in general is also used to spread medical conspiracy theories. If I work on network switches is that also unethical? Compilers? The job of regulating this sort of stuff is the government and the courts. There are laws concerning privacy, and speech, and companies need to operate within those laws. I would say an ethical engineer should not break the law or help the company they're working for break the law. I would interpret IEEE's statement as it affects a single engineer to e.g. ensure that they are following best practices in protecting user information, e.g. if an engineer is asked by Meta to take some shortcut that exposes people's information publicly that would grounds for refusing to do that on an ethical basis. That said as an individual you ofcourse have a choice where you want to work. I can see someone not wanting to work for Meta. I'm sure there are many ways in which Meta also supports safety and health, e.g. some (a lot) of useful and good information is also available on the platform. We should look at the totality of the company's impact, not just one aspect of it. That said I agree that hate amplification and disinformation on the Internet is of concern. It should be up to governments to take action on that. I think with Meta and other Internet companies they have immunity in the US for being sued over content others publish on their platforms. reply bee_rider 3 hours agorootparentOne reason to have an ethics code is that the law can’t possibly cover everything. Law is read in a fairly hostile manner (in the sense that people are looking for technicalities and loopholes), and also has to concern itself with enforceability, jurisdictions, etc. Ethics are self-enforced mostly, and they are to be followed in good faith. An ethical engineer should not break the law, but nobody should break the law, that’s the bare minimum for existing in society. If you thought the internet was overwhelmingly harmful and had little-to-no redeeming value, yes, working on network switches would be unethical. > I would interpret IEEE's statement as it affects a single engineer to e.g. ensure that they are following best practices in protecting user information, e.g. if an engineer is asked by Meta to take some shortcut that exposes people's information publicly that would grounds for refusing to do that on an ethical basis. I don’t see any particular reason to think they just mean exposing personal information to the public, when they talk about protecting privacy. I’d tend to assume Meta, like every other entity, is something we ought to protect people’s privacy from. It is up to you to interpret the meaning personally. But nobody is going to punish you if you don’t obey the code, so why look for an out? Just don’t follow it. reply RangerScience 13 hours agoparentprevYou know I wonder… why do so many of us feel like we need our boss to understand? reply onion2k 12 hours agorootparentMost of us work in organizations where the boss is a former engineer who understands the problem of tech debt perfectly well. If your boss is a former dev and they aren't taking it seriously the problem is usually that they're a bit too keen to say yes to other parts of the business. reply vundercind 6 hours agorootparentprevBecause their opinion of us may contribute to how comfortable we are at work, how well we’re compensated, and whether we get promoted. If they don’t understand what we’re doing or why things take as long as they do, that’s bad for us. reply rapfaria 13 hours agorootparentprevPerhaps it's not a close relationship. Perhaps the boss is insecure about his management skills and has to constantly check on the team, to make sure every variable is under his radar. reply hobs 13 hours agorootparentprevBecause the boss says they want to understand, drags you to endless meetings to understand, but then manages to makes arbitrary decisions based on none of that information. reply thayne 8 hours agoparentprevThat's all well and good until you have to explain that the reason building something they think should take a week will take 6 months because you have to fix tech debt, or avoid adding new tech debt. reply justin_oaks 4 hours agorootparentA professional balances the need for quality with the urgency of the task. Your example shows a clear imbalance that indicates the person is not a professional; that person is someone who can't be trusted with that balance. reply Vinnl 12 hours agoprevOne of my frustrations is how people often seem to drift towards one of the extremes on either side of this argument. Technically, the approach described in this article is called 'pragmatism', but in practice, people have used that term to describe the bad kind of corner cutting. And whenever you're arguing against someone drifting too far to one of the extremes, you'll often get lumped in with those on the other: to a perfectionist, you'll be considered a lazy, sloppy worker, whereas the corner-cutter will consider you to be super pedantic and not the type to 'get things done'. On the other hand, when you do get to have a proper discussion on whether a corner is worth cutting — i.e. if it will actually affect the user's experience — and you end up being able to save time without materially affecting the end product or future maintenance work, that is enormously enjoyable. reply pilgrim0 10 hours agoparentIME, even the word “perfectionism” is weaponized against us who actually mind risks, results and sustainability of work, beyond of just doing it for the sake of completion and compensation, like the other group. Why is it safe to “denounce” perfectionism in the workplace while the opposite, of calling out a shitty job, is seen as offensive and of bad taste? “Perfectionism” has become shield and shelter for the lazy and mediocre. reply Vinnl 3 hours agorootparentEither label needn't be used: if someone thinks the effort isn't worth it, you can have a discussion on what risks will or won't actually play out in practice without having to label anyone anything. reply pilgrim0 9 hours agoprevArticles in this spirit pisses me off so much, and they’re increasingly becoming trendy. I hate it because they promote the wrong ideas. Downscaling and re-scoping a project is NOT the same as corner cutting. You can almost always come up with an execution strategy for doing a good job while utilizing the available resources to perfectly fulfill the agreed upon requirements. The notion of what’s perfect is always relative. I’ve seen and experienced first hand, over and over, people dismiss basic risk management, due diligence and responsible planning and execution as “perfectionism”. Everyone likes durable, safe and robust products for themselves. Everyone appreciates delightful and frictionless experiences for themselves. But somehow, it’s only a minority that actually cares of pursuing such qualities for others. I’ll repeat what I’ve said in another comment: denouncing “perfectionism” has become shield and shelter for the lazy and mediocre, and poorly worded articles like these just serves to validate and excuse poor craftsmanship. So before trying to weaponize “perfectionism” towards those who actually care about results, people need first to identify their own shortcomings and biases. reply swagasaurus-rex 1 hour agoparentPerfectionism in a beginner can often mean focusing on the wrong thing, spinning wheels and going down rabbit holes, leading to burnout. Perfectionism to an expert often means the opposite. The difference is the expert knows what is worth time and effort. Somewhere in the learning process, a beginner must intermediate, and then become an expert. reply ilaksh 13 hours agoprevOftentimes managers will make demands that interfere with ownership of the code. Such as requiring something to be finished in half the time it would take to do it without technical debt. Over and over and over again. I have had a lot of clients like this. It's very common especially for lower budget projects. You can literally cut corners you don't really want to or just get replaced by someone who does. They will often also insist on certain technical decisions like choice of programming language or framework. reply skydhash 12 hours agoparentMore often than not, you have n > 1 implementation solutions possibles. The article said to factor the constraints into the choice of solutions, not the solution itself. Like if it would take 6 months to implement a backend, but just 1 month with Firebase, and time is a more important than data ownership, you go with Firebase. Then you prepare a plan if you ever need to migrate off Firebase. If you don't have time to do a custom UI, you go with a components library. That also means knowing the range of possible solutions (aka mastery). If nothing else, you reduce scope. There's a plethora of ways to respect time/monetary constraints without resorting to shoddy code and duck taping. reply manmal 10 hours agorootparentAbsolutely agree. Relevant here: https://en.m.wikipedia.org/wiki/Project_management_triangle reply justin_oaks 13 hours agoparentprevA cabinet maker has the option to say no because they're the one in charge of accepting the job AND implementing it. At least that's the way the article made it sound. Most of us are cogs in the machine and are given much less autonomy. So what is the \"professional\" to do if given a job and insufficient time to do it? reply jbmsf 13 hours agorootparentOur profession doesn't always work the way you describe. Some companies want cogs, some want professionals. I can't give you advice beyond that, there are too many different circumstances. But in most of the places I've worked, it's been developers, not managers, cutting corners. Sometimes these are good choices, informed by product context and trade-offs. Sometimes it's a perceived urgency that doesn't actually exist. But it's almost never a micromanaging boss asking for something specific. reply Aeolun 10 hours agorootparent> But it's almost never a micromanaging boss asking for something specific. They don’t ask people to cut corners specifically, they just create a culture where speed is paramount above all else. Where the only metric of success is tickets completed per day. reply jbmsf 7 hours agorootparentI'm sure these managers exist, but there's plenty of places where they don't. My recent experience has been that the managers I work with, when asked, almost always say yes to extending technical work to do a better job and that it's inexperienced developers who either assume they can't have more time or don't know how to ask in an effective way; these are professional skills after all. This doesn't mean that there's never a need to hurry, external deadlines exist, but these are the minority in a healthy organization. reply RangerScience 13 hours agorootparentprevYah. Article’s advice about “owning the implementation” is great and all, but limited to places where you’re not blocked from or fired for doing it. Notably, from my small sample size (only small startups) - I don’t think I’ve known anyone who was fired for doing it. So it seems like it’s not a thing we can do… but I bet it’s something we can do in a lot more situations than we think. reply willcipriano 13 hours agoparentprevAdvise them of the issue. If they still want it, their funeral. Anytime I move away from this frame it's not worked out for me. reply travbrack 9 hours agorootparentExactly. If you choose the approach with no tech debt but takes twice as long, how are you going to hide that from your manager? reply advael 12 hours agoprevI agree with everything in this article. However, software developers may be craftspeople, but an infinitessimally small fraction of them are treated like craftspeople by the businesses that employ them The reason software engineers want to push decisions to their managers is that their managers believe in nonsense like slicing up tasks into their tiniest constituent parts and meticulously estimating how much time they will take. Practices like lean and scrum and kanban are managerial conventional wisdom about how to manage a factory, not craftsperson conventional wisdom about how to make good things efficiently As long as the people running the show are going to hold their people responsible to fiddly metrics and demand to know every detail of decisions that affect how their software people are spending their time, they will act according to those incentives, avoid making decisions where possible, quibble about every little detail Software should be a craft. Treating it that way produces better software. In order to foment a mindset of a craftsperson, you must treat them with the trust and respect that we give to craftspeople. Most modern humans don't even have a script for interacting with craftspeople these days, and business jocks drunk on rebranded Taylorism certainly don't apply one when managing people they view as a means to better valuation reply maerF0x0 12 hours agoprev> A professional developer does thorough work when it matters, and cuts irrelevant corners that aren’t worth wasting time on. Extremely productive developers don’t have supernatural coding skills; their secret is to write only the code that matters. IMO the true mark of a professional, a truly talented, engineer is knowing which corners to round off before they cut you. Anyone can cut corners and leave the world full of problems for the next guy. But then the assumptions change. A queue gets really full, or we start needing utf-8 for emojis, or someone wants to rearrage the field order in a CSV. A great engineer would make a system that just works in those cases, because they can be (at least in go, node, python) implemented in just as many lines, just as complex of code, the only thing required is foresight (or its cousin, experience). Many will say YAGNI, but in my experience these things almost always come up (and I'm sure there are many others). Sometimes being a great engineer means reading between the lines of product designs, past sevs, and experience to figure out what the real feature ought to be. reply arnorhs 11 hours agoparentThis is ambiguously stepping into \"overly general / building for the future\" territory. Perhaps that is not what you meant. There is a fine line where I do agree with you. Cases such as creating a relation table for categories, when you could have made a string array field instead. Or structuring a code in a way where you are not preparing for the future, but also not painting yourself into a corner. Examples such as that do come to mind. But as I said it's a fine line. This becomes exceptionally tricky when you are building towards a vision, but only 40% there, and having the code structured for that vision is hard to shake.. reply strken 10 hours agorootparentThe given examples of utf-8 support and reading headers from CSVs are things that (depending on stack etc.) can be nearly or actually free if you build that way from the start. I read the original post as saying something like \"good engineers don't shoot themselves in the foot as much\". reply Vinnl 3 hours agorootparentOne challenge is when eg you're doing a code review and see that someone set it up with a non-utf8 column. In that specific case it's probably easy to fix, but it's still strictly extra work even if it hadn't been if it had been set up as such in the first place. I struggle with making that trade-off in terms of what's worth pointing out. Often what I'll do is point it out, but with an explicit disclaimer that I'm mostly pointing it out because it's good to know for the future, but that it's not a blocker (for larger chunks of work). reply Aeolun 10 hours agorootparentprevIt’s much easier in biz software, where what the customer wants is very likely the same as what everyone before them wanted, even if they haven’t come up with all the requirements yet. reply bee_rider 11 hours agoprevI don’t think software is like a cabinet at all. A cabinet is a well-understood thing, the main job of the cabinet craftsman is to implement the existing cabinet idea. Cabinet makers don’t do sprints because everybody knows what a cabinet is, there isn’t any need to aggressively iterate on the design. The software equivalent to buying cabinets is buying existing software. You don’t employ a bunch of engineers to install cabinets. If you went into workshop with a bunch of mechanical engineers and asked “I need you to reinvent the idea of holding plates,” that might take some sprints. It would be a silly thing to do, but it would make the analogy fit. reply sillysaurusx 14 hours agoprevAm I hallucinating, or does anyone remember the Steve Jobs story as the exact opposite? He used backs of a cabinet as an example of when it’s okay to cut corners, since nobody would ever see it. I don’t remember where I heard it or what the context was, but it was something along the lines of him trying to get one of the early Mac engineers to ship faster. There was a lot of corner cutting to get the original iPhone launched. They barely made it in time. The parts that mattered were polished to perfection, but you wouldn’t want to use an iPhone 1 today — all the other parts you take for granted now would be painfully visible then. reply disruptiveink 13 hours agoparentYou are correct. Steve definitely believed in doing things properly because you know they're there, regardless of who can see it: https://folklore.org/Signing_Party.html > Steve came up with the awesome idea of having each team member's signature engraved on the hard tool that molded the plastic case, so our signatures would appear inside the case of every Mac that rolled off the production line. Most customers would never see them, since you needed a special tool to look inside, but we would take pride in knowing that our names were in there, even if no one else knew. reply tejtm 13 hours agorootparentThat was done in Amiga 1000 cases 1985-ish. I would be surpised if they were the first. https://www.commodore-info.com/computer/item/a1000/en/deskto... reply ricardobeat 13 hours agorootparentThe parent folklore.org link mentions this happened at Apple in 1982, end of first paragraph. reply tejtm 12 hours agorootparentAnd, Jobs visited Amiga in 1983 which also suggests the idea came from him. reply rmnwski 13 hours agoparentprevI think you got it wrong. That's why the motherboards on macs for example are \"beautiful\" and black. reply iwontberude 13 hours agorootparentA cabinetmaker creates the polished front and the unfinished rear. A software/hardware company needs everyone being part of the polished front at least in spirit. I liken it to people dressing for their job. reply c22 13 hours agorootparentI haven't used an Apple product in some time, but to me it always felt like their software was the unfinished rear. reply WWLink 8 hours agorootparentlol it's true but every single tech company does it. Most DIY PC \"gaming\" computer hardware companies do it with their motherboard firmware/software. Microsoft does it with practically everything because their customer isn't the end user, it's the business. Apple also does it in their firmware/low end software (sometimes making laughably goofy bugs/oversights where the edge case is \"you did something apple doesn't do by default\"). You see it an awful lot with internet routers lol. reply brandall10 13 hours agoparentprevI think that's two very different things - a complete set of features done poorly vs. an incomplete set of features done well. In general it's best to err on the side of the latter, it's going to give the customer a better UX. What exactly was the original iPhone missing? The only thing in the OS I recall was copy/paste. The other things we'd take for granted today likely weren't conceived then, including the App Store. The main issues was that it was a frustratingly slow experience, so OS/proc performance, and EDGE which was the best widely available at the time was insufficient. reply paulryanrogers 12 hours agorootparent> The other things we'd take for granted today likely weren't conceived then, including the App Store. IIRC, one could get apps on other platforms like Blackberry, even over the web. Apple's anti-innovation was a closed-garden storefront. reply WWLink 8 hours agorootparentBetter yet, you could download apps and transfer them to your phone manually, or download them from a webpage even - straight onto the phone. It's really sad how something so easy and straightforward was eliminated in the name of \"security\" lol. reply heromal 13 hours agorootparentprevIt didn't have 3G IIRC reply brandall10 6 hours agorootparentBut they had a legitimate reason for not including it, including lack of network availability and poor battery life for the modems at the time of release. It's not something they could have innovated around. reply jweir 14 hours agoparentprev“When you’re a carpenter making a beautiful chest of drawers, you’re not going to use a piece of plywood on the back, even though it faces the wall and nobody will ever see it. You’ll know it’s there, so you’re going to use a beautiful piece of wood on the back. For you to sleep well at night, the aesthetic, the quality, has to be carried all the way through.” reply doytch 14 hours agorootparentNot only would a professional carpenter use plywood, it may be a sounder choice because of its stability/weight ratio. reply germinator 13 hours agorootparentYeah, it's a weird metaphor, especially since plywood is a premium, structural material. I wish my Ikea furniture used plywood. Instead, you get fiberboard. It's also a good example of how the metaphor this blog post is predicated on can fall apart. Your customers care about durability, but they make purchasing decisions based on cost and outward appearance. In a world like that, where you can't satisfy all requirements at once, you inevitably end up cutting corners on the things the customer cares about but can't measure. But is that right? That's how you end up with $200 furniture that lasts a year or two in a home with children or pets. It's also easy to neglect cumulative costs. Back to software: does it matter if your app uses 100 MB of memory when it could be using 1 MB? On an individual basis, no, because RAM is cheap. Cumulatively, when every other app developer thinks the same, and when you multiply it by billions of devices, your decision might have actually cost lives if you consider the increased emissions and countless other distant externalities. A milder version of the blog's claim is definitely true. You should pick your battles. But it's all about trade-offs, there are few problems that truly don't matter to anyone. reply brandall10 13 hours agorootparentprevThe point isn't about utility, it's about aesthetics - it's about how the product is finished. What you use in the back should be the same as elsewhere. reply formerly_proven 13 hours agorootparentEven furniture from The Old Masters doesn’t have beautiful veneered back panels, but plain, unfinished boards. reply brandall10 6 hours agorootparentSure, but I'm just explaining Steve's reasoning here. People are saying it's a bad analogy without understanding what he's actually saying. reply jodrellblank 10 hours agorootparentprevOn the back of the Mona Lisa should be ... another Mona Lisa? reply nextaccountic 13 hours agorootparentprevA plywood back would be prone to water damage, however reply mrob 13 hours agorootparentThe best option is plywood for the dimensional stability, varnish for waterproofing, and solid wood edge trim to give some resilience against mechanical damage that would otherwise risk delamination. This also hides the ugly part of the plywood. reply sillysaurusx 13 hours agorootparentprevThank you for finding the quote. I stand corrected. I think I misread this at the time as him saying that no one will see it, so it doesn’t really matter. It’s an interesting experience to wake up one day and see that the meaning of one of those old stories was exactly the opposite of your takeaway. In hindsight there does seem to be some truth to this. What surprised me about the Hacker News codebase is that pg almost never cut corners. To this day I still find new features I never realized I wanted. On the other hand, pg never coded anything unless it was absolutely necessary for whatever he was trying to accomplish (or at least he presented his few public pieces of code as such), which is something I’ve had trouble emulating. Coding is just too much fun sometimes. reply 6510 13 hours agorootparentprevThe correct answer is: I'm not, you're not either. I've oddly asked a carpenter why he was using plywood, his response was: I'd prefer not to but this is what people want nowadays. (meaning this specific customer) If you are a software engineer you, with pride, rewrite half the company in rust and the other half in elixer. Tell the board you wont be able to sleep. reply jxl62 12 hours agoprevThe ideas aren't as universal as they are made out to be. \"Technical tradeoffs\" are often not just technical, but also a business decision. They can have significantly different risks, costs and implications for business strategy. You may not have the insight into these things like someone who makes it their job. The \"tenons\" comparison frames these decisions as being trivial details that happen in a vaccum which smells like strawman. There are generally more factors to consider in tech decisions, and unlike cabinetry there is often a great deal of uncertainty in how it will play out in the future. This advice could be reasonable in the right company and role, while completely setting someone up for failure in another. There's irony in the article being about corner cutting and tunnel vision. reply ungreased0675 14 hours agoprevI like the last paragraph. Will remember that when an engineer slows down a project talking about code smells and revisiting architecture decisions. > Professional software developers are performing a service for others. That’s the difference between a professional and a hobbyist or an artist. reply dt3ft 14 hours agoparent> code smells and revisiting architecture decisions This is exactly what we're dealing with on top of shipping new features. An architectural decision that got changed 6 months later so the code had to be thrown out and re-written. Don't blame the engineers, blame the architects. If you have any. reply fuzzy2 13 hours agorootparentWhy blame anyone at all? Did everyone work to the best of their knowledge and beliefs? If not, why? And if they did, how are they to blame? I make incorrect decisions all the time. I will continue to make them. Because I do not shy from making decisions and neither should anyone else. reply ungreased0675 14 hours agorootparentprevYou’re correct, I was thinking of a specific software architect when I wrote that. reply bojan 13 hours agoparentprevThere is certainly a a balance to be struck. Are the code smells a structural issue in a piece of software that's supposed to be used and modified for a long time? Then those code smells endanger delivering the service in a medium term and should be addressed. But are we talking about a one-time migration script or a fad mobile app? Then who cares, the service will be delivered regardless of the quality of the code. reply baobabKoodaa 12 hours agorootparentThere are people working in software who will _always_ want to refactor something into \"clean code\", no matter how many times it has been done before. It's just never going to reach a point where that person says \"I'm happy with it, let's get back to building features\". If you want to build features (at all!) then you have to sometimes tell that person \"no, we're not refactoring this again\". reply OtherShrezzing 14 hours agoparentprevI'd probably drop the \"or an artist\" in the retelling though. Lots of artists are definitionally performing for others. reply Havoc 13 hours agoprevFound this particularly tricky with hobby projects around cost. I can spend a bit more time on clever engineering to fit it into free tier but that saves me what. Price of a coffee? For loads of extra complexity and time reply brudgers 12 hours agoprevCabinetmakers were focused on what their customers cared about. More likely, they focused on what their customers would pay for. People with Steve Jobs money might be willing to pay a premium for finished cabinet backs. Most people don’t have Steve Jobs money. But people without Steve Jobs money are no more or less likely to appreciate finished cabinet backs than people with Steve Jobs money…it’s not hard to imagine Charlie Munger having a mid-western view on finished cabinet backs. Since end users usually don’t see any more or less of steaming turd code than well structured code, there must be something besides “cabinet making” at work. My gut is that strong opinions about code are a way to make the banality of code writing interesting. Strong opinions add melodrama to the mundane work that doesn’t matter much beyond the paycheck that comes with it we all must do. Getting paid is the meaningful metric of professionalism. reply afro88 12 hours agoprev> If the technical debt is a problem, 1) we shouldn’t have put it in there I guess it depends on your definition of tech debt, but there are lots of examples of things that aren't tech debt at the time but become tech debt over time. You can make the best decision possible today, but in six months that npm dependency (for example) is still going to be incompatible with that other npm dependency you needed to upgrade, and you need a few days to figure it out. A cabinet maker might not put \"make tenons straight\" in a sprint, they would include it in the estimate and not tell the customer, sure. But they would definitely tell the customer that their lathe tool broke and they needed a few days for repair before continuing work. reply skydhash 12 hours agoparent> You can make the best decision possible today, but in six months that npm dependency (for example) is still going to be incompatible with that other npm dependency This is a solved problem. You either understand the dependency enough to maintain it yourself, or you firewall it through interfaces. Or you tie yourself to sensible projects that gives you time for upgrade. Or you pay for support. Every decision should be taken with a complete understanding of pros and cons, and risk management to reduce the latter. This cabinet make should either have a backup tool or be confident they can do the repair in a way that doesn't impact their consumer. reply afro88 11 hours agorootparentYou do need to understand the pros and cons, and wow there are some incredible time and complexity costs to maintaining all dependencies yourself, or trying to firewall all dependencies through interfaces. I wasn't talking about blindly doing stuff without understanding consequences and risk managing. I was rebutting the point from the article that you shouldn't ever have tech debt. You will, no matter what choices you make today. Firewalling all interfaces to dependencies for example can be future tech debt if you need to move fast to stay ahead of competitors, but all the indirection is making development too slow (or causing developers to find other more enjoyable work). Can't imagine working on a React codebase where React is completely abstracted behind our own interfaces to all of it. reply skydhash 6 hours agorootparent> I was rebutting the point from the article that you shouldn't ever have tech debt. You will, no matter what choices you make today. I agree > wow there are some incredible time and complexity costs to maintaining all dependencies yourself, or trying to firewall all dependencies through interfaces. I'm not saying you should firewall everything. But if it make sense, you do so. You also need someones that understand each core deps enough they can patch it if the maintainers won't resolve an issue (happens more in the npm world). Even with React, you can extract the business logic and reuse it elsewhere. The goal is to avoid depending that much on projects that have less stability than yours (some bad experiences here with React Native). How you do it is contextual, but you should keep it in mind. reply yakkomajuri 12 hours agoprevIn a lot of organizations I feel like #2 is often the difficulty. However, interestingly there is an opposite phenomenon where people are too focused on #2 (understanding customer needs) and thus fail at #1 (owning implementation). I've seen it happen where non-technical users of internal tools request very specific features that get built just because and at whatever cost without the team asking what is the actual problem this feature solves and how can we best build something to address the problem given our knowledge of the technical side. It's often requests for tools that allow people to do things manually when the team should just spend some time automating away the need to manual intervention. reply rfrey 9 hours agoprevCraftsmen work alone and are responsible for the entire product. Most software deve work in teams, sometimes large teams. The entire Craftsman movement under Raskin, Morris etc. started up because of the move of craftsmen into factories, where they worked on parts of the whole and could therefore not be craftsmen anymore. reply fitsumbelay 8 hours agoprevI really love that Monty Don's in the hero image of the referenced Amy Hoy post (man on far right) reply jack_h 12 hours agoprevI'm going to have to disagree with the entire premise of this piece. If you are a cabinet maker you have many individual clients who all want what you're producing. They each have their own budget and preferences that you can work with to get them something they want to buy. The incentives and constraints are clear here. If you can make something the client wants at a price and quality point they can afford you will do well. Many (most?) software developers create a product to satisfy the needs of many customers and non-customers simultaneously. Furthermore you almost never interact directly with customers; instead developers interact with a panoply of different business interests such as engineering managers, product managers, project manager, product owners, etc. Even worse software doesn't have to be done to ship it as it can always be fixed \"later\". With the advent of Agile, Scrum, and SAFe it's clear what the business wants is not someone good at a craft; they want an assembly line. So what are the incentive structures and constraints here? Every other person has incentives for career advancement, bonuses, raises, etc. Most people are too far removed from customers to be directly affected by them. How many times has a cabinet maker been told that the hope chest they're working on for one client also needs to double as a bank vault. Oh and by the way it needs to be done by the end of the quarter (right after layoffs of course) because they are hoping to be bought out. Code bases end up being a fractured mess of bad abstractions, infinite abstractions, and rewrites because developers are trying to accommodate the impossible demands set forth by the business in a way that doesn't cause their job to be abject misery. TLDR: most software developers are not expected to be craftsmen (or women). The company determines, through incentives and constraints, the quality of code it will produce. An individual software developer risks burnout if they think quality above and beyond what the company allows is within their responsibilities or capabilities. reply skydhash 12 hours agoparentWhat I've seen mostly is developers bringing complexity, then cutting corners after that because they don't want to deal with it. In Coders at work, Douglas Crockford said to spend the sixth cycle - whatever the cycle is - on refactoring. It's a sound advice if you care about technical debt as an engineer. Spend some time to revisit the code architecture to see if it still fits the problem you're solving. Instead of spending three cycles on something that could have been done in one because of how fragile everything is. Or having a long list that is tied to the same root cause. Sometimes it means redo your React marketing website in Node and EJS (or the equivalent) instead of trying to make it SSR too. reply jack_h 12 hours agorootparent> In Coders at work, Douglas Crockford said to spend the sixth cycle - whatever the cycle is - on refactoring. It's a sound advice if you care about technical debt as an engineer. My point is that very often it's not up to engineers. If a company doesn't incentivize this refactoring and engineers have to - as some sibling comments suggest - inflate their estimates then the code base will deteriorate over time. Even if your team sets up a policy of doing refactoring ~15% of the time this will be overridden by business interests more often than not. This is essentially a corollary of Conway's Law. You should not expect code bases to be better than the business incentivizes it to be. I'm speaking from personal experience here; this is burnout territory. Keep in mind these are very general observations and every company is different. In some companies what you and Crockford suggest is possible. I'd wager it's the exception rather than the rule though. reply renewiltord 13 hours agoprev“Any fool can build a bridge that stands. It takes an engineer to build a bridge that barely stands”. Absent trade-offs I can do anything. reply maerF0x0 12 hours agoparentor as I like to tell my product folks \"I can build you anything a computer can do, but you might not like the cost\" reply wly_cdgr 11 hours agoprevThere's a reason everyone knows who Steve Jobs is and no one knows or cares who the vast majority of these corner-cutting \"professionals\" are reply v7n 1 hour agoparentEveryone knows that one aspiring Austrian painter and do not know most other heads of states. I'd say not falling into disrepute is a clear secondary goal for many corner cutters. reply JadeNB 11 hours agoprev> A professional developer does thorough work when it matters, and cuts irrelevant corners that aren’t worth wasting time on. The problem, it seems to me, is that, while a furniture maker may have a reasonably stable idea of what their furniture will be used for, I think the makers of truly useful software can have no reasonable idea of the uses to which their software will eventually be put, and so must design so that the program can accommodate the demands put on it, no matter how unexpected or bizarre. How many modern security issues (I ask rhetorically) stem from design decisions rooted, explicitly or implicitly, in the assumption that the software would never be exposed to hostile actors? reply fifteen1506 14 hours agoprev [–] TL;DR. Cutting corners sometimes makes sense -- that's why timeboxing is important. But always? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog explores professional corner-cutting in software development, comparing it to the craftsmanship of cabinetmakers.",
      "Emphasizes owning technical decisions, understanding customer needs, and making deliberate choices when cutting corners in software development.",
      "Stresses focusing on end users, business context, and delivering high-quality work meeting customer needs in software development."
    ],
    "commentSummary": [
      "The discussion evaluates professional corner-cutting in software and hardware development, emphasizing craftsmanship, attention to detail, and quality as crucial factors.",
      "It highlights the influence of marketing on product success, ethical considerations in choosing employers, and finding a balance between speed and quality in professional roles.",
      "The conversation also addresses technical debt, code refactoring, and the complexities of handling dependencies in software development, stressing the importance of prioritizing quality and informed decision-making for sustainable development practices."
    ],
    "points": 160,
    "commentCount": 115,
    "retryCount": 0,
    "time": 1715540139
  },
  {
    "id": 40335599,
    "title": "Yi-1.5: Enhanced Pre-Trained Model for Coding and Reasoning",
    "originLink": "https://github.com/01-ai/Yi-1.5",
    "originBody": "🤗 HuggingFace • 🤖 ModelScope • ✡ WiseModel 👾 Discord • 🐤 Twitter • 💬 WeChat 📝 Paper • 🙌 FAQ • 📗 Learning Hub Intro News Quick Start Web Demo Deployment Fine-tuning API License Intro Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples. Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension. Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card. News 2024-05-13: The Yi-1.5 series models are open-sourced, further improving coding, math, reasoning, and instruction-following abilities. Requirements Make sure Python 3.10 or a later version is installed. Set up the environment and install the required packages. pip install -r requirements.txt Download the Yi-1.5 model from Hugging Face, ModelScope, or WiseModel. Quick Start This tutorial runs Yi-1.5-34B-Chat locally on an A800 (80G). from transformers import AutoModelForCausalLM, AutoTokenizer model_path = '' tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False) # Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM. model = AutoModelForCausalLM.from_pretrained( model_path, device_map=\"auto\", torch_dtype='auto' ).eval() # Prompt content: \"hi\" messages = [ {\"role\": \"user\", \"content\": \"hi\"} ] input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt') output_ids = model.generate(input_ids.to('cuda')) response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True) # Model response: \"Hello! How can I assist you today?\" print(response) Deployment Prerequisites: Before deploying Yi-1.5 models, make sure you meet the software and hardware requirements. vLLM Prerequisites: Download the lastest version of vLLM. Start the server with a chat model. python -m vllm.entrypoints.openai.api_server --model 01-ai/Yi-1.5-9B-Chat --served-model-name Yi-1.5-9B-Chat Use the chat API. HTTP curl http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Yi-1.5-9B-Chat\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"} ] }' Python client from openai import OpenAI # Set OpenAI's API key and API base to use vLLM's API server. openai_api_key = \"EMPTY\" openai_api_base = \"http://localhost:8000/v1\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) chat_response = client.chat.completions.create( model=\"Yi-1.5-9B-Chat\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a joke.\"}, ] ) print(\"Chat response:\", chat_response) Web Demo We have deployed a Yi-1.5-34B-Chat Space on Huggingface. Or you can build it locally by yourself, as follows: python demo/web_demo.py -cFine-tuning You can use LLaMA-Factory, Swift, XTuner, and Firefly for fine-tuning. These frameworks all support fine-tuning the Yi series models. API Yi APIs are OpenAI-compatible and provided at Yi Platform. Sign up to get free tokens, and you can also pay-as-you-go at a competitive price. Additionally, Yi APIs are also deployed on Replicate and OpenRouter. License The code and weights of the Yi-1.5 series models are distributed under the Apache 2.0 license. [ Back to top ⬆ ]",
    "commentLink": "https://news.ycombinator.com/item?id=40335599",
    "commentBody": "Yi 1.5 (github.com/01-ai)148 points by tosh 17 hours agohidepastfavorite53 comments gardnr 15 hours agoYi is led by Dr. Kai-Fu Lee. They have been releasing a lot of really good models over the last ~6 months. Their previous (1.0?) Yi-34B-Chat model ranks similar to GPT-3.5 on Chatbot Arena. [1] A quantized version of that model can be run on a single consumer video card like the RTX 4090. This new set of models should raise the bar again by adding more options to the open source LLM ecosystem. If you inspect the config.json[2] in the model repo on HuggingFace, you can see that the model architecture is LlamaForCausalLM (the same as Meta's Lllama). The difference between the Yi models and a simple fine-tuning is that Yi models have had a different set of data, configuration, and process going back to the pre-training stage. Their models perform well in Chinese and in English. There are a lot of good models coming out of China, some of which are only published to ModelScope. I haven't spent much time on ModelScope because I don't have a Chinese mobile number to use to create an account. Fortunately, Yi publish to HuggingFace as well. [1] https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboar... [2] https://huggingface.co/01-ai/Yi-1.5-34B-Chat/blob/fa695ee438... reply option 12 hours agoparentTry asking their \"chat\" variants about topics sensetive to CCP, like what has happened on Tiananmen square. Same for Baichan models. What other values and biases have been RLHFed there and for what purpose? reply happypumpkin 6 hours agorootparentUSER: What happened at Tienanmen square? yi-34b-chat: Tiananmen Square is a large public square in the heart of Beijing, China. It is named after the Tiananmen (literally \"Gate of Heavenly Peace\"), which is the gate located to the north of the square and was the main entrance to the Imperial City during the Ming and Qing dynasties. The square is famously known for two significant events in Chinese history: The May Fourth Movement (1919): This was a student-led movement that began on May 4, 1919, in response to the Chinese government's handling of the Treaty of Versailles, which granted German concessions in Shandong to Japan rather than returning them to China. The movement sparked a wave of anti-imperialist sentiment and calls for cultural and political reform, and it is considered a key event leading to the birth of Chinese nationalism and the development of the Chinese Communist Party. The Tiananmen Square Massacre (1989): This is the event for which Tiananmen Square is most internationally renowned. In the spring of 1989, there were student-led demonstrations in Beijing and other cities across China, calling for political reform and an end to corruption. The protests were initially peaceful, but on June 3 and 4, the Chinese government responded with a military crackdown, opening fire on the protesters and bystanders, killing and injuring thousands. The exact death toll is unknown, but it is estimated to be in the hundreds or even thousands. The Tiananmen Square Massacre is a significant event in modern Chinese history and has had a profound impact on the country's politics and society. It remains a sensitive topic in China, with government censorship and restrictions on discussing the event. reply polygamous_bat 11 hours agorootparentprevThis is an interesting question. Is there a “controversy-benchmark” perhaps, to measure this? reply HKH2 7 hours agorootparentprevOf course, otherwise it wouldn't be allowed in China. Where can I find LLMs that are uncensored and accessible (without logging in)? reply ekianjo 11 hours agorootparentprevthe American models are similarly censored for specific topics... reply falcor84 10 hours agorootparentOh? Interesting. Which historical events are they unwilling to discuss? reply oceanplexian 6 hours agorootparentI just asked GPT-4 if the government lied when they claimed face masks didn’t prevent COVID-19 early in the pandemic. It evaded the question, and said that masks weren’t recommended because there were shortages. But that wasn’t the question. The question was if the government was lying to the public. I’m going to guess a Chinese model would have a different response, and GPT-4 has been “aligned” to lie about uncomfortable facts. reply dartos 6 hours agorootparentTbf there is a lot more conflicting information on the internet (thus GPT’s training data) about COVID mask restrictions than Tiananmen Square. reply otterley 5 hours agorootparentprevWhy would you ask an LLM whether a government was lying? It’s a language model, not an investigative body with subpoena power charged with determining whether someone made an intentionally false representation. reply rgoulter 6 hours agorootparentpreve.g. Google's Gemini's output had some rather embarrassing biases in its outputs; to the point where asking it to \"draw a 1943 German soldier\" resulted in images of women and black soldiers. https://www.nytimes.com/2024/02/22/technology/google-gemini-... I wouldn't put that on the same level as \"refusing to talk about massacre of civilians\"; but I wouldn't put it to the level of \"free and unbiased\" either. reply ekianjo 9 hours agorootparentprevI did not mention historical events in my comment. And in any case, whatever model you train is going to have the biases of the training datasets, and if you make heavy use of Wikipedia you will have the footprint of Wikipedia in your output, for good or bad. reply InTheArena 8 hours agorootparent\"Similarly censored for similar topics\" implies heavily that hypothetical events such as Tiananmen Square would be similarly surprised by English large language models. reply ekianjo 6 hours agorootparentThe actual comment said \"similarly censored for specific topics...\" reply option 10 hours agorootparentprevI’ll take American values over CCP any day reply ekianjo 6 hours agorootparentWell it's good to have a choice and to compare answers on a broad range of topics and see which one is the most reliable, for what kind of questions, so that you know what you are working with in the end. reply ukuina 10 hours agoprevRemember when they claimed Yi had 200k context length despite it having 16k of usable context? I remember, because I spent non-trivial effort trying to make it work for long-form technical summarization. My lackluster findings were validated by RULER. https://github.com/hsiehjackson/RULER reply Havoc 14 hours agoprevNever had any luck with the Yi family of models. They tend to get sidetracked and respond in Chinese. Maybe my setup is somehow flawed reply Oranguru 5 hours agoparentYou can easily fix this using a grammar constraint with llama.cpp. Add this to the command: --grammar \"root ::= [^一-鿿ぁ-ゟァ-ヿ가-힣]*\" This will ban Chinese characters from the sampling process. Works for Yi and Qwen models. reply segmondy 14 hours agoparentprevYour setup is flawed. reply qeternity 13 hours agorootparentNo, it's not. This is a common issue with Yi models. reply Art9681 10 hours agorootparentThis is my experience too. Could be my setup. But I dont have this problem with any other models. That makes it their problem. reply segmondy 8 hours agorootparentprevI have no issue with it. reply smcleod 11 hours agoprevWhile interesting, Yi 1.5 only has a 4K context window, which means it's not going to be useful for a lot of use cases. reply tosh 15 hours agoprevBenchmark charts on model card: https://huggingface.co/01-ai/Yi-1.5-34B-Chat#benchmarks Yi 34b with results similar to Llama 3 70b and Mixtral 8x22b Yi 6b and 9b with results similar to Llama 3 8b reply GaggiX 15 hours agoparentWe need to wait for LMSYS Chatbot Arena to actually see the performance of the model. reply tosh 14 hours agorootparentI had good results with the previous Yi-34b and its fine tunes like Nous-Capybara-34B. Will be interesting to see what Chatbot Arena thinks but my expectations are high. https://huggingface.co/NousResearch/Nous-Capybara-34B reply zone411 11 hours agorootparentprevNo, Lmsys is just another very obviously flawed benchmark. reply aubanel 10 hours agorootparentPlease elaborate on this: how is it flawed? reply BoorishBears 5 hours agorootparentIt's horribly useless for most use cases since half of it is people probing for riddles that don't transfer to any useful downstream task, and the other half is people probing for morality. Some tiny portion is people asking for code, but every model has its own style of prompting and clarification that works best, so you're not going to be able to use a side-by-side view to get the best result. The \"will it tell me how to make meth\" stuff is a huge source of noise, which you could argue is digging for refusals which can be annoying, and the benchmark claims to filter out... but in reality a bunch of the refusals are soft refusals that don't get caught, and people end up downvoting the model that's deemed \"corporate\". Honestly the fact that any closed source model with guardrails can even place is a miracle, in a proper benchmark the honest to goodness gap between most closed source models and open source models would be so large it'd break most graphs. reply GaggiX 1 hour agorootparentThis is so nonsensical it's hilarious, \"corporate\" models have always been at the top of the leaderboard. reply CuriouslyC 10 hours agorootparentprevFlawed in some ways but still fairly hard to game and useful. reply qeternity 13 hours agoparentprevPretraining on the test set is all you need. LLM benchmarks are horribly broken. IMHO there is better signal in just looking at parameter counts. reply BoorishBears 5 hours agorootparentLooking at tokens they were trained on is also a really great indicator of world understanding. Llama 3 is a game changer for some usecases because there's finally a model that understands the world deeply as opposed to typical models which can be fine tuned into hyper specific tasks, but generalize poorly, especially in D2C usecases where someone might probe the model's knowledge reply 999900000999 12 hours agoprevIs 16 GB of ram enough to run these locally? I'm considering a new laptop later this year and the ram is now fixed to 16GB on most of them. I plan on digging deep into ML during my coming break from paid work. reply tosh 12 hours agoparent16 GB is enough to run quantized versions of 9b and 6b. reply coolestguy 12 hours agoparentprevNo - 16gb of ram is barely enough to run regular applications if you're a power user let alone the most breakthrough computationally heavy workloads ever invented reply 999900000999 11 hours agorootparentThe price difference is about 150$ give or take for the laptops I'm looking at. I'll keep this in mind! reply Zambyte 7 hours agorootparent16 GB of system memory vs 16 GB of VRAM / unified memory (? I think this is the case for recent Apple machines) makes a huge difference. The former is more of a neat party trick (depending on who you hang out with) and the latter is actually something you can use as a tool to be more efficient. I recently bought a 7900 XTX with 24 GB of VRAM, but the model I currently run can easily run in 16 GB (6 bit llama 3 8b). It's fast enough and high enough quality that I can use it for processing information that I don't feel comfortable sharing with hosted services. It's definitely not the best of the best as far as what models are able to do right now, but it's surprisingly useful. reply pinusc 7 hours agorootparentprevAlso keep in mind: 32GB of RAM is more than enough for normal usage, but it's useless for (this kind of state-of-the-art-) ML unless you also have a graphics card of the kind that won't fit in a laptop. Unless of course you were talking about VRAM, in which case 16GB is still not great for ML (to be fair, the 24GB of an RTX 4090 aren't either, but there's not much more you can do in the space of consumer hardware). I don't think the other commenter was talking about VRAM, because 16GB VRAM are very overkill for everyday computing... and pretty decent for most gaming. reply 999900000999 6 minutes agorootparentI'm not planing on developing state of the art ML, I just need to run the models locally and maybe do some light tuning. I don't want to have a laptop over 3 pounds and I'm not spending over 1100$, so a dedicated GPU isn't really an option. reply tosh 3 hours agorootparentprevWith 32 GB RAM you can do inference with quantized 34b models. I wouldn’t call that useless? You don’t need a GPU for llm inference. Might not be as fast as it could be but usable. reply hnfong 5 hours agorootparentprevIt's almost a myth these days that you need top end GPUs to run models. Some smaller models (say <10B parameters with quantization) run on CPUs fine. Of course you won't have hundreds of tokens per sec, but you'll probably get around ~10 or so, which can be sufficient depending on your use case. reply ThaRedah 7 hours agoprevhttps://www.youtube.com/watch?v=q6EoRBvdVPQ&pp=ygUDWWVl reply adt 12 hours agoprevhttps://lifearchitect.ai/models-table/ reply Hugsun 11 hours agoparentThis page is confusing to me. How is it useful to you? I can see some utility but am curious if there's something I'm missing. reply wg0 8 hours agoprevSomeday, \"OpenAI\" will open up their models too. reply xhevahir 15 hours agoprev\"Yi-1.5 is an upgraded version of Yi\" is not a very informative beginning. reply kkzz99 15 hours agoparent\"It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples. Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension. Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.\" Literally after that... reply Jaxan 15 hours agorootparentSo it’s a large language model? reply pohl 8 hours agorootparentprevBut what is Yi? reply mountainriver 15 hours agoprev [–] Is it the same bad license? reply tosh 14 hours agoparent [–] It looks like they switched to Apache 2.0 for the weights. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Yi-1.5 is an enhanced version of Yi, trained on a vast corpus of 500 billion tokens and fine-tuned on 3 million varied samples, offering improved capabilities in coding, math, reasoning, and instruction-following, while retaining language understanding, common-sense reasoning, and reading comprehension.",
      "The models are open source, coming in different sizes, and come with deployment instructions, usage guidelines, and a web demo. Users have the flexibility to fine-tune the models with supported frameworks, and Yi APIs are accessible across various platforms with Apache 2.0 licensing."
    ],
    "commentSummary": [
      "Dr. Kai-Fu Lee's team at Yi 1.5 launched the Yi-34B-Chat language models, akin to GPT-3.5, leveraging the LlamaForCausalLM design, excelling in Chinese and English.",
      "Available on platforms like HuggingFace and ModelScope, these models spark discussions in the machine learning community on performance, biases, hardware needs, and licensing, prompting debates on capabilities, drawbacks, and forthcoming evaluations.",
      "The release signifies a notable advancement in language models, inviting in-depth scrutiny and exploration within the tech sphere."
    ],
    "points": 148,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1715530984
  }
]
