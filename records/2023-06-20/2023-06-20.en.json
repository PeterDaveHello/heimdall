[
  {
    "id": 36393030,
    "timestamp": 1687192384,
    "title": "PostgreSQL reconsiders its process-based model",
    "url": "https://lwn.net/SubscriberLink/934940/3abb2d4086680b78/",
    "hn_url": "http://news.ycombinator.com/item?id=36393030",
    "content": "LWN.netNews from the sourceContentWeekly EditionArchivesSearchKernelSecurityEvents calendarUnread commentsLWN FAQWrite for usUser: Password: | |PostgreSQL reconsiders its process-based model[LWN subscriber-only content]Welcome to LWN.netThe following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!By Jonathan CorbetJune 19, 2023In the fast-moving open-source world, programs can come and go quickly; a tool that has many users today can easily be eclipsed by something better next week. Even in this environment, though, some programs endure for a long time. As an example, consider the PostgreSQL database system, which traces its history back to 1986. Making fundamental changes to a large code base with that much history is never an easy task. As fundamental changes go, moving PostgreSQL away from its process-oriented model is not a small one, but it is one that the project is considering seriously.A PostgreSQL instance runs as a large set of cooperating processes, including one for each connected client. These processes communicate through a number of shared-memory regions using an elaborate library that enables the creation of complex data structures in a setting where not all processes have the same memory mapped at the same address. This model has served the project well for many years, but the world has changed a lot over the history of this project. As a result, PostgreSQL developers are increasingly thinking that it may be time to make a change.A proposalAt the beginning of June, Heikki Linnakangas, seemingly following up on some in-person conference discussions, posted a proposal to move PostgreSQL to a threaded model.I feel that there is now pretty strong consensus that it would be a good thing, more so than before. Lots of work to get there, and lots of details to be hashed out, but no objections to the idea at a high level.The purpose of this email is to make that silent consensus explicit.The message gave a quick overview of some of the challenges involved in making such a move, and acknowledged, in an understated way, that this transition \"surely cannot be done fully in one release\". One thing that was missing was a discussion of why this big change would be desirable, but that was filled in as the discussion went on. As Andres Freund put it:I think we're starting to hit quite a few limits related to the process model, particularly on bigger machines. The overhead of cross-process context switches is inherently higher than switching between threads in the same process - and my suspicion is that that overhead will continue to increase. Once you have a significant number of connections we end up spending a *lot* of time in TLB misses, and that's inherent to the process model, because you can't share the TLB across processes.He also pointed out that the process model imposes costs on development, forcing the project to maintain a lot of duplicated code, including several memory-management mechanisms that would be unneeded in a single address space. In a later message he also added that it would be possible to share state more efficiently between threads, since they all run within the same address space.The reaction of some developers, though, made it clear that the \"pretty strong consensus\" cited by Linnakangas might not be quite that strong after all. Tom Lane said: \"I think this will be a disaster. There is far too much code that will get broken\". He added later that the cost of this change would be \"enormous\", it would create \"more than one security-grade bug\", and that the benefits would not justify the cost. Jonathan Katz suggested that there might be other work that should have a higher priority. Others worried that losing the isolation provided by separate processes could make the system less robust overall.Still, many PostgreSQL developers seem to be cautiously in favor of at least exploring this change. Robert Haas said that PostgreSQL does not scale well on larger systems, mostly as a result of the resources consumed by all of those processes. \"Not all databases have this problem, and PostgreSQL isn't going to be able to stop having it without some kind of major architectural change\". Just switching to threads might not be enough, he said, but he suggested that this change would enable a number of other improvements.How to get thereMoving the core of the PostgreSQL server into a single address space will certainly present a number of challenges. The biggest one, as pointed out by Haas and others, would appear to be the server's \"widespread and often gratuitous use of global variables\". Globals work well enough when each server process has its own set, but that approach clearly falls apart when threads are used instead. According to Konstantin Knizhnik, there are about 2,000 such variables currently used by the PostgreSQL server.A couple of approaches to this problem were discussed. One was pulling all of the global variables into a big \"session state\" structure that would be thread-local. That idea quickly loses its appeal, though, when one considers trying to create and maintain a 2,000-member structure, so the project is unlikely to go this way. The alternative is to simply throw all of the globals into thread-local storage, an approach that is easy and would work, but heavy use of thread-local storage would exact a performance penalty that would reduce the benefits of the switch to threads in the first place. Haas said that marking globals specially (to put them into thread-local storage, among other things) would be a beneficial project in its own right, as that would be a good first step in reducing their use. Freund agreed, saying that this effort would pay off even if the switch to threads never happens.But, Freund cautioned, moving global variables to thread-local storage is the easiest part of the job:Redesigning postmaster, defining how to deal with extension libraries, extension compatibility, developing tools to make developing a threaded postgres feasible, dealing with freeing session lifetime memory allocations that previously were freed via process exit, making the change realistically reviewable, portability are all much harder.An interesting point that received surprisingly little attention in the discussion is that Knizhnik has already done a threads port of PostgreSQL. The global-variable problem, he said, was not that difficult. He had more trouble with configuration data, error handling, signals, and the like. Support for externally maintained extensions will be a challenge. Still, he saw some significant benefits in working in the threaded environment. Anybody who is thinking about taking on this project would be well advised to look closely at this work as a first step.Another complication that the PostgreSQL developers have in mind is that of supporting both the process-based and thread-based modes, perhaps indefinitely. The need to continue to support running in the process-based mode would make it harder to take advantage of some of the benefits offered by threads, and would significantly increase the maintenance burden overall. Haas, though, is not convinced that it would ever be possible to remove support for the process-based mode. Threads might not perform better for all use cases, or some important extensions may never gain support for running in threads. The removal of process support is, as he noted, a question that can only really be considered once threads are working well.That point is, obviously, a long way into the future, assuming it arrives at all. While the outcome of the discussion suggests that most PostgreSQL developers think that this change is good in the abstract, there are also clearly concerns about how it would work in practice. And, perhaps more importantly, nobody has, yet, stepped up to say that they would be willing to put in the time to push this effort forward. Without that crucial ingredient, there will be no switch to threads in any sort of foreseeable future.(Log in to post comments)Aim for the starsPosted Jun 19, 2023 16:11 UTC (Mon) by Wol (subscriber, #4433) [Link]> While the outcome of the discussion suggests that most PostgreSQL developers think that this change is good in the abstract, there are also clearly concerns about how it would work in practice.And you might hit the moon. Aim nowhere and you're going nowhere.Look at the GIL (was that Python?) and the Big Kernel Lock in linux. Whether you get there or not, a lot of the work on the way sounds like it's worth it in its own right. Like getting rid of all those global variables!Even being able to break up each process into a bunch of threads for the easy stuff could lead to massive benefits - threading where it works well, processes where they work well.I wish you all God Speed on the voyage!Cheers,WolAim for the starsPosted Jun 19, 2023 18:18 UTC (Mon) by zoobab (guest, #9945) [Link]Maybe yse zeromq ipc messages between threads?Aim for the starsPosted Jun 20, 2023 4:44 UTC (Tue) by j16sdiz (subscriber, #57302) [Link]ZeroMQ is a big mess when it comes to threading model and error recovery.It do too much magic behind your back. When it comes to database, we need more explicit (or flexible) error handling.Aim for the starsPosted Jun 19, 2023 20:19 UTC (Mon) by nevyn (subscriber, #33129) [Link]Python GIL and Linux Big kernel lock seem like very bad comparisons. In those cases there is/was no Parallelism, here there is Parallelism but _maybe_ the scaling is better if you change \"everything\" and _maybe_ the security/robustness is the same.This is \"closer\" to the apache-httpd move, the main difference being I don't know enough about PostgreSQL and the plans to move to imply the outcome will be that bad.Aim for the starsPosted Jun 19, 2023 22:22 UTC (Mon) by Wol (subscriber, #4433) [Link]It wasn't meant as a comparison. The Big Kernel Lock and the GIL enforced \"single process\". PostgreSQL *is* a single process?Linux and Python decided that removing that restriction was worthwhile. Whether PostgreSQL succeeds or not, the effort they make towards removing that restriction may well be worthwhile.Cheers,WolAim for the starsPosted Jun 19, 2023 23:18 UTC (Mon) by michaelmior (guest, #165680) [Link]Postgres scales by coordinating among multiple processes on a single machine. The proposal is to use multiple threads instead of multiple processes.This is similar to the CPython GIL, but the GIL doesn't enforce a single process. It prevents multiple threads from running concurrently in the same process. In CPython with the GIL, multiple processes are *necessary* to scale CPU-bound code.Aim for the starsPosted Jun 20, 2023 4:44 UTC (Tue) by rtpg (subscriber, #114619) [Link]I would go even further, there are a good amount of people who argue for the GIL to stay in Python ~forever, mostly because the mental model is easier and it rules out entire classes of bugs.The GIL stuck along enough to allow for async, and so you have async for lots of parallelism in one direction, stuff like multiprocessing in the other. Even heavy calculation stuff is pretty \"eh whatever\" because in practice it often calls into other libraries which release the GIL.GILectomy work has been many many many many false starts, and I think we're learning stuff from it (and it might still be the right way to go in the end!), but it's been tough to find work from those projects that end up being usable (namely because of new locking patterns needing to be figured out in the alternative)Aim for the starsPosted Jun 20, 2023 8:13 UTC (Tue) by NYKevin (subscriber, #129325) [Link]Another part of the problem is the fact that CPython is \"good enough.\"Anyone who wants to get rid of the GIL can transpile to C with Cython, annotate any objects that need to be accessed outside the GIL as C types, and then write \"with nogil:\" to release the GIL. It will run much faster than CPython even if you're single-threaded, and can be done incrementally on a module-by-module basis in most cases.The main downsides of this strategy are:* CPython is more mature than Cython.* CPython has a (slightly) more straightforward build process, especially if you have zero non-stdlib dependencies.* Cython specifically requires a C compiler.* C types are not Python types. There are semantic differences. You have to do additional testing if you're converting an existing codebase.* C is not a terribly complicated language, but if you don't know it at all, then you probably need to learn it first.But none of those are hard blockers. They're just friction. If you really strongly need to drop the GIL, this is a perfectly reasonable way of doing it. The fact is, most people asking for a GILectomy either haven't looked into alternatives like Cython, don't want free threading badly enough to overcome the activation energy of this strategy, or have already built a large CPU-bound multithreaded application in Python which is too big to annotate, despite the threading docs explicitly saying not to do that.PostgreSQL reconsiders its process-based modelPosted Jun 19, 2023 19:26 UTC (Mon) by raven667 (subscriber, #5198) [Link]I know nothing of the PostgreSQL internals or the relevant engineering but throwing an opinion out there anyway; is there a way to make a minimal threaded implementation that just covers the necessary features needed for the most extreme large servers where threading could help? If you made a ton of caveats about what features are supportable, ie anything not used by the large instances you want test with, can you reduce the scope of what work is needed to something more manageable that can be iterated on? Steady improvement without taking on a big chunk of risk to rework the whole internal architecture, even if it takes longer, is probably the way to go for an old mature software project like this, right?PostgreSQL reconsiders its process-based modelPosted Jun 19, 2023 19:45 UTC (Mon) by jhoblitt (subscriber, #77733) [Link]Semi-seriously, why not port the postgresql sql dialect to use mariadb as the backend? Mariadb (mysql...) has had a robust threaded model and binary redo logs for literally decades.PostgreSQL reconsiders its process-based modelPosted Jun 19, 2023 19:48 UTC (Mon) by pizza (subscriber, #46) [Link]> Semi-seriously, why not port the postgresql sql dialect to use mariadb as the backend? Mariadb (mysql...) has had a robust threaded model and binary redo logs for literally decades.Because it's not Postgresql's \"dialect\" that matters here, but rather the features and robustness that dialect exposes....Mariadb might as well be on another planet in comparison.PostgreSQL reconsiders its process-based modelPosted Jun 19, 2023 23:19 UTC (Mon) by butlerm (subscriber, #13312) [Link]I believe the short answer is doing that would be tantamount to the PostgreSQL project throwing away nearly everything they have done for the past couple of decades. In addition, unless MariaDB has made remarkable progress in the past few years it isn't anywhere close to implementing PostgreSQL's full feature set or in particular being able to implement those features in a backward compatible manner with PostgreSQL.When you get down into the details relational database implementations tend to be remarkably different from each other in terms of more user level aspects (functions, data types, options, apis) than you can count. I think it is safe to say the PostgreSQL developers have not reached quite that level of desperation yet. But if someone wanted to take that on as a software engineering challenge the results would certainly be interesting to read about.PostgreSQL reconsiders its process-based modelPosted Jun 19, 2023 20:29 UTC (Mon) by flussence (subscriber, #85566) [Link]Oh this is quite some news. I don't mind early adopting performance features, but\u2026In Apache httpd I've been using every experimental threaded/event mpm as it becomes available, because the forking model always felt a bit gross to me. But that's software that has had pluggable backends for decades, and even so it's still a bit rough around the edges. I generally trust the Postgres developers to not screw up but I think this kind of change would need two or three major release cycles before I'd feel comfortable turning it on in production.Copyright \u00a9 2023, Eklektix, Inc.Comments and public postings are copyrighted by their creators.Linux is a registered trademark of Linus Torvalds",
    "summary": "- PostgreSQL, a popular database system, is considering a fundamental change to move away from its process-oriented model.\n- Developers believe that this change could address performance limitations on larger systems and reduce duplicated code, but there are concerns about potential breaking changes and loss of system robustness.\n- The move to threads would present challenges such as reworking global variables and ensuring compatibility with existing extensions, but a previous threads port of PostgreSQL provides some insights for the project.",
    "hn_title": "PostgreSQL reconsiders its process-based model",
    "original_title": "PostgreSQL reconsiders its process-based model",
    "score": 720,
    "hn_content": "- PostgreSQL is considering a shift from its process-based model to a thread-based model.\n- Some developers express concern over potential issues and risks that could arise from this change.\n- The discussion around this change draws parallels to past experiences with PHP 6 and the Lunar Module guidance software, highlighting the importance of careful experimentation and quick failure.\n- There are differing opinions on when and how code should be rewritten, with some advocating for conservative rewriting and others emphasizing the benefits of iterative improvements.\n- The transition to a thread-based model in PostgreSQL could have significant implications for stability, performance, and development processes.\n- The complexity and potential risks associated with the change make it a topic of interest for tech-savvy individuals, as it could have a profound impact on the future of PostgreSQL.\n- It is important to consider the potential benefits and drawbacks of such a transition, as well as the challenges and trade-offs involved.- The discussion is about whether PostgreSQL should switch from a process-based model to a threaded model.\n- One of the primary reasons for the switch is to improve performance by better sharing of caches and reducing overhead.\n- Some argue that the multi-threaded model would simplify things in the application layer and allow for more interesting things with the database.\n- Oracle has already switched to a threaded model in version 12c, although this is optional and the default is still a process model on Linux.\n- Some members of the community are interested in making this transition, including Heikki, a longtime contributor to PostgreSQL.\n- However, there are concerns about the difficulty and potential risks involved in such a significant change.\n- The debate includes discussions on the challenges of testing and ensuring correctness in a multi-threaded environment.\n- Suggestions for alternative solutions include creating an experimental fork with a different name or finding a middle ground between threads and processes.\n- The current process-based model offers resilience against many types of errors, but there is a desire to improve resource sharing.\n- It is also noted that the performance benefits of the multi-threaded model may not outweigh the cost and complexity.\n- Other database options, like CockroachDB, offer more horizontal scaling features.\n- There is discussion about the possibility of modifying the Linux kernel to improve multi-process efficiency, but this is seen as a much larger task.\n- Overall, the community is divided on whether the transition to a threaded model is necessary or worth the effort.",
    "hn_summary": "- PostgreSQL is considering a shift from its process-based model to a thread-based model.\n- The transition could have significant implications for stability, performance, and development processes.\n- The community is divided on whether the transition is necessary or worth the effort."
  },
  {
    "id": 36388219,
    "timestamp": 1687159025,
    "title": "OpenLLM",
    "url": "https://github.com/bentoml/OpenLLM",
    "hn_url": "http://news.ycombinator.com/item?id=36388219",
    "content": "OpenLLMAn open platform for operating large language models (LLMs) in production.Fine-tune, serve, deploy, and monitor any LLMs with ease.IntroductionWith OpenLLM, you can run inference with any open-source large-language models, deploy to the cloud or on-premises, and build powerful AI apps.State-of-the-art LLMs: built-in supports a wide range of open-source LLMs and model runtime, including StableLM, Falcon, Dolly, Flan-T5, ChatGLM, StarCoder and more.Flexible APIs: serve LLMs over RESTful API or gRPC with one command, query via WebUI, CLI, our Python/Javascript client, or any HTTP client.Freedom To Build: First-class support for LangChain, BentoML and HuggingFace that allows you to easily create your own AI apps by composing LLMs with other models and services.Streamline Deployment: Automatically generate your LLM server Docker Images or deploy as serverless endpoint via BentoCloud.Bring your own LLM: Fine-tune any LLM to suit your needs with LLM.tuning(). (Coming soon)\ud83c\udfc3\u200d Getting StartedTo use OpenLLM, you need to have Python 3.8 (or newer) and pip installed on your system. We highly recommend using a Virtual Environment to prevent package conflicts.You can install OpenLLM using pip as follows:pip install openllmTo verify if it's installed correctly, run:$ openllm -hUsage: openllm [OPTIONS] COMMAND [ARGS]...  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557  \u2588\u2588\u2557\u2588\u2588\u2557   \u2588\u2588\u2557   \u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551 \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551 \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551  \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d   \u255a\u2550\u255d An open platform for operating large language models in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.Starting an LLM ServerTo start an LLM server, use openllm start. For example, to start a OPT server, do the following:openllm start optFollowing this, a Web UI will be accessible at http://localhost:3000 where you can experiment with the endpoints and sample input prompts.OpenLLM provides a built-in Python client, allowing you to interact with the model. In a different terminal window or a Jupyter notebook, create a client to start interacting with the model:>>> import openllm>>> client = openllm.client.HTTPClient('http://localhost:3000')>>> client.query('Explain to me the difference between \"further\" and \"farther\"')You can also use the openllm query command to query the model from the terminal:export OPENLLM_ENDPOINT=http://localhost:3000openllm query 'Explain to me the difference between \"further\" and \"farther\"'Visit http://localhost:3000/docs.json for OpenLLM's API specification.Users can also specify different variants of the model to be served, by providing the --model-id argument, e.g.:openllm start flan-t5 --model-id google/flan-t5-largeUse the openllm models command to see the list of models and their variants supported in OpenLLM.Supported ModelsThe following models are currently supported in OpenLLM. By default, OpenLLM doesn't include dependencies to run all models. The extra model-specific dependencies can be installed with the instructions below:Model CPU GPU Installation Model Idsflan-t5pip install \"openllm[flan-t5]\"google/flan-t5-smallgoogle/flan-t5-basegoogle/flan-t5-largegoogle/flan-t5-xlgoogle/flan-t5-xxldolly-v2pip install openllmdatabricks/dolly-v2-3bdatabricks/dolly-v2-7bdatabricks/dolly-v2-12bchatglmpip install \"openllm[chatglm]\"thudm/chatglm-6bthudm/chatglm-6b-int8thudm/chatglm-6b-int4starcoderpip install \"openllm[starcoder]\"bigcode/starcoderbigcode/starcoderbasefalconpip install \"openllm[falcon]\"tiiuae/falcon-7btiiuae/falcon-40btiiuae/falcon-7b-instructtiiuae/falcon-40b-instructstablelmpip install openllmstabilityai/stablelm-tuned-alpha-3bstabilityai/stablelm-tuned-alpha-7bstabilityai/stablelm-base-alpha-3bstabilityai/stablelm-base-alpha-7boptpip install openllmfacebook/opt-125mfacebook/opt-350mfacebook/opt-1.3bfacebook/opt-2.7bfacebook/opt-6.7bfacebook/opt-66bRuntime Implementations (Experimental)Different LLMs may have multiple runtime implementations. For instance, they might use Pytorch (pt), Tensorflow (tf), or Flax (flax).If you wish to specify a particular runtime for a model, you can do so by setting the OPENLLM_{MODEL_NAME}_FRAMEWORK={runtime} environment variable before running openllm start.For example, if you want to use the Tensorflow (tf) implementation for the flan-t5 model, you can use the following command:OPENLLM_FLAN_T5_FRAMEWORK=tf openllm start flan-t5Note For GPU support on Flax, refers to Jax's installation to make sure that you have Jax support for the corresponding CUDA version.Integrating a New ModelOpenLLM encourages contributions by welcoming users to incorporate their custom LLMs into the ecosystem. Check out Adding a New Model Guide to see how you can do it yourself.IntegrationsOpenLLM is not just a standalone product; it's a building block designed to easily integrate with other powerful tools. We currently offer integration with BentoML and LangChain.BentoMLOpenLLM models can be integrated as a Runner in your BentoML service. These runners have a generate method that takes a string as a prompt and returns a corresponding output string. This will allow you to plug and play any OpenLLM models with your existing ML workflow.import bentomlimport openllmmodel = \"opt\"llm_config = openllm.AutoConfig.for_model(model)llm_runner = openllm.Runner(model, llm_config=llm_config)svc = bentoml.Service(  name=f\"llm-opt-service\", runners=[llm_runner])@svc.api(input=Text(), output=Text())async def prompt(input_text: str) -> str:  answer = await llm_runner.generate(input_text)  return answerHuggingFace AgentsOpenLLM seamlessly integrates with HuggingFace Agents.Warning The HuggingFace Agent is still at experimental stage. It is recommended to OpenLLM with pip install -r nightly-requirements.generated.txt to get the latest API update for HuggingFace agent.import transformersagent = transformers.HfAgent(\"http://localhost:3000/hf/agent\") # URL that runs the OpenLLM serveragent.run(\"Is the following `text` positive or negative?\", text=\"I don't like how this models is generate inputs\")Note Only starcoder is currently supported with Agent integration. The example aboved was also ran with four T4s on EC2 g4dn.12xlargeIf you want to use OpenLLM client to ask questions to the running agent, you can also do so:import openllmclient = openllm.client.HTTPClient(\"http://localhost:3000\")client.ask_agent(  task=\"Is the following `text` positive or negative?\",  text=\"What are you thinking about?\",)LangChain (Coming Soon!)In future LangChain releases, you'll be able to effortlessly invoke OpenLLM models, like so:from langchain.llms import OpenLLMllm = OpenLLM.for_model(model_name='flan-t5')llm(\"What is the difference between a duck and a goose?\")if you have an OpenLLM server deployed elsewhere, you can connect to it by specifying its URL:from langchain.llms import OpenLLMllm = OpenLLM.for_model(server_url='http://localhost:8000', server_type='http')llm(\"What is the difference between a duck and a goose?\")Deploying to ProductionTo deploy your LLMs into production:Building a Bento: With OpenLLM, you can easily build a Bento for a specific model, like dolly-v2, using the build command.:openllm build dolly-v2A Bento, in BentoML, is the unit of distribution. It packages your program's source code, models, files, artifacts, and dependencies.Containerize your Bentobentoml containerize <name:version>BentoML offers a comprehensive set of options for deploying and hosting online ML services in production. To learn more, check out the Deploying a Bento guide.TelemetryOpenLLM collects usage data to enhance user experience and improve the product. We only report OpenLLM's internal API calls and ensure maximum privacy by excluding sensitive information. We will never collect user code, model data, or stack traces. For usage tracking, check out the code.You can opt-out of usage tracking by using the --do-not-track CLI option:openllm [command] --do-not-trackOr by setting environment variable OPENLLM_DO_NOT_TRACK=True:export OPENLLM_DO_NOT_TRACK=TrueCommunityEngage with like-minded individuals passionate about LLMs, AI, and more on our Discord!OpenLLM is actively maintained by the BentoML team. Feel free to reach out and join us in our pursuit to make LLMs more accessible and easy-to-use Join our Slack community!ContributingWe welcome contributions! If you're interested in enhancing OpenLLM's capabilities or have any questions, don't hesitate to reach out in our discord channel.Checkout our Developer Guide if you wish to contribute to OpenLLM's codebase.",
    "summary": "- OpenLLM is an open platform for operating large language models (LLMs) in production.\n- It supports a wide range of state-of-the-art LLMs and provides flexible APIs for serving and deploying LLMs.\n- Users can fine-tune LLMs to suit their needs and easily build AI apps by composing LLMs with other models and services.",
    "hn_title": "OpenLLM",
    "original_title": "OpenLLM",
    "score": 625,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginOpenLLM (github.com/bentoml)625 points by fzliu 1 day ago | hide | past | favorite | 164 commentsaarnphm 1 day ago | next [\u2013]Hi all, I'm the main maintainer from the OpenLLM team here. I'm actively developing the fine-tuning feature and will release a PR soon enough. Stay tuned. In the meanwhile, the best way to track the development workflow is at our discord, so feel free to join!!replyuser432678 1 day ago | parent | next [\u2013]Thanks for the great project! Any chance, your team might consider more open platform than Discord for posting updates? I personally find Discord hard to use, and there\u2019s no way to have sensible subscription (like RSS). Discord is usually muted.replypmoriarty 21 hours ago | root | parent | next [\u2013]Discord is a black hole where information goes to die. Its search and scrollback is awful. It's awful at being an archive, as finding anything that was asked more than a day or two ago is impractical.To use Discord in good faith and with open eyes, you have to prioritize communication in the present, and give up hope of archiving anything that was said for people who might need the information in the future.replyjoot82 18 hours ago | root | parent | next [\u2013]Discord is just a rich IRC replacement. You can log and search in IRC too but nobody seriously tries to archive information for research later. And big difference is it's all closed and operated by one entity that can change conditions at will. Don't even try to use it for anything else than real time chat.replypmoriarty 17 hours ago | root | parent | next [\u2013]\"Discord is just a rich IRC replacement\"That's only half true. Yes, Discord does allow a \"rich\" chat experience, with channels and servers, but there the similarities end.IRC is based on an open protocol, with many open source clients available for it, and a decentralized server infrastructure.Discord is closed and centralized, with only a single client available for it.You can easily log IRC channels, but there is no easy way to do that on Discord, if it can be done at all.I've logged every channel I've ever visited on IRC, and I can use powerful text tools to regex search through all of my conversations on IRC and have the results appear instantly. Nothing remotely like that is possible with Discord.Paging through IRC logs is virtually instant on a modern terminal, while Discord makes you wait a long time between every other page load, so if you need to look through more than a handful of pages it's incredibly slow and painful.Some IRC channels have their logs published on the web, making them fully searchable through web search engines, but to my knowledge no Discord channels do that.What happens in Discord stays in Discord.replyutbabya 38 minutes ago | root | parent | prev | next [\u2013]Greping through IRC logs has a 10x better UXreplyorangepurple 17 hours ago | root | parent | prev | next [\u2013]Furthermore, you risk getting banned for deleting messages you wrote in the pastreplywinddude 19 hours ago | root | parent | prev | next [\u2013]agreed.replyKiro 21 hours ago | root | parent | prev | next [\u2013]I find their search amazing. What's your issue with it?replyaardshark 19 hours ago | root | parent | next [\u2013]Here's just one issue:They stem words aggressively, so searching for \"repeater\", which is a less common, specific term, gives you results including \"repeat\", a commonly used word. And there's no way to do an exact word search.replygeysersam 21 hours ago | root | parent | prev | next [\u2013]The issue is it's not indexed by Googlereplywanderingbit 17 hours ago | root | parent | next [\u2013]There was a recent post about an open source tool for indexing Discord content and making it available for Google search:https://news.ycombinator.com/item?id=36383773replymicromacrofoot 20 hours ago | root | parent | prev | next [\u2013]have you used google lately? might as well not be indexed with all the seo spam you get as top resultsreplyandsoitis 20 hours ago | root | parent | next [\u2013]> have you used google lately? might as well not be indexed with all the seo spam you get as top resultsI just googled \"how to use openllm\" as an example to test your thesis, and the results look very relevant to me.https://www.google.com/search?client=safari&rls=en&q=how+to+...replyDragonStrength 20 hours ago | root | parent | next [\u2013]You might want to glance again because all of those results are for a different product.replyandsoitis 20 hours ago | root | parent | next [\u2013]Top of the results page says:\"Showing results for how to use openlmSearch instead for how to use openllm\"replyyrro 18 hours ago | root | parent | next [\u2013]FYI, specifying the nfpr=1 query string parameter will disable Google's idiot attempt to try be helpful by searching for something other than that which you want to link to.replymicromacrofoot 12 hours ago | root | parent | prev | next [\u2013]when I click this google gives me results for \u201chow to use openlm\u201d a commercial product, they literally change your search term if there\u2019s a product that fitsreply3np 1 day ago | root | parent | prev | next [\u2013]Related: As an operator/mod/admin it's fairly straight-forward to bridge a Discord channel to Matrix (and, if one so desires, from there to IRC), allowing users not on Discord to participate. Conservative mods concerned about spam can start with an allowlist for which servers can join.https://github.com/matrix-org/matrix-appservice-discordreplykhimaros 19 hours ago | root | parent | prev | next [\u2013]plugging the open source and self hostable https://revolt.chat which i've found to have great UX and be very performant compared to discord.replyshmolf 3 hours ago | root | parent | next [\u2013]I'm liking revolt. Thanks for the suggestion.replydluc 18 hours ago | root | parent | prev | next [\u2013]good alternative: https://www.linen.dev/replyralusek 19 hours ago | root | parent | prev | next [\u2013]I know this isn't a great time for reddit, but I just made this on your behalf:https://www.reddit.com/r/OpenLLM/I much prefer the HN/Reddit discussion format to Discord and even Stack Overflow.replynologic01 1 day ago | root | parent | prev | next [\u2013]s/rd/urse/greplysamstave 19 hours ago | root | parent | next [\u2013]HAHA this was one of my panel interview questions at Goooog'Q: \"How do you do a search and replace for a string in VI\"Me: I cant recall right now, i'd just google it\"replymdaniel 18 hours ago | root | parent | next [\u2013]What an insulting interview question, I hope it was just in jest or at the end looking to pad the timeHowever, it did make me realize hidden therein is an actual interesting interview question, similar to the \"describe what happens when you type an address into the browser's URL bar and hit enter\": describe what happens after you type `:s/foo/bar` and hit enter. Followup version: what about `:%s/foo/bar`? The kind of thing that can be interesting to watch them reason through even if they don't know the answer, or even know what those syntaxes do.replycountspongebob 18 hours ago | root | parent | prev | next [\u2013]Alt proposed answer \"I'd install emacs\".replybsaul 23 hours ago | parent | prev | next [\u2013]Side question : why are people working on open source project communicating through discord a lot noawadays ?are discord conversations persisted and indexed on search engines ?replyaxismundi 23 hours ago | root | parent | next [\u2013]I find Discord quite versatile and a bit overwhelming at the same time. As to SEO, see https://news.ycombinator.com/item?id=36383773AFAIK most of the gamers choose it for voice chat (Anyone remember TeamSpeak?)replyHendrikto 19 hours ago | root | parent | next [\u2013]In Europe, TeamSpeak is still very popular.replyJLCarveth 19 hours ago | root | parent | next [\u2013]I used to play EVE Online a fair bit, and always thought it interesting how some of the groups used Discord but only for text communications. Voice was still done over Teamspeak or Mumble.replywtf_is_up 17 hours ago | root | parent | next [\u2013]When I played EVE, Mumble was the de facto voice comms since it supported 100s of pilots which happened many times during joint ops and xmpp for text chat and pings.replycinntaile 23 hours ago | root | parent | prev | next [\u2013]Because it's easy, free and it just works.Very few people actually care about indexing the conversations.replypmontra 22 hours ago | root | parent | next [\u2013]So all knowledge is lost and questions have to be asked and answered again and again?replymichaelt 22 hours ago | root | parent | next [\u2013]That didn't stop IRC being popular in the 1990s.There has long been a place in the ecosystem for ephemeral chat. Often alongside non-ephemeral things like written documentation.replysp332 19 hours ago | root | parent | next [\u2013]People didn't put documentation in IRC channels because they didn't want to answer the same questions over and over. Info went into a wiki, and you would get flamed for asking a question on IRC that was answered on the wiki. Discord is not a good place to stash documentation.replyyrro 18 hours ago | root | parent | next [\u2013]It's ok you get scolded for asking an FAQ in many Discord \"servers\" as well.replydarkwater 21 hours ago | root | parent | prev | next [\u2013]> That didn't stop IRC being popular in the 1990s.IRC chats, especially in opensource projects channels, could and would be archived, published over the web and indexed by search engines.replyMatthiasPortzel 17 hours ago | root | parent | next [\u2013]In my experience, I don\u2019t think I\u2019ve ever seen an IRC log in a search result.#haskell on Libra is publicly logged, but I couldn\u2019t get Google to return a quoted phrase from a message a few weeks ago.Many people on IRC don\u2019t enjoy being in logged channels. I\u2019ve also heard that there are GDPR implications to publicly logging people\u2019s messages without their consent.Discussion of the difficulty and downsides of IRC logging, from a coulple years ago:=> https://news.ycombinator.com/item?id=22892015=> https://web.archive.org/web/20200417001532/https://echelog.c...The HN blowback to developers choosing to use Discord is just wildly out of proportion.replydarkwater 3 hours ago | root | parent | next [\u2013]No, it's not. If you work on an opensource / open development project, it totally makes sense to avoid walled gardens for the community chat/forum (a few years ago it was public Slack instance, nowadays it's Discord servers).replypaxys 21 hours ago | root | parent | prev | next [\u2013]So just like Discord then..replypbmonster 19 hours ago | root | parent | next [\u2013]I wasn't aware that was being done.Can you show me how to access the archives of the ask-for-help channel on the openllm Discord server? Right now they're discussing \"loading models on CPU vs GPU\". No matter how explicit I got, google did not find the discussion.replypaxys 18 hours ago | root | parent | next [\u2013]It's up to the server owners/admins to configure archiving, same as IRC.replyscubbo 19 hours ago | root | parent | prev | next [\u2013]Noreplynixass 21 hours ago | root | parent | prev | next [\u2013]Also monks being the only ones who can read and write didn't stop religion to be popular in middle ages./sC'monreplyfalcor84 21 hours ago | root | parent | prev | next [\u2013]Isn't there something really nice about it though? It seems to me that most every community gradually evolves into one where every new message from a new-ish member is answered by something like \"Duplicate, please search first!\". And this in turn makes those newcomers either go away, become passive lurkers, or become part of the \"hive-mind\" (as only likeminded questions get answered).On the other hand, if people have to actually converse to get an answer to their questions (like back in the real world), newcomers can more rapidly become part of the community, and help make it more diverse.replyZak 21 hours ago | root | parent | next [\u2013]I just recently saw a post where someone said something similar about Reddit versus traditional forums.There's a balance between engaging with new members and not turning it into a time sink for older members. This is probably a good use case for LLMs.replyfalcor84 19 hours ago | root | parent | next [\u2013]LLMs could indeed address the first part, but not the second, of bringing the newcomers in via actual conversation with the older members. The only good solution I encountered to this is of having some (preferably not too experienced) member(s) actively take upon themselves the role of welcoming newcomers and answering their questions, whether that's in an official or unofficial capacity.This to me is the real way through this \"Eternal September\", where in every \"cohort\" of newcomers, one or more choose to stay close to the doorway to welcome and guide the next cohort.replywindyfly 18 hours ago | root | parent | next [\u2013]Newcomers are also different. Some are actually experienced vs some are real newbies.I\u2019m wondering how could learn from games, making the content also adaptive to user levels/experiences.It\u2019s prob also the key agenda in education.replyscubbo 19 hours ago | root | parent | prev | next [\u2013]The best of both worlds - a friendly community that welcomes newbies, with a searchable archive - is possible. Limiting to only chat-based support means that support is bottle-necked by the folks who are available and engaged at the time of the question, and that knowledge will \"drop out\" of the community as people forget it.replyfalcor84 19 hours ago | root | parent | next [\u2013]Apologies for my skepticism, but is it just \"possible\", or do you actually have an example of a long-lived community that remained fully welcoming to newbies while utilizing a searchable archive?In any case, I'm not arguing that it's impossible, but rather that the more comprehensive the archive, the less welcoming the community would tend to be, all other things being equal. To take it to the extreme, I'll posit the following law: \"A well-curated archive is the grave of a community\"replythrowaway675309 11 hours ago | root | parent | next [\u2013]Hard disagree. If anything you'll find that the most knowledgeable members get burned out answering the same questions over and over again, so they begin to simplify their answers until they just become copy pasta.You can still have channels open to welcoming new people while at the same time having a large archive of answered questions so that over time a reservoir gets built.Saying that the same questions getting asked over and over again by new people is somehow a more welcoming community, is like saying that there's any meaningful interaction happening when two people say \"What's up?\" followed by the response \"not much\". It's a handshake protocol equivalent without actual depth.replyfalcor84 1 hour ago | root | parent | next [\u2013]  I see friends shaking hands  Saying, \"How do you do?\"  They're really saying  I love you.replyscubbo 6 hours ago | root | parent | prev | next [\u2013]A very reasonable question, and I'll admit that I'm not deeply-entrenched in enough technical communities to give you an actual example. But yeah, intuitively I do agree with the sibling commenter - a well-curated archive is a tool of technology which allows skilled respondents to preserve their time and energy for new and interesting questions. A pointer to search is not necessarily dismissive - there is a world of difference between the following _technically_ equivalent responses:* FFS, read the fuckin' archive noob and stop wasting our time* Hey there, thanks for asking! This is actually a pretty common question, and we have guides written up for just this case. Try entering some of your search terms here [link], and come back with a follow-up question if that doesn't help you!But yes, in fairness, I'll certainly agree that a community which _chooses_ to respond as the former will stagnate and die.replynumpad0 19 hours ago | root | parent | prev | next [\u2013]No, questions don't have to be asked and answered again and again, because all the knowledge is lost, full stop. No one would know anything.replybredren 18 hours ago | root | parent | prev | next [\u2013]Not lost enough to use as a transient space for sharing secret intelligence reports.replymirekrusin 21 hours ago | root | parent | prev | next [\u2013]Maybe it doesn't matter because this type of knowledge is relevant for current week only?replyekianjo 20 hours ago | root | parent | prev | next [\u2013]indexing conversations is secondary for gaming but primary for FOSS projects and Discord sucks at that. its like wiping your ass with a fork.replyOkGoDoIt 17 hours ago | root | parent | prev | next [\u2013]My understanding from asking several people, since I hate discord and want to know why people insist on using it, is that it\u2019s a free alternative to Slack. Simple as that.But it\u2019s crazy, people are aggressive about Discord for some reason. I maintain an OpenAI SDK package for .Net, and I had some random person decide they wanted it to be a Discord community, so they created a Discord claiming it was the official community discord for my library, and submitted a PR updating my readme to say that it\u2019s my project\u2019s official Discord. They also replied to several issues and pull requests telling people to discuss it on that discord. If Discord isn\u2019t paying this person in some guerrilla marketing tactic, they should be...replybelugacat 23 hours ago | root | parent | prev | next [\u2013]short answer: because it's one of the options with least friction to get runninga lot of people who are into tech stuff already have a discord account making joining the community a one click process, the instant nature of it seems to appeal to younger users more than async forums, it's a fairly mature platform so it has a bunch of moderation/customization/integration features you might want, etc.> are discord conversations persisted and indexed on search engines ?nope (and that is a drawback many point out)replydecide1000 23 hours ago | root | parent | next [\u2013]Isn't it a generation thing? If I had the choice everyone would be on IRC still.replyZak 20 hours ago | root | parent | next [\u2013]I've used IRC for a long time and still do, but I do think Discord has a nicer UX for most use cases. In particular, building communities around clusters of channels (\"servers\") and support for rich media (yes, some old people might call that a downside) increase the appeal for most people. It's also a lot more work to have a persistent connection on IRC (bouncers).My main problem with Discord is that it's someone else's centralized, for-profit company and has no apparent barriers to enshittification[0]. As Reddit recently demonstrated, it's probably a mistake to build communities on top of something like that.Matrix is a good candidate for a modern successor to IRC. It's not quite as slick a UX as Discord, but it addresses the main advantages Discord has over IRC.[0] https://pluralistic.net/2023/01/21/potemkin-ai/#hey-guysreplydunefox 19 hours ago | root | parent | next [\u2013]Matrix would be my choice as well but good luck getting people to use the uglier alternative. Discord is great.replysimonw 22 hours ago | root | parent | prev | next [\u2013]Fue problem with IRC is that it's crucial to have really robust read state synchronization across desktop and mobile these days.Slack was the first to really get that right, and Discord effectively emulated them and made it available for free.IRC users could get there with bouncers, but those were always a lot harder to get going with.replychrisan 22 hours ago | root | parent | prev | next [\u2013]Practically all of my friends grew up with IRC, we are in our late 30s, 40s, early 50s.We might reminisce about irc but we all prefer discord.Even the searchability of indexed irc has been surpassed by other knowledge sites. It would have to be something extremely niche these days where the only source of info is in an irc chat logreplyhyperhopper 21 hours ago | root | parent | prev | next [\u2013]IRC doesn't even have history, one of the most basic requirements for a modern rudimentary chat app. It's ridiculous to suggest using it in 2023 when it doesn't have features a freshman homework assignment chat app has.replyufo 23 hours ago | root | parent | prev | next [\u2013]1.People like talking to each other on discord. 2. No. :/replyignoramous 1 day ago | parent | prev | next [\u2013]Discord: https://discord.com/invite/qc3RekjtuYreplymanojlds 1 day ago | parent | prev | next [\u2013]How can we stay tuned if we can't do tuning? :Preplychaoyu 1 day ago | root | parent | next [\u2013]Fine-tuning is coming up in the next release!You can actually try it out on the main branch :Preplyphotochemsyn 19 hours ago | parent | prev | next [\u2013]What's the rationale for telemetry tracking?https://github.com/bentoml/OpenLLM/blob/main/src/openllm/uti...replynacs 18 hours ago | root | parent | next [\u2013]They have a section about it in the README:https://github.com/bentoml/OpenLLM#-telemetryreplymoffkalast 1 day ago | parent | prev | next [\u2013]Very cool, btw it's not mentioned in the readme so I assume it's only for running full precision models or do quantized GGML/GPTQ/etc. also work with it?replyaarnphm 1 day ago | root | parent | next [\u2013]Hi there, 8bit and 4bit is currently supported on main. GPTQ is working in progress, as well as GGMLreplynacs 18 hours ago | root | parent | next [\u2013]GPTQ support would be amazing (AutoGPTQ is an easy way to integrate GPTQ support - it's basically just importing autogptq and switching out 1 line in the model loading code).replymjburgess 1 day ago | prev | next [\u2013]Stray thought: It would be better to specify NNs in terms of their training-size to weight-size in bytes. Rather than \"No. Parameters\", or at least, this ratio with the number of parameters.So, eg., I'd imagine ChatGPT would be, say: 100s PB in 0.5TB.The number of parameters is a nearly meaningless metric, consider, eg., that if all the parameters covary then there's one \"functional\" parameter.The compression ratio tells you the real reason why a NN performs. Ie., you can have 100s bns of parameters, but without that 100s PB -> 0.5TB, which you can't afford, it's all rather pointless.replybjornsing 1 day ago | parent | next [\u2013]I\u2019m not sure ML researchers would agree that number of (compressed) bytes are more meaningful than number of parameters. Parameters have mathematical meaning \u2013 bytes doesn\u2019t.replymjburgess 1 day ago | root | parent | next [\u2013]My point is that they don't have a mathematical meaning. They have a training-time impact, but that's more relevant when creating than using.You'd need to know how many parameters were independently covarying for any given class of predictions. It certainly isnt all of them.You could cite the \"average dropout percent to random-level accuracy on a given class of problems\" (my guess is that this would show 5-20% of parameters could be dropped).My point, I suppose, is that users of NNs arent interested in the architecture characteristics which affect training -- they're interested in how capable any given model will be.For this we really want to know how large the training data was, and how compressed it has been. If it's 1PB -> 1MB then we can easily say that's much less useful (but much faster to use) than 1PB -> 0.5TB.Likewise we can say, if it's a video generator, that 1PB is far too small to be generally useful -- so at best it'll be domain-speicifc.replytyre 17 hours ago | root | parent | next [\u2013]> My point, I suppose, is that users of NNs arent interested in the architecture characteristics which affect training -- they're interested in how capable any given model will be.Yes.A more helpful bit of information could be what the model was pre-trained on. Assuming they\u2019re trying to refine it for a more specific task.Size is helpful for \u201cwhat can I run on my machine\u201d (or how much would it cost to run on a server.) Not all models are created equal, given a byte size, for a given task.replypiker 22 hours ago | root | parent | prev | next [\u2013]Bytes does imply a level of precision, however, which affects the mathematical meaning. Perhaps there\u2019s a metric that captures both.replyxpe 20 hours ago | root | parent | next [\u2013]Allocating more bits for each parameter increases precision, by definition. But that doesn\u2019t come for free.* So it is useful to optimize network performance for a given number of total parameter bytes.I haven\u2019t done a recent literature review, but my hand-wavy guessplanation is that a NN (as a whole) can adapt to relatively low precision parameters. Up to a point.* In general. Given actual hardware designs, there are places where you have slack in the system. So adding some extra parameters, e.g. to fully utilize a GPU\u2019s core\u2019s threads (e.g. 32), might actually cost you nothing.replyYetAnotherNick 1 day ago | parent | prev | next [\u2013]> 100s PBUnrelated and I know it is just a representative number but I have seen the training data to be assumed something in this range few times. Entire training set of ChatGPT is almost surely less than a TB or two with compression which is 5 orders of magnitude lower. I believe that such efficient representation of text is one of the biggest reason why text models are working so well but image understanding models are not.replymjburgess 1 day ago | root | parent | next [\u2013]Perhaps the process was an initial c. 1PB then sampled down to the TBs.Text is extremely lightweight, so I suppose everything ever written is at most 1-10PB.This is one of the illusions of text-generative NNs: a 0.5TB weight set is basically enough to store every book. Making claims to \"out-sample generalisation\" extremely suspicious, and indeed, fairly obviously false.eg., Ask ChatGPT to write tic-tak-toe in javascript and you get a working game; as it to write duck-hunt and you dont.replymoffkalast 1 day ago | parent | prev | next [\u2013]Well it's not a completely meaningless metric as it immediately tells you roughly how much memory you need to load it, which is kind of important?replymjburgess 1 day ago | root | parent | next [\u2013]If you look at my suggestion, it's to state exactly that memory -- rather than to estimate based on bits/parameter.replymoffkalast 1 day ago | root | parent | next [\u2013]Well then do explain a bit further, I still don't fully grasp what \"100s PT in 0.5T\" means exactly. 100 petatokens in half a trillion? Half a terrabyte? 100 seconds?Plus afaik base model training tokens don't have the same effect as fine tuning tokens, so there would need to be a way to specify each of those separately.replyboomskats 1 day ago | root | parent | next [\u2013]FWIW I easily interpreted these as '100s of petabytes' and '0.5 terabytes' without having to give it too much thought. The original comment explicitly specified 'bytes' as the unit being suggested.replymjburgess 1 day ago | root | parent | prev | next [\u2013]I edited to be TB,PB --- I was thinking of these as prefixes on bytesreplyDrNosferatu 21 hours ago | prev | next [\u2013]The project seems great!However, newcomers (like me) are pretty blind about minimum system requirements.Could you please add them to the models list?For example: what minimum hardware do I need to run Falcon-40b?PS: If you only have a few setups \"known to work\" (or just one), listing that would be helpful too.replyleetharris 20 hours ago | parent | next [\u2013]For falcon 40b you probably need an A100 40gb or so.Every model is drastically different.If you want to run something on consumer hardware, your best bet is using anything ported to the ggml framework, especially if you're on Apple silicon.replyAerroon 17 hours ago | root | parent | next [\u2013]To add: usually when you go to download a ggml model you want a quantized version. People like TheBloke will usually have some RAM requirements for running it, eg: https://huggingface.co/TheBloke/vicuna-13b-v1.3-GGMLThe number after q determines how many bits the weights are. Eg q4 means that is 4-bit.If you use something like KoboldCPP you can only put some of the layers onto the GPU and be able to run larger models that way.Eg the above linked Vicuna model requires about 10GB of memory at q4, but I have less VRAM than that. I can still run it though.replyPcChip 15 hours ago | root | parent | next [\u2013]Let's say I wanted to use one of their quantized models with this OpenLLM project. How would I do that?replyAerroon 14 hours ago | root | parent | next [\u2013]Sorry, I don't know. I suspect that it's not possible (yet?). OpenLLM lists a bunch of models in the Github Readme. I think the best way would be to use those for now.replyfrodowtf 19 hours ago | root | parent | prev | next [\u2013]Do you know any \"standard\" way or measures to determine the approximate hardware requirements of a model?replyMacsHeadroom 17 hours ago | root | parent | next [\u2013]Model size in GB = VRAM required for uncompressed inference (16bit aka \"half precision) plus ~1-4GB for context. For 8bit you need half that. For GPTQ/4bit you need 1/4 that.For example, assuming GPTQ 4bit a 100GB 16bit model needs 26-30GB of VRAM. The smallest video cards which meet this requirement will be 32GB or 40GB cards. (two 24GB cards in parallel work as well, e.g. 2x3090)replytyre 17 hours ago | root | parent | prev | next [\u2013]You want the entire model to fit in memory. So if you\u2019re looking at a download size of, say, 100GB then don\u2019t run on less than that. Your machine will swap to/from disk constantly, which will be slow and wear out an SSD.If you want to train a model, that\u2019s a different story.replyluckystarr 20 hours ago | parent | prev | next [\u2013]From the HuggingFace page:> You will need at least 85-100GB of memory to swiftly run inference with Falcon-40B.So it may be possible with less (swap around method), though not as efficiently and also slower.replyphiloko 20 hours ago | parent | prev | next [\u2013]For Falcon 40b, the 8-bit version would probably need about 48GB of VRAM while the 4-bit would need something closer to 28GB.replyaarnphm 17 hours ago | root | parent | next [\u2013]Currently on main, 8bit and 4bit quant is supportedOne can simply do```openllm start falcon --model-id tiiuae/falcon-40b-instruct --quantize int4```Beware that there is no free lunch, meaning the quality of inference will degrade by alot when using int 4 quantizationreplyrustdeveloper 23 hours ago | prev | next [\u2013]Do we know how LLMs available in OpenLLM and other open source LLMs compare to different versions of GPT models? I know there\u2019s a leaderboard on huggingface: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderb... but it doesn\u2019t contain GPT models.replyrahimnathwani 23 hours ago | parent | next [\u2013]https://lmsys.org/blog/2023-05-25-leaderboard/replytomschwiha 22 hours ago | parent | prev | next [\u2013]The GPT4/ChatGPT ones are in the source visible: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderb...replysorenjan 21 hours ago | prev | next [\u2013]What kind of hardware do I need to run something small scale (1 user concurrently) and get reasonable result? Are we talking about Raspberry Pi, Core i5, Geforce 4090?replyrootusrootus 17 hours ago | parent | next [\u2013]Anecdote: I can tell you that Vicuna-13B, which is a pretty decent model, runs about 4 to 5 tokens/second on an Apple M1 with 16GB. Takes about 10GB of memory when loaded. Friend of mine with a RTX 2070 Super gets comparable results.I'm upgrading my M1 laptop on Thursday to a newer model, M2 MAX with 96GB of memory. I'm totally going to try Falcon-40B on it, though I do not expect it to run that well. But I do expect it (the M2, I mean) will be snappier on the smaller models than my original M1 is.replyben_w 21 hours ago | parent | prev | next [\u2013]I think that strongly depends on what you count as \"reasonable\": smaller models take less memory, so there's a trade-off between quality and speed depending on if you can fit it all in graphics VRAM, or system RAM, or virtual memory\u2026Just keep in mind the speed differences between the types of memory. If you've got a 170bn 4-bit parameter model, and you're on virtual memory on a 400 Mbps port, a naive calculation says it will take at best 28 minutes per token unless your architecture lets you skip loading parts of the model. Might take longer if the network has an internal feedback loop in the structure.replyrmbyrro 1 hour ago | root | parent | next [\u2013]Would you mind sharing the calculation memory that reaches at 28 minutes (!) per token?replyben_w 51 minutes ago | root | parent | next [\u2013]If your model is 170bn 4-bit parameters, you have 85 Gbytes that has to be loaded into the CPU or GPU; if those parameters are on the other side of a 400 Mbps port, that takes 85 Gbyte / 400 Mbps = 1700 seconds = 28 minutes 20 seconds.If you don't have sufficient real RAM or VRAM, the entire model has to be re-loaded for each step of the process.Assuming no looping (looping makes it longer) and no compartmentalisation in the network structure (if you can make it so an entire fragment might be not-activated, you have the possibility of skipping loading that section; I've not heard of any architecture that does this, it would be analogous to dark silicon or to humans not using every brain cell at the same time (outside of seizures)).replyHavoc 1 day ago | prev | next [\u2013]This sounds promising. Smaller but custom trained/tuned models would be ideal - works for the task without the overheadreplypzo 23 hours ago | parent | next [\u2013]Agreed, mostly in real life we have specialized experts. You go to doctor to ask about health related stuff, you ask your colleague who is expert in ML only about ML and probably not much about mobile development.Instead if having ML that is expert at coding in all languages probably would be better to allow to switch context.Native mobile dev LLM? - e.g. train only on swift, objc, kotlin, java, c, c++ codePython dev LLM? - train only on python, c, c++, rust codeWould this way final model be smaller, faster and maybe event better?replylogicchains 1 day ago | parent | prev | next [\u2013]In my experience it doesn't work like that. It's like if you take an idiot and spend a bunch of time training them, they'll still perform much worse than a moderately intelligent person with much less training. And smaller models can be pretty idiotic.replysysstemlord 1 day ago | root | parent | next [\u2013]Then why do Chess AI perform much better than LLMs trying to play chess.replyRugnirViking 23 hours ago | root | parent | next [\u2013]Provided that the problem is suited to the strengths of an LLM at all. An example might be a small ai custom trained on documentation for libraries. You ask it a question like \"how do I make the background move with parallax effect when you move the cursor\". It's a little ambiguous, high-level concept, and probably not a single function.Small ai: likely makes up a function or suggests a single function which isn't sufficient. Refuses to budge from its answer or apologies and gets confusedLarge LLM: able to actually understand the question, combine several functions. If it doesn't work you can tell it why and it fixes itreplyseanhunter 23 hours ago | root | parent | prev | next [\u2013]Because there\u2019s a world of difference between a reinforcement learning trained special purpose model and asking a general purpose large language model to have a go at something.replyPeterStuer 17 hours ago | root | parent | prev | next [\u2013]Because they have an explicit model of chess and specific heuristics for learning chess.An LLM could have picked up some chess patterns through osmosis, but it can not reason explicitly in the domain.replyGasp0de 23 hours ago | root | parent | prev | next [\u2013]Because they do completely different things? They literally have nothing to do with each other. Why do planes fly better than ships if ChatGPT can't do math?replysolumunus 23 hours ago | root | parent | prev | next [\u2013]Why would a language model be good at playing chess?replytreprinum 23 hours ago | parent | prev | next [\u2013]Unfortunately, for transformer-based LLMs the magic starts only when they are trained by more that 10^22 TFlops (preferably 10^24) so smaller models might not cut it even for fine-tuned tasks.replyvisarga 22 hours ago | root | parent | next [\u2013]At medium size (13B) Microsoft Orca demonstrated you can trade off size with larger fine-tuning dataset.replytangjurine 17 hours ago | root | parent | prev | next [\u2013]Any references on this?replytreprinum 14 hours ago | root | parent | next [\u2013]https://www.youtube.com/watch?v=tVtOevLrt5Ureplyjohnlyd 23 hours ago | parent | prev | next [\u2013]Agreed.replyvisarga 1 day ago | prev | next [\u2013]Fine-tuning is the most important part, but it is under intense research today, things change fast. I hope they can streamline this process because these smaller models can only compete with big models when they are fine-tuned.replyuniqueuid 1 day ago | parent | next [\u2013]Yes but fine tuning requires a lot more gpu memory and is thus much more expensive, complicated and out of reach of most people. To fine tune a >10B model you still need multiple A100 / H100. Let\u2019s hope that changes with quantized fine tuning, forward pass only etc.replychaoyu 1 day ago | root | parent | next [\u2013]The OpenLLM team is actively exploring those techniques for streamlining the fine-tuning process and making it accessible!replyvisarga 22 hours ago | root | parent | prev | next [\u2013]You can fine-tune medium models 3..60B on a single GPU with QLoRAreplyquickthrower2 1 day ago | root | parent | prev | next [\u2013]What is the $ cost of a fine tune though? $500?replyvczf 23 hours ago | root | parent | prev | next [\u2013]Can you fine tune on an M2 with adequate memory?replycomfypotato 1 day ago | parent | prev | next [\u2013]What exactly do you mean here that the smaller models can compete with the the larger once they are fine-tuned? What about once the larger models are fine-tuned? Are they then out of reach of the fine-tuned smaller models?replyrmbyrro 1 day ago | root | parent | next [\u2013]They're probably referring to fine-tuning on private/proprietary data that is specific to a use case. Say a history of conversation transcripts in a call center.Larger models, like OpenAI's GPT, don't have access to this by default.replyquickthrower2 1 day ago | root | parent | next [\u2013]OpenAI\u2019s API has fine tuning options for older GPT models: davinci, curie, babbage, and adareplychaoyu 1 day ago | root | parent | prev | next [\u2013]Smaller models are likely more efficient to run inference and doesn't necessarily need the latest GPU. Larger language model trend to have better performance over more different type of tasks. But for a specific enterprise use case, either distilling a large model or use large model to help with training a smaller model can be quite helpful in getting things to production - where you may need cost-efficiency and lower latency.replymoneywoes 20 hours ago | parent | prev | next [\u2013]Is there a good checklist or framework for fine tuning vs using a vector db to increase the context sizereplyd4rkp4ttern 22 hours ago | prev | next [\u2013]Suppose I\u2019ve written code that calls the OpenAI API. Is there some library that helps me easily switch to a local/other LLM. I.e a library that (ideally) provides the same OpenAI interface for several models, or if not then at least the same interface.replyabrichr 21 hours ago | parent | next [\u2013]https://github.com/nomic-ai/gpt4allreplyfortydegrees 22 hours ago | parent | prev | next [\u2013]There's the OpenedAI-API extension for text-generation-webui: https://github.com/oobabooga/text-generation-webui/tree/main...replychaoyu 17 hours ago | parent | prev | next [\u2013]OpenLLM plan to provide an OpenAI-compatible API, which allows you to even use OpenAI's python client to talk to OpenLLM, user just need to change to Base URL to point to your OpenLLM server. This feature is working-in-progress.replyd4rkp4ttern 21 hours ago | parent | prev | next [\u2013]Found a couple others that do something like this. Turns out I had bookmarked them a while ago.https://github.com/r2d4/openlmhttps://github.com/hyperonym/basaranreplybart_spoon 21 hours ago | parent | prev | next [\u2013]Langchain offers abstraction across many LLMs, including OpenAI\u2019s.replymonkeydust 22 hours ago | parent | prev | next [\u2013]Langchain might be what you need.replyphsource 1 day ago | prev | next [\u2013]Cool stuff! How does this compare with Fastchat, which seems like another open source project that helps run LLM models?At a glance, it seems like it's going for lots of similar goals (run LLMs with interoperable APIs):https://github.com/lm-sys/FastChatreplychaoyu 1 day ago | parent | next [\u2013]OpenLLM in comparison focuses more on building LLM apps for production. For example, the integration with LangChain + BentoML makes it easy to run multiple LLMs in parallel across multiple GPUs/Nodes, or chain LLMs with other type of AI/ML models, and deploy the entire pipeline on Kubernete (via Yatai or BentoCloud).Disclaimer: I helped build BentoML and OpenLLM.replyAjedi32 20 hours ago | prev | next [\u2013]I like the idea of having a standard API for interacting with LLMs over the network. Many models need to run on beefy hardware and would benefit from offloading to a remote (possibly self-hosted) server, and I think makes logical sense to separate the code for running LLMs from the UI for accessing them.replygolergka 18 hours ago | parent | next [\u2013]It would be great to have this, but the space is rapidly moving and haven't converged on a set of uniformly accepted practices yet. For example, I'm not aware of a single open source LLM that has something similar to OpenAI's function calls.replySimFG 1 day ago | prev | next [\u2013]Looks great! I'm planning to integrate it into my new project(to-chatgpt: https://github.com/SimFG/to-chatgpt), which will provide users of the ChatGPT applications with a wider range of LLM service options.replychaoyu 1 day ago | parent | next [\u2013]Looking forward to it!OpenLLM is adding a OpenAI-compatible API layer, which will make it even easier to migrate LLM apps built around OpenAI's API spec. Feel free to join our Discord community and discuss more!replyhuggingmouth 19 hours ago | prev | next [\u2013]Why doesn't it support llama?replynumpad0 19 hours ago | prev | next [\u2013]Though only related to topic wrt LLMs: It seems LLMs occasionally mix up CJK vocabularies and also generate invalid UTF-8 sequences, due to CJK texts having overlapping code points and inputs being processed by tokenizer. Are there developments in that directions? Aren't CJK ideograms essentially tokens?replyspencerchubb 17 hours ago | prev | next [\u2013]What is the license like for this? Correct me if I'm wrong, but I think the official Llama has a license that allows research use. Would this have a similar restriction if it had the same model architechture but different parameters?replychaoyu 17 hours ago | parent | next [\u2013]OpenLLM itself is under Apache 2 license, which does NOT restrict commercial use. However, OpenLLM as a framework can be extended to support other LLMs which may come with additional restrictions.replyxvilka 23 hours ago | prev | next [\u2013]Does it work only with text? Or image/video processing too?replychaoyu 17 hours ago | parent | next [\u2013]Check out BentoML, which is the underlying serving framework used by OpenLLM, and it supports other type of models and modality such as images and videos.replytmalsburg2 22 hours ago | parent | prev | next [\u2013]From the description of the repo: \"An open platform for operating large language models (LLMs) in production.\"replyhsuduebc2 21 hours ago | prev | next [\u2013]Hello can someone provide usecase for me as an user of this? Is it better because it is cheaper than commercially available apis?replytomschwiha 20 hours ago | parent | next [\u2013]Imo its not cheaper (or has better quality) and is only worth iti) if you want to toy around with itii) don't want to depend on the api to be available (or don't want to be censored or share sensitive information with a third party)iii) finetune your own model that you need to deploy by yourselfreplyasylteltine 20 hours ago | prev | next [\u2013]What does it mean to \u201cserve a model\u201d? Where exactly does the request go and how does it interface with the model?replythecal 11 hours ago | parent | next [\u2013]Think of the model as a gigantic compiled binary where you send in strings in a certain format and get back a response. This is a web API wrapper for that so you only need an HTTP client instead of having to run something like llama.cpp yourself.replymoneywoes 20 hours ago | prev | next [\u2013]Is this the best? Seeing a lot of these projects somewhat confused on what to go withreplycultavix 22 hours ago | prev | next [\u2013]Anyone know what terminal/theme they are using ? haha...replyzoom_enh4nce 17 hours ago | parent | next [\u2013]The theme looks like rose-pine-dawn.replysnowcrash123 22 hours ago | prev | next [\u2013]This looks like a very cool project and much neededreplysputr 20 hours ago | prev | next [\u2013]Question: for someone that wants to play around with self-hosted text generation but has a crap laptop \u2013 are there any hosting providers (like a VPS) where I can run open source models?replytomschwiha 20 hours ago | parent | next [\u2013]You can rent pretty easy a server with a GPU using runpod, vast.ai or datacrunch. Or maybe even use something Like Google Colab.replysputr 17 hours ago | root | parent | next [\u2013]Thanks!reply0i0 19 hours ago | prev [\u2013]very cool.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- OpenLLM is an open platform for operating large language models (LLMs) in production.\n- It allows users to run LLMs on their own hardware instead of relying on commercially available APIs.\n- It provides a framework for serving LLMs and supports various types of models and modalities, including text, images, and videos."
  },
  {
    "id": 36391053,
    "timestamp": 1687182504,
    "title": "Titanic tourist submersible goes missing with search under way",
    "url": "https://www.bbc.com/news/world-us-canada-65953872",
    "hn_url": "http://news.ycombinator.com/item?id=36391053",
    "content": "Titanic tourist submersible goes missing with search under wayPublished11 hours agoShareMedia caption,Watch: Titanic tourist sub search 'a challenge' - Rear Admiral John Mauger of the US Coast GuardBy Gareth Evans & Laura GozziBBC NewsA massive search and rescue operation is under way in the mid Atlantic after a tourist submarine went missing during a dive to Titanic's wreck on Sunday.Contact with the small sub was lost about an hour and 45 minutes into its dive, the US Coast Guard said.Tour firm OceanGate said all options were being explored to rescue the five people onboard.Tickets cost $250,000 (\u00a3195,000) for an eight-day trip including dives to the wreck at a depth of 3,800m (12,500ft).Government agencies, the US and Canadian navies and commercial deep-sea firms are helping the rescue operation, officials said.Titanic's wreck lies some 435 miles (700km) south of St John's, Newfoundland, though the rescue mission is being run from Boston, Massachusetts.The missing craft is believed to be OceanGate's Titan submersible, a truck-sized sub that holds five people and usually dives with a four-day emergency supply of oxygen.On Monday afternoon, Rear Adm John Mauger of the US Coast Guard told a news conference: \"We anticipate there is somewhere between 70 and the full 96 hours available at this point.\"He also said that two aircraft, a submarine and sonar buoys were involved in the search for the vessel but noted the area in which the search is taking place was \"remote\", making operations difficult.Rear Adm Mauger said the rescue teams were \"taking this personally\" and were doing everything they could to bring those on board \"home safe\".Hamish Harding, a 58-year-old British billionaire businessman and explorer, is among those on the missing submarine, his family said.On social media at the weekend, Mr Harding said he was \"proud to finally announce\" that he would be aboard the mission to the wreck of the Titanic - but added that because of the \"worst winter in Newfoundland in 40 years, this mission is likely to be the first and only manned mission to the Titanic in 2023\".He later wrote: \"A weather window has just opened up and we are going to attempt a dive tomorrow.\"OceanGate said its \"entire focus [was] on the crewmembers in the submersible and their families\".\"We are deeply thankful for the extensive assistance we have received from several government agencies and deep sea companies in our efforts to re-establish contact with the submersible,\" it added.The company bills the eight-day trip on its carbon-fibre submersible as a \"chance to step outside of everyday life and discover something truly extraordinary\".According to its website, one expedition is ongoing and two more have been planned for June 2024.The submersible usually carries a pilot, three paying guests, and what the company calls a \"content expert\".The trip sets sail from St John's in Newfoundland. Each full dive to the wreck, including the descent and ascent, reportedly takes around eight hours.The OceanGate website lists three submersibles it owns, and only the Titan is capable of diving deep enough to reach the Titanic wreckage.The vessel weighs 23,000 lbs (10,432 kg) and, according to the website, can reach depths of up to 13,100 ft and has 96 hours of life support available for a crew of five.A vessel called the Polar Prince, which is used to transport submersibles to the wreckage site, was involved in the expedition, its owner told the BBC.David Pogue, a CBS reporter who travelled in the Titan submersible last year, told the BBC about the issues that both the submersible crew and the land crew were likely to be experiencing, saying that there was currently \"no way\" to communicate with the vessel as neither GPS nor radio \"work under water\".\"When the support ship is directly over the sub, they can send short text messages back and forth. Clearly those are no longer getting a response,\" Mr Pogue said.He added that because the passengers were sealed inside the vessel by bolts applied from the outside, \"There's no way to escape, even if you rise to the surface by yourself. You cannot get out of the sub without a crew on the outside letting you out.\"What we know so far about the missing Titan subBritish adventurer among missing on Titanic subThe Titanic, which was the largest ship of its time, hit an iceberg on its maiden voyage from Southampton to New York in 1912. Of the 2,200 passengers and crew onboard, more than 1,500 died.Its wreckage has been extensively explored since it was discovered in 1985.The wreck lies in two parts, with the bow and the stern separated by about 2,600ft. A huge debris field surrounds the broken vessel.Last month, the first full-sized digital scan of the wreck was created using deep-sea mapping. The scan shows both the scale of the ship, as well as some minute details, such as the serial number on one of the propellers.Media caption,Watch: A recently released video shows a 3D view of the Titanic shipwreck, 3,800m (12,500ft) down at the bottom of the AtlanticDo you have information about this story? You can get in touch by emailing haveyoursay@bbc.co.uk.Please include a contact number if you are willing to speak to a BBC journalist. You can also get in touch in the following ways:WhatsApp: +44 7756 165803Tweet: @BBC_HaveYourSayUpload pictures or videoPlease read our terms & conditions and privacy policyIf you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or comment or you can email us at HaveYourSay@bbc.co.uk. Please include your name, age and location with any submission.Related TopicsRMS TitanicSinking of the TitanicNewfoundland and LabradorUnited StatesCanadaMore on this storyWhat it's like on board the Titanic tourist subPublished1 hour agoWhat we know so farPublished2 hours agoFather and son and UK billionaire among missingPublished16 minutes agoScans of Titanic reveal wreck as never seen beforePublished17 May",
    "summary": "- A tourist submarine, the Titan submersible, has gone missing during a dive to the wreck of the Titanic in the mid-Atlantic. The submersible holds five people and was on an eight-day trip costing $250,000.\n- A massive search and rescue operation involving government agencies, navies, and deep-sea firms from the US and Canada is currently underway to locate the missing submersible.\n- The trip to the Titanic wreck is a unique and extraordinary experience offered by OceanGate, with only one manned mission planned for 2023. The missing sub is believed to be the only one capable of diving deep enough to reach the wreckage.",
    "hn_title": "Titanic tourist submersible goes missing with search under way",
    "original_title": "Titanic tourist submersible goes missing with search under way",
    "score": 434,
    "hn_content": "- A tourist submersible that was exploring the Titanic has gone missing, and a search is being conducted to locate it\n- The submersible, called the Titan, was built by OceanGate and has a carbon fiber and titanium hull capable of diving to 4,000 meters\n- The submersible had previously undergone testing for cyclic fatigue and had its depth rating reduced to 3,000 meters\n- The safety and profitability of ventures like this are a point of contention, with some arguing that the risk is worth it for adventurous tourists while others question the ethics\n- This incident highlights the challenges and potential risks associated with exploring deep-sea environments and the importance of rigorous testing and safety measures\n- The use of carbon fiber in the construction of the submersible raises questions about its reliability and effectiveness in high-pressure environments.- The post discusses the search and rescue efforts for a submarine called the Titan, which was on a mission to explore the wreckage of the Titanic.\n- The submarine, owned by OceanGate Inc., lost contact with the surface and there are concerns for the safety of the crew.\n- The most important innovation of this submarine is its real-time hull health monitoring system, which provides early warning detection for the pilot.\n- The search and rescue effort is ongoing and a remotely operated vehicle is being deployed to the site.\n- The submarine is not certified or approved by any regulatory body, but it was designed with safety margins in mind.\n- Deep-diving submersibles face risks such as pressure differentials, micro-buckling, and shear failure.\n- The appeal of exploring the Titanic lies in its historical significance and the tragic irony of its sinking on its maiden voyage.\n- The submersible is equipped with a game controller and uses consumer-grade lighting, which has raised questions about its design choices.\n- It is unclear if the submarine has a tether to the surface, but deploying one at this depth may be challenging.\n- Deep-diving submersibles often have ballast tanks and emergency buoyancy devices to help with ascent.\n- There is ongoing discussion about the risks and rewards of exploring the depths of the ocean in person rather than using remotely operated vehicles.\n- The search and rescue effort may take days to unfold, and the fate of the crew remains uncertain.",
    "hn_summary": "- A tourist submersible called the Titan, built by OceanGate, is missing while exploring the Titanic, and a search is underway.\n- The use of carbon fiber in the construction of the submersible raises questions about its reliability in high-pressure environments.\n- The challenges and potential risks associated with exploring deep-sea environments are highlighted, emphasizing the importance of rigorous testing and safety measures."
  },
  {
    "id": 36390308,
    "timestamp": 1687177854,
    "title": "Twitter lawyers overwhelmed as laid off employees file arbitration claims",
    "url": "https://www.techdirt.com/2023/06/16/twitters-lawyers-admit-theyre-overwhelmed-as-nearly-2000-laid-off-employees-file-arbitration-claims/",
    "hn_url": "http://news.ycombinator.com/item?id=36390308",
    "content": "This comment has been deemed insightful by the community.This comment has been deemed funny by the community.James Burkhardt(profile)says:June 16, 2023 at 12:28 pmIf only there was a legal mechanism for adjudicating legal disputes between a large entity and hundreds of similarly situated individuals with similar claims.ReplyView in chronologyMake this comment the first wordMake this comment the last word",
    "summary": "- Twitter lawyers are facing a surge of arbitration claims from laid off employees.\n- The claims are being filed by a large group of individuals who have similar legal disputes with Twitter.\n- The post highlights the need for a legal mechanism to handle disputes between a large entity and multiple individuals with similar claims.",
    "hn_title": "Twitter lawyers overwhelmed as laid off employees file arbitration claims",
    "original_title": "Twitter lawyers overwhelmed as laid off employees file arbitration claims",
    "score": 351,
    "hn_content": "- Former Twitter employees have filed arbitration claims to seek back pay, expense reimbursements, and severance that they were promised.\n- The procedural mess created by Elon Musk's actions at Twitter is highlighted in the article.\n- Twitter's binding arbitration clause has led to a large number of arbitration cases, overwhelming the company's lawyers.\n- Musk's reputation and his success with companies like Tesla and SpaceX are mentioned, with contrasting opinions on his actions and motivations.\n- Twitter's financial struggles and its impact on legal obligations are discussed, including the loss of top advertisers and potential bankruptcy.\n- The article touches on the use of forced arbitration as a strategy to overwhelm a company's legal resources.\n- The sarcasm and frustration expressed towards Musk, Twitter, and the legal system are noted.\n- The post highlights the interests of different groups, such as executives, normal people, and tech-savvy individuals, in the ongoing situation at Twitter.\n- The disdain and admiration for Musk are mentioned, with some perceiving him as putting tech people in their place and others seeing him as arrogant or reckless.\n- The potential implications of the arbitration cases and the role of class actions in seeking justice are discussed.\n- The snarky and aggressive tone of the lawyers involved in the case is noted.\n- The article raises questions about the fairness of arbitration and the power dynamics between companies and employees in legal disputes.- Executives are impressed with Twitter's operations: layoffs, regrouping, and new features with reduced staff\n- Tech people fear the consequences of the layoffs and the impact on Twitter's success\n- Use of arbitration in cases for severance payment and release of legal rights\n- Estimated collective damages for employees suing Twitter: around $50 million USD\n- Suing an employer is difficult, emotionally draining, expensive, and can harm future job prospects\n- Former Twitter employees have evidence that they were laid off due to company decisions, not their own actions\n- There are more former employees of Twitter suing the company than there are current employees\n- Discussions about Twitter's management and profitability\n- This is an interesting case that highlights the arbitration process, with Twitter as a secondary focus",
    "hn_summary": "- Former Twitter employees have filed arbitration claims seeking back pay, expense reimbursements, and severance that they were promised.\n- Twitter's binding arbitration clause has led to a large number of arbitration cases, overwhelming the company's lawyers.\n- The article raises questions about the fairness of arbitration and the power dynamics between companies and employees in legal disputes."
  },
  {
    "id": 36387030,
    "timestamp": 1687145645,
    "title": "Releasing an indie game on 3 consoles at once & failing financially (2016)",
    "url": "https://juicybeast.com/2016/01/11/releasing-an-indie-game-on-3-consoles-at-once-and-failing-financially/#2-years",
    "hn_url": "http://news.ycombinator.com/item?id=36387030",
    "content": "In May 2013, we attended TOJam and made a local-multiplayer game called Toto Temple. About a year later, we released a first version called Toto Temple Deluxe on Ouya. We kept improving the game afterwards and released the final version of Toto Temple Deluxe on PS4, Xbox One, Wii U and Steam on September 29th 2015. The game didn\u2019t sell very well, and we think we know why.The SalesLet\u2019s not waste your time, we both know that\u2019s what you want to see \ud83d\ude09 As I\u2019m writing this update, the game has been out for 4 months (released on Sept.29th 2015). Here\u2019s how close we are from breaking even:Almost! *curls into a ball and cries*So 4 months later, we sold around 6k copies in total, which is really not a lot as you can see. Maybe we\u2019ll benefit from the long tail eventually, or even better, the stegosaurus tail!Why invest 2 years on such a small game?The truth is, we never actually planned to do anything with Toto Temple (the jam version). We were presented with a chain of opportunities, and we simply decided to take each one of them as the development went on.Year 1: Working with OuyaThe first opportunity we took after the Toronto Game Jam (where the game ended up being one of the favorites), was taking Ouya\u2019s money to make a full fledged game out of Toto Temple. At the time, Matt Thorson\u2019s Towerfall was Ouya\u2019s killer app, and they wanted to cash in on that local-multiplayer trend (who wouldn\u2019t). We took the money and made a bigger version of the game, which we then called Toto Temple Deluxe.The deal was simple, get a bigger game (and more experience) in our portfolio without spending our own money. The game went live on Ouya, people enjoyed the Deluxe version, we even received really nice feedback from the press at E3 (Ouya booth).Here we were, a couple weeks after the Ouya release, with a \u201cconsole friendly\u201d game in our hands, which technically didn\u2019t cost us anything to make. For us, this was something unusual: we used to make small Flash / mobile games, and we now had a \u201cbig\u201d game that could be played in the living room with actual controllers. It felt like a big deal, as we could remember actually saying a couple years ago that \u201cwe would probably never be able to make console games\u201d. At the time, we didn\u2019t have the skills to develop for consoles, and Unity wasn\u2019t really a thing yet.Year 2: Porting to consolesThis is where we took the opportunity to get in touch with the 3 giants. We arranged meetings at GDC and pitched Toto Temple Deluxe to the 3 console owners. They all liked the game and things moved forward pretty quickly after that. We officially became PS4, Xbox One and Wii U developers, which still felt like a really big deal. We were in the big leagues now.Being total beginners in the console market, we quickly realized that we would never get an interesting exclusivity deal with any of 3 console owners, since the game was already out on Ouya.Knowing that, we decided to release the game on all 3 consoles. The decision was mainly based on 2 points:Our thought process back then was that if we can\u2019t strike a good exclusivity deal, then let\u2019s do all 3 consoles for 3 times the revenue.We also thought that we probably wouldn\u2019t get the media\u2019s attention 3 times for the same game (the 3 releases), so let\u2019s release the game on all 3 consoles at the same time. That way, we would get decent attention one time and speak to all console owners at once.Was the game at least fun?We genuinely think Toto Temple Deluxe is a lot of fun, under the right circumstances. We designed the game to be as family friendly as possible, meaning that anyone should have a minimum amount of fun, no matter how skilled their opponents are (auto-balance system, etc). It\u2019s also designed to be played with real people, in the same room, on the same couch. The more yelling, the better!We also witnessed genuine fun and engagement from players who tried the game in public events, like PAX and Comiccon (see the engagement below).We still get goosebumps over this one.Gameplay analysisWe don\u2019t think the game didn\u2019t sell well because it wasn\u2019t fun. On the other hand, Toto Temple Deluxe is still quite different from those other popular local-multiplayer games.Intuitive controlsOne of the first thing you notice when you first play Toto Temple Deluxe is how tricky it can be to effectively move around in the game (by using the dash mechanic). Even if we did our best to explain the controls in the most intuitive way we could think of, there\u2019s still a difficulty curve that you need to overcome to really feel at ease while moving around.It definitely comes in the way of our goal to make the game family friendly. We should either have made the controls simpler, or aimed at a more \u201cexperienced\u201d crowd from the start. Not both.In Towerfall, for instance, everyone can be deadly by simply pressing the \u201cshoot\u201d button once. It\u2019s that simple. In Toto Temple Deluxe, you need to:Get close to the goat carrierUse a combination of 2 buttons to dash towards the goat carrierHope that you were aiming rightIf not, you need to turn around and start overIt\u2019s definitely not a problem once you get the hang of it, but it\u2019s not making the game as easy to get into for new players.The showTricky controls can actually be a good thing if they\u2019re funny to watch. Our friends from Breakfall did just that with their game Starwhal. The game is pretty hard to control, but it\u2019s part of the show, as you watch your starwhal wiggle its way towards your opponent\u2019s heart. This game will touch your heartIn Toto Temple Deluxe, it\u2019s actually pretty hard to make sense of what\u2019s happening on the screen if you don\u2019t know the game already. I mean, look at this: This is probably confusing if you\u2019ve never seen the game before.Toto Temple Deluxe doesn\u2019t really give a \u201cgood show\u201d by default. You never see spectacular kills like in Towerfall or Starwhal, and not many actions are worth their own replay / slow motion sequence (there are, but they\u2019re harder to spot). It\u2019s not making the game less fun to play, obviously, but it\u2019s making it less fun to watch. It might not be that attractive to YouTubers and Twitchers for this reason.Intensity variationsIn most competitive local-multiplayer games where you can \u201celiminate\u201d players in some way (Samurai Gunn, Nidhogg, Towerfall, Starwhal, etc), you often find significant variations in intensity.The tension goes up as you\u2019re fighting / trying to survive, but once you or the other player is eliminated, there\u2019s this drop in intensity that lets you breath (and celebrate) for a couple seconds, which feels right. On paper, it would probably look like the graph on the left:On the other hand, Toto Temple Deluxe\u2019s intensity graph probably looks more like the one on the right. It\u2019s more like a marathon, less like short sprints.You get micro rewards / satisfactions every time you steal the goat or strike a power-up, but the game doesn\u2019t stop nor celebrate it that much, it just keeps going. Your real reward is at the end, when you finally reach 3k points and win the match.We don\u2019t think this kind of intensity is necessarily a bad thing, but it forced us to reduce the time of a match to about 2 minutes maximum. Anything passed that would really drain you out mentally, and a single game was usually more than enough.All of these elements are probably not responsible for the game not selling well, but they do have an effect on the actual feel and accessibility of the game.Theme, title and trailers analysisFollowing the release of the article, we received a lot of feedback regarding the general theme and title of the game. Here\u2019s an update on this particular subject.A big huge part of selling copies of your game is to grab people\u2019s attention when they first encounter your game on a website or in a store. You do that by having a good looking game (originality, style, theme, etc), which comes up in screenshots and trailers. You also need to have a good title that is both memorable/catchy and describes the game well. Finally, your game\u2019s description (elevator pitch) also needs to let people wanting more, or at least generate some kind of wonder in their mind.We didn\u2019t have any of this in Toto Temple Deluxe.The themeThe Mayan / temple theme wasn\u2019t bad, but it wasn\u2019t good either. It\u2019s clearly overused, uninspired and doesn\u2019t make you want to discover the game\u2019s universe because you already know what that kind of universe looks like. Don\u2019t get me wrong though, I think our game is beautifully drawn. We\u2019re solely talking about theme here.The main reason for all of this is because we chose the theme during the initial game jam. It was nicely supporting our first prototype about little servants defending the treasures in a temple for some god. You can see how the gameplay changed drastically during the jam.We ended the jam with the Mayan theme, and we had no reasons to revisit it afterwards. Then Ouya came in and asked us to take what we had (which was good enough), and make something bigger out of it. It was faster to keep it like this and take the money, rather than re-design everything from scratch and lose money in the process.After the Ouya version, we had a finished product that was a good fit for consoles (for the first time ever). It was obvious that we wouldn\u2019t start over from scratch at that point. So that\u2019s how we ended up with this theme.We didn\u2019t really notice that our theme was bad at the time because we were kind of confusing weirdness with originality. The whole \u201cgoat on your head\u201d thing felt unique, unseen, so we were under the impression that our game was unique and memorable. But, yeah, we were wrong.The character designs were also created during the jam, and we never went back to them. I personally still think they\u2019re cool/cute, but it\u2019s true that they aren\u2019t as strong as other characters. For instance, I really like Candyman and Latch from Lethal League (Team Reptile). They\u2019re unique and have strong, memorable features (thanks to Team Reptile for their feedback on the subject).The titleThe title also came during the jam, and our main focus back then was to make it sound catchy (4 successive syllables starting with \u201cT\u201d). It was also a reference to Toronto and ToJam. Toto.By the time we finished the Ouya version, the title had already spread a bit on Google (articles, download sites, etc). We thought that Toto Temple was still catchy, so we decided to keep it and simply add Deluxe to make it distinct from the jam version.Back then, we thought that \u201ccatchy\u201d and easy to find on Google was enough for a good title. We never asked ourselves if it was suggesting the gameplay well enough, of anything else for that matter.We received a lot of feedback regarding the Deluxe part saying that it had a strong \u201cpuzzle\u201d feel to it, which was misleading. At first I was a little bit skeptical, but a quick search on Steam revealed that it might lead to confusion for some users.It\u2019s not ALL puzzles, but I get the point. The title itself isn\u2019t responsible for the poor sales, but it certainly played a role in the whole thing, being one of the first things people see (usually).The trailersI like to think that both trailers are good. They\u2019re not perfect, but they\u2019re good. Again, I might be biased here because I made them myself.The first one that came out, the Goat Interview, was supposed to teach viewers about the game\u2019s mechanics. We wanted to do something like this because we felt like the gameplay was super chaotic and hard to understand just by looking at it.The voice is from Julian Smith, a really good voice actor!The trailer is a bit too long, but as first my first attempt at writing a script ever, I still think that it wasn\u2019t that bad. We were also in the process of getting more powerful computers at the time, so the gameplay footage isn\u2019t exactly smooth either.The second trailer that came out was the \u201cofficial\u201d gameplay trailer, where you can see how crazy Toto Temple Deluxe is. In a perfect world, you would have watched the first one before watching this one. That way, you\u2019d understand what was going on.When you\u2019re exhausted, adding WTF moments in a trailer [0:28] can help with motivation.I might still look like pure chaos in there, but I took particular care to teach important elements in the right order. In other words, the trailers talks to you the whole time. Here\u2019s what it\u2019s telling you:0:03 This game is about a goat0:05 You can play up to 4 players0:06 The goal is to \u201cGet The Goat\u201d0:09 It\u2019s a good game (a bunch of laurels)0:10 You get the goat by dashing in the carrier0:11 Again in slow motion to make sure you really saw what happened0:12 You can block your opponents to keep the goat0:13 Again in slow motion0:17 The temples have traps in them, you\u2019ll need to handle that0:19 Underwater, the game has very different temples0:22 Special boxes, what\u2019s that?0:24 It\u2019s a flaming goat, so there\u2019s multiple types of goats (modes)0:28 We don\u2019t take ourselves too seriously0:32 BOOM! Oh, it was a bomb goat!0:35 The game has teleporters, you can use them to trick/kill your opponents0:37 Again in slow motion0:38 You know everything now, we unleash the chaos and blow your mind[0:41] Oh, we forgot, crazy power-ups too0:53 It\u2019s called Toto Temple Deluxe, available here and here and there and\u2026Everything\u2019s going SUPER fast, and you might not get everything in one view, but at least it\u2019s the idea. I used a lot of repetition and slow motion to help you catch all the details, but even that wasn\u2019t enough to clearly see everything. By the way, if you want to hear more about \u201cmaking good trailers\u201d, I suggest you read this great article by Kert Gartner.Another point that could have helped selling the game to local-multiplayer aficionados is including live action footage of actual players enjoying the game (like this). This way, you really get an idea of what the game is about and how it is best played.A good example of that is the Sportsfriends trailer by Die Gute Fabrik.Looks pretty fun if you ask me.Once again, not a deal breaker to omit this kind of footage, but it can definitely help sell the game to the right crowd.Marketing experimentsWe tend to do everything in-house (contacting media, official websites, video trailers, community management, etc). We\u2019ve never been fans of spending money on marketing for things we could do ourselves. We also try to come up with creative solutions to market our games to counter-balance our low marketing budgets.For Toto Temple Deluxe, we really tried to do more than what we usually do. We went to PAX for the first time (remember, we used to make Flash games), and we also paid for PR for the first time as well. While going to gaming events and paying for PR services is pretty traditional, we also experimented a bit with other things like real goats and automatic Twitch raffles.Paying for nothing PR at PAX EastOur first experience with PR wasn\u2019t super useful in the end, to be honest. Even if we managed to get those services for a reasonable price (being \u201cnew customers\u201d to a super friendly PR company), the end results left us pretty unsatisfied.The services they provided were very good, don\u2019t get me wrong. We managed to get a couple interesting meetings scheduled at PAX (big websites), but the snowstorm that happened that week made some of them cancel. When we got back, the PR company helped us reschedule new online meetings with the sites who canceled, but none of them answered our multiple emails afterwards.One other big website that actually came to the meeting wrote an article about the upcoming game following the event, but they never bothered writing a review once the game released. We contacted them multiple time with no luck, which is weird, considering they already showed an interest in the game.Giving away $17,000 worth of Toto Temple Deluxe at PAX EastWe already wrote a detailed overview of this experiment, which you can read in full here.Yellow warning signs: A pretty good way of getting people\u2019s attention!This particular experiment (the showdown system) was really successful at the event. We attracted big crowds and got noticed by a lot of players. The only problem is that we didn\u2019t get noticed by that much press, which would have helped promote the game later down the road.The showdown system worked so well with player though, that we replicated it at future events (like the Montreal Comiccon), and received the same type of response (only on a smaller scale).It\u2019s definitely something we would try to adapt to our future games, and we encourage anyone to remix this idea to fit their needs.The Real Goat bundleWhen the game came out, we had a plan to join a good cause by promoting the Oxfam Unwrapped program. Oxfam are offering to buy real animals (like chickens, goats, donkeys, etc), to provide communities in need around the world. It\u2019s a sort of donation, basically.Yes, a REAL goat.What we did is we committed ourselves to buy real goats in the name of anyone who would purchase the Real Goat bundle. The buyers would also get 4 copies (Steam keys) for Toto Temple Deluxe to share with their friends.Despite being really well adapted to the theme of the game (goats) and being original to some extent, the Real Goat bundle didn\u2019t sell at all. We think it\u2019s probably due to 3 factors:The steep price of the bundle ($60 USD), which basically just covers the price of the goat. We make no money with the bundle, which actually fits the \u201cgood cause\u201d idea. We\u2019re giving away copies of our game in exchange for a donation to a charity organisation.We only felt comfortable giving away PC versions of the game. The game is more enjoyable / popular on consoles, but we\u2019re not allowed to distribute console codes that way.The game doesn\u2019t support online-multiplayer, so 4 copies of a local-multiplayer game might feel less interesting a deal if it doesn\u2019t let you play with your friends online. We were aiming to give buyers an opportunity to simply gift the game to their friends, but we completely understand the ambiguity.Bot Bet BattleFollowing the release of Toto Temple Deluxe, we organized a 1 day Twitch event called Bot Bet Battle. We had the game\u2019s bots fight themselves in a classic tournament, and viewers could vote for the bot they thought would win. Everything was AI controlled, so we had no input over which bot would win.The number votes would update in real time (or almost), the names of the voters would pop on the screen for everyone to see, and the tournament would start when enough votes were entered to fill the \u201cvotes bar\u201d. At the end of the tournament, everyone who voted for the winning bot would automatically receive Steam key in the chat system (as a whisper, obviously). You basically had a chance out of 4 to win a free copy of the game.We would also ramp up the amount of necessary votes from round to round, so that viewers would invite their friends to accelerate the process.We gave away about 1,400 Steam keys during the event. Knowing that a single account couldn\u2019t win twice, and that a lot of viewers never won a key, we know we reached a lot more than 1,400 viewers.We promoted the event on our social pages and a bunch of related subreddits, so there was probably a decent percentage of \u201cnew customers\u201d in the lot.Even though the event was fun, and that we probably managed to reach new followers, we barely noticed any bump in the game\u2019s sales.CameosThis one took us a lot of time and grew out of proportion pretty quickly. When we started working on the Deluxe version, we knew we wanted to include cameos from other games as a replacement for the goat (what you\u2019re fighting for). By other games, we mean games made by friends of ours and other developers we admire and respect. We also wanted to include people in there (not just game characters), like the developers themselves.All of this was supposed to be a small and cute addition to the game. We were adding unlockable cameos from games we love, and everyone from the team started to pitch in. This is where everything exploded. We created around 150 cameos (and by we I mean JP, our artist), ranging from game characters to their developers themselves. Here\u2019s some examples:We reached out to every single developer to introduce our idea and get an approval if they liked it. Some of them were already our friends, which was easy, but a lot of them were big developers we never spoke with before. Sometimes it took a lot of effort / contacts to get the missing email addresses.We wanted to do things in a clean way, so we wrote a personalized agreement for each of them, stating that the cameos were just cosmetic additions, and that we would never sell them or make money directly from them, etc.The agreement itself let the devs change the name of the cameos, decide on which consoles they would appear, pick the secret code to unlock them, and if they agreed with the whole idea or not. A digital signature was required, and boom, everything was set (no PDF, no emails).A couple months later, on the actual release date, we\u2019ve seen a lot of those devs share their cameos (and their secret code to unlock them) on Twitter and Facebook. Not all of them shared though. There was obviously no way to predict who would share and who wouldn\u2019t, but we still saw a LOT of tweets that day.A tweet by Renaud BedardA tweet by DrinkboxA tweet by Capy GamesA tweet by Brace YourselfIt\u2019s pretty hard to calculate how much visibility we gained through the cameo system, as we can\u2019t access other dev\u2019s Twitter stats, but we probably reached a lot of people who wouldn\u2019t have been aware of the game otherwise. What we can calculate though, is how many new connections we made with developers we admire, and we\u2019re pretty happy about it <3NewsletterA newsletter system is nothing new, but we also toyed with it for the first time. Okay, maybe the second time. Remember that part where we gave out free Steam keys at events? Well they weren\u2019t really free. We actually asked people to subscribe to our newsletter in order to receive the key. You can get all the details about how we did it here.We even created a sweet little backend system (with the help of our good friend Devine Lu Linvega) that automatically distribute the keys without having us send a single email. We just need to upload the keys to a database, and the system handles everything.Users need a unique code printed on the \u201ckey card\u201d they wonThey join our newsletter by entering their email address and unique code in a Mailchimp formMailchimp sends the info back to our backend system, where we validate the code and register the email if it\u2019s goodUsers automatically receive an email with their key in itWe managed to gather 1,437 email addresses around Toto Temple Deluxe (not just with free keys), which all received a single email on the release date (we don\u2019t like spam). Out of these 1,437 emails we sent, 493 (34%) were opened, and 72 (5%) got clicked. Based on certain benchmarks, these numbers aren\u2019t that bad!A unique code is required to get your key!So, why didn\u2019t the game sell?The game was fun and players were genuinely engaged at events. We reached out to a lot of press with the help of a PR company, but barely got any coverage. We tried different and original marketing tactics, but they didn\u2019t resulted in many changes in the sales. So, what happened?There\u2019s a good possibility that the game didn\u2019t have good enough \u201chooks\u201d to captivate people\u2019s attention. Sure, \u201cfighting your friends to put an egg-laying goat on your head\u201d sounds funny and weird, but it doesn\u2019t really give you a clear idea of what it\u2019s like to play Toto Temple Deluxe.At the end of the day, we think the biggest factor is because it\u2019s a local-multiplayer game with no online play. The game is aiming at a pretty niche audience by requiring actual human friends to play, and we can\u2019t ignore the impact it has on sales. A quick look at comments on YouTube, Reddit and such, and it\u2019s obvious that a lot of people are simply not buying the game for this very reason (that along a lack of solo experience). Here\u2019s one example:At least it\u2019s loads of fun!To confirm this idea, one of our contacts from a very well known gaming site explained to us why they wouldn\u2019t write an article on Toto Temple Deluxe. Apparently, it\u2019s simply because local-multiplayer news don\u2019t generate enough clicks to be worth it. We totally appreciated the honesty!On top of all this, we also think the game came out almost 2 years too late. Back in 2014, when the local-multiplayer boom was happening, Toto Temple Deluxe\u2019s development felt more logical. Today, it feels a bit out of place, as we think a lot of player might have bought popular local-multiplayer games in the past, then realized that they would play them less and less frequently. We definitely can\u2019t blame them, since it\u2019s pretty much the same for us.Obviously, these are all based on our current knowledge and experience, and probably doesn\u2019t apply to all local-multiplayer games. If you\u2019re reading this and you\u2019re thinking we missed a point, or have a different opinion on the subject, we\u2019d love to hear it!Porting to 3 consoles at onceReleasing the game on the 3 consoles at the same time felt like a good idea at the time. We were only thinking about it from a revenues and media coverage standpoint, which was obviously a big mistake (for a local-multiplayer game released 2 years too late with no online mode, at least).The actual portsThe worst part in all of this is probably that our only programmer, Alex, did all the heavy lifting by himself (the guy\u2019s a freaking machine). Making the game run smoothly on all 3 consoles took him a good amount of time and energy, especially during the certification process.To give you an idea of the difference between the 3 consoles, here\u2019s how we spent our time on the actual ports over the second year of development:Well that\u2019s not equal at all.Considering that we made about equal revenues on each consoles so far, you can see that one was definitely more expensive to port to. This might change in the future though, as this was our very first time working with consoles. We also received a lot of help from our friends from Chainsawesome Games for the Xbox One port, which saved us a lot of time and headaches!Another factor that greatly extended the port process was having to jump back and forth between the different versions of the game. Yes, Unity makes it easier to port to multiple platforms, but the requirements for each console were so different that we ended up having 3 different Unity projects / branches to support them. Fixing and patching and fixing and\u2026We lost a LOT of time patching little mistakes here and there, like details we added or fixed on one version but forgot to include in the other, etc. With only 2 team members skilled enough to handle the \u201ctechnical stuff\u201d, you can surely imagine how this inefficient workflow became extremely messy during the certification process.What about the other team members?We mentioned that most of the ports were handled by Alex, our programmer, with a lot of additional help from Dom (certs, store metadata, etc), so a Reddit user asked \u201cwhat were the artists/other non-programmers doing during that time?\u201dIt\u2019s a totally legit question, considering I didn\u2019t cover it much in the article. As you already read above, JP, our artist, handled all the cameos. He drew around 150 in total, with a lot of them being \u201ccaricatures\u201d of real people (kind of harder to draw). I was overseeing the whole cameo thing with Dom, but I was also in charge the 2 trailers.My answer on Reddit:\u201cWe made our 2 trailers in house, and one of them was a frame by frame animation / cartoon with scripting and voice acting. We also needed to render, submit and rate both trailers, for 3 companies, split in 2 continents (American and European divisions are all separate entities), which all have different requirements depending if it\u2019s a storefront video or a website video, or a YT channel video, etc. Each company has a required set of video encoding setting that differs based on what I just listed, and we\u2019re not even talking about legal lines at the end of trailers, rating icons (ESRB, PEGI), console logo placement / colors / size requirements (because that\u2019s a thing). Here\u2019s a screenshot of the list of all the different renders we made for just 2 simple trailers:I was happy we chose to only do 2 trailers\u2026!A lot of that year was spent f*cking up and trying to fix things. Big AAA companies usually have complete departments dealing with certs and all that kind of stuff, they\u2019re used to it and they know what they\u2019re doing. We don\u2019t ;)\u201dObviously, the rendering of all these videos is nothing compared to 3 actual ports, but if you\u2019re thinking about doing it yourself, it\u2019s useful to know that it might take you more time than expected.Burning outWe spent 2 years, full time, working on Toto Temple Deluxe. The first year was about developing the game and its content, while the second year was about porting it to 3 different consoles at once. That second year is where we started to feel the burn.We\u2019re a team of creative people, and we get our motivation and energy by exploiting that creativity. We like to create things, invent worlds and mechanics. That second year of development was all about paperwork, unusable console portals, technical problems, bugs and desperate searches for information (consoles and their documentation are not really easy to navigate). Uuuugh\u2026Once we shipped the game on Sept 29th, we all went home and did nothing for a least a week. Porting Toto Temple Deluxe really drained us out, especially for Alex who did a inhuman job all by himself (seriously, a machine).The process of porting the game was hard by itself, but following up a year of non-creative work with a launch that doesn\u2019t make much noise, nor sell a lot, was pretty hard on all of us. We\u2019re slowly getting back in shape for a new project (winning the prize of the most addicting game with Right Click to Necromance for Indie Speed Run shook things up a bit), but we can still taste the bitterness of the unsuccessful launch.No deal, no helpBack in the days, when we were young and naive, we pictured console storefronts as big marketing machines with lots of users and traffic (remember, we were doing Flash games). We realized pretty quickly that it\u2019s not how it works!Talk by Nick Suttner, screenshot by Rob FearonSince we couldn\u2019t really strike an exclusivity deal with any of the consoles, with the Ouya version already available, and we were porting to all 3 consoles at the same time (no \u201csemi-exclusivity\u201d), Toto Temple Deluxe probably wasn\u2019t perceived as a really interesting deal for any of the consoles. Sure, they were getting a new game for their system, but that was pretty much it.Because of that, we weren\u2019t really able to get a featured spot on any the storefronts, except for Nintendo who actually featured us on both the American and European front pages. Thanks Nintendo!Oh, look at that! Photo by Tim McLennanWow, twice? Photo by TenkomanWhat\u2019s also interesting is that being featured twice on the Wii U eShop actually ramped up the sales of the game to almost equal numbers with the other 2.Next timeWe\u2019ve learned a lot with Toto Temple Deluxe, and there\u2019s a lot of things we\u2019d do differently (or not at all) for our next game:No more local-multiplayerThey\u2019re easy to make in a game jam, but super hard to market if you don\u2019t add online play.Strike a deal firstIf we\u2019re to make another game on console, we\u2019ll make sure to strike an exclusivity deal first, so we can get a bit more help and visibility from the console owner.Only one consoleIt goes without saying that with we\u2019ll only do one console if we strike an exclusivity deal, but even without that, we wouldn\u2019t port to 3 consoles at once ever again. It\u2019s a LOT of hard work that didn\u2019t really paid well in the end.Design from a marketing perspectiveVisibility is getting harder and harder to come by, so we\u2019ll try to design our games with marketing in mind from day one. Having a solid hook and making sure there\u2019s a potential audience for your game is super important. I suggest you read this great article on the subject by Ryan Clark from Brace Yourself.I think it\u2019s easy to think of Toto Temple Deluxe as a failure. Even worst, a waste of time and resources. Sure, the game only generated a fraction of what we spent on it, but we\u2019ve learned a lot. Now it\u2019s time to get up, dust off our shoulders and get the creative juice flowing again!",
    "summary": "- The indie game \"Toto Temple Deluxe\" was released on multiple consoles simultaneously, but it did not sell well financially.\n- The game was originally created as a local-multiplayer game and did not have online play, which limited its appeal to a niche audience.\n- The game's theme, title, and trailers did not effectively capture people's attention and generate interest in the game.",
    "hn_title": "Releasing an indie game on 3 consoles at once and failing financially (2016)",
    "original_title": "Releasing an indie game on 3 consoles at once and failing financially (2016)",
    "score": 312,
    "hn_content": "- The post discusses the challenges faced by an indie game developer who released their game simultaneously on three consoles and failed financially.\n- The comments express empathy for the developer and highlight the importance of small developers sharing their experiences.\n- The indie game market has become highly competitive, with a flood of developers creating and releasing games with varying levels of quality.\n- Success in the indie game market often depends on effective marketing and luck rather than just the quality of the game itself.\n- The post and comments showcase the value of post-mortems and honest reflections on game development experiences.\n- The current indie game market requires both high-quality games and effective marketing strategies to stand out.\n- The rise of platforms like Steam has led to a saturated market with a large number of games being released daily.\n- The challenges faced by indie game developers highlight the need for innovation, quality, and differentiation to succeed in the industry.\n- The success of indie games like Valheim and Rimworld demonstrates that quality and uniqueness can still lead to success in the market.\n- Small indie game developers should be supported and celebrated for their contributions to the gaming industry.- The indie game \"Toto Temple Deluxe\" struggled to find success in the highly competitive gaming market\n- The game's local-multiplayer focus without online play was a major factor in its failure\n- The unclear trailer and lack of distinctiveness in the game's art style may have contributed to a lack of interest from players\n- The game faced challenges in marketing and distinguishing itself in a crowded marketplace\n- The post-mortem analysis highlights the importance of creating a unique and engaging gameplay experience to stand out in the industry.",
    "hn_summary": "- The post discusses the challenges faced by an indie game developer who released their game simultaneously on three consoles and failed financially.\n- The comments express empathy for the developer and highlight the importance of small developers sharing their experiences.\n- The indie game market has become highly competitive, with a flood of developers creating and releasing games with varying levels of quality."
  },
  {
    "id": 36389285,
    "timestamp": 1687169245,
    "title": "Display brighter-than-white color on Apple devices",
    "url": "https://github.com/dtinth/superwhite",
    "hn_url": "http://news.ycombinator.com/item?id=36389285",
    "content": "superwhitedisplay a very bright white color on HDR-enabled displays with ~1 KB of video filesuperwhite.mp4if you view this page with a recent iPhone or iPad with low-power mode turned off, you should see a very bright white color above. on a Mac displays with HDR support, you should see a white color that is brighter than #ffffff. on unsupported displays, you should see a normal white color.for example of a practical usage, see: https://notes.dt.in.th/HDRQRCodeplease don\u2019t abuse thisAs data URLdata:video/mp4;base64,AAAAHGZ0eXBpc29tAAACAGlzb21pc28ybXA0MQAAAAhmcmVlAAAAvG1kYXQAAAAfTgEFGkdWStxcTEM/lO/FETzRQ6gD7gAA7gIAA3EYgAAAAEgoAa8iNjAkszOL+e58c//cEe//0TT//scp1n/381P/RWP/zOW4QtxorfVogeh8nQDbQAAAAwAQMCcWUTAAAAMAAAMAAAMA84AAAAAVAgHQAyu+KT35E7gAADFgAAADABLQAAAAEgIB4AiS76MTkNbgAAF3AAAPSAAAABICAeAEn8+hBOTXYAADUgAAHRAAAAPibW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAAKcAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAAw10cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAAKcAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAABAAAAAQAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAAACnAAAAAAABAAAAAAKFbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAABdwAAAD6BVxAAAAAAAMWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABDb3JlIE1lZGlhIFZpZGVvAAAAAixtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAHsc3RibAAAARxzdHNkAAAAAAAAAAEAAAEMaHZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAQABAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAAHVodmNDAQIgAAAAsAAAAAAAPPAA/P36+gAACwOgAAEAGEABDAH//wIgAAADALAAAAMAAAMAPBXAkKEAAQAmQgEBAiAAAAMAsAAAAwAAAwA8oBQgQcCTDLYgV7kWVYC1CRAJAICiAAEACUQBwChkuNBTJAAAAApmaWVsAQAAAAATY29scm5jbHgACQAQAAkAAAAAEHBhc3AAAAABAAAAAQAAABRidHJ0AAAAAAAALPwAACz8AAAAKHN0dHMAAAAAAAAAAwAAAAIAAAPoAAAAAQAAAAEAAAABAAAD6AAAABRzdHNzAAAAAAAAAAEAAAABAAAAEHNkdHAAAAAAIBAQGAAAAChjdHRzAAAAAAAAAAMAAAABAAAAAAAAAAEAAAfQAAAAAgAAAAAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAAQAAAABAAAAJHN0c3oAAAAAAAAAAAAAAAQAAABvAAAAGQAAABYAAAAWAAAAFHN0Y28AAAAAAAAAAQAAACwAAABhdWR0YQAAAFltZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAACxpbHN0AAAAJKl0b28AAAAcZGF0YQAAAAEAAAAATGF2ZjYwLjMuMTAwHTML snippet<video muted autoplay playsinline oncanplaythrough=\"this.currentTime=0\" poster=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQAQAAAAA3iMLMAAAAAXNSR0IArs4c6QAAAA5JREFUeNpj+P+fgRQEAP1OH+HeyHWXAAAAAElFTkSuQmCC\" src=\"data:video/mp4;base64,AAAAHGZ0eXBpc29tAAACAGlzb21pc28ybXA0MQAAAAhmcmVlAAAAvG1kYXQAAAAfTgEFGkdWStxcTEM/lO/FETzRQ6gD7gAA7gIAA3EYgAAAAEgoAa8iNjAkszOL+e58c//cEe//0TT//scp1n/381P/RWP/zOW4QtxorfVogeh8nQDbQAAAAwAQMCcWUTAAAAMAAAMAAAMA84AAAAAVAgHQAyu+KT35E7gAADFgAAADABLQAAAAEgIB4AiS76MTkNbgAAF3AAAPSAAAABICAeAEn8+hBOTXYAADUgAAHRAAAAPibW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAAKcAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAAw10cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAAKcAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAABAAAAAQAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAAACnAAAAAAABAAAAAAKFbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAABdwAAAD6BVxAAAAAAAMWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABDb3JlIE1lZGlhIFZpZGVvAAAAAixtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAHsc3RibAAAARxzdHNkAAAAAAAAAAEAAAEMaHZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAQABAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAAHVodmNDAQIgAAAAsAAAAAAAPPAA/P36+gAACwOgAAEAGEABDAH//wIgAAADALAAAAMAAAMAPBXAkKEAAQAmQgEBAiAAAAMAsAAAAwAAAwA8oBQgQcCTDLYgV7kWVYC1CRAJAICiAAEACUQBwChkuNBTJAAAAApmaWVsAQAAAAATY29scm5jbHgACQAQAAkAAAAAEHBhc3AAAAABAAAAAQAAABRidHJ0AAAAAAAALPwAACz8AAAAKHN0dHMAAAAAAAAAAwAAAAIAAAPoAAAAAQAAAAEAAAABAAAD6AAAABRzdHNzAAAAAAAAAAEAAAABAAAAEHNkdHAAAAAAIBAQGAAAAChjdHRzAAAAAAAAAAMAAAABAAAAAAAAAAEAAAfQAAAAAgAAAAAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAAQAAAABAAAAJHN0c3oAAAAAAAAAAAAAAAQAAABvAAAAGQAAABYAAAAWAAAAFHN0Y28AAAAAAAAAAQAAACwAAABhdWR0YQAAAFltZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAACxpbHN0AAAAJKl0b28AAAAcZGF0YQAAAAEAAAAATGF2ZjYwLjMuMTAw\"></video>Creating the videoI used Final Cut Pro.Create a library with Wide Gamut HDR color processing setting.Create a project.Add a solid white color generator.Set Color to Bright White.Crank up Graphics HDR Level to 100.Add the HDR Tool video effect.Set Mode to SDR to HDR (PQ).Crank up Peak Brightness to 5000 nits.Export the video using HEVC 10-bit as the codec.",
    "summary": "- The post introduces a technique called \"superwhitedisplay\" which allows users to display a very bright white color on Apple devices with HDR support.\n- The technique involves creating a small video file and playing it on the device's display.\n- The post provides instructions on how to create the video using Final Cut Pro and export it using HEVC 10-bit codec.",
    "hn_title": "Display brighter-than-white color on Apple devices",
    "original_title": "Display brighter-than-white color on Apple devices",
    "score": 303,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginDisplay brighter-than-white color on Apple devices (github.com/dtinth)303 points by ash 23 hours ago | hide | past | favorite | 178 commentshbn 19 hours ago | next [\u2013]Apple did a clever thing with HDR content where it can appear amongst other non-HDR content, and only the part of the screen displaying the HDR content is in HDR. But in practice it looks really bad and I hate every time I'm scrolling Instagram and an HDR video appears in my feed. It blows my eyes out and then the rest of my screen looks gray and muted for a few seconds until I get the HDR content off my screen.replymrtksn 18 hours ago | parent | next [\u2013]When done tastefully it\u2019s actually amazing. Especially videos which have bright sun illuminating some surfaces and other surfaces are in the shadows. It really looks like the sun is shining, I can feel the endorphin is shooting through my veins.However, for some reason, the human skin tones can look very weird in some conditions. Maybe the iPhone camera settings should be by default off when it comes to HDR. It also complicates sharing and color editing, so it\u2019s a true \u201cpro\u201d feature.replythrashh 14 hours ago | root | parent | next [\u2013]I have to agree with the iPhone camera in bright settings. It doesn\u2019t look good.Admittedly it can look better than my digital camera with default settings but I can dial some manual settings on my digital camera and the photos will come out really good.It really comes down to managing dynamic range when you are technically limited in capturing it.A lot of people like film because because highlights roll off instead of being linear with clipping like digital, but you can get way better results with digital if you know what to do. Unfortunately I hate how the iPhone camera does it.replynomel 12 hours ago | root | parent | next [\u2013]Have you tried disabling HDR, in the camera settings, so regular mapping is used?replydontlaugh 10 hours ago | root | parent | next [\u2013]It can\u2019t be turned off.replynomel 9 hours ago | root | parent | next [\u2013]Oh! It used to be an option, to keep non-HDR versions, but I see now that it\u2019s gone.replyrollcat 14 hours ago | root | parent | prev | next [\u2013]> When done tastefully it\u2019s actually amazing.Same thing happened with color TV, same thing happened with 3D movies... every time film technology advances, it gets abused at first, until 1. people get fatigued with it, 2. someone figures out how to actually use the new medium.replydheera 15 hours ago | root | parent | prev | next [\u2013]Easily doable on non-Apple displays.(a) Buy a high brightness monitor. There are some industrial ones that go up to 1000, 2500, or even 5000 nits. A Dell U2723QE, for comparison, is 400 nits.(b) Scaling back ALL RGB pixel values linearly from [0,255] to [0,127]. Actually, just bit shift them.(c) Set monitor brightness to 100%, which cancels the effect of (b) under most circumstances.(c) When you want a dose of \"Apple HDR\" white you just issue a [255,255,255] and you get a blast of 1000 nits in your face.In fact I think a lot of newer monitors offer 10 bits per pixel of depth, and considering most images on the web are still 8 bits per channel, you can do all of the above without even losing color resolution from 8 to 7 bits, and instead go from 10 to 9 bits, though I don't know how to implement that in practice (might have to be done on the graphics driver level rather than scaling down pixel values in the OS?)replysaagarjha 11 hours ago | root | parent | next [\u2013]This sounds to me like it would be a very poor approximation of how EDR works on Apple displays.replyJoeri 17 hours ago | parent | prev | next [\u2013]Actually the whole screen is HDR, but the non-HDR parts are dimmed to match their normal brightness. They can do this because they calibrate their displays and know exactly how bright they are.By the way, if brightness is not maxed this works even on the 400 nits macbook air m1, which they call EDR. They ramp up to max brightness and dim the non-HDR parts.replymrtksn 17 hours ago | root | parent | next [\u2013]It works even if the brightness is maxed out(M1 Air), they must be leaving some brightness margin to allow for this to work on any brightness level - maybe except for outdoor lighting conditions? I had to blast the ambient light sensor with my phone's flash to get the screen bright enough so that the HDR content disappears. When the auto-brightness is turned off, you can still see the HDR content up until the top settings, just one bar down is enough to display it.The switch from SDR to HDR is also interesting, when an HDR content is to be displayed it slowly fades into HDR. It must be tamping up the backlight of the LCD when tinting the SDR content simultaneously to create the gradual brightness increase effect in the HDR areas. The fact that the SDR area doesn't flicker when this happens, gives me the impression that the Apple do amazing screen calibration.replycrazygringo 17 hours ago | root | parent | next [\u2013]> It works even if the brightness is maxed out(M1 Air)No it doesn't, I'm trying it out right now on my M1 Air. If my screen brightness is max, the video isn't extra-bright in Chrome or in Safari. If I turn down the screen brightness, it does stand out.I don't know what you're talking about having to \"blast the ambient light sensor\" to get your screen to max brightness, that's not how my M1 Air works and it's not how any Mac laptop is supposed to work. That's used for \"Automatically adjust brightness\" (which I have on) but that's overridden at any moment just by using the brightness buttons on your keyboard to turn it up to the max. So I'm not sure how you think you've got your brightness \"maxed out\" but it seems you don't. You might have some other setting preventing it, like automatic screen dimming in battery mode.replymrtksn 17 hours ago | root | parent | next [\u2013]Check this out: https://dropovercl.s3.amazonaws.com/3957bc45-20b7-47b5-8910-...Also, try not to get worked up for such stuff on the internet. Yas it appears that when plugged to the wall, it does lose the HDR-white on max brightness. Still, even on max - 1, HDR is displayed properly.On iPhone 14 Pro, which has the ability to display full HDR content, also can't display the HDR-White when the brightness is cranked up to the max.In my book, M1 Air and iPhone 14 Pro are both capable of displaying HDR-white. Just don't go to absolute peak brightness, which only happens under direct sunlight anyway(\"blast the ambient light sensor\" part, simulating extremely bright environment).replycrazygringo 16 hours ago | root | parent | next [\u2013]> Also, try not to get worked up for such stuff on the internet. Yas it appears that when plugged to the wall, it does lose the HDR-white on max brightness. Still, even on max - 1, HDR is displayed properly.Correcting misinformation isn't \"getting worked up\", please don't insinuate anything like that. HDR is still fairly new so it's important that misinformation doesn't spread.And as your comment suggests, it's only because your Mac is temporarily reducing its brightness in battery mode (also sometimes due to heavy CPU usage, that the battery can't fully drive the display and chips).But your assertion that \"absolute peak brightness... only happens under direct sunlight\" is false. As you yourself are seeing, you achieve the same when merely plugged in, no ambient light required.replyjimnotgym 11 hours ago | root | parent | prev | next [\u2013]Hang on, Apple don't calibrate each display individualy. They profile a characteristic display, and then apply that characteristic profile to all of their displays.If you want a calibrated display you will need to buy some calibration hardware! Displays change over time too...so frequent calibration is needed.Edit: If you want to prove this to yourself grab the display profile off another mac and compare it. It will be the same.replyJoeri 4 hours ago | root | parent | next [\u2013]TILAnyway, the profile is accurate enough that the dimming of non-HDR parts while ramping up brightness is not noticeable to me if I block off the HDR part.replycornstalks 16 hours ago | parent | prev | next [\u2013]Part of the problem is mobile devices often grade their videos \u201ctoo hot.\u201d This Demuxed[1] talk has a pretty good overview of the problem in the industry and ideas for improving it (and to avoid confusion, the talk is given from the perspective of someone in the future looking back on HDR in the industry).[1]: https://youtu.be/bYS-P2bF2TQreplyflutas 19 hours ago | parent | prev | next [\u2013]Yup, exactly the same impression for me.To me it doesn't make HDR content look better, but it makes the non HDR content around it just look worse.replyhbn 18 hours ago | root | parent | next [\u2013]Honestly I think it kind of makes both look bad. Everything else looks muted, but also the HDR content itself looks cartoonishly over-saturated because your eyes are not in the context of seeing colors like that on your screen as it is.replymunchler 17 hours ago | parent | prev | next [\u2013]So it\u2019s basically the Spinal Tap \u201camp goes to 11\u201d for displays? Genius.replykitsunesoba 18 hours ago | parent | prev | next [\u2013]My opinion of this way of handling HDR is kinda mixed when it comes to phones, but on computers and tablets I prefer it to the way that e.g. Windows handles HDR, where non-HDR parts of the screen look weirdly dim, which makes you want to only turn HDR on when running a game that supports it or something like that.replyLelouBil 18 hours ago | root | parent | next [\u2013]I have this only when turning on HDR in windows, but not in my screen.When both are on, it's good.replykccqzy 18 hours ago | parent | prev | next [\u2013]Fortunately turning on Low Power Mode disables that nonsense. Might as well save some battery while getting rid of this eye-scorching white.replytoxik 15 hours ago | root | parent | next [\u2013]It in fact still works in low power mode.replykllrnohj 17 hours ago | parent | prev | next [\u2013]Android does this as well now, depending on device support https://source.android.com/docs/core/display/mixed-sdr-hdrreplywhalesalad 18 hours ago | parent | prev | next [\u2013]Always wondered wtf was going on here and now it makes sense.replyfifafu 22 hours ago | prev | next [\u2013]This has been known for quite a while and can be used for arbitrary HTML elements with some CSS hacks. I'm suprised no \"super bright\" advertisements have shown up so far. https://kidi.ng/wanna-see-a-whiter-white/ (Safari only)On macOS you can use apps like BetterDisplay, Vivid or BetterTouchTool to enable that HDR mode for the whole display, which makes it significantly easier to work outside. On iOS there is \"Vivid Browser\" - a browser that enables the HDR mode for the whole screen.replytaknil 19 hours ago | parent | next [\u2013]I found out just a few days ago our company tries to be \"super bright\" in its banner ads with .img_ad:hover { -webkit-filter: brightness(120%);}replymastax 19 hours ago | root | parent | next [\u2013]Great. Another feature that gets ruined by advertisers before it even gets used in earnest.replybee_rider 18 hours ago | root | parent | prev | next [\u2013]\u201cIt\u2019s 20% brighter, isn\u2019t it?\u201dreplynyreed 17 hours ago | root | parent | next [\u2013]But why not just make 100% brighter and make 100% be the top number and make that a little brighter?replymattkevan 16 hours ago | root | parent | next [\u2013]\u2026but this goes to 120%replyspartanatreyu 8 hours ago | root | parent | next [\u2013]For anyone who doesn't get the reference: https://www.youtube.com/watch?v=uMSV4OteqBEreplyaendruk 16 hours ago | root | parent | prev | next [\u2013]Backwards compatibilityreplybee_rider 16 hours ago | root | parent | next [\u2013]Ruining then bit with, honestly, a pretty good point.replyjannes 21 hours ago | parent | prev | next [\u2013]I think many people learned this today. \"Known to you\" might not be the same as \"known to everybody else\" :)replyfifafu 21 hours ago | root | parent | next [\u2013]Yep! It's just me wondering why the advertising industry hasn't picked up on this, given that they are usually fast to exploit any possible browser feature/quirk to push their advertisements.replyjannes 21 hours ago | root | parent | next [\u2013]Probably Safari users aren't their top priority right now (after all the anti-tracking features that Apple added).replyfifafu 21 hours ago | root | parent | next [\u2013]I'd assume iOS is still one the most important markets for advertisersreply411111111111111 19 hours ago | root | parent | next [\u2013]That'd heavily depend on the target audience, safari's market share is tiny if it's global.It should be pretty high if the target audience is US only and mainly the higher income brackets, however.replyalwillis 18 hours ago | root | parent | next [\u2013]2+ billion devices is not nothing.reply411111111111111 17 hours ago | root | parent | next [\u2013]Nobody said it's numerically few devices.It's just a question of percentages, do you really want to implement something that increases the brightness of your advertisement on 2-5% of your global impressions?Entirely different story if your advertisement is a high margin product sold on the american market, especially if it's appealing to teenagers/young adults. That'd probably be over 50% of impressions, though that's just me guessing the number.replyllm_nerd 17 hours ago | root | parent | next [\u2013]In the US (and many other countries) the iPhone dominates every demographic except I guess the very poor? The whole \"teenagers\" thing has never been true. Grandma has an iPhone.However worldwide Safari accounts for over 30%+ of mobile device impressions. That is an absolutely enormous market. It tends to be the market with the most cash in almost all markets (I'm not trying to be a booster or start some mobile platform war, and yes Samsung devices are super expensive as well, but demographically you can go to almost any country -- Japan, Russia, India, Brazil, just about everywhere, and the top n% are dominated by iPhones).If advertisers aren't exploiting it it's either ignorance, or more likely that the dominant ad networks know it will get the ads flagged as abusive and Apple will drop the hammer.reply411111111111111 17 hours ago | root | parent | next [\u2013]The discussion was about global ad impressions, which include desktop use and exclude devices with working adblockers.Why are you suddenly quoting exaggerated mobile phone market share numbers in such a context?replyllm_nerd 17 hours ago | root | parent | next [\u2013]All apologies for being direct, but you seem to have absolutely no idea what you're talking about. Your defensiveness about easily verified mobile metrics betrays your real intentions here.Among all browser users, Safari accounts for a 20%+ marketshare (again, easily verified. I assume your metrics were either made up or you're just trolling?). Again almost entirely the people with the most money. Your hysterical claim that this is too small to bother with is a howler, but please keep on.reply411111111111111 15 hours ago | root | parent | next [\u2013]What are you even on about, I'm able to access the web metrics for my current employers consumer facing site which has single digit safari user agents, and I'm betting at least half of them will have enabled content blockers, though I don't have any metric for that.It just depends on your target audience. Some demographics have miniscule apple presence, others have them as a majority. So, depending on your target audience, it might not be worth it. Especially because we're taking about a feature that will merely increase the brightness of the ad.Don't interpret some idiotic agenda into a simple cost/reward speculation.Fwiw, this comment was written on an iPhone whilst procrastinating from work. Which is coding on a MacBook pro whilst checking the resulting website on an iPad Air today. I'm even using AirPods Pro to listen to music...replydawnerd 17 hours ago | root | parent | prev | next [\u2013]We pixel push ads around and deal with single browser fixes so I\u2019d say yes, advertisers do care and will spend the dev time if they think there\u2019s any% rev increase.replyfnordpiglet 17 hours ago | root | parent | prev | next [\u2013]I think your numbers are off by an order of magnitude :https://worldpopulationreview.com/country-rankings/iphone-ma...Further as an advertiser I\u2019m more interested in impressions weighted by free potential spend. A knock off device in Nigeria doesn\u2019t factor into my calculations vs a top of the line iPhone in Norway. The latter just has more money to spend.replySchnitz 15 hours ago | root | parent | prev | next [\u2013]Isn\u2019t this the sort of thing that would give Apple an excuse to detect ads and treat them differently? I\u2019d avoid that if I were a big advertiser.replyrollcat 14 hours ago | root | parent | prev | next [\u2013]https://xkcd.com/1053/replyalin23 18 hours ago | parent | prev | next [\u2013]Lunar developer here, I also use this trick to showcase Lunar's XDR Brightness: https://lunar.fyi/#xdr (which by the way, was the first app to get this feature before Vivid took over with clever Twitter marketing)Some people will worry about battery and display longevity in the comments so I'll also leave some notes I wrote on this: https://lunar.fyi/faq#xdr-safeTLDR: yes, battery and LED lifetime will take a hit.It's a normal consequence of driving the LEDs to a higher voltage which uses more battery power and creates more LED heat.macOS has an ABL (automatic brightness limiter) internally that will forcefully limit the voltage based on the amount of white pixels on the screen, and based on the generated heat. That ABL works at a lower level and cannot be bypassed by apps like Lunar or Vivid.Also for the people wondering about BTT, look for the \"Toggle super brightness\" action.replyjamwil 15 hours ago | root | parent | next [\u2013]I love your app. Thanks for making it.replybrookst 21 hours ago | parent | prev | next [\u2013]+1 to Vivid. Being able to work outdoors with a perfectly readable laptop has been life-changing.replyverse 20 hours ago | root | parent | next [\u2013]Doesn't that kill your battery though?reply130e13a 20 hours ago | root | parent | next [\u2013]wouldn't it, in the long run, also lead to increased potential for screen damage?replyepcoa 19 hours ago | root | parent | next [\u2013]Short answer, probably not damage but it will shorten lifetime, possibly not meaningfully. It\u2019s still running within spec. It will ever so slightly increase the risk of premature failure as running anything closer to its limits tends to do. White LEDs dim and color shift gradually as they age so it will cause it to dim/shift faster. On a good display this is still 10s of thousands of hours of operation which is many years.replyasvitkine 20 hours ago | root | parent | prev | next [\u2013]Does it work on a laptop or so you mean iPad?replyagloe_dreams 19 hours ago | root | parent | next [\u2013]Mini LED Laptops like the 14 inch Macbook Pro. Peak brightness is over 1000 nits but it is locked out by default except for HDR content.replydmd 19 hours ago | root | parent | prev | next [\u2013]Works on my macbook.replye40 19 hours ago | parent | prev | next [\u2013]> On macOS you can use apps like BetterDisplay, Vivid or BetterTouchTool to enable that HDR mode for the whole displayI have BTT and can't figure this out. Can you say how? Thanks.replyanamexis 18 hours ago | root | parent | next [\u2013]It's called \"Toggle super brightness for current display,\" according to the changelog (3.756): https://updates.folivora.ai/bettertouchtool_release_notes.ht...replyjaimehrubiks 19 hours ago | root | parent | prev | next [\u2013]I cannot find it either. I'll wait a bit if he replies, else I'll download one of the others but I love BTT because it bundles most of the things I know in a single app. I see the potential of this for working outside, specially now that summer is coming :)replyfifafu 16 hours ago | root | parent | prev | next [\u2013]Search the actions for \u201esuper brightness\u201c. However I did not spend too much time implementing this and this feature actually only uses public API in BTT. BetterDisplay (I\u2019m not involved in that despite the name) is my favorite app to do this (and much more), but Lunar also seems great. I think they work on a lower level - BTT basically just creates a HDR enabled transparent window which fills the entire display(s), thereby activating the super brightness everywhere on the screenreplyn3storm 18 hours ago | parent | prev | next [\u2013]This can become an accesibility nightmare :_/replymort96 17 hours ago | parent | prev | next [\u2013]What I don't get about Apple is, why aren't things like that available out of the box? Why do I need a third party tool to be able to increase my display brightness?replymusicale 13 hours ago | root | parent | next [\u2013]Battery life \u2013 a core feature and differentiator for Apple devices which is also highlighted in their marketing.If they unlocked it then the devices wouldn't meet their advertised battery life, and Apple could be subject to heavy penalties from the FTC. Also one of the purported advantages of Apple Silicon would be obscured.The CPU is also underclocked for absolute power savings, lower heat dissipation, and better compute per watt efficiency.Headphone and speaker volume are also limited below what is possible in order to prevent hearing damage and speaker damage.replymort96 12 hours ago | root | parent | next [\u2013]Honestly, that's pure bullshit. For my machine, Apple advertises 12 hours of web browsing. They document (with a footnote) that those 12 hours were measured with the brightness set to medium (8 steps). With the current available display options, you can set your display brightness to a value which makes it impossible to reach the advertised battery life, and the FTC obviously doesn't come after Apple for that. Why the hell would the FTC start coming after Apple if you could get even less battery time by cranking up the brightness even more?I don't know what you mean by the CPU being \"underclocked\", these machines don't have Intel CPUs with a base clock you can compare against. But the MacBook Pro machines are certainly capable of draining their batteries very quickly if you try, and they get quite hot in the process. Because having a whole lot of CPU and GPU compute is the very point of the MacBook Pro line. The battery life while spinning all GPU and CPU cores at 100% is certainly not a core differentiator which is highlighted in their marketing.replywwalexander 17 hours ago | root | parent | prev | next [\u2013]For the same reason your car has a rev limiter.replymadethemcry 22 hours ago | prev | next [\u2013]That's interesting. Every normal white feels pale and boring after seeing that white. I'm not writing this as a joke, everything that is supposed to be white looks like a greyish, washed out white right now.replyCthulhu_ 20 hours ago | parent | next [\u2013]Most colours are relative to their surroundings anyway; there's a few cool optical illusions that illustrate that like https://www.washingtonpost.com/news/wonk/wp/2015/02/27/12-fa...replyxp84 17 hours ago | parent | prev | next [\u2013]Same thing happens watching something projected on a screen (in a room of non-zero brightness). Imagine a zebra or a checkerboard projected on a white screen. When you're looking at the projected image, you really see \"black and white\" but only when you consciously reconsider and look at the margins of the screen outside the image, you realize the zebra or checkerboard's \"black\" parts are actually full white (possibly even a touch brighter due to LCD) and the white parts are just ultra bright white. It always amazes me that we're naturally able to perceive any (reasonable) relative gradient of dark to bright as though it's true black-to-white.replygrishka 20 hours ago | parent | prev | next [\u2013]But eventually your vision readjusts to the old levels and normal whites feel normal again.replysweetjuly 13 hours ago | root | parent | next [\u2013]but some part of you will always be chasing that high again, always needing that hit, that rushreplyfeintruled 20 hours ago | parent | prev | next [\u2013]Yes! I thought the Safari example above had just dimmed the rest of the screen as a trick, but then I closed the window and it was all still grey and dull. I have been spoiled now I have glimpsed 'true white'.replyJLCarveth 21 hours ago | parent | prev | next [\u2013]At first I thought the page was playing a trick by applying a tint to the page.replykurishutofu 21 hours ago | prev | next [\u2013]I tried experimenting with something similar using three.js a while ago [0] when I saw the other post [1]. The trick was hiding the HDR video in the page, allowing other elements to surpass a brightness level of 1. I have a post-processing shader that divides all brightness values by 2, along with CSS that applies filter: brightness(2). Finally a shader material on one of the spheres multiplies its fragment output by a value above one.[0] https://r3f-hdr.netlify.app[1] https://kidi.ng/wanna-see-a-whiter-white/replyuser_7832 22 hours ago | prev | next [\u2013]If anyone\u2019s curious how this physically (appears to) work - it seems it boosts the backlight and applies a dark overlay on the rest of the page. If the brightness on my iPad Air 4 is max, the white stays the same. If it\u2019s lower then it\u2019s \u201csuper white\u201d.replytreecle 22 hours ago | parent | next [\u2013]It's an HDR video and it's the OS that controls how it looks. If your display is HDR (OLED and Mini LED Apple displays) it will brighten just the video pixels. If your display is not HDR, it will do as you describe (across the whole OS, not just the webpage). Apple calls this EDR I believe.replyuser_7832 21 hours ago | root | parent | next [\u2013]Yeah I think you're right. I thought that my iPad had an HDR display (it's got a wide gamut coverage and can show HDR content) but apparently it's not \"Apple HDR\".Though - lots of non-OLED/miniLED displays have HDR... why does Apple have a different standard?replytreecle 21 hours ago | root | parent | next [\u2013]I believe they do it this way because they want to be able to display HDR and SDR content on the same display at the same time.replyrxyz 16 hours ago | root | parent | prev | next [\u2013]Because their non-hdr displays don't have enough peak brightness for good looking HDR.replyilyt 18 hours ago | root | parent | prev | next [\u2013]That's.... terrible way to handle it. Make whole screen look worsereplyricardobeat 17 hours ago | root | parent | next [\u2013]Are you on a MBP? It doesn\u2019t actually make anything worse, it raises the screen brightness at the same time as it dims the content, so it stays at the same absolute brightness and the transition is imperceptible.replyfifafu 22 hours ago | parent | prev | next [\u2013]It does not apply any dark overlay, at least not on HDR screens, it just looks like it does due to the big difference in brightness.I would assume the iPad Air 4 doesn't have a HDR screen, but maybe I'm wrong. On HDR capable screens you should still see a huge difference even on maximum brightness.replytreecle 21 hours ago | root | parent | next [\u2013]Apple does some trickery to display HDR on non HDR Apple displays. Look up EDR.replyAlanYx 20 hours ago | root | parent | next [\u2013]On some post-2017 non-HDR Apple displays, the OS actually dynamically adjusts the screen's gamma curve to simulate this effect. On machines that support this, if you hold Option down while opening the Display control panel, there's an additional option that appears to enable/disable this.replykrona 22 hours ago | parent | prev | next [\u2013]On my 2023 M2 MBP (XDR display) there is still a large difference between the white on the page at max brightness and the video whiteness. In fact the video gets even brighter at max brightness.replyuser_7832 21 hours ago | root | parent | next [\u2013]Could you try manually increasing the brightness to maximum if that's possible? (Though I suspect with local dimming zones and the language on the Apple website the whole display can't be kept at \"max\" brightness, as a local zone can still be brighter.)replyNikkiA 21 hours ago | parent | prev | next [\u2013]About 15 years ago I had a monitor from Philips that had the ability to software control the backlight at a reasonably fine level (I don't recall if it was actually the same resolution as the display, but it pretended you could choose it per pixel).The primary use case of the included software was to highlight the focused window and dim the rest. Other than that, it was more of a chore to use.replydylan604 19 hours ago | parent | prev | next [\u2013]On my MBP in Safari, this adjustment is done in several steps. When the page loads, the image is there but takes 6-8 increases before stabilizing at the final brightness. It's not a smooth ramp at all.replyko27 17 hours ago | prev | next [\u2013]> Unfortunately, this color cannot be represented with CSS colors (rgb(999,999,999) doesn\u2019t work)I think this is wrong. Browsers already support HDR with other color space like this one https://oklch.com/replydevit 18 hours ago | prev | next [\u2013]There probably should be a toggle or a separate brightness control (i.e. \"maximum brightness\" and \"default brightness\") if something like that is exposed to the web.Letting content be arbitrarily much brighter than what the user expects seems a terrible design, easily exploitable by advertisers and all sorts of attention thieves.replyRalfp 18 hours ago | prev | next [\u2013]So this is the trick that Facebook was using to make videos stand out more in the feed? I've noticed they are darkening rest of the page a little, but didn't know this also bumps backlighting brightness.replyknolan 18 hours ago | parent | next [\u2013]This will happen with any video recorded in HDR, on iPhone 12 and later for example. You see it all the time on Reddit, YouTube etc.It probably isn\u2019t anything Facebook is doing.replyash 23 hours ago | prev | next [\u2013]Duplicate: https://news.ycombinator.com/item?id=36384625replylondons_explore 22 hours ago | prev | next [\u2013]How many hours can I display super-white brightness before burnin starts to happen?If this is just designed for bright brief 'glints' of light in videos, I can imagine the screen itself might be being substantially overdriven, and there might well be damage if you did this for hours on end (or even maybe minutes, especially on a hot day).OLED isn't known for longevity when displaying bright colors...replyfifafu 22 hours ago | parent | next [\u2013]people have been using Vivid or BetterDisplay for months on macOS to ramp up the brightness (they are awesome for working outdoors) - so far I don't think any damages have been reported. The Apple displays automatically seem to dim when they overheatreplyeyesee 21 hours ago | root | parent | next [\u2013]MacBooks don\u2019t ship with OLED displays, only micro LED (\u201cXDR\u201d) displays which shouldn\u2019t be susceptible. iPhones however do have OLED displays.replylondons_explore 21 hours ago | root | parent | next [\u2013]> The 12.9-inch Liquid Retina XDR display has an IPS LCD panel supporting a resolution of 2732x2048 pixels for a total of 5.6 million pixels with 264 pixels per inch. [1]\"Liquid Retina XDR\" is just a high end LCD. Micro LED isn't yet on anything they sell. All apple devices phone-size and smaller currently use OLED, and everything larger uses IPS LCD.[1]: https://support.apple.com/en-gb/HT212527replygirvo 21 hours ago | root | parent | next [\u2013]I thought Apple also used Mini-LED though, with quite a large amount of zones in comparison to most competitors? At least in the 14 and 16\u201d pro MacBooks?Yes I\u2019m aware that Mini and Micro-LED are different technologyreplylondons_explore 20 hours ago | root | parent | next [\u2013]yes, but the main benefit of more zones is that you can make the display thinner (it is hard to design very thin light pipes for zonal displays). You also get some power saving benefits, and less 'halo-ing' when displaying very bright and very dark things next to eachother.replylondons_explore 21 hours ago | root | parent | prev | next [\u2013]All of Apples laptop range is still LCD right, not OLED?replyjimnotgym 10 hours ago | root | parent | next [\u2013]No some newer ones are mini LEDreplyPeterStuer 3 hours ago | prev | next [\u2013]For those confused by the title: it's not \"brighter than white\", it's the same white but with the HDR pushing more nits.replypindab0ter 2 hours ago | parent | next [\u2013]I don't see how that's a useful clarification. It's very clearly brighter than 100% SDR white.replyunsupp0rted 18 hours ago | prev | next [\u2013]I'm listening to Spotify on a stock M1 MacBook Pro 16\", and when I opened this Github demo the speakers started crackling, until I closed the tab.replyalbertzeyer 18 hours ago | prev | next [\u2013]I was wondering whether such HDR videos are also on YouTube. Yes, same super white video: https://www.youtube.com/watch?v=n__QY3e55LcSome other random HDR video: https://www.youtube.com/watch?v=72_rYwzLhjkreplyZamicol 19 hours ago | prev | next [\u2013]Browser support for \"HDR\" features is already in CSS. Using a video over idiomatic CSS is a hack. https://developer.chrome.com/articles/high-definition-css-co...replyash 3 hours ago | parent | next [\u2013]How can it be used? What browsers and what operating systems support it?replyepgui 10 hours ago | prev | next [\u2013]Am I the only person wondering whether at some point screen brightness becomes a health concern? Looking at this for more than a second almost hurts, and now it's as if I've stared into a flashlight and it takes minutes for my eyes to get back to normal.(not an eye specialist, but I do have a biomedical background and I don't think it's completely ridiculous to be somewhat concerned...)replymkmk 10 hours ago | parent | next [\u2013]I\u2019ve wondered the same on the constant IR flashes from FaceID.It doesn\u2019t seem like it should be a problem compared to going outside in the sun, but it is a bit unsettling to see how bright and frequent the flashes are (a Nest camera pointed at a dark room shows them clearly).replyepgui 10 hours ago | root | parent | next [\u2013]It\u2019s not necessarily the perceived brightness that I am worried about. Illuminated surroundings outdoors on a sunny day are obviously perceived to be brighter than any screen\u2026 But here we have a flickering light source that is microscopically very heterogeneous. LED emission spectra (even for very expensive ones) are extremely non-uniform and \u201cpeaky\u201d at narrow bands of wavelengths.I\u2019m not as worried about it as I am of lasers, but it\u2019s a similar idea, maybe (very loosely speaking).replytda 22 hours ago | prev | next [\u2013]doesn't work with firefox on my M1 MBP, but it does in safari.replymindbyte 21 hours ago | prev | next [\u2013]A simple website blowing my mind is very rare. And now every other white seems so pale & washed out.replygeuis 10 hours ago | prev | next [\u2013]I have an LG 4k HDR display (that really isn't very good in the HDR department), and this works on the display too. I have to set the display to HDR mode, which then displays all colors really weird and mute, but the super bright trick does indeed work.replywolverine876 15 hours ago | prev | next [\u2013]The number of colors defined by software will not represent exactly all the colors a system can display; the software colors are arbitrarily defined and at discrete levels.I expect that any system can physically output colors in addition to the software definitions, including in-between colors and especially, assuming the engineers left margin for error, colors at the extremes of brightness and hue could be generated.replyjoshstrange 19 hours ago | prev | next [\u2013]I'm on an iPhone 14 Pro Max in Safari without power saving turned on but those 2 QR codes look identical to me. I tried turning off dark mode but they still look the same. What am I doing wrong here?replyleidenfrost 19 hours ago | parent | next [\u2013]You need to have HDR enabled on your devicereplyash 2 hours ago | root | parent | next [\u2013]How do you enable HDR on iPhone for viewing?replyaeyes 18 hours ago | parent | prev | next [\u2013]Enable HDR, disable power saving and don't have screen brightness on max.replysequoia 18 hours ago | prev | next [\u2013]Is the \"normal white\" toned down to reduce eye-strain?replyvardump 15 hours ago | prev | next [\u2013]Apparently a tab with that hack consumes a ton of energy, even when hidden. Weird. Google Chrome, MBP M2 16\".replyoniony 18 hours ago | prev | next [\u2013]Persil are going to have a field day with this.replycs02rm0 22 hours ago | prev | next [\u2013]It displays as normal on my M1 MBP and then gradually ramps up over a second or two to the superbright white. Seems better suited to a static white area than a few video frames? At least on this device, I guess others might be more responsive to it?replyCthulhu_ 20 hours ago | parent | next [\u2013]That's probably your OS gradually adjusting instead of just jumping to the desired brightness as encoded in the video.replyTepix 22 hours ago | prev | next [\u2013]Note it will not show superbright colors if you are in low power mode on the iPhone.replylayer8 22 hours ago | parent | next [\u2013]Also not with Smart Invert Colors turned on (where it\u2019s still white, but remains at normal brightness).replyorangepanda 23 hours ago | prev | next [\u2013]What privileges a superwhite QR code has versus a regular white?replyReason077 22 hours ago | parent | next [\u2013]A brighter display with higher contrast is more likely to work reliably with certain barcode scanners. For example, I sometimes struggle to get my Tesco Clubcard QR code (edit: actually an Aztec code) to scan on their old (red laser) self-checkout machines with my iPhone, but on my Apple Watch it displays brighter and seems to work more reliably.replyda768 23 hours ago | parent | prev | next [\u2013]Works better in plain sunlight and with barcode scanners I assume. Pretty much any apps showing a barcode sets your phone on full brightness.replylondons_explore 23 hours ago | root | parent | next [\u2013]Any QR reader in 2023 that cannot read a black and white QR code on a regular brightness phone screen should probably be considered faulty...replyesrauch 23 hours ago | root | parent | next [\u2013]Haven't flown in a few years but I flew a ton 2017-2020 and airlines could never scan the barcode on an eTicket if I forgot to turn the brightness up all the way. I'm guessing it is more because of glare than contrast though if that makes you feel any better.replylondons_explore 22 hours ago | root | parent | next [\u2013]Airline barcodes aren't QR codes. They are Aztec codes or PDF417 codes, and software for reading them is generally less robust and less smart because less investment has gone into clever ways to align the codes quickly. The codes themselves also have less good alignment markers, especially for dealing with lens distortion.replyaccount42 22 hours ago | root | parent | prev | next [\u2013]Phones typically have glossy screens which can be a real problem if there are light sources in the sourrundings that can be reflected - including the scanner's own lights which, yes, should be turned off when scanning a phone barcode but this is not always the case in practice. Upping the brightness of the phone display itself mitigates this.replylondons_explore 22 hours ago | root | parent | next [\u2013]Phone screens are awfully flat though - which means glare from a light is typically a small area. QR codes are robust to small areas being missing. In fact, a typical QR code is readable with about 20% of the code missing.replymichaelt 21 hours ago | root | parent | next [\u2013]Not really - a large thing being reflected means a large area of glare.A lot of scanning takes place out of doors - think of someone scanning e-tickets at a music festival or sports event.replysdflhasjd 22 hours ago | root | parent | prev | next [\u2013]I've been doing some work in this space (eticketing) and it's not as easy as you think. OK, 95% of the time it works, but that's not good enough and when you move into the \"real world\" - cracked screens, smudges, glare, dirty sensors it gets worse. The failure cases waste a huge amount of time.replychakintosh 22 hours ago | root | parent | prev | next [\u2013]Exactly. QR code scanning abilities of new smartphones has gotten so incredibly quick at scanning the code. I barely point my iPhone 14 Pro's camera at the code and it already scans and displays the information. Even with the motion blur on the camera as I'm pointing the phone at the code.I think the focus should be more on the ability to interpret damaged codes via ML capabilities as most QR codes are outdoors.replyjwestbury 22 hours ago | root | parent | next [\u2013]Scanning an on-screen QR code is usually done with barcode scanners, not with phones. Typically, barcode scanners are doing the decoding on-device, and passing out keyboard data (their drivers usually identify them as USB HID). Unfortunately, most scanners don't have loads of onboard processing power, though ability to deal with damaged linear barcodes has been quite good for a long time. Damaged 2D barcodes are harder to deal with, and while ML may be able to solve that, it would require a paradigm shift in how barcode scanners operate -- and essentially bifurcate the market, as some scanners would simply no longer be compatible with embedded systems, and would require specialised software to function (which isn't really ideal; most users of barcode scanners want them to be plug-and-play).(My first post-university job was doing tech support for a company which sold point-of-sale equipment. I'd say about 50% of my job was helping people with barcode scanners.)replyhnbad 23 hours ago | root | parent | prev | next [\u2013]Every time I'm queuing somewhere that requires scanning QR codes or digital barcodes, I see some people going through the back-and-forth of scans not working and the person fiddling with their brightness settings or fighting auto-brightness. I'd say there's a legitimate use case for this right there.replyyootyootr 22 hours ago | parent | prev | next [\u2013]It makes regular white look dim/dull so adverts programmed to take advantage of the superwhite will attract more attention.replySnoozus 23 hours ago | prev | next [\u2013]The reason you can only do this with video is to avoid burn in?replyelvisds 23 hours ago | parent | next [\u2013]This was explained in demo website [1] that was submitted earlier today [2]> It works by displaying a color whose brightness is way outside the standard range of sRGB color space.> Unfortunately, this color cannot be represented with CSS colors (rgb(999,999,999) doesn\u2019t work) or any of the widely-supported image formats. However, an HDR video can represent this color.[1] https://notes.dt.in.th/HDRQRCode [2] https://news.ycombinator.com/item?id=36384625replyJasper_ 22 hours ago | root | parent | next [\u2013]In theory, you could do this with the rec2020 color space included in CSS Color 4. https://www.w3.org/TR/css-color-4/#predefined-display-p3replyjojobas 22 hours ago | root | parent | prev | next [\u2013]Using any monitor in a room with a window on a sunny day you'll want to ramp up brightness way above the sRGB-mandated 80cd/sqm. True sRGB colours only make sense for print designers anyway, anybody else adjusts their white brightness as needed. This is a gimmick and Apple's usual pioneering stunt.replyprogbits 23 hours ago | parent | prev | next [\u2013]No standardized support in HTML/CSS. Video containers can store the color profile information.replycubefox 22 hours ago | root | parent | next [\u2013]I assume CSS colors are defined for an sRGB color space and the iPhone displays are calibrated, so they show a smaller color range than they could. OLED is able to go well beyond sRGB.replyadrian_b 22 hours ago | root | parent | next [\u2013]Not only OLED, but most good LCD monitors support the Display P3 color space (derived from DCI P3), with 30 bit per pixel, which is much larger than sRGB.While Safari has been supporting Display P3 for several years, all major browsers support since the beginning of the year CSS Color Module Level 4, which includes support for Display P3 (for all the other even larger supported color spaces there are no monitors or TV sets of reasonable price that can display them).Most monitors come from the factory configured to display 24-bit sRGB colors, so the user must change the default settings to be able to display 30-bit Display P3 colors.However the color space only affects the maximum saturation of the colors, not the maximum brightness of the colors, which is determined by the brightness setting of the monitor.For a white brighter than the brightest normal white, it is not enough to have a monitor with a wide color gamut, it must also be HDR capable and the software must support HDR too.replycubefox 20 hours ago | root | parent | next [\u2013]I don't quite get the saturation/brightness difference here. Isn't $fff \"whiter\" than $eee? Where whiter means brighter? Then why couldn't the whitest color be even whiter, and hence brighter, in a larger color space? (I assume the answer is that a larger color space contains colors that are \"redder\" or \"yellower\", but not \"whiter\", for some reason.)replyadrian_b 19 hours ago | root | parent | next [\u2013]A color is determined by 3 numbers, when those 3 numbers are chosen to be hue, saturation and brightness, that means that this color cannot be distinguished from a mixture of monochromatic light with a standard kind of white light (e.g. D65), both lights having certain radiant powers (per solid angle and/or per area).In that case, the hue is the frequency or wavelength of the monochromatic light, the saturation is the ratio between the (weighted by the eye sensitivity curve) radiant powers of the monochromatic light and of the total light (so a completely saturated color is monochromatic, without any added white), while the brightness is the total radiant power (weighted by the eye sensitivity curve).The red, green and blue pixel components of a display do not emit monochromatic light, unless the display is a laser projector. For any non-laser display the light emitted by a pixel component is equivalent with a mixture of a completely saturated color of the same hue with white light. This is especially obvious in the red of a sRGB display, which is much less saturated than a color that could be accepted as pure red. When two red colors have the same hue, the more saturated color is \"redder\", as it has less white and more red.\"Redder\" is ambiguous as it can be also used when comparing colors of the same saturation and brightness but of different hues, e.g. when comparing two reddish oranges, where the redder has more red and less green, or when comparing two reddish purples, where the redder has more red and less blue. Any color display can show any color hue, the difference between the displayed color spaces is only in the maximum displayed saturation, which cannot exceed the saturation of the individual R, G and B pixel components.When you specify RGB colors, you specify only relative values, i.e. by ratios vs. the RGB components of the brightest white light that the monitor has been configured to emit through its \"brightness\" setting. For instance, on a certain monitor setting the \"brightness\" to \"75%\" might mean that the brightest white has a luminance of 100 cd/m^2, while on another monitor model the same \"75%\" might correspond to a very different luminance of white, but in any case changing the setting will increase or decrease the brightness of all displayed colors, without changing their hues or saturations.Both \"#f0f0f0\" and \"#e0e0e0\" are white and normally one would say that the former is brighter than the latter.If on the axis of black to white one labels as \"white\" the brightest white and as \"gray\" the white of half brightness, then one may say that the former is \"whiter\" than the latter.However this usage is not recommended, because it is ambiguous. It is much more common to say that a color is \"whiter\" with the intention to say that it is less saturated, i.e. it is a mixture of more white with less of a monochromatic color, than to use \"white\" as a synonym of \"bright\".replyzokier 22 hours ago | root | parent | prev | next [\u2013]CSS color() can do Display-P3 and Rec2020replyornornor 18 hours ago | prev | next [\u2013]> please don\u2019t abuse thisI\u2019m preparing popcorn.replyB1FF_PSUVM 17 hours ago | parent | next [\u2013](marquee) popcorn (\\marquee)replydkasper 16 hours ago | prev | next [\u2013]This hurts my eyeballs.replybaxtr 12 hours ago | parent | next [\u2013]Same for me. Can't look at it for longreplykilljoywashere 18 hours ago | prev | next [\u2013]Uh, that just made my screens darker, so kindly never do that again. That's worse than a blink tag.[edit] not sure why the downvotes. I have an XDR screen and it's quite distracting when my entire screen goes dark for some local video to get bright.replyneoyagami 14 hours ago | prev | next [\u2013]this makes my second screen to panic and glitch some windows when this is presentreplydonatj 22 hours ago | prev | next [\u2013]I\u2019ve never understood why HDR is implemented in the strange way it is, outside the normal color spectrum. Really seems like #fff should just be the whitest white the display can physically support.replyJasper_ 22 hours ago | parent | next [\u2013]That would lead to inconsistent brightness across devices -- white on a 2000 nits HDR TV is extremely bright, you likely don't want that as a paper white background. Standardizing on some good enough value of nits (usually between 80 and 200) for \"SDR\" peak white tends to work better.replyCyberDildonics 14 hours ago | root | parent | next [\u2013]That would lead to inconsistent brightness across devicesPhone brightness is going to be adjusted automatically or manually when someone goes from somewhere bright to somewhere dark, not to mention that different phones already have different brightness.I'm pretty sure there is already \"inconsistent brightness across devices\" so I don't know what this actually means.replytiagod 7 hours ago | root | parent | next [\u2013]The point of HDR is to have the ability to go outside the \"normal range\". this way regular SDR movie won't look dim (or weird), but one that was graded for HDR can go outside the bounds when it's wanted (and still play decently on a regular SDR display)replyaltairprime 19 hours ago | parent | prev | next [\u2013]That\u2019s the color model Firefox used for the past two decades, which is why HN\u2019s orange header is always eye-blastingly neon on Firefox macOS compared to other browsers.Thankfully the CSS Color v3 spec mandates a default of sRGB for #fff codes, so Firefox will have to get with the times someday soon and stop poking my eyes out with HDR-expanded sRGB colors (by default).replyorangepurple 18 hours ago | parent | prev | next [\u2013]Nearly all displays are incapable of accurately representing colors at high brightness (L=100) levels. You can clearly see the sRGB gamut is quite narrow within the CIELAB color space at (L=100). The CIELAB color space represents all colors perceptible by humans in daylight. https://en.wikipedia.org/wiki/File:SRGB_gamut_within_CIELAB_...It is clear that higher brightness levels worsen color volume (cubes at the bottom of page 4) https://www.researchgate.net/publication/323273667_Color_vol...replymarginalia_nu 22 hours ago | parent | prev | next [\u2013]RGB isn't an appropriate model for brightness is why.replydonatj 21 hours ago | root | parent | next [\u2013]Why is that? Isn\u2019t brightness just an R G and B value by physical definition?replybrookst 21 hours ago | root | parent | next [\u2013]Our eyes are non-linear across the spectrum. #A0004F is much darker than #00FF00.Thats why we have the CIE colorspace and HSB/HSL/etc. All of those use a nonlinear geometry that better maps to the way we perceive colors.replydonatj 17 hours ago | root | parent | next [\u2013]Scaling the colors for our eyes seems like something for some sort of psychometrics library and we'd be better served having direct access the just the r g and b brights control of the individual pixelsreplytedunangst 15 hours ago | root | parent | next [\u2013]Why are we better served by wasting bits?replyvirtualritz 22 hours ago | parent | prev | next [\u2013]That would be terrible for color reproduction which is what most use cases are about.HDR is still a niche application outside of games and VFX.replyd_tr 18 hours ago | parent | prev | next [\u2013]Supposedly every piece of content comes tagged with some color space, so the correct thing to do is to display whatever white #fff maps to in that color space. Allowing the user to override that could be an option.replyneoyagami 15 hours ago | prev | next [\u2013]it makes my second screen to panic and glitch some windows when this is presentreplyfoolrush 18 hours ago | prev | next [\u2013]\u201cBrighter-than-white\u201d\u2026Gilchrist enters the chat.http://wexler.free.fr/library/files/gilchrist%20(1999)%20an%...replyxwdv 19 hours ago | prev | next [\u2013]Is there an opposite to this such as \u201csuperblack\u201d?replykristofferR 19 hours ago | parent | next [\u2013]No, pixels can't be more off than completely off.That's why OLEDs have \"perfect blacks\", it's not an exaggeration, it's physically impossible to create better blacks.replyd_tr 18 hours ago | root | parent | next [\u2013]Unless we want to be pedantic and take the ambient light reflected off the screen into account. Old CRT monitors, for example, reflected a lot of ambient light and looked grey when turned off in a lit room. And even when operating in a dark room there would be some diffusion from the lit areas of the screen, which is why they did not actually have \"perfect blacks\" like many believe. If you had a very dark scene you would have less internal diffusion too though so it would indeed look very good. Something similar could be happening in OLEDs, but yeah IMO OLEDs have perfect blacks for all intents and purposes.replyshove 18 hours ago | prev | next [\u2013]No one made the Spinal Tap joke? I\u2019m disappointed in all of you. (Again)replymsla 12 hours ago | prev [\u2013]Why don't they just make #FFFFFF brighter?replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Apple devices can display brighter-than-white colors using HDR technology.\n- Some users find the effect of HDR content on Apple devices to be unpleasant, while others enjoy the realistic sunlit effect it creates.\n- Users have experienced difficulties with skin tones and management of dynamic range when using the iPhone camera in bright settings."
  },
  {
    "id": 36388828,
    "timestamp": 1687165097,
    "title": "We tried to book a train ticket & ended up with a 245k records data breach",
    "url": "https://zerforschung.org/posts/freundschaftspass-en/",
    "hn_url": "http://news.ycombinator.com/item?id=36388828",
    "content": "How we tried to book a train ticket and ended up with a databreach with 245,000 recordsDieser Artikel ist auch auf deutsch erschienenTo celebrate Franco-German friendship, German Transport Minister Wissing and his French counterpart Beaune came up with something special: 30,000 free Interrail tickets per country for travel in Germany and France for young adults between 18 and 27. Codename: \u201cPasse France Allemagne\u201dHowever, many things went wrong when the Interrail passes were distributed. In the following, we want to take you on a journey through the stages of the not-so-well-implemented ticket and show you how you could still get a pass after registration ended.And while we\u2019re on the tracks, we\u2019ll also have a look at a security breach in a similar project at the EU level. Implemented by the same agency - which left the data of about 245,000 registrations almost unprotected on the web.Please stand clear of the doors \u2013 we\u2019re departing! \ud83d\ude84\ud83d\ude83\ud83d\ude83\ud83d\ude83\ud83d\ude83Station 1: The town of DDoSingOn June 12 at 10am, the registration for the \u201cPasse France Allemagne\u201d opened - and the servers were immediately overloaded.The rush could have been expected, as the tickets were distributed on a \u201cfirst come, first served\u201d basis. Anyone who wanted a chance to get one of the coveted tickets had to be quick. As a result, tens of thousands of users in Germany and France tried to get their hands on the pass at 10 a.m. on the dot.Of course, setting up a system that can handle such a rush is not trivial. But it is possible! Organizers of concerts by popular bands and artists, for example, are familiar with similar situations - and can usually cope with them.Something like this has to be well thought out, prepared and tested. If they had done that properly, they would have realized: Damn, we can\u2019t handle so many people - and (hopefully) would have thought of another solution.After all, there are alternatives to the \u201cfirst come, first served\u201d principle: Instead of putting everything on a moment\u2019s notice, you can spread the sale over several points in time. This not only spreads out the rush - it also allows people to attend who don\u2019t have time on a Monday at 10 a.m., for example because they\u2019re at school, in training or at university.In such a system, you can also give interested people a few days to register and then distribute the passes at random amongst all who registered.Because one thing is for certain: Overloaded servers are not a sign of the success of a campaign, but a sign of failure in planning.Station 2: Password-Reset-CityWhile we were still angry about this failure, we got a hint by mail: This wasn\u2019t the only part of the platform that wasn\u2019t properly thought out and tested.How did this show? The lucky few who were able to secure a pass had to provide an email address and come up with a password. With that combination, they could then log into the site to check for the status of their \u201cPasse France Allemagne\u201d.It\u2019s easy to forget or mistype a password - especially when things need to happen quickly. Fortunately, the \u201cForgot Password?\u201d function was invented for such situations. And as usual, you receive an e-mail from the \u201cPasse France Allemagne\u201d with a link to reset your password. What is unusual, however, is that this link leads to an error page:If we take a closer look at the error page, we see that it links to a Vercel project that is not registered. Vercel is a cloud provider where you can host your own web applications. These applications are then available under <project name>.vercel.app. However, if no project is registered with the corresponding name, as is the case here, basically anyone can hijack the address.Such an hijacked application address could then for example be used for targeted phishing. Fortunately, it was an honest person who found the mistake, registered the application themself and told the responsible organisations and us. Thus, the link to the \u201cForgot Password?\u201d function only leads to a harmless test page. This page still does not belong to the organizers of the \u201cPasse France Allemagne\u201d, but at least to a friendly person and not to criminals.Station 3: Free-tickets-for-all-avenueThis brings us to the next stop on our journey: The ordering process.Since only 30,000 passes were supposed to be given out, the ordering system has to pay close attention to that. As soon as the 30,001st person tries to register, the system would have to show an error and no longer allow registration. But that was not the case here: Instead, once the first 30,000 people started filling out the booking form, the form was simply removed from the page. The website now displays a notice stating that all passes have been allocated.But just because you started the form, doesn\u2019t mean you already finished it. So a backdoor was built in: Anyone who had already started an order process but not completed it received an e-mail with a special link. This link could then be used to complete the order process.There was just only one tiny problem: You could generate these codes yourself with a simple command - even if you didn\u2019t start the order process in time.The video shows a new code being generated on the left side of the screen. This code is then copied into the browser on the right, where the order form for the \u201cPasse France Allemagne\u201d then opens. The form is filled out and at the end you see a confirmation that the registration was successful.A few hours later, our new Interrail pass arrived by e-mail.So the whole thing is kind of like having a treasure in your house - and hanging a sign on the front door saying \u201cAll the gold has already been distributed\u201d. But everyone can find the key to the back door.If you want to know exactly when everything happened, you can find a timeline at the end of the article.Detour via ReportingOf course, we wanted to quickly report the problems to the appropriate parties so they could be fixed. Unfortunately, this turned out to be more difficult than expected: a security contact was nowhere to be found.1In the end we contacted all places that seemed like they could change something: The advertising agency that\u2019s named in the imprint of the \u201cPasse France Allemagne\u201d. The IT security team of the german railway (Deutsche Bahn). The contact address from the imprint of the german page of the \u201cPasse France Allemagne\u201d. The contact address of the german Ministry of Transportation. And finally, the german federal computer emergency response team (CERT-Bund). We sent a description of the issue to all these and asked for confirmation of receipt and next steps.Unfortunately, we did not receive an answer at first. Howevery one day later, the extra access was no longer available. Instead, the backdoor page also showed a notice that all passes were already taken.Figuratively speaking, the back door was also provided with a notice: \u201cUnfortunately, all gold bars have already been distributed.\u201dStation 4: Where the wild APIs live.But noone checked if the door were actually locked. As it turns out: By sending api request directly to the order backend, we could keep generating passes.Only a few minutes later, the pass arrived by email:Station 5: \u201cUnfortunately, we came to an unscheduled stop\u2026\u201dAt this point we didn\u2019t know what to do anymore.So we put our heads together and tried to find a solution. We knew that all people in charge were just pointing at the press office of Eurail (i.e. the company behind all Interrail passes, which was probably charged with issuing these passes as well) - and Eurail claimed there was no loophole. So we sent our description to the press office again.Thereupon - and after insistent requests through other channels - we finally got an answer. Eurail thanked us for pointing out the issue, told us that they had fixed the issue and were now looking for the falsely issued passports.How Eurail fixed the issueSo now both the front door and the back door were finally locked.Just to emphasize that again: Publishing an API doc is not a security breach! On the contrary, it should actually be good practice to do so. Because a software does not become more secure by the fact that nobody knows exactly how it works.Your front door doesn\u2019t become more secure by painting it in exactly the same color as your house wall, so it\u2019s not so easy to spot. It may look funny and confuse you at first - but the door will be more secure if you lock it.So lock it, don\u2019t hide it! \ud83e\udd1dStation 6: \u201cData leak of 245,000 records today from track 2 - directly opposite.\u201dBut these doors to more passes weren\u2019t the only ones we found unlocked. We also took a look at the neighboring projects and found another offer similar to the \u201cPasse France Allemange\u201d: DiscoverEU.This program emerged back in 2018 as a result of the #FreeInterrail campaign. 18-year-olds can sign up to win a free Interrail pass and travel Europe.While the \u201cPasse France Allemagne\u201d was dreamed up by the German and French governments, DiscoverEU was the created by the European Commission. However, both offers have one crucial thing in common: This project was implemented by the same agencies - MCI together with Caracal.Information about DiscoverEU is available at start-discover.eu. But we didn\u2019t spend much time looking around. Because through the Certificate Transparency Logs we quickly discovered the domain dashboard.start-discover.eu. The domain sounds exciting - but we are only greeted with a login screen.On a technical level, the site is built similarly to the \u201cPasse France Allemagne\u201d - and thus we already knew the API to create an account. On the off chance we created an account \u2013 and to our surprise we were able to successfully log in with it. With great concern for the work that now followed, we noticed:245,971 registrations for DiscoverEU were retrievable on the dashboard. The following data was displayed:Name of the personE-mail addressCountry of originState of their registrationType of ticketInterrail order numberAll of this personal data was virtually open on the Internet and could be retrieved with little effort or prior knowledge.Technical detailsAnother stop in reportingCompletely shocked by this databreach, we again unpacked our digital stationery and reported very quickly: To the agency, the contact address of Start-Discover.eu, the CERT-EU and the office of the European Commission responsible for DiscoverEU. Fortunately, we already knew the right contact person in the press office of Eurail, who then quickly passed the whole thing on to the responsible people.They reacted quickly and after less than an hour the vulnerabilty was closed - they simply deactivated the registration of new account. They also informed us that that an external security test would now be carried out and the necessary steps defined in the GDPR would be taken.Last Stop: ConclusionThe \u201cPasse France Allemagne\u201d and DiscoverEU are actually great ideas: Free train tickets for young people to get to know their neighboring countries. But instead of simply enabling as many people as possible to enjoy a nice summer vacation and make new acquaintances, this unfortunately once again resulted in a digital disaster.The \u201cPasse France Allemagne\u201d is also a good example of the power imbalances created by bad digital solutions: Those who have enough technical knowledge still get a passport even if all of them are actually already taken. Everyone else goes away empty-handed or even loses their accounts.Such half-baked solutions would already be insufficient to sell a few concert tickets. But if a federal ministry gives away train tickets, special attention has to be paid that everything is well tested.It gets even worse when we can not only create new passports, but even retrieve more than 245,000 records from the DiscoverEU program. We say times and times again: If a website is mature enough to process data, it must also be mature enough to keep it to itself.It\u2019s staggering that such careless work was done here - and that this is apparently just accepted.We can only hope that we really were the first to discover this vulnerability - and that the data has not already been taken away by less benevolent actors.TimelineAll times are CEST.2023-06-12 10:00 \u2013 Start of sale.2023-06-12 \u2013 Unregistered Vercel application is found and reported.2023-06-13 23:00 \u2013 We find the backdoor.2023-06-14 01:15 \u2013 Report to DB-CSIRT, MCI, contact named in the imprint, BMDV, CERT-Bund2023-06-14 22:00 \u2013 Our test addresses receive interrail passes2023-06-15 00:00 \u2013 We send the report to Eurail as well2023-06-15 10:00 \u2013 We notice that the backdoor is also replaced by the \u201csold out\u201d-notice2023-06-15 13:00-14:00 \u2013 We can verify that passes are still registerable via API2023-06-15 17:30 \u2013 Reply from Eurail that the loopholes have been closed2023-06-16 13:00 \u2013 We discover the databreach at Start-Discover.eu2023-06-16 15:30 \u2013 We report the databreach2023-06-16 18:50 \u2013 MCI replies to us, thanks for the report, at 16:20 the vulnerability had been closed\ud83e\udd1dWe shared our findings with Eva Wolfangel for ZEIT Online. You can find her article here (in German).Originally we wanted to publish our findings on Friday, 16th of june 2023. But because we discovered the databreach at DiscoverEU, reported them and waited until everything was secured, we postponed the publication.Documenting, reporting and publishing such issues takes nerves and time - which we would also much rather spend looking out the train window. We do all this on a voluntary basis and in our spare time.If you want to support us, you can find possibilities here: https://zerforschung.org/unterstuetzen/We recommend that every company publishes a security contact and, if necessary, further information on reporting security vulnerabilities on all websites. The easiest way to do this is via a security.txt. This would help (not only) us to be able to report such issues without detours to the right places in companies. \u21a9\ufe0e2023-06-19#databreach #security #trains",
    "summary": "- The article discusses a data breach that occurred during the distribution of free train tickets for young adults in Germany and France. The breach resulted in the exposure of approximately 245,000 registration records.\n- The registration process for the tickets encountered various problems, including overloaded servers and a flawed password reset function. Additionally, a backdoor was discovered that allowed individuals to generate codes and complete the ticket ordering process even after it had officially closed.\n- The data breach also extended to a similar program called DiscoverEU, which offers free Interrail passes for young Europeans. Over 245,000 registration records from DiscoverEU were found to be easily accessible on the internet. The issue has since been addressed and further security measures are being implemented.",
    "hn_title": "We tried to book a train ticket and ended up with a 245k records data breach",
    "original_title": "We tried to book a train ticket and ended up with a 245k records data breach",
    "score": 292,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginWe tried to book a train ticket and ended up with a 245k records data breach (zerforschung.org)292 points by mrzool 1 day ago | hide | past | favorite | 75 comments2rsf 21 hours ago | next [\u2013]> This project was implemented by the same agencies - MCI together with Caracal.I suspect that this is the root cause of this and for many other systems failing. When a project is created by the lowest bidder, as a one time effort with fluffy requirements why would they invest in proper architecture, planning or testing? Why would they invest in securing resources when they are paid anyway?replynness 20 hours ago | parent | next [\u2013]It's a fallacy to believe that all projects are just sold to the lowest bidder.There are probably a dozen reasons why something like this might have occurred, and not giving the vendors a free pass, but assuming that a more expensive vendor would do a better job with security and reviews is just as likely to be a mistaken belief.If a project is too expensive for a client to do well, they should not be doing that work in the first place.replyboringg 20 hours ago | root | parent | next [\u2013]Its actually probably more of a situation that a client cant discern quality. Its impossible to tell if the most expensive or least expensive if the best option. How does a non technical/semi technical actually grade this stuff appropriately?replydavemp 20 hours ago | root | parent | next [\u2013]This is a problem traditionally solved by the professional engineering licensing system. Most engineering curriculum in the USA involve an Engineering Ethics course that goes over such issues.We're quite far from implementing such a system for software \"engineers\".replycomboy 19 hours ago | root | parent | next [\u2013]Just hire pentesters along with the development team. Make sure they are not affiliates. You can put in contract that as long as security issues are present they need to fix them before getting paid, which seems like a reasonable expectation. Even the best make mistakes, so let's at least leave those which are not trivial.We don't trust building ethics, independent inspector comes and checks if everything is as it should be before it can be used by the public.replyotherme123 18 hours ago | root | parent | next [\u2013]This can't happen more often than not, because the client has no idea what a pentest is. They don't even know what a is XSS means, or even API. These things are negotiated by people that only can see the frontend, and if it looks great, _snappy_, _flashy_, with random animations and following current trends it's OK. The contractors know and can easily detect this, so they focus on frontend and don't waste their time in behind-the-scenes polish. You don't polish the security or find a costly query that could bring the site to their knees, but you add a scroll-spy that brings some images from nowhere.replycomboy 18 hours ago | root | parent | next [\u2013]Investors have no idea what thickness the wall should be in their building either.Clients not being experts at the job they are getting somebody else to do is not a new pattern. So while some trust is required, it's best if you can get somebody else to verify.I've seen a few smart clients over the years which when faced with some excuses from a software house hired another one to give them opinion about the codebase and capabilities. It seems pretty intuitive. It seems like a money well spent.replyJohnFen 16 hours ago | root | parent | next [\u2013]> Clients not being experts at the job they are getting somebody else to do is not a new patternIn fact, a whole lot of the time, the entire reason someone hires a professional is precisely because they themselves aren't experts.replyrobocat 14 hours ago | root | parent | prev | next [\u2013]> solved by the professional engineering licensing systemGood narrative: but certification and guilds do not solve the problem.replydavemp 14 hours ago | root | parent | next [\u2013]No, but actual liability does.replynradov 19 hours ago | root | parent | prev | next [\u2013]Hire independent consultants with software experience to work on an hourly basis to write portions of the RFP and evaluate the responses. There is a niche industry of experts who help buyers with this stuff. Of course, if the customer is totally ignorant about software then it can be difficult to know which consultants to trust but generally they can ask around industry circles and check references.replydacryn 19 hours ago | root | parent | prev | next [\u2013]If my company is anything like the others, its very rarely the lowest bidder who wins at all.Usually there is some sort of RFI process, where they ask a few companies 'hey can you build this for us? What are the types of services you would propose'. The list of companies here is already more or less pre-existing partnerships, or ex colleagues or...(it mostly always contains Microsoft, and your boss is ex Accenture, so it involves Accenture, and for good measure to seem like they are open to other options they invite Deloitte and some other players as well, sometimes even IBM has joined the club again)Then they decide who they deem thrustworthy, and you end up with Microsoft and (insert boss previous employer). So not only do you not get the lowest bidder, you can some veeerrryyy generic company that doesn't care and just sends juniors to solve it. This process is called the RFP. And it typically is far from neutralreplysfn42 17 hours ago | root | parent | next [\u2013]Sounds spot on to me. Just let a bunch of unsupervised kids loose to screw shit up as much as they like, then after they've done that for a couple years you call them seniors and charge double for their time.replyTwirrim 19 hours ago | root | parent | prev | next [\u2013]The way that government procurement often works, it isn't the cheap contractors you end up with, either. Not many companies have the resources or willingness to stick through the long time it often takes to go through the bidding process, nor all of the pre-qualification requirements.The whole process is long and drawn out with all sorts of checks and balances built in to it, based on previous learnings from previous contracts that have failed in various ways (particularly if it has embarrassed an elected figure). No doubt on the back of this failure, there will be more conditions added to the bidding process, making it even harder to find a vendor.Usually by the time the entire process is done, there's not many vendors left and in my experience they're usually not the ones you'd actually want to do the work if you had a choice, just often ones that'll at least get you something.reply2rsf 20 hours ago | root | parent | prev | next [\u2013]You are right of course, I was just theatrically Exaggerating.If we assume there is no corruption involved, then lack of competence from the project management side can fail such a project. For this example it could be failing to mention or think about the extra load on the first hours in the SOAreplynewsclues 19 hours ago | root | parent | prev | next [\u2013]Yeah, there is probably some minority/equity politics that is giving the contracts to totally incompetent people that have the right gender claims or have the right skin colour.replypjc50 21 hours ago | parent | prev | next [\u2013]This is where quite a lot of people would insert a rant about \"state capacity\": the ability of the state to actually do things it wants and intends to do. Which requires people to do those things, trained with appropriate skills.The peak of \"state capacity\" was undoubtedly WW2, when governments bypassed market mechanisms and became command economies. Out of necessity - war is the one venture in which failed state capacity can end the state itself, and the personal privileges of those running it and the elite around them.It's not a coincidence that the centralized socialist institutions of the UK, the NHS and state education, date from that period. Heck, the state commissioned the invention and building of cutting-edge computer technology! But since that no longer matters, there's little to no will to build state capacity in computing.replyafavour 21 hours ago | root | parent | next [\u2013]It\u2019s not WW2 mobilisation but I\u2019ve always thought the UK\u2019s Government Digital Service is a wonderful example of what government can achieve in tech:https://www.gov.uk/government/organisations/government-digit...As I understand it they\u2019re effectively a central dev shop for other government agencies. It\u2019s worth their time investing in good practises because they\u2019re going to use them over and over again. And from the user perspective you get a very consistent, reliable set of tools for interacting with government. A win win in my book.replyFourthProtocol 19 hours ago | root | parent | next [\u2013]GDS did a great job building gov.uk, but everythig else they touched was an abject disaster. From the Diabetes project at the NHS, the fiasco that was the Office of the Public Guardian, the even bigger fiasco that happened at Border Force, the NHS, DWP...Sure, when they were building web sites they delivered stellar stuff. Agile, break things and all that. But when you had real complexity they just... couldn't...The Government Gateway is a prime example - single citizen login for ALL government services. It ran well, super robust and mature enough to have ironed out virtually all issues.Then GDS decided that because the Government Gateway was based on a Microsoft stack, it needed to be re-done. The tech lead didn't understand the concept of Identity Federation, let alone SAML tokens, and that you just! can't! do secure code using agile (2-week sprint no good for meaningful security testing...).I spent two long years at GDS banging my head against a wall. And then I left. And unsurprisingly the Microsoft-based Government gateway was never replaced, still going strong.replyfranga2000 20 hours ago | root | parent | prev | next [\u2013]This is something we desperately need more of in other countries. We've found something like a dozen breaches of similar severity in the last 6 years and they all came from systems developed through public tenders by companies that either aggressively under-priced or used other (legal or illegal) dirty tactics to win them. The very few things that were developed in-house have proven to be far more reliable and secure, not to mention developing them was far cheaper and the UX is better and more consistent between them.replysfn42 16 hours ago | root | parent | next [\u2013]As a software \"consultant\" (meaning i do what everyone else does but get paid more) the consultancy business is fucking bullshit. Just padding resumes and counting billable hours, nobody gives a fuck about quality or maintainability or anything. Just money.replyveltas 20 hours ago | root | parent | prev | next [\u2013]It's a shame that (from my perspective anyway) a lot of that state capacity seems to have degassed for the NHS. Right now it's impossible to get a doctor's appointment where I live. If you call any local surgery within a split second of 8:00am then you have a very low chance of getting an appointment, every time I've tried the line's busy or I'm number 60 in the queue and after a 40 minute wait all the appointments are gone. They are also fully booked for advance appointments every day (they limit the time in advance you can book, probably to prevent having e.g. a 2 year wait time on appointments, because it 'looks better' if nobody can go than if there's literally a 2 year advance appointment wait).The only way to see a doctor is to go to A&E, or convince NHS 111 to give you an \"emergency appointment\" of some kind. All of this drains emergency resources and are not an option due to the time investment for average people with precarious employment.Unfortunately many people won't believe these facts, because depending on which area you live in there's always plentiful appointments, in advance or on the day! Where I last lived, getting an appointment was easy, the difference in outcomes based on how wealthy or urban your area is leaves a bad taste in my mouth.Briefly, we had a new surgery open that offered more appointments, at more convenient times of day, and we could actually see the doctor. All the other GP's started losing their patients to them. Then within a few months they were shut down by the local NHS trust, under multiple investigations (one of these investigations was regarding an offensive Facebook post by the surgery's chief, I kid you not). They then later reopened with normal appointment times and no free spaces, like all the other surgeries.I agree with what you say re the command economy of WW2 allowing the creation of the NHS. But it was not this country that founded the NHS: it is some ancient, lost nation that seems utterly alien to me today. I don't believe we could achieve even a small version of what WW2 Britain did anymore, if our survival depended on it. I cite the UK's response to COVID19 as evidence.replyctippett 20 hours ago | root | parent | next [\u2013]This mirrors my exact experience with the NHS \u2013 the futile 8am phone calls, to relying on NHS 111 for any hope at getting medical attention.Coming from Australia and previously NZ, the healthcare system here seems barbaric.replyrobocat 14 hours ago | root | parent | next [\u2013]New Zealand isn\u2019t much better - last time I tried to book a doctor, they said they had n appointment free at the end of the month.Meanwhile, if you actually need to go to the doctor for anything urgent, it is often best to go direct to A&E.replyScarblac 20 hours ago | root | parent | prev | next [\u2013]Same as in the US -- right wing politicians who believe the state is bad at doing things remove the funding that allowed the state to do it. Then when that means those government provided services become worse, they use that to show that the state is bad at doing those things, and therefore even less funding should go there, et cetera.replyjodrellblank 19 hours ago | root | parent | prev | next [\u2013]> \"I cite the UK's response to COVID19 as evidence.\"Here's some things I would love to see an alternate-history version of:1) Vitamin D has some involvement in the immune system. The US Department of Health[1] says \"Your immune system needs vitamin D to fight off invading bacteria and viruses.\". Harvard School of Publich Health says[2] \"laboratory studies show that vitamin D can reduce cancer cell growth, help control infections and reduce inflammation\", \"a large meta-analysis of individual participant data indicated that daily or weekly vitamin D supplementation lowers risk of acute respiratory infections\"2) The UK NHS page on Vitamin D does not mention immune function at all[3] but does strongly imply that everyone in the UK is deficient during winter when it recommends \"since it's difficult for people to get enough vitamin D from food alone, everyone (including pregnant and breastfeeding women) should consider taking a daily supplement containing 10 micrograms of vitamin D during the autumn and winter.\"3) The Harvard page linked earlier says a randomized controlled trial with 340 Japanese school children given either Vitamin D or a placebo, the Vitamin D group had 40% fewer flu infections in winter.What do we know about COVID? It's an infection, it's expected to be more prominent in winter, some of the knock-on effects are to do with inflammation of tissues all around the body - lung, heart, brain, nerves.So, would anything have played out differently if during the early days of no vaccines and no effective treatment, the NHS had leaned hard into Vitamin D testing and supplementation? Anyone presenting to a doctor or hospital or care home of any kind for any medical problem gets a routine blood test for VitD levels as well, any blood tests happening for anything also test for VitD levels, high risk people picked out specifically and called for testing, generic supplments freely available from GPs and pharmacies even without prescription using the NHS's large scale buying and negotiating power, anyone found deficient given a strong dose or large injection to start with, public relations push for the public to supplement or get checked, kept up all through the year leading into the first winter. Would it have made a difference to the ease of it spreading, to the amount of dead people, to the amount of hospitalized people, to the amount of long-term complications, would it have flattened the curve, helped the NHS, reduced or eliminated the lockdowns?I am indoors most of the time, but I eat a lot of the recommended vitamin D foods - dairy, eggs, red meat, sardine, mackerel - and still had 'severely deficient' levels the first time I paid for my own test out of my own curiosity[4], and then 'insufficient' the next time.That seems like the kind of thing a \"national health service\" would be well placed and incentivised to do, whereas a for-profit expensive-pills-and-surgery-and-insurance-profit \"service\" isn't.[1] https://ods.od.nih.gov/factsheets/VitaminD-Consumer/[2] https://www.hsph.harvard.edu/nutritionsource/vitamin-d/ (click to expand the 'immune function' section)[3] https://www.nhs.uk/conditions/vitamins-and-minerals/vitamin-...[4] (by a UK NHS lab, one which doesn't sell supplements so it's not incentivised to report misleadingly low figures)replymarcosdumay 19 hours ago | root | parent | prev | next [\u2013]Governments do in-house development just as well as any company with a large development department.Governments also do development off-shoring just as well as any company with a large off-shored project.The kind of project people are talking about here never works. It doesn't matter who is doing it.replyJoeri 19 hours ago | parent | prev | next [\u2013]As this is a government contract, and there are strict public transparency rules on government contracts, I went digging.Here's the call for tenders: https://etendering.ted.europa.eu/cft/cft-display.html?cftId=...And here's the award: https://ted.europa.eu/udl?uri=TED:NOTICE:120998-2022:TEXT:EN...Some interesting things:1. This is a broad framework contract for marketing, for the eye-watering amount of 300 million euro. The title is \"Belgium-Brussels: Framework Service Contract for the Organisation of Large-scale Travels of Participants in the Context of Erasmus+/DiscoverEU\"The reason they do these kinds of framework contracts is because the legally required tendering procedures surrounding government contracts are so onerous that it's better to do a broad contract once and bundle a lot of projects inside of them, than to have one contract per project.2. The executing party (Caracal) is nowhere to be seen, instead the contract is awarded to EURail and MCI.EURail is the intermediary I suspect, responsible for navigating the wild world of government contracting, and MCI is a marketing agency. They have no doubt built up years of expertise in how to successfully navigate these kinds of tendering procedures, and they probably are not the lowest bidder. Caracal is no doubt subcontracted by MCI, but as MCI is a private company we cannot see how much they were paid or how they were selected. So much for transparency.In my own experience in government contracting, price is a factor but usually not the largest factor. There's a large set of requirements (which you can read through if you follow the first link), and the ability to prove that you will be able to meet them is mostly what determines who wins the contract. However, because it is so difficult to know how to do that, only a few parties will have submitted a tender, and the best of a poor batch may still not be very good.Personally I think this kind of public procurement legislation is well intended but ultimately flawed. It does not result in lower costs, faster turnaround, better transparency, or overall better government. I'm in favor of transparency rules, but they need to be a lot more thorough and they need to cover subcontracting as well. I'm against public tendering legislation, as I think it prevents the government from being efficient.(By the way, how awful is that public tendering website? It's like a flashback to 2003. No doubt built under one of those big framework contracts.)replygermanier 16 hours ago | root | parent | next [\u2013]The linked tender is mostly not about marketing. They tendered basically a pretty large travel agency. Eurail is surely not just a intermediary here. They are in the business of selling train passes and operate lots of the services tendered for decades (e.g. an online train booking platform)replySpringtime 22 hours ago | prev | next [\u2013]Just wanted to mention the photos of the model trains used to show the progression of events was charming.replyTazeTSchnitzel 22 hours ago | prev | next [\u2013]Zerforschung (research to the point of destruction) is a perfect name for a site with this post.replylbriner 20 hours ago | prev | next [\u2013]Lots of people complaining about state-run projects or suppliers who do stuff on the cheap but I think the simple fact is that in most people's minds, buying a \"IT system\" is like buying a car except that the car is built from scratch each time even though the customer wants off-the-shelf prices.How many applications do we create that all do exactly the same thing? Payments, customer details, tasks, shopping baskets, items for sale etc. and how many times have we rebuilt all of that from the ground up with all the risks? Even if we know what we are doing, it is easy enough to forget something, for someone who didn't know what they were doing to build part of it, to cost enormous money to plumb together a tonne of bespoke parts.I think the solution is 1) We need much better regulation of who has the relevant skills to do work to the required standard, we still allow untrained and unqualified people to build banking apps etc. 2) We need to create something that allows us to possibly certify implementations of standard functionality so they can be used to create standard applications, just like Peugeot might buy engines from Toyota that they know already work.We talk about freedom of thought and creativity but the price of reliable and trustworthy software is probably only going to come by establishing a much higher level of quality - hopefully minus some of the BS you get with some accreditations.replyBoxFour 19 hours ago | parent | next [\u2013]It appears as though you are merely rehashing the realm of SAAS and, to compound matters, the labyrinthine government contracting procedure.The market already boasts software solutions that are more or less ready-made, precisely catering to your described needs, particularly concerning areas like payments.However, governmental entities abstain from employing such software, as their provider selection process deliberately embraces a convoluted nature to sidestep any hint of impropriety.Thus, the government contracting industry flourishes\u2014a cohort proficient in maneuvering through the intricate channels of governmental procurement. Most private enterprises that excel in providing top-tier services opt out of engaging in this government contracting labyrinth because it's not worth the headache. It involves an assortment of antiquated procedures and certifications that the private sector seldom finds worthwhile to partake in, as they exclusively pertain to the realm of government contracting and are often accompanied by a disheartening degree of bureaucratic rigmarole.Deciphering a pathway towards resolving this predicament would transcend the mere realm of overhauling regulations; rather, it necessitates the overhaul of modern bureaucracy and solving the arduous struggle government faces to keep pace with fast-evolving fields like technology.Basically: Good luck with that!replyHPsquared 20 hours ago | parent | prev | next [\u2013]It's a lot like building houses. Lots of manual processing and making the standard formula fit the specific application.replypedybr2 20 hours ago | prev | next [\u2013]Made me think about this podcast I listened to the other day: https://www.nytimes.com/2023/06/06/opinion/ezra-klein-podcas...In it Jennifer Pahlka, a high ranking US government official who worked on heathcare.gov and other digital government projects, talks about her book that is about why most of these projects go as poorly as they do. Quite illuminating...replyjs2 19 hours ago | parent | next [\u2013]Came here to mention that episode. Agree that it was very insightful. Transcript:https://pastebin.com/Lg7zfHd9replytheironhammer 14 hours ago | root | parent | next [\u2013]Thank you!replyluplex 22 hours ago | prev | next [\u2013]The sad thing is I don't see the public sector getting any better at this anytime soon.replymysterydip 21 hours ago | parent | next [\u2013]Worse, there's laws in place (in the name of \"cost savings\") that need changed before any policy improvement could be made. A group can't just decide \"it would be better if we didn't use the lowest bidder.\" There's legal repercussions and losers can sue (leading to more expense than if they just went with them in the first place). It's truly terrible.replyHPsquared 20 hours ago | root | parent | next [\u2013]They don't just accept the lowest bid. It's the lowest bid that complies with the requirements. You can tighten up the requirements and conditions.replySoftTalker 19 hours ago | root | parent | next [\u2013]You can also consider the demonstrated qualifications/competency of the bidder. I could submit a low bid for a project and if I have no history of ever completing similar projects my bid can be rejected on that basis.However (at least in the few governemnt bids I've been involved in) if the low bidder does not get the award, they can challenge the award and often do. Then the government has to defend their decision and give the reasons the low bid was disqualified.replyoverboard2 19 hours ago | root | parent | prev | next [\u2013]It's supposed to limit corruption by making it harder to give contracts as political favorsreplymysterydip 16 hours ago | root | parent | next [\u2013]I know there's good intentions, but the system has been gamed/exploited for a long time with no real way to defend thanks to the legal implications.replyDamonHD 20 hours ago | root | parent | prev | next [\u2013]I can tell you for the procurement that I had anything to do with at a UK school, though for projects above a certain cost floor three or more quotes were required, I advised NOT taking the cheapest nor most expensive bids unless there was a specific good reason...replythrowaway049 21 hours ago | parent | prev | next [\u2013]Don't see the private sector doing so either. It's a scramble every year for Glastonbury festival tickets.replyskeeter2020 21 hours ago | parent | prev | next [\u2013]Also sad: the (terrible) response times and efforts detailed here exceed what many organizations, private and public, have shown in their situations.reply_-____-_ 16 hours ago | prev | next [\u2013]You're lucky that you got in touch with someone who understood the report and didn't refer you to the polizei, like happened in Hungary a few years ago when a 17 year old kid figured out he could change the price of a ticket in his browser dev tools.replybanDeveloper 19 hours ago | prev | next [\u2013]How were they able to generate / obtain the `apiKey` shown in the technical details in part 6?replyLegogris 19 hours ago | parent | next [\u2013]Without looking closer I just assumed it was trivially extracted from the frontend.replybanDeveloper 17 hours ago | root | parent | next [\u2013]You're right, it's the Supabase's anonymous API key they send with each anonymous request.replyred_admiral 20 hours ago | prev | next [\u2013]It's a good thing the people writing software for the railway interlockings, unlike these guys, are held to SIL4 standards.replybatch12 19 hours ago | prev | next [\u2013]The age limits on the free tickets stand out to me. I guess it's all ROI forecasting. Either older folks are assumed to have exposure to the target country already, can afford the travel, or aren't worth it?replypetrut_m 19 hours ago | prev | next [\u2013]Although the faults are massive, the time to fix was relatively short, scroll to the bottom and look at the timeline... this is not a fault sitting in the open for months after reporting.replynelox 22 hours ago | prev | next [\u2013]You scored the mystery ridereplyDamonHD 1 day ago | prev | next [\u2013]Completely shambolic schoolboy errors!replyTade0 21 hours ago | parent | next [\u2013]Large, state-backed, entities that predate the internet seem to get away with stuff of this kind all the time.The other day my mother tried to buy a train ticket. The payment went through, but something went wrong on the site and the ticket was not issued.If this were just some e-commerce site, the payment provider would have had their head on a spike.In this case she had to go through the usual return process.replyDougBTX 21 hours ago | root | parent | next [\u2013]In this case, it looks like the private company which did the work is also getting away with it...https://www.caracal.agency/en/projects/discover-eureplypixel3234 22 hours ago | prev [\u2013]Stuff like this is why I prefer to take a bus in Germany.Trains are overbooked with free tickets and promotions (free pass for entire summer for 50 euro). While underlying infrastructure is not ready for such load. It leads to delays and mistakes. Plus railway stations in Germany look like homeless shelters!On other side Germany has excellent motorway network. Flixbus is very cheap, quite comfortable, goes all the way to airport, and always on time!replysc11 21 hours ago | parent | next [\u2013]Let's see. Cologne to Berlin takes ~4:40 hours by train. Flixbus takes 9-10 hours, not counting the time it takes to get to their departure station which would involve a train journey as it's not actually in the city centre.Flixbus is 50\u20ac cheaper when traveling that route tomorrow but that's about all it has going for it.replyHPsquared 20 hours ago | root | parent | next [\u2013]1 hour by plane (+ time hanging around the airport, but train/bus has the same issue there)replycom 20 hours ago | root | parent | next [\u2013]Train and bus normally have that \u201c10-20 minutes ahead\u201d planning to be at the station.Planes? At least an hour, and if you cut into that, and the queues or security theatre more mind boggling than normal, you\u2019ve missed your flights.Eurostar is similar to airports, so I\u2019m glowering at them too!replysc11 19 hours ago | root | parent | next [\u2013]Even if you aren't at the airport that early, it still takes you an hour to get from the airport to the city centre in Berlin, and about half an hour to get to the airport from Cologne's city centre. That's by train, by car it takes even longer.replyHPsquared 19 hours ago | root | parent | next [\u2013]It depends if you're going from \"centre to centre\" or \"somewhere near Cologne to somewhere near Berlin\".replysc11 19 hours ago | root | parent | next [\u2013]Sure but assuming you're traveling from centre to centre, which is where population densities are highest, is a sound assumption. Otherwise you can always find spots where getting to the airport, train station, flixbus stop or whatever takes extremely long with one mode of transport over the other.Doesn't distract from the point that long distances busses are very much not an alternative to rail (or planes for that matter) unless price is the deciding factor. And even the latter is questionable in many cases thanks to the 49\u20ac ticket.replyFreak_NL 19 hours ago | root | parent | prev | next [\u2013]Also no time at all when arriving. Getting out of a train and the station rarely takes more than five minutes; usually less.replyklausa 21 hours ago | parent | prev [\u2013]The 50eur pass doesn't include the trains you'd use for the trips you'd use Flixbus for.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Project implemented by agencies MCI and Caracal, often associated with system failures due to lack of proper architecture, planning, and testing.\n- Price is not the sole determinant of project success; more expensive vendors may not necessarily provide better security or quality.\n- Clients may struggle to discern quality and appropriately grade technical projects, highlighting the need for professional engineering licensing or independent inspectors."
  },
  {
    "id": 36388894,
    "timestamp": 1687165604,
    "title": "TypeScript 5.2's new keyword: 'Using'",
    "url": "https://www.totaltypescript.com/typescript-5-2-new-keyword-using",
    "hn_url": "http://news.ycombinator.com/item?id=36388894",
    "content": "The Empty Object Type in TypeScriptLearn why {} in TypeScript doesn't represent an empty object, and how to use the Record type to represent an empty object.Matt Pocock",
    "summary": "- This post explains why {} in TypeScript does not actually represent an empty object.\n- It introduces the use of the Record type to represent an empty object in TypeScript.\n- The author, Matt Pocock, provides insights on how to use the Record type effectively.",
    "hn_title": "TypeScript 5.2's new keyword: 'Using'",
    "original_title": "TypeScript 5.2's new keyword: 'Using'",
    "score": 289,
    "hn_content": "- TypeScript 5.2 introduces a new keyword called 'Using' for resource management.\n- 'Using' allows for the automatic disposal of resources when they go out of scope.\n- It is based on a TC39 proposal that has reached stage 3, indicating that it is coming to JavaScript.\n- The 'Using' keyword simplifies the process of resource management and makes code more maintainable.\n- It is similar to the 'using' statement in C# and provides a uniform interface for implementing resource disposal.\n- The syntax of 'using' in TypeScript may not be exactly the same as in C#.\n- 'Using' is primarily useful for managing resources like file handles, database connections, and more.\n- 'Using' helps to avoid resource leaks and makes code more reliable.\n- With 'Using', developers have more control over resource management and can avoid manual disposal of resources.\n- TypeScript's implementation of 'Using' is not specific to TypeScript and will eventually be available in JavaScript.- JavaScript is introducing a new feature called \"using\" which allows for automatic resource disposal.\n- The \"using\" keyword combines the opening and closing of a resource into one statement, similar to the concept of RAII in C++.\n- The feature is currently in stage 3 for JavaScript and will likely be refined before official release.\n- TypeScript is including this feature in its next official release, version 5.2.\n- The \"using\" syntax can be used with objects that have a \"dispose\" method.\n- Symbols are used to avoid conflicts with existing methods or properties called \"dispose.\"\n- The feature is similar to Python's \"with\" statement and C#'s \"using\" statement.\n- It is useful for managing resources such as files or connections, ensuring they are properly disposed of when no longer needed.\n- The syntax allows for both synchronous and asynchronous operations, using \"await\" when necessary.\n- Destructuring is not currently supported in the \"using\" statement.\n- TypeScript is introducing the feature with a polyfill, making it accessible to a wider range of users.\n- The proposed syntax has caused some confusion and discussions about alternative syntax options.\n- The feature is seen as a useful addition to TypeScript for backend development, automating input validation and improving code readability.\n- The design choices for the syntax, such as the order of \"await\" and \"using,\" have been a topic of discussion among developers.",
    "hn_summary": "- TypeScript 5.2 introduces a new keyword called 'Using' for resource management.\n- 'Using' simplifies the process of resource management and helps avoid leaks, making code more reliable.\n- The feature is similar to C#'s 'using' statement and will eventually be available in JavaScript."
  },
  {
    "id": 36388821,
    "timestamp": 1687165042,
    "title": "Show HN: Slint \u2013 A declarative UI toolkit for embedded & desktop",
    "url": "https://slint.dev",
    "hn_url": "http://news.ycombinator.com/item?id=36388821",
    "content": "SlintUse CasesDocsPricingCommunityBlogThemeGet StartedDesign to DeployBuild native user interfaces with SlintSee DemosProudly Open SourceDesignDevelopDeployLive-PreviewComplete your UI design through quick iterations using Live-Preview. Tweak everything, like colors, animations, geometries, or text. and verify the changes instantly.ResponsiveBuild a responsive UI from a single design. Target different screen resolution and sizes with flexible layouts.Multi-language supportRedesign your UI while using the same code base and engineering team. Integrate your business logic implemented in C++, JavaScript, or Rust.Browse Slint APIsCode Editors IntegrationsKeep using your favourite IDE. Choose between our generic language server and VS Code extension: Enjoy code completion, live-preview, code navigation, diagnostics, and syntax highlighting.Use with Visual Studio CodeUse with other EditorsNativeEnjoy flexibility that only a native application can provide: Access full operating system APIs, utilize all CPU and GPU cores, connect to any peripheral. Slint compiles your UI design to machine code.LightweightAchieve low footprint and minimal resource consumption. The Slint runtime fits in less than 300KiB RAM, features a reactive property system, and is built with Rust.Watch VideoPerformantDeliver a smooth user experience. Slint uses the optimal graphics rendering method: GPU accelerated, DMA2D, Framebuffer, or Linebuffer.Global Partner NetworkSilver Member of Rust FoundationTestimonials\"I'm building \"Here Now\", an open source virtual office-space for helping remote teams feel more connected. Slint has made it easier to prototype and share ideas directly in Slint markup including breakpoints, animations, and layout logic much more easily than the equivalent in CSS/HTML/JS.\"Cole LawrenceCOLELAWRENCE.COM\"Slint is replacing our HMI written in Qt QML and has reduced or eliminated bugs, improved performance, and made it much easier to rapidly design the UI.\"Luke JonesJasic Technology Europe\u201cWe used Slint on QNX with Rust bindings. A definite recommendation for embedded graphical applications, especially if you are using Rust.\u201dJonathan PallantSenior EngineerFerrous Systems\u201cSlint empowers even smaller companies to access the top-notch UI development experience previously exclusive to larger players.\u201dDawid S\u0142iwaCEOOakDevices\"Slint provides us with the flexibility and features we need, the performance is good and the service and support has been outstanding.\"Harald K.Product ManagerNordic OEMUsed byGet Started with SlintAboutUsInvestorsEventsCareersImprint & PrivacyExploreDocumentationDemosVideosService PartnersSilicon PartnersResourcesPricingCommunityBlogWeeklySupported BoardsCompareQtLVGLFlutterElectronOther Rust GUI ToolkitsContact usEmailChatGitHubCopyright \u00a9 2023 SixtyFPS GmbH",
    "summary": "- Slint is a declarative UI toolkit that allows developers to build native user interfaces for embedded systems and desktop applications.\n- It provides features like Live-Preview, allowing developers to quickly iterate and make changes to their UI design.\n- Slint is lightweight and performant, with a small memory footprint, optimal graphics rendering methods, and support for multiple programming languages.",
    "hn_title": "Show HN: Slint \u2013 A declarative UI toolkit for embedded and desktop",
    "original_title": "Show HN: Slint \u2013 A declarative UI toolkit for embedded and desktop",
    "score": 269,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginShow HN: Slint \u2013 A declarative UI toolkit for embedded and desktop (slint.dev)269 points by ogoffart 1 day ago | hide | past | favorite | 140 commentsSlint is a declarative GUI toolkit written primarily in Rust, with API support for multiple programming languages such as C++ and JavaScript. It is designed for desktop and embedded usage.The Slint website has just been redesigned. And we added a new Royalty-Free License option besides GPL and commercialdavidhyde 22 hours ago | next [\u2013]I just spent the last two weeks choosing an embedded ui framework and then building out a little demo to test out its capabilities. I settled on Slint and have been very impressed with the design of the domain specific language they came up with. It\u2019s small, simple, powerful and easy to learn IMO. The vs code extension for real time gui feedback is amazing. I\u2019ve used swift ui before and Slint is as much of a joy to use. This is a breath of fresh air compared to Microsoft\u2019s Windows Presentation Foundation that took me years to master.My only concern is optimising for binary size. My little microcontroller only has a measly 1MB of flash storage and I\u2019m already at 900kb on a 15 screen app with no icons or images. It would be nice to know what actually uses up all that space (as little as it is). Perhaps I need to use ui element virtualisation or something.Edit: 1kb of flash -> 1MB of flashreplyogoffart 22 hours ago | parent | next [\u2013]Thank you for your comment and feedback! I'm glad to hear that you have chosen Slint for your embedded UI framework and have been impressed with its design and ease of use. Feel free to post on Github for issues or discussions.I assume you meant 1Mb of flash and not 1kb of total flash :-) We might find some way to optimize the generated code to reduce the binary size further.replydavidhyde 22 hours ago | root | parent | next [\u2013]Yes, I meant 1MB, typo! Thanks I\u2019ll engage on GitHub. :)replynicce 22 hours ago | parent | prev | next [\u2013]Out of curiosity, 1kb of flash storage sounds quite tiny for a chip which is expected to provide GUI capabilities. Can you provide more information about the context?replydavidhyde 22 hours ago | root | parent | next [\u2013]Typo, I meant 1MB. The actual specs are 1MB flash, 564KB sram and around 500Mhz mcu speed. I\u2019m using the STM32H735G-DK development board which comes with a few bits extra to drive a 4.2\u201d lcd screen directly. There are already demos for it in the Slint repo which is why I chose that hardware in particular.replyilyt 18 hours ago | parent | prev | next [\u2013]I wish that there was some GUI lib dedicated to small embedded systems, I just want to draw some buttons and sliders without eating hundreds of flash/ram...replyogoffart 15 hours ago | root | parent | next [\u2013]This was our aim with Slintreplyvarjag 13 hours ago | root | parent | prev | next [\u2013]Some popular embedded display controllers have commands to draw primitives.replyilyt 10 hours ago | root | parent | next [\u2013]I'm essentially looking for something like imgui but with ability to draw directly to framebuffer. I looked at egui but that has same issue.replypeelz 18 hours ago | root | parent | prev | next [\u2013]Have you looked into fltk-rs?replyilyt 10 hours ago | root | parent | next [\u2013]looking at required dependencies it couldn't even run on the microcontrollerreplyMallocVoidstar 20 hours ago | parent | prev | next [\u2013]I assume you're already doing this since you're on a microcontroller, but have you set [profile.release] codegen-units = 1 lto = truereplybombela 16 hours ago | root | parent | next [\u2013]This is what I found to reduce code size the most for the 8bit AVR MCU:  [profile.release]  opt-level = \"z\"  lto = true  overflow-checks = false  codegen-units = 1  debug = falseAlso invoking cargo with: cargo build --release -Z build-std=core -Z build-std-features=panic_immediate_abortYou must then implement panic handling yourself:  pub mod std {    #[lang = \"eh_personality\"]    #[no_mangle]    pub unsafe extern \"C\" fn rust_eh_personality() -> () {}    // panic can be replaced by abort with the panic_immediate_abort feature.    // -Z build-std-features=panic_immediate_abort    // This saves space by getting rid of the unwinding.    #[panic_handler]    pub unsafe fn panic_impl(_info: &::core::panic::PanicInfo) -> ! {      loop {}    }    #[no_mangle]    pub unsafe extern \"C\" fn abort() {      // blink led      loop {}    }  }If I still have some room in the flash, I will make some led blink with a noticable pattern on abort.You can also experiment with the inlining threshold:  export RUSTFLAGS+=-C inline-threshold=0replydavidhyde 13 hours ago | root | parent | prev | next [\u2013]Yes I\u2019ve tried these things and more (like setting the opt-level to \u201cz\u201d). I don\u2019t think that this is a show stopper, probably just a recent regression that will be fixed easily since this is all open source and the GitHub issue tracking is very active I see. If I were to hazard a guess it\u2019s probably some small code gen change that results in mostly duplicated code for some ui primitive. But, it could also be me doing something stupid ;)replysamstave 18 hours ago | root | parent | prev | next [\u2013]ELI5 what this does?replyhyperbrainer 18 hours ago | root | parent | next [\u2013]The lto setting controls the -C lto flag which controls LLVM\u2019s link time optimizations. LTO can produce better optimized code, using whole-program analysis, at the cost of longer linking time.The valid options are:  false: Performs \u201cthin local LTO\u201d which performs \u201cthin\u201d LTO on the local crate only across its codegen units. No LTO is performed if codegen units is 1 or opt-level is 0.  true or \"fat\": Performs \u201cfat\u201d LTO which attempts to perform optimizations across all crates within the dependency graph.  \"thin\": Performs \u201cthin\u201d LTO. This is similar to \u201cfat\u201d, but takes substantially less time to run while still achieving performance gains similar to \u201cfat\u201d.  \"off\": Disables LTO.Basically, increases performance at cost of compile time. Great for release builds, not so much for debug builds (because most of the time that rustc spends is on linking anyways.)replyMallocVoidstar 17 hours ago | root | parent | next [\u2013]Also, codegen-units = 1 disables some compiler parallelization which means it can find more optimizations, including for size: https://nnethercote.github.io/perf-book/build-configuration....replyserial_dev 23 hours ago | prev | next [\u2013]The comparison sections are very confusing. It says you will compare your UI toolkit with, for example, Flutter and then there is no comparison there at all. I thought the site is broken on mobile, I tried different browsers, etc, because the comparison was just not there.If you don't have any real content there because you don't want to start a flame war, you could try something like what the Flutter team did, they have an \"onboarding guide\" for XYZ developers that builds upon things you already learned in other frameworks and explains the key differences without coming off as hostile towards the other frameworks.https://docs.flutter.dev/get-started/flutter-for/react-nativ...I'm a Flutter developer by day and Rust enthusiast by night, and I'm looking for a \"good enough\" Flutter alternative that uses Rust.replynu11ptr 21 hours ago | parent | next [\u2013]This is what I do:- Write the backend in Rust- Write the UI in Flutter- Tie them together using gRPC. You get an API and ability to run client and server on different machines as wellI looked at the flutter to rust bridge and started to play with it, but assuming your UI can withstand the slight overhead of gRPC I found it a simpler way to proceed (although I'm not that far in yet)replytimhh 14 hours ago | root | parent | next [\u2013]I have the same but I wouldn't recommend gRPC. It adds a ton of overhead and complexity that you don't want. Plus you have to deal with all the complexity of Protobuf and the fact that everything becomes `Option<>`.Instead I wrote my own RPC system using Serde and Bincode. Communication is over stdio, that way you can support SSH access extremely easily (like VSCode remote).Unfortunately it was for a company so the code isn't public, but there really wasn't much code to the RPC system at all since you don't need to worry about versioning, authentication, transport, etc. I write a very simple schema language, used Nom to parse it and generate Rust, Typescript and Dart code. The Rust code generation is trivial since you can just `#[derive(Serialize...]`. Typescript / Dart was a bit more complicated (you have to implement Bincode) but it's not very difficult really.By far the most complex thing is trying to integrate with SSH. If you call `ssh` directly then you end up having to parse non-machine-readable prompts and errors and so on. But if you use a proper SSH library then you end up having to implement ssh-agent, read `~/.ssh/config` etc. yourself which isn't fun either.replymonroewalker 16 hours ago | root | parent | prev | next [\u2013]I'm looking at something similar with Go as the backend. Why gRPC instead of a pipe? I started with that as well since it was really easy to set up but that comes with the caveat of requiring network permissions even though it's local to local communication.replynu11ptr 16 hours ago | root | parent | next [\u2013]Pros and cons: with a pipe it will be up to you to create your own message delimiters and protocol, so a pipe is somewhat lower level. You also limit comm. to just that machine, but that may be a pro if that is what you are looking for (omits need for things like username/password or locking to 'localhost'). Lastly, you do not gain an API for your users to consume (if that makes sense for your app - it doesn't always).replymonroewalker 7 hours ago | root | parent | next [\u2013]Protoc actually generates \"writeDelimitedMessage\" and \"readDelimitedMessage\" functions for each type. The only thing I needed to add was some buffering on the Dart side to ensure the full message was available. The Go side was able to read the stream and block until input became availablereplysatvikpendem 9 hours ago | root | parent | prev | next [\u2013]Have you seen flutter_rust_bridge? I use that to write business logic in Rust, if I need to use Rust crates. I did that recently for a CRDT library called Automerge.replydist-epoch 19 hours ago | root | parent | prev | next [\u2013]> You get an API and ability to run client and server on different machines as wellSo like a web app.replynu11ptr 16 hours ago | root | parent | next [\u2013]Yes, nothing to stop you from substituting Flutter for your frontend of choice including a web-based SPA, but then you would need the gRPC-web extension to call gRPC from a browser. The Rust Tonic crate has this available for the server side.replypzo 22 hours ago | parent | prev | next [\u2013]Not a Flutter Dev, but have you tried 'flutter_rust_bridge' [0]. Seems provide some interop between flutter and rust and looks like a popular and active project. I'm native mobile dev and just curious about this kind of interop myself. If it's seamless then looks like good balance to do mobile front-end in flutter and mobile backend in rust.[0] https://github.com/fzyzcjy/flutter_rust_bridgereplytracker1 13 hours ago | root | parent | next [\u2013]Cool... was just thinking one could use flutter's web target with tauri's interop and host model.replymadnirua 22 hours ago | parent | prev | next [\u2013]Thanks.. This is a nice reference for an onboarding guide.. We could add a similar section to our documentation page https://slint.dev/docsreplynologic01 22 hours ago | parent | prev | next [\u2013]Made a similar comment about the comparison pages, which is a simple and factual statement yet was downvoted immediately by (pressumably) people with a stake in the project. Does not reflect well...Detailed and unbiased comparisons are extremely useful but also are hard (and costly) to do. Don't promise them if you can't deliver.replymadnirua 18 hours ago | root | parent | next [\u2013]Not sure why or who downvoted.We will add more detailed comparison in our documentation. Thanks.replytimeon 18 hours ago | root | parent | prev | next [\u2013]> (pressumably)Why do you suppose this in particular?replynologic01 17 hours ago | root | parent | next [\u2013]my lack of imagination as to what might be otherwise motivating HN members to \"engage\" in this way :-)I mean I can't imagine anybody having an independent interest in the project (like I do) and actually thinking that the comparison provided was adequate.As said in my original comment, this is an area where a sense of sub-optimal status quo is palpable and people search for some sort of rationalization of the various nascent possibilities and options. Just a few days ago somebody was talking \"parallel futures\" (in mobile context) [1][1] https://news.ycombinator.com/item?id=36340925replyShadowBanThis01 9 hours ago | root | parent | next [\u2013]People downvote perfectly reasonable questions or comments here plenty often. And even more pile on if you dare to call this infantile behavior out.replybrabel 18 hours ago | parent | prev | next [\u2013]Wow, that Flutter Doc page is beautiful... and seems fairly comprehensive, kudos to them.replysamwillis 1 day ago | prev | next [\u2013]Looks really nice!I used to work in the Medical Device, Bio Tech & consumer device space (~10 years ago), good toolkits for building embedded interactive UIs were few and far between. The idea of having one codebase for both an embedded UI on a low spec device and for a mobile app is really compelling.I could see a dev kit, based on an existing dev-board and screen, running JS with bindings to this, as a great way to do rapid prototyping and development. Then being able to take that and optimise down to lower cost production hardware.As with all these toolkits, when it comes to the web and doing rendering via a HTML Canvas you end up with accessibility problems. Not though a fault of not considering it, but its not something you have to consider for an embedded device at all (or you do but it's built from scratch for that specific device). Unfortunately the APIs are just not there yet on the web platform to open up accessibly of these sort of UIs.I suspect the key selling point of the web version is building demos and simulators for design validation. It makes it super easy to distribute to projects stakeholders and focus groups. We did something in that area for an early version of the UI for the first version of the Bosch Robot Lawnmower, we had demo hardware on the mower, and a UI simulator written in JS. Both used the same custom built UI specification system, so we could iterate the design and demo it via the web, then built it into the embedded software for the hardware prototypes.replyogoffart 23 hours ago | parent | next [\u2013]Thank you for your feedback! We are aware of the issue regarding accessibility when rendering via HTML Canvas on the web platform. Our web version of Slint is mainly, as you guessed, designed for demo purposes. While it would be possible to write a backend that uses the DOM, we haven't done it yet because our focus is currently on embedded and desktop.replymwcampbell 23 hours ago | root | parent | next [\u2013]At some point I'm planning to work on a web backend for AccessKit. I know that creating a parallel DOM for canvas-based web applications doesn't solve all accessibility problems, but it'll at least be better than nothing.replyfodkodrasz 23 hours ago | prev | next [\u2013]Looks nice, but actually the per/appliance pricing could be a bit clearer, I hate the Call Us type pricing pages, that is usually where I rather look for the competition...(Just had some hardship with finally getting an atrociously expensive quote after spending workdays of providing the needed inputs to a HW vendor)replymadnirua 23 hours ago | parent | next [\u2013]Disclaimer: I work for SlintI suppose you are referring to the \"Embedded Add-on per application\" pricing? At least the non-commercial use is hopefully clear :)Yes, we need find a better way to show the pricing there since it's dependent on 2 variables - volume and type of embedded system. Maybe some sort of a online calculator?reply__forward__ 22 hours ago | root | parent | next [\u2013]What exactly is a 'user'? Literally one device running the application? In that case the pricing does seem a bit steep in a small business context.replymadnirua 22 hours ago | root | parent | next [\u2013]A user is a person (eg. a developer or a designer etc.) using Slint to build the GUI.replynicce 19 hours ago | root | parent | next [\u2013]Ahh, it makes this much more reasonable.The enterprise pricing made me thought at first that the cost is per application user. (Making enterprise lisence more appealing) \u2026 of course per end-user pricing would be way too expensive.replymadnirua 1 hour ago | root | parent | next [\u2013]We changed the term from \"User\" to \"Seat\" and added an entry in the FAQ section to avoid confusion.replyCoastalCoder 21 hours ago | root | parent | prev | next [\u2013]I always assumed \"call us\" pricing was to give a salesperson the opportunity to negotiate the best deal for the seller.replyyencabulator 17 hours ago | root | parent | next [\u2013]I always read \"call us\" as \"tell us how thick your wallet is\".replySkyPuncher 13 hours ago | root | parent | prev | next [\u2013]For early stage products, it's often more like \"we have no idea how to price this and we don't want to publish it yet\".You can basically come into those calls and name your price or walk away.replyShadowBanThis01 9 hours ago | root | parent | prev | next [\u2013]I would think they want to hear what your device sells for. If you're going to charge me $1 per unit on something that costs only three figures, I'm telling you to take a hike.Qt hasn't quite learned this. Their embedded pricing is absurd.replyfodkodrasz 21 hours ago | root | parent | prev | next [\u2013]Yes, I was thinking of that part, as this kind of stack would make a lot of sense for commercial application.Just made an appliance using a Web technology for the frontend, as the licensing situation was a nobrainer, and we could focus on delivering value instantly, using proven technology, instead of wasting time on negotiating on terms for something this basic element of the project. I mean if I need an LCD screen or a CPU, I don't negotiate, just go to a retailer, and check their prices and availability, and order&pay. No multiple round interactive sales process needed for something that is just a stock item, no customization needed, etc. Also the per-customer pricing seems potentially a bit discriminatory to me.So an online calculator would be great, it would instantly let me know if your product suits outr business.For example with the relatively low volume electric part I have mentioned above I had to make several hoops, provide lot of redundant paperwork, and nag the salespeople for weeks to provide me at least a quote the vendor could have already put on their site, as their competitors do. (This vendor has some niche differentiator aspects which made us focus on them first, not on larger volume ones). They wasted a lot of our time and our project was stalled until we got responses from them, and the prices seemed irrationally high, feeling almost as if they didn't want to sell that piece to us from the start. This is what I feel whenever I see Call Us for a quote, as this was not a unique occasion.replycaptainmuon 12 hours ago | prev | next [\u2013]I'm always happy when a new GUI toolkit comes out and Slint looks really well done, especially if you like QML. I could imagine using it if I do another embedded or kiosk application.However, I find the modern flavor of \"declarative\" is not really for me. QML, Slint, and to some degree XAML and SwiftUI are all like this. They mix widgets with layout elements and presentational elements (like rectangles). There is no separation of content and style. Another smaller problem is that the initial layout is done in the markup language, but any runtime additions have to be done in another language (if at all possible).In my ideal GUI, you could say in code \"give me a detail-item view\" or \"this pane shows the detail of the selected item from this pane\", and then have it layed out according to platform standards, possibly completely different on iOS or on Windows. But then you could go in and customize the placement and look&feel of every element with CSS if needed. Either applying platform styles (\"default inset border\") or completely custom styles.replykookamamie 23 hours ago | prev | next [\u2013]The widgets examples do not look good, unfortunately.Let me elaborate: they seem aliased (everything looks like sampled with nearest neighbor), spacing is non-uniform and overall the style chosen looks somewhat outdated.replyogoffart 23 hours ago | parent | next [\u2013]Just to clarify: are you talking about the image next to Desktop in https://slint.dev/use-cases#desktop-applications ? Indeed, we'll improve on that. Thanks for your feedback!For the story, we initially had screenshot of an actual commercial desktop app, but we were asked to remove it by the author until the app is actually released. So we went with that set of of widget instead, which is a prototype of the Cosmic style for Pop!OS)Edit: We changed the image. (for reference, the previous image was https://slint.dev/assets/img/cosmic_de_popos_light.png )replykolektiv 21 hours ago | root | parent | next [\u2013]I am not a designer (not for 20 years now) but that desktop image (rotating gif of different styles) is not a good advert. There are many, many layout issues with those that I would really spend time to try and make \"default\" look right out of the box (and in line with platform HID guidelines). I am a Rust developer, and I think Slint could be amazing, but those things will put off people who are driven by design quality.replymadnirua 11 hours ago | root | parent | next [\u2013]Thanks for the feedback. We will try to get some designers to improve the widget gallery examples.replyealhad 19 hours ago | prev | next [\u2013]This looks really neat, I played with it a bit and the DSL is well thought out.One thing I can't figure though: how do we set default values for internal Struct?Said default values are mentionned in the documentation (here: <https://slint.dev/releases/1.0.2/docs/slint/src/reference/ty...>). // ok even if a is missing, it will just have the default valuereplyogoffart 4 hours ago | parent | next [\u2013]Currently the default is just whatever empty value for that type (so 0 or \"\")But since this is something that was asked before, I created an issue: https://github.com/slint-ui/slint/issues/2936replyandsoitis 20 hours ago | prev | next [\u2013]Correct and complete text handling is hard. Does Slint handle (did a cursory search in the docs but couldn't find anything that hints at an answer):* multi-line text* languages that use non-Latin characters, as well as mixing them in the same control* RTL and maybe vertical layoutreplyogoffart 15 hours ago | parent | next [\u2013]Correct text handling is indeed a challenging task, especially to get all keyboard shortcut to edit and input methods. While we strive to improve our text handling capabilities, we are not perfect yet.Slint does handle multi-line text and languages that use non-Latin characters. However, we currently do not support vertical layouts.replycriddell 12 hours ago | root | parent | next [\u2013]How\u2019s accessibility? Does Slint play well with things like screen readers?replytronical 12 hours ago | root | parent | next [\u2013]We do have initial support for screen readers. That\u2019s using accesskit, or Qt when using the Qt backend.It\u2019s still basic, some controls like text input need work, or customizing the focus chain.Building blocks are there and will need further polish.replygregulrajani 16 hours ago | prev | next [\u2013]The band is not that bad as well https://en.m.wikipedia.org/wiki/Slintreplyorra 21 hours ago | prev | next [\u2013]The examples start up promisingly fast. Faster than Uno, IMHO, and waaay faster than Qt (on WebAssembly I mean).Great to see competition in this space; I'm sure they'll all benefit in the long run.replyAlifatisk 21 hours ago | prev | next [\u2013]\"Achieve low footprint and minimal resource consumption. The Slint runtime fits in less than 300KiB RAM...\"Well done! This is exactly what we should be better at, ship less bloat and set a higher standard.replyzoogeny 20 hours ago | prev | next [\u2013]There is often a criticism of Javascript, that new frameworks seem to appear every week. But it honestly feels like every week on HN there is a new Rust UI project showing up. Whether it is some new HTML/web thing, a native app thing, a TUI thing, etc.Don't get me wrong, I think that is the sign of a very healthy ecosystem. I am just curious how the Rust evangelists will spin this. Somehow this constant churn on the Javascript side is worthy of snide, satire and ridicule.IMO, UI development is deceptively hard. Rendering scalable graphics efficiently is hard enough. Creating a good layout engine and a set of reliable basic controls is even more difficult. But managing interfaces seems to scale quadratically with the number of controls, views, states, etc. I see programmers, over and over, underestimate how difficult of a task managing that complexity can be. This complexity leads to difficult to use libraries/framework/platforms. It also leads naive and ambitious developers to believe they can do better. This leads to more and more new libraries/frameworks/platforms.reply__david__ 15 hours ago | parent | next [\u2013]Slint has been around for quite a while now. A few years ago it was called SixtyFPS.I used it for a cross compiled Windows app (did 99% of the development on macOS) and it was quite nice. It was a little limited as far as widget customization went (without getting down and writing your own widgets from scratch) but app ended up looking nice and behaving decently. It also lacked a few features I'd say were essential for any non-small desktop app (multiple top level windows weren't a thing a year ago, not sure if they've been added yet).I quite liked their DSL for the UI scripting. It was even typechecked a bit and interfaced with Rust in a fairly clean way. It's one of the nicer UI libraries I've used in a while.replyduped 19 hours ago | parent | prev | next [\u2013]It's a somewhat hot area of development and there isn't a clear winner yet, so of course there are new projects all the time.replya_humean 19 hours ago | parent | prev | next [\u2013]All of these rust libraries and frameworks are in their infancy and there are no clear winners yet. Its probably less of a sign of a healthy ecosystem, and more of a sign of an immature ecosystem. There are a few projects that seem promising, but all of that could be invalidated by next week.Meanwhile in the js ecosystem things are both incredibly stable and also in flux. All of the usual suspects are still around, but competing in ways intergrate both server and client rendering as seamlessly as possible. There is also some movement around introducing optimising compilers - see solid and svelte.But its probably safe to say, ignoring the tooling churn going on with Rust, Go, and Zig seemingly systematically replacing all js tooling like for like, things are pretty stable when it comes to js ui libraries.replynicoburns 18 hours ago | parent | prev | next [\u2013]While there are a lot of Rust UI frameworks, none of them are really recommended for production use yet. I suspect a few of the will die off and work will coalesce a few once things mature a bit.Another nice feature of the Rust UI ecosystem is that lots of it is being built in a modular way. For example I maintain a layout engine [0] library which just does layout and can be easily integrated by anybody creating a UI library. And there a bunch of similar composable libraries covering rendering, text layout, accessibility, window creation, clipboard access, etc.[0]: https://github.com/DioxusLabs/taffyreplykibwen 19 hours ago | parent | prev | next [\u2013]> Somehow this constant churn on the Javascript side is worthy of snide, satire and ridicule.In my experience it is frontend devs, not backend devs, who are most harsh on Javascript's procession of UI frameworks. Rust users are more than happy to take inspiration from Javascript UI frameworks, if for no other reason than the reactive/declarative approach is a better fit for Rust than the prevailing inheritance-heavy/object-oriented approach of Qt and GTK.replymarcosdumay 18 hours ago | parent | prev | next [\u2013]Nobody expects you to go and learn this week's Rust UI project, or to abandon last week's one.If go looking for a job, you will discover the people working with Rust UI hire Rust developers, not Tokyo (yeah, that one was many weeks ago) ones or whatever else. Those developers are expected to be able to learn and adapt to whatever framework the place uses.All of that is completely opposite to how things happen with Javascript.replyzemo 19 hours ago | parent | prev | next [\u2013]it\u2019s a different situation though: very few people have shipped guis in Rust, so the Rust projects are largely greenfield, while a lot of JavaScript front ends are legacy apps.replysamstave 18 hours ago | parent | prev | next [\u2013]>>\" Rendering scalable graphics efficiently is hard enough\"As a not-UI dev guy, what will be awesome is when a stack solves the scaling/swiping.Imagine being able to swipe a UI from one UX to another UX -I See my blood pressure on my E-watch... I swipe right to throw that details onto my laptop... ... I swipe up to throw that shit on the EMR at the doctors office...Or I have a vitals monitor on my watch and when the ambulance picks me up they can just slurp my historics via my watch and an AI starts doing prognoses asap.Those who do this, win all the money. (not to mention the threat this is to HOSPITAL CODES or INSURANCE PAYMENT CODES) -- Yeah those bitches will be upset when this happens.replyn8henrie 17 hours ago | prev | next [\u2013]> Create your application under MIT, BSD, Apache 2.0 or any of the other GPLv3 compatible licenses, provided that the complete work is made available under the GPLv3.> Create your application under a license of your choice, open-source or proprietary, provided that the application includes proper attribution to Slint and retains copyright notices. I've listened to at least one (or two?) podcasts on slint, and I'm still not sure that I understand the license options, which makes me hesitate to invest time to learn.I usually use the MIT license. Is the first license above compatible with MIT, without requiring forks (of my project) to GPL3 their code?Does the latter license mean I can closed-source and commercialize my app and just include a shout-out to Slint and I'm good? (EDIT: if so, why would anyone choose the potentially expensive commercial option?)replyogoffart 16 hours ago | parent | next [\u2013]Forks of your project do not require open-sourcing their code if they adhere to the terms of the royalty-free license, which include attributions and excluding usage on embedded devices. (Alternatively, they can get a paid license)> why would anyone choose the potentially expensive commercial optionThe paid option is for those using Slint in embedded devices (which is not permitted by the royalty-free license) or for those who prefer not to include attribution to Slint in their program.replynazgulsenpai 19 hours ago | prev | next [\u2013]I'm a total laymen on the embedded side, and as such I think this looks great and the GPL and permissive licensing options are a very welcome and (IMO) fair way to engage and empower the community.replyKelteseth 1 day ago | prev | next [\u2013]Nice to see that Qt finally gets some competition! The pricing alone is a no-brainer. Now the only thing I'm missing from the demos is smooth scrolling :)replypzo 23 hours ago | parent | next [\u2013]If you target desktop only probably Qt license (LGPL 3.0) is easy to use even in commercial projects. The problems comes if you want to target embedded, mobile platforms especially iOS or require some of their GPL modules (e.g. Charts, Data Visualization)Otherwise Qt is probsbly much more mature project and already has unofficial rust bindingsreplyKelteseth 22 hours ago | root | parent | next [\u2013]The current main problem is that Qt often releases new modules under GPL and commercial only licenses. For example, the new Qt GRPC implementation and then it is all or nothing. Qt commercial would force us to essentially buy two licenses because we have a second app that acts as a third party dll loading app. These dlls often crash and thus we moved them into a dedicated app we could simply restart, but Qt license would force you two buy two licenses only because they talk to each other. So, no thank you Qt we will stick with LGPL for now, and move to something else in the future. See my blog post \"Current Issues With The Qt Project - From The Outside Looking In\" [1].[1] https://kelteseth.com/post/20-04-2023-current-issues-with-th...replyibsonc 18 hours ago | root | parent | next [\u2013]I think you have misunderstood the Qt licensing model when you say that you need two Qt licenses if you are building two separate applications?The commercial licenses are a) per developer and b) per distributed device (if you distribute your software as part of a device), there is no per application licensing requirement.replyKelteseth 18 hours ago | root | parent | next [\u2013]> An application using Qt Commercial must not communicate with any another application using Qt LGPL-3.0 or Qt GPL, if both applications run on the same device. This is more restrictive than GPL.https://embeddeduse.com/2023/01/06/using-qt-5-15-and-qt-6-un...replyibsonc 13 hours ago | root | parent | next [\u2013]The context there is \"combining commercial and free open-source Qt licenses\"; if you are using either commercial or open source licenses exclusively, there are no such restrictions.replypjmlp 23 hours ago | parent | prev | next [\u2013]Not to diminish Slint authors work, but it is still far from being Qt competition, given the current state of accessibility, IDE integration, or graphical tooling for UI/UX designers.replymadnirua 23 hours ago | root | parent | next [\u2013]Disclaimer: I work for SlintIndeed we are actively working on all of those topics:Accessibility support in Slint is improving. We just merged the initial AccessKit integration into our winit backend -- https://github.com/slint-ui/slint/pull/2865IDE integration - our VSCode extension is pretty stable these days (https://slint.dev/get-started#vscode) and we also support integrating into other IDEs via LSP (https://slint.dev/get-started#other-ide)Graphical tooling for UI/UX designers - our current prototype https://slintpad.com needs more work to become an easy Drag n'Drop editor for UI/UX designers -- but we are on the right path to achieve that :)replyogoffart 20 hours ago | root | parent | prev | next [\u2013]Slint is already a viable alternative to Qt, as evidenced by users actively choosing us over Qt. While Qt is more mature in many areas with its two decades of development and large team, Slint is already good enough for some use cases. And as a young framework, we naturally strive to improve.By the way, when it comes to IDE integration, our VSCode extension and LSP server, which includes live preview, put us on par with Qt's.replymadnirua 23 hours ago | parent | prev | next [\u2013]Disclaimer: I work for SlintThanks. We have a lot of users who are migrating from Qt/QML to Slint.The smooth scrolling part -- is that for the wasm binary or for a native application?replyKelteseth 22 hours ago | root | parent | next [\u2013]Wasm in Firefox.replytensor 18 hours ago | parent | prev | next [\u2013]I can't find it now, but I remember looking at slint a few months back and finding out that native widget support on the desktop required QT. Is this not true?replymadnirua 11 hours ago | root | parent | next [\u2013]Qt is only needed if you want native looking widgets. Otherwise, another style will be used for widget, which does not look native. In the future, we plan to have native backend using the native API, which will allow native widgets without using Qt.Reference: https://github.com/slint-ui/slint/blob/master/docs/install_q...replyWhereIsTheTruth 7 hours ago | prev | next [\u2013]If you need something written in C and very tiny, I recommend LVGL https://lvgl.io/Fits easily in 128kb boardsreplycodethief 15 hours ago | prev | next [\u2013]The in-browser demos (using WebAssembly) are surprisingly fast and responsive (very different from the Flutter demos that I remember). Very cool!Also, I just realized I had checked out this project before when it was still called SixtyFPS, https://slint.dev/blog/sixtyfps-becomes-slint . Maybe it would make sense to mention the name change in your post.replyogoffart 4 hours ago | parent | next [\u2013]It has been over a year since we renamed the project. We previously had a note on every page with the former name. But the new website design no longer includes this note.replytyfon 23 hours ago | prev | next [\u2013]Nice!I have been using libQt since the last millennium for different projects, this looks more like how I use it as I never really liked QML.But the comparison page vs qt is a bit thin, what do I gain here and how do the features compare?It works be nice if the page was more fleshed out in that respect.It's so refreshing when new stuff comes out for native and not just web web web!replymadnirua 22 hours ago | parent | next [\u2013]Thanks :)Since Qt is a much broader toolkit, we are considering of writing dedicated blogs to do an objective comparison between the two toolkits on the GUI part only.replyjoezydeco 12 hours ago | root | parent | next [\u2013]Please do!And, speaking as a Qt programmer, address the biggest pain points that Qt user have.Number one: get a virtual keyboard that supports the biggest world languages without a commercial license. That would make me an instant convert.replymadnirua 10 hours ago | root | parent | next [\u2013]Thanks :)For the short term, we have an example that some of our community users and commercial customers a using to build their custom virtual keyboardhttps://github.com/slint-ui/slint/blob/master/examples/virtu...replyjoezydeco 10 hours ago | root | parent | next [\u2013]I'll try it out, thanks.I'm pretty fed up with Qt and I'm getting kind of excited to start experimenting with this.replynologic01 1 day ago | prev | next [\u2013]The GUI space is crying out for more workable approaches so fresh takes are more than welcome. But going through the website the comparison with other existing options (Qt, Electron etc) is mainly schematic one-liners, good for marketing but not particularly informative.Curious also if there are plans for Python API'sreplyogoffart 23 hours ago | parent | next [\u2013]Thank you for your comment!I acknowledge that the comparisons on our webpage footer is primarily for marketing and SEO purposes. This is inspired by the equivalent content on websites like figma.com. The intention is to highlight Slint's unique selling points without delving into extensive discussions about other frameworks.And yes, we do have plans for Python, that would actually be the next language we want to make bindings for.replySophistifunk 10 hours ago | prev | next [\u2013]What's the catch? There's always a catch, like \"turns out it's just WebView\" or \"turns out only the android and Linux CDE implementations actually exists\" or \"call us for pricing\"replyWorldMaker 10 hours ago | parent | next [\u2013]Looks like \"Call us for pricing\":> Free for non-commercial use.> For commercial use, the runtime fee is based on volume and type of embedded system.replymadnirua 4 hours ago | root | parent | next [\u2013]We plan to add a pricing calculator later, replacing \u201cCall us for pricing \u201creplytracker1 13 hours ago | prev | next [\u2013]Wether it's fair or not.. I generally compare any given UI framework/toolkit to mui.com (Material-UI) component library. They've solved a lot of the issues I've had, nice to work with and in general really well-rounded components. Would love to see an equivalent with Yew or similar, maybe integrated like Tauri.In the end, I think that accessibility is a really hard thing to get right. As are the base components that are generally needed for applications. This is a nice start, and for lighter/embedded it could be nice. I'm not sure that I'd reach for this ahead of Flutter, React(-Native) or alternatives that are more polished for general use though.replyaembleton 11 hours ago | prev | next [\u2013]The link to Javascript at the bottom of this page is broken: https://slint.dev/releases/1.0.2/docs/slint/. It goes to https://slint.dev/releases/1.0.2/docs/nodejs/ which gives a 404.replymadnirua 11 hours ago | parent | next [\u2013]Here is the correct linkhttps://slint.dev/releases/1.0.2/docs/node/replymadnirua 11 hours ago | parent | prev | next [\u2013]Thanks for reporting. Will fix that.replypeepee1982 23 hours ago | prev | next [\u2013]I understand too little about GUI Toolkits to have a strong opinion on the technical merits, but as a music fan, I appreciate the name.replymadnirua 23 hours ago | parent | next [\u2013]Thanks.. Probably we should name our major releases after some of the band's titles :)replydanielscrubs 22 hours ago | prev | next [\u2013]An UI framework in 2023 needs two things: one line animations and a gorgeous starter template.The inconvenient truth is that there are way more good coders than there are good designers so coders need all the help they can get to sell their product and make good ux choices.Any comments on how it compares on those fronts?replyogoffart 20 hours ago | parent | next [\u2013]We do have one line animations [1], we do have starter templates[2]. But not sure what you call \"gorgeous\".I 100% agree with the need of good designers.[1] https://slint.dev/docs/slint/src/reference/animations [2] https://github.com/slint-ui/?q=template&type=all&language=&s...replynyanpasu64 17 hours ago | prev | next [\u2013]The toolkit still seems quite incomplete for desktop use (as opposed to embedded/appliance UIs), as it doesn't support menu bars in an issue open since 2020: https://github.com/slint-ui/slint/issues/38replyogoffart 15 hours ago | parent | next [\u2013]A comprehensive toolkit like Slint is an enormous and ambitious task. We prioritize feature implementation based on user demand and needs. As of now, we haven't received any specific requests from users for the menu bar feature, but we know it is important (we filled the issue ourselves)replyShadowBanThis01 5 hours ago | parent | prev | next [\u2013]What is a \"menu bar?\" Something other than a regular application menu?replynyanpasu64 2 hours ago | root | parent | next [\u2013]The toolkit doesn't support regular application menus or right-click menus.replyShadowBanThis01 2 hours ago | root | parent | next [\u2013]That's a pretty big problem.replydavid_shi 13 hours ago | prev | next [\u2013]Unrelated, but a song by the band was in the 'BlackBerry' movie.https://www.youtube.com/watch?v=CuqEpjcBfaUreplyirq-1 13 hours ago | prev | next [\u2013]Do I need to use rust or cpp? Can I work in js only? I like the slint layout files and the viewer.Are you planing a C interface?replyogoffart 13 hours ago | parent | next [\u2013]You don't need c++ or rust if you want to work with JS. (although right now you will need a rust compiler to compile the binary, but we want to ship binaries in the future)We are considering a C interface, but given we already have a C++ interface, we don't see much gain in having a C interface compared to the effort to maintain it.replyseren 18 hours ago | prev | next [\u2013]Do you support any kind of integration with Vxworks 7 on some platform ?Vxworks has some Rust support so I wonder if you already had some integration with vxworks driversreplymadnirua 17 hours ago | parent | next [\u2013]We investigated integration VxWorks as part of a customer request and we concluded that all the required hooks exist to support Slint.replyseren 17 hours ago | root | parent | next [\u2013]Thanks for the answer !replymeepmorp 23 hours ago | prev | next [\u2013]Is it named after the band?replyDontchaKnowit 16 hours ago | parent | next [\u2013]If youre reading this, go listen to spiderland by slint. Awesome and super unique albumreplylbres 7 hours ago | root | parent | next [\u2013]I\u2019ve always hoped that HN was a place that appreciated music like Slint, now I have my answer.replymadnirua 23 hours ago | parent | prev | next [\u2013]Just a nice co-incidence :)The toolkit was initially named SixtyFPS and was renamed to Slint last year --https://slint.dev/blog/sixtyfps-becomes-slintThe name comes from our design goals - Straightforward, Lightweight, Intuitive, Native Toolkit.replymeepmorp 22 hours ago | root | parent | next [\u2013]Well, I'm still gonna listen to Spiderland because you reminded me, so thanks for everything.replygeekamongus 21 hours ago | root | parent | next [\u2013]Tweez is my favorite album of theirs, though Spiderland is excellent.replyHDThoreaun 14 hours ago | root | parent | next [\u2013]Scorching take. Tweez is nowhere close to spiderland. Now hurry up and get me some new headphones Steve, they're fucked.replydenysvitali 22 hours ago | prev | next [\u2013]Funny how they claim the toolkit is responsive, but then their website isn't.The \"Used by\" section overflows and basically breaks the whole website.replymadnirua 22 hours ago | parent | next [\u2013]Probably because we are using HTML and CSS instead of Slint :)Jokes aside .. which browser and device combo are you using.. We can adjust the css accordingly. Thanks.replydenysvitali 17 hours ago | root | parent | next [\u2013]Pixel 7, Chromium.I think the issue is with one of the logos at the bottomreplymonk_e_boy 19 hours ago | prev | next [\u2013]You should give us educators a free license :) I'd use this in my classes if I could.I'm keen to use it with c++ and rustreplyvbarrielle 18 hours ago | parent | next [\u2013]It looks like you can use the GPLv3 license for this use case.replyCyberDildonics 14 hours ago | prev | next [\u2013]Ignoring for a second that \"Slint\" is only slightly better than \"60fps\" as a name for a GUI library, I think using a custom domain specific language for configuration is a giant mistake.It really is not difficult to just call functions to set values. Laying out GUI components is also rarely what takes time when doing GUI programming.A brand new custom domain specific language has almost no benefits but has huge downsides like having to learn brand new syntax, complete with error messages and debugging. Then you probably have to change GUI values dynamically anyway inside your program. All of this is on top of the added bloat, which itself is strange for a 'embedded usage'.I think a lot of newer GUI libraries would be better off porting FLTK to work on their platform, it is very simple and easy to understand underneath.replyogoffart 12 hours ago | parent | next [\u2013]DSLs are not inherently more difficult to learn than API and conventions of a complex GUI library. DSL offer benefits in terms of expressiveness and conciseness. The focus in UI programming is on rapid iteration and immediate results, which can be facilitated by a live preview in the IDE made possible by a declarative DSLreplyCyberDildonics 10 hours ago | root | parent | next [\u2013]DSLs are not inherently more difficult to learn than API and conventions of a complex GUI libraryIn theory maybe, in practice the indirection and extra layer of syntax, error handling, documentation and edge cases mean this is not true. Also this is an assertion without any evidence.DSL offer benefits in terms of expressiveness and conciseness.Prove it.The focus in UI programming is on rapid iteration and immediate resultsI have never thought that setting up values through functions was not rapid. It has never been a bottleneck or time sink when making a GUI. Unfortunately, using custom markup languages that don't work like they say they should has been a time sink.which can be facilitated by a live preview in the IDE made possible by a declarative DSLMy experience is that this is very minor in terms of overall utility and doesn't actually require a custom markup language.replyTobyTheDog123 16 hours ago | prev [\u2013]>Attribution to SlintMmmm no thanks, I'll pass.While I'm looking for a Flutter alternative, this seems like a trap.replyogoffart 16 hours ago | parent [\u2013]We are not a charity and aim to operate on a sustainable development model. While we offer paid and GPL licensing options, we also provide a free-of-charge license that only requires attribution. If you are unwilling to provide even attribution, then Slint may not be the right choice for you.replyTobyTheDog123 15 hours ago | root | parent [\u2013]Oh it's certainly not for me, no question about it.Yet, it's curious that you compare Slint to LVGL/Flutter/Electron (I don't know about Qt) when none of them have this intrusive requirement.--- >What's intrusive about it?(a) Display the AboutSlint widget in an \"About\" screen or dialog that is accessible from the top level menu of the Application.(b) Display the Slint attribution badge on a public webpage, where the binaries of your Application can be downloaded from, in such a way that it can be easily found by any visitor to that page.(c) You may not remove or alter any license notices (including copyright notices, disclaimers of warranty, or limitations of liability) contained within the source code form of the Software.(d) You allow SixtyFPS to use your Application on the website and in advertising materials of SixtyFPS as a reference and to display your logo and trademark for this purpose.---Please don't pretend like a/b/d aren't big asks by saying \"you wont even provide attribution?\"replyrandom78965 14 hours ago | root | parent [\u2013]Not a dev, but I've often seen software that acknowledge their dependencies in About sections and on their websites. Is that particularly intrusive? Especially given that you could always use the GPL too if you want.replyShadowBanThis01 5 hours ago | root | parent [\u2013]This goes beyond that. I didn't see that list of requirements; some places only say \"attribution.\"I would have no problem listing a library in the About dialog, but dictating the use of special icons and then all of those other requirements goes further than I've seen in any similar product.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Slint is a declarative GUI toolkit written primarily in Rust, designed for desktop and embedded usage.\n- The toolkit offers API support for multiple programming languages, including C++ and JavaScript.\n- Users have praised Slint's design, ease of use, and real-time GUI feedback when using the VS Code extension. Some compare it favorably to Microsoft's Windows Presentation Foundation.\n- One user expressed concern about optimizing for binary size on a microcontroller with limited flash storage.\n- The Slint team acknowledged the concern and mentioned plans to optimize the generated code to reduce the binary size further.\n- Another user expressed interest in a GUI library dedicated to small embedded systems that do not consume a lot of flash or RAM.\n- The Slint team responded that this was their aim with the toolkit.\n- Users recommended alternative GUI libraries, such as fltk-rs and imgui, that might better suit specific needs.\n- Users discussed optimizing code size and performance for microcontrollers, with suggestions like adjusting codegen settings in Rust and experimenting with inlining thresholds.\n- Users engaged in conversations about GRPC, Rust as a backend for UI, and integrating Flutter with Rust.\n- Users expressed interest in better comparison sections on the Slint website and suggested other UI frameworks to consider.\n- Users discussed the pricing structure of Slint and requested more clarity in the pricing information.\n- Users suggested including an onboarding guide and improving the widget gallery examples on the Slint website.\n- Users raised questions about text handling, accessibility support, native widget integration, and plans for Python APIs.\n- Users shared their appreciation for the name \"Slint\" and its connection to the band."
  },
  {
    "id": 36387874,
    "timestamp": 1687155587,
    "title": "My First Impressions of Nix",
    "url": "https://mtlynch.io/notes/nix-first-impressions/",
    "hn_url": "http://news.ycombinator.com/item?id=36387874",
    "content": "My First Impressions of NixJune 17, 2023 12-minute readNix is a tool for configuring software environments according to source files. I\u2019ve been hearing more and more about Nix on Hacker News and Twitter. The idea of it appeals to me, so I\u2019ve been tinkering with it over the past few weeks.My history with infrastructure as codeTen years ago, I discovered Salt, a tool that allows you to define a computer system\u2019s configuration in source code. I loved the idea of a git repo that defined what services were installed on my computers and VMs. I could blow away the computer, re-run the configuration tool, and get it back to the same state.I messed around with Salt for a few years until discovering Ansible, which I felt like executed the same idea better.I do all of my development in VMs on my homelab server. I have a separate VM for each projects, and I manage them all with Ansible.The problems with AnsibleAnsible\u2019s biggest problem is that it\u2019s painfully slow. It typically takes 10-15 minutes for Ansible to run against one of my VMs.Suppose I want to install a new apt package foo. Do I just run sudo apt install --yes foo and have the package in 5 seconds? Or do I pull up my Ansible role, edit the configuration to add a step to install foo, then run the playbook, then wait 15 minutes? Obviously, I end up doing more of the former, so my environments drift from the Ansible files that are supposed to represent them.The other issue is that Ansible\u2019s changes aren\u2019t backwards-compatible. So, if I update a playbook to take advantage of a new Ansible feature, I have to update all of my playbooks to be compatible. But I never feel inspired to rewrite and retest all of my playbooks, so I get stuck on old versions. I\u2019m still on Ansible 2.9, which was released three years ago.The appeal of NixI\u2019m seeing more and more people talk about Nix and NixOS. A lot of the developers I find interesting are talking about their experiments with Nix.The most impactful endorsement I\u2019ve seen was from Mitchell Hashimoto, co-founder of Hashicorp, the company responsible for creating a lot of widely-used open-source infrastructure tools like Vagrant, Packer, Consul, and Terraform. He called Nix, \u201cthe #1 most positively impactful technology I\u2019ve learned in recent years.\u201dThe idea of Nix feels a lot like Ansible. Nix lets you define a configuration in code, and then it brings the system into that state.Nix vs. AnsibleNix differs from Ansible in a few important ways that I find interesting.Nix is faster than AnsibleAnsible has no concept of the \u201cstate\u201d of the system it\u2019s configuring. If it takes 15 minutes to run Ansible on one of my VMs, running the same playbook a minute later would take about 10 minutes. You save a little bit of time because there\u2019s probably no package updates between the two invocations, but it\u2019s still doing almost all the same work.Ansible never says, \u201cOh, I just configured that machine, so there\u2019s nothing for me to do now.\u201d Ansible has to perform every configuration again because anything could have happened since the last time it ran.Nix, on the other hand, does have a concept of state. If you make a one-line change to a 200-line Nix configuration, it doesn\u2019t have to re-do all the work from the other 199 lines. It can evaluate the state of the system against the configuration file and recognize that it just has to apply the one-line change. And that change usually happens in a few seconds.Edit (2023-06-19): A reader with more experience clarified that Nix doesn\u2019t have state in the way I assumed:Nix is not fast because it is stateful. It is fast because it is functional and reproducible, which allows for caching without compromising correctness.I had thought that Nix kept track of which tasks brought me to which system state. For example, imagine that performing tasks X and Y brings me to state A, and performing tasks X, Y, and Z brings me to state B. I thought that a Nix system in state A would know to only perform task Z to reach state B.My new understanding is that if you ask a Nix system in state A to go to state B, Nix will perform tasks X, Y and Z, but Nix cached the results of tasks X and Y, so they happen near-instantly.Nix optimizes for local configurationAnsible is designed to configure systems over the network. You can still specify localhost as the target, but that\u2019s not the scenario that Ansible is optimized for.Nix is designed to configure the environment it\u2019s in. With Nix, you define what should be in the environment, and then Nix creates that environment for you in place. With NixOS, you defined the entire operating system, and NixOS gets the operating system into that state. You can change low-level things like the filesystem, the Linux kernel, or the bootloader.This solves the problem I had with Ansible where upgrading one Ansible playbook to use a later version of Ansible requires me to upgrade all the playbooks on my system. You can have many Nix systems running different versions of Nix, and they work fine. If I have some Ansible files that require Ansible 2.9, some that require 2.10, and some that require 2.14, then it\u2019s a big pain to juggle them all.Nix changes are atomicWith Ansible, it\u2019s easy to fail halfway through a configuration, leaving the system in an undefined state.With Nix, changes are atomic. Nix either gets your system into the desired state or it rolls back to the state before you tried changing the configuration.Nix resources that have been helpfulOne of the biggest complaints I see about Nix is that it\u2019s underdocumented, incorrectly, or poorly documented. My experience is that the documentation feels like it\u2019s aimed at experienced Nix users.A lot of Nix documentation I\u2019ve found says things like, \u201cSimply add these lines!\u201dHuh?Which file? And where in the file do I add those lines?Here are the best resources I\u2019ve found so far:Zero to Nix: This is the best set of introductory Nix tutorials I\u2019ve found. It\u2019s written by Determinate Systems, who also writes beginner-friendly blog posts about using Nix.NixOS for the Impatient: I\u2019d tried to install NixOS a couple of times before this, but this post finally convinced me it was easier than I thought, and it gave me the final push to push through the process.\u201cSome notes on using nix\u201d by Julia Evans: Julia\u2019s also a newcomer to Nix, so it was helpful seeing the blockers she ran into and how she worked around them even though there\u2019s a lot about the ecosystem that\u2019s still new and unfamiliar to her.Failed attempt #1: NixOS in a VMI followed the \u201cNixOS for the Impatient\u201d tutorial on a Proxmox VM, and everything worked at first. Then, I got to the point in the tutorial where you change the hostname, and then I rebooted for it to take effect. But the VM got into a weird state where it seemed like it could boot but not log in. After I entered the password in the login screen, the screen just froze.Your browser does not support the video tag.NixOS installed successfully on my Proxmox VM server, but it hung after login on the second boot.Failed attempt #2: NixOS on the Raspberry Pi 4Since a VM didn\u2019t work, I figured bare metal was the next logical choice. I had a spare Raspberry Pi 4 on hand, and I thought the Pi would be fun hardware to experiment on.I found two different official-looking tutorials for installing NixOS on the Raspberry Pi 4:NixOS Wiki: NixOS on ARM/Raspberry Pi 4nix.dev: Installing NixOS on a Raspberry PiThe problem with both of these tutorials was they assumed that you\u2019re already running a Nix environment. I\u2019m trying to prepare the microSD from my main computer, which is a Win10 system, so I didn\u2019t have Nix already.The NixOS download page lists a 64-bit ARM image. The Raspberry Pi 4 supports 64-bit ARM, so I thought I\u2019d try that.I loaded up Balena Etcher, my preferred tool for flashing microSDs. The first red flag was when Etcher basically said, \u201cHey, what are you thinking? That\u2019s not even a bootable image.\u201dI continued on anyway! But when I tried booting from the microSD, the Pi agreed that this was not a bootable image, so it just got stuck.I tried flashing the same image using the official Raspberry Pi Imager utility, but I got the same results.Success: NixOS on a Dell Mini computerI do most of my testing for work against a Dell Optiplex 7040. It was the only bare-metal machine I had available that I could blow away, so I tried on that.Everything worked exactly like in \u201cNixOS for the Impatient.\u201d The install took about 10 minutes from start to finish. Seven minutes was just copying files. I skipped encryption since this is just a test device, and I wanted to eliminate a password-entry step on every reboot.Your browser does not support the video tag.Installing NixOS on a Dell Optiplex 7040 (I sped up the file copy portion)In the end, I had a full, working NixOS install!Failed attempt #3: NixOS on the Raspberry Pi 4 (again)Now that I had a working NixOS machine, I wanted to give the Raspberry Pi another shot. The blocker before was not having a Nix environment from which to prepare the microSD image, but now I had one.I followed the nix.dev tutorial, which brought me farther than my first attempt, but I still couldn\u2019t boot. The Pi would just reach a stage of showing a multicolored screen and then hang:Your browser does not support the video tag.When I flashed the NixOS Pi aarch64 microSD image from a NixOS system and booted my Pi from it, it hung on a multicolored screen.Task 1: Getting SSH accessOkay, back to my working NixOS install on the Dell Optiplex.I needed to SSH in to the NixOS system from my main machine. To do that, I\u2019d need to get my SSH keys on the system.From NixOS, I opened the Console application and then typed sudo nano /etc/nixos/configuration.nix. Then, I went to the environment.systemPackages and added these lines: environment.systemPackages = with pkgs; [  vim  curl ];To realize the changes, I ran:sudo nixos-rebuild switchNow, I had vim and curl, available, so I could pull down my SSH public key from Github:sudo mkdir -p /etc/nixos/sshGITHUB_USERNAME=\"mtlynch\"curl \"https://github.com/${GITHUB_USERNAME}.keys\" | \\ sudo tee --append /etc/nixos/ssh/authorized_keysNext, I ran sudo vim /etc/nixos/configuration.nix and added these lines: # Enable the OpenSSH daemon. services.openssh.enable = true; users.users.mike.openssh.authorizedKeys.keyFiles = [  /etc/nixos/ssh/authorized_keys ];Finally, I rebuilt and rebooted. I\u2019m not sure if the reboot was strictly necessary:sudo nixos-rebuild switch && sudo rebootAnd it worked! After that, I could ssh into my NixOS system from my main computer.Task 2: Removing Gnome clutterThe first thing that struck me about the OS was that there was lots of clutter. It had a bunch of built-in apps I didn\u2019t want, like a contact list and a weather app:I searched for how to get rid of them and discovered that they\u2019re default applications as part of the Gnome shell. You can disable them by adding this line to your /etc/nixos/configuration.nix:services.gnome.core-utilities.enable = false;Or you can remove each utility one-by-one: environment.gnome.excludePackages = with pkgs.gnome; [  baobab   # disk usage analyzer  cheese   # photo booth  eog     # image viewer  epiphany  # web browser  gedit    # text editor  simple-scan # document scanner  totem    # video player  yelp    # help viewer  evince   # document viewer  file-roller # archive manager  geary    # email client  seahorse  # password manager  gnome-calculator  gnome-calendar  gnome-characters  gnome-clocks  gnome-contacts  gnome-font-viewer  gnome-logs  gnome-maps  gnome-music  gnome-screenshot  gnome-system-monitor  gnome-weather  gnome-disk-utility  pkgs.gnome-connections ];I went with the \u201cdisable everything\u201d option and rebuilt:sudo nixos-rebuild switchAnd voila! All of the clutter disappeared:Task 3: Bringing back the System Monitor (failed)The one Gnome tool I did think was worth keeping was System Monitor. I tried adding it to my list of environment.systemPackages, but then rebuilding failed:$ sudo nixos-rebuild switchbuilding Nix...building the system configuration...error: undefined variable 'gnome-system-monitor'    at /etc/nixos/configuration.nix:130:5:     129|   curl     130|   gnome-system-monitor       |   ^     131|  ];(use '--show-trace' to show detailed location information)I tried other possible names like gnome-shell-system-monitor, but I couldn\u2019t figure out how to install it.Edit (2023-06-19): Thanks to readers who pointed out that the correct package name is gnome.gnome-system-monitor. The piece I was missing was that I can search for packages at search.nixos.org.Things I\u2019d like to understand nextI\u2019m happy with my first few days with Nix and NixOS. It\u2019s about what I expected from what I\u2019ve heard. It seems like it can be incredibly powerful when used well, but it requires a lot of upfront investment and scavenging for information.I\u2019ve only scratched the surface, so here are the things I\u2019d like to learn about Nix next.Using VS Code Remote SSH on NixOS systemsI do all of my development in VS Code over remote SSH. When I try remoting into my NixOS system from VS Code, the install fails. VS Code has to install some type of server on the target system, and VS Code probably doesn\u2019t know how to install on NixOS.There\u2019s a nixos-vscode-server git repo, and that\u2019s probably the solution I need. I haven\u2019t tried it yet.How Nix\u2019s major concepts fit togetherI see words like \u201cflakes\u201d and \u201cderivations,\u201d and I currently don\u2019t know what they mean. I don\u2019t understand Nix\u2019s language syntax, but it\u2019s enough like JavaScript and Python that I can fake my way through at this point. But to use Nix effectively, I\u2019m obviously going to need to learn the language.When does the determinism happen?When I see discussion of Nix, one of the top features I see mentioned is the fact that Nix is deterministic.So far, I don\u2019t get how it\u2019s deterministic. When I specified packages to install, I didn\u2019t specify an integrity hash, let alone a version number. If I ran the same Nix configuration a year from now, I assume I\u2019d get a different system because it would install different versions of the vim and curl packages I specified.I assume there is a way of specifying package versions more precisely, but I haven\u2019t learned it yet.Who am I trusting?When I specified packages, it was a plain list of package names: environment.systemPackages = with pkgs; [  vim  curl ];For me to be able to specify packages like the above, Nix must be pulling packages from a default repository. Are there multiple repositories? How do I pick which repository to use?Be the first to know when I post cool stuffSubscribe to get my latest posts by email.Share onTwitter Facebook LinkedInLoading comments ...",
    "summary": "- Nix is a tool for configuring software environments using source files, and it's gaining popularity among developers.\n- Nix is faster than Ansible and has a concept of state, allowing for quicker configuration changes.\n- Nix optimizes for local configuration and allows for atomic changes, making it easier to manage and upgrade systems.",
    "hn_title": "My First Impressions of Nix",
    "original_title": "My First Impressions of Nix",
    "score": 263,
    "hn_content": "- Nix is a build system that embraces functional programming concepts and allows for reproducible, cached builds.\n- It uses the concept of \"derivations\" to specify how packages are built and stored in the Nix Store.\n- Nix packages can be built with specific versions of dependencies, but this requires additional effort and overrides in Nixpkgs.\n- Nixpkgs aims to minimize the number of package versions in use at one time for maintainability reasons.\n- Python applications can be packaged with Nix, but it may require specifying exact versions of dependencies and testing.\n- Users of Nix packages should be aware of potential breaking changes when updating dependencies.\n- The Nix build system is performant and has caching capabilities to speed up builds.\n- Nix embraces the idea of a purely functional build system, treating builds as pure functions with memoization.\n- The Nix language and ecosystem consist of various tools such as the Nix language, Nix Package Manager, NixOS, Hydra, nix shell, and NixOps.\n- There may be a learning curve and complexity associated with using Nix, but it offers benefits such as reproducibility and deterministic builds.- Some members of the Nix community have a bad reputation for being defensive and frustrated.\n- Nix is known for its adaptability and ability to run useful software as it exists.\n- Nixpkgs does not assume Denver and runs package tests to prevent breakages before merging.\n- The Nix community does not expect perfectly tested, bug-free software.\n- Some Python users have had issues with breaking changes in minor versions and dependencies.\n- There is no universal law of nature that all projects and ecosystems must follow SemVer.\n- The Django site specifies that feature releases should be mostly backwards-compatible.\n- Nix provides reproducibility through its build system and allows for locking versions.\n- Nix optimizes for local configuration and allows easy local patching.\n- Nix is a package manager, a language for expressing builds, and the foundation for NixOS.\n- NixOS is a Linux distribution built on Nix that uses declarative configuration.\n- Nix has similarities to Docker, but does not rely on containers.\n- Nix provides a sandboxed environment for running software.\n- Nix is analogous to Arch Linux, Haskell, Ansible, and other tools.\n- Nix enables the precise control of package versions and dependencies.\n- Nix helps manage system configurations and automates deployment.\n- Nix provides declarative configuration, easy local patching, and is compatible with major distros.\n- Nixpkgs is a large package database containing many closed-source applications.\n- Nix builds are easily reproducible and offer functional and deterministic results.",
    "hn_summary": "- Nix is a build system that embraces functional programming concepts and allows for reproducible, cached builds.\n- Nixpkgs aims to minimize the number of package versions in use at one time for maintainability reasons.\n- Nix provides reproducibility through its build system and allows for locking versions."
  }
]
