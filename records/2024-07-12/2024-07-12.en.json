[
  {
    "id": 40940225,
    "title": "WebVM is a server-less virtual Linux environment running client-side",
    "originLink": "https://webvm.io/",
    "originBody": "qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+|WebVM is a server-less virtual Linux environment running fully client-side| in HTML5/WebAssembly.||WebVM is powered by the CheerpX virtualization engine, which enables safe,| sandboxed client-side execution of x86 binaries on any browser.|| CheerpX includes an x86-to-WebAssembly JIT compiler, a virtual block-based| file system, and a Linux syscall emulator.||[NEW!] Your own WebVM with custom images via Dockerfile:|| https://leaningtech.com/mini-webvm-your-linux-box-from-dockerfile-via-wasm|+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ Welcome to WebVM. If unsure, try these examples: python3 examples/python3/fibonacci.py gcc -o helloworld examples/c/helloworld.c && ./helloworld objdump -d ./helloworldless -M vim examples/c/helloworld.c curl --max-time 15 parrot.live # requires networking",
    "commentLink": "https://news.ycombinator.com/item?id=40940225",
    "commentBody": "WebVM is a server-less virtual Linux environment running client-side (webvm.io)571 points by sebg 22 hours agohidepastfavorite119 comments smusamashah 21 hours agoThere are a few others VMs https://copy.sh/v86/ https://bellard.org/jslinux/ https://jamesfriend.com.au/pce-js/ (https://github.com/jsdf/pce) https://www.pcjs.org/ (lots of hardware and OSes) (https://github.com/jeffpar/pcjs) Mac OS https://infinitemac.org/ (https://blog.persistent.info/2023/03/infinitemac-dot-org.htm...) https://jamesfriend.com.au/projects/basiliskii/BasiliskII-wo... https://jamesfriend.com.au/pce-js/pce-js-apps/ reply zamadatix 19 hours agoparentThese are full system emulators, this is more a compatibility layer (\"Linux syscall emulator\") for emulated userland binaries. I.e. a bit higher level and a bit more usable/flexible as a result. reply Recursing 18 hours agorootparentI think it's also much faster in practice reply grondilu 5 hours agorootparentIt doesn't seem so at all from what I've tried. reply Recursing 4 hours agorootparentWhat have you tried? On webvm.io > $ time python3 -c \"max(range(10**7))\" real 0m1.558s On https://copy.sh/v86/?profile=archlinux > # time python3 -c \"max(range(10**7))\" real 0m5.283s reply 3abiton 11 hours agorootparentprevI'm curious, but what's the catch? reply TimTheTinker 5 hours agorootparentBeyond the userland-only limitation, syscall emulation is probably incomplete at best[0], and likely doesn't pass the Linux test suite for the subset of calls that are supported. To get a flavor of the work required when emulating the kernel, you can read up on the work it took Windows developers to emulate Linux syscalls on top of the NT kernel for WSL 1 here: https://learn.microsoft.com/en-us/archive/blogs/wsl/wsl-syst... WSL devs eventually abandoned that effort in favor of running the actual kernel in a VM -- that's how WSL 2 is implemented. [0] I'd be surprised if sys_fork actually forks the process using a web worker, for example. reply creshal 10 hours agorootparentprevThe obvious one is that it is userland-only, you don't get to play around with custom kernels being emulated on the hardware layer. reply Recursing 4 hours agorootparentprevIt's a commercial project and not open-source https://news.ycombinator.com/item?id=40944412 reply terr-dav 1 hour agoparentprevWhere do the most advanced minecraft computers[1] land on the scale of sophistication compared to all these + WebVM? [1] https://duckduckgo.com/?q=comptuers+built+in+minecraft reply baudaux 21 hours agoparentprevhttps://exaequos.com reply dchest 8 hours agoprevLooks like the underlying project is not open source: \"WebVM is based on a CheerpX build that we host. We encourage users to deploy their own versions of WebVM, but we don’t currently allow self-hosting of the CheerpX engine itself. This public build of CheerpX is provided as-is and is free to use for technological exploration, testing and non-commercial uses. If you want to build a product on top of CheerpX/WebVM, please get in touch:\" https://labs.leaningtech.com/blog/mini-webvm-your-linux-box-... reply jckahn 5 hours agoparentThat’s a shame. I was excited at first, but my interest in this project is now gone. reply Zetaphor 5 hours agorootparentAgreed, even the ability to self host the closed source WASM binary would have been acceptable. reply apignotti 22 hours agoprevHi, Lead Dev of WebVM and CTO of Leaning Technologies here. Happy to answer any questions you might have... potentially in a few hours, it's quite late here :-) Even better, consider joining our Discord: https://discord.gg/yTNZgySKGa reply bilekas 21 hours agoparentHi thanks for taking part! I have probably very silly questions but why? What are the use cases you Invision? Last but not least, an IRC server/channel in lieu of discord? I'm stuck in my ways! Nice work too, I hope this doesn't come across as flippant. EDIT: Side note, I opened vi and escape is.. well maybe a browser issue but 'cant exit vi' .. Never thought id say that again reply gwd 11 hours agorootparent> Last but not least, an IRC server/channel in lieu of discord? I'm stuck in my ways! IRC is nice for people who use it, but it has a huuuuge barrier to entry for new users. The very first thing you have to do is choose a client out of a list of 100 clients, half of which haven't been maintained since 2003. Then you need to set up a bouncer somewhere so that you can see messages that were posted when you were away. Those two things will filter out 99.99% of new people. Or, you could use Discord (or Matrix); people sign up for an account and that's it. Our open-source project switched from IRC to Matrix (after having been on IRC for over a decade before that), and got a noticeable uptick in engagement. Unfortunately, Matrix still has some long-standing UI quirks, which would make me reluctant to use it in a context where I want people to pay me money; particularly if some of those are non-technical users. reply jankovicsandras 3 hours agorootparent\"huuuuge barrier to entry for new users\" 1. go to https://web.libera.chat/ 2. click Start (as a Guest account) reply gwd 3 hours agorootparent3. Discover you can't join a lot of channels because you're not registered; or worse, that your messages sent to the channel are silently discarded And you still have the problem that if you're in the wrong time zone and happen to close the window, you may miss responses. There are advantages to having high barriers to entry for a community; Ham radio is essentially a global chat room with an entrance exam, for instance. But there are costs too. reply zja 19 hours agorootparentprevAnswer to the second question is that no one uses IRC. The trend of using discord over forums does suck though, it’s really hard to search old discussions. reply djbusby 17 hours agorootparentAnother option is a community hosted Mattermost (it's like Discord/Slack/IRC). It's pretty easy to run, all the chat are in PostgreSQL so it's \"easy\" to produce archive logs (like IRC has). Or do any other magic. reply Mutjake 11 hours agorootparentprevIRC\"Answer to the second question is that no one uses IRC.\" I agree! I am a nobody and forever will be a proud nobody! reply Retr0id 13 hours agorootparentprevThe target audience for web-based linux vm is much more likely to use IRC reply edm0nd 15 hours agorootparentprevIRC still has 270k users in 2024 via https://netsplit.de/ It's not like it used to be in the 90s but that's still something. reply varenc 11 hours agorootparentDiscord has around 200 million monthly active users. At least according to one albeit dubious source: https://backlinko.com/discord-users#discord-monthly-active-u... Seems reasonable to go with the growing platform with 1000x more users. reply JoeMattiello 11 hours agorootparentprevHow many of those are bots. 270k is basically no one. IRC is dead reply bilekas 19 hours agorootparentprevI use IRC.. Discord is fine, I just prefer to have both. reply malux85 18 hours agorootparentWhat are some good places for a technical / programming hangouts on IRC? Can you point me at some materials please? reply gwd 10 hours agorootparentAlso see https://oftc.net/, where a bunch of older-school development projects hang out. (That's where the Xen Project moved to after freenode imploded, before eventually switching to Matrix.) reply Carrok 18 hours agorootparentprevhttps://libera.chat/ A channel listing will give you _many_ results. reply pests 17 hours agorootparentI remember back in the day I wouldn't even check the listings, just joining whatever #topic or #hobby or #software and finding it filled with people almost always. reply imadj 14 hours agorootparentprev> What are the use cases you Invision? Not OP, but they said the following in an answer to similar question: https://news.ycombinator.com/item?id=40940707 \"The technology is extremely flexibile, off the top of my head: * Education (Linux, Programming, Security, ...) * Live docs for arbitrary languages and binary libraries * Preservation of historical software and games * Virtualization of legacy Windows enterprise apps. * Dev environment for Web IDEs Just a few examples, the list could go on for long\" reply jgraham20 20 hours agorootparentprevMay it is a browser issue. I was able to exit with ':q' reply bilekas 20 hours agorootparentRight, I can go to insert mode, but to leave is, tricky. I found going to replace mode and escape worked in FF. I'm sure its a browser issue, I also have vimium so probably a nightmare of hooks! reply eonpi 16 hours agorootparentHave you tried with ? aka Ctrl-[, or ^[, as seen sometimes in the terminal. This works for me in most terminals as an alternative to Escape when using vi/vim. reply kuekacang 12 hours agorootparentprevIn mobile (at least in mine, firefox) the input become buferred autocomplete and when accepted via return, the typed word gets duplicated before entered. Workaround is to accept via space and delete as necessary reply d_philla 20 hours agoparentprevHey Alessandro - I'm a huge fan of WebVM + y'all's other work at Leaning Tech. Curious - is your implementation of how you wrangled lwIP for the networking piece open source? I had previously read the article you linked elsewhere, re: networking, but would love to see the specifics of your approach there. reply apignotti 12 hours agorootparentAll the components of the network stack are open source: namely the Tailscale go client and the lwip networking stack. Some (not fully trivial) glue code is also required, which is not currently published. We do plan to cleanup the stack and release it as one of our upcoming internship projects. reply pmontra 11 hours agoparentprevI tested in Firefox on Linux. It works well except: 1. Copy and paste does not work and this could be bad for basically every editing task (programming etc). 2. The screen freezes sometimes and keypresses don't appear. Usually pressing backspace unfreezes the screen. Firefox about:performance doesn't show any CPU usage in the tab. Linux's htop (on my machine) doesn't show anything strange with Firefox. And a question: I saw that new files persist after closing the tab and opening it again. I didn't investigate my localstorage etc with Developer Tools. Did you use that or some other browser feature? How truly persistent are those files? reply apignotti 9 hours agorootparent1. Support for traditional copy/paste shortcuts is problematic, since Ctrl-C is interpreted as the SIGINT signal. Copying via the mouse menu should work. Pasting is supported via Ctrl+Shift+V. 2. We had these reported from several users and we will investigate as soon as we can: https://github.com/leaningtech/webvm/issues/102 To answer you question: filesystem chunks are cached/persisted using a local IndexedDB. It provides a fairly strong level of persistence, although the user can wipe it out of course. reply pmontra 6 hours agorootparentThanks for the reply. Copy&paste via menu works (I didn't think about it) and Ctrl+Shift+V works too. Ctrl+Shift+C opens Firefox's Developer Tools. It's difficult to find a reasonable shortcut. reply efreak 3 hours agorootparent> it's difficult to find a reasonable shortcut I don't believe I've ever seen Ctrl+insert and shift+insert makes to anything _other_ than copy and paste. reply theanonymousone 22 hours agoparentprevHi and thanks for the effort. Is there a roadmap for supporting GUI applications? reply apignotti 22 hours agorootparentYes, we are working on it right now. The immediate priority is booting a complete desktop environment. Next steps will include 3D graphics. reply mcnesium 21 hours agorootparentWhat are some use cases for this? reply apignotti 21 hours agorootparentThe technology is extremely flexibile, off the top of my head: * Education (Linux, Programming, Security, ...) * Live docs for arbitrary languages and binary libraries * Preservation of historical software and games * Virtualization of legacy Windows enterprise apps. * Dev environment for Web IDEs Just a few examples, the list could go on for long reply koolala 19 hours agorootparentYou want it to be 3D!! Loading this is a multiplayer social VR sandboxed WebXR environment would be so amazing one day! I wonder if it could all run off-thread in a Worker and one day maybe use webGPU to update textures async! I hope computers can be fun and care free like a game one day and AI companions will let people learn and do anything! Thanks for working to make this! reply osrec 9 hours agoparentprevCould it be possible to run containers within this environment? I ask because I personally think we are headed towards a future where web browsers are just sandboxed container environments where we download and run containers for each app... Eventually, even calling them \"web\" browsers might feel a little odd, almost the way in which we \"dial\" a phone number today, even though we now mostly key it in on a touch screen. reply apignotti 9 hours agorootparentWell, the environment itself is a container. Supporting Linux \"namespaces\" used to implement containers is also a possibility. As described in the project README we provide a GitHub Action workflow to convert Dockerfiles into bootable images for WebVM: https://github.com/leaningtech/webvm reply koolala 20 hours agoparentprevAre you associated with Tail Scale? Is Tail Scale like a proxy? Are other options available - is Tail Scale open source too? edit: Saw your other reply! https://labs.leaningtech.com/blog/webvm-virtual-machine-with... reply debarshri 22 hours agoparentprevHello there, I havent tried webvm yet but looks similar to v86. There were lot of network related challenges working with v86, for eg. You cant do curl, DNS resolution does not work out of the box. How did you address these challenges in webvm. I think it is very interesting and exciting what you guys are building. reply apignotti 22 hours agorootparentWe support networking via Tailscale, we wrote a detailed blog post some time ago: https://labs.leaningtech.com/blog/webvm-virtual-machine-with... reply hexo 11 hours agoprevLet me fix the title: \"WebVM is a wasm VM running in browser\" reply Animats 12 hours agoprevLove the buzzwords. It's a Linux command line environment running in a browser. Slowly. reply parasti 12 hours agoparentHave to agree, that description is more confusing than anything. I guess JS code that a web browser executes is considered \"serverless\" now. reply koreankid227 11 hours agoparentprevLol I was so confused reply languagehacker 1 hour agoprevFirst thing I tried was `apt-get install emacs` and it hung on downloading. Editor preferences aside, I assume any nonstandard package will have the same problem. Interesting idea, but apart from trivial glue you may not need more than a container or unikernel for anyway, I'm struggling to understand a practical use case. reply jfmc 13 hours agoprevNote that \"CheerpX enables you to run existing 32-bit x86 native binaries\". For some reason support for wasm64 (in browsers) has been stagnated for years, which is a pity. reply pjmlp 12 hours agoparentMost WASM, WebGL and WebGPU features take ages in browsers, measured in decades. reply fragmede 22 hours agoprevNetworking provided by Tailscale. Clever! Works around some potential issues. reply apignotti 22 hours agoparentUsing Tailscale is the most effective solution for in-browser networking, we wrote at length about the topic some time ago: https://labs.leaningtech.com/blog/webvm-virtual-machine-with... reply saurik 21 hours agorootparentI am really confused by one part of your blog post: it says that you had to embed Lwip as applications don't send/receive IP packets... but you also describe this as x86 virtualization running specifically-Linux, and Linux absolutely has its own TCP stack, so I don't understand what Lwip is actually accomplishing here... I would have thought you'd essentially be implemented as a paravirtualized network adapter, but then the article mentions system calls? I guess I just fundamentally don't understand at what level of the stack you have this implemented... did you replace the Linux kernel and reimplement its entire system call layer? reply ncruces 20 hours agorootparentYes: https://labs.leaningtech.com/cheerpx#cheerpx-versus-v86jslin... Also, a little below: CheerpX currently focuses on user mode (ring 3), and does not fully emulate the kernel (ring 0). We do, however, implement a subset of the Linux system call interface, which is enough to run most applications. reply ec109685 19 hours agorootparentHave you looked at gvisor to help implement some of the outstanding system calls given it’s a kernel written in go? reply tech234a 19 hours agorootparentprevCould be cool to see an implementation using Chrome's experimental Direct Sockets API [0]. The API requires using a form of new web packaging known as Isolated Web Apps which have a strict CSP [1] though, so it might not be possible to use it for this since I'd assume it's dynamically executing a lot of code. [0]: https://github.com/WICG/direct-sockets/blob/main/docs/explai... [1]: https://github.com/WICG/isolated-web-apps/blob/main/README.m... reply cchance 20 hours agorootparentprevCan we use custom derp/headscale setups? Would be interesting even for use internally then, hell rolling custom versions with preloaded software could also be pretty cool all connected to a corporate headscale/derp. reply bradfitz 15 hours agorootparentPer their docs, yes: https://github.com/leaningtech/webvm/blob/main/docs/Tailscal... reply spankalee 15 hours agoprevCool, but very often my keystrokes aren't showing up, making this pretty unusable. Also, for the use case of local dev environment for Web IDEs, this doesn't look fast enough. node --version runs fast after the first time, but executing any JavaScript is very, very slow: user@:~$ time node -e \"console.log('Hello')\" Hello real 0m7.693s user 0m0.012s sys 0m0.000s reply tamimio 21 hours agoprevWhat a practical use case of such thing? reply apignotti 21 hours agoparent(Same answer as a more deeply nested comment below) The technology is extremely flexibile, off the top of my head: * Education (Linux, Programming, Security, ...) * Live docs for arbitrary languages and binary libraries * Preservation of historical software and games * Virtualization of legacy Windows enterprise apps. * Dev environment for Web IDEs Just a few examples, the list could go on for long reply getpokedagain 17 hours agorootparentinstalling emacs in my web browser? reply tamimio 21 hours agorootparentprevVery interesting, thanks! and good luck on the project. reply _moof 11 hours agoparentprevMaking your senior engineers take off their glasses, sigh, and pinch the bridge of their nose. reply revskill 11 hours agorootparentI don't get you, how about the juniors ? reply lupusreal 10 hours agorootparentNod their heads with approval to mask their confusion. reply old_hat 13 hours agoparentprevThis isn't WebVM specifically, but one example using WebContainers we've built is https://demo.graphweaver.com/ This runs a NodeJS server and frontend in your browser so you can see what Graphweaver can do without having to install or run anything more than a browser on your computer. The example has an SQLite database joined to a REST API. Graphweaver serves a GraphQL API with the result. reply hexo 7 hours agoparentprevJust a curiosity to play a bit reply danans 21 hours agoprevCan it be made to work offline and keep state locally between reloads? reply apignotti 12 hours agoparentWorking offline is not truly possible since that would require downloading the full 2GB disk image, which is something that is deliberately avoided by the system architecture. State is already locally maintained via IndexedDB, no data is ever sent back or saved on our servers. reply theragra 11 hours agorootparentFor many people that's not an issues, so maybe you need to add it someday. reply alex14fr 22 hours agoprevThis feels way slower than https://bellard.org/jslinux/ reply nullifidian 20 hours agoparentI tried compiling mandelbrot.c (single threaded one) from the benchmarks game and execution on jslinux took 12 seconds, while webvm completed it in 1.2 sec. The host takes 0.03 sec with the same gcc flags. reply alex14fr 11 hours agorootparentIt may be faster for this particular benchmark, yet it feels slow and sluggish for interactive use. reply jedisct1 22 hours agoparentprevHard to compete with Bellard :) reply jbritton 13 hours agoprevWhere are files located, server, client, browser local storage? How do you import a .exe compiled elsewhere, curl, scp? This is a Linux environment, but it can run Windows apps? How does that work? reply nyanpasu64 18 hours agoprevCan we port Adobe Flash to WebVM to avoid having to pay for \"CheerpX for Flash\"? reply fragmede 14 hours agoparentWhy do you want to scam them out of their hard work? reply unsolvedcypher 17 hours agoparentprevHave you tried Ruffle? It's also capable of running Flash in the browser. reply marmaduke 6 hours agoprevCan this run NFS or other network filesystems? reply remram 18 hours agoprevTotally unusable on mobile unfortunately (at least Firefox and Chrome on Android). Might be a limitation of how the keyboard works, but every command gets duplicated (at least) when I enter it. reply zoom6628 18 hours agoparentUsing Brave browser on iPhoneXR works nicely. reply supportengineer 20 hours agoprev$ uptime Error: /proc must be mounted reply xyst 21 hours agoprevLooks like it’s broken on mobile reply nasretdinov 20 hours agoparentWorked on stock iOS in Safari! Must've taken them ages to debug... reply Zambyte 20 hours agoparentprevWorks for me in Firefox Focus on Android reply bfung 13 hours agoprevserver-less - like the server that runs the browser that the VM runs inside. /sarcasm reply ptspts 21 hours agoprevIt doesn't work in my Chrome on Android. The cursor is blinking, nothing happens. If a browser is not supported, the site should display a specific error message. reply cebert 20 hours agoparentWorks fine for me on Android reply ladysera 19 hours agoprevHi, this looks really amazing, will give it a try later thanks reply sitkack 20 hours agoprevIt even emulates fork! reply revskill 12 hours agoprevSweet, could i run a nodejs http api directly from browser ? reply dangoodmanUT 19 hours agoprevungodly slow on apple silicon reply doctor_eval 19 hours agoprevI first used Slackware Linux off floppies in the mid 90s, yet every time I see linux in the browser I am gobsmacked. I’ve said for a while that a browser is basically its own operating system. Now it can host operating systems! reply me551ah 12 hours agoprevNow if only I could install chrome via apt and run it inside of this linux machine. reply matt3210 22 hours agoprevroot password? reply fragmede 22 hours agoparent\"password\", per https://github.com/leaningtech/webvm/blob/main/dockerfiles/d... reply jkhanlar 15 hours agoprevbash: man: command not found reply localfirst 20 hours agoprevcan i run a webserver that can talk with the internet and others can call reply fragmede 14 hours agoparentTailscale funnel should work. I'll double-check when I'm back home if someone else hasn't. reply liamYC 19 hours agoparentprevI suspect you’d have trouble opening ports on the client’s machine. Maybe you could use a dedicated http proxy the client talks to reply koolala 17 hours agoparentprevA custom service worker might be able to proxy for you without needing Tail Scale. reply Uptrenda 13 hours agoprevNow just needs some kind of hack for networking through a browser extension and this would be so awesome. reply talldayo 22 hours agoprev [6 more] [flagged] fragmede 22 hours agoparentOne can still appreciate how well the violinist on the Titanic played while listening from a life-boat (if lucky enough to have been on one). reply satvikpendem 22 hours agoparentprevSomething being cool technically doesn't mean it's wise philosophically. reply darby_nine 22 hours agoparentprevGiven the absolute dysfunction on how browser standards are adopted this is expected behavior. Huge security risk, huge implementation cost, very low competition, and people hate electron apps. reply nxobject 22 hours agoparentprev [–] It’s a cool hack - what can I say. Surprised at how it instantly loads to a usable shell, too, if there’s a lesson to be learned there. Is there a freeze-dried memory image being used? reply apignotti 22 hours agorootparent [–] Absolutely not. The app starts at the entry point and execute instructions as normal. Both the main executable and libraries are all loaded on demand from a disk image delivered chunk-by-chunk over HTTP. The disk backend is based Clouflare workers with multiple levels of caching to maximize perf. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WebVM is a server-less virtual Linux environment that runs entirely client-side using HTML5 and WebAssembly, powered by the CheerpX virtualization engine.",
      "It allows safe, sandboxed execution of x86 binaries in any browser, featuring an x86-to-WebAssembly JIT (Just-In-Time) compiler, a virtual block-based file system, and a Linux syscall emulator.",
      "New feature: Users can now create custom WebVM images using Dockerfile, enhancing flexibility and customization."
    ],
    "commentSummary": [
      "WebVM is a server-less virtual Linux environment running client-side, utilizing a CheerpX build hosted by Leaning Technologies and supporting networking via Tailscale.",
      "Unlike full system emulators, WebVM acts as a Linux syscall emulator for userland binaries, offering greater flexibility and speed.",
      "It is not open-source, and self-hosting the CheerpX engine is prohibited, making it suitable for education, live documentation, software preservation, and web IDE development, though it has limitations like performance issues on mobile devices and partial offline functionality."
    ],
    "points": 572,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1720728981
  },
  {
    "id": 40944505,
    "title": "AT&T says criminals stole phone records of 'nearly all' customers in data breach",
    "originLink": "https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/",
    "originBody": "Login Search Search Startups Venture Apple Security AI Apps Events Startup Battlefield More Close Submenu Fintech Cloud Computing Layoffs Hardware Google Microsoft Transportation EVs Meta Instagram Amazon TikTok Newsletters Podcasts Partner Content Crunchboard Jobs Contact Us Featured Article AT&T says criminals stole phone records of ‘nearly all’ customers in new data breach Stolen data includes millions of AT&T customer phone numbers, calling and text records, and location-related data Zack Whittaker 3:27 AM PDT • July 12, 2024 Comment Image Credits: Mark Makela / Getty Images U.S. phone giant AT&T confirmed Friday it will begin notifying millions of consumers about a fresh data breach that allowed cybercriminals to steal the phone records of “nearly all” of its customers, a company spokesperson told TechCrunch. In a statement, AT&T said that the stolen data contains phone numbers of both cellular and landline customers, as well as AT&T records of calls and text messages — such as who contacted who by phone or text — during a six-month period between May 1, 2022 and October 31, 2022. AT&T said some of the stolen data includes more recent records from January 2, 2023 for a smaller but unspecified number of customers. The stolen data also includes call records of customers with phone service from other cell carriers that rely on AT&T’s network, the company said. AT&T said the stolen data “does not contain the content of calls or texts,” but does include calling and texting records that an AT&T phone number interacted with during the six-month period, as well as the total count of a customer’s calls and texts, and call durations — information that is often referred to as metadata. The stolen data does not include the time or date of calls or texts, AT&T said. Some of the stolen records include cell site identification numbers associated with phone calls and text messages, information that can be used to determine the approximate location of where a call was made or text message sent. In all, the phone giant said it will notify around 110 million AT&T customers of the data breach, company spokesperson Andrea Huguely told TechCrunch. AT&T published a website with information for customers about the data incident. AT&T also disclosed the data breach in a filing with regulators before the market opened on Friday. Breach linked to Snowflake AT&T said it learned of the data breach on April 19, and that it was unrelated to its earlier security incident in March. AT&T’s Huguely told TechCrunch that the most recent compromise of customer records were stolen from the cloud data giant Snowflake during a recent spate of data thefts targeting Snowflake’s customers. Snowflake allows its corporate customers, like tech companies and telcos, to analyze huge amounts of customer data in the cloud. It’s not clear for what reason AT&T was storing customer data in Snowflake, and the spokesperson would not say. AT&T is the latest company in recent weeks to confirm it had data stolen from Snowflake, following Ticketmaster and LendingTree subsidiary QuoteWizard, and others. Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts, a security feature that the cloud data giant did not enforce or require its customers to use. Cybersecurity incident response firm Mandiant, which Snowflake called in to help with notifying customers, later said about 165 Snowflake customers had a “significant volume of data” stolen from their customer accounts. Mandiant attributed the breach to an as-yet-uncategorized cybercriminal group tracked only as UNC5537. Mandiant’s researchers say the hackers are financially motivated and have members in North America and at least one member in Turkey. Some of the other corporate victims of the Snowflake account thefts had data subsequently published on known cybercrime forums. For AT&T’s part, the company said that it does not believe that the data is publicly available at this time. AT&T’s statement said it was working with law enforcement to arrest the cybercriminals involved in the breach. AT&T said that “at least one person has been apprehended.” AT&T’s spokesperson said that the arrested individual was not an AT&T employee, but deferred questions about the alleged criminals to the FBI. An FBI spokesperson confirmed to TechCrunch on Friday that after the phone giant contacted the agency to report the breach, AT&T, the FBI and the Department of Justice agreed to delay notifying the public and customers on two occasions, citing “potential risks to national security and/or public safety.” “AT&T, FBI, and DOJ worked collaboratively through the first and second delay process, all while sharing key threat intelligence to bolster FBI investigative equities and to assist AT&T’s incident response work,” the FBI spokesperson said. The FBI did not comment on the arrest of one of the alleged cybercriminals. This is the second security incident AT&T has disclosed this year. AT&T was forced to reset the account passcodes of millions of its customers after a cache of customer account information — including encrypted passcodes for accessing AT&T customer accounts — was published on a cybercrime forum. A security researcher told TechCrunch at the time that the encrypted passcodes could be easily decrypted, prompting AT&T to take precautionary action to protect customer accounts. Read more on TechCrunch: Data breach exposes millions of mSpy spyware customers Apple warns iPhone users in 98 countries of spyware attacks Evolve Bank says ransomware gang stole personal data on millions of customers OpenAI breach is a reminder that AI companies are treasure troves for hackers Updated with comment from the FBI. More TechCrunch Get the industry’s biggest tech news Explore all newsletters TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices TechCrunch Fintech The latest Fintech news and analysis, delivered every Tuesday. Add TechCrunch Fintech to your subscription choices TechCrunch Mobility TechCrunch Mobility is your destination for transportation news and insight. Add TechCrunch Mobility to your subscription choices No newsletters selected No newsletters Email address (required) Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Tags AT&T, cyberattack, cybersecurity, data breach, mvno, Snowflake AI Experiment finds AI boosts creativity individually — but lowers it collectively Devin Coldewey 47 mins ago A new study examines whether AI could be an automated helpmeet in creative tasks, with mixed results: it appeared to help less naturally creative people write more original stories —… Featured Article HeadSpin, whose founder is in prison for fraud, sold to PE firm in fire sale, sources say In total, HeadSpin raised $117 million since its 2015 inception and was last valued at $1.1 billion in 2020. Mary Ann Azevedo 52 mins ago Government & Policy New Senate bill seeks to protect artists’ and journalists’ content from AI use Aisha Malik 1 hour ago A bipartisan group of senators has introduced a new bill that seeks to protect artists, songwriters and journalists from having their content used to train AI models or generate AI… Venture From Ethan Choi to Spencer Peterson, venture capitalists continue to play musical chairs Rebecca Szkutak 2 hours ago When Keith Rabois announced he was leaving Founders Fund to return to Khosla Ventures in January, it came as a shock to many in the venture capital ecosystem — and… Transportation Archer’s vision of an air taxi network could benefit from Southwest customer data Rebecca Bellan 2 hours ago Archer Aviation and Southwest Airlines are teaming up to figure out what it will take to build out a network of electric air taxis at California airports. Southwest’s customer data… Apps Wikipedia’s mobile website finally gets a dark mode — here’s how to turn it on Ivan Mehta 3 hours ago If you visited the Wikipedia website on mobile this week, you might have seen a pop-up indicating that dark mode is ready for prime time. Featured Article What the AT&T phone records data breach means for you The giant U.S. telco lost the information of around 110 million customers. Here’s what you need to know. Lorenzo Franceschi-Bicchierai 3 hours ago Space SpaceX Falcon 9 suffers rare failure on orbit during Starlink deployment Aria Alamalhodaei 4 hours ago The error brings to a close SpaceX’s incredible streak of 335 flawless launches across the company’s Falcon family of rockets, which also includes the more powerful Falcon Heavy. Commerce Amazon AI chatbot Rufus is now live for all US customers Sarah Perez 5 hours ago The AI chatbot has been trained on Amazon’s product catalog, customer reviews, community Q&As, and other public information found around the web. Social More bad news for Elon Musk after X user’s legal challenge to shadowban prevails Natasha Lomas 6 hours ago If X continues to violate Europe’s data protection rules, the company is on the hook for fines of up to €4,000 per day. Climate A startup set out to fight climate change — it did it by helping plumbers Mike Butcher 6 hours ago HERO Software has closed a €40 million Series B financing round, and plans to expand across Europe. Climate Fusion pioneer Commonwealth Fusion Systems is selling core magnet tech to the University of Wisconsin Tim De Chant 7 hours ago Fusion power may still be a few years away, but one startup is laying the groundwork for what it hopes will become a bustling sector of the economy. Enterprise Boston VCs are pleased that HubSpot will remain an independent company Ron Miller 8 hours ago For months, rumors persisted that Google, and perhaps others, were interested in buying HubSpot, a Boston-based CRM and marketing software company. HubSpot’s market cap ballooned as the rumors persisted, eventually… Apps CapCut will stop offering free cloud storage from August 5 Ivan Mehta 8 hours ago ByteDance’s video editing app CapCut will stop offering free cloud storage to host creative assets starting August 5. In the past few days, users have received notifications about CapCut changing… Government & Policy Europe confirms first clutch of DSA grievances on Elon Musk’s X Natasha Lomas 8 hours ago The platform formerly known as Twitter has earned the dubious honor of being the first very large online platform (VLOP) to face a preliminary finding of breaching the European Union’s… Featured Article AT&T says criminals stole phone records of ‘nearly all’ customers in new data breach The stolen data includes 110 million AT&T customer phone numbers, calling and text records, and some location-related data. Zack Whittaker 8 hours ago Image Credits: Mark Makela / Getty Images AI EU’s AI Act gets published in bloc’s Official Journal, starting clock on legal deadlines Natasha Lomas 10 hours ago The full and final text of the EU AI Act, the European Union’s landmark risk-based regulation for applications of artificial intelligence, has been published in the bloc’s Official Journal. In… Transportation Waymo cameras capture footage of person charged in alleged robotaxi tire slashings Maxwell Zeff 17 hours ago A Castro Valley resident was charged Thursday for allegedly slashing the tires of 17 Waymo robotaxis in San Francisco between June 24 and June 26, according to the city’s district… Featured Article SoftBank acquires UK AI chipmaker Graphcore While the figure of $500 million has been bandied around in various reports for months, in a press briefing early Thursday morning, Graphcore co-founder and CEO Nigel Toon remained coy on the details. Paul Sawers 20 hours ago Social X is building a ‘dislike’ button for downvoting replies Sarah Perez 21 hours ago Elon Musk’s X, formerly Twitter, is continuing to develop a downvoting feature that will be used to improve how replies are ranked. Although the company has not yet officially announced… Featured Article Data breach exposes millions of mSpy spyware customers A huge batch of mSpy customer service emails dating back to 2014 were stolen in a May data breach. Zack Whittaker 24 hours ago Fundraising Shark Tank-backed Kudos raises another $3M for healthier, cotton-based disposable diapers Dominic-Madori Davis 24 hours ago Kudos founder says her company makes a disposable diaper lined with 100% cotton, unlike the major competitors. Space ‘Wild Wild Space’ doc captures the risks and rivalries of the new space race Aria Alamalhodaei 1 day ago Astra CEO Chris Kemp is already pulling out of a parking spot when he warns the person in the passenger seat that he doesn’t have a valid driver’s license. “And… AI ‘Visual’ AI models might not see anything at all Devin Coldewey 1 day ago Although these companies’ claims are artfully couched, it’s clear that they want to express that the model sees in some sense of the word. Transportation Lucid revs up sales, Fisker makes a deal and Uber reignites an old fight Kirsten Korosec 1 day ago Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. Sign up here for free — just click TechCrunch Mobility! Did you… Apps Photo-sharing startup Retro spots Google Photos copying its idea and design Sarah Perez 1 day ago Retro CEO Nathan Sharp isn’t worrying just yet about Google’s plan to copy his app’s experience, despite the numerous similarities. Transportation Tesla reportedly delays ‘robotaxi’ event to October Sean O'Kane 1 day ago Tesla had internally planned to build the dedicated robotaxi and the $25,000 car, often referred to as the Model 2, on the same platform. AI How Apple Intelligence is changing the way you use Siri on your iPhone Aisha Malik 1 day ago Here’s a look at what’s going to change with Siri, and what the introduction of Apple Intelligence will allow you to do with the digital assistant. Media & Entertainment YouTube tempts creators with a half dozen new features for Shorts Sarah Perez 1 day ago The new YouTube features include those that will automatically transform longer videos into Shorts, among others. Robotics Exein raised $15M Series B to stop robotic arms going haywire Mike Butcher 1 day ago The capital will be used to expand in Europe, the U.S. and Asia. About TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Legal Terms of Service Privacy Policy RSS Terms of Use Privacy Placeholder 1 Privacy Placeholder 2 Privacy Placeholder 3 Privacy Placeholder 4 Code of Conduct About Our Ads Trending Tech Topics Tesla Robotaxi Apple Intelligence Spotify Fisker Samsung Galaxy Ring Tech Layoffs ChatGPT Facebook X YouTube Instagram LinkedIn Mastodon Threads © 2024 Yahoo. All rights reserved. Powered by WordPress VIP",
    "commentLink": "https://news.ycombinator.com/item?id=40944505",
    "commentBody": "AT&T says criminals stole phone records of 'nearly all' customers in data breach (techcrunch.com)283 points by impish9208 7 hours agohidepastfavorite214 comments smcin 5 hours agoAT&T stock has already bounced back from much of the initial -2.6% drop this morning, so the market thinks AT&T is immune. Meanwhile Snowflake is -3.9% down (they have many other customers than AT&T). https://www.marketwatch.com/investing/stock/T https://www.marketwatch.com/investing/stock/SNOW reply jader201 5 hours agoparentI never got the impression that the market ever cares about data breaches. It seems most companies are rarely held financially responsible for data breaches anyway. I would bet any effects you’re seeing in stocks is unrelated to this news. reply graybeardhacker 5 hours agorootparentI agree. This is precisely why breaches keep happening and will keep happening. It cost money to implement security. There's no cost benefit to spending that time and money since there are no consequences. Businesses do not spend money unless it will make them money or save them money. There needs to be a hefty federal fine on a per-affected-user basis for data breaches. Also a federal fine for each day a breach is unreported. That money should go into a pool which can be accessed by people who have their identity stolen. reply ThunderSizzle 5 hours agorootparentOr a lawsuit go through where someone can win quite a bit from from data leaks. If each person affected sued and won 100k or so, or even 1k, AT&T would definitely be spending money on security. But it appears $5 or credit monitoring from an agency that also gets hacked is sufficient for class action lawsuits. reply malcolmgreaves 4 hours agorootparentThat requires people to be rich enough to sue. It takes a lot of money and time to sue. Almost no one has enough resources to do this. The courts are not an effective way to implement this policy. Unless you only want rich people to be able to get justice. reply CityOfThrowaway 4 hours agorootparent110M people impacted = class action The lawyers work on contingency reply pas 2 hours agorootparentshowing damages is hard reply unixhero 4 hours agorootparentprevImagine the GDPR fine reply Borg3 4 hours agorootparentprevAnd rich people usually do deals off-court. You will pay me this and we are ok. Because its faster and both sides know they capabilities usually. reply dboreham 4 hours agorootparentprev> It cost money to implement security. Yes, but no amount of money will stop the data in a big database being stolen by someone sufficiently motivated to steal it. It's just bits on someone's disk. The only true solution is to not create the database. But then what would all the data scientists and their MBA masters so with their time? reply cm2187 4 hours agorootparentprevMost breaches are because of developper incompetence. Throwing money at it won't really help. You need better basic security skills. reply slg 2 hours agorootparentNo two people are incompetent in exactly the same way. Hiring two developers to review each other's code leads to better code because they will often find problems that the other one didn't see. In a well managed organization (admittedly not a trivial caveat these days), more people working on security leads to better security. reply cm2187 1 hour agorootparentCertainly, but for instance no sane developer should concatenate a string in a sql query unless there is absolutely certainty the string is safe. This should be reflex, not a matter of money or time. reply slg 57 minutes agorootparentPeople are alway going to make bad decisions. Sometimes that is out of a lack of experience or knowledge which can be fixed by better training (which also requires money). Other times it is out of apathy, laziness, or something else that can't be easily fixed. Either way, time and money can provide extra sets of eyes to find and fix those mistakes before they lead to a breach. reply rybosworld 4 hours agorootparentprevThere is some evidence that it does hurt stock prices: https://www.comparitech.com/blog/information-security/data-b... \"Stocks of breached companies on average underperformed the NASDAQ by -3.2% in the six months after a breach disclosure\" That said, it's not clear what the long term impact is on stock price (if there is any). reply teraflop 4 hours agorootparentUnfortunately, that analysis seems to have made absolutely no attempt to check whether the results are statistically significant. Pick 118 random companies at 118 random points in time. It's vanishingly unlikely that the average returns of that group will exactly track the NASDAQ returns over the following 60 days. It might underperform, or it might overperform. An underperformance of 3.2% could easily just be the result of random chance, and have nothing to do with data breaches. reply jkaptur 2 hours agorootparentMy hypothesis would be that companies with poor operational practices are more likely to underperform the index and have data breaches - in other words, that the study confuses cause and effect. This wouldn't be that hard to test. I suspect that the breached companies underperformed in the six months before the breach as well as the six months after. reply Terr_ 50 minutes agorootparentAlso, events which are not \"just\" data-leaks but also interruptions or degradation in regular operations. I suspect investors may be more sensitive to those events and their fallout, and such events more likely to either be caused by bad-practice or to be somehow connected to data-leaks. reply omoikane 29 minutes agorootparentprevMy reading is that the market thinks Snowflake takes the majority of the blame, and the content of the linked article seemed to suggest as much despite having only AT&T in the headline. reply smcin 5 hours agorootparentprevThey are very much related to the news, that's precisely why I linked to the stock charts: AT&T was flat overnight but opened (9am ET) with a -2.6% spike down, but has been recovering since. Their press release appears to have been Friday 7am ET shortly before market open [https://about.att.com/story/2024/addressing-illegal-download...]. Also as corroboration here's MarketWatch: \"AT&T’s stock slides 3% after company discloses hack of calls and texts\" [https://www.marketwatch.com/story/at-ts-stock-slides-2-9-aft...] reply soulofmischief 4 hours agorootparentI'm not saying there's no way the stock pullback wasn't caused by the hack, but it's also important to note that MarketWatch article only establishes correlation, not causation. reply seadan83 2 hours agorootparentMost linked financial news is auto-generated and auto-correlated. Lots of \"why did..\" when nobody knows, and frankly there often is no why. Perhaps that was the day a retirement fund shifted money, who knows. While this price movement is very well correlated, perhaps causal even, but marketwatch (and all similar bottom feeders that are just trying to make ad revenue), it's a case of a broken clock being right. Those financial news sites which link recent news to stocks, eg Yahoo, benzings, - those recent news headlines are just the same as ad tech now. It is noise. reply weezin 4 hours agorootparentprevReally should be up to the government to fine these companies and pay out to those effected to disincentivize lax security standards. reply hodgesrm 2 hours agorootparentWell, I guess we devs should also be looking at ourselves, then. A lot of the lax security comes from us collectively choosing to build applications using cloud services that talk to each other over the public internet. That pretty much describes the so-called \"modern data stack.\" reply kcmastrpc 4 hours agorootparentprevHow would such damages be assessed or proven? reply Eisenstein 4 hours agorootparentThey would be assessed according to rules written by people who are skilled at writing such rules. The rules would be evaluated by looking at data over time and revised as needed by experts in the industry who are as neutral as possible, maybe with some feedback from the public. The courts exist for any contention regarding responsibility. reply hodgesrm 3 hours agorootparentprev> I never got the impression that the market ever cares about data breaches. It seems most companies are rarely held financially responsible for data breaches anyway. This might also explain why there's little visible effect on other cloud database services either. After all, the attack is pretty simple and potentially affects any cloud database that allows access from the Internet. reply chung8123 4 hours agorootparentprevI think they will care a lot more when it directly impacts them. If all their text conversations were publicly available that would cause some outrage. reply darby_nine 4 hours agorootparentprevI'm certainly not going to defend negligence of data protection but it's extremely difficult to cost as a liability (naively, you might even consider it not a liability at all) without government oversight. reply nashashmi 4 hours agorootparentprevInsurance takes up a lot of the fallout from data breaches. reply xyst 3 hours agoparentprevIt’s priced in. reply squeegee_scream 4 hours agoprevIt's ok everyone! Protecting our data is one of AT&T's top priorities. > Protecting your data is one of our top priorities. We have confirmed the affected access point has been secured. > We hold ourselves to a high standard and commit to delivering the experience that you deserve. We constantly evaluate and enhance our security to address changing cybersecurity threats and work to create a secure environment for you. We invest in our network’s security using a broad array of resources including people, capital, and innovative technology advancements. I hope there's an enormous fine for this kind of negligence reply panarky 9 minutes agoparentThere may be no \"good\" telcos or big tech firms, but some are absolutely worse than others. AT&T is actively hostile in a way others aren't. reply xyst 4 hours agoparentprevThe “fine” will consist of a class action lawsuit that will eventually (3-4 years later) be bargained down to 1/2 the original claim. Lawyers take their 25% (or whatever cut was negotiated) fee. Then the impacted customers (assuming they submitted all of the claim paperwork) get paid out a few dollars. reply nashashmi 4 hours agoparentprevNot their fault. Snowflake was breached. And the data was with Snowflake. reply jeff_tyrrill 1 hour agorootparentYour contractor being breached means you were breached. reply hobs 1 hour agorootparentprevSnowflake was \"breached\" by AT&T users using the same password in Snowflake and another system that was breached. This is just trivial pivoting done with some guesswork done fairly well. reply graybeardhacker 4 hours agoprevFreeze your credit people! It's super easy. It's not a perfect fix but it's so trivial to do and it will help. https://www.usa.gov/credit-freeze You can unfreeze through an app whenever you want/need to. reply pavel_lishin 4 hours agoparentIs there any reason not to keep credit frozen permanently, only unfreezing it when you're making a large purchase that requires it? reply al_borland 4 hours agorootparentThis is how I have operated ever since the Equifax breach. Once that happened, none of the others seemed to matter, everything important for identity theft is out there. I've had no problems. Someone will try to run my credit, it will fail, then I ask which one they're trying to use, and I unfreeze it for a day. Some of them have the option to unfreeze for a single pull with a 1 time code (if I remember correctly), but when I tried to use that the person trying to pull the report seemed clueless, so I had to do the 1 day unfreeze. reply bee_rider 48 minutes agorootparentCredit is a weird ad-how system. At some point, I wonder if folks will realize that having an unfrozen credit report is a sign of imprudence. reply noodlesUK 4 hours agorootparentprevUnfortunately it isn’t an option in every country. In the U.S., you can freeze your credit for free, but in the UK, you can’t. I think we should get rid of the CRAs entirely, but that’s a conversation for another day. reply kodt 4 hours agorootparentprevOne interesting thing I ran into with frozen credit, is that you cannot sign up for USPS informed delivery without them running your credit as a method of address verification IIRC. If it is frozen the process gets stuck in limbo (at least it did many years ago when I ran into this situation) reply golf1052 2 hours agorootparentThis is no longer the case. I signed up for Informed Delivery last year with frozen credit with no issues. reply k4j8 4 hours agorootparentprevKeeping your credit frozen permanently is a great idea. Some of the credit agencies even encourage this with features such as a temporary unfreeze of your credit for a few days/weeks and then back to the permanently frozen state. reply troyvit 4 hours agorootparentprevThat's what I do. It also slows my roll. It's an extra step I have to take before making that large purchase or applying for anything that requires a credit check. reply yelling_cat 4 hours agorootparentIt's an extra step, but a surprisingly simple one. When I opened a checking account recently the bank told me which credit agency they'd use, and I unfroze that account and ChexSystems (another credit agency you should freeze with that is used specifically for new bank accounts) in five minutes using their automated systems. You can supply a re-freeze date when unfreezing as well so you don't need to remember to do that manually once you're approved. reply david422 4 hours agorootparentprevYep. This is what I did after the first Experian data breach, for peace of mind. I am probably financially lucky enough that I don't need to constantly be checking or using my credit... but honestly it seems like this is what everyone needs to be doing. reply rqtwteye 4 hours agorootparentprevThat’s what I do. But it’s a little bit of pain to unfreeze your credit with three bureaus when you want a new credit card. Wish there was a way to do this in one place. reply r3trohack3r 3 hours agorootparentAfter the first time unfreezing, I put the website URL, unlock pins, and concise instructions for all 3 as a single note in my password vault. Doing all 3 takes ~5minutes now - which can usually happen in parallel with whatever paperwork the vendor needs to get in order. reply tnel77 4 hours agorootparentprevIt’s a great idea! I only unfreeze my credit for big purchases like buying a house or car. reply lotsofpulp 4 hours agorootparentprevI open credit cards for the bonuses frequently enough that freezing my credit would be more inconvenience than it’s worth. Also, all the big bank websites seem to offer real time credit history monitoring for free, so I am betting I’ll just deal with any problem if/when they happen. reply neogodless 1 hour agoparentprevYou can also freeze your non-credit banking: https://www.chexsystems.com/security-freeze/place-freeze It was recommended that I do this after a checking account was opened using my identity. As others have stated, my default is \"frozen.\" I put temporary thaws on when applying for credit, though in some cases, you'll be informed exactly which agency/agencies will be queried, and may not need to unfreeze all of them. reply xyst 4 hours agoparentprevI typically don’t “freeze” my credit but do have a handful of services actively monitoring my credit for free (have been involved with many data breaches) and it’s included with my credit cards. > A credit freeze restricts access to your credit report So if I freeze my credit, this will also deny access to the monitoring services AND financial institutions, right? Side note: financial institutions often do “soft” credit pulls on active account holders to determine if they are eligible for credit limit increases. Have been growing my existing credit line for some time now without having to obtain additional credit cards. So far, close to $500K in unsecured credit. Seems more like a nuclear option. reply zzyzxd 3 hours agoparentprevI keep my credit frozen all the time, but still keep getting alerts about new \"no credit check\" bank accounts from companies like chime.com. Then I give them my PII again just to verify and close those accounts, even though I don't have any business with them. reply therealmocker 3 hours agoparentprevI couldn't find a reference to an app on the linked page, could you share more details on the app you use? reply lfmunoz4 3 hours agoparentprevwhat app or website do you use? Seems like you have to sign up for all three websites? Equifax Experian TransUnion? reply psadauskas 4 hours agoparentprevFuck that. I'm gonna open a bunch of credit cards, buy a bunch of cool shit, and when they ask me to pay my bill, just say my identity was stolen. If I have to fight the credit bureaus anyway, I might as well get something out of it. Stealing my own identity seems pretty straightforward. reply 93po 2 hours agoparentprevI was unable to get any of the three to verify my identity last I did this, and one of the three has never once in my 15 years of trying to get my free credit report let me actually get it. reply zsdfgyn 4 hours agoprevKey point of the article: \"Snowflake allows its corporate customers, like tech companies and telcos, to analyze huge amounts of customer data in the cloud. It’s not clear for what reason AT&T was storing customer data in Snowflake, and the spokesperson would not say.\" Finally journalists are asking the question why customer data must be stored with third party cloud providers. AT&T is a long way from Bell Labs, shame on them. reply orochimaaru 4 hours agoparentAll companies use third party cloud providers. A lot of legacy companies have been shutting down data centers to move to the cloud. So there isn’t a question of whether why your data is in the cloud. It’s going to be in the cloud. reply sbarre 4 hours agorootparentAnd honestly, I think I'd rather trust cloud providers with the data than the remnants of a decimated IT team in a large enterprise that's struggling to maintain their own on-prem infrastructure that's super old and probably not up to date on patches. reply Andrex 4 hours agorootparentThe problem is then you have even fewer technically-competent people internally to actually manage the cloud, and combined with AWS's many documented footguns it's not clear to me the \"new normal\" is actually any better for security. You go from being a potentially-small-fry target to getting your data collated in massive breaches. There's risks to both. reply orochimaaru 3 hours agorootparentThat’s the thing though - this was a snowflake breach. It’s not an AT&T miss because of their decimated sw engineering teams. Snowflake has much better sw engineering than AT&T. reply pylua 5 hours agoprevAnd earlier this year my ssn was on the dark web due to their leak (or vendor). One year of monitoring? No, I’m going to need it for life. Security is not a concern. There is no real incentive to change the status quo. Make them pay for monitoring indefinitely . reply ajsnigrutin 5 hours agoparentI never understood the american secrecy about SSN... it should be a \"username\" not a \"password\"... In my country you can calculate our own national id (mix of date of birth, autoincreasing number by each birth that day + 1 checksum number), and if you do/have any kind of personal business, your personal tax number has to be written everywhere, on every receipt you hand out or anything you buy as a business. Somehow knowing that first boy born today will have an ID number of 120702450001X (too lazy to calculate the checksum, but the algorithm is public), doesn't help anyone with anyting bad. reply galdosdi 4 hours agorootparentIt's because it happened gradually / naturally / semi un intentionally, because: 1) SSN was not intended as a national ID, but it so happened to fit the shape of one, in that almost everyone has one and they're unique. 2) It has never been possible to institute an intentional national ID system in the US for political reasons That is the recipe for the problem we have now. Strong demand for a national ID from many business purposes, the existence of something that looks a lot like, but is an imperfect form of, national ID, and the refusal to create a proper national ID, has naturally led to a de facto system of abusing the SSN as a national ID and just kind of everyone being a little annoyed and sketched out about it but putting up with it anyway for lack of alternatives. Incidentally, did you know anyone can generate a valid new EIN (which is a lot like an SSN, and can be used where an SSN can be used for some but not all purposes, specifically filing taxes and ) at this page https://www.irs.gov/businesses/small-businesses-self-employe... ? This isn't legal advice and I'm not a lawyer and I don't know in what situations you personally would be legally permitted to use this (it's meant for businesses, absolutely not some kind of personal alias) -- but technologically, it's just honor system, and anyone can certify they need and are entitled to a new EIN and the IRS web site will provide you with a new unique one. I don't think you even need a legal entity, since you don't need a legal entity to run a business in the US. reply galdosdi 4 hours agorootparentprev> Somehow knowing that first boy born today will have an ID number of 120702450001X It's even worse. Only post-2011 IIRC births have an algoirthmic SSN. So everyone over the age of 13 still has old fashioned sequential SSNs, where XXX-YY-ZZZZ is determined by 1) XXX is the code for the office that issues your card. Can be guessed precisely and accurately by knowing birth location. For example, I can guess what region of the US you were born in (or lived in when you immigrated) by the first digit. 0 or 1 is probably northeast. 4 or 5 is probably near Texas. 7 might be near Arkansas. Etc. 2) YY-ZZZZ is sequential by date! So by knowing just birth day, can be guessed to within a range. In practice, this means it's easy to guess YY alone, but harder to get all 4 digits of ZZZZ 3) For some stupid reason it got popular to print SSNs with all but the last four digits masked. This is horribly bad because those four are ACTUALLY THE MOST SECRET PART! It's the only part that might not be guessable. But since it's common to be more lax with securing them..... it is super easy to recover the full SSN if you find a piece of paper that says something like JOHN SMITH 123 Main St Alabama City, AL 76543 In ref acct: XXX-XX-1234 (2001-03-14) Dear Mr Smith, Your account is overdrawn. Have a nice day. Thinking of you, The Bank It also means if someone is personally known to me, even vaguely, I may be able to reconstruct their social seeing nothing but a scrap of paper that has just the last four, if I can guess approximately where and when they were born or first entered the US. If I'm in a situation where I can try several guesses, it's even easier. reply 5555624 3 hours agorootparent> 1) XXX is the code for the office that issues your card. Can be guessed precisely and accurately by knowing birth location. While the first sentence is true, the second is only true if you were born after the mid-1980s, when a Reagan-era tax reform was enacted. (It required a SSN when claiming dependents.) Prior to that, most people did not get a SSN until they got a job. reply browningstreet 4 hours agorootparentprevA lot of financial things in the US are “secured” or anchored by SSN, that’s the only reason why. That and mother’s maiden name and first vacation and other security questions. It’d be less important with MFA now but SSN is also needed when opening new credit, so having it allows you to pretty easily fake someone else’s identity for credit. KYC hasn’t removed it from the equation. reply athenot 4 hours agorootparentOne mitigation is to make your mother's maiden name the output of: head -c 20 /dev/randombase64 And keep track of the result in your favorite password manager. Fortunately, fewer and fewer orgs are using security questions, but there are still some important ones that only use that and no MFA. reply theluketaylor 3 hours agorootparentThe problem with that plan is social engineering attacks. CSRs are often careless and will accept 'a bunch of random letters and numbers' as the answer rather than validating each character. Better to randomly select a long dictionary word or hypenate a few together. Equally unguessable but easily verified, so it won't be weakened during a phone conversation. reply madcaptenor 4 hours agorootparentprev\"Mother's maiden name\" won't work for my kids - my wife kept her name and the kids' last name is hyphenated, so you just have to guess whose name we put first. reply AuryGlenz 4 hours agorootparentIt's also probably increasing easy to look up. We need a national (preferably RFID-ish) password system. reply strangecharm2 4 hours agorootparentprevThis comment pops up every time someone talks about social security numbers. Yes, they were never supposed to be private, but now they are. So either Congress can do something about it, or big companies can stop leaking them. Clever \"well, actually\"s didn't stop my identity from being stolen recently after a breach, and they never will. reply dboreham 3 hours agorootparentThey're not really private+, and nobody should design a system with the assumption that they are. afaik nobody does these days. There are extra authentication checks done in addition to simply \"I have the SSN\". + e.g. until very recently there were US states that used your SSN as your driver license number. reply alistairSH 4 hours agorootparentprevI never understood the american secrecy about SSN... it should be a \"username\" not a \"password\"... The problem is banks/financial services do a piss-poor job validating identity when issuing credit/opening accounts. \"Oh, you provided an address, a SSN, and [non-random, easily discoverable personal fact]! Sure, here's a CC with a $150k limit!\" It's not the leak that's the problem; it's the ease with which that leaked data is used to either obtain fraudulent credit or access accounts. I don't have a good answer, because at some point, a financial institution needs to trust people to do business. Customer loses their phone, so MFA doesn't work, ok, now what? I guess the customer needs to have one-time use recovery tokens saved somewhere that can't be lost? How many people do that (not nearly enough)? How many banks even issue those tokens? And what if the token store gets hacked? Now you're really fucked. reply ThunderSizzle 4 hours agorootparentprevSSN is too public for it to be private or secret. Multiple employers, schools, medical institutions, financial institutions all ask for it, so it's not private. It's also treated as evidence of who you are, but it isn't tied to identification like an ID is. These institutions use it without ever truly validating it. It's similar to how records fraud can occur - people can record anything to the local registrar office, including fraudulent documents, without any checks. Once it's registered, it becomes evidence against the real owner. It's really messed up. reply dylan604 4 hours agorootparentprevEven the US gov't gave up on the notion the SSN was not to be used as an identifier. My dad's SS card had a phrase printed on it saying so. My SS card did not have that text. reply hermitdev 3 hours agorootparentMy SS card has that text. I got into an argument at the DMV when they asked for it. I relented because I needed my drivers license. Congress could solve this by enacting a simple law. Something to the effect of SSNs shall not be used as a means of identification by any party, governmental or otherwise other than the Social Security Administration. Use of an SSN as identification shall be subject to a $100 fine per each SSN used as identification, per day. reply demondemidi 4 hours agoparentprevWhen I went to college in the late 80s my ssn was automatically used as my student id. When I got my first bank account in 1990, they used my ssn as the account number. reply buildsjets 4 hours agorootparentOur class grades with names snd SSNs were posted on the wall after exams in a list of hundreds of students. Go Jackets. reply galdosdi 4 hours agorootparentAh it was a different time. Societal trust was greater. Without global internetification, the only people who could ever have any opportunity to exploit this information were your fellow campus denizens (students, professors, etc). Without global internetification, there was not as much an average person could really do or would know to do with an SSN alone to exploit it. This story is a good parable for so much of what has changed in the world the last couple decades -- we had a world built for less globalization, then we globalized, and we've been gradually adapting to / dealing with the unintended consequences since then. A real life door can only be picked by your neighbors or anyone else nearby -- attack surface is limited by the nature of physical distance. A virtual door can be picked at by 7 billion people. reply xyst 3 hours agorootparentprevI wonder if the schools actually verified the SSN. Would have been dank to see 666-66-6666 next to your name reply mdavidn 3 hours agorootparentprevMy first big employer in the aughts had my SSN encoded in a bar code on the back of my company ID, which they expected us to display at the office. reply rybosworld 6 hours agoprevConsumers are so numb to data breaches that these events now bring very little outrage. I think without that anger from the consumer, there's little incentive for companies to do more to stop data breaches from happening. reply chefandy 5 hours agoparentWell it's starting to feel like data privacy just doesn't exist anymore. I don't know why administrators for big customer databases even bother setting passwords these days. reply pavel_lishin 4 hours agorootparentMy mother was concerned that some of her information, and mine, leaked because she signed up for another bank account from a place she decided she didn't trust. She said she wasn't worried about the money being stolen, but she was worried about our identities being stolen. My concern was the complete opposite - I assume that my social security number and address are already for sale for a fraction of a cent somewhere, bundled with 10,000 other identities. But if money gets stolen, that's a whole rigamarole, with banks wringing their hands and saying \"identity theft\" as if that clears them from any responsibility. reply reaperman 7 minutes agorootparentClassic Mitchell and Webb skit[0]: Bank: \"No, you see it was your identity that they stole!\" Customer: \"Well I don't know because I seem to have my identity whereas you seem to have lost several thousands of dollars. I'm not clear why you think it's my identity that was stolen rather than your money.\" 0: https://www.youtube.com/watch?v=CS9ptA3Ya9E reply chefandy 3 hours agorootparentprevIf you have at least a fraud watch on your credit which means creditors are supposed to call you on the number they have listed before they open new accounts, then the money is arguably worth protecting more. But if you think it's tough to convince the bank with which you have an existing relationship that you didn't make some withdrawals, imagine trying to convince a bank you've never heard of that you didn't actually approve a loan for 3 Cadillac Escalade Platinums which neither you nor the bank realize are currently in a shipping container on their way to Abu Dabi. (Nothing against Abu Dabi— I just picked a random place not under US jurisdiction where plenty of people have Escalade Platinum money.) reply pavel_lishin 2 hours agorootparentI often choose Abu Dhabi as an \"example destination\", because that's where Garfield kept mailing Nermal in the comics. reply 0cf8612b2e1e 4 hours agorootparentprevAs a nobody, I keep wanting a financial product that is a black hole. Money can go in, but cannot come out without significant pain. Seven+ day waiting period, in person visit, physical mail verification, something, anything that means if I do get hacked my accounts are not drained in milliseconds. When I need a legitimate large withdrawal, I can go through the required effort. reply chefandy 4 hours agorootparentYou can have a financial manager control your accounts for you and just keep a small checking account, (plus they'll help you grow your balances) but they're not free. Well, they're not free if you want them to be unbiased. Given, what's going to keep them from getting scammed? Maybe what you're looking for is several safe deposit boxes. reply 0cf8612b2e1e 1 hour agorootparentI still want my money invested into the economy. I just want Chase/Fidelity/etc to have an understanding that I am never going to withdraw money from these accounts without planning for it. So, “I” should never be authorized to drain the account at a moments notice without extensive approval. Anything to cause friction for would be scammers and only once-a-year (?) pain from me to triply confirm the money can move. reply xyst 3 hours agorootparentprevThis already exists. Withdraw from account to physical cash. Proceed to stash cash in “secret” location. Most businesses don’t even accept cash anymore. Can’t get “hacked” although it’s prone to many other issues — space, humidity, physical theft. reply pavel_lishin 3 hours agorootparentThat sounds like the opposite of what OP wants, because that money can very easily come out, without any pain, and without you even being notified that it's been moved - unless you're re-implementing your own bank-level security, I guess. For example, let's say you have $100k in savings. I think you would be absolutely bonkers to store that in some secret part of your (flammable! break-in-able!) house. I guess you could put it in a safety deposit box, and if you needed to spend it in a non-cash way, you could walk it directly to the teller and deposit it and make it available? The equivalent of a cold wallet, I suppose. reply xyst 3 hours agoparentprevAT&T is a public company. Public company needs to get fined appropriately. Start issuing multi billion dollar fines for these breaches and suddenly companies are invested in security. Unfortunately with government agencies getting defanged as part of recent SCOTUS ruling, it’s likely not possible. Have to rely on civil court to issue fines now (ie, class action lawsuits). reply TeaBrain 4 hours agoparentprevI think many companies think they can solve this issue by throwing money at their cyber security teams. It just happens that cyber security teams are often ineffective. reply softfalcon 4 hours agorootparentMaybe this is how it is at some places, but in my experience, it is not the case. I have friends who have worked in cyber-security for Fortune 500 companies and almost all of those companies would short-change (or outright ignore) the recommended spend and suggestions of their cyber-security employees, contractors, and advisors. Where are you getting your information from? The levels of security negligence I hear about aren't even a big ask. Huge companies neglect to do basic things like \"don't store your passwords in plain text\" or \"make sure you salt and hash your passwords\". I don't think it's fair to say cyber security teams are failing if companies are blatantly doing the worst and most obviously wrong things on the daily at the highest levels. reply marcosdumay 4 hours agorootparentprevHow could they? Everything related to computers is designed to exfiltrate data nowadays. reply kredd 4 hours agoparentprevAfter Equifax debacle, I don’t think anyone cares. It’ll only be a big deal if there’s a huge B2B leak and business-critical data gets exposed, other than the usual name, address and phone number. reply al_borland 4 hours agorootparentI'm still upset the government hasn't started work on a new national ID program after the Equifax breach. The SSN is not a suitable ID number in this day and age. We need something better that can withstand these kind of things without screwing people for life. My credit will be frozen for the rest of my life, and everyone else should do the same. reply strangecharm2 4 hours agoparentprevAnd why didn't they do anything when we WERE angry? reply akshayB 6 hours agoprevThe real problem is that data needs to be deleted over time. There is not much of a use case for customers for go back last year and see who called them and obviously there are use cases like criminal investigations or spying. But customer has no power or ability to dictate how long their records are store and how they are used. Companies should provide tools and features to their customers empowering them with their data. reply mountainb 5 hours agoparentNon-murder criminal offenses typically have very short statutes of limitations. A lot of this could also be solved by encouraging the federal government to enforce federal privacy law as written more aggressively. A good incentive would be to amend the privacy statutes to permit the FTC to keep the funds extracted from settlements and penalties in-house. This would allow them to increase staffing and create a positive feedback loop to deter wrongdoing. This would have a negative effect on incumbent companies and practices, but it would not take long for the message to get across and for practices to change accordingly. Congress tends to prefer keeping agencies on its own budgetary string which paradoxically limits what the agencies are capable of doing. The laws that we think protect us do not protect us because many of them are within the exclusive jurisdiction of a federal agency with very limited powers and funds. In the US the leadership likes to create the illusion that it has made \"Bad Problem\" illegal by writing it into the law, but it does not like creating the conditions in which \"Bad Problem\" could be solved, whether it's because the tradeoffs involved are tough to contemplate or because keeping \"Bad Problem\" around as a visible enemy is clever politics. reply _heimdall 5 hours agorootparent> Non-murder criminal offenses typically have very short statutes of limitations. There's a hidden assumption here. The expectation is that data retention and potential privacy violations are a necessary evil because anyone may later be under investigation for a crime. The data could go uncollected, it isn't AT&Ts job to retain private information on all of us just in case an investigator wants it. Take telecoms out of it and consider a convenience store. Police would like to have video recordings of whatever moment in time they are investigating, but that doesn't mean the video has to be recorded and retained. A shop owner can choose to record videos and only retain them for a week if they want, or they can have cameras installed but not even recording if they're okay with just the effect of deterrence. reply mountainb 4 hours agorootparentMany civil claims have short statutes of limitation as well. It's not really that good for these companies to maintain regular business records going back to infinity that are subject to discovery in disputes that are not even related to anything the telecom company did. Complying with the discovery requests and subpoenas is expensive. The fetish for the somewhat imagined benefits of big data creates open-ended liabilities for these companies. But the pressure that law enforcement and the spy agencies put on the telecom companies to facilitate this has been an open secret for a long time now. A lot of this is on the federal government and Congress for leaving an area in which it has power dormant and within its relatively exclusive control. Thanks for the conversation. reply willmadden 4 hours agorootparentprevThat's another bandaid. The root cause is customer data collection mandated by outdated regulation. People should be able to digitally sign or provide a public key for their personal information without providing the raw text to 3rd parties. Various 1970's style government tax and regulatory rules need to be updated as well. reply softfalcon 4 hours agoparentprevThey have a financial incentive to never delete your data. Storing old data forever creates a perfect paper trail to sell to advertisers and perfect the shadow profile they keep on all of us. I agree that deleting all your data after a year makes sense practically, but they'll never do it because it makes them too much money to keep it around. reply tantalor 5 hours agoparentprevThis isn't data for serving user needs, this is data for spying on users reply MOARDONGZPLZ 4 hours agoprevIs this leak why the spam next messages have gone from “Hi how is your day ?” or “Hi [not my name] please do thing X. Of you’re not [not my name] I’m so sorry perhaps we can be friends.” to “Hi is this [my full name]?” or “Hello [my first name] how is your day ?” reply jeffwilcox 4 hours agoparentAny leak with your mobile and name pair could have done that. As a non-AT&T customer, I get the my-speecific-name pig butchering texts, too. reply MOARDONGZPLZ 3 hours agorootparentTrue. They’re brand new to me though. I’ve been getting the former for years, the latter for only weeks. reply JohnMakin 6 hours agoprevSo where/what is my compensation? (I know there is no recourse). When no one is on the hook for secure practices, like enabling MFA on your effin data stores that contain massive amounts of customer PII, this is the result. Not even an apology, just report it and move on. woops! those gosh darned cyber criminals. reply criddell 5 hours agoparentIf you go to court and ask for compensation you would likely be asked to show harm. Could you? reply JohnMakin 5 hours agorootparentIt really doesn’t matter. Compensation has been dispensed to customers in data breaches such as credit/ssn info, no harm proof needed. Potential for harm is enough. Breach of contract, as a customer do I have a reasonable expectation that this data is not exposed? of course I do. No one could very seriously argue it’s a zero sum. reply EarthLaunch 4 hours agorootparentprevIs there no harm, or is there harm that is hard to show in court? reply latchkey 4 hours agoparentprevI've received checks over the years for various things like this. You end up having to fill out a claim form and then wait about 5 years and one day, you get this check in the mail for some tiny amount of money. reply John23832 6 hours agoprevHow has Snowflake felt ANY recourse for being the source of all of these hacks? reply Aaronstotle 5 hours agoparentits not Snowflake's fault their customers used weak passwords and no MFA. Not enforcing MFA does merit some blame on Snowflake, however, I still think its on the customer to secure your own environment. reply John23832 2 hours agorootparentI feel like this would be true if ONE customer was hacked. At this point it's more than a handful. AND snowflake knew about it. If all the lockboxes in a bank get broken into, is it respectable to say \"ah all of the customers should have used better locks\"? The bank is the party who is supposed to be giving the insight into secure storage. They're not just renting space. reply dghlsakjg 4 hours agorootparentprevTotally, way too many people are trying to blame snowflake. ATT is a technology infrastructure company. Secure transmission of data is one of their core business competencies (theoretically). They are a corporation that we trust to handle incredibly sensitive info. Call records are, in fact, incredibly sensitive data. They should be telling Snowflake what best practices to be using, not the other way around! reply throwway120385 3 hours agorootparentAT&T is a real-estate company that coincidentally sells telecommunications services. My wife used to work for them and given what she's told me I would never in a million years do any business with them intentionally. reply disgruntledphd2 4 hours agorootparentprev> Totally, way too many people are trying to blame snowflake. Well the _actual_ compromise started from one of their employees, so it's pretty unsurprising that they're getting (some of) the blame. reply dghlsakjg 3 hours agorootparentAhh. The linked article didn't have that detail. They attributed it to a lack of 2FA reply yyyfb 4 hours agorootparentprevAT&T and phone carriers in general are not technology companies. They are infrastructure companies that purchase off-the-shelf communication technology, slap a billing system on top, and then spend most of their time on operations (finding places to put towers, keeping the gear up and running) and marketing. The security component of communications isn't built by them, but by the equipment manufacturers that they purchase from. There are no strong penalties for involuntary data leaks - why would they do more? reply dghlsakjg 4 hours agorootparentATT has a rich history of being a technology company. They invented UNIX! That's in the past, fair enough. So they used to develop cutting edge technology, they sell technology, they buy technology, they operate technology, they work with manufacturers to develop new technology, they operate the infrastructure underpinning the modern technology economy, but they aren't a technology company? Even if you want to argue that they aren't a technology company, they sure spend enough time doing everything a technology company does to hold them accountable for their technology failures. reply dahart 3 hours agorootparent> They invented UNIX! They also invented the transistor, C, the photovoltaic cell, radio astronomy, and … the telephone. ;) Yes that’s the past, but AT&T labs still employs almost two thousand people. It’s very funny to try to claim AT&T isn’t a technology company and only peddles services on top of equipment made by others. reply dahart 3 hours agorootparentprev> The security component of communications isn’t built by them Are you claiming AT&T outsourced security and have contracts to back that up? Buying security equipment surely doesn’t amount to having security, that would be hilariously naïve. Equipment manufactures are not responsible for AT&T’s data security, AT&T is. There are laws around security that can hold AT&T liable, in the US and Europe and elsewhere. Whether they will hold the company liable is another question, but these laws will not accept an excuse that AT&T purchased security equipment from another company. reply metabagel 4 hours agorootparentprevIt’s unclear what you’re arguing. That AT&T isn’t capable of securing customer data, and we shouldn’t expect that of them? That they shouldn’t be held liable? If they don’t have the core competency, they need to obtain it as a requirement of doing business. reply smcin 5 hours agorootparentprevSnowflake is saying they knew of unusual activity \"around mid-April 2024\", confirmed \"May 23, 2024\", around which time they made MFA mandatory (although their customer AT&T say they knew of the breach \"Mar 20\"; these timelines keep shifting back): \"Mandatory MFA option unveiled by Snowflake\" - Jul 11, 2024 https://www.scmagazine.com/brief/mandatory-mfa-option-unveil... > \"US cloud storage firm Snowflake has already required the implementation of multi-factor authentication across all user accounts a month following the widespread breach of customer accounts, including those of Ticketmaster and Santander Bank, reports The Register.\" reply mewpmewp2 5 hours agorootparentprevIt's industry standard to enforce MFA for customers of such sensitive data though. There's always going to be weak links. reply chefandy 5 hours agorootparentprevRight. Snowflake facilitated AT&T'S abject negligence, but ultimately the buck stops with AT&T, here. reply taspeotis 6 hours agoparentprev> Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts reply sickofparadox 5 hours agoparentprevThe Mandiant report said that some Snowflake customers declined to use MFA AND had passwords in place for 4+ years[1]. Maybe Snowflake should have pushed for MFA harder but at the end of the day, this is AT&T's fault. [1] https://cloud.google.com/blog/topics/threat-intelligence/unc... reply Ragnarork 5 hours agorootparentI'd say the blame lies halfway between AT&T and Snowflake. If you let your customers have poor security practices, and you have the power to ensure a heightened security level, you're also partly to blame... reply theluketaylor 2 hours agorootparentSnowflake also made it hard to have good practices, giving them further culpability. There was no setting for customers to force their entire tenant to enforce MFA. Customers had to depend on each person with access to do the right thing, something that is unlikely to be universally true. reply wredue 5 hours agorootparentprevNon-expiring passwords is probably no more or less secure, unless you are a rampantly terrible employer known for setting ablaze every bridge ever to the point of atomic annihilation. reply dylan604 4 hours agorootparentAre you suggesting a disgruntled former employee could use the password and do things? At that point, I have questions. How is the former employee accessing the cloud service? If your cloud is allowing public access without a VPN, then you've done something wrong there. If the former employee is still accessing your VPN, again, you've done something wrong. Many other things still come to mind but point back to you well before password rotation rules. reply beardedwizard 6 hours agoparentprevThe dark web and info stealing malware are the source of the hacks. My worry is not only that consumers get numb to breaches, but they consume rampant misinformation and have no idea how to hold appropriate parties accountable. How many times have you held AWS accountable for stolen access keys? Was it AWS fault when rabbit leaked their own keys? Is it snowflakes fault when you lose your creds to infostealing malware? How should snowflake enforce mfa on machine service account credentials? The answers are no, no, and they can not possibly. Not even hyperscalers have this magic. reply edm0nd 5 hours agorootparentEh, iirc the source of the hack was just regular stealers like Redline, not \"the dark web\". It was actually Snowflakes fault. The threat actors were able to find a test/demo account they could log into and from there they were able to access prod things they shouldnt have. reply beardedwizard 2 hours agorootparentThis is exactly the kind of comment I'm talking about. You have not read anything from snowflake, mandiant or crowdstrike on this, and you haven't even read the cnn article that has snowflakes response on this. The snowflake demo account has nothing to do with it. reply kjellsbells 3 hours agoprevI find it interesting that in your typical BigCo breach, they are at pains to point out that credit card details were not stolen. I infer from this that something about credit cards, and how they are secured, has real teeth and BigCo's lawyers are trying to stop them biting. Is this PCI-DSS? Maybe someone can comment. As far as this breach goes, I think it just confirms my gut feel that Snowflake are heading to the wood chipper. reply jeff_tyrrill 1 hour agoparentI think it's a desperate attempt to downplay the severity in any way plausible, taking advantage of the fact that credit card numbers and social security numbers have been mythologized in the American consciousness as nearly-mystical totems of identity and security, as part of the \"identity theft\" meme, even though they play little role in actual information security or privacy. reply OutOfHere 4 hours agoprevUnfortunate as it is, nobody genuinely cares about: 1. Preventing data breaches 2. Properly anonymizing aggregated personally identifiable data 3. Having and using a secure ID and verification system reply mv4 4 hours agoparentI am seeing this mentality as well, and it's disheartening. My company manufactures and sells a privacy-first, fully autonomous, on-prem, video security system for home and SMB. Yet, some people choose a cloud based service (convenient) and are surprised when their private data is either a) hacked, or b) abused by the provider's own employees (see the latest Amazon Ring settlement). With the latest scandals and breaches though, I feel it's gradually starting to change. reply gmd63 4 hours agoparentprevThey don't care because they don't know how the systems they use daily work, much less the costs and risks involved. If they knew, they would care, and that's why representatives care on their behalf. You could say the same about health and nutrition, but people very much do care when a medical issue tangibly affects them negatively. reply stevetron 4 hours agoprevAT&T bought into a significant amount of DirecTV - so much so that everything that had the DirecTV logo on it was changed to the AT&T logo, such as the invoicing. So the AT&T customer base has included, for several years, the Directv customer base. The article doesn't attempt to clarify who the 'nearly all' customers are, and some people will jump to the conclusion that it is the cell phone customers. But it could include the DirecTV customer base whose data is also at risk. reply vel0city 3 hours agoparentAT&T didn't just buy into a significant amount of DirecTV, they owned DirecTV. As in, 100% ownership. So yes, all DirecTV customers were AT&T customers, because AT&T and DirecTV were not separate entities. It wasn't until 2021 that DirecTV was spun off into a separate company again, but still with 70% ownership by AT&T. reply hermitdev 3 hours agoparentprevAT&T does a lot more than just cell phones. Probably also the largest US ISP behind Comcast, I'd expect. I had AT&T fiber to the home at a previous residence, and that was a great product. Far superior to Comcast. reply the8472 4 hours agoprevThe headline could equally say \"AT&T kept data for criminals to steal\". If wiretapping laws didn't exist then most of this data would not be justified to exist. Flat-rate billing doesn't need to keep track of this information. Even usage-based plans could keep cumulative records rather than individual ones, or at least delete them at the end of a billing period. Where there is a trough, pigs gather. reply mdale 5 hours agoprevInteresting that they use the word criminals instead of hackers.. makes it sound like it was a physical heist rather than poor security practices on their part :) reply demondemidi 4 hours agoparentThey are criminals. reply John23832 6 hours agoprevAnd, honestly, how is this info (which I WOULD want to know) meaningfully actionable to customers. We get our information stolen from a myriad of sources everyday. These companies do comparatively nothing to make things right and the burden falls on customers to pick up the pieces if you're in a tranch that is sold and used. reply smcin 6 hours agoparentOf course it's not meaningfully actionable to customers, big time lag in not disclosing since Apr 19. (Why does this not fall under SOX violation with the obligation to report timely to affected parties? It has affected AT&T's stock price -3% in early trading, so should it have also required SEC disclosure?) Wondering what is the significance that most of the stolen records were from the period 5/1-10/31/2022? Does it mean that AT&T enabled 2FA on more recent records, or that more recent records were on a different cloud bucket (or that they mostly stopped using Snowflake since)? reply skybrian 4 hours agoprev> Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts, a security feature that the cloud data giant did not enforce or require its customers to use. And is that going to change? reply dboreham 3 hours agoparentThis is a diversion. Why did they build a system that permitted a bulk database dump of hundreds of millions of rows even with 2FA? reply vel0city 3 hours agorootparent> Why did they build a system that permitted a bulk database dump of hundreds of millions of rows Should all databases be capped at a few million rows total or something? I don't quite understand where you're going with this. reply skybrian 3 hours agorootparentprevBecause that’s what a data warehouse is? You’d think they’d guard them more, though. reply ilteris 5 hours agoprevI am an ATT user and on a pixel which generally good at filtering spam messages. I have noticed I was getting so much spam messages recently (\"wanna make money working remotely for x hours a day only\") I was surprised and thought my number somehow made it to one of those spam networks. This confirms my suspicions. reply jonplackett 4 hours agoprevUnbelievable that they do not enforce 2FA for a client that huge. Absolute madnesss! reply ChrisArchitect 4 hours agoprevOfficial support page: https://www.att.com/support/article/my-account/000102979/ reply buro9 6 hours agoprevIncluding all location metadata associated to that? reply smcin 6 hours agoparentThe reports said celltower-level location data associated with calls and texts (but not datestamps). That would allow inferring their homes, job location, commute, family members, social graph. reply sitkack 5 hours agorootparentYou can still recover that without timestamps. It also looks like if anyone interacted with an ATT customer or used an MVNO your data is in there too. reply dylan604 4 hours agorootparentIt even said land lines had their numbers in the data if an ATT customer contacted one. Edit: I must have read that from a different article than the TFA though. reply sitkack 2 hours agorootparentYeah, all att customers, 2nd party participants and any other user of their network. Not just direct customers. reply mensetmanusman 3 hours agoprevNice way to rule out who is a spy or not. Nice. reply throwaway120724 4 hours agoprevThere's no way to make the software perfectly safe from hackers and from social engineering. So, yes, companies should be more careful with the data and, yes, the data shouldn't be kept forever. I agree companies should be doing more to protect the data. I see lots of outrage at the companies and why isn't the government doing more to punish them and how do I get compensated ... But, I feel like everyone is blaming the victim. Is it the home owners fault when someone breaks in and steals stuff? Where's the outrage at the hackers breaking into these accounts? Where's the \"why aren't the governments tracking these people down?\" Why is no one demanding that the hackers be brought to justice? reply throwway120385 3 hours agoparentThe problem with analogies is that they're a leaky abstraction. You're comparing a single person with maybe a handful of employees to a giant, multinational corporation with corporate offices, hundreds of thousands of employees, enough real-estate to create a small country, and billions of dollars per year in revenue. It's a false equivalence to compare this to door kicking like it was some kind of petty theft. They literally kept everyone's information in a machine that was connected to the internet and then didn't make any effort to treat that with the gravitas it deserves. They are not the victim here, we are. It's a little shameful that you don't see that. reply rightbyte 4 hours agoparentprev> But, I feel like everyone is blaming the victim. Is it the home owners fault when someone breaks in and steals stuff? > Where's the outrage at the hackers breaking into these accounts? The internet is essentially every hooligan in the world about to kick in your dooor. So yes, I blame the home owner. It seems silly to me to condemn anonymous users of the internet. Back in the days when nothing of importance was done on the internet the view was way more healty. If you have sensitive data, don't expose it to the hooligans. Easy as that. reply metabagel 4 hours agoparentprev> There's no way to make the software perfectly safe from hackers and from social engineering. This is a straw man argument. Companies should use best practices in order to prevent most intrusions. When they do not, as in this case, criticism is warranted. reply jen20 6 hours agoprevThis is the kind of breach that really should be company-ending, but will sadly instead likely result in a slap on the wrist. It is high time for the US to have a privacy law with real teeth, and to enforce it with vigour. reply criddell 5 hours agoparentOr maybe it's time to turn software engineering into an actual engineering profession. If the people responsible for designing and maintaining the AT&T system were \"real\" engineers, they could be sued for malpractice or even lose their license to practice. reply lesuorac 4 hours agorootparentSnowflake still works though. What civil engineer has been sued because somebody jumped off their bridge? You get sued when the bridge collapses not when somebody uses it for an unintended action. reply ghaff 5 hours agorootparentprevDo you really think that requiring 4-year degrees and passing a licensing exam would make a big difference? The fact is that, outside of civil engineering which involves a lot of dealing with regulatory agencies, most engineers in the US don't have PEs. I started on the path to get one because, had I stayed on my initial career path, I'd have been sending blueprints etc. to regulatory agencies but I ended up changing careers. reply acuozzo 4 hours agorootparentNo, what will make the difference is being personally liable for the vulnerabilities you introduce. Not the company. You. reply ghaff 4 hours agorootparentHow many individual engineers do you suppose get prosecuted for making errors--even careless ones? I'm guessing very few in the West. And I'm not even sure lopping off a head here and there to encourage the others is even a good idea. reply criddell 4 hours agorootparent> How many individual engineers do you suppose get prosecuted for making errors--even careless ones? Not many but is that because they don't get sued or because professionals who face consequences for negligence make fewer stupid decisions? reply ghaff 3 hours agorootparentI would assume that engineers, at least in the US, are far more concerned about getting fired/eased out than prosecuted if they do stupid things given that companies can do so pretty easily. reply jen20 3 hours agorootparentprevLook at Sarbanes-Oxley for precedent. Management has to be made liable for sufficient cultural shift to occur. reply jen20 3 hours agorootparentprevThe root cause is not whether engineers are licensed (I'm fine with that idea, but it's not going to resolve this specific problem). Instead, it is a culture of not caring about security because the fines are a cost of doing business is, and which comes from management, and treating personal information as an asset instead of a liability. A Sarbanes-Oxley style law that makes the CEO personally criminally responsible for breaches will be vastly more effective than pursuing individual engineers - many of whom will be on the types of visa where they have no effective route of pushback on orders anyway. reply criddell 3 hours agorootparentWhen a doctor is negligent, their employer is often also sued if it can be shown that it knew shenanigans were underway and did nothing. We shouldn't choose between holding engineers or executives responsible. Each should be held responsible for their part. reply jen20 2 hours agorootparentIndeed - but we should start at the place likely to actually make a difference: the executives. reply Ekaros 6 hours agoparentprevClass-action suit sounds reasonable, but sadly those never give penalties in right ballpark. Here it should be hundreds to thousands at least per affected customer. But my guess it is few tens of cents, if that... While lawyer will get nice couple million pop... reply MisterBastahrd 4 hours agoprevBe nice to have a new federal law: you get breached, you pay $5K plus lifetime credit monitoring to each person involved. Non-dischargeable by bankruptcy. No arbitration, no lawsuit. You pay. reply abduhl 5 hours agoprev>> AT&T said it learned of the data breach on April 19, and that it was unrelated to its earlier security incident in March. Why was this not disclosed on AT&T’s earnings call on April 24? At least someone will get compensated for the breach, although it’ll be the lawyers for the class action lawsuit that’s about to hit instead of the customers that got their information stolen. reply BenFranklin100 5 hours agoprevThis is a political problem. Until we pass laws that companies can be find liable for significant damages in the event of data breaches, we will see little progress on data security. This is an area where Congress needs to act. Current law does not adequately protect the public due to the difficulties in establishing standing, tying specific breaches to specific personal damages, other reasons. Such a law would seriously impact current practices of the majority of IT firms, including small app developers, which is why we see little push from silicon valley for such changes. reply smcin 6 hours agoprevOngoing fallout from the Snowflake compromise; AT&T knew on Apr 19 but only disclosed now (Why does this not fall under SOX violation with the obligation to report timely to affected parties? It has affected AT&T's stock price -3% in early trading, so shouldn't it have also required SEC disclosure?) - Records downloaded from Snowflake cloud platform - AT&T will notify 110 million AT&T customers - Compromised data includes customer phone numbers, metadata (but not actual content or timestamp of calls and messages), and location-related data. Not SSNs or DOBs. Mostly during a six-month period 5/1-10/31/2022, but more recent records from 1/2/2023 for a smaller but unspecified number of customers. TechCrunch report has more details including Mandiant's response, the name and suspects location of the cybercriminal group I wonder if Congress manages to summon TikTok-like levels of anger on regulating this one. reply softwaredoug 6 hours agoparent> Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts, a security feature that the cloud data giant did not enforce or require its customers to use. So AT&T put all our call information somewhere and hid it probably behind a weak password with no additional factors. IMO that's actionable negligence and I hope they get sued to oblivion. reply smcin 6 hours agorootparentI'm more stunned that AT&T knew back on Apr 19 [UPDATE: Mar 20] yet feels it had neither an SOX violation or SEC obligation (share price effect) to notify timely. Like, by Apr 22. Not three months later [UPDATE: 4 months later]. Remember the massive Yahoo 2014 hack which Yahoo management failed to notify its own users for 2 years? If SOX violation only literally covers users' own passwords getting breached, but not 2FA or other passwords to access the same data, will Congress amend it urgently? EDIT: apparently they're hiding behind the 3/20 disclosure [0] which is all they disclosed until [1],[2] today. [0]: March 30, 2024 - \"AT&T Addresses Recent Data Set Released on the Dark Web\" https://about.att.com/story/2024/addressing-data-set-release... > \"AT&T has determined that AT&T data-specific fields were contained in a data set released on the dark web; source is still being assessed... > \"AT&T has launched a robust investigation supported by internal and external cybersecurity experts. Based on our preliminary analysis, the data set appears to be from 2019 or earlier [incorrect], impacting... approx 7.6m current and 65.4m former AT&T account holders\"* > \"Currently, AT&T does not have evidence of unauthorized access to its systems resulting in exfiltration of the data set.... As of today, this incident has not had a material impact on AT&T’s operations.\"* [but did it have a material impact on the customers/ex-customers?!] [1]: Jul 12, 2024 - \"AT&T Addresses Recent Incidents Regarding Access to Data\" https://about.att.com/pages/data-incident.html [2]: Jul 12, 2024 - \"AT&T Addresses Illegal Download of Customer Data\" https://about.att.com/story/2024/addressing-illegal-download... > \"Based on our investigation, the compromised data includes files containing AT&T records of calls and texts of nearly all of customers of [AT&T’s cellular and (MVNOs) using AT&T’s wireless network], as well as AT&T’s landline customers who interacted with those cellular numbers between May 1, 2022 - October 31, 2022. The compromised data also includes records from January 2, 2023, for a very small number of customers. The records identify the telephone numbers an AT&T or MVNO cellular number interacted with during these periods. For a subset of records, one or more cell site identification number(s) associated with the interactions are also included.\" reply smcin 4 hours agorootparentSubsequent reporting reveals that the DOJ ordered two ~month-long \"delay periods\" in disclosure: > The Justice Department determined on May 9 and again on June 5 that a delay in providing public disclosure was warranted, so the company is now timely filing the report. > The company [AT&T] is working with law enforcement and believes at least one person has been apprehended, according to the filing. It does not expect the event to have a material impact on its financials. MarketWatch: [https://www.marketwatch.com/story/at-ts-stock-slides-2-9-aft...] reply amanaplanacanal 6 hours agoparentprevAccording to CNN: “The company said the US Department of Justice Department determined in May and in June that a delay in public disclosure was warranted. It’s not clear why that the US government requested that data be delayed. CNN has reached out to the Justice Department for comment.” reply nimbius 5 hours agorootparentMay 16 Dow Jones Industrial Average surpasses 40,000 points for the first time, before closing at 39,869. public disclosure of a cataclysmic security breach in a darling of the stock market could have significant repercussions. reply adamtaylor_13 5 hours agoparentprevIt definitely included SSNs for some of them. Source: me. My data was included in the leak and it included my SSN. It’s been a cluster fuck of a cleanup. reply wredue 5 hours agorootparentMy SIN number has been leaked no less than 4 times tied to basically every standard identifying question about me now, if that helps ease your worry. I guess the new methodology is that a company cannot be sued if they just all leak data, that way nobody knows which one is responsible for your identity theft. reply floatrock 6 hours agoprev> In a statement, AT&T said that the stolen data contains phone numbers of both cellular and landline customers, as well as AT&T records of calls and text messages — such as who contacted who by phone or text — during a six-month period between May 1, 2022 and October 31, 2022. AT&T customer? Prepare for phone calls / text messages from your most frequent contacts saying \"I got stranded / I'm Officer Blahblahman helping your friend get home... please send gift card / venmo\" It's only metadata... reply rustcleaner 5 hours agoparentI just realized this is going to fvck my call blocking strategy up: now creditors will have a bank of known good numbers to spoof into my whitelist with! :^O reply morkalork 5 hours agoparentprevI guess everyone is going to learn what Snowden was worried about the hard way now. I imagine there's going to be extortion attempts over calls to abortion clinics etc. reply smcin 5 hours agorootparentAmong other things. The data's mostly from May-Oct 2022. reply lumb63 6 hours agoprevThis is another consequence of the surveillance state. The same data that can be used to surveil us by the government can be stolen by who-knows-who. We’d all (mostly) be far better off, IMO, if companies didn’t retain such records. reply rustcleaner 6 hours agoparentMy wet dream would be a dump of all SMS or Meta or iMessage messages for a multiyear period for nearly 90% of users. Only when Normie Norman's private chats to his mistress and other little relationship trust disrupting secrets become uncensorably hosted on the darknet and freely searchable, only then will Normie Norman get a clue and install SimpleX/Briar/Cwtch/any other owner-free decentralized p2p chat. reply LinuxBender 31 minutes agorootparentNot unrealistic. I used to have a tail of all SMS texts running 24/7 and was required to grep for specific terms for certain agencies until they eventually had their own access. This was only SS7 based texts and was long before RCS existed. I could have saved it all to my workstation but knew better than to do that. Either way SS7 and text messages are very insecure. reply dtx1 5 hours agorootparentprevWhile I share the sentiment, Normie Norman is not at fault. Meta and other BigCorps are the perpetrators and Norman the Victim. reply rustcleaner 5 hours agorootparentTrue, but you have to admit once you really see Normie Norman you come to understand aristocracy. At least I do anyway. reply shrimp_emoji 5 hours agorootparenthttps://dwm.suckless.org/ > Because dwm is customized through editing its source code, it's pointless to make binary packages of it. This keeps its userbase small and elitist. reply rustcleaner 5 hours agorootparentNot in the way of a narcissist trying to separate himself from the group, but to see that Norman is very much susceptible to cow-like behaviors you can leverage. That's what I mean by understanding aristocracy. Aristocrat : Rancher. reply robcohen 5 hours agorootparentprevI have to disagree. He is a fault. Ultimately, you are the only person who really should care about your own security. When you delegate that responsibility, you are still the one who made that choice. reply doublepg23 5 hours agorootparentI don’t think it’s fair to blame people for not understanding the subtleties of encrypted communication. Everyone only has so much attention to give. reply tsujamin 4 hours agorootparentprevHaving a mobile phone is necessary to securing employment, shelter and sustenance in many cases, yet somehow it’s an individuals fault for choosing to have a phone account when a pair of multibillion dollar companies breach that data through lax security practices? reply Jgrubb 6 hours agoparentprevYes but have you ever asked a dev if they actually need the 8 year old logs in some bucket? reply oriettaxx 5 hours agoprevnext [3 more] [flagged] droopyEyelids 5 hours agoparentPublic? not within our lifetimes. Available to hostile governments? More likely reply pavel_lishin 4 hours agorootparentA friendly government can turn hostile overnight. reply asimpleusecase 5 hours agoprev [2 more] [flagged] megous 4 hours agoparent [–] If the news starts with \"criminals stole...\" and not with \"poor security practices at XYZ resulted in...\" then there's really no hope. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AT&T confirmed a data breach affecting nearly 110 million customers, involving phone numbers, call and text records, and location-related information.",
      "The breach, linked to cloud data provider Snowflake, includes metadata but not the content of calls or texts.",
      "The FBI and DOJ delayed public notification due to national security concerns, marking AT&T's second security incident this year."
    ],
    "commentSummary": [
      "AT&T experienced a data breach affecting nearly all customers, exposing call and text metadata but not content or timestamps.",
      "Despite an initial stock drop, the market remains largely unaffected, reflecting a belief that companies face minimal financial consequences for such breaches.",
      "The incident, linked to Snowflake's cloud platform, has sparked calls for stricter data retention policies, better security practices, and potential hefty fines or lawsuits to enforce corporate accountability."
    ],
    "points": 285,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1720783074
  },
  {
    "id": 40941056,
    "title": "Physics-Based Deep Learning Book",
    "originLink": "https://physicsbaseddeeplearning.org/intro.html",
    "originBody": "Welcome …# Welcome to the Physics-based Deep Learning Book (v0.2) 👋 TL;DR: This document contains a practical and comprehensive introduction of everything related to deep learning in the context of physical simulations. As much as possible, all topics come with hands-on code examples in the form of Jupyter notebooks to quickly get started. Beyond standard supervised learning from data, we’ll look at physical loss constraints, more tightly coupled learning algorithms with differentiable simulations, training algorithms tailored to physics problems, as well as reinforcement learning and uncertainty modeling. We live in exciting times: these methods have a huge potential to fundamentally change what computer simulations can achieve. Note What’s new in v0.2? For readers familiar with v0.1 of this text, the extended section Integrating DP into NN Training and the brand new chapter on improved learning methods for physics problems (starting with Scale-Invariance and Inversion) are highly recommended starting points. Coming up# As a sneak preview, the next chapters will show: How to train networks to infer a fluid flow around shapes like airfoils, and estimate the uncertainty of the prediction. This gives a surrogate model that replaces a traditional numerical simulation. How to use model equations as residuals to train networks that represent solutions, and how to improve upon these residual constraints by using differentiable simulations. How to more tightly interact with a full simulator for inverse problems. E.g., we’ll demonstrate how to circumvent the convergence problems of standard reinforcement learning techniques by leveraging simulators in the training loop. We’ll also discuss the importance of inversion for the update steps, and how higher-order information can be used to speed up convergence, and obtain more accurate neural networks. Throughout this text, we will introduce different approaches for introducing physical models into deep learning, i.e., physics-based deep learning (PBDL) approaches. These algorithmic variants will be introduced in order of increasing tightness of the integration, and the pros and cons of the different approaches will be discussed. It’s important to know in which scenarios each of the different techniques is particularly useful. Executable code, right here, right now We focus on Jupyter notebooks, a key advantage of which is that all code examples can be executed on the spot, from your browser. You can modify things and immediately see what happens – give it a try by [running this teaser example in your browser]. Plus, Jupyter notebooks are great because they’re a form of literate programming. Comments and suggestions# This book, where “book” stands for a collection of digital texts and code examples, is maintained by the Physics-based Simulation Group at TUM. Feel free to contact us if you have any comments, e.g., via old fashioned email. If you find mistakes, please also let us know! We’re aware that this document is far from perfect, and we’re eager to improve it. Thanks in advance 😀! Btw., we also maintain a link collection with recent research papers. Fig. 1 Some visual examples of numerically simulated time sequences. In this book, we explain how to realize algorithms that use neural networks alongside numerical solvers.# Thanks!# This project would not have been possible without the help of many people who contributed. Thanks to everyone 🙏 Here’s an alphabetical list: Philipp Holl Maximilian Mueller Patrick Schnell Felix Trost Nils Thuerey Kiwon Um Additional thanks go to Georg Kohl for the nice divider images (cf. [KUT20]), Li-Wei Chen for the airfoil data image, and to Chloe Paillard for proofreading parts of the document. Citation# If you find this book useful, please cite it via: @book{thuerey2021pbdl, title={Physics-based Deep Learning}, author={Nils Thuerey and Philipp Holl and Maximilian Mueller and Patrick Schnell and Felix Trost and Kiwon Um}, url={https://physicsbaseddeeplearning.org}, year={2021}, publisher={WWW} }",
    "commentLink": "https://news.ycombinator.com/item?id=40941056",
    "commentBody": "Physics-Based Deep Learning Book (physicsbaseddeeplearning.org)283 points by sebg 20 hours agohidepastfavorite20 comments alexb24 4 hours agoIn this dense overview presentation (Oct 2022), Chris Rackauckas introduced Sci ML with diverse examples from many fields: epidemics, gravitational waves, pharmacometrics, ocean simulation... and some open source and proprietary Julia libraries for SciML. Highly informative! https://www.youtube.com/watch?v=yHiyJQdWBY8 reply rmbyrro 2 hours agoparentDoes anyone know what the job market looks like for a \"Physics-simulation ML engineer\" (or whatever it's called)? reply joelthelion 2 hours agoprevI was wondering : does deep learning have the potential to make large-scale quantum physics simulations more tractable? How about plasma physics for fusion reactors? reply fragebogen 9 hours agoprevMaybe I'm blind, but how do I download the entire book as PDF? I only find the download button up top for individual pages? Afaik, it's produced by Jupyter book[1], but find nothing in their docs either. [1] https://jupyterbook.org/en/stable/intro.html reply eru 8 hours agoparentSee https://github.com/tum-pbs/pbdl-book/issues/1 reply privong 6 hours agorootparentDirect link to the arXiv abstract page, where one can download the PDF: https://arxiv.org/abs/2109.05237 reply danielmarkbruce 16 hours agoprevHopefully this is a great book, what a great topic to write a book about. Kudos to the author. reply __rito__ 12 hours agoprevSome other good resources- 1. CRUNCH group YouTube (talks on Math + ML) - https://m.youtube.com/channel/UC2ZZB80udkRvWQ4N3a8DOKQ 2. Steve Brunton's Physics Informed Machine Learning playlist - https://m.youtube.com/playlist?list=PLMrJAkhIeNNQ0BaKuBKY43k... 3. The book \"Data Driven Science and Engineering\" from Steve Brunton 4. Deep Learning in Scientific Computation from ETH Zurich - https://m.youtube.com/playlist?list=PLJkYEExhe7rYY5HjpIJbgo-... reply croemer 11 hours agoprevPrevious discussion in 2021: https://news.ycombinator.com/item?id=28500577 reply Xeyz0r 6 hours agoprevSounds like a valuable resource for both beginners and experienced reply richrichie 17 hours agoprevChris has done good work on this genre. His differential equations Julia package with support for physics or sci ML is pretty cool. https://www.stochasticlifestyle.com/the-essential-tools-of-s... reply jessriedel 19 hours agoprevTBC, this is about deep learning for physics problems, not a general approach to deep learning from a physicist's perspective. > This document contains a practical and comprehensive introduction of everything related to deep learning in the context of physical simulations. As much as possible, all topics come with hands-on code examples in the form of Jupyter notebooks to quickly get started. Beyond standard supervised learning from data, we’ll look at physical loss constraints, more tightly coupled learning algorithms with differentiable simulations, training algorithms tailored to physics problems, as well as reinforcement learning and uncertainty modeling. We live in exciting times: these methods have a huge potential to fundamentally change what computer simulations can achieve. reply esafak 19 hours agoparentThat would have been the more interesting book. There is a lot of intuition that statistical mechanics could bring to deep learning. I would have called this one Deep Learning for Physics. reply beefok 18 hours agorootparentI guess considering their group is called the Physics-based Simulation Group [1], I'm thinking maybe that's just the terminology they've always used? Or maybe it's a German->English translation thing? [1] https://ge.in.tum.de/ reply ayhanfuat 13 hours agoparentprevI think the other one is more commonly known as “physics informed deep learning”. reply jessriedel 2 hours agorootparentI don't think so? Wikipedia suggests that \"Physics-informed neural networks\" is > a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process https://en.wikipedia.org/wiki/Physics-informed_neural_networ... In other words, that seems to refer to giving the model prior info (a bias) about physical laws that generated the data. What I'm talking about is more abstract: using physics-y type math ideas to understand the internal behavior of the networks. Here are a couple examples: https://proceedings.neurips.cc/paper_files/paper/2023/hash/6... https://cgad.ski/blog/where-is-noethers-principle-in-machine... reply sriram_malhar 13 hours agoprevThe title is misleading, no? It seems to be about how to apply deep learning to physics simulations. It is not about borrowing physics concepts and applying them to the NN landscape. That said, it is a lovely set of topics. reply 77pt77 13 hours agoparent> The title is misleading, no? No. I got the correct meaning at first glance. reply makerofthings 10 hours agorootparentHow many IBM Technical Support workers does it take to change a lightbulb? None, we have an identical model here and ours is working fine. reply richard___ 17 hours agoprev [–] The most important question - how to apply these methods to contact dynamics? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Physics-based Deep Learning Book (v0.2) provides a practical introduction to deep learning in physical simulations, with hands-on Jupyter notebook examples.",
      "New in version 0.2: Extended section on integrating differentiable programming (DP) into neural network (NN) training and a new chapter on improved learning methods for physics problems.",
      "Future updates will include chapters on training networks for fluid flow prediction, using model equations as residuals, interacting with simulators for inverse problems, and leveraging higher-order information for better neural networks."
    ],
    "commentSummary": [
      "A new book titled \"Physics-Based Deep Learning\" has been introduced, focusing on applying deep learning to physics simulations rather than general deep learning concepts from a physicist's perspective.",
      "The book includes hands-on code examples in Jupyter notebooks, physical loss constraints, differentiable simulations, training algorithms for physics problems, reinforcement learning, and uncertainty modeling.",
      "The book is produced by the Physics-based Simulation Group and is available for download on arXiv, with additional resources and discussions available on various platforms like YouTube and GitHub."
    ],
    "points": 283,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1720735810
  },
  {
    "id": 40942732,
    "title": "Using S3 as a Container Registry",
    "originLink": "https://ochagavia.nl/blog/using-s3-as-a-container-registry/",
    "originBody": "Using S3 as a container registry a.k.a. Waiter, there's a whale in my bucket! 03 Jul, 2024 For the last four months I’ve been developing a custom container image builder, collaborating with Outerbounds1. The technical details of the builder itself might be the topic of a future article, but there’s something surprising I wanted to share already: you can use S3 as a container registry! You heard it right. All it takes is to expose an S3 bucket through HTTP and to upload the image’s files to specific paths. With that in place, you can actually docker pull from it. Isn’t that neat? In the rest of this post I’ll explain how it all works, but let’s start with a demo for the impatient among us. I created a container image that runs cowsay and mirrored it to a bucket. Here’s what happens when you pull and run it from the bucket’s url: $ docker run --rm pub-40af5d7df1e0402d9a92b982a6599860.r2.dev/cowsay _________________________------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w|| || Don’t you agree with the cow? Note that, for this demo, I’m using R2 instead of S3 (because it has free egress 😎). Fortunately, it doesn’t matter whether you use R2 or S3, since they are API-compatible. As a matter of fact, I used the AWS SDK to push my image to R2. But why? Using S3 is not the traditional approach for hosting container images. You’d normally use a container registry, such as DockerHub, GitHub Container Registry, ECR, etc. What benefits does S3 bring, then, to make deviating from the trodden, boring, path worthwhile? Let’s take a step back. We are developing a custom image builder (or bakery, as we affectionately call it) because of speed. We want to go from requirements to a ready-to-pull image in a few seconds. The easiest container registry to use in our case is ECR, because we are on AWS. However, it turns out there’s a substantial performance difference between S3 and ECR when it comes to upload speed! I discovered the performance gap somewhat by accident. Since speed is important for us, and the first rule of performance optimization is to measure, I instrumented the code to generate traces. Having that, I went hunting for optimization opportunities and came across something unexpected: the traces showed that pushing layers to the container registry accounted for a significant amount of time! That felt off, so I decided to run a small benchmark: to upload a 198 MiB layer to ECR and to S3, and observe the difference in duration. Here’s the outcome:Minimum observed speed Maximum observed speed ECR 24 MiB/s (8.2 s) 28 MiB/s (7.0 s) S3 115 MiB/s (1.7 s) 190 MiB/s (1.0 s) The table shows that S3 is up to 8x faster than ECR, which is almost an order of magnitude2! Of course, there are caveats, but “raw” S3 container registries are nevertheless a promising avenue of optimization. What makes S3 faster than ECR? The big difference between pushing to ECR and uploading objects to S3 is that the latter allows uploading a single layer’s chunks in parallel. Given enough bandwidth, this yields a massive increase in throughput. In fact, parallel chunked uploads are recommended in the AWS docs to maximize bandwidth usage3. Why can’t ECR support this kind of parallel uploads? The “problem” is that it implements the OCI Distribution Spec, which is the standard for container registries (i.e. the reason why you can docker pull and docker push to different registry implementations). According to the specification, a layer push must happen sequentially: even if you upload the layer in chunks, each chunk needs to finish uploading before you can move on to the next one. Needless to say, having a single active connection per layer leaves a significant amount of bandwidth unused! Aside: we also tested the performance of sequential uploads to S3. The result? Throughput went down to ECR-like levels! But S3 is not a container registry! Indeed, S3 is not a container registry in the strict sense of the word. You can’t docker push to it, and the fact that you can docker pull is mostly a happy coincidence. So how does it work? The answer to our question is revealed by looking at the inner workings of docker pull. Spoiler: it’s HTTP requests all the way down. More specifically, I logged4 the requests issued by docker pull and saw that they are “just” a bunch of HEAD and GET requests. As an example, see the log of a docker pull my-image:latest at my self-hosted registry (lines starting with # are comments): # Check whether the image's manifest is present in the registry HEAD /v2/my-image/manifests/latest # Download the image's manifest GET /v2/my-image/manifests/latest # Re-download the image's manifest, now addressed using the manifest's hash # (I think this is a sanity check by docker) GET /v2/my-image/manifests/sha256:dabf91b69c191a1a0a1628fd6bdd029c0c4018041c7f052870bb13c5a222ae76 # Download one of the image's blobs (which happens to be the image's metadata) GET /v2/my-image/blobs/sha256:a606584aa9aa875552092ec9e1d62cb98d486f51f389609914039aabd9414687 # Download the remaining image's blob (which happens to be its only layer) GET /v2/my-image/blobs/sha256:ec99f8b99825a742d50fb3ce173d291378a46ab54b8ef7dd75e5654e2a296e99 That’s it! A docker pull is merely downloading files through HTTP! Which means… You can pull containers from any static file server, as long as it has the necessary files at the expected paths and sets the right Content-Type header for each request. Since a S3 bucket is capable of both, a carefully crafted bucket can become a container registry! Aside: if you want to know more about “manifests”, “blobs”, and such, check out my article on Crafting container images without dockerfiles and the OCI Image Format Specification. Caveats In case it’s not already clear: this is all very experimental. I’m waiting to do more research before making any serious claims. Will it end up in production? Or will you, my dear reader, send me an email explaining how my approach is utterly flawed? Note that, while I haven’t made a survey of the container registry offerings out there, it’s obvious they come with features that make them more attractive than dumping files on a bucket. For instance: you can trust the images you upload are actually valid (because the registry uses the standard push method); you can run automated security scans against your layers and receive warnings if there’s anything fishy; you can natively specify who has access to private repositories; etc. Don’t let these caveats discourage you, though. If it all works as well as I’m hoping, maybe we’ll see a new trend of hosting public container images in Cloudflare’s R2! What would you say to free egress? PS. What about the whale? It’s a pun… go have a look at the docker logo 😉 If you liked this article, feel free to check out past blog posts or to subscribe. Outerbounds is a company founded by Netflix veterans to help data scientists and ML engineers have a smoother development process through better infrastructure. ↩︎ If you are interested in reproducing the experiment, bear in mind that we are running the code on AWS which is internally connected to S3 and ECR through a VPC (i.e. the traffic doesn’t go through the public internet). This gets us the best possible latency and bandwidth. ↩︎ See my download accelerator article if you want to go down that rabbit hole, or the “S3 is fast” section of this blog post. ↩︎ I’m almost sure you can observe the HTTP requests by going through the docker daemon’s logs, but I took the more heavy-handed approach of implementing an in-memory registry and logging the requests on the server side (the OCI Distribution Spec is pretty complete and clear). ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=40942732",
    "commentBody": "Using S3 as a Container Registry (ochagavia.nl)254 points by jandeboevrie 14 hours agohidepastfavorite111 comments watermelon0 5 minutes agoIt seems that ECR is actually designed in a way to support uploading image layers in multiple parts. Related ECR APIs: - InitiateLayerUpload API: called at the beginning of upload of each image layer - UploadLayerPart API: called for each layer chunk (up to 20 MB) - PutImage API: called after layers are uploaded, to push image manifest, containing references to all image layers The only weird thing seems to be that you have to upload layer chunks in base64 encoding, which increases data for ~33%. reply stabbles 10 hours agoprevThe OCI Distribution Spec is not great, it does not read like a specification that was carefully designed. > According to the specification, a layer push must happen sequentially: even if you upload the layer in chunks, each chunk needs to finish uploading before you can move on to the next one. As far as I've tested with DockerHub and GHCR, chunked upload is broken anyways, and clients upload each blob/layer as a whole. The spec also promotes `Content-Range` value formats that do not match the RFC7233 format. (That said, there's parallelism on the level of blobs, just not per blob) Another gripe of mine is that they missed the opportunity to standardize pagination of listing tags, because they accidentally deleted some text from the standard [1]. Now different registries roll their own. [1] https://github.com/opencontainers/distribution-spec/issues/4... reply eadmund 8 hours agoparent> The OCI Distribution Spec is not great, it does not read like a specification that was carefully designed. That’s par for everything around Docker and containers. As a user experience Docker is amazing, but as technology it is hot garbage. That’s not as much of a dig on it as it might sound: it really was revolutionary; it really did make using Linux namespaces radically easier than they had ever been; it really did change the world for the better. But it has always prioritised experience over technology. That’s not even really a bad thing! Just as there are tons of boring companies solving expensive problems with Perl or with CSVs being FTPed around, there is a lot of value in delivering boring or even bad tech in a good package. It’s just sometimes it gets sad thinking how much better things could be. reply steve1977 3 hours agorootparent> it really did change the world for the better. I don’t know about that (hyperbole aside). I’ve been in IT for more than 25 years now. I can’t see that Docker container actually delivered any tangible benefits in terms of end-product reliability or velocity of development to be honest. This might not necessarily be Dockers fault though, maybe it’s just that all the potential benefits get eaten up by things like web development frameworks and Kubernetes. But at the end of the day, todays Docker-based web app development delivers less than fat-client desktop app development delivered 20 years ago, as sad as that is. reply 9dev 2 hours agorootparentIf you haven’t seen the benefits, you’re not in the business of deploying a variety of applications to servers. The fact that I don’t have to install dependencies on a server, or set up third-party applications like PHP, Apache, Redis, and the myriad of other packages anymore, or manage config files in /etc, or handle upgrades of libc gracefully, or worry about rolling restarts and maintenance downtime… all of this was solvable before, but has become radically easier with containers. Packaging an application and its dependencies into a single, distributable artifact that can be passed around and used on all kinds of machines was a glorious success. reply PaulHoule 42 minutes agorootparentCirca 2005 I was working at places where I was responsible for 80 and 300 web sites respectively using a large range of technologies. On my own account I had about 30 domain names. I had scripts that would automatically generate the Apache configuration to deploy a new site in less than 30 seconds. At that time I found that most web sites have just a few things to configure: often a database connection, the path to where files are, and maybe a cryptographic secret. If you are systematic about where you put your files and how you do your configuration running servers with a lot of sites is about as easy as falling off a log, not to mention running development, test, staging, prod and any other sites you need. I have a Python system now with gunicorn servers and celery workers that exists in three instances on my PC, because I am disciplined and everything is documented I could bring it up on another machine manually pretty quickly, probably more quickly than I could download 3GB worth of docker images over my ADSL connection. With a script it would be no contest. There also was a time I was building AMIs and even selling them on the AMZN marketplace and the formula was write a Java program that writes a shell script that an EC2 instance runs on boot, when it is done it sends a message through SQS to tell the Java program to shut down and image the new machine. If Docker is anything it is a system that turns 1 MB worth of I/O into 1 GB of I/O. I found Docker was slowing me down when I was using a gigabit connection, I found it basically impossible to do anything with it (like boot up an image) on a 2MB/sec ADSL connection, with my current pair of 20MB/s connections it is still horrifyingly slow. I like how the OP is concerned about I/O speed and bringing it up and I think it could be improved if there was a better cache system (e.g. Docker might even work on slow ADSL if it properly recovered from failed downloads) However I think Docker has a conflict between “dev” (where I’d say your build is slow if you ever perceive yourself to be waiting) and “ops” (where a 20 minute build is “internet time”) I think ops is often happy with Docker, some devs really seem to like it, but for some of us it is a way to make a 20 sec task a 20 minute task. reply steve1977 2 hours agorootparentprevI’m aware of all of that, I’m just saying that this has not translated into more reliable and better software in the end, interestingly enough. As said, I’m not blaming Docker, at least not directly. It’s more that the whole “ecosystem” around it seems to have so many disadvantages that in the end overweigh the advantages of Docker. reply derefr 2 hours agorootparentIt has translated to reliable legacy software. You can snapshot a piece of software, together with its runtime environment, at the point when it's still possible to build it; and then you can continue to run that built OCI image, with low overhead, on modern hardware — even when building the image from scratch has long become impossible due to e.g. all the package archives that the image fetched from going offline. (And this enables some increasingly wondrous acts of software archaeology, due to people building OCI images not for preservation, but just for \"use at the time\" — and then just never purging them from whatever repository they've pushed them to. People are preserving historical software builds in a runnable state, completely by accident!) Before Docker, the nearest thing you could do to this was to package software as a VM image — and there was no standard for what \"a VM image\" was, so this wasn't a particularly portable/long-term solution. Often VM-image formats became unsupported faster than the software held in them did! But now, with OCI images, we're nearly to the point where we've e.g. convinced academic science to publish a paper's computational apparatus as an OCI image, so that it can be pulled 10 years later when attempting to replicate the paper. reply steve1977 1 hour agorootparent> You can snapshot a piece of software, together with its runtime environment, at the point when it's still possible to build it I think you’re onto part of the problem here. The thing is that you have to snapshot a lot of nowadays software together with its runtime environment. I mean, I can still run Windows software (for example) that is 10 years or older without that requirement. reply 9dev 1 hour agorootparentThe price for that kind of backwards compatibility is a literal army of engineers working for a global megacorporation. Free software could not manage that, so having a pragmatic way to keep software running in isolated containers seems like a great solution to me. reply steve1977 1 hour agorootparentThere’s an army of developers working on Linux as well, employed by companies like IBM and Oracle. I don’t see a huge difference to Microsoft here to be honest. reply sangnoir 4 minutes agorootparentYou'd have a better time working with Windows 7 than a 2.x Linux kernel. I love Linux, but Microsoft has longer support Windows for its operating systems. ahnick 35 minutes agorootparentprevWhat are you even talking about? Being able to run 10 year old software (on any OS) is orthogonal to being able to build a piece software whose dependencies are completely missing. Don't pretend like this doesn't happen on Windows. reply steve1977 15 minutes agorootparentMy point was that a lot of older software, especially desktop apps, did not have such wild dependencies. Therefore this was less of an issue. Today with Python and with JavaScript and its NPM hell it is of course. 9dev 1 hour agorootparentprevIm not sure I would agree here: from my personal experience, the increasing containerisation has definitely nudged lots of large software projects to behave better; they don’t spew so many artifacts all over the filesystem anymore, for example, and increasingly adopt environment variables for configuration. Additionally, I think lots of projects became able to adopt better tooling faster, since the barrier to use container-based tools is lower. Just think of GitHub Actions, which suddenly enabled everyone and their mother to adopt CI pipelines. That simply wasn’t possible before, and has led to more software adopting static analysis and automated testing, I think. reply steve1977 1 hour agorootparentThis might all be true, but has this actually resulted in better software for end users? More stability, faster delivery of useful features? That is my concern. reply watermelon0 16 minutes agorootparentFor SaaS, I'd say it definitely improved and sped up delivery of the software from development machine to CI to production environment. How this translates to actual end users, it's totally up to the developers/DevOps/etc. of each product. For self-hosted software, be it for business or personal use, it immensely simplified how a software package can be pulled, and run in isolated environment. Dependency hell is avoided, and you can easily create/start/stop/delete a specific software, without affecting the rest of the host machine. reply bandrami 1 hour agorootparentprev25 years ago I could tell you what version of every CPAN library was in use at my company (because I installed them). What version of what libraries are the devs I support using now? I couldn't begin to tell you. This makes devs happy but I think has harmed the industry in aggregate. reply twelfthnight 50 minutes agorootparentBecause of containers, my company now can roll out deployments using well defined CI/CD scripts, where we can control installations to force usage of pass-through caches (GCP artifact registry). So it actually has that data you're talking about, but instead of living in one person's head it's stored in a database and accessable to everyone via an API. reply pyrale 2 hours agorootparentprev> But at the end of the day, todays Docker-based web app development delivers less than fat-client desktop app development delivered 20 years ago, as sad as that is. You mean, aside from not having to handle installation of your software on your users' machines? Also I'm not sure this is related to docker at all. reply supriyo-biswas 2 hours agorootparentprevBeing able to create a portable artifact with only the userspace components in it, and that can be shipped and run anywhere with minimal fuss is something that didn't really exist before containers. reply docandrew 2 hours agorootparentJava? reply derefr 2 hours agorootparentprevYeah, but the Open Container Initiative is supposed to be the responsible adults in the room taking the \"fail fast\" corporate Docker Inc stuff, and taking time to apply good engineering principles to it. It's somewhat surprising that the results of that process are looking to be nearly as fly-by-the-seat-of-your-pants as Docker itself is. reply KronisLV 5 hours agorootparentprev> As a user experience Docker is amazing, but as technology it is hot garbage. I mean, Podman exists, as do lots of custom build tools and other useful options. Personally, I mostly just stick with vanilla Docker (and Compose/Swarm), because it's pretty coherent and everything just fits together, even if it isn't always perfect. Either way, agreed about the concepts behind the technology making things better for a lot of folks out there, myself included (haven't had prod issues with mismatched packages or inconsistent environments in years at this point, most of my personal stuff also runs on containers). reply belter 8 hours agorootparentprevWas it really so amazing? Here is half a Docker implementation, in about 100 lines of Bash... https://github.com/p8952/bocker reply redserk 6 hours agorootparentLines of code is irrelevant. Docker is important because: 1) it made a convenient process to build a “system” image of sorts, upload it, download it, and run it. 2) (the important bit!) Enough people adopted this process for it to become basically a standard Before Docker, it wasnt uncommon to ship some complicated apps in VMs. Packaging those was downright awful with all of the bespoke scripting needed for the various steps of distribution. And then you get a new job? Time to learn a brand new process. reply greiskul 2 hours agorootparentprevAnd like the famous hacker news comment goes, Dropbox is trivial by just using FTP, curlftpfs and SVN. Docker might have many faults, but for anybody that dealt with the problems that it aimed to solve do know in that it was revolutionary in simplifying things. And for people that disagree, please write a library like TestContainers using cobbled together bash scripts, that can download and cleanly execute and then clean up almost any common use backend dependency. reply samlinnfer 7 hours agorootparentprevThe other half is the other 90%. Looking at it now, it won't even run in the latest systemd, which now refuses to boot with cgroups v1. Good luck even accessing /dev/null under cgroups v2 with systemd. reply mschuster91 9 hours agoparentprevOn top of that, it's either the OCI spec that's broken or it's just AWS being nuts, but unlike GitLab and Nexus, AWS ECR doesn't support automatically creating folders (e.g. \".dkr.ecr..amazonaws.com/foo/bar/baz:tag\"), it can only do flat storage and either have seriously long image names or tags. Yes you can theoretically create a repository object in ECR in Terraform to mimic that behavior, but it sucks in pipelines where the result image path is dynamic - you need to give more privileges to the IAM role of the CI pipeline than I'm comfortable with, not to mention that I don't like any AWS resources managed outside of the central Terraform repository. [1] https://stackoverflow.com/questions/64232268/storing-images-... reply xyzzy_plugh 40 minutes agorootparentIIRC it's not in the spec because administration of resources is out of scope. For example, perhaps you offer a public repository and you want folks to sign up for an account before they can push? Or you want to have an approval process before new repositories are created? Regardless it's a huge pain that ECR doesn't support this. Everybody I know of who has used ECR has run into this. There's a long standing issue open which I've been subscribed to for years now: https://github.com/aws/containers-roadmap/issues/853 reply hanikesn 4 hours agorootparentprevThat's seem standard AWS practice. Implement a new feature so you can check the box, but in practice it's a huge pain to actually use. reply judge2020 2 hours agorootparentIt’s not as bad as Azure is (was?) with IPv6: https://news.ycombinator.com/item?id=29327773 reply X-Istence 1 hour agorootparentAzure's IPv6 implementation is still flawed and still broken. That has not changed. reply kbumsik 8 hours agoprevActually, Cloudflare open-sourced a container registry server using R2.[1] Anyone tried it? [1]: https://github.com/cloudflare/serverless-registry reply justin_oaks 3 hours agoparentLooks cool. Thanks for linking it. It does mention that it's limited to 500MB per layer. For some people's use case that limitation might not be a big deal, but for others that's a dealbreaker. reply wofo 11 hours agoprevHi HN, author here. If anyone knows why layer pushes need to be sequential in the OCI specification, please tell! Is it merely a historical accident, or is there some hidden rationale behind it? Edit: to clarify, I'm talking about sequentially pushing a _single_ layer's contents. You can, of course, push multiple layers in parallel. reply abofh 2 hours agoparentIt makes clean-up simpler - if you never got to the \"last\" one, it's obvious you didn't finish after N+Timeout and thus you can expunge it. It simplifies an implementation detail (how do you deal with partial uploads? make them easy to spot). Otherwise you basically have to trigger at the end of every chunk, see if all the other chunks are there and then do the 'completion'. But that's an implementation detail, and I suspect isn't one that's meaningful or intentional. Your S3 approach should work fine btw, I've done it before in a prior life when I was at a company shipping huge images and $.10/gb/month _really_ added up. You lose the 'bells and whistles' of ECR, but those are pretty limited (imho) reply orf 33 minutes agorootparentIn the case of a docker registry, isn’t the “final bit” just uploading the final manifest that actually references the layers you’re uploading? At this point you’d validate that the layers exist and have been uploaded, otherwise you’d just bail out? And those missing chunks would be handled by the normal registry GC, which evicts unreferenced layers? reply codethief 2 hours agoparentprevHi, thanks for the blog post! > For the last four months I’ve been developing a custom container image builder, collaborating with Outerbounds I know you said this was something for another blog post but could you already provide some details? Maybe a link to a GitHub repo? Background: I'm looking for (or might implement myself) a way to programmatically build OCI images from within $PROGRAMMING_LANGUAGE. Think Buildah, but as an API for an actual programming language instead of a command line interface. I could of course just invoke Buildah as a subprocess but that seems a bit unwieldy (and I would have to worry about interacting with & cleaning up Buildah's internal state), plus Buildah currently doesn't support Mac. reply wofo 24 minutes agorootparentUnfortunately, all the code is proprietary at the moment. If you are willing to get your hands dirty, the main thing to realize is that container layers are \"just\" tar files (see, for instance, this article: https://ochagavia.nl/blog/crafting-container-images-without-...). Contact details are in my profile, in case you'd like to chat ;) reply codethief 4 minutes agorootparentAh too bad :) Thanks for the link! Though I'm less worried about the tarball / OCI spec part, more about platform compatibility. I tried running runc/crun by hand at some point and let's just say I've done things before that were more fun. :) reply rcarmo 10 hours agoparentprevNever dealt with pushes, but it’s nice to see this — back when Docker was getting started I dumped an image behind nginx and pulled from that because there was no usable private registry container, so I enjoyed reading your article. reply IanCal 10 hours agoparentprevI can't think of an obvious one, maybe load based? ~~I added parallel pushes to docker I think, unless I'm mixing up pulls & pushes, it was a while ago.~~ My stuff was around parallelising the checks not the final pushes. Edit - does a layer say which layer it goes \"on top\" of? If so perhaps that's the reason, so the IDs of what's being pointed to exist. reply wofo 10 hours agorootparentLayers are fully independent of each other in the OCI spec (which makes them reusable). They are wired together through a separate manifest file that lists the layers of a specific image. It's a mystery... Here are the bits of the OCI spec about multipart pushes (https://github.com/opencontainers/distribution-spec/blob/58d...). In short, you can only upload the next chunk after the previous one finishes, because you need to use information from the response's headers. reply IanCal 10 hours agorootparentAh thanks. That's chunks of a single layer though, not multiple layers right? reply wofo 10 hours agorootparentIndeed, you are free to push multiple layers in parallel. But when you have a 1 GiB layer full of AI/ML stuff you can feel the pain! (I just updated my original comment to make clear I'm talking about single-layer pushes here) reply killingtime74 9 hours agorootparentSplit the layer up? reply thangngoc89 8 hours agorootparentYou can’t. Installing pytorch and supporting dependencies takes 2.2GB on debian-slim. reply electroly 3 hours agorootparentIf you've got plenty of time for the build, you can. Make a two-stage build where the first stage installs Python and pytorch, and the second stage does ten COPYs which each grab 1/10th of the files from the first stage. Now you've got ten evenly sized layers. I've done this for very large images (lots of Python/R/ML crap) and it takes significant extra time during the build but speeds up pulls because layers can be pulled in parallel. reply fweimer 7 hours agorootparentprevSurely you can have one layer per directory or something like that? Splitting along those lines works as long as everything isn't in one big file. I think it was a mistake to make layers as a storage model visible in to the end user. This should just have been an internal implementation detail, perhaps similar to how Git handles delta compression and makes it independent of branching structure. We also should have delta pushes and pulls, using global caches (for public content), and the ability to start containers while their image is still in transfer. reply password4321 7 hours agorootparentprevIt should be possible to split into multiple layers as long as each file is wholly within in its layer. This is the exact opposite of the work recommended combining commands to keep everything in one layer which I think is done ultimately for runtime performance reasons. reply ramses0 4 hours agorootparentI've dug fairly deep into docker layering, it would be wonderful if there was a sort of `LAYER ...` barrier instead of implicitly via `RUN ...` lines. Theoretically there's nothing stopping you from building the docker image and \"re-layering it\", as they're \"just\" bundles of tar files at the end of the day. eg: `RUN ... ; LAYER /usr ; LAYER /var ; LAYER /etc ; LAYER [discard|remainder]` reply yjftsjthsd-h 3 hours agorootparentI've wished for a long time that Dockerfiles had an explicit way to define layers ripped off from (postgre)sql: BEGIN RUN foo RUN bar COMMIT reply mdaniel 2 hours agorootparentAt the very real risk of talking out of my ass, the new versioned Dockerfile mechanism on top of builtkit should enable you to do that: https://github.com/moby/buildkit/blob/v0.15.0/frontend/docke... In true \"when all you have is a hammer\" fashion, as very best I can tell that syntax= directive is pointing to a separate docker image whose job it is to read the file and translate it into builtkit api calls, e.g. https://github.com/moby/buildkit/blob/v0.15.0/frontend/docke... But, again for clarity: I've never tried such a stunt, that's just the impression I get from having done mortal kombat with builtkit's other silly parts reply skrause 1 hour agorootparentprevRUNMore specifically, I logged the requests issued by docker pull and saw that they are “just” a bunch of HEAD and GET requests. this is immensely nice and I wish more tech out there made common sense decisions like this, just using what has worked for a long time and not overcomplicating. I am a bit surprised that there aren't more simple container repositories out there (especially with auth and cleanup support), since Nexus and Harbor are both a bit complex in practice. reply lofties 8 hours agoprevThis sounds very, very expensive, and I would've loved to see cost mentioned in the article too. (for both S3 and R2) reply remram 5 hours agoparentThe cost is the S3 cost though. It depends on region and storage tier, but the storage cost per GB, the GET/PUT cost, and the bandwidth cost can be found on the AWS website: https://aws.amazon.com/s3/pricing/ reply rad_gruchalski 9 hours agoprevWhat’s wrong with https://github.com/distribution/distribution? reply _flux 8 hours agoparentI hadn't seen that before, and it indeed does support S3, but does it also offer the clients the downloads directly from S3, or does it merely use it as its own storage backend (so basically work as a proxy when pulling)? reply vbezhenar 5 hours agorootparentIt redirects client requests to S3 endpoint, so yes, in the end all heavy traffic goes from S3. reply donatj 7 hours agoprevI don't do a ton with Docker outside dev tooling, but I have never understood why private container registries even exist? It just smells like rent seeking. What real advantage does it provide over say just generating some sort of image file you manage yourself, as you please? reply figmert 7 hours agoparentYou don't have to use it. You can use docker save and docker import: docker save alpine:3.19 > alpine.tar docker loadIt just smells like rent seeking. You don't need to pay someone to host a private repo for you. There are lots of tools out there so you can self-host. reply JackSlateur 7 hours agoparentprevYou will probably have images that you will not share to the world. Said images will probably be made available to your infrastructure (k8s clusters, CI/CD runners etc). So you have to either build your own registry or pay someone to do it for you. Of course, if you use images for dev only, all of that are worthless and you just store your images on your dev machine reply regularfry 2 hours agorootparentAlso if your infrastructure is within AWS, you want your images to also be within AWS when the infrastructure wants them. That doesn't necessarily imply a private registry, but it's a lot less work that way. reply alemanek 2 hours agoparentprevIntegration with vulnerability scanning utilities and centralized permissions for orgs are nice benefits. reply danmur 7 hours agoparentprevCompanies send young engineers (and older engineers who should know more but don't) to AWS and Microsoft for \"cloud certification\". They learn how to operate cloud services because thats what benefits AWS and MS, so thats what their solutions use. It's a difficult uphill battle to get people interested in how things work under the hood, which is what you need in order to know you can do things like easily host your own package repositories. reply figmert 6 hours agorootparentThis is a odd assessment. I agree certifications aren't all that, but having people learn them isn't about that. It's more that people don't feel like reinventing the wheel at every company, so they can focus on the real work, like shipping the application they've written. So companies like AWS, Docker etc, write things, abstract things away, so someone else doesn't have to redo the whole thing. Yes I can host my packages and write tooling around it to make it easy. But JFrog already has all the tooling around it, and it integrates with current tooling. Why would I write the whole thing again? reply danmur 5 hours agorootparentI am responding to this part of the parent comment: > I don't do a ton with Docker outside dev tooling, but I have never understood why private container registries even exist? You know the options and have made a conscious choice: > Yes I can host my packages and write tooling around it to make it easy. But JFrog already has all the tooling around it, and it integrates with current tooling. Why would I write the whole thing again? So presumably you are not the kind of people I was talking about. EDIT: I'm also assuming by the rent seeking part that the parent is referring to paid hosted services like ECR etc. reply mcraiha 7 hours agoparentprevPrivate (cloud) registries are very useful when there are mandatory AuthuthZ things in the project related to the docker images. You can terraform/bicep/pulumi everything per environment. reply arccy 5 hours agoparentprevand how do you manage them? you use the same tooling that exists for all public images by running a container registry. reply michaelmior 5 hours agoprev> Why can’t ECR support this kind of parallel uploads? The “problem” is that it implements the OCI Distribution Spec… I don't see any reason why ECR couldn't support parallel uploads as an optimization. Provide an alternative to `docker push` for those who care about speed that doesn't conform to the spec. reply wofo 5 hours agoparentIndeed, they could support it through a non-standard API... I wish they did! reply phillebaba 7 hours agoprevInteresting idea to use the file path layout as a way to control the endpoints. I do wonder though how you would deal with the Docker-Content-Digest header. While not required it is suggested that responses should include it as many clients expect it and will reject layers without the header. Another thing to consider is that you will miss out on some feature from the OCI 1.1 spec like the referrers API as that would be a bit tricky to implement. reply 8organicbits 7 hours agoprev> that S3 is up to 8x faster than ECR Awesome. Developer experience is so much better when CI doesn't take ages. Every little bit counts. reply barbazoo 2 hours agoparent> ECR 24 MiB/s (8.2 s) > S3 115 MiB/s (1.7 s) It's great that it's faster but absolutely, it's only an improvement of 6.5s observed, as you said, on the CI server. And it means using something for a purpose that it's not intended for. I'd hate to have to spend time debugging this if it breaks for whatever reason. reply wofo 20 minutes agorootparentTo be clear, the 8x was comparing the slowest ECR throughput measurement against the fastest S3 one. In any case, the improvement is significant. reply lazy_moderator1 4 hours agoprevThat's neat! On that note I've been using S3 as a private registry for years now via Gitlab and couldn't be happier! reply cpa 10 hours agoprevIs there a good reason for not allowing parallel uploads in the spec? reply wofo 10 hours agoparentNo idea... I asked the same question here (https://news.ycombinator.com/item?id=40943480) and am hoping we'll have a classic HN moment where someone who was involved in the design of the spec will chime in. reply benterix 10 hours agoparentprevI believe that even if there was one then, it's probably no longer valid and it's now just a performance limitation. reply wofo 10 hours agorootparentOther than backwards-compatibility, I can imagine simplicity being a reason. For instance, sequential pushing makes it easier to calculate the sha256 hash of the layer as it's being uploaded, without having to do it after-the-fact when the uploaded chunks are assembled. reply amluto 9 hours agorootparentThe fact that layers are hashed with SHA256 is IMO a mistake. Layers are large, and using SHA256 means that you can’t incrementally verify the layer as you download it, which means that extreme care would be needed to start unpacking a layer while downloading it. And SHA256 is fast but not that fast, whereas if you really feel like downloading in parallel, a hash tree can be verified in parallel. A hash tree would have been nicer, and parallel uploads would have been an extra bonus. reply catlifeonmars 10 hours agorootparentprevThat does not make any sense; as the network usually is a much bigger bottleneck than compute, even with disk reads. You’re paying quite a lot for “simplicity” if that were the case reply jtmarmon 10 hours agorootparentprevI’m no expert on docker but I thought the hashes for each layer would already be computed if your image is built reply wofo 10 hours agorootparentThat's true, but I'd assume the server would like to double-check that the hashes are valid (for robustness / consistency)... That's something my little experiment doesn't do, obviously. reply tealpod 7 hours agoprevThis is such a wonderful idea, congrats. There is a real usecase for this in some high security sectors. I can't put complete info here for the security reasons, let me know if you are interested. reply ericfrederich 6 hours agoprevR2 in only \"free\" until it isn't. Cloudflare hasn't got a lot of good press recently. Not something I'd wanna build my business around. reply TheMrZZ 5 hours agoparentAside from the casino story (high value target that likely faces tons of attacks, therefore an expensive customer for CF), did something happen with them? I'm not aware of bad press around them in general reply jgrahamc 5 hours agoparentprevR2 egress is free. reply victorbjorklund 9 hours agoprevBut this only works for public repos right? I assume docker pull wont use a s3 api key reply wofo 7 hours agoparentThat's true, unfortunately. I'm thinking about ways to somehow support private repos without introducing a proxy in between... Not sure if it will be possible. reply jaimehrubiks 10 hours agoprevI experience everyday the slowness of pushing big images (ai related tend to be big) to ECR on our cicd. reply wofo 10 hours agoparentI wonder whether the folks at Cloudflare could take the ideas from the blog post and create a high-performance serverless container registry based on R2. They could call it scrubs, for \"serverless container registry using blob storage\" :P reply zokier 9 hours agorootparentI just point out that AWS ECR is exactly that, thin wrapper around S3. See for example https://docs.aws.amazon.com/AmazonECR/latest/APIReference/AP... reply slig 8 hours agorootparentHopefully someone gets inspired by this and implements a thin wrapper using CF workers. reply slig 6 hours agorootparentHopefully someone gets inspired by this and implements a thin wrapper using CF workers. Edit: CF already did it https://github.com/cloudflare/serverless-registry reply wofo 7 hours agorootparentprevI didn't expect that! It's a pity they don't expose an API for parallel uploads, for those of us who need to maximize throughput and don't mind using something non-standard. reply sshine 10 hours agorootparentprevhttps://www.youtube.com/watch?v=3QbOssRq0Gs [Chorus: Chilli & T-Boz] No, I don't want no scrub A scrub is a guy that can't get no love from me Hangin' out the passenger side of his best friend's ride Trying to holla at me I don't want no scrub A scrub is a guy that can't get no love from me Hangin' out the passenger side of his best friend's ride Trying to holla at me reply iampims 6 hours agorootparentprevYou mean this? A Docker registry backed by Workers and R2. https://github.com/cloudflare/serverless-registry reply wofo 6 hours agorootparentAh, that's great! I'll have to look into it :) reply ericpauley 8 hours agoprevWhere's the source code? reply wofo 7 hours agoparentThe source code is proprietary, but it shouldn't take much work to replicate, fortunately (you just need to upload files at the right paths). reply seungwoolee518 5 hours agoparentprevLike treat path as a object key, and put value as a json or blob? reply filleokus 7 hours agoprev [–] I've started to grow annoyed with container registry cloud products. Always surprisingly cumbersome to auto-delete old tags, deal with ACL or limit the networking. It would be nice if a Kubernetes distro took a page out of the \"serverless\" playbook and just embedded a registry. Or maybe I should just use GHCR reply breatheoften 4 hours agoparentI'm using google's artifact registry -- aside from upload speed another thing that kills me is freakin download speed ... Why in the world should it take 2 minutes to download a 2.6 GB layer to a cloud build instance sitting in the same region as the artifact registry ... Stupidly slow networking really harms the stateless ci machine + docker registry cache which actually would be quite cool if it was fast enough ... In my case it's still faster than doing the builds would be -- but I'm definitely gonna have to get machines with persistent local cache in the mix at some point so that these operations will finish within a few seconds instead of a few minutes ... reply vbezhenar 5 hours agoparentprevKubernetes is extremely bare-bones, there's no way they'll embed a registry. Kubernetes doesn't touch images at all, AFAIK, it delegates that to the container runtime, e.g. containerd. If you want some lightweight registry, use \"official\" docker registry. I'm running it inside Kubernetes and it consumes it just fine. reply mdaniel 2 hours agoparentprev [–] > Always surprisingly cumbersome to auto-delete old tags, Does this not do what you want? https://docs.aws.amazon.com/AmazonECR/latest/userguide/lifec... I can't speak to the other \"registry cloud products\" except for GitLab, which is its own special UX nonsense, but they also support expiry after enough whisky consumption reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A developer has demonstrated using Amazon S3 as a container registry by exposing an S3 bucket through HTTP and uploading container image files to specific paths, allowing for `docker pull` operations.",
      "The main advantage of using S3 over traditional container registries like DockerHub or Amazon ECR is significantly faster upload speeds due to parallel chunked uploads.",
      "This experimental approach lacks features like automated security scans and access controls but could lead to hosting public container images in Cloudflare’s R2 with free egress."
    ],
    "commentSummary": [
      "Using Amazon S3 as a container registry is being discussed, highlighting its potential as an alternative to traditional container registries like DockerHub and AWS ECR.",
      "The conversation critiques the OCI (Open Container Initiative) Distribution Spec for its inefficiencies, such as requiring sequential layer uploads and lacking standardized pagination for listing tags.",
      "Docker's impact on software development is debated, with some praising its simplification of dependency management and others criticizing it for not improving end-product reliability or development speed."
    ],
    "points": 255,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1720758383
  },
  {
    "id": 40943436,
    "title": "As an Employee, You Are Disposable (2023)",
    "originLink": "https://nelson.cloud/as-an-employee-you-are-disposable/",
    "originBody": "As an Employee, You Are Disposable 2023-05-06 · 401 words Opinion The recent tech layoffs have shown that employees are disposable in the eyes of executives. This isn’t surprising though and I’m definitely not the first person that has written about this. I just want to highlight the current situation. Source: TrueUp: Tech Layoff Tracker It doesn’t matter if investor expectations are surpassed, layoffs can still take place. Source: CNBC: Shopify cuts 20% of its workforce; shares surge on earnings beat It’s somewhat understandable if a company is struggling financially and resorts to layoffs. However, there’s plenty of companies that are profitable and still lay off the people that earned the company those profits. Source: Polygon: Microsoft mass layoffs reportedly impact Bethesda, Halo Infinite teams Many companies are not only profitable, but their executives continue to earn huge sums of money amidst layoffs. Source: Ars Technica: Googlers angry about CEO’s $226M pay after cuts in perks and 12,000 layoffs Aside from layoffs, employees may have their pay frozen even though company revenues are up. That’s what happened at Microsoft. Let’s not forget that Microsoft is a $2.5 trillion dollar company (at the time of this writing). Source: Techradar: Microsoft workers protest ’landmark year’ CEO memo following pay freeze It doesn’t matter how much value you’ve delivered. It doesn’t matter how much impact you’ve had in a company. It doesn’t matter how long you’ve been at a company. You are still disposable. Source: Jeremy Joslin (@jcj) on Twitter This article shows the mindset some very wealthy executives have about the average worker/employee. Source: BBC: Tim Gurner apologises over call for more unemployment to fix worker attitudes There are some bits I want to highlight: “There’s been a systematic change where employees feel the employer is extremely lucky to have them,” Mr Gurner said. “We need to remind people they work for the employer, not the other way around.” [Mr Gurner] has previously made headlines by suggesting young people cannot afford homes because they spend too much on avocado toast. In Conclusion… It’s okay to like your job and employer. Just understand that, as an employee, you are disposable. Further Reading Here are some articles I’ve come across that share similar sentiments or are very relevant. I highly recommend giving them a read. Maybe you should store passwords in plaintext Our Company Is Doing So Well That You’re All Fired What Companies Still Get Wrong About Layoffs I Accidentally Saved Half A Million Dollars « Prev Delete All node_modules Directories Recursively in macOS and Linux Next » My Donation Earnings Over the Past Year",
    "commentLink": "https://news.ycombinator.com/item?id=40943436",
    "commentBody": "As an Employee, You Are Disposable (2023) (nelson.cloud)238 points by nelsonfigueroa 11 hours agohidepastfavorite350 comments stavros 7 hours agoPeople here are talking about loyalty, when they really shouldn't be. A company is not your family. Hell, a company isn't even something you can form a bond with. You can be loyal to the people in a company, but the relationship with the company is purely transactional, and it lasts exactly as long as it's mutually beneficial. Thinking that you should stay in a company even after it stops being beneficial for you will not lead to anything good. reply atmavatar 6 hours agoparentDon't anthropomorphize the company. It's not the legal entity which represents your employer that tells you that you're part of a family to try and manipulate you into working for a lower salary. That's done by the hiring manager and your bosses because a) presumably the lie works often enough that it's still worth using, and b) because they are paid a little bit more when they can get away with paying you a little bit less. It's also one or more people who decide that despite being a loyal, hard-working employee for X years that it's time to let you go so you can be replaced by a fresh grad, an overseas hire, or simply to close your position to pad the yearly bonuses of those higher up the chain. The entire concept of a corporation is to act as a shield for blame to prevent any landing on the individuals within - don't fall into that trap. Every decision made by a business, no matter how amoral, immoral, or even illegal it is, was ultimately made by people. reply iainctduncan 4 hours agorootparentThe person you're responding to is not anthropomorphizing at all. When it comes to employee disposability, companies are run by their ultimate decision makers, which is usually the board or direct owners. Even the C-suite is relatively powerless if given direct orders from the board/owners to trim head count. reply mobiuscog 3 hours agorootparentThe C-suite is not powerless in how it's done though - how they communicate, support and respect (or not) those being let go. reply resource_waste 7 hours agoparentprevThe relationship is key. I had a worker do some good work for low pay, because they believed in the idea. Situations changed and I need someone else for the position. I have so much guilt and I must do something for this worker. reply ecjhdnc2025 7 hours agorootparentYou can't change the reasons for which you need a new person, and you've materially benefited from the person working for low pay. But it's not guilt, it's your morality and ethics identifying a gain you made that you feel you shouldn't be comfortable with. One suggestion, perhaps the most obvious: identify the likely pay/compensation of the person who will replace them, and then pay them the difference (or as much of it as you can) in severance over and above what you're required to offer. Second suggestion: help find this person a new job at the kind of pay they should actually have been earning for the quality of work they were doing. But don't feel guilt: it's more like a moral obligation. It's a good thing, not a negative emotion. It means you're thinking like a complete person in a world where it is common to pretend that isn't necessary. reply jrs235 6 hours agorootparent>it's your morality and ethics identifying a gain you made that you feel you shouldn't be comfortable with. What's your definition of guilt then? Just curious. Thanks. reply dasil003 4 hours agorootparentprev> But don't feel guilt: it's more like a moral obligation. It's a good thing, not a negative emotion. It means you're thinking like a complete person in a world where it is common to pretend that isn't necessary. If you have a moral obligation you can’t fulfill because of the rules of the human system you operate in, how exactly do you propose avoiding a negative emotion? reply rexpop 3 hours agorootparentprev> don't feel guilt: it's more like a moral obligation Let's not quibble. English is too imprecise for this kind of argument. reply varispeed 6 hours agorootparentprev> I had a worker do some good work for low pay, because they believed in the idea. Basically preying on someone's low self-esteem and impostor syndrome. It's a vile thing to do, but unfortunately businesses keep exploit workers this way. Did they believe in the idea to make shareholders buy themselves another yacht? reply floating-io 10 hours agoprevThis is not news. If you are loyal to your employer and do not own a significant part of the organization, then you should take a good hard look at why you're loyal -- and whether or not your employer is loyal to you in return. Hint: in most companies, they aren't. It's exceedingly rare. Base your decisions accordingly. I will note, however, that there is also more to the story than simply \"being disposable.\" An argument can be made that laying off however many employees also helps to preserve the ongoing livelihood of the employees who remain. The error is often not in the layoff. It's in over-hiring in the first place. And that changes the \"how evil are these people?\" equation rather drastically IMO. Of course, this is just the random off-the-cuff thoughts of someone who is currently far too alcoholated, so YMMV, and-- Look! A boulder of salt! And on that note, I'm going to bed. Tootles! reply JR1427 10 hours agoparentWhat annoys me, is when companies expect _you_ to be loyal, when they would get rid of you in a heartbeat. For the vast majority of employment situations, it is simply a transaction. There needn't be any talk of \"loyalty\", which in my mind is usually just manipulation. reply suriya-ganesh 10 hours agorootparentEmployment are an adversarial relationship of convenience Given the chance, both parties would expect the other party be loyal while they optimize for something better than what they have. reply JohnFen 3 hours agorootparentAn employment arrangement is a business deal, nothing more and nothing less. Whether or not it's adversarial depends on the nature of the deal and the entities involved. The most common error I see in employment is the employees not understanding this basic truth. That leads to situations such as people thinking that an employer is doing them a favor in some way by hiring them, which leads to people giving up some of their power. The most common way people give up their power is by thinking they don't have any. reply galdosdi 4 hours agorootparentprevNo, not at all. You have missed the mark completely here. This is totally culturally dependent. In a culture where this is expected, this is indeed what will happen. If you've only ever lived in such a culture then, yeah, you might get the mistaken idea this is an intrinsic human universal truth. But cultural norms are really powerful and can absolutely overcome natural game theory / self interest dynamics. If everyone you know would think less of you after you quit a job when it is known that employers are loyal, you won't do it. Same goes for the reverse. reply Culonavirus 9 hours agorootparentprevYea doesn't sound dysfunctional at all... lol. Apply that for marriage for example: > Given the chance, both parties would expect the other party be loyal while they optimize for something better than what they have. Peachy. reply kombookcha 7 hours agorootparentDamn that's crazy, almost like they're completely different and applying the features of one to the other is apples and oranges. ;) reply andreasmetsala 8 hours agorootparentprevWho would want to be married with their company? You are finding dysfunction where there is none. Imagine expecting the employee or company to have the other party for better or for worse, till death do them apart. Talk about dysfunction! reply Culonavirus 4 hours agorootparentBut I am talking about loyalty. Marriage was just an example that points out the absurd nature of the \"loyalty\" the user described. You can't say \"well, I am loyal as long as I don't have any other options\"... that's not how loyalty works. reply rkachowski 9 hours agorootparentprevThe difference between employment relationships and marriage is rather large. reply ajot 6 hours agorootparentYou should be looking at employment as a contract, and marriage (and other personal relationships) as a covenant. If you apply tit-for-tat with your spouse, at least one of you will end dissatisfied and hurt. reply silverquiet 8 hours agorootparentprevThe divorce rate would suggest it isn't that large. reply nkrisc 7 hours agorootparentHistorically, marriages often were almost transactional arrangements of convenience. reply silverquiet 6 hours agorootparentI'd say that also reflects my experience with dating. reply varjag 10 hours agorootparentprevWith change in leadership to GenXers and millennials I don't think it's an expectation really. reply A4ET8a8uTh0 10 hours agorootparentCan confirm. I myself am older and, admittedly, came into the job market as a very naive individual. But over the years I have watched the younger cohort and I am actually kinda impressed with the realistic ronin-like attitude. It is a lot more transactional for a reason; companies themselves have mostly destroyed it. If anything, it is a perverse expression of US 'fuck you, got mine' system. reply kiviuq 7 hours agorootparentThere is too much morally infused overinterpretation of Capitalism going on. Owners and workers have diametrically opposite interests which no law or moral judgment could put aside. What one party perceives as a fair wage is nothing but profit dilution for the other side. Remember, the only purpose of a business is - given money - to make more money. If you can't help them with that, either by being dirt cheap or have some marketable skill, you're literally useless and part of the Capitalist overpopulation... a class people who serve no use in the accumulation process. reply smeej 7 hours agorootparentI think this is true of businesses that outgrow their local economies. And since that's what most people see in their feeds and interact with every day, that's how they expect every company to function. When every company that has any mind share is beholden to a bunch of external shareholders, it's no wonder \"profit as much as possible\" is the only reason for being anyone thinks of. But it didn't used to be this way and it doesn't have to be this way. It's still possible for people to do well by doing good, providing, for example, quality housing at non-exploitative prices, or reliable and safe plumbing to handle their neighbors' water and sewer needs. And it still is being done. Most of us just spend so much of our time online that we only ever hear about the companies with social media/marketing teams, so we get used to that way of thinking. reply harimau777 6 hours agorootparentI think that it may not be just that people are spending too much time reading about non-local corporations. Rather, it may be that many industries today are not viable to operate at a local level. E.g. it's not really possible to have a small town semi-conductor fab. Even in software development, there's probably not much need for local companies besides certain more simple applications like WordPress sites. reply smeej 1 hour agorootparentEven so, it's not the only alternative is gigantic companies trying to get everyone using their product. If every plumber in your area had a subscription to your software that was specifically tailored to your codes and the nuances of your area? Estimating costs based on actual local prices for things? Could you support the number of people it would take to keep such a thing running? Could you sell insights from the data to the local hardware store to help them figure out what kinds of changes (weather, construction, whatever) lead to what kinds of demand for products? It doesn't have to be \"you could walk to it\" local, but if you really created the tool your area's local businesses couldn't live without, could you really not support yourself? reply galdosdi 4 hours agorootparentprevIt's not just internet marketing. The 0.1% largest businesses in the USA represent around 60% of our GDP, and about half our employment. People really do interact with big business a lot. And the share for big business has only been gradually getting bigger over the last few decades. Today we talk of globalization, but the mid to late twenteith century was the heyday of nationalization, of small local and regional businesses like the ones you describe -- businesses that were linked to and cared about their local communities or regions -- getting bought up and consolidated into large national businesses -- local owners getting replaced with hourly shift managers with less autonomy and incentive to care. Today's American may wake up in their Toll Brothers built house, take a morning poop on their American Standard toilet, brush their teeth with Proctor & Gamble products, walk down the stairs built with wood from the Home Depot, pour themselves a bowl of Kellog's cereal into dishes and on a table bought from Ikea, read the news a bit on their Apple device, step into their Ford car sitting on their driveway built of asphalt made by Koch Industrial, then drive to work, stopping for gas at an ExxonMobil gas station, pulling in to their job as shift manager at a TGI Friday's, where they spend their day serving food made from ingredients bought from Sysco, the national distributor, ultimately bought from a few large processed food companies like whoever owns Kraft / Philip Morris these days.... finally after a long day they drive home, stopping at Wal*Mart for some essentials, finishing off the day with the refreshing taste of a Bud Lite. It's frankly amazing small business still managed to produce 40% of our GDP, but make no mistake, it has been and is being driven to the least profitable margins and corners of every industry. Small business gets to operate the restaurant while big business owns the real estate. Small business gets to owner-operate tractor trailers while big business gets the lions' share of the actual retail profits. Etc, etc -- small business is increasingly stuck with the left over pieces that are too unprofitable for big business. Unintentional or not, squeezing out small business owners from their share of the pie has been the process for big capital for decades. We talk a lot about how big capital has been trying to squeeze down the piece of the pie labor gets since the 70s, but we forget that small capital has also been the victim of this. I live next to a Target. It's run by a handful of minimum wage high school kids and a couple of managers who probably earn $40k and have no standing in the community, no share in the success of the business, no way to grow along with it, and no discretion to change their store's behavior to accomodate any local community interest other than distant, distant shareholders getting more profits. A few decades ago instead there would have been multiple stores, each with an OWNER who had all those things -- standing in the community, ownership over the profits, and the discretion to make decisions not purely on profit but also caring about the community they're indelibly tied up with. And this owner could pass the business to their kids, which the Target manager can't do either. reply JohnFen 3 hours agorootparentprev> the only purpose of a business is - given money - to make more money. That's overstating it. Making money is certainly a very important purpose in business, but it's not usually the only purpose. reply shrimp_emoji 6 hours agorootparentprevUS https://youtu.be/MTnFNVojAgc system :D reply cheema33 7 hours agorootparentprev> What annoys me, is when companies expect _you_ to be loyal, when they would get rid of you in a heartbeat. Reality is that neither has to be loyal. Employees leave all the time for better pay/perks etc. You are judging employers too harshly in this respect. reply harimau777 6 hours agorootparentA complicating factor is that it is much easier for corporations to punish employees for being disloyal than for employees to punish corporations for disloyalty. I.e. if your resume shows too much job hopping then that can hurt your ability to get a new job. However, a corporation is not nearly a badly hurt by repeatedly overhiring and laying off employees. reply cpach 6 hours agorootparentWe can safely conclude that there is a bit of power asymmetry going on. reply rekabis 4 hours agorootparent> there is a bit of power asymmetry going on. Which only unions have the power to equalize. reply cpach 3 hours agorootparentIMHO, dampen might be a more suitable verb. Unions are not a panacea to all issues an employee can face. And sometimes the union will put its own interests first, or the company’s, before the interests of the individual employee. But on the whole, unions can play a useful role on the market of labor. reply JR1427 7 hours agorootparentprevYes, I know that neither has to be loyal. But I know of examples where a manager has said things like \"you need to decide if you are loyal to the company\" etc. I really dislike this. reply JohnFen 2 hours agorootparentIf a manager ever said that to me, I'd immediately start planning my exit. I don't want to work for anyone who engages in that level of emotional manipulation. reply amanaplanacanal 5 hours agorootparentprevI would want to ask the manager how the company is going to be loyal to me. I’m curious what they would say. reply cpach 2 hours agorootparentIndeed. And how would the manager make a guarantee for it? reply lotsofpulp 7 hours agorootparentprevPeople lie all the time. For marketing, to be polite, for worse reasons. It’s in everyone’s best interest to stay on their toes and evaluate others’ motivations before accepting words at face value. reply iainctduncan 4 hours agoparentprevThis, 1000 times this. I have said this and seen it so many times (I work in diligence looking at companies). The big sin in tech is over-hiring to look like success. This is the act I consider reprehensible - making people believe they have a career when they got hired at an unsustainable burn rate to make number go up on an investment slide deck - fully aware that post-transaction (sale to next investor) half of them get the headsman's axe. reply JohnFen 3 hours agorootparentWhen I'm evaluating whether or not I want to take a position at a company, once of the things I look for is that company's history of layoffs. Some companies have a habit of engaging in them, and I prefer to avoid them. In part out of self-defense and in part because I think it's indicative of a general business mindset that I wouldn't get along with. reply psychoslave 9 hours agoparentprev> The error is often not in the layoff. It's in over-hiring in the first place. And that changes the \"how evil are these people?\" equation rather drastically IMO. Starting any topic with kind/evil dichotomy is never a way to bring a rational perspective on the table, indeed. Over-hiring can happen through different situation. One is bad luck: market evolution removed the the ground under the feet of a sound projection assuming a reasonably stable context. An other possibility is incompetence: acting in reckless attitude with the business strategy as if it was a past time board game without considering it will have concrete impact on real people that will engage in its progression. Yet an other one is draining all skills in the market to gain dominance on its segment. Thus endowed, it will be easy to kill any possibility of fair concurrence. Then later, drop the monopolized workforce out of the train, that was known to be superfluous to achieve production goals. Of course, this drop should occur only once there is no credible chance these employees could find a decent job in this sector anymore. That is after they contributed to the hegemonic position of the organization. In short, over-hiring is not necessarily an error. reply lordnacho 9 hours agoparentprevI think we need to look at what we really mean when we say someone is loyal. What it means to be loyal is - this is related to the idea of a contract - that when you see a better opportunity, you do not leave your current relationship in favor of the new one. In other words, loyalty is what stops us from taking better opportunities that arise as circumstances change. Same as when you are taking your vows, if you later come across a hotter, younger chick with your exact set if kinks, you do not just drop your wife and go looking for a new gimp suit. Seen from this perspective, it is clear that businesses tend to have very little loyalty. The ethos of most of the business world at the moment is short term bean-counting optimization. If employees can be fired and replaced by cheaper foreign workers, they will be. The only rationale allowable is one that balances costs and benefits, in the bean-counting sense, and it is not an accounting that includes any weight for loyalty. You could imagine a spreadsheet where the company said \"hey I'm gonna put a hurdle of $100k per employee on the 'stay in America' side, and I'll only move the factory if it's still in favor of moving abroad\". But when has this ever happened? On the other hand, employees probably have more loyalty than zero. Almost everyone has a story about they were approached by a recruiter offering a higher salary, but they didn't take it. Part of this is explicable by risk, offsetting the higher salary with a chance of not settling at the new work, that kind of thing. But if you were to count the bean again, almost every knows a bunch of people who could have jumped ship for a higher salary but didn't, even when you count that risk. reply Akronymus 9 hours agoparentprev> If you are loyal to your employer and do not own a significant part of the organization, then you should take a good hard look at why you're loyal -- and whether or not your employer is loyal to you in return. As someone who works at a 2 person company (my boss and me), it's the good relationship with him along with the relaxed work environment. Sure, we have to be profitable, but not exceedingly so. > I will note, however, that there is also more to the story than simply \"being disposable.\" An argument can be made that laying off however many employees also helps to preserve the ongoing livelihood of the employees who remain. > The error is often not in the layoff. It's in over-hiring in the first place. And that changes the \"how evil are these people?\" equation rather drastically IMO. Those are mostly a problem at larger companies, IME. I was exceedingly lucky with most of my jobs so far, in the sense that I mostly got good work environments where employees are actually valued beyond mere numbers. I wish others would be as lucky as I was. reply paulcole 8 hours agorootparent> where employees are actually valued beyond mere numbers Isn’t this the point of the article? That even when you feel this way that you are still disposable? reply Akronymus 8 hours agorootparentI have personal experience of a company keeping me employed WAY beyond where it was profitable for them, as they simply didn't have the need for me due to not enough contracts. They provided me with a lot of learning resources. I went almost half a year in total without any real work for me to do. They were aware of that. They really tried hard to find a way to keep me on, so while I was disposable in the end, they truly did try to keep me on, and to me, that indicates them valuing me beyond the mere numbers. reply Ma8ee 5 hours agorootparentBut they probably thought there would be more contracts, and keeping you on for 6 months, or even a year, would probably be cheaper than hiring and onboarding someone new. reply paulcole 8 hours agorootparentprevWhy’d they keep you on? Are you just a really nice person? Are they just really nice people? Or did they think you’d be profitable to them in the future? Or some combination of all 3? reply Akronymus 6 hours agorootparentOption 2 and 3 for sure. I can't say whether option 1 applies. But my opinion on them is not just informed by keeping me on for longer than necessary, but rather many additional supporting experiences. Like actually getting to talk to the C levels and bringing in suggestions/asking questions and such. My understanding of your messages indicates to me that you think I was saying that the company doesn't care about numbers. While I was meaning that the company treated me as an actual valued human and did well to me. reply newswasboring 10 hours agoparentprev> An argument can be made that laying off however many employees also helps to preserve the ongoing livelihood of the employees who remain. That argument falls flat on its face as soon as you look at the balance sheet of these companies. They are exceeding expectations and still laying people off. reply bryanrasmussen 8 hours agorootparent>That argument falls flat on its face as soon as you look at the balance sheet of these companies. They are exceeding expectations and still laying people off. I thought it was a discussion about generic companies, but it seems to be a discussion about specific ones? reply tsujamin 9 hours agorootparentprevThis reminds me of the argument GE made (from David Gelles book about Welch) that laying off staff is a kindness, as it lets them try and find a new job they’re “better suited for” sooner rather than later reply pdimitar 9 hours agorootparentSounds like somebody who never had a mortgage payment overdue and the bank calling and threatening to confiscate the house / apartment, and with two pre-school kids constantly introducing unexpected expenses. But then again, I never expected the ruling class to be empathetic. reply rty32 8 hours agorootparentExactly. Laying people off isn't necessarily a bad thing by itself if it \"needs\" to happen, but (at least in the US) what follows is loss of health insurance and income money needed for mortgage. Millionaires don't have that problem and can easily tell peasants to suck it up. reply cpach 5 hours agorootparentRegarding unemployment benefits: In many countries there are unemployment insurance funds set up so that one can maintain a somewhat reasonable level of income even in the event of unemployment. E.g. here’s what it looks like in my locale: https://en.wikipedia.org/wiki/Unemployment_funds_in_Sweden The upside of having a system like that is that the burden to ”care for” the employee’s economical safety is not put solely on their current employer, but not solely on the employee either. In order to be eligible for payouts the employee has to pay in to the fund every month (something like ~14 USD) – and all companies are mandated to contribute pay in to this system. Personally, I see few downsides with this system. It’s important for the general economy that companies can be flexible and change direction, e.g. when the economy cools off or in the face of competition. Therefore I believe layoffs are sometimes necessary for the company to survive. But it’s also important – both for ethical and economical reasons – that layoffs doesn’t wreak havoc with the personal finances of the people being let go. Regarding healthcare: To me it does not seem like a good idea to tie a person’s health insurance to their current employer. AFAICT this is a remnant of the US war economy during WW2. But that doesn’t mean that a system like UK’s NIH is superior. To me, the German model looks very reasonable where citizens are required to sign up for insurance but the benefits are not tied to the person’s employment. One can read more about it here: https://www.deutschland.de/en/topic/life/health-in-germany-h... I think Singapore has a similar system. reply tsujamin 8 hours agorootparentprevExactly, the logic was somewhat abhorrent and (given the wider context of GE at the time) clearly disingenuous reply pydry 10 hours agoparentprevIt's not news and I think it's fairly widely accepted, but there are plenty of corollaries that people still reject. I had a coworker who was actively hostile to the practice of writing documentation, for instance. He wasn't up front about it but he was plainly doing it to reduce his disposability. I think it probably worked because they kept him around longer than they clearly wanted to in spite of his very prickly personality. As a strategy I thought it actually made a decent amount of sense given his circumstances - lots of debt/outgoings, he couldn't hop jobs easily and the company treated employees as extra disposable. As the developer who came after him I was perplexed at the cries online people would make that \"this practice only hurts the developer who comes after you\" or \"if the company does well you'll do well\" (which I think is bullshit). It definitely made me a LOT slower, but I still got paid the same. reply louwrentius 10 hours agoparentprevIt's never about the livelihood of those that remain. That's just a side effect. It's only about shareholder value, thus about growing growth. Which is so stupidly unsustainable, the inevitable crash always happens. And it's normal people who will foot the bill for this system. reply palata 10 hours agoprevWork for yourself, not for your employer. Your employer will fire you the second it is profitable for them, that's how it is. It doesn't mean that you have to hate your job. Just stop and reflect from time to time, and make sure you are being compensated for what you do. If not, you need to change something (ask for a raise, look for another job, work less, ...). As developers, IMHO you should always push for open sourcing as much as you can, ideally with a copyleft license and no CLA. Because it is beneficial for you personally: it means that you can reuse your code after you leave, and if you manage to make it copyleft and accept external contributions, it will force your employer to keep distributing the sources after you leave. I really don't get why developers sometimes go out of their way to help their employer keep their code proprietary, it should be the other way round. reply kortilla 8 hours agoparent> As developers, IMHO you should always push for open sourcing as much as you can, ideally with a copyleft license and no CLA. Because it is beneficial for you personally Agree with the rest of your message but this is really dumb and will hurt your career. You need to think about what you’re writing and determine if there is any reason the employer would be supportive of this. Ive been in SWE management for roughly 20 years now and if you get labeled as “the person who wants everything open source”, you get put in a very limiting box. You advertise that your interests aren’t really aligned with the company and you’ll be treated as a mercenary. This seems okay on the surface, but it will exclude you from working on critical business code that would be ridiculous to open source. reply bravetraveler 7 hours agorootparentI'll be honest, getting paid is good for my career. What code I specifically get paid to monkey over weighs on it far less. Totally agree with avoiding labeling yourself more generically, but having spent enough time in the sun... I just want shade. I work to get paid, not get acclaim or friends. At a certain point your professional development stops being at work. Being a mercenary is an interesting way to consider that, actually. For yourself, nobody else will. I'm not even a developer. I do SRE which kind of requires I play one while not being on the friendliest terms with the real devs: \"oh, that guy again\" I guess my point is this: don't subject yourself to whims... but do be amenable. Just not too much. It's easy to become the sucker. Don't accept working on business critical code without business critical pay. It's just a weird way to look at things. reply palata 6 hours agorootparentprev> Ive been in SWE management for roughly 20 years now and if you get labeled as “the person who wants everything open source” Not if you do it right. Don't be \"the person who complains about stuff not being open source\". I believe I have been pretty successful at open sourcing a lot of my code. Sometimes by convincing my management that it was good for the company, sometimes by manipulating them (I guess it would mean I manipulated people in your position :-) ). Not in a way that hurt the company at all: I never open sourced anything where it actually mattered for the company. I see it like this: when given the choice between not open sourcing (no work needed) and open sourcing (some work needed), every rational manager will go for the former (\"I don't care about it, so don't wait my resources for it\"). I just adapt my narrative to make it sound like it is cheaper to open source, and everybody is happy! reply namaria 8 hours agorootparentprevYet all software companies and to a lesser extent the whole economy depend on open source software to function. In my opinion, the toxic attitude is marginalizing developers who see that. reply arp242 3 hours agorootparentprev> labeled as “the person who wants everything open source” If you're always banging on about it, then yeah, that will happen. But that is of course a bad idea, because always banging on about any topic is almost always a bad idea in almost any context. reply pjc50 9 hours agoparentprev> As developers, IMHO you should always push for open sourcing as much as you can, ideally with a copyleft license and no CLA. Because it is beneficial for you personally: it means that you can reuse your code after you leave, and if you manage to make it copyleft and accept external contributions, it will force your employer to keep distributing the sources after you leave. How many people have had this work? Am I unusual in working at places that take an extremely restrictive attitude to their IP? reply kortilla 8 hours agorootparentNo, you're not in an unusual situation and there are very few companies doing anything interesting that would agree to this. The fact that the parent comment is so upvoted is a reflection of what people dream of, not reality. reply palata 6 hours agorootparent> is a reflection of what people dream of, not reality. Only if you understand it as \"open source everything\", it is a dream. But that is not what I say. What I say is that as a developer, you should try to open source as much as you can. Many times that's not nothing. It's really mostly a matter of thinking about it and seizing the opportunities. Related is also: don't be afraid to depend on copyleft libraries! As an employee, it's beneficial: it means that you have to contribute back (actually not exactly, but managers usually don't get the details so you can get away with it :-) ). reply II2II 7 hours agorootparentprev> The fact that the parent comment is so upvoted is a reflection of what people dream of, not reality. I can see that being the case among software developers. On the other hand, the organization I work for is so far removed from that world that my employer doesn't make any claims on most IP. If I decide to write a program to automate a task, the source is mine to do with as I please. They don't even lay claims on much non-software IP, stuff that ordinary businesses would lay claim to, since the attitude is more akin to the IP being \"carpenter's tools\" than the product of the job itself. (Granted, working in the education system helps on this front. The definition of restricted IP is rather narrow when the expected outcome is to share knowledge.) reply orta 9 hours agorootparentprevI've done it pretty successfully ( https://artsy.github.io/blog/2019/04/29/how-did-artsy-become... ) and now regularly look back at prior source code and blogs as I work on something else but with similar patterns I would say that getting to a point where this sort of thing is possible requires a huge amount of out-of-bounds culture and technical work though. Probably outside of the commitment range of folks who just want to work as an employee and not think about work outside of your paid hours reply arp242 3 hours agorootparentprevAt my previous job I extracted and open sourced quite a few libraries and such, although never the entire application itself. And I don't think anyone really would have benefited from open sourcing the application either. The code wasn't all that brilliant, it was hard to run, had a lot of assumptions that would only ever be true for us, and we didn't really want to spend the time fixing/optimising that. Tons of other things to do. Practically we would also have to scrub the commit history, do an audit to make sure we really don't have anything sensitive in there, and stuff like that. One reason I open sourced this stuff was just enlightened self-interest by the way: it enforces \"clean\" library-oriented programming with clean separations. When I joined there was stuff like \"email parser module\" which in principle would parse RFC2822 messages, but then would also call models to do a few database things, and nonsense like that. It was all pretty messy, and this hugely improved things. reply Aeolun 7 hours agorootparentprevI asked this once, and I was told “It is fine to work on/contribute to open-source as long as you don’t connect yourself with the company.” I thought that was a pretty cool thing. My job is essentially subsidizing some open source projects without ever getting the credit for it (or the potentially negative publicity, from their perspective, I guess) reply cpach 2 hours agorootparentOut of curiosity, when you commit to those projects, do you do that under a pseudonym? reply starbugs 9 hours agorootparentprev> Am I unusual in working at places that take an extremely restrictive attitude to their IP? No. reply phito 9 hours agorootparentprev> How many people have had this work? Statistically, almost nobody. reply rty32 8 hours agorootparentprevIt's not even necessarily about employers. I was pushing to open source an internal project I worked on. Turns out other coauthors are not interested in spending any time on it (reviewing code, make adjustments, go through approval etc). What can I do. reply palata 6 hours agorootparent> Turns out other coauthors are not interested in spending any time on it Because you open source it does not mean, AT ALL, that you need to spend time on it. I like Cap'n Proto as an example: it is a great open source project, but don't think of contributing to it: you will probably be ignored (unless you find and fix a very important bug, but good luck to find that there). And that's fine: the fact that it is open source is already great! Who would I be to expect the maintainers to work for free on top of open sourcing great work? reply palata 9 hours agorootparentprevI would argue that it happens more than you may think. You depend on an LGPL library? Tell your manager that either you can make the module that links to it LGPL as well, or that you can keep it proprietary but it will take \"a lot of work\". Often the manager will not value that piece of code so much and will allow you to open source it. Your employer allows you to open source something? Push for a copyleft license and no CLA. Your manager says that \"we could consider open sourcing it\"? Take this as a yes. Learn about licenses, what they mean, and think about IP at work. Many times the employer does not get anything from not open sourcing the code. It's just that they don't care. If you do, you can push them towards doing it. I am not saying that it will be bad for the employer! It may actually be good. I am just saying that it is better for you as a developer, so you should push for it. reply swarnie 9 hours agorootparentprevIf you write something at work does anything realistically stop you retaining enough person documentation / snippets to fully or partially recreate it? My buddy Slim seems capable of it and has built up quite a portfolio of \"ready to go\" solutions for common problems over the years. reply lifeisstillgood 9 hours agorootparentThat’s a legal Minefield wit the law on the side of the mines. reply kortilla 8 hours agorootparentprevYour buddy Slim is gonna get sued if he’s not careful. reply pjc50 9 hours agorootparentprevhttps://www.thedrum.com/news/2022/05/19/yahoo-lawsuit-allege... It's a matter of degree, obviously, but every time you do this it's likely to count as IP infringement. Generic libraries? Probably nobody will ever notice or care. But if they do, it could be expensive for you. reply Arch-TK 9 hours agorootparentIt may be expensive, but that's unlikely to be due to infringement. Most code written by most companies has been written a thousand times already, and there is nothing illegal about recreating a library which behaves the same as something you wrote at a company. reply hgal 9 hours agoparentprev\"Work for yourself, not for your employer. Your employer will fire you the second it is profitable for them, that's how it is.\" Maybe that's true for your employer. But it's bold to generalize. Certainly not true for the company where I work. And a quite sad view. reply surgical_fire 8 hours agorootparent> Certainly not true for the company where I work In the 2 decades I have worked as a Software Engineer, I met numerous people, in numerous different companies, that said those words, exactly. All of them were eventually proven wrong. No exceptions. reply ernst_klim 8 hours agorootparent> All of them were eventually proven wrong. No exceptions. I live and work in EU (and lived in other places) and I'm pretty sure corporate culture in EU, Japan, Russia is very different from that. There are pros- and cons- sure, but this easy-in/easy-out seems like a very US/Anglophone thing from my POV. reply surgical_fire 7 hours agorootparent> I live and work in EU What a coincidence. Me too :) reply dlahoda 8 hours agorootparentprevEastern block is same as Anglophone. reply Aeolun 7 hours agorootparentprevI mean, it’s still possible to be fired, it’s just _a lot_ harder. The relationship between employer and employee doesn’t seem nearly as hostile either. reply mobiuscog 3 hours agorootparentI'm presuming they meant 'made redundant' rather than 'fired'. The former is still fairly easy, and just has some process involved, the latter much harder unless gross misconduct can be demonstrated. It's absolutely true the the UK/EU/etc. have much better rights than the US in this regard, but perhaps the higher pay in the US (at least in tech) pays for that. reply arrrtem 5 hours agorootparentprevlol, you can be fired in a minute in Russia if you're not a CEO's son reply throwaway7ahgb 7 hours agorootparentprevIsn't that just entropy? Nothing can stay forever. I'd be interesting to see over long periods of time and many people who leave first, the employee quitting or employer firing/laying-off. My anecdotes tell me more people leave than get fired, especially the ones with options. reply palata 6 hours agorootparent> My anecdotes tell me more people leave than get fired, especially the ones with options. Sure, but that's not the original point. The original point is that the company won't make you any favour if they get a better deal by firing you. So you should do the same and leave if you find a better deal somewhere else. reply llm_trw 8 hours agorootparentprev>Certainly not true for the company where I work. I'm old enough to have heard this about a very large number of companies and it's always wrong in the long term. reply dspillett 4 hours agorootparentIt can change in the other direction too, through management/ownership changes. My previous regime taught me a lot about how I should limit the loyalty I show, but the current lot (buy out six years ago) do a much better job of not being arseholes. Like all aspects of work/life balance, it is a risk/reward balance that you need to monitor and adjust as circumstances develop. reply cpach 8 hours agorootparentprevPanta rei. reply execveat 8 hours agorootparentprevIt's heartwarming to believe in the \"family\" narrative some companies promote, but it's important to remain pragmatic. This narrative is just a motivational tool to encourage employees to go above and beyond their compensated duties. It's not a binding contract though. In reality, the power dynamic inherently favors the employer. Once an employee has invested their time and energy, the company holds all the leverage. There's little incentive for the company to uphold their end of this unspoken \"deal.\" Leadership changes, company priorities shift, and the \"family\" narrative can quickly fade when faced with financial realities or strategic decisions. reply ta1243 9 hours agorootparentprevAlso maybe it's true for OPs country, not in mine. It costs a lot of money to get rid of someone in my country. So that makes laying people off less profitable. \"it's profitable\" hides a world of problem though. \"it's profitable\" over what timeline - just because an employee isn't doing something profitable this week, doesn't mean they won't be next week. It costs a lot to fire someone even in \"at will\" countries where you literally just tell them to F-Off reply ernst_klim 8 hours agorootparentI have a friend who was laid-off from Google in Switzerland. The guy got salary for months but wasn't allowed in the office. So stupid. Seems like this \"we must do lay-offs\" is such a modus operandi for us big tech they would do it even if it cost them far more than utilize the pretty talented developer. reply palata 6 hours agorootparent> So stupid. Seems like this \"we must do lay-offs\" is such a modus operandi for us big tech they would do it even if it cost them far more than utilize the pretty talented developer. Unless they consider that it is still profitable because they will stop paying him after those 2-3 months. Which would not be stupid. reply htrp 7 hours agorootparentprevPerhaps because of the robust European labor protections that Google had to comply with? reply Aeolun 7 hours agorootparentThat’s why they have to give him money. That’s not why he’s not in the office. reply fn-mote 7 hours agorootparentprevThey don’t want him in the office to prevent espionage or sabotage. I’m sure you can think of other potential negatives, too. Contrast this with the presumably US situation of a reported Google firing by email, instantly gone forever, can’t even say goodbye to colleagues. https://news.ycombinator.com/item?id=40944098 The (nation-)state you work in matters a lot. reply GaylordTuring 7 hours agorootparentprevWhen saying “it’s profitable to lay someone off”, isn’t it implied that it’s profitable in a long-term perspective? Otherwise, it would according to the definition to be profitable to lay someone of as soon as they take their first lunch break. reply laserlight 9 hours agorootparentprev> Certainly not true for the company where I work. How do you know that? Even if so, how do you know that it won't change in future? reply quectophoton 5 hours agorootparentprev> Maybe that's true for your employer. But it's bold to generalize. Certainly not true for the company where I work. And a quite sad view. (Disclaimer: I'm not a business person, so I have no idea what the correct fancy terms are for any of these things.) If the company had to choose between survival thanks to firing someone (\"recovery to positive\" predicted after all legal unemployment stuff has been paid), vs loyalty keeping people on payroll risking bankruptcy[1] if the current trend continues, then I would expect them to choose fire-now-and-secure-survival, rather than don't-fire-because-i'm-sure-it-will-all-work-out-somehow-if-i-stay-loyal. After all, as an employee, I would also choose assured survival (moving companies) over loyalty (staying in the same company) the second I realize staying at the current company isn't beneficial for me (i.e. what the company pays me vs cost of living, not enough for savings, too little time to spend with family, always way too tired, etc). If your company would choose loyalty over being profitable, then that's interesting for sure. (EDIT: Though there's some companies that just run at a loss off the founder's savings, so it could be possible. I'm not denying your claims, just find them unusual.) [1]: Either because company is already on negative profit (\"debt\"? no idea if that is the correct word), or because current profit isn't enough to please VC and risks losing their support which would then put them on a negative profit situation. reply thih9 8 hours agorootparentprev> But it's bold to generalize. Certainly not true for the company where I work. I'd say this is true for all for-profit companies, by definition. Perhaps there is extra cost that makes it not worth it - laws, contract rules, etc. All the more reason to have that in place. reply mobiuscog 3 hours agorootparentI'd say it's more likely for any company relying on venture capital or other external investment or shareholders. If the company is not privately owned without investors, it doesn't really have control over the actions. reply throwaway7ahgb 7 hours agorootparentprevHow is it different for non-profits? What do you think happens when funding/donations dry up? reply thih9 4 hours agorootparentNote that I didn't claim that non-profit companies prioritize employees. I said that when a for-profit company prioritizes something else than profits then it's not acting as a for-profit company. reply dspillett 4 hours agorootparentprevIt is a relatively safe generalisation from my experience, both direct and through family & friends, even though I don't think it is true for my current circumstances. The regime I'm currently under genuinely seems to value me and the others around. I've been afforded quite a lot of understanding with regard to unplanned time off (and sometimes missed deadlines) due to dealing with some personal issues and looking after ill family (damn near lost the old man a couple of weeks ago, scariest day of my existence thus far). Admittedly there are some things that I have a very low bus count on, so they can't just easily kick me out or risk irritating me to the point where I chose to go, but it is more than just that. Of course if it came down to me or the whole company I'd be dropped quickly as is only sensible, so there is a limit to how much I can count on such loyalty (or if it _isn't_ loyalty, their feeling that they need to keep me around). Previous regime¹ was quite different. I wouldn't have trusted some of them enough to lend them a fiver³, and when I got news of the current lot coming in I was very close to telling them to fuck off⁴ in most unprofessional terms even though I didn't have anywhere lined up to jump directly to. ---- [1] I've effectively worked for that same entity for many years², though there have been a couple of complete changes of ownership & company name, the usual staff turnover overall, and changes to the product line-up. [2] due in part to a shocking lack of ambition :) [3] At one point I found out my line manager was leaving the day he did, via an email to the whole company about leaving drinks, management had known for at least weeks. Also working piles of unpaid overtime on a project then being bitched about by the CEO for not being present at 0900 on the dot one morning, was a good indication of how much my efforts were respected (I started working to rule at that point, in at 0859, out at 1731, no overtime without agreement up-front it would be paid for). [4] What had stopped me some days earlier was the brexit vote: that implied potential economic uncertainty which, to my many risk-averse personality segments, demanded I be a little more cautious for a time. reply marcyb5st 8 hours agorootparentprevmaybe true until MBAs take over the board. Then everyone becomes disposable. reply DonsDiscountGas 6 hours agorootparentprevCompanies aren't people. Even if your current direct boss wouldn't do this, they won't be your boss forever, and there's always somebody further up the chain who can overrule them (or fire them too). reply hnbad 9 hours agorootparentprevHi, former employer here who valued employees over profitability. It's true for all employers. Your job as a business owner is not to provide jobs to your employees, it's to keep the business afloat and maintain your income. There are limits to how much of a good person you can be, those limits are set by the economic system we exist in. Ignoring these limits will cause your business to fail. It's a sad view and it's not entirely accurate but it has better predictive power than the rose-tinted glasses you and possibly your employer may be wearing. Profitability is the baseline requirement, all else comes after. That's how a market economy works. If you remain at a stable net-zero that's great as long as nothing ever changes but it means you're extremely vulnerable to any disruption. To be clear: I'm not even talking about shareholders. This is true for private businesses too, maybe even more so. Profit means reserves, reserves mean being able to survive dips and downturns. But when the market changes you need to adjust and adjustment means the value of an employee can change drastically and it may be better to replace someone who was a great fit for the old market with someone who's a better fit for the new market. Sentimentality is a competitive disadvantage. You can balk at the morality and decency of this and I'd agree with you but you shouldn't be upset about the persons, you should be upset about the system that is requiring those persons to act this way in order to stay afloat. reply vladsiv 9 hours agorootparent> There are limits to how much of a good person you can be, those limits are set by the economic system we exist in. > you should be upset about the system that is requiring those persons to act this way in order to stay afloat. How would you change the system or modify the current situation in order to define new limits that are more, let's say, humane? In the article, there is an example of an ex-Google employee that got fired via email, he couldn't say \"Bye\" to his colleagues after 20 years of working with them. That doesn't seem like a system with reasonable limits. reply kortilla 8 hours agorootparent> In the article, there is an example of an ex-Google employee that got fired via email, he couldn't say \"Bye\" to his colleagues after 20 years of working with them. That doesn't seem like a system with reasonable limits. Until there is a system to easily weed out the people that go ballistic when being laid off, cutting off access is completely fine. If you want to keep in touch with people outside of work, exchange personal emails. reply Aeolun 7 hours agorootparent> Until there is a system to easily weed out the people that go ballistic when being laid off, cutting off access is completely fine. There’s literally whole continents where this isn’t an issue. If it is an issue in yours the problem is not the layoffs. reply anal_reactor 8 hours agorootparentprev> Until there is a system to easily weed out the people that go ballistic when being laid off, cutting off access is completely fine. Until there is a system to easily weed out the people who carry dynamite in their asses, anal cavity checks are completely fine. reply Aeolun 7 hours agorootparentThat’s an oddly appropriate username. reply cjfd 9 hours agorootparentprev\"How would you change the system\" is an above human level question. Winston Churchill said that democracy is the worst form of Government except for all those other forms that have been tried from time to time. The same for capitalism. It is an illusion that it is easy to change a system in which millions of people participate. The economic forces as painted here are very real but cruelty is optional. It is not very expensive to give an employee the time to say goodbye to colleagues. reply vaidhy 7 hours agorootparentprevI think you are missing the metapoint (maybe purposefully). In the US, even very profitable companies are laying off tons of workers to boost share price and executive compensation. The only reasons I can think of are: 1. They expect the company to be not profitable in near future and they are taking advance steps. 2. They think the workers add nothing to the top line or bottom line and cannot think of anything else that would gainfully make use of their talents. 3. They think the market would like it if they lay people off and hence it is in their selfish motive to keep laying people off. 1 requires a crystal ball and 2 is just incompetence. 3 seems the most likely answer for these layoffs. If you were running a company at a loss to pay people, you are not running a business, you are a charity. At the same time, if your profitable company wants to move the payoff from labor to shareholders in increasing percentages, your company is detrimental to the society. reply palata 9 hours agorootparentprevThis. I don't mean it as a criticism towards the employer: if they ignored profit and favoured the employees they would go out of business and eventually that would be bad for the employees as well. It's just how it is: your employer is not your friend. It's a contract. reply renegat0x0 9 hours agorootparentLudwig von Mises \"In the market economy the worker sells his services as other people sell their commodities. The employer is not the employee’s lord. He is simply the buyer of services which he must purchase at their market price.\" reply js8 8 hours agorootparentSorry, but that's not how the real world works, Mises is a fantasy for kids. That works for maybe contractors, and even that is muddy in cases like Uber etc. But for employees, they sell labor, not services. Likewise, when someone owns a factory, they don't just own the physical tools and material inputs of production (which they would rent to workers), but they own (control) the whole social structure that is needed to produce, all the human relationships (workers - management - owners - customers) required for the factory to work. That was Marx's main point, and that's why sane countries have labor laws that regulate this relationship. (But even if everybody was truly selling a service, \"free market\" would quickly collapse into oligopolies, as it usually does.) reply jessekv 9 hours agorootparentprevIt is also very demoralising for everyone if the company does not retrain or remove people from roles in which they are chronically underperforming. reply surgical_fire 8 hours agorootparentprev> Your job as a business owner is not to provide jobs to your employees, it's to keep the business afloat and maintain your income. Employee here. There's nothing wrong with that. In fact, I think it is healthy for both the employer and the employee to recognize this as a base of a good professional relationship. reply kortilla 8 hours agorootparentprev> You can balk at the morality and decency of this and I'd agree with you Why would you agree? A business is people voluntarily getting together trying to make money. If it fails to do so, why should other people have to step in and pay? reply TheOtherHobbes 8 hours agorootparentThe only reason that's how \"business\" is defined is centuries of imperial colonialism, much of it shaped by aristocratic patronage of the slave trade. Sooner or later we're going to have to take a look at that definition because the evidence strongly suggests it's doing more harm than good. There have been other more humane models, even in corporate America. reply throwaway7ahgb 6 hours agorootparentWhat are these humane models? My understanding is that there are a lot of very similar economic systems. There are pros and cons to each of these systems. I don't believe one system or government as truly figured it out. Folks on HN and reddit like to use other systems as a better system, but they always have some subjective negatives that are pushed aside. reply igleria 8 hours agorootparentprev> former employer Interested in your opinion about the following: Should employees get their salaries adjusted by inflation once per year? reply throwaway7ahgb 6 hours agorootparentShould employees get their salaries adjusted by inflation once per year? Why yearly, not monthly? Also what if there is deflation, do people get pay cuts? A lot of these issues are very easy to criticize on the surface, I don't think many here have ever tried to employee people and work on profit margins. reply igleria 4 hours agorootparent> Why yearly, not monthly? can be monthly, should be put in a contract. > Also what if there is deflation, do people get pay cuts? Yes, put it written form in a contract too. What I suggest (employer and employee explicitly negotiating inflation/deflation terms) could be better than the standard song and dance we all are used to suffer. reply lazide 9 hours agorootparentprevAlso - every system that has ever existed has had some variant of this. Output and effectiveness towards market needs has to matter or eventually the system collapses under its own weight. Churn has to happen, or ‘deadwood’ builds up through inertia. And when the system collapses, it’s like a forest fire with a massive amount of dead wood - it burns everything to the ground, not to grow again for a generation. As compared to periodic wildfires that may suck, but for which sprouts pop back up again in the spring. reply TheOtherHobbes 8 hours agorootparentThat depends on who gets to define \"market needs\" and how. Currently \"the market\" is literally just a betting shop for shareholders, which seems an odd way to be making important strategic decisions. reply throwaway7ahgb 6 hours agorootparentIt is not literally just a betting shop for shareholders. Most companies aren't even public or have shares. Using FAANG as the example for all jobs is not useful even though this is HN. reply lazide 8 hours agorootparentprevThe market as in who needs things, and is willing/able to pay for them, and who provides things and is willing/able to sell them. I’m not talking about the stock market (which is built on top), I’m talking the actual economy. Of course, command economies exist (and may again), and those tend to shortcut many of the niceties eh? reply disgruntledphd2 6 hours agorootparent> Of course, command economies exist (and may again), and those tend to shortcut many of the niceties eh? Command economies like corporations? reply lazide 5 hours agorootparentInternally a corporation is a command economy, externally it isn’t of course. Other command economy examples - gov’t, the USSR, the US during wartime, Russia right now, etc. Aka when someone in a position of authority says ‘money gets spent here and this way’, regardless of any natural market tendencies (and sometimes with little regard to solvency or pesky things like quality of life). reply jdbernard 7 hours agorootparentprevSounds like a skill issue. Joking aside I know, personally, business owners who have consistently acted with integrity and managed to maintain a successful, profitable business over the course of 30+ years, adjacent to the real estate market, surviving 2008, etc. so it's not like they've never faced had time or tough competitors. In that time they have never had layoffs. They've had to cut pay and reduce benefits in lean times, but providing gainful employment is a core value (above profit!!) and it has worked. reply smitty1e 9 hours agorootparentprevRealistic. It's a job offer, not a marriage vow. It's great if there can be rapport in a public company (as opposed to private) but that culture is a luxury trim at best. reply rat9988 9 hours agoparentprev>I really don't get why developers sometimes go out of their way to help their employer keep their code proprietary I don't do it because I am helpful, but because I'm paid to do so. reply palata 9 hours agorootparentYou're paid to produce code, not to fight against open sourcing it. reply tiborsaas 8 hours agorootparentNot really, code is a means to an end, we are paid to solve problems and sometimes it translates to writing or deleting code. reply tetris11 9 hours agoparentprevWe need more timebomb licenses. The company owns your code for X years, to which they can gain market advantage and make their money. After X years, it goes MIT/GPL. It benefits the coder, it benefits society, and the company gets that sweet temporary window of profitability over their competitors as they always have since time immemorial. reply palata 9 hours agorootparentIf you can convince your company to do that, you can probably convince them to use a copyleft license, I would think. reply tetris11 8 hours agorootparentThe benefit of the timebomb is that it gives a company half a decade or more to make their money. If it was copyleft from the getgo, then it'd be hard for the company to compete at all, which is a position I can understand. reply palata 6 hours agorootparent> then it'd be hard for the company to compete at all It depends on the codebase obviously, but you would have to prove that. On the other hand, if you open source it properly you may get contributions, which is valuable for the company. reply sjamaan 9 hours agoparentprevOpen sourcing is only really beneficial to yourself if the code is usable in your new job. Hopefully you're trying something new and growing - you probably shouldn't be switching jobs to work on the same kind of problems (or even with the same tech stack). Of course, if the open sourced project becomes wildly popular it's great for your CV, but that doesn't typically happen. Also, just flinging the code out in the open isn't enough. The company needs to be a good steward of your open sourced contributions. At my last job I left a few reusable open source Django components which I thought were very nice (and others are using it), but nobody felt responsible to take care of the project, so it languished. Now it is several Django versions behind and has bug reports which aren't being fixed. This was rather disappointing to me. I had really expected better because my colleagues in general were typically pro open source. I'm not using Django anymore, and I'm not inclined to put any personal time into it to keep the project alive. reply palata 9 hours agorootparentAt least it's part of your portfolio: you can show that code. What's the worst case? The code just gets unmaintained and dies. That's not a problem at all. reply andreasmetsala 9 hours agorootparentprev> This was rather disappointing to me. I had really expected better because my colleagues in general were typically pro open source. I'm not using Django anymore, and I'm not inclined to put any personal time into it to keep the project alive. Why would your colleagues want to maintain your pet project instead of making their own? Most open source projects live or die with their main contributors, if you expected the company to spend money maintaining your project then that’s the wrong expectation in my opinion. reply sjamaan 8 hours agorootparent> Why would your colleagues want to maintain your pet project instead of making their own? The projects in question were Django model field types, which I used in the larger projects I was working on. I made them available on PyPi so they're re-usable by others, and for easier installation. Maintaining these should be in their own interest, as the projects in which they are used are mission-critical systems for their customers. Presumably they haven't upgraded their Django in ages, so they didn't need to do any maintenance on these packages. But still, a) they may hit these bugs themselves sooner or later and b) the projects exist under the company's GitHub profile, so it makes them look bad. reply Aeolun 7 hours agoparentprev> I really don't get why developers sometimes go out of their way to help their employer keep their code proprietary I think it’s more that we hate dealing with any kind of bureaucracy? reply palata 6 hours agorootparentFair point. I just feel like it doesn't cost much to learn about open source licenses, and developers would benefit from it. I managed to open source a lot of code just by convincing managers (sometimes in a manipulative way, but that's the game: they do that, too). It did not imply any bureaucracy. reply moffkalast 9 hours agoparentprev> ask for a raise The problem is that employers make this process as toxic as it possibly can be, using every trick in the book of emotional manipulation, making you feel like you're blackmailing them and literally destroying their life. Adds a lot of resentment on both sides every time regardless of outcome and it just accumulates. reply rprospero 7 hours agorootparentWhen I was young, my father worked for a place where asking for a raise was a fireable offence. The founder had been a pioneer in the modern cattle-not-pets attitude toward servers, except he applied it to developers. When an employee asked for a raise, it meant on of two things: 1. The employee was a vain troublemaker who had over-value what they were worth in the market. Firing them would not only remove an inefficiency from the system (as they were likely not to work as hard if they believed that they were underpaid), but it would also helpfully remind the other developers that they were all expendable. 2. The employee was a 10x developer who was vital to the company processes and could command a much higher salary somewhere else. Even if you gave them a raise today, they could be hit by a bus tomorrow. The best course of action was to simply rip off the band aid. Fire the employee, have security immediately escort them from the building, and begin triage to ensure that the critical systems that they wrote/managed could be handled by the next resume in HR's pile. The line I will always remember is: Developer are like eggs. They are heavily undervalued, but also will crack under too much pressure. Thankfully, like eggs, you can buy them for cheap in packs of twelve, so it doesn't matter if you break a few. reply throwaway7ahgb 6 hours agorootparentThis is not an ideal way to run most companies however I can see this work under a few conditions. 1) This policy is known and communicated to current and future hires. 2) The company has found a way to pay each person the current market rate and makes efforts to adjust accordingly. Otherwise why would anyone stay? reply rprospero 4 hours agorootparentMany things about that company were not idea - the founder left his CEO position in handcuffs. However, the policy was not communicated to new and future hires in any manner. So why would anyone stay? The founder was very public with other companies in the area about both his policy of firing 10x developers and hiring any warm body that could put a resume in his hand. He told stories at local business meetings of the various people he hired who couldn't find a computer and were fired on the same day. So, when you found out what the corporate culture was like after about a month on the job, you had two options. 1. Stay on for a year. This cemented to every hiring manager that you were a 1x developer (because you kept the job), but absolutely not a 10x developer. You might get a junior developer position somewhere else, but never more than that. 2. Immediately quit the job. You now had a one month stint at the firm on your resume. Every hiring manager in town knew that 5% of people with a short stint were good developers and the remaining 95% were people who just finished \"COBOL for Dummies\". You'd best just leave the gap in your resume if you didn't want your resume in the trash. reply palata 6 hours agorootparentprevThat's why I argue that eggs should fight back :-). reply moffkalast 5 hours agorootparentprevWell he's probably right. There should be fixed and immutable company policy for automatic raises and bonuses based on independent quantitative measurements, i.e. inflation, local cost of living, and project metrics. No buts, no exceptions, on both the employer's and employee's side. This is how it works in most government jobs and it makes everything fairer, easier, and more predictable overall. reply rprospero 4 hours agorootparentI happen to work at a government job partially because I saw how my father was treated by the private sector. However, our institution is failing at the requirements that you put forth. The government ministers complained back in 2015 that the independent quantitative measurements weren't accurately capturing employee productivity. As you would expect from Goodhart's law, there certainly were certain employees being underpaid and overpaid, respectively. Thus, the measurements were scraped. However, the bureaucracy has prevented a new set of metrics from being put into place. As a result, I've been working for eight years on what I was told would be a six-month probationary salary because there is literally no mechanism for anyone to receive a raise. Thankfully, recent events are looking like this might change to something sane in a year or two, but the last proposal I saw for someone moving out of the bottom of a salary bracket was: \"candidate has won awards from professional bodies in at least three countries across at least two continents\". reply _spl 10 hours agoprevIn my early career, I used to believe in doing whatever it took to stay with a company, becoming emotionally invested in every line of code and system I developed. However, after a few negative experiences with bosses who didn't treat me well, I came to realize a few things: - My code is disposable, and I should not become overly attached to it - Loyalty should be mutual and based on a healthy relationship where both parties are willing to invest reply z33k 10 hours agoprevThis is why we need labor unions. The power dynamics are almost universally stacked in favor of the employer. Unions can equalize this to an extent. You will still be disposable, but at least you will get better terms if you end up getting disposed of. reply taurath 10 hours agoparentOh, but my employer is different and a union would only make business more complicated and less likely to give me raises and promotions! I’m a rising star, you know. - A literal supermajority of software developers reply Akronymus 9 hours agorootparentOver here (austria) the unions, which are pretty powerful, argue on behalf of ALL employees in that field. They basically set the minimum pay/benefits across the country for the kinds of jobs they represent by negotiating with the industrial association. By law, the result of those negotiations are the baseline for everyone. And everyone can still negotiate for better terms with their employer. One benefit the union members have over the non-union ones, though, is that you get insurance for legal counsel, for example. Other than that, union members don't have that many benefits, tbh. I still chose to be a member, because IMO it's the right thing to do. reply throwaway7ahgb 6 hours agorootparentprevThese comments miss so much nuance. This isn't black and white. I don't want to work for a union not because I'm a rising star. It's because I know I'm on the right of a normal distribution. I don't want to revert to median pay. No thanks. reply ryandrake 2 hours agorootparentHow do you know compensation is a normal distribution, and that you are on a particular side of it? Everyone on HN thinks they are a highly-paid, irreplaceable Captain Of Industry who, by their brilliant skill and shrewd negotiating power is making much more than the average. \"Surely a union would bring -my- compensation and working conditions down!\" they imagine. reply Aeolun 7 hours agorootparentprevI make enough money, I’m happy in my position, I feel like my employer treats me fairly. Why should I want a union? I also have nationally strong’ish labor laws. reply 9dev 2 hours agorootparentSo you’re sure it will stay that way, because you trust management to do the right thing? Even as you get older, life treats you harshly, or the economy goes down and suddenly you’re expected to work twenty extra hours? reply lordnacho 9 hours agorootparentprevIs it because of something inherent to software development, or is it just that software development became a big job category in an environment that was skeptical of unions? reply ernst_klim 6 hours agorootparent> Is it because of something inherent to software development, or is it just that software development became a big job category in an environment that was skeptical of unions? Unions are neither a good nor a bad thing. It's a price cartel, which is rent-seeking in its nature, but so are the employers. Thus effects of unions depend on competitiveness of the particular market. In low-paying markets they are clearly beneficial and counterbalance the monopsony. Programmers exist in a competitive market tho. Average programmer has a great bargaining abilities and most people know it. If you are a senior you wont get much from union, it's just a hustle. If you are a junior/immigrant unions will harm you by raising the bar. reply taurath 9 hours agorootparentprevWhen life is working out for folks and they make a decent income, they tend not to complain or want to rock the boat so as not to risk things. Can’t think of anything that would risk ones income capability more than being seen by companies as a union organizer - retribution is illegal but can’t be enforced, so it is de facto legal. See: starbucks suddenly closing all the stores that decided to unionize. People who own companies are not known for being nice but the best way to get the full force and fury of a billion dollars arrayed against you is to suggest unionization. reply dgb23 9 hours agorootparentIt’s interesting how much effort, and money is put into combating unions instead of developing a healthy relationship. reply Aeolun 7 hours agorootparentprev> the best way to get the full force and fury of a billion dollars arrayed against you is to suggest unionization Even if you didn’t personally think it’d be useful for you, that should make you reconsider? reply ThrowawayR2 4 hours agorootparentprevUnions benefit workers by giving them leverage through collective bargaining. For the US specifically, demand has been so astronomical for developers for the past couple of decades that most wouldn't have gained additional leverage from joining a union. Don't like your working conditions? Just job hop and get a pay increase as a bonus. That calculus might change if demand for developers cools down as a long term trend but it's not going to be anytime soon. reply pjc50 9 hours agorootparentprevI do think the high level of autodidacts makes it very different from anything else. It doesn't feel like a \"trade\", those involve building things with your hands. It's perhaps more similar to a \"profession\", like the much older ones of lawyers and doctors, but it hasn't developed the professional organization structure to go with it. Disintermediation also makes a difference. It's possible - very unlikely for any one person, but possible, and keeps happening - to just bootstrap a product out of pure labour and very little capital. At which point they get to keep a lot of the returns. It doesn't at all fit a nineteenth-century economic model, so you can't apply the M word. The \"10x\" phenomenon also makes a difference. Whether it's real or not, I think enough programmers believe it's real and want to be part of the 10x and somehow get a 10x reward. This is the exact opposite of a factory line or mass farmworker situation. (there's lots of interesting business anthropology research on piecework vs hourly rate work, I believe) reply beaglesss 7 hours agorootparentprevIt is because software developers are the capital and managerial class and they are the rich guy with the boot that stomps all the lower earning working class schleps , not the other way around. reply blitzar 9 hours agorootparentprevUnions are for miners and factory workers - software developers are highly paid, special people (just ask one and they will tell you all about it). reply einpoklum 4 hours agorootparentSoftware developers _are_ mostly factory workers, it's just that their factories make software. But titles aside - only some software developers are highly paid; some aren't. And we are not \"special people\" - that's just company propaganda. There are millions and millions of us around the world. And in most of our companies, there are a lot. And our employment conditions are not \"special\", they are like the other SW developers, and - guess what? Pretty much like those of most of the other non-manual-labor workers, even if the salaries differ by profession. Most employers, and the media, do a lot to inculcate us with this belief in distancing ourselves from each other, emphasizing differences and supposed uniqueness, so that our interactions go through them; and that we not think of doing things - professionally and otherwise - by direct coordination and collaboration, but rather through the mediation of management. But if there's anyone who has the capacity to imagine things operating differently than they do today, surely it must be us SW devs - if we don't limit our critical scrutiny to just the computers we work on but direct it also towards surrounding social structures. reply cdogl 9 hours agorootparentprevYou’re not wrong, but in Australia where I work only 12.5% of all workers are in unions. We have some moderately union-hostile legislation (strikes are hard to pull off lawfully) but nothing preventing union organisation in principle for most workers. So evidently it’s not only software developers who think this way. reply resource_waste 7 hours agorootparentprevIf you work at a Union place, you know the complications are absurd. I've worked at both, and the efficiency, friendliness, experience were all absurdly better without unions. With unions, everyone was trying to get each other written up, seniority mattered more than performance, politics were half the conversations at work. Engineering would be miserable with a union, you'd be basically locked into an employer climbing the totem pole. reply maxehmookau 9 hours agorootparentprev> - A literal supermajority of software developers _American_ software developers. Many in Europe don't fall for this way of thinking. reply bjornsing 7 hours agorootparentYes, and then we Europeans make peanuts compared to the Americans. Maybe the two have something to do with each other? reply ernst_klim 6 hours agorootparentprev> Many in Europe don't fall for this way of thinking. Many in Europe leave for the US because EU salaries are ridiculously lower. This is just a survey but it's in line with my perception, salaries on avg are 2-3 times lower in rich EU countries than in the US (after taxes). https://www.jetbrains.com/lp/devecosystem-2023/demographics/ reply maxehmookau 6 hours agorootparentAfter taxes, but before healthcare and the fact that most employment is at-will. I'll take higher taxes, healthcare through general taxation and relatively strong job security over a fixed dollar amount any day. Everyone is different though. reply ernst_klim 6 hours agorootparent> After taxes, but before healthcare and the fact that most employment is at-will. That combined doesn't even remotely cost that gap of 60k per year (plus strictly speaking this is not true since many employers would provide IT people with insurance). > I'll take higher taxes, It's not higher taxes. It's much lower salaries + much higher taxes combined. 60k a year in savings will provide you far more security and bargaining power than any strictest EU trade code would (and this is IMO one of the reasons for higher salaries in the US). reply 9dev 1 hour agorootparentWell. On the other hand, I can not be laid off easily, and will need to be told at least three months in advance; if my girlfriend gets pregnant, I can take a few months off, too; if the child is sick, I can stay at home too*; if I get sick, I’ll continue to get paid; if I have back pain, I can request to get expensive ergonomic equipment from my employer; if I need to see a specialist, or get insanely expensive treatment, I don’t have to worry; I have 30 days of vacation, of which I must take at least two weeks of consecutive time off, without any negative consequences for my job; if my children go to university, it’s pretty much free, as it was for me; I can’t ever get paid less by my employer, only more; I can take a few days of external educational courses of my choice every year, and my employer has to accept that (and pay for it!); and I probably forgot a bunch of other advantages here. Specifically on insurance: how much is that worth if it’s bound to your employer? What if you get laid off and don’t find a job in time, then get sick? What if your father is laid off at 55, nobody wants to hire him anymore due to his age and he develops cancer? I can tell you what happens in Germany: nothing. Both of you go to the MD and get treated as appropriate. You’ll never get me to trade all of this for a bit more money that I need to spend on health care, ridiculous tips, and overpriced apartments anyway. reply hnbad 9 hours agorootparentprevYou'd be surprised. Tech is full of special snowflakes who don't need a union because they're one of the deserving extremely talented net contributors who earned their special privileges and don't have any problems working unpaid overtime because their employer would certainly have their back if they ever needed time off or their productivity declined because of bad health or personal issues and don't want the undeserving underperforming underachievers and talentless diversity hires to get a leg up and steal their glory. I'm not even kidding, this is almost verbatim the attitude of plenty of (white, male, able-bodied tho at times mildly autistic) developers I've talked to throughout my career and at meetups and conferences, though few would be bold enough to spell it out this explicitly. Of course in those cases where they did end up having bad luck (be it health or otherwise) their employers did not in fact have their back, at least not longer than possible without harming profitability. If anything, developers working at smaller tech companies or in technical roles at non-technical companies were worse because they'd often see unions as only relevant for jobs that were \"beneath them\". Granted, most of the people I talked to were in Central and Eastern Europe. reply bjornsing 3 hours agorootparent> Of course in those cases where they did end up having bad luck (be it health or otherwise) their employers did not in fact have their back, at least not longer than possible without harming profitability. If you think unions or taxes will “have your back” in any real sense if you fall seriously ill you will be sorely disappointed. > Granted, most of the people I talked to were in Central and Eastern Europe. Could have something to do with these countries having experienced full blown communism not too long ago. reply redleader55 10 hours agoparentprevI don't want to be \"equalized\". I'm happy to negotiate my own terms. In my experience the union leaders negotiate better terms for themselves than for the people they represent. The company prefers to have a good \"shepherd\" for the flock - the union leader. If you look at where the good salaries are, you will not find them in France, Germany, Italy where the unions are strong, but the Bay Area, London, New York where unions are not really a thing. reply Rinzler89 9 hours agorootparent>If you look at where the good salaries are, you will not find them in France, Germany, Italy where the unions are strong, but the Bay Area, London, New York where unions are not really a thing. True, but those \"good salaries\" are only but a fraction of the total salaries in the countries you mention. Methinks a country should prioritize the welfare and well being of the entire country's people and the average worker instead of focusing on the top 1% SW devs while letting everyone else sink. Maybe keeping public services running for the bottom half of society is more important for society than creating the top SW companies in the world. After all you can't eat software, but we do need garbage men, doctors, pilots, sailors, nurses, handymen, architects, oil & gas and construction workers, farmers, car mechanics, barbers etc. a lot more to survive and run a modern society, than we need web devs to write yet another food delivery, ride sharing or crypto trading app designed just to skirt the laws and scam VCs and clueless investors while the interest rates are low. If your toilets breaks, you still need an actual plumber to show up in person to fix it since he can't push an OTA fix remotely from home, otherwise you'll be rooting in your own shit no matter how advanced your knowledge of K8s and ML-Ops is. Who cares if you're a well paid SW dev in London, NYC or San Fran but you can't walk alone at night because you're surrounded by poor minimum wage struggling and homeless people on substances or mental illness from wealth inequality, lack of welfare/social care and societal collapse due to decades of poor political and financial policies designed only to favor the wealthy? This is just my biased opinion, don't treat it as gospel ground truth. reply robocat 8 hours agorootparentI am interested in your parallel universe where software doesn't underpin every business... What line of business do you work in that doesn't need software for accounting, banking, invoicing, payroll, communications, yadda yadda. Consumer software doesn't run the world. I think your negative examples of software scams and hucksters are consumer based. > just to skirt the laws and scam VCs VCs are not generally seen as victims! > you still need an actual plumber to show up in person to fix And that plumber depends directly or indirectly on plenty of software to get their job done, from the basics of getting paid to the more complex of sourcing parts. And they use a mobile phone, which is 99.9% software with a little bit of hardware. reply Rinzler89 8 hours agorootparent>VCs are not generally seen as victims! I never said they were. reply throwaway7ahgb 6 hours agorootparentprevThis is a heart felt message but has nothing to do with SWE pay. You don't make any suggestions here at all, am I supposed to interrupt this as everyone gets paid the same? If that is the case, say it. If everyone is paid the same, who sets these salaries, unions? Then who sets the non union pay? Or does everyone work for the union or government? reply Rinzler89 6 hours agorootparentWhere did I say everyone should get paid the same? All I said was that focusing on producing the world's top SW companies doesn't seem to produce the societies with the best quality of life, and that that should be the main focus of a healthy society IMO. Yeah, what I said is not a solution to such problems, just an observation to the Original grandparent comment who tried to point out a few cities that have the best SW compensation as if that should be the end goal for every country. reply bjornsing 7 hours agorootparentprevSo we started out with the idea that unions are good for software developers, and now it’s about how you can’t eat software anyway and society should really value someone else? Yeah I get it… reply Rinzler89 7 hours agorootparentNo, I was just pointing out that what London, NYC, SF, are doing might not be the best for everyone even if they host the topo SW companies in the world. reply ChrisMarshallNY 9 hours agorootparentprev> New York where unions are not really a thing. I live in NY, and I can tell you that unions are very much a “thing,” hereabouts. reply throwaway7ahgb 6 hours agorootparentSame here, and unions are not much farther from the old school organized crime gangs. The unions in NYC protect the union. They care about little else. reply ChrisMarshallNY 4 hours agorootparent> The unions in NYC protect the union. They care about little else. That's their fiat. They're supposed to be like that. I'm really hoping that unions are learning a bit of humility from the last few years. We'll see. Japan (and Germany, I hear), has unions that are stakeholders (and, often board members) of corporations. They seem to work quite well, and are very powerful. reply 9dev 1 hour agorootparentParticularly in Germany, unions are quite literally unions of workers that band together, not menacing cartels. By law, companies must not interfere with workers organising themselves in a Betriebsrat, basically a company-internal union, which (as you mentioned) by mere forming becomes a stakeholder and needs to be involved in certain decisions. This doesn’t automatically mean you have a mandatory tariff for everyone, just that there is an organisation in your workplace that you can turn to, which will have your interests in mind, without you having to be a member. They can protect you from overreaching managers, baseless accusations, or unwarranted layoffs. And of course, they usually negotiate on behalf of the entire staff, especially on HR topics. While this isn’t always working perfectly, it’s still a big achievement in terms of equality and worker rights. It’s unfortunate, I think, that the US has such an awful history in terms of unions. There are far more nuances than discussions on HN make it seem, and virtually nobody looses if employees have strong protections against their employers. reply einpoklum 4 hours agorootparentprev> I don't want to be \"equalized\". You already are - by your employer. Mostly equalized at the absolute bottom of every situation where you disagree with your superiors or need to rely on the company for something. > I'm happy to negotiate my own terms. And your employer is even happier for you to believe you are actually negotiating your own terms. > In my experience the union leaders negotiate better terms for themselves than for the people they represent. That is certainly a potential problem. But think about the distribution of status, benefits and compensation of owners, managers, and employees - that's even more extreme, and prevalent, than misdeeds by union leaders. This sounds like a bit of a \"democracy can have corruption so let's support the monarchy\" kind of an argument. > The company prefers to have a good \"shepherd\" for the flock - the union leader. What companies prefer is to shepherd the flock themselves. Failing that, getting a collaborative shepherd. The challenge in unionization is disrupting the shepherding and allowing for conscious collective reflection and action. > in France, Germany, Italy where the unions are strong, but the Bay Area, London, New York where unions are not really a thing. The US is the center of the major world empire right now, and is not really comparable with most other places. However, even if you took the US - it's pretty much a hellscape, socially. Large tech companies are swimming in their ill-gotten gains while a huge number of people are homeless, many can only access contaminated water, students are in massive debt, infrastructure is in poor repair, public facilities and systems are under-developed... ---- reply StefanBatory 4 hours agorootparentprevExactly this. Union members, to an fair extent, are leeching on the back of other, productive employees. reply cnotv 2 hours agoparentprevIt's 100% the same also with labor unions. The company has a deficit and has to lay off. You can give up on that if it is the same in Europe. reply DonsDiscountGas 6 hours agoparentprevIt's why we need UBI and (in the US) universal healthcare, IMHO. there's a lot that's great about a dynamic and flexible economy, as long as it works for the people at large. reply StefanBatory 4 hours agoparentprevLabor unions are deeply corrupt. They serve the employer. At my dad job their leaders of unions are literally paid as \"leaders\" xd No conflict of interest and the fact they always get so well with CEO and never with workers is not suspicious. reply igleria 10 hours agoprevI'm pretty tired of having to argue with leadership about adjusting salaries by inflation. Of course they will begrudgingly eat up increases in other costs due to inflation, but the buck stops with the employees. Keep people that are performing on par or more happy? Naaaah, let's lose internal knowledge and money onboarding a new employee. I'm tired... reply lll-o-lll 9 hours agoparentThis hits hard. I just got my “pay rise”, that is below inflation, with no real comment. “Was it my performance?” “No, your performance was great”. “Can I look at other advancement?” “We will look next year”. So yet again, I feel the pressure to look for a new company. I am so so tired. reply A4ET8a8uTh0 9 hours agoparentprevThis AND you get to read about the great financial position company is in, dividends and stock buyback planned and scheduled, rounds of bonuses for the hardworking executives, who toil in anonymity to ensure good returns for shareholders by culling dead weight. What? You want your salary to match inflation? On the list you go. reply blitzar 9 hours agorootparent> What? You want your salary to match inflation? We dont have the money for that; we just hired a junior for your team that interned at a FAANG and had to pay them 2x your salary. reply hnbad 9 hours agoparentprevI'm a contractor and one of my client has grown considerably and shifted their attitude towards me from \"external expert who helps us in their field of expertise\" to \"employee with fewer protections\" over the years and this is very much true. Previously I'd tell them I need to raise my rate for one reason or another but I'd also temporarily reduce it when they were struggling with the market during COVID or suspend raising it. When I finally told them I needed to raise it because of inflation and nothing else, they kept hiding behind their own bureaucracy and eventually insisted on finalizing the adjusted contract as part of a round of \"pay raises\" in the company. I wasn't \"asking for a raise\", I was informing them of a change in my rate. And it wouldn't have been a raise anyway as it literally barely covered the rise in inflation. When HR eventually sent me the updated contract it felt like they expected me to make a happy dance or something. reply htrp 8 hours agorootparentI'd say the nature of a contractual relationship fundamentally differs from that of an employee-employer dynamic. As a contractor, you're not requesting a raise but rather renegotiating the terms of your engagement, which is a standard practice as contracts come up for renewal. Working with a lawyer for your contract negotiations can ensure that your interests are well-protected. They can help include provisions such as inflation-adjusted rate increases and other legal protections to safeguard your position. Ultimately, the outcome of such negotiations hinges on the power dynamics and mutual dependence between you and your client and your leverage is never higher than at the start of your employment/contract. reply Simon_ORourke 10 hours agoprevIf you believe your company has your best interests at heart then you're just a mark waiting to be given some unpleasant news some day soon. As a manager I've personally saved a few folks over the years from a very arbitrary sacking, and it often comes down to a few VPs scheming in some back room with little or no context on what they're planning, and pure luck that you can get to them before they make any announcements. reply blitzar 10 hours agoparent> If you believe your company has your best interests at heart then you're just a mark If employers believe you are not a mark - you are unemployable. reply Nathanba 8 hours agoparentprevThe same goes for all human relationships, you might as well include your parents or spouse. Or the government or your local grocery store that might close when you least expect it. I think it feels bad in a special way wit",
    "originSummary": [
      "Recent tech layoffs indicate that employees are often viewed as disposable by executives, even in profitable companies.",
      "Companies like Microsoft continue to lay off employees and implement pay freezes despite exceeding investor expectations and having high revenues.",
      "Wealthy executives, such as Tim Gurner, have made comments reinforcing the notion that employees are replaceable, regardless of their contributions or tenure."
    ],
    "commentSummary": [
      "Companies are transactional entities and may let employees go when it benefits them, emphasizing that employees are not indispensable.",
      "Over-hiring and layoffs are often strategies to boost shareholder value, highlighting the importance of employees focusing on their own interests and seeking fair compensation.",
      "Considering unions and open-sourcing code can be beneficial for developers, reinforcing the idea that the employer-employee relationship is fundamentally a business deal."
    ],
    "points": 238,
    "commentCount": 350,
    "retryCount": 0,
    "time": 1720770076
  },
  {
    "id": 40945076,
    "title": "Windows NT for Power Macintosh",
    "originLink": "https://github.com/Wack0/maciNTosh",
    "originBody": "Windows NT for Power Macintosh This repository currently contains the source code for the ARC firmware and its loader, targeting New World Power Macintosh systems using the Gossamer architecture (that is, MPC106 \"Grackle\" memory controller and PCI host, and \"Heathrow\" or \"Paddington\" super-I/O chip on the PCI bus). That is, the following systems: iMac G3 (tray-loading) Power Macintosh G3 (Blue & White) \"Yosemite\" Macintosh PowerBook G3 Bronze Keyboard \"Lombard\" Power Macintosh G4 PCI \"Yikes!\" The ARC firmware itself runs at a low enough level that it should be compatible with Old World systems using the same chipset too, but there is currently no loader for these systems; these are the following: Power Macintosh G3 (beige) Macintosh PowerBook G3 Series \"Wallstreet\", \"PDQ\" There may be issues on your hardware; with real hardware, this has only been tested on a Lombard. NT HAL and drivers have no source present for now. Drivers present in ARC firmware Cuda and PMU (albeit Cuda is untested on real hardware) ADB keyboard Flat 32bpp video framebuffer, set up by the loader. Currently the loader only supports ATI hardware (there may be issues with any ATI hardware with fcode version prior to 1.69, only the ATI Rage Pro LT (as present in Lombard) has been tested) Mac I/O internal IDE controllers, forked from OpenBIOS (there are no drivers for PCI IDE controllers!) USB OHCI forked from OpenBIOS (broken, nonworking, and initialisation code commented out) Drivers currently done for NT HAL, including: NT boot time framebuffer, super I/O interrupt controller, Grackle PCI bus support, Cuda and PMU (including low level ADB), serial port for kernel debugging only (please note Cuda support is currently untested on real hardware) Mac I/O internal IDE controller, forked from atapi.sys from NT4 DDK General HID/storage driver, intended to also contain a USB stack in future but currently only implements ADB keyboard/mouse and ramdisk as floppy drive for installing drivers at text setup time Flat 32bpp video framebuffer miniport driver Software compatibility NT4 only, currently. NT 3.51 may become compatible if HAL and drivers get ported to it. NT 3.5 will never be compatible, as it only supports PowerPC 601. (The additional suspend/hibernation features in NT 3.51 PMZ could be made compatible in theory but in practise would require all of the additional drivers for that to be reimplemented.) Installing Preliminary Grab binaries from the releases page. Burn the image to optical media. Partitioning Disk Boot your PowerMac from the burned optical media. When you get to ARC firmware menu, go to Run firmware setup, then Repartition disk for NT installation. The disk partitioner will first let you enter partition size of the NT partition (up to the 16383x16x63 CHS limit, minus 32 MB ARC system partition + 1 MB for partition tables / MBR backup / OS 9 drivers / ARC environment variable storage, giving a maximum possible size of 8030 MB), then will drop to a menu allowing the creation of additional Mac partitions. After adding a partition to the list, the only way to remove from the list is by cancelling the operation and starting the partitioner again. After you have created all Mac partitions you want, choose Finish partitioning and install, and confirm the operation. When finished, the partitioner will ask to Press any key to restart. Do so, and boot your PowerMac from the CD again. Installing NT Eject CD and insert your NT4 CD. Go to Run a program and enter the path cd:\\ppc\\setupldr - this may be cd01: or cd02: (...) if you have multiple optical drives present on your system. This may error with The file or device does not exist, just go back to Run a program and try again if so. NT4 setupldr will start. You will receive the message Setup could not determine the type of computer you have. Choose Other (default selected option), just press Enter when asked for hardware support disk. Pick your system from the list - all are equivalent and will load the Gossamer chipset HAL halgoss. Next you will receive the message Setup could not determine the type of one or more mass storage drivers installed in your system. Two drivers need to be loaded at this point: press S to pick a driver, choose Other from the list, press Enter when asked for hardware support disk Choose the first driver Mac I/O IDE Controller follow the previous steps again, but this time choose the second driver PowerMac General HID & Storage finally, press Enter to continue You will receive the message Setup could not determine the type of video adapter installed in the system. Choose Other from the list, press Enter when asked for hardware support disk, and choose the only option Open Firmware Frame Buffer. NT will boot and text setup will start. Go through the text setup. Under Setup has determined that your computer contains the following hardware and software components, change Keyboard from Unknown to XT, AT or Enhanced Keyboard (83-104 keys) and Pointing Device from Unknown to No Mouse or Other Pointing Device. Choose the C: drive from the partition list. If you chose to create an NT partition of size 2GB or less, it must be formatted. If you chose to create an NT partition of over 2GB in size, chkdsk will find errors and require a reboot. Boot your PowerMac from the ARC firmware CD again and follow the steps to boot the NT4 text setup again. Proceed through the rest of NT text and graphical setup as normal. Known issues If you are looking for a stable operating system, this is not it. Expect bugchecks, expect graphical setup to fail and restart because of bugchecks, etc. On a laptop system you may wish to remove the battery. At least on Lombard, the only way to power off the system when it bugchecks is via PMU reset or via total power removal. Currently the implemented drivers are the bare minimum to run and use NT. I have observed PMU hard shutdowns on NT boot, fixed only by a PMU reset. No idea what caused this. Dualboot quirks If you create additional Mac partitions, please make note of the following: The Mac partitions are listed in the partition table as HFS partitions but are not formatted. Use Disk Utility from OS X 10.1 or above to format the partitions. (Erase the volumes, not the drive!) The OS X installer, and just booting OS 8/OS 9, will error if a valid MBR is present on the disk at all, which is required for NT. In ARC firmware, go to Run firmware setup then Reboot to OSX install or OS8/OS9 if you wish to boot to those listed operating systems. Booting back to the ARC firmware will fix the MBR, so be sure to always use this option when unsure. In particular, formatting the created HFS partitions in OS X 10.2 and 10.3 will not work when a valid MBR is present! To allow OS 9 to mount the hard disk, boot from an OS 9 CD, run Drive Setup, select the drive and use the Update Driver option from the Functions menu. Building ARC firmware You need devkitPPC. Additionally, a libgcc.a compiled for powerpcle must be present in arcgrackle/gccle. If you need to find one, it should be present on any Void Linux mirror, the current filename to search for as of 2024-07-12 is cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps - decompress it by zstdcat cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps -o cross-powerpcle-linux-gnu-0.34_1.x86_64.tar, then pull the file out of the tarball: usr/lib/gcc/powerpcle-linux-gnu/10.2/libgcc.a. Ensure DEVKITPPC environment variable is set to your devkitPPC directory, usually /opt/devkitpro/devkitPPC Build the big endian libc: cd baselibc ; make ; cd .. Build the ARC firmware loader: cd arcloader_grackle ; make ; cd .. Build the little endian libc: cd arcgrackle/baselibc ; make ; cd ../.. Build the ARC firmware itself: cd arcgrackle ; make ; cd .. Replace stage1.elf and stage2.elf inside the release image. For recreating the image from a folder dump, use your preferred tool to create a hybrid HFS+ISO image, make sure System folder is blessed and BootX file is of type tbxi. Please note that stage1.elf must not be larger than 16KB and stage2.elf must not be larger than 224KB. Acknowledgements libc used is baselibc ELF loader and makefiles adapted from The Homebrew Channel Some lowlevel powerpc stuff, and ARC firmware framebuffer console implementation and font, adapted from libogc Some ARC firmware drivers (IDE, USB) adapted from OpenBIOS USB drivers in OpenBIOS were themselves adapted from coreboot ISO9660 FS implementation inside ARC firmware is lib9660 with some modifications. FAT FS implementation inside ARC firmware is Petit FatFs with some modifications.",
    "commentLink": "https://news.ycombinator.com/item?id=40945076",
    "commentBody": "Windows NT for Power Macintosh (github.com/wack0)209 points by TazeTSchnitzel 6 hours agohidepastfavorite109 comments stahi 4 hours agoNostalgia reminds me of this: https://lowendmac.com/2014/next-openstep-and-the-triumphant-... Amelio and the rest of his senior staff began searching for a way out. They needed a new operating system to buoy their attempts to compete with the Wintel juggernaut. The same search had taken place several times before, but now the company was desperate. Limited Options Ultimately the list of possible targets was narrowed down to five options: License Windows NT from Microsoft and bolt a Mac-like interface onto it. License Solaris from Sun and bolt a Mac-like interface onto it. Narrow the focus of the flagging Copland project and release it in a year and a half. Acquire Be and use BeOS. Acquire NeXT and use OpenStep. reply anyfoo 17 minutes agoparent> License Windows NT from Microsoft and bolt a Mac-like interface onto it. Gosh, am I glad that that didn't happen. I wonder how serious of an option that really was, it seems wildly out of place compared to the other propositions, which are all reasonable. reply faefox 3 hours agoparentprevWhat I'd give for a glimpse of the timeline where Apple bought Be instead of NeXT. reply Damogran6 2 hours agorootparentWhile I LOVE LOVE LOVED BeOS. It resonated with me and it was a great experience...it wasn't multiuser and didn't have some stuff that I forget, but seemed necessary for a system to have long-term success. But mam did it cut out the cruft and wring out ALL of the HP of the hardware of the time. reply pavlov 1 hour agorootparentWhy was multiuser so important? Apple’s most successful operating system in the Jobs 2.0 era isn’t multiuser either. BeOS would have been a fine foundation for smartphones and tablets. But of course it’s an open question whether Apple could have got that far in the 2000s without the return of Jobs. I suspect the company would have been acquired or merged with some unsuitable suitor like Sun. reply bee_rider 33 minutes agorootparentI agree that iOS isn’t multi-user in any real, like, multiple user accounts intended to be used by real people sense. I wonder, though, it is based on MacOS somehow, right? Which is based on BSD. Could there be some left over multi-user plumbing sticking around in a technical sense? reply PlutoIsAPlanet 24 minutes agorootparentYes, iOS still uses users in the technical \"Unix\" sense, they're just not mapped to actual physical users, but instead to various services. Android is in a similar boat. They're still an important way to manage filesystem access of programs. reply meibo 11 minutes agorootparentAndroid isn't in a similar boat, AOSP has full isolated Multi-User support that is realized through Unix users. You can create new users through Settings and they have their own home screen, apps and files. Most vendors have this enabled now and things like work accounts/MDM use the same system. https://source.android.com/docs/devices/admin/multi-user reply anyfoo 6 minutes agorootparentpreviOS barely uses that. Processes commonly run as “mobile” or “root”, but it does not matter very much. POSIX users and access permissions are archaic, and, in my opinion, don’t match with how almost any device is being used nowadays. iOS implements its own concepts through entitlements, containers, vaults, sandboxes etc. (Look up the “Apple Platform Security Guide” for details.) xattt 1 hour agorootparentprevMulti-user would have likely been bolted on one way or another at some point, as had happened when other OSes gained fundamental new features. reply pjmlp 1 hour agorootparentprevAssuming Apple would still be successful, and not close shop. It would have settled C++ as the main systems language on desktop OSes, between Windows, and Mac Be OS. Objective-C would have died, and Swift would never come to be. Clang and LLVM probably would never had gotten Apple's sponsorship. POSIX would have died on the desktop, as Be OS like other non UNIX OSes wasn't that keen in being UNIX like. There wouldn't be a flock of Linux and BSD developers rushing out to buy Apple hardware, instead of sponsoring OEMs shipping PCs with those distributons. reply postmodest 2 hours agorootparentprevI suspect that without Jobs, Apple is dead by 2002. Turned into an arm of, like, RIM. Then Google comes out with the Android phone and it's all over for everyone including Microsoft. reply hylaride 42 minutes agorootparentI suspect if Apple and NeXT never \"merged\", the phone market would be RIM vs (a very different) Android, but we'd be worse off. Blackberries were OK at the time, but my god RIM was so culturally conservative and Google at the time had no design sense that we'd probably be 10 years behind where we are now - and if we even had software keyboards they'd be terrible. reply nailer 1 hour agorootparentprevHTC G1 the original Android phone, was basically another sidekick. We’d have this shitty Android and WinMo for a decade longer and eventually things would get good. reply monocasa 1 hour agorootparentThe slate form factor was already being explored by other manufacturers before the iPhone came out. https://en.wikipedia.org/wiki/LG_Prada reply selectodude 56 minutes agorootparentSure, but unlike the iPhone, that thing was an unbelievable piece of shit. reply pathartl 42 minutes agorootparentIt's not hard to extrapolate that something would have entered the market. I assume it would only have been a couple of years later. reply scelerat 3 hours agorootparentprevI can’t think of a better acqui-hire than NeXT + Steve Jobs reply actionfromafar 3 hours agorootparentYou mean when NeXT bought Apple and forced Apple to finance the deal. A.K.A. that time when Steve Jobs acqui-hired Apple. reply alwillis 2 hours agorootparentApple looked at it as an acquisition, while NeXT looked at it as a merger. NeXT didn’t buy Apple obviously, but Steve Jobs and his lieutenants took over the most important positions within Apple. Which made sense given the situation Apple found itself in. reply faefox 3 hours agorootparentprevFor sure, it was pretty clearly the right call at the time and objectively so with the benefit of hindsight. But BeOS was just too good to be allowed (forced, really) to die on the vine the way that it did. reply tiledjinn 3 hours agorootparentNo need for hindsight. BeOS is still alive. A Haiku for you. reply pzautke 2 hours agorootparentprevAs someone who loved BeOS at the time, past discussion here on HN leads to believe that Apple's adoption of it would have had some real challenges. https://news.ycombinator.com/item?id=22002062 reply nine_k 3 hours agorootparentprevSome other company would release the iPhone then! reply pndy 2 hours agorootparentprevRight innit? It's not just Apple using BeOS but wondering how everything else would be different. You actually need to... think different at this point, aye? reply GeekyBear 3 hours agoparentprevNext's APIs were also ported to run on top of Windows NT and Sun's Solaris. In addition, the entire NextStep OS ran bare metal on several CPU architectures. > originally ran only on NeXT's Motorola 68k-based workstations and that was then ported to run on 32-bit Intel x86-based \"IBM-compatible\" personal computers, PA-RISC-based workstations from Hewlett-Packard, and SPARC-based workstations from Sun Microsystems. https://www.wikipedia.org/wiki/OpenStep reply SoftTalker 3 hours agorootparentYep, I used to run Apple's (maybe it was still NeXT at the time?) Mail app on Windows NT, I can't recall where I got it, maybe it was part of some goodies included with WebObjects? reply dkadams 1 hour agorootparentYes. WebObjects development was possible on NT due to “YellowBox”. It shipped with ProjectBuilder, EOModler, Mail, and other NeXT apps. You could even do Objective-C monkeypatching shennigans on YellowBox, which I remember being needed at the time to get the scroll wheels on certain mice to work on YellowBox apps. http://www.roughlydrafted.com/RD/RDM.Tech.Q1.07/4B800F78-0F7... reply mepian 3 hours agoparentprevThey did end up releasing salvageable bits and pieces of Copland as Mac OS 8. reply nullindividual 3 hours agorootparentSimilar story to Cairo with Microsoft [0]. Both visionary, both only partial successes. WinFS :-( [0] https://en.wikipedia.org/wiki/Cairo_(operating_system) reply cmrdporcupine 3 hours agorootparentI think of it as a time when people became so entranced with the idea of the emerging object-oriented zeitgeist that they imagined that making things OO and component driven would just hand-wave towards success. People rushed to make infrastructure to support this model, and, well, it was no magic bullet. Complexity, fragility, and a lot of hard work remained. reply exe34 2 hours agorootparentthe thing I'd like to see is for software to expose functional/\"rest\"-like interfaces. similar with websites and the content/user data. if you look at what accessibility software achieves by simply looking at the screen/hooking into the OS, now imagine any and all software could be connected like that. some of this functionality is now provided in individual silos, e.g. an email with an appointment in gmail can go into your Google calendar. a ticket from your train company can be added to Google wallet. but if you look at a flight booking system and want to compare the total price of a given set of dates for travel, including hotel and other things at different times - you're back to doing it on paper (or use somebody's website where 20% of the flights or hotels you want aren't included). reply nullindividual 2 hours agorootparent> the thing I'd like to see is for software to expose functional/\"rest\"-like interfaces. similar with websites and the content/user data. You mean, COM? reply postmodest 2 hours agorootparentprevIsn't that the SmallTalk model, but everyone decided to use C++ instead? reply pjmlp 1 hour agorootparentNot everyone, IBM OS/2 SOM had support for Smalltalk, C and C++. Smalltalk was OS/2's .NET, so to speak. Unfortunately it all died alongside OS/2, followed by IBM's pivot to Java. reply alsetmusic 3 hours agorootparentprevI saw Copland at MacWorld Expo Boston at Apple’s booth. It kernel panicked. We didn’t get to use it. But wow do I remember how exciting it was the first time I read about it in MacUser magazine. Basically some GUI mockups and promises of cool under-the-hood improvements, possibly the first time I was excited about future computing. Thank goodness they bought NeXT instead. They had no ability to manage projects at scale at the time. Too bad TimApple has no sense of how to do anything other than chase growth; we need another product person at the helm. Oh well. reply nrr 21 minutes agorootparent\"Too bad TimApple has no sense of how to do anything other than chase growth …\" I feel like this is more emblematic of the wider industry trend than Apple's own strategy that they chose to impose on us. The focus shifted very hard over the past 15-20 years from selling people new and novel gadgets and software to turning us ordinary folks into \"monthly active users.\" Personal computing got a lot less personal, and Apple leaned into it to stay relevant. reply karlgkk 1 hour agorootparentprevI'm tired of hearing \"Tim Apple's Apple Can't Engineer\". VisionOS has some of the most incredible, coolest engineering I've seen in a long time, from top to bottom. Even if you're not interested in AR/VR, the writeups and sessions about the dual processor OS layout they've used, the scene and room graphing technology they've built, and the challenges of building ultra low latency pass through scene reconstruction and mapping are incredible. Vision Pro might honestly be one of the technically coolest things built in the last 10 years, and I really hope they get the price down and fix some of the UX (weight, battery, etc), because it is something I use every day (and in fact, am writing this post on right now). It's for sure a Beta/DevKit/etc, and I wouldn't recommend the casual person to buy one, but again, on engineering chops alone, it is a masterpiece. reply jmb99 1 hour agorootparent> and I really hope they get the price down Very much agreed. I get why it’s expensive and I’m no stranger to the apple tax (and usually don’t mind it all that much), but I got the email today that it’s now available in Canada and $5k for a headset is… steep. reply mixmastamyk 2 hours agoparentprevWhy was A/UX not in the running, it was already written? Just add some polish. https://en.m.wikipedia.org/wiki/A/UX reply lizknope 2 hours agorootparentA/UX was still a licensed version of Unix. This post says the licensing fee was $1,000 per seat. https://www.reddit.com/r/Apple_AUX/comments/11wks6i/what_was... I remember the AT&T / BSD lawsuit in 1992. Maybe Apple looked at NeXTSTEP and since it is based on the Mach microkernel and BSD that they would be able to fix the 10 or so files to avoid the AT&T / Unix System Labs licensing fees like Free/NetBSD did. In the 1980's Microsoft had their own port of Unix named Xenix. Why didn't they push this instead of the very limited MS-DOS? https://www.abortretry.fail/p/the-history-of-xenix > In March of 1982, Western Electric reduced the royalty fees that it charged for UNIX with the introduction of UNIX System III (a combination of UNIX V7 and PWD). This was done by raising the source license cost, but lowering the royalty per user license to $100 (about $318 in 2023) from $250. This meant that Microsoft’s prepayment of $200000 in 1980 (around $700k in 2023) for a discount in volume was voided if they wished to use the newer system. That same month, on the 10th, Paul Allen held a seminar in NYC where he outlined plans for MS-DOS 2.0. reply AshamedCaptain 52 minutes agorootparent> Why didn't they push this instead of the very limited MS-DOS? Hardware requirements. For a (quite long) while the plan was to keep DOS for microcomputers and Xenix for the serious stuff. DOS gained some extensions that made it very Xenix/UNIX-like (like subdirectories, pipes, \"-\" as command line switch char, device files in \"\\dev\" instead of a global namespace, etc.). reply rjsw 2 hours agorootparentprevA/UX runs on 68k NuBus machines, not PowerPC PCI ones. reply mixmastamyk 2 hours agorootparentUnix itself does—seems like a short porting effort compared to the other choices. They had emulation available as well. reply smm11 3 hours agoparentprevI still think Solaris/Mac stood a chance; the hardware would have been miles better than anything else. reply ndiddy 5 hours agoprevThe same developer ported PPC NT 4 to the Wii last year: https://www.youtube.com/watch?v=d8BpUpr1h9U reply lawnchair 4 hours agoprevWindows NT was fascinating. If you want to read a great book about it, check out Showstopper. reply eigenvalue 4 hours agoparentThat was a great book, up there with Masters of Doom and The Soul of a New Machine. reply dboreham 4 hours agoparentprevWas ? reply badgersnake 2 hours agorootparentDunno why this is getting downvoted. Modern Windows is still very much a Windows NT derivitive. reply redox99 2 hours agorootparentIs it a derivative? I'd say it's the latest version of Windows NT. reply pndy 2 hours agorootparentThat's still the same NT line tho they changed how versions are called after Windows 8.1 which was NT 6.3. Initial Windows 10 release was bumped to NT 10.0 but further versions use year-month and year-half year formats [1] - https://en.wikipedia.org/wiki/List_of_Microsoft_Windows_vers... reply badgersnake 2 hours agorootparentprevIt’s the same codebase that was my point. Whether it’s still called Windows NT or whether that name died in the 90s can probably be argued either way. reply sedatk 1 hour agorootparentNT is still the name of the kernel. (ntoskrnl.exe) reply mycall 8 minutes agorootparentI wonder if that is still true for Azure OS. nullindividual 1 hour agorootparentprevThe marketing and front-facing name clearly died. The internal name did not. reply lizknope 2 hours agorootparentprevI think XP is when the DOS based Win95/98/ME was merged with NT reply mewse-hn 47 minutes agorootparentThe Win95 line wasn't really merged, more discontinued. XP had improved DOS/legacy compatibility vs win2000 but not much. reply the_panopticon 4 hours agoprevFascinating work. The ARC std https://en.wikipedia.org/wiki/ARC_(specification) was used to boot the Dec Alpha Windows machines, along w/ MIPS, etc. Anyone know of other open source variants of that in the wild? At intel in 1998 the original efi spec was modeled on & inspired by Arc. The Intel boot initiative (IBI) in fact looked mostly like Arc. EFI (now UEFI) is sort of Arc + installable GUID-based interfaces (aka protocols) a la MS COM https://en.wikipedia.org/wiki/Component_Object_Model. Page 8 of https://www.intel.com/content/dam/www/public/us/en/documents... recounts some of those travails. reply lizknope 3 hours agoparentMy roommate's Alpha had the SRM firmware which he used to boot NetBSD. ARC would boot NT but SRM could not. https://en.wikipedia.org/wiki/SRM_firmware reply UniverseHacker 2 hours agoprevCan someone explain the context here? Surely NT is closed source and was never developed for macs, what actually is this? And what are the chances you can get software for it? Presumably most NT software was compiled for Intel only and closed source. reply fredoralive 1 hour agoparentNT was designed to be portable, so the machine specific parts are cleanly separated, even on the same CPU. So if you port only those parts, the existing binaries for an architecture should work. The three bits are: 1) ARC boot firmware. NT was developed on non-x86 systems like i860 then MIPS, and ARC is the native boot firmware used (on x86 NTLDR emulated it prior to Vista). Similar to a PC BIOS / UEFI, and a PCI PowerMac would use OpenFirmware. As well as providing an ARC compatible environment that loads over OpenFirmware, this project seems to does some fun so the boot firmware can pretend there's a storage device so that driver \"floppies\" can be loaded during the initial stages of setup. (ie an F6 disc) 2) A HAL. The main NTOSKRNL is hardware agnostic, so the idea is that there's one binary for each CPU architecture. But the kernel needs to interface with actual timers, busses etc., so the interface code is in HAL.DLL and the appropriate one is copied by setup. (For example see https://www.geoffchappell.com/studies/windows/km/hal/history... for a list of x86-32 ones in older versions of windows, with various HALs for NEC PC-98, IBM MCA, various early multiprocessing systems, nowadays there's just one AMD64 one that's mostly in the kernel itself). So the main kernel in unaltered, and halgoss handles the Mac specific stuff. 3) Device drivers. Once NT is up and running it does need actual drivers. The specs for 1) are known, so can presumably be emulated, and I guess there's a DDK for 3) (or it can be deduced from another DDK), I guess 2) is probably the one that would need the most insider knowledge, I'm not sure if the leaked NT sources go down to that level as I've never looked at them. As for compatibility, 32 bit PowerPC Win32 binaries, 16 bit x86 Win16 binaries, and whatever x86 DOS stuff will run in an NT4 DOS box. No x86 Win32, only Alpha had an emulator for Win32 x86 stuff (until ARM stuff). reply mycall 5 minutes agorootparentI remember migrating whole Windows installations using image copies, but taking a fresh install's HAL files and copying them onto the images and volla, system transfer completed (device drivers would need reinstalling though). reply nullindividual 2 hours agoparentprevNT4 was developed for PowerPC architectures (and MIPS, Alpha). A large portion of the NT4 source code was leaked [0]. I'm not saying the author used the NT4 source code in any fashion, but I would imagine using such source would make life much easier to accomplish this task. Or the author clean room reversed engineered the bootloader. Or there is enough information out there to forgo the need for any internal knowledge of Windows code. The PPC code base was never explicitly targeted for Macs, but other systems from IBM/Motorola; but because it is a 'common' platform, the binaries themselves on the NT4 ISO do not need modification. And yes, you can find the source code on GitHub. In multiple repos! [0] https://www.neowin.net/news/exclusive-windows-2000--windows-... reply aeyes 1 hour agorootparentIsn't this just drivers and a HAL? Microsoft built the OS in a way so that such ports would be possible so there should be some API documentation out there. The question is how complete it is because everybody just used the MS provided HAL. reply hylaride 16 minutes agorootparentprevYeah, I think the only commercial OEM releases of NT that got sold were x86 and Alpha, though there was support for a handful of specific MIPS and PPC machines, I don't think there were any OEM sales of them with it. I do know that only x86 and Alpha had all the service packs released. There were also at one time internal MS builds on Sparc, but that didn't last long. Alpha did have some pre-release builds of windows 2000 that went out, but there was no final release. A fun fact is that because of where he worked at the time was a DEC shop, the creator of PuTTY originally created it because he got an alpha windows machine and there was no compatible terminal emulator for alpha NT. He supported alpha builds well into the 2000s and IIRC only dropped it as the machine he was using still for builds finally died. reply temac 1 hour agorootparentprevNt has a dynamic HAL layer to adapt it to platforms. I'm not sure if it was publicly documented, but in any case yes the source code leak don't harm that kind of port, even if you \"just\" have to write a HAL. reply sumtechguy 41 minutes agorootparentThe current PE exe format supports it. As well as a few Alpha, a few ARM formats, a few MIPS format, Itanium and of course x86. The big part would be finding a compiler that spits out the right code. The preceding NE format also did a few variations. reply gcp123 5 hours agoprevWow, just when I was looking for a reason to fire up my old 1998 bondi blue iMac G3, this pops up. What a weird, wild, and specific project! reply pwenzel 3 hours agoprevMan, I loved Windows NT back in the day. It was light enough that I could run it on fairly low end late-90s hardware, and it was substantially more stable than Windows 95. reply pjmlp 1 hour agoparentHad Microsoft been serious with POSIX subsystem, and most likely I would never bothered with Linux for our UNIX assignments. Instead I dual booted between both of them. reply hylaride 13 minutes agorootparentIt's a shame. NT did support POSIX, but it was in practice designed to make it easier to port UNIX apps. At co-op job I had back in the day (~2000), I had to fight with Veritas' backup software on NT that wrote to tapes. I don't recall what it was (a device mapping to the drive maybe?), but you could see the UNIX foundations of the software. reply fny 5 hours agoprevVery, very cool, and I can't wait to relive giving up all my DOS games! reply TickleSteve 4 hours agoparentThis isnt an x86 emulator that can run a DOS box, this is NT running natively on PPC. So... no DOS games I'm afraid. reply Narishma 4 hours agorootparentYou misread the comment. They don't want to run DOS games, they want to relive the inability to run them when switching to Windows NT. reply my123 4 hours agorootparentprevNT on PPC had NTVDM afaik (with the licensed SoftPC x86 emulator) reply nullindividual 4 hours agorootparentprevMacintosh PowerPC. NT4 was already compatible with PowerPC systems. The support ended with NT4 Service Pack 2; no further SPs had PPC support. reply duffyjp 4 hours agorootparentSomeone gave me an NT4 CD when I was a kid and on the jewel case it said it supported x86, PPC, I think some other architectures too. I was disappointed I could never get our PowerMac to boot from it. :P reply nullindividual 4 hours agorootparentAlpha and MIPS are the other two architectures. Sad trombone noises for Alpha reply duffyjp 56 minutes agorootparentI had a job in 2006 where the sysadmin ran the entire shop on a single OpenVMS server with two 500Mhz Alpha CPUs. It was our file server, AD server (somehow), DB server, web server, everything. I just looked it up and that CPU came out in 1996, at the same time Intel released their 200Mhz Pentium. Alpha was way ahead of their time. reply FMecha 4 hours agorootparentprevBecause desktop PowerPC was unpopular for anything other than Mac, and PPC NT wasn't designed for Open Firmware. Though you can blame Microsoft's monopoly tactics - in which loyalty to x86 was the prime directive, alternative OSes on PPC Macs were not a widely accepted idea either - people stuck around with classic Mac OS, in (ultimately) vain hopes that Copland/Taligent would deliver. reply nullindividual 4 hours agorootparentWhere are you getting this information? The PReP/CHRP was a venture by Apple, IBM, and Motorola (but primarily IBM) to allow companies to build OSes for a common platform. Apple didn't want to participate in the end. But like other ports of NT to MIPS and Alpha (Alpha enjoyed much more support until Compaq dropped support), the popularity was well beneath the dirt cheap in comparison x86 platform. https://en.wikipedia.org/wiki/PowerPC_Reference_Platform https://en.wikipedia.org/wiki/Common_Hardware_Reference_Plat... This had nothing to do with Microsoft's monopoly. And nothing to do with exclusively 'desktop' PPC -- remember, NT4 Server also had PPC support through SP2. IBM's intention was for servers, not desktops (though they did produce compatible laptops/desktops), to support operating systems such as AIX and S[l]o[w|l]aris AIX isn't exactly a desktop OS. reply FMecha 1 hour agorootparent>Where are you getting this information? From personal perception every time I read articles on Taligent, Copland, and Workplace OS (which did result in a demo PowerPC port of OS/2). >This had nothing to do with Microsoft's monopoly. Admittedly I had that comment in mind to pre-empt any potential narrativeposting. reply yjftsjthsd-h 3 hours agorootparentprev> AIX isn't exactly a desktop OS. If so, only artificially; AIX was born on a workstation and seems to still support running a graphical desktop environment. reply lizknope 2 hours agorootparentAIX was the first Unix I ever used in 1991 on an IBM RT with the ROMP CPU https://en.wikipedia.org/wiki/IBM_RT_PC I wanted one on my desktop so bad. DOS/Win3.1 PCs were just so bad to me. Then I saw SunOS and really just wanted any Unix system on my desk. I bought a PC in 1994 just to install Linux. reply icedchai 31 minutes agorootparentI had one of these back in the early 90's, bought used, of course. It was running AIX 2.x. I moved on to Linux pretty quick. reply nullindividual 3 hours agorootparentprevJust like NT4 Server :-) Intended use vs. potential use. And yeah, NT4 Server was just NT4 Workstation with some registry fiddly bits flipped. reply yjftsjthsd-h 3 hours agorootparentRight; NT was and is likewise quite capable of being a desktop or server OS, the difference is that MS actually pushed that and NT 10.0 is still actively used in both contexts. reply xbar 4 hours agorootparentprevRead it again. reply xattt 3 hours agoprevThis is one of those things that I’d have nerdy arguments about with my nerdy friends in middle school. reply technothrasher 3 hours agoparentDude, shut up, the Amiga is totally better. Cooperative multitasking is lame. Have you seen HAM mode?? reply xattt 1 hour agorootparentNo, Amiga sucks, it can’t even run UNIX! Do you even know what it is? My dad uses it for work. reply pjmlp 1 hour agorootparentDepends on what one considers UNIX. https://en.m.wikipedia.org/wiki/Amiga_Unix reply ngneer 4 hours agoprevAny screenshot? reply unleaded 4 hours agoparenthttps://social.nano.lgbt/notes/9vl4xznolng53nce reply farmerbb 1 hour agoparentprevhttps://twitter.com/virtuallyfun/status/1811812177805767078/... reply tiahura 1 hour agoprevMicrosoft killed the PowerPC 615 IBM engineer offers up 615 information https://www.theregister.com/1998/10/01/microsoft_killed_the_... reply walterburns 3 hours agoprevIf I had a dime for every person that's asked for this. reply actionfromafar 3 hours agoparentYou'd have a dime? reply Connector2542 3 hours agoprevhow far away is it from being a (even as a joke) daily driver? reply Palomides 2 hours agoparentconsidering how there's basically zero software for PPC Windows even if it were running smoothly, very far reply roytam87 2 hours agorootparentbut at least there is VC4 RISC Edition available for PowerPC, you can compile something on it. reply runjake 4 hours agoprevIt's been mentioned here before, but Dave Plummer's YouTube interview[1] with Dave Cutler[2] (NT lead architect) is a must-watch with never revealed before history about Windows NT. 1. https://www.youtube.com/watch?v=vxmZPMg7vIs&list=PLF2KJ6Gy3c... 2. https://en.wikipedia.org/wiki/Dave_Cutler reply gigatexal 4 hours agoprev [7 more] [flagged] space_ghost 4 hours agoparentThe truest of arts require no purpose other than to exist. reply itomato 4 hours agoparentprevExploring the walled-off corridors of prep and chirp? Maybe this will enable OS/2 PPC to run? More reading on NT & PPC: https://virtuallyfun.com/wordpress/2020/08/02/the-lost-histo... reply nullindividual 4 hours agorootparentThis is specifically for NT4 on Macintosh PowerPCs. Not OS/2; not AIX; not NetBSD; not . Not some random other PPC variant. reply clwg 4 hours agoparentprevBecause it's kind of awesome. I think the better question is why not? reply jackstraw14 4 hours agoparentprev [–] Hard to say why. Windows NT was something pretty interesting though. It was meant for a corporate environment but was well known as a much more \"stable\" version of Windows than 95 and 98. All happening in parallel, with the Power Macintosh even! reply nullindividual 4 hours agorootparent [–] NT4 was more stable, to a point. But it had it's serious annoyances. IP address change? Reboot. I can't remember other scenarios, but there was a lot ofin NT4. Graphics and print drivers? Moved from user space (NT 3.5, 3.51) to kernel space -- and oh boy, graphics card vendors SUCKED at writing drivers (printer manufactures never improved to this day, hence the elimination of vendor print drivers). BSOD city. NT4 was also hobbled as a desktop OS for consumers due to only supporting up to DirectX 3. But! It was super popular for CAD/CAM/3D modeling. FireGL cards. 3DLabs cards. Yissss those were awesome monsters. It also required a hefty 16MB RAM but as with all minimum system requirements from Microsoft, it really shined on 32MB+. This is one of the reasons we had Windows 9x -- memory was too expensive to run NT4. One of my favorite Microsoft-led NT4 (and SQL Server) proving grounds was TerraServer [0][1]. I miss the days of NT4. Computing was much more interesting and varied. [0] https://en.wikipedia.org/wiki/Terraserver.com [1] https://www.microsoft.com/en-us/research/publication/the-mic... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The repository contains source code for ARC firmware and its loader, targeting New World Power Macintosh systems using the Gossamer architecture, including models like iMac G3 and Power Macintosh G3.",
      "The ARC firmware supports various drivers, but only NT4 is currently compatible, with potential future support for NT 3.51.",
      "Installation involves downloading binaries, partitioning the disk, and following prompts to set up NT4, with known issues such as instability and specific steps required for dual-booting with Mac partitions."
    ],
    "commentSummary": [
      "A developer has ported Windows NT to run on Power Macintosh, sparking nostalgia and discussions about alternative OS histories.",
      "Windows NT was designed to be portable, supporting various architectures like PowerPC, MIPS, and Alpha, though it was primarily known for x86.",
      "This project highlights the technical feasibility of adapting NT to different hardware platforms, showcasing the modularity of its design."
    ],
    "points": 209,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1720788718
  },
  {
    "id": 40939703,
    "title": "Gene-silencing tool shows promise as a future therapy against prion diseases",
    "originLink": "https://news.mit.edu/2024/charmed-collaboration-creates-therapy-candidate-fatal-prion-diseases-0627",
    "originBody": "A new gene-silencing tool shows promise as a future therapy against prion diseases and paves the way for new approaches to treating disease. Greta FriarWhitehead Institute Publication Date: June 27, 2024 Press Inquiries Caption: CHARM — which stands for Coupled Histone tail for Autoinhibition Release of Methyltransferase — can turn off disease-causing genes such as the prion protein gene, and potentially genes coding for many other proteins implicated in neurodegenerative and other diseases. Credits: Image: Madeleine Turner/Whitehead Institute Drug development is typically slow: The pipeline from basic research discoveries that provide the basis for a new drug to clinical trials and then production of a widely available medicine can take decades. But decades can feel impossibly far off to someone who currently has a fatal disease. Broad Institute of MIT and Harvard Senior Group Leader Sonia Vallabh is acutely aware of that race against time, because the topic of her research is a neurodegenerative and ultimately fatal disease — fatal familial insomnia, a type of prion disease — that she will almost certainly develop as she ages. Vallabh and her husband, Eric Minikel, switched careers and became researchers after they learned that Vallabh carries a disease-causing version of the prion protein gene and that there is no effective therapy for fatal prion diseases. The two now run a lab at the Broad Institute, where they are working to develop drugs that can prevent and treat these diseases, and their deadline for success is not based on grant cycles or academic expectations but on the ticking time bomb in Vallabh’s genetic code. That is why Vallabh was excited to discover, when she entered into a collaboration with Whitehead Institute for Biomedical Research member Jonathan Weissman, that Weissman’s group likes to work at full throttle. In less than two years, Weissman, Vallabh, and their collaborators have developed a set of molecular tools called CHARMs that can turn off disease-causing genes such as the prion protein gene — as well as, potentially, genes coding for many other proteins implicated in neurodegenerative and other diseases — and they are refining those tools to be good candidates for use in human patients. Although the tools still have many hurdles to pass before the researchers will know if they work as therapeutics, the team is encouraged by the speed with which they have developed the technology thus far. “The spirit of the collaboration since the beginning has been that there was no waiting on formality,” Vallabh says. “As soon as we realized our mutual excitement to do this, everything was off to the races.” Co-corresponding authors Weissman and Vallabh and co-first authors Edwin Neumann, a graduate student in Weissman’s lab, and Tessa Bertozzi, a postdoc in Weissman’s lab, describe CHARM — which stands for Coupled Histone tail for Autoinhibition Release of Methyltransferase — in a paper published today in the journal Science. “With the Whitehead and Broad Institutes right next door to each other, I don’t think there’s any better place than this for a group of motivated people to move quickly and flexibly in the pursuit of academic science and medical technology,” says Weissman, who is also a professor of biology at MIT and a Howard Hughes Medical Institute Investigator. “CHARMs are an elegant solution to the problem of silencing disease genes, and they have the potential to have an important position in the future of genetic medicines.” To treat a genetic disease, target the gene Prion disease, which leads to swift neurodegeneration and death, is caused by the presence of misshapen versions of the prion protein. These cause a cascade effect in the brain: the faulty prion proteins deform other proteins, and together these proteins not only stop functioning properly but also form toxic aggregates that kill neurons. The most famous type of prion disease, known colloquially as mad cow disease, is infectious, but other forms of prion disease can occur spontaneously or be caused by faulty prion protein genes. Most conventional drugs work by targeting a protein. CHARMs, however, work further upstream, turning off the gene that codes for the faulty protein so that the protein never gets made in the first place. CHARMs do this by epigenetic editing, in which a chemical tag gets added to DNA in order to turn off or silence a target gene. Unlike gene editing, epigenetic editing does not modify the underlying DNA — the gene itself remains intact. However, like gene editing, epigenetic editing is stable, meaning that a gene switched off by CHARM should remain off. This would mean patients would only have to take CHARM once, as opposed to protein-targeting medications that must be taken regularly as the cells’ protein levels replenish. Research in animals suggests that the prion protein isn’t necessary in a healthy adult, and that in cases of disease, removing the protein improves or even eliminates disease symptoms. In a person who hasn’t yet developed symptoms, removing the protein should prevent disease altogether. In other words, epigenetic editing could be an effective approach for treating genetic diseases such as inherited prion diseases. The challenge is creating a new type of therapy. Fortunately, the team had a good template for CHARM: a research tool called CRISPRoff that Weissman’s group previously developed for silencing genes. CRISPRoff uses building blocks from CRISPR gene editing technology, including the guide protein Cas9 that directs the tool to the target gene. CRISPRoff silences the targeted gene by adding methyl groups, chemical tags that prevent the gene from being transcribed, or read into RNA, and so from being expressed as protein. When the researchers tested CRISPRoff’s ability to silence the prion protein gene, they found that it was effective and stable. Several of its properties, though, prevented CRISPRoff from being a good candidate for a therapy. The researchers’ goal was to create a tool based on CRISPRoff that was just as potent but also safe for use in humans, small enough to deliver to the brain, and designed to minimize the risk of silencing the wrong genes or causing side effects. From research tool to drug candidate Led by Neumann and Bertozzi, the researchers began engineering and applying their new epigenome editor. The first problem that they had to tackle was size, because the editor needs to be small enough to be packaged and delivered to specific cells in the body. Delivering genes into the human brain is challenging; many clinical trials have used adeno-associated viruses (AAVs) as gene-delivery vehicles, but these are small and can only contain a small amount of genetic code. CRISPRoff is way too big; the code for Cas9 alone takes up most of the available space. The Weissman lab researchers decided to replace Cas9 with a much smaller zinc finger protein (ZFP). Like Cas9, ZFPs can serve as guide proteins to direct the tool to a target site in DNA. ZFPs are also common in human cells, meaning they are less likely to trigger an immune response against themselves than the bacterial Cas9. Next, the researchers had to design the part of the tool that would silence the prion protein gene. At first, they used part of a methyltransferase, a molecule that adds methyl groups to DNA, called DNMT3A. However, in the particular configuration needed for the tool, the molecule was toxic to the cell. The researchers focused on a different solution: Instead of delivering outside DNMT3A as part of the therapy, the tool is able to recruit the cell’s own DNMT3A to the prion protein gene. This freed up precious space inside of the AAV vector and prevented toxicity. The researchers also needed to activate DNMT3A. In the cell, DNMT3A is usually inactive until it interacts with certain partner molecules. This default inactivity prevents accidental methylation of genes that need to remain turned on. Neumann came up with an ingenious way around this by combining sections of DNMT3A’s partner molecules and connecting these to ZFPs that bring them to the prion protein gene. When the cell’s DNMT3A comes across this combination of parts, it activates, silencing the gene. “From the perspectives of both toxicity and size, it made sense to recruit the machinery that the cell already has; it was a much simpler, more elegant solution,” Neumann says. “Cells are already using methyltransferases all of the time, and we’re essentially just tricking them into turning off a gene that they would normally leave turned on.” Testing in mice showed that ZFP-guided CHARMs could eliminate more than 80 percent of the prion protein in the brain, while previous research has shown that as little as 21 percent elimination can improve symptoms. Once the researchers knew that they had a potent gene silencer, they turned to the problem of off-target effects. The genetic code for a CHARM that gets delivered to a cell will keep producing copies of the CHARM indefinitely. However, after the prion protein gene is switched off, there is no benefit to this, only more time for side effects to develop, so they tweaked the tool so that after it turns off the prion protein gene, it then turns itself off. Meanwhile, a complementary project from Broad Institute scientist and collaborator Benjamin Deverman’s lab, focused on brain-wide gene delivery and published in Science on May 17, has brought the CHARM technology one step closer to being ready for clinical trials. Although naturally occurring types of AAV have been used for gene therapy in humans before, they do not enter the adult brain efficiently, making it impossible to treat a whole-brain disease like prion disease. Tackling the delivery problem, Deverman’s group has designed an AAV vector that can get into the brain more efficiently by leveraging a pathway that naturally shuttles iron into the brain. Engineered vectors like this one make a therapy like CHARM one step closer to reality. Thanks to these creative solutions, the researchers now have a highly effective epigenetic editor that is small enough to deliver to the brain, and that appears in cell culture and animal testing to have low toxicity and limited off-target effects. “It’s been a privilege to be part of this; it’s pretty rare to go from basic research to therapeutic application in such a short amount of time,” Bertozzi says. “I think the key was forming a collaboration that took advantage of the Weissman lab’s tool-building experience, the Vallabh and Minikel lab’s deep knowledge of the disease, and the Deverman lab’s expertise in gene delivery.” Looking ahead With the major elements of the CHARM technology solved, the team is now fine-tuning their tool to make it more effective, safer, and easier to produce at scale, as will be necessary for clinical trials. They have already made the tool modular, so that its various pieces can be swapped out and future CHARMs won’t have to be programmed from scratch. CHARMs are also currently being tested as therapeutics in mice. The path from basic research to clinical trials is a long and winding one, and the researchers know that CHARMs still have a way to go before they might become a viable medical option for people with prion diseases, including Vallabh, or other diseases with similar genetic components. However, with a strong therapy design and promising laboratory results in hand, the researchers have good reason to be hopeful. They continue to work at full throttle, intent on developing their technology so that it can save patients’ lives not someday, but as soon as possible. Share this news article on: X Facebook LinkedIn Reddit Print Paper Paper: \"Brainwide silencing of prion protein by AAV-mediated delivery of an engineered compact epigenetic editor\" Check for open access version(s) of the research mentioned in this article. Press Mentions USA Today Sonia Vallabh and Eric Minikel, senior group leaders from the Broad Institute have created a gene-editing tool to combat prion diseases, reports Karen Weintraub for USA Today. The approach “should also work against diseases such as Huntington's, Parkinson's, ALS and even Alzheimer's, which result from the accumulation of toxic proteins,” Weintraub writes. Full story via USA Today → Related Links Sonia Vallabh Jonathan Weissman Whitehead Institute for Biomedical Research The Broad Institute of MIT and Harvard Department of Biology School of Science Related Topics School of Science Biology Broad Institute Whitehead Institute Howard Hughes Medical Institute (HHMI) Health sciences and technology Genetics CRISPR Research Disease Drug development Proteins Related Articles An on-off switch for gene editing Epigenomic map reveals circuitry of 30,000 human disease regions Eleven from MIT named to the 2016 TR35",
    "commentLink": "https://news.ycombinator.com/item?id=40939703",
    "commentBody": "Gene-silencing tool shows promise as a future therapy against prion diseases (news.mit.edu)204 points by gmays 23 hours agohidepastfavorite63 comments rockinghigh 4 hours agoThe actual paper: Brainwide silencing of prion protein by AAV-mediated delivery of an engineered compact epigenetic editor https://www.science.org/doi/10.1126/science.ado7082 reply atleastoptimal 17 hours agoprevThis will be necessary as prions have been proven to persist in the environment (plants that grow over the soil where deer with CWD decompose can transmit the disease to animals that eat them). Prions are both 100% fatal and virtually indestructible. It's like some sort of glitch universal kill-switch biology happened upon encoded by hell itself. reply TeMPOraL 10 hours agoparent> It's like some sort of glitch universal kill-switch biology happened upon encoded by hell itself. Proving once again, that true Eldritch horrors are not out there, but right here; not in the cosmos, but in microcosm. Nature is the ancient mad god, and it's not the telescope but the microscope that will take away your sanity. reply yareally 5 hours agorootparentConsidering some prions slowly take away your ability to sleep, it really is the stuff of nightmares reply tetris11 1 hour agorootparent(nods soberly, whilst sipping 10th coffee of the day) reply complaintdept 15 hours agoparentprev> plants that grow over the soil where deer with CWD decompose can transmit the disease to animals that eat them I've heard this before but the last I tried looking into it I could only find a single study with these findings. Do you know if this was ever replicated? If so, I'd be interested in reading it -- I have to wonder how careful the researchers were to isolate the raw plant material from the contaminated soil, which may have non-plant matter transmitting the prion. Plants transmitting a prion is just hard for me to believe. reply A_D_E_P_T 8 hours agorootparentThat single study came out like six months ago: https://www.sciencedirect.com/science/article/pii/S258900422... It'll probably be a year or two until an attempt at replication is published. Anyway, the details are all there. It's an open-access paper. (I'm enjoying this new trend where almost everything worth reading is published in open-access journals.) reply flobosg 4 hours agorootparentHere’s a study from 2015: https://www.sciencedirect.com/science/article/pii/S221112471... reply complaintdept 3 hours agorootparentprevThanks, sibling to this comment posted the one I was thinking of. The study you posted appears to confirm their findings. This is...alarming. reply roenxi 6 hours agoparentprev> ... Prions are both 100% fatal... I'd bet they aren't. Fatal once visible symptoms are present, probably. COVID had a CFR that was beyond horrible while the only cases that they were tracking were the ones that had made it to hospital. 100% CFR is certainly possible, but unless we have cheap ways of measuring prevalence it seems improbable. There could be common prion diseases that have managed to fly under the radar so far. reply flobosg 4 hours agorootparenthttps://en.wikipedia.org/wiki/List_of_human_disease_case_fat... reply ch4s3 5 hours agorootparentprevI think the underlying biology here is pretty well known. They bump around in your body causing proteins to misfold which cascades as badly folded proteins bump into other proteins and cause misfolding. This eventually kills you, because there's no immune mechanism to purge misfolded proteins or to correct them. reply Enginerrrd 4 hours agorootparentYes, but there's quite a bit of variation in how susceptible an individual's PrP is (genetic variation) to a particular strain of prion. That matters a LOT in terms of likelihood of getting infected, and duration of disease progression. reply Thorrez 11 hours agoparentprevThis new treatment disables a gene that creates prions. It doesn't help you against prions from the environment. reply amluto 10 hours agorootparentIf the treatment works, it should work against acquired prions. Acquired prions are dangerous because even a minute amount that makes it to susceptible tissue (the brain) can convert endogenous prion protein to the dangerous, misfolded form. If the treatment successfully turns off production of endogenous prion protein, then there won’t be any there to convert, and the disease (theoretically) won’t progress, and there’s some reason to believe that it might even reverse itself to at least some extent. reply Enginerrrd 3 hours agorootparent>If the treatment successfully turns off production of endogenous prion protein, then there won’t be any there to convert, and the disease (theoretically) won’t progress Yeah that's true, but given how ubiquitous PRNP is in mammals, this could also produce some form of memory, cognitive, or other deficit. reply j_bum 16 hours agoparentprevPrions: the final selfish g̶e̶n̶e̶ protein reply beambot 13 hours agoparentprevPrions are the ice-nine of biology... reply jcul 6 hours agorootparentGreat analogy, and one of my favorite books, of which I'm due a re-read, thanks for the reminder. reply teeray 5 hours agorootparentprevBiological strangelets reply vaylian 13 hours agorootparentprevSounds like you are making a reference. Can you link to an explanation? reply bell-cot 13 hours agorootparenthttps://en.wikipedia.org/wiki/Ice-nine reply dudeinjapan 12 hours agorootparentprevhttps://youtu.be/moSFlvxnbgk?si=oySyULUikorE8Gj_ reply el_benhameen 21 hours agoprevThe Wired deep dive on the couple whose lab this came from is worth a read: https://www.wired.com/story/sleep-no-more-crusade-genetic-ki... reply gumby 18 hours agoparentThank you for that. I'm normally uninterested in the \"people behind the story\" articles but this one was very good and I'm glad to have read it. reply dyauspitr 15 hours agoparentprevThere’s a decent TED talk as well by the woman involved (Vallabh) reply gushogg-blake 21 hours agoprevDoes this affect infectious prion diseases as well, or just the genetic ones? Article makes it sound like everyone has the prion protein gene, so not sure what the different etiologies are. My understanding was that prions were just self-replicating proteins with no biological mechanisms involved, but apparently it's more complex than that. reply Terr_ 21 hours agoparentIt sounds as if this is mostly a case where the initial infection is a matter of time, and the goal is to stop it from occurring: > CHARMs, however, work further upstream, turning off the gene that codes for the faulty protein so that the protein never gets made in the first place. [...] In a person who hasn’t yet developed symptoms, removing the protein should prevent disease altogether. That said, it may still be relevant as a helpful therapy for someone already infected, if the accidental production of \"fresh\" prions--happening in parallel in all cells--is a much bigger problem than an existing prion floating around and catalyzing neighbors: > Testing in mice showed that ZFP-guided CHARMs could eliminate more than 80 percent of the prion protein in the brain, while previous research has shown that as little as 21 percent elimination can improve symptoms. reply throwawaymaths 20 hours agoparentprevPossibly. A corrections: a prion (the disease agent) is believed to be a self replicating protein fold, so it needs a source protein (\"prion protein\", aka PrP, one word) to feed the folding. Now, although the evidence is pretty solid that the fold is the infectious agent, there's a lot of unanswered questions: we can try to make prions in the lab but iiuc no one has made a fiber of PrP in the lab without seeding from an infectious sample that itself was infectious. Moreover, a lot of related Alzheimer's research has come under intense scrutiny recently[0], so much so that the viral hypothesis of Alzheimer's is now dominant and no longer the \"prion-like hypothesis\", so, it's possible that the self-propagating protein fold is not the infectious agent for prion disease either. If it is, then it's likely that turnjng down PrP in the brain will slow or even reverse the disease. The PrP fibers probably can be degraded, if very slowly, and turning off the spigot of source material for the fold will push the equilibrium in the other direction [0] see marc tessier levigne (current CEO of AI/biotech form Xaira)'s firing?resigning? from stanford due to fraudulent data in his lab reply thatoneguy 21 hours agoparentprevBoth the communicable and noncommunicable prion diseases still require a particular protein (and typically protein isoform since some folks are resistant to vCJD) to be present so I think this would work on both forms. That said, someone doesn’t normally now they have a prion disease until it’s probably too late to do anything about it. reply LinuxBender 2 hours agoprevPlease forgive my naive question, this is not my area of expertise. Will this be something I can inject my deer with to cure or prevent CWD, or does it require injecting something into the brain or spine? Or for that matter, how will it be administered to humans? reply tim333 25 minutes agoparentIt's a bit hypothetical with the deer. In the article it looks like they injected the thing into the brains of mice. reply LinuxBender 6 minutes agorootparentinjected the thing into the brains of mice That's a bit beyond my skill-set. I would probably end up with some paralyzed animals. reply nikolay 19 hours agoprevA family became scientists to find a cure and their solution is pretty simple: https://www.youtube.com/watch?v=04PmEJaYKd0 reply dekhn 19 hours agoparentThere are no simple solutions to complex biology problems that work in a medical context. reply tim333 14 minutes agorootparentScurvy? The history on that one is quite interesting https://en.wikipedia.org/wiki/Scurvy#History reply piker 19 hours agorootparentprev…penicillin? reply dekhn 19 hours agorootparentusing a byproduct of mold to cure a bacterial infection isn't simple. Merely noticing it was a rare occurrence (Fleming got lucky with his dirty lab), it only works on gram positive bacteria, and importantly, use of penicillin leads to evolution of antibiotic resistance, reducing the utility of penicillin. That last point especially is a highly non-simple process. Like just about anything in medical biology, there is a simple narrative taught to the general public, and then there's the reality as it stands in modern medical research and practice. For prions, I'm sure people can find any number of ways to module prion activity in vitro, most of which won't work in vivo/in the real world, for messy complex reasons. To use machine learning parlance, a feature like the existence of a TED talk by the creators of a medical treatment being True highly correlates with the ineffectiveness of the treatment (same as \"If published in Nature, it's almost certainly wrong!\") reply nikolay 18 hours agorootparentprevAspirin? reply dekhn 18 hours agorootparentAspirin causes stomach bleeding due to its anti-clotting effect due to nonselective binding and inhibition to COX-1 and COX-2; see the list of https://en.wikipedia.org/wiki/Aspirin#Adverse_effects to see all the complications that arise from this relatively innocuous drug. 43 deaths in the US in a single year (mostly people taking the drug as a poison). See details in https://en.wikipedia.org/wiki/Salicylate_poisoning showing the all the kinds of human biology aspirin interacts with that don't directly contribute to pain suppression. The main reason for fewer people dying from aspirin was a shift to other drugs. reply nikolay 13 hours agorootparentA lot more people die from Tylenol, and millions accumulate damage to their livers, but it is still OTC, and people believe it's less dangerous than Aspirin. reply nucleardog 4 hours agorootparentExcept that for women that are pregnant or nursing and babies under 6 months old, Tylenol _is_ less dangerous than Aspirin. (It's the only OTC painkiller or antipyretic available that's considered safe.) Which kinda leads us back to the greater point here... There are no simple answers to complex biological problems. reply nikolay 3 hours agorootparentProbably, but the other damage from Tylenol is that it's been misused to lower fevers that don't need to be lowered and thus disarming the immune system. reply dyauspitr 15 hours agorootparentprevMillions of people use aspirin everyday to relieve aches and pains. 43 deaths is nothing compared to the benefits, especially since those people are intentionally overdosing with the intention of killing themselves. reply vaylian 13 hours agorootparentI don't think that this is GP's point. The point is rather that biology is complex and solutions to biological problems are always complex even if they are accompanied by a simple narrative. reply FeepingCreature 12 hours agorootparentSimple solutions can still have complex effects. There's no contradiction here. reply thinkfaster 14 hours agorootparentprevSome of us are explicitly forbidden to take aspirin by their Dr. so the population of asprin takers is biased towards a cohort for whom asprin is safe. reply nikolay 15 hours agorootparentprevThat's a huge misconception and poor science. In fact, enteric-coated aspirin is worse as it could cause bleeding in the intestines, which is worse. There are plenty of studies that show taking aspirin with vitamin C or DGL does not cause bleeding! I think taking it alongside collagen/gelatin also prevents bleeding. There was a sublingual form, which I cannot find (Fasprin). Also, there's Vazalore, which might be liposomal. I don't know, but the effervescent Aspirin C by Bayer is amazing! It works very well for gargles for sour throat, too! reply flobosg 10 hours agoprevRelated discussion, from last week: https://news.ycombinator.com/item?id=40859876 reply oigursh 20 hours agoprevIt's believed the UK will suffer from a batch of hidden vCJD prion disease in the next decade or so. Well needed therapy. reply masfuerte 19 hours agoparentThis is very detailed: https://www.cjd.ed.ac.uk/sites/default/files/report31.pdf The vCJD death rate is increasing but still very low. Most of the increase is in the over seventies, which supports the theory that this is due to improved testing rather than increased illness. I'd be more concerned about the states in the USA where it is still illegal to test for BSE. reply mikeodds 18 hours agorootparentWhat’s the reasoning for making it illegal to test for BSE? reply tptacek 17 hours agorootparentFrom the USDA site: Q. Why can't we test all beef for BSE safety? A. BSE tests are not conducted on cuts of meat, but involve taking samples from the brain of a dead animal to see if the infectious agent is present. We know that the earliest point at which current tests can accurately detect BSE is 2-to-3 months before the animal begins to show symptoms. The time between initial infection and the appearance of symptoms is about 5 years. Since most cattle that go to slaughter in the United States are both young and clinically normal, testing all slaughter cattle for BSE might offer misleading assurances of safety to the public. reply masfuerte 18 hours agorootparentprevA positive test would damage the cattle industry. reply logicprog 18 hours agorootparentprevWhich states are those? reply wlll 19 hours agoparentprevCite please. reply unsupp0rted 12 hours agoprevIf this works, I wonder if we’ll discover that a bunch of diseases and conditions we thought were genetic, bacterial or viral are actually just prions causing us to go nuts. reply im3w1l 11 hours agoprevAmazing work. But I have some questions. The tool lowers the amount of the protein by 80%. That seems a bit low? Like I'm sure it's way better than nothing, but is it really enough? Also they say that the gene is turned off permanently, but is that really so? I thought epigenetic changes gradually revert themselves - though I suppose that just means you have to reapply the treatment every n years. reply vikramkr 1 hour agoparentThose are great questions. The answer is going to vary (quite dramatically) by the target disease, target cell type, intended effect, etc. For example they mention that 21 percent elimination can 'reduce symptoms', so you could be looking at a situation where 80% elimination slows down the progression enough that the low-symptom phase of the disease lasts long enough that you can die of some other age related complicated instead. Die with the disease instead of die because of the disease - you see that as a common goal in oncology now, and in treating diseases like HIV. Rate of reversion might depend on the cell type. Add in the aspect that it looks like they want to work with a new AAV vector optimized for the brain (looks like the compactness of this epigenetic editor is one of the advantages which tracks, can fit in an AAV then). So if there are muliple applications required there's also questions of, what if in future applications there's an immune response against the AAV used that reduces uptake, etc etc. These questions (along with all the others) tend to be super complicated to answer, which is why all this stuff (more mouse trials, clinical trials, manufacturing, etc) can be so long/complicated/expensive. Clinical trials are also lit in that you boil all the unknown knowns and unknowns into 'let's just check and see if it's safe and if it works.' reply willmadden 4 hours agoprev [–] Prions aren't highly communicable. You could also be struck by lightning or be hit by a car. This isn't a big priority. reply VyseofArcadia 2 hours agoparentIt is for the researchers. They're a husband-wife team who got into the field because she carries the gene for fatal familial insomnia. reply tim333 1 hour agoparentprevStill the tech may help with prion like diseases which are common: >There is also evidence suggesting prions may play a part in the process of Alzheimer's disease, Parkinson's disease, and amyotrophic lateral sclerosis (ALS); these have been termed prion-like diseases. (wikipedia) reply vikramkr 1 hour agoparentprev [–] You're saying you'd rather society not invest in fundamental research into new mechanisms of curing disease with applications to prions (medically and economically devastating), genetic disease, and potentially neurodegeneration? Also even if you were correct about this area of research not being a big priority, failing to invest in small priorities is a pretty solid way to lose your technological edge and competitiveness as a nation so let's not kneecap ourselves for no reason. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "CHARM (Coupled Histone tail for Autoinhibition Release of Methyltransferase) is a new gene-silencing tool developed by the Broad Institute and Whitehead Institute, showing potential for treating prion diseases and other neurodegenerative conditions.",
      "This tool uses epigenetic editing to silence disease-causing genes, including the prion protein gene, without altering the underlying DNA, potentially offering a one-time treatment.",
      "The research team, led by Sonia Vallabh and Jonathan Weissman, has made significant progress, creating a compact, effective, and low-toxicity gene silencer, and is now refining CHARM for clinical trials."
    ],
    "commentSummary": [
      "A gene-silencing tool, delivered via Adeno-Associated Virus (AAV), shows potential as a therapy for prion diseases, which are currently 100% fatal and resistant to destruction.",
      "This tool works by silencing the prion protein gene, potentially preventing or reversing the disease, and could be effective against both genetic and acquired prions.",
      "While promising, the tool's long-term effects on memory and cognition are still under investigation, with ongoing research exploring applications for other prion-like diseases such as Alzheimer's and Parkinson's."
    ],
    "points": 204,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1720725668
  },
  {
    "id": 40946033,
    "title": "Tau: Open-source PaaS – A self-hosted Vercel / Netlify / Cloudflare alternative",
    "originLink": "https://github.com/taubyte/tau",
    "originBody": "Open Source Git-Native CDN PaaS An alternative to: Vercel, Netlify, Cloudflare, Amazon Lambda with CloudFront, S3, ElastiCache & SQS, Etc... Explore the docs » Quick Start Guide · Try our Sandbox Cloud · Join Our Discord Tau is a framework for building low maintenance & highly scalable cloud computing platforms that software developers will love! tau is a single binary with no external dependencies except standard system libraries. On top of that, it requires minimal configuration. These are the main steps: Install Tau curl https://get.tau.link/taush Configure tau config generate -n yourdomain.com -s compute --services all --ip your_public_ip Launch tau start -s compute For a complete step-by-step guide, refer to Deploy tau. Building tau youself is a straightforward go build given you have Go installed. Background The cost and time required to build software, take it from the development environment to production, and then scale it effectively to meet end-user demand are extremely high. Developer-friendly platforms, like the major cloud computing providers, are expensive, lock users in, and overlook local development and E2E testing. This is really a two-sided problem. Do you save on infrastructure cost, or do you lower development time? If you invest in your own platform, it's a rocky road that impedes the speed of development and generally ends up costing more. We all know the Kubernetes fairy tale does not end well! If you invest in development speed, you're limited by your provider's features and cost. To us, solving this problem means: Giving you, or your very small team, the ability to build and maintain a cloud computing platform that will go head-to-head with the ones backed by thousands of engineers. Setting software developers free from infrastructure and operational constraints. We refer to this as \"Local Coding Equals Global Production.\" tau solves for building and maintaining a cloud computing platform, and also provides the foundations for an amazing developer experience. Minimal Configuration One of the reasons tau requires minimal configuration is because it has built-in auto-discovery. Just like a self-driving car gathering information through sensors, tau will gather information and try to find the best ways to be reachable, available, etc. That said, some configuration like bootstrap peers is necessary. Unless you're running a single-node cloud, each node will need to know at least one other peer. A Cloud built with tau is very dynamic; at a low level, nodes communicate assets, routes, and services, and they also exchange information about other peers. Enriched by distributed services like seer and gateway, the cloud can load-balance incoming requests to ensure optimal performance and reliability. This behavior is built into cloud resources as well. For example, a protocol we call hoarder ensures object storages and databases are replicated; all you need to do is enable it on a few nodes. Local Coding Equals Global Production In your traditional setup, the platform is a complex set of templates, pipelines, and integrations that ultimately help turn configuration into API calls and code into assets. Because of that complexity, and also the fact that many components need to run inside a very complex environment of their own, it's impossible to satisfy the 'local == production' equation. Granted, there are some solutions that either mock or reroute to dev/prod resources, enabling developers to build or debug locally. However, it's still a 3rd party service you need to integrate and manage. In order to satisfy the equation, we decided to build tau so it simplifies, ports, and/or sandboxes every aspect of the cloud. Git-Native Traditionally, you interface with infrastructure through API calls. This is the case for every cloud computing provider alongside orchestration solutions like Kubernetes. A few years back, the concept of GitOps started to make waves, and that was around the time we started building, so we decided to cut the unnecessary garbage between the definition of a cloud resource, which should be stored in Git, and its instantiation. As a result, tau has no API calls to create a serverless function, for example. Instead, it adopts Git as the only way to alter infrastructure. Also, git being core to tau means that nodes in the cloud do tune to a specific branch, by default main or master. Among what it enables is an easy way to set up development environments, for example. A specific use case is local development in which case dream-cli nodes can also be tuned to the current branch. In addition to the nodes being on a branch, the application registry, managed by the 'tns' protocol, uses commit ids to version entries, allowing nodes serving the assets to detect new versions, or a roll-back for that matter. Networking Internally, tau, using libp2p, builds an overlay peer-to-peer network between the nodes, enabling some pretty cool features like: Automatic node and protocol discovery & routing. If, for example, a node is down, changes its IP address/port, or the services it supports, other nodes will update the info automatically. Transport independent. Nodes can use any combination of TCP/IP, WebSocket, QUIC, and more. NAT Traversal & Circuit Relay, which allow nodes that are not public to be part of the cloud. Unless absolutely required, which is extremely rare, no well-designed software should rely on IP addresses and ports. This is why every tau cloud is identified with an FQDN (i.e., enterprise.starships.ws) so no absolute network reference is used in an application. Under the hood, the Cloud will transparently take care of DNS resolution and HTTP load balancing, eliminating the need to set these up. Storage In every other cloud computing implementation, storage means a location and generally a path. For example, https://tau.how/assets/logo-w.svg has two main components tau.how, which translates to an IP address and a location, and /assets/logo-w.svg, which is a path relative to the location. This way of addressing, called \"location-based addressing,\" is simply not portable. Why? you might ask. Well, for starters, nothing guarantees the data returned is an SVG logo in this case. The other issue is the tau.how host we connected to might not have it. To solve this issue, tau uses content-addressing, a concept introduced by torrent networks and popularized by IPFS. So when you request https://tau.how/assets/logo-w.svg, which is actually hosted by a tau Cloud, the host that handles the request will resolve (host=tau.how, path=/assets/logo-w.svg) to a content address, or CID, then retrieve the content reader and then forward it through an HTTP writer to you. A few cool facts about this approach: Content is chunked and then stored in a DAG, which means it's deduplicated. Content can be downloaded from multiple peers in parallel. Content can be verified as the CID is its hash. When content is in demand, the cloud automatically dedicates more peers to its distribution. Computing As of today, tau supports WebAssembly for computing. The reason we started with it is that it's highly portable and sandboxed. We support containers for CI/CD but not for computing yet. We're working on a way to implement containers and virtual machines while abiding by our principles of portability and sandboxing. Code, binary, images, along with any attached assets, are stored and retrieved using the same principles described in Storage, which considerably reduces provisioning time and brings computing close to data (data gravity) and/or user (edge computing). E2E Testing If you're looking to create E2E tests for projects hosted on tau, you can use dream, a sub-package within tau. We don't have documentation for it yet, but you can quickly learn from tests like services/seer/tests/dns_test.go. Running a Local Cloud While you can't practically run tau on your local machine, you can do so using dream-cli, which is a CLI wrapper around dream. It creates local cloud environments mirroring production settings. Unlike tau, it offers an API for real-time configuration and testing. Extending Tau tau can be extended using a plugin system we call orbit. An open-source example is ollama-cloud, which demonstrates how to add LLM capabilities to your cloud. Documentation To learn more, check: Introduction to Taubyte Be Competitive in a Few Minutes: Deployment Guide For comprehensive documentation, visit our documentation. Support Questions or need assistance? Ping us on Discord!",
    "commentLink": "https://news.ycombinator.com/item?id=40946033",
    "commentBody": "Tau: Open-source PaaS – A self-hosted Vercel / Netlify / Cloudflare alternative (github.com/taubyte)178 points by thunderbong 4 hours agohidepastfavorite58 comments sqeaky 1 hour agoSelf-hosted platform as a service?! Isn't the whole point of platforms as a service (from the customer perspective) that you don't need to do the hassle of self hosting. There are pros and cons to using an external service and to self hosting. And just throwing all these words at me together makes me feel like there isn't a coherent mental model of what this is trying to be, or if there is it isn't clearly communicated. If this is some sort of CDN software or attempt at running Lambda-like code Snippets on your own distributed cluster that's cool. But a description of that would be nice. The GitHub read me jump straight into how this is just a single binary and how deploying it is easy, but not what the hell it is. CloudFlare can do like a million things, which features from cloudflare is this competing with? I just really want to know what the pros and cons of this are compared to other ways of rolling my own servers or renting out someone else's platform? reply Kinrany 1 hour agoparent\"Self-hostable as a service\" is a common pattern because it provides the benefits of as-a-service without the threat of vendor lock-in. Plus you can run the same software in temporary or test environments. reply makestuff 10 minutes agorootparentIt will be interesting to see how these companies evolve their business strategy once PE/VCs are pressuring them to IPO/get bought out. It seems like any customer that is large enough to have significant billing would just bring the platform in house instead of paying for the hosted version. I guess they could take the docker desktop approach with their licensing that >X million in revenue still requires a license of some sort. reply st3fan 1 hour agorootparentprevWhere Vendor can also be an open source project. The cost of moving away from a project like Tau can be equally high as a closed source PaaS of course. reply rrix2 47 minutes agorootparentand if that cost of moving away is high enough, a team or org \"locked in\" to a FOSS solution can continue to pay humans to support it internally while evaluating off-ramps instead of being told they need to re-arch their cloud stack in three months' time. reply hosh 1 hour agoparentprevFurther down on the README, it explains how it uses libp2p for network autodiscovery, IPFS for distributed storage, and how it can distribute and share routes and assets and automating load-balancing. It is Webassembly-native, so you don't have to mess with compiling dependencies or execution environments. If it works as well as described, then the underlying technology (and the constraints they have) allows it to be self-hosted while having some of the benefits for a managed platform. reply sqeaky 53 minutes agorootparentThanks for digging in. Would be nice if READMEs opened up with what the thing was and maybe a one or two sentence problem and solution description. reply revskill 8 minutes agoparentprevYes, so that one doesn't need to learn those helly AWS config. reply jauntywundrkind 59 minutes agoparentprevThe point of platform isn't just that someone else hosts it. It's that it's a consistent target for your teams & different projects, with a well defined set of capabilities. Having patterns to deploy & ship software, that also can bring up & manage other resources along the way (databases, load balancers, geo-reicatikn, etc etc). reply seangrogg 39 minutes agorootparentWhile I think this is a valid take for the term \"platform\" I do think that \"Platform as a Service\" implies that someone else is running the platform and I don't have to deal with the headache of managing it, I just use it. reply mlaretallack 11 minutes agorootparentIn a company, there may me multiple teams, each doing there own projects, PaaS can be within the same company and provide a common way to do stuff without each team having to start from scratch each time. reply phendrenad2 19 minutes agorootparentprevYes, from the developer's perspective, someone else is running the platform (the platform team). (At least, as long as the developers can avoid assuming ownership of the platform by managing terraform files or something, which in practice I've yet to see anyone avoid...) reply joshmanders 1 hour agoprevWhat's with the vilification of kubernetes? 99% of this document (https://tau.how/99-Misc/kubernetes/01-k8s-cons/) boils down to \"You have to understand what a pod, deployment, container, etc is\" once you remove every line that discusses the cons of managing a cluster yourself, because nobody actually does that except extremely large orgs. All of these problems go away when you utilize a managed offering like DOKS, EKS, AKS or GKE. reply llama052 1 hour agoparent100% this.. There's also exciting projects like Talos, Rancher, and the like for self-hosting Kubernetes that makes it entirely more manageable. So much saturation in this space of people trying to create one off solutions, which on some level I admire. However the further off the main path you go the more you lock yourself into problems you can't troubleshoot or edge cases that aren't supported. Abstraction these days is alluring, and it's cool! However you want something well known, well supported, (from multiple companies ideally) and documented. The hate for understanding kubernetes is just hate for having to understand layers of orchestration, or worse the layers behind the application. If it's too complicated then you might not need it. Any platform you use will have those same layers, it just depends on how much is assumed or exposed to you. If you don't want to see any dials or options then use a managed solution, not a roll your own platform tool. That's of course assuming a few virtual machines managed by hand doesn't satisfy your needs, but if that's the case you don't need a platform solution (and hopefully it's not production). reply p-o 52 minutes agoparentprevI like your take on this. I think K8s offers a _lot_ and it has a bad reputation because of its early days. Kubernetes has room to improve, like everything else, but the API now are becoming a lot easier to work with and the Custom Resources allows folks to extend Kubernetes. I still think that projects like this one come from necessity. Folks want to have an alternative for vendor lock-in. I'm building something like that too (https://github.com/pier-oliviert/sequencer) for Kubernetes, and it's also out of necessity. Vercel, Heroku and others have a lot of helpful tools that are empower developers, and I think people want to have those without being locked-in. It goes without saying that I'm totally bias :) reply thinkxl 1 hour agoparentprev> What's with the vilification of kubernetes? because a well-configured k8s cluster nullifies the need for this project. also hi! reply tln 27 minutes agorootparentThere's a fair amount of friction to going from 0 to a well-configured k8s cluster with gitops and a local dev story.... reply jpgvm 3 minutes agorootparentGitops and a local dev story are more about your application than your deployment environment. Especially because along the way you need to consider building, testing, CI, etc. It's really hard to fault k8s these days, all the original problems are solved and all that remains is necessary complexity that can't be abstracted without lowering power. That doesn't mean you can't abstract over it, you can and should but you should do so in the scope of your team or organisation where you already know which pieces of power you need/want or otherwise know the way you which you want to leak those capabilities. reply philippemnoel 12 minutes agoparentprev\"What's with the vilification of kubernetes?\" I ask myself that question every day reply aduwah 47 minutes agoparentprevI disagree hard with this. I love kubernetes to bits, but managing complex networking issues, having proper security, or just simply rightsizing the nodes is definitely not going away with having a managed control plane. reply mynameisvlad 1 hour agoparentprevIf your goal is to self-host your own PaaS, why would you use a managed k8s offering? reply abound 1 hour agorootparent\"Self-hosting\" doesn't necessarily mean you own the hardware. Many people self-host on cloud providers at different levels (Fly, Digital Ocean/Hetzner, GCP/Azure/AWS), and most of those have some managed K8s offering. reply joshmanders 1 hour agorootparentprevWhy wouldn't you? If you decide to not use a PaaS and self host your own servers would you reach for a VPS or would you manage your own rack in a colocated data center? reply jpgvm 1 minute agorootparentPrecisely, it's just about which abstraction and responsibility level you want to engage with. Managed k8s (from good vendors) means the scheduler is as far as you need to go which is enough to do a great many \"self hosted\" things. reply zerkten 1 hour agoparentprevI found it interesting that this was the focus when users of Vercel etc. have probably decided against k8s already. For me, a k8s comparison would make sense if this was a platform for running containers/VMs in a more traditional server model. reply turtlebits 1 hour agoprevLooks compelling, but the docs are extremely vague and full of fluff. The \"Why One Binary\" is hilariously bad. Almost feels like content to impress managers/recruiters. https://tau.how/02-concepts/03-one-binary/#the-genesis-of-ta... reply Ambroos 1 hour agoparentPretty much all of the docs are very clearly the output of what the typical LLM produces. It's just words, no meaning. reply mplewis 1 minute agorootparentNothing like “docs by LLM” to warn you about the quality of the code you’re about to trust your entire infrastructure to. reply teruakohatu 1 hour agoparentprevI thought you were probably exaggerating… you were not. reply hosh 2 hours agoprevThere are some neat ideas here -- building a PAAS off of p2p technologies to enable network autodiscovery, automated load-balancing, distributed storage, Webassembly-native, etc. Having put stuff through production though, I'm a bit skeptical about how well it works out in the wild, though I am interested in learning how well it does and what its failure modes are. If it works well-enough, it has the potential for democratizing production apps. I'm not sure how they are going to make money with their enterprise offering though. reply bitpush 2 hours agoprevFirst coolify[1] and now Tau. More competition the better for users. From the quick look it seems like coolify is more fully featured? [1]: https://coolify.io/ reply ffsm8 1 hour agoparentBefore Coolify came CapRover and dokku. My toy server (64ram, ryzen 5600G 60tb HDD, 4tb NVMe) is currently running fedora/caprover, though I've been considering just putting truenas scale on it, as it added custom deployments as well... reply satvikpendem 1 hour agorootparentI found CapRover and dokku to have some limitations such as lack of a GUI or long setup process. In contrast with Coolify, it Just Worked™. I'll have to check out Tau as well. reply bibstha 57 minutes agorootparentprevLove Caprover. Using this on production to host many docker containers. reply udev4096 1 hour agorootparentprevThat's a beast server. What are you using it for? reply ffsm8 1 hour agorootparentIt sounds bigger then it is, raid 10 cuts it in half after all (30tb HDD, 2tb NVMe) It's mostly just for the *arr stack, various self hosted services like vaultwarden, seafile etc and my personal toy projects. I.e. a pwa book reader along with the occasional dev tool I wanna experiment with. reply TechDebtDevin 2 hours agoparentprevCoolify needs a lot of work imho. reply KomoD 1 hour agoprevDon't call this a Cloudflare alternative because it simply isn't. reply OJFord 11 minutes agoparentCloudflare Pages I assumed, from the title, since it already didn't make sense to compare Cloudflare (overall) to 'Vercel / Netlify' reply yayr 2 hours agoprevvery interesting... here is a comparison of the community and enterprise offering https://taubyte.com/pricing/ who is actually behind this? reply KomoD 1 hour agoparent> who is actually behind this? \"Samy Fodil\" https://github.com/samyfodil reply VyseofArcadia 1 hour agoprevI've been out of web dev for a few years, but my understanding of the appeal of serverless is that it is theoretically pay only for what you use. But if you're hosting Tau to do serverless via Tau, well, it's not really serverless anymore. You are now definitely paying for the server running the serverless infra. Why would anyone target Tau serverless, then? What am I missing? reply cal85 1 hour agoparentThat’s not the only appeal of serverless. In fact it’s not even really true - I pay Vercel a flat rate every month whether I’m heavily using it or not. The appeal of serverless for me is simplicity. It abstracts the server away. Less to think about, more brain capacity focused on unique business logic. reply joshmanders 1 hour agorootparent> The appeal of serverless for me is simplicity. That's interesting, because serverless is far from simple and Vercel is about the same distance from simplicity as the sun is from the edge of the observable universe. reply memset 1 hour agoprevThis is really neat! I'm working a message queue in go (drop-in replacement for SQS) and also thinking about autoscaling. I've been playing with raft vs using a central store (ie, postgres) to coordinate. Can you tell me more about IPFS - I've never used it before. How has that been working, and can you tell me what you've observed when you have many nodes which need to coordinate? reply pdimitar 1 hour agoparentLately I've stumbled upon https://www.goqite.com/ but haven't had a good use case for it yet. reply sneak 1 hour agoparentprevIPFS is slow and impractical. The design of the content addresssble system is cool, but having tried approximately annually to run it for production usage since it was released, I can say that it is still firmly in the “research project” category, a decade later. reply taraparo 1 hour agoprevWouldn't using talos be better than having to run this on custom managed ubuntu servers? reply pdimitar 1 hour agoparentWhich Talos, can you send a link? reply llama052 1 hour agorootparentI assume https://www.talos.dev/ Basically a small OS that will prop itself up and allow you to create/adopt into a Kubernetes cluster. Seems to work well from my experience and pretty easy to get set up on. reply taraparo 1 hour agorootparentprevhttps://www.talos.dev/ reply sandGorgon 50 minutes agoprevhow is this achieving scale-up and scale-to-zero ? from my (rudimentary) investigation only knative k8s had scale-to-zero implemented well enough. and if you dont have scale-to-zero, you cant claim a vercel alternative. reply TheAnkurTyagi 48 minutes agoprevWhat key features make Tau a compelling self-hosted PaaS alternative? reply Ambroos 1 hour agoprevI've been reading through the docs and skipping through the one recentish YouTube tutorial trying to make sense of what this actually is. While it seems like an impressive thing for what appears to be a one-man-project, the almost complete lack of documentation makes it feel like a bit of a hard no in the current state. There seems to be a history of it being heavily linked to Web3 things that also feels weird. Some suggestions for this to be able to succeed: - Documentation, documentation, documentation, the only place where I could that the three supported ways to write a serverless function are with Go, Rust and AssemblyScript is somewhere hidden in a tutorial. It all has to compile to WebAssembly so I guess that's the limiting factor. - Examples? - Using git as source of truth for the configuration/state of a system is cool. Please link to sample repos so I can see what a system with a website, some functions that touch DB and files, and the configuration etc looks like. - How does the database part work? Client SDKs? - There are lots of protocols with unclear names that are only briefly mentioned here but then seen in random places in configuration: https://tau.how/01-getting-started/01-local-cloud/#protocols - The Concepts part of the documentation is buzzword soup, it's impossible to derive any meaning from it other than that the author dislikes Kubernetes and probably used some generative AI for the content. - Roadmap, plans, versioning, plans on how Tau version upgrades should go, ... reply alfonsodev 2 hours agoprevWould this be a good combo to combine with Hetzner server auctions, or just too much trouble ? or even with todays connections having a server at home ? any success stories ? reply winrid 2 hours agoparentThey only auction servers in one region reply localfirst 37 minutes agoprevhow does this compare to coolify and caprover already established and mature PaaS ? this is a welcome addition reply breck 1 hour agoprev [–] I love the verbiage. When I see a folder in the source named \"dream\", then files named \"Universe\" and \"multiverse\", it pulls me in :). I also love the single binary in Go. That's on my todo list for a few things. Well done! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tau is a new framework designed for building low-maintenance, highly scalable cloud computing platforms, serving as an alternative to services like Vercel, Netlify, and AWS.",
      "Key features include minimal configuration, Git-native infrastructure changes, peer-to-peer networking, and support for WebAssembly, with future plans for containers and VMs.",
      "Tau aims to reduce infrastructure costs and development time, providing a developer-friendly platform with tools for local development and end-to-end testing."
    ],
    "commentSummary": [
      "Tau is an open-source, self-hosted Platform as a Service (PaaS) alternative to Vercel, Netlify, and Cloudflare, utilizing libp2p for network autodiscovery and IPFS for distributed storage.",
      "It is WebAssembly-native, offering benefits such as avoiding vendor lock-in, but users have noted that the documentation is vague and the concept is unclear.",
      "Comparisons to Kubernetes and other PaaS solutions like Coolify and CapRover are common, indicating the project's potential but highlighting the need for better documentation and clearer communication of its features and benefits."
    ],
    "points": 178,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1720795282
  },
  {
    "id": 40939707,
    "title": "Karpathy: Let's reproduce GPT-2 (1.6B): one 8XH100 node 24h $672 in llm.c",
    "originLink": "https://github.com/karpathy/llm.c/discussions/677",
    "originBody": "karpathy / llm.c Public Notifications Fork 2.4k Star 21.7k Code Issues 61 Pull requests 90 Discussions Actions Projects Security Insights Let's reproduce GPT-2 (1.6B): one 8XH100 node, 24 hours, $672, in llm.c #677 karpathy started this conversation in General Let's reproduce GPT-2 (1.6B): one 8XH100 node, 24 hours, $672, in llm.c #677 karpathy · 8 comments · 10 replies Return to top edited karpathy Maintainer In this post we are reproducing GPT-2 in llm.c. This is \"the GPT-2\", the full, 1558M parameter version that was introduced in OpenAI's blog post Better Language Models and their Implications in February 14, 2019. llm.c does so directly in C/CUDA (total of ~5,000 lines of code), without the typical training stack that would involve the Python interpreter and a significantly more complex deep learning library like PyTorch/JAX, huggingface/transformers, or etc. In 2019, training GPT-2 was an involved project from an entire team and considered a big model run but, ~5 years later, due to improvements in compute (H100 GPUs), software (CUDA, cuBLAS, cuDNN, FlashAttention) and data (e.g. the FineWeb-Edu dataset), we can reproduce this model on a single 8XH100 node in 24 hours, and for $672, which is quite incredible. There are some caveats and asterisks involved though - llm.c is still not perfectly tuned, sufficiently stabilized (we still see loss spikes and bad activation ranges now and then), and our evals are not comprehensive (e.g. we do not carefully eval multilingual, code, math); A lot of this work is still ongoing. Unicorns. The natural place to start is probably with unicorns. In the GPT-2 blog post, OpenAI prompted GPT-2 with this unicorn prompt. Let's see what our model thinks about English-speaking unicorns in the Andes mountains: Prompt: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. Completion: It may sound like a hoax, or just a dream of the mythical animals, but it appears this may be a real case of the unicorn language. The Andean area has an abundance of minerals and is also very rich in biodiversity. According to the researchers' study, the unicorns' language is called Elveseo. To produce it, the animal forms a consonant from the lips and makes a vowel sound with the teeth, then produces a consonant sound with the mouth and makes an 'ow' sound from its chest. \"The question is, what caused this, and what does this tell us about intelligence and communication?\" commented study co-author Paul Marek. Speaking in the video, the unicorns had the right way to speak, the animals were able to communicate and talk in perfect English. Fun! :) The model is fairly coherent and qualitatively somewhere around the level of GPT-2. You can find 20 samples from both GPT-2 and the llm.c model here, or generate many more using instructions down below. Training. Training a GPT-2 with llm.c is quite simple because it is written in C/CUDA, so there is no need for minconda, Python, PyTorch, etc. You will want an 8XH100 GPU box, I recommend spinning one up from Lambda labs. But llm.c is flexible on its compute - if you have only 1 GPU you can still get your GPT-2, you'll just have to wait 8 days instead of 1. If you have 16 GPUs (e.g. using the new Lambda 1 Click Clusters), you'll be able to train multinode and only have to wait 12 hours. Once you spin up your node, here are the complete instructions to train your GPT-2 (this only takes a ~minute from blank box to start stepping): # install cudnn so we can use FlashAttention and run fast (optional) # https://developer.nvidia.com/cudnn-downloads # for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb sudo apt-get update sudo apt-get -y install libcudnn9-dev-cuda-12 # \"install\" cudnn-frontend to ~/ git clone https://github.com/NVIDIA/cudnn-frontend.git # install MPI (optional, if you intend to use multiple GPUs) # (you might also have to install NVIDIA NCCL if it doesn't come with your setup) sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev # download and enter llm.c repo git clone https://github.com/karpathy/llm.c.git cd llm.c # download the \"starter pack\" (~1GB download) # contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s ./dev/download_starter_pack.sh # download the training dataset (FineWeb-Edu 100B token) .bin data shards # note: this is a total of 1001 data shards. If you only want to test things # out and don't want to do an actual run, feel free to append the number of # training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10) # the full dataset is ~200GB, we can store it here in dev/data directory. cd dev/data ./edu_fineweb.sh # compile (~1 min 1st time for cuDNN mostly, few sec from then on) cd ../../ make train_gpt2cu USE_CUDNN=1 # and train! (wait 24 hours here) mpirun -np 8 ./train_gpt2cu \\-i \"dev/data/edu_fineweb100B/edu_fineweb_train_*.bin\" \\-j \"dev/data/edu_fineweb100B/edu_fineweb_val_*.bin\" \\-o \"log_gpt2_1558M\" \\-v 250 -s 300000 -g 384 \\-h 1 \\-b 16 -t 1024 \\-d 1048576 \\-r 0 \\-z 1 \\-c 0.1 \\-k \"cosine\" \\-l 0.0006 \\-q 0.1 \\-u 700 \\-n 2000 \\-x 32000 \\-ge 1 \\-y 1 \\-e \"d48\" I will describe the args in a second. You'll see a bunch of prints scroll through and then the optimization will begin: num_parameters: 1557686400 => bytes: 3115372800 allocated 2971 MiB for model parameters batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576 => setting grad_accum_steps=8 created directory: log_gpt2_1558M allocating 40409 MiB for activations val loss 11.129390 allocating 2971 MiB for parameter gradients allocating 742 MiB for AdamW optimizer state m allocating 742 MiB for AdamW optimizer state v allocating 742 MiB for master copy of params step 1/32000loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-073056.36 ms42.6% bf16 MFU343080 tok/s step 2/32000loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-062747.19 ms47.4% bf16 MFU381690 tok/s step 3/32000loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-062753.25 ms47.3% bf16 MFU381259 tok/s step 4/32000loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-062741.47 ms47.5% bf16 MFU381690 tok/s step 5/32000loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-062752.07 ms47.3% bf16 MFU381507 tok/s step 6/32000loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-062751.01 ms47.3% bf16 MFU381431 tok/s step 7/32000loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-062753.03 ms47.3% bf16 MFU381327 tok/s step 8/32000loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-062748.91 ms47.3% bf16 MFU381348 tok/s step 9/32000loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-062748.73 ms47.3% bf16 MFU381367 tok/s step 10/32000loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-062749.40 ms47.3% bf16 MFU381369 tok/s ... We can see that each step is about 2.75 seconds and there are 32,000 of them, so now we wait ~24 hours. At every step, this training run takes a chunk of ~1 million tokens of FineWeb-EDU (these are educational web pages from the internet), and updates the 1558 million weights of the model to be slightly better at predicting the next token in a sequence. By the end we'll have processed 32,000 * 1048576 = 33.6B tokens in total. The loss goes down as we do a better job predicting the next token. The norm will stabilize around 0.1-1, the learning rate is being warmed up over the first few steps. Our model flops utilization (MFU) is around 50%, i.e. quite efficient. Now wait 24 hours for this to finish, then you can visualize the main.log log file using the dev/vislog.ipynb jupyter notebook. For this you will need to also have Python and matplotlib installed, and you will see the following: Evals. On the left we are tracking the loss on FineWeb-EDU validation data. If you simply run the GPT-2 released by OpenAI and evaluate its loss on this split, you get the red horizontal line (loss 2.83). You see that our run outperforms this very very quickly, by step ~5,000. However, this is not a fair comparison because GPT-2 was trained on the never-released WebText dataset, so there is a possibly large distribution shift. So e.g. if you finetune the OpenAI model for 1,000 steps at LR 1e-4, the loss quickly plunges to the blue line (loss 2.61), because it's quickly adapting to the new data statistics. I like to look at the validation loss as a sanity check, but for the actual comparison we'd want to look at fixed, 3rd party evaluations. One of the well-behaved, smooth, common, often-cited evals that also offer early signal is the HellaSwag eval. These are simple common sense scenarios and the model has to pick the correct continuation. We evaluate HellaSwag on the right pane, where we see that we cross over the GPT-2 model around step ~25K (earlier than GPT-2, which is estimated to have been trained on ~100B tokens. This possibly has to do with increased data quality, as we also observed in our earlier 124M run). The green line is the GPT-3 model of the same size, which is pretty much the same model architecture as GPT-2 with minor differences (context length 1024 -> 2048) but trained for 300B tokens (i.e. ~10X more tokens than what we trained on here). I should say that even HellaSwag is not an ideal single point of comparison because it tests simple English and common sense, it does not test e.g. multilingual, math or code. It could have been that the WebText data mixture was a lot heavier on these, and these domains were \"stealing\" model capacity to some extent, we don't know because it was never released. Lastly, in general, good evals are harder at low model capability like GPT-2 because e.g. the models don't understand multiple choice, and their samples are not high enough quality to make above chance dent into standard math or code evals. Args guide. Let's look at the args we passed into the training now in more detail. The GPT-2 release from OpenAI included model weights but very few details, while GPT-3 release had no weights but many details. So in many cases, we follow the GPT-3 paper hyperparameters because the GPT-2 paper has very very little information: -i -j are training and validation splits token files, downloaded earlier with edu_fineweb.sh -o is the output directory to write logs and checkpoints into -v 250 asks to evaluate and log the validation loss every 250 steps -s 300000 asks to sample some tokens every 300000 steps. Because the total number of steps will be less than this, this is hacky way to turn sampling off and we will only sample a single time at the very end. -g 384 sets the number of tokens to be sampled at the end to be 384 -h 1 asks to evaluate the HellaSwag accuracy -b 16 sets the micro-batch size to 16 . If you are running out of memory, decrease this value, e.g. try 8, 4, 2, all the way down to 1 potentially. -t 1024 sets the maximum sequence length to 1024, as GPT-2 did -d 1048576 asks that the total batch size be 2 to the power 20, following the GPT-3 paper hyperparameters table. The code will make sure to meet this desired total batch size and calculate the needed gradient accumulation \"inner loop\" steps of the optimization. For example up above, we saw that we have 8 GPUs each doing 16 X 1024 tokens, so that is 8 X 16 X 1024 = 131,072 tokens per micro-step (a single forward backward), so the code calculated gradient accumulation steps of 8 to meet the desired 1M batch size per step. i.e. it does forward+backward 8 times and then a single update. -r 0 sets recompute to zero. Recompute is a way to trade off compute and memory. If -r 1, then we recompute a piece of the forward pass (the GeLU) during backward. This means we don't have to cache it and save memory, at the cost of some more compute. So if you're running out of memory, try -r 1, or -r 2 (also recompute layernorms). -z 1 turns on ZeRO-1 (i.e. optimizer state sharding) across multiple GPUs. If you're training with > 1 GPU, this setting is a no-brainer and should basically always be on. On 1 GPU this setting is a no-op. -c 0.1 sets the weight decay to 0.1. Only (2D) weights are decayed exactly as in GPT-2, and this number comes from the GPT-3 paper -k \"cosine\" sets the cosine learning rate schedule, which is the default so this is a bit spurious. -l 0.0006 sets the maximum learning rate to 6e-4. The GPT-3 paper says to use 2e-4 for this model size, but here we triple and it and seems to train faster and without any issues. This wasn't tuned very carefully yet. -q 0.1 says that we will decay the learning rate to 10% of max LR over the course of training, following GPT-3 paper. -u 700 says that we will ramp up the learning rate from 0 to max learning rate over the first 700 iterations, which at total batch size 0.5M is 350M tokens, following GPT-3 paper. -n 2000 asks to save model checkpoints every 2000 steps. -x 32000 asks for 32K steps in total. I chose this number because it is a nice number, and just fits into 24 hours. -ge 1 sets a very recently merged gelu recompute setting for CublasLt (optional) -y 1 sets the \"resume\" flag on. If your training for any reason crashes or hangs, you can CTRL+C and re-run this command, and it will attempt to resume the optimization. llm.c is bitwise-deterministic, so you'll get the identical result as if you didn't crash. -e \"d48\" asks to initialize, a depth 48 GPT-2 model from scratch. Memory guide. The biggest constraint most people will probably face is that their GPU doesn't have 80GB. That's okay you should still be able to run everything above if you are patient, it would just run slower. So if the model doesn't fit, what do you play with? The most important one is the micro batch size -b. Try to decrease it but keep it to nice numbers. So e.g. 16 -> 8 -> 4 -> 2 -> 1. From there, try to also play with the recompute setting -r which is 0 (fastest, a lot of memory), 1 (very slightly slower, but a huge memory saving), or 2 (slightly slower, smaller memory saving). The next thing you can do is disable master weights in fp32, which you can do with -w 0 (1 is default). We won't maintain fp32 copy of params. Empirically in a few runs before this seems to be okay, likely due to our use of stochastic rounding. If even that doesn't fit (that's unlikely right?), you could try to decrease the maximum sequence length with -t, default is 1024 you can take it down to 512, 256, etc., but now you are making your model worse because you're decreasing its maximum attention span. Code. Certainly I feel biased but llm.c is quite beautiful: It only requires basic CUDA dependencies to run. It is a direct, minimal and readable implementation in C/CUDA. llm.c totals about 5,000 lines of C/CUDA code. We try to be mostly C, not C++ to keep it simple. Neural net training is just one while loop of the same, simple arithmetic operations (think +, -, *, /) on a single float array, it really shouldn't be that complicated. It compiles and runs very quickly (few seconds), so you're doing more stepping and less waiting. It allocates all of its GPU memory a single time at the start and from then on during training has an exactly constant memory footprint. So once you start stepping, you know you're good for the rest of the run and won't OOM. It is bitwise deterministic. It is efficient, at just below ~50% MFU. The main entry point and the majority of the code is in the file train_gpt2.cu. It contains the GPT-2 model definition and the training loop in ~2,000 LOC, and it imports a bunch of helper files with various utilities and the individual layer implementations from the llmc directory. cloc llmc reports 23 files with 3170 LOC, and cloc train_gpt2.cu is 1353 LOC atm. Multi-node training. If you are part of the privileged GPU-rich upper class, llm.c supports multi-node training and the most GPUs I've seen someone train llm.c with is ~500 GPUs. This biggest run I've done personally so far is on Lambda's new 1-click cluster feature with 16XH100 GPUs in 2 nodes. The downsides of unemployment. The lambda team has put up detailed instructions on how you can train llm.c models on their 1-click clusters. E.g. with the 512-GPU H100 cluster for $2,300/hr, you might be able to train your GPT-2 in ~30 minutes. You'd have to increase the total batch size (e.g. to ~8M) and possibly tune the hyperparameters a little. I haven't tried but it probably works and would be very cool :) PyTorch comparison. A relatively comparable run in PyTorch would I think look something like this, using our parallel PyTorch implementation: torchrun --standalone --nproc_per_node=8 train_gpt2.py \\ --input_bin \"dev/data/edu_fineweb100B/edu_fineweb_train_*.bin\" \\ --input_val_bin \"dev/data/edu_fineweb100B/edu_fineweb_val_*.bin\" \\ --write_tensors 0 \\ --model d48 \\ --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \\ --dtype bfloat16 \\ --compile 1 \\ --tensorcores 1 \\ --flash 1 \\ --num_iterations 32000 \\ --warmup_iters 700 \\ --weight_decay 0.1 \\ --overfit_single_batch 0 \\ --learning_rate 0.0006 \\ --zero_stage 1 The PyTorch code is meant as a testing reference not an actual implementation, so the training loop is a little bit different in some places (e.g. the dataloader doesn't permute the shards, etc.), but this is still possibly useful as a point of reference. I also hacked the default vocab size to be 50257 -> 50304 to get added efficiency, then the currently PyTorch nightly gives: step 16/32000train loss 8.903997norm 8.3474lr 1.37e-05(3381.88 ms310057 tok/s) step 17/32000train loss 8.870140norm 3.7936lr 1.46e-05(3381.95 ms310051 tok/s) step 18/32000train loss 8.875732norm 9.4993lr 1.54e-05(3393.09 ms309033 tok/s) step 19/32000train loss 8.817432norm 2.8345lr 1.63e-05(3379.75 ms310253 tok/s) step 20/32000train loss 8.798056norm 4.1234lr 1.71e-05(3386.53 ms309631 tok/s) step 21/32000train loss 8.777574norm 2.8010lr 1.80e-05(3386.05 ms309675 tok/s) ... Now I wouldn't say I have full confidence that the PyTorch script is maximally tuned, but the following observations can be made. PyTorch seems to be taking a lot more memory (this run is ~80GB), while llm.c is at 57GB (29% improvement). Memory is important because it allows you to crank up the batch size (e.g. llm.c can go up to 24 microbatch here), which goes a bit faster. Second, we're seeing about 3386 vs. 2750ms per iteration, so llm.c is stepping ~19% faster. Some of the gains here have known origin, e.g. llm.c includes optimizations like the Fused classifier that kicks off the backward pass, which is something torch.compile does not do today afaik. But it's also possible that this script isn't fully maximally tuned, but in any case I'm showing the comparison in case 1) others would like to take a look, play with, compare, help tune and 2) to just say that llm.c is quite optimized and fast - in the specific case of GPT-2/3 training. The final model. A few links that may be helpful, for posterity: The main.log file. The model_00032000.bin llm.c bin model file The model converted to huggingface transformers GPT-2 model I uploaded here: karpathy/gpt2_1558M_final2_hf. Model export. The model export can be done as follows, for example: python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export This then lets you run the Eleuther eval harness, or run the huggingface sampling pipeline to get model samples: # take model for spin import torch output = \"./gpt2_1558M_final2_hf\" # set pytorch seeds torch.manual_seed(42) torch.cuda.manual_seed(42) prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\" from transformers import AutoModelForCausalLM, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(output) model = AutoModelForCausalLM.from_pretrained(output, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16, device_map='cuda') model.eval() tokens = tokenizer.encode(prompt, return_tensors=\"pt\") tokens = tokens.to('cuda') output = model.generate(tokens, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, num_return_sequences=4) samples = tokenizer.batch_decode(output) for sample in samples: print('*30) print(sample) Also have a look at dev/eval for instructions on how to run the Eleuther Evaluation Harness, the evals from the HuggingFace Open LLM Leaderboard, etc. 400B token run. I have also made the attempt to train GPT-2 for significantly longer than 33B tokens. In particular, I changed -x to 400,000 to train for 420B tokens (even more than GPT-3 model of this size, which was trained with 300B). This model run looked great until about step 330,000: This model dramatically beats GPT-2 and GPT-3 of its size on HellaSwag (it gets up to ~61%), but sadly becomes unstable there on and explodes. There are more smaller spikes along the way but the code is configured to detect the more simple instantaneous instability and skips update (I used the flags -sl 5.0 -sg 5.0), which helps mitigate and defers issues. However, I think we're not yet being sufficiently careful with our initialization, activation ranges, and overall model training stability and there are deeper issues that gradually drift the model into instability, especially for larger models and over long training duration. To be continued. If you have ideas or recommendations for stabilizing LLM model training please contribute your experience in the discussion below. FAQ: Can I sample from the model in llm.c? kind of, but it's inefficient and a bit weird, and even more hacky if you'd like to prompt the model. Use the huggingface paths above for now. Can I chat with it? no, this is currently only pretraining, not chat finetuning. Can you train in fp8? No, we're currently mostly training in bf16, but early versions are very much work in progress. I have a non-NVIDIA GPU can I run llm.c? No, llm.c supports C/CUDA only, but good forks exist (see main README). For example there is an actively maintained AMD fork by @anthonix that is quite good. GPT-2 (124M). I wanted to also link to an earlier post on training the GPT-2 (124M) model in llm.c, which has some more related information to llm.c runs. 124M is a smaller model in the GPT-2 miniseries, only 124M parameters compared to 1558M parameters. Authors Substantial contributions to llm.c came from what now feels like the llm.c core dev team, in addition to self: @ngc92 in all aspects of the code base @ademeure in CUDA kernel optimization, low precision training, cudnn, cublas, ... @gordicaleksa in all aspects of whatever is next on the TODO list, from algorithms to code to multi-node or etc. @rosslwheeler in CI and Windows support. If you're happily running llm.c on Windows you should definitely thank Ross :) Lambda labs for sponsoring the GPUs used in the development of llm.c. The history here is that I've happily used Lambda for several years and then a few months ago I pretty please asked if they are open to not charging my account for llm.c dev work and they agreed so here we are thank you for supporting llm.c! Nvidia and Ubicloud (www.ubicloud.com) for providing the GitHub Nvidia GPU Runners for our CI. Coming up. Some of the next big steps we are interested in and looking at these days: Further optimize GPT-2 training hyperparameters. For some reason, the hyperparameters cited by OpenAI in the GPT-3 paper appear to be quite suboptimal, e.g. @Yuchenj_UW on X found that you can 3X the learning rate and get faster training with no apparent downsides. There might be other similar low-hanging fruit. Improve training and scaling stability, e.g. more stable optimizers, schedulers, clipping, norming, muP. (Some of these PRs already exist, if you have tips on stabilizing LLM runs please reach out with ideas to try!). Mixed precision++: training with fp8 (imminent!). Model inference, e.g. KV cache is the low hanging fruit here. Finetuning: SFT, RLHF Multimodal extensions, VQVAE and friends More modern architectures, support for Llama / Gemma model series. The goal of llm.c remains to have a simple, minimal, clean training stack for a full-featured LLM agent, in direct C/CUDA, and companion educational materials to bring many people up to speed in this awesome field. Please feel free to use the Discussions for any FAQ and related, or if you'd like something faster, #llmc on Discord, or #llmdotc on CUDA MODE Discord. We'll see you next time! 44 11 15 55 9 1 Replies: 8 comments · 10 replies Oldest Newest Top gordicaleksa Next up \"this is how to train Llama 3 8B in 72 hours for 1500$\"🫡🫡🫡 19 4 replies YuchenJin I also wonder how hard it is to modify the current codebase to train llama3 8B ngc92 RMSnorm is fairly easy, removing biases is easy (both do have a PR already), SwiGLU should also be straightforwards, I think the main challenge will be group-query attention with rope-encoding. What is quite trivial is just scaling up the current model to 8B; in fact, I'm planning to make a PR that just adds a few more options for the model init. One question is how to continue the model series; two options would be: // deeper else if (depth == 60) { channels = 1920; num_heads = 30; } // 2.7B else if (depth == 72) { channels = 2880; num_heads = 30; } // 7.3B else if (depth == 84) { channels = 3456; num_heads = 36; } // 12.2B // wider else if (depth == 56) { channels = 1920; num_heads = 30; } // 2.6B else if (depth == 64) { channels = 2880; num_heads = 30; } // 6.5B else if (depth == 72) { channels = 3840; num_heads = 30; } // 12.9B This roughly matches the GPT3 series in paramter count, but both are much deeper. YuchenJin Supporting the whole GPT3 series would be interesting! VatsaDev @ngc92 I've looked at scaling like that, https://vatsadev.github.io/articles/Layers.html, theres a certain amount of balance thats best, but here layers are already wide enough to just focus on depth MathiasSchindler 400B token run: \"This model dramatically beats GPT-2 and GPT-3 of its size on HellaSwag (it gets up to ~61%), but sadly becomes unstable there on and explodes. \" Would you be able to release the model right before the explosion? I would be interested to learn what instability and explosion look like in a model. 6 2 replies karpathy Maintainer Author Ok I'll upload it later today. I'm trying to revive it still and get the full 400B to complete. I took a close look at the weights last night and sadly I couldn't see any major issues. All of them look well behaved in good ranges, the worst I saw is the c_attn biases ranged -50 to 50, which seems rather broad. Everything else was in nice ranges. 2 michaelklachko @karpathy I'd check for overflows in layernorm layers: if input x if FP16, variance might overflow: https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.py#L12 greydanus I love the simplicity, power, and attention to detail. Well done! I hope to experiment with this code myself someday soon. 0 1 0 replies ex3ndr What about new flash attention 3? Will it slash price in half? 5 2 replies astelmach01 You'd have to rewrite the kernels, as flash-attention is hardware-dependent ngc92 you don't have to rewrite the kernels, you'd \"just\" need to figure out how to integrate FA3 (including its dependencies) into the current codebase. That being said, no, FA3 will not cut the cost in half. A significant amount of compute is spent in good old matrix multiplications (e.g., for the MLP part), which is completely unaffected by FA. jonready Thanks again for your work to democratize AI. I'm only part way through makemore now, but am blown away by how simple you can make these tough topics. 4 0 replies rosslwheeler Also want to thank Ubicloud (www.ubicloud.com) for providing the GitHub Nvidia GPU Runners for CI and Nvidia for sponsoring this. Thank you!!! CC: @karpathy 5 1 1 reply karpathy Maintainer Author Bleh very good callout thank you @rosslwheeler and Ubicloud! 2 Daudsarfraz Hi, Andrej I'm watching your videos. Especially llama.c. Can you please share resources how you created own transformer. 2 0 replies bil-ash @karpathy I have an integrated GPU(ryzen) with 16GB VRAM. Would like to train a 256m gpt with 32k context length (using https://github.com/anthonix/llm.c). Will 16GB VRAM permit doing so? Time is not an issue because I am ready to wait for weeks. Also, what should be the size of dataset- 10B(fineweb-edu), 5B or anything less than that? 1 1 reply edited ngc92 With an integrated GPU, I'm not sure if weeks are the order of magnitude you're looking at. 32k context with just 16GB also sounds very challenging, not sure if that is going to work with the current codebase. More recomptuation will help; not sure if it'll be enough, though. IMO, the thing to do is download, build, and try it out. Start with standard context of 1k, and increase it until you hit OOM. Not sure about the dataset size required, but if you want to train 32k context, you probably need to at least filter the dataset to have enough documents of sufficient length. Edit: Maybe with a batch size of 1, the required activation memory isn't too big actually, and you still get sufficient tokens per update. Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Category General Labels None yet 14 participants",
    "commentLink": "https://news.ycombinator.com/item?id=40939707",
    "commentBody": "Karpathy: Let's reproduce GPT-2 (1.6B): one 8XH100 node 24h $672 in llm.c (github.com/karpathy)177 points by alecco 23 hours agohidepastfavorite53 comments tomalaci 23 hours agoWith how much NVidia is developing AI-workload accelerating hardware, I expect this will cost maybe few dozen dollars and train in few hours within next few years. What I think will be interesting is when commodity hardware can run cheap inference from very capable, specialized models. Pretty sure it will spawn a new golden age of AI-powered desktop applications. For example, video game space has already been trying to create AI-powered NPCs, world generation and story-telling (e.g. Inworld AI). reply swatcoder 23 hours agoparent> For example, video game space has already been trying to create AI-powered NPCs, world generation and story-telling (e.g. Inworld AI). This'll be a niche for a long, long time. Games are generally carefully crafted to deliver a specific mechanical and/or narrative experience. A world populated by LLM/etc bots or content is one choice of what that experience might be, but it's not going to be a very satisfying one for many game designers -- especially given the current/near state of the technology. There will be games and experiments that explore it, for sure, but the vast majority of games just don't have any need for it. reply 123yawaworht456 22 hours agorootparentfor narrative/dialogue, yes, text generation is currently useless. censorship, slop, extreme positivity bias. even jailbroken Opus is shit. but audio generation we already have is pretty much good enough, and this is big. it's not AAA tier yet, sure, but still lightyears better half-assing it with mediocre voice actors. it is now an option to only use real voice actors for a few key characters, and even that won't be necessary within the next decade. even indie video games will be fully-voiced soon. reply miki123211 16 hours agorootparentSame for dubbing. Voicing a game in English is doable, voicing a game in n languages is only doable if you're the size of EA / Activision / Ubisoft. reply glial 20 hours agorootparentprevSimilar for textures - it would be really neat if textures were auto-generated to add detail when you get close to something. As it is, the sprites just look bad. reply jetrink 22 hours agorootparentprevThose don't have to be mutually exclusive though. To take AI out of it, think of those murder mystery parties where actors interact with attendees. Actors have roles to play and things they must do to move the story forward, but they improvise their dialog when talking with the players and sometimes each other. Or if you've ever played D&D, you have experienced talking with NPCs that are controlled by your DM. I think video game AI could be a lot like that, where NPCs use natural language instead of rigid dialog trees, but otherwise, they behave a lot like they do today. reply swatcoder 22 hours agorootparentYes, that's the intuition for where the technology might go. Someday. But actors and DM's are much more disciplined than LLM's, partly because they have careers and friendships on the line for misbehavior. For what amazing things they can do in good weather, LLM's are not really reliable when you want them to consistently deliver something very specific, very secure, or very artfully crafted. They may get there, but their design makes it a very hard problem that we're still a long way from seeing commercialized. reply jononor 19 hours agorootparentAgreed that \"live\" in-game LLMs for NPCs is probably a little while away. Need better tools than now that allows to strike a balance between constrained/directed output and variation. I suspect that LLMs are already used by (some) game developers to aided dialogue creation tools for game developers, where they curate and select more or less a fixed vocabulary. I do think that there are many viable steps to \"generic\". For example, if the game developer could specify possible dialog trees (as in the possible branches and outcomes), then an LLM could ideally fill in the concrete text when reaching the different paths. Or an LLM could add in \"small talk\" - meaning things that reflect the world state, recent events, but still have the quest type dialog be practically hardcoded (so there is no risk of unreachable states etc). reply thatguymike 22 hours agorootparentprevAh, like this: https://www.youtube.com/watch?v=Kw51fkRiKZU reply headcanon 22 hours agorootparentprevI do think there is a big opportunity for widely supported hardware-accelerated matrix algebra in games. Currently most of that is geared towards graphics (naturally) but being able to easily encode arbitrary models and have them run on-device would open up a lot of opportunities for games (like deep simulation) that weren't possible before. Its currently possible of course but requires custom tooling and (relatively) niche hardware like a high-end graphics card. I see the development energy around LLMs as a way to open up support for that. reply opyate 11 hours agorootparentprevPerhaps Llama are good enough now to add flavour NPCs to the world, i.e. NPCs not on your critical path, but maybe on the town square where you can hear them talk about mundane life stuff. reply ImHereToVote 22 hours agorootparentprevI don't have needs. I have wants. You see, I don't need. I want. reply up2isomorphism 23 hours agoparentprev> For example, video game space has already been trying to create AI-powered NPCs, world generation and story-telling (e.g. Inworld AI). To me this is a downside compared to the NPC generated by humans, since that’s the only reason I would like to read them. reply bick_nyers 23 hours agorootparentWhat if the LLM that a specific NPC utilizes was handcrafted/fine-tuned by a human? reply chongli 22 hours agorootparentprevYeah. I have played a bunch of the roguelike Caves of Qud [1] and it has both hand-written text and procedurally generated text. The former is quite interesting and relevant to both gameplay and plot. The latter is mostly uninteresting and irrelevant, though it does work as \"filler.\" This is similar to how procedurally-generated grass can give a more natural look to a hill than you'd get with tiles (which are incredibly easy to spot unless a ton of work is put into hiding the seams and repeating patterns). I still long for the day when we can have procedurally-generated stories and quests that are actually interesting to play through. I have no idea how that is going to work though! [1] https://www.cavesofqud.com reply FooBarBizBazz 5 hours agorootparent> I still long for the day when we can have procedurally-generated stories and quests that are actually interesting to play through. It's an interesting artistic and technical challenge to investigate, absolutely. I hope people work on it, and I'm sure they already are. However, let me also offer a counterpoint with something much simpler: coffee. It is possible to fully automate the process of making an espresso drink. You can buy cheap versions of these machines that will sit on your kitchen counter; the quality of the drinks so produced is not the highest, but I suspect it's entirely possible to build a machine that would actually make high-quality drinks, perhaps with even more precision than a human can. Yet the opposite trend has prevailed over the last few decades. Not long ago, Americans drank drip coffee (hence the approximation thereof as the Americano). This is an easy process to make in large batches with minimal labor. It is almost fully automatic, without any \"automation\". And certainly this still exists all over the place. But we now also have coffee shops everywhere -- witness the explosion of Starbucks out of its home in Seattle -- where people, baristas, individually make espresso drinks for customers. This is immensely popular. Why are tons of people now employed in this way, and why hasn't it been automated? Is it really just a matter of cost? I don't think so. I think the knowledge that another person took the trouble to do something for you is in fact part of the product. Consider the practice of drawing designs, like leaves or hearts, in the milk foam. This is entirely unnecessary, from a taste perspective. If you put a lid on it, as is commonly done, you will not even see it. But it carries a message -- that somebody gave enough of a damn to do it. (We could then analyze how this phenomenon gets watered down and eventually destroys itself when it tries to turn itself into a mass-produced fast-food franchise staffed by underpaid/exploited proletarians, but that would take this post in another direction.) My point is, I think something similar happens in video games. The very knowledge that another person was involved is important. You are receiving communication from this person. It does something to synchronize, partially, your mind with theirs. And this is a thing we appreciate. If there's no person on the other end, why should we care? This general phenomenon of paradoxiciality can be a bad thing. It can \"feel like\" socializing, without producing the actual, real, thick social networks that give us rich lives. There can be an exploitative and even druglike dynamic. It isn't entirely a good class of phenomena. But in small doses I think it's good, useful, important. I think it's a core part of what makes art valuable. And of course, it applies to games. This is why, I believe, we will not fully automate the telling of stories, or the making of lattes. The person doing it is important. reply chongli 3 hours agorootparentI think the knowledge that another person took the trouble to do something for you is in fact part of the product. It's an interesting thesis but I don't for a second believe Starbucks sells billions of coffees because people want the barista experience. Starbucks would absolutely automate their business with fancy machines if they could. The problem is that the drinks are so complicated and customizable that no one has built a machine capable of making them all. Plus there are plenty of people who order Starbucks drinks through an app and never actually meet the barista who made them. If those drinks were made at some fully automated commissary and delivered by drone they'd be just as happy. The very knowledge that another person was involved is important. You are receiving communication from this person. That's important for some people, and some games, but not for everyone. I play roguelikes mainly for the challenge. The procedurally generators in these games can create bizarre and very challenging situations no human could ever come up with. If there's no person on the other end, why should we care? Because it's a puzzle for your mind to figure out. It's why people play solitaire games (with a deck of cards), random sudokus, tetris, etc. reply MisterBastahrd 23 hours agorootparentprevYou don't even necessarily need to have them coming up with valid speech. Simply giving out random quests and rewards would keep people running on a loot treadmill for most open world multiplayer games. reply LeanderK 23 hours agorootparentI think at first even background NPCs that don't give quests and rewards would be nice. Sort of give everyone a character and let them just babble. It breaks the immersion to hear the same phrases repeatedly. You can still handcraft every important quest and interaction to achieve high fidelity, but I would like random NPCs you bump into to not just repeat things all the time. reply swatcoder 22 hours agorootparentYou're more likely to see studios (who want lots of more varied content) use generative AI in the studio, where they might generate and review it before release. Letting generators run free on the client sets up different kinds of immersion-breaking, where NPC's hallucinate misleading details about the story/world, can be tricked into reciting off-topic absurdities or age-rating violations, etc. AAA studios can't afford the embarrassment of that and smaller designers with pride of craft won't see their signature come through the art in it. Surely, some designers will figure out ways to make it work great for some specific idea, but it's not the best way to use the technology in most cases. reply swatcoder 22 hours agorootparentprevTo make this work, you need your LLM-based AI to outperform any other form of generating quests and rewards -- and that performance is measured on things like player enjoyment, game/story progression, exploitability, client system requirements or server operating costs, etc and most of those things are very hard to constrain or optimize for with an LLM right now. While the costs are hidden from end users and are going down quickly, good LLM's remain very expensive to run and very hard to keep on track compared to other options. reply MisterBastahrd 2 hours agorootparentYou're trying to big brain something that isn't that complicated or special. I never said that every interaction with an NPC must necessarily require AI to create a new quest and reward from scratch. Players aren't that savvy. You just generate a pool of quests and assign them to NPCs in the instance. Players who are addicted to loot treadmills will stay in the game and pay for boosts and other rewards as long as there is content to engage them. This more than pays for the AI service. reply techjamie 22 hours agorootparentprevYou could maybe pull this off in a game like Borderlands where the loot is basically just the same dozen guns but with different numbers and effects. But as is, the LLM text isn't going to be much different than a sufficiently large AdLib system. I think there is value to be gained in having LLMs as part of the development process, maybe even the game itself, but I think conventional methods are about as sufficient for quests. reply robbomacrae 22 hours agoparentprevWhilst I agree with the reservations of the other replies I think you were implying in the future and I'm sure the LLM's will be more trustworthy and up to the task at some point. What I would really like to see now is all the new TTS models being used more widespread. There are still so many games that have text only output. My kid love Alba: A wildlife Adventure but the eldest still isn't quite ready to read all the text so I have to sit with them reading out all the lines. If anyone has a way of applying universal mods / accessibility features to existing games I'd love to see someone solve this and happy to help with the TTS! reply talldayo 23 hours agoparentprev> I expect this will cost maybe few dozen dollars and train in few hours within next few years. I wouldn't count on it. Nvidia's been cleaning up shop, but their best option for expanding right now is through parallelization (bigger clusters, basically). Now that Blackwell is on TSMC, Nvidia is alongside Apple waiting for new and denser nodes to upgrade to. A real \"generational leap\" in training cost is going to require some form of efficiency gain that we're not seeing right now. It's possible that Nvidia has something up their sleeve, but I'm not holding my breath. > What I think will be interesting is when commodity hardware can run cheap inference from very capable, specialized models. What's funny is, you basically already can. The problem is becoming integration, and in the case of video games, giving the AI a meaningful role to fill. With today's finest technology, you can enjoy an AI-generated roguelike that is nigh-incomprehensible: https://store.steampowered.com/app/1889620/AI_Roguelite/ As time goes on, I really think developers are just going to not use AI for video games. Maybe I'm missing the \"minecraft moment\" for procedurally-generated stories here, but the sort of constraints needed to tell a story of create an interactive experience don't exist within LLMs. It's a stochastic nightmare of potential softlocks, contradictions or outright offensive requests. The majority of places I've seen AI applied today isn't for content creation, but instead automated moderation. reply charlescurt123 22 hours agoparentprevI imagine we could do this now but not the way you think. have a human created story and text as a guideline. With that have genAI make the text per stage, you would get different statements every time and would stay on track. Would be interesting to play a game where all players say the same information in slightly different ways every single playthrough. reply HPsquared 22 hours agoparentprevSimilar scaling to genome sequencing. First genome was a huge undertaking, now routine after a few Moore-esque cycles. reply forrestthewoods 23 hours agoparentprev> For example, video game space has already been trying to create AI-powered NPCs, world generation and story-telling (e.g. Inworld AI). Current AI isn't even close to good enough for video game NPCs and related. We're several breakthroughs away from that being possible at any cost. Those breakthroughs might happen in 3 years, or they might not happen in 10. Hard to predict. reply Tiberium 23 hours agorootparentAre you sure? Models like Claude 3.5 Sonnet are both good at writing and instructions, as long as you set some guardrails for the model, they can be great NPCs. reply forrestthewoods 22 hours agorootparent> Are you sure? Absolutely. Current LLMs have insufficient world state. Imagine a game like Stardew Valley. It's got a town with 30 NPCs or some such. They all have personalities and the player builds a relationship with them over time. Current LLMs can't do that. They hallucinate waaaaaaay too much. You can't reliably define and evolve relationships. Amongst many other short comings. I'm super pro AI and use ChatGPT all the time for programming. So I'm not being an AI hater. But I am a gamedev and I can say that what exists today simply isn't good enough. reply Tiberium 22 hours agorootparentBut you're saying that you want a single model to handle all NPCs and the whole world. Of course this isn't possible currently. But using a separate model with separate context for each character is. Also, if you use ChatGPT for programming, try Claude 3.5 Sonnet - it's really better than GPT-4o for programming. reply forrestthewoods 21 hours agorootparent> But you're saying that you want a single model to handle all NPCs and the whole world. No, I did not say that at all. I didn't specify how the LLMs may or may not be structure. I'm saying that current LLMs - and yes I've used Claude 3.5 Sonnet - are insufficient. There is no existence proof that they're sufficient. LLMs are great. They aren't great enough for video game NPC. Not yet. Further innovation is needed. You're free to disagree. I can't prove a dispositive. But there is no working example. reply ebalit 18 hours agorootparentAs a gamedev, what would you use currently instead for NPCs? And what would you say makes this/those solutions better than LLMs? reply forrestthewoods 16 hours agorootparentDialogue systems vary by game and are generally custom. But they amount to hard coded if-else branches if you squint. The only one I ever wrote was a simple announcer for a sports-ish shooter. Your question is weird. Every video game ever made has shipped using not LLMs. Not a single commercial game has ever shipped with an LLM. So I’d say what makes classic NPC systems better is they’ve shipped tens of thousands of commercial successes over 50+ years. And what makes LLMs worse is they haven’t once been proven viable for even a single title. Nor have they produced even a compelling tech demo. Geez people. reply Tiberium 5 hours agorootparentThere are multiple games that use LLMs, available in Steam. reply forrestthewoods 3 hours agorootparentName them and demonstrate they don’t suck. reply alecco 23 hours agoprevAlso https://x.com/karpathy/status/1811467135279104217#m reply iforiq 23 hours agoparentHow much did gpt2 training cost when it came out in 2019? reply ozr 23 hours agorootparentAbout $50,000. reply withinboredom 23 hours agorootparentprevThey probably spent more on the training data, to be honest. They had to get it the hard way. reply alecco 23 hours agoprevIt will be interesting to see this with today's FlashAttention 3 for H100. reply rurban 22 hours agoprevWould be free for us because we have those H100`s, but currently it's way too hot now. They will reach 70°C, even watercooled. reply jamestimmins 23 hours agoprev [–] Anyone have an idea if this is feasible to do on a Macbook with a built-in GPU? reply michaelmior 23 hours agoparentProbably not with the same amount of training time, but I'd imagine a recent MBP GPU could handle GPT-2 training. The biggest challenge is that the training would need to be reimplemented for Metal instead of CUDA. reply rty32 22 hours agorootparentSlightly off topic -- I just saw people saying how Mac's unified memory makes it a strength to train models on Macs: https://www.macrumors.com/2024/07/10/apple-leads-global-pc-g..., and how energy efficient they are etc. But what I am seeing is that people don't often even touch Macs at all -- they write code with CUDA and that's it. I find this kind of conversation fascinating. reply jamestimmins 23 hours agorootparentprevAh so I couldn't just run this on my laptop for ~48 hours? That's too bad. reply danielmarkbruce 23 hours agorootparentHe does it on 8 H100's in 24 hours, ie 192 H100 hours. It's going to be thousands of laptop hours. reply mmoskal 22 hours agorootparentprevH100 SXM is 2000 TFLOPS at FP16. Multiply by 8. M3 Max is 28 TFLOPS at FP16. Based on FLOPS alone, it would be more like a year or two. reply Davidzheng 22 hours agorootparentCan you estimate how long it would take to replicate alphago zero today on one set of 8xH100. reply karpathy 22 hours agorootparentprev(H100 SXM is 1000 TFLOPS, *2 is from \"with sparsity\", which is not used here.) reply latchkey 17 hours agorootparentMI300x is 1300 TFLOPs at FP16 (without sparsity). Looking forward to seeing the results. https://www.amd.com/en/products/accelerators/instinct/mi300/... reply mmoskal 22 hours agorootparentprevRight... and there are probably some communication overheads over NVLink that would not be present on single laptop. So a few months maybe :) reply arthurcolle 23 hours agoparentprev [–] Will take more hours reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have reproduced GPT-2 (1.6B) using llm.c, a C/CUDA implementation, on a single 8XH100 node in 24 hours for $672, bypassing typical Python-based deep learning stacks like PyTorch.",
      "Despite some tuning and stability issues, the model's performance is comparable to GPT-2, evaluated using metrics like validation loss and HellaSwag accuracy.",
      "The post also covers memory management, multi-node training, and compares the implementation with a PyTorch version, with future plans to optimize hyperparameters and improve training stability."
    ],
    "commentSummary": [
      "Andrej Karpathy successfully reproduced GPT-2 (1.6 billion parameters) on a single 8XH100 node in 24 hours at a cost of $672.",
      "AI hardware advancements could potentially lower costs and reduce training times in the future.",
      "While AI in video games shows promise for enhancing NPCs and storytelling, current technology still faces significant limitations, particularly in text generation for narrative and dialogue."
    ],
    "points": 177,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1720725707
  },
  {
    "id": 40942141,
    "title": "Floppy8 – A Tiny Computer, in a Floppy Drive (2023)",
    "originLink": "https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive",
    "originBody": "💾 Floppy8 - A Tiny Computer, in a Floppy Drive February 21, 2023 Abraham Haskins This post covers the design and creation of the Floppy8, a microcomputer and cartridge system which fits inside a floppy drive. To see it in action, checkout this video! The Floppy8 plays 4K movies and games on custom cartridges, features wireless controllers, status lights, motorized cartridge ejection and more! Similar to a Famicon or Super Nintendo, the cartridge sticks out, allowing the label to still be partially visible. The front button safely unmounts and ejects the cart when pressed. The RGB indicator LED flashes different colors for waiting, mounting, running and error states. The wireless controllers are modified off-the-shelf NES clones which have a beige color and replaced d-pads along with extra added weights for that premium feel. If you'd like see more video of the Floppy8, I featured it a bit in my recent video about my misadventures attempting to make SD cards feel more nostalgic. How (and why) I Made It Please note: This post contains affiliate links to many of the parts used in this build! If you're like me, you often find yourself browsing Ebay (the online auction site) for weird items in the middle of the night (the time that isn't day) . One evening, while searching for floppy drives, I came across this little gorgeous little guy Strangely, the listing identified it as an Amiga 1010 disk drive. Maybe it is? But it doesn't look like one and I can not find anything that looks similar to it. The mystery intrigued me slightly, but the beautifully simple industrial design intrigued me more. I was led to this disk drive that night by a growing fixation on physical media which had been bubbling inside me for the previous months. I began to realize that when you have the ability to watch any film or play any game in seconds... it removes some of the joy. Not to mention, physical media which become tarnished or worn over years of love come to tell a story beyond the media contained within. There's a warmth to holding your favorite movie in your hands and that idea had led me to this drive. The longer I laid there, basking in the bright white light of the photos of this mysterious Ebay floppy drive, the more I became obsessed. The texture of the front cover, the smooth shiny beige metal outside, the playful and complex rainbow ribbon cable. Before I knew it, I had spent much too much for a disk drive that had to travel much too far to arrive at my door. Luckily, I was on a three week work trip while I waited the three weeks of shipping, so the time passed quickly and the question rattled in my brain \"what do I do with such a satisfying device?\" After all I don't own an Amiga and I don't intend to start, so what's a lad to do. I came to the conclusion that the best use of this device was to pull the innards (as carefully as possible) and replace them with a framework of my own design which allowed the drive to be useful and fun for years to come. But in 2022 we don't use floppy disks - they are much too impractical, so we'll need a new media, which means we'll need a new drive mechanism, which means we'll need the spirit of adventure because I have no idea how to do that. Week 0: The Plan I had a simple dream for this project... Design a new physical media format that hit a mix of nostalgia and practicality Mount some computer inside the case to read that media Design a drive that had a satisfying tactical feel when inserting a cartridge / floppy Replace the status LED Replace the mechanical front-eject button with one to eject the new cart type Leave all original pieces unmodified so they could be put back if needed Make it actually practical to use and enjoy! On paper a simple task, in practice this project tested my patience and was one of the hardest things I've ever done. As someone who has never done any really practical 3D printing... or electrical engineering... or anything - we had a lot to tackle. Week 1: It's real small Once the drive finally arrived, I realized how small it really was (which implies that if it is some Amiga 1010 variant that it's probably a late model due to its scale). The device shipped from Australia and still has a \"Bruining Headlam Computers\" sticker on it, I assume this is from when it was last serviced many decades ago. This did not help in solving the mystery of what this drive actually was, but I kept the sticker for nostalgia and to keep with my goal of being able to restore the drive to it's original state if needed. Although it was about 105mm by 43mm and around 5\" deep the real concern for me was the height of the floppy disk hole in the front cover. I knew I needed to retain this part so whatever new disk / cartridge format I invented would need to fit within this narrow slow. Luckily, this drive has a slightly taller than average hole, which allows our new cartridges about 4mm of height to play with. The other mind-blowing thing was the amazing condition of the device. It looked almost new, with just a few hints of wear. I was as careful as possible to avoid new scratches, dings or other tarnish as I began work on the device. Sadly over the weeks of working with this device, some of the paint around the front has some very small chips. This saddens me, but I'll be even more careful in the future. Week 2: We need Cartridges Before I could begin laying out any internals, I needed a sense of how the new physical media would work so designing the new media / floppy / cartridge (whatever you'd like to call it) was the top concern in the first few days. I needed to validate that I could manufacture something which was only 4mm tall and held some form of flash media and fit my aesthetic goals. Initially, I thought I might retain the visual elements of a 3.5\" floppy. I enlisted my brother to help develop our first idea - SD cards attached to custom circuit boards which looked like floppies but extended the SD card wires to an edge-connected pin set. Wow! My brother's designs, visually, looked fantastic. They replicted the original disks very closely and they were easy to manufacture. The simplicity was great, but I began to realize I needed something which lended itself well to a high-quality glossy label (more like a video game cart than a floppy) and more importantly, it needed to be made of a fairly soft material. I was worried that the sharp edges of a circuit board could damage the front cover of the drive. This concern, of doing this project non-destructively, held for the whole build. With the exception of the back sticker which was cut to reach a screw, this drive was never harmed and could be restored for historic value. I investigated a few ideas to encase these circuits, but with only 4mm of space, I could never conquer an idea which fit all the requirements, so we moved to fully 3D printed carts. The initial prototype was compelling. I used the free tool, Penpot to modify existing art for a game and printed a label on a cart I designed. Initially, I intended to use small USB flash memory sticks embedded in the cart, but I found that the square edges of a USB make it hard to align into a socket (more on this later). They were also slow when benchmarked and just kinda ugly. I would soon pivot to embedding micro SD cards in the 3D printed carts - which works well. This was the first tangible element of the device I sometimes called the Floppy-puter or occasionally Floppy8. I designed the carts in the free browser-based, grade-school focused 3D modeling tool Tinkercad. I used this tool for all 3D models for this project. While designing the carts, I purchased a small dye-sub printer, called the Polaroid Hi-print and fit the label indent on the cart to the photo size the printer produced. I also bought a small tool to round paper corners. Together along with my existing straight cutter, I was able to produce high-quality labels. Here is an overview of the whole cart creation process. The cart design was finalized pretty quickly and aside from small fit tweaks the design stayed the same from early on. It was a nice constant to have and a good motivator to hold and look at during frustrating days. Eventually, I would make a few carts of games or films I liked. I wanted to make them out of different filament as I loved the mismatched look of different colored 3.5\" floppies and wanted to retain that. Around this same time, I also began work on the software which would power the computer. I initially began with a Raspbian base (when I thought I would use a Raspberry Pi) and began customizing it to my liking, reducing the desktop UI, changing splash screens, etc. I knew I would need a fair bit of logic to recognize the insertion of the carts, mount them, run an autostart, then once the program exited or the eject button was pressed it needed to attempt to safely kill the program (if that failed, non-safely kill it) then unmount the drive and trigger a servo to eject the cart. While this was happening it would also be updating the front LED to indicate the device's status. Originally, I wrote the code as a bash script and this worked fine until I had many signals coming from different places (e.g. a program exit or eject button press may trigger the eject process). I eventually gave up and wrote the whole software stack in Node and used firmata to talk to the Arduino hardware. In order to get the LED status to indicate an inserted, but unmounted cart, I used dmesg to read the kernel logs early and reflect insertion before the cart was mounted. I then used fairly straight forward logic to handle cart, LED, and program status. The final setup requires each cart to have autostart.sh and an autokill.sh along with any binaries / media it needs. Ideally those scripts will boot the program then safely kill it once the eject button is pressed, but it will also force quit if the program hangs. The software never changed much after this point, just tweaks to make it more reliable. However, what did change a lot after this point was my approach to getting a Micro SD card reader to fit in the small gap inside the carts where the edge connector of the Micro SD card sticks out. After buying and ripping apart a dozen or so SD card readers, I finally found this incredibly tiny USB-C reader which fits perfectly in the gap in the cart. Designing the rest of the drive bay would mean figuring out how to mount this card reader while also aligning the rest of the cart properly so it would smoothly insert when pushed in. This wasn't that hard but did involve a lot of prototyping as tolerances on 3D printers are not great, so even though something was modeled to fit, that was no guarantee. Week 3-6: So many unknowns... Once I had a mostly-finished cart design I had to begin prototyping the computer, I knew I had a few big unknowns I had to explore. Some easier than others - but all challenges none-the-less. Is a Raspberry Pi powerful enough to do everything I want to do? How can I create an eject mechanism for the cartridges? How can I control the front-facing LED? Do I need fans? Will the metal case block wifi? How will I fit all the wires? Let's start at the beginning... Is a Raspberry Pi powerful enough to do everything I want to do? No - they are not. My initial tests showed the Pi of barely being capable of playing 4K video and definitely not at an acceptable framerate. In my tests, emulator and game performance was terrible for anything but the oldest consoles. This made me question all the Raspberry Pi emulator devices around - either my hardware / software was just terrible or folks aren't really playing those devices much. I ended up purchasing a Latte Panda 3 Delta - an obscure but really wonderful device. Aside from having a built-in Arduino, it also has headers for USB, power, and more. This made running wires for buttons, ports, lights, etc trivial and despite my constant wiring mistakes the board held up and worked really well. In my tests the performance was really reasonable for a small single board computer and in many ways, I don't think the Floppy8 would exist without this device. Sure other single board options exist, but the Latte Panda was really perfect. The only complaint I have is that documentation is minimal, but because the hardware is basically an x86 machine with an embedded Arduino in a tiny package, I did not feel the need for complex docs. Perhaps the luckiest element of this build was the fact that the Latte Panda just barely fit into the floppy drive. I printed this rose-gold plastic piece with the internal dimensions of the case to see the fit and as you can see, it is snug length-wise, but it sure fits. How can I create an eject mechanism for the cartridges? I knew early on that I wanted the device to be able to \"eject\" carts. I experimented with many types of spring-loaded mechanisms, latches, etc but in the end I ended up simply placing a servo with a cog which rotates and pushes the cart out. The actual implementation was simple, but getting power to the servo was tricky. I tried various electircal-engineery ways to get a high amperage 5v line for the servo, but nothing worked. At one point I thought I'd put a battery pack inside the device which would be kept charged and used only for the high-amp spikes for the servo, but somehow in testing I managed to miswire that and burn up the servo. So my final decision was to pull from the 5v line on the LattePanda but put a 0.4amp fuse to protect the board in the case of a jammed servo causing an amperage spike and harming the board. In practice this has worked perfectly fine and despite jams / etc the fuse has never tripped and the board has never shown any issue. How can I control the front-facing LED? Controlling an RGB LED is, apparently, trivial. After soldering up some resistors (for some reason, idk it's what the internet said) you can control the colors straight from a couple PWM pins on an Arduino. This worked quickly and reliably. The terrible mess pictured above was replaced by a nicer heat-shrunk LED later, but the first attempt also worked, it was just ugly. Do I need fans? Early on, I did some heat stress testing by placing the Latte Panda inside the case and completely blocking the front and back with scrap plastic. I let the device sit playing 4K video for several hours. The Latte Panda kept a steady 69c internal temp. Warm, but reasonable. The metal case works as a decent radiator and helps expell the warmth into the surrounding air. This was not completely similar to the final build, but it was close enough that I was confident the onboard fan would keep the device cool enough. Eventually I printed a heat sensitive cartridge, placed it in the front slot and let the device run for a while. This was not informative, but it was cool to look at. Will the metal case block wifi? This is a quick one - no. Nice. How will I fit all the wires? I ended up crimping many custom wires for this project. I invested in some nice wire for another project and used it all over this one. I also bought a wire-stripper and a crimp kit. The most critical and difficult wire to create was the short USB-C wire which attached the Micro SD reader for the carts to the USB headers on the Latte Panda. This required soldering to a small USB-C female port. Most of the other wires were simple - just short :) Week 7-10: 3D Printing! As the prototyping of core elements began to subside it became time to begin iterating on the most complex element of the Floppy8, the designs for the internal brackets / drive which would actually hold all the components securely. As I mentioned before, I used Tinkercad for all the CAD work. This screenshot shows a small percentage of all the digital artifacts of this build. In total I would design and print 201 STL files for this project. Slowly tweaking and iterating fits, tolerance, feels - not to mention iterating on the overall designs as needs came up. I've never 3D printed anything which required multiple parts so figuring out how to make parts which could be screwed together and hold strongly took time. The final internal design uses 11 parts... Button (Dark Grey) - This is used to replace the stock front button with one which has a short throw and flairs at the bottom. Button Mount (Orange) - This bracket holds the small circuit board which the front button is mounted to. Cover Mount (Red) - The original front cover and the button mount screw to this bracket. It has additional holes to attach it to the cart shifter. Cart Shifter (Light Blue) - This L shaped piece has two feet on the backside which the Latte Panda rests on and a slight slope on the front to help shift the cart upward and into the actual bay. Drive Bay (Brown) - This holds the actual cart when inserted along with the Micro SD card reader. It also serves as a mount for the Servo bracket (Pink) and LED bracket (Light Purple / Beige). Servo Bracket (Pink) - This bracket holds the micro-sized servo and screws to the Drive Bay and Cover Mount to add rigidity. LED Bracket (Beige and Light Purple) - This wraps about the LED assembly and holds it against the Cover Mount while being screwed to the Drive Bay. Rear Bracket (Yellow) - This bracket has the feet for the rear Latte Panda screw holes and also has a mount point for the power switch in the top left. It also has screw holes for the back cover. Back Cover (Green) - This cover / vent serve as the back plate for the device. Port Cover (Orange) - This removable port cover has carefully cut holes for the USB-C and HDMI ports on the Latte panda, but because the ports are a bit sunk into the device, the piece pops out in case the cables need to go deeper than the plate allows. Metal Rods (Dark Grey) - These pieces are made from 3mm risers which have been screwed together to the proper length, then sealed in heat-shrink tubing. They are included in the model for context. It's worth nothing all the parts are printed without supports except the Cover Mount which has those small corners cut out to allow for the heads of screws to attach to the metal riser rods. As that corner is an unsupported overhand on two sides, it needed supports. Week 11: Assembly As I printed I would attempt to put pieces together, iterate, print again. I learned to design pieces to be small to save print time and slowly over weeks the device started to emerge. With some assembly working correctly, I was able to check how the Floppy8 would feel with a cart inserted. I wanted it to resemble a Super Nintendo or a Famicom, with the cart label still visible when inserted. One of the last elements to sort out was how to mount the internal switch for the front button (and how to replace the front button with one which matched the plastic of the front cover). I tried various options to recreate the visible element of the button, I tried making a mold and casting it in resin, etc. I eventually just printed it and resigned to sanding it smooth. I also found these adorable PCBs on Amazon which are perfect to mount a single button. I like this close up because it helps to show how fucked up projects are when you look at them up close. In real life the Floppy8 looks fantastic because it fits in the original shell, but up close the internals have lots of bits which were printed bad or screw holes I had to drill out or whatever. These things aren't as perfect as photos make them look, so don't get down on yourself if your project isn't perfect. Anyway, after getting these last elements sorted the whole thing felt fantastic to me - it felt real. I kept iterating then one day, like the flip of a switch, I realized we were almost done. I reprinted all the existing parts in black to help reduce unintended internal LED glow and began putting together the final bits. Putting all the pieces together, including a lot of wire crimping, we ended up with a functional but unhoused Floppy8. Testing at this point confirmed that our cartridges were read, the servo ejects them, the LED reflects the status, it all worked. Perhaps amazingly it also all fit in the case. Week 12: Final Polish and Controllers As the final bits and bobs began to line up I was finally able to do some real testing for heat, cartridge seating, and ~vibe~. Heat wise, amazingly, even with all the junk in the case the Latte Panda stayed cool. However, there are some air vents directly below the cartridge slot which pump a lot of heat up around the cart. Originally the SD cart reader was mounted in the place with hot glue... you can see where this is going. Aside from dripping glue down onto the Panda (which luckily missed the fan by a few mil) the glue was also prone to shifting over the course of a long gaming session or film. I redesigned the cartridge slot as two parts, one which aligns the carts and one which firmly holds the SD reader. This redesign made the feel of inserting a cart way better. I also took this opportunity to work on input devices! I picked up this pair of clone NES controllers. I also purchase the official Nintendo Switch Bluetooth NES controllers, but the after-market clones are actually beige, not grey like the official ones - so I opted to go with those. I made a few small modifications to the controllers. I replaced the cheap looking matte d-pad with the one from the official controllers so it had the imprinted arrows and felt more premium. Then I took a note out of the book of toy-makers I added a bunch of weight to the controller to make it feel more real. I glued in a ton of ball bearings and tbh this makes way more difference than you'd think! The other very small detail was the light bleed from the connection LED. This small detail highlighted the cheapness / thinness of the plastic, so I used a small piece of heat-shrink tubing around the LED to help focus the light. It's another super tiny detail, but it makes everything feel more polished. I also purchased a replacement Fire TV remote (which is just bluetooth and works great with Linux) along with an after-market beige shell for it. Sadly the shell is shipping from China and has a ~2mo wide delivery window. So it hasn't arrived yet. When it does, I'll look into removing branding from the remote's buttons and replacing the shell so we have a proper remote! I considered buying an actual old beige remote, but they realistically didn't have all the buttons I wanted, but if I find the right one, I may migrate the insides of the Fire TV remote to a whole new home in something truly retro! Anyway, we're done... Finally after weeks of tweaking, ups and down, frustrations and almost giving up at least once a day for weeks, it was done. I was able to sit, watch a film, play a game, etc. I made a lot of mistakes, wasted a lot of filament, but at the end of the day I'm very proud that I kept pushing and didn't give up. I'll probably never use the Floppy8 much, but I'm proud to have it on my desk. Thanks for reading and if you're interested in more of this type of thing you should check out my Youtube channel! Back to blog",
    "commentLink": "https://news.ycombinator.com/item?id=40942141",
    "commentBody": "Floppy8 – A Tiny Computer, in a Floppy Drive (2023) (abe.today)159 points by philips 17 hours agohidepastfavorite36 comments farkanoid 12 hours agoThis somehow reminded me of the \"Access A1200\"; A complete and upgraded Amiga 1200 clone that fit in a 5.25\" floppy drive bay [1] I remember the Amiga community absolutely frothing over it a couple of decades ago (myself included). It makes me sad that I can't find any info about it anymore, except for the Soft3 page advertising used versions on the Internet Archive. [1] https://web.archive.org/web/20030421150743/http://www.soft3.... reply nickt 8 hours agoparentFascinating, I’d not heard of this. I did find a bit more info [1]. Seems it was used as a kiosk multimedia playback system. [1] https://amiga.resource.cx/mod/access.html reply shiroiushi 11 hours agoparentprevIt's too bad that the PCB designs and other design specs aren't available for this, and for many other no-longer-produced electronic things as well. reply kjs3 5 hours agoparentprevThere was the Ross SPARCPlug[1] that was a whole Sun SPARCStation 20 clone the size of a 5.25\" hard drive. Now you can have your Unix workstation and Windows machine in one box![2] [1] http://www.bitsavers.org/components/rossTechnology/Ross_SPAR... [2] It was a way better idea than reality, mostly because M-bus CPUs ran really, really hot so the whole thing was kinda flaky and loud. reply buescher 8 hours agoprevAmpro, who would go on to create the PC104 SBC motherboard standard, got their start with a cp/m computer that would mount right on a 5 1/4\" floppy drive: https://oldcomputers.net/ampro-little-board.html reply abeisgreat 5 hours agoprevOh hey that’s my Floppy8! Never woken up to find my stuff on HN before! So cool! I actually just released a video earlier this week about the progression of my SD card based cartridges[0] which started with the floppy cartridges for this. If you like the Floppy8 you’ll probably enjoy seeing the dozen iterations I went through after this blog post haha I’m mostly a YouTuber not a blogger so I have a bunch of other projects on my channel[1] in a similar vein which you might enjoy as well. [0] https://youtu.be/END_PVp3Eds [1] https://youtube.com/@abetoday reply elevaet 56 minutes agoparentYou are absolutely insane. Bravo! reply onimishra 3 hours agoparentprevHeh, was just about to post that this design ended up having issues with chafing between the sdcard read and the sdcard and that the new design (for the reader part at least) addressed this. I love the new version and have been contemplating making one myself. Very nice videos and description, so thank you for sharing your projects reply abeisgreat 3 hours agorootparentHaha yes, I'm still not used to people knowing so much about my projects, thank you for watching! reply trinsic2 3 hours agoparentprevHow did you find the servo to eject the cartridge? You mentioned occasional jams, is that because of the style of cog, or the lack of pressure from the servo? I wonder if there is some tensing or alignment issues from the plastic cog.. I wonder if metal would better. reply abeisgreat 2 hours agorootparentIt actually never jammed I was just afraid that it would! It's just a generic micro servo, I think from Adafruit probably. reply shrx 5 hours agoparentprevWhich temperature-sensitive filament did you use for the thermal test? reply abeisgreat 5 hours agorootparentIt was the AMOLEN Color Change PLA. Sadly I just took a quick look and don’t see it available anymore, but I might be missing it. reply shrx 3 hours agorootparentThanks, I did a quick search and I only saw TPU options. reply nick238 4 hours agoprevI thought it was going to be a computer in a floppy disk that spoofed data into the read heads. I wonder if you could emulate a floppy disk, and if that would let you cheat orders of magnitude more data into a \"disk\". reply kjs3 2 hours agoparentThey did it for cassette tapes, so it should be possible. reply trinsic2 3 hours agoprevNice work. Good perseverance. I like the idea and the way it came about. Some of the design aspects are similar to a project I am working on that i hope to finish before the end of this year. Its a mainboard on the wall for data recovery with custom made housing brackets for usb ports and switches. Im hoping to add a small secondary display to pipe command output to so I can monitor at a glance my recovery jobs. reply fredsmith219 2 hours agoprevI am reminded of Rana systems, who used to make disc drives for Apple II Systems. They announced the new product that was basically an IBM PC built into a disk drive so you could run IBM PC applications on your Apple. They sank the company trying to make it work. At least that’s how I remember it through the fog of memory. The 80s were a while ago. reply Rhapso 5 hours agoprevAdding weight to the controller stands out to me. On one side, the fact materials and electronics are so small and light is awesome, on the other it might be worth making \"bespoke\" electronics like this in heavier and stronger materials like wood or resin. I get that the retro feel demands cheap plastic, but going forward, if we are going to DIY computers like this, we can make them to last a lot longer than the commercial world would consider. reply squarefoot 12 hours agoprevTangentially, if you have some old external USB CD/DVD box collecting dust, especially those with their internal power supply but all will fit, don't throw them away as they are about the perfect case for fitting a SBC inside with the CD drive removed and swapped with a multi card reader+USB ports panel. Bringing out HDMI from the rear in a clean way can be achieved by using \"HDMI panel socket\"s that can be purchased online for cheap. reply dougdimmadome 8 hours agoprevThis thing is beautiful. Makes me want to do something like this but I dont have the time or skill or a 3d printer reply chickenchase-rd 9 hours agoprevLove the build. I think the printer you used was genius in terms of how it gives a super retro look to the label. When I saw how you had to cut the edges I thought I would share that When printing my own custom labels or stickers, I use a local print shop that has a vinyl printer+cutter. Im not sure what media they would use to make this cool retro look, as I order the durable vinyl for my applications so far. (such as custom bottle labels for a friends birthday). No glue is needed and I have saved a fair amount of cash by using their printer instead of buying one. Home versions of these printer+cutters start at around 300-400 bucks. Thanks for posting your build, I have found it very inspirational. reply notpushkin 12 hours agoprevI'm kinda sad he didn't go for regular size SD cards (maybe with microSD through an adapter). With more sparsely spaced contacts it should be easy to make your own adapter which doesn't get torn apart when you eject, and it would look slightly more like those PCB-based cartridges IMO. Still a cool project! I'd love to make something like that. reply hinkley 14 hours agoprevSo what’s the most data we can put on a floppy now? reply Dylan16807 13 hours agoparentIf you mean the shell of a floppy you can fit about 50 microsd cards in there with 1.5TB each. reply hinkley 13 hours agorootparentNo I mean spinning rust on plastic. reply Dylan16807 13 hours agorootparentLaser-guided SuperDisks got up to 240MB, and Zip drives with floating heads got up to 750MB. At that point you're basically operating a hard drive without a dust seal, so it would be pretty hard to go much further even with modern tech. But if you did figure it out, let's use the data density of LTO. If we estimate a 3.5\" floppy as 50 square centimeters, then that's the same area as 40 centimeters of tape, which fits about 7GB on LTO-9. A bit more if you can get double-sided writes to happen and not interfere with each other. Less if you have the dead zones of an actual floppy. reply shiroiushi 11 hours agorootparentA ~5GB floppy disk actually sounds like a pretty useful thing at first, until you realize that you can get USB thumb drives that size almost for free these days. At least the floppy disk would probably be a reasonably secure way to store data for decades, unlike the flash drive, as long as the disk (and the drive) were made with 1980s/early-90s quality and not late-1990s/2000s quality (floppy disks in the 80s were supremely reliable, whereas ones in the late 90s were hilariously unreliable). reply retrac 9 hours agorootparentprevBeyond about ~100 MB historically speaking the industry shifted towards magneto-optical systems. They do store data magnetically - but they're read with lasers, since tracking and physical wear and tear from a head in contact with the medium, were the major limitations when it came to increasing floppy disk density. There's a neat effect where certain kinds of crystals vary their reflectivity depending on magnetization, which is how they work. Sony's MiniDisc was an example but in the '80s and '90s there were many competing variations. reply password4321 7 hours agorootparentprev2GB https://en.m.wikipedia.org/wiki/Jaz_drive reply pacifika 9 hours agoprevReally nice write up and amazing skillset! reply dwighttk 8 hours agoprevImagine a Beowulf cluster of these in a Zip drive reply metadat 13 hours agoprev [–] That's a 5.25\" inch floppy drive.. not a real floppy drive, to me. 90's kid. https://en.wikipedia.org/wiki/Floppy_disk Edit: Thanks @alberto_ol, link fixed. reply richrichardsson 9 hours agoparentI'm not sure what you're getting at here. The OP is a 3.5 inch floppy drive, I don't think I ever saw a 5.25 inch drive for Amiga. I remember some people used to day 3.5 inch floppies \"aren't floppy\", but it was just the case that was more rigid than the prior formats; the actual rust-on-plastic was still \"floppy\", opposed to a hard drive platter that is rigid. reply metadat 2 hours agorootparentYeah, you are correct! I was confused, somehow I thought it was a 5-1/4\" drive until I came back to it this morning. I'm glad this has been downvoted, I even appreciate if it got flagged to death, because it was a dumb mistake. Please accept my apologies for the noise. reply alberto_ol 13 hours agoparentprev [–] I think there is a missing \".\" in your link. The correct link should be: https://en.wikipedia.org/wiki/Floppy_disk reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Floppy8 is a microcomputer and cartridge system built inside a floppy drive, capable of playing 4K movies and games with wireless controllers and motorized cartridge ejection.",
      "The project involved repurposing an Amiga 1010 disk drive, designing a new media format, and extensive 3D printing and electrical engineering to fit all components inside the drive.",
      "The creator used a Latte Panda 3 Delta computer and custom 3D-printed cartridges, overcoming challenges in size constraints, eject mechanisms, and LED control to complete the project."
    ],
    "commentSummary": [
      "Floppy8 is a tiny computer built into a 3.5\" floppy drive, reminiscent of past compact computing projects like the Amiga 1200 clone and Ross SPARCPlug.",
      "The creator, abeisgreat, shared a video detailing the project, including SD card-based cartridges and other technical aspects, sparking interest and admiration in the tech community.",
      "Discussions highlight the nostalgia and innovation in fitting computers into small form factors, with references to historical attempts and modern DIY projects."
    ],
    "points": 159,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1720748705
  },
  {
    "id": 40941853,
    "title": "AuraFlow v0.1: a open source alternative to Stable Diffusion 3",
    "originLink": "https://blog.fal.ai/auraflow/",
    "originBody": "Introducing AuraFlow v0.1, an Open Exploration of Large Rectified Flow Models cloneofsimo, Team Features & Labels Jul 12, 2024 • 7 min read Open-source AI is in jeopardy. As community interest in AI models skyrocketed over the past year, we noticed that development of new open-source foundational models came to a halt. Some even boldly announced that open-source AI is dead. Not so fast! We are excited to present you the first release of our AuraFlow model series, the largest yet completely open sourced flow-based generation model that is capable of text-to-image generation. AuraFlow is a reaffirmation of the open-source community's resilience and relentless determination. Auraflow is exceptionally good at prompt following. Prompt1: \"A photo of a beautiful woman wearing a green dress. Next to her there are three separate boxes. The Box on the Right is filled with lemons. The box in the Middle has two kittens in it. The Box on the Left is filled with pink rubber balls. In the background there is a potted houseplant next to a Grand Piano.\" Prompt2: \"a cat that is half orange tabby and half black, split down the middle. Holding a martini glass with a ball of yarn in it. He has a monocle on his left eye, and a blue top hat, art nouveau style How do I use it? If you want to try out a few quick prompts, go to fal’s model gallery to start playing around. If you want to build some cool Comfy workflows with the model, get the latest version of Comfy and download the model weights from our HuggingFace page. We would love to give a huge shout out to ComfyUI and the HuggingFace 🤗 diffusers 🧨 teams for supporting AuraFlow natively on Comfy and diffusers on day 0! A fine selection of AuraFlow v0.1 generations How this collaboration happened Simo is one of our favorite researchers in the wild world of generative media models. You may know him from the amazing adaptation of the LoRA paper for text-to-image models. Few months ago, Simo wanted to implement MMDiT from scratch, and see if he would be able to reproduce it. His initial attempts at https://github.com/cloneofsimo/minRF and its initial result Lavenderflow-v0 came out to be promising. Soon, he found various aspects that could be optimized to train the model on a larger scale more efficiently. Timing couldn’t have worked out better. Right around this time, we were convinced that a SOTA open-sourced model is the way forward for this space to move forward. We wanted to bring serious resources and compute to scale up the model. We were aligned very well, and thus begun the collaboration. AuraFlow demonstrates that collaborative, transparent AI development is not only alive but thriving, ready to tackle the challenges and opportunities of tomorrow's AI landscape. Technical Details Here, we wanted to share some initial technical details that stand out. We are planning on following up with a more detailed report and possibly a paper as well. 1. MFU as a first-class citizen Most layers don’t need MMDiT Blocks: While MMDiT achieved good performance, we found that removing many layers to just be single DiT block were much more scalable and compute efficient way to train these models. With careful search in the small-scale proxy, we’ve removed most of the MMDiT blocks and replaced them with large DiT Encoder blocks. These improved the model flops utilization at 6.8B scale by 15%. Number of Double Layers and Optimal Learning Rate Improved training with torch.compile: At fal, we are already big fans of Torch Dynamo + Inductor, and build on top of this tooling (with a custom dynamo backend) to run our inference workloads super fast (and efficiently utilizing the underlying hardware). Since PT2’s torch.compile is able to handle both forward and backwards passes, AuraFlow’s training was further optimized with its primitives on each layers forward method, and further able to improve MFU by extra 10% ~ 15% depending on the stage. 2. Unlock zero-shot learning rate transferIt is clear that we are not Meta, and would like to have very good hyperparameters even without sweeping them. Fortunately, we noticed MMDiT architectures were also zero-shot LR transferred with maximal-update-parameterization was utilized.Compared to SP, muP was clearly the winner in terms of predictability of learning rate at scale. Standard Parametrization Maximal Update Parametrization 3. Re-captioned, everything.It is common trick to recaption everything to make sure there are no faulty text conditions in the dataset. We used our in-house captioner & external captioned dataset to train these models, which improves the quality of the instruction-following significantly. We followed the DALL·E 3 approach to the extreme, and we had no captions that were alt-texts. 4. Wider, shorter, better!To further investigate the optimal architecture, we were interested into making a fatter model, i.e., making the architecture overall utilize largest matmul divisible by 256. This lead us into searching for optimal aspect ratio under optimal learning rate found by muP.With these findings, we were confident that aspect ratio of 20 ~ 100 is indeed suitable at larger scale, which was similar with findings from Scaling Laws for Autoregressive Generative Modeling. We ended up using 3072 / 36, which resulted in model size of 6.8B parameters. Number of Parameters / Loss In the end, we did the best of our ability to improve and effectively find the optimal configurations for large scale training. Utilizing the findings from above, we were able to train a text-to-image model from scratch in our largest possible settings for 4 week of compute time, including 256x256, 512x512, 1024x1024 pre-training and aspect ratio fine-tuning. Final model achieves a GenEval score of 0.63~0.67 during pretraining, and similarly 0.64 after 1024x1024 pretraining. But with prompt-enhancement pipeline similar to DALL·E 3, we were able to achieve 0.703! Prompt for prompt-enhancement A caption is a way that a person would describe an image separated by commas when necessary. All in lower case. Expand the input below into a more detailed caption without changing the original relative positions or interactions between objects, colors or any other specific attributes if they are disclosed in the original prompt. Clarify positional information, colors, counts of objects, other visual aspects and features. Make sure to include as much detail as possible. Make sure to describe the spatial relationships seen in the image. You can use words like left/right, above/below, front/behind, far/near/adjacent, inside/outside. Make sure to include object interactions like \"a table is in front of the kitchen pot\" and \"there are baskets on the table\". Also describe relative sizes of objects seen in the image. Make sure to include counts of prominent objects in the image, especially when there is humans in the image. When its a photograph, include photographic details like bokeh, large field of view etc but dont just say it to say something, do it only when it makes sense. When its art, include details about the style like minimalist, impressionist, oil painting etc. Include world and period knowledge if it makes sense to, like 1950s chevrolet etc. Challenges of distributed training on multi-modal data One of the harshest realities of training image models is that, unlike LLMs, the modality of the data itself can be a real pain to deal with. During AuraFlow’s training, we leveraged our expertise from dealing with distributed storage as well as managing a large fleet of thousands of GPUs. Some of this expertise was directly transferable from production grade inference/fine-tuning systems, where we were able to use open source projects like JuiceFS and some were more novel challenges like how do you stream massive amounts of data in and out of multiple nodes while leveraging local NVME space as a staging ground to not to reduce the MFU. Be on the lookout for a detailed post on how we choose our storage mediums, where we trained this model, how we evaluated GPU performance and managed large clusters! What is next? We are not done training! This model is an initial release to kickstart some community engagement. We will continue training the model and apply our learnings from this first attempt. We also noticed that smaller models or MoE’s might be more efficient for consumer GPU cards which have a limiter amount of compute power, so follow closely for a mini version of model that is still as powerful yet much much faster to run. In the meantime, we encourage the community to experiment with what we are releasing today. Our goal is to make this model a standard backbone that other innovative work can be built on top of. We look forward to community contributions. If you want to train finetunes, IP-Adapters, or quantizations of the current model, we are happy to support you in any way we can. There is already a vibrant community around fal and Aura models in our Discord. We invite you to join if you want to get involved. For business inquiries, please email us at hello@fal.ai 😄",
    "commentLink": "https://news.ycombinator.com/item?id=40941853",
    "commentBody": "AuraFlow v0.1: a open source alternative to Stable Diffusion 3 (fal.ai)137 points by treesciencebot 18 hours agohidepastfavorite31 comments viraptor 14 hours agoPasses the \"woman on grass\" test ; - ) Seriously though, there are some minor hand issues and a rare missing body part. \"Correct anatomy, no missing body parts.\" seems to fix it mostly. Still pretty good for an early 0.1 announcement. Following full sentences is pretty good. Although this: \"A photo of a table. On the table there's a green box on the right, a red ball on the left. There's a yellow cone on the box.\" keeps putting the cone on the table. Not trained on naked bodies though - generates blob monsters instead. reply konata390 14 hours agoparentCan you give me your prompt that generates passable humans? Even stuff that worked on SD3 generates flesh demons for me in the playground linked in the post. reply viraptor 13 hours agorootparentYou're right. Turns out I've just been lucky and got 5/5 good results. Every time I try now, I get blob demons as well. The joys of random generation... reply kyriakos 13 hours agoparentprevIs there any model that can actually generate realistic naked human body? Thought they are all deliberately avoiding the subject in order to steer away from compliance issues. reply RobotToaster 13 hours agorootparenthttps://civitai.com/models has plenty of fine tuned models. reply viraptor 11 hours agorootparentprevSure, there's lots of people really into that. Discords for apps like DrawThings have NSFW sections where people share models/processes/results. reply lukan 10 hours agorootparentWhich is not surprising, since porn is a multi billion dollar market. So there are hobby enthusiasts going to optimize their own kinks - but also professional players who want a share of the cake. reply gowld 48 minutes agorootparentProfessional players don't like being banktrupted and going to jail. reply GaggiX 13 hours agorootparentprevGo to CivitAI, many models are able to generate naked bodies, they are SD finetuned models and you can download them to run them locally. reply Der_Einzige 10 hours agorootparentprevWell over more than 50% of all SD models in existence have a focus on NSFW. It's more like \"Which models DON'T generate realistic hardcore porn?\" reply crngefest 11 hours agorootparentprevOh boy you know nothing. There are hundreds of models for every lewd aspect that you can think of. It’s the main area of “research” and generating porn has really supercharged the Sd development reply smusamashah 12 hours agoprevPrompt adherence is great. I copied a few prompts from ideogram (which also adheres to prompt) and results were good until they involve female bodies. This for example https://ideogram.ai/g/ENMWd7PrQ32dIWSF91uMJQ/2 comes out exposing that training didn't have enough naked bodies. Prompt adherence is very very good otherwise. Can try top images of the day/hour from ideogram to test. reply Mashimo 9 hours agoparentJust FYI, the ideogram.ai link is behind a login. reply stavros 9 hours agorootparentNot only that, but the only ways to sign up are either Google or Apple. What dystopia is this? reply halr9000 5 hours agorootparentworth checking out. ideogram has the best creation of english text out there. Nightcafe (https://creator.nightcafe.studio/) sturck a deal with them recently and added support for the ideogram model in their tool also, if you want to try that, which I also highly recommend for general SD online image creation services. reply stavros 5 hours agorootparentI'll try Nightcafe, thanks! reply skybrian 3 hours agoprevFails on “piano keyboard” (shows a full piano) and “close up of piano keyboard,” (bizarre duplicate keyboard monstrosity.) It’s a difficult prompt. Nobody gets the grouping of black keys right. Maybe someday? reply halr9000 6 hours agoprevIn case you missed it, the authors were pretty smart to include that folded section in the middle, \"Prompt for prompt-enhancement\". I slapped that into gpt (https://chatgpt.com/share/2e53403e-4bd7-4138-ac34-55378e2ed3...) and made a few prompts. Ran those on their online demo. Initial impressions: - prompt adherence is really good - it's somewhere between SD15 and SDXL at creating pictures of text - aesthetic quality is good, but leaves some to be desired Gonna play more with it in ComfyUI. reply executesorder66 10 hours agoprevAIs are still not able to understand negations. Try \"ramen without egg\" or \"ramen with no egg\" and it will show ramen WITH egg. Or \"man without striped shirt\" will give \"man WITH striped shirt\" reply viraptor 9 hours agoparentIt's not trained for it, because that use case is handled differently. It would be mostly a waste of time to train the concept compared to other things you want to achieve. Instead you put things you don't want in the negative prompt. This example doesn't expose the option, but you can try it here for a different model: https://huggingface.co/spaces/gokaygokay/Kolors Set the seed to 0 and prompt to \"man in a loud shirt\" - you get flowers. Sweet the negative prompt to \"floral shirt\" - no not flowers. Sentence processors can definitely understand negation, (any non-trivial LLM can) but it would be a waste of time to train that in the image generators -vs- making other ideas better. reply gowld 46 minutes agorootparentWhy can't an imagen generate run a tiny little automatic text-to-text rewrite first, to apply these special linguistic rules? reply pyinstallwoes 9 hours agoparentprevThat’s what negative prompt is for. Stable diffusion also isn’t like llms. LLMs certainly understand negation. reply executesorder66 9 hours agorootparentI did mean AI's in general, so I have edited my original post. > That’s what negative prompt is for. This is what I mean by it \"not understanding negations\" You need whole separate prompt, just to say you want e.g. \"ramen without egg\" instead of just saying it in a single prompt that it understands. reply viraptor 5 hours agorootparentYou're not correct about AIs in general. Both chat LLM models and sentence embeddings can handle negation just fine. (Ask any chat \"what clothes would a person wear if they weren't wearing a hat\") Here it was simply not trained for that purpose. Maybe it wasn't worth it, maybe the creators thought that the negative prompt is enough, maybe the time was better spent on other examples. They way, it's not AIs in general, and it's not a tech limitation. reply spywaregorilla 5 hours agorootparentprevYou are not correct. LLMs understand \"ramen without egg\". Image gen models generally do not as this is not how images are described. If you want to generate ramen without egg, you'll want _negative weighted_ prompts. \"eating ramen, (egg:-1)\" reply GaggiX 7 hours agoparentprev>AIs are still not able to understand negations. AIs are able to understand negations, just ask an LLM a question. Text-to-image models are the ones that struggle the most with this, they usually do not have a very nuanced understanding of text. reply gorkemyurt 13 hours agoprevhttps://x.com/ostrisai/status/1811620901420429441 reply notachatbot1234 11 hours agoparent> The prompt comprehension is incredible! #auraflow > \"a cat that is half orange tabby and half black, split down the middle. Holding a martini glass with a ball of yarn in it. He has a monocle on his left eye, and a blue top hat, art nouveau style \" Plus an image that somewhat resembles that prompt. The cat has a human-like hand with a chopped off thumb and 6 fingers in total, differently colored eyes, a branch in front of its face, the ball of yarn is somehow floating in mid-air.] reply viraptor 10 hours agorootparentThese are somewhat valid issues. But given the currently available open models, this is a massive improvement. The human-like hand and changing the styles on the sides of the head isn't even bad - those are valid artistic choices you'd see on similar illustrations - they're just badly executed here. reply Kiro 5 hours agorootparentprevSomewhat resembles? Come on. reply stale2002 9 hours agoprev [–] So, now that this is released are we no longer going to have pendant people complaining that this \"isn't real open source\"? Here is your model, complainers. I'm not really sure why you'd be so insistent on that, as opposed to just fine tuning the \"totally not open source, but instead just open weights\" models. But go ahead, I guess. Now we can get back to talking about capabilities, usage, and results, as opposed to arguing about the definition of words. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AuraFlow v0.1 is an open-source, flow-based text-to-image generation model, demonstrating the resilience of the open-source AI community.",
      "The model excels in generating detailed images from text descriptions and incorporates advanced techniques like zero-shot learning rate transfer and re-captioning.",
      "With 6.8 billion parameters, AuraFlow was trained over four weeks, achieving high GenEval scores, and future plans include further training and developing efficient models for consumer GPUs."
    ],
    "commentSummary": [
      "AuraFlow v0.1 is an open-source alternative to Stable Diffusion 3, showing promise despite being an early release.",
      "The model performs well on simple prompts but struggles with complex ones and negations, indicating areas for improvement.",
      "Discussions highlight the model's limitations in generating realistic human bodies and handling nuanced text prompts, reflecting ongoing challenges in AI image generation."
    ],
    "points": 137,
    "commentCount": 31,
    "retryCount": 0,
    "time": 1720744963
  },
  {
    "id": 40940241,
    "title": "Beating the L1 cache with value speculation (2021)",
    "originLink": "https://mazzo.li/posts/value-speculation.html",
    "originBody": "← 2021-07-20 Beating the L1 cache with value speculation If we have a heuristic to guess some value cheaply, we can remove a data dependency in a tight loop using the branch predictor. This allows the CPU to run more instructions in parallel, increasing performance. If this explanation does not make much sense to you, keep reading to learn about some of the magic making your CPU fast! Per Vognsen’s twitter feed is full of neat low-level curiosities, usually leveraging CPU features for some performance benefit. Recently he tweeted about a trick that I had never heard of – value speculation.1 The trick exploits the branch predictor to guess values, enabling more instruction parallelism and therefore removing a bottleneck on the L1 cache. Note that the bottleneck is not due to L1 cache misses, but on L1 cache hits introducing unwanted data dependencies. Per, in turn, referenced a blog post by Paul Khuong with a real-world example deploying this trick. Paul, in turn, references side-channel attacks.↩︎ In this post I explain the machinery involved, including a primer on branch prediction and CPU caches, so that anybody with a passing knowledge of C and how code is executed on CPUs should be able to follow. The code for the post is available here. All the numbers are from a Xeon E5-1650 v3, an Intel Haswell processor with L1 / L2 / L3 cache of 32kB, 256kB, and 15MB respectively. The code was compiled with clang -O3, and not with gcc, for reasons explained later. Before starting, I’d like to stress that L1 cache hits are almost certainly not the bottleneck of your application! This is just a very neat trick that illuminates some CPU features, not a guide on how to improve the performance of your average piece of C code. The setup – summing linked lists # We have a simple linked list data type, and a function summing all the elements of a given linked list: typedef struct Node { uint64_t value; struct Node *next; // NULL for the last node } Node; uint64_t sum1(Node* node) { uint64_t value = 0; while (node) { value += node->value; node = node->next; } return value; } So far so good. Our test case works as follows: build a linked list where the nodes live sequentially in contiguous memory, then see how long it takes to sum them all up: // Allocate 5MB of linked list nodes, and link them sequentially, with // random data in the `value`s. uint64_t n = 312500llu; // 312500 * sizeof(Node) = 312500 * 16 bytes = 5000000 bytes Node *nodes = malloc(n * sizeof(Node)); for (uint64_t i = 0; i value; node = node->next; } return value; } and its very readable assembly version: ; rdi = node and rax = value. ; rax is the return value register (we're returning value) sum1: xor rax, rax ; value = 0 test rdi, rdi ; if node is NULL, exit, otherwise start loop je end loop: add rax, qword ptr [rdi] ; value += node->value mov rdi, qword ptr [rdi + 8] ; node = node->next test rdi, rdi ; if node is not NULL, repeat loop, jne loop ; otherwise exit end: ret The loop body is made out of 4 instructions, the last of which a jump. Without special measures, every instruction up to the jne must be executed before proceeding to the next instruction, since we need to know if we’ll go to the beginning of the loop or continue. In other words the conditional jump would introduce a barrier in the instruction level parallelism internal to the CPU. However, executing many instructions at once is so important that dedicated hardware – the branch predictor – is present in all modern CPUs to make an educated guess on which way we’ll go at every conditional jump. The details of how this works are beyond the scope of this blog post, but conceptually your CPU observes your program as it runs and tries to predict which branch will be taken by remembering what happened in the past.5 Apart from the ever useful Agner Fog (see Section 3 of the microarchitecture document), Dan Luu has a nice blogpost explaining less dryly various ways of performing branch prediction.↩︎ Even without knowing much about the branch prediction, we expect the predictor to do a great job for our test case – we always go back to the beginning of the loop apart from when we stop consuming the list. On Linux, we can verify that this is the case with perf stat: $ perf stat ./value-speculation-linux ... 2,507,580 branch-misses # 0.04% of all branches The branch predictor gets it right 99.96% of the time. So the CPU can parallelize our instructions with abandon, right? …right? Data dependencies tripping us up # Let’s focus on the loop body of sum1: ; rdi = node and rax = value. loop: add rax, qword ptr [rdi] ; value += node->value mov rdi, qword ptr [rdi + 8] ; node = node->next test rdi, rdi ; if node is not NULL, repeat loop, jne loop ; otherwise exit To increment value (rax), we need to know the value of node (rdi), which depends on the mov in the previous iteration of the loop. The same is true for the mov itself – it is also dependent on the result of the previous mov to operate. So there’s a data dependency between each iteration of the loop: we must have finished reading node->next ([rdi + 8]) at iteration n before we can start executing the add and mov at iteration n+1. Moreover, reading the node->next ([rdi + 8]) is slower than you might think. Diagram showing the CPU caches for the processor used in this post. Generated with lstopo. Modern CPUs are a lot better at adding numbers than reading from memory. For this reason, a series of fast caches exist between the CPU and main memory. All reading and writing from main memory normally goes through the cache – if the data we are interested in is not already present, the CPU will load a chunk of memory (a “cache line”, 64 bytes on x86) which contains our desired data into the cache.6 The fastest cache is usually called L1 (successive caching layers being predictably called L2, L3, …). Our setup is the best-case scenario when it comes to CPU caches – we read a bunch of memory sequentially, utilizing every byte along the way. However, even if the L1 cache is very fast, it is not free: it takes around 4 CPU cycles to read from it. This will make our mov and add take at least 4 cycles to complete. The other two instructions, je and test, will take only one cycle.7 So the number of cycles needed to go through a single loop iteration is bounded by the 4 cycles it takes to read from L1 cache. The data I get from the Xeon I tested the program with is roughly consistent with this: I say “normally” because the cache can be avoided using streaming SIMD instructions, which can write or copy memory bypassing the cache. However these methods are opt-in, and by default all memory goes through the cache.↩︎ Again, Agner Fog’s page on performance is the best resource I could find to source these numbers. For example, if one wanted to find these numbers for a Haswell CPU: The L1 latency (4 cycles) is in section 10.11 of the microarchitecture guide; The numbers of cycles it takes to execute mov, add, test, and jne are in the Haswell section of the instruction tables. ↩︎ 16kB, 10000 iterations sum1: 8465052097154389858, 1.12us, 14.25GB/s, 3.91 cycles/elem, 1.03 instrs/cycle, 3.48GHz, 4.01 instrs/elem 128kB, 10000 iterations sum1: 6947699366156898439, 9.06us, 14.13GB/s, 3.95 cycles/elem, 1.01 instrs/cycle, 3.49GHz, 4.00 instrs/elem 5000kB, 100 iterations sum1: 2134986631019855758, 0.36ms, 14.07GB/s, 3.96 cycles/elem, 1.01 instrs/cycle, 3.48GHz, 4.00 instrs/elem 4294MB, 1 iterations sum1: 15446485409674718527, 0.43 s, 9.94GB/s, 5.60 cycles/elem, 0.71 instrs/cycle, 3.48GHz, 4.00 instrs/elem The important numbers are cycles/elem and instrs/cycle. We spend roughly 4 cycles per list element (that is to say, per loop iteration), corresponding to a throughput of roughly 1 instruction per cycle. Given that the CPU in question is designed for a throughput of 4 instructions per cycle, we’re wasting a lot of the CPU magic at our disposal, because we’re stuck waiting on the L1 cache. Value speculation bailing us out # We finally get to the trick. As discussed, we are stuck waiting on reading what the next node address is. However, in our setup we allocate the list in a contiguous block of memory, and therefore the nodes are always next to each other. So here’s the key idea: try to guess the next node by just bumping the previous value. If the guess is wrong, set the node to the “real” next value. In C, this is how it would look like: uint64_t faster_sum(Node* node) { uint64_t value = 0; Node* next = NULL; while (node) { value += node->value; next = node->next; // Guess the next value node++; // But fix it up if we guessed wrong (in case the nodes are not // next to each other). if (node != next) { node = next; } } return value; } This looks quite bizarre. We are still reading node->next in the comparison node != next to make sure our guess is right. So at first glance this might not seem like an improvement. This is where the branch predictor comes in. In the case of lists where most nodes are next to each other (as is the case in our test code), the branch predictor will guess that the if (node != next) { ... } branch is not taken, and therefore we’ll go through loop iterations without having to wait for the L1 read. Note that when the branch predictor is wrong (for example when the list ends, or if we have non-contiguous nodes) the CPU will need to backtrack and re-run from the failed branch prediction, which is costly (15 to 20 cycles on our processor8). However, if the list is mostly contiguous, the trick works and makes our function 50-200% faster. See “Misprediction penalty” for Haswell processor in Agner Fog’s microarchitecture document.↩︎ However there is one last challenge remaining to reach the final code and show you numbers – convincing compilers that our code is worth compiling. Getting compilers to emit the right code # Let’s go back to the code we showed for value speculation in C: uint64_t faster_sum(Node* node) { uint64_t value = 0; Node* next = NULL; while (node) { value += node->value; next = node->next; node++; if (node != next) { node = next; } } return value; } Both gcc and clang easily deduce that the guessing is semantically pointless, and compile our trick away, making the compiled version of faster_sum the same as sum1. This is an instance where the compiler smartness undoes human knowledge about the underlying platform we’re compiling for. Per Vognsen’s gist uses the following trick to get compilers to behave – this is the first improvement to our sum1, sum2: static uint64_t sum2(Node *node) { uint64_t value = 0; while (node) { value += node->value; Node *predicted_next = node + 1; Node *next = node->next; if (next == predicted_next) { // Prevent compilers optimizing this apparently meaningless branch away // by making them think we're changing predicted_next here. // // This trick, however, does not work with GCC, only with clang. GCC here // derives that `next` and `predicted_next` are the same, and therefore // merges them into the same variable, which re-introduces the data // dependency we wanted to get rid of. asm(\"\" : \"+r\"(predicted_next)); node = predicted_next; } else { node = next; } } return value; } However gcc still doesn’t fully fall for it, as explained in the comment.9 Moreover, clang’s generated loop is not as tight as it could, taking 10 instructions per element. So I resorted to manually writing out a better loop, which we’ll call sum3:10 This is why I stuck to clang for this post. I don’t know what compiler Per is using for his tests.↩︎ Here I show the assembly version in Intel syntax, but in the code I write inline assembly, using AT&T syntax since it is better supported.↩︎ ; rax = value, rcx = next, rdi = node ; Note that rax is the return value register (we are returning the value) sum3: xor rax, rax ; value = 0 xor rcx, rcx ; next = NULL test rdi, rdi ; if node is null, go to the end, je end ; otherwise start loop loop_body: add rax, qword ptr [rdi] ; value += node->value mov rcx, qword ptr [rdi + 8] ; next = node->next add rdi, 16 ; node++ cmp rcx, rdi ; if node is equal to next, je loop_body ; restart loop, otherwise fix up node mov rdi, rcx ; node = next test rdi, rdi ; if node is not NULL restart the loop, jne loop_body ; otherwise exit. end: ret The code relies on the fact that node can’t be NULL after we increment it if it is equal to next, avoiding an additional test, and taking only 5 instructions per element (from loop_body to je loop_body in the happy path).11 The original version of sum3 took 6 instructions per cycle, until Rihalto pointed out a needless jump.↩︎ Results # These are the final numbers for our four functions: 16kB, 10000 iterations sum1: 8465052097154389858, 1.12us, 14.25GB/s, 3.91 cycles/elem, 1.03 instrs/cycle, 3.48GHz, 4.01 instrs/elem sum2: 8465052097154389858, 0.57us, 27.97GB/s, 1.99 cycles/elem, 5.02 instrs/cycle, 3.48GHz, 10.01 instrs/elem sum3: 8465052097154389858, 0.36us, 44.96GB/s, 1.24 cycles/elem, 4.05 instrs/cycle, 3.48GHz, 5.01 instrs/elem 128kB, 10000 iterations sum1: 6947699366156898439, 9.05us, 14.14GB/s, 3.95 cycles/elem, 1.01 instrs/cycle, 3.49GHz, 4.00 instrs/elem sum2: 6947699366156898439, 4.51us, 28.38GB/s, 1.97 cycles/elem, 5.09 instrs/cycle, 3.49GHz, 10.00 instrs/elem sum3: 6947699366156898439, 3.79us, 33.80GB/s, 1.65 cycles/elem, 3.03 instrs/cycle, 3.49GHz, 5.00 instrs/elem 5000kB, 100 iterations sum1: 2134986631019855758, 0.35ms, 14.09GB/s, 3.95 cycles/elem, 1.01 instrs/cycle, 3.48GHz, 4.00 instrs/elem sum2: 2134986631019855758, 0.19ms, 26.27GB/s, 2.12 cycles/elem, 4.72 instrs/cycle, 3.48GHz, 10.00 instrs/elem sum3: 2134986631019855758, 0.17ms, 28.93GB/s, 1.93 cycles/elem, 2.60 instrs/cycle, 3.48GHz, 5.00 instrs/elem 4294MB, 1 iterations sum1: 15446485409674718527, 0.44 s, 9.66GB/s, 5.76 cycles/elem, 0.69 instrs/cycle, 3.48GHz, 4.00 instrs/elem sum2: 15446485409674718527, 0.33 s, 13.19GB/s, 4.22 cycles/elem, 2.37 instrs/cycle, 3.48GHz, 10.00 instrs/elem sum3: 15446485409674718527, 0.30 s, 14.20GB/s, 3.91 cycles/elem, 1.28 instrs/cycle, 3.47GHz, 5.00 instrs/elem The numbers are provided by the Linux perf_event_open syscall. The first three datasets are meant to fit in the L1 / L2 / L3 cache. In those cases, the improvements are very pronounced, and sum3 is crunching the data at around 4 instructions per cycle, which should be close to the limit on the processor I tested the code on. When the data does not fit in the cache, the bottleneck becomes filling it, and we process the data at roughly 15 GB/s. I believe that this is as fast as one can go with “simple” single-threaded reading from RAM, and it’s consistent with data from sysbench: $ sysbench memory --memory-block-size=1G --memory-oper=read --threads=1 run ... 102400.00 MiB transferred (15089.75 MiB/sec) ... The RAM-reading speed could probably be improved using SIMD streaming instructions or by reading from multiple threads, although the implementation would be significantly more complicated. And so we complete our journey into this low-level trick! If you want more of this, I can’t reccomend Per’s account enough – figuring out how his tricks works has been very educational. Thanks to Alexandru Scvortov, Niklas Hambüchen, Alex Appetiti, and Carter T Schonwald for reading drafts of this post. Niklas also clarified some details regarding RAM speeds, and suggested sysbench to measure single threaded RAM reading speed in particular. Also thanks to Per Vognsen and Jason Rohem for spotting a few typos, and to Rihalto for pointing out a better sum3 and some misleading wording. Bonus track – a compiler friendly C version # Alexander Monakov suggested a more robust C function which works well with both gcc and clang, performs as well as sum3, and does not resort to any assembly: uint64_t sum5(Node *node) { uint64_t value = 0; Node *next = NULL; for (; node; node = node->next) { for (;;) { value += node->value; if (node + 1 != node->next) { break; } node++; } } return value; } f@mazzo.li · twitter · mastodon · source",
    "commentLink": "https://news.ycombinator.com/item?id=40940241",
    "commentBody": "Beating the L1 cache with value speculation (2021) (mazzo.li)125 points by nickdevx 7 hours agohidepastfavorite29 comments mwkaufma 2 hours agoThe optimization is the linear memory layout of the nodes -- value speculation is decoration. reply pfedak 16 minutes agoparentThe example is poorly chosen in terms of practicality for this reason, but otherwise, no, this is a poor summary that misses something interesting. The memory layout isn't changing in the faster versions, and there are no additional cache misses. It's easy to convince yourself that the only difference between the naive linked list and assuming linear layout is the extra pointer load - but TFA shows this is false! The execution pipeline incurs extra costs, and you can influence it. reply metadat 2 hours agoprevIn case your knowledge of the mechanics of `struct' vs `typedef struct' in C are rusty like mine, here are nice refreshers: https://stackoverflow.com/a/23660072 https://stackoverflow.com/a/1675446 reply queuebert 4 hours agoprevI appreciate the elegant blog design. Reminds me of Edward Tufte's books. reply candiddevmike 4 hours agoparentIt's impressive that it doesn't have a mobile view and still looks great. reply ahoka 3 hours agorootparentWhat? It looks horrible. reply AnthOlei 3 hours agoparentprevHa, I think this site is styled by a single-sheet CSS called Tufte.css reply notpushkin 3 hours agorootparentI don't think it is. In Tufte CSS, sidenotes are implemented using float: right [1], while here CSS Grid is used instead. [1]: https://github.com/edwardtufte/tufte-css/blob/957e9c6dc3646a... reply oersted 2 hours agoprevI enjoyed the read and it taught me new things, I just wish that the reference example would have some minimal practical value. I don’t think there is any reasonable scenario where you would be using a linked list but the memory is contiguous most of the time. reply sfink 1 hour agoparentIt's not that unlikely. - Create the list in some weird order. You know you're going to traverse it a bunch, so you sort it. - The list is in creation order, and creation is in allocation order. Once in a while you go back and insert or delete a small handful of nodes, hence the linked list. - There is a natural sort order that you can make contiguous, but you support relatively rare alternative orderings and optimize for the natural order. Then again, I pretty much agree with you. I think it's a clever trick, but I can't come up with a time when I would use it. Largely that's probably because if the memory is contiguous most of the time, then you should probably be using a vector instead. You can insert/remove by shifting stuff around to handle the rare cases that require the linked list. If performance cliffs are a problem, you can mitigate with a segmented array. reply bobmcnamara 6 hours agoprevThe nodes are adjacent. reply gergo_barany 7 minutes agoparentThe nodes are adjacent in sum1. The nodes are adjacent in sum2, and sum2 executes more instructions than sum1, and sum2 is faster than sum1. The nodes are adjacent in sum3, and sum3 executes more instructions than sum1, and sum3 is faster than sum1. reply mnw21cam 5 hours agoprevThe article states that the CPU has a limit of 4 instructions per cycle, but the sum2 method issues 5 instructions per cycle. Presumably one of them (maybe the increment) is trivial enough to be executed as a fifth instruction. reply rostayob 3 hours agoparentgpderetta is right -- test/cmp + jump will get fused. uiCA is a very nice tool which tries to simulate how instructions will get scheduled, e.g. this is the trace it produces for sum3 on Haswell, showing the fusion: https://uica.uops.info/tmp/75182318511042c98d4d74bc026db179_... . reply xiphias2 2 hours agorootparentIt's cool, I would love to have this for ARMv8 Mac reply gergo_barany 13 minutes agorootparentThe LLVM project has a tool called llvm-mca that does this. Example: https://gcc.godbolt.org/z/7zcova1ce The version in the Compiler Explorer wouldn't work on AArch64 without an -mcpu flag and I didn't know what to pass, so I copied -mcpu=cyclone from https://djolertrk.github.io/2021/11/05/optimize-AARCH64-back.... You'd have to look up the correct one for your Mac's CPU. reply gpderetta 3 hours agoparentprevsome nominally 4-wide intel cpus can execute 5 or 6 instructions per cycle when macrofused. For example a cmp and a conditional jXX can be macrofused. reply gpderetta 3 hours agoprevNice article! Incidentally, value speculation (or prediction) is a way to break causality in concurrent memory models. reply IshKebab 4 hours agoprevNeat trick. Though it seems unlikely to be very useful in practice. How often are you going to know the probably value of a pointer without knowing the actual value? I would guess it's pretty rare. Interesting anyway! reply gus_massa 4 hours agoparentIt's possible to have a \"sparse\" matrixes where most of the values are 0 and only a few are not null. So you can guess 0 and cross your fingers. (There are libraries that implement sparse matrixes in a more memory efficient way. I needed them for a program in Python, but I'm not an expert in Python. I found a few ways, but they are only useful for big matrixes with very few coeficients and have other restrictions to get an improved speed. I finaly gave up and used a normal np matrixes.) reply dmoy 4 hours agoparentprevWell in the articles case, it's the linked list `next` pointers: https://mazzo.li/posts/value-speculation.html#value-speculat... In a happy case, those will be laid out sequentially in memory so you can guess the value of the pointer easily. (That said your comment still stands, since using linked lists in the first place is much more rare). But I suppose there's probably a lot of other domains where you might have a performance critical loop where some hacky guessing might work. reply account42 3 hours agorootparentNot only are linked lists rare, they are also mainly useful exactly in situations where you cannot guarantee a (even mostly) linear allocation order. reply kevin_thibedeau 9 minutes agorootparentMemory pools are commonly allocated in a contiguous block and sliced into nodes placed onto a free list. They will be sequential until the pattern of allocations mixes them up. reply gpderetta 3 hours agorootparentprevthe optimization could be vaguely interesting if you are implementing a lisp (or some other list heavy language) and don't want to perform too aggressive non-local optimizations to optimize the layout of lists. reply twoodfin 58 minutes agorootparentAs I recall, at least one of the Lisp machine Lisps used a bit in the cons pair to declare if the cdr (tail element) was immediately following. EDIT: https://en.wikipedia.org/wiki/CDR_coding reply Bootvis 4 hours agoparentprevIt might be useful in cases where you pre-allocate a large array which you don't randomly access and whose structure doesn't change much but sometimes it does. Then you could either reallocate the array and pay a (large) one time cost or use this trick. reply gpderetta 3 hours agoparentprevIn principle a compiler via JIT or PGO could do this optimization automatically. reply bell-cot 4 hours agoparentprevBigger-picture, this method amounts to manually assisted speculative execution. And it's not about knowing the not-yet-loaded value, but about knowing what will (very likely) happen as a consequence of that value. reply Atharshah 3 hours agoprev [–] I will change my game level reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses using value speculation to enhance CPU performance by leveraging the branch predictor to guess values and reduce data dependencies in tight loops.",
      "By speculating the next node's address in a linked list summation function, the technique aims to bypass L1 cache read delays, resulting in significant performance improvements.",
      "Despite compilers often optimizing away this trick, using inline assembly ensures the speculation remains, leading to a 50-200% increase in function speed when data fits within CPU caches."
    ],
    "commentSummary": [
      "The article \"Beating the L1 cache with value speculation\" (2021) by mazzo.li explores optimizing memory layout in linked lists through value speculation, emphasizing the importance of linear memory layout of nodes.",
      "Commenters debate the practicality and effectiveness of this method, with some appreciating its cleverness but questioning its broad applicability.",
      "Discussions also cover related technical details like instruction execution, memory models, potential use cases, and the possibility of compilers automating such optimizations."
    ],
    "points": 125,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1720729078
  },
  {
    "id": 40939627,
    "title": "Apple Vision Pro U.S. Sales Are All but Dead, Market Analysts Say",
    "originLink": "https://gizmodo.com/apple-vision-pro-u-s-sales-2000469302",
    "originBody": "Apple Vision Pro U.S. Sales Are All But Dead, Market Analysts Say Anyone in the U.S. who wanted a $3,500 Vision Pro may already have one. The only hope for increased sales is a rumored cheaper model in 2025. By Kyle Barr Published July 11, 2024Comments (25) 𝕏 Copied! ©Kyle Barr / Gizmodo Those still holding on to their Apple Vision Pros may remain in a rather exclusive club throughout this year. Market research shows that sales for Apple’s first big, expensive headset will remain low in 2024. The latest reports from those keeping tabs on the Cupertino, California company say AVP will have dropped off 75% by the end of August. The true test for Apple’s spatial dreams may rest on the rumored (slightly) cheaper headset. The market analyst firm IDC told Bloomberg the Apple Vision Pro has yet to sell 100,000 units. It’s an expensive headset, and Apple wasn’t expecting it to sell like an iPhone. Supply chain analysts have reported that Apple cut its sales expectations for its $3,500 “spatial computer” in April. But this latest report shows that sales will have dropped off a cliff in the U.S. in the third quarter of this year and will continue to slacken through the holidays. Last month, Apple launched the Vision Pro in international markets, including Europe, the U.K., China, Japan, and Singapore. IDC expects the AVP’s interest in those markets to keep the headset’s sales afloat until next year. The real pick-me-up for Apple’s spatial dreams would be a new, less expensive headset. Those in the know have hinted Apple is working on a “budget” Vision device slated for the latter half of 2025. Even if the next Vision device costs half the Pro model, it will still cost $1,750 and one of the most expensive consumer-end VR/AR headsets you can buy. Rumors hint that the next device could remove the pointless exterior display to save on manufacturing costs. It could also reduce the FOV and use a less-capable chip than the current M2. Bloomberg hinted that Apple was even considering tethering it to an iPhone or Mac for daily use, which would drastically reduce its portability. We don’t have pure statistics on how many folks returned their Vision Pro after buying it during the initial hype rush, but analysts have noted that many who bought one were confused by its more complicated setup and what they were supposed to use it for in their daily lives. Sales expectations will put even more pressure on Apple engineers to design something that can compete with devices like the $500 Meta Quest 3 while justifying the higher price tag. Fans of the Cupertino company are already used to paying an “Apple tax” on their products, but not when the cost is literally thousands of dollars more. Apple is working on a visionOS update to improve the faux-3D spatial photos, add a few new gesture controls, and allow for a panoramic Mac screen mirroring. The latest version of visionOS won’t have a public beta, so we’ll have to wait and see if the changes will give the few on-the-fence customers a reason to pick up the ultra-expensive headset. AppleApple Vision ProIDCIPhoneMacVisionOS You May Also Like ReviewsWearables Samsung Is Already Taking Shots at Apple Watch’s Blood Oxygen Woes The Galaxy Watch 7 is touting new health monitoring tech prior to Unpacked, while Apple may postpone some of its Watch 10’s wellness features. By Kyle Barr Published July 8, 2024 How To How to Make Your Home Screen Prettier and Less Boring Don't settle for the same old wallpaper every single day. By David Nield Published July 4, 2024 ReviewsPhonesGoogle Leaked Google Pixel 9 Is So Disgustingly Pink You Can Taste It Another leak shows a Pepto Bismal-colored Pixel. Plus, some details on Google’s new AI offerings. By Florence Ion Published July 2, 2024 Tech NewsApple Apple Rumored to Be Working on AirPods With Built-in Cameras Infrared cameras could make Apple's spatial-computing experience more lifelike. By Harri Weber Published June 30, 2024 ReviewsPhonesApple The iPhone 16 Might Get a Replaceable Battery Wouldn't it be nice to see more manufacturers follow suit? By Florence Ion Published June 27, 2024 ReviewsPhonesApple The Green Bubble Nightmare Is Over, Apple Messages Now Support RCS The developer beta is the only way to solve the Android vs. Apple feud for now, but it’s excellent news for the future. By Florence Ion Published June 26, 2024 Latest news This Limited Edition Analogue Pocket Is the Most Luxurious Way to Play Your Game Boy Games A Woman’s Covid Cough Caused Her Guts to Spill Out Kevin Feige Will Appear on a Surprising Comic-Con Panel Samsung Galaxy Z Flip 6 Early Impressions: Still as Limited as its Predecessor Faster-Than-Light Particle Fits With Special Theory of Relativity, Physicists Say SpaceX’s Falcon 9 Grounded After Second Stage Breaks Apart in Space Cybercriminals Stole Text and Call Records From ‘Nearly All’ of AT&T’s Customers 13 Cool Things We Noticed in the Captain America: Brave New World Trailer This Limited Edition Analogue Pocket Is the Most Luxurious Way to Play Your Game Boy Games 7/12/2024, 2:25 pm A Woman’s Covid Cough Caused Her Guts to Spill Out 7/12/2024, 2:20 pm Kevin Feige Will Appear on a Surprising Comic-Con Panel 7/12/2024, 2:16 pm Samsung Galaxy Z Flip 6 Early Impressions: Still as Limited as its Predecessor 7/12/2024, 2:00 pm Acer Swift Go 16 Review: I Refuse To Call It An “AI Machine” 7/12/2024, 9:05 am Lenovo Yoga Slim 7x Review: A Good Laptop With Lackluster AI Capabilities 7/11/2024, 8:30 am Motorola Razr 2024 Review: A $700 Steal Despite An Underwhelming Camera 7/10/2024, 9:16 am The Samsung Galaxy Ring Reminds Me Why I Don’t Like Wearing Rings 7/10/2024, 9:05 am",
    "commentLink": "https://news.ycombinator.com/item?id=40939627",
    "commentBody": "Apple Vision Pro U.S. Sales Are All but Dead, Market Analysts Say (gizmodo.com)121 points by ripjaygn 23 hours agohidepastfavorite208 comments Sir_Twist 23 hours agoI wonder whether the behavior Apple has exhibited around their developer relations (especially their EU / notorization related troubles) has had a chilling effect on people's willingness to support a new platform like this. Without the raw number of users that something like the iPhone has, does Apple make it worthwhile to create apps / games for their platforms? I'm not a developer, so maybe there is a more technical reason for this, but from the outside it seems as though both indie developers and large tech companies (like Google not making a YouTube app, and Netflix) are reluctant to support VisionOS. Or, maybe it isn't Apple-specific; maybe there simply isn't enough users, or VR/AR as a software paradigm is currently too far removed from how companies design their applications, and it is just a matter of time until they adapt. Like I said, I am not a developer, so maybe I'm missing something obvious here. reply haunter 23 hours agoparent>maybe I'm missing something obvious here. It's an expensive gadget for a niche [0] market. $3500 is an incredibly high price for what it is. 0, https://store.steampowered.com/hwsurvey/ 1.75% of Steam users have a VR headset as of June this year which is lower than the people who are using Linux for gaming (2.08%). So you can see how small the VR market is. And the top alternatives are $500 (Quest 3) and $1000 (Valve Index) which are expensive on their own but $3500 (that you can't even use on Steam gaming lol) is just an astronomical difference reply jonwinstanley 23 hours agorootparentExactly. Revisit this stat in a couple of years once we are all talking about Vision Pro v3? This product is going to take a while to a) get good and b) go mainstream reply schmidtleonard 22 hours agorootparent\"It's a dev kit for those willing to spend on early access to Apple's AR/VR ecosystem\" was always the sane take, it was really weird watching so many people convince themselves that it was a mass-market product and then convince themselves that the failure of a mass-market audience to materialize was some kind of a disaster. I have no visibility inside Apple so I don't know if they drank their own kool-aid on this one or if this is just Ye Olde Yellow Journalism, but it's a strange amount of drama over something that in any other industry would be a complete non-story, even at ten times the dollar amount. Probably more than that, depending on the industry! reply zamadatix 22 hours agorootparentApple has no problems clearly announcing dev kits as dev kits, just look at the Developer Transition Kit hardware they released for the PowerPC->Intel and Intel->Apple Silicon transitions. These kinds of things are not only clearly labeled as dev kits they have required paying to join dedicated dev kit programs which expressly forbid getting the (loaned, not bought) device for non-porting or non-development related work. Apple Vision Pro was nothing like that, it had consumer focused announcements at consumer focused events, had consumer ads, sold through the normal consumer channels, and used standard consumer accounts. The other thing that doesn't line up with this take is how Apple announcing they've cancelled the next Vision Pro to work on a lower cost version is squared up. Even if you still take the Vision Pro as having always been intended a dev kit for early access, needing to cancel the follow on model development to rescope to a lower end product for the market is no less a miss than the initial launch in that the entire program was completely misaligned with the market until well after launch. I think half of the news stories are a result of Apple pushing this to release to market instead of just shuttering the version internally and the other half are a result of a launch problem like this being relatively rare for the most valuable company in the world and not necessarily about viewing the product itself with blinders. reply bhj 21 hours agorootparent> how Apple announcing they've cancelled the next Vision Pro to work on a lower cost version is squared up Apple's made no such announcement, and the presence of the \"Pro\" moniker implies that a non-Pro version has always been planned. reply majewsky 20 hours agorootparentIf that is so, then they will probably release it at the same time as Final Cut. :) reply schmidtleonard 19 hours agorootparentiMovie exists. Technically speaking. Ever since they lobotomized it I too have tried hard to forget, but it has an icon and presumably someone finds it useful or at least clicks on it by accident every now and again. I think zamadatix convinced me that Apple drank their own kool-aid on VR though. That's too bad. I'm sure someone had a bonus large enough to prevent them from seeing straight. Hopefully it doesn't turn into another Scott Forstall situation, where Apple fires them and then it turns out that sins notwithstanding they were the voice of reason when it came to something else. Dammit, it's been a decade and Apple still hasn't given up on the Fisher Price widgets. Ah well. The world spins on. reply zamadatix 17 hours agorootparentprev> Apple's made no such announcement They're plenty of reports they announced that change to hardware partners. I wouldn't hold my breath for a press release if that's what you're referring to though. > and the presence of the \"Pro\" moniker implies that a non-Pro version has always been planned. What's this to do with the price of tea in China? Apple halted work on the next Pro version to work on the lower cost version. This doesn't say Apple just now invented the idea of releasing a non-pro version. It does say Apple has realized the initial market tier they were developing for was the wrong one to start with and they need to stop any more work on that tier for now to instead work on getting said lower tier model to market next. reply TillE 22 hours agorootparentprevRight, $3500 is a really clear signal that Apple knows what it's doing, that this is not even an attempt to be a mainstream product yet. Who knows if they'll succeed in the end, but this is just the beginning of a long term project. reply ryandrake 20 hours agorootparentI mean, when it's early in the failure, you can say this about any company's failed product: Oh, well [COMPANY] is really just thinking ahead, and might actually be playing 5D Chess... Think of what the long term might bring! reply afavour 22 hours agorootparentprev> it was really weird watching so many people convince themselves that it was a mass-market product and then convince themselves that the failure of a mass-market audience to materialize was some kind of a disaster. They were presumably following Apple’s lead. Apple predicted far higher sales than we’ve seen and have had to cut those predictions down to size. reply CivBase 22 hours agorootparentprevI think it's pretty optimistic to expect either of those will be realized in a couple of years. If Apple does release a v3 in that time (big if), I'm doubtful it will amount to more than a minor iteration on the first gen model. I think Apple is probably happy to let this stay a niche product with a very slow release cadence. After all, why would they release a v2 next year? What significant improvements could they make in that time that would entice new customers - or entice existing customers to upgrade? The MacBook is a work device used by many professionals and can easily be justified as a recurring business expense. The iPhone is a relatively cheap device whose sales are bolstered by affordable payment plans offered by US carriers. These devices sell in huge volumes and it makes sense for people upgrade them often, so there's a lot of money to be had in frequent iterative releases. That's just not the case with the Vision Pro. If Apple releases a new Vision Pro, it will need to be a big upgrade - or a cost-down model with a more accessible price point. reply jonwinstanley 8 hours agorootparentSorry, I shouldn’t have been so specific. I meant come back in a few years and see where the product is once it’s had a few revisions. I personally think it’ll be a big success but have no idea how long it might take. reply Someone 21 hours agorootparentprev> $3500 is an incredibly high price for what it is. I think “$3500 is an incredibly high price for what it does for me” is true for most consumers, but if that were incredibly high for what it is, we’d see other companies sell comparable hardware at half the price. I also think that means they have a bigger problem than just bringing the price down. What it is is incredible, but what it does for me? Mwah. I don’t think it would fly off the shelves (for a company of Apple’s size) for half the price. reply ocodo 12 hours agorootparent\"Mwah\" is an onomatopoeia for air-kiss, I guess you mean \"Meh\" reply autoexecbat 23 hours agorootparentprevI'd happily buy one for $500, but $3500 is way too much reply bee_rider 22 hours agorootparentIt seems like a developer tool for building VR apps, which could make sense if they were planning on making a $500 device later. But, absent clear plans there, I can’t really see why anyone would buy it. OTOH they wouldn’t sell any at all to consumers if they announced there was a sensibly priced one planned for the future. reply rafaelmn 22 hours agorootparentApple is the last company I want a VR headset from. Like I'd buy iPad instantly if it could run a real OS (like a surface pro) - but Apple loves the app store margins soo much they will fight tooth and nail too keep you locked in, at heavy expense of capability. Likewise for Apple vision pro : - productivity - only if it fits the store model - entertainmen - only if daddy Apple approves - hackability - no way MacOS devices are a legacy from a different era in Apple business model - they would not build such devices today - and are not really expanding the range. iPhone I can live with the lockdown for the integration between MacOS, likewise for accessories (even here I might switch due to the taste their ecosystem business leaves in mouth). But VR and iPad are things where I want fully capable/hackable environments - that's not going to happen with Apple App Store business model. reply lurking_swe 22 hours agorootparentprevconsidering it has a CPU that is as powerful as the MacBook Pro, I find your target price extremely optimistic. hopefully once the display tech becomes cheaper, the overall price will come down quite a bit. But $500? unrealistic expectations IMO. Maybe in 10 years. reply afavour 22 hours agorootparentThen it won’t be a success for another ten years. You can call them unrealistic expectations if you’d like but I’ll drop thousands on a laptop because I use it day in day out. There’s simply no chance I’ll be doing the same with a Vision so the price I’m willing to pay is adjusted accordingly. reply wkat4242 21 hours agorootparentprevDoes it really need a CPU as powerful as a MacBook Pro though, I wonder. It certainly doesn't offer the same capabilities as one. This looks like one of the primary targets to start cutting. reply elabajaba 13 hours agorootparentVR needs a more powerful CPU and GPU than a MacBook Pro needs if you're trying to break into the VR gaming market (not saying you need a faster CPU than an M3, but VR is extremely demanding and will use all the CPU and GPU you can throw at it to maintain 90+fps with multiview rendering), especially since Apple keeps pushing Metal and not supporting Vulkan. (Metal tends to have higher CPU overhead vs Vulkan, and means you have to add an additional rendering backend for existing VR games) reply TulliusCicero 12 hours agorootparent> VR needs a more powerful CPU and GPU than a MacBook Pro needs if you're trying to break into the VR gaming market No, it doesn't. The Quest series has done well for itself with boosted smartphone chips. Surely, that means simpler graphics, but the graphics aren't really the thing holding VR back right now. Ease of use, comfort, weight, eye strain, motion sickness, physical feedback, these are all bigger issues imo. (Though I'll admit that larger FoV would help, and that's tied to graphical power) reply wkat4242 11 hours agorootparentprevIf you really want powerful VR yes, but this is not what Apple is going for at all. They're only doing some very basic AR usecases with floating windows. reply duped 22 hours agorootparentprev$500 is what a new gaming console costs. If Apple wants to compete for the attention of people with better gaming/entertainment experiences then their price needs to come down. I think it's unrealistic of Apple to expect sales for more than $500, and the market has proven that to be correct. reply SapporoChris 21 hours agorootparentprevhttps://backlinko.com/steam-users 132 million active subscribers. So perhaps 2.3 million VR sets on steam. Any VR producer capturing 10% of the market has 230k VR sets sold. Doesn't seem like that small of a market. But I completely agree, the high price for a VR set without good support is absurd. reply TulliusCicero 23 hours agorootparentprevLinux for gaming presumably includes the Steam Deck, no? reply haunter 23 hours agorootparentYes If you check the GPU stats it's the \"AMD Custom GPU 0405\" which stands at 0.62% accross all GPUs reply ocodo 12 hours agorootparentprev> just an astronomical ~~difference~~ waste of money. reply lylejantzi3rd 22 hours agorootparentprev> It's an expensive gadget for a niche [0] market With an interesting chicken and egg problem. There aren't enough users to justify making content for it and nobody wants to buy it because there isn't enough content. Pretty much all of the VR headsets have this problem. So did the Amazon Echo devices. reply LordKeren 23 hours agoparentprevApple has definitely, definitely soured their relationships with third party developers— but I think the far larger issue is the install base. There isn’t enough AVPs sold to ever recoup the cost of developing a VisionOS native application. There are also effectively 0 well qualified developers for VisionOS — it’s so new that no one outside Apple has a significant amount of experience. So even beyond pure build hours alone, getting an application launched on the platform is a non-trivial amount of developer training. reply jsemrau 23 hours agorootparentReminds me of the time where I taught myself AR skills and launched some mildly successful apps for iOS then applied for a bunch of roles only to be rejected soundly for all of them. reply epistasis 23 hours agoparentprevI doubt developer relations have anything to do with this at all. There's no users, no plan to scale to lots of users, and therefore no developers. It might be partially because of the size of the VR/AR market, but Apple's approach with Vision Pro was never to try to capture all of it. It seemed mostly to gain experience in manufacturing and start to see if anything actually materialized. In many ways it's a very anti-Apple product launch in that it didn't have a very clear use for the common person from day 1, which has been the mantra for everything from the iPhone to AirPods. reply HumblyTossed 22 hours agorootparent> It might be partially because of the size of the VR/AR market, but Apple's approach with Vision Pro was never to try to capture all of it. It seemed mostly to gain experience in manufacturing and start to see if anything actually materialized. Did they say this, because otherwise I'm calling ... you know what. This is a failed product by Apple. reply epistasis 22 hours agorootparentThe price was $3500. With pretty much zero killer applications. I do agree that it's a failed product, but I also don't think they had any plan for \"success\" with it. reply bastien2 23 hours agoparentprevIt's more that VR/AR is a high-cost solution still in search of a problem that makes it profitable. So far the only market for VR is niche entertainment. VRC is fun, but VR games have been arounds for decades and never gotten past novelty. AR has value as an accessibility tool because it can get around the high cost of some infrastructure changes. But like all accessibility tools, profit-focused eyeballs see it as a small, low-margin market. Enabling human rights is not considered profit. reply wkat4242 21 hours agorootparent> So far the only market for VR is niche entertainment. That's not true. VR also thrives in corporate. Niches, yes. But it does do well there. Training and simulation scenarios in particular. There's some tools like uptale and arthur that capitalise on this. And VR games are really really good and add a lot of immersion. I don't like to game without it anymore. I don't think VR is for everything and everyone but there certainly are usecases for it. reply tomwojcik 22 hours agoparentprevI used to work in game dev ~9 years ago. iPhone is where the users are, so naturally that's your main target. But your game will accomplish success only if you're featured in the app store (I don't own apple so this might have changed drastically). But in order to get featured... Haptic feedback in the newest iphone? You need to support it in order to be featured. You can play games on apple watch? If you support it, a guaranteed feature. New localisation guidelines? Featured only if you support all of them. So back then (not sure how it is right now) you needed to implement useless or at the very least something unnecessary in order to be featured on iPhone and make a buck. Otherwise apple wasn't able to sell their new technologies. If nothing has changed, people will try to port their games to apple vr so they are more likely to be featured on iPhone. All our games (6, iirc), were featured. reply tomwojcik 22 hours agorootparentI can't seem to be able to edit. My wording was incorrect. It was not needed to implement all those useless features to get your game featured in the store, but very helpful. And it makes sense, imo. reply fingerlocks 10 hours agorootparentMy app was featured many years ago, and yes it’s true that they request changes to support the latest SDK. They weren’t feature requests though, more like removal of deprecated API calls, following the UI guidelines, and accessibility was a big one. But no I wouldn’t call any of those “useless features”. reply crooked-v 23 hours agoparentprevIt's a combination of two things: there's not enough users, and the device just isn't comfortable enough for that niche of ultra-enthusiast users to use it for extended periods. Apple really shot themselves in the foot here with the headstrap design and by weighing it down with the metal and glass housing. Even the \"good\" strap is unbalanced and uncomfortable and so practical for very little but consuming content in a reclined position. By contrast, the Quest 3 is noticeably lighter and has an entire ecosystem of head straps that allow for extended wear without discomfort, to the point that the limiting factor is the battery. Between that and the much lower price, the market of people willing to actually buy apps and services (mostly games, but also stuff like Bigscreen or workout apps) is orders of magnitude larger. reply crazygringo 22 hours agorootparent> practical for very little but consuming content in a reclined position. You can create content in a recline position too. It's utterly fantastic, actually, if you just have a little tray in your lap for a wireless keyboard, and a mouse on a table on your armrest. I really think the future of VR is mostly in a reclined position, actually -- headset weight becomes a non-issue, and it just avoids developing postural problems. It's infinitely better for your back and neck and shoulders than sitting in a chair at a desk all day long. The Meta Quest finally recently gained the ability to shift your environment to a vertical angle, for reclining or lying down in bed. I don't really see VR's killer app being immersive games while standing up or sitting down, because of motion sickness. I think it's going to be reclining, doing work on big screens as well as watching content on big screens and immersively. I think we're just waiting for the displays to increase their resolution a bit more, and for prices for those displays to come down. reply adastra22 22 hours agorootparentThat just sounds so utterly dystopian that I would never want to be part of it. reply crazygringo 22 hours agorootparentIf you have lower back pain from sitting in a chair with bad posture for decades, it's a godsend. When you're \"in the zone\" programming on a screen or writing a book or editing a movie, it's not like you're particularly aware of your environment anyways. And like I said -- not building up the lower back problems that will haunt you later in life. reply adastra22 22 hours agorootparentI already work reclined. But not strapped into goggles that isolate me from my environment and the people around me. reply crazygringo 22 hours agorootparentThen you know how crazy expensive it is to have to mount one or more monitors above you, hanging from some kind of support. And like I said, isolation while working is actually kind of a feature, when you're deep in productive work. Just think of all the people who wear noise-cancelling headphones in offices to tune out the distracting environment. I don't want to use it for video calls or while multitasking, but for serious work I want to be isolated -- that's a feature not a bug. And passthru is a kind of nice in-between mode. reply SAI_Peregrinus 18 hours agorootparentI've got a dual-monitor desk mount that can tilt ±40°, allwoing for easy reclined use[1]. $156 MSRP, hardly \"crazy expensive\" compared to a $3500 Vision Pro, even counting the monitors and computer. [1] https://www.madebygabor.com/product/16666/Gabor-DM_552-Levit... reply adastra22 21 hours agorootparentprevI don't actually because my laptop screen is sufficient :) I could see what you say being true if I was young, single, and lived alone. I have a wife and kids though and explicitly don't want to be isolated from that. reply crazygringo 21 hours agorootparentI think we're talking about two different things. Sure you don't want to be isolated from your family while you're checking e-mails or watching a movie together. That goes without saying. But most people I know with a partner and kids lock themselves in an office anyways while they're doing serious work in order not to be constantly distracted. If they are lucky enough to have a dedicated office at home. And there's nothing dystopian about doing your work reclining with a VR headset, any more than it's dystopian for video editors work in a darkened room with no windows, or software engineers to work in open-plan offices with two or four monitors and noise-cancelling headphones. reply adastra22 14 hours agorootparentWe're really not talking about separate things. I have a home office, but I keep the door open and have moments of interaction throughout the day. reply wkat4242 21 hours agorootparentprevWhen I'm in the office, isolating me from the people around me is a feature, not a bug. Especially since this hotdesking/flexworking crap when I sit beside randos from other departments I have nothing to do with. In fact the only reason I still come there is because sometimes I have to. Not because I want to or because it adds any kind of value. reply peppertree 23 hours agoparentprevThe relationship is transactional. If there's money to be made developers will be there. The core problem is there's no killer app for Vision Pro and price is way to high to be a impulsive purchase for most. reply HumblyTossed 22 hours agoparentprev>Or, maybe it isn't Apple-specific; maybe there simply isn't enough users, or VR/AR as a software paradigm is currently too far removed from how companies design their applications, and it is just a matter of time until they adapt. Pretty much this. People, generally, just are NOT going to wear this on their face for very long, if at all. This product solved zero problems that most people have. It's literally an iPad you wear on your face and most people would much rather spend $500 on an iPad. reply jmspring 23 hours agoparentprevApple’s issues with the EU/etc have nothing to do with it. Apple has a hard core base market. That said, the Vision Pro/headset market is small and Apple is only really going to get the Apple fanboy portion of that. reply renewiltord 22 hours agoparentprevNah, people aren’t religious. It’s just not a large enough platform. Too niche and it looks like they’ve given up on it. reply stcredzero 22 hours agoparentprevOr, maybe it isn't Apple-specific; maybe there simply isn't enough users, or VR/AR as a software paradigm is currently too far removed from how companies design their applications, and it is just a matter of time until they adapt. Like I said, I am not a developer, so maybe I'm missing something obvious here. Long term, here's what I suspect may happen. Robotics+AI is going to eat the lunch of VR. VR is only used when it's not economical to have the actual stuff, but the realm of nifty real world stuff is going to expand tremendously. This may well result in a societal bifurcation, where the rich have a bunch of robots, and the poor have to settle for VR. reply ProfessorLayton 23 hours agoprev\"...analysts have noted that many who bought one were confused by its more complicated setup and what they were supposed to use it for in their daily lives.\" There's the root of the problem right there. Apple's best successes had a very clear purpose in mind from the beginning: - iPod: 1,000 songs in your pocket - iPhone: A phone, an internet communicator, an iPod - iPad: Media consumption AVP didn't have a clear purpose right out the door, and it's very un-Apple to launch a new product category without figuring that out first. reply dawnerd 23 hours agoparentI returned mine. It was amazing, but very uncomfortable like all other VR head units. I spent an entire workday in it. The next day got half way through the day before getting a massive headache. My neck was sore for a week following. Apple store employees said I wasn't the only one. They need to focus on weight, them making it have the glass front+screen and being made of metal was a terrible idea. reply SkyPuncher 21 hours agorootparentGiven it already requires a battery tether, I can’t believe they didn’t put everything they could in the pack. The front facing display also seems absolutely needless. reply karmakaze 16 hours agorootparentprevNot all VR head units. Check out the Bigscreen Beyond. reply lagniappe 22 hours agorootparentprev>My neck was sore for a week following. neither here nor there, and I know you didn't ask, but from one tech worker to another, this isn't normal and unless you had some condition before you should be addressing this with exercise. reply adastra22 22 hours agorootparentHow old are you? Once you get of a certain age, doing anything new or different to your body leaves it aching for a week. You’re right that you address it with exercise though. reply lolinder 18 hours agorootparentWhere \"a certain age\" is no later than 30. Peaking physically at 25 is no exaggeration. reply dawnerd 20 hours agorootparentprev> you should be addressing this with exercise. Oh 100%, not afraid to admit that! And it probably wasn't wise to just dive right into it without building up to all day use. reply foldr 21 hours agorootparentprevSeems pretty normal after spending an entire day with a weight attached to your head? Your neck muscles obviously won’t be adapted to the additional load. reply wrsh07 23 hours agoparentprevI'm not sure that's right. Apple launched the watch thinking \"apps like iphone\" A few versions in, they settled on \"watches are for health and fitness\" Ben Thompson discusses the AVP as an immersive iPad, and that's kind of the problem. It's too locked down to be my productivity device (something that could justifiably command a premium price), and my entertainment budget isn't that high for one person (I already have a tv in that price range - cheaper tbh - that's really really good) reply lancesells 22 hours agorootparentAnd a TV can be viewed by other people at the same time. And you can get up and answer the door, or use the bathroom, or watch it out of the corner of your eye while you cook dinner. Even watching the VR sports that they are trying to make happen, you'll be doing that alone. For a lot of people watching sports is a communal experience and you'll never get that with the Vision Pro. reply jpc0 6 hours agorootparentprevMight not be the popular experience but for me there are some killer features I like the apple watch for, fashion isn't one of them. The health and fitness stuff is nice, but more so for productivity it's nice. I never need to take my phone out of my pocket most days purely because notifications and phonecalls pop up in my watch. More on the health and fitness front is that I can leave home with airpods and my watch and not be cut off from the world when I go out for a jog, previously I would need to deal with a holster for my phone. The thing I generally miss the most when I don't have my Apple watch on, I can't lift my arm to look at the time or get today's date. And that leads me to want to go back to my old analogue watch. reply taeric 22 hours agorootparentprevThe watch is a bit odd, though? It is, far and away, more of a fashion purchase than it is anything else. Yes, I think it has some good uses for health/fitness. I can't bring myself to think it is the driver for people buying it. reply crazygringo 22 hours agorootparent> Yes, I think it has some good uses for health/fitness. I can't bring myself to think it is the driver for people buying it. I think you're just mistaken, then. Health and fitness absolutely is the driver. They tried to market it as a fashion purchase in v1, and then gave up on that. It can still be fun with different straps, but I don't know of anyone who considers it a fashion purchase. reply taeric 22 hours agorootparentI'm comfortable disagreeing here. Watches are, by and large, a fashion piece. Even without getting custom bands. To the point that it is not at all uncommon for people to have a watch on that is dead. Happy to see data proving otherwise. I'm also more than willing to believe a lot of people will use some of the asserted benefits as a reason to justify getting the fashion item. But even the enthusiasts I know that get some of the health tracking rings and such drop off after a hilariously short honeymoon with the devices. The only exception I have seen to this, so far, are active users that are more likely to have a garmin. reply wrsh07 16 hours agorootparentI'm not sure you're wrong about \"apple watch as fashion\" (my gut is to disagree), but it's worth noting that apple greatly increased the size of the watch market. That is to say: while they converted some existing watch wearers, they greatly expanded the market. What I'm not certain of is \"why\". Fitness is an easy answer (that's something an apple watch does and my Seiko doesn't), but fashion is also a reasonable answer (apple is a premium brand) I think the true answer might be \"defaults are powerful\" Airpods are generally not the best sound quality you can buy, but they're well-integrated with the apple ecosystem. You can assume they'll just work. I would be surprised if they are a fashion piece, but I suspect their dominance looks similar to apple watch dominance reply taeric 3 hours agorootparentI am almost certainly presenting a stronger argument than I intend here. My assertion is that it is somewhat fashionable to wear a watch, and there is surprisingly little utility in it. Adding any utility is enough to get many to make the jump. (Note that this is very different than me only claiming the Apple Watch is a fashion item. Largely, my claim is ALL watches are fashion oriented.) Do I think this is the only reason at all? Probably not. I will agree that there are certainly many people that do get it for the utility alone. Happy to see numbers going over any of this, of course. Mostly all I have to go on are anecdotes regarding Apple watches. reply secstate 21 hours agorootparentprevI think you're exactly right here. It's not that people don't think of it as a health device, and that probably helps sales via justification. But most people I know who have it, when asked, have no F-ing clue what about their health they're tracking. And yeah, if you do care that much (amateur or pro athlete training) there's no way the A-watch is the primary device for you. Garmin is the king of the hill in that market. reply BitwiseFool 22 hours agorootparentprevCharging $49.99 for each wrist band was a huge misstep in this regard. Yes, Apple does like charging high prices but the cognitive dissonance around the 'Apple Tax' is usually softened when people feel like they're getting a higher quality product than the competition. But with those wrist bands, I really don't feel like I'm getting a unique and special product for that price. If the bands were $19.99 I would have bought at least a few others. I just can't justify buying some silicone for $49.99. reply asdff 11 hours agorootparentprevgarmin watches are so much better for that though. reply jdmoreira 23 hours agoparentprevHeadphones for your eyes. That's the product and its inevitable in some form or another. Its just too early reply HumblyTossed 22 hours agorootparentPeople don't like stuff on their face. People wear contacts or get surgery to avoid wearing glasses. People don't like stuff on their face. I paid for college working in the eyeglass industry and it's nearly universal that people \"wish they didn't have to wear glasses\". People don't like stuff on their face. reply hengheng 18 hours agorootparent> People don't like stuff on their face. If there's one thing we were able to learn with 100% certainty over the last couple years, it is this. I still think it'll have to be transparent, or at least Google Glass like, to achieve mass adoption. reply jdmoreira 10 hours agorootparentprevI didn't like headphones before AirPods. It might take a long time... maybe it's even a neuro plug at some point. I don't know but people will want to hijack their vision for sure. reply HumblyTossed 2 hours agorootparentNot the same. Many people have no problem wearing a hat either. Your _face_ is different. Maybe it's a biological protection mechanism, but _it is different_. reply bitwize 22 hours agorootparentprev> Headphones for your eyes. \"I need a SinoLogic 16, Sogo-7 data gloves, a GPL stealth module, one Burdine intelligent translator, and Thomson eyephones.\" reply elicash 23 hours agoparentprevI think Apple gave it a clear purpose: productivity and media. Basically, it's a giant screen anywhere you go. It's just not that useful, or alternatively way too expensive. But I think they communicated their theory pretty well. reply scottLobster 23 hours agoprevSounds like the consumer tech version of \"$70k EVs aren't selling anymore!\" It was extremely pricey and saturated the market, and from what I read most of the market was underwhelmed by its capabilities. I'm still waiting on my virtual workstation where I can have infinite virtual monitor space, instead we keep getting various whiz-bang gimmicks that are good for a demo but have no real use. reply JellyBeanThief 23 hours agoparent> I'm still waiting on my virtual workstation where I can have infinite virtual monitor space This is it. Right now my work has to shrink down to fit on four monitors. Virtual desktops can help, but only so much. When I'm trying to puzzle something out over 20 windows--source code files, git history, documentation for two or three libraries, Wikipedia, Slack, StackOverflow, I spend a non-trivial amount of time just keeping all that organized and switching between things. What I want is a room where I can spread out. And I need it for $1500 or less. reply mjamesaustin 23 hours agoparentprevApple is extending the monitor capabilities in their next software update, which at least to me is a good sign that they plan to keep expanding support for that mode. I'm in the same camp as you – my main objective for a headset would be infinite portable monitor space. I really hope we get there in the next few years. reply stcredzero 22 hours agoparentprevSounds like the consumer tech version of \"$70k EVs aren't selling anymore!\" Misinformation. The good EVs are selling quite well. It's just that their price has effectively dropped quite a lot. My wife's Model Y which cost us nearly $80k (we bought at peak price: the prior Corolla got totalled) now has the equivalent 2024 model selling for $42k! The crappy EVs (ie most everyone else's) aren't selling, because they are inferior in efficiency and software implementation. Rivian and Lucid vehicles are pretty good, but those companies are still at risk of never showing a profit. I've been in a Hyundai Ionic 5, and that seemed decent too. reply LordKeren 23 hours agoprevWhile I think this was largely expected from everyone following AVP, it will be very interesting to see how Apple attempts to salvage this project. They’ve really backed themselves in to a corner with the pricing of their headset far exceeding what all but the most enamored enthusiasts are willing to stomach. On top of that, the sales are so bad that it is not worth companies of nearly any size to dedicate time to creating apps or content for the platform. Apple is probably going to have to light a gigantic stack of cash on fire to try to course correct this. Or they’ll cut their losses and write it off entirely reply walterbell 23 hours agoparent> Apple is probably going to have to light a gigantic stack of cash on fire to try to course correct this. Or they’ll cut their losses and write it off entirely Much cheaper: ship a bug that allows OS jailbreak to run VMs. See what unshackled creators develop. Add official interfaces to support those use cases. Rinse & repeat. reply LordKeren 23 hours agorootparentAs someone who spent a lot of time tinkering with jailbroken iPhones back in the day, there was always an ongoing rumor in the jailbreak community (maybe better to call it a myth or legend) that Apple knowingly let jailbreak vulnerabilities out. Apple probably could benefit massively from letting enthusiast users tinker with the OS in a way that gives Apple a bit of plausible deniability. reply Shawnj2 22 hours agorootparentWith modern security measures that’s definitely a thing of the past. If Apple intentionally adds bugs it will be mostly used to make spyware so they can’t do that anymore reply walterbell 20 hours agorootparentcheckm8 exists for iDevices with unpatchable boot hardware flaw. There is also TrollStore -- two days ago, https://x.com/fce365/status/1810761291306323997 iOS 17.0 TrollStore INSTALL METHOD Finally Achieved! reply Shawnj2 20 hours agorootparent17.0 is a year old lmao reply walterbell 20 hours agorootparent> 17.0 is a year old It's the current iOS major version and apps will support this version for years to come. Those who saved blobs with https://github.com/airsquared/blobsaver can freely switch between new and old iOS versions, e.g. if they have a use case which needs https://trollstore.org/: TrollStore is a tool that allows non-jailbroken iOS users to install and run IPA files without App Store verification. It works on iOS 15.5-16.6.1 and iOS 17.0, and supports automatic app refreshing and resigning. reply Shawnj2 20 hours agorootparentSo? You can buy an AppDB pro subscription or something similar for $20 a year to do the same thing on newer iOS versions with way less effort. Yes trollstore can do some other things but either way Apple did definitely not intentionally add the core trust but Trollstore uses and it’s more or less irrelevant now tbh reply walterbell 19 hours agorootparent> buy an AppDB pro subscription Seems legit, https://appdb.to/info/empire-strikes-back reply TheLoafOfBread 11 hours agorootparentprev> ship a bug that allows OS jailbreak to run VMs That's why I am always surprised when somebody calls it a devkit. It is just a coping mechanism for a failed product. Furthermore if Apple wants to turn it into such devkits, why do such weird and unnecessary mating rituals? Just let anyone who has active developer license on Apple ID to modify Vision OS and see what they are going to come up with. Apple will then add it into Vision OS and ban people who come up with the solution at a first place. As it is done in AppStore today. reply hu3 23 hours agorootparentprev> ship a bug that allows OS jailbreak to run VMs Apple lacks that courage reply JoshTriplett 23 hours agorootparentTo do it intentionally, sure, but don't worry, it'll happen whether they intend it or not. reply hu3 22 hours agorootparentI highly doubt. They don't even allow emulators in iOS. It took EU a decade to make Apple allow third party stores and even then Apple hasn't really complied still. Same for browser engines. reply walterbell 20 hours agorootparentM2 onwards has silicon support for nested virtualization. iOS 17 uses \"Secure eXclaves\" to isolate some code, which may be similar to the Android 14 pKVM use of nested virtualiation, https://android-developers.googleblog.com/2023/12/virtual-ma... With AVF virtual machines become a core construct of the Android operating system, similar to the way Android utilizes Linux processes. Developers have the flexibility to choose the level of isolation for a virtual machine: One-way isolation: Android (the host) can control and inspect the contents of the VM. These are most commonly used for sandboxing and separation.. Two-way isolation (Isolated VM): Android (the host) and the virtual machine (the guest) are completely isolated from each other.. This has 2 main properties: 1. The workload and data inside the VM is inaccessible (confidential) from the host (Android). 2. Even if Android is compromised all the way up to (and including) the host kernel, the isolated VM remains uncompromised. reply revscat 22 hours agorootparentprevThis would never happen. Apple is terrified to the core (apologies) of breasts and panties and cursing and such. The thought of someone being caught doing those things means the chances of this happening is less than zero. reply anakaine 23 hours agorootparentprevThere's still the cost of hardware problem. With so few sales it's unlikely that even this functionality would tip the scale. reply JeremyNT 21 hours agorootparentYeah, there's a recurring meme that all VR/AR needs is some \"killer app\" to show up and suddenly break through to the mass market. The device is simply far too expensive. No software is going to meaningfully change the value proposition relative to its competitors. reply crooked-v 22 hours agoparentprevI think it's more likely than not that the entire AVP project is just meant as a way of recouping a little cash from an ongoing dev program. What they really want is Apple Glasses ten or fifteen years down the line that will be a practical phone/iPad alternative, but they know they need to start building the expertise and UX workflows right now or Meta will beat them to that point. reply GeekyBear 22 hours agoparentprev> Apple is probably going to have to light a gigantic stack of cash on fire to try to course correct this. It would certainly cost an absolute fortune to acquire the streamung rights, but live sports is an area where fans have demonstrated that they are willing to spend a fortune on AV equipment and subscription fees. reply 1986 22 hours agorootparentBut people like to watch sports together, not alone reply helloooooooo 23 hours agoparentprevIt can also be that AVP is an “emergent” product while they work on the parts for something larger. I would imagine a lot of the “harder” parts for AVP line up closely with whatever car Apple is attempting to build. reply LordKeren 23 hours agorootparentIt is widely believed that the car project has been canceled https://www.theverge.com/24085629/apple-car-dead-project-tit... While Apple’s R&D budget can rival some small nations’ GDP, I think it will be exceedingly difficult for Apple to continue to pour resources into these project amid the apparent flop of AVP reply dgritsko 23 hours agorootparentprevApple recently \"pulled the plug\" on their car project, apparently. https://www.npr.org/2024/02/27/1234315814/apple-cancels-elec... reply voidfunc 23 hours agorootparentprevThey cancelled the car project. reply skilled 23 hours agoprevThe thing costs a bazillion dollars, what did they expect… people to suddenly wear this for day to day tasks? I said it on the day they first announced it, as soon as the media hype was over, no one would ever mention it again. Other than price, not sure what else they could do to make it appealing. reply zamadatix 23 hours agoparentI don't think the price itself is the root problem. I mean don't get me wrong if it were $350 it'd sell a lot more than $3,500 but the point is it still wouldn't sell well anyways so why only ever talk about how it costs a lot. To me the problem with the Vision line is it doesn't give any compelling reason to want it now or in the imminent future, even if it were priced similarly to lower end headsets, to really drive large numbers of sales on yet another VR walled garden. There is no killer app you'll spend hundreds of hours in, it's not going to actually replace/upgrade using your traditional apps, VR gaming is already relatively niche in established and open marketplaces but this has a brand new walled garden marketplace, and it offers nothing particularly \"revolutionary\" people haven't been promised 100 times about VR/AR to be disappointed with already. If it offered something meaningful against all of these points then it'd still sell decent numbers even at $3,500 but the problem is it's just lame despite being so high end. reply anakaine 22 hours agorootparentWhen the price is higher than many people's average monthly post tax income, and its not a must have device like a fridge or washing machine, it's definitely a pricing problem. In 2022 the US median personal income was $47,000. That's pre tax. Consider approximations with averages and marginal tax rates and take out 12% for tax, and you have $3445 per month median personal income. The apple headset is $3500. Take out average rent of $1350. Average credit card debt per consumer is just over $6300 in 2024, which seems high, but is also nearly twice monthly post tax income (holy crap). Average car payment for used vehicles is just over $500. I've not discussed food or utilities. Given the above are for the average consumer, I'm not at all surprised a luxury and discretionary niche item such as the AVP has had next to no sales. reply zamadatix 22 hours agorootparentThe entire VR market, including budget headsets, sold somewhere near 8 million units globally last year. In the US alone over 15 million new vehicles were sold at an average price greater than the personal income you quoted. Individual luxury car models even sold more than 100k units in the same timespan. One can get a good new vehicle for significantly less than $47,000 so there are clearly tens of millions of Americans willing to pay thousands of dollars for more premium products. The problem isn't that the price of this particular VR headset was too high for average people to hope to afford it's that there is no compelling reasons for average people to buy VR headsets like this in the first place. Particularly when that company is just entering the market. Put another way: Even if a person were given a Vision Pro for free at launch I doubt they would have spent 100 hours in it by now. Is that not at least worth mentioning instead of just deferring to price when talking about why nobody wants it? And, again, if the headset were targeted towards the budget market there is still no reason to expect it would have sold particularly well. E.g. there is a cheaper (~$550) but similar headset from an already established VR company with a lot more compelling a marketplace/ecosystem for an already existing line of products. They're (Meta, Quest 3) in the same ballpark with total revenue and less than a million units on their latest headset in a similar time period despite all these advantages and having the \"right\" price. reply Shawnj2 22 hours agorootparentprevAVP is incredibly expensive but Meta makes a consumer friendly budget option for $500 and that’s not selling crazy well either. reply thiht 9 hours agorootparentprevI don't know if it's the root problem, but it's the first and biggest problem. I earn a comfortable salary in France and although I think I would like to get one (the movie watching experience looks really cool), there's absolutely no way I'll spend more than 1 month of salary in this. I don't even know if I would like the thing, even if I were to buy it, I can't afford to buy it and then not using it because it turns out to be useless. reply SkyPuncher 21 hours agorootparentprevI think the price is a massive problem. I have the Xreal Airs. They run $300 for a 1080p screen. Apples screen is much better, but not 10x better. Apple really needed to come in the $1k price range for this to be viable. reply zamadatix 17 hours agorootparentOne could say the same thing about TVs where in 2023 the average price was $400. While the screen on a $4,000 TV is not \"10x better\" any more than the screen on the Vision Pro is \"10x better\" manufacturers don't have any trouble at pushing 100k units of TVs in the latter price range because whether the screen is 10x better isn't the whole story on premium purchases. Again I'm not saying the high price was the optimal value, I'm saying the high price is far from the whole story on why the device didn't sell well and that a downmarket version still would not have sold well, even compared to existing peers in the lower cost space. reply itsoktocry 23 hours agoparentprevThe cynics are right more often than not, but the poor \"why do I need Dropbox?\" guy still gets trotted out. reply Damogran6 23 hours agoprevI think VR is a symptom that we're both running out of fertile tech to mine, and the population at large is tired of it. In order to make a compelling experience the tech is Very Expensive...and what it really is is a funky monitor and funky mouse. Well monitors are a couple hundred bucks and a mouse is $50. And even when it _is_ affordable, it's just not 'sticky' (yes, I know, poor choice of words) I've used 'em since the Vive and some of the experiences are outstanding....and yet they end up collecting dust for weeks at a time. I use it, it's great, I enjoy it...and then it goes back to collecting dust. And I'm pretty sure the cheap, sunglasses, 4x better display of the future would be the same. reply anakaine 22 hours agoparentGot to agree here. Outside a very small number of cases beyond the honeymoon period with VR, it's just not something that I'm interested in wearing. The AVP looks like a fantastic device, bit sitting in my own world of isolation with a screen on my face does not sit well as an every day interaction. Not even every week or month these days. I suspect AR will be similar for your average consumer. Info overload is also real. If it's built into a car, or something else it then has a function and is good. reply LordKeren 22 hours agoparentprevI think this is approaching the end of what this VR form factor can produce. Compact pass through glasses + Dramatically improved “AI” assistants is almost certainly going to be the end goal for what these technologies look like. Google glass may have been on the right course all along reply GeekyBear 22 hours agoprevLeaks of Apple's attempts to create a cheaper consumer version of this tech in an eyeglasses form factor go back to around the time of ARKit's release in 2017. Every time they restart the effort, they eventuslly come to the conclusion that the tech just isn't there yet. Currently, the short term plan (now that developers have something they can work with) is to produce a lighter/cheaper version of the shipping headset, while they continue long term work on an eyeglasses form factor for the consumer market. reply brink 23 hours agoprevMight have been exactly the plan. Soak up all the sucker enthusiast money with the expensive model while establishing vision OS as \"premium\", then bring that \"premium\" product to the masses by coming out with something much cheaper later for the main market. reply zamadatix 23 hours agoparentIs VisionOS itself seen as particularly premium just because the launch hardware was? I mean it had some good features but, outside of hardware, Meta has done a good job of catching the big ones they lacked before anyone has really tried VisionOS. reply jkestner 23 hours agoparentprevSure, but you still need to reach the critical mass of developer support. No one downmarket is any more likely to buy one if there aren’t any apps. reply bwy 23 hours agoparentprevThe Tesla playbook? reply cmilton 18 hours agoprevNo one wants this. It’s a gimmick. Google glass was 2013. Trick my brain into believing it is in another world. That is what is exciting. This is what your consumers want. An escape from this world into another. Sometimes it’s there, most of the time it’s not. reply walterbell 23 hours agoprevDid Vision Pro inherit limitations from iPadOS? reply snapcaster 23 hours agoparentYes, I want to like mine and use it more but the software support is the real problem. The hardware is truly amazing (ignoring price point) and i've never had a VR/AR experience anywhere close but doing anything like my current computer flow is such a pain reply nox101 23 hours agorootparent> i've never had a VR/AR experience anywhere close What VR experiences have you had? I was not impressed at all. Have had a Rift since they launched. Rift-S, Quest 2, Quest 3, PSVR, PSVR2. Nothing about the Vision Pro did anything for me. The \"try to hold your eyes still to point\" is the worst possible UX interaction I've ever used. Super un-natural and un-intuitive. That's not how my eyes work. I knew that the first time I tried \"stare to point\" on PSVR2. It was awful there and it's awful on Vision Pro It also felt horrible. I'm not saying any of the other HMD are great but Vision Pro's weight was awful. It's either too tight to stay on or resting on my nose. Someday that will be fixed (glasses?) but in the context of shipping devices, the devices that put the weight on the forehead are way more comfortable. I think PSVR(1) was the first to do that. Rift-S copied. Quest is a step back unless you get a 3rd party accessory The \"you get one screen from your mac\" was also disappointing. I've had 6 screens in VR from my PC since Rift shipped. It's built into the Rift/Link OS. Yes, it's got issues but 1 screen is still a step back of many years, not forward. reply mjamesaustin 23 hours agorootparentI've got a Quest 3 and I have to say I thought the Vision Pro interface was about 100x better. Windows stay concretely fixed in place (instead of skittering and jittering about), and the interaction paradigm required no training and worked flawlessly for me. Every time I pick up my Quest now, I feel the disappointment of having to use a controller or the terrible hand tracking. reply snapcaster 23 hours agorootparentprevAlso if you're having fit/weight issues worth switching to the extra strap they ship and investing a good amount of time trying to optimize it. I did that and now wearing it for 8 hour stretches isn't a big deal. YMMV reply snapcaster 23 hours agorootparentprevI mean the actual AR experience, every other headset i've used is so janky and can't persist objects enough to actually immerse you. AVP windows I've actually attempted to avoid while walking because my brain forgets they aren't real after a little bit reply nox101 18 hours agorootparentMaybe my confusion was you wrote VR/AR instead of just AR. Though even then, while visually the Vision Pro does better then the Quest 3 at AR, the interactions leave a lot to be desired. AR (and VR) make me want to reach out and touch things. But when I tried, instead of letting me interact by touching it required me to \"stare and hold eyes perfectly still and then tap fingers somewhere not on the thing I actually wanted to interact with\" Like if an app window is close enough to touch then I just want to touch it like a giant floating touch screen. Not \"stare and tap pants\". reply snapcaster 4 hours agorootparentYeah my bad the VR is good but the AR is what feels like best in class. agreed on your point there is some support for that (was doing development and some example apps let you interact with \"real world\" objects like you're describing) but i don't know why they didn't just use that as the dominant paradigm. Maybe was seen as too janky or difficult for non techies reply walterbell 23 hours agorootparentprevIn the absence of a visionary like Steve Jobs, new devices need \"accidental\" jailbreaks for creatives to seed Sherlock fields for Apple harvest. reply LordKeren 22 hours agoparentprevYes. It is not useful for most common office work tasks without connecting it to a Mac reply BitwiseFool 23 hours agoprevI enjoyed demoing my friend's Vision Pro but I regularly found myself displeased with the gesture controls. Once the novelty of interacting with the demo features wore off, I found myself thinking that the device was best suited as a monitor replacement. To that end, I would absolutely need to use a mouse and keyboard. I also wonder if Apple's decision to eschew any special pointing or peripheral device in favor of hand tracking was the best idea. If I owned one of these things and used it for casual internet browsing, I would much prefer some little handheld accessory so I could press buttons without having to gesticulate and make sure my posture was correct so that the cameras could pick up my hand movements. reply nottorp 23 hours agoparentMinority Report style UIs look good in movies. Use them 8 hours per day every day and they're just tiring. reply BitwiseFool 22 hours agorootparentI noticed two things very quickly, my arms started getting tired and I was never quite sure if I was holding my hands in the right place to have my gestures get picked up properly. reply nottorp 22 hours agorootparent> I was never quite sure I held them in the right place to have my gestures picked up by the cameras. That's okay, I'm sure Apple will try a flat keyboard with no tactile feedback soon :) They're making noise they're aiming for thinness again. reply snapcaster 23 hours agoparentprevYep my daily use for this is just using it as a mac os x external display reply LordKeren 23 hours agorootparentGenuinely asking — do you work outside an office a lot and/or travel with it? reply snapcaster 18 hours agorootparentDon't work outside a lot, but do travel with it. If you mean travel while wearing it no, because the fact that it's a video camera makes me uneasy about it possibly shutting off at any moment and rendering me blind + the low battery life. I have worked out in my office gym wearing it and that was nice reply nox101 18 hours agoprevMy feeling is, if most of Apple's employees are almost all not using the thing then it's a failure. I suspect 98% of all Apple employees use a iPhone, a Mac, and probably an iPad, and an Apple TV. I betWhat if Zuck made the Meta Quest Pro as a psy-op to get Apple to make a pro headset and waste resources, simply to get back at them for clamping down on user tracking?reply supernova87a 23 hours agoparentHah, if that's true, they both got burned to the tune of $5B each, I'm sure. So not much payback achieved. And inside the company, everyone can see that it's an embarrassing loss of interest from MZ after 2 years of urgent enthusiasm that didn't pan out. On to LLMs now! reply lancesells 22 hours agorootparentMeta is also a company with little to no business in entertainment or content creation. Nintendo, Sony, Microsoft, Epic, Valve, etc. have a better chance of making a metaverse happen than whatever Meta is trying to buy their way into. Meta's entire business doesn't exist without it's user's giving them content. They are so far away from a creative company. reply matwood 22 hours agoparentprevThere could be an insurance angle. IF VR took off, Apple wanted something to point to. Welcome to the age of trillion dollar companies peacocking each other. reply nunez 20 hours agoprevThe super-expensive Tesla Model S and X barely sold until the cheaper Model 3 took off. Given that the people who have bought the Vision Pro consider this device revolutionary, this is probably headed on the same trajectory. I would have gotten one if the DPI for text was as good as a monitor and I didn't need to depend on a MacBook. (I don't watch videos, but I'd love a monitor-less solution for coding on the plane.) reply schaefer 21 hours agoprevI fancy myself a \"VR for productivity\" enthusiast. I own a PCVR setup, an Areo by Varjo[1]. And I do not use it. My cost out the door was higher than the cost of the Vision Pro and I learned some very painful lessons: 1) For VR productivity, I need my desktop application windows to run native in a VR 3d window manager. So far, all I've found are variations on the following: you get to remote into your existing desktop window manager which is projected as a 2d plane into your 3d space. This is practically useless. To actually use VR for productivity... I need to have my task windows not be co-planer. 2) Last time I checked up on this, practically no one [2] is working on this, I have stopped even watching the VR space. The hardware is here, but no one is making the software I want. 3) Vision Pro (as I understand it) is not a productivity VR headset. It's an ipad on your face that can remote into your desktop - but not in a way that integrates into the 3d window manager. [1]: https://varjo.com/products/aero/ [2]: Okay, techincally SimulaVR is working on a native 3d window manager, but they opened preorders years ago and still don't have a ship date. reply freeopinion 22 hours agoprevLet's say you are committed to AR/VR research. It's expensive. Let's say your research has generated tech that can be built into $2500 headset. You still have a long road to get better tech into a lower price range. Then somebody proposes that you sell 100,000 of what you have right now for $3500. This provides $100 million dollars of profit to offset your research costs. In the end, you manufacture 80,000 and sell 60,000 of them. So you net $10 million plus 60,000 free testers (well, 1000 after everyone stops using them at all) plus 20,000 surplus units to use in various research. Gizmodo can paint this as a commercial failure. Apple could sell the 20,000 unsold units for $100 each. That would generate another $2 million of profit even at a $2400 marginal loss per unit. They would fly off the shelves and Gizmodo would paint this as a huge commercial success. Even though Apple still wouldn't hit their 100,000 unit target. It would create panic in Apple's competitors even though they actually make a profit on $500 headsets. So read this however you want. It's a commercial failure because it only netted $10 million dollars and might only net $1 million/year going forward. Or it's a successful funding round on the road to cheaper and more capable future products. I'm not particularly an Apple fan, but I think it's silly to portray this in such a bad light. Would somebody like SimulaVR love to experience such a \"failure\"? (All numbers above except the 100,000 target units and the $3500 price are hypothetical.) reply pjmlp 23 hours agoprevAnyone that has been following VR/AR headsets for the last 30 years was already expecting this. How come does Apple come to a market that was already going down, pretending that Quest and HoloLens never did developer sessions showing exactly the same kind of content, a decade ago? To make matters worse, their current attitude that developers should feel grateful and blessed to even be able to touch their SDKs, means that very few feel like targeting a device that costs as much as it does. reply stephc_int13 23 hours agoprevI remember having conversations here on HN about how the launch of this device would change everything in the field, because only Apple knows the magic sauce, and they always win, or something along those lines. Well, I think they confused Apple with Steve Jobs, yes Jobs had almost a Midas touch regarding product design, especially when given the time to reflect on competition mistakes, and people forget that he also \"missed the mark\" a few times. Tim Cook is a different character, his special ability is his uncanny focus on squeezing the supply chain and keeping the money printer churning. This is not the Jesus goggle the fanboys were looking for, even if the tech seems impressive, I think they should not have launched it. reply ericmay 22 hours agoparent> Tim Cook is a different character, his special ability is his uncanny focus on squeezing the supply chain and keeping the money printer churning. Apple Watch, AirPods, Apple Silicon… plenty of great products and services have come out under Cook’s time as CEO. In my lifetime it has been exceedingly rare for Apple to launch a failing product. The company tends to be pretty conservative in what new features and products it launches to protect the brand equity. It’s not impossible, but I would be hesitant to consider Apple Vision Pro and their AR/VR efforts to be a failure unless you are negative on the entire AR/VR industry and future applications. reply asadotzler 19 hours agorootparentA couple of iPhone accessories and an extension of the iPhone processor to Macs isn't really making the case for Cook as a \"product guy\" much less a product genius. His two big swings at the fence, the car and this AVP monstrosity, are both duds. Elsewhere he's just riding Jobs' coat tails. reply ericmay 18 hours agorootparent“Tim Cook isn’t Steve Jobs” isn’t the greatest argument, and it wouldn’t be one I would make because clearly Tim Cook isn’t a product guy of the same caliber as Steve Jobs, but under Cook Apple has performed extraordinarily well, and they’ve introduced some great products along the way and continued to improve those as well. I don’t think that makes him only a “supply chain” and “money printer” guy. This is just a trope that’s been repeated since he took over. I’d also challenge you in your comparison about the Apple Car and Apple Vision Pro, because one of those were actual products that shipped to market, and even so the Apple Vision Pro hasn’t been out very long so it’s not really fair to say it has failed or that it was a dud. It could be, but I think if so it’s less so to do with the price and more so to do with the AR/VR industry itself being a dud. reply newaccount74 23 hours agoprevI don't think the price is the problem. People spend a lot on hobbies (it amazes me how many people ride 5000-10000€ road bikes). It's obviously too expensive that everyone buys one, but there are still lots of people who could afford it. The bigger problem is that I just see absolutely no reason to buy it. reply jayski 23 hours agoparentThey spend that amount on a bike, because they first bought a $300 bike, then upgraded to a $1500 one, etc. It's the same with guitars, you can start cheap and figure out if it's really for you. reply twoWhlsGud 22 hours agoparentprevI agree with the exception of you being surprised that folks will spend 5000E on a road bike. Of all my discretionary purchases my custom roadbike might well be my happiest. A well fitting allroad bike can take you to the grocery store or up a gravel road to a mountain pass and make both journeys joyful. I'm on mine most days to do something practical made fun and healthy and sustainable, too : ) reply andrewsuzuki 22 hours agorootparentWell-fitting bikes are nice, but 95% of people that buy custom frames have fit coordinates that can be easily achieved on stock geometry. reply solardev 6 hours agorootparentEven the stock stuff could easily cost $12k or more, before upgrades. On a bike that probably would've been half that pre-covid, and maybe 5 lbs lighter than a $1.2k bike. reply wombat-man 23 hours agoparentprevI was hyped. 100% ready to buy. Then I did the demo. It is too heavy and the field of view is too narrow. I simply don't want to watch movies in it or use it for long periods. Maybe future iterations reply snapcaster 23 hours agoprevDamn, i was hoping at least the hype would last long enough for google to release their apps on it reply lern_too_spel 22 hours agoparentIt's not even worth the bother to release an ads SDK for. reply medhir 22 hours agoprevPersonally, such a product is a non-starter if it faces the same restrictions that an iPad does. If I’m spending “pro computer” money, I want the pro computer. Having no way to access the file system or run code is just crazy to me. reply freeopinion 22 hours agoprevAn Apple 15\" Macbook Air runs $1300. An Apple 27\" Studio Display with stand runs $2000. I'm amused that Apple fans are not willing to spend $3500 for a computer with a 3000+\" display that you don't have to hold on your lap or desk. What would they pay for a 15\" Macbook Air with two fold-out screens? I can't help but feel that somebody is going to get things just right and disrupt computing as we've known it. Even if it takes several \"failed\" experiments like Glass or AVP to find the sweet spot. reply root_axis 21 hours agoprevMakes sense. The only VR market up till now has been the gaming market, and the AVP is a locked down device that's incompatible with the preeminent gaming platform. This is probably why Apple has marketed it as an AR device, but although AVP is a nice step forward towards that vision, AR experiences will have a gigantic technology gap for the foreseeable future due to battery life issue. That's all without considering the show-stopping price for AVP. reply dagmx 22 hours agoprevIt’s important to note that predicted numbers for the whole year are limited toThe true test for Apple’s spatial dreams may rest on the rumored (slightly) cheaper headset. They're gonna have to do a lot better than \"slightly\" there. At least to make it a success in more than just the US. reply duped 22 hours agoprevDisney should buy them up at a discount and set up some AR \"shows\" in their parks to handle crowd capacity while during the ongoing renovations in Orlando and Anaheim. reply kernal 23 hours agoprevThe hubris of Apple thinking people would buy a $3500 USD VR/AR headset. reply pirate787 23 hours agoprevI wanted to buy but it requires custom fitted lenses for glasses wearers, and Apple doesn't support my prescription. reply nottorp 23 hours agoparentEven if it did, they should publish the specs and allow me to decide where I get my lenses. reply wnevets 23 hours agoprevThe best part about product launches like this one is that it helps me know whose opinions to ignore. reply mjhay 23 hours agoprevThe hype on VR dying so fast does not bode well for gen AI. Unfortunately, C-suite brass will continue to insist that everyone, deep down, wants it. If they don't think they want it, it's only because they haven't had it shoved in their face nearly enough. reply pennomi 23 hours agoparentVR was always a solution looking for a problem. Gen AI has obvious uses even today, and billions of uses once it becomes smarter. reply bowsamic 23 hours agoparentprevHow are VR and gen AI related? reply mjhay 22 hours agorootparentThey're not particularly related technologically, but they share the characteristic of reaching insane valuations because of whizz-bang sci-fi coolness and neophilia being mistaken for actual utility. The tech industry has been through a few of these. reply pjmlp 23 hours agorootparentprevI guess, Apple coming late to the VR party, when everyone else that did headsets was already in the AI party. reply efnx 23 hours agorootparentprevI think the sentiment is that they’re both hype bubbles. reply rubyn00bie 23 hours agoprevI think about buying a Vision Pro from time to time, but Apple’s lackluster support for gaming keeps me from it. I spend a fair bit of my free time sim racing, and I think the Vision Pro would be amazing for it. But! I really don’t want to spend $3K for an unsupported experience. Apple needs to accept that even a “pro” product needs be usable for non “pro” tasks, and needs to support them. If their Vision line is going to be successful they’re going to need to have more first party support of things like gaming. Partner with Steam, and/or Sony, to get more titles on it. Or pay developers to port games to it and ensure the experience is stellar. It won’t be a short term strategy, but I think a lack of good and unique applications will create a chicken and egg problem. No one wants one, because there are no killer apps, and no one is making those killer apps, because there isn’t a large enough audience. I personally like Sony’s PSVR2, a dumb headset, but has good fidelity. I hope Apples next cheaper VR headset has the same visual quality but less of an OS. reply snapcaster 23 hours agoparentTotally agree with all your points. Honestly if they just made it run mac os x it would be my favorite computer I own reply quux 23 hours agoparentprevCheck out the used markets in your area (Craigslist, FB marketplace, eBay, etc) People are selling used Vision Pro's for a steep discount. In my area (NYC Metro) the lowest I've seen was $1,500 and that was a couple of months ago. When they cross below $1,000 I think I might get one. reply hdhshdhshdjd 22 hours agoprevIt’s kinda interesting seeing Google try and fail to find the next revenue source after search ads, and now seeing Apple try and fail to diversify away from iPhone sales. They are both wildly successful, but I’m ready for a new crop of competitors in the market. reply ShakataGaNai 22 hours agoprevThis is really not a shock. If you wanted one, you bought one already. But the number of people who wanted one is fairly minimal: Super early adopters, those with more money than sense, and developers. It's too expensive for the every-person to buy, too early for even most early adopters, has very little practical uses so far. And those who know, know this device is effectively a devkit in retail clothing. The first iPhone sucked. The first Apple watch wasn't great (and quickly lost support). The first iPod was probably one of the better \"firsts\" because it was better than what was out there, same with the iPad. But it generally takes even Apple a couple generations before their products really get going. Just wait till the AVP2 or 3. reply Theodores 8 hours agoprevI can remember the virtual IO headset from 25 years ago, great for playing games and for watching TV whilst doing the washing up in the kitchen - or so the marketing men said. This last application amused me because nobody does washing up, they just use dishwashing machines or eat disposable junk food, plus the family unit has been atomised. I spoke enthusiastically about these headsets to a friend and his conclusion was that people just would not want these things on their faces. People go out to be seen as well as to see. In recent years we have had people scorn the failed IO Goggles for the latency, low resolution and price, to claim that Meta and others have fixed these problems. They might have done, but they haven't conquered games or transport. You would think kids would be wearing these things on long car journeys and grown ups using them on plane flights. Nothing has changed significantly for the mass market and these Apple VR headsets are clunkier than what we had 25 years ago, I am sorry to say. They are more expensive too. If they were an open platform, a glorified monitor, they would have a niche with VR applications, such as when you want to present a 3D model to someone, for example, a building or a car, where the ability to have a proper walkaround has some advantage in impressing the client. Civilians, whom this product is aimed at, do not and will never have the hard to learn 3D content creating skills. Nobody is going to remodel their kitchen with these gadgets. Really it is for gaming where this product should win. The elephant in the room is glasses, anyone with the money for this gadget wears glasses and they don't really mix. reply tunesmith 22 hours agoprevI've been on the verge of buying it for a while, because I normally work from my laptop and can't be bothered to sit at a desk with a big clunky monitor. OS 1 had disappointing mac mirroring, only slightly bigger resolution than my laptop. But later OS versions talking about large-resolution or panoramic mac mirroring... that's super compelling for my use case. The productivity benefits of larger or multiple monitors, without having to sit at a desk. reply hsavit1 23 hours agoprevno developer wants to waste their time developing apps for a product that nobody other than the .001% will ever use reply gizajob 23 hours agoparentSomeone should make an app that flashes “I AM RICH” on the front screen and plays a klaxxon as you walk around Dubai with your Vision Pro. reply OnionBlender 22 hours agorootparentThe APIs are extremely limiting. You can't do interesting things like draw to the external display. Things like reading QR codes or accessing the camera feed are \"enterprise\" features that require Apple's permission to even access. The docs also suck and too much information is only available in WWDC videos reply gizajob 23 hours agoprevI’ve said it before on this forum: Tim Cook needs to go for this. reply zamadatix 22 hours agoparentFor this specifically? I dunno, I mean let Apple release a new flop AVP every year for the next 5 years and so long as the rest of the company performs like it has during the last decade then it still isn't really more than a dent against the guy. Vanishingly few want leadership out because of a single botched product launch that didn't even prevent the quarterly numbers from being extremely strong. Maybe there are other reasons that factor into why you think Tim should go as well but that's not really \"for this\" then (and probably still not a generally popular conclusion but at least more reasonably based). reply LordKeren 22 hours agoparentprevI’m not sure many influential people at Apple would be looking for Tim Cook to resign over this— especially given that Apple is now sitting at a 3.5 trillion dollar evaluation under his leadership. reply nottorp 23 hours agoprev> Sales expectations will put even more pressure on Apple engineers to design something that can compete with devices like the $500 Meta Quest 3 while justifying the higher price tag. ... and it's not like the oculus thingies have sold in the billions. VR still has no killer app. reply christkv 22 hours agoprev [–] To be fair to Apple, seems Sony is also struggling to shift their VR googles for the PS5. Maybe the mass market novelty of VR is ramping down or is the Meta Quest 3 still selling well? reply oidar 20 hours agoparent [–] Each of the newest headsets are disappointing for different reasons. PSVR 2 is suffering from lack of original content on the PS5 and not making PSVR 1 content automatically backwards compatible - that would make the PSVR 2 a no brainer winner for gaming. Also the fresnel lenses on the PSVR 2 are distracting after you've used pancake lenses. Quest 3 is suffering from image compression for video streaming and poor battery life. The game catalog / app catalog is large, but has the typical android feel for the apps - never smooth, unified, or polished. But I feel in that the Quest 3 is probably the best positioned right now for the average user d/t most content, no wires, and cost. AVP is just too expensive for anyone atm, plus they don't have a catalog of VR games, so they are falling behind. If they had maybe 2-3 AAA games at launch, there would be a different sales arc. Not enough apps or content is available. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple Vision Pro U.S. sales are declining sharply, with a projected 75% drop by the end of August 2024, as most interested buyers have already purchased the $3,500 headset.",
      "The current model has not reached 100,000 units sold, leading Apple to lower its sales expectations and consider a cheaper model, rumored to be around $1,750, for 2025.",
      "Apple is also developing a visionOS update to enhance the user experience, which may help sustain interest until the release of the more affordable headset."
    ],
    "commentSummary": [
      "Apple Vision Pro U.S. sales are underperforming due to limited user interest and developer support.",
      "The high price of $3500 and restricted content are significant obstacles, deterring both consumers and developers.",
      "Analysts suggest Apple may need to reconsider its strategy, potentially by creating a more affordable version or improving its software ecosystem to broaden appeal."
    ],
    "points": 121,
    "commentCount": 209,
    "retryCount": 0,
    "time": 1720725140
  },
  {
    "id": 40946644,
    "title": "Intel is selling defective 13-14th Gen CPUs",
    "originLink": "https://alderongames.com/intel-crashes",
    "originBody": ". alderongames.com Cloudflare 8a234958da1c8298 • 20.109.37.7 •",
    "commentLink": "https://news.ycombinator.com/item?id=40946644",
    "commentBody": "Intel is selling defective 13-14th Gen CPUs (alderongames.com)113 points by signa11 3 hours agohidepastfavorite59 comments ComputerGuru 2 hours agoThis seems like deflection to me. There’s no reproducer, no technical info, no objective facts besides what is basically a meta study that lumps their code quality with intel’s hardware correctness as one metric. If it’s as they say, switching to AMD shouldn’t lead to 100x fewer crashes, it should lead to no crashes. Lumping both 13th and 14th gen hardware together makes it harder to take the claims as easily at face value. 13th gen have been around long enough that you’d think they’d be able to dig up a kernel mailing list bug report or some other corroboration of their accusation. I clicked expecting basically an undocumented errata with specific, 100% reproducible instructions and a deep dive into the internal architecture changes that could have caused this. I’m tempted to flag the submission, honestly. Claims of such magnitude demand at least some baseline evidence. reply thadt 2 hours agoparentIf they were the only ones noticing, I might agree. But there are some groups I trust to know their hardware inside and out, and one of those is RadGames[1]. [1] https://www.radgametools.com/oodleintel.htm reply ndiddy 2 hours agorootparentPrevious discussion: https://news.ycombinator.com/item?id=39478551 reply jchw 2 hours agoparentprev> Lumping both 13th and 14th gen hardware together makes it harder to take the claims as easily at face value. 13th gen have been around long enough that you’d think they’d be able to dig up a kernel mailing list bug report or some other corroboration of their accusation. This is pretty damn reasonable actually. The 14th gen Intel desktop hardware is for all intents and purposes, essentially the same as the 13th gen hardware. It behaves nearly identically, it benchmarks nearly identically. reply sliken 15 minutes agoparentprev> Lumping both 13th and 14th gen hardware together makes it harder to take the claims as easily at face value. 13th gen have been around long enough that you’d think they’d be able to dig up a kernel mailing list bug report or some other corroboration of their accusation. First of all the 13th and 14th gen are nearly the same CPUs. Slight differences in clock, TDPs, and core counts, but all using the identical raptor lake cores. Intel's been widely ridiculed for calling them 14th gen when there's no difference in the cores. There have been various posts about these stability problems going back months on numerous forums. Often things like BSOD, not enough VRAM errors, decompression errors, failure to load game assets, which looks like a NVMe error, etc. Part of the reason it's flying under the radar is a combination of factors. It's not all chips, it's not a specific instruction, and the behavior changes over time by getting worse. Sadly if a game or OS crashes nobody is terribly surprised, which can make hardware problems less obvious. From what I can tell there Intel clock/voltage/temp curves were pushed, trying to be competitive with AMD despite a disadvantage Intel has with their 10nm fabs. Not only does this cause errors, but it causes damage, so the error rate increases over time. The best source found seems to be the telemetry built into unreal engine that provides an unbiased report on crashes (but not hangs, BSoD aren't reported). Sure some gamers overclock, have poor airflow, poorly applied headsink goo, etc. But I found the following pretty compelling since it's based on workstation hardware, without overclocking, using the W series workstation chipset not the Z series consumer chipset which allows overclocking: In a test population of more than 210 W680-based systems, 47.1% of these systems experience at least one incident of instability over a 168 hour test window. This distribution is the same to within 0.4% between Asus brand W680 and Supermicro W680 based boards. For people running these boards on the server side they actually are charging more because of support issues related to CPU replacements. Certainly a failure of 50% per week per node is crazy high and unacceptable on workstation class hardware. reply db48x 2 hours agoparentprevAll computers have a baseline crash rate due to environmental factors such as overheating, cosmic rays, low bit error rates of modern hardware, etc, etc. My 5950X has never crashed, but I bet over a large enough population the crash rate would be measurable and consistent from game to game. reply sysadmin420 2 hours agorootparentI used to be a hard core Intel guy through my career in IT and systems, but man my AM4 has been a super great system, the 5600x was stable, then upgraded to the 5950x to extend the system and it made me a true AMD fanboy overnight. reply fourfour3 2 hours agoparentprev13th and 14th gen desktop parts are nearly identical - so lumping them together makes sense. Edit: 14th gen top end parts are basically 13th gen parts with different binning and higher clocks. reply hdhshdhshdjd 1 hour agoparentprevI’ve had extensive problems with the 13th gen, nearly all of the reporting I’m seeing is what I’ve experienced. Including weird NVME issues that I couldn’t figure out until I saw reports of it being related to these issues. I’m done with Intel. reply jandrese 2 hours agoparentprevThe strangest part of the whole thing is how they claim that the CPUs will work fine initially, but then degrade over time in a deterministic way. How is nobody else seeing this? Are these guys executing code paths that are normally not used? It sounds a bit like their code is buggy in a strange way that doesn't cause problems on AMD but does on later generation Intel, but that wouldn't explain why code would work initially but fail later unless maybe the problem was brought about by a microcode update from Intel? I fully agree that demanding Intel recall all of the CPUs at this point is premature. They've done nowhere near enough research to pin the problem on the processor and not their code. reply jchw 2 hours agorootparent> How is nobody else seeing this? Well, ... they are. This has been being discussed for a while. It came up a bit ago, many people were seeing it in games with a weird error about low VRAM which apparently wound up being linked to Intel's 13th and 14th gen processors. Exactly why is anyone's guess. https://www.tomshardware.com/pc-components/cpus/nvidia-blame... Level1Techs is relatively well-trusted, and they seem to have some contacts that can also corroborate these issues. https://www.youtube.com/watch?v=QzHcrbT5D_Y reply willcipriano 2 hours agorootparentOne idea: Get 500 watt power supply, it powers new chip fine with it's unlimited configuration. 500 watt power supply becomes less efficient over time, now it's not quite 500 watts, parts of the chip brown out. Undefined behavior. reply sliken 3 minutes agorootparentSure, that might explain it, except when people get replacements things are improved, but then start getting worse. Some even are on their 3rd CPU. What's making news is that Intel is starting (in at least one case) to deny RMA replacements after the first, despite shipping defective CPUs as replacements. reply eqvinox 2 hours agorootparentprevHaving built a few power management circuits myself: this is not how PSUs degrade, at least not in the first 5 years, and generally not \"ever\" in most cases. The PSU (notably in this case the M/B VRM, not the ATX PSU) can't really trigger brownouts. 1.05V is 1.05V, and if asked to do so, the PSU will deliver 1.05V. It will just get hotter while doing so over time (and specifically: capacitors degrading.) The voltage references are quite stable over time, and so are the window comparators that ensure \"1.05V\" is between, say, 1.035V and 1.065V. (Voltage references have essentially continuous engineering history back to the 50s≈60s, it's a \"solved\" problem.) reply sliken 0 minutes agorootparentVaries by brand, some use better parts. I've definitely seen file servers that won't boot after 3 years, then I pull a few drives to lower the power needed and it boots. After another year or two I have to pull more drives. Even with generously specced power supplies, server class hardware, redundant power supplies, and enough smarts to stage the drive spin ups to avoid a large peak inrush current when 16 drives try to spin up at once. jandrese 2 hours agorootparentprevThe problem description says it happens gradually over time, but affects roughly 100% of the machines client and server alike. I have a hard time believing that everybody has underspecced and failing power supplies, especially on their servers. reply jchw 2 hours agorootparentprevHmmm, I think this is unlikely. For one thing, most people's power supplies are quite wildly overspecced in gaming rigs since its genuinely pretty hard to find good small ATX PSUs these days. For another, even crummy low-quality PSUs tend to mostly work pretty well for a reasonably long time in most cases, so it'd be weird to see this so much in high-end gaming rigs with brand new CPUs. I also don't know exactly what failure mode you are suggesting with this (voltage rail out of spec?) but I think unless it starts feeding too much voltage into the CPU it is probably unlikely to be able to cause damage. (Instability, sure.) It also doesn't explain why Intel's 12th generation CPUs are still running strong, along with most of AMD's lineup, by comparison. Don't get me wrong, every SKU of CPU has its issues, but all of the major issues with other CPU lines we have explanations for; the issues with Intel Raptor Lake are thusfar unexplained. Intel has blamed motherboard manufacturers quite extensively, but so far the evidence of this is pretty uncompelling, because the failures can apparently also be seen in abnormally high numbers on motherboards that never had unreasonable configurations. reply eqvinox 2 hours agorootparentprev> Are these guys executing code paths that are normally not used? Yes: games are notorious for having poor multithreading and hitting one core with very high load. And unlike servers where you're rarely the only workload, the rest of the CPU will be rather idle on a desktop/gaming system. This pushes the CPU to a much more \"imbalanced\" mode (putting all the power and heat into a very small area) than is common elsewhere. reply hdhshdhshdjd 1 hour agorootparentI have these issues on a dedicated Postgres server running Linux and literally nothing else. It’s not just games, it’s just a lot of gamers buy these chips. reply doctorpangloss 2 hours agoparentprevThe problem is real. Based on Fortnite’s suggested fix in their help, it sounds like there there is some flaw in the Intel hardware that makes it crash later in life when using the shipping voltage, which is maybe its most important parameter. When that is forced to be higher the part gets much hotter and performance per watt declines. reply blain 2 hours agoparentprevI agree there should be something more specific but in the resources links they provide there is an epic games instructions page [1] to do msinfo for people getting crashes for these specific CPUs, so its no just them saying it. [1] https://www.epicgames.com/help/en-US/c-Category_Fortnite/c-F... reply the8472 2 hours agoparentprev> If it’s as they say, switching to AMD shouldn’t lead to 100x fewer crashes, it should lead to no crashes. No, this does not follow, at all. Few things in life are monocausal and systems (here the aggregate of hardware and software) tend to have more than one bug. If one bug dominates and you remove it then of course you're still left with residuals. It doesn't have to be intel's fault though. It could be mobo vendors defaulting to unsafe settings. reply eqvinox 2 hours agoparentprev> This seems like deflection to me. There’s no reproducer, no technical info, no objective facts besides what is basically a meta study that lumps their code quality with intel’s hardware correctness as one metric. It's just laziness. The RAD game tools statement/article[1] has much more detail. I'm not sure to what degree the people behind the current article verified that they're seeing the same issue, but there is definitely a issue (apparently power/scaling/temperature related). [1] https://www.radgametools.com/oodleintel.htm reply risho 2 hours agoparentprevthis isn't something that is just coming from them. there has been a lot of buzz from a lot of sources discussing this exact topic. https://www.youtube.com/watch?v=QzHcrbT5D_Y reply maxwell_smart 2 hours agoparentprevThe links that are included shed more light on things. The write-up linked is a little light on details, and I also expected an actual \"flaw\". What the other links show is a series of anecdotal reports that the k-series intel processors have been found by end-users to become unstable, and that setting more conservative power management settings in the BIOS helps these chips regain stability. The argument is that Intel is selling chips that at their out-of-the-box recommended use patterns can wear out extra quickly, and by default are over-volted. So it's not a \"flaw\" so much as not having a great engineering safety margin for premature wear. reply rcarmo 2 hours agoparentprevThere's been a few interesting videos on this, some like Wendell's - https://www.youtube.com/watch?v=QzHcrbT5D_Y He's reached out to folk who gather game telemetry and got some interesting data to play with. reply jeffbee 2 hours agoparentprevYou can't necessarily publish a 100% repro for glitchy CPUs. It's stochastic both for whether it happens and whether it's detectable or silent corruption. My glitchy 14900K fails with illegal instruction, or sometimes machine checks. But who can say what else is happening that wasn't detected? I can definitely tell you that at least my instance of the 14900K cannot complete a `bazel test` of Abseil without crashing. reply forrestthewoods 2 hours agoparentprev> If it’s as they say, switching to AMD shouldn’t lead to 100x fewer crashes, it should lead to no crashes. No? There’s always multiple sources of crashes. Switching from Intel may eliminate 100% of one crash source, but not 100% of all crashes. One of my favorite stories is that ArenaNet once got Guild Wars servers so stable that any crash could be reliably attributed to hardware failure. Usually memory IIRC. reply ComputerGuru 2 hours agorootparentThat’s kind of my point. They’re clearly running a buggy codebase to begin with that I wouldn’t trust a crash to mean hardware failure. reply eqvinox 2 hours agorootparentCrashes caused by hardware failures look very different from crashes caused by a buggy codebase. 80% of code bugs will have very unisonous crash reports. The other 20% of code bugs — memory corruption and race conditions, notably — have more random crash reports, but are still identifiable as such. Random hardware failures generally cause crash reports that look like someone put your bits in a mixer and made a smoothie. (NB: random hardware failures like we're talking about here. CPU design/logic erratas can and do look like code bugs; but that's not what's being discussed here.) reply tonyarkles 2 hours agoparentprevYeah, I'm super curious here. It's a very bold claim. It's possible that they've got some kind of data race/memory barrier/other threading issue that's more likely to get tickled by Intel processors than AMD processors. Hell, it could even be a compiler bug that generates incorrect assembly that trips more often on Intel processors. reply eqvinox 2 hours agorootparentThere is no such thing as \"incorrect assembly\", modern CPUs don't have undefined instructions like the 6502 did. Either it gets executed deterministically, or you get an #UD exception. reply cpgxiii 1 hour agorootparentThat's not quite true, though. While there are no singular undefined instructions, there are sequences of instructions that can result in undesired/undefined behavior. Read through the published errata for modern CPUs, there will be a large number of combinations of instructions/actions that should be avoided entirely or modified to avoid misbehavior. reply darthShadow 2 hours agoprevRelated Videos: Intel has a Pretty Big Problem (by Level1Techs): https://www.youtube.com/watch?v=QzHcrbT5D_Y Intel's CPUs Are Failing, ft. Wendell of Level1 Techs (by Gamers Nexus): https://www.youtube.com/watch?v=oAE4NWoyMZk reply trynumber9 2 hours agoprevOodle, a very optimized texture compression library used by Unreal, noted some problems months ago. It is a bit odd that after months of this being well-known problem Intel still hasn't released a root cause. Is it an instruction sequence? Is their code subtly wrong in a way that works sometimes on these chips but always on other chips? Are some processors defective? reply signa11 2 hours agoprevhmm, there seems to be this : https://www.youtube.com/watch?v=oAE4NWoyMZk piece on gamer-nexus right now... reply segasaturn 2 hours agoprevThis issue is specifically affecting Intel's K-series line, which are engineered to run up against the CPU's thermal limits. Basically, as long as there is load on the CPU and the CPU is running below 100°C, it keeps cranking up the frequency and voltage until it hits that barrier. I am quite sure that this behavior is what is causing CPU lifespan issues as pumping a theoretically unlimited amount of volts into a CPU can't be healthy. AMD is doing this performance hack on their newest AM5 Ryzen CPUs as well. reply necheffa 2 hours agoparentYou may want to take a look at some of the sleuthing Wendell over on Level1Techs did. He was able to identify a large group of CPUs used in W680 chipset motherboards (i.e. using stock Intel power curves, never seeing a lick of overclocking/overvolting) which exhibited issues. It would seem enthusiast ricing is, at best, an aggravating factor to some other underlying cause. reply mafuy 44 minutes agorootparentYou misunderstood. Some modern CPUs are designed to overclock themselves. The user does not have to do it anymore. It happens automatically. reply necheffa 10 minutes agorootparentIt isn't \"overclocking\" if you conform to the Intel specification. What you are getting at is Turbo boost, which is not a new thing. We can split hairs over the incremental enhancements to Turbo boost over the years of course. reply ThatMedicIsASpy 47 minutes agorootparentprevThe large group of W680 are the game servers running 13900 & 14900. reply necheffa 7 minutes agorootparentIs this just an informational statement or were you trying to highlight a perceived shortcoming of what I had said? If the latter, I am afraid I don't see the point you are hinting at and need more clarification. reply doctorpangloss 2 hours agoparentprevI don’t think any of what you say is true. reply segasaturn 2 hours agorootparent> Intel® Thermal Velocity Boost (Intel® TVB) is a feature that opportunistically and automatically increases clock frequency above single-core and multi-core Intel® Turbo Boost Technology frequencies based on how much the processor is operating below its maximum temperature and whether turbo power budget is available https://www.intel.com/content/www/us/en/support/articles/000... reply librasteve 1 hour agoprevthey are using the FPU heavily - in my experience an FPU design can be prone to gnarly and unpredictable bugs that are related to certain combinations of bit patterns (eg hammering all ‘1’s to all ‘0’s) in successive pipe stages which my be issued in an out of order sequence - it is very hard to diagnose and correct such failures and impossible to screen for them on a production tester reply more_corn 2 hours agoprevAs someone who used to spec data center hardware I assure you that AMD is going to have a series of kick ass quarters. We’ve had a slight preference for Intel for a decade but this probably is enough to flip 50% of large scale cpu purchases for the next year. reply eqvinox 2 hours agoparentWe're not a huge company, but my employer has moved to only buying AMD a while back (a year ago or so, it was a gradual shift but it is complete now). They're just less (costly) hassle. reply hdhshdhshdjd 1 hour agoparentprevYup, I’m having these issues and my next purchases are going to be AMD. reply jeffbee 2 hours agoprevI have my 14900K power capped at 225W/125W using RAPL and that seems to have stabilized it. I don't game with it, but it was unstable under all-core C++ compilation workloads. With these settings it is essentially a 14900. While trying to stabilize it I wasted a lot of time, bought a new motherboard, and bought all new DIMMs. I feel that if Intel can fix it they should provide owners with working replacements, a refund, and a goodwill discount on a future Intel product. reply tibbydudeza 1 hour agoprevI got a 13900 rather - not a gamer but developer so the extra bucks for K variant did not make sense to me - fortunately it seems. reply KennyBlanken 2 hours agoprevThis was widely covered by the tech press back in April and was common knowledge among gaming enthusiasts much earlier. Intel has been working with motherboard suppliers to leave aggressive overclocking features turned on, and it's also pretty clear they're not correctly binning their chips. For a while now Intel hasn't been \"innovating\", just turning up the wick. They're clearly lagging in R&D while also trying to chase the very tippy-top of gaming benchmarks which makes no sense, even for world-class competitive FPS players. Nvidia's been flat out telling people complaining about games crashing on 13th and 14th gen CPUs to 'talk to Intel': https://www.techspot.com/news/102611-nvidia-advises-crash-pr... If you overlooked Ryzen because of issues like memory latency in the early days, I encourage looking at them again. If you do your homework, you can find motherboards that support ECC ram to varying degrees (ie from \"it'll run but with ECC disabled\" to \"error correcting but doesn't report it\" to \"full support, just not listed as such\".) I believe AM4-socket ECC support is much more prevalent, if that's important to you. Familiarize yourself with AMD's Precision Boost Overdrive, which can be used for overclocking and/or undervolting. Unlike AMD's graphics cards (which are space heaters, though the current 7xxx series is an improvement), Ryzen processors are fairly efficient in stock form; more so if you do even a bit of mild undervolting. Don't waste your time with the stock coolers supplied with any of the chips. They'll keep the chip cool enough, but are noisy as hell compared to a well-performing $30 dual-fan air cooler (GamersNexus has found several in their reviews.) Keep your BIOS up to date for AGESA (AMD microcode) improvements, and keep your AMD chipset drivers up to date as well. reply newzisforsukas 2 hours agoprevhttps://community.intel.com/t5/Processors/June-2024-Guidance... I've ran a 13900K from launch, no issues. reply cjbgkagh 2 hours agoparentI guess that proves it reply newzisforsukas 2 hours agorootparentThey've acknowledged the issue for a long time it seems. reply swatcoder 2 hours agoprev [–] Wow, I would not make a public statement like this without narrowing down the issue and making a replication case. Given what little insight they have now, the crash-triggering fault could just as likely be in the peculiarities of their own code, some flaw in their build toolchain, some flaw in a runtime library, etc With all respect to the folks at this studio, this statement reads like a junior dev throwing up their arms and blaming others over a bug they can't wrap their head around. Even if they have a hunch that it's a CPU bug and can't afford to prove it, a more professional approach would be to perform the hardware change to AMD and toss out a more curious and less authoritative statement -- \"Anybody else seeing more crashes on these processors? Have you figured out what from?\" vs \"These are broken and should be recalled\". reply eightysixfour 2 hours agoparentThe links under the \"Resources\" header literally brings you to other companies and journalists confirming the issue. reply KennyBlanken 2 hours agorootparentDownvote and move on, folks. HNers really need to stop feeding the low-effort drive-by commenters. All you're doing is making the rest of us scroll through more to reach comments actually saying something intelligent. reply jeffbee 2 hours agoparentprev [–] Your comment and the rest of these are making me imagine one of those bell curve memes: Idiot: There's something wrong with this CPU Midwit: Nooo it's never the CPU if you think it's the CPU you must be a junior dev Genius: There's something wrong with this CPU reply swatcoder 2 hours agorootparent [–] That's definitely a real curve. And there could certainly be a CPU issue here. My critique is mostly about them trying to make such a strong statement about it with little specific evidence, despite not needing to make a statement at all. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Intel is allegedly selling defective 13th and 14th Gen CPUs, leading to crashes, BSODs (Blue Screen of Death), and other errors, with some reports indicating the issues worsen over time.",
      "Trusted sources such as RadGames and Level1Techs have confirmed these instability problems, which may be linked to Intel's aggressive power and thermal management strategies.",
      "Due to these persistent issues, many users are contemplating switching to AMD as an alternative."
    ],
    "points": 113,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1720799212
  },
  {
    "id": 40943253,
    "title": "GE Aerospace Successfully Develops and Tests New Hypersonic Dual-Mode Ramjet",
    "originLink": "https://www.geaerospace.com/news/press-releases/ge-aerospace-successfully-develops-and-tests-new-hypersonic-dual-mode-ramjet",
    "originBody": "GE Aerospace Successfully Develops and Tests New Hypersonic Dual-Mode Ramjet July 10, 2024 Share Download image/jpeg (523 KB) A dual-mode ramjet was developed and tested in less than 11 months Testing demonstrates a threefold increase in airflow compared to previously flight-tested hypersonic technology demonstrators CINCINNATI – July 10, 2024 – GE Aerospace announced today the successful demonstration of a new, cutting-edge hypersonic dual-mode ramjet. This achievement – which could enable high-speed flight and longer range across numerous multi-mission aircraft – represents the most recent milestone in a diverse portfolio of hypersonic programs. The dual-mode ramjet began testing in March of this year in the clean air, continuous flow, high-speed propulsion testing facility in Evendale, OH, just 11 months after the launch of the design effort. The testing delivered promising results, exceeding performance expectations and demonstrating robust operation of a dual-mode ramjet with a threefold (3X) increase in airflow compared to previously flight-tested hypersonic technology demonstrators. \"The rapid progression from design to testing underscores our commitment to driving innovation in hypersonic technologies,” said Amy Gowder, president and CEO of Defense & Systems at GE Aerospace. “This milestone not only shows the exceptional talent and dedication of our team but also reaffirms our position as a leader in the pursuit of hypersonic flight.\" The successful development and testing of the dual-mode ramjet in such a short period of time was made possible through the collaboration of GE Aerospace’s team of engineers, Innoveering – a company acquired by GE Aerospace in 2022 that specializes in hypersonic propulsion – and GE Aerospace’s Research Center. “The technology’s robust performance paves the way for the next phase of development, which will focus on continued testing and technology demonstration in alignment with our roadmap for integrated high-speed propulsion solutions,” said Mark Rettig, vice president & general manager of Edison Works Business & Technology Development at GE Aerospace. About GE Aerospace GE Aerospace (NYSE: GE) is a global aerospace propulsion, services, and systems leader with an installed base of approximately 44,000 commercial and 26,000 military aircraft engines. With a global team of 52,000 employees building on more than a century of innovation and learning, GE Aerospace is committed to inventing the future of flight, lifting people up, and bringing them home safely. Learn more about how GE Aerospace and its partners are defining flight for today, tomorrow and the future at www.geaerospace.com. Media Contact: Amanda Mayfield Amanda.Mayfield@ge.com 321-442-1259",
    "commentLink": "https://news.ycombinator.com/item?id=40943253",
    "commentBody": "GE Aerospace Successfully Develops and Tests New Hypersonic Dual-Mode Ramjet (geaerospace.com)113 points by NoRagrets 12 hours agohidepastfavorite116 comments TrainedMonkey 8 hours ago> A dual-mode ramjet was developed and tested in less than 11 months Incredibly impressive if true. But below quote makes it sounds like they purchased a company with mostly ready design and made a prototype. Still pretty awesome. > The successful development and testing of the dual-mode ramjet in such a short period of time was made possible through the collaboration of GE Aerospace’s team of engineers, Innoveering – a company acquired by GE Aerospace in 2022 that specializes in hypersonic propulsion – and GE Aerospace’s Research Center. Now I want real engineering / everyday astronaut like breakdown of how it works. reply vasco 7 hours agoparent> Incredibly impressive if true. But below quote makes it sounds like they purchased a company with mostly ready design and made a prototype. Still pretty awesome. We're probably reading a lot from a few sentences but I had a similar impression. Probably more similar to writing a software project for the second time after you throw away your first version - but now with more resources. Like you said still pretty cool. reply colechristensen 4 hours agorootparent>more similar to writing a software project for the second time after you throw away your first version - but now with more resources All aircraft design is basically this. You don't/can't ever really start from scratch, there's an enormous volume of historical aircraft designs and all new planes look very similar to old planes. There are very rarely opportunities to solve problems in new ways. reply bobthepanda 30 minutes agorootparenteven if you have a novel design your customers are rarely interested. Airlines are extremely conservative because pilots are expensive and hard to train, which is how we got the whole 737MAX fiasco instead of a clean-sheet, optimized design. Though i wonder why they didn't go for commonality with the 787 similar to how the a330 and a350 are common type ratings. reply throwaway81523 11 minutes agorootparentSays hypersonic ramjet in the title, so the initial applications may be missiles rather than aircraft. It does talk about aircraft use in the article. reply nimbius 5 hours agoparentprevi for one look forward to experiencing this next generation of ramjet technology firsthand in my next three-star motel room air conditioner, the most common and widespread use of ramjet technology in the United States today. reply schmidtleonard 4 hours agorootparentI just got a new battery-powered air duster that says \"TURBO FAN\" on the side. It's about 100,000 horsepower short of an actual turbo fan, but it's a good duster and I enjoy the enthusiasm. reply SAI_Peregrinus 3 hours agorootparentThere are some RC aircraft turbofans available, very loud and fuel hungry but not quite 100,000 HP over your air duster. reply Scene_Cast2 2 hours agorootparentOh interesting. I've only seen turbojet and turboprop variants (with a single stage centrifugal compressor) for sale. reply fooblaster 1 hour agorootparenthttps://www.chiefaircraft.com/radio-control/turbine-gas-glow... wild reply kube-system 3 hours agorootparentprevIf this is a joke about GE's appliances -- that version of GE is long in the past. GE has pretty much been dismantled and is just an aerospace company at this point. What was once GE Appliances was sold to Haier 8 years ago. reply dmoy 1 hour agorootparentI thought it was a joke about how noisy hotel HVAC is (at least in cheaper hotels) reply inasio 3 hours agorootparentprevI heard a rumour that you can find repurposed project Orion technology in 2-star motel ACs reply api 3 hours agoparentprevWhen companies reach a certain size they basically become banks, either funding smaller teams under their umbrella to do things or purchasing and integrating them into a larger portfolio. If they purchased the design it's because GE Aerospace has the manufacturing and sales arms to actually make it real. reply class3shock 5 hours agoprevAnother similar thing from the end of last year: https://www.geaerospace.com/news/press-releases/other-news-i... They really seem to love to talk about how fast they move except wait: https://www.geaerospace.com/news/articles/100-year-anniversa... Per the above in 2019 they had been developing RDE technology for 5 years. Now developing the tech is different then building a prototype but it's good to take these timetable numbers with a large dose of salt. It is interesting to see RDE's getting alot of headlines recently. Really cool tech that works off of a principle known for a long time but only just recently hitting an inflection point in terms of development. Cool video from Scott Manly talking about RDE's: https://www.youtube.com/watch?v=rG_Eh0J_4_s reply bobthepanda 10 hours agoprevHow much fuel does it use, I wonder? Supersonic has been DoA for a while because post-deregulation airlines are only interested in ever more fuel efficient aircraft. (It’s also why major states like Japan, China and Russia have for the most part failed to build a competitive airliner against the duopoly.) reply shiroiushi 10 hours agoparent>It’s also why major states like Japan, China and Russia have for the most part failed to build a competitive airliner against the duopoly. I don't think that has anything to do with it at all. Building a competitive large airliner requires a huge industrial base. And since it's so safety-critical, it takes a lot of time and expertise to build the reputation needed to unseat the incumbents. China has an aircraft manufacturer, but time will tell if it goes anywhere, since they don't exactly have a reputation for high quality. Russia has built many large aircraft in their Soviet days, but their industrial base has eroded so they probably just don't have the capacity to build competitively-priced aircraft in volume now, plus they're not well-liked by most countries that would want airliners anyway. Japan might be able to do it, but just isn't geared up for building lots of large aircraft so it would be a huge investment to build such an industry almost from scratch. Fuel efficiency isn't a factor because building an airliner doesn't involve building engines for that airliner, and engines determine an aircraft's fuel economy for the most part. Boeing and Airbus don't make jet engines: those are all made by Rolls-Royce, Pratt & Whitney, and GE. reply sofixa 10 hours agorootparent> Russia has built many large aircraft in their Soviet days, but their industrial base has eroded so they probably just don't have the capacity to build competitively-priced aircraft in volume now, plus they're not well-liked by most countries that would want airliners anyway Russia did design and build a few designs recently - SSJ-100, MS-21, Tu-204/214. Almost all of them used Western parts (such as engines/avionics) to speed the process up because Russian manufacturers are a few decades behind in some critical parts (like engine efficiency). That being said, they're currently trying to ramp up production on a domestic only Tu-204/214 version, but it's slow, probably quite expensive, and super inefficient. > Japan might be able to do it, but just isn't geared up for building lots of large aircraft so it would be a huge investment to build such an industry almost from scratch. Mitsubishi had a regional jet design in progress, the SpaceJet, which got cancelled after a few billion in R&D and years in delays. Ukraine also had decent manufacturing facilities and capabilities with Antonov and Motor Sich, including decent recent designs and subcontracting work for other companies.. but with the war a lot of things were destroyed (like Antonov and Motor Sich factories and facilities), and their focus is mostly on drones and missiles now. > Fuel efficiency isn't a factor because building an airliner doesn't involve building engines for that airliner, and engines determine an aircraft's fuel economy for the most part. Boeing and Airbus don't make jet engines: those are all made by Rolls-Royce, Pratt & Whitney, and GE. Fuel efficiency is a major factor. Outside of countries big enough to prop up a domestic airliner industry with enough spare cash to compensate for inefficiency (which is a list of 1 country, China), nobody will buy a jet which is less efficient than an alternative. Airline margins are razor thin. And not everyone can buy an engine from Rolls-Royce, GE/CFM(their joint venture with Safran that designs manufactures short-haul engines and the future open fan design) and Pratt and Whitney. The Russian UAC cannot. The Chinese COMAC got sold a decade+ old variant for their latest jet out of fear of industrial espionage (the Chinese engine manufacturers have failed at producing reliable and efficient jet engines for any application, usually relying on Russian designs). TL;DR is that modern airliners are extremely complicated, expensive, and have very long lead times. A new design won't be making any money within the first 10-15 years of its life, which means that only countries with very developed and advanced industrial bases, lots of money, and the desire for strategic airliner autonomy will invest in them. reply throwaway4good 8 hours agorootparentBoth China and Russia are building alternatives to Boeing/Airbus commercial airliners. But are in two completely different situations. COMAC relies on international suppliers, probably hoping that will make it easier for them to be allowed to fly outside China and then over time they can use more and more domestic suppliers. Whereas the Russians are completely cut off from international suppliers and travel, and have to resurrect their airliner projects which have been more or less dormant since the Soviet times. I think both will be succesful over time; they have huge internal markets that can sponsor quite a bit of development cost. reply TheOtherHobbes 8 hours agorootparentChina might. Russia has decided to move its economy in a different direction. reply throwaway4good 7 hours agorootparentEven at war and cut off from the world the Russian economy is growing. Driving by buoyant commodity prices. They have tech for civilian aviation in their massive military industry. And just the sheer size of the country guarantees the demand. reply phonon 5 hours agorootparentRussia's GDP is a bit more than half of California's. Civil aviation there is a few hundred planes (about the size of Southwest--in total), and is heavily government subsidized. https://www.reuters.com/business/aerospace-defense/russia-sp... reply cpursley 3 hours agorootparentThe reason they dipped in terms of planes is directly related to sanctions and western countries pulling out based on the article you shared. Worth noting, the foreign parts Sukhoi Superjet 100 (a very good regional jet) are in the process of being fully \"Russified\" and the MC-21 is actually a true domestic plane vs the Chinese C-919: - https://aviacionline.com/2022/05/sukhoi-tests-100-russian-av... - https://www.youtube.com/watch?v=E32TCdBgkoY Also, Russia just surpassed Japan to become #4 in terms of GDP PPP (the number that matters), was recently reclassified as a high-income county and is expected to experience relatively high GDP growth, and that's even under the heaviest sanction regime on the planet x3 and after getting cut off from SWIFT: https://www.worldbank.org/en/about/leadership/directors/eds2... reply sofixa 3 hours agorootparent> Worth noting, the foreign parts Sukhoi Superjet 100 (a very good regional jet) A very good based on what measurement? Operators have complained about extremely poor reliability, and there have been a high amount of crashes. > and the MC-21 is actually a true domestic plane vs the Chinese C-919: The MS-21 isn't domestic, it uses Western avionics and engines. There's a russified version which is supposed to be launched in 2025-2026, but it's unknown if that would happen considering it's using the Aviadvigatel PD-14 engines which are still undergoing design refinement and testing. It could literally be years before the engines are ready, so when the jet enters service is anyone's guess. > Also, Russia just surpassed Japan to become #4 in terms of GDP PPP (the number that matters), was recently reclassified as a high-income county and is expected to experience relatively high GDP growth, and that's even under the heaviest sanction regime on the planet x3 and after getting cut off from SWIFT: A big issue with these metrics is that they use the official exchange rate for RUB to USD exchange rate, which is entirely theoretical since it's impossible to trade the two due to the sanctions. The real exchange rate is probably much worse for the RUB, so these metrics based on the official theoretical one are highly misleading. reply MaxPock 53 minutes agorootparentprevHow much steel and electricity does California produce ? reply willcipriano 2 hours agorootparentprevCalifornia spends 50k per homeless person[0] and can only house them when Xi Jinping[1] comes to town. [0]https://www.cbsnews.com/sanfrancisco/news/california-homeles... [1]https://wpde.com/news/nation-world/san-francisco-cleans-up-b... GDP is sort of a nonsense metric for \"ability to get things done\". A huge portion of that GDP are things like rents extracted via intellectual property so it doesn't tell you anything about ability to manufacture anything. Example: California woman sends her kids to daycare a few weeks after birth and goes back to her job (high GDP), Russian woman stays home with her children (no GDP). Now wait 50 years until you only have neglected iPad kids raised by busy parents left, how many planes you building now? And the ones you manage to build? Maybe they start falling out the sky, doors coming off the hinges, that sort of thing. Maybe the engineers don't have enough of a backbone to stand up to their boss because they never really experienced secure attachment, and some people die. reply dralley 7 hours agorootparentprevThe economy is growing because of massive government expenditures on defense. The same thing happened with Nazi Germany. It's not sustainable. \"Buoyant commodity prices\" is also known as inflation. They have high inflation despite having a federal interest rate of 18%, which is sky high. Not great signs for their economy. reply HPsquared 5 hours agorootparentSilicon Valley was built on defense spending. reply signatoremo 3 hours agorootparentNot at the same scale. At the peak of the Vietnam War, the US spent less than 10% of GDP on the war. Most of the time it was only half of that (for comparison it’s less than 3% at the moment). Russia currently spend about 40% of the GDP on Ukraine war ( coming off my memory, may not be 100% accurate). That can’t be sustainable and they will have long lasting impact. reply cpursley 3 hours agorootparentIt's 6% which is not a dealbreaker for a country with one of the lowest GDP to Debt ratios (assuming the war does not stretch for another 5 years). https://carnegieendowment.org/russia-eurasia/politika/2023/0... reply racional 3 hours agorootparentprevThe figure I've seen for Russia's spend is 40% of its budget, not its GDP. More like about 10 percent of GDP. Still an insane amount, for a war that has been obviously insane from the very beginning. But a different number, in any case. reply treflop 1 hour agorootparentprevEven if it was, many of the products that came out were not defense products. Nazi Germany spent defense money to build defense products. The war was going to end eventually. Like spending too much money on remote work products during a pandemic that will eventually end. It’s going to blow up. reply brnt 4 hours agorootparentprevNobody is interested in 1970’s airliners though. reply HPsquared 4 hours agorootparentIn that part of the world, if those are the only ones available... reply brnt 2 hours agorootparentThen it aint no Silicon Valley. reply conradev 4 hours agorootparentprevCOMAC’s C919 is flying domestically and their next goal is EASA type certification I imagine it will be more geopolitics than anything else, but the EASA says 2026 is “too soon”. reply sofixa 3 hours agorootparent> I imagine it will be more geopolitics than anything else, but the EASA says 2026 is “too soon”. Certifying a new jet from scratch is usually a multi-year endeavour, so why would it be geopolitics? The C-Series, latest jet to be certified, took a good 2.5 years from first flight to type certification. And the C919 hasn't started EASA certification yet. reply throwaway4good 8 hours agorootparentprevAnd the Chinese have a modern engine under development that can be used in a commercial airliner: https://en.wikipedia.org/wiki/ACAE_CJ-1000A reply shiroiushi 9 hours agorootparentprev>Fuel efficiency is a major factor. Outside of countries big enough to prop up a domestic airliner industry with enough spare cash to compensate for inefficiency (which is a list of 1 country, China), nobody will buy a jet which is less efficient than an alternative. No, it's not really a factor, because, as I wrote before: airliner manufacturers don't make jet engines. They can only get engine manufacturers to build them engines, and integrate those into their airframe designs. Of course, actually having access to modern, efficient engines is necessary to be competitive, which as you point out is problematic for Russia and China, but if, for instance, a major Japanese company decided to suddenly get into the jumbo-jet business, this shouldn't be an issue for them, provided the existing engine manufacturers have enough manufacturing capacity to provide them engines in addition to those they provide to Airbus/Boeing. So yes, being fuel-efficient is important to the end customers (passenger airline companies), but for the airframe makers, that's really determined by the engines they have access to, not really by any technical capabilities of the aircraft makers. The problem with lack of supply in large passenger jets isn't the engines (AFAIK), it's really just the Boeing/Airbus duopoly and how quickly they can build new aircraft, which use the engines from those suppliers. reply twelve40 8 hours agorootparent> company decided to suddenly get into the jumbo-jet business, this shouldn't be an issue for them I don't think it's that simple. You can count jumbo jet makers and their engine suppliers on one hand. It's not like you can go shop for a new line of engines on Craigslist, these things are designed with the plane maker in mind and production numbers planned for a decade+ ahead. Both parties have to work very closely together to produce something with acceptable fuel efficiency. reply ethbr1 2 hours agorootparentprev> Chinese COMAC got sold a decade+ old variant [engine] for their latest jet out of fear of industrial espionage The most cyberpunk thing I've ever seen is the international aerospace company bunkers at Beijing's airport. Literally, bunkers. Barbed wire. Structural access control gates. Tiny one-room building. I assumed that was the only privileged link back to HQ for information connectivity. reply notahacker 9 hours agorootparentprevthe major engine OEMs aren't investing enormous amounts in optimising for a startup airframe program though, so at best you're getting a derivative of the engine on the equivalent sized Boeing/Airbus program with roughly the same fuel efficiency but worse expectations in every other relevant area (expected reliability, competitive local maintenance markets, type-rated crew availability, commonality with existing fleet, residual values etc, and probably worse aerodynamics and structural weight) due to the airframe program's relative immaturity reply anovikov 9 hours agorootparentprevFuel efficiency is more about plane itself (aerodynamic optimisation as well as weight saving) than about engines. reply bboygravity 7 hours agorootparentIt's not. The most fuel-efficient aircraft possible would be an electric one (for the sake of argument lets assume batteries of high enough Wh / kg would exist) flying at much higher altitudes (20km+) than the current fuel and oxygen fired engines. That definitely involves both engine and aircraft design. Even for jet engines: an engine from the 50's is not going to cut it in terms of fuel efficiency on ANY airframe compared to current flying machines. Even if you had unlimited budget and time to design the perfect airframe for those engines. reply sofixa 5 hours agorootparent> (for the sake of argument lets assume batteries of high enough Wh / kg would exist) Highly unlikely, it's not even close right now, and nothing in recent progress indicates that soon we might have batteries that are denser and lighter by a factor of 10 any time soon. https://www.youtube.com/watch?v=VNvzZfsC13o reply tonyarkles 5 hours agorootparentprev> weight savings > engines At this point the 737 Max 8 and A320 both are sitting around an 18:1 L/D. You can't really go a whole lot higher than that without affecting controllability. Things like winglets give you incremental improvements on the order of single digit percentages. But weight savings... that's huge. For every pound you shave off, all other things equal, means the engine needs to make less thrust and the aircraft itself needs to carry less fuel. reply nine_k 7 hours agorootparentprevI wonder where Embraer fits in your picture. The Ukrainian aircraft industry was indeed interesting, but it was all about cargo planes. An-225 was a magnificent device, but it could not serve as an airliner. Neither could An-124. BTW both were designed back in the USSR times. reply sofixa 6 hours agorootparent> The Ukrainian aircraft industry was indeed interesting, but it was all about cargo planes Not exclusively, they had the An-148/158 regional civilian airliners (and related An-178 military cargo jet). They definitely had catching up to do (the An-148/158 didn't perform really well, in large part due to issues with the supply chain which was part Ukrainian, part Russian, and the war(s) between the two complicated things a lot), but there was a good base on which to build. > I wonder where Embraer fits in your picture. They're the only ones that can do it, but probably won't because they don't have the capital and there's the example of Bombardier which bankrupted themselves trying to make a new short to medium haul airliner. reply notahacker 9 hours agorootparentprev> A new design won't be making any money within the first 10-15 years of its life, which means that only countries with very developed and advanced industrial bases, lots of money, and the desire for strategic airliner autonomy will invest in them And to add to that, the desire for strategic airliner autonomy tends to be inversely proportional to domestic companies' chances of non-trivial international sales. There will be a few airlines and lessors that passed on the Russian SSJ-100 out of longstanding concerns about Russian supply chains (even though it had an Italian marketing JV so you didn't necessarily have to deal with them directly) that will be patting themselves on the back for making the correct decision, and presumably the Russian state picked up the tab for all the losses Sukhoi made on the programme. reply teractiveodular 7 hours agorootparentThe SSJ's track record of three hull losses, including one on a demo flight that killed 45 sales reps and prospects, probably isn't helping. reply notahacker 7 hours agorootparentTbf that one was counted as pilot error IIRC, but never a good idea to kill your customers. The few non Russosphere SSJ customers pretty much all going bust except for CityJet who dropped them from their fleet citing reliability issues probably didn't help either reply holoduke 7 hours agorootparentprevAirbus is heavily depending Russian titanium. Even with the current tensions they decided to keep buying Titanium from Russia. Money is the only reason why players are not having inhouse solutions. Give it time and reason and any country can get a fuel efficient engine. reply lobsterthief 7 hours agorootparentIt takes an insane amount of expertise and endurance testing to produce a safe, reliable jet engine. These companies have been doing so for 60+ years at this point. There’s a lot of knowledge and investment that has allowed them to produce these things at any kind of scale at this point. I used to work at Pratt & Whitney, and trust me a LOT goes into this. We also worked with some of the Rolls-Royce guys. There’s a reason Boeing and others don’t even attempt it. reply vl 2 hours agorootparentBoeing used to make engines and even had their own airline. They were split due to anti-trust regulations. Airline became United, and engine manufacturer - Pratt & Whitney! reply lstodd 9 hours agorootparentprevwell, ssj and ms10/21 were attempts at partially outsourcing, yes. 204/214 are no longer built I think. but the PS90 turbofan is still the mainstay along with D30 derivatives. avionics is a separate question ofc reply sofixa 9 hours agorootparent> 204/214 are no longer built I think. They are now being built again, because it's the only design that wasn't built with the assumption that Western providers (of avionics, specialised parts, engines) will be available and therefore it's the easiest one to manufacture with an entirely Russian supply chain. Still painful because most of that supply chain is decades old and very rusty, but theoretically possible. reply Beretta_Vexee 7 hours agoparentprevIt's a hypersonic thruster, i.e. 5 or more times the speed of sound. Far too fast for civil aviation. It's probably even too fast for \"safe\" manned flight. This is the speed at which the air ionizes around the vehicle. It's like flying inside a fireball. The first use that comes to mind is a hypersonic cruise missile or unmanned bomber. Intercontinental missiles are also hypersonic, but their re-entry vehicles are gliders with limited maneuvering capabilities. The Russians have developed and used hypersonic air-to-ground missiles in Ukraine, the Kinzhal. It is very difficult to intercept a missile at these speeds, and almost impossible if the missile is maneuverable. This has re-launched research into this type of missile and propulsion system. Russia is ahead in this field. The USA, China and France have carried out various experiments (mach 5 to 25), but do not really have combat-ready hypersonic weapons. One of the great difficulties of hypersonic flight is that you generally need one engine and one stage per flight regime. The majority of demonstrators operate according to the following scheme: - a rocket or aircraft propels the vehicle to supersonic speed, so that it can start its engine. - The vehicle accelerates to Mach 5. The vehicle starts its hypersonic engine and detaches from the supersonic stage. - Climb to the edge of space. - Descent and acceleration. - Detachment from the hypersonic engine and final gliding approach. The fireball moving off at mach 20 kind of approch. The engine shown here would enable to have just one engine for supersonic and hypersonic flight, greatly simplifying the vehicle. The fact that it's an engine and not a rocket is important, as it means that the thrust is much longer and can potentially last for the whole flight. At these speeds, it's largely the engine's thrust that makes maneuvering possible, and this is what distinguishes these vehicles from the hypersonic glider/reentry vehicle found on intercontinental missiles. reply dralley 6 hours agorootparentIf you define \"hypersonic weapon\" as merely traveling at or beyond Mach 5, then practically every medium range ballistic missile since the V-2 qualifies. There is nothing special about a missile that can hit hypersonic speeds. The real difference is maneuvering, but most of Russia's \"hypersonic weapons\" cannot actually do this. Hence why Ukraine has shot down nearly every Khinzal that has been fired at Kyiv and about half of the Zircon missiles too, using technology that has existed for decades. IIRC one of the Khinzal missiles was even knocked out by a PAC-2 missile, which wasn't designed for that purpose. Maybe AvanGuard is more competent and less overrated/overhyped than their other efforts, but there's no particularly good reason to believe that. Meanwhile the last time the US successfully tested a hypersonic weapon it was immediately greenlit for serial production, and I don't think that's because they are freaking out so much as that the US sandbags our capabilities while some of our competitors inflate theirs. You claim the Russians are ahead, but they keep putting their hypersonic engine researchers in jail. I don't buy it. reply jandrewrogers 2 hours agorootparentprev> The Russians have developed and used hypersonic air-to-ground missiles in Ukraine, the Kinzhal. It is very difficult to intercept a missile at these speeds, and almost impossible if the missile is maneuverable. Speed does not make hypersonic missiles difficult to intercept. They are considerably less maneuverable than slower missiles due to the limits of material physics. The primary advantage of hypersonic weapons is that it reduces the available reaction time of the target such they may not be able to respond effectively or mount a defense. Any \"difficulty of intercept\" is predicated on there not being enough time to execute the intercept after the missile is detected. US air defense systems have very low reaction latencies by design, as demonstrated in Ukraine. I am unclear why anyone thinks Russia is ahead in this field, beyond their penchant for marketing ballistic missiles like Kinzhal as \"hypersonic\". The US has been building and testing hypersonic weapon systems longer than most people have been alive. Unlike Russia, the US requires hypersonic missiles to have precision terminal guidance, which is an extremely difficult engineering problem within the atmosphere. It took decades for the US to figure out how to reliably deliver terminal guidance for hypersonics. reply FuckButtons 5 hours agorootparentprevKinzhal isn’t really a true hypersonic weapon. They just strapped an iskander tbm to a su34, which is why the pac3 patriot interceptors can actually shoot them down, they were developed to intercept tbms. reply ahoka 3 hours agorootparentIt’s like the US was yeeting Lance missiles from F-106s and insisting that it’s a hypersonic weapon and it’s ahead of everyone else. Pathetic. reply pfdietz 3 hours agorootparentprev> It's a hypersonic thruster, i.e. 5 or more times the speed of sound. Far too fast for civil aviation. It's probably even too fast for \"safe\" manned flight. This is the speed at which the air ionizes around the vehicle. It's like flying inside a fireball. Ionization occurs at considerably higher temperature. The hypersonic regime is described as when vibrational excitations of air molecules begin to significantly affect the specific heat. After that, dissociation sets in, and only after that does ionization become significant. Temperature ranges, as I understand it, are (for air at 1 bar): Vibrational excitations: > 800 K Dissociation of oxygen: > 2500 K (nearly complete ~4000 K) Dissociation of nitrogen: > 4000 K (nearly complete ~9000 K) Ionization: > 9000 K Lower pressure will reduce the temperatures at which dissociation and ionization become significant. reply whalesalad 10 hours agoparentprevThis is hypersonic. Way faster, way more heat just from moving through air. We will not see passenger aircraft going hypersonic for a long time. It will be hard enough to make a hypersonic aircraft that doesn’t melt itself during flight. The main reason supersonic flight is dead is the sonic boom. Until that issue is resolved they are banned over land, unfortunately. reply af78 9 hours agorootparentnext [–]The main reason supersonic flight is dead is the sonic boom Is it? The Concorde would fly at subsonic speeds over land and move to supersonic over the Atlantic. Given the reduced flight time at supersonic speeds I thought economic considerations prevailed. reply rsynnott 9 hours agorootparentThe sonic boom made Concorde's economics harder, but they'd have been hard anyway. It confined it to largely only being useful for that transatlantic route, and that was borderline economically viable for airlines. However, as there was really only one route where it was useful, there were not many Concordes flying and the fixed costs were considerable. Eventually Airbus pulled the plug, and ended maintenance, and that was the end of that. It was probably spurred on by a high-profile disaster, but even without that, the writing was on the wall; if you were an airline Concorde looked economically viable, but without subsidy by the state or by Airbus or both, it never had been. In a world where there had been many viable Concorde routes, this could all have worked out a lot differently, Concorde B might have happened (which would have allowed a Pacific crossing), and maybe we'd still see supersonic airliners. reply kitd 7 hours agorootparentEven without the sonic boom, it was painfully loud on takeoff and final approach. It used to fly over us near Heathrow and the ground would shake as it went over at a few thousand feet. reply rsynnott 7 hours agorootparentConcorde B, apparently, would’ve fixed that (though not the sonic boom, of course). reply filereaper 9 hours agorootparentprevGenuinely trying to understand. Didn't the SR71 fly between LA and NY at above supersonic speeds? Don't military jets fly at supersonic speeds above land? What makes this allowable in terms of sonic booms? Is it that they're much smaller and the their booms don't cause as much of a \"wake\" on the ground? reply rsynnott 7 hours agorootparent> Didn't the SR71 fly between LA and NY at above supersonic speeds? Don't military jets fly at supersonic speeds above land? I think not routinely in either case? Most supersonic aircraft can't actually sustain supersonic speeds for a significant period; the Concorde and Tu-144 are, as far as I know, the only aircraft ever designed with supersonic flight as the effective _default_ mode of operation. For both planes, _getting_ to supersonic was rather expensive, and required the use of afterburners, so an ideal flight profile would involve ramping up to supersonic once, and then staying that way for the rest of the flight. The SR71 was, unusually, _capable_ of sustained long-range supersonic flight, but I can't imagine it was routinely used above land when not actually on missions; why would it be? reply kube-system 3 hours agorootparent> The SR71 was, unusually, _capable_ of sustained long-range supersonic flight, but I can't imagine it was routinely used above land when not actually on missions; why would it be? Everything a military plane does is a mission, and the SR-71 absolutely did supersonic training missions over the US. And as I understand, the military can and still does do supersonic training missions over land, but they do it at higher altitudes and in locations where it isn't disruptive to population centers. reply ceejayoz 5 hours agorootparentprevThe F-22 can also cruise at supersonic speeds without afterburners. https://en.wikipedia.org/wiki/Supercruise. reply rsynnott 5 hours agorootparent_Can_, but does it routinely outside of training/deployment situations? Like if you're just moving one from one side of the US to the other, you're probably going subsonic. reply kube-system 3 hours agorootparentprev> Don't military jets fly at supersonic speeds above land? It can happen, but it is disruptive at best. It isn't something you'd want happening around a populated area on a routine basis in peacetime. And it typically is not, unless under emergency circumstances. https://www.youtube.com/watch?v=G8G-mgKycUM reply denton-scratch 2 hours agorootparentThd only time I've ever heard a sonic boom was as a kid, on holiday in the south of France. They would regularly fly Mirage jets over the beach resorts. I suspect these flights were for when Gen. de Gaulle was in residence at the Chateau d'If. reply adgjlsfhk1 8 hours agorootparentpreva couple things: 1. smaller lighter, and higher flying reduce the sonic boom size 2. way fewer planes 3. the military gets some latitude to ignore the rules that everyone else has to follow reply twelve40 8 hours agorootparentprevsmaller, and SR71 could cruise at 85000 feet reply dghughes 7 hours agorootparentprev>The sonic boom made Concorde's economics harder... I live in south-east Canada in the province of PEI and I hope no supersonic aircraft are made. Right now all commercial air traffic from NA flies right over my house. Especially every evening the rumble of high flying jets can he heard. It's not loud but it's certainly noticeable for something 10,000m above me. reply lstodd 9 hours agorootparentprevThis I agree with. There were not enough viable routes. Same thing happened with TU-144 - no destinations. reply twelve40 8 hours agorootparentSad for both. Saw Concorde at Le Bourget. Both were miracles of engineering in their day. reply jjk166 3 hours agorootparentprevIt's a little more complicated then that. The main impetus for the development of the Concorde was actually fuel efficiency - once you get high enough above the sound barrier supersonic flight is more fuel efficient than subsonic flight, all else being equal. Back in the 50s, as fuel costs became more and more of a concern for airlines, this seemed like the obvious future: you get your passengers to their destinations faster, you spend less on fuel, and you can make more flights per day. So Super Sonic Transports (SSTs) looked great on paper. SSTs need to use turbojet engines, which all passenger jets in the 50s were using anyways. But shortly after Concorde's design was locked in high bypass turbofan engines were developed whose fuel efficiency was way higher than turbojets, and indeed much higher than the jump to supersonic flight could achieve. Now it was a choice between speed and fuel efficiency. This really could have gone either way, but when Boeing got out of the SST game they lobbied for overland supersonic flight to be restricted because of sonic booms, which eliminated the potential domestic market. Concorde was reduced from the future of air travel to an offshoot for a niche market. Still it was profitable. People were willing to pay a premium for faster travel, it just wasn't a big enough success to warrant further development. The aircraft aged, unable to take advantage of many of the improvements that came to subsonic jets, and there was no replacement in sight. The Concorde was increasingly expensive to operate compared to more modern aircraft, not because of fuel but because of maintenance and staffing. Between a high profile Concorde crash in 2000, then the reduction in aircraft travel after 9/11, and the security increases which essentially tacked on 2 hours to every trip by air, the Concorde ceased to be viable. reply lstodd 9 hours agorootparentprevGreen and nimby bullshit as usual. I lived at a summer home in 80s which was near a (soviet) air force base for many summers, and listened to sonic booms daily. In no way it was even disturbing. reply dotnet00 2 hours agoparentprevSupersonic has also been dead because passenger supersonic flight is banned over the continental US (and probably similar restrictions exist in Europe). In the US the regulations will be getting reviewed depending on the results of studies on building supersonic jets that make less noise (after which it'd be down to if they can be made efficient enough to be profitable). reply kevin_thibedeau 40 minutes agoparentprevYou don't need to make an airliner. There are enough billionaires in the world to support a small industry of supersonic private jets. All of the marketing hype about supersonic for the masses is a figleaf to avoid being pilloried over tech that only serves the ultra wealthy. reply dghughes 7 hours agoparentprevHalf for a cheeky answer. The air is part of the fuel and onboard fuel hydrogen or some other thing to burn is the other half. Really at >Mach 5 you'll be there fast. At Mach 5 NY to London in 54 min. Mach 10 it's 12 minutes. reply 15155 7 hours agoparentprevCruise missiles have different efficiency goals than commercial airliners. reply GartzenDeHaes 7 hours agoparentprev> which could enable high-speed flight and longer range across numerous multi-mission aircraft This is DoD language, so more secret drones I would think. reply lynx23 10 hours agoparentprevSupersonic is only dead in civil aviation. The military doesn't care about regulations :-) reply thrillgore 3 hours agoparentprevPeople are forgetting why they are really building this hypersonic rocket: Nuclear stockpile. reply 15155 2 hours agorootparentWhy would you not just use a ballistic missile for nuclear delivery? What advantage does an air-breathing engine have in this capacity over MIRVs and decoys? reply wooplanes 6 hours agoprevI’ve been saying this for years, but I’m extremely skeptical the DARPA hypersonic programs just ceased. The waverider program was a huge success, then suddenly funding stops and nothing is heard about it anymore. That was over 10 years ago, and now we have a commercially-developed dual cycle engine in under a year. It’s my personal opinion that the technology was developed further internally for weapons and surveillance platforms like the SR-71 was back in the day. The technology just has too much potential. reply pmayrgundter 3 hours agoparentThe SR-72 is thought to be a real hypersonic from Lockheed flying since 2017. Extended reportage here: https://youtu.be/QlMX6TYdU7I reply banku_brougham 8 hours agoprevSorry for the naive question: Back in my day they called it \"Supersonic\", why do we call it \"Hypersonic\" now? Are there trademark issues? reply Beretta_Vexee 7 hours agoparenthttps://www.grc.nasa.gov/www/k-12/airplane/mach.html We speak of hypersonic flight when we approach or exceed Mach 5. At this speed, the heat is such that the ionized air forms a plasma around the vehicle. To put it simply, you're flying in the middle of a white hot blowtorch. This dramatically alters flight characteristics. Some prototypes approach or exceed mach 20. At this speed you're closer to a meteorite burning up in the atmosphere than to any form of aircraft. reply icegreentea2 6 hours agorootparentThis distinction is important, but in the contexts of this engine, hypersonic is important because it roughly aligns with a performance boundary for conventional ramjets. In a conventional ramjet, you basically have some static/static-ish structures in your inlet that uh.... \"rams\" into the air. This entire structure slows down the supersonic airflow to subsonic speed, and causes the pressure/temperature of the air to rise to allow for combustion. The problem is that ramjets become less and less efficient and feasible as Mach number increases, and around Mach 6 (so technically past the hypersonic boundary), you basically can't push them any harder, and so you need a supersonic ramjet (scramjet). In a scramjet, you only slow down the airflow to low supersonic speeds (instead of all the way to subsonic), and you feed that compressed supersonic flow into your combustion chamber. I believe expectation for scramjets in general is that you'd only really get to Mach 10-12 or so, since above that speed rockets start providing more favorable performance. reply pfdietz 3 hours agorootparentThe reason one cannot push the ramjet any higher is the air, slowed to subsonic speed, is simply too hot. If it's sufficiently hot the materials of the engine cannot withstand it, and even combustion stops being effective (because the temperature drives reactions in reverse, preventing heat production.) reply denton-scratch 1 hour agorootparentprevThat ball of ionized air is opaque to radar, making the rocket hard to detect. Or, so I read somewhere. I don't know at what speed a rocket has to be going, before the kinetic impact of 200Kg of solid metal exceeds the energy of 200Kg of explosives. reply throwaway4good 8 hours agoparentprevSupersonic = +1 mach, hypersonic = +5 mach. reply dghughes 7 hours agorootparentBut toothbrushes are ultrasonic. I can never find mine. reply magicalhippo 6 hours agoparentprevThese lecture notes[1] goes into some details of what makes hypersonic aerodynamics different. Similarly to how the supersonic regime is different enough from the subsonic regime that it's useful to distinguish the two, the hypersonic regime is different enough that it warrants its own name. There's no exact definition, but typically around Mach 3-5 things start to behave differently enough. [1]: https://archive.aoe.vt.edu/mason/Mason_f/ConfigAeroHypersoni... reply carabiner 3 hours agorootparentThis should be upvoted more, but I know HN prefers the binary explanations over the accurate ones in aerospace. \"Hypersonic\" isn't strictly defined like supersonic. Hypersonic phenomena begin to manifest at around 3 mach. reply paganel 10 hours agoprevTo what (I guess future) planes would this engine be assigned to? Is there a new US project after the F35? reply icegreentea2 7 hours agoparentHypersonic dual-mode ramjets are not suitable for 'general' applications. What makes them 'dual-mode ramjets' is that a single engine can work at both supersonic and hypersonic speeds. It's still a ramjet and so needs another means to get the engine/vehicle up to supersonic speeds. As sibsling notes, maybe it could go into SR-72 or similar penetrating ISR platform is a reasonable guess. It might go into some sort of missile carrier (the carrier gets the expensive reusable dual-mode ramjet, so it can launch missiles with cheaper disposable scramjets directly at hypersonic speed?). reply zakary 10 hours agoparentprevMissiles and Drones. There’s essentially no viable use case for a hypersonic manned jet reply banku_brougham 8 hours agorootparentIn the world we have built, the answer to \"what purpose will this new technology serve?\" is usually dispiriting. reply TrainedMonkey 8 hours agorootparentA huge portion of modern technology was originally developed for military applications. reply hoorayimhelping 4 hours agorootparentprevThis isn't the world we've built, it's the world we live in. We didn't create this world, and we aren't yet powerful enough to change the nature of reality, so to blame humanity for the necessity of defense is silly. You'll be a lot less dispirted if you accept that the world we live in isn't a human creation and that we still haven't made it out of the struggle to survive. reply prewett 3 hours agorootparentThere's nothing in the nature of reality that forces us to fight each other, unless you mean human nature. But we have a choice in whether we choose to be self-serving, which inevitably leads to conflict, or we choose to love our enemy. Loving your enemy requires dying to yourself, usually metaphorically, but it is an option. Humanity is entirely complicit. In fact, each person is a microcosm of that dynamic. Each time we sacrifice a little bit of someone else's well-being for our own, we are engaging in a small bit of that warfare. Cut someone off in traffic because you're in a hurry; cheat a little to get ahead; don't clean up your public mess because laziness/rushed. It's all essentially that same dynamic. reply dotnet00 2 hours agorootparent>There's nothing in the nature of reality that forces us to fight each other The reality that we are each capable of disagreeing with each other and having conflicting priorities, combined with the limited resources we have access to, is the nature of reality that forces us to fight each other. reply Xss3 10 hours agoparentprevI think it's likely it'll be used in the sr72. Apparently they had to completely rethink and refocus the project part way though due to new technology becoming available. That tech is probably this rotating detonation dualmode ramjet. reply andrewflnr 4 hours agorootparentWhere are you getting \"rotating detonation\" from? As far as I know that's an unrelated project/problem. reply jabl 7 hours agorootparentprevFrom what I read (on that source of all truth, twitter), SR-72 was effectively canceled due to unforeseen technical challenges. But might be that with a functional dual mode ramjet, assuming that was the major holdup, perhaps the project could be on track again? reply lpcvoid 10 hours agoparentprevThis will hopefully be used to counter russian and Chinese threats in the future. reply preisschild 8 hours agorootparentThere is a certain military that is happy to test those weapons against real russian targets :) reply jabl 7 hours agorootparentI think they wouldn't want to use the absolutely newest and greatest tech there for fear the Russkies will figure out something from the wreckage. There's plenty of not quite the latest cutting edge stuff sitting in storage that could be donated, though. reply spywaregorilla 6 hours agorootparentAnd so the war of 1990s tech vs. 1970s tech in the 2020s continues reply zaps 2 hours agoprev [–] Exclusive photo of the project’s lead engineer: https://gifyu.com/image/Stgkj reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GE Aerospace has successfully developed and tested a new hypersonic dual-mode ramjet in under 11 months, demonstrating a threefold increase in airflow over previous technologies.",
      "This achievement underscores GE Aerospace's commitment to advancing hypersonic technology, with the next phase focusing on further testing and technology demonstration.",
      "The project was a collaborative effort between GE Aerospace, Innoveering, and GE Aerospace’s Research Center, highlighting the rapid progress and dedication of the team."
    ],
    "commentSummary": [
      "GE Aerospace, in collaboration with Innoveering and their Research Center, has developed and tested a new hypersonic dual-mode ramjet in under 11 months.",
      "The technology, likely based on an existing design, is intended for missiles or high-speed aircraft, not commercial aviation, due to the extreme speeds and conditions.",
      "This project underscores significant advancements in hypersonic propulsion, which are vital for military applications."
    ],
    "points": 113,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1720767259
  }
]
