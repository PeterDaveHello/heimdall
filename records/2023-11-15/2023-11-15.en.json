[
  {
    "id": 38262315,
    "title": "Blender 4.0: Exciting Updates and Compatibility Changes",
    "originLink": "https://wiki.blender.org/wiki/Reference/Release_Notes/4.0",
    "originBody": "Developer Projects Docs Blog Forum Builds Developer Documentation View View source History Personal tools Log in Building Blender Windows macOS Linux Develop Getting Started Code Documentation Style Guide Release Notes Process Communication Modules Bug Tracker Code Review More Google Summer of Code Python FAQ Contents 1 Blender 4.0 Release Notes 1.1 Animation & Rigging 1.2 Geometry Nodes 1.3 Modeling 1.4 Sculpt & Paint 1.5 Shading & Texturing 1.6 Cycles 1.7 Color Management 1.8 Compositor & Sequencer 1.9 Import & Export 1.10 Core 1.11 User Interface 1.11.1 3D Viewport 1.11.2 Node Editor 1.11.3 Keymap 1.12 Add-ons 1.13 Python API 1.14 Asset Bundles 1.15 Compatibility 1.15.1 Blend Files 1.15.2 Graphics Cards Reference/Release Notes/4.0 < ReferenceRelease Notes Blender 4.0 Release Notes Blender 4.0 was released on November 14, 2023. Check out out the final release notes on blender.org. Animation & Rigging Geometry Nodes Modeling Sculpt & Paint Shading & Texturing Cycles Color Management Compositor & Sequencer Import & Export Core User Interface 3D Viewport Node Editor Keymap Add-ons Python API Asset Bundles Compatibility Blend Files The mesh format changes from previous versions are now included in the Blender file format. Blender 3.6 LTS can read files saved with 4.0, but earlier versions cannot. Blender 3.6 LTS will save meshes in a format compatible with older versions of Blender, so it can be used as conversion tool if needed. See the section in the Python API section for more details. The new blendfile compatibility policy (#109151) has been implemented (see PR #110109). Unused linked data is not kept anymore when saving and re-opening .blend files. See the core page for details. Graphics Cards The minimum required OpenGL version has been increased to 4.3 for Linux and Windows. (3478093183) On macOS only Metal is supported now, and the OpenGL backend has been removed. A GPU with Metal 2.2 support is required. Intel Macs without an AMD GPU require a Skylake processor or higher. (cdb8a8929c) Due to driver issues, support for Intel HD4000 series GPUs has been dropped. Retrieved from \"https://wiki.blender.org/w/index.php?title=Reference/Release_Notes/4.0&oldid=37566\" info This page was last edited on 14 November 2023, at 14:40. Content is available under Creative Commons Attribution-ShareAlike unless otherwise noted. places Privacy policy About Blender Developer Wiki Disclaimers Utilities What links here Related changes Special pages Printable version Permanent link Page information Navigation Main page Recent changes Random page Help Creative freedom starts with Blender – The Free and Open Source 3D Creation Suite",
    "commentLink": "https://news.ycombinator.com/item?id=38262315",
    "commentBody": "Blender 4.0 release notesHacker NewspastloginBlender 4.0 release notes (blender.org) 660 points by TangerineDream 21 hours ago| hidepastfavorite224 comments EatFlamingDeath 20 hours agoThe short movie is looking amazing: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=u9lj-c29dxI reply mr_sturd 19 hours agoparentI love it. Though I&#x27;m really going to miss seeing screen grabs of Big Buck Bunny as the placeholder image in media-adjacent projects. reply unshavedyak 17 hours agorootparentThere have been a lot of these (they&#x27;re basically always dogfooding the next one, i believe). Is there something special about Big Buck Bunny and Wing It? reply vanderZwan 13 hours agorootparentBBB was the first one that was really famous for some reason, so I guess it has the benefit of familiarity. That&#x27;s what makes it a good video equivalent to Lorem Ipsum I guess reply vdnkh 17 hours agorootparentprevdooooooo dooo dooooo, doooooooo doooo doooooooo doooo reply tomstockmail 17 hours agoparentprevPeertube link: https:&#x2F;&#x2F;video.blender.org&#x2F;w&#x2F;ee623d80-fd8d-4a2a-8aae-1f5acf79... reply greggsy 20 hours agoparentprevI like the reference to ”…the big red shiny button…” in Space Madness reply twoquestions 20 hours agoparentprevThis is amazing, it reminds me of Wile E Coyote cartoons which were always my favorite.Thank you for posting this :) reply satvikpendem 17 hours agorootparentThere&#x27;s a new Wile E Coyote movie coming. It was scrapped initially but due to backlash from fans, it was revived today. reply EatFlamingDeath 19 hours agorootparentprevYou&#x27;re welcome :) I would totally watch it if they created an animated series out of it. reply mekster 5 hours agoparentprevSo many software had been through the same fate. Premature launch, senseless patching to keep it going and then finally crash... reply qwertox 12 hours agoparentprevI loved it, in my opinion the best one so far, but I&#x27;ve seen it around two months ago. I think previously these shorts were released specifically to highlight new features which got added to Blender on a mayor release, which doesn&#x27;t appear to be the case with that short. reply lyu07282 15 hours agoparentprevamazing to see the geometry node setups in the outro, the smoke trail from splines (3:38) so cool! And you can get the blend files yourself to learn exactly how they did it. reply i_am_a_peasant 20 hours agoparentprevBrooooo how do you even. I wish I had that time it takes to get this good at Blender. I might actually end up releasing some solid indie games.Demoscene people are the guys I&#x27;m most jealous of. Not some linux kernel maintainer of some fancy filesystem. Nah, but the things that can transport you into an entire new universe in your head. reply flkenosad 20 hours agorootparentMy favourite Blender Studio short: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_cMxraX_5RE reply olddustytrail 15 hours agorootparentPeertube link https:&#x2F;&#x2F;video.blender.org&#x2F;w&#x2F;a69d68a5-a0e0-4a80-9d66-49f093c9... reply Vinnl 19 hours agorootparentprevOh wow, I thought I&#x27;d seen them all, but somehow I missed this one. Thanks for sharing, I think that&#x27;s my favourite too now! reply i_am_a_peasant 20 hours agorootparentprevThis makes me laugh so hard, it&#x27;s so trippy reply foobarian 19 hours agorootparentprevI&#x27;m a noob and I when kid asked me for help with Halloween costume I dug in making each triangle by hand, moving each vertex by hand, basically huge slow pain. Then I found the \"remesh\" button and the push&#x2F;pull tools which felt like a superpower. reply capableweb 20 hours agorootparentprev> Brooooo how do you even. I wish I had that time it takes to get this good at Blender. I might actually end up releasing some solid indie games.Keep in mind that the Blender Open Movies are made by professionals who&#x27;ve been doing what they been doing for a long time, and there is a whole team making those, with roles specific to the area.I don&#x27;t think you could single-handedly create something like that at the same timescale as they created it. They basically have made a proper studio at this point and they&#x27;re fine-tuning the workflows and processes of Blender by doing these movies.So don&#x27;t feel bad if you never would be able to create this alone, it is a team effort after all. reply i_am_a_peasant 19 hours agorootparentI still have dreams of being a 20x Engineer.I can be a 5x when I really love what I&#x27;m doing and I&#x27;m surrounded by fantastic people that I love being around. But I think I need another 10 years of experience to get to 10x, then maybe another 20 years to get to 20x. reply dumbo-octopus 17 hours agorootparentprevIf you want an actual answer, you don&#x27;t do it by yourself. I counted 50+ people credited. But if you want to start to play a part in doing it, basically just learn a lot of math. My gf is a technical director at a large animation studio and she got in by being an expert in linear algebra and spending a ton of time studying animation textbooks&#x2F;tutorials&#x2F;etc.That said, the market is evaporating. Almost everything has moved ~to India~ [e: abroad] and there are barely any jobs left in USA, and those that do exist are being fought for by the many folks who have been recently laid off stateside. Sounds familiar... reply dagmx 16 hours agorootparentYour comment about the market is incredibly wrong and jingoistic. The majority of popular feature and TV animation is made outside of India.Canada is a larger competitor to the US market than anything else, and very few companies have Indian outsourcing for feature&#x2F;TV animation. Its more prevalent for VFX jobs, but even then many studios still have a large presence in North America&#x2F;Europe.Studios like wild brain, titmouse etc are all North American based, while most feature animation is a mix of North America (Pixar, Dreamworks, Disney, Sony) and Europe (Illumination, Skydance) reply i_am_a_peasant 14 hours agorootparentI&#x27;ve never been hit by offshoring but I have been on the other end of it. When I lived in a relatively affordable-workforce central European country we once had to do a knowledge transfer from our Canadian office which all got laid off.We took all their work, the whole experience was bittersweet, some of the folks there took it well others less so. But I can say that the project itself was reaching a stagnating phase where not much new work needed to be done, and we were mostly doing maintenance&#x2F;bug fixing. The company itself wasn&#x27;t doing anything innovative either and there wasn&#x27;t leadership to put the Canadian guys skills to good use.Eventually most of those guys were hired by Intel and all got to work on exciting new technology that none of us were qualified to do.I think this is more or less okay when it happens, if a company does massive layoffs I take it more as a sign that they are not producing much anymore. And when many companies fire tens of thousands of people all at once I basically take it as a sign that the tech sector as a whole is taking a big downturn. Maybe things will improve when we finally stumble upon some tech that needs developing that has a great potential to be profitable and takes a lot of people to develop. reply dumbo-octopus 16 hours agorootparentprevI am sharing the lived experience of a person I am very familiar with. Do you have specific industry experience to back up your claims? If you know a place actively hiring technically minded animators, verily I say we would love to hear about it.I know for a fact that several of the studios you list as \"North American\" have recently laid off almost all their animation staff in favor of Indian vendor studios. Not feature, perhaps. But TV people do work too. Or did, rather. reply dagmx 16 hours agorootparentYes I’ve been a supervisor in both feature film and vfx at major studios, as has my partner who currently still works in the industry. I maintain close ties with several major studios and am still a well known entity in the industry. I feel like I can speak with quite a reasonable level of confidence in this spaceYou made two claims, that the jobs have been laid off (correct) and moved to India (incorrect). The layoffs are parts of market changes due to the strikes and production cuts before then. The implication that it was caused and will lead to outsourcing is not borne in reality for feature&#x2F;tv animation.But yes several studios are hiring still. The job fair at the recent Spark conference in Vancouver had several studios open. Feature and Tv animation hasn’t seen the slowdown that other areas of the market have and will bounce back faster. reply dumbo-octopus 16 hours agorootparentIt is most certainly incorrect to claim that the jobs have not (at least in part) moved to India. It is also incorrect to claim that laid off workers are not being replaced with foreign vendor studios, largely in India. This is a 100% verifiable fact for at least one of the \"North American\" studios you mentioned.It would appear you have taken your own experience, which I&#x27;m sure is vast, and made the mistake of assuming it applies to every studio. I can tell you that it does not.I can also tell you that the job market is quite dry indeed, and that studios that were attempting to headhunt just months ago will now no longer even reply to applications. reply dagmx 14 hours agorootparentI think you&#x27;re greatly overestimating how many jobs have moved to India. The largest company to do so is MPC, but for feature and TV work, it hasn&#x27;t really impacted job locality.It appears you&#x27;re taking your own internal biases about India outsourcing and applying it more expansively.I didn&#x27;t say the job market isn&#x27;t dry. Again, I&#x27;m pushing back on the conflation of layoffs and jobs being offshored to India. Canada and Europe are much bigger source of offshoring for the US. However that isn&#x27;t to say studios aren&#x27;t hiring which was what you asked about, and I answered. It&#x27;s definitely a lot lower, but it&#x27;s not due to outsourcing. There&#x27;s so many other factors that I already mentioned (lowered production, strikes) that play in first. Animation is affected to a much smaller degree and several studios in Canada are hiring in reasonable numbers at the moment.You can do with that info what you will, but it sounds like you don&#x27;t actually want to hear an answer that contradicts your own and are doubling down on something that is not borne out of the reality of where the work is being done right now. It&#x27;s especially exasperating because you&#x27;re not even involved in said industry, outside of your partner, whereas I am actively every day.Largely, the main jobs that get outsourced to India is stuff like match move and rotoscoping. There&#x27;s very little Core Animation, lighting or rendering work for most of the major studios done there, with the exception of MPC reply dumbo-octopus 12 hours agorootparentDreamworks has specifically stated that lighting and rendering work is being outsourced to India and that this was the reason for their recent laying off of their entire TV animation department (HUB). You act like somehow me directly knowing someone who was directly told this information by the people making the decision is somehow not a strong enough source? Compared to you who isn&#x27;t in this country and previously stated you don&#x27;t even work in the industry anymore? reply dagmx 12 hours agorootparentDreamworks has been scaling back their India operations but has been included Indian animation in their work for over a decade and a half now.https:&#x2F;&#x2F;www.cartoonbrew.com&#x2F;business&#x2F;breaking-dreamworks-ani...https:&#x2F;&#x2F;www.latimes.com&#x2F;business&#x2F;la-xpm-2011-oct-29-la-fi-ct...They set up their Indian studios in 2008 and have had portions of their work done there and in China (now Pearl) for years now. They haven&#x27;t increased that outsourcing within the last few years, and as I mentioned have been reducing it so your points regarding the current state of things isn&#x27;t very valid. It also never resulted in job reductions in North America, but rather more films per year.In fact, Dreamworks is a key example of a studio that is outsourcing their work to Canada, which is what spurred their most recent layoffs: https:&#x2F;&#x2F;www.cartoonbrew.com&#x2F;studios&#x2F;dreamworks-shifting-away...And so what if I don&#x27;t work directly in the industry anymore? I&#x27;m very involved in it still, and my partner is still in the industry. And we did work in the US for several years, so your point is moot.Is it possible that your girlfriend is not a reliable source of information here? Because I have multiple friends who are fairly senior at both DWA Glendale and Bangalore, and coupled with the news, you&#x27;re clearly not lining up with anything they&#x27;re saying. And if she&#x27;s laid off, then she&#x27;s no longer in the industry either, so you can&#x27;t really claim that as a statement piece either.I would really examine your own biases here as to why you&#x27;re so hellbent on focusing on India here, and not any of the other countries that are actually taking the jobs away from them. Again, Canada is a MUCH bigger source of job diversion for the industry than India is. reply dumbo-octopus 12 hours agorootparentFrankly I don&#x27;t really care if Canada or India is the bigger outsourcing destination and I don&#x27;t see why you&#x27;re getting so hung up on the specific country. The fact of the matter is Dreamworks TV laid off their entire HUB staff in favor of Indian vendor studios (not Dreamworks Bangalore. Mikros is one example.), and there is no job market in America (look at the \"Careers\" pages for every American studio you listed).I don&#x27;t know why I&#x27;m debating this with someone who is so far removed from anything actually going on.This is what&#x27;s happening. These are the facts. If you refuse to accept them, that&#x27;s your own internal biases, not mine. reply dagmx 12 hours agorootparentYou clearly seem hung up on one country over another, and continue to ignore the specifics of the situation or any nuance to the situation. I think that speaks volumes. Especially when Dreamworks TV has been heavily outsourced based since the beginning. Just look at how many of their shows are produced at Bardel in Vancouver.Anyway, agreed there&#x27;s no point arguing with someone who has no actual roots in this industry. Have a good day, I hope your partner finds work soon, but maybe don&#x27;t place the blame where it doesn&#x27;t belong. reply dumbo-octopus 12 hours agorootparentYou&#x27;re hung up on India bro. I mentioned them in passing as that&#x27;s where that specific department was laid off in favor of, then went back and edited that out when you commented the first time. You&#x27;ve been making this whole thread about India when the point is the job market. The country is incidental to everyone but you. reply JBits 16 hours agorootparentprevAs someone who&#x27;s keen on linear algebra but knows little about animation, how did linear algebra help her? reply dagmx 16 hours agorootparentComputer graphics is based on linear algebra. Behind the artists are lots of engineers and technical director jobs where you&#x27;re building software tools, render engines or character rigs. Animation is a lovely meld of art and technology, and has been so since the very first animations existed. Its one of the reasons I really enjoy the field. reply JBits 16 hours agorootparentThe application of linear algebra to rendering I can understand. Character animation less so, apart from the physics aspect.Sounds like a nice field to work in! I didn&#x27;t realise it used maths to that extent.Would you have any recommendations for someone of a linear algebra &#x2F; programming background if they were interested in this field? reply dagmx 14 hours agorootparentSo one caveat I would mention is that you will likely earn more outside of the animation industry with those skills than within it.With that out of the way, I think there are several avenues to get involved. Picking up a graphics engineering book, or learning OpenGL&#x2F;Metal&#x2F;DirectX will make you valuable as a realtime engineer.But otherwise I would recommend finding an open source project and contributing to it as a way to build up the repertoire that you can use to apply for jobs with. Blender is an excellent place to start, but so are any of the projects under the academy software foundation at https:&#x2F;&#x2F;www.aswf.ioGetting more into the character animation side, you can look into Rigging, which is the process of setting up the armatures that move the characters. There are also things like simulation for cloth&#x2F;hair etc...Blender is a good place for a lot of them. reply truckerbill 11 hours agorootparentWhat might be a better paying area with that background?Interesting that you say that technical art roles don’t need a reel always assumed otherwise. reply dagmx 10 hours agorootparentAnything in tech would pay significantly better with the same skills. Graphics engineers are in high demand at many major tech companies like Meta, Apple, Google.But film&#x2F;games will still pay well. Just not as well.As for needing a role, well it depends. Tech art is a very wide role with very little definition. But also those skills apply to engineering instead of tech art as well.Engineers don’t need reels. Tech artists would if they’re more on the art side (shaders, characters) but less so on the tools side. reply truckerbill 15 hours agorootparentprevYou&#x27;ll need a demo reel. reply dagmx 14 hours agorootparentNot for an engineering role. reply dumbo-octopus 16 hours agorootparentprevHelpful when writing shaders to describe surfaces and lighting, as well as working on the constraint optimization engines that go into physical body simulations. reply gamblor956 12 hours agorootparentprevAlmost everything has moved ~to India~ [e: abroad] and there are barely any jobs left in USA, and those that do exist are being fought for by the many folks who have been recently laid off stateside.This is simply wrong, and if anything the reverse is true: the jobs that had been outsourced to India are being brought back to the U.S. after several years of subpar work. See, for example, the most recent Marvel movies and TV shows. The abysmal VFX work was the product of outsourced VFX shops. You can bet your rear that Disney won&#x27;t be repeating that mistake in the future. reply pjmlp 19 hours agorootparentprevHaving been a trader, regular guest on coding and Protracker parties, yep those were the days.I guess shadertoy and similar are where the Demoscene spirit lives on. reply crawsome 12 hours agoparentprevIt looks so much better than average rig animation you see in most cartoons nowadays, but I still prefer hand-drawn animation. The bending&#x2F;squeezing&#x2F;rigging of assets still feels like a program&#x27;s procedure rather than an artist&#x27;s touch. reply numlock86 16 hours agoprevBlender for 3D is what Postgres is for databases. It&#x27;s really an exceptional piece of open source software and totally stands out. There&#x27;s not many software projects like this. reply lvl102 12 hours agoparentI will go one step further and say they should teach kids Python using Blender. Such a powerful tool and so many learning possibilities. reply adriano_f 10 hours agorootparentI&#x27;ve started teaching my (homeschooled, 10yo) son Python, and he&#x27;s been briefly exposed to some 3D software (including Blender).I&#x27;m curious: how do you imagine this working? Can you name a few examples of the learning possibilities that you have in mind? reply raihansaputra 7 hours agorootparentblender has a python api and everything you can do in blender can be replicated with scripts. for a start you can draw geometric shapes to show variables and a for loop. reply raymond_goo 1 hour agorootparentexcept extruding with scale restrictions: https:&#x2F;&#x2F;blender.stackexchange.com&#x2F;questions&#x2F;87165&#x2F;python-scr... replyCognitron 15 hours agoprevSince nobody&#x27;s mentioned Ian Hubert yet, his Dynamo Dream series is made in Blender:Episode 1: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LsGZ_2RuJ2AEpisode 2: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xlqhdaLhRVYEpisode 3: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=JM_WPiT6NRQ reply Kablys 11 hours agoparentThanks, I didn&#x27;t know there were more episodes released. reply dagmx 17 hours agoprevHere’s the Blender 4.0 content reel while you wait for the site itself to updatehttps:&#x2F;&#x2F;youtu.be&#x2F;eoY1Mc70uTo?si=ttU7szWsbWNXwLEzThe new features overviewhttps:&#x2F;&#x2F;youtu.be&#x2F;LcQkk7NbOoY?si=ldo4tKz2WonBSe0hThe introduction to Node Tools https:&#x2F;&#x2F;youtu.be&#x2F;Y8Udi1AkdGY?si=95AKQ0tUg4FJSLpM reply justinclift 1 hour agoparentAny idea why there was no announcement of the RC1 (eg \"Ready for Testing!\") on the Blog when it was apparently actually ready for people to test?Being that that&#x27;s the normal place people expect stuff like that to be announced. ;) reply butz 17 hours agoparentprevAny alternative platforms to watch those videos? reply perryprog 16 hours agorootparentThe content reel was uploaded to their PeerTube[1]; I don&#x27;t see the other two videos on there yet, though.Edit: The node tool introduction is now also live[2]![1] https:&#x2F;&#x2F;video.blender.org&#x2F;w&#x2F;ni4S8WYzVG9kqQ6mDjnY1s [2] https:&#x2F;&#x2F;video.blender.org&#x2F;w&#x2F;hyq7PB9uaUUKkjwSENxid5 reply ragebol 16 hours agorootparentprevBlender is on peertube: https:&#x2F;&#x2F;video.blender.org&#x2F;videos&#x2F;local?s=1 reply perryprog 17 hours agoprevThe release page is now live at https:&#x2F;&#x2F;www.blender.org&#x2F;download&#x2F;releases&#x2F;4-0&#x2F;. reply CooCooCaCha 19 hours agoprevThis seems a bit premature. The announcement page isn&#x27;t up yet and the download link still points to 3.6 reply raxxorraxor 1 hour agoparentBlender is putting out right now like nothing. I didn&#x27;t even adapt to the latest versions fully.Nevertheless what they currently do is nothing else than completely awesome and I am really looking forward to it. reply jansan 18 hours agoparentprevHN:Ahead of scheduleUnder budgetNever wrong reply executesorder66 18 hours agoparentprevKarma whores do this all the time. It used to be a big problem in &#x2F;r&#x2F;firefox for a long time, and somehow stopped happening eventually.Just flag it and move on. reply guitarlimeo 20 hours agoprevI recently started doing basic modeling work for my small hobby projects. I&#x27;m amazed at how professional and finished Blender is for being an open source project. It feels like I&#x27;m using an Adobe product or similar. Big props to the team behind it, you&#x27;re awesome. reply dagw 20 hours agoparentUnlike most open source projects, Blender started its life as a professional piece of software by and for artists who had to deliver commercial projects on a dead line. First as an in house tool for an animation studio, and later as a commercial software product. It only became open source later in its life cycle after the commercial Blender company went bankrupt. It has also been headed by the same lead developer from its earliest days as an in-house tool right up until today. All these things really shine through and make it quiet unique among open source applications. reply crote 18 hours agorootparentBlender only became successful well after it was made open source.As a commercial project it was a failure. It only became successful due to two decades of open-source development, and the willingness of its users to invest in it - even if only to stimulate the development of a competitor to expensive proprietary software.Blender only became an even remotely viable option in 2011, after being open-source for 9 years. Its popularity only really started in 2019 after a massive UI rework made it actually nice to use. This and related changes led to Blender receiving a $1.2M grant in 2019, leading to other companies re-evaluating it and awarding even more grants.If anything, compared to today&#x27;s successes its initial proprietary development should be seen as nothing more than a historical curiosity. reply PaulDavisThe1st 17 hours agorootparentBlender received huge infusions of cash before 2019.Back in the mid 2000s, I was on the \"FOSS talk circuit\" in Europe talking about Ardour. Blender stuff was often happening in the room next door (way more people ... graphics ... sigh :)Blender got huge grants from both the EU and also, I think, Apple back in the mid-to-late 2000s that in retrospect look critical in boosting them out of their somewhat \"stuck\" development status. reply fb03 17 hours agorootparentThank you for writing Ardour. Lovely piece of software reply grobbyy 17 hours agorootparentprevIt&#x27;s not a historical curiosity. The decisions needed to make an architecture viable on the long term happen early on and need to be sustained as code is being developed.A lot of that comes down to days structures used, system modules, abstractions, If that&#x27;s done right, it&#x27;s possible to make a good UX later.That&#x27;s very hard in modern software processes which emphasize short term spirits. reply wonrax 6 hours agorootparentprevI still remember when I first discovered Blender. It was around the time Blender 2.8 had just been released with its new UI. At that point, some people still preferred using 2.7 because the new UI was too different for them. The first thing that blew my mind was the size of the download, which was around 50MB. I had expected to have to download much more than that, which is typical for creative, graphical applications. After using it for a while, I recognized how polished and powerful Blender is compared to other opensource projects I had used before such as GIMP. 3D is not my professional work, I use it as a hobby, and I&#x27;ve learned and created many 3D works that were only made possible because Blender is free. The community on YouTube and Stack Exchange is large and friendly as well. I just love Blender so much. reply neovive 17 hours agorootparentprevBlender has such a rich and amazing history! I used it for some hobby projects many years ago, but the UI was very complex and the learning curve was high. Recent versions have greatly simplified the UI and the features feel on par with many of the proprietary industry tools. The future definitely seems bright. reply orbital-decay 17 hours agorootparentprevMost of this is technically true but not really relevant, and no, this is not what makes it stand out.Very little of what constitutes modern Blender came from NeoGeo&#x2F;NaN. The original software was uncompetitive and unremarkable. In fact, Blender had to change most of its original UI conventions to feel less alien for CGI professionals. (because the original paradigm was from the ancient times before the commonplace UI conventions)What made it stand out in the open-source community is devs using it for actually creating something (best investment!), positioning it at the intersection of interests of non-competing companies that need to get shit done, and also attracting a massive army of game modders. reply somat 16 hours agorootparentNot really, I find the blender workflow is still very similar to the old 1.7, fits on a floppy disk, days.It&#x27;s the same modal editor with a million hotkeys and dense packed stacked dialogs, the most intrusive workflow change was when they swapped the default mouse keys.Not to say the blender project has not done amazing work making things more discoverable. But as a occasional blender user from the sgi 1.X days. I suspect \"the complete 3.0 overhaul making blender more standard\" was more marketing than anything else. Blender had got a reputation has being hard to learn(all 3d programs are hard to learn). So while they did do a lot of work on the UI and it is much better, mainly it was loudly saying \"we made the UI easier\" and everybody sort of went along with it. reply agumonkey 19 hours agorootparentprevInteresting but the special part about blender is the open source part, they managed to keep it alive, revamp the UI fully and bring on new and hard features on a regular basis. I don&#x27;t remember another foss project of that kind. reply crote 17 hours agorootparentKiCad is slowly inching in this direction! It used to be an absolute pain to use 3-4 years ago, but it has significantly matured since then.With the death of EAGLE it&#x27;s rapidly becoming the obvious choice for all hobbyist use, and it is powerful enough for quite a bit of commercial work too. It definitely isn&#x27;t at Altium&#x27;s level yet, but unless you&#x27;re designing something like a motherboard you probably won&#x27;t be missing much. reply joshfee 19 hours agorootparentprevkrita.org is a similarly polished and professional open source project, but I agree these types of projects are the exception rather than the rule (though I suppose the same could be said about most products). reply jansan 18 hours agorootparentKrita is nowhere near the level of Blender. It is more on the level of Inkscape. reply danShumway 16 hours agorootparentKrita isn&#x27;t on the same level as Blender, but it&#x27;s not on the same level as other Open Source projects either. I&#x27;d put it in the same camp as projects like Ardour, and arguably above projects like Godot (although I&#x27;m sure some people would debate me on that).It&#x27;s an exceptional drawing tool that is well-suited for professional work and is headed in a really promising direction. It&#x27;s got a couple of weaknesses (vector layers) but it really shouldn&#x27;t be discounted as a professional-level tool.Unless you&#x27;re claiming that Inkscape is in the same position and has gotten a lot better than the last time I used it, which :shrug: could be true, I don&#x27;t know. I&#x27;m not trying to bash Inkscape here. reply syntheweave 12 hours agorootparentI can vouch for Krita and Inkscape pairing well as vector art programs and even doing things that CSP, whose vector layers are pretty well liked, can&#x27;t match. The issue is that drawing in Inkscape is a little bit broken(it can be done, but the current UX is death by papercut) - thus I approach it through the other program, which is basic but consistent.So the workflow I end up using for digital inks is: Open both programs, sketch in Krita, copy-paste the vector data into Inkscape, stroke->path, then use tweak tool to sculpt the lines. This adds line weight in seconds-to-minutes. Alternately, I can apply path effects instead of stroke->path, if I want a more programmatic design. If I want to paint, I can copy-paste the shape back into Krita. reply danShumway 11 hours agorootparentI&#x27;ll have to give that a try some time, thanks for the tip! reply hellcow 17 hours agorootparentprevAs a hobbyist digital painter and 3D artist who has used both Blender and Krita, I strongly disagree with this. Krita is simply best-in-class at digital painting on a PC. Both are exceptional pieces of software.It did take me a week or two to adjust to Krita’s hotkeys coming from a lifetime of Photoshop, but the software in the end is leagues better for painting. reply agumonkey 15 hours agorootparentprevno but he&#x27;s right, there&#x27;s a level of polish that was higher than average there too, i actually thought about it but couldn&#x27;t remember the name when i wrote. reply dmazzoni 16 hours agorootparentprevWould you consider Audacity another example? (I&#x27;m one of the original authors.) It survived for ~20 years as a community-supported project - it was popular but clunky and limited in many ways. Since being acquired by Muse the UI has been cleaned up and modernized quite a bit, while still keeping it open-source. reply agumonkey 15 hours agorootparentAudacity is pretty cool, but it&#x27;s not the magnitude of blender in terms of complexity. reply croes 19 hours agorootparentprevWasn&#x27;t the user interface fundamentally changed after the open source release because it was so cumbersome to use? reply sbuk 18 hours agorootparentNot for a while. The first revamp was 2.5, then another big one with 2.8, which was the starting point for the current UI, in 2019. reply taneq 17 hours agorootparent(That&#x27;s a \"yes\". Blender 2.4 was an in-house gamedev asset authoring tool that was open sourced, Blender 2.8+ is very different and apparently pretty pro-level. reply thomastjeffery 16 hours agorootparentprevNot fundamentally. Some very carefully thought out tweaks were made to make the interface more approachable.The overall UX structure has not diverged much from the old 2.x days. reply yterdy 19 hours agorootparentprevThis is an innocent misrepresentation of Blender&#x27;s development history. While nothing you&#x27;ve said is false, Blender has been open source for more than 2 decades. It began that portion of its life as something of a mess, with a truly Byzantine user interface that made even trivial tasks troublesome, and lacking a vast majority of the features it&#x27;s now known for. Getting it to its contemporary state was a long and painful process, including at least two major front-end overhauls and who-knows-how-many under-the-hood, and a commendable (though not unimpugnable) humility from developers who (finally) found the wherewithal to put the user experience before FOSS dogma or their own ambitions.That&#x27;s what separates it from most open source projects: not that it started as a commercial product, but because its designers and developers stowed their egos and worked diligently on creating a solid piece of software (and documentation and community and support) for a long, long time. In this way, it surpasses even many of its commercial contemporaries, which are driven by a profit motive to become increasingly paywalled and enshitified. reply jasode 19 hours agorootparent>This is an innocent misrepresentation of Blender&#x27;s development history. While nothing you&#x27;ve said is false, Blender has been open source for more than 2 decades. It began that portion of its life as something of a mess, with a truly Byzantine user interface that made even trivial tasks troublesome, and lacking a vast majority of the features it&#x27;s now known for. [...] That&#x27;s what separates it from most open source projects: not that it started as a commercial product, I think your minimization of its original commercial nature with additional facts about the UI is also an innocent misrepresentation.Even though the 2003 Blender didn&#x27;t have the optimal UI, what the commercial investment did for Blender was put enough value into software such that it had a headstart and momentum for subsequent investment from corporate donors&#x2F;sponsors and volunteers to create the later UI overhauls. The substantial €4.5M business investment set the stage for the later developments. That it happened 20 years ago isn&#x27;t the key. What&#x27;s key is the financial investment to help motivate 20 additional years of work.Compare that to the open-source development of Octave (a MATLAB alternative) where the developer is lacking money and is looking for employment: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=13603575Observers can say Octave is not as polished as MATLAB. But a similar hypothetical investment of €4.5M of 20 years ago might have helped give it the \"Matthew Effect\" like Blender got. That the hypothetical Octave v2023 has a revamped UI compared to Octave v2003 would not remove all the causal effects of that hypothetical investment. reply danShumway 16 hours agorootparentThis feels like a bit of an over-extrapolation. Blender succeeded in no small part because of investment.As a commercial product, it failed. We shouldn&#x27;t say that being commercial is a model for success, Blender tried being commercial and it didn&#x27;t work. If it had stayed commercial and had not been Open Sourced, it likely wouldn&#x27;t exist today.As a side effect of its origins and what people saw as the opportunity behind the project, it then got a lot of investment, and it turns out that Open Source projects with heavy involvement from their userbase (and not just from programmers) and with heavy monetary investments and a positive community that gets excited about the project -- it turns out that yields exceptional tools. But it&#x27;s not the commercial aspect that caused that, the investments caused that, and other Open Source projects could be given the same level of investment -- after all, many of them are as good or as competitive (if not more competitive) than Blender was when it was Open Sourced.As an Open Source program Blender has introduced architectural improvements and structural improvements that rival its origin, and it was able to do that without going commercial, which is evidence that this kind of investment and funding can exist for a non-commercial project if a significant portion of a community and businesses think the project is worth funding. It is arguable that starting out as a commercial project helped fuel that optimism. But to say Blender owes its success to being commercial feels backwards. Blender owes its success to the fact that it stopped being commercial, which was (and is) a contributing factor to why people and organizations feel so good donating to it. Like if we&#x27;re going to take a lesson away from Blender, that lesson might be, \"want to compete with Maya? Dissolve your company and give away your code.\"I very much believe that Blender wouldn&#x27;t have a community today if it had stuck with its commercial origins. reply jasode 15 hours agorootparent>As a commercial product, it failed. We shouldn&#x27;t say that being commercial is a model for success, [...] But to say Blender owes its success to being commercial feels backwards. You&#x27;re misunderstanding what my attempted explanation is about. You&#x27;re arguing about reasons for Blender&#x27;s success. That&#x27;s not my angle.My explanation is about something else: why Blender&#x27;s level of polish (not \"success\") seems to be so advanced in relative comparison with other open source projects out there such as Gimp, etc.In other words, people&#x27;s expectations of open-source software usability&#x2F;polish is so low that Blender&#x27;s level of execution is surprising. This is the gp&#x27;s particular wording I&#x27;m commenting on, >\" I&#x27;m amazed at how professional and finished Blender is for being an open source project.\" (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38262825)The negative (usability) connotations with the phrase \"for being an open source project\" is something I&#x27;ve analyzed before: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29605885...all those open-source projects (Gimp, SageMath, etc) that are GUI instead of command-line that people complain of not having the polish of their peers -- don&#x27;t have Blender&#x27;s unique accidental history of having the (failed) commercial product kickstart the momentum and evangelism of a community to make subsequent Blender versions eventually exceed the expectations of typical open source projects. The multi-millions gave it a unusual headstart that Gimp, Octave, etc do not have.I do agree with the following parts of your comment which don&#x27;t contradict my argument:>Blender owes its success to the fact that it stopped being commercial, which was (and is) a contributing factor to why people and organizations feel so good donating to it. [...] I very much believe that Blender wouldn&#x27;t have a community today if it had stuck with its commercial origins. reply danShumway 15 hours agorootparentSorry for the misunderstanding. Unfortunately, I sort of feel like I would disagree more with your statement now that I understand it better? :DI think the majority of Blender&#x27;s polish came after being Open Sourced, and I don&#x27;t think that movement was kickstarted by its origins. I do think its origins arguably played a role in its investment, but early Blender had a reputation of being exactly like many other Open Source projects -- powerful if you knew how to use it, but unpolished and arcane and difficult to learn.When I first ran into the Blender, its interface was the biggest criticism I saw online about the project. Wherever the momentum came from to say \"we could refactor this and make it attractive to regular modelers\" I don&#x27;t think that momentum was present in its early days or right after it was Open Sourced.I&#x27;m tempted to dig into the history a bit more now, because I bet if I went back in HN history to the early 2.0 days, I would find comments saying that Blender is just like other Open Source projects in that its too confusing and difficult to use. The interface was what everyone trashed about Blender.Of course investment helped with giving Blender the ability to undergo those kinds of radical transformations, but the transformations were sorely needed and I&#x27;m not sure that its UI was what fueled that investment. I think both overstate the connection, but I&#x27;m more sympathetic to a claim that Blender&#x27;s commercial origins fueled its architecture or resources than I am to a claim that it fed its interface polish or excitement about its interface.----Personally, I think the biggest contributor to Blender&#x27;s interface is the fact that its community is made up of a ton of artists and not just programmers. Additionally, a lot of its investment is coming from studios -- notably not from a company catered to studios, but specifically from people who are using the tool in-house as they develop it. Greasepencil in particular is heavily influenced by this; the current rewrite for Greasepencil 3.0 is being pushed and developed by people who are intimately familiar with Greasepencil as a creative tool.In theory, Open Source should be a much better match for this than commercial software, because commercial software caters to a client-base but is developed usually by a player outside of that space that is seeking to monetize it. Open Source often falls into the same trap, but at least has the possibility to be developed and driven and to have feature requests prioritized primarily by artists and community members who rely on the software rather than by a separate entity that is interested primarily in how the software is marketed or sold. That Open Source very often fails at this is (imo) more of a commentary on the lack of community experience and community investment that most projects have. Stereotypical GUI Open Source projects are very often the result of external efforts from people who are not trained in GUI work and who are not quite as closely tied to their users.I&#x27;d bring up Krita as another example here -- Krita has no commercial origins (https:&#x2F;&#x2F;krita.org&#x2F;en&#x2F;about&#x2F;krita-releases-overview&#x2F;) but is miles ahead of Gimp in terms of building a polished interface that caters to artists. One immediately obvious difference is that Krita&#x27;s artist community is heavily engaged in the project and in regular communication with the developers and regularly gets involved in feature development and prototyping and testing. In fact, Krita started out as a fork of Gimp, and yet has surpassed Gimp in terms of ease of use and polish. reply Applejinx 14 hours agorootparentIn a recent comment on one of my DSP plugin videos I directly addressed this:\"I think this is because plugins are made for people to pay attention to them and then buy more plugins. I just do patreon, so I make plugins to be used, and only by those who need them. They should be boring and never change, but the sound should be amazing and just immediately there so you can pay attention to the music, not the plugin, and then not have to buy more.\"That&#x27;s the secret to open source software if we choose to maintain it. There&#x27;s a catch: OSS projects also gain mindshare through promotion and attention, putting them on exactly the same grounds as commercial software, and if you had an ideal project that perfectly met a need without drawing any attention to itself, that need would prosper and the OSS project would languish.It&#x27;s the old &#x27;tiny piece of unsupported OSS software on which the world depends&#x27; problem. You absolutely can do that and the cost is that the project either languishes or fails (in the sense that it can&#x27;t be maintained, not that it fails its task). Or you can lean towards seeking payback, in the form of money or in the form of attention, and it costs the users something but sustains the project more fully. reply olddustytrail 14 hours agorootparentprevKrita was never a fork of Gimp (and that&#x27;s not what your link says). The idea of a Qt wrapper around Gimp was a kind of inspiration for Krita in a roundabout way but the codebases are entirely separate. reply danShumway 14 hours agorootparentGood correction, I misunderstood what that part of the link was implying. replycrote 17 hours agorootparentprevThe €4.5M business investment in 2000 was to make it a freemium product. This obviously failed, which is why the community was able to acquire the rights via a €110k crowdfund. Just because they shoveled a lot of money into it, doesn&#x27;t mean they actually created a great product. Blender wouldn&#x27;t have been open-source today if the investors didn&#x27;t decide to shut down the (quite incomplete) project due to disappointing sales.The next substantial investment was a €1.2M grant in 2019, and it was a direct result of the development of 2.80, both the new GUI and other improvements. It suddenly became an actual viable alternative to commercial products, making it worth investing in. These developments only happened due to the hard work of mostly volunteers, and a lot of donations from primarily the community. reply jampekka 18 hours agorootparentprevMATLAB is a mess that&#x27;s in use solely due to history, network effects, stockholm syndrome and vendor lock-in. Octave is doing laudable job in the latter, but I fully expect it to die out after MATLAB whimpers away. And I don&#x27;t think the GUI is much of a priority for Octave. reply yterdy 17 hours agorootparentprev>Even though the 2003 Blender didn&#x27;t have the optimal UI, what the commercial investment did for Blender was put enough value into software such that it had a headstart and momentum for subsequent investment from corporate donors&#x2F;sponsors and volunteers to create the later UI overhauls. The substantial €4.5M business investment set the stage for the later developments.It did not, because Blender went 7+ years without substantial improvements to its modeling and rendering tools and interface. It was a zombie, with a lot of resources wasted on the now-defunct game engine. The pivot away from what it had been is what made Blender what it is today. reply jasode 17 hours agorootparent>yterdy: Blender went 7+ years without substantial improvements to its modeling and rendering tools and interface. It was a zombie, with a lot of resources wasted on the now-defunct game engine. The pivot away from what it had been is what made Blender what it is today.>crote: Just because they shoveled a lot of money into it, doesn&#x27;t mean they actually created a great product.I&#x27;m citing these 2 comments because it&#x27;s another example of mentioning observations that can be true -- but are still not a good explanation of _why_ Blender&#x27;s level of polish is different from other open-source projects.You guys are emphasizing the artifacts of the software as the proof of Blender&#x27;s unique situation. Instead of the artifacts, I&#x27;m emphasizing the _community_ and _why_ they&#x27;re invested in Blender to motivate the work on the subsequent artifacts (e.g. revamped UI) that you&#x27;re referring to. The explanation of the community momentum starts from the business investment. In this framing, it doesn&#x27;t matter that Blender v2003 wasn&#x27;t great software. What matters is that v2003 (with whatever flaws) -- attracted enough community -- to keep working on it (and eventually \"pivoting\") for 20 years.I go back to this you claimed as the key reason :>yterdy: , but because its designers and developers stowed their egos and worked diligently on creating a solid piece of softwareThat would be a more convincing argument if the developers used the 2002 crowdsource money to build a clean-room rewrite from scratch instead of buying the existing codebase to get the millions in sunk development work at a steep discount. E.g., if what truly matters is the humble developer egos rather than the value of the exiting codebase, then there was no need to buy the old codebase. Just advertise the 2002 crowdsource money as paying for humble diligent developers to build a new 3D modeler from scratch. But that&#x27;s not what happened. Both the crowdfunders and the original developers wanted that old codebase that was already paid for by business investments as a starting point. Even though the later v2.5 was a big rewrite, that doesn&#x27;t change how the community thought of the v2002 software. It already has the interest level and evangelism to attract future work leading to the 2.5 rewrite.If Blender was \"zombie\" software, _why_ were people working on \"bad software\" to make it better? Work backwards from that. Consider the motivations and interests. Saying \"Blender v2003 wasn&#x27;t great\" doesn&#x27;t really explain things. reply yterdy 14 hours agorootparent>The explanation of the community momentum starts from the business investment. Again, an emphatic, \"No.\" Blender&#x27;s massive improvements in the early-to-mid 2010s are what brought investment from outside sources, not the other way around. It was an artifact around which interested parties could rally; that says nothing of its value as an executable piece of software, but rather its value as a focus to fulfill a need, which could be freely used to fulfill it. Its commercial codebase was not valuable to anyone but the people who meant to work on it as an open-source project, and only because it could be picked-through, modified, even most of it scrapped if desired. This can be - and has been - done with software that descended from non-commercial codebases, because the motivation to create a polished product does not necessarily lie in a profit motive, and it definitely is not a product of some esoteric design homeopathy wherein that profit motive lies buried somewhere in Blender&#x27;s code.I think it&#x27;s an insult to the hard work of the people who&#x27;ve produced the modern version of Blender that you insist on attributing the decisions they&#x27;ve made and implemented to some dedication to maintaining standards that did not exist for the product before they arrived.What I will concede is that everything that has happened was necessary for Blender to be precisely what it is precisely at this moment, for better or worse. Including it&#x27;s commercial history. I don&#x27;t like that this conversation is becoming so contentious and I would suggest you think about what like concessions you can make to your counterargument before becoming married to a wholly antagonistic stance. reply huijzer 18 hours agorootparentprev> That&#x27;s what separates it from most [...] projects: [...] and worked diligently on creating a solid piece of software (and documentation and community and support) for a long, long time.I&#x27;m starting to believe more and more that this is the essence to most things that look beautiful&#x2F;simple&#x2F;elegant. reply DonHopkins 18 hours agorootparentprev>and a commendable (though not unimpugnable) humility from developers who (finally) found the wherewithal to put the user experience before FOSS dogma or their own ambitions.That&#x27;s just what I was getting at when comparing Blender to GIMP in this other discussion:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38235871>All of these ideas could be applied to Gimp too, of course, but I&#x27;ve found the Blender developers to be much more open to entertaining other people&#x27;s ideas and contributions about user interface design than the Gimp developers, who have been historically NIH-limited and stubborn (especially about changing the name to something less offensive to the general public). At least Blender already supports pie menus well, and changed the default mouse bindings in response to user demand, and has made huge strides in usability lately. At this point I think it would be much easier to just add a great image editor to Blender, integrated with its video editor, than try to change the minds of the Gimp developers.[...]https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38232543>>Blender has something else. Loads of money compared to Gimp.>Blender EARNED every cent of its loads of money be being RESPONSIVE TO ITS USERS. reply DonHopkins 18 hours agorootparentprevWhat&#x27;s I really love about it now is how deeply and sincerely it&#x27;s integrated with Python. That didn&#x27;t used to be the case originally, but I believe it happened kind of early on, so it&#x27;s had a lot of time to mature, and there&#x27;s all that RNA&#x2F;DNA stuff to automate Python bindings. I&#x27;m working on learning more about that stuff myself! Do you know any more of the history, trials, and tribulations of that? reply yterdy 17 hours agorootparentI do not, but thank you for bringing it up. The free extensibility of the core software is related to its Python integration and a major part of Blender&#x27;s current success. Some essential add-ons have even become part of that core software over time.It would definitely be nice to know more about that history, though. reply i_am_a_peasant 20 hours agoparentprevAnd the user interface is all OpenGL... man I can&#x27;t imagine how much time it would take to write a decent user interface starting with nothing but OpenGL. Like just rendering text to the screen is a pain. And it&#x27;s so snappy and responsive and it looks sooooo good. Definitely not a one man project, there&#x27;s just no way. reply rabf 20 hours agorootparentRolling your own UI in openGL is very doable and can make a lot of sense for any application that will require non standard custom widgets. Use your preferred truetype rendering library to generate your text textures, blender uses freetype I think. Widgets can all be done with vectors and gradients, start with functions that create primitives such as rounded boxes or different line types and build from there. reply i_am_a_peasant 20 hours agorootparentYeah, it&#x27;s doable in a few months of doing nothing but that. ;DBut then again you may be a much more productive programmer than I.For a side project? I could never do it, not with everything else I got going on. reply arriu 19 hours agorootparentHaving done both web and gl ux for a living, I think you might be overestimating the complexity of a gl implementation and underestimating the complexity of meeting the same specs using a web implementation.While not quite the same thing, if you have the time, dip your toes into some immediate mode UI, for example imgui. It is enjoyable not a grind. reply PaulDavisThe1st 17 hours agorootparentOh yeah, sure my friend. You&#x27;re going to implement * text entry (right to left as well as left to right) * file browser * treeviews that scale to 50k nodes or 10x that * menusin GL in a month. Not believable. reply subtra3t 2 hours agorootparentNever did they ever say that would implement the UI of Blender (I assume that&#x27;s what you were referring to) using OpenGL. I think what they meant was that implementing just a UI in OpenGL isn&#x27;t as hard as the other guy thought reply i_am_a_peasant 18 hours agorootparentprevDo you think wasm has any chance of replacing the whole html&#x2F;css stuff with just webgpu in a canvas? I have been playing a bit around with wgpu in rust and I can compile the same project either as a native binary or a .js that just renders to the browser. It seems to work pretty cool. Photoshop seems to be runable in the browser now, and I&#x27;ve seen a lot of other cool stuff, but things like fluid simulations seem to still be very laggy. reply panzi 17 hours agorootparentRendering everything into a canvas will realistically mean total lack of accessibility features. Also you won&#x27;t be able to use the DOM inspector. I don&#x27;t think that would be an improvement at all. If one could work with the DOM via Wasm (plus source maps so you can still use the debugger) it might be something. reply dist-epoch 18 hours agorootparentprev> webgpu in a canvasInternally that&#x27;s what Chrome&#x2F;Firefox are, they render much of html&#x2F;css on the GPU using OpenGL&#x2F;DirectX&#x2F;Vulkan&#x2F;Metal.> Do you think wasm has any chance of replacing the whole html&#x2F;css stuff with justNo, you also need the DOM to enable frameworks like React which are used by a large number of sites. reply gosub100 14 hours agorootparentprevI have an OpenGL personal project that I&#x27;m struggling with the UI on (using QT5 currently with GLCanvas with buttons around it). If I wanted to switch to a GL-only UI (and discard QT altogether), how would I add event listeners? I can get a borderless window as a GL viewport, but I don&#x27;t know how to detect clicks and match them to which GL object. reply tedunangst 12 hours agorootparentYou have a click at pixel 43, so what did you draw between pixels 40 and 50? That&#x27;s what they clicked. You have to know what&#x27;s on the screen, but you should know this, because you put it there. Or just use dear imgui. reply robin_reala 19 hours agorootparentprevThere’s so much more than that though. You need to build your own accessibility tree and hooks into the OS’s assistive tech infrastructure just for a start. reply dumbo-octopus 19 hours agorootparentOr you can just not do that, which is what Blender does.To be fair, there&#x27;s not all that much use in Blender for folks who cannot see a screen clearly. reply robin_reala 19 hours agorootparentAssistive tech isn’t just screen readers. But yeah, Blender specifically is a visual design tool, I’ll give you that. reply yjftsjthsd-h 18 hours agorootparentI know it&#x27;s not just screen readers, but what a11y concerns would make sense for blender? It barely uses audio and what it does probably can be covered by external tooling. For visuals... the only things that occur are probably covered by Blender&#x27;s UI scaling&#x2F;zoom. But I could easily be forgetting something or ignorant.Edit: Oops, somehow forgot input - but there again, I would natively expect most things to work via keyboard&#x2F;mouse emulation, and beyond that you&#x27;d probably need custom integration, but it&#x27;s got the Python hooks to facilitate that. reply dumbo-octopus 18 hours agorootparentWhat comes to mind is someone with a tremor that is unable to use classic pointing devices and might have better luck using tab&#x2F;arrow-key navigation to move through the buttons&#x2F;menus&#x2F;etc. From my cursory examination of the product I don&#x27;t see much support for keyboard navigation, though as a professional tool I&#x27;m sure there&#x27;s a plethora of keyboard shortcuts that one could learn. reply yjftsjthsd-h 17 hours agorootparentGood point, I&#x27;d never noticed that before - menus seem fine once you get them open, but I can&#x27;t find a way to open any menu without clicking, and ex. the preferences pane does seem completely impossible to navigate via keyboard. So yes, I agree that that appears a downside to their own toolkit. reply panzi 17 hours agorootparentThough I think all&#x2F;pretty much all menu items can be accessed by pressing space and then typing the name of it. (If you use the setting space for command search.) There will be an auto completion list and it remembers the last action. So that is even better than what many other GUI applications do, where you have to search for ages in deeply nested menus for the action that you know how it is named, but don&#x27;t know where the hell it is hidden. Quite frankly every program should have that feature. reply dumbo-octopus 17 hours agorootparentprevFWIW you need to implement that yourself with basically any advanced enough toolkit. Even in HTML-land, any list widget worth it&#x27;s salt is handling keystrokes itself. replydoubled112 19 hours agorootparentprevIs Blender accessible this way? Or is it, as they said, just an OpenGL surface with some widgets painted on?My apologies for the word \"just\" doing a lot of work in that sentence.https:&#x2F;&#x2F;devtalk.blender.org&#x2F;t&#x2F;blender-and-accessibility-for-...> As a multi platform OpenGL app everything we draw is quite hidden from screen readers. Without cross-platform open source libraries available I can’t think of a feasible way of interacting with existing screen readers.https:&#x2F;&#x2F;blender.stackexchange.com&#x2F;questions&#x2F;301899&#x2F;blender-a... reply panzi 20 hours agorootparentprevWell, they basically wrote their own GUI toolkit. And IMO a very good one, with tiling windows and everything is nicely scaleable. I especially like the command search with space and little things like dragging over a column of checkboxes to toggle them all. reply themerone 20 hours agorootparentprevIf your choices are Motif or the Athena toolkit, rolling your own UI is an understandable choice. reply mr_sturd 20 hours agorootparentThey should roll it out in to a separate library. Call it the Blender Toolkit, or BTK. reply i_am_a_peasant 20 hours agorootparentI think there have been attempts.. but it&#x27;s too deeply married to the rest of Blender for it to be worth extracting I think. reply DonHopkins 18 hours agorootparentprevBTK has a nice ring to it -- and it&#x27;s an even more brilliant public relations disaster than the name \"GIMP\"!What kinds of widgets would Dennis Rader have in his \"Bind, Torture, Kill\" tookit? ;)It couldn&#x27;t be any worse than Motif.BTK would also imply the existence of:BLIMP: Blender Image Manipulation ProgramOh the humanity!BLOME Desktop: Blender Object Model EnvironmentI would totally be down for that. reply antod 7 hours agorootparentor...BIOME: Blender Interactive Object Model Environment reply KittyCatCuddler 18 hours agorootparentprevThey&#x27;re actually porting everything over to Vulkan now. That, and Apple devs are also working on maintaining and developing a Metal backend. reply i_am_a_peasant 14 hours agorootparentI wouldn&#x27;t bother with vulkan and just go with wgpu, rewrite it in rust while you&#x27;re at it ;D reply PaulDavisThe1st 17 hours agorootparentprevVulkan is a drawing API, not a GUI toolkit. reply tpush 16 hours agorootparentPresumably they meant porting from OpenGL to Vulkan & Metal. reply gosub100 16 hours agorootparentprevwhen the whole application is OpenGL it makes perfect sense. Just like how the interface to managing sql databases is done using SQL tables. reply redox99 20 hours agorootparentprevI hate so much how it breaks many conventions. For example, when you right click, the menu opens onPress instead of onRelease.I wish they had at least a setting to change that. reply jdiff 20 hours agorootparentIt makes it way faster when you can right click, hold, and release on the item in one smooth motion. I believe most pro software is like this, it&#x27;s hardly breaking convention. reply redox99 19 hours agorootparentI understand why some people like it, and yes a lot of software does that as well, but in my case sometimes I accidentally end up pressing some menu option because I&#x27;m not perfectly steady. That&#x27;s why I&#x27;d like it to be an option. reply mrob 19 hours agorootparentprevDid you get those the right way round? All other software I tested opens menus on button press. IIRC, Blender defaults to opening menus on button release, adding unnecessary latency and making it feel slow. You can change it, but you have to go through the whole Keymap Preferences and find every menu and change it individually, so it&#x27;s annoying. reply redox99 19 hours agorootparentOn my Windows machine, all the \"native\" software like the file explorer and so on, opens when releasing the right click. Blender 3.6 opens as soon as you press the right click (allowing you to \"drag\" the cursor to the menu option and releasing it there) reply redwall_hp 19 hours agorootparentThat&#x27;s the traditional Mac way. e.g. historically you would \"pull down\" on the menu bar at the top of the screen. You would click and hold, and slide the mouse down, releasing the button to choose an item. If you were to just click on a menu, it would quickly open and close again. I want to say it&#x27;s typical for Linux desktop environments to open contest menus on mouse down, as well, but don&#x27;t recall offhand.Blender dates back to 1994, when many modern conventions weren&#x27;t really a thing yet (Windows 95 didn&#x27;t even exist)...and they have put a lot of time into modernizations in recent years. Back when I was dabbling in Blender, in the early 2000s, Ctrl+S was the keyboard shortcut for \"erase everything and create a new scene.\" reply Manozco 20 hours agorootparentprevI&#x27;ve not used blender but it seems to me that Chrome is doing that too (at least it&#x27;s doing that on my machine ) reply capableweb 19 hours agorootparentI think it&#x27;s UX that commonly works differently in consumer software vs pro software. I think parent is talking about professional software.Pro software made for speed and efficiency, you usually want to be able to do things quickly, even if sometimes people not used to the software might screw up. Holding right-click, selecting menu item and releasing I think is one of those things.In consumer software, users would be confused because maybe they long-press the right-click, drag the mouse a little while holding down then releasing, and the menu would just appear and disappear. Confusing UX for most users, I bet. reply Manozco 19 hours agorootparentIt looks like Chrome is doing the pro software then.Parent said blender was opening the menu onPress, which according to your comment is OK for pro software, Chrome is also doing the menu onPress reply capableweb 18 hours agorootparentMaybe it depends on the platform? For me, on Windows with Chrome, it only opens the context menu once you&#x27;ve stopped holding down the button. In Blender (on Windows), the menu opens as soon as you hold the button. reply redox99 19 hours agorootparentprevI think I&#x27;ve seen that on some Linux distros. But on Windows, chrome works onRelease (like all built in windows programs do) reply Manozco 19 hours agorootparentHum that might explain the situation here. I&#x27;m on Ubuntu with KDE replyaequitas 19 hours agoparentprevI started using Blender to do modelling 20 odd years ago when I was still in school finding my direction in life. The graphics design path didn&#x27;t stick, but Blender integrated with Python to allow automations, which I learned along the way and it put me firmly on the path of software development, where I still happily am, still programming (some) Python. Funny that I have my career partly to thank to the software choices of fellow Dutchmen. reply ragebol 16 hours agorootparentWow, very similar story here. In highschool, I think I messed up a week of exams because I was too busy modeling a Fellbeast&#x2F;nazgul from LoTR in Blender. I still have the model somewhere, copied over in 20 years of USB sticks and external HDDs.Learned python for use in blender, am a software dev in robotics now, using a lot of python. reply seritools 20 hours agoparentprev> It feels like I&#x27;m using an Adobe product or similar.So it&#x27;s slow, bloated, and crashes regularly? reply nkozyra 20 hours agorootparent> So it&#x27;s slow, bloated, and crashes regularly?I think Adobe has really improved the last decade or so in this regard.Blender is really amazing and just seems to get more powerful every year. reply virtualritz 18 hours agorootparent> I think Adobe has really improved the last decade or so in this regardNope. Been using Photoshop since version 1, AfterEffects since its inception (by a company called Cosa AFAIR) and InDesign since version 1. 30+ years give or take.There all indeed so slow and bloated that they feel mostly unusable to me today.And it&#x27;s the opposite. It got to this over the last 10–15 years. Except Are. That was always slow but for motion graphics it&#x27;s hard to get around it.All the freelance VFX artists I know use old versions of PS and Ae.InDesign is so buggy that I switched to Affinity three years ago.Only younger people, who never experienced the snappy desktop systems & software of the 90&#x27;s or early 2000&#x27;s, think the state of things today is somehow normal. reply nkozyra 15 hours agorootparentNot version 1 but I&#x27;ve been on it since Photoshop 3 and have had the exact opposite experience. It was rough early on, crashed frequently and often. Got better around Photoshop 4&#x2F;5 then had another rough period.I&#x27;ve found the latest Creative Cloud versions to be the most stable. I cannot recall a single Photoshop crash. reply AuryGlenz 16 hours agorootparentprevI spend a good amount of time in Photoshop and I disagree that it’s slow and bloated, in general.I know after a new version a year or two ago some actions were noticeably slower, but I complained to the project manager on Reddit on the specifics and it got tightened back up pretty quickly.Also, anyone using old versions of Photoshop are missing out. Both the new Remove tool along with the generative AI fill are absolute gamechangers. reply jwells89 15 hours agorootparentprevYep. In my experience CS1 and CS2 are somehow more responsive feeling running on now-ancient and comparably resource-bare hardware like PPC G5 or Core 2 Duo Macs than PS CC is on a modern armed-to-the-teeth workstation, which is ridiculous.The bloat was ramping up pretty aggressively through CS3, CS4, and CS5 but the shift to subscription model really gave Adobe a license to not care about efficiency or UI snappiness. reply CyberDildonics 17 hours agorootparentprevThe solution is to just to avoid adobe stuff as much as possible. A lot of what would be done in photoshop can be done in nuke and after effects can just be thrown in the trash when using nuke. reply timeon 20 hours agorootparentprev> improved the last decadeI was experiencing less bloat and crashes in versions before Creative Cloud. reply orbital-decay 17 hours agorootparentprev> I think Adobe has really improved the last decade or so in this regard.They massively increased the complexity of their products after introducing the Creative Cloud. It&#x27;s an absolute mess that can cause delays in Microsoft (!) just to make the new Windows release work with it. And Photoshop in particular has the byzantine PSD format that requires them to keep old versions of the code to read old versions of the format.Adobe software in general resembles Windows nowadays - they are trying to modernize it but legacy decisions keep them from doing too much, as they can&#x27;t break backward compatibility, their entire business model depends on keeping the users locked in. As a result, their old software is a terrible mishmash of UIs from different eras. reply Maken 19 hours agorootparentprevAlso, it won&#x27;t shut up about it&#x27;s couple AI utilities. reply cptskippy 17 hours agorootparentHave you tried the AI Feedback Utility? reply cableshaft 18 hours agoparentprevYeah, Blender is maybe the best open source software I use. I don&#x27;t feel a need to use any other 3D modeling software, it&#x27;s so robust and works so well (from what I can tell, I&#x27;m still mostly an amateur and have barely scratched the surface of Blender still). reply jve 19 hours agoparentprevThey have quite a list of sponsors: https:&#x2F;&#x2F;fund.blender.org&#x2F;Like EPIC giving them 1.2M: https:&#x2F;&#x2F;www.blender.org&#x2F;press&#x2F;epic-games-supports-blender-fo... reply ics 19 hours agorootparentIs the €134k&#x2F;mo figure including all corporate sponsors? Approximating that as one developer salary per month then what they&#x27;ve continued to accomplish is still very impressive. reply robin_reala 19 hours agorootparentNo one developer gets paid €134k&#x2F;month in Europe. Average salary is probably more like €65k&#x2F;year across the EU. reply ics 19 hours agorootparentI was going to ask if all Blender developers were in Europe but was able to answer my own question from https:&#x2F;&#x2F;www.blender.org&#x2F;development&#x2F;top-27-committers-2022&#x2F;It does appear that the majority of top contributors are from European countries but overall pretty diverse: China, Australia, New Zealand, Russia, Brazil, Egypt are represented as well.That said, if I revised my comment to say two developer salaries per month, or 3, 4, etc. it is still impressive in my view. reply agent327 15 hours agorootparentprevWhere in the world can you earn €134k&#x2F;mo as a developer? reply jve 15 hours agorootparentprevThat is probably excluding onetime donations reply marcodiego 19 hours agoparentprev> I&#x27;m amazed at how professional and finished Blender is for being an open source project.I&#x27;m amazed at how much prejudice people still have about open source projects. reply sbuk 18 hours agorootparentWhen you compare it to GIMP, it&#x27;s night and day. Not only is Blender open source, it is professional software. reply crote 17 hours agorootparentYup. GIMP feels like it&#x27;s being developed by a bored engineer in their spare time to play around with image editing algorithms. Blender feels like a professional workhorse you can build your whole business around. reply beezlewax 12 hours agorootparentIf gimp did a UI overhaul akin to what Blender did and changed their name they could do much better that the current state of things.Blenders ui overhaul made it an entirely different beast reply jasode 19 hours agoparentprev> I&#x27;m amazed at how professional and finished Blender is for being an open source project. It feels like I&#x27;m using an Adobe product or similar. The \"open source\" label applied to Blender today inadvertently minimizes the commercial origins of it.Blender&#x27;s development has a unique (accidental) history that other open-source projects can&#x27;t replicate deliberately: https:&#x2F;&#x2F;docs.blender.org&#x2F;manual&#x2F;en&#x2F;latest&#x2F;getting_started&#x2F;ab...In summary:- Blender was originally paid commercial software that attracts venture capitalists to invest €4.5M and funds the salaries of 50 people to work on it- The VC investors later sell it at a loss for €100k back to the original team to release it as open source.Because many users of Blender are unfamiliar with the timeline of its development, they wonder why other open-source software like Gimp \"isn&#x27;t more polished\" like Adobe Photoshop. Well, Gimp never had investors write off €4.5M of development on it. reply yterdy 19 hours agorootparentThat was literally over 20 years ago. The majority of Blender&#x27;s development has taken place while it was open-source; most of the features it&#x27;s known for did not exist in the commercial version. reply danShumway 16 hours agorootparentprev> Gimp never had investors write off €4.5M of development on it.Notably, there is literally nothing stopping other Open Source projects from being given grants of millions of dollars other than social convention.We saw this with Godot after Unity. There&#x27;s a coordination problem here, but we all mostly recognize that many of these fundamental tools would be better if we all collectively put our resources into an Open tool that is focused on serving the community rather than exploiting it.It&#x27;s just that without shocking events that prompt the bigger players to say, \"you know what, heck this, let&#x27;s just fund a good tool\", it&#x27;s very tough to get people to make that kind of investment, even though it would very likely be better for them and in the long-run more cost effective for them if they did.I would argue that the majority of Blender&#x27;s development (both architecturally and in terms of cost) happened after its commercial origins. But Blender continued to get investment because the community was invested in building a usable tool that wouldn&#x27;t force them to deal with the crap of the other commercial products in the 3D industry. reply magpi3 19 hours agorootparentprevHaven&#x27;t used it in ages, but Blender did not feel like an Adobe product when it was first open sourced. IIRC its UI was considered notoriously unintuitive. reply js8 18 hours agorootparentprevWith selling it as OSS, they only wrote off 4.4M euros. So given the situation it was still a better option for investors. reply Finnucane 18 hours agoparentprevThe difference between Blender and Adobe is that the Blender folks actually give a shit if their product is good. reply boredtofears 17 hours agoparentprevWhat did you use to get started?Every other year I follow that \"make a realistic looking donut\" tutorial but mine always comes out lumpy and misshapen. reply Zetobal 20 hours agoprevBlender 4.0 RC1 would be a fitting headline.... reply justinclift 17 hours agoparentThere&#x27;s no official RC1 build for download either, and hasn&#x27;t been from the first of Nov (when it was supposed to be available).I&#x27;ve been looking every few days from the 1st of Nov, and the Release Candidate date on the projects page was pushed back to the 8th, but not updated since then.The official Projects page still lists the Blender 4.0 goal as only \"92% Completed\":https:&#x2F;&#x2F;projects.blender.org&#x2F;blender&#x2F;blender&#x2F;milestone&#x2F;7There&#x27;s nothing on the Blender Blog about an RC1 being available for testing:http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231114142752&#x2F;https:&#x2F;&#x2F;code.blend...Nor is there any mention in yesterday&#x27;s weekly Devtalk:https:&#x2F;&#x2F;devtalk.blender.org&#x2F;t&#x2F;13-november-2023&#x2F;31960If there really is a 4.0 release that&#x27;s available now, then they&#x27;ve seriously gone wrong with the communication parts of their release process. :( :( :( reply justinclift 5 hours agorootparentLooks like that really is the Blender 4.0 release.But there&#x27;s still absolutely no mention of it on the official blog:https:&#x2F;&#x2F;code.blender.orgNor of the RC1 being available at some point before.WTF? :( :( :( reply wccrawford 20 hours agoparentprevI think the point is that it&#x27;s no longer RC as of today. Someone else said that that page is lagging a little bit. reply Zetobal 19 hours agorootparent...but it is because there is no release yet. reply tokai 20 hours agoparentprev\"please use the original title, [...] don&#x27;t editorialize.\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply geenat 20 hours agoprevWould really love to see more resources be put into smoothing out the workflow of the video editor.. even just quality of life features such as \"always showing waveforms\" and auto-snapping clips when you cut would make a drastic difference and make it highly competitive with Premiere because you already have a world class 3D suite built in. reply throw_m239339 19 hours agoparentIts interface is pretty horrible but it&#x27;s serviceable for basic video editing, and parameters can be keyframed, so that&#x27;s something. The issue is that the video editor uses the same UI philosophy as the 3D editor... and it feels awkward. Same with the 2D editor, Grease Pencil.I don&#x27;t think it would ever be able to compete with Premiere or Da Vinci Resolve, unless they put significant resources on that stuff.There is no better free alternative for non CAD 3D modeling, sculpting, texturing or 2D animation AFAIK. There are alternative rendering engines though.For video editing, compositing there are free alternatives that are much better than what Blender has. reply geenat 18 hours agorootparentIt&#x27;s okay-ish for projects that involve planned, pre-rendered scenes of pre-determined length.For anything involving live-action clips (using it for \"talking head + b roll\".. tutorials, documentaries etc) mainly could use the features I noted above.More workflow polish the better, though. Makes it good for a wider variety of video types. reply throwaway17_17 19 hours agorootparentprevWhat is the current leader for compositing in the open source space? reply chabad360 18 hours agorootparentProbably Natron. Although, Blender is certainly catching up. reply rspoerri 18 hours agoprevThere are Builds that are listed as \"Blender 4.0.0 - Stable\" in various formats and for all platforms on the Daily Builds page:https:&#x2F;&#x2F;builder.blender.org&#x2F;download&#x2F;daily&#x2F;But i don&#x27;t know whether those are the ones that will go official. reply dmalik 16 hours agoprevNice! Time to create another donut. reply CobrastanJorji 15 hours agoparentIt&#x27;s kind of amazing how a single tutorial project became so universal. I wonder if there are any other disciplines &#x2F; platforms &#x2F; projects that are equally universal in their domain. Programmers have \"hello, world,\" but it&#x27;s awfully tiny by comparison. reply kaveh808 13 hours agoprevIs there any movement towards a C&#x2F;C++ API in Blender 4.0? I feel extensibility may be where it lags behind Maya and Houdini. reply dagmx 9 hours agoparentNo. Unfortunately this is something they’re philosophically against as they feel you should just build from source if you need it. They’re worried it could undermine the spirit of the GPL if people were sharing compiled plugins instead reply pixelbyindex 16 hours agoprevIs anyone here that might be a Blender contributor who could offer some insights or some tips on how to achieve the same spark with the Godot community? From the outside, the Blender community iterates very quickly, seem to work well together, and has figured out an approval process for integrating code that just works™.I would love to see a fire like this in Godot reply brundolf 16 hours agoparentFrom my understanding Blender started getting a ton of corporate sponsorship a few years ago, and that was when development suddenly went into overdrive reply JellyBeanThief 8 hours agorootparentGodot hasn&#x27;t been standing still on that: https:&#x2F;&#x2F;fund.godotengine.org&#x2F;So if you want to boost Godot and you haven&#x27;t the skills or time, there&#x27;s always donating. reply brundolf 7 hours agorootparentTrue, though I&#x27;m not sure any amount of individual donations could have the level of effect corporate sponsorship has had on Blender reply brundolf 17 hours agoprevAmazing that even blender.org isn&#x27;t immune to the hug of death (though probably from more than just HN) reply specproc 17 hours agoprevAs a noob who&#x27;s still finding his way around, space to search menus looks great.https:&#x2F;&#x2F;projects.blender.org&#x2F;blender&#x2F;blender&#x2F;commit&#x2F;35d3d525... reply Zee2 16 hours agoparentThis (in some form) has been in Blender since at least 10 years ago. It used to be the default behavior. reply RobKohr 17 hours agoparentprevSomeone should make a video on this feature. I also am a noob who is lost in a sea of options when starting up blender. reply qiller 14 hours agoprevI still have the old 2.30 book signed by Ton from the original fundraiser, so glad it all worked out great.May be it&#x27;s time to finally let go of 2.7 keymap and learn the new one :-\\ reply Tomte 21 hours agoprevNot yet released. Will we have that for every major project now? reply misnome 21 hours agoparentIt&#x27;s been tagged, so I assume this page is just lagging a little. That said, it could have waited.... reply wewxjfq 20 hours agorootparentYeah, especially since Blender puts a lot of effort into their release websites. reply Tomte 17 hours agorootparentHere it is: https:&#x2F;&#x2F;www.blender.org&#x2F;download&#x2F;releases&#x2F;4-0&#x2F; reply KolmogorovComp 16 hours agorootparent@dang please update the top-link to the release link above ^ reply yjftsjthsd-h 19 hours agoparentprev> Will we have that for every major project now?Don&#x27;t we already? Pretty sure it happened for recent freebsd and Debian releases, at least reply tokai 20 hours agoparentprevHave what? Release candidates? reply Tomte 20 hours agorootparentA race to prematurely submit not-yet-released software. reply iamgopal 17 hours agoprevSlightly unrelated, but is there anything that can be scripted using code for making animation ? reply cshimmin 17 hours agoparentblender has tightly integrated support for python scripting. reply justinclift 17 hours agoparentprevBlender can be controlled using Python, so yes. reply babypuncher 15 hours agoprevA bit tangential, does anyone know of a good resource for learning Blender, suitable for someone who has no real experience with 3D modeling&#x2F;animation? reply golergka 15 hours agoprevHere&#x27;s a more user-friendly and beautiful page about 4.0 release: https:&#x2F;&#x2F;www.blender.org&#x2F;download&#x2F;releases&#x2F;4-0&#x2F; reply mentos 18 hours agoprevThe control scheme in Blender is so foreign to me.When they support rmb + WASD to fly around maybe I&#x27;ll be able to use it heh reply genpfault 18 hours agoparentFly&#x2F;Walk Navigation[1]?[1]: https:&#x2F;&#x2F;docs.blender.org&#x2F;manual&#x2F;en&#x2F;latest&#x2F;editors&#x2F;3dview&#x2F;nav... reply coder543 18 hours agoparentprevhttps:&#x2F;&#x2F;github.com&#x2F;SpectralVectors&#x2F;RightMouseNavigation reply SV_BubbleTime 18 hours agoprev [–] I’m hoping for a built-in resource sharing mechanism. I have a dual 4090 server accessible but right now all I can do is run blender on it. I’d love a way to just transparently use its resources while working on my machine.Seems I’ll have to keep waiting. reply lyu07282 18 hours agoparentI&#x27;m not sure waiting will change anything, usually you save the blend file somewhere and a render cluster picks it up for render. Blender already supports everything you would need to set this up and there are already tons of cloud services and third party solutions for that. reply opencl 16 hours agoparentprevDoes Flamenco[1] not meet your requirements? Technically not built in but it gives a button right in the Blender UI to send render jobs to remote machines.[1] https:&#x2F;&#x2F;flamenco.blender.org&#x2F; reply rvrs 18 hours agoparentprev [–] >dual 4090 serverWhat do you mean? 4090s don’t have SLI. reply valine 17 hours agorootparent [–] You don&#x27;t need SLI to render with dual GPUs in cycles. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Blender 4.0 was released on November 14, 2023, with significant updates and improvements in various areas such as animation, rigging, modeling, and more.",
      "The release includes changes to the mesh format and new compatibility requirements for graphics cards.",
      "The update also brings enhancements in areas like geometry nodes, shading, texturing, cycles, and the user interface, among others."
    ],
    "commentSummary": [
      "Users delved into different topics surrounding Blender, such as its release notes, job prospects in animation, and the use of linear algebra and programming in rendering and character animation.",
      "The success of Blender as an open-source project and its user interface development were also discussed.",
      "Users compared Blender to Adobe software and explored the potential for a built-in resource sharing feature."
    ],
    "points": 660,
    "commentCount": 224,
    "retryCount": 0,
    "time": 1699963890
  },
  {
    "id": 38261415,
    "title": "EU Parliament Takes Steps to Protect Children from Online Sexual Abuse",
    "originLink": "https://www.europarl.europa.eu/news/en/press-room/20231110IPR10118/child-sexual-abuse-online-effective-measures-no-mass-surveillance",
    "originBody": "Child sexual abuse online: effective measures, no mass surveillance Press Releases LIBE Yesterday Share this page: Facebook Twitter LinkedIn WhatsApp Balance between need to fight child sexual abuse online and to avoid generalised monitoring of the internet Platforms need to assess risks concerning their service If mitigation measures not enough, court-validated detection orders to be used to track down illegal material Parliament wants to exclude end-to-end encrypted material from detection On Tuesday, the Civil Liberties Committee adopted its position on new measures to protect children online by preventing and stopping child sexual abuse. The draft Parliament position was adopted by the Committee on Civil Liberties, Justice and Home Affairs with 51 votes in favour, 2 against, and 1 abstaining. Inter-institutional negotiations were authorised with 48 in favour, 2 against, and 4 abstaining. To protect children online, the new rules would mandate internet providers to assess whether there is a significant risk of their services being misused for online child sexual abuse and to solicit children, and to take measures to mitigate these risks. MEPs want mitigation measures to be targeted, proportionate and effective, and providers should be able to decide which ones to use. They also want to ensure that pornographic sites have adequate age verification systems, flagging mechanisms for child sexual abuse material (CSAM) and human content moderation to process these reports. To stop minors being solicited online, MEPs propose that services targeting children should require by default user consent for unsolicited messages, have blocking and muting options, and boost parental controls. Detection orders To avoid mass surveillance or generalised monitoring of the internet, the draft law would allow judicial authorities to authorise time-limited orders, as a last resort, to detect any CSAM and take it down or disable access to it, when mitigation measures are not effective in taking it down. In addition, MEPs emphasise the need to target detection orders to individuals or groups (including subscribers to a channel) linked to child sexual abuse using “reasonable grounds of suspicion”. In the adopted text, MEPs excluded end-to-end encryption from the scope of the detection orders to guarantee that all users’ communications are secure and confidential. Providers would be able to choose which technologies to use as long as they comply with the strong safeguards foreseen in the law, and subject to an independent, public audit of these technologies. EU Centre for Child Protection The law would set up an EU Centre for Child Protection to help implement the new rules and support internet providers in detecting CSAM. It would collect, filter and distribute CSAM reports to competent national authorities and Europol. The Centre would develop detection technologies for providers and maintain a database of hashes and other technical indicators of CSAM identified by national authorities. The Centre would also support national authorities as they enforce the new child sexual abuse rulebook, conduct investigations and levy fines of up to 6% of worldwide turnover for non-compliance. MEPs finally propose to create a new Victim’s Rights and Survivors Consultative Forum to make sure that victims’ voices are heard. Quote Rapporteur Javier Zarzalejos (EPP, Spain) said: “To meet this compelling challenge effectively, we have found a legally sound compromise supported by all political groups. It will create uniform rules to fight the sexual abuse of children online, meaning that all providers will have to assess if there is a risk of abuse in their services and mitigate those with tailor-made measures. As a last resort, detection orders can be used to take down abusive material still circulating on the internet. This agreement strikes a balance between protecting children and protecting privacy.” Next steps The draft Parliament position still needs to be endorsed by the plenary. On 20 November, the start of negotiations will be announced, and MEPs have until the end of the following day to object. If a sufficient number choose to do so, there will be a vote during the same session. Contacts: Janne OJAMO Press Officer Contact data: Phone number: (+32) 2 284 12 50 (BXL) Mobile number: (+32) 470 89 21 92 E-mail: janne.ojamo@europarl.europa.eu E-mail: libe-press@europarl.europa.eu Twitter account: @EP_Justice Further information Documents, including compromise amendments Procedure file EP Study: Combating child sexual abuse online (12 June 2023) Committee on Civil Liberties, Justice and Home Affairs Product information Ref.: 20231110IPR10118 Share this page: Facebook Twitter LinkedIn WhatsApp Sign up for mail updates PDF version",
    "commentLink": "https://news.ycombinator.com/item?id=38261415",
    "commentBody": "EU Parliament Civil Liberties Committee adopts position on CSARHacker NewspastloginEU Parliament Civil Liberties Committee adopts position on CSAR (europa.eu) 553 points by orian 23 hours ago| hidepastfavorite211 comments mbwgh 21 hours agoContrary to what this heading suggests, nothing has been \"decided\" here. This article is not only inaccurate, it is falsely misleading.The Committee on Civil Liberties, Justice and Home Affairs (LIBE) adopted a \"draft Parliament position\" [0] and that&#x27;s that.This still needs to go through so-called \"tri(a?)logue negotiations\", held between the EU parliament, commission and council. [1]Still a tad early for calling this a win![0] - https:&#x2F;&#x2F;www.europarl.europa.eu&#x2F;news&#x2F;en&#x2F;press-room&#x2F;20231110IP...[1] - https:&#x2F;&#x2F;netzpolitik.org&#x2F;2023&#x2F;ueberwachung-eu-innenausschuss-... (German) reply justin66 20 hours agoparent> Contrary to what this heading suggests, nothing has been \"decided\" here. This article is not only inaccurate, it is falsely misleading.The heading indicates a committee adopted a position, and that&#x27;s true. reply mici 19 hours agorootparentThe original title was \"EU Parliament Decides That Your Private Messages Must Not Be Scanned\" and it was linked to a different article, I think this comment was written before the title and the url were changed.https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231114102908&#x2F;https:&#x2F;&#x2F;news.ycom... reply mcv 18 hours agorootparentThey changed both the title and the article? Or did they move the discussion to a different post? It feels a bit like the Post of Theseus. reply justin66 19 hours agorootparentprevInteresting! Thanks. reply latexr 20 hours agoparentprev> tri(a?)logueIt’s not a trial-ogue. It’s a tri-logue because it involves three parties. reply JrProgrammer 20 hours agorootparentAnother spelling for trilogue is trialogue** https:&#x2F;&#x2F;dictionary.cambridge.org&#x2F;dictionary&#x2F;english&#x2F;trialogu... reply latexr 14 hours agorootparentI stand corrected. Thank you. In retrospect it makes sense, since we also write dialogue and not dilogue. reply d1sxeyes 20 hours agorootparentprev“Di” as a prefix is normally adequate when referring to two (dimension, dilemma, dichotomy, etc) and yet it acquires an “a” in “dialogue”. Possibly a similar word constructed from the “tri” prefix should also have an “a” inserted. reply epicalex 19 hours agorootparentThe etymology of dialogue is from the Greek `dia` meaning `through` and `logos` meaning `words`.https:&#x2F;&#x2F;www.etymonline.com&#x2F;word&#x2F;dialogue explains the common confusion with di- reply d1sxeyes 13 hours agorootparentThank you! TIL. reply readyplayernull 20 hours agorootparentprevBut \"a\"&#x2F;\"an\" is also used to negate so it can&#x27;t be placed at will.Could &#x2F;&#x2F;di-a-logos&#x2F;&#x2F; mean dividing (like in dissecting) something that ISN&#x27;T understood? reply aziaziazi 20 hours agorootparentAnalogs systems aren’t about dividing, are they ? reply readyplayernull 20 hours agorootparent\"di\" is the part that means \"two\" or \"cut in two\", while \"a\" is negation. reply Obscurity4340 20 hours agorootparentprevTripartite might also work reply danaris 19 hours agorootparentAh, yes: negotiations between the EU Father, the EU Son, and the EU Holy Spirit. reply Obscurity4340 18 hours agorootparentWas also gonna drop trilateral reply Obscurity4340 14 hours agorootparent> danarisPrinccccesssssssssss! reply danaris 14 hours agorootparentYou may be thinking of Daenerys?I&#x27;m just Dan Aris. reply Obscurity4340 12 hours agorootparentYour name has no right to be that slapping ;) reply Obscurity4340 18 hours agorootparentprevAre we wrong XD? reply yccs27 20 hours agorootparentprevIt&#x27;s also not a dial-ogue, but a di-a-logue because it involves two parties. reply Obscurity4340 18 hours agorootparentAh, so this is what the whole polylogue has been surrounding reply nehal3m 22 hours agoprevI was upset at the parliament for even considering this and extremely relieved that they chose not to pursue this draconic instrument. That said, we as citizens will have to remain vigilant and let our voices be heard because the onslaught on our privacy is constant from both commercial interests, spy agencies and state actors. These parties have no issue keeping pressure up over decades whereas we the citizens can become exhausted and exasperated.We need some laws to swing our way; enshrine our rights to privacy in clear terms so implementing laws like chat control become a non-starter. reply eigenket 22 hours agoparentThe EU parliament did its job pretty much 100% correctly in my opinion. Their job is to consider the laws the EU commission suggests and thats what they did. They correctly determined it was a shitty law and voted it down.In my opinion you should be upset at the EU commission, and especially commissioner Ylva Johansson from Sweden who seems to be the one pushing this stupid stuff. reply nehal3m 22 hours agorootparentI have to admit I&#x27;m not very knowledgeable about the process, so I thank you for correcting me. :-) reply lock-the-spock 20 hours agorootparentThe previous commenter is not correct. Parliament votes for its own position on this law, which is then negotiated with the Council ( the 27 national government representatives) which itself has already developed a position. The two institutions negotiate a compromise (wir support from the commission as broker) which then both institutions must vote on in order for it to become EU law. So they can still vote something down later, but generally if it comes to a vote on the final text it is already a position parliament agrees with. In the EU processes laws that are unlikely to be agreed usually don&#x27;t even get to a vote, rather the commission withdraws it&#x27;s proposal and provides a new one. reply lost_tourist 17 hours agorootparentprevI think the problem is that they didn&#x27;t rebuke these groups and tell them that the people don&#x27;t want to approve stazi tactics in order make it easier for police surveillance for their pet \"concern\". We all know the primary reason is to slowly sneak in surveillance everywhere, not necessarily for evil purposes, but a system which once set becomes extremely easy to expand upon for surveillance all the time, for any reason because a democratic society just accepted it. reply pimterry 22 hours agoparentprev> We need some laws to swing our way; enshrine our rights to privacy in clear terms so implementing laws like chat control become a non-starter.I think those are already in place - one major point against the previously suggested approach was that it would conflict with a bunch of existing regulation, and so it would never get past the courts even if it was passed.Two convenient examples:- Article 8 of the EU convention of human rights guarantees a right to privacy, specifically that \"Everyone has the right to respect for his private and family life, his home and his correspondence\": https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Article_8_of_the_European_Conv.... Clearly conflicts with \"let&#x27;s scan everybody&#x27;s correspondence\".- The E-Commerce Directive defined the rules for online business in the EU back in 2000, and specifically prohibits states from ever imposing general monitoring obligations: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Electronic_Commerce_Directive_....For all its problems, in areas like this the EU is actually pretty well set up. reply rsynnott 21 hours agorootparent> Article 8 of the EU convention of human rightsWorth noting that this isn&#x27;t just a regulation; since the Lisbon Treaty it is effectively part of the EU&#x27;s _constitution_, and can&#x27;t simply be regulated or legislated away.Though also note that it&#x27;s the European convention on human rights, not the EU one. It&#x27;s from the Council of Europe, a separate body, but the Lisbon Treaty effectively enshrined it in EU law.EDIT: Nope, see comment below. The terminology is a bit of a mess... reply arlort 21 hours agorootparent> but the Lisbon Treaty effectively enshrined it in EU lawActually no, ECHR rulings and the ECHR itself are considered guiding principles when the ECJ decides related questions but the EU is not technically bound by the ECHRThis is made moot by the fact that the EU doesn&#x27;t have independent enforcement so all EU law is enforced by the member states and all member states are members of the CoE and the ECHR has already ruled that a member state can&#x27;t violate the ECHR and justify itself by saying they were following EU lawBut the comment above could also be referring to the EU charter of fundamental right which is binding on EU institutions and EU member states (when they&#x27;re implementing&#x2F;enforcing EU law), article 8 of the charter is about the protection of personal data so you can read the original comment both ways.Either they said EU instead of European and were talking about the ECHR&#x27;s \"Right to respect for private and family life, home and correspondence\" or they said convention of human rights instead of \"charter on fundamental rights\" and were referring to the EU&#x27;s \"Protection of personal data\"Fun reply rsynnott 20 hours agorootparentOh, fair, yep. Ugh, the terminology is a mess. reply Vinnl 21 hours agorootparentprevAnd just to make sure, note that the \"Council of Europe\" is a different (and non-EU) body than the Council of the European Union and European Council. reply Maken 22 hours agoparentprevThe European Parliament has no saying in what laws it votes. The European Commission draft those, and the Parliament can at best rectify or reject them. reply gpderetta 22 hours agorootparentThe Parliament can ask the Commission to table a specific law though. And it can dismiss the commission at will.So it doesn&#x27;t have de-jure legislative powers, but de-facto it does. reply xxs 21 hours agorootparent> Commission to table a specific law thoughJust for the folks on the West side of the Pond, pretty much means to drop the bill. \"To table\" is so weird. reply SiempreViernes 21 hours agorootparentNo, \"to table\" in terms of (EU) parliamentary procedure means to begin the consideration, it&#x27;s the US that uses \"to table\" to mean dismiss[1].[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Table_(parliamentary_procedure... reply stephen_g 20 hours agorootparentVery strange, the US usage seems to be like what we would use “to shelve” for (as in take it off the table and put it on the shelf to forget about), at least in Australia and the UK. reply gpderetta 20 hours agorootparentprevAs if the English language was not complex enough :D reply gpderetta 21 hours agorootparentprevSorry for the confusion, I&#x27;m not a native speaker (although I live in UK). I meant that the parliament can ask the commission to present a specific law for discussion.Of course the parliament doesn&#x27;t need to ask the commission to drop a law, it can simply vote against it. reply bjornsing 22 hours agoparentprevYes, and the sad reality is that even if each one of these absurd legislative proposals has only a 1% chance of passing, sooner or later they will. That’s just math &#x2F; probability theory. We have to find some way of reducing the inflow. reply rsynnott 21 hours agorootparent... Wait, no, that&#x27;s not how it works. The commission (broadly representing the will of the member states) proposes laws, the parliament (broadly representing the will of the people of the member states) votes on them. It&#x27;s perfectly plausible that these remain permanently out of whack.I think you see this dynamic in action more with the commission vs EU parliament dynamic than you do with national government vs national parliament because in many countries there are, in practice, consequences to the government losing a vote in parliament, so governments will generally mostly restrict themselves to bills that they think they can win. There are no such consequences in the EU system, so you see a lot of this. reply orian 23 hours agoprevFrom the source:> Chat control - one of the worst EU plans that is also being described as a surveillance monster - must be stopped. And the EU Parliament has just decided to do so! In a historic agreement on the EU Commission&#x27;s Child Sexual Abuse Regulation (CSAR) the European Parliament wants to remove chat control requirements and safeguard secure encryption. The decision came after extensive backlash against the original proposal from technology and security experts, to international scientists and to citizens across Europe. This is a great win for our right to privacy and for upholding our democratic values in Europe, but the fight continues!What did the EU Parliament decide?Breyer writes on his website that internet services and apps must be \"secure by design and default\". The EU Parliament has agreed to: \"safeguard the digital secrecy of correspondence and remove the plans for blanket chat control, which violate fundamental rights and stand no chance in court. The current voluntary chat control of private messages (not social networks) by US internet companies is being phased out. Targeted telecommunication surveillance and searches will only be permitted with a judicial warrant and only limited to persons or groups of persons suspected of being linked to child sexual abuse material.\"A huge win for our privacy rights is also that the EU Parliament has decided to \"clearly exclude so-called client-side scanning\".In contrast to the original chat control proposal, the version of the EU Parliament wants that a new EU Child Protection Centre proactively searches publicly accessible parts of the internet for child sexual abuse material with automatic crawling, which can also take place in darknet and would be much more efficient than private surveillance measures by providers. Found abuse material must be reported and taken down by the provider. Fight is not overWhile the EU Parliament&#x27;s decision is a huge win, the fight is not over. It is expected that the EU Commission will continue to push for general surveillance chat control measures. Now is the time for each and everyone of us to join this fight! reply jdiez17 23 hours agoparentWell, that is a big relief. Common sense prevails, which is absolutely not a given these days. reply TacticalCoder 21 hours agoparentprev> ... which violate fundamental rights and stand no chance in courtThankfully the EU still has the European Convention on Human Rights and an associated court which individuals can go to and sue their state: the European Court of Human Rights. This is unlike the European Court of Justice which cannot directly be seized by individuals.That EU Convention on Human Rights contains the \"right to privacy\" (art. 8).This may be what they meant by saying that this horrible text stood no chance in court: a deluge of individual going to to the EU Court of Human Rights invoking article 8.Now I don&#x27;t doubt that the sold outs and enemy of the EU states at the European Commission are going to come back with other horrible measures.As a sidenote this whole \"good cop (European Parliament) &#x2F; bad cop (European Commission)\" is a bit of a farce played on the EU people too. reply ta1243 22 hours agoparentprevThe fight is never over.That&#x27;s why we have strong checks and balances, the commission (made up of appointees of the 26 EU government heads) will push things, the council (made up of those heads) have to agree, and the Parliament (made up from a popular vote) has to agree, then if all that fails the courts step inBut the fight isn&#x27;t as much against the government, it&#x27;s the hearts and minds of the people to make them care more about their own privacy and security rather than \"someone think of the children\". reply jliptzin 21 hours agorootparentI don’t think the average person cares about privacy, sadly. “I have nothing to hide,” etc. The best argument I heard against this crazy policy (only heard recently during these hearings) was that many creators&#x2F;sharers of this CSAM material is kids themselves! Young teens exploring their sexuality, swapping nudes with each other. So that means that if you’re scanning all messages and you come across one of these chats, instead of that conversation staying between the 2 teens (like it should), it’s getting scanned by the provider, flagged, uploaded upstream to some law enforcement center where god knows how many other people are going to be looking at it, only to realize that there was no abuse happening. And of course that comes with the risk of leaks&#x2F;hacks&#x2F;rogue employees spreading it even further. Completely insane! reply nonrandomstring 20 hours agorootparentprev> The fight is never over.Checks and balances are working this time hopefully. Regardless our (good) multi-stage processes and multi-chamber structure and even, regardless matters of lobbying and money - in the moment of quiet while smoke is still on the wind - look for the shooter. \"Who wants this?\" and \"What are their fears?\" leads to a better leverage point closer to values than parameters.There are a lot of people in the world right now anxious about the digital future. The EU Commission seem to hear too much from Chicken-Licken&#x27;s gang of sky-repellant gizmo salesmen and not from calmer humane optimists. reply ta1243 16 hours agorootparentI can&#x27;t see any specific benefit to this for a corporation, my feeling is it&#x27;s a political response to a general disquiet with the tech industry abusing people, along the lines of \"something must be done, this is something, therefore this must be done\"I&#x27;m sure some companies would make a fortune, but their lobbying power would be outweighed by other multinational companies like whatsapp and smaller companies like mullvad. reply gmerc 22 hours agoparentprevThe price of freedom is eternal vigilance wasn’t a platitude reply PoignardAzur 22 hours agoparentprevHuh, they took a pretty holistic approach to the issue. reply eigenket 23 hours agoprevDespite the lobbying from American organisations (Google, Facebook, Microsoft, Amazon, Palantir and others who worked with Thorn on this [1][2]) the EU Parliament did the right thing this time.[1] https:&#x2F;&#x2F;www.thorn.org&#x2F;partnerships&#x2F;[2]https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20130420162917&#x2F;http:&#x2F;&#x2F;www.wearet... reply eddtries 22 hours agoparentThe thing that I find shocking about lobbying is just how cheap it is. In the FTX trial the amount they spent lobbying politicians for pro-crypto laws was released and we&#x27;re talking low five figures. SBF even said lobbying was very cheap for the impact you can get.Naively I thought it would cost millions to get a politician to support you but actually it&#x27;s cheap enough a FAANG engineer could individually pay enough to lobby someone lol.Edit: Literally the user base on HN would be able to crowdfund and organise a lobbying group greater than the NRA ($2.93M in 2022) if we wanted to - lobbying is such a smaller industry than I intuitively expect reply rsynnott 22 hours agorootparent> organise a lobbying group greater than the NRA ($2.93M in 2022)This is... a little misleading. They also have at least two PACs (here&#x27;s the big one: https:&#x2F;&#x2F;www.opensecrets.org&#x2F;political-action-committees-pacs...)Now, you could claim, though not particularly credibly, that money spent on PACs is not lobbying money. reply antupis 21 hours agorootparentStill that is rather small like here Finland average organization uses 34 – 57 million euros to lobbying. source https:&#x2F;&#x2F;julkaisut.valtioneuvosto.fi&#x2F;bitstream&#x2F;handle&#x2F;10024&#x2F;1... reply pavlov 21 hours agorootparentYou’re misreading the study. It says they asked 115 people for a low and high range of organizational spend on lobbying, and the sum of the estimates was 34-57 million.So divide 57 by 115, and you get about 0.5 million euros on average for the high estimate.(Some of the respondents may work for the same organization which complicates calculating the average. Hence why they’re reporting the sums instead, I guess.) reply eddtries 21 hours agorootparentprevFair enough, I googled and used the first reputable source I could find. But sure, including the two other PACs, this is still IMO small change. I thought we were talking hundreds of millions or even billions regarding the NRA. reply jojobas 21 hours agorootparentprevSo, 10-30 million all up per election cycle, chump change for many venture investors for example.>Now, you could claim, though not particularly credibly, that money spent on PACs is not lobbying money.There are PACs and then there&#x27;s AIPAC with a 70M&#x2F;year budget. reply andrewinardeer 22 hours agorootparentprevThat money is just to employ lobbyists and their associated expenses.I&#x27;m sure there is additional money for lobbying that is not on the books. Holidays, houses, good bullion, crypto, free memberships to exclusive clubs and the like that is gifted to politician&#x27;s and their spouses by big industry. reply eddtries 22 hours agorootparentSo with regards to FTX these were listed in the same spreadsheet as \"Gifts\", and there were some, but we&#x27;re still talking low five figures for the majority of politicians. And it&#x27;s not like FTX were not willing to spend more, they spent $68M on Tom Brady and Giselle for a two year contract to advertise FTX, or $500k on hotel rooms for interns in Miami. reply justinclift 22 hours agorootparentIs that low five figures each? reply eddtries 21 hours agorootparentYeah, each! reply TacticalCoder 21 hours agorootparentprevDoctors do get weekends in fancy hotels and subscriptions offered by big pharmas for sure: I&#x27;ve got friends who get those. reply eddtries 21 hours agorootparentIt&#x27;s getting far more regulated though, and that&#x27;s a good thing - big Pharma pushing drugs to doctors is one of the most sickening parts of capitalism. reply vintermann 21 hours agorootparentprevIt&#x27;s about relationships - soft power - more than money. Speaking fees, revolving door&#x2F;sinecure jobs, and donations to your charities will be there for you should you need it, so there&#x27;s no need to ask for things up front. reply scrps 21 hours agorootparentprevAlso it has to do with wink wink nudge nudge \"Come see us if the election doesn&#x27;t work out.\"Knowing you are losing an election and getting dumped into a $350k&#x2F;yr easy-mode job takes the edge off. reply underseacables 21 hours agorootparentprevIn the movie, Thank You for Smoking, there is a great scene where the main actors are sitting in a restaurant, and there&#x27;s a sign just behind them that says:\"Be thankful for your government, it&#x27;s the best that money can buy\" reply Lutger 21 hours agorootparentprevHow sure can you be that your lobby is successful? Can you like &#x27;buy&#x27; a politician, effectively a legal bribe, or is it more nuanced?In the EU, a lot of laws are effectively written or co-authored by companies, which saves the politicians and their staffs incredible amounts of time and expertise. This must be quite expensive, just to have the lawyers on payroll who are able to do that. reply Eddy_Viscosity2 21 hours agorootparentIf your company is literally writing the laws that will govern them, then yeah I&#x27;d say the lobby was successful. reply conception 20 hours agorootparentprevI think there is a theme in this thread that misses a key aspect that I think Noam put well here -https:&#x2F;&#x2F;youtu.be&#x2F;1nBx-37c3c8?si=vA0IIew7ripABUTDI think the idea that if an opposing idea went to an NRA friend and offered more money, in almost all cases they would refuse because the lobbying dollars aren’t to convince but to support someone who asked believes what you do.In some cases, probably like crypto laws, a politician might think “sure I don’t care or have an opinion, you bought some donations and I don’t hate your opinion so I’ll help out”, but that is a lot different than “you pay me x and I vote y.”I think taking the agency out of the politicians hands, in most cases, is the wrong perspective. reply ajb 20 hours agorootparentprevIt&#x27;s a downpayment, though. Political careers are often short; subsequently the politician will get a high paying &#x27;job&#x27; at one of the firms funding the lobbying. That&#x27;s the real payoff. reply zo1 22 hours agorootparentprevI think we&#x27;re partially mistaken about the true value and reason for lobbying existing as a concept. It&#x27;s a cover that muddies the waters in discussion about laws, a coping mechanism for \"The Other Side\" and at the same time an endless stream of content for potential reporting by reporters&#x2F;media. reply CPLX 20 hours agorootparentprev1) You’re right it’s cheaper than people think.2) You aren’t going to move the needle much with 5 figures for anything at a national level.The real work comes when you hire the congressman’s favorite PR firm for X and their cousins polling firm for Y and their former chief of staff for $15k a month retainer and so on. The vast majority of this gets done via “consulting” agreements and public relations firms and law firms, where the nominal work is irrelevant and the relationships and introductions are the product. reply bernardlunn 22 hours agorootparentprevWow!! reply rippeltippel 22 hours agoparentprevI&#x27;m still waiting for the day when we&#x27;ll replace the word \"lobbying\" with \"corruption\". reply michaelteter 22 hours agorootparentThe better word is \"informing\". Lobbyists inform policymakers about perspectives of certain groups (those employing the lobbyists).It just happens that most lobbyists are paid by groups who are seeking to enrich themselves at the cost of everyone else.However, there are some lobbyists who work for organizations that attempt to guide policy that helps under-represented groups (like nature, animal welfare, human welfare). Those lobbyists are fewer and poorly paid (as their \"clients\" typically have little or no money), but they work hard to at least inform policymakers of their perspective. reply guerrilla 20 hours agorootparentNo, this is just more doublespeak that ignores what&#x27;s actuallygoing on. Corruption is the correct word. reply sofixa 20 hours agorootparentIn the EU, lobbyists need to get registered.Here&#x27;s a few examples of registered organisations that lobby the EU:* the Electronic Frontier Foundation https:&#x2F;&#x2F;ec.europa.eu&#x2F;transparencyregister&#x2F;public&#x2F;consultatio...* the Mozilla Foundation https:&#x2F;&#x2F;ec.europa.eu&#x2F;transparencyregister&#x2F;public&#x2F;consultatio...Do you think it&#x27;s corruption when they inform the EU of the risks of legislation (like eIDAS) and fight against potential loopholes (successfully in the case of eIDAS)? reply guerrilla 17 hours agorootparentI think in general it is corruption. That&#x27;s what 99% of lobbying is. Rare exceptions don&#x27;t change that. reply michaelteter 17 hours agorootparentprevAs my partner is an animal welfare lobbyist, I am qualified to say that not all lobbyists are corrupt or pedaling corruption. reply jacquesm 22 hours agorootparentprevAgreed, that is exactly what it is. It&#x27;s a huge stain on democracy and in many ways subverts it. Companies don&#x27;t have the right to vote and should not be able to do an end-run around the electoral process with their money. reply robertlagrant 21 hours agorootparent> Companies don&#x27;t have the right to vote and should not be able to do an end-run around the electoral process with their money.There&#x27;s no difference between companies and people here; no one, spending their company&#x27;s money or their own, should be able to influence solely through money. They only can because of corrupt state employees, who should be replaced. reply jacquesm 20 hours agorootparent> There&#x27;s no difference between companies and people here; no one, spending their company&#x27;s money or their own, should be able to influence solely through money.There is because individuals are not normally the clients of lobbyists, nor do they - normally - approach politicians directly with money in hand except for some countries where campaign donations are a thing. They shouldn&#x27;t be because they are effectively corruption but unsurprisingly countries where this practice is established never get around to abolishing it because it put the people who are in power in power in the first place.> They only can because of corrupt state employees, who should be replaced.If the state employees receive that money off the books then yes, but if it is structural it is not the employees that should be replaced but the system that should be replaced. And that is a much harder task. Because you could replace employees until the cows come home, if the system remains the same nothing will really change. reply robertlagrant 18 hours agorootparent> And that is a much harder task. Because you could replace employees until the cows come home, if the system remains the same nothing will really change.There are lots of things where we trust individuals to do a good job. If we can identify things as being the results of corruption via lobbying, why not fire them or prosecute them? reply jacquesm 18 hours agorootparent> If we can identify things as being the results of corruption via lobbying, why not fire them or prosecute them?That&#x27;s an excellent question. In many places &#x27;lobbying&#x27; is legal, technically it is supposedly to inform the clueless legislators about various interests. But in practice it very quickly turns into &#x27;soft&#x27; corruption, meetings in holiday resorts (oh, do bring your family) and so on. Whatever lines are drawn the amount of money available to get around them is practically infinite and politicians (and civil servants) are not all equally good at determining when they are targeted and might be across the line before they realize it (and then it gets much harder to go back than to have never crossed it before).Occasionally people are terminated, and occasionally there are prosecutions. But there is a very large amount of information about who may have been involved in corruption and only a limited amount of prosecution and investigatory power so the bulk of these cases will end up being ignored. reply robertlagrant 16 hours agorootparent> politicians (and civil servants) are not all equally good at determining when they are targeted and might be across the line before they realize it (and then it gets much harder to go back than to have never crossed it before).I don&#x27;t think many people would get such a pass. The whole point of paying them from money taken from people&#x27;s incomes is so they can be impartial. There&#x27;s no point having them if they don&#x27;t add value. reply jacquesm 15 hours agorootparent> I don&#x27;t think many people would get such a pass.You&#x27;d be surprised. Especially if the last review of the rules is a while ago or if they have been recently updated. People are sloppy, especially if they think nobody is looking and that it doesn&#x27;t matter.> The whole point of paying them from money taken from people&#x27;s incomes is so they can be impartial.That&#x27;s the theory, but as the US supreme court proves that doesn&#x27;t mean much.> There&#x27;s no point having them if they don&#x27;t add value.It&#x27;s never that black and white except for the most extreme cases and those are the ones that in the end usually do make it to prosecution. Also note that in the EU different member states have entirely different views on what constitutes corruption and normality. replyTerretta 22 hours agorootparentprevLobbyists are just your duly capitalized representatives to your duly elected representatives. reply andrekandre 10 hours agorootparentprevnext [–]> Companies don&#x27;t have the right to voteit seems they do...https:&#x2F;&#x2F;www.businessinsider.com&#x2F;some-cities-are-allowing-cor... reply jacquesm 10 hours agorootparentSome cities are more idiotic than others, apparently, but in case it wasn&#x27;t clear the context was general elections. Not that what you point out isn&#x27;t a travesty and should be dealt with before someone figures out that you can create as many companies as you want. reply rollcat 21 hours agorootparentprevOh it is called corruption - when you&#x27;re talking second or third world countries.Same with \"expats\" vs \"immigrants\". reply gruez 21 hours agorootparentThere&#x27;s an objective difference between high corruption and low corruption countries though. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Corruption_Perceptions_IndexLet&#x27;s not pretend that a slick lobbyist hired by vested interests to talk to representatives with no money changing hands is comparable to the stuff that goes down in high corruption countries, where there&#x27;s literally briefcases full of cash given to politicians. reply antonvs 21 hours agorootparentThey used to literally walk around the House floor handing out donor checks. From https:&#x2F;&#x2F;www.washingtonpost.com&#x2F;archive&#x2F;politics&#x2F;1997&#x2F;02&#x2F;14&#x2F;h... :> “…it was publicly disclosed that Boehner in the last week of June 1995 walked around the House floor delivering six or more of the Brown & Williamson Tobacco Corp. PAC checks.> “in the same week Boehner was giving out the checks on the House floor, the House Appropriations Committee met in its room in the Rayburn House Office Building and voted down (17 to 30) an amendment that would have ended the government&#x27;s price support program for tobacco. Seven Appropriations Committee members each had received a $500 check from Brown & Williamson&#x27;s PAC, including one for the committee chairman, Rep. Bob Livingston (R-La.).”This kind of activity seems to refute the “objective difference” you’re imagining. reply gruez 20 hours agorootparent1. You realize this thread is about EU?2. 500, even in 1995, is a token amount of money to the representatives. I highly doubt they changed their vote for that little money. reply rollcat 19 hours agorootparent> 500, even in 1995, is a token amount of money to the representatives.In the post-soviet countries (many of them now in EU), you can&#x27;t imagine how much you can \"get done\" by \"gifting\" the right person a bottle of their favourite poison - which costs a token amount by almost any standard (like, high school pocket money).My point is that these are two names for the same thing, an attempt to justify the \"rules for you but not for us\" on the moral spectrum. Microsoft can launder open source code with generative AI, but don&#x27;t you dare even look at their sources.Same shit, dressed pretty. reply Lutger 21 hours agorootparentprevYou are being sarcastic right?> The CPI measures perception of corruption due to the difficulty of measuring absolute levels of corruptionSo the difference consists primarily in perception, the slickness of it all as you put it? How well we can hide corruption with a facade of legality and civility. Somehow the crude briefcase full of cash feels more honest and direct. reply gruez 20 hours agorootparent\"a facade of legality and civility\" is literally the point of rule of law. It&#x27;s why if you detain someone it&#x27;s a crime, but if the police do it it&#x27;s normal law enforcement. reply piva00 21 hours agorootparentprev> Let&#x27;s not pretend that a slick lobbyist hired by vested interests to talk to representatives with no money changing hands is comparable to the stuff that goes down in high corruption countries, where there&#x27;s literally briefcases full of cash given to politicians.That&#x27;s just a make-up, parfumerie on top of the same exact concept: use money in some way to corrupt decision making.Just because in some countries it&#x27;s done with a veneer of legitimacy, in a way that doesn&#x27;t look as dirty and disgusting as \"those other over there with their dirty hands full of bags of money\", it&#x27;s just corruption with a façade of high-class. It&#x27;s still the same thing, just has more layers of indirection and make-up on top. reply zajio1am 21 hours agorootparentprevLobbying is just people or organizations making their case to politicians in a private way (e.g. meeting MP in their office) instead of a public way (e.g. putting an article to press). reply Lutger 21 hours agorootparentThat is what it is supposed to be. Offering a perspective someone said. In a michelin star restaurant, private resort full of prostitutes or after having donated substantial amounts of cash to their campaign ensuring their hold on power.Somehow it seems different when &#x27;presenting the case&#x27; is accompanied by gifts and money. reply mrmanner 19 hours agorootparent> after having donated substantial amounts of cash to their campaign ensuring their hold on powerThis is the main problem, and there is a solution. If campaigns are financed by enough public money, donations have less power and can be regulated more heavily.Similarly, if individual politicians get enough from the state to feel secure even if voted out, we take away the power of the promise of a next job and can enforce a grace period before working on anything related. reply 0dayz 20 hours agorootparentprevYou&#x27;re throwing the baby with the water here.Lobbying I&#x27;d argue is an essential part of democracy because it allows groups of people that have a shared concern to come together and make their case to the politicians. reply squarefoot 20 hours agorootparent> ... groups of people that have a shared concern to come together and make their case to the politicians.That would be solved by referenda, with their results and number of people affected being presented to the politicians so that they must deal with the problem. Lobbying as it is now in many countries (possibly all of them) has nothing to do with that and to me is just legalized corruption. reply mrmanner 19 hours agorootparentWho pays to let people know there’s a referendum they should vote in? reply xkcd1963 20 hours agorootparentprevBut that shouldn&#x27;t come down to the money. If only the people who amassed huge amounts of money can influence there is no democracy. reply 0dayz 7 hours agorootparentLobbying does not have to be done via donations.Otherwise I&#x27;m very curious how NGOs can lobby. reply waihtis 22 hours agoparentprevThe expert panel for this was truly a sight to behold, with majority representation coming from outside the EU (mostly american, but also canadian, new zealand and australian representation): https:&#x2F;&#x2F;twitter.com&#x2F;SimoKohonen&#x2F;status&#x2F;1722635234116506052 reply pas 22 hours agorootparent> https:&#x2F;&#x2F;www.patrick-breyer.de&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;02&#x2F;SKM...page 26 (or page 24 in in-document numbering) ... amazing that Thorn was just an \"NGO\" .. instead of the company that wants to sell filtering tech.so, can we get the actual expert opinion of these wonderful folk? reply eigenket 22 hours agorootparentprevI count 24 experts from 5 eyes countries and 8 from non 5 eyes countries. Not sure if thats relevant but it is funny. reply waihtis 22 hours agorootparentI just think you might be onto something reply api 21 hours agorootparentprevLooks like some of the AI doom regulatory capture panels I’ve seen. reply waihtis 21 hours agorootparentOh yes. Strong industry lobby groups from the US, a handful of useful idiots from the EU. Amazing power balance. reply ajsnigrutin 22 hours agoparentprev> thing this timeI&#x27;m sure it&#x27;ll get reintroduced in a year or so, and if it doesn&#x27;t get enough media attention, it&#x27;ll pass.They only have to win once, we have to win every goddamn time. reply eigenket 22 hours agorootparentThe EU also has a right to privacy enshrined via the Treaty of Lisbon, so even if this law had got through the parliament somehow it should&#x2F;would have been stuck down by the courts. reply Run_DOS_Run 19 hours agoparentprevThorn and all its supporters should be branded for what they are: Enemies of the free world. Enemies of democracy and fighters for oppression and dystopian police states.If you look at the developments in Hungary and Poland (or the polls in Austria), any form of surveillance will be just used as another vehicle to keep autocrats and would-be dictators in power.I doubt that the election in Poland in 2023 would have turned out like this if the PiS had seamless protocols of the opposition&#x27;s communication. reply PoignardAzur 22 hours agoparentprevContrary to popular opinion, lobbying isn&#x27;t as simple as \"pay for expensive restaurant, get votes\".If legislators feel strongly ideologically about an issue, no amount of lobbying will make them vote the other way. reply eddtries 22 hours agorootparentHow do you explain big Pharma lobbying to stop reducing drug prices? For example, Democrats who held up drug pricing reforms were the largest beneficiaries of lobbyist money.https:&#x2F;&#x2F;www.reuters.com&#x2F;business&#x2F;healthcare-pharmaceuticals&#x2F;... reply martin8412 22 hours agorootparentThat&#x27;s a US problem. The prices of all medicines in for example Denmark are publicly available. The medicine is purchased by a single entity(and there are proposals to combine the entities within the Nordic countries to form a single large purchaser), so to sell medicine in Denmark you&#x27;ll have to play ball. The prices are therefore quite reasonable. Insulin which is a commonly used example of overpriced medicine in the US is around 40-50 USD in Denmark. That is the brand version from Novo Nordisk.The prices are available at https:&#x2F;&#x2F;www.medicinpriser.dk&#x2F;?lng=2 reply eigenket 22 hours agorootparentprevThe short answer is that we&#x27;re talking about the EU. Thats a US thing.As far as I can tell looking from the outside the US system is set up so that if you aren&#x27;t ridiculously wealthy yourself you pretty much need lobbyist money to fund your campaign to get elected. reply sofixa 20 hours agorootparent> As far as I can tell looking from the outside the US system is set up so that if you aren&#x27;t ridiculously wealthy yourself you pretty much need lobbyist money to fund your campaign to get elected.Considering their whole country started as a place where only wealthy male landowners could vote, and they literally deify some of those wealthy male land and slave owners, as well as their written works, it&#x27;s no surprise really. reply eddtries 22 hours agorootparentprevThat&#x27;s probably true, there is lobbying in the UK so I assume countries within Europe have similar issues but I don&#x27;t truly know. reply qwytw 22 hours agorootparentGenerally political advertisements, campaign spending and especially donations are strongly limited. E.g. where I am a only private individuals can only donate and only up to 10k per year. The downside of that is that most funding parties receive comes from the state and it&#x27;s based on their previous election performance (which is problematic due to very obvious reasons...).Also if everyone can give&#x2F;spend as much as they want legally it at least stays semi transparent so in theory voters can base their decisions on that. Illegal bribes, kickbacks etc. are a bit harder to track. reply drstewart 21 hours agorootparentprev>As far as I can tell looking from the outsideOut of curiosity, how accurately do you feel are the opinions of outsiders with no experience living in your country who comment on your country&#x27;s state of affairs? reply Timwi 21 hours agorootparentWell, on the one hand of course you could argue that outsiders are less experienced with living in your country.But outsiders have no emotional stakes in convincing themselves that your country is the best on earth. Consequently, I feel the non-US media are freer than US mass media to discuss the true state of American healthcare. Outsiders are less susceptible to your country&#x27;s patriotic propaganda.This does go both (or all) ways. The US are an example here, not a singular special case. Every country tries to convince its citizens that it&#x27;s better than everywhere else. reply eigenket 21 hours agorootparentprevI have previously lived in the USA, I&#x27;m not sure why you assumed I have no experience of that.In general I would expect someone who speaks the language of the country I live in now at roughly the same level I speak English to have a pretty accurate understanding of what is going on here. It would, frankly, be quite weird to speak the language and have no idea of what is happening here. Of course America is a special case because of how the internet is dominated by US media, commentary and content. reply Lutger 21 hours agorootparentprevSometimes their perspectives could be more accurate as they don&#x27;t have the same biases. Or maybe its better to say understanding and including their perspectives contributes to a more accurate understanding. reply rsynnott 21 hours agorootparentprevLobbying is likely easier in a polarised two-party system where you really only have to swing a few people on the edges to deadlock things. The European Parliament has 6 large parties (>50 seats), and they&#x27;re not particularly cohesive, so cases where the pre-ordained decision can be swayed by getting to this six people would be much rarer. reply Lutger 21 hours agorootparentThat is likely true, though there is still extensive lobbying going on in the EU and within EU countries. It is just done in a different way, with different mechanics. reply zakary 22 hours agorootparentprevEveryone has a price, and drug companies can usually find a way to meet it for even the most steadfast reply JoshTriplett 22 hours agorootparent> Everyone has a priceSome people really do have principles they won&#x27;t violate. We just don&#x27;t get enough people in politics who don&#x27;t have a price. reply verisimi 22 hours agorootparentNot politicians. Not many normal people either. And that presumes that most people even know what &#x27;principles&#x27; are - which they do not. reply eddtries 22 hours agorootparentprevYeah I agree, I was responding to> If legislators feel strongly ideologically about an issue, no amount of lobbying will make them vote the other way. reply madeofpalk 22 hours agorootparentprevThey don&#x27;t feel strongly ideologically about an issue reply eddtries 22 hours agorootparentYou might be right, but having politicians who are not ideologically pro-affordable healthcare is a really sad state of affairs reply eigenket 22 hours agorootparentWell one of these guys is Bob Menendez, who is currently indicted on his second federal corruption charges and who happily accepts cash and gold bars from Egyptian nationals. reply qwytw 22 hours agorootparentprevSome do, unfortunately oftentimes it&#x27;s the crazy ones that feel the strongest about things. reply nonameiguess 22 hours agorootparentprevI think the explanation is these legislators are not the ones who feel ideologically strong about cutting drug prices. Of the three people listed in this article, Kyrsten Sinema is no longer a Democrat and Bob Menendez is currently under indicment on federal corruption charges, which is now the second time for him. reply nottheengineer 22 hours agorootparentprevIt&#x27;s a numbers game. You can&#x27;t convince everyone, but lobbyists are generally good at their job and know how to find people with malleable opinions.I don&#x27;t think there&#x27;s a topic where lobbying doesn&#x27;t work to at least some degree. Or do you have an example? reply PoignardAzur 20 hours agorootparent> I don&#x27;t think there&#x27;s a topic where lobbying doesn&#x27;t work to at least some degree.I wasn&#x27;t saying lobbying doesn&#x27;t work at all. reply collyw 22 hours agorootparentprevSure. reply aremat 20 hours agoparentprevIt&#x27;s not just lobbying. There are entire industries built around the revolving doors of think tanks, NGOs, boards, advisories, \"research\" institutes, etc. reply victorbjorklund 20 hours agoparentprevOr did they just think there was too much attention and they will instead approve something else under a new name in the future? reply wodenokoto 21 hours agoparentprevELI5, why does google want regulation that tells them to scan user content for various suspicious activities? reply waihtis 21 hours agorootparentGoogle has well enough resources to handle such an effort whereas a newer, competing entity will likely not. (\"entry barriers\") reply moffkalast 22 hours agoparentprevThe EU parliament usually does the right thing if you look at past votes, there was never any doubt.Well maybe a tiny bit of doubt. reply Ekaros 22 hours agoprevMakes lot of sense. In some EU countries like Finland the privacy of communication is a CONSTITUTIONAL right. So you need good reason to break it. And I don&#x27;t think generic muh terrorism passes the bar. reply eigenket 22 hours agoparentEven more than that, its article 7 of the European Charter of Fundamental Rights so its baked into the foundation of the EU itself via the Treaty of Lisbon (which is why its unsurprising this law failed to pass the EU parliament). Even if somehow this law did get passed the parliament the EU&#x27;s courts should have struck it down. reply theonlybutlet 22 hours agoprevI&#x27;d love to know who was in favour of the new better proposal and who was in favour of the old proposal. A clear insight into their values. It&#x27;s EU parliamentary elections next year, I&#x27;d nearly argue those supporting this new better proposal would be a good voting choice next year and to steer clear of those in favour of the old proposal. reply layer8 22 hours agoparentEU Parliament votes are published, so you&#x27;ll be able to look it up. reply qwytw 22 hours agoparentprevUnfortunately only a tiny proportion of population in the EU keep track of what their MEPs are actually doing and most vote more or less the same way they&#x27;d vote in national elections (where I am it&#x27;s more like an opinion poll preceding the &#x27;real&#x27; elections which always happen ~6 months after the European ones). reply TacticalCoder 21 hours agoprevPrivate messages won&#x27;t be scanned for now, but what about the certificates in web browsers that could be swapped at will by any certificate in the control of some EU apparel so that \"encrypted\" web traffic could be sniffed and MITMed?Which moreover came with a fineprint specifying that it&#x27;d be illegal for browsers to warn users about certificate being swapped?Is that out of the window for now too? reply sofixa 19 hours agoparent> Private messages won&#x27;t be scanned for now, but what about the certificates in web browsers that could be swapped at will by any certificate in the control of some EU apparel so that \"encrypted\" web traffic could be sniffed and MITMed?That was a (probably) unintended consequence of the eIDAS legislation, where specific Certificate Authorities must be trusted by browsers to enable digital certificates and signing to work EU-wide. This has since been corrected and the legislation explicitly states that those CAs and the regular CAs can and should be kept separate, thus MITM won&#x27;t be possible unless the browser chooses to mix things. reply waihtis 22 hours agoprevI&#x27;ve become so jaded by the EU that was expecting the worst from this, so kudosLet&#x27;s see how quickly it&#x27;ll resurface again reply sebstefan 22 hours agoparentI was jaded by this proposal as well, but jaded by the EU in general?GDPR, forced interoperability from gatekeepers, the 2 year warranty on anything bought onlineThis attempt at breaking encryption completely stood out with the usual thingsThe EU seems like the only governmental organization that&#x27;s working well to improve my life, in my country. Everything else is either decaying or opposing my values. reply waihtis 1 hour agorootparentJust one example:https:&#x2F;&#x2F;last-chance-for-eidas.org&#x2F;update-151123 reply Vespasian 21 hours agorootparentprevSpecifically in the digital space the DMA and DSA try (and will probably at least partially succeed) to break the plattform feudal rule and move their role more towards that of a public utility by restricting how they can use their market power.That&#x27;ll have a massive (I think positive) effect on the digital economy. reply xsdu 22 hours agorootparentprevI&#x27;m still bitter about their half-baked cookie law that instantly made web browsing a much worse experience, regardless of how well-intentioned it may have been. reply yreg 22 hours agoprevHopefully, this settles it once and for all in the EU, similar to the Net Neutrality regulation from 2015. reply jacquesm 22 hours agoparentUnlikely. What will happen is that instead of one big push to the EU they will try again on the local level. That&#x27;s how companies abuse the EU all the time. First try to lobby the EU itself for a one-stop-shop approach, and if that fails they go after the member states. reply qwytw 22 hours agorootparent> will try again on the local levelThat will be a lot harder though cause way more people actually pay attention to what their governates are doing compared to the EU (assuming of course a significant proportion of the population actually cares and opposes stuff like this). reply jacquesm 22 hours agorootparentYes, true, it is harder and will cost more and it will take more time. But I&#x27;m pretty sure that they&#x27;ll try. You can expect the same for the DSA, which is a major thorn in the side of the advertising industry. reply yreg 22 hours agorootparentprevDo you have examples? Ideally related to digital things or privacy.(Thanks) reply jacquesm 22 hours agorootparentPrivacy is an excellent example. At first it was locally dealt with, and big tech would lobby the individual countries for all kinds of exceptions and ways to effectively get regulatory capture. Then, in 1995 the EU created the DPD.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_Protection_DirectiveBig tech and the advertising industry responded by sidestepping the new regulation to make deals with the individual countries to undermine it, and actually managed to get close to getting their way, and possibly to get it repealed completely.https:&#x2F;&#x2F;www.nu.nl&#x2F;internet&#x2F;2849758&#x2F;druk-van-lobby-opstelten-...But the EU responded with the GDPR.And now big tech is again lobbying and pressuring member states for exemptions, for example, Facebook:https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2019&#x2F;mar&#x2F;02&#x2F;facebook-...This is just venue shopping, can&#x27;t get what you want in one place then you just fragment it and try to get multiple smaller deals. So I totally expect a similar thing to happen around this subject, the stakes are just too high for them to ignore the whole EU for their games. reply yreg 21 hours agorootparentThanks. Sounds like living in one of the smaller member states is an advantage in this case, since it&#x27;s probably less worth it to lobby there. reply SiempreViernes 20 hours agorootparentWell, the smaller states are also easier to pay off, especially if they have already adopted the strategy of being the least regulated place in the EU.Though admittedly the example Cyprus indicates that the strategy could be to simply quietly ignore the law when it comes to foreign interests. reply jacquesm 19 hours agorootparentprevThe DSA is another one that I totally expect to get this treatment. replyschmudde 21 hours agoprevPrefect combination with Article 45 (https:&#x2F;&#x2F;www.eff.org&#x2F;deeplinks&#x2F;2023&#x2F;11&#x2F;article-45-will-roll-b...). You can roam around EU versions of websites that rely on government own certificate authorities and allow you to log in with your eIDAS id - which will be a requirement for all \"very large online platforms.\"The EU Parliament giveth and taketh away. reply gnfargbl 21 hours agoprev> In contrast to the original chat control proposal, the version of the EU Parliament wants that a new EU Child Protection Centre proactively searches publicly accessible parts of the internet for child sexual abuse material with automatic crawling, which can also take place in darknet and would be much more efficient than private surveillance measures by providers. Found abuse material must be reported and taken down by the provider.This is a good start, if it is sufficiently well-funded and appropriately staffed.I hope that they crawl much more than the public \"clearnet\" and \"darknet\", since a lot of media is shared inside the various walled gardens that make up the internet here in the &#x27;20s. reply sva_ 21 hours agoprevI&#x27;m very relieved by this. It is shocking that there was even the possibility of such a law passing, because it would&#x27;ve turned a lot of people, including myself, absolutely against the EU. reply greatgib 20 hours agoprevWhat really blow my mind is that now we need so much energy and be that thankful to the EU for something that should be so basic and obvious.When you compare how scandalous and impossibly excessive was looking the story of \"1984\" a few dozen years ago and that now it is the new normal. In a lot of countries, even democratic ones, we are already far worse than what was described in the book. But very little persons are shocked about that... reply sixhobbits 22 hours agoprevWhat&#x27;s the closest thing to EFF for these kinds of things in Europe? I always feel like there&#x27;s a huge difference in quality of reporting where EFF produces some of the best content I&#x27;ve ever read on any topic, and stuff about EU privacy is often a lot harder to follow.I know about statewatch and some individuals I follow who do a pretty good job, but feels like there is a gap for an organization to step and replicate what EFF does in the US.I would happily support with money and time. reply schmudde 21 hours agoparentYou might try Brave New Europe? They don&#x27;t look at tech specifically like EFF, but tech touches their stories on economics, regulation, and the media: https:&#x2F;&#x2F;braveneweurope.com&#x2F;?s=tech reply skowalak 20 hours agoparentprevMaybe EDRi, which also acts as an umbrella for smaller and local orgs.https:&#x2F;&#x2F;edri.org&#x2F;about-us&#x2F;who-we-are&#x2F; reply rkangel 22 hours agoprevThis website is a throwback to a design choice that (thankfully) has mostly died off - choosing a mid-grey for your text, making it much more effort to read.I used to have Stylebot pinned to my extensions to fix it, but haven&#x27;t had to do it in ages. Designers - please don&#x27;t do this.(I think it comes from people designing on much higher contrast Apple monitors and not testing on anything else) reply yreg 22 hours agoparentIronically, your own (downvoted) comment on this site is even more difficult to read.But you are right, the Tuta article has a contrast ratio of 4.35:1, which doesn&#x27;t pass even the lower WCAG level (AA = 4.5:1).Accessibility is important. We are all going to depend on it if we live long enough. reply ale42 22 hours agoparentprevIndeed it looks like all the text is grayed out... like \"I&#x27;m waiting for you to click that damn accept-cookie button before actually showing the page\". reply Twisell 21 hours agoprevMore in depth analysis from a NGO lobbying for online privacy: https:&#x2F;&#x2F;edri.org&#x2F;our-work&#x2F;eu-parliament-committee-rejects-ma... reply codeptualize 21 hours agoprevThis is exactly as I expected it would end. We&#x27;ll see this happen again, and again, and again.Some politician gets the genius idea to have backdoors in encryption, initial support, then reality sets in and the plans are abandoned.There is just no sensible way to implement this, therefore it&#x27;s not going to happen.This iteration did go a bit further than usual. reply zaxomi 21 hours agoprevWow, EU Parliament did the right thing this time.When I heard an interview with a Swedish EU politician, I thought it was a lost cause. She was completely blinded by the possibilities and saw no downside whatsoever. reply omgmajk 20 hours agoparentI guess you are talking about Ylva Johansson, who is the spearhead of this. Every time she opens her mouth on this subject any person with knowledge on this subject breaks on the inside a little more, she has no idea what she is trying to do. Or maybe she does and it&#x27;s all a smokescreen, I don&#x27;t really know. reply NoboruWataya 21 hours agoprevSurely not - I only ever hear that the Parliament is toothless body that rubber stamps whatever the Commission proposes. reply pierrelf 22 hours agoprevHuge privacy W, I was really worried for a while there. Wonder what effect the emails and protests had! reply kypro 21 hours agoprevPerhaps worth noting that the only branch of EU which is directly elected is the parliament. reply DocTomoe 21 hours agoprev... unless it is them who does the scanning.The \"Mandatory government-issue SSL certificate\" is still on the table. reply tasubotadas 21 hours agoprevThank you reply throwaway2990 22 hours agoprevWhatsApp will claim its end to end encryption and can’t scan the messages when they can and do. reply rkangel 22 hours agoparentCitation? reply throwaway2990 21 hours agorootparentNo citation needed. If the statement was false we wouldn’t get advertising based on conversations that happen in what’s app. reply rkangel 19 hours agorootparentI&#x27;ve heard this claim many times, and have yet to see anything to substantiate it.I&#x27;m not saying I don&#x27;t believe it - I have zero trust in Meta, and use Matrix and Signal with everyone that I can. But I would like evidence of foul play before making specific claims. reply _zoltan_ 20 hours agorootparentprevwe don&#x27;t? and meta refuted even planning it. reply qxfys 22 hours agoprevthe fact that this thing needs to be discussed in the parliament in the first place is an absolute waste of time. reply AlecSchueler 22 hours agoprevThe EU needs to stop it with all this heavy handed regulation. At the end of the day it only hurts businesses. Businesses rely on the data contained within these communications in order to improve their services. This is why US tech companies are growing while EU equivalents are not.It&#x27;s all well and good to consider user privacy and user safety but not when it stifles the market.[Please note that this is a satirical comment based on some of the arguments I&#x27;ve seen here in the past] reply pelorat 21 hours agoparentThose comments are not entirely wrong. Regulation does hurt, but the main issue is that even though the EU is a single market in theory, it&#x27;s made up of nations that each have their own market and speak their own language. Not a lot of new tech companies target the entire EU, they mostly target their domestic market and certainly not the USA.In the USA it&#x27;s easy for a new tech company to put out a commercial to target the entire US population of more than 300 million people. This is practically impossible in the EU. The market here is actually very fractured.We have \"big\" tech companies in each EU nation, but they cater only to the domestic market. \"Big\" as in they are dominant in their field inside their nation.Take online payment systems for instance. While there are global EU companies like Klarna, most EU nations has their own system that everyone uses. So while you usually have a bunch of payment options to chose from, 99% picks the national one (usually no processing fees).This also applies to a bunch of other apps in the EU.If you create an app or a service in the EU and you want it to succeed, you need to target your domestic market first. However chances are there&#x27;s already an app or service for your idea and you&#x27;ll have zero chance to compete on the international market, even if you translate your service to as many languages you can think of. reply sofixa 19 hours agorootparentAnd? I fail to see how having a big EU-wide tech company is more important than good regulations that protect EU citizens and give them rights big tech companies don&#x27;t want them to have (e.g. privacy, data portability, right to be forgotten, etc.) reply zabzonk 21 hours agoparentprev> while EU equivalents are not.doh, ARM?edit: please add \"edit\" when you edit reply d3w4s9 22 hours agoparentprevIs that you Zuckerberg? reply matt_j 22 hours agoparentprevSure, Satan. reply asimpletune 22 hours agoprevNext step would be OS maintainers to make APIs for providing e2e encryption functionality and then app stores requiring these apis to be used for private messaging. reply bradley13 22 hours agoparentBecause you trust closeed-source from Microsoft, Google and Apple? Seriously?Open standards, open-source code. That&#x27;s really the only option for code people are supposed to trust. reply asimpletune 22 hours agorootparentAll security is built on trust. If your threat model is trust nothing, then the solution is do nothing. What I’m talking about is called anchoring, where you force a critical flow through a single anchor by design, and thus reduce the places that you have to audit. It’s the same reason they say that all security should be baked in the keys (strength, mgmt, exchange, etc…).Do I trust Apple and Microsoft? I think sort of.I don’t trust them to be perfect, but if your prior is to say that you don’t trust them at all, then it means you basically can’t use them at all bc no amount of security will get around an untrustworthy OS.They control what gets displayed on screen, they control how memory is laid out and accessed for a program. There are already so many more important things we entrust to them. So, yeah, I prefer OS’s (all vendors) to provide APIs, and for app stores to enforce their use. I especially would trust this more than EU laws, and I certainly would trust that more than everyone doing their own thing, regardless if it’s open source.If for no other better reason I trust the OS more, since all of these open solutions will still run on those supposedly untrustworthy os vendors.You basically have to trust your OS, Don’t you think? Otherwise, the answer is you do nothing. reply qwytw 22 hours agorootparentprevWhy wouldn&#x27;t you trust them? To some extent at least?I mean if they are claiming their messaging system is E2E and it turns out it isn&#x27;t the cost to them (not only financial) would be much higher than whatever they earn from having access to your data. reply Lio 21 hours agorootparent> Why wouldn&#x27;t you trust them?Because historically we&#x27;ve already caught these companies doing dubious things and they&#x27;re still in business, still making money hand over fist.For example, Facebook saying they would only use phone numbers for two-factor authentication and just ignoring that because it was profitable.Or Biobank saying they wouldn&#x27;t share private medical data with insurance companies and then just ignoring that because it was profitable.Or Microsoft and its subsidiaries bribing foreign officials to gain sales and block use of FOSS alternatives because it was profitable.The naughty list is long and it doesn&#x27;t seem to cause much reputational damage if any. reply qwytw 21 hours agorootparent> because it was profitableYeah I agree that&#x27;s the core issue. I&#x27;d only \"trust\" them because I don&#x27;t see how promising E2E and then breaking it on a widescale would be profitable for them replycontrarian1234 22 hours agoprev [–] Isn&#x27;t this is already a solved problem? This feels like a huge loss for player like Signal that actually ensure privacy by design reply yreg 22 hours agoparentThis decision is about abandoning a plan to make end-to-end encryption illegal. reply orian 22 hours agoparentprevIt&#x27;s not that simple. If the country introduces a legislation, every company operating in it&#x27;s border must comply.If EU would introduce such legislation, it could potentially make software doing end2end encryption illegal. In such case, google would be removing it from EU Play Stores, and this would be more&#x2F;less end of such messaging apps unless they comply. :-)This is why it&#x27;s important to have a reasonable legislature and laws. reply ta1243 22 hours agorootparent> This is why it&#x27;s important to have a reasonable legislature and laws.And multiple points of control. In the EU the council acts as a check on populism through parliament (even if the nazis took over parliament it wouldn&#x27;t give them a lot of power), parliament acts as a check on the council (even if the heads of 70% of EU countries decided something, parliament gets its say). Neither of those are the executive so they would be unable to push through laws which favour specific countries or groups of countries (as the commission are supposed to act primarily on behalf of the union, in the same way the US president is supposed to not favour his home state). Then outside of government you have the judiciary who look at the laws passed and interpret them in line with other laws, throwing out ones which are incompatible. reply contrarian1234 22 hours agorootparentprevIt&#x27;d give people all the more incentive to sideload and get off Google Play Store. That&#x27;s a win win in my book. And Signal is a non-for-profit. I don&#x27;t see why they&#x27;d care. Designing system that subvert authoritarian regimes.. what&#x27;s more punk than thatIf you have to ask governments for permission then that&#x27;s a bad design and your system will be taken away from you when the next think-of-the-children populists are electedIf you work on the assumption all governments are eternally nice and well interventioned .. then E2E chat systems aren&#x27;t all that necessary in the first place reply mattmanser 22 hours agoparentprevSignal would have had to introduce a back door if they wanted to continue operating legally in the EU. reply moffkalast 22 hours agoparentprev [–] If this were to pass, Signal would become illegal and banned in the EU. reply rsynnott 21 hours agorootparent [–] Well, that&#x27;s possibly simplistic, because it&#x27;s unlikely this would have held up in the ECJ in any case. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Civil Liberties Committee has approved new measures to protect children from online child sexual abuse.",
      "Internet providers would be required to assess the risk of their services being misused and take steps to prevent child sexual abuse.",
      "If prevention measures fail, court-validated detection orders could be used to locate illegal material, and an EU Centre for Child Protection would be established to enforce the rules."
    ],
    "commentSummary": [
      "The EU Parliament Civil Liberties Committee has removed chat control requirements and protected encryption in the proposed Chat Control regulation, but negotiations for finalization are still pending.",
      "Concerns about the potential misuse of scanning messages for child sexual abuse materials have been raised.",
      "The rejection of backdoors in encryption and scanning of private messages by the EU Parliament is seen as a victory for privacy advocates, but tech companies may lobby for exemptions."
    ],
    "points": 553,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1699957055
  },
  {
    "id": 38264641,
    "title": "Google's GraphCast AI Model Revolutionizes Weather Forecasting, Outperforms Traditional Methods",
    "originLink": "https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/",
    "originBody": "Research GraphCast: AI model for faster and more accurate global weather forecasting Published 14 November 2023 Authors Remi Lam on behalf of the GraphCast team Share Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute The weather affects us all, in ways big and small. It can dictate how we dress in the morning, provide us with green energy and, in the worst cases, create storms that can devastate communities. In a world of increasingly extreme weather, fast and accurate forecasts have never been more important. In a paper published in Science, we introduce GraphCast, a state-of-the-art AI model able to make medium-range weather forecasts with unprecedented accuracy. GraphCast predicts weather conditions up to 10 days in advance more accurately and much faster than the industry gold-standard weather simulation system – the High Resolution Forecast (HRES), produced by the European Centre for Medium-Range Weather Forecasts (ECMWF). GraphCast can also offer earlier warnings of extreme weather events. It can predict the tracks of cyclones with great accuracy further into the future, identifies atmospheric rivers associated with flood risk, and predicts the onset of extreme temperatures. This ability has the potential to save lives through greater preparedness. GraphCast takes a significant step forward in AI for weather prediction, offering more accurate and efficient forecasts, and opening paths to support decision-making critical to the needs of our industries and societies. And, by open sourcing the model code for GraphCast, we are enabling scientists and forecasters around the world to benefit billions of people in their everyday lives. GraphCast is already being used by weather agencies, including ECMWF, which is running a live experiment of our model’s forecasts on its website. Watch A selection of GraphCast’s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed. The challenge of global weather forecasting Weather prediction is one of the oldest and most challenging–scientific endeavours. Medium range predictions are important to support key decision-making across sectors, from renewable energy to event logistics, but are difficult to do accurately and efficiently. Forecasts typically rely on Numerical Weather Prediction (NWP), which begins with carefully defined physics equations, which are then translated into computer algorithms run on supercomputers. While this traditional approach has been a triumph of science and engineering, designing the equations and algorithms is time-consuming and requires deep expertise, as well as costly compute resources to make accurate predictions. Deep learning offers a different approach: using data instead of physical equations to create a weather forecast system. GraphCast is trained on decades of historical weather data to learn a model of the cause and effect relationships that govern how Earth’s weather evolves, from the present into the future. Crucially, GraphCast and traditional approaches go hand-in-hand: we trained GraphCast on four decades of weather reanalysis data, from the ECMWF’s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to ‘fill in the blanks’ where the observations are incomplete, to reconstruct a rich record of global historical weather. GraphCast: An AI model for weather prediction GraphCast is a weather forecasting system based on machine learning and Graph Neural Networks (GNNs), which are a particularly useful architecture for processing spatially structured data. GraphCast makes forecasts at the high resolution of 0.25 degrees longitude/latitude (28km x 28km at the equator). That’s more than a million grid points covering the entire Earth’s surface. At each grid point the model predicts five Earth-surface variables – including temperature, wind speed and direction, and mean sea-level pressure – and six atmospheric variables at each of 37 levels of altitude, including specific humidity, wind speed and direction, and temperature. While GraphCast’s training was computationally intensive, the resulting forecasting model is highly efficient. Making 10-day forecasts with GraphCast takes less than a minute on a single Google TPU v4 machine. For comparison, a 10-day forecast using a conventional approach, such as HRES, can take hours of computation in a supercomputer with hundreds of machines. In a comprehensive performance evaluation against the gold-standard deterministic system, HRES, GraphCast provided more accurate predictions on more than 90% of 1380 test variables and forecast lead times (see our Science paper for details). When we limited the evaluation to the troposphere, the 6-20 kilometer high region of the atmosphere nearest to Earth’s surface where accurate forecasting is most important, our model outperformed HRES on 99.7% of the test variables for future weather. For inputs, GraphCast requires just two sets of data: the state of the weather 6 hours ago, and the current state of the weather. The model then predicts the weather 6 hours in the future. This process can then be rolled forward in 6-hour increments to provide state-of-the-art forecasts up to 10 days in advance. Better warnings for extreme weather events Our analyses revealed that GraphCast can also identify severe weather events earlier than traditional forecasting models, despite not having been trained to look for them. This is a prime example of how GraphCast could help with preparedness to save lives and reduce the impact of storms and extreme weather on communities. By applying a simple cyclone tracker directly onto GraphCast forecasts, we could predict cyclone movement more accurately than the HRES model. In September, a live version of our publicly available GraphCast model, deployed on the ECMWF website, accurately predicted about nine days in advance that Hurricane Lee would make landfall in Nova Scotia. By contrast, traditional forecasts had greater variability in where and when landfall would occur, and only locked in on Nova Scotia about six days in advance. GraphCast can also characterize atmospheric rivers – narrow regions of the atmosphere that transfer most of the water vapour outside of the tropics. The intensity of an atmospheric river can indicate whether it will bring beneficial rain or a flood-inducing deluge. GraphCast forecasts can help characterize atmospheric rivers, which could help planning emergency responses together with AI models to forecast floods. Finally, predicting extreme temperatures is of growing importance in our warming world. GraphCast can characterize when the heat is set to rise above the historical top temperatures for any given location on Earth. This is particularly useful in anticipating heat waves, disruptive and dangerous events that are becoming increasingly common. Severe-event prediction - how GraphCast and HRES compare. Left: Cyclone tracking performances. As the lead time for predicting cyclone movements grows, GraphCast maintains greater accuracy than HRES. Right: Atmospheric river prediction. GraphCast’s prediction errors are markedly lower than HRES’s for the entirety of their 10-day predictions The future of AI for weather GraphCast is now the most accurate 10-day global weather forecasting system in the world, and can predict extreme weather events further into the future than was previously possible. As the weather patterns evolve in a changing climate, GraphCast will evolve and improve as higher quality data becomes available. To make AI-powered weather forecasting more accessible, we’ve open sourced our model’s code. ECMWF is already experimenting with GraphCast’s 10-day forecasts and we’re excited to see the possibilities it unlocks for researchers – from tailoring the model for particular weather phenomena to optimizing it for different parts of the world. GraphCast joins other state-of-the-art weather prediction systems from Google DeepMind and Google Research, including a regional Nowcasting model that produces forecasts up to 90 minutes ahead, and MetNet-3, a regional weather forecasting model already in operation across the US and Europe that produces more accurate 24-hour forecasts than any other system. Pioneering the use of AI in weather forecasting will benefit billions of people in their everyday lives. But our wider research is not just about anticipating weather – it’s about understanding the broader patterns of our climate. By developing new tools and accelerating research, we hope AI can empower the global community to tackle our greatest environmental challenges. Learn more about GraphCast Read our paper in Science Read an open access copy of our paper* Access GraphCast on Github View GraphCast live on ECMWF Related posts View all posts Research Nowcasting the next hour of rain Our lives are dependent on the weather. At any moment in the UK, according to one study, one third of the country has talked about the weather in the past hour, reflecting the importance of... Company Using AI to fight climate change AI is a powerful technology that will transform our future, so how can we best apply it to help combat climate change and find sustainable solutions?",
    "commentLink": "https://news.ycombinator.com/item?id=38264641",
    "commentBody": "GraphCast: AI model for weather forecastingHacker NewspastloginGraphCast: AI model for weather forecasting (deepmind.google) 527 points by bretthoerner 18 hours ago| hidepastfavorite166 comments meteo-jeff 16 hours agoIn case someone is looking for historical weather data for ML training and prediction, I created an open-source weather API which continuously archives weather data.Using past and forecast data from multiple numerical weather models can be combined using ML to achieve better forecast skill than any individual model. Because each model is physically bound, the resulting ML model should be stable.See: https:&#x2F;&#x2F;open-meteo.com reply brna 55 minutes agoparentHi Jeff, Great work, Respect!I just hit the daily limit on the second request at https:&#x2F;&#x2F;climate-api.open-meteo.com&#x2F;v1&#x2F;climateI see the limit for non-commercial use should be \"less than 10.000 daily API calls\". Technically 2 is less than 10.000, I know, but still I decided to drop you a comment. :) reply Fatnino 12 hours agoparentprevIs there somewhere to see historical forecasts?So not \"the weather on 25 December 2022 was such and such\" but rather \"on 20 December 2022 the forecast for 25 December 2022 was such and such\" reply meteo-jeff 11 hours agorootparentNot yet, but I am working towards it: https:&#x2F;&#x2F;github.com&#x2F;open-meteo&#x2F;open-meteo&#x2F;issues&#x2F;206 reply berniedurfee 7 hours agorootparentprevI’ve always wanted to see something like that. I always wonder if forecasts are a coin flip beyond a window of a few hours. reply CSMastermind 7 hours agorootparentI know at a minimum that hurricane forecasts have gotten significantly better over time. We can nowhttps:&#x2F;&#x2F;www.nhc.noaa.gov&#x2F;verification&#x2F;verify5.shtmlOur 96 hour projections are as accurate today as the 24 hour projections were in 1990. reply boxed 16 hours agoparentprevOpen-Meteo has a great API too. I used it to build my iOS weather app Frej (open source and free: https:&#x2F;&#x2F;github.com&#x2F;boxed&#x2F;frej)It was super easy and the responses are very fast. reply caseyf7 5 hours agoparentprevHow did you handle missing data? I’ve used NOAA data a few times and I’m always surprised at how many days of historical data are missing. They have also stopped recording in certain locations and then start in new locations over time making it hard to get solid historical weather information. reply Guestmodinfo 3 hours agoparentprevI always suspect that they don&#x27;t tell me the actual temperature. Maybe I am totally wrong but I suspect. I need to get my own physical thermometer not the digital one in my room and outside my house and have a camera focussed on it. So that later I can speed up the video and see how much the weather varied the previous night. reply kubiton 2 hours agorootparentWhat? Why? reply Vagantem 12 hours agoparentprevThat’s awesome! I’ve hooked something similar up to my service - https:&#x2F;&#x2F;dropory.com which predicts which day it will rain the least for any locationBased on historical data! reply polygamous_bat 10 hours agorootparentYikes, after completed three steps I was asked for my email. No to your bait and switch, thanks! reply Vagantem 10 hours agorootparentIt can take up to 10 min to generate a report - I had a spinner before but people just left the page. So I implemented a way to send it to them instead. I’ve never used the emails for anything else than that. Try it with a 10 min disposable email address if you like. Thanks for your feedback! reply polygamous_bat 5 hours agorootparentOk, seems like your UI is not coming from a place of malice. However, pulling out an email input form at the final step is a very widespread UI dark pattern, so if nothing else please let people know that you will ask their email before they start interacting with your forms. reply 3abiton 1 hour agoparentprevAre multiple data sources supported? reply Omnipresent 7 hours agoparentprevThis is great. I am very curious about the architectural decisions you&#x27;ve taken here. Is there a blog post &#x2F; article about them? 80 yrs of historical data -- are you storing that somewhere in PG and the APIs are just fetching it? If so, what indices have you set up to make APIs fetch faster etc. I just fetched 1960 to 2022 in about 12 secs. reply meteo-jeff 1 hour agorootparentTraditional database systems struggle to handle gridded data efficiently. Using PG with time-based indices is memory and storage extensive. It works well for a limited number of locations, but global weather models at 9-12 km resolution have 4 to 6 million grid-cells.I am exploiting on the homogeneity of gridded data. In a 2D field, calculating the data position for a graphical coordinate is straightforward. Once you add time as a third dimension, you can pick any timestamp at any point on earth. To optimize read speed, all time steps are stored sequentially on disk in a rotated&#x2F;transposed OLAP cube.Although the data now consists of millions of floating-point values without accompanying attributes like timestamps or geographical coordinates, the storage requirements are still high. Open-Meteo chunks data into small portions, each covering 10 locations and 2 weeks of data. Each block is individually compressed using an optimized compression scheme.While this process isn&#x27;t groundbreaking and is supported by file systems like NetCDF, Zarr, or HDF5, the challenge lies in efficiently working with multiple weather models and updating data with each new weather model run every few hours.You can find more information here: https:&#x2F;&#x2F;openmeteo.substack.com&#x2F;i&#x2F;64601201&#x2F;how-data-are-store... reply comment_ran 12 hours agoparentprevHow about https:&#x2F;&#x2F;pirateweather.net&#x2F;en&#x2F;latest&#x2F; ?Does anyone have a compare this API with the latest API we have here? reply meteo-jeff 12 hours agorootparentBoth APIs use weather models from NOAA GFS and HRRR, providing accurate forecasts in North America. HRRR updates every hour, capturing recent showers and storms in the upcoming hours. PirateWeather gained popularity last year as a replacement for the Dark Sky API when Dark Sky servers were shut down.With Open-Meteo, I&#x27;m working to integrate more weather models, offering access not only to current forecasts but also past data. For Europe and South-East Asia, high-resolution models from 7 different weather services improve forecast accuracy compared to global models. The data covers not only common weather variables like temperature, wind, and precipitation but also includes information on wind at higher altitudes, solar radiation forecasts, and soil properties.Using custom compression methods, large historical weather datasets like ERA5 are compressed from 20 TB to 4 TB, making them accessible through a time-series API. All data is stored in local files; no database set-up required. If you&#x27;re interested in creating your own weather API, Docker images are provided, and you can download open data from NOAA GFS or other weather models. reply _visgean 8 hours agoparentprevThere is also https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;weatherbench2 which has baselines of numerical weather models. reply just_testing 10 hours agoparentprevI was going to ask about air quality, but just opened the site and you have air quality as well! Thanks! reply willsmith72 11 hours agoparentprevthis is really cool, I&#x27;ve been looking for good snow-related weather APIs for my business. I tried looking on the site, but how does it work, being coordinates-based?I&#x27;m used to working with different weather stations, e.g. seeing different snowfall prediction at the bottom of a mountain, halfway up, and at the top, where the coordinates are quite similar. reply mdbmdb 16 hours agoparentprevIs it able to provide data on extreme events. Say, the current and potential path of a hurricane? similar to .kml that NOAA provides reply meteo-jeff 15 hours agorootparentExtreme weather is predicted by numerical weather models. Correctly representing hurricanes has driven development on the NOAA GFS model for centuries.Open-Meteo focuses on providing access to weather data for single locations or small areas. If you look at data for coastal areas, forecast and past weather data will show severe winds. Storm tracks or maps are not available, but might be implemented in the future. reply dmd 13 hours agorootparentI would love to hear about this centuries-old NOAA GFS model. The one I know about definitely doesn&#x27;t have that kind of history behind it. reply K2h 13 hours agorootparentSome of the oldest data may come from ships logs back to 1836https:&#x2F;&#x2F;www.reuters.com&#x2F;graphics&#x2F;CLIMATE-CHANGE-ICE-SHIPLOGS... reply meteo-jeff 13 hours agorootparentprevSorry, decades.KML files for storm tracks are still the best way to go. You could calculate storm tracks yourself for other weather models like DWD ICON, ECMWF IFS or MeteoFrance ARPEGE, but storm tracks based on GFS ensembles are easy to use with sufficient accuracy reply mdbmdb 14 hours agorootparentprevAppreciate the response. Do you know of any services that provide what I described in the previous comments? I&#x27;m specifically interested in extreme weather conditions and their visual representation (hurricanes, tornados, hails etc.) with API capabilities reply swells34 12 hours agorootparentGo to: nhc.noaa.gov&#x2F;gis There&#x27;s a list of data and products with kmls and kmzs and geojsons and all sorts of stuff. I haven&#x27;t actually used the API for retrieving these, but NOAA has a pretty solid track record with data dissemination. replyrobertlagrant 17 hours agoprevThis is fascinating:> For inputs, GraphCast requires just two sets of data: the state of the weather 6 hours ago, and the current state of the weather. The model then predicts the weather 6 hours in the future. This process can then be rolled forward in 6-hour increments to provide state-of-the-art forecasts up to 10 days in advance. reply counters 16 hours agoparentIt&#x27;s worth pointing out that \"state of the weather\" is a little bit hand-wavy. The GraphCast model requires a fully-assimilated 3D atmospheric state - which means you still need to run a full-complexity numerical weather prediction system with a massive amount of inputs to actually get to the starting line for using this forecast tool.Initializing directly from, say, geostationary and LEO satellite data with complementary surface station observations - skipping the assimilation step entirely - is clearly where this revolution is headed, but it&#x27;s very important to explicitly note that we&#x27;re not there yet (even in a research capacity). reply baq 14 hours agorootparentYeah current models aren’t quite ready to ingest real time noisy data like the actual weather… I hear they go off the rails if preprocessing is skipped (outliers, etc) reply Imanari 16 hours agoparentprevInteresting indeed, only one lagged feature for time series forecasting? I’d imagine that including more lagged inputs would increase performance. Rolling the forecasts forward to get n-step-ahead forecasts is a common approach. I’d be interested in how they mitigated the problem of the errors accumulating&#x2F;compounding. reply broast 16 hours agoparentprevWeather is markovian reply hakuseki 7 hours agorootparentThat is not strictly true. The weather at time t0 may affect non-weather phenomena at time t1 (e.g. traffic), which in turn may affect weather at time t2.Furthermore, a predictive model is not working with a complete picture of the weather, but rather some limited-resolution measurements. So, even ignoring non-weather, there may be local weather phenomena detected at time t0, escaping detection at time t1, but still affecting weather at time t2. reply Al-Khwarizmi 16 hours agoparentprevI don&#x27;t know much about weather prediction, but if a model can improve the state of the art only with that data as input, my conclusion is that previous models were crap... or am I missing something? reply postalrat 15 hours agorootparentRead the other comments. reply xnx 17 hours agoprevI continue to be a little confused by the distinction between Google, Google Research and DeepMind. Google Research, had made this announcement about 24-hour forecasting just 2 weeks ago: https:&#x2F;&#x2F;blog.research.google&#x2F;2023&#x2F;11&#x2F;metnet-3-state-of-art-n... (which is also mentioned in the GraphCast announcement from today) reply mukara 17 hours agoparentDeepMind recently merged with the Brain team from Google Research to form `Google DeepMind`. It seems this was done to have Google DeepMind focused primarily (only?) on AI research, leaving Google Research to work on other things in more than 20 research areas. Still, some AI research involves both orgs, including MetNet in weather forecasting.In any case, GraphCast is a 10-day global model, whereas MetNet is a 24-hour regional model, among other differences. reply xnx 16 hours agorootparentGood explanation. Now that both the 24-hour regional and 10-day global models have been announced in technical&#x2F;research detail, I supposed there might still be a general blog post about how improved forecasting is when you search for \"weather\" or check the forecast on Android. reply kridsdale3 14 hours agorootparentIIRC the MetNet announcement a few weeks ago said that their model is now used when you literally Google your local weather. I don&#x27;t think it&#x27;s available yet to any API that third party weather apps pull from, so you&#x27;ll have to keep searching \"weather in Seattle\" to see it. reply daemonologist 13 hours agorootparentIt&#x27;s also used, at least for the high resolution precipitation forecast, in the default Android weather app (which is really part of the \"Google\" app situation). reply mnky9800n 16 hours agorootparentprevThat would require your local weather service to use these models reply danielmarkbruce 16 hours agorootparentprevIs there a colab example (and&#x2F;or have they released the models) for MetNet like they have here for GraphCast? reply mukara 16 hours agorootparentMetNet-3 is not open-source, and the announcement said it&#x27;s already integrated into Google products&#x2F;services needing weather info. So, I&#x27;d doubt there&#x27;s anything like a colab example. reply stabbles 13 hours agoprevIf you live in a country where local, short-term rain &#x2F; shower forecast is essential (like [1] [2]), it&#x27;s funny to see how incredibly bad radar forecast is.There are really convenient apps that show an animated map with radar data of rain, historical data + prediction (typically).The prediction is always completely bonkers.You can eyeball it better.No wonder \"AI\" can improve that. Even linear extrapolation is better.Yes, local rain prediction is a different thing from global forecasting.[1] https:&#x2F;&#x2F;www.buienradar.nl [2] https:&#x2F;&#x2F;www.meteoschweiz.admin.ch&#x2F;service-und-publikationen&#x2F;... reply je42 3 hours agoparentHowever, tools like buienrader seem to have trouble in the recent months&#x2F;years to accurately predict local weather. reply bberenberg 12 hours agoparentprevInteresting that you say this. I spent in month in AMS 7-8 years ago and buienradar was accurate down to the minute when I used it. Has something changed? reply bobviolier 2 hours agorootparentI don&#x27;t know how or why, but yes, it has become less accurate over at least the last year or so. reply supdudesupdude 12 hours agoparentprevFunny to mention. None of the AI forecasts can actually predict precip. None of them mention this and i assume everyone thinks this means the rain forecasts are better. Nope just temperature and humidity and wind. Important but come on, it&#x27;s a bunch of shite reply serjester 16 hours agoprevTo call this impressive is an understatement. Using a single GPU, outperforms models that run on the world&#x27;s largest super computers. Completely open sourced - not just model weights. And fairly simple training &#x2F; input data.> ... with the current version being the largest we can practically fit under current engineering constraints, but which have potential to scale much further in the future with greater compute resources and higher resolution data.I can&#x27;t wait to see how far other people take this. reply wenc 16 hours agoparentIt builds on top of supercomputer model output and does better at the specific task of medium term forecasts.It is a kind of iterative refinement on the data that supercomputers produce — it doesn’t supplant supercomputers. In fact the paper calls out that it has a hard dependency on the output produced by supercomputers. reply carbocation 15 hours agorootparentI don&#x27;t understand why this is downvoted. This is a classic thing to do with deep learning: take something that has a solution that is expensive to compute, and then train a deep learning model from that. And along the way, your model might yield improvements, too, and you can layer in additional features, interpolate at finer-grained resolution, etc. If nothing else, the forward pass in a deep learning model is almost certainly way faster than simulating the next step in a numerical simulation, but there is room for improvement as they show here. Doesn&#x27;t invalidate the input data! reply danielmarkbruce 15 hours agorootparentBecause \"iterative refinement\" is sort of wrong. It&#x27;s not a refinement and it&#x27;s not iterative. It&#x27;s an entirely different model to physical simulation which works entirely differently and the speed up is order of magnitude.Building a statistical model to approximate a physical process isn&#x27;t a new idea for sure.. there are literally dozens of them for weather.. the idea itself isn&#x27;t really even iterative, it&#x27;s the same idea... but it&#x27;s all in the execution. If you built a model to predict stock prices tomorrow and it generated 1000% pa, it wouldn&#x27;t be reasonable for me to call it iterative. reply kridsdale3 14 hours agorootparentIt is iterative when you look at the scope of \"humans trying to solve things over time\". reply danielmarkbruce 14 hours agorootparentlol, touche. reply andbberger 14 hours agorootparentprev\"amortized inference\" is a better name for it reply borg16 14 hours agorootparentprev> the forward pass in a deep learning model is almost certainly way faster than simulating the next step in a numerical simulationIs this the case in most of such refinements (architecture wise)? reply danielmarkbruce 14 hours agorootparentPractically speaking yes. You&#x27;d not likely build a statistical model when you could build a good simulation of the underlying process if the simulation was already really fast and accurate. reply westurner 15 hours agorootparentprev\"BLD,ENH: Dask-scheduler (SLURM,),\" https:&#x2F;&#x2F;github.com&#x2F;NOAA-EMC&#x2F;global-workflow&#x2F;issues&#x2F;796Dask-jobqueue https:&#x2F;&#x2F;jobqueue.dask.org&#x2F; :> provides cluster managers for PBS, SLURM, LSF, SGE and other [HPC supercomputer] resource managersHelpful tools for this work: Dask-labextension, DaskML, CuPY, SymPy&#x27;s lambdify(), Parquet, ArrowGFS: Global Forecast System: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Global_Forecast_SystemTIL about Raspberry-NOAA and pywws in researching and summarizing for a comment on \"Nrsc5: Receive NRSC-5 digital radio stations using an RTL-SDR dongle\" (2023) https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38158091 reply whatever1 15 hours agorootparentprevSo best case scenario we can avoid some computation for inference, assuming that historical system dynamics are still valid. This model needs to be constantly monitored by full scale simulations and rectified over time. reply silveraxe93 15 hours agorootparentprevCould you point me to the part where it says it depends on supercomputer output?I didn&#x27;t read the paper but the linked post seems to say otherwise? It mentions it used the supercomputer output to impute data during training. But for prediction it just needs:> For inputs, GraphCast requires just two sets of data: the state of the weather 6 hours ago, and the current state of the weather. The model then predicts the weather 6 hours in the future. This process can then be rolled forward in 6-hour increments to provide state-of-the-art forecasts up to 10 days in advance. reply serjester 15 hours agorootparentYou can read about it more in their paper. Specifically page 36. Their dataset, ERA5, is created using a process called reanalysis. It combines historical weather observations with modern weather models to create a consistent record of past weather conditions.https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;DeepMind.com&#x2F;B... reply dekhn 13 hours agorootparentI can&#x27;t find the details, but if the supercomputer job only had to run once, or a few times, while this model can make accurate predictions repeatedly on unique situations, then it doesn&#x27;t matter as much that a supercomputer was required. The goal is to use the supercomputer once, to create a high value simulated dataset, then repeatedly make predictions from the lower-cost models. reply silveraxe93 15 hours agorootparentprevAh nice. Thanks! reply pkulak 15 hours agorootparentprevWhy can&#x27;t they just train on historical data? reply _visgean 9 hours agorootparentERA5 is based on historical data. See it for yourself https:&#x2F;&#x2F;cds.climate.copernicus.eu&#x2F;cdsapp#!&#x2F;dataset&#x2F;reanalysi..., https:&#x2F;&#x2F;www.ecmwf.int&#x2F;en&#x2F;forecasts&#x2F;dataset&#x2F;ecmwf-reanalysis-...I don&#x27;t using raw historical data would work for any data intensive model - afaik the data is patchy - there are spots where we don&#x27;t have that many datapoints - e.g. middle of ocean... Also there are new satelites that are only available for the last x years and you want to be able to use these for the new models. So you need a re-analysis of what it would look like if you had that data 40 years ago...Also its very convinient dataset because many other models trained on it: https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;weatherbench2 so easy to do benchmarking.. reply xapata 14 hours agorootparentprevWe don&#x27;t have enough data. There&#x27;s only one universe, and it&#x27;s helpful to train on counter-factual events. reply thatguysaguy 16 hours agoparentprevThey said single TPU machine to be fair, which means like 8 TPUs (still impressive) reply lispisok 17 hours agoprevI&#x27;ve been following these global ML weather models. The fact they make good forecasts at all was very impressive. What is blowing my mind is how fast they run. It takes hours on giant super computers for numerical weather prediction models to forecast the entire globe. These ML models are taking minutes or seconds. This is potentially huge for operational forecasting.Weather forecasting has been moving focus towards ensembles to account for uncertainty in forecasts. I see a future of large ensembles of ML models being ran hourly incorporating the latest measurements reply counters 16 hours agoparentAbsolutely - but large ensembles are just the tip of the iceberg. Why bother producing an ensemble when you could just output the posterior distribution of many forecast predictands on a dense grid? One could generate the entire ensemble-derived probabilities from a single forward model run.Another very cool application could incorporate generative modeling. Inject a bit of uncertainty in a some observations and study how the manifold of forecast outputs changes... ultimately, you could tackle things like studying the sensitivity of forecast uncertainty for, say, a tropical cyclone or nor&#x27;easter relative to targeted observations. Imagine a tool where you could optimize where a Global Hawk should drop rawindsondes over the Pacific Ocean to maximally decrease forecast uncertainty for a big winter storm impacting New England...We may not be able to engineer the weather anytime soon, but in the next few years we may have a new type of crystal ball for anticipating its nuances with far more fidelity than ever before. reply wenc 16 hours agoparentprevNot to take away from the excitement but ML weather prediction builds upon the years of data produced by numerical models on supercomputers. It cannot do anything without that computation and its forecasts are dependent on the quality of that computation. Ensemble models are already used to quantify uncertainty (it’s referenced in their paper).But it is exciting that they are able to recognize patterns in multi year and produce medium term forecasts.Some comments here suggest this replaces supercomputers models. This would a wrong conclusion.It does not (the paper explicitly states this). It uses their output as input data. reply boxed 16 hours agorootparentI don&#x27;t get this. Surely past and real weather should be the input training data, not the output of numerical modeling? reply counters 16 hours agorootparentWell, what is \"real weather data?\"We have dozens of complementary and contradictory sources of weather information. Different types of satellites measuring EM radiation in different bands, weather stations, terrestrial weather radars, buoys, weather balloons... it&#x27;s a massive hodge-podge of different systems measuring different things in an uncoordinated fashion.Today, it&#x27;s not really practical to assemble that data and directly feed it into an AI system. So the state-of-the-art in AI weather forecasting involves using an intermediate representation - \"reanalysis\" datasets which apply a sophisticated physics based weather model to assimilate all of these data sets into a single, self-consistent 3D and time-varying record of the state of the atmosphere. This data is the unsung hero of the weather revolution - just as the WMO&#x27;s coordinated synoptic time observations for weather balloons catalyzed effective early numerical weather prediction in the 50&#x27;s and 60&#x27;s, accessible re-analysis data - and the computational tools and platforms to actually work with these peta-scale datasets - has catalyzed the advent of \"pure AI\" weather forecasting systems. reply goosinmouse 15 hours agorootparentGreat comment, thank you for sharing your insights. I don&#x27;t think many people truly understand just how massive these weather models are and the sheer volume of data assimilation work that&#x27;s been done for decades to get us to this point today.I always have a lot of ideas about using AI to solve very small scale weather forecasting issues, but there&#x27;s just so much to it. It&#x27;s always a learning experience for sure. reply boxed 2 hours agorootparentprevOh yea, sure. But the article makes it seems like the model is trained on some predictive model, instead of a synthesis model. That seems weird to me. reply mnky9800n 16 hours agoparentprevIt uses era5 data which is reanalysis. These models will always need the numerical training data. What&#x27;s impressive is how well the emulate the physics in those models so cheaply. But since the climate changes there will eventually be different weather in different places.https:&#x2F;&#x2F;www.ecmwf.int&#x2F;en&#x2F;forecasts&#x2F;documentation-and-support reply kridsdale3 14 hours agoparentprevThis is basically equivalent to NVIDIA&#x27;s DLSS machine learning running on Tensor Cores to \"up-res\" or \"frame-interpolate\" the extremely computationally intensive job the traditional GPU rasterizer does to simulate a world.You could numerically render a 4k scene at 120FPS at extreme cost, or you could render a 2k scene at 60FPS, then feed that to DLSS to get a close-enough approximation of the former at enormous energy and hardware savings. reply Vagantem 12 hours agoprevRelated to this, I built a service that shows what day it has rained the least on in the last 10 years - for any location and month! Perfect to find your perfect wedding date. Feel free to check out :)https:&#x2F;&#x2F;dropory.com reply helloplanets 2 hours agoparentWas interested to check this out for Helsinki, but site loads blank on Safari :( reply crazygringo 10 hours agoprevMaking progress on weather forecasting is amazing, and it&#x27;s been interesting to see the big tech companies get into this space.Apple moved from using The Weather Channel to their own forecasting a year ago [1].Using AI to produce better weather forecasts is exactly the kind of thing that is right up Google&#x27;s alley -- I&#x27;m very happy to see this, and can&#x27;t wait for this to get built into our weather apps.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Weather_(Apple) reply blacksmith_tb 9 hours agoparentWell, Apple acquired Dark Sky and then shut it down for Android users[1], and then eventually for iOS users as well (but rolled it into the built in weather app, I think).1: https:&#x2F;&#x2F;www.theverge.com&#x2F;2020&#x2F;3&#x2F;31&#x2F;21201666&#x2F;apple-acquires-w... reply _visgean 9 hours agoparentprev> Apple moved from using The Weather Channel to their own forecasting a year ago [1].AFAIK they don&#x27;t have their own forecasting models, they use same data sources as everyone else: https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT211777 reply crazygringo 6 hours agorootparentYour linked article says they use their own, if you&#x27;re on a version later than iOS 15.2. reply freedomben 16 hours agoprevweather prediction seems to me like a terrific use of machine learning aka statistics. The challenge I suppose is in the data. To get perfect predictions you&#x27;d need to have a mapping of what conditions were like 6 hours, 12 hours, etc before, and what the various outcomes were, which butterflies flapped their wings and where (this last one is a joke about how hard this data would be). Hard but not impossible. Maybe impossible. I know very little about weather data though. Is there already such a format? reply tash9 16 hours agoparentIt&#x27;s been a while since I was a grad student but I think the raw station&#x2F;radiosonde data is interpolated into a grid format before it&#x27;s put into the standard models. reply kridsdale3 14 hours agorootparentThis was also in the article. It splits the sphere surface in to 1M grids (not actually grids in the cartesian sense of a plane, these are radial units). Then there&#x27;s 37 altitude layers.So there&#x27;s radial-coordinate voxels that represent a low resolution of the physical state of the entire atmosphere. reply layoric 11 hours agoprevI can&#x27;t see any citation to accuracy comparisons, or maybe I just missed them? Given the amount of data, and complexity of the domain, it would be good to see a much more detailed breakdown of their performance vs other models.My experience in this space is that I was first employee at Solcast building a live &#x27;nowcast&#x27; system for 4+ years (left ~2021) targeting solar radiation and cloud opacity initially, but expanding into all aspects of weather, focusing on the use of the newer generation of satellites, but also heavily using NWP models like ECMWF. Last I knew,nowcasts were made in minutes on a decent size cluster of systems, and has been shown in various studies and comparisons to produce extremely accurate data (This article claims &#x27;the best&#x27; without links which is weird..), be interesting on how many TPUsv4 were used to produce these forecasts and how quickly? Solcast used ML as a part of their systems, but when it comes down to it, there is a lot more operationally to producing accurate and reliable forecasts, eg it would be arrogant to say the least to switch from something like ECMWF to this black box anytime soon.Something I said as just before I left Solcast was that their biggest competition would come from Amazon&#x2F;Google&#x2F;Microsoft and not other incumbent weather companies. They have some really smart modelers, but its hard to compete with big tech resources. I believe Amazon has been acquiring power usage IoT related companies over the past few years, I can see AI heavily moving into that space as well.. for better or worse. reply alxmrs 4 hours agoparentI’m so happy you asked about this! Check out https:&#x2F;&#x2F;sites.research.google&#x2F;weatherbench&#x2F; reply shmageggy 10 hours agoparentprevI think the paper has what you are looking for. Several figures comparing performance to HRES, and \"GraphCast... took roughly four weeks on 32 Cloud TPU v4 devices using batch parallelism. See supplementary materials section 4 for further training details.\" reply rottc0dd 1 hour agoprevHow long does this forecasting hold, given butterfly effect et al? reply Gys 17 hours agoprevI live in an area which regularly has a climate differently then forecasted: often less rain and more sunny. Would be great if I can connect my local weather station (and&#x2F;or its history) to some model and have more accurate forecasts. reply tash9 16 hours agoparentOne piece of context to note here is that models like ECMWF are used by forecasters as a tool to make predictions - they aren&#x27;t taken as gospel, just another input.The global models tend to consistently miss in places that have local weather \"quirks\" - which is why local forecasters tend to do better than, say, accuweather, where it just posts what the models say.Local forecasters might have learned over time that, in early Autumn, the models tend to overpredict rain, and so when they give their forecasts, they&#x27;ll tweak the predictions based on the model tendencies. reply speps 16 hours agoparentprevBecause weather data is interpolated between multiple stations, you wouldn&#x27;t even need the local station position, your own position would be more accurate as it&#x27;d take a lot more parameters into account. reply dist-epoch 14 hours agoparentprevThere are models which take as input both global forecasts and local ones, and which then can transpose a global forecast into a local one.National weather institutions sometimes do this, since they don&#x27;t have the resources to run a massive supercomputer model. reply Gys 13 hours agorootparentInteresting. So what I am looking for is probably an even more scaled down version? Or something that runs in the cloud with an api to upload my local measurements. reply supdudesupdude 12 hours agorootparentHate to break it but one weather station wont improve a forecast? What are they supposed to do? Ignore the output of our state of the art forecast models and add an if statement for your specific weather station?? reply pyb 13 hours agoprevCurious. How can AI&#x2F;ML perform on a problem that is, as far as I understand, inherently chaotic &#x2F; unpredictable ? It sounds like a fundamental contradiction to me. reply vosper 13 hours agoparentWeather isn’t fundamentally unpredictable. We predict weather with a fairly high degree of accuracy (for most practical uses), and the accuracy getting better all the time.https:&#x2F;&#x2F;scijinks.gov&#x2F;forecast-reliability reply sosodev 13 hours agorootparentI&#x27;m kinda surprised that this government science website doesn&#x27;t seem to link sources. I&#x27;d like to read the research to understand how they&#x27;re measuring the accuracy. reply crazygringo 10 hours agoparentprevBecause there are tons of parts of weather where chaos isn&#x27;t the limiting factor currently.There are a limited number of weather stations producing measurements, and a limited \"cell size\" for being able to calculate forecasts quickly enough, and geographical factors that aren&#x27;t perfectly accounted for in models.AI is able to help substantially with all of these -- from interpolation to computational complexity to geography effects. reply keule 13 hours agoparentprevIMO a chaotic system will not allow for long-term forecast, but if there is any type of pattern to recognize (and I would assume there are plenty), an AI&#x2F;ML model should be able to create short-term prediction with high accuracy. reply pyb 13 hours agorootparentNot an expert, but \"Up to 10 days in advance\" sounds like long-term to me ? reply joaogui1 13 hours agorootparentI think 10 days is basically the normal term for weather, in that we can get decent predictions for that span using \"classical\"&#x2F;non-ML methods. reply pyb 13 hours agorootparentIDK, I wouldn&#x27;t plan a hike in the mountains based on 10-day predictions. reply keule 13 hours agorootparentprevTo be clear: With short-term I meant the mentioned 6 hours of the article. They use those 6 hours to create forecasts for up to 10 days. I would think that the initial predictors for a phenomenon (like a hurricane) are well inside that timespan. With long-term, I meant way beyond a 14-day window. reply kouru225 13 hours agorootparentprevBut AI&#x2F;ML models require good data and the issue with chaotic systems like weather is that we don’t have good enough data. reply joaogui1 13 hours agorootparentThe issue with chaotic systems is not data, is that the error grows superlinearly with time, and since you always start with some kind of error (normally due to measurement limitations) this means that after a certain time horizon the error becomes to significant to trust the prediction. That hasn&#x27;t a lot to do with data quality for ML models reply kouru225 13 hours agorootparentThat’s an issue with data: If your initial conditions are wrong (Aka your data collection has any error or isn’t thorough enough) then you get a completely different result. reply nl 10 hours agorootparentEvery measurement has inherent errors in it - and those errors are large if the task is to measure the location and velocity of every molecule in the atmosphere.You also need to measure the exact amount of solar radiation before it hits these molecules (which is impossible, so we assume this is constant depending on latitude and time)These errors compound (the butterfly effect) which is why we can&#x27;t get perfect predictions.This is a limit inherent in physical systems because of physics, not really a data problem. replykouru225 13 hours agoparentprevYes. Very accurate as long as you don’t need to predict the unpredictable. So it’s useless.Edit: I do see a benefit to the idea if you compare it to the Chaos Theorists “gaining intuition” about systems. reply pyb 13 hours agorootparentIDK if it&#x27;s useless, but it&#x27;s counter-intuitive to me. reply brap 13 hours agoprevBeyond the difficulty of running calculations (or even accurately measuring the current state), is there a reason to believe weather is unpredictable?I would imagine we probably have a solid mathematical model of how weather behaves, so given enough resources to measure and calculate, could you, in theory, predict the daily weather going 10 years into the future? Or is there something inherently “random” there? reply counters 13 hours agoparentWhat you&#x27;re describing is effectively how climate models work; we run a physical model which solves the equations that govern how the atmosphere works out forward in time for very long time integrations. You get \"daily weather\" out as far as you choose to run the model.But this isn&#x27;t a \"weather forecast.\" Weather forecasting is an initial value problem - you care a great deal about how the weather will evolve from the current atmospheric conditions. Precisely because weather is a result of what happens in this complex, 3D fluid atmosphere surrounding the Earth, it happens that small changes in those initial conditions can have a very big impact on the forecast on relatively short time-periods - as little as 6-12 hours. Small perturbations grow into larger ones and feedback across spatial scales. Ultimately, by day ~3-7, you wind up with a very different atmospheric state than what you&#x27;d have if you undid those small changes in the initial conditions.This is the essence of what \"chaos\" means in the context of weather prediction; we can&#x27;t perfectly know the initial conditions we feed into the model, so over some relatively short time, the \"model world\" will start to look very different than the \"real world.\" Even if we had perfect models - capable of representing all the physics in the atmosphere - we&#x27;d still have this issue as long as we had to imperfectly sample the atmosphere for our initial conditions.So weather isn&#x27;t inherently \"unpredictable.\" And in fact, by running lots of weather models simultaneously with slightly perturbed initial conditions, we can suss out this uncertainty and improve our estimate of the forecast weather. In fact, this is what&#x27;s so exciting to meteorologists about the new AI models - they&#x27;re so much cheaper to run that we can much more effectively explore this uncertainty in initial conditions, which will indirectly lead to improved forecasts. reply willsmith72 11 hours agorootparentis it possible to self-correct, looking at initial value errors in the past? Is it too hard to prescribe the error in the initial value? reply counters 10 hours agorootparentYes, this is effectively what 4DVar data assimilation is [1]. But it&#x27;s very, very expensive to continually run new forecasts with re-assimilated state estimates. Actually, one of the _biggest_ impacts that models like GraphCast might have is providing a way to do exactly this - rapidly re-running the forecast in response to updated initial conditions. By tracking changes in the model evolution over subsequent re-initializations like this, one could might be able to better quantify expected forecast uncertainty, even moreso than just by running large ensembles.Expect lots of R&D in this area over the next two years...[1]: https:&#x2F;&#x2F;www.ecmwf.int&#x2F;en&#x2F;about&#x2F;media-centre&#x2F;news&#x2F;2022&#x2F;25-yea... reply _visgean 8 hours agoparentprevSee https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Numerical_weather_prediction> Present understanding is that this chaotic behavior limits accurate forecasts to about 14 days even with accurate input data and a flawless model. In addition, the partial differential equations used in the model need to be supplemented with parameterizations for solar radiation, moist processes (clouds and precipitation), heat exchange, soil, vegetation, surface water, and the effects of terrain.I think there is a hope that DL models wont have this problem. reply danbrooks 13 hours agoparentprevSmall changes in initial state can lead to huge changes down the line. See: the butterfly effect or chaos theory.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Chaos_theory reply ethanbond 13 hours agoparentprevAFAIK there&#x27;s nothing random anywhere except near atomic&#x2F;subatomic scale. Everything else is just highly chaotic&#x2F;hard-to-forecast deterministic causal chains. reply joegibbs 8 hours agoprevWhen will we have enough data that we will be able to apply this to everything? Imagine a model that can predict all kinds of trends - what new consumer good will be the most likely to succeed, where the next war is most likely to break out, who will win the next election, which stocks are going to break out. One gigantic black box with a massive state, with input from everything - planning approvals, social media posts, solar activity, air travel numbers, seismic readings, TV feeds. reply drakenot 3 hours agoparentSounds a bit like the premise for the Asimov series, \"The Foundation\" reply comment_ran 14 hours agoprevSo for a daily user, to make it a practical usage, let&#x27;s say if I have a local measurement of X, I can predict, let&#x27;s say, 10 days later, or even just tomorrow, or the day after tomorrow, let&#x27;s say the wind direction, is it possible to do that?If it is possible, then I will try using the sensor to measure my velocity at some place where I live, and I can run the model and see how the results look like. I don&#x27;t know if it&#x27;s going to accurately predict the future or within a 10% error bar range. reply dist-epoch 14 hours agoparentNo, this model uses as input the current state of the weather across the whole planet. reply hammad93 8 hours agoprevI think it&#x27;s irresponsible to call first on this because it will hinder scientific collaboration. I appreciate this contribution but the journalism was sloppy. reply whoislewys_1 7 hours agoprevPredicting weather and stock prices don&#x27;t seem too far apart.Is it inevitable that all market alpha gets mined by AI? reply max_ 14 hours agoprevWhat&#x27;s the difference between a \"Graph Neural Network\" and a deep neural network? reply dil8 13 hours agoparentGraph neural networks are deep learning models that trained on graph data. reply RandomWorker 11 hours agorootparentDo you have any resources where I could learn more about these networks? reply user_7832 16 hours agoprev(If someone with knowledge or experience can chime in, please feel free.)To the best of my knowledge, poor weather (especially wind shear&#x2F;microbursts) are one of the most dangerous things possible in aviation. Is there any chance, or plans, to implement this in the current weather radars in planes? reply tash9 15 hours agoparentIf you&#x27;re talking about small scale phenomena (less than 1km), then this wouldn&#x27;t help other than to be able to signal when the conditions are such that these phenomena are more likely to happen. reply csours 11 hours agoprevMakes me wonder how much it would take to do this for a city at something like 100 meter resolution. reply syntaxing 15 hours agoprevMaybe I missed it but does anyone know what it will take to run this model? Seems something fun to try out but not sure if 24GB of VRAM is suffice. reply kridsdale3 14 hours agoparentIt says in the article that it runs on Google&#x27;s tensor units. So, go down to your nearest Google data center, dodge security, and grab one. Then escape the cops. reply azeirah 14 hours agorootparentYou could also just buy a very large amount of their coral consumer TPUs :D reply supdudesupdude 12 hours agoprevI&#x27;ll be impressed when it can predict rainfall better than GFS &#x2F; HRRR &#x2F; EURO etc reply knicholes 11 hours agoprevWhat are the similarities between weather forecasting and financial market forecasting? reply KRAKRISMOTT 11 hours agoparentBoth are complex systems traditionally modeled with differential equations and statistics. reply sonya-ai 11 hours agoparentprevWell it&#x27;s a start, but weather forecasting is far more predictable imo reply haolez 14 hours agoprevAre there any experts around that can chime in on the possible impacts of this technology if widely adopted? reply _visgean 8 hours agoparentIt will get adopted, eventually we will have more accurate weather forecasts. Thats good for anything that depends on weather - e.g. energy consumption and production, transportation costs... reply supdudesupdude 12 hours agoparentprevIt doesnt predict rainfall so i doubt most of us will actually care about it until then. Still it depends on input data (the current state of weather etc). How are we supposed to accurately model the weather at every point in the world? Especially when tech bro Joe living in San Fran expects things to be accurate to a meter within his doorstep reply counters 10 hours agorootparentGraphCast does predict rainfall - see https:&#x2F;&#x2F;charts.ecmwf.int&#x2F;products&#x2F;graphcast_medium-rain-acc?... for example. reply dnlkwk 13 hours agoprevCurious how this factors in long-range shifts or patterns eg el nino. Most accurate is a bold claim reply miserableuse 14 hours agoprevDoes anybody know if its possible to initialize the model using GFS initial conditions used for the GFS HRES model? If so, where can I find this file and how can I use it? Any help would be greatly appreciated! reply counters 13 hours agoparentYou can try, but other models in this class have struggled when initialized using model states pulled from other analysis systems.ECMWF publishes a tool that can help bootstrap simple inference runs with different AI models [1] (they have plugins for several). You could write a tool that re-maps a GDAS analysis to \"look like\" ERA-5 or IFS analysis, and then try feeding it into GraphCast. But YMMV if the integration is stable or not - models like PanguWx do not work off-the-shelf with this approach.[1]: https:&#x2F;&#x2F;github.com&#x2F;ecmwf-lab&#x2F;ai-models reply miserableuse 13 hours agorootparentThank you for your response. Are these ML models initialized by gridded initial conditions measurements (such as the GDAS pointed out) or by NWP model forecast results (such as hour-zero forecast from the GFS)? Or are those one and the same? reply counters 13 hours agorootparentThey&#x27;re more-or-less the same thing. reply simonebrunozzi 13 hours agoprevAmazing. Is there an easy way to run this on a local laptop? reply sagarpatil 15 hours agoprevHow does one go about hosting this and using this as an API? reply max_ 14 hours agoprevHas anyone here heard of \"Numerical Forecasting\" models for weather? I heard they \"work so well\".Does GraphCast come close to them? reply carabiner 14 hours agoprev> GraphCast makes forecasts at the high resolution of 0.25 degrees longitude&#x2F;latitude (28km x 28km at the equator).Any way to run this at even higher resolution, like 1 km? Could this resolve terrain forced effects like lenticular clouds on mountain tops? reply dist-epoch 14 hours agoparentOne big problem is input weather data. It&#x27;s resolution is poor. reply carabiner 14 hours agorootparentYeah, not to mention trying to validate results. Unless we grid install weather stations every 200 m on a mountain top... reply jauntywundrkind 16 hours agoprevFrom what I can tell from reading & based off https:&#x2F;&#x2F;colab.research.google.com&#x2F;github&#x2F;deepmind&#x2F;graphcast&#x2F;... , one needs access to ECMWF Era5 or HRES data-sets or something similar to be able to run and use this model.Unknown what licensing options ECMWF offers for Era5, but to use this model in any live fashion, I think one is probably going to need a small fortune. Maybe some other dataset can be adapted (likely at great pain)... reply _visgean 8 hours agoparentYou can get some of the historical data also from here: https:&#x2F;&#x2F;cloud.google.com&#x2F;storage&#x2F;docs&#x2F;public-datasets&#x2F;era5 (if the official API is too slow. )To use the data in live fashion I think you would need to get license from ECMWF... reply sunshinesnacks 14 hours agoparentprevERA5 is free. The API is a bit slow.I think that only some variables from the HRES are free, but not 100% sure. reply hokkos 10 hours agorootparentThe API is unusably slow, the only way is to use the AWS, GCP or Azure mirrors, but they miss a lot of variables and are updated sparingly or with a delay. reply max_ 14 hours agoprevI have far more respect for the AI team at DeepMind even thou they may be less popular than say OpenAI or \"Grok\".Why? Other AI studios seem to work on gimmicks while DeepMind seems to work on genuinely useful AI applications [0].Thanks for the good work![0] Not to say that Chat GPT & Midjourney are not useful, I just find DeepMinds quality of research more interesting. reply cryptoz 16 hours agoprevAgain haha! Still no mention of using barometers in phones. Maybe some day. reply amluto 16 hours agoprev [–] I&#x27;ve never studied weather forecasting, but I can&#x27;t say I&#x27;m surprised. All of these models, AFAICT, are based on the \"state\" of the weather, but \"state\" deserves massive scare quotes: it&#x27;s a bunch of 2D fields (wind speed, pressure, etc) -- note the 2D. Actual weather dynamics happen in three dimensions, and three dimensional land features, buildings, etc as well as gnarly 2D surface phenomena (ocean surface temperature, ground surface temperature, etc) surely have strong effects.On top of this, surely the actual observations that feed into the model are terrible -- they come from weather stations, sounding rockets, balloons, radar, etc, none of which seem likely to be especially accurate in all locations. Except that, where a weather station exists, the output of that station is the observation that people care about -- unless you&#x27;re in an airplane, you don&#x27;t personally care about the geopotential, but you do care about how windy it is, what the temperature and humidity are, and how much precipitation there is.ISTM these dynamics ought to be better captured by learning them from actual observations than from trying to map physics both ways onto the rather limited datasets that are available. And a trained model could also learn about the idiosyncrasies of the observation and the extra bits of forcing (buildings, etc) that simply are not captured by the inputs.(Heck, my personal in-my-head neural network can learn a mapping from NWS forecasts to NWS observations later in the same day that seems better than what the NWS itself produces. Surely someone could train a very simple model that takes NWS forecasts as inputs and produces its estimates of NWS observations during the forecast period as outputs, thus handling things like \"the NWS consistently underestimates the daily high temperature at such-and-such location during a summer heat wave.\") reply WhitneyLand 15 hours agoparentHow does it make sense to say this is something you’ve “never studied”, followed by how they “ought to be” doing it better?It also seems like some of your facts differ from theirs, may I ask how far you read into the paper? reply amluto 10 hours agorootparentI read a decent amount of the paper, although not the specific details of the model they used. And when I say I \"never studied\" it, I mean that I never took a class or read a textbook. I do, in fact, know something about physics and fluids, and I have even personally done some fluid simulation work.There are perfectly good models for weather in an abstract sense: Navier-Stokes plus various chemical models plus heat transfer plus radiation plus however you feel like modeling the effect of the ground and the ocean surface. (Or use Navier-Stokes for the ocean too!)But this is wildly impractical. The Earth is too big. The relevant distance and time scales are pretty short, and the resulting grid would be too large. Not to mention that we have no way of actually measuring the whole atmosphere or even large sections of it in its full 3D glory in anything remotely close to the necessary amount of detail.Go read the Wikipedia article, and contemplate the \"Computation\" and \"Parameterization\" sections. This works, but it&#x27;s horrible. It&#x27;s doing something akin to making an effective theory (the model actually solved) out of a larger theory (Navier-Stokes+), but we can&#x27;t even measure the fields in the effective theory. We might want to model a handful of fields at 0.25 degrees (of lat&#x2F;long) resolution, but we&#x27;re getting the data from a detailed vertical slice every time someone launches a weather balloon. Which happens quite frequently, but not continuously and not at 0.25 degree spatial increments.Hence my point: Google&#x27;s model is sort of learning an effective theory instead of developing one from first principles based on the laws of physics and chemistry.edit: I once worked in a fluid dynamics lab on something that was a bit analogous. My part of the lab was characterizing actual experiments (burning liquids and mixing of gas jets). Another group was trying to simulate related systems on supercomputers. (This was a while ago. The supercomputers were not very capable by modern standards.)The simulation side used a 3D grid fine enough (hopefully) to capture the relevant dynamics but not so fine that the simulation would never finish. Meanwhile, we measured everything in 1D 2D! We took pictures and videos with cameras at various wavelengths. We injected things into the fluids for better visualization. We measured the actual velocity at one location (with decent temporal resolution) and hoped our instrumentation for that didn’t mess up the experiment too much. We tried to arrange to know the pressure field in the experiment by setting it up right.With the goal of understanding the phenomena, I think this was the right approach. But if we just wanted to predict future frames of video from past frames, I would expect a nice ML model to work better. (Well, I would expect it to work better now. The state of the art was not so great at the time.) reply counters 10 hours agorootparentWeather models are routinely run at resolutions as fine as 1-3 km - fine enough that we do not parameterize things like convection and allow the model to resolve these motions on its native grid. We typically do this over limited areas (e.g. domain the size of a continent), but plenty of groups have such simulations globally. It&#x27;s just not practical (cost for compute and resulting data) to do this regularly, and it offers little by way of direct improvement in forecast quality.Furthermore, we don&#x27;t have to necessarily measure the whole atmosphere in 3D; physical constraints arising from Navier-Stokes still apply, and we use them in conjunction with the data we _do_ have to estimate a full 3D atmospheric state complete with uncertainties. reply kridsdale3 14 hours agorootparentprevNo need, they&#x27;re a software engineer (presumably). That just means they&#x27;re better than everyone. reply Difwif 15 hours agoparentprev [–] I&#x27;m not sure why you&#x27;re emphasizing that weather forecasting is just 2D fields. Even in the article they mention GraphCast predicts multiple data points at each global location across a variety of altitudes. All existing global computational forecast models work the same way. They&#x27;re all 3d spherical coordinate systems. reply amluto 10 hours agorootparent [–] See page three, table 1 of the paper. The model has 48 2D fields, on a grid, where the grid is a spherical thing wrapped around the surface of the Earth.There is not what I would call a 3D spherical coordinate system. There’s no field f defined as f(theta, phi, r) — ther are 48 fields that are functions of theta and phi. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has developed an AI model called GraphCast that can make more accurate and faster medium-range weather forecasts compared to traditional methods.",
      "The model predicts weather conditions up to 10 days in advance, allowing for earlier warnings of extreme weather events like cyclones and floods.",
      "GraphCast is trained on historical weather data using machine learning and graph neural networks, outperforming the industry gold-standard weather simulation system in terms of accuracy and providing better predictions for future weather.",
      "The model's code has been open sourced, allowing scientists and forecasters worldwide to benefit from it and potentially save lives and support critical decision-making in various industries and societies."
    ],
    "commentSummary": [
      "Google DeepMind has developed a new AI weather forecasting model called GraphCast, which combines data from multiple numerical weather models using machine learning to improve accuracy.",
      "The discussions revolve around the limitations of existing APIs, availability of historical weather data, and integration of data on extreme events.",
      "There are potential benefits and challenges of using machine learning in weather prediction, as well as competition in the industry. This technology could have implications for various industries relying on accurate weather forecasts."
    ],
    "points": 527,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1699976543
  },
  {
    "id": 38266773,
    "title": "Bug in Certain Processors Can Cause Glitch State, Poses Security Risk for Cloud Providers",
    "originLink": "https://lock.cmpxchg8b.com/reptar.html",
    "originBody": "Reptar Tavis Ormandy Introduction Discovery Solution Notes We have a CPU mystery! We found a way to cause some processors to enter a glitch state where the normal rules don’t apply, but what does that mean…? If you’re interested what can go wrong inside modern CPUs, read on! Introduction If you’ve ever written any x86 assembly at all, you’ve probably used rep movsb. It’s the idiomatic way of moving memory around on x86. You set the source, destination, direction and the count - then just let the processor handle all the details! lea rdi, [rel dst] lea rsi, [rel src] std mov rcx, 32 rep movsb The actual instruction here is movsb, the rep is simply a prefix that changes how the instruction works. In this case, it indicates that you want this operation repeated multiple times. There are lots of other prefixes too, but they don’t all apply to every instruction. Prefix Decoding An interesting feature of x86 is that the instruction decoding is generally quite relaxed. If you use a prefix that doesn’t make sense or conflicts with other prefixes nothing much will happen, it will usually just be ignored. This fact is sometimes useful; compilers can use redundant prefixes to pad a single instruction to a desirable alignment boundary. Take a look at this snippet, this is exactly the same code as above, just a bunch of useless or redundant prefixes have been added: rep lea rdi, [rel dst] cs lea rsi, [rel src] gs gs gs std repnz mov rcx, 32 rep rep rep rep movsb Perhaps the most interesting prefixes are rex, vex and evex, all of which change how subsequent instructions are decoded. Let’s take a look at how they work. The REX prefix The i386 only had 8 general purpose registers, so you could specify which register you want to use in just 3 bits (because 2^3 is 8). The way that instructions were encoded took advantage of this fact, and reserved just enough bits to specify any of those registers. modr/m example Simple 2-byte instructions that use modr/m might be encoded like this, for example mov eax, ebx. This is an 8-bit opcode, 2 bit addressing mode (labeled m), and 3 bits each for the source (s) and destination (d). Well, this is a problem, because x86-64 added 8 additional general purpose registers. We now have sixteen possible registers..that’s 2^4, so we’re going to need another bit! 😆 The solution to this is the rex prefix, which gives us some spare bits that the next instruction can borrow. When we’re talking about rex, we usually write it like this: rex.rxb rex is a single-byte prefix, the first four bits are mandatory and the remaining four bits called b, x, r and w are all optional. If you see rex.rb that means only the r and b bits are set, all the others are unset. These optional bits give us room to encode more general purpose registers in the following instruction. rex example The rex prefix can lend the next instruction extra bits to use for operands, so now we can encode all 16 possible general purpose registers! Now we’re fine until someone adds another register! 😂 Encoding Rules So now we know that rex increases the available space for encoding operands, and that useless or redundant prefixes are usually ignored on x86. So… what should this instruction do? rex.rxb rep movsb The movsb instruction doesn’t have any operands - they’re all implicit - so any rex bits are meaningless, right? If you guessed that the processor will just silently ignore the rex prefix, you would be correct! Well… except on machines that support a new feature called fast short repeat move! We discovered that a bug with redundant rex prefixes could interact with this feature in an unexpected way and introduce a serious vulnerability, oops 🙂 Fast Short Repeat Move FSRM is a new feature introduced in Ice Lake that fixes some of the shortcomings of ERMS. Hopefully that clears up any confusion. 😆 Just kidding, let’s quickly look at ERMS. The hard part of moving strings around efficiently is getting all the buffers aligned so you can use the widest possible stores available. You could do this in software, but if we do it in microcode then the processor can just transparently make your existing code faster for you. This requires some expensive setup, but once that’s done you get vastly improved throughput. This feature is known as enhanced repeat move/store, ERMS. If you have a processor with ERMS support, simple rep movsb operations can sometimes perform comparably with more complicated hand-tuned vector move operations. However, there is a problem with ERMS. That initial setup is so expensive that it just isn’t worth it for very short strings. This is what FSRM is designed to solve, it handles the case of only moving 128 bytes or less and makes that faster too! I’m not aware of any documentation that explains exactly how FSRM works, but you can check if you have a processor that supports it by looking at the flags line in /proc/cpuinfo: flags : fpu vme de pse tsc msr pae mce cx8 [...] fsrm Some of the processors that have this feature include: Ice Lake Rocket Lake Tiger Lake Raptor Lake Alder Lake Sapphire Rapids Note: This list may not be comprehensive, please see Intel advisory INTEL-SA-00950 for a complete list. Discovery I’ve written previously about a processor validation technique called Oracle Serialization that we’ve been using. The idea is to generate two forms of the same randomly generated program and verify their final state is identical. You can read more about Oracle Serialization in my previous writeup. In August, our validation pipeline produced an interesting assertion. It had found a case where adding redundant rex.r prefixes to an FSRM optimized rep movs operation seemed to cause unpredictable results. We observed some very strange behavior while testing. For example, branches to unexpected locations, unconditional branches being ignored and the processor no longer accurately recording the instruction pointer in xsave or call instuctions. Oddly, when trying to understand what was happening we would see a debugger reporting impossible states! This already seemed like it could be indicative of a serious problem, but within a few days of experimenting we found that when multiple cores were triggering the same bug, the processor would begin to report machine check exceptions and halt. We verified this worked even inside an unprivileged guest VM, so this already has serious security implications for cloud providers. Naturally, we reported this to Intel as soon as we confirmed this was a security issue. Reproduce We’re publishing all of our research today to our security research repository. If you want to reproduce the vulnerability you can use our icebreak tool, I’ve also made a local mirror available here. $ ./icebreak -h usage: ./icebreak [OPTIONS] -c N,M Run repro threads on core N and M. -d N Sleep N usecs between repro attempts. -H N Spawn a hammer thread on core N. icebreak: you must at least specify a core pair with -c! (see -h for help) The testcase enters what should be an infinite loop, and unaffected systems should see no output at all. On affected systems, a . is printed on each successful reproduction. $ ./icebreak -c 0,4 starting repro on cores 0 and 4 ......................................................................... ......................................................................... ......................................................................... ......................................................................... ......................................................................... In general, if the cores are SMT siblings then you may observe random branches and if they’re SMP siblings from the same package then you may observe machine checks. If you do not specify two different cores, then you might need to use a hammer thread to trigger a reproduction. Analysis We know something strange is happening, but how microcode works in modern systems is a closely guarded secret. We can only theorize about the root cause based on observations. μops The CPU is split in two major components, the frontend and the backend. The frontend is responsible for fetching instructions, decoding them and generating μops to send to the backend for execution. The backend executes instructions out of order, and uses a unit called the ROB, reorder buffer, to store and organize results. We believe this bug causes the frontend to miscalculate the size of the movsb instruction, causing subsequent entries in the ROB to be associated with incorrect addresses. When this happens, the CPU enters a confused state that causes the instruction pointer to be miscalculated. The machine can eventually recover from this state, perhaps with incorrect intermediate results, but becoming internally consistent again. However, if we cause multiple SMT or SMP cores to enter the state simultaneously, we can cause enough microarchitectural state corruption to force a machine check. Questions I’m sure some readers will have questions about what is possible in this unexpected “glitch” state. Well, so do we! We know that we can corrupt the system state badly enough to cause machine check errors, and we’ve also observed threads interfere with execution of processes scheduled on SMT siblings. However, we simply don’t know if we can control the corruption precisely enough to achieve privilege escalation. I suspect that it is possible, but we don’t have any way to debug μop execution! If you’re interested in studying this, then we would love to get your input! Credit This bug was independently discovered by multiple research teams within Google, including the silifuzz team and Google Information Security Engineering. The bug was analyzed by Tavis Ormandy, Josh Eads, Eduardo Vela Nava, Alexandra Sandulescu and Daniel Moghimi. Solution Intel have published updated microcode for all affected processors. Your operating system or BIOS vendor may already have an update available! Workaround If you can’t update for some reason, you could disable fast strings via the IA32_MISC_ENABLE model specific register. This will cause a significant performance penalty, and should not be used unless absolutely necessary. Notes If you’re interested in more CPU bugs, we publish everything we find! Not all the bugs we discover have security consequences, but they’re usually worth reading! For example, did you know that sometimes movlps just doesn’t work? or that registers can sometimes roll back to previous values? HOME • ABOUT • CONTACT",
    "commentLink": "https://news.ycombinator.com/item?id=38266773",
    "commentBody": "ReptarHacker NewspastloginReptar (cmpxchg8b.com) 469 points by abhi9u 15 hours ago| hidepastfavorite150 comments tazjin 1 hour agoCan we get a better title for this? \"Reptar - new CPU vulnerability\" or something. I thought it was some random startup ad until I picked up the name somewhere else. reply weinzierl 11 minutes agoparentIf it is changed to what you suggested a question mark would be warranted, because it is not yet clear what can be done with this \"glitch\" (as the article calls it). reply Lammy 14 hours agoprev> the processor would begin to report machine check exceptions and halt.I get it https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dXekDCcw2FE reply shadowgovt 8 hours agoparent... it literally took me all Goddamn day. Well done.Credit where credit is due: Google has some of the best codenames. reply atesti 2 hours agoprevI don&#x27;t understand \"ERMS\" and \"FSRM\" and there seems to be nothing good on google about it.Are these just CPUID flags that tell you that you can use a rep movsb for maximum performance instead of optimized SSE memcpy implementations? Or is it a special encoding&#x2F;prefix for rep movsb to make it faster? In case of the later, why would that be necessary? How does one make use of fsrm? reply ithkuil 2 hours agoparentFSRM is just the name of a cpu optimization that affects existing code.Choosing an optimal instruction choice and scheduling can be done statically during compile time or dynamically (via chosing one of several library functions at runtime, or jitting).In order to be able to detect which is the optimal instruction scheduling at runtime you need to know the actual CPU. You could have a table of all cpu models or you could just ask your OS whether the CPU you run on has that optimization implemented.Linux had to be patched so that it can _report_ that a CPU does implement that optimization.https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Intel-5.6-FSRM-Memmove reply tommiegannert 2 hours agoparentprevFound this [1], which also links to the Intel Optimization Manual [2].Seems like ERMS was a cheaper replacement for AVX and FSRM was a better version, for shorter blocks.> Cheapest versions of later processors - Kaby Lake Celeron and Pentium, released in 2017, don&#x27;t have AVX that could have been used for fast memory copy, but still have the Enhanced REP MOVSB. And some of Intel&#x27;s mobile and low-power architectures released in 2018 and onwards, which were not based on SkyLake, copy about twice more bytes per CPU cycle with REP MOVSB than previous generations of microarchitectures.> Enhanced REP MOVSB (ERMSB) before the Ice Lake microarchitecture with Fast Short REP MOV (FSRM) was only faster than AVX copy or general-use register copy if the block size is at least 256 bytes. For the blocks below 64 bytes, it was much slower, because there is a high internal startup in ERMSB - about 35 cycles. The FSRM feature intended blocks before 128 bytes also be quick.[1] https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;43837564[2] http:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;dam&#x2F;www&#x2F;public&#x2F;us&#x2F;en&#x2F;documents&#x2F;... reply dang 12 hours agoprevRelated: https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;identity-security&#x2F;goo...(via https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38268043, but we merged the comments hither) reply malkia 10 hours agoprevKonrad Magnusson from Paradox Interactive (Victoria 3) team found something related to that and mimalloc -> https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;mimalloc&#x2F;issues&#x2F;807Not sure if fully related, but possibly. reply saagarjha 5 hours agoparentSeems unlikely unless they somehow emitted redundant prefixes reply lights0123 5 hours agorootparentThe article mentions> This fact is sometimes useful; compilers can use redundant prefixes to pad a single instruction to a desirable alignment boundary.so I imagine that could happen under the right optimization mode. reply ithkuil 2 hours agorootparentWhy would a compiler prefer a redundant prefix over a nop for alignment? reply Vecr 1 hour agorootparentIt can be faster (at runtime). replykrylon 46 minutes agoprevThis is very well written. I know little about assembly programming and Intel&#x27;s ISA, let alone their microarchitectures, but I could follow the explanation and feel like I have a rough understanding of what is going on here.Does anyone know if AMD CPUs are affected? reply mike_d 13 hours agoprevThe most awesome part:> This bug was independently discovered by multiple research teams within Google, including the silifuzz team and Google Information Security Engineering. reply xyst 14 hours agoprevReading this makes me realize how little I know of the hardware that runs my software> Prefixes allow you to change how instructions behave by enabling or disabling featuresWhy do we need “prefixes” to disable or enable features? Is this for dynamically toggling feature so you don’t have to go into BIOS? reply db48x 13 hours agoparentRead https:&#x2F;&#x2F;wiki.osdev.org&#x2F;X86-64_Instruction_Encoding#Legacy_Pr...The REP prefixes are the most common; they just let you perform the same instruction a variable number of times. It looks in the CX register for the count. This makes many common loops really, really short, especially for moving objects around in memory. The memcpy function is often inlined as a single REP MOVS instruction, possibly with an instruction to copy the count into CX if it isn’t already there.I suppose the REX (operand size) prefix is pretty common too, since 64–bit programs will want to operate on 64–bit values and addresses pretty frequently.None of the prefixes toggle things that can be set globally, by the BIOS or otherwise. They all just specify things that the next instruction needs to do. reply pclmulqdq 13 hours agorootparentThe ModR&#x2F;M and SIB prefixes are probably the most common prefixes in instructions. They are so common that assemblers elide their existence when you read code. REX is in the same boat: so common that it&#x27;s usually elided. The VEX prefix is also really common (all of the V* AVX instructions, like VMOVDQ), and then the LOCK prefix (all atomics).After all of those, REP is not that uncommon of a prefix to run into, although many people prefer SIMD memcpy&#x2F;memset to REP MOVSB&#x2F;REP STOSB. It is slightly unusual. reply epcoa 3 hours agorootparentThis isn&#x27;t correct. ModR&#x2F;M and SIB are not prefixes. They are suffixes and essentially part of the core instruction encoding for certain memory and register access instruction. they are the primary means of encoding the myriad addressing modes of the x86. And their existence is not elided in any meaningful way, their value is explicitly derived from the instruction operands (SIB is scale, index, base), so when you see an instruction like:mov BYTE PTR [rdi+rbx*4],0x4SIB is determined by the register indices of rdi, rbx, and 4, all right there in the instruction. Likewise, Mod R&#x2F;M encodes the addressing mode, which is clear from the operands in the assembler listing. Though x86 is such as mess that there are cases where you can encode the same instruction in either a Mod R&#x2F;M form or a shorter form, eg PUSH&#x2F;POP.REX is a prefix, but it is a bit special as it must be the last one, and repeats are undefined. It is not elided because of commonality but because its presence and value is usually implied from the operands, it is therefore redundant to list it.For instance, PUSH R12 must use a REX prefix (REX.B with the one byte encoding). reply EarlKing 12 hours agorootparentprevThere&#x27;s a good reason for using vector instructions over REP: Until relatively recently that was how you got maximum performance in small, tight loops. REP is making a comeback precisely because of ERMS and FSRM, so unfortunately this will become a bigger problem going forward. reply bonzini 13 hours agorootparentprevModRM and SIB are not a prefix, they&#x27;re part of the opcode (second and third byte after all the prefixes and the 0Fh&#x2F;0F38h&#x2F;0F3Ah opcode map selectors) reply EarlKing 12 hours agorootparentMore specifically, they&#x27;re affixed to certain opcodes that require them. There are a number of byte-sized opcodes that do not require a ModRM or SIB byte (although a number of those got gobbled up to make the REX prefix, but that&#x27;s another story).TL;DR Weeee! Intel machine language is crazy! reply shenberg 14 hours agoparentprevPrefixes are modifiers to specific instructions executed by the processor, e.g. to control the size of the operands or enable locking for concurrency. reply Tuna-Fish 13 hours agoparentprevx86 was designed in 78, basically for the purpose of running a primitive laser printer (or other similar workloads). The big problem with this is that the encoding space for instructions was \"efficiently utilized\". When new instructions, or worse, additional registers were later added, you had to fit the new instruction variants in somehow, and you did this by tacking on prefixes. reply mschuster91 13 hours agorootparentNah, x86 goes even earlier in its heritage - it was, effectively, a bolt-on on Intel&#x27;s way older designs, as a huge part of the 8086 was being ASM source-compatible with the older 8xxx chips, even as the instruction set itself changed [1]. What utterly amazes me is that the original 8086 was mostly designed by hand by a team of not even two dozen people - and today, we got hundreds if not thousands of people working on designing ASICs...[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intel_8086#The_first_x86_desig... reply irdc 1 hour agorootparentAcckkghtually, if you go back far enough you end up at the Datapoint 2200. If you want to understand where some of the crazier parts of the 8086 originate from, Ken Shirriff has a nice read: http:&#x2F;&#x2F;www.righto.com&#x2F;2023&#x2F;08&#x2F;datapoint-to-8086.html reply hulitu 3 hours agorootparentprevIt is because testing plays a bigger part today than back then. The complexity has also increased (people do not design at transistor level anymore). reply thaumasiotes 53 minutes agorootparentprev> x86 was designed in 78, basically for the purpose of running a primitive laser printerIt&#x27;s interesting that ASCII is transparently just a bunch of control codes for a physical printer&#x2F;typewriter, combining things like \"advance the paper one line\", \"advance the paper one inch\", \"reset the carriage position\", and \"strike an F at the carriage position\", all of which are different mechanical actions that you might want a typewriter to do.But now we have Unicode, which is dedicated to the purpose of assigning ID numbers to visual glyphs, and ASCII has been interpreted as a bunch of glyph references instead of a bunch of machine instructions, and there are the control codes with no visual representation, sitting in Unicode, being inappropriate in every possible way.It&#x27;s kind of like if Unicode were to incorporate \"start microwave\" as part of a set with \"1\", \"2\", \"3\", etc. reply jasonwatkinspdx 7 hours agoparentprevYou got some great answers already, but to your first point check out Hennessey and Patterson&#x27;s books, namely Computer Architecture and Computer Organization and Design.The latter is probably more suited to you unless you wanna go on a dive into computer architecture itself. There&#x27;s older editions available for free (authorized by the authors) on the web.I first read the 3rd edition of Computer Architecture and besides being one of the most clear textbooks I&#x27;ve ever read it vastly improved my understanding of what&#x27;s going on in there in relation to OoO speculative execution, etc. reply epcoa 13 hours agoparentprevThat&#x27;s a very poor summary of what prefixes are. My advice, just skip the original article which isn&#x27;t very good or interesting and read taviso&#x27;s blog that is linked in the top comment (it gives a few concrete examples of these prefixes). They are modifiers that are part of the CPU instruction. reply jeffbee 14 hours agoparentprevIt&#x27;s just because x86 as an ISA has accreted over the course of 40+ years, and has variable-length instructions. Every time they extend the ISA they carve out part of the opcode space to squeeze in a new prefix. This will only continue, considering that Intel has proposed another new scheme this year. reply ajross 12 hours agoparentprev\"Prefixes\" in this case mostly expand the instruction encoding space.So rarely-used addressing modes get a \"segment prefix\" that causes them to use a segment other than DS. Or x86_64 added a \"REX\" prefix that added more bits to the register fields allowing for 16 GPRs. Likewise the \"LOCK\" prefix (though poorly specified originally) causes (some!) memory operations to be atomic with respect to the rest of the system (c.f. \"LOCK CMPXCHG\" to effect a compare-and-set).All these things are operations other CPU architectures represent too, though they tend to pack them into the existing instruction space, requiring more bits to represent every instruction.Notably the \"REP\" prefix in question turns out to be the one exception. This is a microcoded repeat prefix left over from the ancient days. But it represents operations (c.f. memset&#x2F;memmove) that are performance-sensitive even today, so it&#x27;s worthwhile for CPU vendors to continue to optimize them. Which is how the bug in question seems to have happened. reply tedunangst 14 hours agoprevTheir diagnosis reminds me of what happened when qemu ran into repz ret. https:&#x2F;&#x2F;repzret.org&#x2F;p&#x2F;repzret&#x2F; reply writeslowly 13 hours agoprevI noticed the Intel advisory [1] says the followingIntel would like to thank Intel employees:[...] for finding this issue internally.Intel would like to thank Google Employees: [...] for also reporting this issue.[1] https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;security-center&#x2F;advi... reply narinxas 13 hours agoparentI wonder how much sooner than google did intel employees found this issue reply narinxas 12 hours agorootparentbut what I am really wondering about is how much money (if any) was the vulnerability worth up the moment when google also discovered this? reply ajross 12 hours agorootparentAs described it&#x27;s just a CPU crash exploit that requires local binary execution. Getting to a vulnerability would require understanding exactly how the corrupted microcode state works, and that seems extremely difficult outside of Intel.So as described, this isn&#x27;t a \"valuable\" bug. reply sweetjuly 1 hour agorootparentThe blogpost describes that unrelated sibling SMT threads can become corrupted and branch erratically. If you can get a hypervisor thread executing as your SMT sibling and you can figure out how to control it (this is not an if so much as a when), that&#x27;s a VM escape. The Intel advisory acknowledges this too when they say it can lead to privilege escalation. This is hardly a useless bug, in fact it&#x27;s awfully powerful! reply dgacmu 11 hours agorootparentprevIt&#x27;s not super-valuable yet, but it would keep you mount a really nasty DoS on cloud providers by triggering hard resets of the physical machines. Some people would probably pay for that, though it&#x27;s obviously more interesting to push on privilege or exfiltration.Particularly since the MCEs triggered could prevent an automatic reboot. Would depend what the hardware management system did - do machines presenting MCEs get pulled? reply toast0 11 hours agorootparentIf I&#x27;m a cloud provider and somebody&#x27;s workflow is hard resetting lots of my physical machines, I&#x27;m going to give them free access to single tenant machines at the very minimum. If they keep crashing the machines that only they run on, I guess that&#x27;s ok. reply dgacmu 10 hours agorootparentYou can exploit this from a single core shared instance.So you go and find yourself a thousand cheap &#x2F; free tier accounts, spin up an instance in a few regions each, and boom, you&#x27;ve taken out 10k physical hosts. And run it in a lambda at the same time, and see how well the security mechanisms identify and isolate you.Causing a near simultaneous reboot of enough hosts is likely to take other parts of the infrastructure down. reply ajross 10 hours agorootparentI&#x27;m curious what part of this scheme involves \"not ending up in jail\"? Needless to say you can&#x27;t do this without identifying yourself. To make this an exploitable DoS attack you need to be able to run arbitrary binaries on a few thousand cloud hosts that you didn&#x27;t lease yourself. reply dgacmu 2 hours agorootparentI mean, you kinda can. There&#x27;s a depressingly thriving market for stolen cards and things like compromised accounts. A card is a couple of dollars. There are many jurisdictions that turn a blind eye to hacking us companies. Look at how hard it&#x27;s been to rein in the ransomware gangs and even &#x27;booter&#x27; (ddos-for-rent) services.DoS isn&#x27;t as lucrative as other things; I assume that most state actors would far prefer to find a way to turn this into a privilege escalation. But being able to possibly take out a cloud provider for a while is still monetizable. reply blibble 10 hours agorootparentprevthere exist people outside of your jurisdictione.g. the GRU reply mschuster91 9 hours agorootparentprev> I&#x27;m curious what part of this scheme involves \"not ending up in jail\"? Needless to say you can&#x27;t do this without identifying yourself.Stolen credit cards are a dime a dozen, and nation state actors can just use their domestic banks or agents in the banks of other countries in a pinch to deflect blame or lay false trails.If I were Russia or China, I&#x27;d invest a lot of money into researching all kinds of avenues on how to take out the large three public cloud providers if need be: take out AWS, Google, Microsoft and on the CDN side Cloudflare and Akamai and suddenly the entire Western economy grinds to a halt.The only ones who will not be affected are the US government cloud services in AWS, as this runs separate from other AWS regions - that is, unless the attacker gets access to credentials that allow them executions on the GovCloud regions... reply vbezhenar 8 hours agorootparentIf clouds use shared servers to run their management workloads and if very important companies use shared servers to run their workloads, they would deserve it.But I don&#x27;t believe it. People are not that stupid. reply mschuster91 53 minutes agorootparent> If clouds use shared servers to run their management workloads and if very important companies use shared servers to run their workloads, they would deserve it.Why target the management plane? Fire off payloads to take down the physical VM hosts and suddenly any cloud provider has a serious issue because the entire compute capacity drops. reply ajross 8 hours agorootparentprev> If I were Russia or China, I&#x27;d invest a lot of money into researching all kinds of avenues on how to take out the large three public cloud providersThis subthread started with \"is this issue a valuable exploit\". Needless to say, if you need to invoke superpower-scale cyber warfare to find an application, the answer is \"no\". Russia and China have plenty of options to \"take out\" western infrastructure if they&#x27;re willing to blow things up[1] at that scale.[1] Figuratively and literally reply dgacmu 2 hours agorootparentCountries have proven far more reticent to use kinetic options vs. cyberattacks. Or, put differently, we&#x27;re all hacking each other left and right and the responses have thus far mostly remained in the digital realm.See, e.g., https:&#x2F;&#x2F;madsciblog.tradoc.army.mil&#x2F;156-what-is-the-threshold...> responses are usually proportional to and in the same domain as the provocation reply mschuster91 48 minutes agorootparent> Or, put differently, we&#x27;re all hacking each other left and right and the responses have thus far mostly remained in the digital realm.Which is both good and bad at the same time. Cyber warfare has been significantly impacting our economies and our citizens - anything from scam callcenters over ransomware to industrial espionage - to the tune of many dozens of billions of dollars a year. And yet, no Western government has ever held the bad actors publicly accountable, which means that they will continue to be a drain on our resources at best and a threat to national security at worst (e.g. the Chinese F-35 hack).I mean, I&#x27;m not calling for nuking Bejing, that would be disproportionate - but even after all that&#x27;s happened, Russia and China are still connected to the global Internet, no sanctions, nothing. reply TeMPOraL 9 hours agorootparentprevSo Replit, Godbolt, and whatever other cloud-hosted compilers are there? replyderefr 11 hours agorootparentprevThis assumes that either 1. partners and interested sponsor-state state actors aren&#x27;t kept abreast Intel&#x27;s microcode backend architecture, or 2. that there hasn&#x27;t been at least one leak of this information from one of these partners into the hands of interested APT developers. I wouldn&#x27;t put strong faith in either of these assumptions. reply ajross 10 hours agorootparentIt does, but the same is true for virtually any such crash vulnerability. The question was whether this was a \"valuable exploit\", not whether it might theoretically be worse.The space of theoretically-very-bad attacks is much larger than practical ones people will pay for, c.f. rowhammer. reply ethbr1 8 hours agorootparent>> Getting to a vulnerability would require understanding exactly how the corrupted microcode state works, and that seems extremely difficult outside of Intel.Intel knows exactly how their ROB works.Therefore Intel knows the possible consequences of this bug and how to trigger them.If there is a privilege execution path from this, Intel knows. And anyone Intel chose to share it with knew.Thankfully, since it&#x27;s public now, the value of that decreases and customers can begin to mitigate. reply ajross 8 hours agorootparent> If there is a privilege execution path from this, Intel knows. And anyone Intel chose to share it with knew.No, or at least not yet. I mean, I&#x27;ve written plenty of bugs. More than I can count. How many of them were genuine security vulnerabilities if properly exploited? Probably not zero. But... I don&#x27;t know. And I wrote the code! reply saagarjha 5 hours agorootparentIntel said it can be used for escalation if that answers your question. reply lmm 4 hours agorootparentDid they confirm that it can definitely be used for escalation? The description I saw was \"may allow an authenticated user to potentially enable escalation of privilege and&#x2F;or information disclosure and&#x2F;or denial of service via local access\" which sounds like they&#x27;re covering all their bases and may not actually know what is and isn&#x27;t possible. reply saagarjha 14 hours agoprevSee also Intel’s advisory, which has a description of impact: https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;security-center&#x2F;advi...> Sequence of processor instructions leads to unexpected behavior for some Intel(R) Processors may allow an authenticated user to potentially enable escalation of privilege and&#x2F;or information disclosure and&#x2F;or denial of service via local access. reply yborg 3 hours agoparent&#x27;Some&#x27; appears to be almost any Intel x86 CPU made in the last 6 years. reply Borg3 1 hour agoprevUhm.. Why not padding using NOP ? Looks much more safer that slapping around random prefixes. reply tommiegannert 2 hours agoprevNice find. That indeed sounds terrible for anyone executing external code in what they believe to be sandboxes. Good thing it can be patched (and AFAICT, it seems to be a good fix, rather than a performance-affecting workaround.) reply jefc1111 13 hours agoprevThis was a lot more fun than the Google puff piece. reply bobim 14 hours agoprevIs it even possible to design a cpu with out-of-order and speculative execution that would have no security issue? Is the future leads to a swarm of disconnected A55 cores each running a single application? reply Tuna-Fish 13 hours agoparentThis vulnerability was not caused by OoO or speculative execution. It was caused by the fact that x86 was designed 45 years ago, and has had feature after feature piled on the same base, which has never been adequately rebuilt.The more proximate cause is that some instructions with multiple redundant prefixes (which is legal, but pointless) have their length miscalculated by some Intel CPUs, which results in wrong outcomes. reply gumby 11 hours agorootparent> It was caused by the fact that x86 was designed 45 years ago, and has had feature after feature piled on the same base, which has never been adequately rebuilt.Itanic would like to object! Unfortunately it can’t get through the door. reply epcoa 13 hours agorootparentprevNot entirely pointless, redundant prefixes are occasionally the useful method for alignment. reply TheCoreh 13 hours agorootparentA more sensible approach for that use-case would be IMO to have well-defined specialized prefixes for padding, instead of relying on the case-by-case behavior of redundant prefixes. (However I understand that there&#x27;s almost certainly a good historical reason why this was not the way it was done) reply kccqzy 12 hours agorootparentThe easiest way of doing padding is to add a bunch of `nop` instructions which are one byte each.If you read the manual, Intel encourages minor variations of the `nop` instructions that can be lengthened into different number of bytes (like `nop dword ptr [eax]` or `nop dword ptr [eax + eax*1 + 00000000h]`).It is never recommended anywhere in my knowledge to rely on redundant prefixes of random non-nop instructions. reply epcoa 11 hours agorootparentNOPs are not generally free.It&#x27;s a pretty old and well known technique:https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;48046814&#x2F;what-methods-ca...Note that this technique is really only legitimate where the used prefix already has defined behavior with the given instruction (\"Use of repeat prefixes and&#x2F;or undefined opcodes with other Intel 64 or IA-32 instructions is reserved; such use may cause unpredictable behavior.\"), and of course the REX prefix has special limitations. The key is redundant, not spurious. It is not a good idea to be doing rep add for example. But otherwise, there is no issue. reply epcoa 12 hours agorootparentprevThe prefixes are redundant so it&#x27;s not really case-by-case behavior. You&#x27;re just repeating the prefix you would be using anyway in that location.Using specialized prefixes wastes encoding space for no real gain. You realize on most common processors NOP itself is a pseudo-instruction? Even the apparently meme-worthy (see sibling comment) RISC-V, it&#x27;s ADDI x0, x0, 0. reply tedunangst 11 hours agorootparentAnd then there are CPUs that retcon behavioral changes onto nops.> Moving a register to itself is functionally a nop, but the processor overloads it to signal information about priority.https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;oldnewthing&#x2F;20180809-00&#x2F;?p=99... reply _a_a_a_ 10 hours agorootparent> A program can voluntarily set itself to low priority if it is waiting for a spin lockWhat does this even mean? How can a program do this when thread priority is an OS thing? It&#x27;s seems just weird. reply epcoa 9 hours agorootparentHardware threads as in SMT means thread priority is also a hardware thing. reply tedunangst 9 hours agorootparentprevIt&#x27;s an SMT CPU that dynamically assigns decode, registers, etc. https:&#x2F;&#x2F;course.ece.cmu.edu&#x2F;~ece740&#x2F;f13&#x2F;lib&#x2F;exe&#x2F;fetch.php?med... reply bobim 13 hours agorootparentprevAre new ISA solving this? Time to move to Risc V? reply dontlaugh 11 hours agorootparentRISC V is not great at this either, with the compression extension being common and variable length.ARM 64 gets this right, with fixed length 32 bit instructions. reply snvzz 5 hours agorootparent>ARM 64 gets this right, with fixed length 32 bit instructions.At the expense of code density, yet RISC-V is easy to decode, with implementations going up to 12-way decode (Veyron V2) despite variable length.ARM64 hardly \"gets it right\". reply camel-cdr 7 minutes agorootparentI wouldn&#x27;t say ARM64 gets it wrong either, I think both are viable approaches. epcoa 11 hours agorootparentprevN&#x2F;A and No. reply shadowgovt 9 hours agorootparentprevUsually, the historical reason is that adding the logic to do something well-defined when unexpected prefixes are used is going to cost ten more transistors per chip, which is going to add to cost to handle a corner case that almost nobody will try to be in anyway. Far better to let whatever the implementation does happen as long as what happens doesn&#x27;t break the system.The issue here is their verification of possible internal CPU states didn&#x27;t account for this one.(There is, perhaps, an argument to be made that the x86 architecture has become so complex that the emulator between its embarrassingly stupid PDP-11-style single-thread codeflow and the embarrassingly parallel computation it does under the hood to give the user more performance than a really fast PDP-11 cannot be reliably tested to exhaustion, so perhaps something needs to give on the design or the cost of the chips). reply iforgotpassword 12 hours agorootparentprevBecause they cost no&#x2F;less cycles compared to NOPs? reply tedunangst 12 hours agorootparentSee http:&#x2F;&#x2F;repzret.org&#x2F;p&#x2F;repzret&#x2F; reply akoboldfrying 13 hours agoparentprevWell, the bug in this specific case (based on the article by Tavis O. linked elsewhere in comments) looks to be the regular kind -- probably an off-by-one in a microcode edge case. That is, here it&#x27;s not the case that the CPU functions correctly but leaves behind traces of things that should be private in timing side channels, as was the case for Spectre. reply trebligdivad 12 hours agorootparentYeh just a fun bug rather than anything too fundamental. Still, it is a fun bug. reply lmm 4 hours agoparentprev> Is it even possible to design a cpu with out-of-order and speculative execution that would have no security issue?Yes, of course. But we&#x27;d have to put actual effort in, and realistically people wouldn&#x27;t pay enough extra to make it worthwhile. reply nextaccountic 13 hours agoparentprevI think formal methods could help designing of such machine, if you can write a mathematical statement that amounts to \"there is no side channel between A and B\"Or at least put a practical bound on how many bits per second at most you can from any such side channel (the reasoning being, if you can get at most a bit for each million years, you probably don&#x27;t have an attack)Then you verify if a given design meets this constraint reply tsimionescu 12 hours agorootparentFormal methods are widely used in processor design. It is hard to formalize specs to assert behaviors that bugs we haven&#x27;t thought about don&#x27;t exist. At least hard while also preserving the property of being a Turing machine. reply nextaccountic 12 hours agorootparentI know. I mean applying formal methods to this specific problem of proving side channels don&#x27;t exist (which seems a very hard thing to do and might even require to modify the whole design to be amenable to this analysis) reply less_less 11 hours agorootparentAs a tidbit, this was part of how one of the teams involved in the original Spectre paper found some of the vulnerabilities. Basically the idea was to design a small CPU that could be formally shown to be free of certain timing attacks. In the process they found a bunch of things that would have to change for the analysis to work... maybe in a small system those wouldn&#x27;t actually lead to vulnerabilities, but they couldn&#x27;t prove it (or it would require lots of careful analysis). And in big systems, those features do lead to vulnerabilities. reply nextaccountic 8 hours agorootparentThat&#x27;s amazing!Do you have some link about the designed CPU? reply bobim 13 hours agorootparentprevWhat would be the typical size of such a constraint-based problem, and do we have the compute power to translate the rules into an implementation? And what if one forgot a rule somewhere… Deeply interesting subject. reply less_less 12 hours agorootparentI think you&#x27;d want it to be a theorem (in Lean, Coq, Isabelle&#x2F;HOL or whatever) instead of a constraint problem. So it would be more limited by developer effort than by computational power.Theoretically you can do this from software down to (idealized) gates, but in practice the effort is so great that it&#x27;s only been done in extremely limited systems. reply mgaunard 13 hours agorootparentprevA program is itself a formal specification of what an algorithm does. reply SmoothBrain12 13 hours agoparentprevYes, but they won&#x27;t clock as fast because they&#x27;ll be waiting for RAM. reply bobim 13 hours agorootparentWe need to keep programs small so they fit in the cache. reply moffkalast 12 hours agorootparentWe need 2 GBs of L1 cache, thus solving the cache miss problem once and for all. reply rep_lodsb 12 hours agorootparent640K should be enough for anyone ;) reply JohnBooty 13 hours agoparentprevnext [–]Is the future leads to a swarm of disconnected A55 cores each running a single application?don&#x27;t you dare tease me like that reply bobim 11 hours agorootparentAnd programmed in… Forth! reply rep_lodsb 12 hours agoprevThe REX prefix is redundant for &#x27;movsb&#x27;, but not &#x27;movsd&#x27;&#x2F;&#x27;movsq&#x27; (moving either 32- or 64-bit words, depending on the prefix). That may have something to do with the bug, if there is any shared microcode between those instructions? reply farhanhubble 10 hours agoprevThis is such an interesting read, right in the league of \"Smashing the stack\" and \"row hammer\". As someone with very little knowledge of security I wonder if CPU designers do any kind of formal verification of the microcode architecture? reply saagarjha 5 hours agoparentYes. reply tasty_freeze 13 hours agoprevBenchmarking is always problematic -- what is a good representative workload? All the same, I&#x27;d be curious if the ucode update that plugs this bug has affected CPU performance, eg, it diverts the \"fast short rep move\" path to just use the \"bad for short moves but great for long moves\" version. reply akoboldfrying 13 hours agoparentIn the article by Tavis O. linked elsewhere in comments, he suggests disabling the FSRM CPU feature only as an expensive workaround to be taken only if the microcode can&#x27;t be updated for some reason. That suggests to me that he, at least, expects the update to do better. reply ReactiveJelly 12 hours agoparentprevThat would be the conservative thing to do. If there&#x27;s no limit on microcode updates, if I was Intel, I&#x27;d consider doing that first and then speeding it up again later. Based on the 5-second guess that people who update everything regularly will care that we did the right thing for security, and people who hate updates won&#x27;t be happy anyway, so at least the first update will be secure if they never get the next one.(I think there is a limit on microcode, they seem conservative to release new ones - I don&#x27;t remember the details) reply asylteltine 5 hours agoprevInteresting write up. The submission needs a better and more accurate title though reply quietpain 13 hours agoprevnext [–]...our validation pipeline produced an interesting assertion...What is a validation pipeline? reply ForkMeOnTinder 13 hours agoparentIt&#x27;s described one paragraph earlier.> I’ve written previously about a processor validation technique called Oracle Serialization that we’ve been using. The idea is to generate two forms of the same randomly generated program and verify their final state is identical. reply 1f60c 13 hours agorootparentSounds like the real story should be that Google solved the halting problem. :-P reply kadoban 12 hours agorootparentYou&#x27;re free to solve the halting problem for restricted sets of programs, that doesn&#x27;t break any rules of the universe.They also could be just discarding any where it runs for longer than X time, or a bunch of other possibilities. reply tonfa 13 hours agoparentprevThe blog has a link to https:&#x2F;&#x2F;lock.cmpxchg8b.com&#x2F;zenbleed.html#discovery which presents the concept. reply eigenform 10 hours agoprevI wonder which MCEs are being taken when this is triggered? reply quotemstr 12 hours agoprevIf the problem really is that the processor is confused about instruction length, I&#x27;m impressed that this problem can be fixed in microcode without a huge performance hit: my intuition (which could be totally wrong) is that computing the length of an instruction would be something synthesized directly to logic gates.Actually, come to think of it, my hunch is that the uOP decoder (presumably in hardware) is actually fine and that the microcoded optimized copy routine is trying to infer things about the uOP stream that just aren&#x27;t true --- \"Oh, this is a rep mov, so of course I need to go backward two uOPs to loop\" or something.I expect Intel&#x27;s CPU team isn&#x27;t going to divulge the details though. :-) reply blauditore 13 hours agoprevCan someone give a TL;DR for non-CPU experts? All technical articles seem pretty long and&#x2F;or complex. reply kmeisthax 12 hours agoparentx86 has a builtin memory copy instruction, provided by the combination of the movsb instruction and a rep prefix byte, that says you want the instruction to run in a loop until it runs out data to copy. This is \"rep movsb\". This instruction is fairly old, meaning a lot of code still has it, even though there&#x27;s faster ways to copy memory in x86.Intel added two features to modern x86 chips that detects rep movsb and accelerates it to be as fast as those other ways. However, those features have a bug. You see, because rep is a prefix byte, you can just keep adding more prefix bytes to the instruction (up to a maximum of 16 AFAIK). x86 has other prefix bytes too, such as rex (used to access registers 8-16), vex, evex, etc. The part of the processor that recognizes a rep movsb does NOT account for these other prefix bytes, which makes the processor get confused in ways that are difficult to understand. The processor can start executing garbage, take the wrong branch in if statements, and so on.Most disturbingly, when multiple physical cores are executing these \"rep rep rep rep movsb\" instructions at the same time, they will start generating machine check exceptions, which can at worst force a physical machine reboot. This is very bad for Google because they rent out compute time to different companies and they all need to be able to share the same machine. They don&#x27;t want some prankster running these instructions and killing someone else&#x27;s compute jobs. We call this a \"Denial of Service\" vulnerability because, while I can&#x27;t read someone else&#x27;s computations or change them, I can keep them from completing, which is just as bad. reply BlueTemplar 12 hours agorootparent> they all need to be able to share the same machineDo they ? As these issues keep piling up, it just seems that it&#x27;s not worth the hassle, and they should instead never do sharing like this... reply jrockway 12 hours agorootparentTo some extent, anyone with a web browser is sharing their machine with other people. That&#x27;s Javascript.If you ever download untrustworthy code and run it in a VM to protect your main set of data, that&#x27;s another case.The success of cloud computing is from the idea that multiple people can share the same computer. You only need one core, but CPUs come with 128, but with the cloud you can buy just that one core and share 1&#x2F;128th of the power supply, rack space, motherboard, ethernet cable, sysadmin time, etc. and that reduces your costs. That assumption is all based on virtualization working, though; nobody wants 1&#x2F;128th of someone else&#x27;s computer, they want their own computer that&#x27;s 1&#x2F;128th as fast. Bugs like these demonstrate that you&#x27;re just sharing a computer with someone, which is bad for the business of cloud providers. reply Arnavion 13 hours agoparentprevSome x86 instructions can have prefixes that modify their behavior in a meaningful way. Such a prefix can be applied generally to any instruction, but it&#x27;s expected to have no effect when applied to an instruction it doesn&#x27;t make sense with. But it turns out the CPU actually misbehaves in some cases when this is done. Intel released a CPU firmware update to fix it. reply Flow 14 hours agoprevWould be possible to describe a modern CPU in something like TLA+ to find all non-electrical problems like these? reply sterlind 13 hours agoparentI&#x27;ve heard Intel does use TLA+ extensively for specifying their designs and verifying their specs. But TLA+ specs are extremely high-level, so they don&#x27;t capture implementation details that can lead to bugs. And model checking isn&#x27;t a formal proof, only (tractably small) finite state spaces can be checked with TLC. And even there, you&#x27;re only checking the invariants you specified.That said, I&#x27;m sure there&#x27;s some verification framework like SPARK for VHDL, and this feels like exactly the kind of thing it should catch. reply dboreham 13 hours agoparentprevFormal methods have been used in CPU design for nearly 40 years [1] but not yet for everything, and the methods tend to not have \"round-trip-engineering\" properties (e.g. TLA+ is not actually proving validity of the code you will run in production, just your description of its behavior and your idea of exhaustive test cases).[1] https:&#x2F;&#x2F;www.academia.edu&#x2F;60937699&#x2F;The_IMS_T_800_Transputer reply foobiekr 9 hours agoparentprevCPU designers are so professional about verification and specification that they _dwarf_ software. There&#x27;s just no comparison. reply boxfire 14 hours agoparentprevThere are still bit flipping tricks like rowhammer for RAM, I wouldn&#x27;t be surprised if there are such vulnerabilities in some CPUs. reply sterlind 13 hours agorootparentRowhammer is an electrical vulnerability though. PP specified non-electrical vulns. reply varispeed 14 hours agoprevIt&#x27;s going to be a pain for cloud and shared hosting.Most likely dedicated resources on demand will be the future. Some companies already offer it. reply doublerabbit 14 hours agoprevAny reason to why its named after the dinosaur from the cartoon Rugrats? Or was that what was on TV at the time?Maybe I should start hacking while watching Teenage Mutant Ninja Turtles. reply AdmiralAsshat 13 hours agoparentIf you discover a major processor vulnerability and wanna name it Shredder&#x2F;Krang&#x2F;Bebop&#x2F;Rocksteady, I feel like you will have earned that right! reply Blackthorn 14 hours agoparentprevI think from the memey line \"Halt! I am Reptar!\" Plus the rep prefix reply 2OEH8eoCRo0 14 hours agoparentprevrep is an assembly instruction prefix reply frontalier 13 hours agoprevThe date on the article is for tomorrow? reply bitwize 13 hours agoparentCereal Killer: Check this out, it&#x27;s a memo about how they&#x27;re gonna deal with those oil spills on the 14th.Acid Burn: What oil spills?Lord Nikon: Yo, brain dead, today&#x27;s the 13th.Cereal Killer: Whoa, this hasn&#x27;t happened yet! reply yodon 13 hours agoprevDupe: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38268043(As of this writing, this post has more votes, the other has more comments) reply dang 12 hours agoparentWe&#x27;ll merge that one hither. Please stand by! reply ZoomerCretin 12 hours agoprevIntel is a known partner of the NSA. If Intel was intentionally creating backdoors at the behest of the NSA, how would they look different from this vulnerability and the many other discovered vulnerabilities before it? reply thelittleone 12 hours agoparentBut so is Google. It would be some very crafty theatrics if it&#x27;s all coordinated. reply ZoomerCretin 6 hours agorootparentOnly the people inserting the backdoor or using it would need to be bound by a National Security Letter&#x27;s gag order. I doubt anyone at Google (including those subject to NSL gag orders) was made aware of this specific vulnerability.# Google’s commitment to collaboration and hardware security## As Reptar, Zenbleed, and Downfall suggest, computing hardware and processors remain susceptible to these types of vulnerabilities. This trend will only continue as hardware becomes increasingly complex. This is why Google continues to invest heavily in CPU and vulnerability research. Work like this, done in close collaboration with our industry partners, allows us to keep users safe and is critical to finding and mitigating vulnerabilities before they can be exploited.There&#x27;s a tension between the NSA wanting backdoors and service providers (CPU designers + Cloud hosting) wanting secure platforms. It&#x27;s possible that by employing CPU and security researchers, Google can tip the scales a bit further in their favor. reply rep_lodsb 12 hours agoparentprevMy guess is that it would be something that could be exploited via JavaScript. And no JIT would emit an instruction like the one that causes this bug. reply tedunangst 12 hours agoparentprevHow would you distinguish this backdoor from one inserted by an unknown partner of the NSA? reply gosub100 12 hours agoparentprevthe backdoor would just be an encrypted stream of \"random\" data flowing right out the RNG. there&#x27;s some maxim of crypto that encrypted data is indistinguishable from random bytes. reply ShadowBanThis01 12 hours agoprevIs what? Another useless title. reply rvba 14 hours agoprev [–] It looks like Intel was cutting corners to be faster than AMD and now all those thigs come out. How much slower will all those processors be after multiple errata? 10%? 30%? 50%?In a duopoly market there seems to be no real competition. And yes I know that some (not all) bugs also happen for AMD. reply akoboldfrying 13 hours agoparentNot sure what other errata you&#x27;re referring to, but this looks like an off-by-one in the microcode. I would expect the fix to have zero or minimal penalty. reply arp242 13 hours agoparentprevIt&#x27;s not clear to me this fix will have any performance impact. I strongly suspect it will be negligible or zero.This seems like a \"simple\" bug of the type that people write every day, not deep architectural problems like Spectre and the like, which also affected AMD (in roughly equal measure if I recall correctly). reply kmeisthax 12 hours agorootparentParent commenter might be thinking of Meltdown, a related architectural bug that only bit Intel and IBM PPC. Everything with speculative execution has Spectre[0], but you only have Meltdown if you speculate across security boundaries.The reason why Meltdown has a more dramatic name than Spectre, despite being the same vulnerability, is that hardware privilege boundaries are the only defensible boundary against timing attacks. We already expect context switches to be expensive, so we&#x27;re allowed to make them a little more expensive. It&#x27;d be prohibitively expensive to avoid leaking timing from, say, one executable library to a block of JIT compiled JavaScript code within the same browser content process.[0] https:&#x2F;&#x2F;randomascii.wordpress.com&#x2F;2018&#x2F;01&#x2F;07&#x2F;finding-a-cpu-d... reply mschuster91 13 hours agoparentprev [–] > And yes I know that some (not all) bugs also happen for AMD.Some of these novel side-channel attacks actually even apply in completely unrelated architectures such as ARM [1] or RISC-V [2].I think the problem is not (just) a lack of competition (although you&#x27;re right that the duopoly in desktop&#x2F;laptop&#x2F;non-cloud servers for x86 brings its own serious issues, I&#x27;ve written and ranted more often than I can count [3]), it rather is that modern CPUs and SoCs have simply become so utterly complex and loaded with decades worth of backwards-compatibility baggage that it is impossible for any single human, even a small team of the best experts you can bring together, to fully grasp every tiny bit of them.[1] https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;arm-cpus-impacted-by-rare-side...[2] https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S004579062...[3] https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&page=0&prefix=true&que... reply snvzz 5 hours agorootparent>Some of these novel side-channel attacks actually even apply in completely unrelated architectures such as ARM [1] or RISC-V [2].Possible? Yes. But far less likely.Complexity carries over and breeds bugs. RISC-V is an order of magnitude simpler than ARM64, which in turn is an order of magnitude simpler than x86.And it is so w&#x2F;o disadvantage[0], positioning itself as the better ISA.0. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38272318 reply bobim 12 hours agorootparentprev [–] So no saving grace from the ISA… humans just lost ground on CPU design, and I suspect the situation will worsen when AI will enter the picture. reply mschuster91 12 hours agorootparent [–] > and I suspect the situation will worsen when AI will enter the picture.For now, AI lacks the contextual depth - but an AI that can actually design a CPU from scratch (and not just rehashing prior-art VHDL it has ... learned? somehow), if that happens we&#x27;ll be at a Cambrian Explosion-style event anyway, and all we can do is stand on the sides, munch popcorn and remember this tiny quote from Star Wars [1].[1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Xr9s6-tuppI reply nwmcsween 8 hours agorootparent [–] Once AI can create itself, we will most likely be redundant. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Multiple research teams at Google have discovered a bug in certain processors that can cause them to enter a glitch state, resulting in unpredictable behavior and system halt.",
      "The bug involves the use of redundant rex prefixes and the fast short repeat move (FSRM) feature.",
      "Intel has released updated microcode and a workaround to address this vulnerability, which has serious security implications for cloud providers."
    ],
    "commentSummary": [
      "\"Reptar\" is a CPU vulnerability that triggers machine check exceptions and processor halts, which has been discovered by Google and other research teams.",
      "The article discusses the impact of \"Reptar\" on AMD CPUs and highlights the challenges faced in CPU design.",
      "Another vulnerability affecting Intel processors is mentioned, potentially causing system crashes. The article also explores the use of formal methods in CPU design and the potential disruptions from sharing computing resources. The complexity of modern CPUs and the potential influence of AI on CPU design are also mentioned."
    ],
    "points": 468,
    "commentCount": 150,
    "retryCount": 0,
    "time": 1699984183
  },
  {
    "id": 38262124,
    "title": "Google sues individuals for using fraudulent DMCA takedown notices to crush competition",
    "originLink": "https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/",
    "originBody": "HOME > LAWSUITS > Two men who allegedly used 65 Google accounts to bombard Google with fraudulent DMCA takedown notices targeting up to 620,000 URLs, have been named in a Google lawsuit filed in California on Monday. Google says the men weaponized copyright law's notice-and-takedown system to sabotage competitors' trade, while damaging the search engine's business and those of its customers. While all non-compliant DMCA takedown notices are invalid by default, there’s a huge difference between those sent in error and others crafted for purely malicious purposes. Bogus DMCA takedown notices are nothing new, but the rise of organized groups using malicious DMCA notices as a business tool has been apparent in recent years. Since the vast majority of culprits facing zero consequences, that may have acted as motivation to send more. Through a lawsuit filed at a California court on Monday, Google appears to be sending the message that enough is enough. Defendants Weaponized DMCA Takedowns Google’s complaint targets Nguyen Van Duc and Pham Van Thien, both said to be residents of Vietnam and the leaders of up to 20 Doe defendants. Google says the defendants systematically abused accounts “to submit a barrage” of fraudulent copyright takedown requests aimed at removing their competitors’ website URLs from Google Search results. “Defendants have weaponized copyright law’s notice-and-takedown process and used it not for its intended purpose of expeditiously removing infringing content, but instead to have the legitimate content of their competitors removed based on false allegations. Defendants’ illegal, fraudulent scheme harms consumers, third-party businesses, and Google; stifles competition; and threatens to tarnish Google’s trusted brand.” Over the past few years, Nguyen, Pham and those working with them, are said to have created at least 65 Google accounts to send confirmed bogus notices targeting 117,000 URLs, plus another 500,000 URLs via notices that Google suspects are fraudulent too. “Defendants appear to be connected with websites selling printed t-shirts, and their unlawful conduct aims to remove competing third-party sellers from Google Search results. Defendants have maliciously and illegally exploited Google’s policies and procedures under the DMCA to sabotage and harm their competitors,” the complaint adds. Google Aims to Put an End to Abuse, Hold Defendants Accountable Google goes on to highlight its position as a major intermediary that processes DMCA notices targeting 600 million URLs every year, and the requirement under the DMCA to remove or disable content notified as allegedly infringing. If the company fails to act expeditiously once in receipt of a DMCA notice that complies with the statutory requirements, the company risks losing its safe harbor protection, Google notes. Since Google must often rely on the accuracy of statements made in DMCA notices, fraudulent notices can result in content being wrongfully taken down. That damages the company’s search engine advertising business, and the business Google’s customers hoped to attract. In this matter, the defendants’ embarked on a campaign that exploited Google’s systems and the DMCA takedown process to undermine their competitors. Fake Names, Fraudulent Representations The misrepresentations in notices sent to Google were potentially damaging to other parties too. Under fake names, the defendants falsely claimed to represent large companies such as Amazon, Twitter, and NBC News, plus sports teams including the Philadelphia Eagles, Los Angeles Lakers, San Diego Padres. In similarly false notices, they claimed to represent famous individuals including Elon Musk, Taylor Swift, LeVar Burton, and Kanye West. The complaint notes that some notices were submitted under company names that do not exist in the United States, at addresses where innocent families and businesses can be found. Google says that despite these claims, the defendants can be found in Vietnam from where they proudly advertise their ‘SEO’ scheme to others, including via YouTube. “Bad actors like Defendants use this tactic to attack and fraudulently suppress competitors’ websites and products in Google Search results, making consumers more likely to buy the same or similar products from the bad actors or their affiliates,” the complaint continues. “Such bad actors know that a fraudulent takedown request often has the same effect as a legitimate one; if a takedown request contains all the elements required under Section 512(c)(3)(A), it likely will trigger removal by Google. “Unfortunately, to ensure compliance with the DMCA and in reliance on the information submitted in Defendants’ takedown requests, Google’s system removed a significant number of thirdparty website URLs targeted by Defendants for a period of time before Google and/or the websites’ owners figured out what was going on and took appropriate steps to reinstate the URLs.” A particularly damaging batch of fraudulent notices targeted more than 35,000 URLs operated by a Google customer that spends tens of millions of dollars per year on Google search ads. The effect was a significant drop in traffic during the holiday season, revenue losses for the customer and its sellers of $5 million, and a loss to Google of between $2 and $3 million. Holding Defendants Accountable Those who knowingly make false statements in a DMCA notice can be held liable for damages, costs, and attorneys’ fees. In this matter the defendants’ conduct is said to have caused Google to suffer economic harm due to lost advertising revenue, damage to business relations, and the allocation of significant resources to investigate their wrongdoing. Google seeks attorneys’ fees and damages under 17 U.S.C. §512(f), in an amount to be determined at trial. The complaint adds that when the defendants created dozens of Google accounts, each time they entered into enforceable agreements with Google. While Google says it has “performed all its obligations” under those contracts, the actions of the defendants amount to breaches of their contractual obligations to Google and intentional interference in contractual relationships between Google and its advertising customers. Google says the defendants should be required to pay all general, special, and actual damages that Google “has sustained or will sustain” due to the fraudulent notices. Google further requests an order to restrain the defendants (and anyone working in concert with them), from submitting any further fraudulent takedown notices and/or creating any Gmail accounts. Google also wants a ban on the defendants using any of its products or services to advertise their websites or products. The complaint is available here (pdf)",
    "commentLink": "https://news.ycombinator.com/item?id=38262124",
    "commentBody": "Google sues two men who weaponized DMCA notices to crush competitionHacker NewspastloginGoogle sues two men who weaponized DMCA notices to crush competition (torrentfreak.com) 371 points by HieronymusBosch 22 hours ago| hidepastfavorite168 comments RecycledEle 18 hours agoGreat, now do the Dallas and DeSoto, TX police departments who took down my videos of police brutality and violent home invasions with phony take-down notices to YouTube in addition to destroying my computers and cameras and stealing computer discs with video of their misconduct from the US Mail with the help of (then DeSoto post master Linda H. Norwood. reply rc_mob 18 hours agoparentHave you written about this story anywhere? I feel like multiple subreddits would be very interested in this story. reply galangalalgol 16 hours agorootparentYeah, that went federal. Very interesting. reply kolanos 14 hours agorootparentprevNot to mention prominent channels on YouTube that cover police matters like this. [0][0]: https:&#x2F;&#x2F;www.youtube.com&#x2F;@LackLusterMedia reply gamblor956 14 hours agoparentprevIf true, that story would make national news...In additional to the state-level crimes committed by local law enforcement, your claim suggests a number of federal crimes as well. reply qingcharles 10 hours agorootparentI know prosecutors, police officers who have committed multiple state and federal crimes. I&#x27;ve tried contact dozens of media outlets and they are all like \"Mmkay\". reply gamblor956 7 hours agorootparentI agree with the media outlets on this...Lots of people \"know\" law enforcement that have \"committed\" multiple \"crimes.\" But when asked what the crimes were, they can&#x27;t actually articulate any specific acts, or what they do articulate aren&#x27;t crimes, or they don&#x27;t have any evidence supporting their claims.Media outlets love to report on public corruption. It&#x27;s what gets journalists awards and fame. If they don&#x27;t believe you, it&#x27;s not because they&#x27;re part of some conspiracy, it&#x27;s because you haven&#x27;t given them enough evidence to make it worth their while to investigate. reply smoldesu 17 hours agoparentprevYou probably want to host that sort of content yourself. If your only backup is on Google&#x27;s server, you have to prepare for a reality where they get rid of it. reply binarymax 16 hours agorootparentSounds like they did, and it was destroyed&#x2F;stolen? reply rokkitmensch 13 hours agorootparentNot a bad use case for backing up to other people&#x27;s computers. reply viknesh 16 hours agoprevI think a meta point that is interesting here is that nobody could be more motivated to lobby for DMCA changes than large platforms like Google. Wrongfully targeted victims take out their anger on the platform, who thus far wrings their hands and says \"sorry, it&#x27;s the law, file a counterclaim if you want.\"If it&#x27;s enough of a problem for the platform (and not the victims), they can just pay off their congresspeople to write a law allowing DMCA abuse detection systems. reply mqus 15 hours agoparentonly in this case it mostly google which makes matters worse. They don&#x27;t or badly check DMCA claims, don&#x27;t monitor or publish the filers that are most frequently counterfiled against and to top it off: mostly use their own contentid system anyway which heavily favors ip holders. reply mjevans 15 hours agorootparentGoogle, nor any other corp, should be a Judge on a legal matter. That&#x27;s why a Judge should be involved.The entire idea of of the DCMA is just massively flawed and biased towards a world where only big publishers and big criminal enterprises exist. reply bruce511 14 hours agorootparentThere are 600 million DCMA notices to Google along every year. Let&#x27;s be insanely Conservative and assume there are a billion across all platforms.Let&#x27;s say a judge can review a notice every 5 minutes. 12 an hour, say 100 in a day. Say 20 000 per year. So 50 per million, 50 000 judges per billion requests.Let&#x27;s say a judge costs 100k per year. With say another 100k in costs. That&#x27;s a price tag of 10 billion per year. I&#x27;m guessing the real number is likely well over double that.So your plan to include humans, never mind actual judges who went to law school and so on, is, well a non-starter. reply 0xffff2 14 hours agorootparentThis is an argument to re-write the DMCA to prevent frivolous claims, and thus reduce the total number of claims. It&#x27;s not an argument to throw up our hands and pretend that it&#x27;s unreasonable to expect human involvement. reply s1artibartfast 14 hours agorootparentI don&#x27;t think anyone is advocating for the status quo, but that doesn&#x27;t mean human involvement is required either at an initial stage.One proposal is that you should need full legal identification to file a takedown claim, and be on the hook for damages and penalties if is malicious. This would solve 99.999 of the problem. reply devrand 11 hours agorootparent> One proposal is that you should need full legal identification to file a takedown claimHow exactly? Require a government issued ID? What if it&#x27;s fake? How would you validate that the ID is authentic? Even if it is authentic, how do you ensure that person has proper rights to issue the takedown?> and be on the hook for damages and penalties if is malicious.How? They could either not be who they say they are and&#x2F;or not located in the United States. Like in this exact case, the two defendants are located in Vietnam. There&#x27;s zero chance they&#x27;re going to show up, so it&#x27;ll result in a default judgement that will never be collected on.The best suggestion I&#x27;ve seen so far is to require an escrow deposit on takedowns that is forfeit on fraudulent&#x2F;malicious claims. However, this then raises the issue of who would determine that. Also, this deposit could potentially tie up a lot of money of legitimate claimants, becoming a financial burden for them and preventing them from issue further claims (which further adds to that burden). reply jbishop156 5 hours agorootparent> How? They could either not be who they say they are and&#x2F;or not located in the United States.Require either US Identity papers or a corporate identity that is registered in the US in order to file a DMCA (orid&#x2F;corpid for filing acopyright claim)Furthermore, tweaking the reporting times for DMCA would help: 1. Claim made, soft-takedown immediately (delist but don&#x27;t remove) 2. Proceed to hard-takedown after 24 hrs if no counterclaim is made. 3. Counterclaim made, reinstate and inform original claimant. 4. Original Claimant can then either sue or obtain a court-ordered injunction. 5. Optionally Claimant can pay a nominal fee for \"human decision-making\" by Google or a mutually agreeable arbitrator. 6. Respondent has 14 days to file their own nominal fee to move it to arbitration or can proceed to countersue. reply throwaway2037 4 hours agorootparentI like this idea. It adds enough friction to scare away most scammers. reply s1artibartfast 9 hours agorootparentprev>How exactly? Require a government issued ID? What if it&#x27;s fake? How would you validate that the ID is authentic? Even if it is authentic, how do you ensure that person has proper righgtsThere is an infinite list depending on how strict you want to be. They could require government ID. They could require a notary. They could require a court order.There will always be a balance between ease of takedown for legitimate claimants vs fighting false claims. reply jakderrida 11 hours agorootparentprev> and be on the hook for damages and penalties if is malicious.Honestly, I think both conditions might be met with an already vetted credit card number, which obviously involves an associated identification of a person or company.When signing up for cloud service providers, I&#x27;m always terrified that I&#x27;ll leave something on and incur a massive bill. Basically, every cloud service move I treat as if I&#x27;m walking on eggshells. I&#x27;d assume those issuing a DMCA would end up the same way. reply qingcharles 10 hours agorootparentprevPetty technicality, but most jurisdictions in the USA don&#x27;t require judges to have gone to law school, passed the bar or even know how to read. reply thomastjeffery 10 hours agorootparentprevYou mean you actually have to pay a significant number of humans to run a giant monolithic content platform responsibly?!But our profits! reply nerdponx 14 hours agorootparentprevFlood the system as a form of protest. reply advisedwang 14 hours agorootparentprevThe DMCA does not give service providers leeway to check DMCA claims. Ignoring a correctly filed notification, even if it is garbage, prevents a service provider getting the relief from liability.Source: 17 USC 512(c) reply nerdponx 14 hours agorootparentAnd \"Big Copyright\" prefers it that way. They are only interested in maximizing sensitivity (probability of taking down a true violation), they have no incentive to care about the trade-off with precision (probability that a taken-down violation is a true violation). reply kevingadd 14 hours agorootparentprevThey process incorrectly filed notices too, for example notices that don&#x27;t contain the information required. I had my content taken down once in response to a notice that just had a person&#x27;s name and no other information (they were not the rightsholder) reply withinboredom 15 hours agorootparentprevYeah, and what is funny, is that it totally misses content on smaller artists while incorrectly IP on older, open-domain, classical music performed by smaller artists (think high-school bands performing Beethoven). reply rsingel 15 hours agorootparentprevAlso the DMCA doesn&#x27;t require Google to takedown sites based on obviously fraudulent DMCA notices.It&#x27;s just that Google would lose its liability shield if the claim turned out to be real. reply notatoad 14 hours agorootparenti thought that was the main flaw with the DMCA - it actually does require you to obey all properly formatted and submitted DMCA notices, regardless of how obviously fraudulent they are, until a court decides they&#x27;re not fraudulent. reply dragonwriter 14 hours agorootparentDMCA doesn&#x27;t require you to do anything, because it only shields you against liability you would otherwise have to the filer (or the owner whose agent they are), and if the notice is obviously not from thr owner or agent, or obviously incorrect about the content being infringing (or even existing), then there is no source of liability.Yes, it&#x27;s cheaper not to check, but its not required by the DMCA. reply s1artibartfast 13 hours agorootparentHonest question, Does the DMCA allow hosts to require proof before action?If their liability isn&#x27;t contingent on being provided valid evidence beyond a simple statement, then hosts don&#x27;t have much of an ability to perform an assessment. reply dragonwriter 12 hours agorootparent> Honest question, Does the DMCA allow hosts to require proof before action?The DMCA (well, the Safe Harbor provision under discussion, the DMCA has lots of other provisions that are irrelevant to the discussion) doesn&#x27;t require action in the first place, so it necessasrily allows anything before action.> If their liability isn&#x27;t contingent on being provided valid evidence beyond a simple statement, then hosts don&#x27;t have much of an ability to perform an assessment.The DMCA doesn&#x27;t create liability in the first place. reply s1artibartfast 12 hours agorootparentWhat stops the Google&#x27;s of the world from simply ignoring every dmca takedown request? Alternatively what do you think prevents them from requiring rigorous proof in a machine readable format before honoring a request? What do you think the incentives at play are? reply dragonwriter 12 hours agorootparent> What stops the Google&#x27;s of the world from simply ignoring every dmca takedown request?The same thing that leads them to make deals with big money copyright-based industries for copyright systems that go far beyond what the DMCA safe harbor requires for situations it covers: the fact that they are deeply and actually aware of the copyright violation they facilitate (putting them outside of the DMCA safe harbor to start with), and rely almost entirely on the fact that that is hard and expensive to prove (which the DMCA negates, but which forces them to go beyond the DMCA for industries for which the cost and expense of proving actual knowledge would otherwise be worthwhile.) reply mr_toad 7 hours agorootparentprev> What stops the Google&#x27;s of the world from simply ignoring every dmca takedown request?They become liable for copyright infringement if the work in question is actually in violation of copyright.You’d want to be very sure that a work you were hosting wasn’t in violation of copyright before outright ignoring a DCMA request. reply thomastjeffery 10 hours agorootparentprevGoogle&#x27;s primary customers are rights-holders: particularly giant media corporations and very-popular youtubers. reply s1artibartfast 4 hours agorootparentShouldn&#x27;t giant media corporations also care about illegitimate take down notices against them? reply thomastjeffery 4 hours agorootparentAll of their content is on a totally isolated platform like Netflix or YouTube Red.The law binds the out group to profit the in group. reply mrguyorama 12 hours agorootparentprevThe lawsuit with Viacom in the mid 2000s that could have basically killed the entire platform over copyright complaints with big studios. ContentID was a direct result of that lawsuit with the explicit goal of making sure google would never be held responsible for copyright violations.The vast majority of \"Copyright claims\" on youtube are not DMCA takedowns.Google doesn&#x27;t require proof or ask for evidence or anything because they don&#x27;t give a fuck about the rights of a creator. The entire goal of youtube&#x27;s copyright programs are about the continuation of youtube as a profitable entity. They do that by purposely making it easy for big rightsholders to get what they want, and in the process have made it easy for strangers to bogus claim things and possibly get advertising revenue for a bit unless the creator has enough clout and access to youtube support reps to get it fixed. reply s1artibartfast 4 hours agorootparentDon&#x27;t big rights holders also care about having their content taken down? replythomastjeffery 10 hours agoparentprevGoogle has another set of motivation: rights-holders are its primary customers.It&#x27;s pretty clear at this point which motivator has won Google&#x27;s favor. reply arlattimore 12 hours agoprev> Our lawsuit targets bad actors who set up dozens of Google accounts and used them to submit thousands of bogus copyright claims against their competitors. These fraudulent claims resulted in removal of over 100,000 businesses’ websites, costing them millions of dollars and thousands of hours in lost employee time.So Google needs to remove the sites first, likely to be legally compliant themselves, hurt the businesses in question, then sue to be able to re-instate the websites that they knew shouldn&#x27;t have been taken down in the first place? reply resolutebat 11 hours agoparentYes, but blame the DMCA, because this is literally what it requires content providers like Google to do:> OSP must accommodate and not interfere with \"standard technical measures.\" \"Standard technical measures\" are defined as measures that copyright owners use to identify or protect copyrighted works, that have been developed pursuant to a broad consensus of copyright owners and service providersBasically, if somebody claiming to be a copyright owner (even if they aren&#x27;t) files a claim saying they have copyright over someting (even if they don&#x27;t) using \"standard technical measures\", Google \"must accommodate and not interfere\", meaning they need to take down first and ask questions later. reply prox 10 hours agorootparentWhat a shitty law, how did that get passed? Or is there something I missing from all this? reply mr_toad 8 hours agorootparentCopyrights are basically a long lineage of shitty laws on top of older shitty laws all due to people lobbying to become rent seekers, most on works that they themselves never created. reply yieldcrv 10 hours agorootparentprevUnanimously approved by Congress to implement a treaty obligationas this was the mid-late 1990s, both the law and the treaty obligation did not consider the future state of the internet, and it mostly focuses on physical but electronic mediumsdistinctive bills to patch the DMCA have largely failed due to general partisan paralysis in Congress for decades now, it simply needs to be a rider in must-pass bills but so far it hasnt been reply prox 10 hours agorootparentI see, so it never got updated properly?Also knowing how little the average politician probably knows about this subject I don’t see it changing. reply berniedurfee 7 hours agorootparentprevFollow the money. reply dragonwriter 11 hours agoparentprevIf they are following DMCA procedures and notifying the targets, they can (even if there would be actual copyright liability otherwise) restore without risk 10 days after getting and notifying the complainant about a counternotice, at which point the copyright claimant needs to sue the end user.(Some hosts are lax about the end-user side of DMCA safe harbor process, because they have no user-side liability that they feel the need to avoid, and there&#x27;s no other benefit to them, besides PR, for taking any action after a takedown.) reply kccqzy 11 hours agoparentprevI&#x27;m not an expert in DMCA but yes I believe that to be the case. The only recourse for the victims is to file a counter-notice, which obligates the service provider to start a waiting period. Then the victim has to wait out that period before having the content reinstated. reply Zigurd 11 hours agoparentprevIt would be fair to say google should put more work into vetting DMCA takedowns, but DMCA is a bad law, and it is enforced badly. A long term coordinated effort at fraudulent takedowns should be criminal in several ways: The false report should be criminal. It should be a sworn statement. There should be a statutory penalty against the reporting party. Reporting parties should be bonded on a sliding scale. The criminality should not be able to hide behind the corporate veil. reply Twirrim 11 hours agorootparent> It would be fair to say google should put more work into vetting DMCA takedownsThey&#x27;re effectively not allowed to, by the law, that forces them to assume every take down is good, or they end up at legal risk of being liable for the content. DMCA is just fundamentally broken. reply gunapologist99 11 hours agorootparentprevGoogle is suing, and it&#x27;s not even the harmed party. Obligatory IANAL disclaimer applies, but surely a strong first-party case could be made for, at least, tortious interference? reply dragonwriter 10 hours agorootparent> Google is suing, and it&#x27;s not even the harmed party.Google is a harmed party; the false reporterts have engaged in the crime of theft (of services&#x2F;labor) by false pretenses against Google.(Its true that that crime was merely instrumental to their goal of harming other businesses, but its a real crime and a real harm to Google.) reply blacksmith_tb 9 hours agorootparentprev\"A particularly damaging batch of fraudulent notices targeted more than 35,000 URLs operated by a Google customer that spends tens of millions of dollars per year on Google search ads. The effect was a significant drop in traffic during the holiday season, revenue losses for the customer and its sellers of $5 million, and a loss to Google of between $2 and $3 million.\" So you&#x27;d think the competitor firms could certainly sue also, if they were so inclined. reply Dwedit 19 hours agoprevThis is one of the few cases where the \"perjury rule\" actually matters. The \"perjury rule\" says that it is considered perjury to falsely claim that your are authorized by the rights holder to make the DMCA takedown request. Contrary to popular belief, the perjury rule does not say that it&#x27;s perjury to falsely identify something as the copyrighted work. reply abtinf 17 hours agoparentThat’s an interesting distinction. And, I would think, a much broader rule. It is inclusive of the “popular belief”: if you falsely identify a work as copyrighted, but it’s not, then you are misrepresenting your authorization because there is no way for you to have obtained it. reply gmiller123456 16 hours agorootparentSadly, that&#x27;s not how it works. The claimant just has to have authorization on behalf of the work they are claiming is being infringed upon. Essentially making this part of the law useless. IMHO, it was just added to make the layman think there is some protection or responsibility on behalf of the people making the claim. reply nashashmi 15 hours agorootparentNot sure why the sad unfortunate face here.If you know of a work that is copyrighted and you find this work on a website that is not given the license to the copyright, and you are not the copyright holder but instead the copyright holder is your cousin, you should not have the authority to issue a DMCA takedown noticem, even if you may be aware of who your cousin gave the license to.Instead, only your cousin should have this authority and the people your cousin assigns. reply notatoad 14 hours agorootparenti think the scenario prompting the sadface is that the law doesn&#x27;t prevent your cousin from picking any random URL on the internet and claiming that his copyright (which he does legitimately own the rights to) is being infringed on that page. even if he knows perfectly well that it&#x27;s not his content on the page.what the parent comments seem to be saying is:not perjury: \"i own this content (true) and it&#x27;s available at this unauthorized url (false) and i want it taken down\"perjury: \"i own this content (false) and it&#x27;s available at this unauthorized url (true) and i want it taken down\"but both statements have the same effect of getting the url taken down. reply Terr_ 14 hours agorootparentprevThe problem is there&#x27;s no penalty for blatantly lying about whether the target content is infringing or even related. reply thomastjeffery 10 hours agoparentprevUnfortunately, perjury only applies in court. DMCA is extrajudicial. reply nhubbard 17 hours agoprevWow, this is probably the most brazen misuse of the DMCA process I&#x27;ve seen in a while. Actively advertising on YouTube how merchants can file fake DMCA claims with Google to improve their business is insane. reply jrockway 16 hours agoparentThis is kind of a tragedy of the commons situation that works for everyone. Because DMCA abuse is so rampant, it encourages lawmakers to come up with another system. reply kevingadd 14 hours agoparentprevPart of why this works is that Google refuses to provide you information on claimants if you want to sue them, presumably because it&#x27;s more work for their customer service staff to respond to requests like that reply dragonwriter 14 hours agorootparentSo, file a vs. John Doe lawsuit and then subpoena Google for the idebtifying information they have.That&#x27;s what copyright owners do, in reverse, when providers don&#x27;t give customer information (which they generally don&#x27;t.) reply qingcharles 10 hours agorootparentExactly. The court will have to allow you some discovery to find the defendants. reply Coreleen 16 hours agoprevI very rarely feel like I take the side of Google, but in this case, feel like this is a win for everyone and not just the big guy reply phpisthebest 15 hours agoparentDont give them too much credit, Notice they did not go after any of the big media companies that are just as abusive.They picked a small time organization to target, they would not want to upset advertisers, hell I bet they got the blessing of the big media companies to even go after these guys..If they really wanted side with creators they would push back against their RIAA and MPAA abusers, but of-course they will not reply NPC82 11 hours agorootparentMy understanding is if Google wins it would still set a legal precedent, which would help future lawsuits with this sort of thing. Of course not many criminals make a YouTube video on how they execute their own crimes... that would certainly help. reply bryanrasmussen 21 hours agoprevI believe this is what everyone has been suggesting someone do about fake DMCA for a while so, although it pains me to say it, thank you Google! reply jasode 20 hours agoparent>I believe _this_ is what everyone has been suggesting someone do about fake DMCA for a while so,The problem is the \"_this_\" in this thread&#x27;s example is a full-blown lawsuit by Google. Yes, this is a change from just blindingly obeying DMCA notices but filing lawsuits is not scalable to apply to all the other frivolous and illegitimate takedown requests. These 2 men just did it in high enough volume to attract the attention of Google&#x27;s lawyers.An example of all the other illegitimate DMCA takedowns \"flying under the radar\"...A few years ago, I was selling some Total Training DVD tutorials of Adobe products on ebay like these: https:&#x2F;&#x2F;www.ebay.com&#x2F;sch&#x2F;i.html?_nkw=total+training+adobe+cr...Ebay then mysteriously cancelled my listing after a few days. Why? Because Ebay has a VeRO (Verified Rights Owners) program and Total Training sent a DMCA takedown request to remove my legitimate listing. I then sent a harsh email with my invoice to Total Training that proved I bought the DVDs directly from them so they are not counterfeit pirate copies and warned them not to interfere when I re-listed it. They didn&#x27;t takedown my listing so I was finally able to sell it. Total Training wasn&#x27;t really trying to stop piracy; their real intention was to prevent lower-priced used DVDs from competing with the sales of their new DVDs by intimidating sellers on ebay.That&#x27;s the type of small-scale DMCA abuse that happens constantly on Ebay, Youtube, Google search results page, etc. Abuse the DMCA to their advantage but not enough to attract lawsuits. reply hnbad 21 hours agoparentprevFalse DMCA claims are technically considered perjury I think. The problem is that most so-called DMCA claims are actually not using the DMCA mechanism at all: filing a bogus copyright claim on YouTube for example doesn&#x27;t have to involve the DMCA at any point. reply tanaros 19 hours agorootparent> False DMCA claims are technically considered perjury I think. The problem is that most so-called DMCA claims are actually not using the DMCA mechanism at all […]The other issue is that, if I recall correctly, the perjury penalty is not about whether the claim is accurate. It’s about whether you actually represent the rights holder of the content you claim is being infringed.That is, if I make a DMCA claim that says this video infringes on Frozen, it’s perjury only if I’m not the rights holder (or representative thereof) for Frozen. reply sumtechguy 20 hours agorootparentprevI have heard that perjury thing before. However, what is the actual consequence of doing that? Also in this particular type of thing? Is it a &#x27;liar liar pants on fire&#x27; designation, or some sort of fine, or time in jail? What do the actual statutes say, and what is the case law here? reply lazide 20 hours agorootparentIn California, perjury is a felony punishable by 2,3, or 4 years in jail. [https:&#x2F;&#x2F;leginfo.legislature.ca.gov&#x2F;faces&#x2F;codes_displaySectio....]Theoretically.[https:&#x2F;&#x2F;leginfo.legislature.ca.gov&#x2F;faces&#x2F;codes_displayText.x...]It’s also explicitly listed as a deportable offense for Green Card holders. reply beeboobaa 18 hours agorootparentLet me know when Sony has been put in jail for 2 years. So far they&#x27;ve been getting away with bogus DMCA notices for years. reply Sohcahtoa82 14 hours agorootparentYou, like many others, are misunderstanding the \"perjury\" part of the DMCA.It&#x27;s only perjury if you&#x27;re don&#x27;t have legal authority to act on behalf of the company issuing the DMCA takedown.In other words, Sony falsely claiming to have ownership over something is not perjury. Some random person claiming to be working for Sony when they don&#x27;t and claiming ownership and sending a DMCA takedown is. reply lazide 10 hours agorootparentprevNear as I can tell, almost no one wants to prosecute Perjury.Partially because lawyers are so good at weaseling out of it, but secretly I think it’s because lawyers are afraid of what would happen if it was consistently applied.For Family court in particular, I imagine it could turn 1&#x2F;4 of the state into felons.The only cases I’m aware of were very high profile, ‘successful’ (as in folks believed it, and took action based on it) lies under oath with severe and high profile damages, that later evidence clearly showed was done in bad faith.And by severe and high profile damages, I don’t mean something as simple as a single (non famous) person being dead when they didn’t have to be, unless it’s a huge scandal anyway.[https:&#x2F;&#x2F;www.jstor.org&#x2F;stable&#x2F;1142605] reply wredue 15 hours agorootparentprevSony lawyers represent Sony so don’t fall under the perjury rule. reply qingcharles 10 hours agorootparentprevPerjury cases are very, very rarely prosecuted even in cases where the violation is blatant (from my experience).Occasionally the system works, though. I remember one suspect I knew defending himself against what he said were totally bogus domestic violence charges. The woman came and testified against him and he lost. A week after the trial, and before his sentencing, his lawyer came and told him the government had just disclosed that they had discovered the complainant had three separate perjury convictions in three other states for being caught lying in domestic violence testimony. reply Suppafly 17 hours agorootparentprevWhenever you see some small content creator complaining about people stealing their videos and such, they never actually file a DMCA to get relief, they just use youtube&#x27;s bogus form and then complain when nothing happens. reply anonzzzies 21 hours agoprevNice and finally. People have been saying this for a long time already.Shame Google doesn&#x27;t the same with people crushing the competition by adwords&#x2F;adsense and SEO abuse. reply SoftTalker 17 hours agoparentGoogle started to care when it started costing them money. reply tlogan 16 hours agoprevnext [–]the defendants can be found in Vietnam from where they proudly advertise their ‘SEO’ scheme to others, including via YouTube.Nothing to see here: the business of sending of fake DMCA notices is booming and I do not know how this lawsuit will change a bit. They will just open a new company and continue doing this. reply infamouscow 16 hours agoparentPeople forget this goes both ways. Legal systems completely break down across international borders except for the most esteemed international criminals.The number of hoops someone has to jump through to launch international investigations is incredibly expensive, both in time and money. It&#x27;s also a lot of luck and playing hyper-political games like throwing a party for an ambassador. Ask how I know :)In short, one could hire locals to dish out vigilante justice with virtual impunity. reply withinboredom 15 hours agorootparent> Ask how I knowOh, I know. There was a guy working remotely in a country that doesn&#x27;t extradite and embezzled millions. They invited him (and the rest of the company) to a \"company party\" in the US just to arrest him while giving everyone else in the company free booze for a week. reply thomastjeffery 10 hours agoprevOne of these days we will start treating fraud as seriously as we do copyright. reply PedroBatista 11 hours agoprevDoes that include YouTube too?If that&#x27;s the case Google will be suing half the lawyers in the US, many very powerful.Either way, decades later looks like they want to put an end to it, when figured out they are spending millions with this BS, not because it was wrong in the first place and was ruining people&#x27;s business and lives. reply reaperman 11 hours agoparentNo, its a select few relatively non-powerful entities. They will not be taking on the RIAA or MPAA, for example. reply thomastjeffery 10 hours agoprev> Two men who allegedly used 65 Google accounts to bombard Google with fraudulent DMCA takedown noticesHere, the problem has revealed itself: Google is the weapon here, not just DMCA.How does your own medicine taste, Alphabet? reply doublerabbit 9 hours agoparent> How does your own medicine taste, Alphabet?Tastes like money and lawsuit.Cherry flavour. reply arbuge 15 hours agoprevHow exactly does a lawsuit filed in California reach two defendants located in Vietnam?Wouldn&#x27;t suing them in Vietnam be appropriate here? reply andylynch 13 hours agoparentThe claim is under US law. Enforcement is a different matter!- incidentally there are also 20 Does whose locations aren’t mentioned. reply ecommerceguy 11 hours agoprevBe nice if Amazon did this on the merch platform. reply dgudkov 10 hours agoprevCan&#x27;t stop but think that somehow public justice has been delegated to Google. reply ykonstant 10 hours agoprev(those fake DMCA takedown submitters that do not have enough money to fight back) reply qingcharles 10 hours agoparentLiterally none of them are going to fight this. In a case like this you&#x27;re going to be found bogus enough to have to pay Google&#x27;s legal fees on top of your own. You&#x27;ll be half a million in the hole before you blink.99% of them either won&#x27;t accept service on the Complaint or will fail to attend court. Google will win by default judgment on most of the Complaints. reply RecycledEle 18 hours agoprevGreat, now do Kay Bailey Hutchison who took down my political web sites by filing fony DMCA notices with DirectNic. reply Suppafly 17 hours agoparentDid you file the counter notices that force them to put the content back up? reply indymike 16 hours agorootparentOften the damage from fraudulent takedowns is relative to timing.For example (and this is just an example): Let&#x27;s say I put up an expose of bad behavior by a politician two days before an election. Something awful that will get coverage in the media and social media, too. The campaign fake DMCAs it, the content is gone, and the election is won before the content is re-posted after the reply is processed.There are situations where the notice&#x2F;counter notice does not work and causes almost irreparable harm. reply Suppafly 13 hours agorootparentSure, although if you know for a fact that your content is true, nothing is stopping you from reposting it multiple times&#x2F;places for those two days. Regardless, I suspect the previous poster didn&#x27;t follow the DMCA process and instead just wanted to complain. It&#x27;s trivial to send a response and have your content rehosted, but most people don&#x27;t bother. reply justinclift 18 hours agoparentprevWhy would Google have anything to do with this?If you pointed out their bogus YouTube copyright strikes problem, that&#x27;d be relevant.But DirectNic doesn&#x27;t seem like it has anything to do with Google. (?) reply pugworthy 11 hours agoparentprevThen the headline could be, \"Google Sues Woman Who Weaponized DCMA Notices...\" It definitely makes the headline&#x27;s genderfication a bit more ridiculous looking. reply qingcharles 10 hours agoprevAnyone have the case nos.&#x2F;captions (+ court) for the DMCA suits? Would like to read the Complaints and see who the defendants are. reply qingcharles 10 hours agoprevAnyone have the case nos.&#x2F;captions (+ court) for the DMCA suits? Would like to read the Complaints and see who the defendants are. reply pulse7 20 hours agoprev\"GOOGLE LLC, Plaintiff, v. NGUYEN VAN DUC, PHAM VAN THIEN, and DOES 1-20, Defendants.\"What are those \"Does 1-20\"? Are they 20 no-name individuals? reply iudqnolq 20 hours agoparentFor boring reasons sometimes people tack on placeholders in case more defendants are identified later. reply TheCleric 19 hours agorootparentPre-allocating memory so you can use it later instead of doing it in the render (justice) loop and slowing that down. Good call. reply boomboomsubban 19 hours agoparentprev\"John&#x2F;Jane Doe\" is a common placeholder name, it&#x27;s twenty people they couldn&#x27;t identify. reply sowbug 14 hours agorootparentRoe is less common, but not unusual. Most memorably, \"Roe\" in Roe v. Wade was a pseudonym for the plaintiff who was called \"Jane Roe\" to protect her privacy. I don&#x27;t know the mechanism, but I assume the judge in such cases has the real identity of a pseudonymous plaintiff to make sure a real party has brought the case. reply fusslo 17 hours agorootparentprevAh! I read it as \"DOES\" as in &#x27;do&#x27;, not \"more than one DOE\"Thanks for clearing that up reply Eisenstein 21 hours agoprevHow do they plan on enforcing US law in Vietnam? Since the defendants live in Vietnam won&#x27;t they have to either extradite them to the USA or place a judgement against them which will never be paid, as long as they don&#x27;t have any assets in the states? I have wondered for a while how this works, so anyone who knows specifics is welcome to respond. reply papercrane 20 hours agoparentIt depends on how the Vietnam legal system works, but in theory you can enforce a civil judgment across borders.After winning a judgment, Google would have to go to court in Vietnam and ask for the judgment be enforced. The court would look at wether proper notice was given, the jurisdiction was proper and that the underlying conduct was something you could sue for. They may also look at the award to determine it isn&#x27;t excessive. If they&#x27;re satisfied the court can then issue an order to enforce it.This all depends heavily on how the Vietnam legal system works, which I have no idea. Obviously every country has their own quirks and processes. reply Kye 20 hours agoparentprevMight be relevant:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_parties_to_internation...https:&#x2F;&#x2F;www.wipo.int&#x2F;treaties&#x2F;en&#x2F;notifications&#x2F;wct&#x2F;treaty_wc... reply ViSpace 9 hours agoparentprevConsidering the gov&#x27;s antique about copyright there, and the state of US-VN relations, Google will get what they want. reply slashdev 20 hours agoparentprevMaybe through their credit card payment provider?I’m guessing that’s the weak point, but I don’t know how that would work exactly. reply prmoustache 14 hours agoprevWhy are they suing them in California and not in Vietnam? reply andylynch 13 hours agoparentThe complaint is claiming that this group should be held liable under the section of the DMVA relating to misrepresentation. California since that’s where Google principal place of venue is and also that given in the TOS allegedly breached. reply notimetorelax 13 hours agoparentprevProbably because this is where ToS set the jurisdiction. reply prmoustache 12 hours agorootparentSo presumably the defendants will just shrugs their shoulders and not even appear to the court. reply bitshiftfaced 21 hours agoprevWhat was their angle? Did they try to funnel people to their own pirating site or something? reply hnbad 21 hours agoparentBased on TFA it reads like they used them for SEO by filing illegitimate notices against higher-ranking competitors. reply anonzzzies 20 hours agorootparentThis happens a lot. It&#x27;s all pretty rotten. reply freitzkriesler2 20 hours agoprevDidn&#x27;t Google used to have a service where they would publish dmca take down notices? Chilling effects or something like that? reply darrenf 20 hours agoparentGoogle contribute to Chilling Effects, which got renamed to Lumen https:&#x2F;&#x2F;www.lumendatabase.org&#x2F;[edited to remove “used to”] reply tommica 21 hours agoprevGreat job! reply ajsnigrutin 20 hours agoprev> Holding Defendants Accountable> Those who knowingly make false statements in a DMCA notice can be held liable for damages, costs, and attorneys’ fees. In this matter the defendants’ conduct is said to have caused Google to suffer economic harm due to lost advertising revenue, damage to business relations, and the allocation of significant resources to investigate their wrongdoing.Does this apply only to \"two vietnamese men\" or also companies like sony, nintendo etc., when they happen to forget about the fair uses clauses? reply eli 18 hours agoparentNope. The part of a DMCA notice that is under penalty of perjury is that you own the copyright in question. That&#x27;s what the defendants here lied about.You can&#x27;t get sued if you believe something is infringement but the person using the copyrighted material believes it&#x27;s fair use. Honestly I&#x27;m not even sure how that could work. reply gnopgnip 17 hours agorootparentIn Lenz v. Universal Music Corp the courts found that rights holders do need to consider fair use when submitting a DMCA takedown, and they are liable for damages if they fail to do so reply Kalium 17 hours agorootparentprevCounter-notice and it goes to court, I believe. reply gumby 20 hours agoparentprev> Does this apply only to \"two vietnamese men\" or also companies like sony, nintendo etc., when they happen to forget about the fair uses clauses?Who advertises on Google: \"two vietnamese men\" or “companies like sony, nintendo etc”? Both cost Google money through bogus DMCA filings, but one cohort also brings in a lot of cash. reply kmfrk 20 hours agoparentprevThe \"DMCA abuse\" rule feels a lot like perjury where it&#x27;s technically illegal but a legal and prosecutorial mess that means it never rarely punished and makes headlines when it finally does. reply Andrex 20 hours agorootparentIt&#x27;s there to prevent the most obvious potential abuses.Very little in the copyright sphere is \"obvious\" though. Is this riff original or was it stolen? Etc. reply ajsnigrutin 20 hours agorootparentThese four chords are totally unique and original!https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=oOlDewpCfZQ reply aidenn0 17 hours agorootparentI like the live version of that better[1], and while the Axis of Awesome wins in the shear number of songs, I like Rob P&#x27;s presentation[2] of the same idea more.1: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5pidokakU4I2: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=JdxkVQy7QLM reply tyingq 20 hours agoparentprevUrgh, yeah. If you&#x27;re trying to set an example, suing two individuals in a US court... individuals that don&#x27;t even reside in the US, that&#x27;s an odd choice.Suing an actual company and recovering some of the judgement would surely send a stronger message.Edit: maybe a more cynical view is that racking up default judgements is used as low-effort proof of \"we&#x27;re policing the system...see all of our lawsuits\" if government or public pressure rises. reply jfoutz 20 hours agorootparentThat&#x27;s how it works though. You go after the weakest , least sympathetic, least able to respond, and set the precedent.Sorta like weev. There are plenty of reasons to not like the guy. But that&#x27;s the case that made changing the numbers in a URL a possible felony.Anyway, once they get that first win, they pick on a slightly larger target, and cite the first win as precident. reply ipaddr 15 hours agorootparentWhy not sue in their home country? Google has a business presence. This is more for show and headlines. reply tyingq 20 hours agorootparentprevDoes a default judgment (no show) set any citable precedent? reply jfoutz 19 hours agorootparentNot a lawyer.I&#x27;d imagine if they rack up a bunch of no-shows They could change their dmca handling process. We&#x27;re not honoring these without property X, because these last hundred or thousand or whatever turned out to be fake.I guess, I&#x27;d assert google lawyers are not dumb, and they probably have a reason to start stacking \"wins\". But I don&#x27;t know what the strategy or end goal might be. I&#x27;m just some random guy on the internet with an opinion. reply tyingq 19 hours agorootparentThat makes sense to me. \"Procedural precedent + metrics\" versus actual legal precedent. reply bluGill 19 hours agorootparentprevNot really. Sure you can refer to them, but the first lawyer who shows up will point out the others were no shows and the judge won&#x27;t pay much attention to results. reply AwaAwa 20 hours agorootparentprevMight they be trying to set a precedent and receive a quick &#x27;no show&#x27; judgement? reply manicennui 18 hours agorootparentprevDMCA is a US law. reply patmorgan23 18 hours agorootparentThat implements the requirements of international copyright treaties. reply marcosdumay 17 hours agorootparentHum, no, international copyright treaties get nowhere the contents of the DMCA. The US has an entire copyrights law for implementing those. reply chii 20 hours agoparentprev> when they happen to forget about the fair uses clauses?oh those are just innocent omissions and no malice nor intent is implied! reply ajsnigrutin 20 hours agorootparentI mean.. who could expect highly paid copyright lawyers in their massive legal teams to know such obscure rules such as fair use :) reply gjsman-1000 19 hours agorootparentThey know about it; but I think they know:A. Are you going to fight it?B. Fair use, from a legal perspective, is much narrower than people think and narrower than the wording suggests. A reaction video is not considered sufficient commentary or criticism, for example. Making even a single word change to a cover song, even if you paid the compulsory license fee, is not fair use. Etc.C. Especially in video games, everyone loves quoting Sega v Accolade or Sony v Bleem as proof emulation is perfectly legal. There’s just one problem: Both of those cases were decided before the mother-of-all-DRM laws, the DMCA, was passed. They are both likely irrelevant cases now. This happens elsewhere too - quoting old cases as proof of legality without other modern developments. reply wang_li 18 hours agorootparent>Especially in video games, everyone loves quoting Sega v Accolade or Sony v Bleem as proof emulation is perfectly legal.There&#x27;s no need to prove emulation is legal. There&#x27;s a need to prove it&#x27;s illegal. There&#x27;s a long history of making compatible things. Everything from toner cartridges to PC compatibles to auto parts. And especially in the context of copyright law, explain the possibility of a copyright violation in a piece of software written to emulate a hardware platform when there is no ROM or firmware distributed by the authors? Now how most people choose to run emulators is copyright violation from horizon to horizon, but I don&#x27;t see how that has anything to do with people making emualtion products. reply gjsman-1000 18 hours agorootparent> explain the possibility of a copyright violation in a piece of software written to emulate a hardware platform when there is no ROM or firmware distributed by the authorsIt specifically has to do with the DRM keys used for decrypting the games; which would appear to run afoul, in most cases, of the DMCA.It’s also not theoretical. Unauthorized DVD players and ripping software have had their authors legally prosecuted repeatedly (even big companies at the time, like RealPlayer). Gary Bowser also went to prison and had his fines set, in part, for violating that law (notice that it was a criminal case, not a civil one; Nintendo could not have dismissed those charges voluntarily.) Apple sent Psystar into bankruptcy for daring to use their keys to make macOS run on non-Macs as a commercial service. Psystar is actually the best example because they went through the entirety of the legal system to where there was nothing left but appeal to SCOTUS, and they still lost.I think that the biggest danger to emulation, by far, which archivists should be panicking more about, is the people who emulate to avoid buying a game. Which, according to most estimates, is 90%+ of emulation. On an old console who cares? On the Nintendo Switch though, that’s actual market harm (the lack thereof being one of the pillars of determining fair use in a courtroom). reply thwarted 17 hours agorootparentprevMaking even a single word change to a cover song, even if you paid the compulsory license fee, is not fair use.Wouldn&#x27;t obtaining a license supercede any claims of fair use? The defense wouldn&#x27;t be \"this falls under fair use\", the defense would be \"I&#x27;ve licensed this, there can&#x27;t be any complaint\". reply gjsman-1000 17 hours agorootparentI am speaking of the compulsory license that music companies are obligated to offer if you want to make a commercially-produced cover of a song. Currently, it costs 9.1 cents per copy that you sell of a covered song.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Mechanical_licenseYou might read the above and go, “wait, that wasn’t Fair Use this whole time?” Nope. Fair Use doesn’t cover nearly as much as people think. reply singron 19 hours agorootparentprevYou just have to consider fair use to make a dmca claim. It takes 1 second in your head. The bar is very low. reply xinayder 19 hours agoparentprevEveryone knows that rules don&#x27;t apply to big corpos because they have the monies and lawyers. reply kozinc 19 hours agorootparentWell, they do, technically, the issue is that they aren&#x27;t enforced... because they have the monies and lawyers. :( reply boomboomsubban 21 hours agoprev>Elon Musk, Taylor Swift, LeVar Burton, and Kanye West.LeVar Burton? That&#x27;s a bizarre inclusion on that list. Some Google lawyer is probably a Trekkie. reply Eisenstein 20 hours agoparentThey were using DMCA to shut down competitors websites, and they were in the business of selling printed T-shirts, so anyone with a face or brand that would be printed on a shirt could be used as the claimant on the DMCA notice. reply boomboomsubban 19 hours agorootparentThe four listed in the complaint were mentioned as examples of who they imitated. They could have stopped at three examples, the sports teams had three examples while the bands had one. The inclusion of Burton was almost certainly because the lawyer is a fan. reply seanhunter 18 hours agorootparentMaybe the lawyer thinks the likely judge is a fan. That would be a ninja move. reply waterhouse 18 hours agorootparentIs the likely judge Otis Wright? https:&#x2F;&#x2F;www.abajournal.com&#x2F;news&#x2F;article&#x2F;judge_boldly_uses_st... replychris_wot 12 hours agoprevYou know, I&#x27;d feel a lot safer if Google would fix the mountain of malware in their Chrome app store. reply tomohawk 16 hours agoprevWhy is &#x27;Men&#x27; featured in the title? It&#x27;s like the title goes out of the way to call attention to gender here. It&#x27;s distracting and adds nothing. reply elzbardico 14 hours agoparentBecause they are men. And yes, it adds context, as it makes easier for you to picture concrete people with your imagination. Having a gender is part of the concrete reality of the human species, so having this information provides a clue of concreteness. reply jfax 14 hours agoparentprevI agree, it&#x27;s a bit disorienting. reply manicennui 18 hours agoprev [–] Only companies like Google are allowed to use the weapon that is DMCA. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has initiated a legal proceeding against two individuals from Vietnam accused of sending fake DMCA takedown notices using 65 Google accounts.",
      "The defendants allegedly abused the notice-and-takedown system, posing as reputable entities to remove legitimate content and harm Google's business.",
      "Google intends to bring the defendants to justice and is seeking compensation for damages and legal fees."
    ],
    "commentSummary": [
      "Google has taken legal action against two people who sent false DMCA notices in an attempt to stifle competition.",
      "This lawsuit brings attention to the flaws in the DMCA system and emphasizes the importance of reforms to prevent future abuse.",
      "The individuals targeted by Google's lawsuit used fake DMCA notices to suppress competition, revealing a concerning aspect of the current system."
    ],
    "points": 371,
    "commentCount": 168,
    "retryCount": 0,
    "time": 1699962517
  },
  {
    "id": 38267588,
    "title": "Vehicles with Higher, More Vertical Front Ends Increase Risk to Pedestrians: Study",
    "originLink": "https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians",
    "originBody": "Vehicles with higher, more vertical front ends pose greater risk to pedestrians Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too. By November 14, 2023 Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too, new research from the Insurance Institute for Highway Safety shows. Whatever their nose shape, pickups, SUVs and vans with a hood height greater than 40 inches are about 45 percent more likely to cause fatalities in pedestrian crashes than cars and other vehicles with a hood height of 30 inches or less and a sloping profile, an IIHS study of nearly 18,000 pedestrian crashes found. However, among vehicles with hood heights between 30 and 40 inches, a blunt, or more vertical, front end increases the risk to pedestrians. “Some of today’s vehicles are pretty intimidating when you’re passing in front of them in a crosswalk,” IIHS President David Harkey said. “These results tell us our instincts are correct: More aggressive-looking vehicles can indeed do more harm.” Pedestrian crash deaths have risen 80 percent since hitting their low in 2009. Nearly 7,400 walkers — more than 20 people a day — lost their lives in 2021 after being struck by a vehicle. While speeding and poorly designed infrastructure have helped fuel the increase, many safety advocates have also drawn a connection to the growing portion of the U.S. vehicle fleet made up of pickups and SUVs. Over the past 30 years, the average U.S. passenger vehicle has gotten about 4 inches wider, 10 inches longer, 8 inches taller and 1,000 pounds heavier. Many vehicles are more than 40 inches tall at the leading edge of the hood. On some large pickups, the hoods are almost at eye level for many adults. To examine the connection between fatality risk and vehicle size and shape, IIHS researchers analyzed 17,897 crashes involving a single passenger vehicle and a single pedestrian. Using Vehicle Identification Numbers to identify the crash-involved vehicles, they calculated key front-end measurements corresponding to 2,958 unique car, minivan, large van, SUV and pickup models from photographs. They excluded vehicles with pedestrian automatic emergency braking systems and controlled for other factors that could affect the likelihood of a fatality, such as the speed limit and age and sex of the struck pedestrian. Vehicles with hoods more than 40 inches off the ground at the leading edge and a grille sloped at an angle of 65 degrees or less were 45 percent more likely to cause pedestrian fatalities than those with a similar slope and hood heights of 30 inches or less. Vehicles with hood heights of more than 40 inches and blunt front ends angled at greater than 65 degrees were 44 percent more likely to cause fatalities. “Manufacturers can make vehicles less dangerous to pedestrians by lowering the front end of the hood and angling the grille and hood to create a sloped profile,” said IIHS Senior Research Transportation Engineer Wen Hu, the lead author of the study. “There’s no functional benefit to these massive, blocky fronts.” While sloping front ends did not reduce the risk posed by vehicles with the tallest hoods, they did make a difference for vehicles with hood heights of 30-40 inches. Compared with low and sloped vehicles, medium-height vehicles with blunt fronts were 26 percent more likely to cause pedestrian fatalities. In contrast, the risk of a fatality was about the same for medium-height vehicles with sloped fronts as for low vehicles with either blunt or sloped fronts. The researchers looked at several other vehicle characteristics, including the angle of the windshield, length of the hood and angle of the hood. Among these, the slope of the hood had the biggest effect. There was a 25 percent increase in the risk of a fatality for vehicles with flat hoods — those with angles of 15 degrees or less — compared with vehicles with more sloping hoods. That was true regardless of height and front-end shape. To better understand how vehicles of different geometries injure pedestrians, IIHS examined detailed records from 121 crashes collected by the International Center for Automotive Medicine Pedestrian Consortium. In each crash, the front end of a car, pickup or SUV struck a teenager or adult. The data included detailed crash reconstructions, including information about the motion of the pedestrian’s body during the crash and the nature and severity of their injuries. The reports also included the year, make and model of the striking vehicle and the height of the pedestrian. The researchers used the same measurements as those used in the larger study to define vehicles with blunt and sloped front ends and tall and short ones. For this study, however, they divided the involved vehicles into only two height groups because of the smaller sample size. Taller vehicles were defined as those with a hood leading edge more than 35 inches off the ground. Shorter ones were those with a hood leading edge 35 inches or less from the ground. In general, vehicles taller than 35 inches were more dangerous to pedestrians than the shorter ones, mainly because they tended to cause more severe head injuries. Among vehicles taller than 35 inches, those with vertical front ends were more dangerous than those with sloped front ends. Torso and hip injuries from these vehicles were more frequent and severe. Unlike all other vehicle types, tall and blunt vehicles primarily inflicted torso injuries with their front ends rather than with the tops of their hoods. They were more likely to injure pedestrians by throwing them forward, while tall and sloped vehicles usually rolled them onto the hood of the vehicle first. Pedestrians who were shorter relative to the height of the striking vehicle also suffered more severe injuries. “It’s clear that the increasing size of the vehicles in the U.S. fleet is costing pedestrians their lives,” Harkey said. “We encourage automakers to consider these findings and take a hard look at the height and shape of their SUVs and pickups.” Comparative risk of pedestrian fatality by hood leading edge height and shape Related The association between passenger-vehicle front-end profiles and pedestrian injury severity in motor vehicle crashes Research paper Vehicle front-end geometry and in-depth pedestrian injury outcomes Research paper IIHS response to NHTSA proposal to add pedestrian protection information to its vehicle safety ratings Regulatory comment More about pedestrians and bicyclists Media contact Email Joe Young Director of Media Relations office +1 434 985 9244 mobile +1 504 641 0491 For more information, visit our press room. Get news directly to your inbox",
    "commentLink": "https://news.ycombinator.com/item?id=38267588",
    "commentBody": "Vehicles with higher, more vertical front ends pose greater risk to pedestriansHacker NewspastloginVehicles with higher, more vertical front ends pose greater risk to pedestrians (iihs.org) 304 points by leotravis10 15 hours ago| hidepastfavorite428 comments sickofparadox 15 hours agoThe CAFE regulations have been a disaster for the safety and affordability of vehicles and US regulators need to figure out a way to legislate fuel effeciency in such a way that doesnt lead to the difference between a 1990&#x27;s F-150 and a 2020&#x27;s F-150. It would also do the US a world of good to get rid of the chicken truck import tax. reply jgeada 14 hours agoparentIt doesn&#x27;t help that vehicular manslaughter is basically not punished at all in the US. You can run over a pedestrian or cyclist due to inattention or dangerous driving and get away with a no-jail sentence of community service of 60 days or so.The incentives are all aligned to maximize vehicle use and minimize any inconvenience to incompetent vehicle owners, regardless of the death toll. reply bryanlarsen 13 hours agorootparentStudies show that the length of sentence has little impact on criminal behaviour. The likelihood of getting caught has much more impact.If everybody was guaranteed to get 60 days of community service every time they did something that could have killed a pedestrian then the behaviour would drop right off. reply dsego 12 hours agorootparent> Studies show that the length of sentence has little impact on criminal behaviour.Is the purpose of punishment only to deter criminal behavior? How about expressing social condemnation for the committed criminal act, strengthening citizens&#x27; trust in the legal order based on the rule of law, increasing awareness of the danger of committing criminal acts and of the fairness of punishment, and enabling the perpetrator to rejoin the society. reply phil21 12 hours agorootparentI generally agree, but don&#x27;t all the latter items simply in the end have the desired result of deterring future criminal behavior?If distracted driving was caught 95% of the time, and the sentence was something like 40 hours of community service (and escalated for repeat offenders in some manner), I don&#x27;t think society has nearly as difficult of a moral problem on it&#x27;s hands when a 19 year old kills a cyclist due to checking a text on their phone.Studies repeatedly show \"random\" punishments (even if harsh) are simply not very effective vs. consistent enforcement. When you drop the hammer on someone who more or less got \"unlucky\" that day by engaging in what amounts to otherwise normalized behavior due to lack of enforcement - it gets kind of hard to see how that can be remotely effective at accomplishing much for society beyond vengeance. While that is important, it certainly isn&#x27;t everything. reply singleshot_ 12 hours agorootparentprevDisincentivisation of recidivism, disincentivisation of others, isolation of dangerous individuals from society, sanitized vengeance on behalf of the victim would be the ones that someone with a different political persuasion would use. I’ve added a couple of yours to my list. Yours was a thought provoking post. reply JohnFen 11 hours agorootparentprevDoesn&#x27;t all of that fall under the umbrella of \"deter criminal behavior\"? reply neuralRiot 12 hours agorootparentprevBetter yet, include pedestrian safety in NHTSA vehicle safety ranking (it was proposed in 2020 apparently) reply eptcyka 12 hours agorootparentprev60 days of community service means that most people won&#x27;t think much about driving attentively. To put it very cynically, if I didn&#x27;t care for people around me, 60 days of community service sounds like an OK forced retreat. However, we shouldn&#x27;t rely on people&#x27;s conscience to deter manslaughter, and instead have a dual-pronged approach - appeal to both people&#x27;s conscience and fear of having to spend time in prison. reply bryanlarsen 11 hours agorootparentAFAICT some people drive inattentively daily. 60 days of community service for every time they drive is a lot more than a forced retreat. reply eptcyka 2 hours agorootparentMaybe they shouldn’t drive at all then? reply David_Axelrod 12 hours agorootparentprevLink to studies? That is a bold claim. reply bryanlarsen 11 hours agorootparenthttps:&#x2F;&#x2F;www.ojp.gov&#x2F;pdffiles1&#x2F;nij&#x2F;247350.pdf contains references. reply David_Axelrod 10 hours agorootparentThanks! reply yieldcrv 12 hours agorootparentprevWhich has no impact on the punitive revenge seeking approaches of American institutions as a reflection of American citizens demands reply jplrssn 14 hours agorootparentprevI agree that is outrageous.But I&#x27;d imagine most people don&#x27;t consider the survivability of pedestrians in a collision when they buy a car, since people generally overestimate their driving ability and underestimate their likelihood of ending up in an accident.So harsher penalties might not improve car designs much. reply NegativeLatency 13 hours agorootparentI think some people do consider it (although possibly from the wrong angle and not very deeply) otherwise you wouldn&#x27;t see stuff like massively lifted trucks, who&#x27;s purpose is to look \"aggressive\" and \"scary\" to make the person inside feel better about themself. reply mordechai9000 12 hours agorootparentI think the noise and the image are designed to impress other drivers safely cocooned within their own vehicles. You don&#x27;t even need to modify the stock vehicles to get this effect. When I have no protective shell around me, even standard passenger cars seem to be aggressive and hostile. reply mikrotikker 13 hours agorootparentprevCan confirm I got just a 2\" body lift on my truck and it feels... Empowering. I want to go higher now so I&#x27;ll be buying a 2\" lifted suspension kit, nothing crazy like the 4\" my friend is going for (his truck looks amazing with it). Unfortunately it won&#x27;t fit in my shed then so I need to build a carport before I do that.The pedestrians are safe(ish). For now. reply Symbiote 12 hours agorootparentThe pedestrians are already in danger since you&#x27;re driving a truck. reply alamortsubite 14 hours agorootparentprevIf harsher penalties were adopted only in the case of killing or injuring someone, perhaps not, but if we made fines for all moving violations proportional to vehicle weight, people might suddenly be more thoughtful. reply snthd 14 hours agorootparentprevYou&#x27;re on to something there - apply the penalties to manufacturers. reply rsynnott 12 hours agorootparentIt is arguably weird that no-one has successfully taken this approach; _in general_, if you make a thing, and then knowingly make a less safe version of a thing, and it kills someone due to your changes, then, well, you&#x27;ll want a good lawyer. But cars do seem to be treated as something of a special case; all the responsibility is on the operator, even as the product is made knowingly more dangerous to bystanders. reply TedDoesntTalk 13 hours agorootparentprevVehicular manslaughter:California:A felony vehicular manslaughter conviction is punishable by 2, 4 or 6 years in state prison.Colorado:Under Colorado law, vehicular manslaughter in the second degree is considered a Class 4 felony, as outlined in Colorado Revised Statutes Section 18-3-106. The potential penalties for this offense include imprisonment for a term ranging from 2 to 6 years and a fine between $2,000 and $500,000 reply morepork 12 hours agorootparentHow often do those charges get used?For example: https:&#x2F;&#x2F;www.nbclosangeles.com&#x2F;news&#x2F;driver-killed-south-la-si...A driver, allegedly watching youtube, runs over and kills 2 children of 12 and 14 at a crosswalk. But they were only charged with the lesser vehicular manslaughter, not felony vehicular manslaughter. reply TedDoesntTalk 12 hours agorootparentUnbelievably tragic story. Thank you for sharing. I did not know about it. reply itslennysfault 13 hours agorootparentprevThose laws exist, but if it is a legitimate accident, and the person doesn&#x27;t have a criminal history otherwise it is very likely that they walk away without any jail time. reply jgeada 13 hours agorootparentDefine \"accident\". There are very few circumstances where correct attentive driving technique ends up as an \"accident\". Most so called accidents are not \"accidental\". It had a predictable causation chain caused by the driver&#x27;s behavior. Personally, I hate that word, much prefer \"incident\".Driving too close is not an accident. Driving too fast for the conditions is not an accident. Weaving around in traffic to gain a spot. Not looking at who is in the crosswalk before driving through it. Not yielding at a merge. Racing the lights to get there before it is red&#x2F;or just after. Not having your brakes, tires, lights, suspension, etc in correct working condition. Driving too tired, or under the influence. Sure, most of the time nothing happens, so people get used to thinking these are how driving is done (and it is in the US). But just because one normally gets away with it does not mean these are not dangerous behaviors, and they&#x27;re absolutely under the drivers control.Not accident, incident.Read up on \"normalization of deviance\" (wiki page is pretty light but it is a start: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Normalization_of_deviance). Aviation has long recognized this and training emphasizes not getting caught in that behavioral trap. reply Karrot_Kream 12 hours agorootparentAviation is a great example of how a culture of safety can be built up. Driving culture in the US is markedly different. It&#x27;s absolutely normalized to overtake a slow moving vehicle even when it&#x27;s not safe and passing conditions are unclear, to be distracted and use your phone, to drive drunk, to drive along the shoulder of a crowded highway if you&#x27;re in a hurry, to double park in an urban area if you \"just need to stop and pick something up\". The enforcement aspect is just the lagging edge of that. Hearing people talk, you&#x27;ll casually hear \"oh I accidentally bumped the car next to me in the parking lot while parking oops, I just reparked somewhere else.\" It shows; the US has the highest car crash incidence among developed nations by a long shot, with Canada a distant 2nd place.The reason the US is considered a driving culture is only partially legislation and only partly infrastructure, it&#x27;s due to a culture that treats driving as a loosely enforced activity where faults should be given the benefit of the doubt. This is the opposite of aviation where safety is emphasized in every part of the process and scary moments are encouraged to be reported anonymously through ASOS. Infrastructure is stressed as the ideal solution because it involves very little cultural change but due to a reluctance to raise taxes and aforementioned driving culture, people resist infrastructural changes which help non-drivers. reply flyaway123 1 hour agorootparentFor aviation, I believe the fact that the \"driver\"&#x27;s life is at stake helps (also consider the perspective of the passengers). Comparing that to the imbalance between the drivers in a protective shell and the pedestrians on the road. reply pclmulqdq 12 hours agorootparentprevPedestrian runs across the street and doesn&#x27;t notice the oncoming car; driver starts to brake but the car doesn&#x27;t stop in time. That is a very common type of accident that occurs even when the driver is going below the speed limit and is paying full attention. It is so common that drivers in suburban areas pay extra attention to the behavior of pedestrians, who very often run into the street for one reason or another.Contrary to the popular narrative, not everyone who drives a car does so drunk, high, and&#x2F;or too fast - a lot of people are actually very good drivers. Many (most?) of them do all of the recommended maintenance on the car in a timely manner. That doesn&#x27;t mean that every driver has the reflexes of a racecar driver, and nor can you expect them to.Blaming every accident on the driver by default and expecting jail time for anything with bad consequences is pretty toxic. Even the FAA doesn&#x27;t do that for pilots. reply tmnvix 12 hours agorootparentNot a popular opinion, but in my mind every driver should at all times be prepared for a child to chase a ball onto a street. What that means in practice is a whole lot of inconvenience for drivers - but it is their responsibility and they should own it. Roads (other than motorways&#x2F;expressways&#x2F;etc) are not the exclusive domain of cars and people shouldn&#x27;t assume that they are. reply pclmulqdq 12 hours agorootparentI will say this more directly than above: pretty much every driver going through a suburban neighborhood does prepare for a child to chase a ball into a street. Not all of those collisions are preventable, however. It takes time to engage the brake and it takes time for the car to stop after the brake is engaged. In that time, the car travels forward. In some cases, crushing a child. Even if the driver does everything right, a child still dies. reply Johnny555 12 hours agorootparentpretty much every driver going through a suburban neighborhood does prepare for a child to chase a ball into a streetWhere do you live? That&#x27;s not how it is in my neighborhood, despite a 25mph sped limit that turns to a 20mph zone in front of my house (due to the proximity of a playground), drivers routinely drive 35+ mph. Neighborhood complaints have so far gotten a \"Slow - Children\" sign posted.The one thing that helped for about a week was when they parked a radar speed trailer on the side of the street, it would flash a bright blue light when drivers exceed the speed limit by 5mph.After they took away the trailer, cars started speeding again. reply tmnvix 8 hours agorootparentprev> pretty much every driver going through a suburban neighborhood does prepare for a child to chase a ball into a streetSo why do most drivers appear to be much more cautious in a parking lot?If drivers were truly taking more responsibility for the dangers they create for others, whenever they travel on a street alongside parked cars they would limit themselves to a speed that is at least unlikely to cause a pedestrian fatality ( Roughly 68 percent of crashes resulting in a pedestrian fatality occurred at non-intersections in 2010Also half of these pedestrian-driver crashes involve someone (pedestrian or driver) being inebriated, but they certainly do occur when the driver is paying full attention.> I&#x27;ve actually been in the crosswalk and had to stop for a right-on-red turning car who didn&#x27;t manage to see a person in the street in broad daylight.I&#x27;ve been in the same position as you here, but I&#x27;ve had a lot more drivers wait for me. I&#x27;ve also seen drivers (particularly in Florida) acknowledge my existence by making eye contact before rudely turning directly in front of me. This particular piece of American car culture drives me crazy, too, but is not an indication of broad negligence or disregard for the lives of others. reply Johnny555 11 hours agorootparentI&#x27;m not sure how that backs up your statement that it&#x27;s very common for a pedestrian to run across the street in front of a car that not only saw them and started braking appropriately, but was also going below the speed limit. reply pclmulqdq 9 hours agorootparentI said that it was common, and even happens when people are driving well. I think you may have misunderstood me. replyJohnFen 11 hours agorootparentprev> Define \"accident\".An unintended adverse event. reply stickfigure 13 hours agorootparentprevYou want people thrown in jail for accidents where negligence was not involved?Do you drive at all? You could be a perfect driver, fully sober and alert, and one moment&#x27;s distraction can easily kill. Maybe a sudden glare in the window, or your misbehaving kid in the backseat throws something. It happens every day.I don&#x27;t see any social good to be gained from jailing people when negligence isn&#x27;t involved. Human beings have failure rates. You&#x27;re talking about taking an already horrible situation and destroying more lives and families. reply LeifCarrotson 12 hours agorootparentYes, I do. That should be a risk that you accept when you get in the driver&#x27;s seat. It would affect behaviors positively by disincentivizing driving.If you&#x27;re unable to see what&#x27;s in front of your 2-ton mile-a-minute machine because of a glare, maybe you shouldn&#x27;t be driving a mile a minute? Clean the windows before you leave. Wear sunglasses. If the glare is \"not that bad\" so you don&#x27;t regularly clean the windows and don&#x27;t make sure you have sunglasses and occasionally someone dies because of this that is unacceptable. Glare does kill, as evidenced by increased collision rates when the sunrise aligns and misaligns with peak traffic - change the incentives so that people take steps to mitigate this.If you&#x27;re responsible for managing the behavior of a kid and also piloting a car, maybe you should have a second adult in the car. Maybe the kid should be in a seat they can&#x27;t easily unbuckle, or not have access to toys they can throw in front of you. You should think about the risk to other road users and your potential jail time when you take your eyes off the road to handle some other responsibility.Yes, I drive. Professionally! I deliver industrial equipment and travel all over the Midwest to perform upgrades, maintenance, and bug fixes on it. I take the safety of myself, pedestrians, and fellow drivers seriously when I have 20,000 lbs of steel on a gooseneck - more seriously than a lot of drivers I see around me. When I&#x27;m not traveling, though, I bike to work for 9+ months a year. And I&#x27;ve had my share of beer cans thrown out windows, and rearview mirrors hitting me in the butt, and coal rolled in my face, and panic from people scrolling TikTok and not seeing me until the last second. I haven&#x27;t been killed yet, but I keep my life insurance paid up. reply vwcx 13 hours agorootparentprevThe burden of proof of negligence is problematic. Until we surrender additional privacy to a \"more objective\" monitoring system, it&#x27;s too easy to murder someone with your vehicle and claim innocence.When I&#x27;m riding my bicycle in a perfectly legal way on a roadway, and a driver gets irritated and tries to clip me, it&#x27;s very difficult for me to prove that they acted in malice. If they clip me and I die hitting a guardrail, it&#x27;s incredibly easy for them to claim it was an accident.But that&#x27;s an edge case and this type of culpability problem is common across many facets of humanity. The bigger issue may be that driving isn&#x27;t treated with the seriousness it should warrant. In conditions where \"sudden glare\" is possible, one should drive their 4000lb vehicle at a speed that allows them to mitigate all but the most rare (freak) accident, like a 200lb alligator popping out of a manhole cover right in front of them. The human failure rate is real, but there&#x27;s more we can do with process + tech + practice to mitigate it even smaller.This level of care as a driver is directly counter to what the automobile industry wants as the image of their product: an easy, convenient and essential tool to be used every day. reply trgn 12 hours agorootparentprev> You could be a perfect driver, fully sober and alertThese people should not be jailed.> and one moment&#x27;s distraction can easily kill. Maybe a sudden glare in the window, or your misbehaving kid in the backseat throws something. It happens every dayBy phrasing it that way, you also admit that driving is incredibly dangerous, essentially murderous in the aggregate. The right response then is that we need to establish a transportation system where a moment of inattention doesn&#x27;t cause death and destruction. Traffic calming, slower speed limits, more mobility options (walk, bike, bus, train), ... and to the point of the article, smaller and lighter vehicles. reply eptcyka 12 hours agorootparentprevYes. People are wielding 2 tonne sledgehammers, often fueled by paleolithic compost. With great power comes great responsibility. reply CoffeeOnWrite 12 hours agorootparentprevFor clarity, the vehicular manslaughter laws referenced by parent commenter are specifically for when [criminal] negligence is involved (and yes parent commenter’s use of “accident” does imply absence of negligence, so I appreciate your calling them out, they don’t seem to understand the law):> In cases of criminal negligence, the defendant is commonly charged with unintentional vehicular manslaughter.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vehicular_homicide reply ToucanLoucan 13 hours agorootparentprevI mean this is just the consequences of our increasingly atomized car-dependent society going further and further down the chain of human misery. The average commute in the US is 26 minutes, meaning people spend roughly 1 in 16 of their waking hours behind the wheel, doing a task most people view as a begrudging obligation. Most people don&#x27;t like driving, therefore they do not put great effort into driving, or seek to be better drivers which is why the majority of drivers are utterly mediocre at it. And because it&#x27;s basically a necessity of life here, we can&#x27;t genuinely punish people too badly for fucking up at it, no matter how deadly it is: our prison population is already massive on a per-capita basis, the last thing we need is even more people in that system.The real problem is as the parent comment states, CAFE standards which have massively fucked with the incentives for car manufacturers to aggressively market previously SUVs and increasingly pickup trucks as family vehicles that are absolutely ridiculously oversized for their stated purpose (while also being remarkably and hilariously unable to actually carry cargo efficiently) and ludicrously inefficient, with all the blind-spots and difficulty to handle that that implies. Their higher stance and larger bodies make them infinitely more dangerous to pedestrians and smaller vehicles, which also incentivizes other drivers to buy the enormous fucking things, so if they get t-boned by someone driving them they don&#x27;t have basically no chance of surviving. Which is not to say that a reasonably sized sedan can&#x27;t also kill a pedestrian, but an Escalade has an infinitely better shot of managing it, both by size, and because the higher ride height greatly increases the odds of people going under the damn wheels.Speaking of going under the damn wheels, there&#x27;s been a huge increase in the number of kiddos going under the wheels of these big stupid things in recent decades, which is a uniquely American phenomenon occurring basically nowhere else because these enormous stupid vehicles do not make economic sense anywhere they aren&#x27;t subsidized like they are here. reply rob74 12 hours agorootparentTrust me, it&#x27;s not uniquely American. Cars have been getting bigger in Germany too, and SUVs are the most popular category here too. Ok, most of them are not as big as in the US, but there are some (ex-Dodge) Rams (even the name is scary when you think about it) driving around here too. Every time I see one, I wonder where they find a parking space for the damned thing... reply pandaman 9 hours agorootparentIf you think sheep are scary don&#x27;t think what phone chargers are named after. reply rob74 1 hour agorootparentI was thinking more about the verb: https:&#x2F;&#x2F;www.dictionary.com&#x2F;browse&#x2F;ram reply thot_experiment 13 hours agorootparentprevNo, the problem begins when you get in the car and begin driving it. I&#x27;m not pro jail in general so I probably agree with you there, but, you&#x27;re speaking as though \"driving at all\" is a given. It is not and it is a big part of them problem that people have that view. It&#x27;s honestly really disgusting to see this blase attitude. You must take responsibility for putting people&#x27;s lives in danger and when you drive a car you are doing that, you make that choice every time you get behind the wheel. There are better, less convenient ways of transport, if you care about human life at the expense of your inconvenience you would use those, when you don&#x27;t you make the choice to endanger lives. I still drive, this is the view I take every time I am behind the wheel, it seems the only responsible position. reply sleepybrett 14 hours agorootparentprev+ civil penalties from the family of the slaughtered. reply pclmulqdq 13 hours agorootparentprevThis is completely false. Vehicular manslaughter carries a long sentence (2-4 years is not uncommon), especially if you are driving dangerously at the time. The driver can also get sued by the family of the person who was run over for massive damages.This all assumes, however, that the accident was actually the fault of the driver. In many cases, it&#x27;s actually not their fault, in which case the lawsuit from the family goes out the window and the sentence also tends to get reduced. The cases you are thinking of probably involved a pedestrian running into the street without seeing the car or a cyclist doing something equally stupid. reply jgeada 13 hours agorootparenthttps:&#x2F;&#x2F;www.wusa9.com&#x2F;article&#x2F;news&#x2F;local&#x2F;maryland&#x2F;driver-hit...https:&#x2F;&#x2F;www.bicycling.com&#x2F;news&#x2F;a30607392&#x2F;group-ride-florida-...https:&#x2F;&#x2F;www.gainesville.com&#x2F;story&#x2F;news&#x2F;crime&#x2F;2019&#x2F;04&#x2F;16&#x2F;no-c...etc etc etc reply pclmulqdq 12 hours agorootparentNotice that none of those are \"vehicular manslaughter,\" which means no implication of actual criminal negligence&#x2F;recklessness on behalf of the driver. You get the news story because someone died, but notice how they tend to be relatively light on detail. It takes two to tango, and bad car accidents usually have elements of fault from both parties.In the examples you gave:1. \"a traffic offense for causing serious injury or death to a vulnerable individual while operating a motor vehicle\"2. \"Despite that evidence, prosecutors declined to seek criminal charges against Vanderweit, saying her carelessness didn’t amount to vehicular homicide\" - \"Careless driving\" is a charge they give drivers when the prosecutor wants to make a quick deal and doesn&#x27;t think they can prosecute anything higher. I wonder if there&#x27;s more to the story here given that there were at least 13 witnesses who could explain exactly what happened...3. \"pleaded no contest to overtaking and passing a cyclist without keeping a distance of at least 3 feet\" reply TedDoesntTalk 11 hours agorootparentprevhttps:&#x2F;&#x2F;www.wusa9.com&#x2F;article&#x2F;news&#x2F;local&#x2F;maryland&#x2F;driver-hit...\"Current laws in many ways do not give substantial protections to victims struck by vehicles. As legislative liaison for the Maryland State’s Attorneys’ Association, I would welcome any discussion and examine any changes proposed to the legislature. We share concerns about keeping our community members safe on the roads,” said State’s Attorney John McCarthy.\" reply oatmeal1 14 hours agoparentprevThey just need to close the \"light truck\" exception that allows vehicle manufacturers to skirt the rules with SUVs and trucks. They won&#x27;t though. reply bertil 14 hours agorootparentNo need to close it: just demand a commercial license to drive one of those. Yearly check-up, learn to look at your blind sides. If you mess up, lose your job.If those are getting tax benefits for being used by specialized industries, they should be treated as such. I want emission standards, but if you tell me that we don’t get a snowplow without exception, I’m amenable. Snow on the road is bad. Where we should all draw the line is having soccer moms do the school run in a snowplow. reply riversflow 14 hours agorootparentprevPerhaps the light truck exemption should stay, but all exempt vehicles pay a punitive mileage tax. My feeling here is that pickups as _actual_ tool&#x2F;utility vehicles is valid, but we should punish the use of such dangerous vehicles when they don’t serve a purpose. A tradesmen owning a truck that he uses to transport materials and tools is one thing. The dude who works in sales at a software company and has used the bed once since he bought the truck to go camping is another.I’m not sure though, the overarching problem is that the US is vast, and what is good in one place is bad in the other.A large 4x4 truck is a menace in a dense city, but a car is crap in places with poor roads and snow. Both are common. reply dghlsakjg 12 hours agorootparent> A large 4x4 truck is a menace in a dense city, but a car is crap in places with poor roads and snow. Both are common.As someone who grew up in Colorado and now lives in Canada, a large 4x4 truck is a menace on poor roads and snow as well.Trucks are heavy, with poorly distributed weight, and are harder to control in snow if they have a true (locked center diff) 4x4 system. A Subaru with snow tires is the stereotypical mountain car for a reason. If I have to drive an hour through a blizzard I&#x27;ll take the car if given the choiceIf you need a truck, you need a truck, but there should be much stronger limits on what is CAFE exempt, and what you can use a CAFE exempt vehicle for. Perhaps since the exemption is intended to avoid hurting commercial interests, CAFE exempt vehicles should require a commercial license and insurance. reply dsego 12 hours agorootparentprevYou just need to make them look like Iveco Daily, VW Transporter or other similar european dropside trucks, which make more sense for businesses and no one will buy them for their attractiveness. reply ska 13 hours agorootparentprev> but a car is crap in places with poor roads and snow.That&#x27;s a false dichotomy. 4x4 cars often do better on poor roads and snow, lower center of gravity is better if you don&#x27;t need ground clearance, and weight distribution of a pickup isn&#x27;t great for snow etc. unloaded. Of course this assuming actual roads, not off-road.The whole 4x4 truck things is even more complicated - good off-road performance and good freeway performance are in many ways at cross purposes.The real intrinsic value of a pickup is open air bed space (easier access than a van, but less protected), I don&#x27;t mean they don&#x27;t do other things well, just that is what the form factor gets you. The way the market developed they have also turned into the vehicle of choice for towing, but that&#x27;s not intrinsic, just useful. Outside of 5th wheels, I suppose, where you need the platform. That isn&#x27;t most towing in that size.The % of light pickups is more a regulatory incentive and marketing phenomena than a reflection of real utility. reply kwhitefoot 12 hours agorootparentprevWhy should any vehicles be treated differently? Surely all should be expected to be safe.> A large 4x4 truck is a menace in a dense city, but a car is crap in places with poor roads and snow. Both are common.Then forbid trucks where the roads are good and where there is no snow. reply NegativeLatency 13 hours agorootparentprev> car is crap in places with poor roads and snowMy FWD Volvo Station wagon would like a word with youThere are (or were) plenty of small cars that did great off road that are no longer sold in the US, if I was looking for a new car and could buy one in the US I&#x27;d consider something like the Sizuki IGNIS where it&#x27;s got 4WD and can hold people comfortably. reply ponector 12 hours agorootparentOld Fiat Panda 4x4 is amazing cheap car. reply davchana 13 hours agorootparentprevCalifornia already treats every vehicle with a bed as commercial (& thus weight fee), very rare exceptions. Although there are subgroups within that rightfully based on vehicle weight. reply Enginerrrd 13 hours agorootparentprevThis won&#x27;t work because the people with the greatest need also have the least ability to pay. It would only make sense if you include a measure of how urban the area is. reply cheekibreeki2 13 hours agorootparentprevIn the usa we live in a largely lawless society I don&#x27;t know who is gonna enforce something like that when we barely go after armed teen carjackers. reply Nifty3929 12 hours agoparentprevThe correct answer is to not directly regulate autos themselves, but instead to implement a universal carbon emissions tax that would apply to any and all sources themselves (e.g. gasoline, propane, cement, etc), proportional to the amount of carbon emitted. No carve-outs or exceptions. No offsets. No protected industries. No change with scale (volume discount). Then you let people&#x2F;companies figure out all on their own how to respond to that tax.And yeah, the chicken tax along with all the other tariffs can go. reply xyst 14 hours agoparentprevWhy can’t CAFE regulations just be modified to include these modern SUVs and trucks?If the exclusions are removed then the manufacture cafe ratings across products will go down. Thus be forced to be more efficient and sell smaller cars. reply kelnos 14 hours agorootparentBecause no one in power wants to make the cars that most people want to buy more expensive. reply trgn 12 hours agorootparentPeople are stupid, and tastes can be manufactured. There&#x27;s nothing set in stone in human psychology that Americans need an SUV. People are only buying because the current incentive structures, convoluted and interwoven as they are, converge in SUVs and trucks as the current default. reply Nifty3929 12 hours agorootparentprevIt&#x27;s slightly different than that. The light truck sales essentially pay for all of the other regulations the politicians want. They regulate all other vehicles into unprofitability, but avoid destroying the manufacturers by allowing them to make all their money on huge \"light\" trucks.If they didn&#x27;t have this exception, they wouldn&#x27;t be able to enact all the other regulations without killing the auto industry. reply raydev 13 hours agoparentprevThis is constantly brought up as the primary cause, but I think people forget that SUVs and trucks started their expansion in the mid&#x2F;late 90s, before the modern CAFE regulations. The people demanded bigger and they got bigger.We had entire news cycles about the size of the Ford Excursion and GM&#x27;s Hummer 1 back in the day. reply CWuestefeld 12 hours agorootparentNo. CAFE regulations started in the mid 70s. As it&#x27;s evolved hand-in-hand with the auto industry, it&#x27;s led to the death of the station wagon and the rise of the minivan, and only later did the proliferation of SUVs start.Corporate average fuel economy (CAFE) standards are regulations in the United States, first enacted by the United States Congress in 1975,[1] after the 1973–74 Arab Oil Embargo, to improve the average fuel economy of cars and light trucks (trucks, vans and sport utility vehicles) produced for sale in the United States. -- https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Corporate_average_fuel_economy reply dragonwriter 12 hours agorootparentprev> but I think people forget that SUVs and trucks started their expansion in the mid&#x2F;late 90s, before the modern CAFE regulations.CAFE was adopted in 1975 and effective from 1978. Are you confusing \"modern CAFE regulations\" with the late 2000s narrowing of the \"car\" and \"light truck\" CAFE regulations (which, yes, did push automakers to try to push more vehicles that fell out of the \"light truck\" category, just as the earlier standards pushed from cars to light trucks), which has effectively shifted back the old gap after the actual (and more complex) modern (2011+) regulations came into play. reply kelseyfrog 14 hours agoparentprevGiven that that \"Chicken War\" ended in 1964, that sounds fairly uncontroversial. reply Pxtl 14 hours agoparentprevThis isn&#x27;t just CAFE. Even a giant road-tank could still have nice rounded low bumper that is compatible with pedestrians and other vehicles.I mean, an F-150 Lightning has great emissions. It will also crush me into chunky marinara it hits me with its brick-wall grill and 9000lbs of bulk. reply hristov 14 hours agoparentprevThe CAFE regulations are not causing this, in fact they are acting in the opposite direction, making vehicles safer. More aerodynamic vehicles are more fuel efficient and more likely to have lower and more sloped fronts and thus are also safer for pedestrians.Furthermore, vehicles with higher fuel efficiency tend to have smaller engines and thus also have shorter noses and more sloped hoods which make them safer for pedestrians.The only connection with the CAFE regulations you can make is that they are obviously not strict enough. Especially when it comes to trucks and SUVs. And I fully agree with this. Stricter CAFE regulations would help not only with emissions but with pedestrian safety.The main reason for the difference between a 1990s f150 and 2020s f150 is that people have been preferring to buy bigger trucks and Ford has catered to that preference by making the trucks bigger CAFE regulations be dammed. To their credit Ford has done some very good r&d work, trying to keep their ever larger trucks relatively fuel efficient, such as by their very well publicized conversion to aluminum. But they could and should be doing more and the government should be doing more to force them to do more. reply paulgerhardt 14 hours agorootparentThey are not making trucks bigger despite CAFE regulations. They are making trucks bigger because of CAFE regulations.This video details why: https:&#x2F;&#x2F;youtu.be&#x2F;azI3nqrHEXMThe short version is there is an exploit to meet standards by increasing footprint that is easier to implement than to make vehicles more fuel efficient. That exploit needs to be nerfed. reply sjburt 14 hours agorootparentprevStarting in 2014, trucks with a larger size were subject to less strict fuel efficiency requirements. This was because American automakers (who sell a lot more trucks) were unable to compete with foreign automakers that sold a lot of passenger cars that could bring their CAFE down.However, the government screwed up the scaling factor and it basically became impossible to build a small truck, while larger trucks had much easier-to-meet requirements. That led to automakers basically abandoning the passenger car market, and upsizing trucks significantly. reply raydev 12 hours agorootparent> However, the government screwed up the scaling factor and it basically became impossible to build a small truckFord left the third generation Ranger to rot for nearly 15 years because people kept choosing the ever-increasing-in-size F-150. Chevy discontined the S10 back in 2004 in favor of the larger Colorado.Customer preferences were already clear. reply weaksauce 14 hours agorootparentprevno, the regulations are to blame in a lot of the way. I&#x27;d love to have a tacoma that was the size of a small truck than the size of the tundra of the same era. the new tundras are absolute behemoths. there&#x27;s a curve that the auto manufacturers are having to hit and the curve tightens every year and the requirements are also tied to the sqft of the truck. so larger footprint == less stringent mpg requirement.https:&#x2F;&#x2F;www.thedrive.com&#x2F;news&#x2F;small-cars-are-getting-huge-ar... reply baggy_trough 14 hours agorootparentprevLeaving a giant loophole for trucks and very heavy vehicles is why they are causing this, by incentivizing a class of vehicles to get heavier than they otherwise would. replybeej71 14 hours agoprevOur Honda Fit was totaled by a Bigass Truck. Driver didn&#x27;t see it over the hood and A-pillar as we were approaching from a point downhill and right of their vantage. An entire car they didn&#x27;t see. I&#x27;m glad I was not on my bicycle or motorcycle. reply zenzen 14 hours agoparentI’m guessing they didn’t lose their license?In the US we’re terrified to take away someone’s access to a car because there are so few valid alternatives in most of the country. To the point where driving tests are basically a rubber stamp, and re-tests are nonexistent. reply beej71 7 hours agorootparent> I’m guessing they didn’t lose their license?They did not. Cited, points, and I&#x27;m sure their insurance went up.We got to fight with their insurance company to try to get enough money for another car and to cover medical expenses. That was fun.Even though there was no contest that the other driver was at fault, I started driving with a 4K dashcam after that. And I found something interesting: I started driving more carefully myself because if there was an accident, I wanted the record to show I was blameless. reply interestica 13 hours agorootparentprevAnd then there&#x27;s the absurdity of using a \"driver&#x27;s license\" as some form of universal government ID. reply mksybr 13 hours agorootparentIf you don&#x27;t have driving abilities you can get a state ID card. reply bmicraft 12 hours agorootparentYou mean a \"non-drivers driver licence\"? reply Sohcahtoa82 11 hours agorootparentYou do know that state-issued ID cards exist, right? They usually just say \"Identification card\" or something.Yes, they look similar to a driver&#x27;s license card, but so what?We have ID cards. We have driver&#x27;s license cards. The latter is so ubiquitous that there&#x27;s really no reason to require drivers to carry both. Just allow the second to be used as the first. AFAIK, in most states, they&#x27;re both issued at the DMV (or your state&#x27;s equivalent organization) anyways.I&#x27;m just really struggling to see what you&#x27;re trying to get at. Your question is posed in such a way that I&#x27;m interpreting it to be an attempt at some sort of \"gotcha\" question. reply joveian 6 hours agorootparentThey are often called driver&#x27;s licenses, though, because it is so common that the official state issued id is a driver&#x27;s license. In less formal language (in my experience) \"identification\" usually means random student id or bus passes or any picture id would be ok while \"driver&#x27;s license\" means official state id or military id. reply bmicraft 10 hours agorootparentprevIt was just a joke because I&#x27;ve heard those are a thing too replyWirelessGigabit 13 hours agoparentprevTBF, the A-pillar problem is not exclusive to big trucks.I drive a small BMW 2 series (M240i, not the M235i-based-on-mini-abomination).I have seen entire civilizations disappear in my A-pillar. I need to look around it like an owl moves its head. reply ortusdux 13 hours agorootparentI had someone on a one-wheel approach an intersection at the same speed as me and stay hidden behind my 3\" wide A-pillar the entire time. reply WirelessGigabit 6 hours agorootparenthttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SYeeTvitvFUInteresting intersection that had this problem (now fixed). reply ajuc 14 hours agoprevSUVs should cost like 10 times more than regular cars to insure. They destroy the environment and kill people and the only benefit is slightly better status signaling. reply nickthegreek 14 hours agoparentThey obviously have real tangible benefits. I regularly use my SUV to haul stuff that would not fit in a car while also maintaining the ability to fit passengers when I am not using that space. While being lighter than a van. reply ajuc 13 hours agorootparentMy grandpa hauled more in a cheap trailer behind a 23 HP, 650 kg Fiat 126p than you can fit inside an SUV. It was a little extreme but that&#x27;s what everybody had.You can fit more in a station wagon like VW Passat than in an SUV and it will be more efficient AND safer.There&#x27;s no reason to waste fuel every trip accelerating all that useless mass other than fashion. reply nucleardog 7 hours agorootparentI don’t know why, North America just doesn’t “do” trailers. It’s pervasive—many vehicles that are rated to tow a pretty reasonable amount in other countries (UK, Australia) say not to tow with them in Canada and the US.My wife’s car is rated to tow 1,650 lbs unbraked… in Australia.In North America, it’s rated “No.”.Everyone here thinks towing == big truck, and I can see why they would think that. When I moved to an acreage everyone (including my wife) told me it was time to sell the “fun” car and buy a truck.Instead I spent $500 on a 5x10 utility trailer.I regularly tow that 5x10 utility trailer… with a Subaru WRX sports sedan. The thing weighs 3,300 lbs, has something like 280HP and 260ftlb peak torque, massive brake rotors, etc. At a 10% tongue weight, I can hit nearly 3000lbs on the rear end.It accelerates and brakes like there isn’t even anything there. The only time I noticed any real difference was when I had ~2500lbs on there between the trailer and some concrete patio blocks. I had to downshift from 6th to 5th going up a steep incline to maintain a bit over 60mph.I can fit a full 4x8 sheet of plywood or a stack of 8’ lumber in there without it hanging out the back (unlike most trucks). My car gets notoriously bad mileage, yet I still get better fuel economy towing the trailer than my wife’s crossover does carrying two adults. And when I’m done I take the trailer off and get sedan fuel economy.Oh, and it can very comfortably carry four adults while doing this (five if someone wants to cuddle up in that middle seat). Rear seat will fit three car seats at once if your life has taken that sort of turn.In the _very_ rare situation that that’s not enough… I can go rent something.I have way more respect for people that can at least admit “I’m going to pay a $40k premium and burn twice as much gas because I want to.” rather than try to justify it as “I need to haul something twice a year and U-Haul doesn’t exist.” or “Yeah, but I need it for the snow.”. (That’s a whole separate rant, but snow tires people. I’ve used my car to pull Jeeps out of the ditch because they thought 4wd + mudding tires was a good winter setup.)That went a little off the rails but… yes, trailers are the answer you’re looking for. reply trgn 12 hours agorootparentprevExactly. This exact combo used to be captured by a trailer. reply mitthrowaway2 14 hours agorootparentprevMinivans usually have a very low front hood with decent visibility. I&#x27;m not sure why SUVs don&#x27;t have the same. reply hnav 14 hours agorootparentBecause it makes them look uncool. See how Mazda discontinued the 5 for a bunch of CUVs that fit less stuff in the same footprint. Somehow a lifted compact car with slats for windows is cooler to the average person. reply dzhiurgis 13 hours agorootparentprevElectic minivan costs over 100k, while tesla model y around 40… reply tecleandor 12 hours agorootparent100k? Doesn&#x27;t a Ford E-Transit go for around 50K?Although I feel like there&#x27;s more variety here (Europe) for electric minivans having Citroen, Nissan, Ford, Fiat, Mercedes, Opel... electric models that don&#x27;t seem to arrive to (or survive in) the US market. reply dzhiurgis 10 hours agorootparentVolvo EM90 is $114,000 reply tecleandor 9 hours agorootparentBut that&#x27;s not even available in the US! reply mitthrowaway2 13 hours agorootparentprevEVs usually feature a frunk, so I don&#x27;t see any reason they can&#x27;t have a nice low front hood with good visibility and pedestrian safety. reply dzhiurgis 10 hours agorootparentIt&#x27;s impossible to even see front hood on tesla model y which sorta sucks for parking reply ddlatham 12 hours agorootparentprevYou can get the Pacifica plug-in hybrid for about 50k before tax credits, or a used one for 25k. reply dzhiurgis 10 hours agorootparentAaah, the worst of both worlds. reply lotsofpulp 14 hours agorootparentprevA minivan can fit more people and has more cargo room than an SUV.There is zero reason to get an SUV other than status signaling, and&#x2F;or wanting to be the bigger vehicle in a collision. reply zombielinux 13 hours agorootparentNope.My SUV has the capability of hauling all the people and their stuff I regularly have to while ALSO towing a fully loaded horse or stock trailer.I skip the missing pickup truck bed by using a trailer (that I can tow with our old car too).I fully recognize its size and how if I didn&#x27;t have a farm it would be 100% unnecessary. reply ajuc 12 hours agorootparenthttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=q0HeV1zHYhcThis car has 23 HP and weights 650 kg. It&#x27;s a little extreme, but any modern car can haul a trailer no problem. You don&#x27;t need an SUV. reply scruple 7 hours agorootparentNo, but I do need to put 3 kids into car seats and carry a lot of stuff at the same time. I don&#x27;t have many great options. I sincerely don&#x27;t understand what people with 4 or more children do in these situations, work vans? I&#x27;d love to get an 80s station wagon, honestly, but those don&#x27;t exist. reply nucleardog 7 hours agorootparentprevTaken to an absurd extreme: https:&#x2F;&#x2F;www.motor1.com&#x2F;news&#x2F;583580&#x2F;toyota-prius-gooseneck-hi...(It’s a Prius towing a goose neck trailer.) reply 1-6 12 hours agorootparentprevIs it a SUV by the true definition of one or a crossover?The term SUV has been manipulated to the point that unibody sedans on larger wheels now qualifies as an SUV at a dealer showroom. reply nickthegreek 14 hours agorootparentprevMore people at more weight for many minivans. My vehicle is the size I need for my needs. I don&#x27;t need to hold 8. Why do I want to add another 1,000lbs to lug around. Your answer is worse for the environment than an SUV. reply lotsofpulp 13 hours agorootparentYou might be referring to compact SUVs. 3 row SUVs are basically the same weight or more than minivans. reply nickthegreek 13 hours agorootparentOh wow, there are like 5 different classes of \"SUV\". This only further complicates conversations around this vehicle class. I drive a VW Tiguan, which appears to fall under the compact SUV. replyjjav 9 hours agorootparentprev> I regularly use my SUV to haul stuff that would not fit in a carDepends on which SUV and which car, granted, but on average SUVs don&#x27;t have that much interior room. Particularly given their huge size one would expect them the have huge cargo capacity, but most of them don&#x27;t hold more stuff than your average sedan.> While being lighter than a vanNot usually. According to [1] mid-size SUV average around 5000lb and full-size SUVs are 5300-6000lb.Vans are from ~4000lb to ~6000lb with most models around ~5000ish. So weights are pretty much the same. Unless you have a tiny SUV, but then it can hardly carry anything.[1] https:&#x2F;&#x2F;www.jdpower.com&#x2F;cars&#x2F;shopping-guides&#x2F;how-much-does-a... reply liamwire 9 hours agorootparentprevAllow Australia to introduce the concept of: the ute. Functional tray and carrying capacity, without the pedestrian murder potential of the Yank Tanks™. reply morepork 12 hours agorootparentprevA station wagon (essentially the same vehicle, but not as tall), or a minivan could do the same, surely? The only thing an SUV is better at than those vehicles is some serious offroading reply alamortsubite 11 hours agorootparentEven then, SUVs and pickup trucks are poor choices for off-roading, due to being extremely heavy and having long wheelbases. For driving on dirt roads, they&#x27;re fine, but then so is a station wagon. reply hnav 14 hours agorootparentprevWe need wagons&#x2F;estates, but those only do well in select markets in the US, pretty much only blue intellectuals would buy one over a crossover. reply lotsofpulp 14 hours agorootparentSienna&#x2F;Odyssey&#x2F;Pacifica&#x2F;Sedona cover this segment. reply eep_social 13 hours agorootparentI disagree, I think a wagon or estate can be designed to look cool, like the Audi RS6 Avant, to take it to the extreme, a minivan not so much. reply slt2021 13 hours agorootparenthave you seen Kia Carnival ? reply eep_social 8 hours agorootparentI had not! Looks like it was the Sedona in North America until recently, which I have seen. Kinda looks like a minivan wearing a SUV trench coat. Not bad but I’m not in love ;) reply bdavbdav 13 hours agorootparentprevThey look goofy next to German wagons. replyverve_rat 13 hours agorootparentprevI lament the loss of station wagons. reply everybodyknows 13 hours agoparentprevWell, there is the penalty at purchase time of the \"gas guzzler tax\":https:&#x2F;&#x2F;nepis.epa.gov&#x2F;Exe&#x2F;ZyPDF.cgi&#x2F;P100F3YZ.PDF?Dockey=P100...But guess what, the rates have not been raised since 1991! There&#x27;s been some CPI inflation since then. The irrationality of Congressional energy policy continues evergreen. reply everybodyknows 11 hours agorootparentLooking again, current regulation is even worse than first appears:> This tax does not apply to minivans, sport utility vehicles, and pick-up trucks. Congress did not impose a gas guzzler tax on these vehicle types because in 1978, at the time the law was enacted, they represented a relatively small fraction of the overall fleet of passenger vehicles and were used more for business purposes than personal transportation.And the proffered list of current vehicles is a 404!www.epa.gov&#x2F;fueleconomy&#x2F;guzzler reply cvccvroomvroom 14 hours agoparentprevThey&#x27;re also more dangerous to their occupants than mid-sized sedans.There should also be an SUV tax. reply 1-6 12 hours agoparentprevNot only do they destroy the environment, they tear up paving with the gross vehicle weight and heftier tires. reply bdcravens 14 hours agoparentprevA lot of people drive compact SUVs, and most of the planned EVs fall into that category.I feel like sporty cars like the Mustang, Charger, modified Civics, etc are a greater risk. reply Pxtl 14 hours agoparentprevWhy outsource that cost to the private sector? Make a point-system for external-risk value on the road to other vehicles, pedestrians, etc. Figure in mass (yes, Teslas are risky too with their mass), bumper compatibility (SUV bumpers don&#x27;t hit car bumpers), thick a-pillar blind-spots, pedestrian-hood-impact-behavior, frontover risk, stopping distance, etc.Then assign vehicles with high external-risk-score a stricter regimen for infractions like speeding and higher registration fees.You drive a Yaris like an absolute maniac? You can get busted a lot more times than the guy in the Rivian before they take your license away. reply the_doctah 14 hours agoparentprevThey are better at snow driving reply bdcravens 14 hours agorootparentThen that leaves a huge part of the US that has no (or an extremely rare) need for that advantage. I can&#x27;t haul a couch in my sedan, but I&#x27;m not going to buy an F-250 \"just in case\" reply fooker 13 hours agorootparent2023 saw more than 70% of the lower 48 states covered in snow. If you include Alaska, the largest state, it’ll increase a bit more. reply bdcravens 13 hours agorootparentLarge states like Texas may as well be considered several states for purposes of identifying trends. Houston hasn&#x27;t had any snow since the freeze in 2021; we commonly go several years without snow that doesn&#x27;t melt in an hour. So again, it wouldn&#x27;t make sense to base a purchase on a rare event.I&#x27;d like to see a statistic that shows what percentage of the US has persistent snow (say, 24 hours). reply lapetitejort 13 hours agorootparentprevWhat percentage of counties? My state got snowed on because it has a mountain range, but not my county since it&#x27;s in the desert near sea level. reply kelnos 14 hours agorootparentprevMy AWD sedan is only worse than an SUV for snow driving in a single dimension: ground clearance. I can&#x27;t drive over snow berms quite as high as those an SUV can. In practice this doesn&#x27;t matter much, and the sedan is more (vertically) stable in the snow than an SUV. (Yes, I&#x27;ve driven both in these conditions, and vastly prefer the sedan.)The main issue is that AWD&#x2F;4WD in sedans isn&#x27;t that common, while they&#x27;re nearly standard on an SUV. But that&#x27;s a matter of market dynamics (people think that SUVs are required for this sort of driving, which is laughable), not any inherent technological or cost limitation. reply mrmuagi 13 hours agorootparentprevTires make or break snow driving, no? reply jkaptur 14 hours agorootparentprevThey&#x27;re really no better than plenty of reasonably-sized cars. reply sneeze-slayer 14 hours agoprevGreat, now let&#x27;s see the NHTSA (in the US) implement some pedestrian safety rules in car requirements. We have known this for years and just watched the pedestrian fatalities surge upward. reply cvccvroomvroom 14 hours agoparentNever gonna happen. $$$$. reply DoingIsLearning 12 hours agorootparentAs a European I was recently disappointed to see Biden doing a photo op in an F-150.I get that it was an electric variant and aligns with the green message. But to see the political gravitas of the POTUS backing up a design that arguably should not be road legal is very disappointing.But indeed $$$ reply kbar13 15 hours agoprevit&#x27;s so scary when i see people on the streets driving vehicles where they can barely see over the hood. like i can only see the top of their head when i&#x27;m facing them head on. this means that they cannot see me at all.how on earth is anyone comfortable driving around not being able to see where they&#x27;re going? reply vineyardmike 14 hours agoparent> how on earth is anyone comfortable driving around not being able to see where they&#x27;re going?Because they’re not the one that’s going to get hurt.But seriously, there’s some psychological thing going on with big cars. Like I drive a tiny car by American standards, and honestly don’t feel confident driving a big car, but I get it. You get to be higher up, which feels like you can see easily (even if objectively not true). It feels safer to drive a tank when every other car is huge (this is probably true). They have more storage, more space, which feels more comfortable. There’s probably nothing more damning than the incentive you have to be the biggest car on the road in case an accident happens.Of course intellectually it’s obvious that big cars make roads dangerous overall, they’re bad for the environment, they’re more expensive, they require a bigger garage, etc, but it’s hard to get past feelings and bad incentives. reply david422 13 hours agorootparent> Because they’re not the one that’s going to get hurt.It&#x27;s the prisoners dilemna. 2 small cars crash, they are both better off. 1 big car and 1 small car crash, the big car is better off. 2 big cars crash, they are both worse off. People choose the big one. reply MarkusWandel 14 hours agorootparentprevFriends of ours who lived for a long time in Scandinavia occasionally get Scandinavian visitors. Two out of two times so far when they&#x27;ve dropped by with said visitors in their rental vehicle to borrow the canoe, the rental vehicle has been the absolutely biggest, bad-assest one they could get a hold of. The kind where you&#x27;d have to stand on a stool just to really look into the engine compartment. Why? Well, yes, it costs more to rent and more in fuel, but it&#x27;s just so... something.Luckily here in Ontario, Canada, small cars are still somewhat normal, with about 1&#x2F;3 of the road traffic being in the \"compact car\" class and certainly less than 1&#x2F;3 in the \"gratuitously huge truck&#x2F;SUV\" class. Most inbetween is compact SUVs and minivans. reply Symbiote 12 hours agorootparentSeveral friends and colleagues in Denmark have avoided driving in North America because the available cars were so huge.A colleague and I cancelled a rental for a work trip (took taxies instead) as the only vehicle they had was some huge pickup, which neither of us were willing to drive.I think it&#x27;s far more common for Europeans to be put off from driving huge American vehicles than attracted to it. reply bertil 14 hours agoparentprevI remember seeing a prankster passing in front of a car and miming stepping over a big obstacle. The driver looks concerned and steps out to check. Laughs.Until I realized: they genuinely had to. The driver was not checking because they saw the guy jumping over nothing and were confused: they literally could not see what was below that guy’s belt. It’s one of those things (like “ads for tax prep software” or “kindergarten-appropriate shooter drill”) that feels quaint where you don’t connect the dots because you don’t know the country, but when you do… reply kayodelycaon 13 hours agorootparentIt&#x27;s really at the point where you need a bunch of cameras to get a 360° view of what&#x27;s around your car.I&#x27;d never realized how bad the visibility was before having a car with that feature. I&#x27;m surprised I haven&#x27;t run over anything important in all of the years I&#x27;ve been driving. reply bertil 11 hours agorootparentIf you want a violent contrast, cycling is out there. Most cars feel like submarines after riding a bike. reply HeyLaughingBoy 12 hours agorootparentprevIn 1990 I watched a pickup truck crush its radiator into a parking bollard that the driver couldn&#x27;t see. This is by no means a new problem. reply jjulius 15 hours agoparentprevSimilarly, it always weirds me out when I&#x27;m behind someone and can&#x27;t see their head or at least part of their face in their rearview. Tells me that they probably can&#x27;t see what&#x27;s behind them. reply barbazoo 13 hours agoparentprevAlso stuff on the dash like stuffies, religious paraphernalia, darkened windows, it&#x27;s clear that some people don&#x27;t care about seeing others. reply hospitalJail 14 hours agoparentprev>how on earth is anyone comfortable driving around not being able to see where they&#x27;re going?Its not about comfort, its about style.We see this in the tech world too. You can get a high tech, cutting edge feature rich, low cost, easy to use device. Or you can get one that has the cool logo on the back that makes your text messages turn blue.It creates a negative externality because people are using outdated&#x2F;dangerous&#x2F;worse technology simply because its the style. reply achenet 1 hour agorootparentto be fair, having used both Android and iOS devices, my experience is that the former will tend to have more frustrating software bugs. reply Izikiel43 8 hours agorootparentprevAlso, the one with the logo and blue messages has guaranteed software updates for like 5&#x2F;6 years, has great performance and is very comfortable to use, but who cares about that right? reply 1-6 14 hours agoprevHonda has been pioneering lower front-end vehicles for years now. It&#x27;s up to auto manufacturers to agree upon common bumper to bumper height so vehicles can sustain collisions.Beside the overall looks, safety-wise, when I look at Cybertrucks, I cringe because they&#x27;ve abandoned the pedestrian knee height rule.https:&#x2F;&#x2F;www.repairerdrivennews.com&#x2F;2019&#x2F;05&#x2F;29&#x2F;capa-discusses... reply MetaWhirledPeas 14 hours agoparentWhen I look at this side-by-side photo I see the Cybertruck hood being lower than its competitors.https:&#x2F;&#x2F;encrypted-tbn0.gstatic.com&#x2F;images?q=tbn:ANd9GcQTeRP0... reply elil17 14 hours agoparentprevWhy should it be up to auto manufacturers as opposed to regulators? reply 1-6 13 hours agorootparentI’d rather have auto manufacturers figure out creative solutions to this problem rather than screw up auto designs for generations because of some broad-sweeping regulation. reply elil17 12 hours agorootparentWhat if it was something like crash safety testing, where an outcome (forces on a crash test dummy) is mandated rather than a specific design? You could have something like a visibility requirement (for instance, driver can see the pavement three feet ahead of their front bumper) and then car companies could figure out how to comply with this (lower the height of the vehicle, make a larger windshield, etc). reply cvccvroomvroom 14 hours agorootparentprevUnsafe at any speed. reply kps 13 hours agoparentprevCommon headlight height, while you&#x27;re at it. reply 1-6 12 hours agorootparentAlthough nice in practice but I’ve seen some Jeeps and Hyundai Konas with lights that look upside down (LED daytime lights on top, halogens on the bottom). All I can say is that they look ugly. reply jocaal 12 hours agoprevThe fact that the market for cars is shifting in the direction of gargantuan pieces of metal is what finally convinced me there is nothing that can be done about global warming. Forget pedestrians, if everyone is starting to drive around in cars 1.5x - 2x the weight of the average car a decade ago, we are gonna waste a lot of energy. reply 1-6 12 hours agoparentSometimes the decision to move to a bigger car is triggered by roads that have been deteriorating as a result of aging infrastructure.My decision to move up to a larger car with bigger tires is based on my experience of Bay Area roads. reply Mawr 11 hours agorootparentI have terrible news. Your new, bigger and heavier vehicle damages the roads disproportionately more than the old one: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fourth_power_law reply 1-6 11 hours agorootparentIf that’s the case, heavy SUVs and heavy EVs should be taxed for causing heavier damages to roads.EVs bypass the gas tax so I guess there’s that to look into. reply DoingIsLearning 12 hours agorootparentprevYou are post-rationalising your decision and unless &#x27;Bay Area roads&#x27; approximate Russian trunda you are very much part of the problem. reply ponector 12 hours agorootparentprevIt&#x27;s funny to hear such reasoning. Why emerging markets are dealing ok with bad roads and small cars? reply stevenwliao 2 hours agorootparentEmerging markets don&#x27;t buy small cars as a conscientious environmental decision, they buy them because it fits their budget. Small cars cost less than large cars to buy and run. reply 1-6 12 hours agorootparentprevEmerging markets actually have newer roads in comparison to developed markets. reply DoingIsLearning 12 hours agorootparentFlawed logic, most emerging markets have new road infrastructure only around main arteries and large city connections.Unfortunately also road construction quality is lacking because of low budgets and&#x2F;or corruption leading to faster degradation, often new roads are heavily damaged after a few rain seasons. reply ponector 10 hours agorootparentprevIn the capital cities - maybe. But outside? You can travel on street view to Egypt or Russia, there are bad roads and no off-road vehicles. reply I_Am_Nous 12 hours agorootparentprevWhich sucks because that just makes the roads crumble faster. The extra weight of EVs is a concern since they ALSO won&#x27;t pay the highway tax that&#x27;s included in the price of gasoline. reply 1-6 11 hours agorootparentAgree with you there. The proposed solution? All lane freeway tolls https:&#x2F;&#x2F;www.ktvu.com&#x2F;news&#x2F;renewed-push-for-all-lane-freeway-...A car’s carbon footprint should also measure the wear and tear it puts on infrastructure. Plus, tire manufacturing is also petrol-rich. Last time I read, Model 3s go through tires very quickly. If that’s the case, EVs aren’t always the best solution.My humble Prius sips gas, aerodynamically cuts through wind, and is overall lightweight but a giant Lucid Air, Tesla Model X, or Rivian is considered ‘greener.’ It’s hip to look like you’re saving the world but really, it’s greenwashing. reply I_Am_Nous 11 hours agorootparentI&#x27;m honestly surprised there aren&#x27;t more plug-in hybrid options. It seems like the logical stepping stone for car manufacturers because they can keep most of their existing production lines intact, the cars weigh less because they have smaller battery banks so they need less rare earth metals per car compared to a full EV, it reduces emissions compared to a standard ICE vehicle. I guess the greenwashing marketing has placed EVs at the top in people&#x27;s minds. reply api 11 hours agorootparentYou lose the near zero maintenance advance of EVs, which is pretty huge. No more oil changes and like 10% the moving parts. reply I_Am_Nous 9 hours agorootparentThat is true, but there could be a reduction of maintenance on hybrids as well, especially if the majority of stop&#x2F;go driving is handled by the electric motors. Synthetic oils are sometimes rated at 10k miles before needing changed, so the ICE does still have maintenance but it&#x27;s not so constant.It seems like a sensible middle ground because hybrids are also cheaper than EVs in the US so it can get more pure ICE cars off the road. replygiraffe_lady 12 hours agorootparentprevGood lord that&#x27;s precious. Mind-boggling you consider this a weighty and valid justification. reply 1-6 12 hours agorootparentI’ve driven compact cars all my life. Uneven pavement, potholes, dried&#x2F;crusted&#x2F;rocky asphalt affects my car more than someone with a larger sedan. reply giraffe_lady 12 hours agorootparentI believe you my surprise is more that someone would admit to making such a damaging choice for such a pathetic reason. replyMarkusWandel 15 hours agoprevThe thing is the \"blunt vertical front end\" thing has even taken over in the small car segment. My new 2023 Honda Civic, while not suffering in terms of forward visibility, has gone for the \"squared off\" look vs. the previous (2012) one. Here they are side by side...https:&#x2F;&#x2F;wandel.ca&#x2F;pic.cgi?81b4b0ec reply ryandrake 14 hours agoparentUS car and truck designs have been trending towards \"belligerent\" looks for decades. My totally unqualified armchair sociologist&#x27;s theory is that these designs are closely tracking some overall American cultural trend towards aggression and hostility. As our society becomes more and more selfish assholes, our cars are aesthetically and functionally becoming assholes, too. reply roughly 13 hours agorootparentAn oldie but a goodie: https:&#x2F;&#x2F;popula.com&#x2F;2019&#x2F;02&#x2F;24&#x2F;about-face&#x2F; reply lawlessone 13 hours agoprevIt creates a feedback loop too. You see a lot less in traffic when all the cars around you are SUV&#x27;s... so you buy an SUV.Then you get some less self aware people complaining because they think there is some grand conspiracy to shrink parking spaces and roads.Neither have changed, your car got bigger and it&#x27;s changed your perception of the environment around you. reply design-and-code 13 hours agoprevIncreasingly heavy vehicles create a safety arms race that incentives every vehicle on the road to get bigger and heavier in hopes of surviving a collision with one of these behemoths.Maybe instead crash safety regulations should hold vehicles responsible for absorbing their own kinetic energy. You want to make a 6000 pound SUV, well then you need to engineer in ways to reduce the damage it does to smaller passenger cars. reply falcolas 15 hours agoprev> While sloping front ends did not reduce the risk posed by vehicles with the tallest hoods, they did make a difference for vehicles with hood heights of 30-40 inchesAnd these benefits is fairly likely to immediately be countered by a lift kit by the end user, bringing them back up against the 40\" problem range. It&#x27;s amazing to me how often lift kits are added to pickups; amazing as in \"it&#x27;s not actually adding to the ground clearance beneath the diff, it&#x27;s mostly cosmetic.\"Anyways. Good article, but I have serious doubts their advice would ever be followed. reply Arcanum-XIII 14 hours agoparentAnd those lift kit are more than probably not legal in Europe. I’m amazed (and a bit jealous) of the amount of customization possible in the US. I understand the need of regulation but… it’s way less fun. More secure though, so, I’m conflicted.In my country changing the color of the vehicle will trigger a need to recertify the vehicle. For the fucking color. reply falcolas 14 hours agorootparentGiven some of the absurdities I&#x27;ve seen personally on the road in the Midwest US, I personally wish for EU limitations. And yeah, I&#x27;m a boring \"old\" fogie who drives a Prius.From rolling coal to 2&#x27; (.6m) lift kits, to effectively black windows and turn signals, to light bars used offensively against other drivers, reduced diameter dualie road tires with insufficient suspension support for all that torque... and that&#x27;s just pickups.Yeah. The roads are perhaps not the place for this kind of weaponized \"fun\". reply maerF0x0 14 hours agorootparentalso these kinds of wheel modifications&#x2F;styles which essentially turn them into meat grindershttps:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Austin&#x2F;comments&#x2F;8rjfu4&#x2F;wheel_spikes... reply Sohcahtoa82 11 hours agorootparent\"How is this legal?\"It&#x27;s not. But the law is only as strong as its enforcement. reply baz00 12 hours agorootparentprevWhat absolute fucktard thinks that&#x27;s a good look?!?!?! reply barbazoo 13 hours agorootparentprevI don&#x27;t think many people care if these adjustments are legal simply because there is no enforcement. Here in BC, tinting your windshield or front side windows is illegal, tons of people do it anyway. You&#x27;re supposed to have a license plate in the front, lots of folks here don&#x27;t do that either. No one cares because police doesn&#x27;t enforce any of the rules. reply iambateman 13 hours agoprevAs the parent of two young kids, this report is both obvious and heartbreaking.It’s so frustrating to see trucks and SUV’s which look like tanks driven 65mph down a road with a 35mph speed limit which I walk across daily. reply conductr 12 hours agoparentDoes it make you less upset when you see small vehicles driving 2x the limit on that same road? reply trgn 12 hours agorootparentYes. They carry half the momentum. So I&#x27;m half upset. reply jjav 9 hours agorootparentUltimately, does it make any difference?If the 7000lb monster SUV drives over you at 65mph, you&#x27;re dead.But if an old 1800lb Honda Civic drives over you at 35mph, you&#x27;re still dead. reply tmnvix 7 hours agorootparentDifference is you&#x27;re much more likely to be flattened and go under a vehicle with a large, high front-end. reply alamortsubite 10 hours agorootparentprevSo long as F=ma, definitely. reply za3faran 7 hours agorootparentKE = 1&#x2F;2 mv^2 is the relevant one here. reply I_Am_Nous 12 hours agoprevThe huge Ford Excursion had an initial design flaw where it would do cool stuff like monster truck over smaller cars in a head on collision>During the development of the chassis, Ford learned that its initial design caused smaller vehicles (such as a Ford Taurus) to become severely overridden in a head-on collision. In the test, the tire of the Excursion drove up to the windshield of the Taurus (reducing the chance of survival for its driver). As a response, Ford modified the chassis to include an under-bumper \"blocker beam\"; the device was initially tested by the French transportation ministry in 1971. For the rear of the chassis, Ford chose to include a trailer hitch as standard equipment in production to reduce underriding in rear-end collisions by smaller vehicles.We&#x27;ve known huge vehicles are less safe for people other than the driver&#x2F;passengers for at least 20 years. There&#x27;s similar pushback on automatic driving cars...nobody wants to drive a car they know might make a choice to kill them to save others, and most people want their car to protect them at the expense of others. reply CalRobert 14 hours agoprevSadly, even in places that are supposed to be reasonably safe for pedestrians, high, large, heavy vehicles are becoming more and more common. The Netherlands has a suprisingly high frequency of Dodge Rams. reply mikrotikker 12 hours agoparentThey&#x27;re such a nice vehicle I wish I could own one. Especially the special models like the super bee or the srt10. Pretty sure you need to upgrade the tie rods if you want to do anything serious in them tho. But for me it&#x27;s about as practical (but more preferable) as a Ferrari, since in my country the price can be on par with a Ferrari which makes then fancy shiny trucks more than a productivity vehicle reply CalRobert 12 hours agorootparentThey&#x27;re probably great fun in the right setting but the narrow streets where my six year old daughter is biking to school isn&#x27;t it. reply everdrive 14 hours agoprevPeople rightly complain about men who want a “big strong masculine” truck for no reason other than bravado. But, people tend to forget that women are the primary ones who want the other problematic category of vehicle: SUVs. They wrongly believe them to be safer, (in other words, they buy them so they can feel safer) and drive 80% of all new vehicle purchases. reply Sohcahtoa82 13 hours agoprevThe people driving those types of vehicles simply do not care. As far as they&#x27;re concerned, whatever fate the pedestrian has after they hit them is not their problem. If anything, the killing of a pedestrian that dared to step in front of their path is probably a feature to them. reply mikrotikker 12 hours agoparentCan confirm. I see it as aiding natural selection. There should be consequences for not paying attention and stepping into traffic. reply liamwire 9 hours agorootparentDoes your belief in this hold true for children, the disabled, the visually impaired, etc? reply HumblyTossed 14 hours agoprevWait, I thought we got the higher blunter front ends because those were deemed safer for pedestrians.Edit: My understanding (from memory) was that the lower, pointier front ends would kneecap pedestrians, causing them to impact the windscreen. Because the pilars around the windscreen are not designed to collapse, that is worse for the pedestrian. So, manufacturers made blunter front ends that would collapse in an impact. reply WkndTriathlete 12 hours agoparentMaybe on the planet Krypton where you live and everyone is like Superman you can get a steel front end to bend around you when it hits you, but here on earth hitting a person with that blunt front end is like taking a sledgehammer to a water balloon with about the same effect. reply HumblyTossed 8 hours agorootparentStop it. reply rconti 11 hours agoparentprevYup, I clearly remember the EU regulators pushing for higher front ends. reply nforgerit 14 hours agoprevcaptain obvious here: put a pedestrian next to an SUV and you see the heights nicely match the most vulnerable parts of the pedestrian&#x27;s body. add emotions (due to cultural wars) and significantly worse oversight and you get what we have. shame on everyone who was part in this kind of \"progress\". reply bertil 14 hours agoprevGiven how manufacturers love advertising how aggressive, violent, and deadly their cars are, I’m not sure they will try to bury those results. I’m 50&#x2F;50 on them trying to change the news cycle on those keywords vs. going all in and hiring more ad executives from firearms companies, and starting to talk about penetration and resistance.Think I’m off? What’s the next hot car coming out next month? Did it try to spin up the release with some word-of-mouth advertising “leaking” a photo of that car riddled with bullets on social media? Has that brand led the market in terms of practice in the auto industry? reply josefresco 14 hours agoprevDoesn&#x27;t help that the current trend in SUV design is the boxy \"Range Rover Defender\" look.*https:&#x2F;&#x2F;www.landroverusa.com&#x2F;defender&#x2F;index.html reply lucb1e 14 hours agoprevOne feels like one has a lot better overview with a higher vehicle. A higher, more vertical front doesn&#x27;t seem a necessity for that, though. I wonder if a combination might have the lowest risk due to the overview combined with survivable hood (if one can&#x27;t just take public transport in the first place). Are there vehicles like that? reply trgn 12 hours agoparentIf it&#x27;s overview you want, you&#x27;d want more glass, larger windows, heck, cab forward&#x2F;over designs. All these big vehicles are submarines once inside. reply nedt 13 hours agoprevNo shit sherlock. After the decades of creating safety for the people inside the vehicle it&#x27;s time to care for the others. You have airbags, belts, what not ... But that only makes you more careless, because you are safe in case of an accident. Now do the same for the cyclists or pedestrians you might hit. Don&#x27;t tell me that can&#x27;t be done. Make the car slow when pedestrians are around you (20mph) and brake automatically before hitting someone and if it can&#x27;t be avoided give them a soft cushion hit. If it can&#x27;t be done that vehicle better stays out of any city. reply elzbardico 14 hours agoprevBeing run over by a steel wall is more dangerous than sloping over a steel ramp. Who could imagine that? reply t0bia_s 11 hours agoprevIt&#x27;s interesting phenomena. We care about CO2 consumption and yet, produce, buy a drive heavier vehicles to transport every year.It&#x27;s hard to believe, that we really care. reply avar 14 hours agoprev> \"Pedestrian crash deaths have risen 80 percent since hitting their low in 2009.\"It&#x27;s way too common to read articles where clearly cherry-picked prose takes the place of a more informative graph, only to lookup said graph and find that the article seems to be trying to intentionally distort the data to make some point.Here&#x27;s more complete data, the US population was 250 million in 1990, 330 million in 2021. So the 7400 that year was ~8400 in 1990 if it&#x27;s adjusted for population growth:https:&#x2F;&#x2F;www.planetizen.com&#x2F;news&#x2F;2019&#x2F;07&#x2F;105095-us-pedestrian... reply advisedwang 14 hours agoparentThat article lists the minimum death rate at 4109, and 7400 today. So it went from 4109&#x2F;250000000=1.64x10^-5 to 7400&#x2F;330000000=2.24x10^-5. Thats an increase of 37%. That&#x27;s still shocking. reply avar 19 minutes agorootparentSure, but now we&#x27;ve gone from an increase of 80% to less than half of that just by adjusting for population.Now, why do you think they picked 2009? Were cars magically safer that year, or perhaps because it&#x27;s the year after the economic crash, and therefore people were simply driving less?Which was my point, not that there&#x27;s nothing to be said about vehicle design contributing to this, but that it&#x27;s annoying to read articles where the authors are clearly massaging the data to fit their agenda. reply praestigiare 14 hours agoparentprevThis is not a significant change, the trend still clearly shows an increase since the low in 2009. reply quadrifoliate 14 hours agoparentprev> It&#x27;s way too common to read articles where clearly cherry-picked prose takes the place of a more informative graph, only to lookup said graph and find that the article seems to be trying to intentionally distort the data to make some point.Huh? The article doesn&#x27;t say anything about 1990. It says they rose by 80% after hitting their low in 2009. Population rose steadily from 1990 to 2023, so if you are trying to cite population increase as a factor in the rise from 2009 to 2023, you also need to explain how it didn&#x27;t contribute to the drop from 1990 to 2009.I would say that, actually, you are (perhaps unintentionally) distorting the data to fit your worldview while trying to criticize the article. reply zemo 14 hours agoparentprevdid an suv write this reply hateful 14 hours agoprevAnd then they back into parking spots! Every car since 2016 comes equipped with a backup camera that gives a wide view, but now they see nothing when they pull out! reply mikrotikker 12 hours agoparentYea backing them into parking spots is a lot easier with a bigger vehicle and I&#x27;m guessing the camera helps then see if they&#x27;ll hit another vehicle.It&#x27;s very easy to tell if a vehicle is leaving a parking spot, so use your pattern recognition given to you by evolution and don&#x27;t stand in front of it.Personal responsibility people! reply thepedestrian 11 hours agorootparentIt&#x27;s very easy to tell if pedestrians are walking on a parking lot, so use your pattern recognition given to you by evolution (sic) and don&#x27;t drive on them. Personal responsibility, driver! reply micromacrofoot 14 hours agoprevtax consumer vehicles based on weight! it&#x27;s long overdue, put it on an exponential scale and demolish anyone that&#x27;s buying these monstrosities reply ta1243 14 hours agoparentIt seems that taxing by axel weight to the 4th power times by miles driven -- which is thus relative to how much damage is caused -- seems sensible.An 2600lb Nissan Micra would cost say 1c per mile and a 4100lb F150 would cost 6c per mile. reply notact 14 hours agorootparentUnintended consequence: Ford adds another rear axle to the F150. reply chung8123 12 hours agorootparentThis would reduce the gas mileage negating whatever tax savings there were as well as increase maintenance. This would also damage the roads less. reply notact 8 hours agorootparentMy original comment was kinda ridiculous and was not worthy of a serious response ;) But, to respond to this more seriously, the people I personally know who drive gigantic trucks would most likely be proud of their reduced gas mileage. They already deliberately drive a 15mpg vehicle primarily for grocery pickup. reply toast0 13 hours agorootparentprevA lot of bridge tolls are by axle count, so there&#x27;s pressure not to increase those. reply maerF0x0 14 hours agoparentprev> tax consumer vehicles based on weight!This essentially does exist in an urban environment due to fuel taxes. Still open debate if they&#x27;re appropriately set (high&#x2F;low) or if they funnel to the correct hands.In an urban environment (excluding hybrids and electric vehicles) the weight will directly correlate with gas consumed. More weight -> more gas, more gas -> more taxes paid reply micromacrofoot 13 hours agorootparentyes, gas prices are kept artificially low... but I think weight tax is an easier sell than gas tax reply xyst 14 hours agoprevThe rise in SUV and “modern truck” sales doesn’t help either. USA transportation infrastructure is fundamentally broken. We spend trillions of dollars (highway infra, street infra, parking infra, new builds and subsequent maintenance) on this inefficient transportation system and it’s slowly starving us to death.We need to end this car centric transportation hell hole we are in. Make our infra as difficult as possible for these modern SUVs and trucks and suddenly the office workers commuting from age Styx will decrease (ie, make commute times soar, parking fines increased, decreasing available parking, narrowing roads, get rid of archaic “parking minimums” for buildings)The suburban experiment is a tremendous failure supported by endless subsidies (federal, state, local). We need to be more efficient in urban planning. Give back the land stolen for highways, redirect them so they do not run through urban cores.So much we can do. Yet this country continues to throw their hands up in the air and do nothing. reply judge2020 14 hours agoparentWe ended up with this because SUVs and Trucks had reduced fuel efficiency requirements. When we have fuel efficiency requirements (mandated by aerodynamics[0,1]), SUVs bring their fronts down and curve them, improving pedestrian safety since more of the force comes from the hood instead of from hitting the ground.0: https:&#x2F;&#x2F;airshaper.com&#x2F;validation&#x2F;tesla-model-y-wind-tunnel-t...1: https:&#x2F;&#x2F;www.kia.com&#x2F;us&#x2F;en&#x2F;ev6&#x2F;gallery.exterior reply NewJazz 13 hours agorootparentWe ended up with confusing fuel efficiency requirements because we failed to tax fuel appropriately. I mean for gosh sakes, fuel taxes only pay for a portion of road costs! We spend additional subsidies on top of that. reply nickff 13 hours agorootparentprevHigher hood-lines are often used to provide crumple zones pedestrian impacts, as required in some jurisdictions. reply Sohcahtoa82 13 hours agorootparentThis makes absolutely no sense at all and flies in the face of all the statistics pointed out in the article.Lower hood-lines save lives. You don&#x27;t need a tall crumple zone. reply nickff 11 hours agorootparentI agree, but those rules require a crumple zone under the hood (basically between the hood line and top of engine). I didn’t write the rules, and I don’t enforce them. reply seadan83 13 hours agorootparentprevCrumple zone and soft tissue do not really seem to go together. When a car hits a person, the person _is_ the crumple zone. reply kube-system 13 hours agorootparentprevOn some global-market cars that had previously very low hoods, yes. But it&#x27;s not why US market SUVs have high hoods. reply xbar 13 hours agorootparentprevRidiculous. No jurisdiction requires > 40 inches (100cm) hoodlines. reply kelnos 14 hours agoparentprev> Yet this country continues to throw their hands up in the air and do nothing.It&#x27;s not that. It&#x27;s that most people in the US like car-centric culture, and like suburbs. They don&#x27;t want to live in dense spaces and have to rely on transit.I disagree with these people, but politicians are not going to be able to enact the things you&#x27;ve described without getting kicked out of office by the people who don&#x27;t actually want them to do these things. reply NegativeLatency 14 hours agorootparentI think some people do like the suburbs (and they&#x27;re allowed to), but the sky high housing costs in \"nice\" or \"popular\" cities indicate that there is unmet demand to live in dense walkable areas.The car industry has been very very successful with equating driving with freedom though which makes this a hard conversation to have with people.It should all be priced accordingly, you want to drive your private vehicle into my city? Then be prepared to pay a fair value for parking and road maintenance. reply kelnos 10 hours agorootparentBoth things can be true though. I agree that there&#x27;s unmet demand in urban centers, but that doesn&#x27;t mean that there wouldn&#x27;t be a massive revolt if laws and regulations made the suburban lifestyle much harder or more expensive. reply mikrotikker 12 hours agorootparentprevFuck that I love driving, it&#x27;s one of my favourite hobbies. I can and do drive for hours, 26 hours is about my max.With my truck I know I can handle anything on the way and be self reliant. I can take lots of water. I can take almost all my tool for any repairs on the road. I can sleep in it. I can traverse off road terrain with the large off road tires. The 4wd can help with icy road conditions. I have a winch to self recover or help others in trouble. The large and bright lights and higher sitting position give good visibility of the road.These things are popular because they engender self reliance which contributes to practicality and convenience for the driver.Not everyone has a boring daily commute to a cubicle job in their tiny ev that has 100km range and coming home to put on nature videos while they run on their treadmill. Stop trying to make everyone fit into the same box as you reply NegativeLatency 12 hours agorootparent> Fuck that I love having my lifestyle subsidized by other peopleInteresting that you&#x27;re actually the one trying to make me fit in your box, when I said that it&#x27;s fine for people to like what they like. Nobody&#x27;s saying you can&#x27;t drive your truck, go off road etc.I would like to be able to travel the 1&#x2F;3 of a mile without having conflicts with drivers when I&#x27;m trying to walk. Additionally I&#x27;d like the freedom to be able to get places with dignity without paying many thousand per year in costs associated with a motor vehicle.Also since you brought up \"self reliance\" almost any bicycle is going to be way more repairable than a motor vehicle, and _much_ more reliable, more capable off road etc. Literally children can repair and maintain them. reply psunavy03 13 hours agorootparentprev> The car industry has been very very successful with equating driving with freedom though which makes this a hard conversation to have with people.It can&#x27;t possibly be a \"hard conversation to have\" because people disagree with you, huh? It has to be that the car companies brainwashed them?I see this everywhere on social media. \"I have an opinion, my opinion is the right one, and if you disagree with me, you didn&#x27;t actually come to an independent conclusion, you were just brainwashed or had an emotional response.\" reply NegativeLatency 13 hours agorootparentNobody makes ads that make walking look cool or get you to where you’re going in style, because there’s nothing to sell you in that case.They do make ads of cars taking shortcuts, going to nature, driving really fast, looking really cool etc (closed course, professional driver, do not attempt)Demand as seen by prices and lack of affordability in walkable areas in the US is sufficient evidence that it’s an underserved market.BTW You sorta proved my point at how convinced people are, and how it&#x27;s such a hard conversation to have. reply gosub100 13 hours agorootparentprev> if you disagree with me, you didn&#x27;t actually come to an independent conclusionit goes in line with taking rights and responsibilities away from the individual. You want to go somewhere? take the government&#x27;s approved method of getting there. You dont like smelling urine or seeing trash or getting accosted&#x2F;threatened by homeless at the train station? too bad, should have voted for my guy, he&#x27;s 6 votes away from funding the next \"program\" to help fix it. Sorry we can&#x27;t fund police, they don&#x27;t respond to property crimes any more. we need to raise taxes again to fund the next $CHILDREN program, get with the times. and on and on. reply Mawr 9 hours agorootparentYou&#x27;re right, it&#x27;s about freedom. The freedom of not having to use a car to get literally anywhere. reply hightrix 13 hours agorootparentprevI love living in the suburbs and have zero desire to live in the city. Being accessible by car, I can get to anything I need in 15 minutes. The people promoting 15 minute cities completely miss the mark here. I, and many others, already live in \"15 minute cities\" since we can get to anything in 15 minutes by car.For me, it is great to be able to drive to Home Depot, then Target, then stop and get lunch, and finally return home with a load of goods that would have been impossible for me to carry alone.As you mentioned, I will strongly vote against, and even campaign against, any politicians threatening my way of life. reply Aaronstotle 13 hours agorootparentHave you ever lived somewhere where you can walk to get lunch for example?I really detest having to get into my car and drive 15 minutes each way (30 minutes round trip) to do any errand. reply stickfigure 13 hours agorootparentNot the parent, but I lived 17 years in the heart of SF before moving to the country. I loved it in my 30s, but was pretty over it by my 40s. Now I love cooking at home, letting my kid play outside, and throwing parties without worrying about neighbors.People like different things. The things people like change. Give the parent (and people like him&#x2F;her) the benefit of the doubt. reply hightrix 12 hours agorootparentprevYes. I used to live in an apartment in the middle of city. It was easy to walk to lunch&#x2F;dinner&#x2F;drinks. It was great for those activities but absolutely awful for many others. Grocery shopping? Still need a car. Going to another part of the city where my friends are? Still need a car. Etc.I enjoyed city living for a short period of time but it got old, very fast. It is NEVER quiet. Your selection of restaurants and bars is actually quite limited, or you’d need to taxi&#x2F;Uber. And overall I felt my quality of life much lower.Now I have a yard and a garage, extra storage in my house, and don’t have to deal with renting.Life is MUCH better, for me, in the suburbs. reply Mawr 10 hours agorootparentYou&#x27;re comparing living in a car dependant city vs living in car dependant suburbs. It&#x27;s not going to be meaningfully different. You mention the constant noise, but what do you think causes it?To see what everyone means by car dependency, you&#x27;d need to see the few places that aren&#x27;t.Here&#x27;s what a 100% car-free island looks like: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VWDFgzAjr1kHere are some progressive European cities that have started to undo the damage that&#x27;s been done by cars:• Freiburg, Germany: https:&#x2F;&#x2F;youtu.be&#x2F;6Vil5KC7Bl0?t=36• Haarlem, Netherlands: https:&#x2F;&#x2F;youtu.be&#x2F;ztpcWUqVpIg?t=90• Paris, France: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GSQSBoHmG8sCheck out these background noise levels:• A very busy mixed-use street in Amsterdam: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pqQSwQLDIK8• Exiting the train station in Delft, Netherlands: https:&#x2F;&#x2F;youtu.be&#x2F;CTV-wwszGw8?t=289• Cycling streets in the Netherlands: https:&#x2F;&#x2F;youtu.be&#x2F;bMJaMy-0ChA?t=242. You can even hear birds chirping at 4:29. reply kelnos 10 hours agorootparentprev> Grocery shopping? Still need a car. Going to another part of the city where my friends are? Still need a car. Etc.I think that we also need to understand the whys of some of this.Grocery shopping or going to another part of the city to see friends should not require a car. If it does, that means the city&#x27;s transit infra isn&#x27;t sufficient for residents&#x27; needs. And the reason that&#x27;s the case is because car culture is prioritized above all else in the US.For larger grocery &#x27;trips&#x27; I usually do delivery these days. But we also have a small grocer a couple blocks away (very well stocked, huge variety of stuff, great produce, etc.) that easily meets daily&#x2F;weekly needs.I do have a car, but I drive it only a few times a month. I fill up the gas tank once every couple months. I probably shouldn&#x27;t own a car, but I can&#x27;t kick the habit. It&#x27;s great for long trips outside the",
    "originSummary": [
      "Vehicles with taller and more vertical front ends have been found to pose a higher risk to pedestrians, according to research by the IIHS.",
      "Vehicles with hood heights above 40 inches are around 45% more likely to cause fatalities in pedestrian crashes compared to those with hood heights of 30 inches or less and a sloping profile.",
      "Vehicles with hood heights between 30 and 40 inches and a blunt front end also increase the risk to pedestrians. The study emphasizes the importance of designing vehicles with lower front ends and angled grilles to mitigate the danger to pedestrians."
    ],
    "commentSummary": [
      "The discussion encompasses multiple areas such as vehicle safety, regulations, consumer preferences, and urban planning.",
      "Key topics include the effect of vehicle design on pedestrian safety, the call for more stringent regulations and penalties for accidents, the correlation between regulations and car size/fuel efficiency, the rise of SUV and truck popularity, and the advantages and disadvantages of different transportation modes.",
      "The conversation showcases a diverse range of perspectives, highlighting the complexity of driving and transportation."
    ],
    "points": 304,
    "commentCount": 428,
    "retryCount": 0,
    "time": 1699987370
  },
  {
    "id": 38263294,
    "title": "A Lot of Damage in Grindavík: Earthquakes and Deformation Impact Buildings and Infrastructure",
    "originLink": "https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/",
    "originBody": "NewsIceland MonitorMon 13 Nov 202316.30 GMT Facebook Twitter Email Reddit Share A lot of damage in Grindavík Looking at the fissure. mbl.is/Eggert Jóhannesson A lot of damage can be seen around Grindavík after the earthquakes and the formation of a deformation that is moving downwards towards the magma intrusion that is underneath the ground. This depression formation is now estimated to be over 1.2 meters in the northwest end of Grindavík. A fissure passes through a large part of the town and passes through the road to the sports center. A hot water pipe has been broken in the earthquakes and the sports center seems to be sitting on a meter high pedestal. A journalist and a photographer from mbl.is have been travelling around the town today looking at the situation and the damage in town. As can be seen in these two pictures from the sports center, the power of the earthquakes has been substantial. The faulting is up to one meter and it is like the pavement at the front has been severed from the building. The pavement around the sports center has moved considerably and it now looks like the building is on a pedestal. mbl.is/Eggert Jóhannesson The faulting is up to one meter in some places around the sports center. mbl.is/Eggert Jóhannesson A short distance from the sports center, a huge fissure can be seen through the road, and a more than meter-long opening stretches through a part of the town. It has, among other things, at one point broken a hot water pipe and hot air is steaming out of the fissure. Here the hot water pipe has been severed in the earthquakes and you can see the hot steam coming up from the pipe which is in the fissure that goes right through the town. mbl.is/Eggert Jóhannesson The grass is soft and yielding widely close to cracks when walking on it, indicating that there is a widespread lack of soil under the grass. Thus, a rescuer found himself stepping with one foot in one place through grass without being harmed. A journalist at mbl.is has therefore been directed to stay on the tarmac because of this weakness in the soil. Today, residents have been allowed into town to retrieve the main necessities and valuable property. They have only a short time to do so. Initially, residents were required to accompany the rescue teams, but as the day went on, it was decided that people could enter in their own cars. However, there are security posts around town where rescue teams are located, pushing people. People had been made aware that they were only expected to be in their home for 5-7 minutes. Damages are visible on many houses, especially cracks in the concrete, but our journalist from mbl.is could not see any building that could be written off as completely ruined. The fissure goes through a big part of Grindavík. mbl.is/Eggert Jóhannesson Rescue workers are on alert helping people to get their belongings and making sure they don't stay too long, since 5-7 minutes is the window they are given. mbl.is/Eggert Jóhannesson mbl.is/Eggert Jóhannesson mbl.is/Eggert Jóhannesson Facebook Twitter Email Reddit Other recent news Working on defense walls Does not rule out liability for damages Sudden evacuation in Grindavík Deformation still active and eruption likely Seismic activity stable since midnight What is going on in Grindavík today Grindavík evacuated due to possible magma intrusion beneath the town Video: Products fell off the shelves and the store was evacuated A series of strong earthquakes Threat of civil protection declared",
    "commentLink": "https://news.ycombinator.com/item?id=38263294",
    "commentBody": "A Lot of Damage in GrindavíkHacker NewspastloginA Lot of Damage in Grindavík (mbl.is) 302 points by perihelions 20 hours ago| hidepastfavorite148 comments DavidPeiffer 19 hours agoI have found geology professor Shawn Willsey&#x27;s YouTube channel to have high value, no drama interpretations of the situation in Grindavik. He has been posting 1-2 videos per day for the past few days as new information arises.https:&#x2F;&#x2F;youtu.be&#x2F;xvlZOpZE2KE?si=c2_Ew7LDKHrEpoN9 reply coryfklein 17 hours agoparentThis is a much better introduction to what has happened this week; the original article gives absolutely no context about the related volcanic activity.TLDR; Grindavík is located in Iceland, a country known for its volcanic activity. Earthquakes and land deformation have exploded in frequency in the area under and around the small town of 4,000 people and geologists expect a volcanic eruption is imminent. The town has been evacuated. reply ptero 17 hours agorootparentTo add a note that while 4000 people is a small town almost anywhere, it is over 1% of Iceland&#x27;s population. So the impact on the country is non negligible. My 2c. reply brewdad 16 hours agorootparentIt also sits about 5 miles from Iceland&#x27;s only international airport, so could prove especially disruptive to the entire nation. reply lastofthemojito 16 hours agorootparent> Iceland&#x27;s only international airportThat&#x27;s not strictly the case. For example, Akureyri has some international flights: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Akureyri_AirportKeflavík is obviously the only major international airport in Iceland though.A while back I remember hearing a proposal about expanding the airport in Egilsstaðir to be capable of replacing Keflavík if (when?) it is disrupted by lava flow. I think it proved too expensive for much action to be taken though. It would be interesting logistically to see what would happen if Keflavík was knocked out of service. reply brucethemoose2 15 hours agorootparentI wonder how many airports in the world are at such significant risk of \"lava flow\" and other catastrophic natural disasters.I actually see Tampa International this way. Its literally on the coast, and just barely missed near two near Cat-5 hurricanes in two years. And that risk is not shrinking. reply don_esteban 14 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2021_Mount_Nyiragongo_eruptionLava flows came very close to Goma airport reply chanandler_bong 13 hours agorootparentprevREK can also handle most jets if necessary.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reykjav%C3%ADk_Airport reply dagurp 46 minutes agorootparentprevExperts aren&#x27;t worried about the airport. The main concern (besides the town) is the Svartsengi geothermal plant. If that is destroyed you will have 30.000 people without heating. reply gardarh 14 hours agorootparentprevCloser to 9 miles (15km). That&#x27;s plenty (even though it doesn&#x27;t sound like much), considering what kind of eruption this would be and how the fault lines lie. reply sbuttgereit 16 hours agoparentprevI actually watch Shawn Willsey&#x27;s content for everything but this topic. I find his Grindavik coverage isn&#x27;t that great. It&#x27;s overlong, over-speculative, and sometimes missed more current or extended information available elsewhere. Some of this I think is due to his being a relatively newer YouTuber (in terms of gaining traction) and his personal interest in Iceland: to be clear, its not that he&#x27;s disingenuous or unqualified... its more that he&#x27;s not yet crafted a public presentation style which fits the medium better. However his roadcut geology series is interesting even if you&#x27;re only getting his on the spot opinions of what he&#x27;s observing.By far, the best source about what&#x27;s going on in Grindavik I&#x27;ve found is just the Icelandic Meteorological Office website: https:&#x2F;&#x2F;en.vedur.is&#x2F; (English version, which is quite good). That in combination with some residents of Iceland that post shorter updates the keeps close to the reported local news seems to be the best. reply latchkey 17 hours agoparentprevnext [7 more] He starts off with saying that \"there is a line of magma that is 15km long and 9 miles in length.\"Do you know why he would mix metrics like that?Edit: he&#x27;s talking about a rectangle and his wording, to me, implied that he was using X&#x2F;Y coordinates to talk about how big the rectangle is. Except that it sounded like he was mixing metrics. Other commenters suggest that he&#x27;s just trying to help us dumb Americans. That could make sense too, but wasn&#x27;t obvious in his wording to me. 15km ~= 9 miles, I should have done that math. =) reply schiffern 16 hours agorootparentnext [–]> He starts off with saying that \"there is a line of magma that is 15km long and 9 miles in length.\"The real explanation is rather prosaic: bad transcription. He doesn&#x27;t use the word \"and.\"An accurate transcription would be: > there is a line of magma that is 15 km long—about 9 miles in length.He&#x27;s just (parenthetically) converting the units. reply jrace 16 hours agorootparentprevThat does read weird\"15km long, or 9miles\"Would be much betterThe mixing of &#x27;length&#x27; and &#x27;long&#x27; was odd. reply earthnail 17 hours agorootparentprevIsn’t he just trying to explain it for a wide audience? He‘s just translating units here. reply fhars 17 hours agorootparentprevFor the benefit of the American viewers, who might not immediately know what 15km are. reply bitcharmer 16 hours agorootparentprev> 15km long and 9 miles in lengthThis is not mixing. He&#x27;s just presenting the length in two different measurement systems. reply anonu 16 hours agorootparentprevIf he had said 15km long and 9 miles wide my head would&#x27;ve exploded reply mhandley 17 hours agoprevIn the last hour they have detected increased levels of SO2 in Grindavik, so they&#x27;ve evacuated residents who were being allowed in to retrieve their valuables:https:&#x2F;&#x2F;www.ruv.is&#x2F;english&#x2F;2023-11-10-liveblog-reykjanes-pen...No signs of an actual eruption yet though on the cameras (the steam is a geothermal power station): https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=804nPrAUAxg reply ______ 18 hours agoprevThat&#x27;s pretty close to Blue Lagoon, one of the most popular tourist destinations in the area, which is now closed as a precaution: https:&#x2F;&#x2F;www.bluelagoon.com&#x2F;seismic-activity reply orlp 17 hours agoparentIt&#x27;s also a waste of time tourist trap. The normal Icelandic baths are much cheaper and just as good. reply baz00 17 hours agorootparentMaybe. Avoided it. Went to Mývatn instead. Turned out that it means midge-lake. Spent 2 days eating flies. They didn&#x27;t bite but weren&#x27;t tasty. I&#x27;ve never driven the fuck out of somewhere so quickly. reply hef19898 16 hours agorootparentWell, Myvatn is basically on the other side of the island... Cam only recommend the non-Blue Lagoon hot baths and tubs, easily as good and a lot cheaper. reply esfandia 14 hours agorootparentprevMaybe, but the surreal atmosphere when the sun rises, you are surrounded by blue everywhere, you are jetlagged from just having landed half hour ago, and you have a drink in hand is indescribable. I went to other Icelandic baths during my stay but nothing matched that experience. reply YeBanKo 12 hours agorootparentprevNot just as good, but better. There are no morons around you who are constantly live streaming. reply bell-cot 17 hours agoprev@dang - might we change the title to \"Spreading & Subsidence Damage in Grindavík\"?With all the \"Lava & Eruption!\" hype around this situation, that would make what is actually happening now in Grindavík much clearer. reply hk__2 16 hours agoparent> With all the \"Lava & Eruption!\" hype around this situation, that would make what is actually happening now in Grindavík much clearer.Is it really the right place for it, though? HN is not CNN; if you want that sort of news there are probably better sources.> If they&#x27;d cover it on TV news, it&#x27;s probably off-topic.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply bell-cot 16 hours agorootparentTrue...but this is currently the #1 story on HN, so evidently \"good hackers\" find it interesting.And if the current situation is \"nothing special happening\", then let&#x27;s not waste any good hacker&#x27;s time with a pretty-ambiguous headline. reply kseistrup 18 hours agoprevRÚV has a liveblog with the latest news about the situation in English, Icelandic and Polish: https:&#x2F;&#x2F;www.ruv.is&#x2F;english&#x2F;2023-11-10-liveblog-reykjanes-pen... reply kd5bjo 18 hours agoparentNote that the Icelandic edition [1] understandably gets more frequent and timely updates than the others. If you&#x27;re ready to put up with the quirks of machine translation, there&#x27;s a lot more information there to see.[1] https:&#x2F;&#x2F;www.ruv.is&#x2F;frettir&#x2F;innlent&#x2F;2023-11-14-ryma-grindavik... reply jahnu 15 hours agoparentprevYour comment prompted me to look up how many Poles live in Iceland. Turns out they are the largest minority which is mildly surprising but shouldn’t be I guess.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Poles_in_Iceland reply kseistrup 12 hours agorootparentWhen I started reading RÚV I wondered what the red and white flag was. My mind tried with “Greenland”, but Greenland because of the Nordic affiliation, but Greenland itself has a population of less than 60k, so how many greenlanders could there be in Iceland?When the flag turned out to be Polish I had to turn to the Woodchuck Book and was surprised, just like you. I should have mentioned it. Thanks for the link. reply omegant 18 hours agoprevFor anyone guessing how it could go to the town, just check the recent case of the La Palma volcano lava that runned over a town.https:&#x2F;&#x2F;youtu.be&#x2F;kjg-1BemSOo?si=H_We-hOXtCHT3WgcThe eruption in Hawai some years ago also destroyed a town and a harbor. reply mrtksn 19 hours agoprevWhat I find eerie is that they stroke out Grindavik from the signs: https:&#x2F;&#x2F;twitter.com&#x2F;grindaviknews&#x2F;status&#x2F;1724286780914962487I know, they are doing it to inform people that they shouldn&#x27;t go there but it gives it such a post apocalyptic vibe. reply tangus 18 hours agoparentIt&#x27;s the standard practice when the road is closed. It doesn&#x27;t give apocalyptical vibes to me, but maybe roads break too often where I live :) reply strken 18 hours agoparentprevNot that it&#x27;s quite the same, but I wonder how common it is to have a place struck off of official maps and signs for safety purposes? I know of one, Wittenoom[0] in Australia, which was removed due to being heavily contaminated with asbestos, but I wonder what other examples are out there.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wittenoom,_Western_Australia reply Rebelgecko 16 hours agorootparentThere was a lake that was removed from the maps of a state park near me, although that was to prevent it from being over-visited reply _ph_ 18 hours agoparentprevIt is the usual way of denoting that there is no turn right which leads to the designated location. Usually because of some road closure. Of course here it has a strange vibe to it. reply rob74 18 hours agorootparentYeah, standard procedure (here in Germany too), except usually it&#x27;s the road that&#x27;s closed, and not the entire town... reply alttab 16 hours agorootparentThis is an American interpretation. Here in the US, cones, lights, or lane-blockers are used to close exits.Culturally, its because American&#x27;s won&#x27;t read a sign and follow the instructions if we can still \"do\" it. You have to physically block us, and its too expensive to change all the signs. reply rob74 15 hours agorootparentNah, it&#x27;s actually the same here in Germany too - I live near a railway crossing which was recently rebuilt and couldn&#x27;t be used for a few weeks. The \"strike-through\" at the intersection was just the first step, then there was a first barrier blocking half the street, with a sign \"residents only\", and then a bit later a second barrier, but still some people just absolutely had to drive past all those barriers until they were literally in the middle of the construction site to find out that no, it&#x27;s really not possible to cross the tracks... reply ceejayoz 16 hours agorootparentprevMy favorite is Chicago&#x27;s reversible express lanes, which have thirteen physical gates you have to blow through in order go to the wrong way. I always wonder if they add another set every time someone still drives through them all. reply meheleventyone 12 hours agorootparentprevThey aren’t closing the exit though as it’s needed so people can get down there for quite a few reasons. There are actual manned roadblocks a bit further in. reply 4ggr0 19 hours agoparentprevWell, to be honest, for Grindavik we could currently be talking about a Pre-Apocalyptic situation :) reply Nifty3929 18 hours agorootparentI wonder if this is a fiction genre. Can I read a book that starts with a happy town with not much going on, builds up to just before a catastrophe, and then ends? Maybe like Part 1 of Seveneves. reply rob74 17 hours agorootparentI read this book recently: https:&#x2F;&#x2F;www.amazon.com&#x2F;Anomaly-Novel-Herv%C3%A9-Tellier&#x2F;dp&#x2F;1... - don&#x27;t want to give away too much (that&#x27;s why I&#x27;m not linking to the Wikipedia article), but it fits the genre you are proposing - except spanning the entire world, not just a town. reply 317070 16 hours agorootparentprevSounds like the movies \"Don&#x27;t look up\" and \"Melancholia\"Not much really happens, all everybody knows is that doom is imminent. The movies are mainly about how the different characters handle this situation.That said, I am still waiting for a movie that just suddenly ends with a car crash where the main protagonist dies, fully unrelated to the story. reply HeyLaughingBoy 13 hours agorootparentA bit reminiscent of the ending of No Country for Old Men, although you never find out if he actually dies. reply prewett 16 hours agorootparentprevThe last episode of Downton Abbey, Season 3 is reasonably close to that. reply 4ggr0 17 hours agorootparentprevMaybe you can find something on this list, https:&#x2F;&#x2F;www.goodreads.com&#x2F;shelf&#x2F;show&#x2F;pre-apocalypticI looked at a few books on this list, but not every book seems to really be about pre-apocalypse. Pre-Apocalyptic fictions sounds like literary edging :D reply hef19898 16 hours agorootparentprevThat was my hope for Fear the Walking Dead, a show about society collapses under a zombie apocalypse. Instead we got some hints in a pre-zombie LA, went straight to a cordoned of quarantine zone and a time jump to \"now the world, except your little suburb, went to hell\". Seems writing convincing stories about why everything would collapse is a lot harder than stories set after an assumed collapse. reply mananaysiempre 17 hours agorootparentprev“The Doomsday Book” by Connie Willis kind of has that feeling for a while. I couldn’t really get into it, though; it’s undeniably a well-written book, just not the kind of book I wanted to read. reply Aerbil313 14 hours agorootparentprevOn The Beach. So much doom I literally can’t finish it. Ok it’s post-apocalyptic but reads like it’s pre-apocalyptic. reply manojlds 16 hours agoparentprevApocalyptic vibe would be signs overrun with weeds and such and cars stranded on highways reply lisper 17 hours agoparentprevnext [9 more] Just FYI, the past tense of \"strike\" is \"struck\", not \"stroke\". It could be argued that it should be \"stroke\" to parallel words like \"break-broke\" and \"wake-woke\" but it&#x27;s not. English is weird. (And the past tense of \"hike\" is \"hiked\", not \"huck\" nor \"hoke\", which aren&#x27;t words at all [UPDATE - turns out they are, see child comment from u&#x2F;GrinningFool], but they probably should be.) reply GrinningFool 16 hours agorootparenthuck - to throw or toss; to launch oneself in the airhoke - to give a contrived, falsely impressive, or hokey quality to reply lisper 16 hours agorootparentWell, whadyaknow. TIL. reply Tempest1981 17 hours agorootparentprevOr they meant this def&#x27;n of stroke:> a mark made by drawing a pen, pencil, or paintbrush in one direction across paper or canvas. reply lisper 16 hours agorootparentYes, but that&#x27;s a noun, not a verb.The word \"stroke\" can be a verb, but its meaning (to rub lightly) doesn&#x27;t make any sense in context. reply mrtksn 16 hours agorootparentprevWell, IIRC I just tapped whatever autocomplete suggested :) reply ska 14 hours agorootparentprevEven more confusingly, if it is past participle usage, it&#x27;s \"stricken\"e.g. \"Strike it off\", \"I struck it off\", \"It was stricken off\"same with drive&#x2F;drove&#x2F;drivenOne of those fun things for ESL folks I am sure. reply dragonwriter 14 hours agorootparent“Struck” is the general past participle of “strike\", “stricken” is a past participle available specifically for the sense of removing from a list (but the general form “struck” is also correct and often used for that sense, too.) reply ska 14 hours agorootparentIt&#x27;s not that narrow - e.g. \"was stricken by a sense of foreboding\"You also would say panic-stricken, not panic-struck.Agree it&#x27;s not universal though, helpfully, and sometimes is it \"struck\" in non adjectival past participle.But not in the OP case, I think (much like striking off a list, striking off a sign). I probably should have mentioned that it wasn&#x27;t universal for struck though.I could just have confused things. replyhlesesne 9 hours agoprevDoes anyone know how the large datacenters in Iceland are faring?I know that most of the big ones are north east (near the international airport in Keflavík).They were really popular for crypto miners, but have been moving into HPC very rapidly. I’d assume there is a ton of extremely expensive equipment less than 10-15 miles from where all this activity is. reply rwmj 15 hours agoprevI hear in Japan the reason they have electric wires on poles and houses have individual gas cylinders is to provide resiliency during earthquakes -- the earth becomes a liquid. reply nerdponx 15 hours agoparentElectric wires on poles are also more likely to get damaged by wind, ice, fire, and flood, either directly or by trees falling onto them. There are tradeoffs involved. reply q1w2 13 hours agorootparentThey&#x27;re also much cheaper to construct. reply SoftTalker 15 hours agoparentprevGetting OT but I&#x27;ve noticed in Scandinavian countries they use pavers a lot where in the USA we&#x27;d more likely have poured concrete or asphalt&#x2F;tarmac. Sidewalks, parking surfaces, driveways, etc.Pavers would seem to me to be both more expensive to install and require higher maintenance. Is there a reason they are used so much? reply runeofdoom 13 hours agorootparentPurely a guess on my part: concrete or asphalt will be destroyed by frost heave[1] and be difficult to patch well, while pavers will endure it better and are easier to restore.1. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Frost_heaving reply yread 14 hours agorootparentprevThere are machines that are super fast at laying them. And digging them up and replacing them is a lot less disruptive than asphalt. Plus it makes cars drive a bit slower. And they drain naturally reply InCityDreams 14 hours agorootparentprevFor the uk readers: I&#x27;m guessing &#x27;pavers&#x27; are &#x27;flags&#x27;, or &#x27;flagstones&#x27;.Humped a few of them in my time...thank goodness for Atari, and all that followed.!! reply nmeofthestate 13 hours agorootparentThey&#x27;re called pavers, or block paving, or monoblock in the UK. Flagstones are big slabs of stone. reply arbuge 15 hours agoparentprevGases passing through dirt&#x2F;sandy substances at high pressure will tend to do that.See for example: https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;EImO4agHqOg reply m4rtink 13 hours agorootparentVery interesting! Now I have to think how this could be used to make something productive! :D Maybe some sort of special bearing floating on the fludized sand you can turn of and stop&#x2F;stabilize by turning thr gas pipe off ? reply danaris 13 hours agoparentprevWe have electric wires on poles in the northeast US, too—largely because the ground freezes solid for several months of the year.It&#x27;s not that unusual a precaution.(Though yes, as a sibling commenter notes, we do have to deal with them getting taken down by wind, lightning, and the occasional hapless driver from time to time. There&#x27;s no perfect solution.) reply HeyLaughingBoy 13 hours agorootparentMy guess is that it&#x27;s more likely due to the cost of digging&#x2F;right of way, etc. It gets significantly cold here in MN and most powerlines are underground, but we have a lot of open space.A statistic I remember from when I lived there is that Connecticut has more trees per mile of powerline than any other state. Every time there was an ice storm, thousands of people lost power. reply kalleboo 3 hours agorootparentprevIn Sweden power lines are buried, even in the very north of the country. I guess it&#x27;s a cost thing. reply the_third_wave 50 minutes agorootparentThat very much depends on which part of Sweden you&#x27;re talking about. Around most cities they are buried but in the countryside &#x27;luftledning&#x27; - literally &#x27;air cable&#x27;, denoting cabled hung on impregnated wooden poles - is still more the norm than the exception. Where I live (about 60 km north of Göteborg) Vattenfall has been talking about burying those cables for decades but they&#x27;re still there and regularly are taken down by trees, errant drivers and similar mishaps. reply WXLCKNO 19 hours agoprevIceland is the only place I&#x27;ve been to twice in two years when traveling.I usually go a new place everytime but I simply couldn&#x27;t get enough of it just once and had to go again ASAP.Hopefully this situation turns out better than the worse predictions. reply bmelton 19 hours agoparentSame. Wife and I went for New Years this year and it was by far one of the most satisfying vacations I&#x27;ve ever taken. Absolutely nothing about the place is overrated, and we lament how little of it we&#x27;ve experienced.It is as close to magic as anywhere I&#x27;ve been, and their New Year celebration is insanity. reply cdelsolar 18 hours agorootparentI went up to the hill in the center of Reykjavik for New Years and was astonished, completely astonished by the amount of fireworks that could be seen. reply ctennis1 18 hours agorootparentprevWe have plans to visit for this coming New Years, (unless things get dramatically worse I suppose). Excited in reading your comment. reply bmelton 18 hours agorootparentIf you can, see if you can get to a bonfire a few hours beforehand. Travel &#x2F; tourism agencies can arrange this.Also, if you&#x27;d enjoy seeing the Aurora Borealis, know that due to weather, clouds, etc., it isn&#x27;t very reliable, and it may be best to plan on multiple attempts at it. The experiences we had ranged from disappointing to terrifically amazing, but if we&#x27;d only gone the one time, we&#x27;d have left without having experienced it. reply hef19898 16 hours agorootparentBeen there three times in winter, never saw an aurora brealis once... Still great trips so! reply baz00 16 hours agorootparentprevThe hotdogs are very overrated! Hospital in Selfoss is amazing though, especially after eating one :) reply WXLCKNO 16 hours agorootparentI thought how can a hotdog be better.. its a hotdog.They blew my mind, best hot dog i&#x27;ve ever had. Even ordered some directly from Iceland after coming back with all the toppings. reply baz00 16 hours agorootparentI think I ate three over two weeks and they were all awful. The only thing that was really nice there was pepper cheese. reply alistairSH 16 hours agoparentprevWe visited in January 2017 and I&#x27;d love to go back when there&#x27;s more than 5 hours of daylight. The trip was spectacular. And it&#x27;s an easy flight from DC and 5 nights was plenty to hit most of the easy tourist spots. reply hef19898 16 hours agorootparentWinter is the ideal time to go northern lights hunting. Either way, I&#x27;ve been propably too often, 5 times by now, twice in summer including one highland crossing, and three times in winter. My recomondation would be, if budget and vacation time allows, to visit Iceland once in summer and once in winter. reply xenospn 17 hours agoparentprevHonestly. It’s a beautiful country full of wonderful people and I can’t wait to go back again. Wish them nothing but the best! reply rob74 17 hours agoprevWhat&#x27;s interesting to see in the pictures is that they already built a detour around the fissure: it starts in the area of fresh asphalt to the right of the guy in this photo https:&#x2F;&#x2F;cdn.mbl.is&#x2F;frimg&#x2F;1&#x2F;45&#x2F;18&#x2F;1451863.jpg, then goes through an also freshly paved-over parking lot&#x2F;former green space and rejoins the road in the background (behind the construction fences). reply saevarom 14 hours agoparentI think you are misinterpreting this photo. Nobody has been paving over the fissures in the town. I think it&#x27;s a combination of moisture, different coloured asphalt sections, and light&#x2F;shadows that tricks the eye. As you can see in this drone footage [1] the car park asphalt is darker, and the hot water pipes ruptured in the ground create steam that spreads moisture over the asphalt, making it darker. Also, you have a building next to the car park which makes it even darker than the street.[1] https:&#x2F;&#x2F;ruv-vod-clips.akamaized.net&#x2F;76a26e8d-1190-492c-9163-... reply pcrh 18 hours agoprevIceland has had active volcanoes all the time it has been inhabited.Does anyone know what the \"traditional\" response to these sorts of ground movements was? reply CaptArmchair 18 hours agoparentHistorically?The 1783 Laki eruption comes to my mind. Back then, a 25km long fissure with 130 vents opened. About 14km3 of lava and 1km3 of tephra was emitted. Lava fountains were estimated to have reached between 800m and 1400m.Some 20-25% of the population died, 50% of cattle and 50% of the horse population perished.It is argued that this eruption was the catalyst for the French Revolution, as the amount of emitted gasses caused extreme weather patterns across Europe (and the world) throughout the 1780s leading to famine, diseases and social unrest.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;LakiIn contemporary times, there&#x27;s the 1973 eruption of Eldfell on the island of Heimaey. It destroyed some 400 homes and led to a temporary evacuation of some 5.300 inhabitants. By 1975, some 80% of the population had returned. Moreover, today it&#x27;s the foremost fishing center of Island with over 1&#x2F;3rd of the total fish catch originating from its harbor.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eldfell reply pcrh 11 hours agorootparent>The parish minister and provost of Vestur-Skaftafellssýsla, Jón Steingrímsson (1728–1791), grew famous for the eldmessa [ˈɛltˌmɛsːa] (\"fire mass\") that he delivered on 20 July 1783.I looked this up, and found this: \"In his introduction to the translation Guðmundur E. Sigvaldson classifies Jón’s Eldritið as a “scientific classic as well is a literary jewel.” [0]It chronicles the aftermath of the eruption in some detail (i.e. people did not flee the region, for some reason....), e.g.:>The poisonous compounds leaked out of the Laki craters caused, as Jón depicts so graphically, the skin to rot off the spines of horses, swelling in their heads, jaws, and joints, rotting insides, and shrinking bones. The sheep and cattle suffered similarly. The meat from these animals was “both foul-smelling and bitter and full of poison, so that many a person died as a result of eating it” (Fires of the Earth, 76).[0] https:&#x2F;&#x2F;www.medievalist.com&#x2F;articles&#x2F;strongjn-versus-the-vol... reply avgcorrection 18 hours agorootparentprev> By 1975, some 80% of the population had returned.Of course they had to come back to “home island”. reply troyvit 17 hours agoparentprevThis article talks about the Eldfell eruption in 1973. It&#x27;s one of the few cases where people were able to divert a lava flow.https:&#x2F;&#x2F;www.usgs.gov&#x2F;observatories&#x2F;hvo&#x2F;news&#x2F;volcano-watch-wh... reply m4rtink 13 hours agorootparentIIRC, they also used the remaining solid lava flow for district heating for a while so that it cooled down faster and could be trucked out of the way where necessary. reply hutzlibu 17 hours agoparentprev\"Does anyone know what the \"traditional\" response to these sorts of ground movements was?\"The very old tradition was to increase the amounts of sacrifices to the gods, because clearly they were angry. reply pcrh 11 hours agorootparentIs there documentation for this? Icelanders are Christian (since when, I&#x27;m not sure), and would more likely pray. Also I don&#x27;t think there is a history of human sacrifice in Norway&#x2F;Denmark&#x2F;Scandinavia. reply hinoki 1 hour agorootparentNothing specific to earthquakes and volcanoes or other bad omens, but there was probably some human sacrifice.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Blóthttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Ättestupa reply hutzlibu 2 hours agorootparentprevWith the very old tradition I meant the pre christian times (which is why I wrote \"the gods\").And I was not necessarily talking about human sacrifice, even though it seems very likely, that this happened as well.https:&#x2F;&#x2F;scandinaviafacts.com&#x2F;did-the-vikings-perform-human-s... reply j-a-a-p 13 hours agoprevSomebody observed in this thread https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38233905 seeing the town shake on camera reply q1w2 13 hours agoparentSeems more likely to be the wind blowing the camera - hard to tell reply runarberg 13 hours agorootparentMost likely wind. However this thread was from Friday and the ground was constantly shaking that day. I was there and there were rather large earthquakes like every 10-20 minutes (and smaller earthquakes every 20-30 seconds) between 16:00 and 23:00 (when I evacuated). It is highly likely that the poster in this thread was actually seeing earthquakes. reply KeyXiote 18 hours agoprevAny public data on this? I would love to collab on some predictive modeling for cases like this. There are some sources but couldn&#x27;t find anything comprehensive enough to be accurate, need more historical context comparisons etc. reply saevarom 14 hours agoparentHere is at least some interpretation of data with interesting graphs, like displacement graphs etc: https:&#x2F;&#x2F;en.vedur.is&#x2F;about-imo&#x2F;news&#x2F;a-seismic-swarm-started-n... reply gorbachev 18 hours agoparentprevIcelandic Met Office probably.https:&#x2F;&#x2F;en.vedur.is&#x2F;earthquakes-and-volcanism&#x2F;earthquakesI haven&#x27;t explored the site enough to see if they have downloadable datasets, however. reply bandyaboot 19 hours agoprevHave they evacuated the town?Edit: I see that it looks like they have but residents will be allowed in to retrieve belongings. Is there any chance that an eruption comes in the form of a large scale collapse above the magma chamber…or is that not plausible? reply bbarn 18 hours agoparentWhile anything&#x27;s possible, the trend of recent eruptions in the Reykjanes peninsula have been more of the slow trickling small kind. They have also been relatively off the beaten path, so it&#x27;s hard to compare ground shifting in the previous places since you likely wouldn&#x27;t notice it after the fact like you will with pavement and buildings. reply bandyaboot 17 hours agorootparentGot it. Here’s hoping that this will be a continuation of the trend. reply rob74 18 hours agoparentprevIt only needs a small scale collapse if your house is standing over the sinkhole while you&#x27;re inside, so evacuation is probably still the safest bet... reply m4rtink 13 hours agorootparentAlso some volcanic gases that might be released are quite poisonous & can be blown by wind around the place. reply bandyaboot 17 hours agorootparentprevOh yeah, I wasn’t meaning to suggest that evacuation might not be the correct course, it seems at this point not doing so would be irresponsible. Was just curious about the possibility of something very large occurring given the sinking above the chamber. reply cvccvroomvroom 15 hours agoprevLive webcam https:&#x2F;&#x2F;youtube.com&#x2F;live&#x2F;Qvw5qh5K4wI reply h1fra 16 hours agoprev1m displacement is crazy, I can&#x27;t imagine how much destruction it would cause inside a major city. reply pierat 19 hours agoprevThey&#x27;ve also mostly rescued all animals as well from the area, including animals in homes, along with sheep and horses in pastures.When the volcano blows, it should only be property, building, and road damage. reply icepat 19 hours agoparentThere&#x27;s a non-zero risk of it going off in the ocean, which could cause ash fallout, depending on the magnitude of the blast.Property and infrastructure damage is not the only outcome here. We could end up with ash fallout and flight disruptions. There&#x27;s also a major power station in the immediate risk zone, which is a major loss if it gets destroyed.If and when this blows up, it&#x27;s going to be big. The question is just if it&#x27;s big and causes fallout, or just big. And when we&#x27;re talking about property damage, it&#x27;s less damage and more \"will the town be obliterated or not\". The magma is most shallow under the town, and there&#x27;s a 15 kilometer magma pipe extending to the ocean. So the possibility of the eruption happening in Grindavik itself is also very real. reply 4ggr0 19 hours agorootparent2010 showed what kinds of impacts can occur when a volcano erupts in Iceland. [1]> In response to concerns that volcanic ash ejected during the 2010 eruptions of Eyjafjallajökull in Iceland would damage aircraft engines, the controlled airspace of many European countries was closed to instrument flight rules traffic, resulting in what at the time was the largest air-traffic shut-down since World War II.> On 16 April 2010, 16,000 of Europe&#x27;s usual 28,000 daily scheduled passenger flights were cancelled and on the following day 16,000 of the usual 22,000 flights were cancelled. By 21 April 95,000 flights had been cancelled.> IATA stated that the total loss for the airline industry was around US$1.7 billion (£1.1 billion, €1.3 billion).[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Air_travel_disruption_after_th... reply icepat 19 hours agorootparentEyjafjallajökull was a special case, it was not an oceanic blast, but a blast under a glacier. Fjallajökull means \"glacier mountain\". The blast flash-melted several tones of ice, which caused the lava to cool rapidly forming the ash cloud. If my memory serves, the glacier was in the caldera itself.But the effect would be very similar. The water would vaporize and create an aerosol of ash. reply repelsteeltje 19 hours agoparentprev> When the volcano blows, it should only be property, building, and road damage.Mind you, those roads in Iceland might be expensive, with luxurious built-in heating. ;-)https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Snowmelt_system reply bbarn 18 hours agorootparentI can tell you for sure there are no such heated roads in Grindavík. The one in Reykjavík is on a street that cars can drive down, but in practice don&#x27;t very often. Large plows would struggle there and the pushed off snow would clog the walking areas in a very popular tourism location. reply avar 18 hours agoparentprev> When the volcano blows, it should only be property, building, and road damage.People had the option to get their animals, but not everyone did so. If the eruption destroys the town some animals will be among the casualties, if they haven&#x27;t starved etc. already. reply RobertVDB 18 hours agoprevAren&#x27;t earthquakes super common in Iceland? Or am I wrong on that? reply ajmurmann 17 hours agoparentEarthquakes aren&#x27;t uncommon, but what&#x27;s happening in Grindavik is more than a earthquake. They were having hundreds of quakes an hour, a fissure opened and magma might come up soon. All in a little town and near a geothermal power plant which also fuels the Blue Lagoon. This also isn&#x27;t super far from the main airport and the road that leads to it. reply Scarblac 17 hours agoparentprevDuring Covid a new volcano erupted quite near Grindavik, that was very tourist friendly: close to Reykjavik, not too large or dangerous, nothing it could damage nearby, a few km hiking from a road. Many web cams were setup and lots of beautiful drone footage was uploaded to YouTube as well. It erupted again 2021 and 2022. So it&#x27;s a well known volcano now to the Internet public.Now it seems there will be an eruption on the same fissure again, but a) there are many more earthquakes than before, b) the detected movement of magma is far larger, and c) it could erupt anywhere on a 15km fissure line, but right now it seems to do so right in the middle of the nearby town. So it looks like this time it will be a disaster.There is also a geothermal power plant that heats the whole peninsula nearby, as well as tourist attraction the Blue Lagoon, that are both at risk. reply jwozn 17 hours agoparentprevYou aren&#x27;t wrong, but the topic of interest here is the volcanic eruption. They are also relatively common compared to elsewhere in the world, but most are small and not in populated areas. See: https:&#x2F;&#x2F;www.visiticeland.com&#x2F;eruption&#x2F; reply bandyaboot 17 hours agoparentprevThey are and they tend to be small because of the type of plate boundary they’re associated with. But when they’re coming in large swarms near a volcano which is the case here, it’s a pretty strong signal of an impending eruption. reply micromacrofoot 17 hours agoparentprevyes, and earthquakes are also super common precursors to volcanic eruptions reply Scarblac 19 hours agoprev [–] Looking for Grindavik news on X etc right now, there is already an annoying amount of video and images of erupting volcanoes, from previous years, or from somewhere else completely. Even though there is no eruption at all yet. Annoying. reply gosub100 17 hours agoparentReminds me of nearly every time there is an aircraft incident with, say a 737 but the media show a stock photo of a completely different plane that has 4 engines. reply sebazzz 15 hours agoparentprevInstead of becoming a more reliable source of news, thanks to ad payouts and the resulting engagement farming by blue checkmarks Twitter&#x2F;X is becoming increasingly less reliable. reply mock-possum 15 hours agorootparentSince the effect is so obvious, it’s hard to imagine that hasn’t been the objective. reply haunter 19 hours agoparentprevAm I using a different Twitter? I only see local videos no \"annoying amount of video and images of erupting volcanoes, from previous years, or from somewhere else completely\" reply Scarblac 17 hours agorootparentYesterday it was way worse. Then the cracks in the middle of town opened and it seems that&#x27;s sensational enough to use instead. reply seizethegdgap 18 hours agorootparentprevI was seeing a lot of what Scarblac said they were seeing, but looking right now athttps:&#x2F;&#x2F;twitter.com&#x2F;search?q=grindavik&f=liveand it&#x27;s pretty clean. You&#x27;ll occasionally see the famous drone video from the 2021 eruption being touted as what&#x27;s happening in Gridavik right now. reply hef19898 19 hours agoparentprevNot just Twitter, I saw a \"symbol\" picture of a volcanic eruption on a major German news site about Grindavik... reply namuol 17 hours agoparentprevYou may want to consider waiting a few more minutes to get news from reliable sources. reply tempaccount1234 10 hours agorootparentYou can watch the live cams on RÚV.is or the tv channel ruv2 that streams from there. That way you also get RÚV radio sound, with their great music choices that now clash with the night images… reply cryptoegorophy 19 hours agoparentprevCommunity notes need to speed up reply rjmunro 18 hours agoparentprev [6 more] This is why you can&#x27;t use X (or facebook or any other social media) for news. reply styren 18 hours agorootparentAll depends on the accounts that you follow but I agree that the search function of X is horrible. reply stainablesteel 16 hours agorootparentThe TL;DR of how you can search on X:• from:twittername• keyword1 OR keyword2• min_faves:2000• min_retweets:2000• filter:links• filter:images• until:YYYY-MM-DD• since:YYYY-MM-DD• near:location within:15miits actually a pretty nice search ngl reply LesZedCB 17 hours agorootparentprev [–] makes you wonder about this very thread then reply Arubis 17 hours agorootparentHN has plenty of flaws and cultural issues, but the stated intention here is to promote discussion, not \"user engagement\". Where Twitter&#x2F;X promotes outright lies, here I typically take more issue with folks&#x27; lenses and biases. tl;dr: I trust HN orders of magnitude more than Twitter and crew. reply _jal 16 hours agorootparentprev [–] Is Ycombinator paying out to their \"content generators\" here?Also, while I can imagine one&#x27;s HN reputation helping someone a job or find a venture partner in this little bubble, there is no comparison with Xitter&#x27;s reach. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The town of Grindavík in Iceland has suffered damage from earthquakes and a deformation moving towards an underground magma intrusion.",
      "A large fissure has appeared in the town, damaging buildings and infrastructure like a hot water pipe and the sports center.",
      "Residents have been given limited time to gather their belongings and rescue teams are on standby, although no buildings have been completely destroyed. The authorities are closely monitoring the situation."
    ],
    "commentSummary": [
      "The town of Grindavík in Iceland is facing increased seismic activity and land deformation, leading to the possibility of a volcanic eruption.",
      "The town has been evacuated, and concerns are raised about the impact on Iceland's population and the country's only international airport.",
      "Reliable sources of information recommended include a geology professor's YouTube channel and the Icelandic Meteorological Office website."
    ],
    "points": 302,
    "commentCount": 148,
    "retryCount": 0,
    "time": 1699970081
  },
  {
    "id": 38264937,
    "title": ".NET 8: The Latest LTS Version with Performance and Productivity Enhancements",
    "originLink": "https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/",
    "originBody": "Announcing .NET 8 Gaurav Seth November 14th, 202329 18 Download .NET 8 today! We are happy to announce the availability of .NET 8, the latest LTS version of one of the world’s leading development platforms, starting today. .NET 8 delivers thousands of performance, stability, and security improvements, as well as platform and tooling enhancements that help increase developer productivity and speed of innovation. The .NET team, our partners, and the .NET community will be talking about what’s new in .NET 8 as well as what people are building with .NET today to meet their needs of tomorrow at .NET Conf 2023, a three day virtual event (November 14-16). Come, join us! With this release, .NET reshapes the way we build intelligent, cloud-native applications and high-traffic services that scale on demand. Whether you’re deploying to Linux or Windows, using containers or a cloud app model of your choice, .NET 8 makes building these apps easier. It includes a set of proven libraries that are used today by the many high-scale services at Microsoft to help you with fundamental challenges around observability, resiliency, scalability, manageability, and more. Integrate large language models (LLMs) like OpenAI’s GPT directly into your .NET app. Use a single powerful component model to handle all your web UI needs with Blazor. Deploy your mobile applications to the latest version of iOS and Android with .NET MAUI. Discover new language enhancements that make your code more concise and expressive with C# 12. Let’s look at what’s new in .NET 8. Unparalleled Performance – Experience the fastest .NET to date .NET 8 comes with thousands of performance improvements across the stack. A new code generator called Dynamic Profile-Guided Optimization (PGO) that optimizes your code based on real-world usage is enabled by default and can improve the performance of your apps up to 20%. The AVX-512 instruction set, which is now supported, enables you to perform parallel operations on 512-bit vectors of data, meaning you can process much more data in less time. The primitive types (numerical and beyond) now implement a new formattable and parsable interface, which enable them to directly format and parse as UTF-8 without any transcoding overhead. Every year we talk about the performance gains across .NET. This year we continue our quest to push the performance of .NET to new heights. From the latest TechEmpower benchmarks with .NET 8, we’re seeing improvements in the JSON API scenario of 18%, hitting nearly one million requests per second with ASP.NET Core Minimal APIs. The Fortunes scenario is closer to a real-world workload, including database access and server-side HTML rendering. In this test, we see an even larger improvement of 24%, now over 300K requests per second with ASP.NET Core. .NET Aspire – An opinionated stack to build observable, production-ready cloud-native applications .NET Aspire is a stack for building resilient, observable, and configurable cloud-native applications with .NET. It includes a curated set of components enhanced for cloud-native by including telemetry, resilience, configuration, and health checks by default. Combined with a sophisticated but simple local developer experience, .NET Aspire makes it easy to discover, acquire, and configure essential dependencies for cloud-native applications on day 1 as well as day 100. The first preview of .NET Aspire is available today. .NET 8 Container Enhancements – More secure, compact, and productive Package your applications with containers more easily and more securely than ever with .NET. Every .NET image includes a non-root user, enabling more secure containers with one-line configuration. The .NET SDK tooling publishes container images without a Dockerfile and are non-root by default. Deploy your containerized apps faster due to smaller .NET base images – including new experimental variants of our images that deliver truly minimal application sizes for native AOT. Opt-in to even more security hardening with the new Chiseled Ubuntu image variants to reduce your attack surface even further. Using Dockerfiles or SDK tooling, build apps and container images for any architecture. Native AoT – Journey towards higher density sustainable compute Compile your .NET apps into native code that uses less memory and starts instantly. No need to wait for the JIT (just-in-time) compiler to compile the code at run time. No need to deploy the JIT compiler and IL code. AOT apps deploy just the code that’s needed for your app. Your app is now empowered to run in restricted environments where a JIT compiler isn’t allowed. Artificial Intelligence – Infuse AI into your .NET applications Generative AI and large language models are transforming the field of AI, providing developers the ability to create unique AI-powered experiences in their applications. .NET 8 makes it simple for you to leverage AI via first-class out-of-the box AI features in the .NET SDK and seamless integration with several tools. .NET 8 brings several enhancements to the System.Numerics library to improve its compatibility with Generative AI workloads, such as integrating Tensor Primitives. With the rise of AI-enabled apps, new tools and SDKs emerged. We collaborated with numerous internal and external partners, such as Azure OpenAI, Azure Cognitive Search, Milvus, Qdrant, and Microsoft Teams, to ensure .NET developers have easy access to various AI models, services, and platforms through their respective SDKs. Additionally, the open-source Semantic Kernel SDK simplifies the integration of these AI components into new and existing applications, to help you deliver innovative user experiences. Various samples and reference templates, showcasing patterns and practices, are now available to make it easy for developers to get started: Customer Chatbot Retrieval Augmented Generation Developing Apps using Azure AI services Blazor – Build full stack web applications with .NET Blazor in .NET 8 can use both the server and client together to handle all your web UI needs. It’s full stack web UI! With several new enhancements focused towards optimizing page load time, scalability, and elevating the user experience, developers can now use Blazor Server and Blazor WebAssembly in the same app, automatically shifting users from the server to the client at run time. Your .NET code runs significantly faster on WebAssembly thanks to the new “Jiterpreter”-based runtime and new built-in components. As a part enhancing the overall authentication, authorization, and identity management in .NET 8, Blazor now supports generating a full Blazor-based Identity UI. .NET MAUI – Elevated performance, reliability, and developer experience .NET MAUI provides you with a single project system and single codebase to build WinUI, Mac Catalyst, iOS, and Android applications. Native AOT (experimental) now supports targeting iOS-like platforms. A new Visual Studio Code extension for .NET MAUI gives you the tools you need to develop cross-platform .NET mobile and desktop apps. Xcode 15 and Android API 34 are now supported allowing you to target the latest version of iOS and Android. A plethora of quality improvements were made to the areas of performance, controls and UI elements, and platform-specific behavior, such as desktop interaction adding better click handling, keyboard listeners, and more. C# 12 Features – Simplified syntax for better developer productivity C# 12 makes your coding experience more productive and enjoyable. You can now create primary constructors in any class and struct with a simple and elegant syntax. No more boilerplate code to initialize your fields and properties. Be delighted when creating arrays, spans, and other collection types with a concise and expressive syntax. Use new default values for parameters in lambda expressions. No more overloading or null checks to handle optional arguments. You can even use the using alias directive to alias any type, not just named types! Collection expressions // Create a list: List a = [1, 2, 3, 4, 5, 6, 7, 8]; // Create a span Span b = ['a', 'b', 'c', 'd', 'e', 'f', 'h', 'i']; // Use the spread operator to concatenate int[] array1 = [1, 2, 3]; int[] array2 = [4, 5, 6]; int[] array3 = [7, 8, 9]; int[] fullArray = [..array1, ..array2, ..array3]; // contents is [1, 2, 3, 4, 5, 6, 7, 8, 9] See more about the latest version of C# in Announcing C# 12. .NET 8 support across Visual Studio family of tools We have a set of great tools that help you be the most productive in your development workflow and take advantage of .NET 8 today. Released alongside .NET 8, the Visual Studio 2022 17.8 release brings support for .NET 8, C# 12 language enhancements, and various new productivity features. VS Code and C# Dev Kit is a great way to get started with .NET 8 if you’re learning and/or want to quickly kick the tires of the runtime and is available on Linux, macOS, or in GitHub Codespaces. The new GitHub Codespaces template for .NET, which comes with the .NET SDK and a set of configured extensions, is one of the fastest ways to get started with .NET 8. Additional features in .NET 8: ASP.NET Core. Streamlines identity for single-page applications (SPA) and Blazor providing cookie-based authentication, pre-built APIs, token support, and a new identity UI. and enhances minimal APIs with form-binding, antiforgery support to protect against cross-site request forgery (XSRF/CSRF), and asParameters support for parameter-binding with Open API definitions ASP.NET Core tooling. Route syntax highlighting, auto-completion, and analyzers to help you create Web APIs. Entity Framework Core. Provides new “complex types” as value objects, primitive collections, and SQL Server support for hierarchical data. NuGet. Helps you audit your NuGet packages in projects and solutions for any known security vulnerabilities. .NET Runtime. Brings a new AOT compilation mode for WebAssembly (WASM) and Android. .NET SDK. Revitalizes terminal build output and production-ready defaults. WPF. Supports OpenFolderDialog and Enabled HW Acceleration in RDP ARM64. Significant feature enhancements and improved code quality for ARM64 platforms through collaboration with ARM engineers. Debugging. Displays debug summaries and provides simplified debug proxies for commonly used .NET types. System.Text.Json. Helps populate read-only members, customizes unmapped member handling, and improves Native AOT support. .NET Community Toolkit. Accelerates building .NET libraries and applications while ensuring they are trim and AOT compatible (including the MVVM source generators!) Azure. Supports .NET 8 with Azure’s PaaS services like App Service for Windows and Linux, Static Web Apps, Azure Functions, and Azure Container Apps. F# 8. Includes significant language changes, new diagnostics, improvements in usability, and performance enhancements in project compilation, as well as upgrades to the FSharp.Core standard library. What’s new in .NET 8. Check out our documentation for everything else! Get started with .NET 8 For the best development experience with .NET 8, we recommend that you use the latest release of Visual Studio and Visual Studio Code’s C# Dev Kit. Once you’re set up, here are some of the things you should do: Try the new features and APIs. Download .NET 8 and report issues in our issue tracker. Test your current app for compatibility. Learn whether your app is affected by default behavior changes in .NET 8. Test your app with opt-in changes. .NET 8 has opt-in behavior changes that only affect your app when enabled. It’s important to understand and assess these changes early as they may become default in the next release. Update your app with the Upgrade Assistant. Upgrade your app with just a few clicks using the Upgrade Assistant. Know you’re supported. .NET 8 is officially supported by Microsoft as a long term support (LTS) release that will be supported for three years. Bonus: eShop Sample for .NET 8. Follow all the best coding and architecture practices with our new eShop sample, now updated for .NET 8! Celebrate .NET 8 .NET Conf 2023. Join us November 14-16, 2023 to celebrate the .NET 8 release! What’s next in .NET? Get involved and learn the latest news on .NET 8 and the next version of .NET. Get C# Certified. Earn a badge of honor with a freeCodeCamp C# certification. Learn .NET 8. Free tutorials, videos, courses, and more for beginner through advanced .NET developers. All updated for .NET 8! See Developer Stories. Take a look at success stories of developers migrating to modern .NET. Read about why .NET?. Read through our recent blog series about the convenience of .NET. .NET ❤ Our Community We would just like to end by saying one big… Gaurav Seth Partner Director of Product, Developer Platforms Follow Posted in .NET .NET CoreTagged .net 8 featured",
    "commentLink": "https://news.ycombinator.com/item?id=38264937",
    "commentBody": ".NET 8Hacker Newspastlogin.NET 8 (microsoft.com) 296 points by runesoerensen 18 hours ago| hidepastfavorite321 comments xeromal 17 hours agoTangentially related but I was impressed with .NET recently. I was recently tasked with tackling an extremely old and proprietary video format that embedded GPS, video, audio, and several other components. The files were huge and just a mess to understand and I eventually found an old player application that was also proprietary.I decompiled it into C# and stripped the player aspect out of it leaving the code the peeled back the container and allowed me to export MP4 and other pieces. After getting to build into a console application on windows on .net 4 I think I decided to give it a go to upgrade it enough to compile AND run on linux. I upgraded 2x and then updated around 10-15 lines of code that no longer compiled. It was extremely easy considering there were 1000s of lines of code to parse this format. I finally got it running on linux and it only took about a day of full effort. Pretty crazy considering the code I was running was over 10 years old. reply declan_roberts 17 hours agoparentBackwards compatibility is something they take really seriously in the enterprise world, but something thrown out the window in the rest of the industry. reply jayd16 15 hours agorootparentI would argue that C#&#x2F;.NET goes just far enough but not too far with backwards compatibility. As seen in the anecdote, manual changes were required. Java, for example, is more likely to &#x27;just work&#x27; but I prefer the language designers&#x27; approach to C# and making the occasional breaking change to keep the language in good shape. I really appreciate their good taste. reply RajT88 17 hours agoparentprevThe .Net ecosystem has a lot of great tooling, for sure.The main issue I have had is if you are trying to get into the .Net Ecosystem without spending money. There are quite good free tools, but you have to figure out what works best for you if you&#x27;re used to the non-free ones.I assume instead of Reflector for the decompilation, you used DotPeek? reply bob1029 16 hours agorootparent> The main issue I have had is if you are trying to get into the .Net Ecosystem without spending money.I suggest not trying to do this.You can certainly make most of .NET8 work with a pure OSS toolchain, but your overall development experience is going to be destitute compared to that of the official tool chain. To be clear - I think paid alternatives, such as Rider are fantastic too, but even so I&#x27;ve had some trouble at the edges with paid 3rd party (Blazor apps, UWP, etc).At the end of the day, you have to ask yourself about what your hourly rate is. If you are going to spend a 20 hour premium per week bandaging up a \"free\" .NET toolchain, whereas the official, $100&#x2F;m stack costs you 1-2 hours, which one actually costs you more? What are your principles worth in terms of your free time? reply JAlexoid 16 hours agorootparent>At the end of the day, you have to ask yourself about what your hourly rate is.At the end of the day it&#x27;s a question of how much ROI are you getting. Your hourly rate is irrelevant, when you&#x27;re just learning a new tech... because when you&#x27;re hacking&#x2F;learning the hourly rate is $0. reply squeaky-clean 15 hours agorootparentIf you spend 20 hours fixing something that wouldn&#x27;t occur with the paid toolchain, at what price does it become worth it to skip those 20 hours when they&#x27;re not directly applicable to what you&#x27;re trying to accomplish? reply JAlexoid 14 hours agorootparentWhen you literally don&#x27;t have $20 in your account. Then you can spend 100 hours and it&#x27;s still worth it. reply jayd16 15 hours agorootparentprevJust because you&#x27;re not getting paid doesn&#x27;t mean your time is worthless (or your hourly rate changes). reply JAlexoid 14 hours agorootparentThat&#x27;s where ROI comes in.Should I have spent my full income for 3 months on an IDE, vs spending an extra 2 hours with the free tools?It&#x27;s not a hypothetical, that was the reality of my life in 2004. I had a lot of extra time, couldn&#x27;t use it to make money and buying \"enterprise grade\" tooling wouldn&#x27;t produce any extra value.The ROI of an professional grade tool would have been - \"you get to sleep extra two hours and have no food for 3 months\" reply neonsunset 16 hours agorootparentprevRider is absolutely great, give it a try. I have used and continue to use professionally macOS + VS Code&#x2F;Omnisharp combo and it has been working nicely with Rider serving as an escape hatch for more complicated debugging scenarios, profiling or when you need to step through (possibly decompiled) 3rd party code. reply SamFold 16 hours agorootparentprevWhat would you say is the current best in class toolchain for .NET (assuming I&#x27;m running on a ARM Mac)? reply gv83 16 hours agorootparentJetbrains’ Rider by a very long shotVscode stuff is ok too if you dislike big and featureful ides, but it’s behind rider. dotnet cli to run commands is sufficient until you have very strange builds.Keep in mind the language has many ways to do the same thing, so rider helps you doing the “modern” things. The base class library is also very vast. Take your time, C# is great but it has a ton of features. reply starik36 16 hours agorootparentprevWhat is the issue with using Visual Studio Community Edition? Or if not on Windows, using VS Code with the plugin. All free. reply wiseowise 16 hours agorootparentWasn’t there some issue with debugger on Non-Windows platforms? reply oaiey 14 hours agorootparentThe issue that it is free as in beer but not open source as in MIT. reply pjmlp 12 hours agorootparentI think you mean open source as in GPL, given that MIT doesn&#x27;t force to give the source code, hence why it is beloved for commercial products. reply hit8run 16 hours agorootparentprevVisual Studio for Mac is discontinued and will be retired next year:https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;visualstudio&#x2F;mac&#x2F;what-happ... reply starik36 14 hours agorootparentRight. They rewrote the .NET plugin for VSCode as a replacement. It&#x27;s reasonable. reply oaiey 14 hours agorootparentWell, the C# DevKit has to fix quite some bugs. I love it but it needs more work. replyguhidalg 17 hours agorootparentprevI&#x27;ve used ILSpy instead of JetBrain&#x27;s tools for this.If you&#x27;re a real OG, you used `ildasm`. reply xeromal 16 hours agorootparentI learned this from Jeffry Ritcher (https:&#x2F;&#x2F;www.amazon.com&#x2F;CLR-via-C-Developer-Reference-ebook&#x2F;d...)A lot of what I know about C# is from his books. I wish every language had someone of his caliber write a book about it.I think the version of the book I read was \"CLR via c#\" about 10 years ago reply fomine3 3 hours agorootparentprevdnSpy was the best reply xeromal 16 hours agorootparentprevI used dotPeek. Made quick work of it! reply starik36 16 hours agorootparentprevI use JustDecompile. It&#x27;s free, last I checked, and it works great. reply codegeek 16 hours agoparentprevMicrosoft has lot of issues but when it comes to Backward compatibility, no one comes close to them. Solid. reply kaba0 15 hours agorootparentWindows is good at it. .NET is not bad, but also not particularly good — compared to the Java side, at least. reply Salgat 12 hours agoparentprevThe compatibility shims that come implicit with .NET are incredible. In my experience many of the .NET Framework libraries work right out of the box on .NET (Core), even on Linux. reply financltravsty 17 hours agoprevLots of very cool stuff here (AOT, Aspire, etc.)..NET has been held down by the image of its early days, but it has become a pure joy to work with recently. The improvements in tooling and ergonomics have made it a replacement for Go in our org (we migrated from .NET Core 3.1 to Go back to .NET 6 recently). reply davewritescode 17 hours agoparentI just starting using .NET professionally very recently and coming from Go and Java a lot of the .NET platform is just downright pleasant. reply menacingly 17 hours agoparentprevWith consistent, reasonable-by-default threading abstractions baked in reply bmitc 17 hours agorootparentAre those new? What are they by name? reply neonsunset 17 hours agorootparent.NET has had first-class parallelism and async&#x2F;await since around .NET Framework 4.0 release which was...13 years ago huh. There have been many improvements in the underlying runtime since then though.A garden variety of keywords is async&#x2F;await, TPL (tasks&#x2F;futures and structured concurrency) and PLINQ (Rust&#x27;s Rayon). reply orra 13 hours agorootparentYou&#x27;re right the basic support&#x27;s been around a long time. However, originally you always had to use Task methods. The pleasant async&#x2F;await keywords didn&#x27;t arrive until C# 5.0, in 2012.Admittedly that&#x27;s also a long time ago. reply WorldMaker 12 hours agorootparentprevIn addition to the base TPL (System.Threading.Tasks), IAsyncEnumerable in recent .NET versions has deepened and simplified even more useful concurrency patterns (Async Generators), especially when paired with Ix.NET (System.Interactive.Async) which adds a lot more LINQ&#x2F;PLINQ operators, many of which are designed to intentionally mirror ReactiveX.NET (System.Reactive), the even older but just as useful push-based (IObservable) relative of IAsyncObservable. reply leosanchez 17 hours agorootparentprevI think the parent comment is talking about System.Threading.Tasks.I might be wrong though. reply bmitc 17 hours agorootparentOh, I see. I was hoping for something new. I keep on hoping some sort of BEAM-like (from Erlang) process gets added to .NET. reply zengid 15 hours agorootparentYou might be interested in Orleans: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;dotnet&#x2F;orleans&#x2F;overview reply bmitc 13 hours agorootparentOrleans is definitely interesting, but I have never looked into it much because it&#x27;s my understanding that it is a cloud-only solution, in that it requires some Orleans or Azure service. Is that correct? reply Rapzid 10 hours agorootparentFurther to the other reply you can run it alongside asp.net; one process can be an Orleans node and and an asp.net server. reply jen20 13 hours agorootparentprevNo, you can run in standard processes - there&#x27;s even documentation [1] about hosting it in Kubernetes.[1]: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;dotnet&#x2F;orleans&#x2F;deployment&#x2F;... reply valcron1000 16 hours agorootparentprevIt was tried and the dotnet team decided to drop it: https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtimelab&#x2F;issues&#x2F;2398 reply custard42 15 hours agorootparentinvestigations continue: https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtime&#x2F;issues&#x2F;94620 reply menacingly 16 hours agorootparentprevI have considered the that, but I ultimately decided they work because they are a fabric of what that ecosystem _is_, not what it has.It&#x27;s the same reason two sites that are more or less the same will have entirely different communities. There isn&#x27;t any reason you couldn&#x27;t implement the one on the other, but it&#x27;s not in the core substance. replywhoknowsidont 7 hours agoparentprev>.NET has been held down by the image of its early days,I have been in .NET since 2009. I don&#x27;t what would lead anyone to replace Go with .NET? That seems like it&#x27;s asking for a lot of problems you previously did not have.Can you expound on your decision? reply gettodachoppa 16 hours agoparentprev> .NET has been held down by the image of its early days, but it has become a pure joy to work with recentlyPeople have been saying that for the past 4 years.As someone who&#x27;s never used .NET but develops embedded apps for Linux, I had high hopes for it become the best story for crossplatform development, it still feels like the crossplatform is held by ductape. It&#x27;s still Xamarin for mobile, Avalonia (a 1-man project) for desktop Linux as the only choice, 2 confusing different UI frameworks (WinUI vs MAUI). They don&#x27;t take this as seriously as I&#x27;d hoped.When they made it open-source and cross-platform 6-7 years ago, they should&#x27;ve thrown enough manpower that within 3 years (or even 2, this is Microsoft with endless coffers) it was unquestionably the best development option for nearly every scenario.Additionally, it&#x27;s my understanding they don&#x27;t release proper debuggers on Linux? I&#x27;m not gonna install Visual Studio just to debug my .NET app bro. reply GordonS 16 hours agorootparent> I had high hopes for it become the best story for crossplatform development, it still feels like the crossplatform is held by ductape.Sorry to be pedantic, but it&#x27;s cross-platform GUI that still doesn&#x27;t have a good, officially supported story.For non-GUI apps, cross-platform works very well! I&#x27;ve deployed a range of apps to production across Windows, Linux (both x64 and ARM), and Docker on Linux too (also both x64 and ARM) with great success.I&#x27;d like to see an officially supported build for BSD, but that&#x27;s about my only cross-platform want. reply Const-me 11 hours agorootparent> cross-platform GUI that still doesn&#x27;t have a good, officially supported storyYeah, but in the meantime there’re decent unofficial options. Once I compiled NanoVG into a DLL, consumed with C#, and implemented rich GUI on top of that. Worked pretty good overall. Eventually I’ve patched NanoVG for optimal font quality on low resolution touch screen of my target Linux device: https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;nanovg reply majora2007 15 hours agorootparentprevI just saw in the dotnet-labs project they&#x27;ve been building FreeBSD support.https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtimelab&#x2F;tree&#x2F;feature&#x2F;FreeBSD reply mdasen 15 hours agorootparentprevI think that it takes many years to change a reputation - more than 4. A lot of devs don&#x27;t like learning new things and they would have gotten deep into a different ecosystem before .NET was really open source.You note that they open sourced it 7 years ago, but the original .NET Core was kinda an experiment. Microsoft didn&#x27;t quite seem to know where they&#x27;d ultimately go with it. I think Microsoft really committed to it in 2018 or 2019. I think you also might have unrealistic expectations of how quickly things can be accomplished. Flutter is over 6 years old and still feels a bit mediocre to me - and I think Flutter is trying to solve a simpler problem since it isn&#x27;t using native widgets.For cross-platform UI, I think there is some disappointment in the .NET community. MAUI has succeeded Xamarin (except for Linux), but I think your sentiment that Microsoft isn&#x27;t investing in it enough is shared by many in the community.I think the problem is that Microsoft bit off more than it can chew with MAUI. Avalonia, Flutter, Compose Multiplatform, Electron, etc. tend to just paint non-native widgets via Skia or JS&#x2F;HTML. With MAUI, Microsoft is offering actual native interfaces which means having to deal with a lot of platform differences to implement that rather than just painting their own widgets with a graphics library.I&#x27;m not trying to talk you out of being disappointed with MAUI. It has been disappointing. However, I&#x27;m still hopeful for it simply because it&#x27;s the only effort I see trying to offer a native cross-platform UI dev experience. Flutter, Avalonia, Compose Multiplatform, etc. are all saying \"We&#x27;ll just take a window and paint our stuff in it. Do you really care about an app feeling native?\" It isn&#x27;t always important, but it does feel like so many given up on native apps.I&#x27;m hopeful that MAUI will keep getting better (as it has been) and that we&#x27;ll have the opportunity for a native UI experience in the future. reply theshrike79 15 hours agorootparentprevThis is not a .NET problem.There really isn&#x27;t a simple way to do cross platform (Windows&#x2F;Linux&#x2F;macOS) UIs with any language. The least bad option is to make it a web page, maybe wrap it with Electron. reply oaiey 13 hours agorootparentYeah, but MAUI is basically react native, Blazor on Maui basically Cordova&#x2F;electron (just with much better plugin support) and Maui also had once some support for rendered control.While I agree it is hard, there is no one who could rule this space better than .NET if they just would like.Problem is: this is developer division funding which is not the Windows (which owns a UI stack) or office division (react native actually). DevDiv previous UI toolkits WPF was loved but then abondon because of the duplicity of UI toolkits in Microsoft. reply gettodachoppa 13 hours agorootparentprevIt doesn&#x27;t need to be 100% perfect. Just 99%. Don&#x27;t let perfect be the enemy of great.I write Qt GUIs for Linux+Windows, and no Windows user can tell it&#x27;s not \"native\" (not that there&#x27;s a native Windows look anymore, what with MS being schizophrenic about scrapping the UI framework and starting a new one everytime a new PM wants attention, only the classic widget look associated with Windows is still the 2000&#x2F;XP&#x2F;7 is recognizable).Qt apps are super performant, and write once, run anywhere for 99% of GUIs anyone wants to create.OSX users might be the only one that complain, but they&#x27;re a tiny minority of users, and it&#x27;s good enough in most cases, with fantastic performance to shut up the doubters.Electron apps are hideous, bloated, and slow, relative to the Qt apps I write. I&#x27;m only held back by C++ being a shitty archaic language&#x2F;ecosystem, and if .NET offered an alternative I&#x27;d jump ship immediately, that&#x27;s why I&#x27;m so disappointed it&#x27;s an afterthought.P.S. Languages designed for \"hypertext markup\" are not meant to create rich UIs, and can never compete with a native framework. Everything in the JS world to create rich applications is a hack and you can see it in the code and the endless churn of frameworks. When I write something like QML, the intent is clear. When I look at the JS UI frameworks people are hacking hideous HTML templates and conditional logic, list iteration, etc in HTML templates. And nearly every JS UI framework project is written by 1-2 intermediate devs with comparatively little API design talent, who inevitably break backwards compatibility within a year or two. reply tomcam 12 hours agorootparentprevLivecode does the cross platform thing reply jcpst 15 hours agorootparentprevMaybe the desktop UI story isn’t quite there yet. But I’ve been using .NET >= 5 web apps for years on linux, and it’s pretty solid with Rider. No issues debugging.While I would prefer if I could use free tooling, Rider pays for itself in time savings very quickly. reply mjevans 16 hours agorootparentprevWindowing and Widgets; those are the two things that always seem to be the biggest issue for cross-platform apps that should be trivial if the groundwork were in place.I solidly blame MS for shipping something like 10+ of their own frameworks and never E.G. including QT or anything else open. Apple&#x27;s just as bad, if not actively worse, for relegating even open 3D modeling languages to second class. reply neonsunset 16 hours agorootparentprevWhen it comes to the OSS aspect itself, debugging experience is not the strongest suit but if you overlook it - both Linux and macOS are very well supported by either VS Code &#x2F; or any other LSP compatible editor + Omnisharp (or DevKit nowadays I guess?) or Rider (which is commercial but is often even better than Visual Studio, definitely faster too). reply oaiey 14 hours agorootparentprevI would love that they support a framebuffer (rendered controls) or Linux native control rendering in Maui for exactly that reason. But no, QT continues it&#x27;s painful reign. reply desi_ninja 16 hours agorootparentprevMAUI is built using WinUI. It is same platform. reply oaiey 13 hours agorootparentMaui is built on top of winui, Android ui and iOS UI. It just looks like WPF, which again influenced winui reply geitir 17 hours agoparentprevWow that’s a lot of busy work reply haolez 17 hours agoparentprevAt least until it gains an immense market share and Microsoft starts monetizing it again. They&#x27;ve already tried in .NET Core 7, no? I&#x27;m staying away for now. reply oaiey 17 hours agorootparent.NET 7? There was no monetizing in it. If you want to look somewhere, look at the C# DevKit for VS Code. reply dragonwriter 17 hours agorootparentprevThere is no such thing as .NET Core 7, “Core” was a temporary distinction when the Windows-only . NET Framework was still being actively developed, retired with the consolidation to a single .NET with .NET 5.If you got .NET Core 7 somewhere, I’m sure it is monetized: that&#x27;s often the point of a scam. reply pjmlp 12 hours agorootparentAs someone that knows .NET since it was only available to MSFT partners, I find quite valuable the Core and Framework suffixes, especially when discussing with people not versed in the .NET history, as many people get confused why .NET 4 isn&#x27;t compatible with .NET 7 for example. reply haolez 16 hours agorootparentprevThe user is not to blame for the confusing naming that the platform owner chooses. reply SideburnsOfDoom 1 hour agorootparentprevMicrosoft&#x27;s main monetization strategy is to get people to run their code on the Azure cloud.If it means running code in any lang, on a linux vm in Azure, so be it.If it means giving away dotnet, so be it.If it means that dotnet on AWS is also popular, so be it.They&#x27;re pretty clear and open about that, you don&#x27;t have to look for hidden motives. reply endianswap 17 hours agorootparentprevCan you share any details about .NET Core 7&#x27;s monetization? I couldn&#x27;t find anything on Google. reply 9cb14c1ec0 17 hours agorootparentIt&#x27;s all open source and free, on Github. I&#x27;ve used .NET for about 10 years now, and the only money it has cost me is a few third party libraries that I couldn&#x27;t find good open source competitors for. reply haolez 16 hours agorootparentBut you&#x27;ve never seen it with a market share of more than 90%, which could happen eventually. It&#x27;s a matter of incentives and those play against you. reply Salgat 12 hours agorootparentMicrosoft owns the .NET trademark but the language and source code for the current runtime is freely forkable and usable by any person&#x2F;company who wants to do their own implementation. It&#x27;s no different than the situation with Oracle and Java. reply haolez 9 hours agorootparentThat&#x27;s actually a very good point. I&#x27;ll rethink my position. reply mdasen 16 hours agorootparentprevI don&#x27;t agree with the person&#x27;s suspicions, but I can speak to your question.If you&#x27;re making over $1M&#x2F;year or have more than 250 employees, you have to pay for Visual Studio or the C# DevKit in VS Code. Microsoft has been taking the open-core of VS Code and putting proprietary extensions into it. However, this isn&#x27;t just C#. Microsoft has also replaced the open-source Python extension with something closed source.There is an alternative to Microsoft&#x27;s tooling with JetBrains&#x27; Rider. Rider is pretty amazing. It&#x27;s much higher quality than any of the other JetBrains tools and part of that is that the .NET ecosystem is really amenable to tooling (and paying for tooling) and that JetBrains had been creating ReSharper for Visual Studio for a long time (so they already had all the .NET intelligence built out).There was a thing where Microsoft was going to remove hot-reload from the dotnet CLI tool and make it only available in Visual Studio. I kinda felt like some of this was motivated by timing more than strategy, though. The CLI&#x27;s hot reload had some rough edges and .NET has corporate style release dates with fanfare like .NET Conf. But the .NET hot-reload is quite excellent. Most edits can be hot reloaded and you can set it to simply re-compile and re-run for the ones that can&#x27;t hot reload (and it just takes a couple seconds). It&#x27;s dead simple to use and works (and the amount of time I&#x27;ve wasted to get hot reloading in Java is just incredible).Really, every ecosystem tries to find some way to monetize. The creator of Elm has a great talk on this: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XZ3w_jec1v8.Microsoft&#x27;s monetization feels reasonably benign to me. If I&#x27;m bringing in $1M&#x2F;year or have 250 employees, I can pay $500&#x2F;year&#x2F;engineer (or maybe less with volume or annual licensing; or $250-420&#x2F;year&#x2F;engineer for Rider for an organizational license). Sure, there are costs, but it&#x27;s not like $500 is that interesting if I&#x27;m paying engineers $150,000&#x2F;year. Likewise, Microsoft likes .NET as a bit of a halo project to get people into Azure. I don&#x27;t use Azure at all and there aren&#x27;t really Azure specific features, but in the same way that the iPod&#x2F;iPhone brought a lot of people to the Mac, .NET probably brings some people to Azure. Even if you&#x27;re a developer that knows better, you might have someone in your org who thinks \"oh, it makes sense to deploy with Microsoft if we&#x27;re developing with Microsoft.\" There&#x27;s a lot of money up for grabs from that thinking. It likely also drives sales of things like Microsoft SQL Server. .NET works great with PostgreSQL or SQLite (and I&#x27;d assume MySQL as well, but I haven&#x27;t used it), but I&#x27;m sure plenty of organizations pay for MS SQL Server with the \"why not all from Microsoft\" thinking.So yes, Microsoft isn&#x27;t looking for zero monetization, but the IDE license fees don&#x27;t seem like a problem - either you&#x27;re small enough that it doesn&#x27;t matter or you&#x27;re large enough that the fee is small. The rest of it is just stuff I can ignore (Azure, MS SQL Server).But Microsoft&#x27;s money often comes with some nice things. There&#x27;s good documentation, a lot of great improvements each year, lots of great video sessions, and some truly amazing blog posts. .NET really has some amazing blog posts - and part of that is that Microsoft gives people the time to create some really high-quality content. If you haven&#x27;t watched the Elm video I linked above, I highly recommend. It really talks about a lot of the things that money brings to a community. .NET comes with a lot of polish - more than I&#x27;ve seen in any other ecosystem and I&#x27;ve been programming a long time (and I&#x27;ve never been a Windows user).If the cost of a healthy ecosystem is that companies with enough money to pay a few hundred dollars a year have to kick in a little support, I&#x27;m good with that. If I create something that starts bringing in $1M&#x2F;year, I think it&#x27;s reasonable to think I should kick in some money. I also understand others who don&#x27;t like that - especially since it&#x27;s kicking in money to Microsoft, not to some non-profit community foundation. But .NET feels practical to me. The money in the ecosystem means that a lot of high-quality stuff gets produced. reply neonsunset 15 hours agorootparent(perhaps downvoted by those who never had to face with realities of running OSS projects or otherwise) reply haolez 17 hours agorootparentprevI think it was related to tooling. Something about hot reloading. Can&#x27;t remember the details. reply Smaug123 17 hours agorootparentYou&#x27;re thinking of https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;sdk&#x2F;issues&#x2F;22247 reply Thaxll 17 hours agoparentprevBig difference is that Go is stable and conservative, net core keep adding a billion features every release. Looking at some net core 2&#x2F;3 stuff and now to see how many things changed.Good luck with your code base in 5 years when someone decide to add all the new stuff. reply chrisandchris 17 hours agorootparentI don&#x27;t know Go that much, but the part about .Net is so untrue. First, C# was published in 2001, while Go was released in 2009. So there&#x27;s 8 years of \"more stuff\".Also (see in comments too), upgrading .Net is very easy. There are man not-to-small codebases can be \"easily\" upgraded from .Net Core 3 to 8 without any hassle. Backwards comptability was always baked into Microsofts thinking..Net 7 and 8 did also (IMHO) not contain that many groundbreaking features, just some \"smaller\" stuff. reply Thaxll 14 hours agorootparentDo you want to compare the number of features added between go and c# in the last 15 years? Just a single major release of C# added more stuff than probably the last 15 years of Go.I&#x27;m not saying Go is 100% right in that, but it&#x27;s much more conservative about stuff added into the language. reply HackerThemAll 13 hours agorootparentOf course, they conservatively added the most idiotic way of formatting dates as strings (while knowing other methods exist). And, while the whole bunch of other languages would call a method string.startsWith(\"a\"), they use hasPrefix. This language is a mix of half-baked concepts and utter trolling. If I have to learn a lang with an awkward syntax, I&#x27;d prefer Rust, Julia, Erlang, Elixir or Haskell. Go is no-go. It&#x27;s not even widely used at Google, its creator. So no, thanks. reply WorldMaker 12 hours agorootparentAlso, why would force error returns without also including some sort of nice Monadic do-notation or Monad-inspired combinator syntax sugar in the 2020s? We&#x27;ve known for decades now how do that better.I also think that Go looks like a practical joke that escaped some INTERCAL-like satire committee in the 1970s. reply hollerith 12 hours agorootparentYou probably mean sum types and convenient syntax for testing and de-structuring sum types. reply WorldMaker 11 hours agorootparentNo I specifically meant what is often called the Either or Result Monad. These are often implemented as Sum types, but they also follow the Monad laws and supports things like monadic binding&#x2F;transformation&#x2F;combination and benefit from syntax sugar like Haskell&#x27;s \"do-notation\"&#x2F;OCaML family&#x27;s \"computation expressions\"&#x2F;the \"async&#x2F;await\" pattern of so many other languages now.Good sum types destructuring would probably also be useful in more languages like Go, but many languages today provide \"async&#x2F;await\" pattern syntax benefits without anything at all like \"proper\" sum types. Sum types are \"merely\" a possible implementation detail of the Monad and it&#x27;s the Monad laws that empower some things beyond just sum types. replyjen20 11 hours agorootparentprevThe real problem was upgrading from .NET Framework, or Mono, to .NET Core, which has been stable since.That comparison is not a fair comparison to Go, which has a much stricter guarantee around source and library compatibility within the 1.x series. reply Kwpolska 16 hours agorootparentprev.NET is stable, they very rarely remove things. Code written for .NET Framework 2.x will probably work fine on .NET 8 (unless it uses the things that didn&#x27;t make the jump, like WCF, but there are libraries that provide SOAP support.)Can a single developer remember all of .NET? No, but that applies to most languages&#x2F;ecosystems. But if you encounter something you&#x27;ve never seen, you can always check the docs.And the thing hasn&#x27;t been named \".NET Core\" for 4 consecutive releases now. reply merb 16 hours agorootparentActually most wcf code does run on .net 7+ they open sourced it into corewcf. (At least a lot of the things) Webforms however is dead. reply papichulo2023 16 hours agorootparentprevEhh, that is the thing about Go, most Go developers know most of the std (compared to other languages at least) reply starik36 16 hours agorootparentprev> Code written for .NET Framework 2.x will probably work fine on .NET 8WebForms would like a word. reply Kwpolska 13 hours agorootparentHence “probably”. WebForms should die in a dumpster fire. reply WorldMaker 12 hours agorootparentI worry that Blazor is that dumpster fire sometimes. reply custard42 8 hours agorootparentI&#x27;m curious, why do you say that? replypjmlp 16 hours agorootparentprevThankfully .NET folks are confident that their audience has the skills to manage those features. reply The_Colonel 16 hours agorootparentprev.NET &#x2F; C# seems to be following C++ by including... everything. I wonder if they&#x27;re just not worried about feature bloat or what&#x27;s their thinking... reply Metasyntactic 16 hours agorootparentHi there, C# Lang Designer here. :)We&#x27;re always thinking about the bloat concern when it comes to language development. However, our philosophy on it is that bloat primarily comes when you add replacement systems that are expected to supersede the previous mechanisms, not compliment them. So we try to do the former sparingly. In the history of C# there are very few times we&#x27;ve actually done this, and we do view those times as unfortunate cases where we likely rushed a feature too early and then regret having to live with those features forever.To help combat this, we tend to go through long periods of design and experimentation, where we propose features, create prototypes of them, and then interact with a large set of diverse community groups to try things out. The feedback from this is tightly bound into our design process and allows us to refine (or even jettison) designs rapidly.We also normally will both break up work into lots of smaller pieces (composing large language changes into small orthogonal, complimentary, composable blocks), and do designs over many years if appropriate. We think this approach has helped us create a language that is 25 years old, while being both very rich, and still very cohesive. There are a few mistakes we&#x27;ve made along the way (\"anonymous-delegates\", i&#x27;m looking at you), but we&#x27;re very happy that our ratios here are very good given our continued investment in this space. reply terandle 14 hours agorootparentThe new array literals seem to supersede some previous syntaxes like new[] { 1, 2, 3 } goes to [1, 2, 3] in a lot of cases. But overall they are a _great_ addition to the language playing around with them now so bravo.Still not really sure how I feel about class primary constructors on the other hand, will give them some time to marinate. reply WorldMaker 12 hours agorootparentHaving moved to writing records by default in many places such as models and \"DTOs\", primary constructors are a very useful \"transition\" tool in those cases of \"this started as a record, but now needs to be a mutable class for X reasons\" and means a little bit less code to entirely rewrite in the first pass. It&#x27;s also useful for even just \"I aesthetically like for my class &#x27;View Models&#x27; to mirror their record &#x27;Model&#x27; definitions a little closer\". reply Smaug123 11 hours agorootparentprevClass primary constructors have always been the default thing in F#, and I found it deeply annoying not to have them whenever I had to write C#, for what it&#x27;s worth! reply oaiey 13 hours agorootparentprevShare the exactly same worries. Let us see. reply zengid 15 hours agorootparentprevC# has been able to add new features while simultaneously making the syntax more ergonomic and elegant, while C++ hasn&#x27;t. reply Capricorn2481 16 hours agorootparentprevPeople can write dumb, inconsistent code anywhere. A stable library will not protect you from a lack of PR reviews reply neonsunset 16 hours agoprevIt may go under the radar of many but .NET 8 comes with DynamicPGO which is now enabled by default (improved since its earlier iterations in 6 and 7, which were opt-it).It will help numerous abstraction-heavy codebases the most thanks to guarded devirtualization of interface&#x2F;virtual calls, delegate inlining and branch reordering which has progressively more impact the more bloated code is. reply jordigh 17 hours agoprevCan I get an explanation of the relationship between .NET and Mono?I have vague memory of reading somewhere that Mono or maybe just MonoBuild was completely obsolete since everything was now open source in .NET.Does Mono still have a reason to exist? Is it all incorporated into .NET now?(btw, I still don&#x27;t get why it&#x27;s called .NET, and I don&#x27;t know if assemblies are native code or bytecode wrapped in the same binary format as .exe, where is the \".NET for hardcore open source Linux nerds\" blog post?) reply martindevans 17 hours agoparentThere used to be .NET Framework (which was a Windows only runtime built by Microsoft) and Mono (an open source implementation for various other platforms).In 2016 they started building .NET Core which is new open source implementation (built mostly by Microsoft) which runs on more platforms. For a while all three existed side by side.Eventually .NET Core caught up and overtook the other implementations. These days Framework is legacy. Core has been renamed to just .NET (since it&#x27;s now _the_ runtime) and Mono (as far as I know) has been totally replaced by it.This is all from memory, so apologies for any inaccuracies! reply oaiey 17 hours agorootparentWhile correct, modern .NET supports currently two VM: CoreCLR and MonoVM. The later mainly as AOT compilation target for iOS, Android and WebAssembly. Mono is still better suited for that, but the AOT compilation of CoreCLR is progressing.I think there is nothing else left from the Mono project like the class library or the compiler&#x2F;toolchain. reply pjmlp 16 hours agorootparentprevActually, .NET Framework is the Python 2 of the .NET world, with all the references that it entails.Still too many enterprise products stuck in the old ways. reply Semaphor 15 hours agorootparentUnlike with python, some things simply can&#x27;t be ported, though. I maintain a webforms page, the only upgrade path would be a full rewrite reply pjmlp 13 hours agorootparentPython 2 => 3 also has the issue that some things don&#x27;t exist in Python 3, or are done in incompatible ways.As I mentioned a few times, I have been involved in projects were that full rewrite happened to be .NET Framework => Java, which shows how the customers were mad at what was left behind.Although there is some irony in that, as Java&#x27;s Python 2 problem is the transition into Java 9. reply Semaphor 6 hours agorootparent> as Java&#x27;s Python 2 problem is the transition into Java 9Binary incompatibility between 7 and 8 as well, we got an enterprise product that got bought by SAP and will eternally be stuck on Java 7, but at least unlike the other one, it doesn’t require much maintenance as it’s only used internally ;)> Python 2 => 3 also has the issue that some things don&#x27;t exist in Python 3, or are done in incompatible ways.What doesn’t exist? I don’t like python, but it always seemed like those changes were all relatively minor? reply pjmlp 3 hours agorootparentThe way unicode was handled, integer division semantics, some libraries went away, and a few other things change enough that 2to3 wouldn&#x27;t handle everything, which in a dynamic language is a big pain to track down in a huge codebase.Naturally some changes on the C API for native libraries as well. reply Semaphor 2 hours agorootparentFair enough, thought that still sounds like things where an upgrade path is available, just not automatically, instead of \"you have to throw the code away because it simply doesn’t work anymore\". replybmitc 17 hours agoparentprev.NET used to be Windows-only. Mono was a cross-platform implementation of .NET done by a third party and not by Microsoft.Microsoft eventually created .NET Core, their own cross-platform implementation of .NET, which eventually became .NET 5, 6, 7, and now 8.Mono should not be used for new projects. .NET 8 is vastly superior. Mono is only still used for things like Unity who have decided to not or slowly migrate off of Mono. reply Karliss 16 hours agorootparentIt&#x27;s not that Unity doesn&#x27;t want to move away from Mono, it&#x27;s just that Mono is tightly integrated into the Unity engine making migration difficult. They have announced that they are working on it, but it will likely take years until there is anything usable publicly available. reply sander1095 17 hours agoparentprevMono is still used by Unity, and by .NET MAUI for their iOS and Android implementations. https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;dotnet&#x2F;maui&#x2F;what-is-maui#h....For most .NET apps you can use .NET 5&#x2F;6&#x2F;7&#x2F;8. reply neonsunset 17 hours agorootparentApparently .NET MAUI iOS target can run on NativeAOT[0] instead (with better performance and smaller binary size) but I&#x27;d expect it to be a bit unstable experience for now since it&#x27;s very new.[0] https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;dotnet&#x2F;dotnet-8-performance-i... reply anaisbetts 17 hours agoparentprev> (btw, I still don&#x27;t get why it&#x27;s called .NET, and I don&#x27;t know if assemblies are native code or bytecode wrapped in the same binary format as .exe, where is the \".NET for hardcore open source Linux nerds\" blog post?).NET can now do either one - you can ship PE bytecode and require the dotnet runtime, which works in practice similar to e.g. Python or Ruby (you run `dotnet .&#x2F;myapp.dll`), or you can precompile it into a single platform-specific binary that is just a native code app, similar to Go&#x27;s static binaries. reply piaste 17 hours agoparentprevOne reason is that Mono supports a bunch of minor platforms [0] that .NET doesn&#x27;t - yet (for example, FreeBSD support has an active Github issue [1]... although it&#x27;s been active for _eight years_).If you&#x27;re running Windows, Mac, or Linux I&#x27;m not aware of major reasons to use Mono.[0] https:&#x2F;&#x2F;www.mono-project.com&#x2F;docs&#x2F;about-mono&#x2F;supported-platf...[1] https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtime&#x2F;issues&#x2F;14537 reply blackoil 17 hours agoparentprevIn olden days .net was Windows only. Mono was an independent effort to build a cross platform runtime. With .net core MS made it open source and cross platform. They also bought Mono but that is more or less abandoned I believe latest versions of .net run well on Linux and Mac and now also in browser. They have also dropped Core from the name and there is only one .net which is open source and cross platform.Edit: seems some part of Mono still live. reply WorldMaker 12 hours agorootparentMono wasn&#x27;t abandoned so much as \"code merged\" (a beauty from that both are now open source). Mono merged into the main .NET repo and the best parts of it became \"a single runtime\". There are still useful parts of Mono left that aren&#x27;t exactly inside the \"a single runtime\" yet, but the amount of \"Mono code\" shrinks with each version and the more it is all \"just\" .NET. reply strich 17 hours agoparentprevThe answer is complicated, that&#x27;s why. In a good way kinda - you can build IL bytecode exes that are portable across all platforms and you can also build fully native platform specific binaries too (though there can be gotchas around library support). reply orra 17 hours agoparentprevMonoVM lives on in the .NET Runtime repo. This is used for non-AOT WebAssembly (or at least it was in .NET 5). reply SideburnsOfDoom 17 hours agoparentprev> Does Mono still have a reason to exist?From .NET 5 onwards, the framework is cross-platform. So as a separate thing, Mono isn&#x27;t usually needed or part of the mainstream any more.> I don&#x27;t know if assemblies are native code or bytecode wrapped in the same binary format as .exe,Yes, they are. Either, depending on your build target. reply pdntspa 17 hours agoprevI just wish its cross-platform UIs weren&#x27;t such a mess. Particularly with its MVCish implementation, it was like it couldn&#x27;t decide what it wanted to be.I really really wish there was a good, straightforward desktop GUI for .Net that simply worked crossplatform and wasn&#x27;t a complete pain in the ass to program. reply creshal 17 hours agoparentIn their defence, does any other language have one, these days? The landscape hardly looks good, which is how electron et al. got so popular to begin with. reply collaborative 17 hours agorootparentJava and Python have a few. I still don&#x27;t get how electron got so popular. It&#x27;s always been pretty straightforward to ship an app with a self-contained Java runtime, much the same as electron ships with v8. Java GUIs have always been worse than c++ ones, but still.. much better than electron I&#x27;d say reply creshal 17 hours agorootparentAnd now try to get either to work mobile, or in a browser, or at least try to share frontend code between them. That&#x27;s where electron shines, and all the \"classic\" frameworks really don&#x27;t. QtQuick and JavaFX kiiiind of work for at least some platforms, but it&#x27;s fairly rough all considered, even compared to the inelegant brute force solution of electron.I&#x27;d put my money on the competing \"electron, but smarter\" solutions that keep bubbling up, rather than any language-centric framework. reply pdntspa 16 hours agorootparentI hate that we are completely enamored with browsers. Desktop apps and browser webapps are different platforms, with different methodologies and motifs.I hate hate hate it when a desktop app looks and acts like a web app. reply collaborative 16 hours agorootparentprevI wish that CodenameOne could have been the norm instead of Electron. Unfortunatrly, its desktop port never matched its mobile one in terms of functionality (still uses JavaFX for web views). I still prefer it over Electron where web views are not required i.e. nowhere reply pdntspa 16 hours agorootparentprevpySide6 has been a dream for me, it &#x27;just works&#x27; and QT is quite robust.I am currently fucking with Java via GraalVM, and packaging my app for distribution has been absolute hell:jlink throws errors.jpackage somehow gets stuck in an infinite loop that generates a directory tree of seemingly infinite depth (so deep that even Explorer can&#x27;t delete it)GraalVM native-image can&#x27;t complete and throws all sorts of weird error messagesTrying to modularize the app has been hell too, it seems like every one of your dependencies needs to be modularized as well?And like all I am using is Swing, FlatLaf, and MigLayout :(How the hell are people getting apps running in a standalone environment?I really like Java the language, but getting a desktop app running on it in 2023 is a nightmare. reply vips7L 15 hours agorootparent> I really like Java the language, but getting a desktop app running on it in 2023 is a nightmare.This is honestly because you are taking the hard path and trying to do AOT compilation. Just use Swing with HotSpot. Native Image isn&#x27;t meant for GUI applications and Swing internally uses a TON of reflection.Using jlink and jpackage with HotSpot has been trivial in my experience. reply pdntspa 15 hours agorootparentI&#x27;ll look into that, thanks reply vips7L 15 hours agorootparentAlso check out Hydraulic if you need more than just packaging, it includes things like autoupdate and custom launchers: https:&#x2F;&#x2F;conveyor.hydraulic.dev&#x2F;12.0&#x2F;configs&#x2F;jvm&#x2F;#overview reply pregnenolone 6 hours agorootparentprev>jpackage somehow gets stuck in an infinite loop that generates a directory tree of seemingly infinite depth (so deep that even Explorer can&#x27;t delete it)This happens when your jar is in the same folder as the working directory: https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;64743880>I really like Java the language, but getting a desktop app running on it in 2023 is a nightmare.I think it&#x27;s very easy. Do you use a build tool? reply pdntspa 5 hours agorootparentI am using MavenGetting the jar file is easy. But its deploying it that has been difficult reply Capricorn2481 16 hours agorootparentprevI don&#x27;t know anything about the QT on python as a runtime. Is it supposed to be just as fast since it&#x27;s bindings to C++, or is it noticeably slower? reply pdntspa 16 hours agorootparentI haven&#x27;t done anything terribly performance-sensitive with it, but what I have written has performed extremely well. My app starts up to its main window instantly (which SHOULD be table stakes...) reply mike_hearn 15 hours agorootparentprevTry https:&#x2F;&#x2F;conveyor.hydraulic.dev&#x2F;, we replace or drive the whole deployment stack and have plenty of JVM using customers. It&#x27;s a lot easier, try it out and see if it works for you. I should say though, that the support for native image is experimental. reply pdntspa 15 hours agorootparentWhen I am ready to mark the github repo as public I was going to consider Conveyor, as paying money for it is a no-go right now (unemployed and broke rn) reply collaborative 13 hours agorootparentprevNot sure if it suits your use case, but the best I have experienced is jDeploy reply pdntspa 12 hours agorootparentInteresting... I like how it piggybacks off npm reply booleandilemma 15 hours agorootparentprevI think you had a massive number of exclusively JS developers out there who couldn&#x27;t build a UI outside of a website but wanted to. Then Electron came along and gave them that power. That&#x27;s how it got so popular, imo. reply pdntspa 16 hours agorootparentprevLazarus for FreePascal, write your code hit compile and get a nice EXE that starts up on Mac&#x2F;Windows&#x2F;Linux reply Capricorn2481 16 hours agorootparentI think you can always train devs, but lots of companies will avoid Lazarus&#x2F;Delphi like the plague reply strich 17 hours agoparentprevI would say Blazor is really very very good now. One of the most awesome things about it is that your skill with it transfers almost directly between a desktop app, web API, or web site. All the code is almost the same. reply Someone1234 17 hours agorootparentBlazor is just history repeating itself. This is Web Forms with a shiny coat of paint and the \"magic\" this time is WebAssembly and a different proprietary web interface that is undebuggable.I&#x27;ve heard this \"Microsoft has finally created a completely cross platform UI framework\" like twenty times... reply nu5500 10 hours agorootparentIt is nothing like Web Forms (I&#x27;ve built dozens of Web Forms and Blazor apps so I know), and WASM is just one delivery vehicle for it. You can also render it server side or locally in-process in a full .net runtime (no WASM needed). The debuggability for the WASM runtime has improved alot as well since .net 7. reply viraptor 17 hours agoparentprevNot first-party, but AvaloniaUI is doing pretty well in the cross platform department. reply 9cb14c1ec0 17 hours agorootparentAvaloniaUI is miles ahead of any other cross platform library that I&#x27;ve ever used. It actually looks good! reply pdntspa 16 hours agorootparentI forgot to mention that I was referring to Avalonia, but when I tried to write something relatively simple beyond the music-player demo app, I got lost in the weeds extremely quickly. Avalonia needs a lot of work on its documentation, there are huge gaps, the quality is really inconsistent, and its abstractions fall apart really quickly reply akgoel 17 hours agoparentprevThey should have fixed up Wine to run Winforms flawlessly, and it would have solved the cross-platform desktop problem. reply pdntspa 16 hours agorootparent50MB) made me use Flutter for my personal projects (App size ~15MB for the same feature set). For some Apps it might be ok..NET may also still be incompatible &#x2F; unfinished on other OS and platforms (e.g. RISC hardware or BSD operating systems). So if you only target the mainstream systems like x64 and the raspberry pi, it might not be a real problem.[1]: https:&#x2F;&#x2F;github.com&#x2F;sandreas&#x2F;tone reply poulpy123 12 hours agorootparentI would target the main desktop, and maybe raspberry and android but it&#x27;s very far from my horizon and not a criteria.I was thinking that flutter is a dart library&#x2F;framework ? How is it compatible with C# ? reply sandreas 9 hours agorootparentFlutter is written in Dart, nothing to do with C#, but a cross platform UI library.I think Flutter has a lot of advantages over the C# UI libraries and is worth learning when planning a Cross Platform UI or App.It&#x27;s possible with C#, but you have to work around a lot of shenannigans depending on the use case.Command line apps are totally ok with C#... reply poulpy123 9 hours agorootparentThanks ! reply neonsunset 16 hours agoparentprevIt would. Compiling .NET binaries to a single executable that does not require installing a runtime has been a thing for quite some time actually, but NativeAOT definitely improves the \"simple and lightweight\" part of your inquiry:JIT: dotnet publish -p:PublishSingleFile=true -p:PublishTrimmed=trueAOT: dotnet publish -p:PublishAot=true reply poulpy123 12 hours agorootparentLooks interesting! Does nativeAOT works for GUI ? reply neonsunset 11 hours agorootparentIt&#x27;s a bit of a hit and miss as of today. CLI, back-end and natively compiled libraries (think dll&#x2F;so&#x2F;dylib or even .lib&#x2F;.a - you can statically link NAOT binaries into other \"unmanaged\" code) work best, GUI - requires more work.Avalonia[0] and MAUI[1] have known working templates with it, but YMMV.[0] https:&#x2F;&#x2F;github.com&#x2F;lixinyang123&#x2F;AvaloniaAOT &#x2F; https:&#x2F;&#x2F;github.com&#x2F;AvaloniaUI&#x2F;Avalonia&#x2F;[1] https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;maui (try out with just true in csproj - it is known to work e.g. on iOS) reply poulpy123 9 hours agorootparentThanks ! reply imiric 16 hours agoparentprevGo seems like a good fit for that. reply kaba0 15 hours agorootparentGo is insanely verbose compared to both C# and Java due to its low expressivity, especially around its error “handling”. reply imiric 14 hours agorootparentI think you&#x27;re confusing Go&#x27;s simplicity and explicitness with verbosity. Yes, it won&#x27;t help you write the least amount of code, but this forces you to write readable code and avoid cleverness, which is almost always a good thing. Its error handling is an exhausted topic, but avoiding exceptions and being forced to decide how to handle errors where they happen is also a good thing IMO.On the other hand, I would absolutely use verbose to describe both C# and Java. They are huge languages, with many features, a complex syntax, and conventions and patterns that lead to writing a lot of hard to understand code.I&#x27;d use Go over both of them any day, especially when it comes to writing cross-platform GUI or console apps that can be quickly compiled and easily distributed.Other options to consider might be Nim or Zig, but these are not as mature as Go. reply kaba0 3 hours agorootparent> I think you&#x27;re confusing Go&#x27;s simplicity and explicitness with verbosityI think those are quite innately related, yes. But I believe you also mix verbosity up in your very next sentence, for what it’s worth — readable, maintainable code is not as close a concept. In fact, certain type of verbosity helps readability, e.g. I argue that Go’s identifier capitalization harms readability, and marking it at a single place by public&#x2F;private is much better, at a tiny price of more verbosity.“Clever” code is another topic, there is definitely overly clever code, which is a net negative, but in this particular comparison, is a hand-written, 3-times nested for loop with any number of side effecting statements more clear to read than the “smart” stream&#x2F;linq expression? I would argue the latter is much more readable and maintainable. For another, even better example there is that math proof, which can be done an order of magnitude faster by choosing a clever notation, but I can’t look up its name yet.Re error handling, no, go doesn’t force you to handle the errors. It forces you to do some stupid meaningless if err ritual, that won’t result in correct handling of the error case, and will in fact often swallow errors. Java’s checked exception does force you to correctly handle it, though, but even runtime exceptions at least bubble up, making the default action (doing nothing) correct. reply eddtries 15 hours agorootparentprevYup, Go is absolutely the language for this! And almost every scripting thing you need to do is probably in the standard library, which is helpful. reply poulpy123 12 hours agorootparentprevIt seems to be the more mature language for my use case. How is the GUI development and C++ interop ? reply neonsunset 12 hours agorootparentAbysmal. Almost no one does GUI in Go and .NET has had zero&#x2F;low-cost interop as one of its main features in mind since the very first versions of .NET Framework back in the day. reply poulpy123 9 hours agorootparentThanks ! reply60 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      ".NET 8 has been announced as the latest Long-Term Support (LTS) version of the development platform.",
      "It brings performance, stability, and security improvements, as well as platform and tooling enhancements.",
      ".NET 8 enables the development of intelligent, cloud-native applications and high-traffic services that can scale on demand."
    ],
    "commentSummary": [
      "The discussions are centered around different aspects of the .NET ecosystem, including positive experiences, areas for growth, and the development of cross-platform apps.",
      "Participants suggest alternative frameworks like Xamarin and Livecode for cross-platform UI development and discuss the challenges in this area.",
      "Some participants express a preference for Go due to its simplicity and cross-platform capabilities, but note limitations in GUI development and C++ interop."
    ],
    "points": 296,
    "commentCount": 321,
    "retryCount": 0,
    "time": 1699977631
  },
  {
    "id": 38263435,
    "title": "Backblaze Reports Decrease in Drive Failure Rates in Q3 2023",
    "originLink": "https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/",
    "originBody": "Backblaze Drive Stats for Q3 2023 November 14, 2023 by Andy Klein // 2 Comments At the end of Q3 2023, Backblaze was monitoring 263,992 hard disk drives (HDDs) and solid state drives (SSDs) in our data centers around the world. Of that number, 4,459 are boot drives, with 3,242 being SSDs and 1,217 being HDDs. The failure rates for the SSDs are analyzed in the SSD Edition: 2023 Drive Stats review. That leaves us with 259,533 HDDs that we’ll focus on in this report. We’ll review the quarterly and lifetime failure rates of the data drives as of the end of Q3 2023. Along the way, we’ll share our observations and insights on the data presented, and, for the first time ever, we’ll reveal the drive failure rates broken down by data center. Q3 2023 Hard Drive Failure Rates At the end of Q3 2023, we were managing 259,533 hard drives used to store data. For our review, we removed 449 drives from consideration as they were used for testing purposes, or were drive models which did not have at least 60 drives. This leaves us with 259,084 hard drives grouped into 32 different models. The table below reviews the annualized failure rate (AFR) for those drive models for the Q3 2023 time period. Notes and Observations on the Q3 2023 Drive Stats The 22TB drives are here: At the bottom of the list you’ll see the WDC 22TB drives (model: WUH722222ALE6L4). A Backblaze Vault of 1,200 drives (plus four) is now operational. The 1,200 drives were installed on September 29, so they only have one day of service each in this report, but zero failures so far. The old get bolder: At the other end of the time-in-service spectrum are the 6TB Seagate drives (model: ST6000DX000) with an average of 101 months in operation. This cohort had zero failures in Q3 2023 with 883 drives and a lifetime AFR of 0.88%. Zero failures: In Q3, six different drive models managed to have zero drive failures during the quarter. But only the 6TB Seagate, noted above, had over 50,000 drive days, our minimum standard for ensuring we have enough data to make the AFR plausible. One failure: There were four drive models with one failure during Q3. After applying the 50,000 drive day metric, two drives stood out: WDC 16TB (model: WUH721816ALE6L0) with a 0.15% AFR. Toshiba 14TB (model: MG07ACA14TEY) with a 0.63% AFR. The Quarterly AFR Drops In Q3 2023, quarterly AFR for all drives was 1.47%. That was down from 2.2% in Q2 and also down from 1.65% a year ago. The quarterly AFR is based on just the data in that quarter, so it can often fluctuate from quarter to quarter. In our Q2 2023 report, we suspected the 2.2% for the quarter was due to the overall aging of the drive fleet and in particular we pointed a finger at specific 8TB, 10TB, and 12TB drive models as potential culprits driving the increase. That prediction fell flat in Q3 as nearly two-thirds of drive models experienced a decreased AFR quarter over quarter from Q2 and any increases were minimal. This included our suspect 8TB, 10TB, and 12TB drive models. It seems Q2 was an anomaly, but there was one big difference in Q3: we retired 4,585 aging 4TB drives. The average age of the retired drives was just over eight years, and while that was a good start, there’s another 28,963 4TB drives to go. To facilitate the continuous retirement of aging drives and make the data migration process easy and safe we use CVT, our awesome in-house data migration software which we’ll cover at another time. A Hot Summer and the Drive Stats Data As anyone should in our business, Backblaze continuously monitors our systems and drives. So, it was of little surprise to us when the folks at NASA confirmed the summer of 2023 as Earth’s hottest on record. The effects of this record-breaking summer showed up in our monitoring systems in the form of drive temperature alerts. A given drive in a storage server can heat up for many reasons: it is failing; a fan in the storage server has failed; other components are producing additional heat; the air flow is somehow restricted; and so on. Add in the fact that the ambient temperature within a data center often increases during the summer months, and you can get more temperature alerts. In reviewing the temperature data for our drives in Q3, we noticed that a small number of drives exceeded the maximum manufacturer’s temperature for at least one day. The maximum temperature for most drives is 60°C, except for the 12TB, 14TB, and 16TB Toshiba drives which have a maximum temperature of 55°C. Of the 259,533 data drives in operation in Q3, there were 354 individual drives (0.0013%) that exceeded their maximum manufacturer temperature. Of those only two drives failed, leaving 352 drives which were still operational as of the end of Q3. While temperature fluctuation is part of running data centers and temp alerts like these aren’t unheard of, our data center teams are looking into the root causes to ensure we’re prepared for the inevitability of increasingly hot summers to come. Will the Temperature Alerts Affect Drive Stats? The two drives which exceeded their maximum temperature and failed in Q3 have been removed from the Q3 AFR calculations. Both drives were 4TB Seagate drives (model: ST4000DM000). Given that the remaining 352 drives which exceeded their temperature maximum did not fail in Q3, we have left them in the Drive Stats calculations for Q3 as they did not increase the computed failure rates. Beginning in Q4, we will remove the 352 drives from the regular Drive Stats AFR calculations and create a separate cohort of drives to track that we’ll name Hot Drives. This will allow us to track the drives which exceeded their maximum temperature and compare their failure rates to those drives which operated within the manufacturer’s specifications. While there are a limited number of drives in the Hot Drives cohort, it could give us some insight into whether drives being exposed to high temperatures could cause a drive to fail more often. This heightened level of monitoring will identify any increase in drive failures so that they can be detected and dealt with expeditiously. New Drive Stats Data Fields in Q3 In Q2 2023, we introduced three new data fields that we started populating in the Drive Stats data we publish: vault_id, pod_id, and is_legacy_format. In Q3, we are adding three more fields into each drive records as follows: datacenter: The Backblaze data center where the drive is installed, currently one of these values: ams5, iad1, phx1, sac0, and sac2. cluster_id: The name of a given collection of storage servers logically grouped together to optimize system performance. Note: At this time the cluster_id is not always correct, we are working on fixing that. pod_slot_num: The physical location of a drive within a storage server. The specific slot differs based on the storage server type and capacity: Backblaze (45 drives), Backblaze (60 drives), Dell (26 drives), or Supermicro (60 drives). We’ll dig into these differences in another post. With these additions, the new schema beginning in Q3 2023 is: date serial_number model capacity_bytes failure datacenter (Q3) cluster_id (Q3) vault_id (Q2) pod_id (Q2) pod_slot_num (Q3) is_legacy_format (Q2) smart_1_normalized smart_1_raw The remaining SMART value pairs (as reported by each drive model) Beginning in Q3, these data data fields have been added to the publicly available Drive Stats files that we publish each quarter. Failure Rates by Data Center Now that we have the data center for each drive we can compute the AFRs for the drives in each data center. Below you’ll find the AFR for each of five data centers for Q3 2023. Notes and Observations Null?: The drives which reported a null or blank value for their data center are grouped in four Backblaze vaults. David, the Senior Infrastructure Software Engineer for Drive Stats, described the process of how we gather all the parts of the Drive Stats data each day. The TL:DR is that vaults can be too busy to respond at the moment we ask, and since the data center field is nice-to-have data, we get a blank field. We can go back a day or two to find the data center value, which we will do in the future when we report this data. sac0?: sac0 has the highest AFR of all of the data centers, but it also has the oldest drives—nearly twice as old, on average, versus the next closest in data center, sac2. As discussed previously, drive failures do seem to follow the “bathtub curve”, although recently we’ve seen the curve start out flatter. Regardless, as drive models age, they do generally fail more often. Another factor could be that sac0, and to a lesser extent sac2, has some of the oldest Storage Pods, including a handful of 45-drive units. We are in the process of using CVT to replace these older servers while migrating from 4TB to 16TB and larger drives. iad1: The iad data center is the foundation of our eastern region and has been growing rapidly since coming online about a year ago. The growth is a combination of new data and customers using our cloud replication capability to automatically make a copy of their data in another region. Q3 Data: This chart is for Q3 data only and includes all the data drives, including those with less than 60 drives per model. As we track this data over the coming quarters, we hope to get some insight into whether different data centers really have different drive failure rates, and, if so, why. Lifetime Hard Drive Failure Rates As of September 30, 2023, we were tracking 259,084 hard drives used to store customer data. For our lifetime analysis, we collect the number of drive days and the number of drive failures for each drive beginning from the time a drive was placed into production in one of our data centers. We group these drives by model, then sum up the drive days and failures for each model over their lifetime. That chart is below. One of the most important columns on this chart is the confidence interval, which is the difference between the low and high AFR confidence levels calculated at 95%. The lower the value, the more certain we are of the AFR stated. We like a confidence interval to be 0.5% or less. When the confidence interval is higher, that is not necessarily bad, it just means we either need more data or the data is somewhat inconsistent. The table below contains just those drive models which have a confidence interval of less than 0.5%. We have sorted the list by drive size and then by AFR. The 4TB, 6TB, 8TB, and some of the 12TB drive models are no longer in production. The HGST 12TB models in particular can still be found, but they have been relabeled as Western Digital and given alternate model numbers. Whether they have materially changed internally is not known, at least to us. One final note about the lifetime AFR data: you might have noticed the AFR for all of the drives hasn’t changed much from quarter to quarter. It has vacillated between 1.39% to 1.45% percent for the last two years. Basically, we have lots of drives with lots of time-in-service so it is hard to move the needle up or down. While the lifetime stats for individual drive models can be very useful, the lifetime AFR for all drives will probably get less and less interesting as we add more and more drives. Of course, a few hundred thousand drives that never fail could arrive, so we will continue to calculate and present the lifetime AFR. The Hard Drive Stats Data The complete data set used to create the information used in this review is available on our Hard Drive Stats Data webpage. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data to anyone; it is free. Good luck and let us know if you find anything interesting. print Category: Cloud Storage, Featured, Featured-Cloud Storage, Hard Drive StatsTag: B2Cloud",
    "commentLink": "https://news.ycombinator.com/item?id=38263435",
    "commentBody": "Backblaze Drive Stats for Q3 2023Hacker NewspastloginBackblaze Drive Stats for Q3 2023 (backblaze.com) 263 points by caution 19 hours ago| hidepastfavorite67 comments nicolaslem 18 hours agoThe article discusses the impact of high absolute temperature on the longevity of drives, however from my amateur knowledge the range of temperature during a day is also an important factor.I always assumed that having a stable 40°C is better than a drive constantly swinging between 20°C and 40°C, so I am surprised that the article only mentions alerts on reaching a high threshold. reply andy4blaze 18 hours agoparentAndy Klein from Backblaze here. Your point is a good one in that temperature fluctuation can be an important factor. We actually sample smart stats, which contain the temperature attribute, multiple times a day looking for such changes. The Drives Stats data is captured once a day, so it looks static, but behind the scenes the monitoring is more dynamic. reply rft 17 hours agorootparentDo you have plans to look at whether higher fluctuations translate into higher failure rates? Not sure whether you have historical data on this, but I would be really interested in this aspect, even if you can only run the stats on a smaller number of drives or shorter time periods.Maybe dividing the drives roughly in \"higher than average variability\" and \"low variability\" and then looking at the AFR for this subset can show some relation. Of course as the AFR for many drives is already quite low, the effect might be too small to distinguish from noise.On the topic of temperatures: Have you run an analysis whether a drive increasing in temperature (or maybe even decreasing) compared to its base line and \"neighbors\" results in a higher chance of failure? reply defrost 4 hours agorootparentprevCount me in as a second to the question from rft.I&#x27;d be interested if you have data to compare similar drives with stable tempretures against diurnal tempreture cycling.I&#x27;d imagine you have fairly constant data centre type environments though which would confound analysis for such questions. reply icelancer 15 hours agoprevThe best blog series going. Great technical writeups that I wish more companies would do - we&#x27;ve been doing it at our small business and customers really get a lot out of it. Also helps with marketing. reply 1-6 17 hours agoprevMy simple rule still stands even after 20 years: Avoid Seagate. reply gosub100 15 hours agoparentThey had a couple bad runs a few years back. If you keep following your simple rule, you&#x27;ll eventually get a bad WD and not be able to use spinning platters at all. A better simple rule would be to never skimp on buying out-of-warranty drives, as well as having a proper backup regimen. I&#x27;ve had bad seagates back in ~2008 but my current nas has 5x 16T exos and they work fine. reply AnonC 15 hours agoparentprevMy simple rule in the same period: avoid WD at all costs, prefer HGST (which later became part of WD) and use Seagate mostly. I’ve seen variations of these personal rules, and during a time when Backblaze didn’t exist yet (and came up with better measurements on a larger scale), it was like one of those holy wars between tabs and spaces (or vi and emacs). reply joecool1029 10 hours agorootparent> prefer HGST (which later became part of WD)Toshiba got the 3.5\" division. WD got the 2.5\" division, anti-trust divesture (just 3 companies left): https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;8&#x2F;87&#x2F;Diagram_... reply kstrauser 16 hours agoparentprevConversely, I’ve had nothing but bad luck with WD the last few years, and my Seagates have been flawless.My simple rule is that all drives suck, and always have good backups. reply Netcob 15 hours agorootparentMy WD drives failed pretty consistently, so I&#x27;m now giving Seagate a try.Well, my main reason was that WD decided that just failing \"naturally\" after a few years wasn&#x27;t enough, but that a drive having been on for 3 years should be considered the same as \"failing\" (communicated through WDDA), which led to Synology adopting that for a while. Not sure what the current state is of that, but I intend to swap drives when they fail, not when they turn 3. reply xurbax 3 hours agorootparentWell, for me, WD Reds seem to start to go bad at about 3 years, so... reply pcurve 16 hours agorootparentprevYup. All drives kick the bucket at some point. That&#x27;s why I use BB. I don&#x27;t trust myself with NAS.The ones that never failed me were any drives made by Quantum using SCSI interface. 5 drives, zero failure over 10+ years. But those were slower and cooler running units. reply kstrauser 16 hours agorootparentI’ve got a NAS backed up to Backblaze. It’s a nice setup. I can quickly recover from local data loss, or replace a RAID disk when needed, but if the NAS gets hit by a bus then I still haven’t lost everything. reply Mistletoe 17 hours agoparentprevI always found it hilarious when they were sponsoring the datahoarder subreddit. I’d like to meet that marketer and shake his or her hand. reply 1-6 17 hours agorootparentI&#x27;m surprised that Seagate has consistently kept its &#x27;slightly less reliable&#x27; crown after all these years.It&#x27;s like the student who copies the &#x27;A&#x27; student&#x27;s exam. They&#x27;ll purposefully get a couple of the answers wrong to avoid suspicion. reply Dalewyn 15 hours agoparentprevI used to swear by WD HDDs, but when I literally couldn&#x27;t tell which of their NAS drives were CMR and SMR a few years ago I wrote them off and went to Seagate who clearly labeled their drives.Combine that with their lackluster reputation in solid state as of late and I probably won&#x27;t buy their HDDs until Seagate one day gives me the \"WTF are you even selling?\" rigmarole too. reply ksec 16 hours agoparentprevNot quite as long as 20 years, but for the past 15 years: Just buy HGST. reply AnonC 15 hours agorootparentHGST was bought by WD long ago. Has that had any impact on the types of drives sold or the quality? reply ksec 15 hours agorootparent>has that had any impact on the types of drives sold or the quality?No. To the point when WD tries to rebrand those drives away from HGST, the market demanded HGST and they brought it back a year later. reply HankB99 13 hours agorootparentAt what point do they rebrand crappy WD drives as HGST?They already have a reputation of obfuscating information (SMR vs. CMR) replycalamari4065 10 hours agoprevClose to 10 years ago, I bought a \"new\" hard drive on Amazon. When I got it, I happened to notice some suspicious signs of wear, like corrosion on the PCB from someone&#x27;s greasy fingerprint. I dive into the SMART stats and it&#x27;s completely zeroed. Not quite what I&#x27;d expect from a new drive.So then I start scanning the drive. The partition table was deleted, but it was full of data, most of it encrypted. It was also badly fragmented, so it was likely in some type of array. What I could recover in cleartext implied that it had been spun up for thousands of hours, in a Backblaze datacenter. It wasn&#x27;t conclusive enough to go to Backblaze about it, so I just returned it to Amazon. I probably did a zero pass on it first, can&#x27;t remember.Encrypted or no, if that was a backblaze drive, they were disposing of drives with customer data still on them. I&#x27;m not surprised someone tried to pass it off as new on Amazon, that scam is old hat. I was very shocked to see the data still intact though. reply gruez 10 hours agoparent>it was full of data, most of it encrypted. [...] Encrypted or no, if that was a backblaze drive, they were disposing of drives with customer data still on themWho cares if there was \"customer data still on them\" if it was encrypted? One of the nice things about encryption is that you don&#x27;t have to worry about wiping drives. reply jbverschoor 9 hours agorootparentSure.. until it gets decrypted reply gruez 7 hours agorootparentIs that a realistic threat with AES-256 and randomly generated keys? reply gymbeaux 4 hours agorootparentThe local electronics recycling company where I live (US-FL) shreds hard drives by default and many of their enterprise clients apparently ask for it when they “donate” their old PowerEdge servers, NASes and whatnot. Now obviously the recycling company could try to discourage this in lieu of a 3 (or 7 or 100 or whatever) zero of the drives and then resell them as they do everything else they get “donated” to them… many are really expensive, high-capacity SAS drives that are only a few years old. But I guess nobody wants to be that guy who compromises company data or whatever just so the local recycling company can make money off their old drives in addition to their old servers, UPSes, racks, et al.Of course if these companies were really smart, they’d have wiped the drives before going to the recycling company. I’m sure many do. Still, they don’t risk it and want the drives shredded.Eventually, AES-256 can probably be bruteforced in a reasonable amount of time. If you write all 1s and then all 0s (or vice-versa) to the drive, on the other hand… there’s no way to recover the data. There’s a lot of debate about that statement, but ultimately, if the drive is in fact zeroed twice, it’s physically impossible to recover the data. The debate seems to be mostly around whether zeroing a drive really does zero every bit and that’s not straightforward to prove (many drive erasure programs will offer a printable “certificate” once a drive has been “secure-wiped”, which often mentions a “million dollar guarantee” or whatever… it’s a sham because how do you prove the program failed to erase the data on the drive? Especially days, weeks, or years later?). reply patrakov 32 minutes agorootparent> Eventually, AES-256 can probably be brute-forced in a reasonable amount of time.No. See https:&#x2F;&#x2F;security.stackexchange.com&#x2F;questions&#x2F;6141&#x2F;amount-of-...Time is not the bottleneck, energy is.They invoke Landauer&#x27;s principle which states that irreversible computation has an intrinsic cost in terms of energy per elementary operation, namely, k T ln(2) where k is the Boltzmann constant. Assuming brute-force search, more than 2^256 elementary operations would be needed, but that would require more energy than available if one converts the whole Sun&#x27;s mass into energy. reply Dalewyn 6 hours agorootparentprevThere&#x27;s a reason that the industry standard for proper disposal of storage media, including HDDs, is nothing short of physical destruction.It doesn&#x27;t matter if the data is encrypted or not, the point of the matter is the data is still there when presumably that data should not exist outsidepremises. Encryption serves as a mitigation against theft or accidental leakage of data, its purpose is not to facilitate data disposal.Put another way, you have to answer Yes to this question for liability purposes: \"Is the data gone?\" The only way to say Yes with reasonable certainty is physically destroying the storage medium the data resides on. reply gymbeaux 4 hours agorootparentWell, if you write 1 to every bit of the drive, and then write 0 to every bit of the drive, the data is gone… but to be fair, I think the concern is proving that actually occurred before disposal. It’s easy to see the data destroyed when the drive is ground up before your eyes. replyhiatus 19 hours agoprevSurprised by the number of drives with 0 failures, though it seems not all of the drives were run for the required time to qualify for the rating.> In Q3, six different drive models managed to have zero drive failures during the quarter. But only the 6TB Seagate, noted above, had over 50,000 drive days, our minimum standard for ensuring we have enough data to make the AFR plausible. reply kqr 18 hours agoparentI have always wondered why they don&#x27;t use techniques from survival analysis to be able to draw conclusions even from sets with lower failure rates. Or for that matter to avoid slight bias even for drives they do report. reply andy4blaze 16 hours agorootparentAndy Klein from Backblaze here. We have done some survival analysis (kaplan-meier curves). In our case, we need to have a reasonable number of failures over the observation period to get decent results. You can take a look at some of our work here: https:&#x2F;&#x2F;www.backblaze.com&#x2F;blog&#x2F;hard-drive-life-expectancy&#x2F; and see if that is what you were expecting. reply kqr 4 hours agorootparentYes! Thank you. I&#x27;ve been thinking about scratching this itch myself but it turns out you did already.In particular the shape of the survival curve interests me -- you hear so many things about exponential here, bathtub there, but ver little data. I will read once I have a spare moment. reply Lorin 17 hours agoprevI wonder if they have special hardware recycling arrangements with their vendors for decommissioned drives to reduce their footprint. How many magnets are laying around the office? :) reply bimguy 8 hours agoprevCool article! Looks like HGST (formally Hitachi) perform very well overall. Toshiba 4GB ran 101 months with no failures, holy moly!I used to like Samsung hard drives personally, Lacie used them in their rugged series and I found them to have pretty low failure rates. Seagate bought out Samsung&#x27;s disk drive business in 2011 apparently. I guess Samsung saw the future of SSD&#x27;s? reply dehrmann 17 hours agoprev> The average age of the retired drives was just over eight yearsDidn&#x27;t realize they can last so long. reply gymbeaux 4 hours agoparentI have a 3TB Seagate from around 2009 that still works. It was shucked from an external USB enclosure and has moved desktops several times. Granted its use has been pretty intermittent. Right now it’s just sitting on my desk, as it often has.I have some ~140GB SAS drives HP branded that are probably of similar age based on the capacity, and they still work… but again, they haven’t exactly been active for the last 15 years straight. reply margalabargala 17 hours agoparentprevThey can last much longer. I have operational IDE drives from the early aughts (not with anything important on them) reply nucleardog 9 hours agoparentprevI have a WD Black 1TB drive that did service as my main desktop drive for about 6 or 7 years.After that it went into my server at home. It was used for various things. At this point it’s spent the last 5 years as the disk my DVR records to. (Because 5 years ago I was expecting it to die any day and didn’t want anything more important on it…) So it’s being continuously written and rewritten 24 hours a day, 7 days a week.It’s now about 17 years old and has spent almost the entirety of that time powered on. It’s been packed up and sent on two cross country moves. Still kicking.I have a number of WD Green 1.5TB drives that are nearly as old and still in daily use in the same server.Maybe I’ve just had great luck, but I’d be more surprised by a drive dying sooner than eight years. reply thebiss 16 hours agoparentprevUnder my desk right now is a too-underutilized-to-upgrade-NAS that has been spinning 1TB Western Digitals since 2010. Between RAID-Z2 and cloud backups, there&#x27;s almost no reason to get rid of them except for performance, which doesn&#x27;t matter here. reply toast0 15 hours agorootparentIf all of your disks are about the same age, from the same vendor, it might be reasonable to replace them over time to mitigate the risk that a firmware or manufacturing issue results in them all failing around the same time. Many vendors have had firmware errors where counters rolled over and the drive becomes inaccessible (generally much sooner than 13 years though). reply pathartl 11 hours agoparentprevI just retired some drives out of my home array. 3TB WD Reds with 10.5 years of power on time, no logged errors. Ran them through a full block check and had no errors. reply BOOSTERHIDROGEN 12 hours agoprev> this chart is the confidence interval, which is the difference between the low and high AFR confidence levels calculated at 95%.How to calculate low and high AFR ? reply vouaobrasil 14 hours agoprevBackblaze is interesting but it&#x27;s not very easy to use. Its interface is rather basic and it was difficult to select which drives to back up and which not to. It kept trying to back up directories on my computer that I specifically told it not to, and there was no way to efficiently update program&#x27;s behaviouir from the the \"what to backup list\". It might be nice if you just want to backup your computer and all your drives but the moment you want only parts of your computer backed up, it&#x27;s frustrating. reply cvccvroomvroom 14 hours agoprevI run a bunch of WUH721414ALE6L4 and WUH721414ALE604 in numerous RAID10 volumes. Haven&#x27;t had a failure yet.L = without power disable, 0 = with reply hardware2win 18 hours agoprevKinda low sample sizes for 0 and highest AFRs reply Titan2189 12 hours agoprev> ambient temperature within a data center often increases during the summer monthsWhat? Isn&#x27;t your Data Center supposed to be temperature controlled, where the A&#x2F;C has a setpoint which it&#x27;s keeping the entire environment to within a degree?Being able to tell what season it is based on your HDD SMART temperature (armchair expert here) sounds bad. reply brnt 12 hours agoparentIs moving the setpoint with the seasons, within tolerance of course, not a common energy savings method? reply ska 12 hours agorootparentThat makes more sense when you are housing people than servers, no? reply brnt 3 hours agorootparentDoes it? Everybody wants to save energy. If the hardware can handle it, why not? reply dist-epoch 18 hours agoprev [–] Is there any backup software which continuously uploads your files to an AWS&#x2F;GCP&#x2F;Azure long term storage account that you control and pay for? Something like CrashPlan, which from time to time performs automatic maintenance and deletes old versions of files. reply eli 18 hours agoparentTons of choices: Restic, Borg, Duplicity, KopiaThough Backblaze is pretty good at what it does and you can set your own encryption key, if that&#x27;s the concern. reply aeries 17 hours agorootparentUnfortunately Backblaze requires[1] you to provide them this private key to restore.[1] https:&#x2F;&#x2F;help.backblaze.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;360038171794-Wh... reply eli 17 hours agorootparentYeah I mean you gotta trust them on some level. Backblaze could also push a client update that nerfs the encryption. If you&#x27;ve got really sensitive data I&#x27;d probably pick something else. reply ecwilson 17 hours agorootparentprevWhat does restore mean this context? I’ve downloaded files from their online portal without providing a key. Perhaps restoring in this context means having them mail you a hard drive. reply eli 17 hours agorootparentReally? I thought the file metadata is encrypted so it needs the key to even identify what files are available to restore. reply davrosthedalek 13 hours agorootparentprevYou can always encrypt yourself and rclone to b2. Likely cheaper if you are not a data hoarder. reply notRobot 17 hours agoparentprevPerhaps this can help https:&#x2F;&#x2F;rclone.org&#x2F; reply danbtl 18 hours agoparentprevI&#x27;m using rclone to sync with Backblaze nightly, executed directly from a cronjob. reply freedomben 17 hours agorootparentSame. Rclone is wonderful because it supports a ton of different backends which makes it super easy to mirror. It&#x27;s also got some great features like crypt where you can encrypt everything locally, thus sending the data all as ciphertext. reply kasperset 18 hours agoparentprevArq backup. I have used it before in the past(2-3 years ago) but not using it currently. reply southernplaces7 14 hours agorootparentMy experience with Arq has been terrible so far. The interface keeps glitching up, for a while it failed repeatedly until I gave it a unique permission in Windows services, and the backup process is far from intuitive, with multiple backups showing up as restore options, but each with different file sizes and specific files saved. Overall, just too many ambiguities for it to be reliable.Oddly, SpiderOak has been a background go-to for years and always worked smoothly for backing up everything I select in a wholesale way, keeping a fairly clear record of what was saved, removed or moved, and adjusting immediately to any file changes or deletes I do. The SO interface is shitty and often freezes, and lacks many basic features like being able to see file sizes or scrolling through long file lists easily, but at least overall, I can quickly and easily see when backups are happening, how they&#x27;re being done and what&#x27;s being saved. Also, for restoring files, it&#x27;s surprisingly fast despite a reputation held by many that it&#x27;s slow. reply aeries 18 hours agorootparentprevArq is the best set-and-forget option I know of for macOS. reply briffle 18 hours agoparentprevBackblaze software is pretty reasonable. But I have a linux machine, so restic and their B2 storage is a buck or two a month to backup a few computers in my house. (around 185GB of photos, etc) reply encom 16 hours agorootparent>Backblaze software is pretty reasonable.Hard disagree. The jankiness of their software was what made me cancel my sub after several years. Firstly it doesn&#x27;t follow the OS date and number formatting. It&#x27;s a minor thing, but it&#x27;s so annoying having to parse the dumb M&#x2F;D&#x2F;Y format and comma thousands separator etc (being da-DK). It&#x27;s not a deal breaker, but on the other hand, it&#x27;s such a low-hanging thing that most other software gets right immediately.But far more importantly, the BB client would some times just decide to re-upload several hundred gigabytes of data that I know for sure didn&#x27;t change, which makes me wonder if it&#x27;s just the client being retarded or if the data got lost server-side. And it takes absolutely forever to detect USB harddrives being plugged in. And its log files will grow to absurd sizes, and you&#x27;re not allowed to purge them or the client will become brain damaged. And one time I needed to do a restore, it took literal days for BB to prepare it and I had to get support involved. I feel I just can&#x27;t trust the BB stack, the client being the weakest link by far, and backup I can&#x27;t trust is worthless. reply briffle 15 hours agorootparentYou are probably correct. But there is a reason I put Backblaze client on my mom&#x27;s laptop, rather than a scheduled powershell task to run restic to a cloud storage. And a scheduled task to run the prune weekly. reply thebiss 16 hours agoparentprev [–] Define \"continuously\". Does the data need to be mirrored immediately upon write? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Backblaze monitored over 263,992 hard disk drives (HDDs) and solid state drives (SSDs) in their data centers in Q3 2023.",
      "The quarterly failure rate for all drives decreased from 2.2% to 1.47% compared to the previous quarter.",
      "Backblaze retired 4,585 aging 4TB drives and will closely monitor a small number of drives that exceeded their maximum temperature to compare their failure rates with drives operating within manufacturer specifications. The overall lifetime failure rate for all drives remained consistent at around 1.4%."
    ],
    "commentSummary": [
      "The article discusses the impact of temperature on the lifespan of hard drives and suggests comparing drives with stable temperatures to those with fluctuations.",
      "The importance of data disposal and the security of encryption are emphasized, noting the need to physically destroy hard drives.",
      "Comments from users cover topics like drive reliability, backup software, and user experiences, with positive experiences shared alongside concerns about firmware errors and the trustworthiness of certain backup solutions."
    ],
    "points": 263,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1699970839
  },
  {
    "id": 38272132,
    "title": "U.S. Reenters Nuclear Fuel Industry with Successful Production of Uranium Fuel",
    "originLink": "https://spectrum.ieee.org/nuclear-power-plant-2666199640",
    "originBody": "ENERGY NEWS U.S. Reenters the Nuclear Fuel Game Centrus Energy delivers first batch of uranium that’s critical for advanced reactors PRACHI PATEL13 NOV 20234 MIN READ Centrus Energy’s cascade of centrifuges produced its first batch of uranium fuel in November. CENTRUS ENERGY",
    "commentLink": "https://news.ycombinator.com/item?id=38272132",
    "commentBody": "U.S. reenters the nuclear fuel gameHacker NewspastloginU.S. reenters the nuclear fuel game (ieee.org) 260 points by beefman 8 hours ago| hidepastfavorite249 comments cameron_b 7 hours agoAn important detail to go with the statement that we rely on Russia for most of our nuclear fuel is that the market for nuclear fuel was disrupted for ten years by the Megatons to Megawatts program [0]Centrus, the company cited in the ieee article, was the US based side of that exchange, then under the name of the US Enrichment Corporation. [1]While the program was successful at nuclear disarmament ( to the tune of 20,008 warheads [wiki] ) it tanked the market by effectively &#x27;dumping&#x27; the fuel, driving centrifuge operators out.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Megatons_to_Megawatts_Program[1] https:&#x2F;&#x2F;www.centrusenergy.com&#x2F;who-we-are&#x2F;history&#x2F;megatons-to... reply irjustin 6 hours agoparentI argue: it goes back further than that - Chernobyl and 3-mile island[0]. Deeply ingrained in the collective psyche that nuclear - when it goes wrong goes _really_ wrong.That really set the tone for where US was willing to go and even if it&#x27;s overall a better solution people weren&#x27;t willing to back it while it was too easy to kill politically. Jack Welch&#x27;s book \"Straight from the Gut\" talks about how they shifted their entire business to nuclear servicing from development because they saw the changing winds - the correct bet.I&#x27;m super excited by this latest development as we accept that we need to learn how to tame the beast that is nuclear fission because the benefits really do outweigh the drawbacks.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Three_Mile_Island_accident reply jasonwatkinspdx 2 hours agorootparentI would be hesitant to use Jack Welch and correct bet in the same sentence.His overall strategy at GE was to shift it from it&#x27;s traditional roles in heavy industry to instead focus on finance and real estate. This worked temporarily as the company rode the real estate bubble. When the bubble pop&#x27;d the company lost half its value.In other words, don&#x27;t evaluate his statements on nuclear without context or critical thought. reply avar 1 hour agorootparentHe retired in 2001, and GE matched or outperformed the S&P 500 until 2017: https:&#x2F;&#x2F;seekingalpha.com&#x2F;article&#x2F;4141020-ge-true-value-stockThey had a relatively larger fall from grace in the recession than the rest of the index.In any case, even if he was a terrible executive what does that have to do with whether he was correct about general trends in the nuclear industry? reply jasonwatkinspdx 1 hour agorootparentJack himself said he should be judged by where the company was 10 years after he left. That comment did not age well.I&#x27;m saying he had a larger agenda at the time that comment was made, one that had him divesting not just from nuclear but many other of GE&#x27;s historic heavy industry and power related business lines. reply cinntaile 10 minutes agorootparentAfter he left the stock outperformed for 16 years, in my book that qualifies as aging well. What is your metric? reply robertlagrant 28 minutes agorootparentprev> Jack himself said he should be judged by where the company was 10 years after he left. That comment did not age well.How so? Don&#x27;t you agree with it? reply kalessin 6 hours agorootparentprevWhat really went wrong at 3 mile island? No one died or was injured afaik. reply lazide 6 hours agorootparentit was a lot closer call than should have been possible.And everyone got very scared, and the gov’t lost a lot of credibility.It was no Chernobyl, but it made it clear the US wasn’t perfect either. reply roenxi 34 minutes agorootparentThat is what happened, but I can&#x27;t pass by without highlighting how irrational the response was.I&#x27;ve been watching a livestream of Reykjanes in the vague hope of seeing some magma. Somewhere globally needs to be evacuated every year or so. Going from base rate to base rate +1 per 2 decades or thereabouts is a perfectly acceptable deal. Nobody is going to die (which is more than can be said about coal). We&#x27;d get cheap clean energy.And instead people decided that the best plan was to panic and we end up with the gently building energy crisis that has been rolling on for a few years now. It is entirely plausible we see the bloodiest war in human history as a result of the Western derailment of the transition to nuclear power. It was really foolish.Who cares that the US government are incompetent? Yeah they&#x27;re incompetent. They&#x27;ve been incompetent for a long while. They still snuffed out one of the most (arguably the most?) promising technologies of a generation out of fear and ignorance. reply nickpp 13 minutes agorootparentThat irrational response was heavily fueled by fear mongering from environmental activists and financed by oil producing hostile countries.In the 70s and 80s leftists couldn&#x27;t much keep pushing socialism (like they are now) since the horrors of their ideology was quite visible to everybody in the example of USSR. So they embraced environmentalism - just another way for them to fight capitalism and consumerism.Russia (through the KGB) was quite happy to finance their cause. It meant Western countries (and especially Western Europe) stayed dependent on them for their energy. The folly of our strategy became quite apparent during the last few years with the Ukraine invasion and revelations of EU politicians fully paid and owned by the Russians.Together with infiltrating the Western Academia, this was probably one of the most successful undercover secret service operation ever. reply metabagel 6 hours agorootparentprevIt was an uncomfortably close call. I believe it was a near meltdown. Could have gone a different way. reply kalessin 5 hours agorootparentIt was a partial meltdown in a pressurized water reactor, it cannot be compared to Chernobyl, it&#x27;s a completely different technology. PWRs can&#x27;t explode like what happened in Chernobyl. reply lazide 3 hours agorootparentSomething like 20 tons (!) of fuel rods, almost the entire load, melted. [https:&#x2F;&#x2F;commons.m.wikimedia.org&#x2F;wiki&#x2F;File:Graphic_TMI-2_Core...]It took years to find that out though.The containment vessel held, and most of the radiation released was in the form of xenon and krypton gas vented from the reactor.“ It was later found that about half the core had melted, and the cladding around 90% of the fuel rods had failed,[21][76] with 5 ft (1.5 m) of the core gone, and around 20 short tons (18 t) of uranium flowing to the bottom head of the pressure vessel, forming a mass of corium.[77] The reactor vessel—the second level of containment after the cladding—maintained integrity and contained the damaged fuel with nearly all of the radioactive isotopes in the core.”Definitely not Chernobyl, but it was a significant amount of damage to the reactor. It was totaled. reply Animats 1 hour agorootparentThere is a lot to be said for big, strong containment vessels. Fukishima&#x27;s was too small and overpressure broke it open. Chernobyl didn&#x27;t have one. Three Mile Island had a good one.Many of the \"small modular reactor\" schemes say they don&#x27;t need a big, strong, expensive containment vessel because, reasons. You can read those arguments for NuScale in NRC documents. The prototype was going to be built at the Idaho National Laboratory, formerly the National Reactor Testing Station, which is in outer nowhere, just in case. reply redprince 4 hours agorootparentprevPWR is a \"Pressurized Water Reactor\" using (light) water under pressure as the primary coolant. The Chernobyl block #4 RBMK-1000 was certainly a PWR.While the precise mechanism by which the #4 reactor in Chernobyl was destroyed in 1986 was rooted in the flawed design combined with unsafe operation, this does not mean that other reactor designs cannot fail catastrophically with loss of containment and release of radioactive material. Particularly when operated outside of their specification through operator error, accidents or a combination thereof. reply beefman 3 hours agorootparentRBMK is not considered a PWR because it is graphite-moderated. Most reactors are classified first by their moderator. PWRs and BWRs are both LWRs, moderated by light water, as opposed to HWRs moderated by heavy water, or graphite-moderated reactors like RBMK, or fast reactors which have no moderator at all.Any reactor can fail and any can be operated safely. The reactivity coefficients of RBMK made it harder to control, perhaps, than a PWR. Modifications made after the Chernobyl accident have improved this.The main issue with Chernobyl 4 was its lack of a containment building. Even so, the response was an over-reaction that made the situation worse. reply hef19898 1 hour agorootparentRMBKs are an unsafe design. Chernobyl No. 4 was build in an unsafe manner and operated in an unsafe manner.Not sure which response to the accident was an over-reaction in your opinion so. reply ChatGTP 3 hours agorootparentprevIsn’t that what physicists thought about the Chernobyl reactor ? Then it exploded ? reply NegativeK 3 hours agorootparentThe fact that it could explode was known and classified. reply hef19898 1 hour agorootparentTo be fair, the show over dramatized the KGB angle. But yes, the Soviets knew RBMKs were not the safest design, reactor 4 was built violating safety standards, operators for the test were not properly trained and then safety procedures were ignored during the test. The official incident report is a fascinating read, and should be mandatory reading for everyone studying with goal of having the word engineer in his future job title.Edit: That should be the one https:&#x2F;&#x2F;www-pub.iaea.org&#x2F;MTCD&#x2F;publications&#x2F;PDF&#x2F;Pub913e_web.p...You also have this one: https:&#x2F;&#x2F;www.nrc.gov&#x2F;docs&#x2F;ML0716&#x2F;ML071690245.pdfEdit 2:What with regards to the effect of graphite tipped control rods was, IMHO, as bad as having a dramatic KGB effort to keep it secret: it was forgotten. In 1983, there was an incident in an other RBMK reactor, the HBO series claims the KGB kept it secret, in reality this happened (from the INSAG-7 report and the cited USSR investigative reports):>> The SCSSINP Commission (Annex I, Section 1-3.8) reports that, after discov- ery of the positive scram effect at Ignalina in 1983, the chief engineering organiza- tion informed other organizations and all nuclear power plants with RBMK reactors that it intended to impose restrictions on the complete withdrawal of control and safety rods from the core. Such restrictions were never imposed and apparently the matter was forgotten. reply olivermuty 1 hour agorootparentprevIf the HBO show had the science somwhat correct, then the operators did everything they could to make it go boom. In failure modes already defined and warned about. reply hef19898 1 hour agorootparentprevThat is a pretty low bar to pass: success, our product failed but didn&#x27;t injure or kill anyone unintentionally, nothing to see, move on. reply avar 1 hour agorootparentIt&#x27;s a pretty high bar. Three mile island has a lifetime production of ~250 TWh, coal kills ~25 people per TWh.So if it had killed ~6000 people it would be within the accepted norms of the energy industry for anything that isn&#x27;t nuclear.https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;death-rates-from-energy-p... reply waveBidder 57 minutes agorootparentprevCompared with the constant deaths from coal, yeah, it&#x27;s a much better bet. Thankfully moot these days, but the overreaction to these problems set us back generations at solving climate change. reply Civitello 6 hours agorootparentprevI believe that a significant part of the reason the nuclear is so unpopular is due to the influence of the Simpsons. reply acyou 5 hours agorootparentHow do we separate cause from effect here? Anti nuclear sentiment is quite a bit older than Homer Simpson. reply lazide 6 hours agorootparentprevNuclear had PR issues long before the first episode of the simpsons. reply doubloon 4 hours agorootparentprevdon&#x27;t forget Silkwood. now i know a lot of people are not convinced by her story, but the fact remains that even without her story, the nuclear energy industry in Oklahoma was a complete shit show, they were spreading nuclear waste on farm fields for cows to eat, to save money, and one time they blew open a huge container of uranium hexafluoride, killed a guy, injured a bunch of people, and sprayed a cloud of material over several counties because they couldn&#x27;t follow simple procedures. reply albert180 4 hours agorootparentSo all this Simpsons Moments (Homer pushing a button where nuclear waste sprays on fields etc...) were actually references to Silkwood? reply wolverine876 2 hours agorootparentprev> the nuclear energy industry in Oklahoma was a complete shit showAny enterprise needs to be sufficiently safe as a complete shit show, because it will happen. reply Animats 1 hour agoprevSigh. No, the US hasn&#x27;t been out of the \"nuclear fuel game\". There are commercial separation plants, including URENCO in Nevada. That fuels existing reactors and new conventional designs such as the AP-1000. This press release is about \"high-assay, low-enriched uranium\", which isn&#x27;t used by many reactors yet.If any. NuScale was going to use this stuff. But that project was cancelled a few weeks ago. Costs too much. There are advanced reactors being talked about. Does anyone know of any that actually need this fuel right now? Maybe outside the US there are some.(NRC: \"Natural uranium is made of about 0.7 % U-235 and traditional reactor fuel is enriched to between 3% and 5% U-235.\" High-assay, low-enriched uranium is, per the NRC, 5% - 20% u-235. Weapons grade is around 90%.) reply resolutebat 5 hours agoprev> The company is on track to produce 20 kilograms of HALEU by the end of the year, and then expects to produce 900 kg in 2024> Centrus and the Russian state-owned company Tenex, which are the only two outfits that can produce HALEU in the world> For now, the DOE intends to purchase about 25 tonnes of HALEU per year to kick-start the industry and give HALEU producers secure contracts from which they can expand productionSo in 2024 the DoE will buy 0.9T from the only American producer, and the remaining 24.1T from Russia? reply quickthrower2 3 hours agoparentI assume it goes it again in 2025? reply demondemidi 6 hours agoprevWe have 93 nuclear reactors? I&#x27;m on Team Greta: go nukes. reply acidburnNSA 6 hours agoparentYes, and they make roughly 20% of our electricity, and about 50% of our low-carbon electricity.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nuclear_power_in_the_United_St... reply wateralien 4 hours agoparentprevI&#x27;m on team Generation 4 Nuclear. I think the distinction is important.Eg: https:&#x2F;&#x2F;www.energy.gov&#x2F;ne&#x2F;articles&#x2F;3-advanced-reactor-system... reply spullara 2 hours agorootparentI would build 100 3 mile islands instead of using fossil fuels. reply Cthulhu_ 21 minutes agorootparentHere&#x27;s the thing: you wouldn&#x27;t have to, there&#x27;s no need for compromise. We can build a safe nuclear energy infrastructure, go 100% carbon emission free, etc.But there&#x27;s still no solution for safely storing or disposing of nuclear waste, so all those power plants currently have a stockpile of spent fuel rods, which is a huge risk imo. But everyone&#x27;s like \"not in my backyard\", even though the backyard is a deep cave in a mountain or down in the earth where the stuff will be put in lead and reinforced concrete, the cave sealed off or collapsed, forgotten by time and where it will remain dormant and slowly go inert over the next geological era. reply wateralien 1 hour agorootparentprevMmmm, I think that would be a mistake. The former residents of Fukushima prefecture and Chernobyl would probably agree. reply unglaublich 20 minutes agorootparent- 20.000 died because of the Tsunami. - 2.000 died in the evacuation attempt. - ONE person died because of actual radiation.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fukushima_Daiichi_nuclear_disa... reply nickpp 35 minutes agorootparentprevBut the millions killed each year by pollution created burning gas, oil and coal would strongly disagree - if they were still alive. reply hanniabu 3 hours agorootparentprevGeneration doesn&#x27;t matter, the weakest link will always be human greef, changing the spec to save cost, changing safety tests to extend the lifetime past when it should be shut down, poor maintenance, etc reply spullara 2 hours agorootparentThose are all just money. We could spend more for energy. reply arunharidas 3 hours agoparentprevGreta is not pro-nukes. Most of the environment activists are not. reply mrweasel 1 hour agorootparentDanish TV had a documentary about the nuclear opposition in the country in the 60s and 70s. One of the most interesting lines is pretty much a throwaway line by one of the former activists. The interviewer remarks that it&#x27;s a little strange to be anti-nuclear, when the alternative is burning fossil fuel and the activist interjects with \"But we didn&#x27;t believe&#x2F;understand that the alternative was coal and oil, we thought it would be solar and wind\". That was never going to work, nuclear power was available as an option, wind turbines and solar cells where never going to be able to fulfill the energy requirement at the time, certainly not at a price the country could afford.To me that really highlighted the naivety of many environmental activists. They mean well and we need them, but they so often fail to look at the problems holistically and zooms in on single issues. reply davedx 1 hour agorootparentThat&#x27;s very fair. I think it extends to people who care about energy in general though, not just environmentalists. For example on the Internet, there is always a highly vocal pro-nuclear camp to be found. Some of them -- but not all by all means -- will often claim that nuclear is the \"One True Solution\" to clean energy.I think at the end of the day it comes down to tribalism, sadly. People choose their \"side\" and pitch in to defend its merits and attack the other \"side\"&#x27;s deficiencies. As with many things, there isn&#x27;t a whole lot of room left for holistic approaches.I&#x27;m personally not a huge fan of nuclear because I&#x27;m a pragmatist, and I think most people are pragmatists at the end of the day, being human. And pragmatists don&#x27;t make good operators of nuclear power plants. But I definitely don&#x27;t think any existing nuclear plants should be closed. They should be (safely) continued to run as long as possible to provide the clean energy we desperately need while other safer (and often cheaper) renewables+storages ramp up. reply mrweasel 44 minutes agorootparentKeeping existing plants running is the sensible approach in my mind. We can&#x27;t build new nuclear power to reduce CO2 emission, it&#x27;s much to late for that. We want drastic reductions in 5 - 6 years, the first nuclear power plant won&#x27;t be ready for another 10, 15, 20 years. If we wanted to go nuclear we should have started in 2000. Right now we start reducing consumption, build wind turbines like crazy and close the coal and oil fired power plants first, then gas and nuclear last. reply AdamN 40 minutes agorootparentprevCountries like France went all-in on nuclear - didn&#x27;t change much really. Theoretical nuclear is risk free and low carbon but actual nuclear (that was available in the 60s&#x2F;70s) is neither. reply iso8859-1 3 hours agorootparentprevShe said the Atomausstieg was a mistake: https:&#x2F;&#x2F;world-nuclear-news.org&#x2F;Articles&#x2F;Closing-nuclear-in-G... reply SV_BubbleTime 3 hours agorootparentprevTo the surprise of no one that has looked behind the curtain to see “the only solution” is always you need to pay us more taxes and keep us in power. reply cycomanic 3 hours agorootparentThat comment is hilarious considering that nuclear has recent several multiples of the subsidies that renewables received and is still much more expensive. And we are not even counting the military (the nuclear industry would likely be in a much worse state if it wasn&#x27;t for the militaries demand for nuclear engineers and scientists and financing much of the R&D). reply belorn 5 minutes agorootparentEnergy subsidizes in Europe looks different. Wind&#x2F;Solar are the single biggest recipient of subsidies and receive about as much as the combined total of all other subsidies in the energy sector. They get far more per watt produced than anything else, mostly going to grid connections, building, deconstruction, and price guaranties. (The yearly report is produced and published by the European commission at ec.europa.eu)Nuclear get subsidies in term of research, building, deconstruction, waste storage and price guaranties. Around 70% goes to the single fusion research project called ITER (international research, non-military).Hydro receives an increasing amount of subsidies for repair and modernization. Dam repair and flooding protection is expensive and with climate change there is even bigger need for fixing Europe old hydro power dams. They are also in general non-compliant with the European environmental regulations (several species are going extinct), but that is not a subsidies issues directly. Fixing the dams so they allow for fish to pass is however a subsidies issue, but as far the budget to fix that has yet to be allocated and the costs are estimated to be exceedingly high.And last we have fossil fuel subsidies. A large portion of the \"reserve energy\" plan in eu in order to address increased gird variability is based on keeping a large number of fossil fuel plants on stand-by, paid through subsidies. Then there is subsidies on extracting the fuel itself, subsidies on trading fuel, and subsidies on storage of the fuel, and transportation of the fuel. This is not accounting for the environmental cost from burning fossil fuels, which some see as a form of subsidies.Subsidies-like part not included are insurance against nuclear accidents, insurance against floods from dam failures, and insurance against forest fires. It is also not accounting for land usage nor damage to wildlife. reply thedrbrian 17 minutes agorootparentprevseems that wind isn&#x27;t profitable without massive subsidies eitherhttps:&#x2F;&#x2F;www.telegraph.co.uk&#x2F;business&#x2F;2023&#x2F;11&#x2F;14&#x2F;wind-farm-or... reply SV_BubbleTime 2 hours agorootparentprevOk, so the argument is that we know we went nuts regulating it to oblivion, and this is a fault in the technology and not ourselves - then you go to admit that it is a massive tactical and technological advantage we need?Ok, strange flex but this about what I have come to expect from this “debate”. reply cycomanic 2 hours agorootparentNuclear is not so expensive because of regulation that&#x27;s another myth. Considering the scale and possible impact wind and solar often have much more regulations (just look at Bavaria where the ruling party essentially blocked all new wind installations by creating requirements that reduced potential sites to nearly 0). A nuclear power plant is essentially a thermal plant plus the nuclear part so how would it be cheaper than a coal plant? reply ben_w 1 hour agorootparentI&#x27;m not disputing your claims, but as a suggestion:> A nuclear power plant is essentially a thermal plant plus the nuclear part so how would it be cheaper than a coal plant?Fuel costs? reply cycomanic 1 hour agorootparentI was talking capital costs, not operational costs. My point is, to build a nuclear power plant you essentially need to build a thermal power plant first and then add things for the nuclear part (which is arguably more complicated&#x2F;safety critical than for e.g. a coal plant).So the whole argument for economics of scale doesn&#x27;t work, thermal power plants have not reduced dramatically in price over the last decades despite lots of them being build. Half of your plant not really reducing much in price, will limit the benefits you can get even if the other half sees massive cost reductions. reply ben_w 57 minutes agorootparent> I was talking capital costs, not operational costs.Ok, but surely the only cost that matters is the lifetime cost? (Including whatever cleanup is needed for both nuclear and whichever fossil and&#x2F;or renewable+storage combination it is compared against).> Half of your plant not really reducing much in price, will limit the benefits you can get even if the other half sees massive cost reductions.Sure, absolutely. But coal is pretty expensive over the course of a year, so it can look like a good opportunity (if only for the reality hadn&#x27;t turned out so fragile and, when it goes wrong, severe). reply audunw 2 hours agorootparentprevNuclear is arguably only safe due to those regulations. The case of falsified certificates in Korea shows that you need plenty of margins to account for some of the regulations not being followed properly, even in a first world country.Maybe there is 20% too many regulations. That could be the case. But having 20% too many regulations is far preferable than 20% too few.Even if you are staunchly pro-nuclear you should want regulations that reduces the chance for even a minor accident to almost exactly 0%, because even a minor accident will cause fear that’ll set nuclear back by two decades. Maybe that fear is irrational. Tough luck. Humans are irrational. Most of them would rather be slowly poisoned by coal and die a couple years early, than living with the thought of maybe having to suddenly have to abandon their home and established life like in Fukushima. reply nickpp 29 minutes agorootparent> But having 20% too many regulations is far preferable than 20% too few.The problem is when that extra 20% regulation makes the technology so expensive the world chooses to keep burning coal, oil and gas thus poisoning and killing millions through pollution and endangering life on the entire planet through Climate Change.Right now nuclear is so frozen and so useful that I&#x27;d take the change of a (PR) disaster and (slowly, carefully, partially) deregulate: it can&#x27;t get much worse than already is. replyphotochemsyn 6 hours agoparentprevWhere are you going to get the uranium ore from to run this project? Where are the high-grade uranium ores that can be economically processed into the required ~200 tons of rods that each 1 GW nuclear power plant requires each year? Let&#x27;s say we have 0.1% uranium ore, that means we need 1000 tons of ore for each ton of uranium. Huge energy costs, huge waste stream to get that one ton of uranium, by the way. And each 1 GW reactor needs 200 tons - per year. So that&#x27;s 200,000 tons of ore per year.Now, in comparison, a 1 GW coal-fired power plant needs to be fed with 3-4 million tons of coal per year (energy content of coal varies a bit, in relation to oxygen-based combustion at least).And, a 1 GW reliable 24&#x2F;7 solar&#x2F;wind&#x2F;storage system needs how many tons per year? (OK battery replacement is an issue... work it out yourself). reply defrost 3 hours agorootparentAssuming you&#x27;re asking a literal question and want an answer, the current global total of \"reasonably assured resource\" uranium (known, tested to a degree of certainty by sampling, modelled for economic recovery costs) that can be extracted at less than $130 US &#x2F; kg usable uranium is 3.8 million tonnes, roughly 28% of which is in Australia, 10% in Canada, ~1% in the USofA, etc.You can find a full table, distribution maps, guesstimates for known but as yet untested deposits, etc in the big Red Book of Uranium stuff (free to download):https:&#x2F;&#x2F;www.oecd-nea.org&#x2F;jcms&#x2F;pl_28569&#x2F;uranium-resources-pro...> a 1 GW reliable 24&#x2F;7 solar&#x2F;wind&#x2F;storage system needs how many tons per year?Currently panels drop to 80% efficacy after 10 years and are doorstops in 20 - replacementrecovery is an ongoing process and that currently requires raw materials that come with a cost - including slave labour.https:&#x2F;&#x2F;blog.ucsusa.org&#x2F;charlie-hoffs&#x2F;mining-raw-materials-f...There are no magic problem free resources. reply davedx 1 hour agorootparent> Currently panels drop to 80% efficacy after 10 years and are doorstops in 20Oh come on. This is an absurd claim with no evidence. Don&#x27;t be ridiculous. reply photochemsyn 3 hours agorootparentprevMonocrystalline silicon panels last >30 years at near-initial production unless subjected to physical damage.Yes, investors in the uranium fuel rod production pipeline will feel a lot of pain as a result of what&#x27;s going on right now, but it&#x27;s not like they weren&#x27;t warned. Like Theranos investors, they failed to do their due diligence.[edit] Just to add, the metric you want is ENERGY not $US, because what really matters is the amount of energy you have to put into mining the ore, extracting the uranium from the ore, enriching the uranium-235 in the ore to the point where it&#x27;s useful in a nuclear reactor, and then packaging that refined product into highly expensive fuel rods made of fairly expensive materials. By the time you do that energy calculation, per-year, for the reactor (on top of ridiculously high initial construction costs), the whole thing looks like a massive long-term liability. reply defrost 2 hours agorootparentMonocrystalline is the most expensive one to produce, the process of manufacturing monocrystalline solar cells is very energy-intensive and produces a big amount of silicon waste.They&#x27;re not especially robust in cold weather and while they do last longer, they do also degrade in output over time just as the poly versions do.To maintain a 1 GW farm requires ongoing upkeep, etc.Again, there are no magic problem free resources, just like low volume uranium mining, large volume mining and production processing to sustain solar power has toxic side effects and impacts people. reply photochemsyn 2 hours agorootparentI think that the overall cost of producing 1 GW of monocrystalline solar PV (a technique China has apparently mastered) is very comparable to the cost of producing a single year&#x27;s worth of enriched uranium fuel rods, at that&#x27;s from comparing the costs of acquiring the raw materials, putting them through the requisite processing stages, and producing the finished product.It&#x27;s not even close, with some limited exceptions, like Finland and other Artic Circle zones. reply _huayra_ 3 hours agorootparentprevIt&#x27;s a bit disappointing how the solar&#x2F;wind folks neglect to consider the fact that such a storage system does not exist. Before one goes to the \"but muh batteries\" rebuttal, it&#x27;s a useful exercise to see how much Lithium would need to be mined to provide reliable baseload power.Simon Michaux has already done this. It&#x27;s worth a read through his reports [0].Also, it&#x27;s quite possible to reprocess spent fuel. France has been doing it for decades. [1] The price has not been enough to justify it, but eventually it will (or if other costs are imposed, like a tax on the mineral rights that would favor reprocessing fuel).[0] https:&#x2F;&#x2F;www.simonmichaux.com&#x2F;gtk-reports [1] https:&#x2F;&#x2F;www.iaea.org&#x2F;newscenter&#x2F;news&#x2F;frances-efficiency-in-t... reply davedx 59 minutes agorootparent> \"but muh batteries\"Thanks for arguing in such bad faith by default. But let&#x27;s just say there are other ways of storing energy that don&#x27;t need huge amounts of processed lithium.Have you perhaps heard of \"muh pumped hydro\", \"muh flow batteries\", \"muh hydrogen\", and the myriad of other less mature storage technologies currently being developed? reply rahen 1 hour agorootparentprev>Also, it&#x27;s quite possible to reprocess spent fuel. France has been doing it for decades.Wasn&#x27;t that the SuperPhoenix and Astrid reactors? I thought both had been cancelled because of politics?https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Superphénixhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ASTRID_(reactor) reply Maakuth 40 minutes agorootparentThose are breeder reactors. In breeders, less fissile materials are transformed into more fissile ones. That&#x27;s interesting technology and very nice on paper, but there are practical challenges.Reprocessing is another thing entirely. It relies on the fact that used nuclear fuel is mainly not reusable due to accumulation of neutron poisons. In reprocessing, these poisons are removed so that the rest of the fuel can be reused. This too has many practical challenges as you end up with liquid radioactive materials and a lot of it is waste. So far it has proven more practical than breeders, though. Perhaps that is because the same processes can be used to extract materials for nuclear weapons. reply photochemsyn 2 hours agorootparentprevIf you&#x27;re not limited by the need to move around, then lithium doesn&#x27;t make as much sense for power arbitrage technologies. There are literally dozens of such technologies capabale of storing intermittent solar&#x2F;wind inputs in physical or chemical forms that can be pumped back out onto the grid as needed.It&#x27;s so ridiculous to think that human civilization would collapse just because we will eventually run out of stuff to dig out of the ground and burn, when the biosphere has been humming along on sunlight for about three billion years. Why is this so hard to explain to people? reply bdsa 0 minutes agorootparentHow much energy does all of humanity use compared to the rest of the biosphere? Genuine question not intended to be leading, it&#x27;s a very interesting point. ben_w 1 hour agorootparentprevI agree, but we also need a storage system that can move around, and the aggregate scale of that is inherently more than enough for grid storage. This is why I&#x27;m not going to rule out hydrogen despite it having not worked out even back during the 1973 OPEC fuel crisis when alternatives were desperately needed and nuclear power (which is useful for among other things electrolysis) was The Future.(Also, on paper one doesn&#x27;t need any grid storage, if only we could get governments to cope rates on a planetary grid. Texas doing it&#x27;s own thing and Japan having two frequency zones shows how hard this would be in practice, but in principle it can work just fine). reply doublespanner 5 hours agorootparentprevThe highest yield current mining and enriching processes beat that by about a factor of 10; there are a few sites globally that achieve that, Canada would be the most relevant.Don&#x27;t forget that only the high density fuel needs to be shipped long distance as well...For a fully reliable 1gw of wind, that&#x27;s something like 600 turbines (to simply), each turbine lasts about 20 years, and weighs about 200 tonnes. So about 5000 tonnes of finished wind turbine a year, with an equivalent waste stream. reply d4mi3n 6 hours agorootparentprevPebble bed reactors are thing. Thorium reactors are a thing. The tech has improved even if the active reactors have not. reply bumby 5 hours agorootparentTbf, I think the commenter was responding in the context of the 93 existing reactors previously mentioned. reply vasco 3 hours agorootparentprevIt&#x27;s funny you&#x27;d assume they haven&#x27;t done simple math of where to get the fuel for billion dollar investments. reply legulere 7 hours agoprevAnother unwanted dependency on Russia is the finished fuel for Russian reactor designs. Luckily a lot of countries start to go to Westinghouse to let them produce an alternative, but as far as I see it’s only for one nuclear reactor design (VVER-440) reply necheffa 5 hours agoparentWestinghouse also supports VVER-1000 too. https:&#x2F;&#x2F;www.westinghousenuclear.com&#x2F;Portals&#x2F;0&#x2F;operating%20pl... reply konart 2 hours agoparentprevAt the same time Russia is building new plants in Turkey, India, Hungary, Egypt, China, Iran and Bangladesh. reply spullara 2 hours agoprevBeing dependent on an enemy for weapons is as stupid as being dependent on an enemy for food like they are. reply romeo88 2 hours agoparentDependence is mostly no final decision. The cost and lead time for independence is important in strategic considerations. reply rlt 3 hours agoprev> The company is on track to produce 20 kilograms of HALEU by the end of the year, and then expects to produce 900 kg in 2024> 3 tablespoons of HALEU can supply a lifetime’s worth of power for the average U.S. consumerSo if I did my math right[1], 20 kg is enough for 1867 U.S. consumers for one year, and 900 kg is enough for 84,000 U.S. consumers? It&#x27;s a start, I guess.1. https:&#x2F;&#x2F;www.wolframalpha.com&#x2F;input?i=20+kg+%2F+%28%283+table... reply SV_BubbleTime 3 hours agoparentLifetime of 85,000 people isn’t that bad. You have 79 years in that calc.That’s 6,700,000 for one year. There only 16 states with populations greater than that.IDK. Seems decent. If we wanted better we should have started 50 years ago. Although now is good too. reply rlt 3 hours agorootparentI don&#x27;t think that&#x27;s right. I multiplied by average lifespan in my original calculation.3 tbsp of Uranium is about 0.85 kg.900 kg &#x2F; 0.85 kg = 1060 US consumer lifetimes worth of energy.Multiply by 79 (average lifespan) and you get 83,740 US consumer years worth of energy. reply Horffupolde 6 hours agoprevReënters reply Dylan16807 1 hour agoparentWow, I think you got flagged for a spelling suggestion. reply scythe 7 hours agoprev>However, the type of uranium fuel those reactors use is not going to cut it for the advanced reactors expected to go on line in the coming yearsIf you&#x27;re like me, you read this sentence and thought: \"hey, wasn&#x27;t that canceled last week?\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38199061But apparently that was NuScale&#x27;s reactor that was canceled, while TerraPower is still going, which may have something to do with the exceptional wealth of its founder:https:&#x2F;&#x2F;abcnews.go.com&#x2F;Technology&#x2F;bill-gates-future-nuclear-...And they seem to have found a way to get even more billionaire bucks:https:&#x2F;&#x2F;www.theguardian.com&#x2F;us-news&#x2F;2021&#x2F;jun&#x2F;03&#x2F;bill-gates-w...There&#x27;s a \"hero we deserve\" meme hiding in here somewhere... reply windows2020 7 hours agoprevIt seems so clear that nuclear is our ultimate energy source. We&#x27;re wasting so much time on toy windmills and solar panels, and all the batteries required to compensate. There&#x27;s so many complaints about cost overruns, etc. OK... let&#x27;s figure it out! Let&#x27;s put more focus into this and make safe, efficient and clean nuclear happen! This article indicates we&#x27;re moving closer to that. reply tuatoru 7 hours agoparent> There&#x27;s so many complaints about cost overruns, etc.This is ironic. I recommend you read How Big Things Get Done[1], by Bent Flyvbjerg, the world&#x27;s foremost expert on megaprojects. The book discusses an analysis of about 10,000 projects of over a billion dollars each.Which projects were most likely to come in on time and on budget, and deliver the promised returns? PV solar.Which projects were third most likely to overrun by 100% or more, be 100% late or more, and deliver only a fraction of the promised returns? Nuclear power plants.Edit: The book&#x27;s ratings are based on construction phase only, after all the permitting and other planning work has been completed.(Third worst? What could be worse? Answer: nuclear waste storage sites, and Olympic Games hosting.)1. https:&#x2F;&#x2F;www.penguinrandomhouse.com&#x2F;books&#x2F;672118&#x2F;how-big-thin... reply acidburnNSA 7 hours agorootparentWho&#x27;d have thought that the targets of perhaps the world&#x27;s largest anti-anything campaign can slow them down so much? In the 1991 the anti-nuclear industry published its strategy to use everything in their power to slow down nuclear plants and especially nuclear waste sites. It&#x27;s in black and white. [1][1] https:&#x2F;&#x2F;atomicinsights.com&#x2F;anti-nuclear-movement-strategy-ci... reply RNAlfons 27 minutes agorootparentAre you joking? This is a blog post by some guy from a venture which invests into nuclear outlining a conspiracy theory where the Nuclear Information Resource Service (NIRS), the Safe Energy Communication Council (SECC), and Greenpeace slowed down nuclear around the world? What? reply cycomanic 2 hours agorootparentprevSo how to anti nuclear activists cause massive cost overruns? The nuclear industry is a multibillion dollar industry with extremely close ties to government (probably one of the closest after the military industry). But somehow a movement, that still gets largely dismissed by politicians as environmental nutters managed to get them to overrun costs?I mean the myth of the supposedly all powerful anti-nuclear lobby is actually hilarious. I mean took fukushima to get Germany (arguably on of the most nuclear sceptical countries) to decide on stopping nuclear (despite the majority of the population being against nuclear power for years prior) and under quite favourable conditions for the nuclear industry. reply roenxi 25 minutes agorootparentIf you think about it, what is the argument for why nuclear is to plan? Nuclear is impossible to plan because ... pixies steal the computers the planners are using? Note that whatever factors you identify, the people estimating the plant build process are aware of that.The planners know exactly how long it takes to build a nuclear plant, and they learn just as quickly as everyone watching from the sidelines. Their assumptions are being thrown by something - almost certainly anti-nuclear campaigners in government or regulatory forces warping incentives. Those are the only things that can consistently diver overruns. Otherwise it is hard for overruns of a nuclear plant to be different from any other project. reply tuatoru 6 hours agorootparentprevHow does that work in China and Russia? reply acidburnNSA 6 hours agorootparentChina and Russia regularly deliver nuclear plants (VVERs, Hualong One) roughly on time and budget. reply tuatoru 6 hours agorootparentBut not at a rate that can keep up with the growth in demand, especially in China. Demand in China has grown more than 20-fold over the 21st Century but nuclear installs have not come close to keeping up with that.In contrast solar PV and wind have now grown to the point where they are overtaking the growth in demand, meaning that fossil plants can soon start to be be retired. reply emmelaich 5 hours agorootparentprevAnti-pollution activists share a lot with anti-nuclear activists.In China and Russia they don&#x27;t have as much clout, so they use lots of coal, if it&#x27;s cheaper. reply konart 3 hours agorootparentAccording to https:&#x2F;&#x2F;www.worldometers.info&#x2F;coal&#x2F;coal-consumption-by-count... Russia&#x27;s coal consumption is notably lower than that of the US per capita. reply BenFranklin100 6 hours agorootparentprevThis is a misguided response. Nuclear power plants have all been essentially custom, one-off projects built by inexperienced crews. It’s little surprise the cost overruns. It’s a very different proposition with identical, smaller modular plants that are being proposed that will be built in the tens to hundreds. Costs will drop dramatically while safety will increase, both by the design of the plants and the economies of scale. reply rtpg 3 hours agorootparentFrance, France, France, France, France. I need nuclear proponents to read about France.A nation made a bet on \"let&#x27;s do all nuclear\". It basically worked, but there was no magical cost decrease. They have an immense amount of institutional knowledge, huge educational system, massive amounts of resources. Their nukes are big and expensive and not really on time. There is no magic at the end of the tunnel if Greenpeace disappears.The world isn&#x27;t Sim City, power plants need water access and have location concerns, and it&#x27;s not a question of just dropping a bunch of truck-sized plants across the world and magically solving everything.Having said that, France is a success story of course. It&#x27;s cool that it&#x27;s all set up. But even after building it up, running these plants are expensive, there&#x27;s often load issues meaning they are not fully utilized... \"nuclear is free easy energy\" is science fiction. That&#x27;s fine, though! It&#x27;s normal that we have to do stuff to power society.EDIT: to be clear, I&#x27;m cool with nuclear, but I am not picky about where my watts come from. reply ViewTrick1002 2 hours agorootparentFrance went \"Negative learning by doing\" [1] in their initial build out and Flamanville 3 [2] is looking like a complete boondoggle.[1]: https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S03014...[2]: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Flamanville_Nuclear_Power_Pl... reply rtpg 2 hours agorootparent> The French nuclear case illustrates the perils of the assumption of robust learning effects resulting in lowered costs over time in the scale-up of large-scale, complex new energy supply technologies. The uncertainties in anticipated learning effects of new technologies might be much larger that often assumed, including also cases of “negative learning” in which specific costs increase rather than decrease with accumulated experience.Oof, talk about a nasty abstract. Will need to find a way to look at this paper later. reply tjmc 2 hours agorootparentprevI&#x27;ve heard it said that in France there are 3 types of nuclear reactor and 100 types of cheese. In America it&#x27;s the other way around. reply spullara 2 hours agorootparentprevThere is a magical end that we can optimize where it is just about making nuclear fission energy more efficient. reply troupo 2 hours agorootparentprevFrance also suffered from the same anti-nuclear sentiment as the rest of the world. To the point that they a couple of years ago they were forced to shutdown their plants for maintenance after a decade or more of neglect. reply WinstonSmith84 13 minutes agorootparentit was in fact the last winter just at the worst moment possible when electricity price was exploding. Half of them were in maintenance for various reasonshttps:&#x2F;&#x2F;www.tf1info.fr&#x2F;economie&#x2F;crise-de-l-energie-nucleaire... reply bruce511 4 hours agorootparentprevYou are correct - one offs incur huge \"unknowns\" which bloat budgets.You are correct that the obvious solution is standard production of modular designs are the solution to that.This has all be true and obvious for several decades. Yet the realisation of this approach seems to still be far away.It also doesn&#x27;t factor in the non-technical hurdles, costs, and delays that are involved here. Environmental impacts, and local-population-resistance are significant, often terminal, factors in plant construction.Put another way, nobody cares if I have solar panels on my roof. But if I have a tiny modular reactor in the garden the neighbours will (not surprisingly) object.I say this as a nuclear supporter. reply eru 4 hours agorootparent> Put another way, nobody cares if I have solar panels on my roof. But if I have a tiny modular reactor in the garden the neighbours will (not surprisingly) object.Yes, though your neighbours will probably also object to you putting up one of those big wind turbines in your backyard. Or a coal fired power plant. reply bruce511 3 hours agorootparentIm not sure they&#x27;d complain if I put up a small wind turbine. If I was in a more rural area I doubt theyd object to a big wind turbine. (The extent of their worry being noise, and danger if it self destructed in bad weather.)Clearly a coal-fired plant would be an issue, to the degree to which they pollute the neighbours. There are diesel generators here attached to factories etc, and nobody really cares.Objection to nuclear though is more visceral. With diesel I can hear the noise and decide if I don&#x27;t like it. With coal I might notice the smell. But I don&#x27;t think either would silently kill me. Nuclear leaks are invisible, and accumulative, so it can be (disproportionately) scary. Certainly those who are opposed to it will use fear to rally other people to their side. Fear is a strong motivating emotion that is easily manipulated. reply abstractbeliefs 6 hours agorootparentprevI think you misunderstand. The comment you reply to I think acknowledges that there are problems with nuclear project management - they&#x27;re saying that with that in mind, let&#x27;s figure out how to do it better, to make nuclear projects less risky, more reproducible, and subsequently more commercially attractive. reply epistasis 7 hours agoparentprevThe strongest argument in favor of nuclear seems to be that it sounds futuristic and it&#x27;s what we grew up believing would be the future of energy.However in practice it has never turned out to be that great. There&#x27;s little to recommend it: it requires massive amounts of labor, double checking high skill welds, documentation, and nobody in the industry seems the least bit interested in making this an efficiency process.Nuclear rockets sound cool. Nuclear electricity plants are pretty awful in practice.Batteries are about 1000x more useful on the grid than nuclear, it will greatly enhance reliability as they are super scalable and have the potential to massively increase the efficiency of grid transmission and distribution, the most expensive part of electricity for most. Nuclear has none of those benefits. reply bdzr 7 hours agorootparent> The strongest argument in favor of nuclear seems to be that it sounds futuristic and it&#x27;s what we grew up believing would be the future of energy.It&#x27;s not that it&#x27;s one of the safest energy sources in existence that works in all weather conditions? reply stephenr 4 hours agorootparent> that works in all weather conditions?Didn&#x27;t France very recently have to shut down or limit a bunch of reactors due tothe river water being too warm.It&#x27;s one of the safest when compared to coal. How exactly do you accurately calculate the \"safety\" of something that produces toxic waste with a lifespan in the thousands of years? reply delroth 4 hours agorootparent> Didn&#x27;t France very recently have to shut down or limit a bunch of reactors due tothe river water being too warm.Because of environmental limits to protect the river ecosystems, not because of any safety or operational constraints. Many of these limits were set years ago, when the water temperature in rivers was lower, and haven&#x27;t been increased since - leading to very tight margins. When the grid is stretched, the plant operators usually get a temporary authorization to exceed the environmental limits.Every year this is big news for some reason, even though it amounts to a roughly 0.3% reduction of yearly electricity generation. It only impacts river cooled plants, and only those that don&#x27;t also have a cooling tower. reply stephenr 4 hours agorootparent> Because of environmental limits to protect the river ecosystemsSo what you&#x27;re saying is, nuke plants can operate in any weather if you give zero fucks about how they affect nature..? reply mpweiher 1 hour agorootparentNope.This affects all thermal plants, it has nothing to do with nuclear.And as the parent pointed out, it only affects plants where you cheap out and skip building the otherwise required cooling towers.So it’s trivially solvable: build cooling towers. reply spullara 2 hours agorootparentprevYes. If you care more about warm water over killing people, carbon emissions and pollution. reply fodkodrasz 1 hour agorootparentKilling wildlife would be okay? Wow we saved some carbon emission (so it can wasted somewhere else...) and saved some lives (i don&#x27;t see the connection you are implying), so now we can live in a barren land with plenty energy. We have already done a lot of harm to the environment, we should try to preserve as much as possible of what is left.(And this goes for hydro especially, and somewhat for solar and wind as well, all have environmental impacts, which must be taken into account and sensible compromises need to be made). reply fodkodrasz 1 hour agorootparentprevThey can, if they are designed and built for those weather conditions in mind. Cooling towers are more expensive but could work in the mentioned case. reply troupo 2 hours agorootparentprev> Didn&#x27;t France very recently have to shut down or limit a bunch of reactors due tothe river water being too warm.No, no they didn&#x27;t. Most reactors that were shut down that summer were shut down for maintenance after a decade or more of neglect. reply RNAlfons 23 minutes agorootparentSo if even THE nuclear nation on this planet \"neglects\" its reactors, wouldn&#x27;t that be an argument against nuclear?I mean, if that claim of negligance would be true which I doubt it is of course...those things grow old and France failed to diversify it&#x27;s power generation in the last decades but hey...new ones are planned...maybe in a decade one will even be finished...More details on the problems: https:&#x2F;&#x2F;www.neimagazine.com&#x2F;features&#x2F;featuredealing-with-cra... reply goodpoint 53 minutes agorootparentprevNo, because it&#x27;s not. reply rtpg 3 hours agorootparentprevIt&#x27;s definitely versatile but it does need access to a lot of water, and a lot of trained staff to run it. PV and wind also have staff constraints, of course, but plenty of smaller scale PV setups are fairly hands-off. reply acidburnNSA 7 hours agorootparentprev> Batteries are about 1000x more useful on the grid than nuclear,Citation needed! Also: did you know that all existing commercial nuclear plants can ramp up and down in power at about 2-3% (~25 MWe) per minute? They often simply choose not to because the current market structure doesn&#x27;t incentivize them to. Add a price to low-carbon load following and all the plants in the US will start doing it.Batteries have a big disadvantage in that they do not generate any electricity. They also have pretty miserable energy density, which correlates eventually to chemical waste. To store power in batteries for a night in the US, you&#x27;d need many thousands of skyscrapers full of batteries. reply cycomanic 3 hours agorootparent> > Batteries are about 1000x more useful on the grid than nuclear,> Citation needed! Also: did you know that all existing commercial nuclear plants can ramp up and down in power at about 2-3% (~25 MWe) per minute? They often simply choose not to because the current market structure doesn&#x27;t incentivize them to.That&#x27;s a nice way of saying that nuclear power is much too expensive to not run plants at maximum capacity all the time.>Add a price to low-carbon load following and all the plants in the US will start doing it.Citation needed. Considering the price of wind or solar, investors are much mor elikley to over provision using those technologies. Nuscale just had to axe their SMR (supposedly the future of nuclear) project in Idaho, because they couldn&#x27;t find subscribers and their cost of overrunning massively.> Batteries have a big disadvantage in that they do not generate any electricity. They also have pretty miserable energy density, which correlates eventually to chemical waste. To store power in batteries for a night in the US, you&#x27;d need many thousands of skyscrapers full of batteries.Except you don&#x27;t need to store the power to run all of the US for a night, you&#x27;re just making a strawman. reply mpweiher 1 hour agorootparent> That&#x27;s a nice way of saying that nuclear power is much too expensive to not run plants at maximum capacity all the time.Nope. It means that the variable costs are low. The fuel costs for a nuclear plant are so low that it’s essentially free.Which is also the primary reason we throw away 95% of the fuel unused: given the low cost of fuel, recycling just isn’t viable. And again, not because recycling is so expensive, but because new fuel is so cheap. reply ViewTrick1002 2 hours agorootparentprev> Also: did you know that all existing commercial nuclear plants can ramp up and down in power at about 2-3% (~25 MWe) per minute?How often can they repeatedly ramp without getting into trouble with reactor poisoning?To load follow with nuclear France devised an entire scheme where their monopoly owner would let reactors take turns reducing their output.Then on top of that the fueling schedule is synced since the later a reactor is in the fuel lifecycle the less it can ramp.Try that on a free market. reply troupo 2 hours agorootparent> How often can they repeatedly ramp without getting into trouble with reactor poisoning?Every day, several times a day. That is literally the requirement for modern reactors (where modern means any reactor in the past 30 years, or more, and most retrofitted reactors from previous years)Graph and text on page 8: https:&#x2F;&#x2F;www.oecd-nea.org&#x2F;upload&#x2F;docs&#x2F;application&#x2F;pdf&#x2F;2021-12... reply irjustin 6 hours agorootparentprevThis post seems too naive.> Batteries are about 1000x more useful on the grid than nuclearEven if I agree batteries are the answer, how do you intend to charge this batteries?!Solar is great, wind is great, Hydro is great - what happens when these don&#x27;t work? What will you use? reply ceejayoz 6 hours agorootparent> Solar is great, wind is great, Hydro is great - what happens when these don&#x27;t work? What will you use?The chances of a simultaneous and extensive wind, solar, and hydro outage on a national grid scale is... fairly low. reply plasmatix 2 hours agorootparentBut is it low enough?Solar drops to zero every night.For hydro, there&#x27;s already seasonal variation with rain&#x2F;snow melt. Beyond that, it looks like the majority of hydro in the US is on the west coast, which has been experiencing a drought for years and is projected to get worse. The west coast is big on removing dams for habitat restoration, too.Wind: Admittedly I have no idea what sort of consistency wind has. But it does seem reasonable to assume the overall reliability of wind power will decrease as installed capacity increases, assuming we started with the most productive geographies first and then move to increasingly marginal areas.In the same way that storm fronts travel through an area and bring high speed winds perhaps there is the opposite, where relatively sudden, large-scale lulls form?To minimize the likelihood of blackouts we&#x27;d need to either:1) Build sufficient excess capacity of wind&#x2F;solar&#x2F;hydro to overcome variance in output. This may not be feasible, if even possible, considering the points above.2) Maintain fossil fuel peaker plants.3) More nuclear for base load.Having written these thoughts out I now realize your statement presumes we can ever (and always) meet 100% of electricity demand with wind&#x2F;solar&#x2F;hydro in the first place. reply goodpoint 52 minutes agorootparent> But is it low enough?Yes. reply mpweiher 1 hour agorootparentprevNot nearly as low as is believed and promulgated by advocates, who apparently naively believe these to be uncorrelated both in time and space.They are not. reply troupo 2 hours agorootparentprevA quiet night will take out both solar and wind. And that happens multiple times a year.As I write it right now Denmark is producing 0 solar, and only 12.5% of its wind production. Because its cloudy and there&#x27;s little wind.So they have to import almost 40% of electricity from Norway and Sweden where electricity generation is propped up by nuclear reply tuatoru 6 hours agorootparentprevIt&#x27;s never sounded futuristic to me. Nuclear just boils water to make steam, the same as James Watt&#x27;s steam engine. (Well, actually, steam turbines, but still pretty old technology.) Boiling water to make electricity? How quaint. reply eru 4 hours agorootparentSlight tangent: radioisotope thermoelectric generators use a different mechanism to generate electricity.Though honestly, they just heat some pieces of metal, instead of heating water.Photovoltaic heats a bunch of hydrogen ions until they glow. (But those ions are in the sun.) Wind power mostly comes from air and water being heated unevenly.Tidal power is perhaps the coolest here: no heat involved, the energy comes from the motion of the earth and the moon. reply Ygg2 6 hours agorootparentprevBatteries also have other downsides. Poor density, chemical waste, usually limited discharge cycle.The PV&#x2F;turbines are intermittent and will suffer more in extreme climate conditions (winds forcing shutdown and PV getting caught in dust storms), than nuclear (which are more vulnerable to droughts). reply tempestn 38 minutes agoparentprevFortunately we don&#x27;t need to choose between nuclear and solar or wind. We can do both. reply afterburner 7 hours agoparentprev> let&#x27;s figure it outRenewables already figured it out. They&#x27;re already cheaper. Nuclear already lost.Until next-gen nuclear is developed, it&#x27;s over. They&#x27;re welcome to keep researching it though. Meanwhile, renewables are also getting better, and much faster. And they&#x27;re already better than nuclear. reply rr808 7 hours agorootparentThey are not cheaper. They appeared cheaper for a while when there was a bubble and interest rates are zero. Now wind and solar farm projects are failing. reply matthewdgreen 7 hours agorootparentI’ve now posted this three times, but I don’t think people on this site are aware of it, since we’re still seeing the same obsolete debate. China built 415 TWh of wind and solar in the first 9 months of 2023 (adjusted for capacity factor), more than the combined total of all 26 nuclear plants under construction. Based on this surge of construction, Chinese emissions are expected to begin a structural decline next year.The debate at this point isn’t about whether renewables will scale: China has already answered that question. There is a very real question about whether the West will be able to keep up. There seem to be a lot of people who want to sit around and fantasize about 1970s-era nuclear tech while our neighbors undergo what is effectively a second Industrial Revolution. reply rr808 6 hours agorootparentChina is a special case because it has to import all its oil which makes it a huge strategic weakness. Wind and Solar is still a tiny part of its electricity production. You can&#x27;t rely on either for cold dark winters. Its emissions are already higher than Europeans per capita and still growing.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Electricity_sector_in_China#&#x2F;m... https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Greenhouse_gas_emissions_by_Ch... reply matthewdgreen 6 hours agorootparentThe thing about exponential deployment curves is that, at a certain point, posting charts that end 1 to 2 years in the past is like posting charts that end in 2000. Things are moving extremely rapidly now. https:&#x2F;&#x2F;amp.theguardian.com&#x2F;business&#x2F;2023&#x2F;nov&#x2F;13&#x2F;chinas-carb... reply rtpg 3 hours agorootparentprevaccording to the wiki numbers, wind + solar make up 17% (with hydro being another 17%). Nothing to sneeze at! Though coal is extremely present. reply BenFranklin100 6 hours agorootparentprevYou can post it three more times and it won’t make it any less true that renewables are plagued with shortcomings regarding 24&#x2F;7 uptime and massive safety issues with installation and eventual end-of-life environmental costs.440 people die for every 1000TW of solar, compared to 90 for nuclear: https:&#x2F;&#x2F;www.engineering.com&#x2F;story&#x2F;whats-the-death-toll-of-nu...Solar panel disposal impact: https:&#x2F;&#x2F;hbr.org&#x2F;2021&#x2F;06&#x2F;the-dark-side-of-solar-powerRenewables have a place in our energy future. Fanaticism about them does not. reply somenameforme 4 hours agorootparentHave you read what you linked to, while trying to have a skeptical eye? That article, especially when you&#x27;re claiming others are fanatics, is not something you probably want to link to.\"The number of deaths for every 1000TWh of energy generated by rooftop solar panels is 440. Put simply, this means that for every 1000TWh of energy produced via rooftop solar power, 440 people lose their lives. Other estimates [obfuscated hyperlink to a Forbes page] place this number to be around 150. These deaths are mostly the result of electrocution and other hazards that occur during rooftop solar panel installation (such as falling).\"So they&#x27;re not happy with the official statistics bureau that is Forbes and chose to stick with their uncited 440. It sounds made up to me. Searching for that exact datum actually yielded the original source [1], and yeah - it&#x27;s made up. It&#x27;s from a 2008 article where some guy just started hand-waving hard in a not entirely coherent fashion.A quick search for something more reasonable turns up this. [2] The data comes from multiple studies and the UN Committee on the Effects of Atomic Radiation. Wind&#x2F;Nuclear&#x2F;Solar all have pretty much the same mortality rate: 0.04&#x2F;0.03&#x2F;0.02 per unit of electricity, respectively. If I want to be disingenuous I can claim [accurately] that nuclear is 50% deadlier than solar (and obviously many many magnitudes deadlier per installation), but the numbers in terms of energy&#x2F;watt are low enough to be irrelevant.[1] - https:&#x2F;&#x2F;www.nextbigfuture.com&#x2F;2008&#x2F;03&#x2F;deaths-per-twh-for-all...[2] - https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;death-rates-from-energy-p... reply jurip 3 hours agorootparentAlso, even if that 440 number was accurate, comparing fatalities occurring when random people install solar panels on their rooftops to the decade long massive construction projects that are nuclear power plants is just meaningless. reply r00fus 7 hours agorootparentprevThe problem isn’t due to bubble economics but contractual fixed rates that no longer make sense given post-covid rampant inflation.Higher interest rates make everything more difficult of course - ask the auto or mortgage industries. reply cycomanic 2 hours agorootparentprevThat argument doesn&#x27;t make any sense.If solar profitability is affected by higher interest rates (I&#x27;m not arguing this point) nuclear would be much more effected. Nuclear power is the most capital intensive form of power, they are a huge up front investment with very long ROI (incidentally they almost always require significant price guarantees over for decades and backed by the government to derisk the investment). reply afterburner 5 hours agorootparentprevWhy would high interest rates not be even worse for nuclear, which requires a far larger and earlier initial capital investment? reply jayd16 6 hours agorootparentprevHow is it that nuclear didn&#x27;t benefit from the same bubble? reply eru 4 hours agorootparentMostly red tape. It&#x27;s much tighter regulated, and takes longer to get any project off the ground. reply barney54 7 hours agorootparentprevRiding a bike is cheaper than driving. Driving a car is dead. reply toomuchtodo 7 hours agorootparentChina produces more power from renewables than the entire generation capacity of the UK. You can build more firm generation capacity with solar and batteries than a typical nuclear generator, in a year, at a cheaper price. Hell of a bike.Nuclear has a niche, but it got steamrolled by the S curve of solar, wind, and batteries. reply tuatoru 7 hours agorootparentSlight amendment: China is installing more wind and solar PV generation capacity this year than the entire generation capacity of the UK. The increment, not the total.Edit: Nuclear power generation in China has, IIRC, tripled in the 21st century.But demand has gone up much more than twentyfold though, and this year we are the crossover point where wind an PV manufacturing and install rates, and transmission line build rates, exceed the growth of demand, so fossil plants can start to be retired there.If nuclear can&#x27;t be built fast in China, where can it? reply toomuchtodo 7 hours agorootparentCitation:https:&#x2F;&#x2F;www.theguardian.com&#x2F;business&#x2F;2023&#x2F;nov&#x2F;13&#x2F;chinas-carb...> The most striking growth has been in solar power, according to Myllyvirta. Solar installations increased by 210 gigawatts (GW) this year alone, which is twice the total solar capacity of the US and four times what China added in 2020.> The analysis, which is based on official figures and commercial data, found that China installed 70GW of wind power this year – more than the entire power generation capacity of the UK. It is also expected to add 7GW of hydro power and 3GW of nuclear power capacity this year, said the report. reply csomar 6 hours agorootparentprevIt would be interesting to see what the consequences of that would be. China solar addition scale is from another fourth dimension. Not only it is massive but it seems to be accelerating too. They added more in PV Solar in 2022 than the whole PV installations of Germany. It seems that the politicians there realize that being dependent on unstable regions (ie: Middle-East) for fuel is not wise. They are not B.S.ing their population with green propaganda and instead going full speed for PV solar and electric vehicles.Edit: I found the 2022 numbers on Wikipedia and got curious what the 2023 outlook is going to be. It is shocking: https:&#x2F;&#x2F;www.rystadenergy.com&#x2F;news&#x2F;china-s-solar-capacity-sur... Their 2023 numbers are double the 2022. reply dalyons 6 hours agorootparentprevYou won’t get an answer to this. The energy world has changed very quickly in the past few years, and many can’t adjust their opinions to take new data and trends into account. reply brianaker 4 hours agorootparentprevPrivileged myopia is certainly is alive in thriving!I say this as someone who bikes their kid around most of the time; I love biking but it is way of life that only a minority of people can make work. reply beretguy 7 hours agorootparentprevUnless you are in USA because we like to live each in a separate house instead of apartment complex. reply afterburner 5 hours agorootparentprevTerrible metaphor, obviously. All electricity generating methods are compared in the same way, cost per power generated. reply epistasis 7 hours agorootparentprevElectricity is fungible. reply barney54 7 hours agorootparentBut not easily storable. reply somenameforme 4 hours agorootparentNot entirely true. There&#x27;s all sorts of mechanical storage systems, the most common being artificial hydroelectric and flywheels - spin something really fast and than extract energy from its rotational velocity later.But a practical issue here is that there hasn&#x27;t really been much motivation to develop this industry, because there&#x27;s been relatively little critical necessity for high capacity at-scale energy storage. reply cyberax 7 hours agorootparentprevRiding a bike requires you to live in a shoebox-sized apartment.Shoeboxes are dead. reply civilitty 7 hours agorootparentprevNext gen nuclear was developed fifty years ago for the Mars program before it was cancelled. See nuclear lightbulb research.It’s taken that long for renewables to get marginally cheaper without solving the base load problem one bit. Meanwhile natural gas has dominated both. reply kibwen 7 hours agorootparentYou conveniently remember that nuclear power was unfairly sidelined for decades, and then conveniently forget that solar power was also unfairly sidelined for decades.\"By 1986, the Reagan administration had gutted the research and development budgets for renewable energy at the then-fledgling U.S. Department of Energy (DoE) and eliminated tax breaks for the deployment of wind turbines and solar technologies—recommitting the nation to reliance on cheap but polluting fossil fuels, often from foreign suppliers.\"https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;carter-white-hous...Also, \"marginally cheaper\" can&#x27;t help but sound like sour grapes; the cost-per-watt of solar has fallen by 500x in the past 50 years (https:&#x2F;&#x2F;www.iea.org&#x2F;data-and-statistics&#x2F;charts&#x2F;evolution-of-...). How many hundreds of times cheaper per-watt is nuclear since then? reply reaperman 7 hours agoparentprevI foresee fusion though, ultimately. I think it mostly solves the nuclear waste \"problem\" and should promise huge cheap energy, I&#x27;m excited by some outlandish things which could become feasible if electricity is truly cheap. reply kibwen 7 hours agorootparentI have good news: we have a working nuclear fusion reactor that produces power on the order of 300 septillion watts per second. In their wisdom, its engineers conveniently placed it about 1 AU from the Earth, meaning that all we need to do is catch the energy it&#x27;s currently transmitting to us. reply FabHK 6 hours agorootparentPresumably 300 septillion Watts (=Joule per second).And it&#x27;s true that the sun shines about 100 PW, or 5000x humanity&#x27;s total energy usage (≈20 TW) on the surface of earth on a nice day. reply weberer 1 hour agorootparentprevAnd how would I go about that during Finnish winter? reply quickthrower2 3 hours agorootparentprevWe are doing that more efficiently with higher levels of CO2 reply rgmerk 7 hours agorootparentprevSure, it doesn&#x27;t create long-lived waste, but it&#x27;s a big stretch to assume that fusion reactors will be cheap.Even if you assume that the fusion part is cheap, you still have to convert the energy to electricity or at a minimum mechanical work (unless you&#x27;re using the process heat, which is admittedly a possibility for a fair bit of current demand).Steam turbines aren&#x27;t exactly cheap to build.Yes, there are alternative energy conversion possibilities for some possible fusion fuel cycles, but as I understand it those fuel cycles are much more difficult than D-T fusion and therefore are even further away from being a practical power source. reply acidburnNSA 7 hours agorootparentprevWhat do you consider the biggest remaining challenge associated with the nuclear waste problem? reply xenadu02 4 hours agorootparentActinide burning reactors are IMHO a better path forward. There are various proposals and research reactors, some using alternate fuel cycles.The bottom line is the nastiest stuff in spent nuclear fuel are the actinides. They&#x27;re all naturally radioactive and most of the stuff with long (10,000+ year) half lives that requires long-term storage. You need Yucca Mountain mostly for the actinides. If instead you smash them with fast neutrons you can get them to fission, releasing some energy (though sub-critical) and transmuting them into either much more stable isotopes (little radioactive decay) or really unstable ones (nasty but they decay so fast you only need to hold onto them for a few days&#x2F;months&#x2F;years).The short version is besides reprocessing spent fuel we should be putting the nasty stuff through the sausage grinder again to turn it into much less nasty stuff instead of fighting over where to keep the nasty stuff safe for 100,000 years. reply zdragnar 7 hours agorootparentprevThere aren&#x27;t technical problems, only political ones.Yucca mountain has been designated as the long term storage site since 1987 by Congress. Every change of party majority since then has either stopped or restarted work on it. reply icy_deadposts 4 hours agorootparentprevHanford has been consuming roughly 10% of the deparment of energy&#x27;s annual budget for many years and is projected to continue to do so for decades. and the cleanup effort has been fraught with scary mishaps. reply reaperman 7 hours agorootparentprevPersonally, I think Yucca mountain is a great place to store it in the USA but that will likely never be politically viable. And I&#x27;m not sure what solutions exist for nations outside of North America.I&#x27;m also personally still concerned about transportation accidents resulting in permanently contaminated areas. reply acidburnNSA 7 hours agorootparentOutside the US, Finland&#x27;s Onkalo repository is on track to be the first operational commercial repository [1]. We have some operating military ones, like WIPP already.[1] https:&#x2F;&#x2F;www.posiva.fi&#x2F;en&#x2F;index.htmlAs for transportation, this has been a big concern for some time but has pretty rock-solid technical solutions [2]. We regularly ship high-level nuclear waste around (e.g. from nuclear submarines to Hanford) and haven&#x27;t really had any incidents. I&#x27;d claim that the issue is basically solved with these robust containers.[2] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?vW=hlextDSoVkQ reply eru 4 hours agorootparentFor comparison, we have chemicals like mercury (and much worse) that stay dangerous forever, and we mostly handle them just fine.There&#x27;s only a tiny amount of high level nuclear waste. reply7373737373 2 hours agoparentprevYou are not factoring in resiliency reply candiddevmike 7 hours agoparentprevNuScale is the company to watch here. Modular reactors are the future. reply acidburnNSA 7 hours agorootparentSadly they&#x27;re in very rough shape. https:&#x2F;&#x2F;www.msn.com&#x2F;en-us&#x2F;money&#x2F;markets&#x2F;nuscale-shares-plung... reply candiddevmike 7 hours agorootparentAFAIK they have quite a few MOUs with other utilities, may not be down for the count yet. Seems like the US government is all in on SMRs, so they could end up subsidizing... reply RNAlfons 18 minutes agorootparentThey have other problems too: https:&#x2F;&#x2F;www.barrons.com&#x2F;articles&#x2F;nuscale-power-corp-investig... reply breckenedge 7 hours agorootparentprevI doubt we will see federal subsidization for nuclear in the near term with the GOP poised to take over the Senate and House still up for grabs. It’ll be “drill-baby-drill.” reply darklycan51 7 hours agoparentprevBuilding reactors TODAY would take way longer than we have to hit net neutrality let alone reducing anything.China built as much solar as the US has entirely last year alone, it can be done reply vuln 4 hours agorootparentWith what workforce? reply matheusmoreira 7 hours agoparentprevI agree with you about windmills, they just waste way too much useful space. I don&#x27;t think solar panels are a problem though. Massive solar plants make no sense but they&#x27;re great roof replacements for homes and provide distributed energy generation. reply ceejayoz 7 hours agorootparentWindmills use tiny amounts of land. Drive through Indiana and they’ll be scattered about within cornfields; drive through Pennsylvania and they’ll be on otherwise unproductive hillsides. In both cases using like 20x20 foot pedestals. reply matheusmoreira 7 hours agorootparentWind turbines require a lot of space between each other. Any dedicated wind farm is going to have a ton of empty space.If you just plant a single wind turbine in some unused space of an otherwise productive piece of land you can obviously derive value from it. That&#x27;s not what I was talking about though. I was talking about massive wind farm power generation operations. There&#x27;s better uses for that area. Same logic applies to just laying massive amounts of solar panels over a wide area. Better to put them on everyone&#x27;s roofs. reply ceejayoz 6 hours agorootparent> I was talking about massive wind farm power generation operations.Which are almost universally placed in said farm fields, unproductive hillsides, and open ocean. No one&#x27;s clearing city blocks to place a wind turbine. These \"massive wind farm power generation operations\" already exist in the middle of Indiana&#x27;s cornfields. Drive from Chicago to Cincinnati and you&#x27;ll quickly see how little land they can eat up.Concrete example: https:&#x2F;&#x2F;www.google.com&#x2F;maps&#x2F;place&#x2F;40%C2%B035&#x27;55.2%22N+87%C2%...> Same logic applies to just laying massive amounts of solar panels over a wide area.Same response; they only make financial sense to place on low-value land. A solar farm will be very happy on shitty land that&#x27;s not particularly suitable for farming, grazing, or habitation. They can even benefit farmland in some climates; https:&#x2F;&#x2F;news.cornell.edu&#x2F;stories&#x2F;2023&#x2F;03&#x2F;made-shade-growing-.... reply dalyons 6 hours agorootparentprevDo you know how much empty unproductive space there is in the world? It’s an awful lot more than you think.Putting them on peoples roofs is actually a terrible option from a GW&#x2F;$ equation due to install costs, and lack of operational scale. reply eru 4 hours agorootparentRooftop installation is also where the majority of accidents associated with solar power happen. reply tstrimple 6 hours agorootparentprev> Any dedicated wind farm is going to have a ton of empty space.You seem to have a funny definition of \"empty space\" when the post you&#x27;re replying to explicitly called out the uses for land in between wind turbines or leveraging wind turbines in areas where much of the land isn&#x27;t usable for much else. If your argument requires completely ignoring points made by someone else, you probably don&#x27;t have very strong of an argument. reply dalyons 7 hours agorootparentprevPlease explain how we are running out of space for windmills, especially offshore ones? Or why solar plants make no sense reply photochemsyn 8 hours agoprevThe opening sentence is rather inaccurate: \"The 93 currently active nuclear-power reactors in the United States burn about 2,000 tonnes of uranium fuel each year.\"Isn&#x27;t not &#x27;burnt&#x27; by any reasonable interpretation of the word, what happens is that the fuel becomes too hot to handle in any of today&#x27;s working reactors because of the buildup of radioactive fission products as well as transuranic species. If the fuel rods were left in the reactor, they&#x27;d probably swell and eventually block circulation of the primary coolant, and the high radioactive flux would have other negative effects on the reactor core material itself (as well as on any workers that had to fix the mess).This is well known and many proposals have been made to recover this energy from the &#x27;spent&#x27; &#x27;burnt&#x27; fuel rods, but it&#x27;s mostly been an expensive mess. Reprocessing to recover the transuranics for use in MOX (plutonium-based fuel, basically) is really expensive as well as generating large amounts of highly toxic waste as well as raising the plutonium-nuclear-weapon proliferation issue.Instead, this 2000 tons per year represents an ever-increasing long-term headache that generally has to be stored onsight for about a decade at the nuclear power plant before it can be transferred to long-term dry cask storage, all at considerable expense and with high security costs.With costs for solar&#x2F;wind&#x2F;storage dropping every year, isn&#x27;t it time to give up on this pipe dream? reply Manuel_D 8 hours agoparent\"burnt\" is a pretty intuitive way of saying the usable energy in the fuel is released. It&#x27;s a nuclear reaction, not a chemical reaction like combustion, but it&#x27;s a pretty descriptive term that&#x27;s also used inside of the nuclear industry itself (I heard a talk from a nuclear engineer about rotating fuel rods to achieve an \"even burn\", in his own words).Storage remains a big barrier to wide-scale deployment of intermittent power sources in a manner truly independent of a fossil fuel grid. While costs may be dropping, scale and competition with electric vehicles means grid storage isn&#x27;t growing all that quickly. Many solar plants that are employing storage are only installing 2-4 hours worth of storage, which isn&#x27;t sufficient to even out the duck curve let alone seasonal variations. Wind also has big swings in seasonal production. Even with significant overproduction, most grids are still looking at several days worth of storage to account for a string a cloudy or less-windy days.Another factor is that wind and solar have significantly different output depending on geography and weather. Nuclear&#x27;s only geographic requirement is a source of water for cooling - and that doesn&#x27;t have to be potable water, it can be salt water or wastewater. And since human beings need water, too, this is rarely an impediment to deploying nuclear power plants near the population centers that use the most power. By contrast, wind and solar often need to be produced hundreds of miles away from population centers. This tends to overload the transmission capacity of rural power infrastructure [1], sometimes halting wind and solar projects.1. https:&#x2F;&#x2F;www.vox.com&#x2F;videos&#x2F;22685707&#x2F;climate-change-clean-ene... reply photochemsyn 7 hours agorootparentI&#x27;d go with &#x27;transmutation&#x27; - &#x27;burn&#x27; is simply used to imply that using uranium is no different than using wood&#x2F;oil&#x2F;coal&#x2F;gas as a fuel, but that&#x27;s completely unscientific. Burning implies an redox process at the very least, involving an electron acceptor like oxygen mixed with an electron donor like a hydrocarbon and resulting in the destruction of some molecules and the creation of other molecules (e.g. CH4 + 2O2 -> CO2 + 2H2O).It&#x27;s just yet another example of how the nuclear industry tries to spin its activies as safe and harmless. They probably spend as much money on PR these days as they do on R & D. reply JumpCrisscross 6 hours agorootparent> &#x27;burn&#x27; is simply used to imply that using uranium is no different than using wood&#x2F;oil&#x2F;coal&#x2F;gas as a fuel, but that&#x27;s completely unscientificStars \"burn\" their fuel. This usage is deeply precedented in astrophysics. reply tekla 7 hours agorootparentprevNo it doesn&#x27;t. That would be &#x27;combustion&#x27;. Burning is a loose term.The IEAA says so -> https:&#x2F;&#x2F;www.iaea.org&#x2F;publications&#x2F;10959&#x2F;high-burnup-fuel-imp...Nuclear physics says so -> https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Carbon-burning_processThe term gets used in rocketry, and not all rockets use combustion (doing a burn with a cold gas thruster)Hey, might as well get into CD&#x27;s -> https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Optical_disc_authoringWhy is it that the entire industry uses the term just fine, but not you? Because you&#x27;re wrong. reply photochemsyn 7 hours agorootparentPower production via uranium (or other related heavy element) fission is simply unlike any other process. It&#x27;s basically a proximity effect, as if you place the fuel rods close enough to one another with water in between them, you initiate the process. The U-235 fuel is continually decaying, releasing neutrons. In the context of enriched fuel rods in the proper orientation in a watery environment, this means other U-235 nuclei will absorb such neutrons, becoming unstable and splitting into two smaller nuclei, known as fission fragments - of a remarkably wide variety, many of them unstable and generating further nuclear reactions. Along with the fragments, several fast neutrons and a significant amount of energy are released. The water in the reactor acts as a moderator, slowing down the fast neutrons released in the fission process. Slower neutrons are more likely to cause subsequent fissions in other uranium-235 nuclei, sustaining the chain reaction.Now, the fission fragments are highly radioactive and unstable isotopes of lighter elements. These fragments undergo beta and gamma decay, emitting high-energy particles and photons, transforming into stable isotopes over time. Furthermore, some neutrons, instead of inducing fission in uranium-235, are absorbed by uranium-238 nuclei, typically actinides, like plutonium and neptunium.So this ridiculously toxic brew of rather random radioactive heavy elements is what we call a &#x27;spent fuel rod&#x27;. If you were to pack a few kilos of high explosive around one of these &#x27;spent&#x27; fuel rods and detonate it in a city center, it would probably cost at least a billion dollars to clean up, if that.It sounds kind of ridiculous when you can build 1 GW of reliable 24&#x2F;7 wind&#x2F;solar&#x2F;storage power generation for the same cost as a 1 GW nuclear power plant, and then you never have to buy fuel or deal with the waste stream ever, doesn&#x27;t it? reply Manuel_D 6 hours agorootparentThe amount of spent fuel produced by US nuclear electric grid generation fits in a volume the footprint of a football field and less than 10 yards high: https:&#x2F;&#x2F;www.energy.gov&#x2F;ne&#x2F;articles&#x2F;5-fast-facts-about-spent-... We could easily do what Finland and other countries are already doing: bury nuclear waste in bedrock.Using spent nuclear fuel as a terror weapon would be vastly less deadly than more conventional means. Think about the immense effort of planning and manpower to steal nuclear waste, weaponize it, and then deliver that weapon to a population center. Now think about all that planning and manpower going towards a more conventional attack, like flying a plane into a building or detonating a normal explosive bomb.Nuclear waste can be deadly, but it&#x27;s a slow killer. We regularly test our water supply for uranium not out of any fear of a dirty bomb, but because naturally occurring uranium is common enough that it&#x27;s a problem: https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC359425&#x2F;In short, fears over nuclear waste are incredibly contrived. reply photochemsyn 5 hours agorootparent\"The amount of spent fuel produced by US nuclear electric grid generation fits in a volume the footprint of a football field and less than 10 yards high\"... and if properly distributed, is enough to kill every human being on the planet.Which proves the ridiculousness of such comparisons, i.e. enough bullets were produced in World War Two to wipe out the global human population too. replyHPsquared 8 hours agoparentprev2000 tons per year for the whole country though. 20 tons per reactor per year doesn&#x27;t sound too bad.Compare to the approx 1 billion tons of oil consumption per year, or 4 billion tons of CO2 emissions per year that the rest of the economy burns through, to get a sense of relative scale. reply wkat4242 8 hours agorootparent> 20 tons per reactor per year doesn&#x27;t sound too bad.Not great, not terrible :)It is super radioactive for centuries though. We really really need to get the long-term storage worked out. reply Moldoteck 2 hours agorootparentOr just build reprocessing plants like france, where abt 90% is sent back to the reactors. The remaining real waste is solidified to be stored more easily. Or just build breeding reactors that perform this by default (except solidification, for that we still need vitrification). Solidified waste is dangerous for abt 300 yrs reply ethbr1 7 hours agorootparentprev> We really really need to get the long-term storage worked out.We did.And there&#x27;s $52+ billion in the Nuclear Waste Fund.Nevada is just really good at complaining. reply acidburnNSA 7 hours agorootparentExactly. We have the money and technology to build and operate repositories that responsibly handle all our nuclear waste. We just need the willpower to go out and do it. reply eru 4 hours agorootparentOr just give some other country money to do it for you?Finland might do it? (They are also sophisticated and stable enough, that you don&#x27;t need to worry about the fuel being mishandled. At least not any more than for the US.) reply ekianjo 6 hours agorootparentprevThere is a lot of space to store that small amount of product underground. Thats a non issue. reply cyberax 7 hours agorootparentprev> If the fuel rods were left in the reactor, they&#x27;d probably swellFun fact: nuclear waste becomes less radioactive than the original uranium in about 300-500 years. The problem with the waste is that it&#x27;s concentrated in a tiny amount of space that will stay dangerous for geological periods. reply acidburnNSA 7 hours agorootparentThat with full recycling, including partitioning and transmutation. In a once-through cycle it takes more like 10,000 years.https:&#x2F;&#x2F;www.iaea.org&#x2F;publications&#x2F;7112&#x2F;implications-of-parti...Edit: I&#x27;m a nuclear engineer. reply cyberax 7 hours agorootparentIt might be using different measures of radioactivity (e.g. Curies instead of Watts). I did calculations myself long ago, while studying nuclear power in a university, and it indeed was 300-500 years depending on fuel type.I&#x27;ll try to re-do calculations and see what I get. reply acidburnNSA 7 hours agorootparentI&#x27;m a professional nuclear engineer (Ph.D. & PE) with 20 years of experience in advanced reactors and fuel cycle. I promise you it&#x27;s not 300-500 yrs unless you do full recycle with minor actinide partitioning and transmutation.The publication above shows the graph vs. time in terms of radiotoxicity, which is widely considered to be the key metric.What&#x27;d you use for the calculations? Generally you need to use something like ORIGEN to get it right. reply cyberax 6 hours agorootparentPerhaps I was a bit unclear. The waste will _definitely_ be dangerously radioactive for tens of thousands of years. No questions here.But in aggregate, it will be less radioactive than the uranium (and its natural daughter products in secular equilibrium) that was expended to produce the waste. reply acidburnNSA 6 hours agorootparentWe&#x27;re aligned on that metric. I&#x27;m using the &#x27;less radioactive than the ore from which it was mined&#x27; as the metric here as well. It&#x27;s between 10,000 and 100,000 years. reply eru 4 hours agorootparentThough couldn&#x27;t you get under &#x27;less radioactive than the ore from which it was mined&#x27; by just mixing your waste with a big enough quantity of eg ocean water? replyalex_young 5 hours agorootparentprevDon&#x27;t most of these techniques wind up producing a fair amount of highly enriched products which could easily be used for nuclear weapons? Seems like a big downside at the scales we&#x27;re talking about. reply acidburnNSA 5 hours agorootparentThat&#x27;s certainly what Jimmy Carter was worried about when he cancelled the USA&#x27;s national breeder reactor project, the Clinch River Breeder. He then banned reprocessing entirely, but Reagan unbanned it in 1981, so it is at least legal now.This has been an ongoing debate though. Scientists have developed more sophisticated reprocessing processes that keep all the actinides together rather than pulling out Plutonium on its own. And in any case, the plutonium is reactor-grade rather than weapons grade, (but super-sophisticated weapons designers can make bombs out of any-grade plutonium).George Bush Jr. tried something called the Global Nuclear Energy Partnership, where existing weapons states would run a bunch of reprocessing plants and contribute fuel to an international fuel bank. Non-weapons countries could reliably get fuel from it and give the spent fuel (aka nuclear waste) back for further recycling&#x2F;reprocessing&#x2F;treatment. It wasn&#x27;t super popular among the &#x27;have not&#x27; countries, who didn&#x27;t want to be beholden to some random fuel bank. reply peter422 7 hours agorootparentprevThe CO2 released by fossil fuels also can have century spanning impacts. reply Turing_Machine 7 hours agorootparentprevRadioactive, yes. \"Super\" radioactive, no.Longer half-life -> less radioactive, by definition. Infinite half-life -> not radioactive at all. reply wkat4242 6 hours agorootparentThat is an oversimplification, it disregards a few things: the concentration of radioisotopes (which is extremely high in spent fuel) and type of decay products (which each have their own half lives, in many cases the radioactivity goes up for some time after the initial decay while decay products break down and eventually reach a stable isotope) and the type of emissions.Also containers that don&#x27;t break down under centuries of ionising radiation bombardment haven&#x27;t been invented yet. reply ars 3 hours agorootparentprev> It is super radioactive for centuries though.It doesn&#x27;t have to be that way. With nuclear fuel reprocessing you can burn all the highly radioactive stuff completely, and what&#x27;s left either decays quickly or isn&#x27;t very radioactive. reply photochemsyn 1 hour agorootparentprevAnd you didn&#x27;t compare that to the costs of maintaining a solar&#x2F;wind&#x2F;storage system, for some odd reason... these PR lines are getting pretty stale. reply cyberax 7 hours agoparentprev> Isn&#x27;t not &#x27;burnt&#x27; by any reasonable interpretation of the wordIt actually is. Spent nuclear fuel goes from 3-5% of U-235 at the start to 0.3% at the end of the fuel campaign. There will also be some fissile plutonium, but not much.Having just 1&#x2F;10-th of initial fuel left definitely qualifies as \"burnt\".That&#x27;s also why reprocessing isn&#x27;t economical right now. You just won&#x27;t get that much new fuel.> If the fuel rods were left in the reactor, they&#x27;d probably swellThat can&#x27;t physically happen. Nuclear fuel is made of tablets of sintered oxide powders, and they are permeable to gases (helium is the main one produced as a result of fission). reply irfwashere 7 hours agoparentprevI highly recommend Doomberg&#x27;s substack for energy related analysis. His article \"Angels on a Pin\" explains how storing nuclear waste is not as scary as people make it out to be.Direct link here: https:&#x2F;&#x2F;doomberg.substack.com&#x2F;p&#x2F;angels-on-a-pin reply whatshisface 7 hours agorootparentCongress (verbally) fried GM because stories like that hurt the car industry unless people believe they are being dealt with, not because 100 deaths is a large number. reply necheffa 6 hours agoparentprev> Isn&#x27;t not &#x27;burnt&#x27; by any reasonable interpretation of the wordFYI, this is a pretty typical industry term. Even to the point where \"burn up\" is a unit of measure tracked at operation and within model predictions. reply prpl 8 hours agoparentprevIAEA even used the word burn to denote a nuclear process. Just because it doesn’t meet your definition doesn’t mean it’s not colloquially used within the field reply Moldoteck 3 hours agoparentprevDidn&#x27;t France have a plant specifically for this? And real waste was vitrified to store with less headaches?Anyway, too bad we have too few breeding reactors reply M3L0NM4N 8 hours agoparentprevAre you saying nuclear is a pipe dream or reprocessing? reply ekianjo 6 hours agoparentprev> With costs for solar&#x2F;wind&#x2F;storage dropping every year, isn&#x27;t it time to give up on this pipe dream?There is no beating the energy density of uranium. reply eru 4 hours agorootparentWell, but the power plant you need to unlock that energy is pretty big. reply gustavus 8 hours agoparentprev> With costs for solar&#x2F;wind&#x2F;storage dropping every year, isn&#x27;t it time to give up on this pipe dream?When will we give up the pipedream that it&#x27;s always going to be sunny and windy, especially as climate change continues to exacerbate destructive weather patterns.And when will we give up the pipedream that everywhere in the world is sunny and windy all the time to. reply rgmerk 7 hours agorootparentThe question is whether one or more of the seasonal storage technologies on the horizon, when combined with the seasonal storag",
    "originSummary": [
      "Centrus Energy has achieved a significant milestone by successfully producing its initial batch of uranium fuel.",
      "This accomplishment signifies the return of the United States to the nuclear fuel industry.",
      "The development is particularly significant for advanced reactors, highlighting the importance of this achievement."
    ],
    "commentSummary": [
      "The discussion revolves around multiple facets of nuclear energy, such as the US's reentry into the nuclear fuel market and safety concerns.",
      "It also includes a cost comparison between nuclear energy and coal, as well as the possibility of reprocessing spent fuel.",
      "The conversation delves into the challenges, limitations, advantages, and disadvantages of nuclear energy, including its integration with batteries in the power grid and China's adoption of renewables."
    ],
    "points": 260,
    "commentCount": 249,
    "retryCount": 0,
    "time": 1700010735
  },
  {
    "id": 38266340,
    "title": "Rivian software update causes unresponsive infotainment system, fix elusive",
    "originLink": "https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/",
    "originBody": "Rivian software update bricks infotainment system, fix not obvious Seth Weintraub Nov 14 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38266340",
    "commentBody": "Rivian software update bricks infotainment system, fix not obviousHacker NewspastloginRivian software update bricks infotainment system, fix not obvious (electrek.co) 244 points by carlivar 16 hours ago| hidepastfavorite340 comments latchkey 12 hours agoI built a whole remote software update mechanism for a control binary that ran on 25k+ servers across multiple data centers.Rest assured that after the first time I messed it up (which required ssh into each box individually), I wrote a lot of unit and integration tests to make sure that it never failed to deploy again. One of the integration tests ensured that the app started up and could always go through the internal auto update process. This ran in CI and would fail the build if it didn&#x27;t pass.While I fully understand that this is hard to get right 100% of the time, a mess up of this level by a car manufacturer is pretty amazing to me. reply foobiekr 9 hours agoparentRivian is an embedded use case, though, which is not at all like a fleet of servers.Having worked for companies that produce network devices - including devices that are unreachable for example for 6 months of the year - and on software installation and upgrade, I am baffled how this bricking is possible. For one thing, you generally use some kind of confirmed boot mechanism - you upgrade a standby partition, set an ephemeral boot value that causes device to boot the alternate image, and reboot - only when the image is declared \"up\" does that get persisted (and then the alternate is upgraded, in order to prevent rollback in the event of a media error). You use watchdogs that are tied to actual forward progress (and not just some demon that the kernel schedules and bangs on the watchdog even if the rest of the system is hung) and if they fail, the WD reboots you. (This is one of the reasons that event driven programming is somewhat preferred - actually processing events from a single dispatch thread makes it easier to reason about the system.)On top of that, you make sure that the core system is an immutable filesystem so that you can validate the _offline_ alternate image before rebooting (write-and-read-back-uncached) and periodically scrub the alternate image (same).Like.. this is all embedded 101, stuff people have been widely doing since the mid 1990s and I think I can find examples going back to the 70s. Sometimes you get a little more sophisticated (allow sub-packages or overlays and use a manifest to check the ensemble instead of just a single image), but it&#x27;s very standard. reply dcow 1 hour agorootparentAssuming Rivian does know embedded 101, my guess is that the infotainment system is running Android and the watchdog reported all green once the system services all came online and that it doesn&#x27;t actually check whether the application layer is really working because, as you know, that would require the watchdog to run a full regression suite before giving the okay, which isn’t practical. Since the update swapped the system to an internal dev cert, they cant push an immediate update to change the boot args because the management plane daemon won’t connect to the C&C server, or it can but the blob they push wouldn’t pass signature validation, or the TEE won’t unlock the device keys because the roots changed. Whatever the case, someone has to go blow a fuse and re-flash the thing, or at least rewrite the boot args via serial. Just a guess.If it is the most likely “management plane TLS certs” issue, I bet the watchdog won’t confirm the new boot args until the command dispatch daemon gets a pong from the C&C server moving forward (: reply KingMachiavelli 1 hour agorootparentprevDid you just use standard Yocto or similar tools to build such images? Are there standard daemons for managing hardware watchdogs (besides systemd since that&#x27;s too simple as you say)? I think there&#x27;s a lot of niche knowledge in the embedded space and many programmers are used to cloud systems and at most target. The most embedded experience most programmers have is likely iOS&#x2F;Android development where all of the actual embedded concerns are handled for you. Even Google (soft)bricked a bunch of phones with the latest Android 14 update [1].IMO there&#x27;s not a lot of regular OSS for building embedded systems that comes with A&#x2F;B partitioning, watchdogs, secure and verified boot - it&#x27;s all custom at every org and tailored for individual products.[1] https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2023&#x2F;11&#x2F;android-14-patches-r... reply MarkSweep 38 minutes agorootparentI quit my job before I got to deploy this, but RAUC looked like it would handle this for Yocto:https:&#x2F;&#x2F;github.com&#x2F;rauc&#x2F;rauc https:&#x2F;&#x2F;github.com&#x2F;rauc&#x2F;meta-raucFor microcontrollers, Memfault had a good article:https:&#x2F;&#x2F;interrupt.memfault.com&#x2F;blog&#x2F;device-firmware-update-c... reply ikiris 9 hours agorootparentprevThat sounds out of scope for the MVP. We can worry about redundancies later after we ship. reply roland35 9 hours agorootparentHey now, preventing SEVs doesn&#x27;t lead to impact. If we all collectively let this become a raging dumpster fire we can all heroically fix it and greatly exceed expectations for the half. reply bluGill 8 hours agorootparentYou too have noticed most great employee rewards go to someone who if they had done their job well wouldn&#x27;t have been notived reply ikiris 8 hours agorootparentprevThis guy FANGs reply aaronbeekay 11 hours agoparentprevAs somebody currently working at an automaker on software systems, the amazing thing to me is that a mess up of this level doesn’t happen weekly. It’s rough out here. reply jacquesm 4 hours agorootparentThank you. At least you&#x27;re honest about it, the other day someone was trying real hard to convince me that software developers at automakers are made of magic fairy dust. reply kalleboo 2 hours agorootparentI&#x27;m amazed anyone would argue that after the Toyota firmware analysis. reply bozhark 11 hours agorootparentprevWhat&#x27;s the priority then, telemetry data? Why is it rough out there? reply jacquesm 4 hours agorootparentRelatively crappy pay, complex toolchains, long build times, layer upon layer of (really bad) legacy code, badly specified (if they&#x27;re specified) protocols between subsystems, subsystems that are completely opaque (no source code provided), homegrown OS&#x27;s or older RTOS&#x27;s, subset-of-C to keep it safe(r), tricky debugging environments and if you&#x27;re really unlucky anemic hardware.I hope I didn&#x27;t miss anything but I wouldn&#x27;t be surprised if I did. reply ahartmetz 1 hour agorootparentYeah, I think you missed something. The \"software architects, heavy enterprisey tooling, and minions\" approach to development where some of the architects could be good developers, but they don&#x27;t develop, and the minions are often not that good and also not given any autonomy, so they are in a state of learned helplessness and just do what they&#x27;re told without much thinking or initiative. It results in over-abstracted, over-complicated, slow, unreliable, and sometimes just stupid code. reply reactordev 10 hours agorootparentprevProbably due to fires, failures, and fatigue. reply firtoz 4 hours agorootparentGames have AAA, autos have FFF reply foobiekr 10 hours agorootparentprevdo you guys not have confirmed boot and swizzling to fallback images? reply AlotOfReading 9 hours agorootparentAutomotive varies widely between \"basically modern Linux systems with proper updates\" and the most janky, home-grown update systems imaginable, sometimes even within the same components and teams. reply foobiekr 9 hours agorootparentYah, I know from friends at ford and vw that there&#x27;s still vxworks and qnx, but even there, good grief, a-b with confirmed boot is about as basic as you can get.I confess I&#x27;ve seen incredible sloppiness about when a confirmation is done (too early, including in the initial init stages which is way too soon) and watchdogs (spawn off a process that has a while loop stroking the wd - just absolutely pointless). reply cozzyd 7 hours agorootparentI&#x27;ve seen kicking and petting the watchdog, but this is my first time seeing stroking reply WWLink 4 hours agorootparentI&#x27;ve heard all of the above, often \"stroking\". I never used those because I like systems where you have a random challenge code to respond to. Then the software has to not be acting as wonky to react correctly. reply LoganDark 5 hours agorootparentprevSometimes the watchdog needs to have fun too, you know. reply ahartmetz 1 hour agorootparentprevFrom experience, QNX is actually very nice. I wouldn&#x27;t say \"still using QNX\" like it&#x27;s some crap that nobody would want. replycjbprime 7 hours agoparentprev> This ran in CI and would fail the build if it didn&#x27;t pass.I don&#x27;t mean to be pedantic, but since we&#x27;re talking about what should happen instead, this is insufficient. It works until the day you realize you made some kind of manual change to your CI infra, or that CI has some non-standard configuration that makes it work for you but not some significant fraction of the fleet.People should do what you described in CI, but as well as that, you need phased rollout, where e.g. the build can only be rolled out to the next percentage point of randomly selected users in a specific segment (e.g. each hardware revision and country as independent segments) after meeting a ratio of successful check-ins, in the field, from the new build by production customers in that segment. That&#x27;s the actual metric for proceeding with the rollout: actual customers are successfully checking in from the new version of the software.Except, that&#x27;s actually not sufficient either. What if the new build is good, but it contains an update to the updater which bricks the updater? Now you&#x27;re getting successful check-ins from the new version in the field, but none of those customers will ever successfully auto-update again. So, test the new updater&#x27;s ability to go forwards successfully, too. reply quailfarmer 2 hours agorootparentA good way to handle the who-updates-the-updater issue is to use a triple partition updater. A updates B, and then B updates C, then C updates A. If anything about the new version prevents it from properly updating its neighbor, that neighbor won&#x27;t be able to close the loop, and you&#x27;ll fall back to A. This simplifies the FSBL, because it just boots the three partitions in a loop, no failure detection required. You don&#x27;t need to triplicate the full application either, just the minimum system needed to perform an update, and then have the \"application\" in it&#x27;s own partition to be called by the updater. reply latchkey 6 hours agorootparentprev> It works until the day you realize you made some kind of manual change to your CI infra, or that CI has some non-standard configuration that makes it work for you but not some significant fraction of the fleet.Nah, my CI process was solid. This was proven in the field over the course of years.> I don&#x27;t mean to be pedantic... you need phased rolloutYou don&#x27;t need to be pedantic, but better to ask the question rather than assume that was all that I did. =) You have to realize that what I built, worked flawlessly. It wasn&#x27;t easy either, took a lot of trial and error.I did have a CIDR based rollout. I could specify down to the individual box that it would run a specific version. Or I could write \"latest\" to always keep certain boxes running on the latest build. This was another part of my testing, but ended up not being fully necessary because I had enough automated testing in CI that \"latest\" always worked.> but it contains an update to the updater which bricks the updater?This happened, so I wrote a lot of test code to make sure that would never happen again. My CI would catch that since I was E2E testing that it could actually run the upgrade process.Once I implemented all of this, I never had a single failure and would routinely, several times a day, deploy to the entire cluster, over the course of a couple years.It was all eventually consistent as I could also control the \"check for update\" frequency as well. reply cjbprime 4 hours agorootparentI think there&#x27;s a minor confusion here, where you think the purpose of my response involves doubting whether your system was successful. I understand it was successful. My response is to the sense in which your comment can be interpreted as advice to other people on what they should build.I think the fact that you were able to survive with CI-only doesn&#x27;t mean that we should encourage others to skip implementing a phased rollout based on verified customer successes, including testing of their new updaters before the first time they accidentally brick all the updaters, rather than afterwards. That&#x27;s what I was hoping to help avoid, through my comment. reply jacquesm 4 hours agorootparentprevAnd you need to verify the vehicle is not in motion. reply psychlops 10 hours agoparentprevHaving worked on 25K machines, I can assure you that it never deployed to every single machine and failed to do so in interesting ways all the time. reply latchkey 6 hours agorootparentIt always deployed. It was eventually consistent. Any failure would automatically be resolved after a period of time. reply postalrat 7 hours agorootparentprevAs a frontend web developer I&#x27;m constantly deploying software to many thousands of machines. And you know what? It&#x27;s pretty damn simple. reply drdaeman 4 hours agorootparentI used to wear your shoes in IE6&#x2F;7 ages (no longer, I gave up during the \"framework of the week\" race and went all-backend), and it wasn&#x27;t simple at all. Browser compatibility with all their rendering nuances, individual system oddities and all sort of fragile stuff.And fortunately, no one bats an eye at a slightly broken site, but everyone hates even a slightly broken vehicle. reply jrumbut 4 hours agorootparentprevIt&#x27;s simple because we tolerate certain limitations in the web platform.If you had a hard requirement that a page load could never take more than 100ms, regardless of network conditions, you&#x27;d have quite a challenge on your hands. reply onion2k 2 hours agorootparentThe laws of physics are definitely very challenging. If you&#x27;ve got a solution please write a blog post. reply jrumbut 2 hours agorootparentNo blog post required! You just install the whole app on a dedicated piece of hardware on site.But then deployment becomes more challenging ;) reply onion2k 2 hours agorootparentprevI&#x27;m not really a frontend dev any more but I was for a long time. I can assure you that the only reason you think your code works is because no one tells you it&#x27;s broken. If you use an error logging or telemetry service (Sentry, Rollbar, New Relic, etc) you will be aware that errors happen in frontend code all the time. It&#x27;s just that most of the time bugs don&#x27;t crash the app, and the user doesn&#x27;t know what to expect so they see a broken feature and think it&#x27;s meant to be like that. reply donmcronald 11 hours agoparentprev> While I fully understand that this is hard to get right 100% of the time, a mess up of this level by a car manufacturer is pretty amazing to me.I feel like it&#x27;s going to happen to someone that makes network devices eventually. I&#x27;m always scared to update my (several hundred) UniFi devices. Their update process isn&#x27;t foolproof and they push auto-updates via the UI pretty hard.Several years ago they caused some people&#x27;s devices to disconnect from the management controller when they enabled &#x27;https&#x27; communication. Prior to that, if you were pointing devices at &#x27;https:&#x2F;&#x2F;example.com:8080...&#x27; they would ignore the &#x27;https&#x27; part and do an &#x27;http&#x27; request to port &#x27;8080&#x27;. Then they pushed their &#x27;https&#x27; update which expected an &#x27;https&#x27; connection and didn&#x27;t fall back to the old behavior for anyone that was mistakenly using &#x27;https&#x27; in their URL initially. Some people on their forums complained about having to manually SSH to every device to fix the issue.It was caused by an end-user mistake, but they knew it was a potential issue. AFAIK, their attitude on it hasn&#x27;t changed and a lot and at the time their response was that they knew it would break some people, but that it wouldn&#x27;t be that many (lol).IMO, the issue with those systems is that basic communication back to the update &#x2F; config server is part of the total package which is too complex (ie: a full Debian install). I&#x27;d rather see something like Mender (mender.io) where the core communications &#x2F; updates come from a hardened system with watchdog, recovery, rollback logic.Think of how crazy it is to have something like pfSense doing package based updates rather than slice based updates. At least with boot environments they could add some watchdog and rollback type logic, but it&#x27;ll still be part of the total system instead of something like a hardened slice based setup where the most critical logic is isolated from everything else and treated like a princess.Do you have any insight on package vs slice based systems for updates? Did you isolate update logic from the rest of the system or am I out of touch with that opinion? reply akira2501 9 hours agoparentprevWhen possible, I used a fail back mechanism. If the update failed to fully come up, then the watchdog timer would catch it, the bootloader would notice the incomplete boot, and attempt to boot from the previous known working image in that case. reply code_runner 11 hours agoparentprevout of morbid curiosity.... how long did it take to ssh into and fix all of those servers? I imagine even automating a fix (if possible) would still take a good amount of time. reply latchkey 11 hours agorootparentgnu parallel and sshpass is your friend.The way I built my app was that I could install it cleanly via a curlbash.So, I just had a simple shell script that iterated through the list of IP addresses (from the DHCP leases), ran curlbash and that cleaned up the mess pretty quickly. reply jdechko 9 hours agoparentprevAs a non-developer, the whole situation with a bad software update to the Voyager spacecraft really puts things into perspective as far as how bad remote updates can be.It’s also a testament to the way that the system was designed that they were able to get it back online. reply ugh123 1 hour agoparentprevPlease tell me you scripted that ssh into across your 25k servers! reply latchkey 1 hour agorootparenthttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38270986One thing my little control process did on the box was to always set the password to be the same... user&#x2F;1.None of these boxes needed inbound connections, so it wasn&#x27;t a big deal to do that. reply gravitronic 16 hours agoprevI used to work for a company that built satellite receivers that would be installed in all sorts of weird remote environments in order to pull radio or tv from satellite and rebroadcast locally.If we pushed a broken update it might mean someone from the radio company would have to make a trip to go pull the device and send it to us physically.Our upgrader did not run as root, but one time we had to move a file as root.. so I had to figure out a way to exploit our machine reliably from a local user, gain root, and move the file out of the way. We&#x27;d then deploy this over the satellite head end and N remote units would receive and run the upgrade autonomously. Fun stuff.Turns out we had a separate process running that listened on a local socket and would run any command it received as root. Nobody remembered building or releasing it but it made my work quick. reply singleshot_ 12 hours agoparentThe person who built and released this might not have ever worked for your company, which might be why no one remembers building or releasing it. reply gravitronic 6 hours agorootparentNo no, I figured that out afterwards, in a past development iteration someone added it on purpose and then forgot all about it - \"oh yeah we needed that to \".So... worse than subterfuge? That being said it only listened on the local socket, so it&#x27;s slightly less bad, and I don&#x27;t want to get into the myriad of correct ways that original problem could have been solved, but lets just say that company doesn&#x27;t exist anymore. reply cjbprime 6 hours agorootparentprevI admire your restraint in writing this comment. :) reply ThePowerOfFuet 3 hours agorootparentprevThis is one of the very finest comments I have ever seen on HN (or anywhere else, for that matter). reply nomel 15 hours agoparentprev> Turns out we had a separate process running that listened on a local socket and would run any command it received as root. Nobody remembered building or releasing it but it made my work quick.No offense, but what a shit show. It makes me assume no source control, and a really good chance that state actors made their way into your network&#x2F;product. This almost happened at a communication startup I know, with three letter agencies helping resolve it. State actors really like infiltrating communication stuffs. reply gravitronic 15 hours agorootparentoh, yeah, this place was a total shit show. BUT we were ISO9001 certified!! So we had source control (CVS) and a Process (with a capital P) to follow. In this case that code was added in a previous development iteration because someone needed to run something as root when a user pressed a certain button on the LCD panel in front and this was the decoupled solution they wrote intentionally. Somehow I feel like that makes it worse than if it was a malicious three letter agency lol. reply nomel 11 hours agorootparentPackage this up and send it to https:&#x2F;&#x2F;thedailywtf.comIt&#x27;s beautiful. reply qmarchi 16 hours agoprevIt&#x27;s crazy to me that this is possible in the first place. Standard practice is to have a fleet of test vehicles that are effectively production except in an early release group.Or, you know, having an A&#x2F;B boot partition scheme with a watchdog. Things that have been around for decades at this point.Disclaimer: Former Googler, Worked closely with Automotive. reply michaelt 15 hours agoparentTo me it&#x27;s all-too-understandable how this is possible.Maybe they&#x27;ve got a test fleet, but it accepts code signed with the test build key.Maybe they&#x27;ve got a watchdog timer, but it doesn&#x27;t get configured until later in the boot process.Maybe they&#x27;ve got A&#x2F;B boot partitions, but trouble counting their boot attempts - maybe they don&#x27;t have any writable storage that early in the boot process.I wouldn&#x27;t be surprised if, as a newer company, they&#x27;d made a &#x27;Minimum Viable Product&#x27; secure boot setup & release procedure, and the auto-fallback and fat-finger-protection were waiting to get to the top of the backlog. reply qmarchi 15 hours agorootparentSo, using Polestar as a reference as it&#x27;s both a vehicle that I&#x27;ve worked on, and one that I personally drive.> Maybe they&#x27;ve got a test fleet, but it accepts code signed with the test build key.Polestar solves this by only delivering signed updates to their vehicles. The vehicle headunit will refuse to flash a partition that isn&#x27;t signed by the private key held by Polestar. Pulls double duty to prevent someone from flashing a malicious update, as well as corruption detection.> Maybe they&#x27;ve got a watchdog timer, but it doesn&#x27;t get configured until later in the boot process.Based on what the Rivian reports are showing (Speedometer, cameras, safety systems are working), they likely are running their infotainment as a \"virtual machine\" within their systems. Again, something that Polestar does.Implementation of a watchdog with a \"sub-system\" like this is relatively braindead simple.> Maybe they&#x27;ve got A&#x2F;B boot partitions, but trouble counting their boot attempts - maybe they don&#x27;t have any writable storage that early in the boot process.Generally, A&#x2F;B partitioning is part of the bootloader, the first program that executes after the reset (on many modern processors) pin is released. This also leads to reboot counters and such being stored as part of the NVRAM that is available at boot.Opinion: Maybe I&#x27;m biased, but maybe if you can&#x27;t develop something yourself, there&#x27;s reason for you to get an off the shelf option that handles a lot of these things.Disclaimer: Former Googler, Worked closely with Automotive. reply Gud 15 hours agorootparentTo be honest, I don&#x27;t think Polestar set a very high bar for software quality. I am currently renting a Polestar 2 from Hertz, and sometimes the HUD doesn&#x27;t work(it&#x27;s 50&#x2F;50 if it will turn on). That means, I don&#x27;t see speed, battery charge, etc, while driving. Infotainment system is working though. reply qmarchi 14 hours agorootparentYou&#x27;re definitely correct in the fact that Polestar isn&#x27;t the highest in software quality, but it was the examples of what they did right that I wanted to focus on.They&#x27;re garbage when it comes to their mobile app and some of the controls on the infotainment system.> sometimes the HUD doesn&#x27;t work(it&#x27;s 50&#x2F;50 if it will turn on).You should definitely reach out to Hertz and ask for a car swap. Sounds like there&#x27;s a bad connection between the display and the IHU. Both screens are operated by the same system, so it&#x27;s unlikely to be a software failure. reply BlueTemplar 12 hours agorootparentprevSounds ridiculous. How is that even road legal ?!? reply FireBeyond 11 hours agorootparentTeslas occasionally need to reboot &#x2F; hard reset their software too, when driving no less, and during that period all that information, and most of the controls, are unavailable (like windshield wipers, etc.) reply Terr_ 10 hours agorootparent> Teslas occasionally need to reboot &#x2F; hard reset their software too, when driving no lessMove over, Microsoft, I think I&#x27;ve found an update policy I hate even more... reply yreg 8 hours agorootparentIt&#x27;s not related to update policy, it happens when the infotainment crashes. reply yreg 8 hours agorootparentprev>and most of the controls, are unavailableAll the controls related to driving are available. You can use turn signals, wipers, change lights, honk, shift gears, etc.But you are correct that you don&#x27;t see your speed during the reboot. reply ClassyJacket 8 hours agorootparentThat&#x27;s not true. You cannot shift gears in the Model X, S, or 3 without the touchscreen - the only way to change into or out of park, drive, or reverse is to swipe on the touchscreen. Only the Model Y has a stalk and it is being removed in the next version.Also even on older Teslas lights are controlled exclusively via touchscreen. reply yreg 2 hours agorootparentI don&#x27;t know what to tell you, you are wrong. I drive a Tesla, I have specifically checked all of what I mentioned when I had this (in my opinion highly problematic) issue happen to me. Have you actually tried it?Even the new stalkless cars have gear selectors. reply mrsirduke 7 hours agorootparentprevThere are captive buttons to change gears&#x2F;direction in the X (2023) where the hazards are, as outlined in the docs: https:&#x2F;&#x2F;www.tesla.com&#x2F;ownersmanual&#x2F;modelx&#x2F;en_us&#x2F;GUID-E9B387D... reply brewdad 9 hours agorootparentprevAs someone who lives in a place where it rains almost every day for 9 months of the year, this reinforces my decision to never buy a Tesla. Does this kill the headlights too? reply xenadu02 4 hours agorootparentFWIW in two years of ownership we have never seen the touchscreen reboot while driving. reply yreg 1 hour agorootparentI had it happen two times this year, but the original comment is wrong, you can use the wipers (and also the headlights) while the touch screen is rebooting. reply serf 9 hours agorootparentprevsome ex-Boeing engineer probably came up with that &#x27;fix&#x27;.[0][0]: https:&#x2F;&#x2F;www.seattletimes.com&#x2F;business&#x2F;boeing-aerospace&#x2F;faa-o... reply iJohnDoe 7 hours agorootparentprevThere are some comments from Tesla developers in previous posts about what a shitshow the code is that runs Tesla cars and what the OTA process is. reply refulgentis 15 hours agorootparentprevOpinion: I&#x27;m a little confused as to how you&#x27;re confused as to how test might not match prod sometimes.Observation: \"[if you write buggy software], there&#x27;s reason for you to get an off the shelf option\"Question: Are you saying if they used Android Automotive this could never have happened?Reference: similar event for Android, last week: https:&#x2F;&#x2F;linustechtips.com&#x2F;topic&#x2F;1538248-pixel-phones-using-m...Disclaimer: Former Googler, did not Work closely with Automotive, Worked closely with Android. reply qmarchi 14 hours agorootparentAnswer: To clarify, more that a company should stick to their core competencies unless there&#x27;s a drastic need or opening in the market that could be filled (and to build a new competency).In this particular case, there&#x27;s nothing particularly unique that Rivian is doing with their Infotainment system that couldn&#x27;t already be handled by an incumbent in the space, (Android Automotive, QNX, etc.) especially given how modular the systems themselves are.As State Farm says, \"We know a thing or two because we&#x27;ve seen a thing or two\". reply hermitdev 13 hours agorootparent> As State Farm says, \"We know a thing or two because we&#x27;ve seen a thing or two\".That&#x27;s Farmers, delivered by J.K. Simmons, not State Farm. State Farm is Jake, in his khakis. reply bo1024 10 hours agorootparentprevRivian thinks of itself as a software company. The first thing you sign when you go to buy a vehicle is a software copyright notice IIRC. The first thing in the owner&#x27;s manual is a notice that the software copyrights and intellectual property belong to Rivian, etc etc. reply jacquesm 3 hours agorootparentI&#x27;m not in the market but I would laugh all the way to the door if that happened. replypsunavy03 15 hours agorootparentprevExhibit A of why a Minimum Viable Product still needs a proper Definition of Done which includes quality standards. reply LoganDark 5 hours agorootparentprev> Maybe they&#x27;ve got A&#x2F;B boot partitions, but trouble counting their boot attempts - maybe they don&#x27;t have any writable storage that early in the boot process.You do not report a successful boot until and unless the entire system loads up successfully. You will definitely have writable storage by then. reply worik 15 hours agoparentprevWhat amazes me is that any grown up person thinks it is a good idea to update vehicles as if they were telephonesOwners should have to bring the vehicle into a shop to have changes made, and they should be very rare.This lazy, control freakery of the worst kindSomething very bad is going on happen and people will die before we realize that it is a stupid dangerous practice reply qmarchi 15 hours agorootparentI understand the sentiment, but think about the alternatives.There are a few different kinds of updates that can be applied, each with their own protective layers.Infotainment updates, like what happened to Rivian aren&#x27;t that dangerous. You lose \"convienience features\" like maps, air con, etc, but generally nothing that could kill you or someone else.Then there&#x27;s system updates, which is where danger noodle things happen. Automotive manufacturers are significantly more risk averse to updating these components, and generally, if _anything_ within the system looks wonky, it&#x27;s an immediate revert.If I, as a Polestar owner, wanted to get an update for my vehicle, the nearest service center is 1.5h away. If I lived in Montana (United States), it would be realistically impossible for me to update my car. Thus, if we want to enable competition within the markets, we shouldn&#x27;t have regulations that force a new manufacturer to have a global network just to add CarPlay to a screen. reply mulmen 10 hours agorootparentDad has a 1966 Oldsmobile with air conditioning. In the last 57 years General Motors has never found a need to update the switch. It still works flawlessly.It’s stupid that we invented a way to not only remotely break an on&#x2F;off switch but also a culture that rolls the dice on that until the inevitable happens. reply neoromantique 11 hours agorootparentprev>Infotainment updates, like what happened to Rivian aren&#x27;t that dangerous. You lose \"convienience features\" like maps, air con, etc, but generally nothing that could kill you or someone else.Also speedometer, which is hardly a convenience feature. reply fyrn_ 11 hours agorootparentThe dashboard panel is working, at least according to Twitter updates. \"Only\" affects the infotainment console. reply brewdad 9 hours agorootparentprevLosing air con in Phoenix in July might not kill you but you&#x27;d wish you were dead. reply LastMuel 11 hours agorootparentprevOn the other hand, we update irreplaceable spacecraft billions of miles away with new software.It should be fine to push software updates out, as long as the correct safety and fallback procedures are in place. It simply has to be designed to handle failure and procedures need to be in place to mitigate risks.It sounds like that wasn&#x27;t the case here. Also, why wouldn&#x27;t you have a small initial release pool when you have such a large potential for disruption? reply brewdad 9 hours agorootparentNASA&#x27;s philosophy is the polar opposite of \"move fast and break things\". reply bradleyjg 11 hours agorootparentprevThe art of shipping software—like on a disk, where once it’s out the door, it’s out the door and you may never get another shot—is dead or dying. Even in some embedded areas of the industry now. reply fargle 3 hours agorootparentprev> What amazes me is that any grown up person thinks it is a good idea to update vehicles as if they were telephonesWhat amazes me is that any grown up person thinks it is a good idea to update telephones as if they were software and not phones.Or rather that it is a good idea to have phones that need updates? Either way, we&#x27;re all one 1&#x2F;2 assed push update to a fridge, vacuum, washing machine, phone or car away from a really annoying day. reply vore 12 hours agorootparentprevAs the update only affects infotainment and not critical systems, it seems like a reasonable tradeoff to me. Just because a car can fail in ways that kill people doesn&#x27;t mean all parts of a car are equally critical. reply windexh8er 11 hours agorootparentThis isn&#x27;t true. If you look at the release notes for any of Rivian&#x27;s updates they all include vehicle related firmware changes. This is not simply infotainment.Beyond that \"infotainment\" includes driver critical information - like the speedometer which, for many affected, means there&#x27;s no working driver screen. reply Daneel_ 11 hours agorootparentThe article states that the speedometer is unaffected:\"Speedometer, charging, backup cameras, locks, lights, wipers, and turn signals are all still functional with the 2023.42 error.\" reply DannyBee 11 hours agoparentprevRivian does have a test fleet, and they test it for weeks before releasing. This particular issue is because they apparently distributed the firmware signed with the wrong cert.Not a bug in the software itself.That is independent of testing the software, but still a distribution issue. reply mytailorisrich 11 hours agorootparentMy 2c based on your comment:* \"signed with the wrong cert\" should mean the software package is rejected before it it is installed.* software upgrades are tricky and there should be at least 2 versions available so that fallback to the previous is possible and automatic in case of issues. reply DannyBee 11 hours agorootparentThe software package probably is signed right but contains multiple signed binaries of which one is signed wrong.Or is multiple signed packages and one is wrong.Or the test cars accept prod and test certs.Or some combo of the above.There are lots of ways this could have broken that doesn&#x27;t amount to rivian not being able to write software reply mytailorisrich 10 hours agorootparentI am not criticising Rivian, not least because I don&#x27;t know the details.That being said, \"signed wrong\", including all your hypotheses, that results in a bricked unit is definitely a serious oversight in general.This also might highlight why production tests should be run on exactly production units. No tweaks allowed. reply DannyBee 7 hours agorootparentIt is great to live in an ideal world, and in fact, in most software, you can do what you are suggesting quite cheaply. But once you get past the sort of \"quip on hacker-news\" level of thinking about this, or trivial and cheap production testing scenarios, people have to make real tradeoffs because it&#x27;s never that simple.Talking about those is much more interesting than just asserting that everything should be a certain way, without any consideration for real world constraints, like cost of units, etc. reply mytailorisrich 4 hours agorootparentI am not sure why you are being so defensive.It would be interesting if you could develop about what you mean by real world constraints and how cost of units affect what I wrote, which I did from real world experience. replyjandrese 11 hours agorootparentprevYeah, but how did the vehicle not just reject the wrong cert and refuse to flash the update? reply mkipper 10 hours agorootparentI&#x27;ve never worked in automotive but it&#x27;s pretty easy to imagine how this might play out in a car, where a single update might bundle updates for several programmable devices.It&#x27;s easy to imagine a central SoC receiving the update, verifying its signature against a local key and then reprogramming some MCU over an internal interface. But then after resetting the MCU, you realize that the image you just flashed isn&#x27;t compatible with the boot security keys burned into that MCU. It&#x27;s not uncommon for a device performing the OTA update to not have access to the \"source of truth\" keys &#x2F; certificates used to verify the updated image at boot time.Not that this is a great excuse. If you add OTA updates to a product that has this design, you should really be confident in your recovery solution. reply DannyBee 11 hours agorootparentprevThe firmware is probably not just a signed package but signed binaries in the package as well. One is probably signed with the wrong cert.This would not cause the updater to fail unless it verified the certs of all the binaries in the package, which most don&#x27;t reply spaceywilly 9 hours agoparentprevYeah... I worked on an embedded project with literally 2 engineers, and we had an A&#x2F;B partitioning scheme, and a recovery partition (we fully qualified the recovery image and it was flashed to the units on day 1, it was guaranteed to boot and it would just sit and wait for the user to initiate a firmware load). The app on the device would reset a U-boot variable once it was successfully loaded, so U-boot could check the number of failed boot attempts. If it was >= 5 reboot attempts without booting successfully, it would go into the recovery partition.There&#x27;s really no excuse from Rivian on this, this is shoddy reply mlyle 16 hours agoparentprevThe code went through early release tests successfully; the problem came with how it was more broadly released.They should have had further staging of the rollout (randomizing when it is offered to users). reply whalesalad 16 hours agorootparentA&#x2F;B partitions tends to solve that. You will only switch to the new partition when the update is 100% verified installed. If it doesn&#x27;t complete in an atomic manner, your device will just boot into the previous healthy partition. reply AlotOfReading 15 hours agorootparentA&#x2F;B gets complicated in the real world. BL1 may not support A&#x2F;B for example, so to implement A&#x2F;B bootloaders you may need a shim that can read&#x2F;write NVM to handle that. Your HSM may not have slots for multiple keys to have different signatures, so upgrading one may trample the other if your update code doesn&#x27;t check that.Lots of ways to screw this up, especially in automotive where you&#x27;re likely to be dealing with TI and their (in)secure boot.I&#x27;ve solved this problem god only knows how many times now and I&#x27;ve rarely found an automotive board that doesn&#x27;t introduce fun, new edge cases. OTA can&#x27;t exceed x kilobytes of memory, the processor isn&#x27;t fast enough to verify signatures and write the image inEdit: Also, why the heck isn&#x27;t the entertainment system completely air gapped from the software running the car?As for this, the entertainment system can control basically every feature of the car and is often the primary or only way to accomplish certain things. Even in much much dumber cars the infotainment is still part of the CAN bus and is able to interact with the rest of the vehicle.https:&#x2F;&#x2F;rivian.software&#x2F;category&#x2F;public&#x2F; reply hef19898 14 hours agorootparentFunny, our 2020 MY Jaguar controls car functions from the digital screen in front of the driver, the middle console screens only control AC, entertainment, phone, navigation and other non-car related stuff. No idea how the architecture looks behind all that so. But seriously, even if on the same bus, just don&#x27;t the media player, radio and connected phone access to the systems actually running the car from engine to brakes. And please, please, finish developing the embedded software running on car before shipping said car. Then it can be air gapped, if not it requires OTA and internet access, raising all kinds of security issues... reply CamperBob2 12 hours agorootparentAnd please, please, finish developing the embedded software running on car before shipping said car.Sorry, but things no longer work that way, and never will again. This is a good thing, as long as processes are improved to avoid situations like this one. reply jacquesm 3 hours agorootparentThings work like that just fine: buy an old car and call it a day.The less software in my car the better. reply lotsoweiners 9 hours agorootparentprev> This is a good thing, as long as processes are improved to avoid situations like this one.How do you figure. I can’t think of a single thing in my vehicle that could be improved by software. When I buy a car I’d prefer it was done. reply LoganDark 5 hours agorootparentIt&#x27;s supposedly a good thing because more features can be added over time, but if they were features worth adding in the first place, the car should have shipped with them already. reply hef19898 1 hour agorootparentThat is the big difference between hardware and software engineering: Once hardware is shipped, there is nothing you can do besides repaurs and retrofits requiring a workshop. Software can updated and changed today anytime.My biggest issue with modern cars, and it seems this is spreading to other embedded systems than cars, is treating those as software: connect them to the web and run OTA updates everytime you need to fix a bug. That requires some form of inzernet cinnection, which requires regular security updates, which require OTA update capability. All that because, bluntly, software devs cannot be bothered with just finishing software running on non-connected hardware that just works, not can they be bothered following hardware development in case hardware is the main component, as in cars. And no, no car manufacturer is a software company, nor phone maker like Apple.The proper way of fixing software bigs in automotive used to be, again thank you Tesla for breaking something that just worked, to recall the affected cars to a workshop to conduct the software update. Honestly, the only thing on a car that should be updated by the owner is maps for the GPS unit. replyjacquesm 3 hours agorootparentprevYou may not have a say in the matter. Most cars are on some kind of IoT private network with their own cell modems and if the manufacturer decides to push an update there isn&#x27;t much you can do about it short of ripping out the cell modem. Which may well have unpredictable consequences. reply MichaelZuo 16 hours agorootparentprevThe &#x27;early release tests&#x27; weren&#x27;t testing an identical copy of the actual update? reply DannyBee 11 hours agorootparentIt&#x27;s probably closer to:The test vehicles accept test&#x2F;prod signed versionsRegular vehicles only accept prod signed versionsThey are otherwise identical.The test vehicles were sent test signed versionsThe prod vehicles were sent the exact same update, signed with test.This would not be uncommon since the test vehicles probably occasionally run test releases for debugging.Further, the update is probably multiple signed pieces, and the only part accidentally signed with test was likely infotainment software.Or something like this.It&#x27;s hard to believe they wouldn&#x27;t test sending badly signed updates, so i have to imagine it&#x27;s a particularly weird badly signed update.In other words, i would not assume they are idiots. reply AlotOfReading 11 hours agorootparentIt&#x27;d be pretty silly to implement an OTA scheme that didn&#x27;t check signatures before installing updates. That would mean any random attacker could soft-brick the module by sending an invalid image, which a development image should be to a production vehicle.You could get this situation if the application code accepted signatures the bootloader does not though. I can imagine that accidentally occurring. reply DannyBee 7 hours agorootparentSure, but it could also be signed binaries inside the OTA, a variant of what you are talking about.IE OTA is a signed package, inside package are also signed binaries. OTA itself is properly signed, a single binary (infotainment) is signed with wrong key.While most OTA verifiers will verify the OTA signature (which this would pass), most don&#x27;t verify the individual inside-package binary signatures at install time, only at runtime. reply AlotOfReading 3 hours agorootparentThat&#x27;s not how I&#x27;ve ever implemented OTA, but I&#x27;ll grant that it&#x27;s possible if it was designed by someone with no idea what they were doing. Certainly not a good OTA process though, for this among many other reasons. reply intern4tional 9 hours agorootparentprevLikely not their code.OTA is generally developed by tier 1, so this is probably a bug in the tier 1&#x27;s code. (Samsung, Panasonic, Sony, etc are common tier 1s in this space.) reply refulgentis 15 hours agorootparentprevRollouts don&#x27;t solve problems, they limit who they effect. reply mlyle 14 hours agorootparentIs not reducing the effective cost of a bad update by 10x or more worthwhile?Sure, but if you are rolling out to 1% of users per hour, you detect the problem in a couple of hours and much fewer than 2% of users will have applied the update. This is a relatively small support problem.While if you roll out to everyone at once, you&#x27;ll detect the problem sooner (within an hour) but have 10x as many affected. reply xyst 15 hours agoparentprevWhen a car company is losing money on every car sale. C level execs going to cut corners reply dewski 15 hours agorootparentThis is a bad take. reply xyst 14 hours agorootparentRivian layoffs earlier this year [2] combined with reports of $33K loss per sale [1]. Rivian is hemorrhaging money right now.RVN IPO’d at $150&#x2F;share. Now it’s trading at $16&#x2F;share.All of these indicators of poor leadership to me. No sustainability. Burning cash. Poor company outlook. Poor products.[1] https:&#x2F;&#x2F;tfltruck.com&#x2F;2023&#x2F;10&#x2F;rivian-financial-results-losses...[2] https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;2&#x2F;1&#x2F;23581642&#x2F;rivian-layoff-ev-... reply OneLeggedCat 11 hours agorootparentprevThis is an inadequate comment. reply worik 15 hours agorootparentprevWhy? reply cs702 16 hours agoprevIt&#x27;s easy to underestimate how hard and expensive it is to build, deploy, and remotely upgrade software that runs reliably on a fleet of diverse cars (different models, different years, slightly different components from batch to batch, etc.). It makes updating a mobile phone OS look trivial in comparison.So far, only Tesla seems to be able to update car software remotely, regularly and reliably. I&#x27;m certain it&#x27;s neither easy nor cheap.All things considered, physical buttons and dials are probably easier and cheaper, because they don&#x27;t require software updates! reply VyseofArcadia 15 hours agoparentForget updates entirely. My car is one of the few places I expect to get software that works the first time.If you absolutely must have updates, then at least not OTA updates. Have them done at the dealership or service center so any issues can be dealt with immediately.Come on, is this engineering or hacking? This is a car, not a CRUD app. Get. It. Right. reply dagmx 9 hours agorootparentThat’s how things used to be and it resulted in lots of long standing bugs because the update rates were low, and so manufacturers didn’t push updates. Many people don’t live near dealers or service centers or can afford the continued cost (it’s not free usually unless it’s a recall)OTA is better for consumer when done properly. Other manufacturers manage it fine, and one bad example shouldn’t be what we base things on. It’s what we should learn from and improve on. reply jacquesm 3 hours agorootparentBut average quality was a bit higher because nobody thought &#x27;oh, we&#x27;ll fix that next week in the OTA update&#x27;. reply dalyons 10 hours agorootparentpreveh i guess i disagree. We had that (& still do for some cars) for decades, and it universally resulted in terrible software that you were stuck with for the life of the car. Hard to update == hard to iterate == bad software. reply ClumsyPilot 9 hours agorootparentbad software is the one that kills people. Ugly software that works is fine reply dalyons 5 hours agorootparentin cars it is often is ugly AND doesnt work. reply w0m 12 hours agorootparentprevrandom new features via OTA updates was one of the deciding factors when i bought my car ... :)I also mostly WFH so... yea. lol. reply matrss 15 hours agoparentprev> All things considered, physical buttons and dials are probably easier and cheaper, because they don&#x27;t require software updates!I am pretty sure there is a market for a dumb modern car, but no one is building it. I am thinking of an electric car without anything \"smart\" in it. Modern safety features can stay, if they work completely self contained and without requiring an external connection ever over the lifespan of the car. reply iso8859-1 3 hours agorootparentI wonder if it is somehow possible to use an open source battery management system to build a car like this. See https:&#x2F;&#x2F;foxbms.org&#x2F; reply jacquesm 3 hours agorootparentprevRegulatory pressure may well get you to do stuff you wouldn&#x27;t want to do. reply NotYourLawyer 11 hours agorootparentprevI’d buy that car today. reply wannacboatmovie 16 hours agoparentprevThis isn&#x27;t a bunch of Windows PCs home-built from a hodgepodge of components.They designed, built, and shipped all the hardware. There is ABSOLUTELY NO excuse for not having a database of the exact hardware configs by serial number. They have the ability to test every single shipped configuration.If they don&#x27;t, they have already failed as a car company. reply AlotOfReading 16 hours agorootparentI guarantee they have a database with the hardware configs. It&#x27;s required by NHTSA to do recalls and notices. They&#x27;ll undoubtedly be using that to inform the right people to come in.The update servers almost certainly don&#x27;t talk to that system though. reply wil421 16 hours agoparentprev> So far, only Tesla seems to be able to update car software remotely, regularly and reliably. I&#x27;m certain it&#x27;s neither easy nor cheap.My Jeep Grand Cherokee has OTA for over 5+ years. BMW has been doing it since 2018.I’m almost positive a family member had it with GMC on star back in the late 2000s. reply willio58 16 hours agorootparentI don&#x27;t think the Jeep or BMW infotainment systems are nearly as fleshed out or complex as Rivian&#x27;s, especially not Tesla&#x27;s. Maybe I&#x27;m wrong! reply phpisthebest 15 hours agorootparentWell then we need to ask why is their infotainment systems so complex? and does it need to be?I want my infotainment systems go connect to Android Auto. That is is.Make it do that, and only that.This drive to make EV&#x27;s as complex as possible is one of the reason i am not planning on buying oneEV&#x27;s are suppose to be SIMPLER than ICE. Make me a Simple Car with simple controls, and just replace the ICE with a battery and Electric Motor, give me an app for my Phone that can do the Charging Trip Calculators and interface with other systems.I do not want a compplex SaaS app on wheels reply bdamm 11 hours agorootparentWell, good news, Bollinger has made your product! reply LoganDark 5 hours agorootparentNo they haven&#x27;t. Bollinger only makes commercial trucks. reply cobalt 15 hours agorootparentprevandroid auto is just a different brand of infotainment system reply worik 15 hours agorootparentAnd if it fails it is inconvenient reply wil421 16 hours agorootparentprevThat’s a huge plus for me. CarPlay or nothing. BMW is becoming closer to a Tesla like screen. GM is supposed to drop CarPlay in favor of whatever they are doing on their EVs.I don’t want my ICE&#x2F;EV to become a SaaS app where I’m paying $500 a year to use my own car. reply vel0city 14 hours agorootparent> GM is supposed to drop CarPlay in favor of whatever they are doing on their EVsIIRC its not just their EVs its all cars. reply fma 4 hours agorootparentShouldn&#x27;t make such a bold IIRC statement correcting someone without first doing a quick Google to verify.https:&#x2F;&#x2F;www.caranddriver.com&#x2F;news&#x2F;a43488135&#x2F;gm-apple-carplay...Specifically it&#x27;s for the new Ultium EVs. Bolts, Lyriqs, Hummers will still have it. reply vel0city 2 hours agorootparentIf I had known it for a fact I wouldn&#x27;t have prefaced it with \"IIRC\".Guess I didn&#x27;t recall correctly. Thanks for pointing that out. reply jacquesm 3 hours agorootparentprevBMW in particular is an interesting case, a (late) friend of mine drove just about every model the day after it came out (BMW fan) and they spent more time in the shop for software issues than they did driving. To the point that he&#x27;d get attached to some of the loaners, it really was that bad. reply duped 15 hours agorootparentprevUpdating software is orthogonal to the complexity of the software application being updated, unless you have horribly designed your architecture. I know, because I&#x27;ve made that mistake. reply bri3d 16 hours agoparentprev> All things considered, physical buttons and dials are probably easier and cheaper, because they don&#x27;t require software updates!Almost all automotive control modules have firmware, whether that firmware is parsing touchscreen inputs or a rotary encoder. reply NotYourLawyer 11 hours agorootparentWell sure, but the rotary encoder can’t get moved to a different menu tree by a software update, and I can use it without taking my eyes off the road. I know which I prefer. reply xyst 15 hours agoparentprevTech junk shouldn’t go in cars, period. Cars shouldn’t be as pervasive and prevalent in society (at least in USA). Yet here we are. Car manufacturers have spent an insane amount of money over decades to get to this point (buying legislators, forcing highway infra, subsidies, profit driven strategy over sustainability) reply treesknees 7 hours agorootparentDecades? Try almost a century. For better or worse, our cities and various economies were built around the automobile.It&#x27;s still a free market - these companies could choose not to put tech into their product. But look at the backlash against GM when they announced they wouldn&#x27;t support Apple Car Play or Android Auto. Consumers want it. reply Aurornis 16 hours agoparentprev> (different models, different years, slightly different components from batch to batch, etc.). It makes updating a mobile phone OS look trivial in comparison.Not really. Vehicle computers aren’t vastly different on every model year and every trim level or option package. These parts are standardized, tested, and carried across model years.Even with changes, the teams would be expected to have the different variants in their development and test cycles. The 2020, 2021, and 2022 model infotainment systems likely share a lot more in common than an iPhone 13, iPhone 14, and iPhone 15 with all of the non-Pro, Pro, and Max variants. reply tech_ken 15 hours agoparentprev> All things considered, physical buttons and dials are probably easier and cheaper, because they don&#x27;t require software updates!If it ain&#x27;t broke it&#x27;s ripe for disruption reply dylan604 15 hours agoparentprevif (cpu == A) do codeelse if (cpu == B) do other codeThey invited the multiple combination vampire into their house. They know what devices are being used. If you don&#x27;t want a dedicated update per piece of equipment, it&#x27;ll be a large binary with lots of branching. Saying they don&#x27;t know what device is where is just lazy. Ask the device what it is, and have a branch for it. If the device IDs itself as something unknown, don&#x27;t do anything. reply FireBeyond 11 hours agoparentprev> So far, only Tesla seems to be able to update car software remotely, regularly and reliably. I&#x27;m certain it&#x27;s neither easy nor cheap.Tesla, whose computer systems quite regularly need to be hard rebooted while the car is driving? That Tesla? reply code_runner 11 hours agorootparentI had to do this once or twice (its very very infrequent in my experience) and one time it was genuinely terrifying, as I had lost blinkers etc where a few interstates all intersect and merge etc.I still do love the car though.... but a very sketchy moment that I shouldn&#x27;t have brought on myself while driving in that situation. reply xienze 11 hours agorootparent> I had to do this once or twice (its very very infrequent in my experience)This is something that’s _never_ supposed to happen.> but a very sketchy moment that I shouldn&#x27;t have brought on myself while driving in that situation.How on earth can you rationalize a Tesla performing an update&#x2F;hard reset while driving as _your_ fault? It should never be allowed to happen! reply pcchristie 4 hours agorootparentTesla doesn&#x27;t do that, you&#x27;re misunderstanding. You can do a hart reboot on the computer by yourself, by holding down both steering will scroll wheels for 10 seconds. Fixes any glitches with the screen. reply fma 4 hours agorootparentSo the commenter is driving through intersections without blinkers and supposed to do that? I&#x27;m not sure if this is a joke or not.I need to teach my grandma how to hard reboot a car without crashing? reply vmladenov 1 hour agorootparentA lot of silliness has been tossed around about Teslas in this thread so let&#x27;s be clear about how they work. A Tesla vehicle have two main independent systems:- The \"car\" that operates the motors, blinkers, shifting, Autopilot, etc. It also supplies standard readouts via OBD that you can monitor independently e.g. with a phone app. As far as I&#x27;m aware, it is impossible to turn this machine off except when the car is stationary, in park, and the brake is fully pressed (or by disconnecting the 400V battery leads under the rear seat).- The infotainment computer is a small Linux machine that displays the map, speedometer, music, plays the sound of the blinkers through the speakers, and random other stuff. This system can be reset at any time by holding the two scroll wheels.There are valid things to criticize about this setup e.g. by not having a standard fallback display of speedo, but you can still read speed from OBD at all times. And Tesla has broken things in OTA updates before, but their rollouts are heavily staged. The last major revert I can remember was FSD beta 10.3 in 2021 back when it had reached a few 10s of testers. replyetchalon 16 hours agoparentprevMy Volvo XC90 gets regular OTA updates without issue, and so did my Land Rover Discovery before it. reply xgbi 16 hours agoparentprevFrom what I read somewhere, Tesla was able to do that because they have remote ssh capability.In at least one instance, they fixed the cars manually by running a massive remote command on all cars after a messed up update: https:&#x2F;&#x2F;lobste.rs&#x2F;s&#x2F;v42zil&#x2F;former_tesla_employee_ssh_d_as_ma...I wouldn’t call that very reliable , but they indeed do it regularly reply SoftTalker 15 hours agorootparentIt sounds like, in this case, the updates clobbered the ssh authorized keys (or equivalent in their system) and so now they cannot access the cars remotely. So they are going to have to go into the shop and have the authorized keys restored. reply FireBeyond 11 hours agorootparentprevAnd it&#x27;s not like they&#x27;d ever abuse that ability, like when someone pokes around in their car and discovers references to a new unannounced model, and then Tesla reaches in, force downgrades the vehicle to older software with no references, and then disables the ethernet port on the vehicle, and for a final fuck you disables its ability to ever get another update.They&#x27;d never do that, except when they did do that. reply kccqzy 4 hours agorootparentThe Twitter thread linked by the link posted GP actually contained a reasonable explanation of why that happened. https:&#x2F;&#x2F;x.com&#x2F;atomicthumbs&#x2F;status&#x2F;1032939644621545473 reply reneberlin 8 hours agoprevWhen will humans be crazy enough to update the firmware of artificial hearts OTA?Updating cars with new features OTA, even \"just\" an Infotainment can possibly cost lives, because the driver might get confused and isn&#x27;t putting eyes on the streets.It should be forbidden and every change should be made clear to the driver, shown in detail, and should need verification twice before being accepted. There must not be any kind of surprise in a car for the driver.It should even be possible to skip an update or stop updating at all. reply rekoil 1 hour agoparentNot updating cars OTA (yes, even \"just\" the infotainment) can potentially cost lives as well, as security holes would not get patched until the next service appointment. reply scardycat 16 hours agoprevBringing CI&#x2F;CD mindset to cars is probably not a great idea. Software updates to commuter vehicles should have a high bar for operational standards, and a simple thing such as an expired certificate should have never been deployed. Having isolated networks in vehicles helps but doesn&#x27;t prevent broken updates from, eventually, bricking the cars. reply nomel 15 hours agoparentI think this shows more of a fundamental flaw in their update mechanism, than anything.I don&#x27;t think a botched update is a big deal. It happens, and should be expected, in a sane design. The fact that the customer noticed is a big deal.There are many implementations that could be used for an \"auto rollback\" feature. They either failed to implement that in a sane way, or they were goobers, and assumed things would always be rosy. reply gitfan86 15 hours agorootparentThe Tesla update is slow probably for this reason. It is probably verifying that it can rollback at any point of failure. reply liminalsunset 11 hours agorootparentI believe one of the reasons it is slow is because it is also updating the firmware on any number of connected ECUs over the CAN bus. This typically means the image has to be sent over a 500kbit&#x2F;s bus so there is a limit to how long it has to take. reply yjftsjthsd-h 14 hours agorootparentprevI would naively expect it to just do A&#x2F;B updates, which unless I&#x27;m forgetting something shouldn&#x27;t incur a speed penalty? (Other than that the update doesn&#x27;t get applied until restart) reply babypuncher 15 hours agorootparentprevI would be pretty pissed if I went out to my garage to head to work one morning and found that a damn software update bricked my car overnight. This shouldn&#x27;t even be a thing, why does a car need regular software updates to keep functioning? reply bink 15 hours agorootparentIt doesn&#x27;t need regular updates to keep functioning. It offers regular updates as they add new features. For instance, in this update a new feature was added to allow for proximity locking at home but disable proximity unlocking. That would lessen the number of times the car would lock and unlock accidentally as you walk in and out of the garage. No one was forced to install the update. reply theandrewbailey 15 hours agorootparentprevCars 20 years ago, even most of them 10 years ago, never got any updates unless they got recalled. Nothing broke, nothing got hacked, and most are probably still working fine.What happened to cars today? I refuse to believe that it&#x27;s solely because these are electric cars, as if the way the car stores and uses energy dictates that it must be part of the internet of things.Edit: there were electric cars over 100 years ago. I bet they never got software updates. reply jdminhbg 13 hours agorootparentCars 20 years ago didn&#x27;t have realtime traffic on big touchscreens that you can use to look up your destination and plan out a route that also lets you schedule fueling&#x2F;charging stops, oh and also stream humanity&#x27;s entire library of recorded music, books, and podcasts. It&#x27;s a tradeoff that the vast majority of people want. reply mckn1ght 12 hours agorootparentAll that stuff should be done via smartphones and the screen in the car should be a dumb display for it. reply xeromal 4 hours agorootparentRequiring a cell phone to replicate features a car should have just makes more problems IMO. reply vmladenov 1 hour agorootparentprev> Nothing broke, nothing got hackedThis needs to be heavily qualified or else it is outright false. reply ryandvm 14 hours agorootparentprevAs software eats the world, it becomes more and more apparent to the non-developers of the world that software engineering is not, and never has been, a real engineering discipline.Tech Support: \"Oh your garage door is bricked after last nights update? Yeah, apparently the [totally uncredentialed] contractor that wrote that update is only 3 weeks out of coding bootcamp and was just copying and pasting from ChatGPT. Lmao\" reply jdminhbg 15 hours agorootparentprevThere&#x27;s never been any car that 100% will work in the morning when you go to the garage. It&#x27;s all tradeoffs. reply theandrewbailey 15 hours agorootparentNow we can add &#x27;bad software update&#x27; to the list of things that can go wrong with cars. We didn&#x27;t use to have that. reply jdminhbg 13 hours agorootparentAt the same time, we&#x27;re losing tons of mechanical problems that use to go wrong with cars. The amount of time lost to car malfunctions is way down over the past couple decades, even with slight regressions like this one. reply BuckRogers 15 hours agorootparentprevIt doesn’t. People and these tech companies are tools. And do it largely in search of ways to take more of your money. It’s not a favor. reply theandrewbailey 15 hours agorootparentIf ICE tech was the hot new thing in cars, things like spark plugs would have a chip so that it would fire n times then break, but don&#x27;t worry, there&#x27;s a subscription for new ones and they will be automatically ordered when the car says so. If the credit card on file expires, your spark plugs won&#x27;t work anymore, even if you just replaced them. reply 1234letshaveatw 15 hours agoparentprevFrom a few days back- Its software has been a “key differentiator” https:&#x2F;&#x2F;electrek.co&#x2F;2023&#x2F;11&#x2F;10&#x2F;rivian-using-software-to-scal... kind of humorous in hindsight reply wannacboatmovie 16 hours agoprevInteresting to note that Ford&#x27;s approach of updating software is far more conservative and car-like. It can be done fully offline via USB, but requests that you kindly upload the log files written to the memory stick back to them when complete, in the instructions as a necessary step. Presumably so they can track and stop incidents like this before they happen fleet-wide.Rivian seems more like a \"ship it and we&#x27;ll fix it in the next sprint!\" company.How do other manufacturers handle updates? reply sturza 16 hours agoparentA&#x2F;B partitions reply barryrandall 14 hours agorootparentThe last time I built something like that, it used partition 1 for the current version, 1 for the last version, 1 with the as-shipped version, and 1 that could restore A or B from the internet or USB. reply post_break 11 hours agoparentprevFords approach is flawed however. You can still update sync with a bad update and bork it over usb. Ask me how I know. reply r00fus 11 hours agorootparentPray tell, how painful was your discovery? reply gunapologist99 11 hours agoprevThis is why I don&#x27;t really want my car to have any antenna (that receives&#x2F;interprets code) or receive OTA updates, ever.I&#x27;d like to please force any attackers to at least be within 50 feet of my TPMS, instead of being literally anywhere on the planet.A car doesn&#x27;t need data updates, and definitely not code updates[1]1. source: every car built in previous century. reply pard68 9 hours agoparentMy insistence on only driving cars made prior to 2005 keeps making more and more sense.(2005 is just an arbitrary date I settled on, nothing significant about it) reply qudat 16 hours agoprevWhat a nightmare. This is where software engineering meets \"real\" engineering, where a \"bug\" has potentially life threatening consequences. reply nomel 15 hours agoparent> where a \"bug\" has potentially life threatening consequences.What are you referring to? That is not relevant to this story, and would require a deep understanding of the system to make such a claim of negligence.“The issue impacts the infotainment system. In most cases, the rest of the vehicle systems are still operational ...”Also, you can&#x27;t do an update while driving. reply jawns 15 hours agorootparentBased on the photo included in the article, what they&#x27;re calling an infotainment system is actually two separate components, one of which appears to be taking the place of a traditional dashboard. If that&#x27;s the case and there&#x27;s no other way to monitor speed, fuel levels, engine temperature, warning lights, etc., I&#x27;d say that&#x27;s quite a bit more worrisome than just not being able to play your favorite music while driving. reply nomel 15 hours agorootparentI understand, but the risk of life wouldn&#x27;t be from the bug, it would be from conscious choice of driving without a speedo. There&#x27;s a critical distinction there.In this case, mileage&#x2F;battery are still present, and I would assume safety critical warnings would still be displayed. reply jonplackett 15 hours agorootparentIf you need thr car in an emergency and it&#x27;s unusable, that almost counts. reply prmoustache 13 hours agorootparentIf it is unusable you just don&#x27;t use it.That any car can have a dead &#x2F;failing battery&#x2F;fuel pump&#x2F;whatever one morning or fail and leave you stranded on the road to an hospital has never been considered a safety risk. reply thrill 14 hours agorootparentprev\"almost\" only counts in horseshoes and nukes. reply ct0 15 hours agorootparentprevYou&#x27;ve never been to death valley without air conditioning Or Russia without heat. I think the infotainment system in this case has a broken climate control function. There are workarounds, but why if you don&#x27;t have your phone? reply nomel 13 hours agorootparentThis is a great point. I would claim that it would be a bad choice to initiate the update (it&#x27;s a manual process, requiring intent) when you&#x27;re in these conditions. But, a less tech savvy person may not understand that updates can be risky, and give it a shot at a remote charging station. reply BlueTemplar 11 hours agorootparentprevI have been talking recently to someone whose job involves sometimes driving to other continents, and they mentioned that cars more recent than ~2003 were out of the question because outside of the EU you cannot expect random mechanics to have the computers required to interface with the car&#x27;s computers - required for repairs. reply PeterisP 11 hours agorootparentIf that&#x27;s the only problem, then that would be fixed simply by taking \"computers required to interface with the car&#x27;s computers\" i.e. aWhat are you referring to?Not the specifics of this article, but more generally about the gravity of the situation car makers (and their software engineers) operate under. The very idea that an OTA software update that causes a bug within more critical features of a car could be life threatening. So my point isn&#x27;t about the specifics of this particular bug, rather the capacity for a bug that could kill. reply nunez 8 hours agoparentprevcritical safety systems&#x2F;functions appear to be unaffected by this outage. reply nicholasjarnold 16 hours agoprevIs it possible, as a licensee of the Rivian vehicle system, to disable the automatic OTA updates without having expert-level knowledge or tooling?Also, yes, I&#x27;m specifically avoiding using the word \"owner\" above for obvious reasons. reply 55873445216111 15 hours agoparentRivian \"licensee\" here. So far all updates have required you to press a button (in the car or on the app) to launch the update installer. Not sure how many weeks you can ignore it for as I never tried. reply bo1024 10 hours agoparentprevConfirming that updates are not automatic, and can be ignored indefinitely. For now. reply martin8412 16 hours agoprevStuff like this is why I don&#x27;t want OTA updates in my cars. Let the car dealership deal with it during regular maintenance. They&#x27;ll be on the hook for fixing it before handing the car back to me. reply galangalalgol 15 hours agoparentDon&#x27;t even need OTA. A seattle radio station bricked a bunch of mazdas.https:&#x2F;&#x2F;www.autoblog.com&#x2F;2022&#x2F;02&#x2F;09&#x2F;seattle-radio-station-br... reply cozzyd 15 hours agorootparentAmazing. Can&#x27;t wait for some car software stack to be so poorly designed that an FM transmitter can remote takeover. reply galangalalgol 15 hours agorootparentIt quite possibly could have with a well formed digital fm payload. reply cozzyd 15 hours agorootparentWe&#x27;ll hopefully no car manufacturer is dumb enough that the FM radio software can affect vehicle control functions but I wouldn&#x27;t be surprised .. reply aftbit 15 hours agorootparentI&#x27;m pretty sure they are all dumb enough for this. This is why people can steal (modern) cars by ripping a headlight out and accessing the CAN bus through the gap.https:&#x2F;&#x2F;kentindell.github.io&#x2F;2023&#x2F;04&#x2F;03&#x2F;can-injection&#x2F;AIUI the military uses \"CAN bus firewalls\" to prevent this. There is also some sort of encrypted&#x2F;authenticated CAN bus protocol in the works. Neither are common in production cars as of 2022 or so, though they may be soon enough. reply lttlrck 10 hours agorootparentEncrypted&#x2F;authenticated CAN sounds like \"Apple component pairing coming to car near you\". No thanks. replyevanelias 14 hours agoparentprevRegardless of OTA vs dealership-only updates, software bugs can have problematic effects long after the update occurred.So far I&#x27;ve had to take my Chevy Bolt to the dealership twice due to major software problems causing the \"service needed\" indicator to be lit (equivalent to \"check engine\"), and I&#x27;ve owned it for barely over a year.The first time, some random bug made the car think there was something wrong with the transmission under some extremely specific set of circumstances, and as a safety precaution it would refuse to shift into drive if not serviced within 100 key cycles.The second time, it was a bug with the software that manages battery health making the car think the battery had a severe problem. In that situation, as a safety precaution, the car refuses to charge above 40%, disables regenerative breaking, limits the HVAC usage, and slightly limits max acceleration.This is getting very irritating. I bought an EV because I thought it would require fewer maintenance visits to the dealer! reply SoftTalker 15 hours agoparentprevAlso worth considering that a manufacturer like Rivian is pretty small. Every town has a Ford dealer. There are many states, however, that don&#x27;t even have a single Rivian service center. reply 0xffff2 15 hours agorootparentCoverage is even thinner than I would have guessed. California has 6 Rivian service centers, but they&#x27;re strongly clustered in the Bay area and LA&#x2F;OC&#x2F;SAN. Even in California if you live in Fresno&#x2F;Bakersfield&#x2F;Santa Barbara etc you&#x27;re looking at several hours round trip to visit an official service center. reply ben_jones 16 hours agoparentprevBut what if your car doesn’t have the latest emojiset or social sharing functionality within the notes app? reply dilyevsky 15 hours agoparentprevThere is nearly zero regular maintenance to be done on EVs though. No oil, no belts, no fuel filter, spark plugs etc. Even the brakes will likely last entire lifetime of the car reply wannacboatmovie 15 hours agorootparentEVs should be subject to mandatory German-style inspection by law to counteract this delusion. reply dilyevsky 15 hours agorootparentHey if you want to pay a dealer 300-500 euro to inspect your tires and swap air filter I’m not against that - you do you. Also if you buy a german car (ev or not) then yes a mandatory inspection is warranted reply rconti 12 hours agorootparentprevJust EVs? reply martin8412 15 hours agorootparentprevBullshit. BEVs eat through tires because they&#x27;re heavy. The air cabin filter and pollen filter need frequent replacement. In the case of Tesla, you better check the undercarriage so you can potentially spot control arms soon to fail. The shield for the battery should be inspected. reply vmladenov 1 hour agorootparentBEVs don&#x27;t eat through tires because they&#x27;re heavy. They eat through tires because they have fast acceleration and their drivers make use of said acceleration. A Tesla Model 3 weighs around what a BMW M4 weighs and less than all trims of an M5. reply dilyevsky 15 hours agorootparentprevYou can’t rotate&#x2F;change tires at local tire shop? It’s not under warranty anyway.> The air cabin filter and pollen filter need frequent replacementYeah “regularly” like every two years lol, you gon wait for that long to update your software and pay $500 to a dealer to do it? reply nkingsy 15 hours agorootparentprevHertz seemed to find teslas cost double ice counterparts to maintain.Maybe it’s auto company smoke but source: https:&#x2F;&#x2F;fortune.com&#x2F;2023&#x2F;10&#x2F;27&#x2F;tesla-elon-musk-hertz-evs-ren... reply aftbit 15 hours agorootparentI&#x27;m not sure maintenance costs are really the relevant part. It seems like the problem is that Teslas are cheaper now and thus Hertz&#x27;s fleet is worth less than it was before. Additionally, they find that Teslas suffer more damage, likely from collisions or similar. Routine maintenance costs are not mentioned in that article at all.>In short, the declining value of the Tesla cars in Hertz’s fleet—a decline directly caused by Musk’s price cuts—has hit Hertz squarely in its profits.>Without explaining precisely why, Scherr said Hertz is suffering a higher incidence of damage specifically with its EV fleet, where the repair costs are roughly double that of a comparable gas-fueled car.>“Studies of current EV ownership evidence lower incidence of damage and collision than for ICE vehicles, not higher as we are experiencing,” he revealed. Musk’s price cuts then become an acute problem when one of the Hertz EVs sustains so much damage that the cost of repair is more than the asset itself.>“Where a car is salvaged, we must crystallize at once any difference between our carrying value and the market value of that car,” Scherr explained. “The [price] declines in EVs over the course of 2023, driven primarily by Tesla, have driven the fair market value of our EVs lower as compared to last year, such that a salvage creates a larger loss and, therefore, greater burden.”>In short, Hertz then needs to book a noncash accounting charge. Together with the higher repair costs this led to significant profit margin headwinds.https:&#x2F;&#x2F;archive.ph&#x2F;leFdf reply BlueTemplar 11 hours agorootparentSucks to be them, but then maybe they should have listened to Musk when in the very beginning of Tesla he said the goal was to make EVs cheap ? reply dilyevsky 15 hours agorootparentprevThat’s because it is run by idiots who ran it into bankruptcy reply Lightbody 10 hours agoprevI have preorders in for the R1S, the Volvo EX90, and the Kia EV9. I passed once already on buying the R1S when they had one in town available for immediate purchase, simply because they refuse to adopt CarPlay.This incident does NOT give me confidence that Rivian is likely to offer a better alternative to CarPlay, despite their statements otherwise.I suspect the EX90 will be what I land on eventually. reply p_j_w 10 hours agoparent>This incident does NOT give me confidence that Rivian is likely to offer a better alternative to CarPlat,I have complete faith that, 5 and maybe even 10 years from now, no auto maker will have delivered anything that can compete with either CarPlay or Android Auto. The fact that an auto maker thinks they can do better is a sign of a really high level of either arrogance or outright greed. Complete deal breaker. reply eschneider 16 hours agoprevThis is a bit of a nightmare scenario and why when remote updating, you always test update to your own fleet first. Always. reply toddmorey 16 hours agoparentIt sounds like it was tested on their own fleet but they accidentally pushed the wrong bits when deploying the update more widely out to customers. reply eschneider 15 hours agorootparentThe usual \"best practice\" thing for IoT deploys, is to deploy to \"your\" devices, what for everyone to go green, then allow that build to deploy more widely. In a well-functioning system, it shouldn&#x27;t be possible to swap bits between those stages.But who knows what these guys were doing. :&#x2F; reply bink 12 hours agorootparentReading between the lines of their public comments it sounds like they did run a full test through their test fleet, their employees, and then were rolling out to customers when the promotion process was \"fat fingered\". Maybe someone accidentally promoted the wrong release version. reply carlivar 16 hours agorootparentprevMaybe they should have an additional phase between test deploy and customers such as \"employee personal vehicles\". reply kevin_nisbet 16 hours agoparentprevYes. And also things like rolling out the update in batches, and then also things like golden images, where if there are two crashes or failures in the first 24 hours of the update, change to the last known good software version. reply 88 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A software update from Rivian has caused the infotainment system to malfunction, rendering it unresponsive.",
      "A solution to fix the issue is not currently available or easily accessible.",
      "Users are currently experiencing difficulties with their Rivian infotainment systems due to this software update."
    ],
    "commentSummary": [
      "The discussion focuses on software updates in the automotive industry, highlighting the risks and challenges of deploying updates to a large number of devices.",
      "Topics covered include the importance of testing and phased rollouts, as well as ensuring that software updates do not cause critical issues.",
      "Debates revolve around the complexities of infotainment systems, the use of OTA updates, and the potential hazards of updating software while driving. There is also criticism directed towards Rivian for a recent software update error."
    ],
    "points": 244,
    "commentCount": 340,
    "retryCount": 0,
    "time": 1699982570
  },
  {
    "id": 38261539,
    "title": "Reducing Reliance on crates.io: An Alternative Approach to Rust Packaging",
    "originLink": "https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html",
    "originBody": "Rust without crates.io 14 Nov 2023 • Thomas Karpiniec Rust is a lovely programming language but I’ve never quite come to terms with crates.io, or any other of these language-specific repositories where everyone uploads and downloads code willy-nilly. I have several objections: If crates.io goes down or access is otherwise disrupted then the Rust community will stop work. It is profoundly unresilient to have a single point of failure like this. Certainly some people will have vendored their deps and others will have a panamax mirror handy, but for most, Rust as we know it stops if this one particular web service goes down. There is no mediation of any kind between when a new library/version is published and when it is consumed. You need only one author in your maybe-hundreds-of-dependencies tree to be hacked, coerced or in a malicious mood for you to have a really bad day. Any tampering with crates.io itself (espionage, disgruntlement, national security) could have an incredibly wide blast radius, or a incredibly wide set of targets from which to choose. Since crates.io is the source for crates, it is normal for both developers and CI machines to be hitting this web service all the time. Opportunities for mischief are exacerbated when clients are phoning home so frequently. So what’s the alternative? I think we all need to take a step back from the altar of developer velocity and take a deep breath. I don’t want dependencies hot off the press. Ideally I want someone independent of the authors playing a curatorial role. Now, actually getting some human review of dependency updates is quite a hard thing to do. cargo-crev has been trying for years to make this happen. I would love if it was the solution but it isn’t yet, and I think it’s a little ambitious. Yes we would like to have super-experienced software developers reviewing all our libraries with cryptographic stamps of approval, but if they’re not available we could be the target of remote shell in a build.rs. Surely there’s a middle ground here? What’s interesting is that this problem is largely solved for C and C++: Linux distributions like Debian package such a wide range of libraries that for many things that you want to develop or install, you don’t need any third-party libraries at all. It’s just a matter of finding the right apt-get incantations and off you go. Even if you can get 95% of your libraries from a common trusted source then your risk is decreased considerably. Rust libraries don’t work quite the same as C/C++ ones. Normal Rust code can’t be dynamically linked—a binary will have all of its dependencies statically linked at build time, so you won’t typically see .so files for Rust libraries that are going to be consumed by other Rust code. Since there is no .so file, Debian has no package that installs the library. However if they want to ship a binary that was written in Rust, their builders can’t just be downloading stuff from crates.io. They need a way to package all of the software that represents that Debian release. To solve that problem they’ve taken all these little dependencies and put their full Rust source code in packages with names like librust-cratename-dev. Hmm, how many such packages? Running on trixie (testing)… $ aptitude search librust-grep -vE \"^v \"wc -l 2336 This is starting to look like a serious curation of the most important Rust crates, available from any Debian mirror. There are some double-ups to be sure, since in some cases multiple incompatible versions of the same crate had to be packaged. Still. Maybe there is enough Rust in Debian now that it’s viable to write interesting Rust software independently of crates.io? That would solve basically all my concerns and the situation is only going to improve as more Rust software gets packaged. To be clear, I don’t expect that Debian Developers are auditing these packages in the manner of cargo-crev. The good thing is that they don’t actually need to for it to be a major improvement. A DD isn’t going to upload a new patch release just ‘cause. It’s going to be because it has an important fix or because some other program has depended on it. On crates.io a maintainer is free to create new releases for any reason and cargo update is not going to evaluate how good that reason is. A simple time delay will allow egregious malware like malicious build.rs scripts to be caught, whether that’s the super-long Debian stable cycle or even the several days required to migrate from unstable to testing. I assume that an urgent security issue would be distributed the same as any other Debian update. They might decide to give the diff at least a cursory look, which is better than nothing. How do we do this? It’s actually quite easy because the big-brained Debian developers have arranged all the Rust dependencies to follow the format of a cargo Directory Source. That is, all of the packages are installed in their own directories under /usr/share/cargo/registry, including implementing a cheeky workaround for the required .cargo-checksum.json files. You can then add some brief incantations to your .cargo/config.toml, whether on a project- or user-wide basis: [net] offline = true [source] [source.apt] directory = \"/usr/share/cargo/registry\" [source.crates-io] replace-with = \"apt\" This overrides the default crates.io source and ensures dependencies can only be fulfilled locally by installing the relevant packages. This happily doesn’t require any changes to your projects themselves—you just have to be careful to use versions in your Cargo.toml (and Cargo.lock) that are resolvable on Debian, since it is a subset of those available on the wider crates.io. I am quite certain that Debian wouldn’t have enough coverage yet for the monorepo at work, but I gave this a go on my one of my little CLI projects that has half a dozen dependencies. Apart from having to downgrade copypasta from 0.8.2 to 0.8.1 in the Cargo.lock, this builds and runs just fine. What a treat. This little investigation has given me much more confidence in using Rust generally into the future. I feared that the “grab any dependency version you like” approach facilitated by crates.io would render Rust impervious to any sort of curation effort, such that anyone who was serious about my earlier concerns would have to stick to a language used to the old ways like C++. Fortunately, Debian is here to prove me wrong. A+ work by their Rust packaging team. All power to those who like to live on the edge; I’ll be over here trying to minimise different types of dependencies.",
    "commentLink": "https://news.ycombinator.com/item?id=38261539",
    "commentBody": "Rust without crates.ioHacker NewspastloginRust without crates.io (thomask.sdf.org) 225 points by todsacerdoti 23 hours ago| hidepastfavorite217 comments chrismorgan 22 hours ago> What’s interesting is that this problem is largely solved for C and C++: Linux distributions like Debian package such a wide range of libraries that for many things that you want to develop or install, you don’t need any third-party libraries at all.This does not reflect my observations at all. It works if you’re an end user and the software you want has been packaged. But if either of those is not the case, you’re left in the mud. Rather, you end up with builds that are difficult to set up or reproduce except on specific platforms, require a lot more maintenance over time because bitrot occurs naturally (your code depends on libexample 1, but Debian only package version 2 now, so good luck dusting off that project you made five years ago and getting it to build again), vendoring upon vendoring, more diverse and generally lower-quality dependencies for a particular task, and other problems like these. You’ve solved one problem, but at a very significant cost that must be acknowledged, even if you decide it’s still worth it.Take an app written five years ago that depends on majorish C++ libraries, and there’s a fair chance compiling it will be a nightmare and require invasive surgery due to version incompatibilities. It’s not improbable that the most pragmatic solution may be to find and rebuild the transitive package tree for some older version of some package, and that’s generally quite painful (and can be outright incompatible).Take an app written five years ago in pure Rust, and it’ll almost certainly (p>0.99) still work fine. Take an app written five years ago in Rust that binds to some C&#x2F;C++ library, and there’s a fair chance you’ll run into trouble, but it will almost certainly be a good deal easier to resolve, much more likely to work with only a version bump and no other changes than in C&#x2F;C++.As a developer, I know which general mode of operation I prefer. Debian are welcome to twiddle the dependencies in the way that suits their chosen requirements, but the canonical sources are still crates.io, and I know that when they’ve stopped packaging example-1.0.0, I won’t be beholden to their packaging but can just use `cargo build` and it’ll still work (contingent upon extern bindings).I’m dubious that any meaningful curation is happening, and curious if there’s even cursory review of crate contents. But most of all, I’d be interested in a statistical comparison of the crates that Debian ships with the most-downloaded crates from crates.io. I suspect the sets would look very similar. reply josephg 21 hours agoparentYeah. The part that gives me goosebumps with that proposal is: as an author of some not very popular packages, what do I do? Is it my job to package and promote my packages on Debian, redhat, homebrew, and all the others? Do I need usage before they’ll pick it up? Sounds like a chicken and egg problem. And doing all that packaging work sounds like a boring, time consuming job that I don’t want.It’s also quite common for me to pull out bits of code I’ve written that I want to share as a dependency between different programs I write. I have dozens of packages in npm along these lines - and it’s quite elegant. I factor out the code, package it independently, write a readme and some tests and publish it. My other packages can then just depend on the brand new package directly. In this brave new world the author is proposing - what? I make a .deb, argue for it on some mailing lists I’m not on, wait who knows how long, eventually gain permission to depend on my own code, then realise I can’t install it on my Mac laptop and do the same dance in homebrew? Oh - there’s a bug. Better manually update all the package repositories? Sounds like a complete nightmare to me.I’m sympathetic to the problem. Supply chain attacks are way too easy. But maintaining up to date packages on every single Linux distribution, homebrew, ports, and whatever windows does sounds like a full time job. A boring full time job. And it would still not work as well for my users, because when a new feature is released in one of my upstream dependencies I can’t use it until all of those package repositories ship the new version of my upstream dependency too. I’d rather eat sand.“It works for C&#x2F;C++” by making developers working in those languages pay in blood for every dependency they pull in. You can tell by looking at just how few dependencies the average C program uses. It’s like that because of how painful it is. Anything under a certain size tends to get either reinvented every time, badly (hash table libraries in C) or vendored (eg json parsers). Common libraries like rust’s rand don’t exist in C because it’s just too much effort to pull in dependencies. So everyone rolls their own crappy version instead. Ask GTA5 how well that went for them. And that’s not the developer ecosystem I want to work in. It’s frankly a terrible proposal. reply no_wizard 19 hours agorootparent>I’m sympathetic to the problem. Supply chain attacks are way too easyThis is the crux as to why crate.io is great: you have one official place for packages that can be continuously audited and monitored. This is the way to do security, in my opinion, because you are always able to look at incoming and outgoing packages.Now, you can argue if we are doing enough with this, and that is definitely up for debate, however, I contend that the premise of having one canonical place for package origination and hosting makes it easier to monitor for security purposes reply josephg 11 hours agorootparentI - and many others - don’t think this is enough to mitigate supply chain attacks. It’s too easy for a malicious developer to add cryptolocker ransomware in a point release. But I also don’t think having a package in Debian will somehow magically solve this problem either. Debian package maintainers do not - and never have - done security audits on the packages they add. It’s magical thinking to assume using packages in Debian will somehow protect you from supply chain attacks. Asking developers to keep their dependency trees small so we can minimise the risk is a losing battle. Using lots of small, popular dependencies is just too convenient.There is a solution out there that I wish got more traction. And that is, capability based security. The idea is simple: we partition programs into application code and library code. Library code can never access the filesystem or network (or other protected data) directly. And this is enforced by the compiler or language runtime. Application code can. If you want a library to interact with a file, you first open the file in application code and pass the file handle to the library. Similar wrappers can be made for directories, network sockets and so on. If I want express to listen on a specific port, I pass it that port in a capability object. Not just an integer. (Eg app.listen(net.listenPort(8000))). The amount of code that would need to be changed is tiny since well behaved libraries don’t usually need many capabilities to do their job.It is ridiculous that my 3rd party json library can invisibly access all the sensitive files on my computer. With capabilities, we take that privilege away. I have no problem trusting a random json library if it literally only has access to the json string I pass it. If we had a capability based security model, I would have no qualms about pulling in hundreds of transitive dependencies. My libraries can still only access exactly what I pass them.Unfortunately this problem requires language level buy-in. It would be very hard to retrofit rust to work this way and that’ll probably never happen. Raw pointers and inline assembly also make a bit of a mess of things too. (Though pointer provenance might help). But a man can dream… Maybe in rust’s successor. reply dinosaurdynasty 10 hours agorootparentThis is similar to how WASI (standard system interfaces) for WASM work. reply josephg 7 hours agorootparentYeah I&#x27;m really keen for that - though I think compiling a bunch of dependencies into individual wasm modules will be kind of awful to use in practice.Apparently in firefox they&#x27;ve been compiling some of their dependencies using wasm, then translating the wasm back to C. The end effect of all that is that it gives C code wasm&#x27;s sandboxing and safety. I wonder what it would look like to do that without needing wasm as a middle step. reply couchand 11 hours agorootparentprevYou don&#x27;t need the language to be involved to use this strategy, if you can rely on the OS. Lots of programs are structured this way already: open your secure port as root, then daemonize as an untrusted user.However, I quite like the idea of using the application&#x2F;library interface to enforce the restrictions structurally, rather than temporally. reply josephg 10 hours agorootparent> open your secure port as root, then daemonize as an untrusted user.That works for server applications. Well - so long as bad things don’t happen in a static initializer before the process drops privileges. But I also want it to work for other kinds of applications. I want to be able to fearlessly depend on a library and not need to worry about that library attacking my computer. Then we can have our cake and eat it too with big dependency trees made up of lots of small libraries written by untrusted contributors.Another nice thing about this is it doesn’t depend on OS level capability support. Once implemented in a language, it should be pretty easy to make it work on every OS.Interestingly, this is something wasm provides out of the box. And Firefox’s weird “compile deps with wasm as convert the result back to C” also achieves this end. reply ashishbijlani 9 hours agorootparentCreator of Packj [1] here. How do you envision sandboxing&#x2F;security policies will be specified? Per-lib policies when you&#x27;ve hundreds of dependencies will become overwhelming. Having built an eBPF-based sandbox [2], I anticipate that accuracy will be another challenge here: too restrictive will block functionality, too permissive defeats the purpose.1. https:&#x2F;&#x2F;github.com&#x2F;ossillate-inc&#x2F;packj flags malicious&#x2F;risky NPM&#x2F;PyPI&#x2F;RubyGems&#x2F;Rust&#x2F;Maven&#x2F;PHP packages by carrying out static+dynamic+metadata analysis.2. Sandboxing file system w&#x2F;o superuser privileges: https:&#x2F;&#x2F;github.com&#x2F;sandfs&#x2F;sandfs.github.io reply josephg 8 hours agorootparentCool library!I’m not imagining something that specifies policies at all. Capability based security systems shouldn’t need them. Instead, the main application can create capabilities programmatically - for example by opening a file for reading. The file handle is a capability. Libraries you call don’t get access to the OS, or anything outside their module scope that hasn’t been passed in as an explicit parameter. A 3rd party library can access the file handle you pass it as a function argument, but it can’t ask the OS to open any other files on your computer. Or open a network socket.OS APIs aren’t in a scope that any of the libraries you use have access to.There are a few things capabilities need to be able to do. They need to be able to be tightened. (So for example, if you have a file with RW flags, you can make a copy of the file handle that’s read only). And they need to be unforgeable (file handles can’t be created from an integer file descriptor). We’d also need a handful more “file handle” like objects - and this is where granularity matters. Like we need an equivalent for directories that gives you access to everything recursively in that directory and nothing outside of it.If the library I call delegates some work to another library, it’s pretty simple. It just passes its file handle through to the other library as a function argument. There’s no policy to maintain or update. Nothing even changes from the point of view of the original caller.The tricky part is stuff like debug output, environment variables (NODE_ENV=production) or stdout. Do libraries implicitly get access to stdout or not? Maybe they get stderr but not stdout, and they can open new temp files? You would also need to restrict JavaScript code from arbitrarily messing with the global scope or global libraries like String.But honestly, any answer to these problems would be a massive improvement over what we have now. Being able to know your binary search library can’t make network requests, and that your database can’t interact with files outside its database directory would do so much for supply chain security it’s not funny. A good capability system should just change the APIs that your software can use a bit. Generally in good ways that make dependencies more explicit. Done right, there shouldn’t be any new syntax to learn or deno style flags to pass at the command line or anything like that. replysamtheprogram 16 hours agorootparentprevThat’s like saying it’s best to run everything on AWS, because there’s only one place to monitor for outages.That’s the exact opposite of how supply chain mitigation works. reply iudqnolq 15 hours agorootparentIt&#x27;s not at all like saying that.If you want to mitigate downtime in your supply chain then diversity is good. If you want to mitigate security vulnerability then it&#x27;s bad.Imagine you have 100 api servers that all have database credentials. If one server goes down you have 99 working servers (ideally). If one is compromised then the attacker has your database credentials, game over. reply mid-kid 21 hours agorootparentprevWhen you make a new library for C&#x2F;C++, you generally just install it locally at first (pkg-config and PKG_CONFIG_PATH make this trivial). It doesn&#x27;t (need to) get packaged by a distribution until a program using it is packaged. You don&#x27;t need to do this yourself, unless you yourself use a distribution and want it packaged there.As for the amount of dependencies, you have to wonder if it&#x27;s better to have tonnes of small dependencies or whether it&#x27;s better to have a few bigger ones. When I need something in a library, I don&#x27;t make a new library for it, I first try contributing to it, and failing that I vendor and patch it. I feel this is less common in the brave new world of everyone just doing whatever and pushing it to a largely unmoderated repository... reply josephg 21 hours agorootparentI suppose I could manually traverse my whole dependency tree myself and install all those packages myself. Set up cmake and pkg-config for all of them. If anyone else wants to try out my software, they’d have to manually look up all the dependencies in my github (?) and install those too. And hope the versions all line up nicely - which is a big if during development. Maybe I could script all of that dependency tracing - if only there were some standard software to do that. Guess not.If C&#x2F;C++ is all you know, you’re used to how painful all of that is. But what you’re proposing is a significantly worse experience in every way compared to what I can do with cargo or npm today. None of the claimed benefits seem remotely worth the cost of wading back into C dependency management hell. And cargo and npm show just how unnecessary all that pain is in C!Just make good tooling. We can have nice things. I stopped using C not because I decided to write assembler. I stopped when better languages came along. You want me to stop using cargo? Nobody wants to go back to manual dependency management. Make something better. reply cozzyd 16 hours agorootparentKeeping track of C&#x2F;C++ dependencies is basically the point of Linux distros...Cargo&#x27;s main problem is that it&#x27;s (mostly) rust only. Same problem that pip&#x2F;npm have, relegating you to (mostly) single-language silos. reply josephg 11 hours agorootparent> Keeping track of C&#x2F;C++ dependencies is basically the point of Linux distros...Judging by the proliferation of docker I don’t think they’re doing a great job of even that. Docker shouldn’t be needed if apt was up to the task. Nixos is a much better design imo.> Cargo&#x27;s main problem is that it&#x27;s (mostly) rust only.True. But apt’s main problem is that it only works on a few distributions of Linux. Limiting the users of my rust library to rust developers is usually not a problem. Limiting the users of my rust library to the people who use homebrew or arch or something is not fine. And I don’t want to redo all the packaging work on every operating system and major Linux distribution. Life is too short for this make work BS. Especially given it could be automated away anyway. Rust packages already explicitly define all their dependencies. Why can’t apt just generate .dpkgs from my Cargo.toml?Apt’s other problems are that packages need to be installed globally. And they make a mess of versions. And there’s no feature flags. Run `apt search vim` some time and see how many packages they have! Or the same for llvm. It’s a dog’s breakfast.The time that cargo’s rust focus hurts it is with binary packages. It’s not enough for ripgrep to live in cargo since it’s designed to be useful for people who aren’t rust developers. And I don’t have a good answer for how to solve that. I wish I did. reply cozzyd 11 hours agorootparentWe&#x27;ll this also works for rust libraries as they&#x27;re usually useless for other languages, whereas libraries like gdal, opencv, fftw, etc. can be used by many languages. (Though to be fair, until rust gets a stable ABI it&#x27;s not going to be a good choice for this sort of thing).I personally consider docker&#x2F;podman just completely punting on dependency management, and try to avoid them, but I&#x27;m probably in the minority... reply humanrebar 11 hours agorootparentprev...except pip&#x2F;npm often build C or C++ while packages install, at least in some cases.And Rust has the same issues, at least when shipping or consuming libraries with C APIs is in the picture. reply cozzyd 10 hours agorootparentYes, sometimes, but it won&#x27;t resolve C&#x2F;C++ dependencies in general. reply navi-desu 17 hours agorootparentprevmeson actually solves this problem on the build system, their wrap subsystem allows you to use the system provided lib if found, or fetch from a database during the build process if not.best of both worlds, imo. reply mid-kid 21 hours agorootparentprevIf you&#x27;re smart about it you&#x27;re not gonna have a very deep or wide dependency tree for your own deps. If you need more than maybe two of your own deps, maybe you need to reconsider how you structure your libraries, or you can join multiple libraries into the same project&#x2F;build system umbrella (e.g. glib&#x2F;gobject or the whole of QtBase) reply josephg 20 hours agorootparentWhy would I want to join unrelated modules together into a kitchen sink library? The only benefit of that is to work around bad dependency management systems that make dependencies a hassle to work with. But otherwise, they’re a bad idea. Packages should do one thing and do it well. If I want a good random number library, I don’t want to import bob’s junk draw of random code. I want a small, reusable random number library. I want one that I chose. And my users want one they chose. And if that choice is bad, I want to be able to replace it without throwing out half my ecosystem or breaking downstream compatibility.QtBase is a symptom of C++’s bad package management. 8 unrelated utilities should be in 8 small packages that users can pick and choose based on their needs. reply mid-kid 16 hours agorootparentIt&#x27;s less a workaround around bad dependency management, and more about the fact that it&#x27;s simply easier to reason about a smaller set of dependencies regardless of the ecosystem. This includes both vetting, ensuring version compatibility, checking license compatibility and reducing the SBOM, as well as governance over the project and integration between different modules.Too often have I seen the equivalents of frameworks in other languages being split over sometimes hundreds of packages, that don&#x27;t always make it clear that they&#x27;re to be updated in tandem, what their exact relationship is, and that the same organization manages all of them.As for QtBase, it&#x27;s a superproject, but that doesn&#x27;t mean that you can&#x27;t use its individual modules separately, and depending on the distro (e.g. debian) install them separately as well. A singular project installing multiple related libraries makes a lot of sense. reply josephg 13 hours agorootparent> This includes both vetting, ensuring version compatibility, checking license compatibility and reducing the SBOM, as well as governance over the project and integration between different modules.Auditing large code bases takes disproportionately longer than auditing small code bases. So I don’t think that’s a win. License compatibility is trivial to check. SBOM is strictly larger if I pull in a kitchen sink package because I’m probably not using most of the stuff inside. Better to just pick out the components I want.The one thing I’ll grant you is that shared ownership and visibility means it’s less likely that one rogue person will sneak ransomware into the dependencies.Personally I’d love a capability based package manager that lets me pass a package exactly the capabilities it needs to do what I downloaded it for. Why does every package need access to my files when I install or run it? That’s ridiculous. Totally unnecessary and a massive security risk. We could solve that C++ style by hobbling package managers and using fewer, jumbo packages. But that doesn’t solve the root problem. I want a package system in a language which lets me pass capabilities to each library I use, when I need to use them. Eg “Please open your web server on the port associated with this capability token.” This needs language support too but it’d be so much better from a security point of view than anything that came before. reply SkiFire13 20 hours agorootparentprev> or you can join multiple libraries into the same project&#x2F;build system umbrella (e.g. glib&#x2F;gobject or the whole of QtBase)This also seems worse than the status quo. So instead of having 3 or 4 dependencies I now only one that combines 20 different ones I don&#x27;t need. reply chalsprhebaodu 20 hours agorootparentprevWhy is this inherently better than better tooling for dependency management? reply Ar-Curunir 20 hours agorootparentprevThis is an example where your tooling limits your solution space. There’s nothing inherently wrong with a deep or wide dep tree. reply patrick451 19 hours agorootparentprev> And cargo and npm show just how unnecessary all that pain is in C!Leftpad would like a word.Every dependency is a liability. It&#x27;s another thing to vet, another thing which can disappear. Having few dependencies in c is a feature, not a bug. The approach to dependencies in rust was a major turn off to me. I recall 15 just to get random numbers in the intro tutorial. No thanks. reply josephg 13 hours agorootparentJust in case you don’t know, packages can no longer be pulled from npm by the developer. At least not without emailing someone. And you need a very good reason - like that version is ransomware.The particular pulled package version identifier (foo@1.0.1) can also never be reused. If you audit 1.0.1, it will never change out from under you.The leftpad fiasco was hilarious and embarrassing. But it can’t happen again because of changes in npm policy. reply takluyver 15 hours agorootparentprev> It doesn&#x27;t (need to) get packaged by a distribution until a program using it is packaged.I think this is a key difference in approach. In languages with their own packaging systems, you routinely package libraries before there are programs using them. Publishing them on Crates&#x2F;PyPI&#x2F;npm&#x2F;whatever is the bare minimum if you expect anyone to use them!The number of tiny dependencies can go too far - I don&#x27;t think I need to mention left-pad. But the difficulty of using dependencies in C&#x2F;C++, and the results of reinventing stuff, vendoring, or using kitchen-sink dependencies like Qt, don&#x27;t seem optimal either. There must be a happy medium somewhere. reply josephg 11 hours agorootparentAs I said in another thread, I think the happy medium is lots of small packages (people want that). And a capability security model within programming languages so small dependencies are limited to interacting with their parameters (and any resources their parameters provide them) and can’t speak to the OS directly. That would solve 98% of the supply chain problem.Leftpad has already been solved by a npm policy change forbidding packages from being unpublished. reply lmm 7 hours agorootparentprev> As for the amount of dependencies, you have to wonder if it&#x27;s better to have tonnes of small dependencies or whether it&#x27;s better to have a few bigger ones.You don&#x27;t have to wonder, we&#x27;ve done the experiment. Anyone working in an ecosystem with non-joke package management knows that tonnes of small dependencies are a lot better. reply galangalalgol 21 hours agorootparentprevC++ apps don&#x27;t pull in that few dependencies. Take something modern that needs to talk over flatbuffs or protobuffs and maybe has a web ui based on emscripten? Then run ldd on it and recoil in horror. C++ dependency trees now often have a hundred or more items. Many of those are packaged with the OS, but that really shouldn&#x27;t comfort you. reply pclmulqdq 11 hours agorootparentIt kind of does comfort me that they are packaged with the OS, because it&#x27;s an OS problem (and is relatively major) if they are broken. reply galangalalgol 11 hours agorootparentIt often takes months for distros as large as rhel to fix med. Or even high severity CVE. That is assuming your OS is getting updated properly. Maybe it is a container and you are doing that as part of your pipeline. But all that it means having the dependency in an rpm or deb file vs in a github repo pulled by conan or similar, is that there is someone you can sue that is supposed to notice the cve and roll out a new rpm or deb file. They aren&#x27;t doing massive pipelines full of fuzzers and custom unit tests for all those packages, they are just... packaging... them. Otherwise we wouldn&#x27;t be seeing things like the curl and openssl vulns. Yes you might have some recourse to bug rhel to fix it, where a crate author might not care, but that really isn&#x27;t the experience I am having. If anything I end up pulling dependencies into my system to build if I can so I don&#x27;t have to wait on the OS company. Between corporate IT and the OS companies delay as middle-men, my OS based dependencies are always the most aged and vulnerable of all my dependencies. reply zozbot234 21 hours agorootparentprev> In this brave new world the author is proposing - what?In practice, you just vendor your code as part of the source package. It&#x27;s the distro maintainers&#x27; job to factor it out as a separate dependency. reply mid-kid 21 hours agorootparentIf you do this, remember to provide an option to factor it out. Meson (mesonbuild.org) makes this completely trivial and pitfall-free, and you should use it if you can&#x27;t get away with just installing your libraries locally. reply varl 20 hours agorootparentI went to mesonbuild.org and it doesn&#x27;t match the description (some sort of betting site? I didn&#x27;t stick around ...), and a search turned up:https:&#x2F;&#x2F;mesonbuild.com&#x2F; reply mid-kid 16 hours agorootparentThanks, I mistyped it. reply mattpallissard 11 hours agorootparentprev> “It works for C&#x2F;C++” by making developers working in those languages pay in blood for every dependency they pull in.I think the industry may have thrown the baby out with the bath water. Now they&#x27;re is a complete aversion to RYO anything, no matter how simple. Personally, I&#x27;m waiting for the pendulum to swing back towards the middle a bit more. reply josephg 3 hours agorootparentEh. I love rolling my own lots of things. At some point I really want to make a little OS kernel. How hard can it be?But then there’s things like rust’s rand crate which implements about 10 different random number generators, all with different properties around speed, whether they’re cryptographically secure and so on. And fed from a variety of entropy sources to make it work everywhere from Linux to windows to wasm. Look at this documentation. It’s an absolute master class.https:&#x2F;&#x2F;rust-random.github.io&#x2F;book&#x2F;guide-rngs.htmlDo you need this? Probably not. You can probably get by with some wrapper around ar4random or whatever the recommended entropy source is today on your particular operating system. But I’ll be honest, until the day I decide to care about how rngs work, I’m going to just import the rand package. And serde & friends for json support. Criterion for benchmarking. And so on.Cargo is full of great libraries that are much higher quality than anything I’d ever throw together in a side project. For most of this stuff, you’d be mad to roll your own.This thread has reminded me that lots of people don’t know how nice it is to use a package manager like cargo. Here’s my favorite crate browsing website. You can see the most popular packages in every category and search. Look how much great stuff is in here. It’s humbling.https:&#x2F;&#x2F;lib.rs&#x2F; reply IAmNotACellist 9 hours agorootparentprev>Yeah. The part that gives me goosebumps with that proposal is: as an author of some not very popular packages, what do I do? Is it my job to package and promote my packages on Debian, redhat, homebrew, and all the others? Do I need usage before they’ll pick it up? Sounds like a chicken and egg problem. And doing all that packaging work sounds like a boring, time consuming job that I don’t want.Github premium (or similar) will have an army of AI builder bots that spin up VMs and modify your codebase until it works, then send you PRs to approve. People would pay a ton for that service. All your projects, automatically maintained and packaged. reply zajio1am 13 hours agorootparentprev> as an author of some not very popular packages, what do I do?Depends. Do you want your software to be used by users? For me as a user, software that is not in distribution repository almost does not exist. It is too much hassle to install, manage and uninstall it compared to installing software from Linux distribution. If i really, really need it, i will make exception, but that are individual cases. reply paulddraper 16 hours agorootparentprevYou create apt, rpm repos.And users install from there.Not saying that&#x27;s a nice answer, but that is the answer. reply josephg 13 hours agorootparentYou create apt, rpm repos. And arch. And gentoo. And FreeBSD ports. And homebrew and macports. And whatever windows does. And you need that to be true, on all those systems, for all your dependencies too. And at a recent, usable version.Or, I just use cargo. Up to date version. One command install that works on every operating system. Project scoped dependencies - so there’s no need for sudo, and different projects can depend on different versions of their dependencies. And I can publish new versions of my packages with 1 command and they’re available everywhere.Just in my house I have a Linux mint workstation, a windows gaming pc, a FreeBSD server and a Mac laptop. Cargo lets me publish code that all of them can consume with 1 command. So does npm.So long as apt is worse than cargo I won’t use it for this stuff. Why does everything have to get installed system wide? Nix doesn’t need that. Why doesn’t it have a programmatic mirror of cargo so we don’t need to mirror rust projects in it by hand? Why do major versions have different package names? And why don’t packages support feature flags? Last time I looked there were about 30 vim packages with random combinations of feature flags set. Why can’t evergreen packages like nodejs or cargo stay up to date in Debian? Old nodejs is useless because it can’t run modern JavaScript.I really want apt to solve all these problems because I hate docker. If apt and friends did their job well, docker wouldn’t be necessary or popular.Cargo isn’t pretty either. But it has all the features I need to get my job done. Make apt a compelling tool and I’ll switch to it. The feature list developers want is hardly a mystery. reply paulddraper 11 hours agorootparentCorrect, there are many operating systems.That is why each modern language has introduced their own package manager. reply josephg 11 hours agorootparentI assume you’re being flippant, but it’s kind of true. You support 20 different things by making a 21st thing that papers over all of them. And along the way you can try to fix some of the design problems.The web. The C programming language. Electron. It’s a tale as old as time. reply takluyver 15 hours agorootparentprevUsers install from there, so long as they&#x27;re using Linux, a Debian or Red Hat based distro, and have sudo access. As you say, not such a nice answer.For certain kinds of software, this approach is getting more viable, thanks to containers. In some contexts, you can tell everyone to run your software in a container with a specific Linux distro inside. But that software is often distributed as container images directly, rather than distro packages to be used in containers. reply mid-kid 21 hours agoparentprevAs a distro contributor, I don&#x27;t share these views at all. Yes, it&#x27;s sometimes a bit of a pain when some application needs an older, incompatible library, but in 99% of the cases, the fallout is minor, or there&#x27;s some compatibility library or shim I can install. The advantage of doing this being that the old application will now support all modern audio&#x2F;video formats (e.g. ffmpeg) and the new graphical and audio subsystems on linux (e.g. sdl), as well as whatever security fixes.If I really need an old library, it&#x27;s generally not hard to install that in a temporary prefix and set the relevant PATH variables for it, though I do wish this was easier sometimes. It gets better as more projects just use pkg-config.That all said, Meson[1] solves all of these issues in a way that keeps both developers and distributions&#x2F;users happy. But as is with all things C, getting everyone and especially windows users to adopt it is gonna take a good while.[1]: https:&#x2F;&#x2F;mesonbuild.org&#x2F; reply saidinesh5 9 hours agorootparent> The advantage of doing this being that the old application will now support all modern audio&#x2F;video formats (e.g. ffmpeg) and the new graphical and audio subsystems on linux (e.g. sdl), as well as whatever security fixes.That&#x27;s the theory. In practise, it just means small app developers putting up with:1) #ifdefs for different versions of same libraries in different distros2) distro specific bug reports for an operating system with less than 2% user base3) binary releases that target just &#x27;Linux&#x27; being out of question for most proprietary applications &#x2F; apps that can&#x27;t&#x2F;won&#x27;t be included in distro repositories for various reasons.It&#x27;s not as if distro maintainers will test your random little application every time they update ffmpeg anyway. It&#x27;s the users, for whom the application breaks. reply lolinder 11 hours agorootparentprev> As a distro contributor, I don&#x27;t share these views at all.That&#x27;s because you&#x27;re in the very small group of people that knows how your distro&#x27;s packaging process works in detail. The majority of developers don&#x27;t use Linux at all, and those that do typically know just enough to install a few blessed packages, not enough to hunt down and install the correct shim in a temporary directly.Multiply that lack of training by the dozens of Linux distros being used by users of a language, and it&#x27;s no wonder most modern languages have a standard language-specific package manager. The alternative is for them to be constantly fielding impossible-to-answer questions from developers who (unlike you) don&#x27;t understand their distro well enough to make compilation work.It&#x27;s far easier for the language to have a single cross-platform package management system where the answer is usually as simple as \"X, or Y if you&#x27;re on Windows\". reply Gigachad 10 hours agorootparentprevI just don&#x27;t attempt to compile C&#x2F;++ software these days because the process is always an absolute nightmare where you have to attempt to compile, then google the error to see what package it&#x27;s related to, try to work out what the distro name for this package is, install, recompile, repeat 10 times. And hope you don&#x27;t get completely stuck where you distro just doesn&#x27;t have the package or version you need.Something in rust is always just `cargo run` and you&#x27;re done. reply chrismorgan 14 hours agorootparentprevWell, clearly what I wrote has resonated with people, as this is now my second-highest-voted comment, currently at +101.(My >4000 comments range in score from −4 to +76 plus one outlier at +287 because people really hate stale bots on issue trackers. See https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32541391 for methodology. This comment has also clearly evoked a strong emotional reaction.)All I can say is that I’ve had a lot of trouble over the years building older C&#x2F;C++ software, often seeking to consume it from Python but sometimes just from C&#x2F;C++. Maybe if I lived and breathed those languages I’d cope better, but at the very least they consistently take extra effort that requires at least mildly specialised knowledge, compared with my experiences of pure-Rust or even mostly-Rust software. reply FullyFunctional 9 hours agorootparentI too have moaned and ranted about this the problem with installing \"modern\", dependency heavy apps -- it&#x27;s to the point where it&#x27;s borderline anxiety inducing. I take the upvotes as a sign it&#x27;s widespread. There are examples of a really valuable package that myself and two coworkers independently have given up installing because the dependency nightmare was just too horrid (a C++ app).When I started using Rust it truly felt like magic. I could any app by anyone and essentially `cargo build` would always just work. So effortlessly. reply dahhowl 20 hours agorootparentprev(wrong link, you mean .com) reply j1elo 8 hours agoparentprevAs a C++ dev myself, and lots of past experiences with code archeology, nowadays I wouldn&#x27;t even bat an eye before reaching for a Docker container of the appropriate system version, where to build an old and unmaintained C or C++ codebase. Otherwise, pain ensues...If we add loading binary libs built for an older system, then there is no question. Who wants to deal with incompatible versions of the GLIBC? reply NobodyNada 12 hours agoparentprevIt&#x27;s even worse when you consider cross-platform or even cross-distro builds. Take a C++ application developed on Linux and try to build it on Mac, or vice versa. This can be really painful even without waiting 5 years because oftentimes two different package managers have incompatible versions of packages, or even if the versions are the same they&#x27;re built with different settings&#x2F;features. Not to mention the pain of search paths and filenames differing across platforms and breaking assumptions in build systems. reply JohnFen 19 hours agoparentprev> Take an app written five years ago that depends on majorish C++ libraries, and there’s a fair chance compiling it will be a nightmareInteresting. My experience doesn&#x27;t back this up at all. I may have issues with 20+ year old code, but I don&#x27;t remember ever having anything but minor issues otherwise.I wonder what the difference between us is? reply bfrog 18 hours agorootparentC++ is horrendous when it comes to backwards compatibility and builds.If you believe that you won&#x27;t run into a compiler bug in a large enough sampling of C++ code to build against you are misrepresenting reality or haven&#x27;t ever looked at the gcc or clang bug trackers.Rust actually does a really nice job along these lines by continuously building and running test for some subset of crates in crates.ioThe only thing maybe even close to that with gcc&#x2F;clang involved I&#x27;ve seen is nixos&#x27;s hydra. reply estebank 18 hours agorootparentNo subset, all of crates.io gets built for every beta amd then stable right before release looking for any changes from the prior stable.Note that this is not all Rust code that exists, but it does help a lot. reply JohnFen 18 hours agorootparentprev> If you believe that you won&#x27;t run into a compiler bugI never made anything like that claim. Nor was I making any comment about the utility of something like crates.io.I was just relating my personal real-world experience working with code. Interesting that I got downvoted, as if what I&#x27;ve experienced in my career was somehow wrong. I certainly wouldn&#x27;t claim that devs reporting the opposite experience from mine are wrong. It&#x27;s just that the difference is stark, and I was curious as to why. reply Aerbil313 17 hours agoparentprevTake a look at nix-shell scripts. Much simpler, lightweight and faster alternative to Docker and friends for being able to build a project forever. Start of a build script from a C project of mine: #!&#x2F;usr&#x2F;bin&#x2F;env nix-shell #! nix-shell -i fish --pure #! nix-shell -I https:&#x2F;&#x2F;github.com&#x2F;NixOS&#x2F;nixpkgs&#x2F;archive&#x2F;4ecab3273592f27479a583fb6d975d4aba3486fe.tar.gz #! nix-shell --packages fish gcc gnumake gmp git cacert ...build script goes here...-i fish: I am using fish shell.--pure: Environment is as pure as it can reasonably get.--packages: Packages from nixpkgs to be made available in the environment.-I : pinned to a specific git commit of nixpkgs. This script will give you the exact same environment with the exact same packages with exact same dependencies forever.You can drop into a shell with the exact same configuration by using the cli command nix-shell with the same options.Admittedly this is not as declarative as using flakes, since it&#x27;s a script, but hey, I&#x27;m lazy, still didn&#x27;t sit down to learn them once and for all.Reference: https:&#x2F;&#x2F;nixos.org&#x2F;manual&#x2F;nix&#x2F;stable&#x2F;command-ref&#x2F;nix-shell reply cge 17 hours agorootparent> Take a look at nix-shell scripts. Much simpler, lightweight and faster alternative to Docker and friends for being able to build a project forever.As researchers using nix derivations to preserve usability of code meant to be archived with our papers, we found that it was not particularly effective for this task. Even with nixpkgs pinning, our code failed to compile after only a few years. Outside of nixos itself, it does not appear that the build environment is the same; aspects of compilation change, and we ended up needing to change compiler flags a few years later, while expecting that we might need to again at some point.Overall nix has been quite disappointing for us from an archival standpoint. reply deredede 2 hours agorootparentThings shouldn&#x27;t change if the nixpkgs version didn&#x27;t change, you didn&#x27;t disable the sandbox, and you don&#x27;t have master branches of random repos as dependencies. If that&#x27;s the case, sounds like a major nix bug.If the sandbox is disabled (as it is by default outside of nixos I believe) then yes, you need to be very careful that there is no outside state that leaks into your build (usually through environment variables, PATH but not only - GCC is a big offender here), which sounds like what happened in your case? For archival purposes you really should be enabling the sandbox. reply jolux 8 hours agorootparentprevJust curious, have you written anything up about these experiences with Nix? I&#x27;ve been learning it recently and the reproducibility angle is a huge part of why, if it&#x27;s oversold then I feel like I should reevaluate how much time I&#x27;m putting into it. reply Aerbil313 13 hours agorootparentprevNix-shell scripts are ad-hoc, what nix does here is to provide an environment. Nix derivations are different and can be fully declarative and pure. Nixpkgs pinning does not mean specific nixpkgs commit pinning or pinning a specific version of a package. You can do these as well. I wonder what was the underlying reason for your condition. reply oneshtein 14 hours agorootparentprevRPM .spec is more readable, IMHO.Archive of system repo + mock + SRPM does the same thing, but offline. reply bfrog 6 hours agorootparentRpm assumes deps and toolchain and doesn’t prevent accidental dependencies last I used it, admittedly many years ago. Are builds somehow done in a limited chroot now? Is the toolchain version dependency tracked and managed? reply oneshtein 3 hours agorootparentIn Fedora, mock[0] is used to build packages in clean chroot for multiple of distros. In SuSE, Open Build Service[1] i used.[0]: https:&#x2F;&#x2F;fedoraproject.org&#x2F;wiki&#x2F;Using_Mock_to_test_package_bu...[1]: https:&#x2F;&#x2F;en.opensuse.org&#x2F;openSUSE:Build_Service_Tutorial replytikkabhuna 22 hours agoprev> If crates.io goes down or access is otherwise disrupted then the Rust community will stop work.As someone who has worked in a big corporate their entire career this statement is bizarre. This has been solved for Java for over a decade.Builds don&#x27;t access the internet directly. You host a proxy on-premise (Nexus&#x2F;Artifactory&#x2F;etc) and access that. If the internet location is down, you continue as normal.> You need only one author in your maybe-hundreds-of-dependencies tree to be hacked, coerced or in a malicious mood for you to have a really bad day.There are products (Nexus Firewall) that can check dependencies for vulnerabilities and either block them from entering the network or fail CI pipelines. reply brabel 22 hours agoparentThe main problem the author is talking about is actually about version updates, which in Maven as well as crates.io is up to each lib&#x27;s author, and is not curated in any way.There&#x27;s no technical solution to that, really. Do you think Nexus Firewall can pick up every exploit, or even most? How confident of that are you, and what data do you have to back that up? I don&#x27;t have any myself, but would not be surprised at all if \"hackers\" can easily work around their scanning.However, I don&#x27;t have a better approach than using scanning tools like Nexus, or as the author proposes, use a curated library repository like Debian is doing (which hopefully gets enough eyeballs to remain secure) or the https:&#x2F;&#x2F;github.com&#x2F;crev-dev&#x2F;cargo-crev project (manually reviewed code) also mentioned. It&#x27;s interesting that they mention C&#x2F;C++ just rely on distros providing dynamic libs instead which means you don&#x27;t even control your dependencies versions, some distro does (how reliable is the distro?)... I wonder if that could work for other languages or if it&#x27;s just as painful as it looks in the C world. reply oblio 21 hours agorootparentEveryone is focusing on Nexus Firewall. Nexus&#x2F;Artifactory themselves are private repos&#x2F;proxies&#x2F;caches that are in widespread use in the Java world so up to a point most companies prune their own repos.This is NOT standard practice in the JS&#x2F;Python&#x2F;PHP&#x2F;whatever world because these products do not exist or are not widely available, everyone \"drinks\" directly from the open internet source.Yes, again, solutions exist, technically, but how many companies do you know, especially smaller ones, that do this?For Java almost every company bigger than 5 devs tends to set up their own Maven repo at some point, many do it on day 1 because at least 1 dev is familiar with this practice from their former BigCo and it&#x27;s easy to set up Nexus&#x2F;Artifactory. reply claudex 19 hours agorootparentArtifactory works well with as a proxy for pip for Python reply oblio 19 hours agorootparentYeah, I know that these days they can do everything, but the cultural bias is there.The average Java dev will try to reach for Artifactory&#x2F;Nexus much sooner than the average Python&#x2F;Ruby&#x2F;PHP&#x2F;Javascript&#x2F;... dev, in my experience. reply pjmlp 16 hours agorootparentOnly because of company culture, in most enterprise projects I work on, doesn&#x27;t matter the stack, everything gets legal and IT approvement into the internal Nexus.Devs might do whatever they feel locally, however if those files are going through CI&#x2F;CD their dependencies better be in Nexus. reply TeeMassive 9 hours agorootparentprevWe use Artifactory as a PyPI mirror and a private package index and it works really great. reply mid-kid 21 hours agorootparentprevIt works in the C world because everyone is super fixated on library compatibility, and the programs are expected to be rebuilt by the distribution when necessary. When an API is broken, this entails a new major version and installing both versions side by side until all maintained software is ported. This doesn&#x27;t happen often enough to be an issue - that&#x27;s the perk of having a really mature ecosystem. reply Aerbil313 17 hours agorootparentOn the other hand, having tasted Nix, the process you are describing seems as backwards and archaic as operating punched card computers to me. So much wasted human time, serving a need (of an ecosystem) now obsolete, thanks to Nix. reply brabel 16 hours agorootparentI&#x27;m trying to use Guix (just prefer it due to the language, Guile, over Nix) but unfortunately it doesn&#x27;t have a lot of software packaged yet. I tried to build a project I have but it doesn&#x27;t even have a \"builder\" for the language I used , Dart (which compiles to binary on all OSs), and I can&#x27;t find out how to create a \"builder\" for it... someone did this for Nix to build pub packages so it&#x27;s possible, but it seems if you want to use Nix&#x2F;Guix you&#x27;re stuck on a very limited set of things you can use? Is that a fair analysis? Do you have hints to build your own software when it&#x27;s not available on Nix&#x2F;Guix? reply dannymi 11 hours agorootparentIn general adding a build system to Guix works like this:1. Git clone https:&#x2F;&#x2F;git.savannah.gnu.org&#x2F;git&#x2F;guix.git2. Nicolò Balzarotti already has a work in progress patchset adding Dart itself--see https:&#x2F;&#x2F;issues.guix.gnu.org&#x2F;44926 . Patch your guix checkout with the files you can download on that page via the downarrow buttons on the right.3. Create a file guix&#x2F;build-system&#x2F;dart.scm (I&#x27;d copy it from guix&#x2F;build-system&#x2F;dub.scm and then s&#x2F;dub&#x2F;dart&#x2F;g and s&#x2F;ldc&#x2F;dartcc&#x2F;g) or whatever it is. This file is the \"front end\" and is support for dart for the guix package manager script.4. Create a file guix&#x2F;build&#x2F;dart-build-system.scm (I&#x27;d copy it from guix&#x2F;build&#x2F;dub-build-system.scm and then s&#x2F;dub&#x2F;dart&#x2F;g). This file is the \"back end\" and is responsible for the build of your package inside a new container.5. Add references to your new files to gnu&#x2F;local.mk6. In your user package script (whatever you want to compile in dart), add #:use-module (guix build-system dart) near the top and then add (build-system dart-build-system) to your package.7. guix environment --pure guix8. .&#x2F;bootstrap9. .&#x2F;configure --localstatedir=&#x2F;var10. make -j511. exit12. .&#x2F;pre-inst-env guix build -f yourpackage.scm or whatever13. Fiddle with the \"invoke\" parts in guix&#x2F;build&#x2F;dart-build-system.scm until your user package works.14. Submit your results to 44926@debbugs.gnu.org or guix-devel@gnu.org (no need to be subscribed).As you can see, adding a new programming environment is quite some work. Because every programming environment has its own idiosyncracies, I don&#x27;t think it can be avoided even in principle. reply brabel 2 hours agorootparentThanks for the detailed information. Looks like it&#x27;s a huge effort. That Pull Request is adding Dart 2.8 while the current version is 3.1.5 (and it&#x27;s released often, at least once a month). Most pub packages don&#x27;t even work on Dart 2.x anymore. An unrelated question: what do you mean by my \"guix checkout\"? I saw this in some docs (to add guile-guix to the guile load path so emacs can help me write guix code) and I don&#x27;t know where that is as I am using Guix OS (in a VM) and as far as I know there&#x27;s no sources in it? reply Aerbil313 14 hours agorootparentprevSee https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38171379Also, nix can do arbitrary building (arbitrary commands). You don’t need a specialized builder. You can write a specialized builder for your language too, if it doesn’t exist. reply jjgreen 22 hours agorootparentprevCoding mainly in C for 20 years, I&#x27;ve never had a program break on minor version of system shared library conflict. Occasionally need to recompile on OS version update when the major version increments. reply lmm 7 hours agorootparentThe C community basically doesn&#x27;t bother with libraries. They&#x27;re so cumbersome to do in C that you&#x27;ll tend to only have, like, 20 giant libraries in your (non-vendored) dependencies. reply goodpoint 21 hours agorootparentprevThat does not solve supply chain attacks. reply mid-kid 21 hours agorootparentIt solves most supply chain attacks. Distro maintainers are users, not developers. They put the needs of a user first, and their work is reviewed during submission. If one of them goes rogue, the damage is usually caught before reaching the user and otherwise minimal, as the update is simply reverted by other maintainers and a different maintainer can take over their packages. reply goodpoint 19 hours agorootparentSorry, I misread jjgreen&#x27;s comment. I 100% agree with what you are saying. reply amelius 20 hours agorootparentprev> There&#x27;s no technical solution to that, really.There is. Theoretically speaking, you could confine libraries to their own sandbox.But Rust can&#x27;t do that. reply pertique 17 hours agorootparentDo you have other info on languages that do? In my mind, if a library is compromised and incorporated in software that touches sensitive data then there&#x27;s no way to meaningfully sandbox it. Sure, the library lives in the sandbox, but so does the stuff you&#x27;re trying to protect. I&#x27;m not sure that&#x27;s a Rust-specific problem. reply amelius 15 hours agorootparentThe library should live in its own sandbox, separated from the rest of the system including any other libraries. All system calls have to go through a permissions system that (e.g.) denies access by default.There is at least one language that tries to do that (someone posted it here in a comment some time ago, but I can&#x27;t find it). reply marwis 9 hours agorootparentprevJava (look up Security Manager).Unfortunately it is deprecated and scheduled for removal because too much security is annoying and not enough people care to use it. reply wongarsu 21 hours agoparentprevAnd this is easy to do with Rust as well. The article mentions both major options: store your dependencies in your repo using the built-in `cargo vendor` command, or run a mirror like panamax that just mirrors all of crates.io (or just the packages you need, but then you run into issues with updating if crates.io is ever down).Mirroring all of crates.io takes about a terabyte of storage, not much bandwidth and is really easy to set up, but judging by the download numbers of obscure packages there are only about 20 places in the world doing that. Maybe people haven&#x27;t bothered because crates.io is so stable, maybe everyone goes the vendoring route instead. reply galangalalgol 21 hours agorootparentWhy would you ask panamax to mirror everything though? You are maintaining a manifest of all your organizations dependencies right? Why would I mirror ancient versionsbof clap and that crate that checks your aol mail? Also, nexus doesn&#x27;t have a functioning cargo plugin, it is community made and stagnant. Artifactory does support it. And I&#x27;m guessing artifactory is what most people are using for a private registry too given the state of the others. reply wongarsu 20 hours agorootparentMirroring everything gives me the ability to not just build existing software independently of crates.io outages or decisions, but also to not be impacted in software development by such incidents. To me development involves using packages or package versions I might not have used before.Of course if your development is already set up in a way that prevents you from easily adding, experimenting with or updating dependencies, maybe because of how the vetting process works, that might not be an issue for you. And many places are completely fine with just being able to build reply devman0 18 hours agorootparentHaving worked in enterprise software, it has been my experience that the majority of in-house repos operate in a caching mode and not mirroring. The critical element is being able to build things in CI that are being PR&#x27;d&#x2F;reviewed or are already merged and in both cases the relevant dependencies should already be cached. That and being able to re-build any tagged releases, again should be cached. New libraries being temporarily unavailable are generally not going to hold up critical processes, and the upstreams have been available enough for that to never be a serious issue. reply jamesfinlayson 3 hours agorootparentAgreed - I can&#x27;t remember what was used at my last job but you submitted a package to be mirrored and then each night it pulled in any new releases. reply andrewaylett 14 hours agorootparentprevI&#x27;m quite happy -- even in commercial development -- to not be able to download new things if the package source is down. But not being able to access my existing dependencies is a blocker: I must be able to redeploy my own stuff, and to make fixes that don&#x27;t involve version bumps.It&#x27;s unlikely that I&#x27;ll need a new dependency at the exact moment that the source is down, but fairly likely that if the source is down then I&#x27;ll want to build something. For personal projects I&#x27;ll have stuff cached locally, and I don&#x27;t mind so much if I need to wait for CI. For work stuff I need to be able to fetch my dependencies in CI.Also, hammering upstream with requests for every single build is both slow and also sort of rude. reply galangalalgol 19 hours agorootparentprevYeah, that makes sense and we should probably expand a bit. I can&#x27;t imagine wanting versions that are older than anything we support or what those things bring in. Also we prune stuff that gets unmaintained or has a cve pop up. And we don&#x27;t mess with copyleft, not against it, but when everything is statically linked you get into incompatible combinations quickly. reply sgu999 21 hours agorootparentprev> `cargo vendor`I had no idea \\o&#x2F; Thanks! reply api 21 hours agorootparentprevThese days some laptops have enough storage to comfortably mirror all of multiple language package repositories especially if you use a compressed volume for them. reply tgv 21 hours agoparentprev> There are products (Nexus Firewall) that can check dependencies for vulnerabilities and either block them from entering the network or fail CI pipelines.Once they&#x27;re known. I suppose nexus firewall let log4j pass. reply ashishbijlani 21 hours agorootparentI’ve been building Packj [1] to detect publicly UNKNOWN dummy, malicious, abandoned, typo-squatting, and other \"risky\" PyPI&#x2F;NPM&#x2F;Ruby&#x2F;PHP&#x2F;Maven&#x2F;Rust packages. It carries out static&#x2F;dynamic&#x2F;metadata analysis and scans for 40+ attributes such as num funcs&#x2F;files, spawning of shell, use of SSH keys, network communication, use of decode+eval, etc. to flag risky packages. Packj Github action [2] can alert if a risky dependency is pulled into your build.1. https:&#x2F;&#x2F;github.com&#x2F;ossillate-inc&#x2F;packj 2. https:&#x2F;&#x2F;github.com&#x2F;ossillate-inc&#x2F;packj-github-action reply livealight 21 hours agorootparentprevIt let log4j pass for as long as it was known to be good. Within hours of the CVE opening the tool was blocking it. The purpose of dependency firewalls is to avoid two things: known badly vulnerable packages AND known malicious packages that serve no other purpose than to steal data or drop a trojan. No security is 100% bulletproof, but it&#x27;s really surprising how much of the damage is done by 7 year old CVEs. Firewalls can be useful in exactly that. reply oblio 21 hours agorootparentprev> Once they&#x27;re known.Isn&#x27;t that how life works? Unknown unknowns? :-)> I suppose nexus firewall let log4j pass.It should be reasonable to assume that a product dedicated to something will, on average, do this job better&#x2F;faster than a random developer, though. Since that&#x27;s their job.So Nexus Firewall presumably blocks these vulnerabilities faster than the average app&#x2F;system developer notices them. There&#x27;s value in reducing the delta between the vulnerability exploit day and an entire ecosystem being patched&#x2F;fixed. reply vilunov 21 hours agorootparentprevDebian devs also let log4j pass. reply frakkingcylons 9 hours agorootparentIt&#x27;s pretty absurd to expect distribution maintainers to be discovering 0days. reply tikkabhuna 18 hours agorootparentprevYes, but nothing can prevent you from consuming unknown vulnerabilities.Once log4shell was discovered it was trivial to eliminate it from the organisation. We knew exactly what projects were using it and updated them accordingly. Access logs showed us that it was no longer used and we delete it. reply KronisLV 21 hours agoparentprev> As someone who has worked in a big corporate their entire career this statement is bizarre. This has been solved for Java for over a decade.> Builds don&#x27;t access the internet directly. You host a proxy on-premise (Nexus&#x2F;Artifactory&#x2F;etc) and access that. If the internet location is down, you continue as normal.Right now I&#x27;m using Sonatype Nexus (the free version) for my personal needs: https:&#x2F;&#x2F;www.sonatype.com&#x2F;products&#x2F;sonatype-nexus-repositoryAnd I do mean most things: apt packages, Docker images, Helm charts, as well as packages for Go, Java, .NET, Node, Python and Ruby, maybe even more in the future. I&#x27;m very glad that it is possible to do something like that, but honestly it&#x27;s hard to do and I can easily see why most wouldn&#x27;t bother: even though the configuration UI is okay and you can create users with appropriate permissions, you&#x27;ll still need to have plenty of storage, manage updates and backups, setup automated cleanup tasks (especially for Docker images), in addition to the whole process on the developer&#x27;s side sometimes being a nuisance.With npm you get --registry which is one of the simpler approaches, Ruby Gems needed some CLI commands along the lines of \"bundle config mirror\" and other stuff, Maven needs an XML file with the configuration, in addition to publishing your own packages to those also needing more configuration. Sadly, it seems like this fragmentation of approaches is unavoidable and complexity increases with the amount of languages you use.On the bright side, I keep control over my data, as well as have really good download&#x2F;upload speeds, even on a cheap VPS. Plus you learn some vaguely useful things the more you explore if you want to go further, like how I also start with Debian&#x2F;Ubuntu base container images and provision whatever tools or runtimes I need for more specialized images (e.g. my own JDK, .NET, Node etc. images). reply tikkabhuna 18 hours agorootparentThe UI does leave a lot to be desired, definitely! I wish they&#x27;d add specific UIs for the different formats.I agree with the pain that some languages&#x2F;package managers add as well. Its a shame package managers aren&#x27;t necessarily designed to easily support writing your own mirror or switching repository.Fair play for running your own Nexus for personal use! reply oblio 21 hours agorootparentprevI don&#x27;t know if they ever implemented it, can it offload the storage to S3? The cost is peanuts and you stop bothering about the annoying recurrent storage issues. reply KronisLV 8 hours agorootparent> I don&#x27;t know if they ever implemented it, can it offload the storage to S3?There are actually multiple different kinds of blob store backends: https:&#x2F;&#x2F;help.sonatype.com&#x2F;repomanager3&#x2F;nexus-repository-admi... though so far I&#x27;ve only used the File one, because my servers have incremental backups that can be restored in a few clicks (that I&#x27;ve sometimes done in the past). reply hun3 21 hours agoparentprev> There are products (Nexus Firewall) that can check dependencies for vulnerabilitiesProducts can only do best effort scanning for known patterns of vulnerabilites. Those are only added after someone (or something) discovers it and verifies that it&#x27;s not a false alarm. In between the time gap, scanners are ineffective and anything in your infrastructure can run the malicious code and get hacked. (Google supply chain incidents on npm or pypi.)In general, there is always a way to bypass it, since the language is turing complete and unsandboxed. Anything going beyond that is a lie, or simply impractical. Systems with latest software and antivirus get hacked all the time. Nothing can really stop them. Why?- https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20160424133617&#x2F;http:&#x2F;&#x2F;www.pcworl...- https:&#x2F;&#x2F;security.stackexchange.com&#x2F;questions&#x2F;201992&#x2F;has-it-b...- https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rice%27s_theorem> and either block them from entering the network or fail CI pipelines.Also, relying solely on an endpoint security product for protection is dangerous since antiviruses themselves get hacked all the time. Sonatype for an example:- https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;search&#x2F;results?form_type=Basic&res... reply hoosieree 21 hours agorootparentMy research is about detecting semantically similar executable code inside obfuscated and stripped programs. I don&#x27;t know what commercial antivirus or vulnerability scanners use internally, but it&#x27;s possible to generate similarity scores between an obfuscated&#x2F;stripped unknown binary and a bunch of known binaries. I suspect commercial scanners use a lot of heuristics. I know IDA Pro has a plugin for \"fingerprinting\" but it&#x27;s based on hashes of byte sequences and can be spoofed.My approach is basically: train a model on a large obfuscated dataset seeded with \"known\" examples. While you can&#x27;t say with certainty what an unknown sample contains, you can determine how similar it is to a known sample, so you can spend more of your time analyzing the really weird stuff.The hardest part in my opinion is generating the training data. You need a good source code obfuscator for your language. I&#x27;ve seen a lot of papers that use obfuscator-llvm[1] to obfuscate the IR during compilation. I use Tigress[2] to obfuscate the source code because it provides more diversity, but it only supports C.[1]: https:&#x2F;&#x2F;github.com&#x2F;obfuscator-llvm&#x2F;obfuscator&#x2F;wiki&#x2F;Installat...[2]: https:&#x2F;&#x2F;tigress.wtf&#x2F; reply hun3 18 hours agorootparentGreat work! For unobfuscated or lightly packed ones, I guess your approach could mostly work.One question: how do you detect when a binary is intentionally made to statically look similar to one binary, while its behavior actually mimics another? reply richardwhiuk 22 hours agoparentprevEvery serious enterprise runs a Rust proxy - and indeed Artifactory &#x2F; Azure Artifacts and others support this. reply jrpelkonen 19 hours agorootparentSure, Artifactory, etc. have their places. But I am objecting to your \"no true scotsman\"[1] type of argument. Furthermore, the most bizarre setups I have seen have been in \"serious\" enterprises. Hardly an endorsement.1: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;No_true_Scotsman reply devman0 18 hours agorootparentI have to agree with the poster you are replying to. If an entity is not doing the bare minimum to QA software supply chain, then they are not \"serious\". That being said, I have seen many a large enterprise that do not treat software development as a serious endeavor and it shows. reply brightball 22 hours agoparentprevGitlab has this built in as well.EDIT: Has a proxy for packages.https:&#x2F;&#x2F;docs.gitlab.com&#x2F;ee&#x2F;user&#x2F;packages&#x2F;package_registry&#x2F; reply basedrum 19 hours agorootparentThat isn&#x27;t a proxy, it&#x27;s a registry for packages you build reply basedrum 21 hours agorootparentprevHas which built in? reply brightball 20 hours agorootparentPackage registry proxy (linked in original comment) reply keep_reading 16 hours agoparentprevI have CI pipelines fail when Github goes down because of crates.io being inaccessible.Does anyone have a proxy for Rust? I&#x27;d love to run it. Please. Someone... helpedit: just learned Panamax exists for this... will try it reply duped 14 hours agoprevYou are fundamentally limiting your ability to build Rust projects if you rely on the system&#x27;s package manager.A Rust program `p` can depend on `q` and `v` which depend on `u`, but `q` requires `u` @ 1.0.0 and `v` requires `u` @ 2.0.0. In the Debian world, you would have to change `p` and `v` to support a mutually compatible version of `u`. If those changes introduce bugs, they&#x27;re Debian&#x27;s bugs, but the author of `p` is the one who gets the reports.Cargo naturally handles this case, it installs multiple incompatible versions at unique locations in its cache, and correctly invokes the system linker to link the compiled objects correctly. If `p` was a C program, we could not handle this case, because `u` would be a shared library and you cannot link two different versions of the same shared object into a single address space at runtime (if they were static libraries, a sufficiently smart build system could deal with it, just like cargo - but there is no unified build system and package manager for C, so good luck).Further, Cargo creates lock files for your projects, and authors commit and push them for executables. If you build against a lockfile. This prevents most of the cases that the author is bemoaning. People don&#x27;t run Cargo update willy nilly.Frankly, the \"problems\" the author is claiming don&#x27;t exist in practice and the \"solution\" makes life worse as a Rust developer.I&#x27;m getting tired of hearing people praise broken package managers like Debian. They make life worse for everyone for no benefit. How many times do people need to read testimony that \"C&#x2F;C++ package management sucked and Rust is great just for Cargo alone!\" before they get it? reply TeeMassive 9 hours agoparentLock files are only one part of the solution. Even with a lock file dependencies need to be fetched from somewhere and if a particular version of a package was deleted by its author then you&#x27;re back to case one.\"People don&#x27;t run Cargo update willy nilly.\"We don&#x27;t know the same people then. Most devs like to keep their stuff up to date. Auditing the dependencies tree before updating a lock file is like reading a ToS. Most people don&#x27;t even if they should.The problem of securing the chain of production is not solved with a simple lock file. reply estebank 8 hours agorootparent> Even with a lock file dependencies need to be fetched from somewhere and if a particular version of a package was deleted by its author then you&#x27;re back to case one.That&#x27;s not quite correct. Even if a library version is yanked (which hides it in crates.io and makes it so cargo will skip that version during resolution), if it appears in a lock file, cargo will dutifully find it and use it. This is explicitly to avoid the leftpad behavior of a library disappearing breaking your build. reply duped 7 hours agorootparentprevThey get fetched from your local cargo cache, unless you purged that too. You&#x27;re also not back to case 1, you get a compilation error that tells you that the package doesn&#x27;t exist and you&#x27;ll need to audit a replacement. This is a lot better than trusting someone to drop a replacement in automatically. But if you want that, there&#x27;s cargo update - and like you said, it has issues (because updating dependencies always has issues).> The problem of securing the chain of production is not solved with a simple lock file.I didn&#x27;t say it was. I said the issues the author describes almost all are. Relying on distro maintainers doesn&#x27;t solve them either. reply nindalf 18 hours agoprevThe author makes one big assumption - that the packaged code in Debian is better because it has been reviewed and vetted by someone competent. If this was true, it would make this approach worthwhile, but I don&#x27;t think it is. The author could have pointed to some data that demonstrates this, but didn&#x27;t. I remain unconvinced.As far as I can tell, this is more about the feeling of security because of familiarity. I can completely understand why they would feel more comfortable using something they&#x27;ve successfully used for many years, but that feeling doesn&#x27;t actually translate to improvements in security. There is a feeble attempt to suggest that slowing down the rate of change improves security, but I would prefer to have the latest versions of the language toolchain and libraries available immediately, so I can make a decision on what to use myself.If security actually needs to be addressed, it&#x27;s going to be by paying maintainers and paying people to review dependencies. And that would likely be in the framework of PyPI&#x2F;crates.io&#x2F;npm, because none of them place constraints on where the code can be used, unlike using a single Linux distro&#x27;s package manager. reply Arnavion 18 hours agoparent>The author makes one big assumption - that the packaged code in Debian is better because it has been reviewed and vetted by someone competent.They do not make this assumption, and in fact state the opposite. reply nindalf 17 hours agorootparentThey state that> They might decide to give the diff at least a cursory look, which is better than nothing.So yes, they do value the cursory review, which isn’t that useful. And they talk up the security benefit of only pulling in a subset of published updates, which is even more dubious.Contrast this with the approach that Google is taking here - https:&#x2F;&#x2F;opensource.googleblog.com&#x2F;2023&#x2F;05&#x2F;open-sourcing-our-.... Actual, careful review of the code that anyone can take advantage of. And no limitation of having to choose one flavour of one OS. This review helps you audit your supply chain regardless of OS you’re developing and deploying on. reply Arnavion 15 hours agorootparentYou read and quoted the third point. Now why you don&#x27;t read and quote the first two? reply hobofan 23 hours agoprev> There is no mediation of any kind between when a new library&#x2F;version is published and when it is consumed.> A simple time delay will allow egregious malware like malicious build.rs scripts to be caught, whether that’s the super-long Debian stable cycle or even the several days required to migrate from unstable to testing.Normal cargo + crates.io usage has the same property via the existence of lockfiles. For most people that care about stability, this will provide a natural delay (which you can also see in crates.io download graphs) to consuming the latest version.-----The other points brought up in the article are definitely valid, but if that&#x27;s the main strategy of defense you are looking for, you might already have it. reply dathinab 22 hours agoparent> if that&#x27;s the main strategy of defensemostly, there can be a small attack gap when adding new dependencies to the project and at the specific point in time when you run `update`through you can also pin versions in `Cargo.toml` and then review any updates, maybe except for a few highly trusted sources (it&#x27;s a bit annoying and costly (time wise) but viable)Through trying to vendor things, especially with Debian, seems like a horrible solution. And there is a lot of precedence for this causing tons of headaches, wrong bug reports and similar for developer (== time loss == time is money so we could probably be speaking about multiple millions of monetary damages).Through I have been thinking for a while that it could be interesting to have a partial crates.io \"proxy\" which only contains and updates crates which have gotten a \"basic screening\" in all of their code and dependencies probably with some tool assistance this might not find bugs but it should find many forms of supply chain attacks. Through given that this is costly to provide it probably would not be a free service. reply hobofan 22 hours agorootparent> there can be a small attack gap when adding new dependencies to the projectMost package managers will keep the versions of transitive dependencies as unchanged as possible when adding a new direct dependency.Of course if the only solution to satisfy the dependencies of the new direct dependency is to upgrade a transitive dependency, that will be done.(I&#x27;ve seen a lot of people treat dependency additions as completely unpredictable operations that regenerate the whole lockfile in the past, which is why I wanted to clear this up.) reply jiripospisil 22 hours agoparentprev> Normal cargo + crates.io usage has the same property via the existence of lockfiles. For most people that care about stability, this will provide a natural delay (which you can also see in crates.io download graphs) to consuming the latest version.Be aware however that lock files are ignored by default. You need to specify \"--locked\" if you want Cargo to use the \"locked\" versions from Cargo.lock.> By default, the Cargo.lock file that is included with the package will be ignored. This means that Cargo will recompute which versions of dependencies to use, possibly using newer versions that have been released since the package was published.https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;commands&#x2F;cargo-install.html#... reply MrJohz 21 hours agorootparentThis is installing binaries from crates.io directly (i.e. running \"cargo install ripgrep\" instead of \"sudo apt...\" or whatever equivalent). Admittedly, this is often a suggested installation method for some built-in-Rust tools, but it&#x27;s usually an option of last resort, and most people should be using their usual package manager for this.Cargo does respect the package lock file when building a project from source (i.e. when you checkout a project, and run `cargo build` in that project directory). This happens, for example, when running builds in CI, or preparing a new release for each individual package manager. reply jiripospisil 21 hours agorootparentI responded in a sibling comment because it came first. reply hobofan 22 hours agorootparentprevThis only applies to the `cargo install` command to install binaries from crates.io.If you work in a repository on your own project and use basically any other cargo command (e.g. `cargo build`, `cargo run`), the lock file is of course taken into account (otherwise it would be near-useless). reply jiripospisil 21 hours agorootparentRight but that&#x27;s not the only use case for `cargo install`. You can also use it to install binaries system wide from cloned repositories (`cargo install --path .`). I use this often for small utilities (for which I have no intention of publishing to crates.io) or when testing before packaging for Arch. reply Dowwie 22 hours agoprevThese concerns pertain to every language using a centralized package authority. None of them-- crates.io, npm, pypi, hex -- are adequately funded by the multi-billion dollar industries they enable. So, they aren&#x27;t as resilient and secure as they can be. The solution isn&#x27;t to create yet another package authority but to reinforce those that the ecosystems are already relying on. reply metaltyphoon 19 hours agoparentNpm is owned by Microsoft no? reply jjice 18 hours agorootparentYes https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;microsoft-buys-javascript-deve... reply pasloc 23 hours agoprevIn my opinion this just fragments the rust ecosystem and creates more problems than it solves. What about to just have mirrors of crates.io?And I completely disagree that that someone else than the authors should distribute or manage the package. They know the best how and what there is to do in certain situation (security fixes and so on). Of course it is good to have independent reviews, but these independent people should not be in charge of updating, maintaining and updating these packages&#x2F;libraries. reply jkbbwr 23 hours agoprevHaving worked with C&#x2F;C++ projects. Managing dependencies is downright painful. About the best option we found was to treat everything as build from source and then git submodule our dependencies in. This is still not good but at least it gave us a consistent environment. reply palata 22 hours agoparent> Managing dependencies is downright painful.The risk when it is too easy is that you suddenly pull half of the Internet into your project. In C++, the cost of having a dependency makes me choose carefully if I want a dependency or not, and if I do, if I want this particular dependency (Who develops it? Is it maintained? If it stops being maintained, can I easily move to an alternative, or could I take over maintenance? ...).> About the best option we found was to treat everything as build from source and then git submodule our dependencies in.If you mean having a git submodule and doing something like `add_subdirectory(dependencyA)` in CMake, please don&#x27;t do that. Building from source is fine, but then you can totally install the dependency locally and have your build system (be it CMake, Meson, ...) fetch it. At least I started doing it the way it is described here [1] and it works well for me.[1]: https:&#x2F;&#x2F;www.acarg.ch&#x2F;posts&#x2F;cmake-deps&#x2F; reply vilunov 21 hours agorootparentRequirement to \"install dependencies locally\" is part of the pain with C++ dep management. Not having to do it makes builds much easier to define, as you don&#x27;t need to supports lots of distros and hope that everyone of them is up to date. reply palata 21 hours agorootparentI don&#x27;t know, I feel like it is a bit of a tradeoff.You can totally say \"here is my code, it&#x27;s open source, and the build system will look for those dependencies on the system\" and let the users (who are developers) handle the dependencies the way they want. This way the user gets to choose if they trust the dependencies or not. Ideally the distro ships with those dependencies, and the user can just install them (better: write a package that depends on those dependencies and install that).It seems like developers don&#x27;t really know how to do that anymore, and language package managers allow them to not learn about it. But at the cost of control: they don&#x27;t know anymore what dependencies they are pulling, or even how many they are pulling.The modern way seems to be mostly about productivity, and therefore devs don&#x27;t want to learn anything about dependencies, they want everything to \"just work\". But that is arguably bad for security. reply josephg 21 hours agorootparentI remember doing that whenever I wanted to build a C program on GitHub from source. “Oh cool, so I’m going to need these 8 dependencies. Let’s see which ones are supported on my distribution. Ok, I have 6 of them but 2 are weird versions from many years ago. No way that will cause problems! The other 2 I can install from source - oh, but the latest version of this dependency in git needs something else I don’t have…” and there goes my whole evening.Is my computer more secure for it? Am I in more control? Absolutely not. I have weird 3rd party programs installed in weird places all over my system. I have a computer that is almost impossible to rebuild if my hard drive dies. Finally I can make fragile builds that might have bugs that nobody else can reproduce because of the idiosyncratic combination of package versions I’m using.This is so unbelievably worse than cargo or npm. With cargo I can take any package on GitHub and reasonably install and run it on any computer I own (running any OS) with one command. The only time this breaks down is when part of the dependency tree is C code. It’s miraculous.Give me that experience in apt and I’m there. But the experience apt currently provides is unbelievably bad for developers compared to cargo&#x2F;npm&#x2F;etc. I can only assume nobody who works on Debian has seriously tried npm or cargo. reply palata 20 hours agorootparent> oh, but the latest version of this dependency in git needs something else I don’t have…” and there goes my whole evening.Why not start by building the version of that dependency supported by the project you want to build? It&#x27;s not like you ask cargo to take the latest versions of all the dependencies, is it?> This is so unbelievably worse than cargo or npm.I agree that in many cases it is less convenient. But in your example, 6 out of the 8 dependencies come from distro packages, which I think is better. For the last two, you could actually contribute community packages.Have you ever tried Arch Linux, for instance? There is the Arch AUR where the community can contribute packages. I don&#x27;t remember having had to compile a dependency from source, because everything I need is either in the official repo, or on the AUR. It is explicit when I need to get it from the AUR, so I know that I may want to pay more attention to what I am pulling. And usually I do and have a quick look at the package recipe (and its dependencies). In this case I would have to check 2 of them, and not 8, which is a big win.That is of course less convenient, but I believe it is more secure. I see it as a tradeoff. reply josephg 20 hours agorootparent> Why not start by building the version of that dependency supported by the project you want to build?Because I don’t know which version that is half the time. Does the readme say to use version 2.2 or 2.0.1? Are they meaningfully different? Maybe 2.2 is needed to be compatible with something else on my system. (Nvidia has entered the chat). I have no idea what is in the changelog of some random dependency that I’ve never heard of before - and I just remembered, I don’t care. I never care.> For the last two, you could actually contribute community packages. Have you ever tried Arch Linux, for instance?That sounds like an even more effective way to waste an entire evening. Maybe multiple evenings! When I see a cool program someone’s written that I want to play around with, what better use of my time could there possibly be than figuring out how to submit community made packages to arch Linux for the random dependencies of some random software someone linked me? All this before I’ve even built the program I want to try out? No thanks.And how is it more secure? Do programs magically get security audits as part of their addition to arch Linux community packages?Your comment reminds me of that infamous response in Dropbox’s “Show HN” thread. “Well, the problem Dropbox solves sounds like something easily done with rsync which is already part of arch Linux. Have you tried arch Linux? Instead of starting a billion dollar company, you could submit a simple bash script as a community contribution to arch. Using a bash script in arch Linux is of course less convenient. But I believe it is a trade off.”And to answer the unspoken question, no. Arch Linux doesn’t have the tens of thousands of up to date packages that are already in cargo. And are already available on every operating system under the sun. Manually adding them to arch sounds like a pointless task that would only serve to make updating my dependencies more difficult and make my software less compatible on all the other systems it already, effortlessly works on. (Like Debian, FreeBSD, macOS and windows.) reply palata 20 hours agorootparent> Does the readme say to use version 2.2 or 2.0.1? Are they meaningfully different?If the library is done right, then 2.2.0 should work if it requires 2.0.1, and the reverse may not work (if the program you want uses a feature that was added after 2.0.1).> and I just remembered, I don’t care. I never care.Yeah, I think it is a big reason why language package managers are so popular: most don&#x27;t care.> When I see a cool program someone’s written that I want to play around with, what better use of my time could there possibly be than figuring out how to submit communityFirst, probably those packages were already contributed by someone else. Someone who cared.Then... for the rare packages that may not already be there, I would hope that you could consider spending a couple hours contributing something back to the community you are most likely using for free and complaining about. For most libraries it should not even take two hours, except maybe the first time ever you do it.> And how is it more secure? Do programs magically get security audits as part of their addition to arch Linux community packages?As I said above, the ones that are shipped by the Arch official repo seem more secure. For the community ones, it&#x27;s less clear, but I would argue that AUR users are at least not less likely to review a package than the cargo users are to review a transitive dependency.> Using a bash script in arch Linux is of course less convenient. But I believe it is a trade off.”Well if you want to use this as an argument, you should say why it is a tradeoff. Why is it a tradeoff? Are you saying that rsync is more secure than dropbox, but less convenient? reply josephg 13 hours agorootparent> Then... for the rare packages that may not already be there, I would hope that you could consider spending a couple hours contributing something back to the community you are most likely using for free and complaining about.Is me volunteering my time and expertise to write and publish opensource code not virtuous enough? “If you really loved opensource, you’d also do this other task constantly that doesn’t even take 2 hours each time”.Why does arch even need rust packages to be added by hand? I’ve already programmatically expressed the contents and dependencies of my rust package. And so have all my dependencies. Why not just mirror that programmatically if you want my dependency tree in arch?> the ones that are shipped by the Arch official repo seem more secure. For the community ones, it&#x27;s less clear, but I would argue that AUR users are at least not less likely to review a package than the cargo users are to review a transitive dependency.It seems more secure? You’re making security assessments based on vibes?I can’t speak for others, but I’m personally much more likely to review my dependencies in rust or npm than in Debian or whatever because the source code is available and linked from the cargo package page. And I can control+click in to my dependencies (or debug into them) and read their code. And with npm they all live in node_modules. Source and all. I dive in there all the time. Does arch do that too? I have no idea where to look for the source code of a package I installed using apt. I’d probably need to Google the package and hope apt hasn’t patched it in any significant ways that make the version on GitHub out of date. reply palata 11 hours agorootparentJust to be clear: I mentioned arch as an example of a distro that has a very active community repo (AUR).> It seems more secure? You’re making security assessments based on vibes?I shared an opinion, I am not publishing a security paper. You also say a ton of apparently uninformed stuff that expresses your \"feeling\". Like \"with Arch I would constantly have to contribute packages myself\" (Did you try it? Do you have statistics and proofs?). You are entitled to disagree with my opinion, just as I am entitled to have one.I think that we won&#x27;t agree here, and for the record I was not saying that language package managers were fundamentally bad. I was merely noting that I see pros and cons on both approaches. I tend to defend the \"distro maintainers\" side more often, because in my experience, most developers don&#x27;t know about it.> I have no idea where to look for the source code of a package I installed using apt. I’d probably need to Google the package and hope apt hasn’t patched it in any significant ways that make the version on GitHub out of date.Exactly my point. You don&#x27;t like the distro way because you don&#x27;t know anything about it. Not saying it is not a valid point: you are entitled to your opinion.My opinion is that there are pros with distro package managers, like the fact that maintainers put together a set of packages that they ship as a distribution (that is the whole point of a distribution). reply josephg 10 hours agorootparentI agree that we probably won’t agree on this.Re: security, specifics matter. AUR “feeling” more secure than cargo doesn’t mean it is. If the principle reason to use it is security, tell that story. How is it better?> You don&#x27;t like the distro way because you don&#x27;t know anything about it.I’ve been using Linux since a version of Slackware I installed off floppy disks. I remember trying apt for the first time and thinking it was an absolute revolution. The best thing since sliced bread. I haven’t tried arch but for what it’s worth, gentoo gets the source thing right. You can see the source of anything on your system with 1 command. And better yet, it all builds. There’s no stupid -dev version of packages like there is in apt (that drives me nuts). I’ve still never submitted a package to any package manager - and maybe you’re right. Maybe I should.I’m upset by all of this because I want apt and arch and all the rest to be better. But as far as I can tell, apt hasn’t improved in decades. And now docker (which I hate even more) has come along and solved a bunch of apt’s problems for it by making something even uglier on top. What I want is a package manager with these features:- Reproducible, hermetic environments (nix)- Local dependency installation (nix, docker, cargo, npm)- Cross-platform packages (docker, cargo, npm. Nix is trying but not there yet)- Cross language packages (nix, apt, docker)- Feature flags (cargo)- Support for the same package to be depended on at multiple different versions in the dependency tree (cargo, npm)- Semver API compatibility checks (nobody does this, but Cargo is working on it.)- Simple, cross platform publishing. None of this “here’s how you install it on a few Linux flavours we got working manually”. I don’t want to have to play a guessing game of which distributions package which dependencies and what they call them. (Cargo and docker do a good job of this)- Support for binaries and libraries (apt, cargo, npm, nix. Basically everything but docker.)- I can just take a random project on github and run it reliably without spending hours pissfarting around first. (Cargo, npm. Apt gets negative points because software packaged with apt breaks so often on newer Ubuntu &#x2F; Debian distributions.)I can’t speak for arch but apt doesn’t cut the mustard any more. I wish it did. But it doesn’t. And no amount of “just work around it by manually submitting more packages to this one distribution specific package manager” will satisfy me. I change computer and Linux distribution all the time. I want something that works reliably everywhere. The other day I ran some rust code I wrote on FreeBSD. There are about 100 transitive dependencies, and I bet almost none of them test on FreeBSD. But it doesn’t matter. Once I had rust installed, I checked out the project and ran cargo test. Everything built and worked perfectly. That is how good life is with cargo. Or docker or npm. Or even Python once you get your head around conda or venv.It’s 2023 and that’s table stakes for a package manager. Distribution specific package managers need to keep up because they don’t pass muster any more. replyskitter 20 hours agorootparentprevMy experience of using the AUR is that a decent part of the packages I try to install fail to build, which is certainly pretty secure. reply palata 20 hours agorootparentYeah I guess YMMV, I personally never had issues. Maybe once in a while a package has an issue and there are already comments on the AUR website so I don&#x27;t even need to debug it myself. Maybe I had to debug once in ten years or something. reply c-hendricks 20 hours agorootparentprev> There is the Arch AUR where the community can contribute packages. I don&#x27;t remember having had to compile a dependency from source, because everything I need is either in the official repo, or on the AURWhen you install from AUR you are compiling from source though. Unless you&#x27;re using chaotic aur, which only packages a subset of what&#x27;s available. reply palata 16 hours agorootparentYes, my mistake, and good catch! I meant \"having to write the recipe myself\". reply vilunov 21 hours agorootparentprevI am a developer, I know what dependencies I am pulling in my projects, what they do and I even read the code. What good will offloading this to my users do? They have to decide whether they trust my apps with all its dependencies, they can do that by reviewing the code of all deps transitively, but there is not much difference between a dependency from the distro repository and the dependency from crates.io.> Ideally the distro ships with those dependenciesThis is all good and well in the world where there is one single Linux distro, but usually you want to target all mainstream distros and macOS and Windows and what now. Depending on system packages becomes a brittle solution in these cases, since who knows which version of the necessary lib is packaged on this ancient Debian installation. If you depend on a newer version, then you are just forcing you users-developers to either spend time packaging it properly or just running `make install` and littering the system with untracked files. Honestly, stuff like `cargo` fixes this elegantly so I never have to think about it again. reply palata 20 hours agorootparent> I am a developer, I know what dependencies I am pulling in my projects, what they do and I even read the code.I am convinced that you are more the exception than the rule. For node projects that pull hundreds packages transitively, I can&#x27;t believe for one second that the devs even read the list (and even less that they would start considering reviewing the code).> but there is not much difference between a dependency from the distro repository and the dependency from crates.ioWho reviews what goes into crates.io? I know for a fact that other package repositories don&#x27;t have any check at all: anyone can push anything. Whereas the distro repository is reviewed by the distro maintainers. Big distros have a security team even.I think that is a noteworthy difference.> but usually you want to target all mainstream distros and macOS and Windows and what now.Of course, usually you want everything, for fre",
    "originSummary": [
      "The author raises concerns about relying solely on crates.io as a repository for Rust code and proposes an alternative distribution method similar to Linux distributions like Debian.",
      "The author commends Debian's Rust packaging team for curating a collection of Rust libraries, which reduces the dependency risks associated with relying solely on crates.io.",
      "This alternative approach increases the author's confidence in using Rust in the future."
    ],
    "commentSummary": [
      "The article explores the challenges of using third-party libraries and managing dependencies in languages like C and C++, including build setup difficulties, maintenance issues, and version compatibility problems.",
      "Rust's packaging system, crates.io, is highlighted as a solution to these challenges, offering better control over dependencies and ensuring compatibility.",
      "The need for improved security measures, such as protection against supply chain attacks, is emphasized, along with the importance of managing dependencies securely and utilizing scanning tools and curated library repositories."
    ],
    "points": 225,
    "commentCount": 217,
    "retryCount": 0,
    "time": 1699958048
  },
  {
    "id": 38261982,
    "title": "Developing a Programming Language: Challenges, Tips, and Insights",
    "originLink": "https://yorickpeterse.com/articles/a-decade-of-developing-a-programming-language/",
    "originBody": "Homepage Articles Atom Feed Resume A decade of developing a programming language Published on: November 14, 2023 In 2013, I had an idea: \"what if I were to build my programming language?\". Back then my idea came down to \"an interpreted language that mixes elements from Ruby and Smalltalk\", and not much more. Between 2013 and 2015 I spent time on and off trying different languages (C, C++, D and various others I can't remember) to see which one I would use to build my language in. While this didn't help me find a language I did want to use, it did help me eliminate others. For example, C proved to be too difficult to work with. D seemed more interesting and I managed to implement something that vaguely resembles a virtual machine, but I ultimately decided against using it. I don't remember exactly why, but I believe it was due to the rift caused by the differences between D version 1 and 2, the general lack of learning resources and packages, and the presence of a garbage collector. Somewhere towards the end of 2014 I discovered Rust. While the state Rust was in at the time is best described as \"rough\", and learning it (especially at the time with the lack of guides) was difficult, I enjoyed using it; much more so than the other languages I had experimented until that point. 2015 saw the release of Rust 1.0, and that same year I committed the first few lines of Rust code for Inko, though it would take another two months or so before the code started to (vaguely) resemble that of a programming language. Fast-forward to 2023, and Inko is in a state where one can write meaningful programs in it (e.g. HVAC automation software, a Markdown parser, a changelog generator and more). Inko has also changed considerably over the years: whereas it was once a gradually typed interpreted language, it's now statically typed and compiles to machine code using LLVM. And whereas Inko used to draw inspiration heavily from Ruby and Smalltalk, these days it's closer to Rust, Erlang and Pony than it is to Ruby or Smalltalk. Given it's been 10 years since I first started working towards Inko, I'd like to highlight (in no particular order) a few of the things I've learned about building a programming language since first starting work on Inko. This is by no means an exhaustive list, rather it's what I can remember at the time of writing. ℹ You can find discussions about this article on Reddit here and here, on Hacker News, and on Lobsters. Table of contents Avoid gradual typing Avoid self-hosting your compiler Avoid writing your own code generator, linker, etc Avoid bike shedding about syntax Cross-platform support is a challenge Compiler books aren't worth the money Growing a language is hard The best test suite is a real application Don't prioritize performance over functionality Building a language takes time Avoid gradual typing A big change I made was to switch Inko from being a gradually typed language to a statically typed language. The idea behind gradual typing was that it would allow you to build a prototype or simple scripts in a short amount of time using dynamic typing, then over time turn the program into a statically typed program (where beneficial). In reality, gradual typing ends up giving you the worst of both dynamic and static typing: you get the uncertainty and lack of safety (in dynamically typed contexts) of dynamic typing, and the cost of trying to fit your ideas into a statically typed type system. I also found that the use of gradual typing didn't actually make me more productive compared to using static typing. The result was that I found myself avoiding dynamic typing in both Inko's standard library and the programs I wrote. In fact, the few places where dynamic typing was used in the standard library was due to the type system not being powerful enough to provide a better alternative. Gradual typing also has performance implications. Consider this example using keyword arguments: let x: Any = some_value x.foo(b: 42, a: 10) Here x is typed as Any, which used to mean the value is dynamically typed. Because we don't know the type of x in x.foo(...), we can't resolve the keyword arguments to positional arguments at compile-time. This meant Inko's virtual machine had to provide a runtime fallback, and the keyword arguments had to be encoded into the bytecode. While the cost wasn't significant, in a statically typed language the cost is zero because we can resolve the arguments at compile-time. Another issue is that the presence of dynamic types can inhibit compile-time optimizations, such as compile-time inlining (and all the optimizations that depend on it). If a language uses a Just In Time (JIT) compiler, such as JavaScript (and by extension TypeScript), you can optimize the code at runtime, but that means having to write a JIT compiler which itself is a massive undertaking. The presence of dynamic types also means that even statically typed code may be incorrect, though this depends on how you approach casting dynamically typed values to statically typed values. If such a cast doesn't require a runtime check, you may end up passing incorrectly typed data to statically typed code. If you do perform some sort of runtime check, this may affect performance when such casts are common. Recommendation: either make your language statically typed or dynamically typed (preferably statically typed, but that's a different topic), as gradual typing just doesn't make sense for new languages. ℹ The emphasis here is on new languages, as applying gradual typing to an existing language can be useful, especially as an intermediate step towards the language becoming fully statically typed. Avoid self-hosting your compiler Early in the development of Inko, I decided that I wanted to write the compiler in Inko itself, commonly referred to as a \"self-hosted compiler\". The idea was that by doing so, the compiler could be exposed through the standard library, and to have a sufficiently complicated program to test everything Inko has to offer. While this seems great on paper, in practise it turns into a real challenge. Maintaining a single compiler is already a challenge, but maintaining two compilers (one to bootstrap your self-hosted compiler, and the self-hosted compiler itself) is even more difficult. The process of building the compiler is also more complicated: first you have to build the bootstrapping compiler, then you can use that to build the self-hosted compiler. Ideally you then use that self-hosted compiler to compile itself a second time, so you can ensure the behaviour doesn't subtly change depending on what compiler (the bootstrapping or self-hosted compiler) is used to compile your self-hosted compiler. Because of these challenges, I abandoned this idea in favour of writing the compiler in Rust, and keeping it that way for the foreseeable future. Recommendation: defer writing a self-hosted compiler until you have a solid language and ecosystem. A solid language and ecosystem is infinitely more useful to your users than a self-hosted compiler. Avoid writing your own code generator, linker, etc When writing a language, it's tempting to take on more than you can or probably should handle. In particular, it may be tempting to write your own native code generator, linker, C standard library, and so on (i.e what languages such as Zig and Roc are doing). My general recommendation is to avoid this unless you have established a clear need for this. And when you do think there's a need, I'd still avoid it. Writing a language is hard enough as-is and can easily take years. For every such component (a linker, a code generator, etc) you add on top, it will take several more years before the stack as a whole becomes useful. That's ignoring the painful fact that such bespoke components are highly unlikely to outperform the established alternatives. Recommendation: there are many developers who think they can write a better linker, code generator, and so on, but few developers who actually succeed in doing so. As harsh as it may sound, you are probably not one of them. Of course once you have an established language, you're free to reinvent as many of these wheels as you see fit. ℹ If you're writing an interpreted language, it's fine and probably even needed to write your own (byte)code generator (unless you target an existing virtual machine such as the JVM), as bytecode generators are typically not that complicated to implement. Avoid bike shedding about syntax The syntax of a language and how its parsed is one of the most boring aspects of building a language. Writing parsers in general is pretty dull, and there's not a lot you can innovate upon. And yet, it's a subject many developers building their own language seem to spend way too much time on. There are also plenty of articles titled something along the lines of \"How to build your own programming language\", only covering the basics of writing a parser and nothing more. For Inko I took a different approach in its early days: I used an S-expression syntax, instead of designing my own syntax and writing a parser for it. This meant I was able to experiment with the semantics and virtual machine of the language, instead of worrying over what keyword to use for function definitions. Recommendation: use an existing syntax and parser when prototyping your language, allowing you to focus on the semantics instead of the syntax. Once you develop a better understanding of your language you can switch to your own syntax. Cross-platform support is a challenge This shouldn't be entirely surprising, but supporting different platforms (Linux, macOS, Windows, etc) is hard. For example, Inko used to support Windows when it used an interpreter. When switching to a compiled language, I had to drop support for Windows as I couldn't get certain things to work (e.g. the assembly used for switching thread stacks). Running tests on different platforms is also not nearly as easy as it should be. Take GitHub Actions: you can use it to run tests on Linux, macOS, and Windows. Unfortunately, the free tier (at the time of writing) only supports AMD64 runners, and while it does support macOS ARM64 runners, these cost $0.16 per minute. The cost isn't even the biggest problem here, because depending on how often tests run it may not be that big. Rather, the problem is that paid runners typically aren't available for forks, meaning pull requests from third-party contributors won't be able to run the tests using these runners. And this is ignoring the problem of supporting platforms not supported by your continuous integration platform (e.g. GitHub Actions) of choice. FreeBSD is a good example of this: GitHub Actions just doesn't support it, so you need to use qemu or similar software to run FreeBSD in a VM. Even if you just support Linux, you still have to deal with the differences between Linux distributions. For example, Inko uses a Rust wrapper for LLVM (Inkwell), but the low-level LLVM wrapper (llvm-sys) it uses doesn't compile on Alpine Linux, and so Inko doesn't support Alpine Linux for the time being. The extend to which this is a problem depends on the language you're trying to build. For example, if you're building an interpreter written in Rust it probably won't be that bad (though Windows is always going to be a challenge), but it is something you need to be prepared for. Recommendation: if you're uncertain about supporting a certain platform, err on the side of not supporting it and document this, instead of sort-of-but-not-quite supporting it. Compiler books aren't worth the money While there are plenty of books on compiler development, they tend to not be that useful. In particular, such books tend to dedicate a significant amount of time to parsing, arguably the most boring part of a compiler, then only briefly cover the more interesting topics such as optimizations. Oh, and good luck finding a book that explains how to write a type-checker, let alone one that covers more practical topics such as supporting sub-typing, generics, and so on. Recommendation: start with reading Crafting Interpreters, and read through /r/ProgrammingLanguages on Reddit. If you're interested in learning more about pattern matching, this Git repository may prove useful. Growing a language is hard Building a language is a significant challenge on its own. Growing the number of users using your language and the libraries written in your language? That's even more difficult. In particular, it seems languages either explode in terms of popularity/interest, even if that may not be warranted (looking at you, V), or it takes years for them to get even a handful of users. Making a living off a programming language is exceptionally difficult, as the number of people willing to donate money is even smaller than those willing to try out your new language. This means either dedicating a lot of spare time towards building your language, or quitting your job and funding the development yourself (e.g. using your savings). This is what I did by the end of 2021 and while I don't regret doing so, it's a bit painful to watch your wallet shrink over time. As far as advice goes, I'm not sure how to approach this as I'm still figuring that out myself. What I do know is that a lot of existing advice isn't helpful at all, as it amounts to \"Just get more users, LOL\". Perhaps in another 10 years from now I'll know the answer. The best test suite is a real application This one is a bit obvious, but worth highlighting regardless: writing unit tests for your language (e.g. for the standard library functions) is important and useful, but nowhere near as useful as writing a real application in the language. For example, I wrote a program to control my house's HVAC system in Inko, revealing various bugs and areas of improvement in the process. Such applications also act as a showcase for your language, making it easier for potential users to develop an understanding of what an average project in your language might look like. Recommendation: write a few sufficiently complicated programs that are actually useful in your language, then use these as a way of testing functionality and stability of your language. If you can't think of any programs to write, consider porting this changelog generator written in Inko, as it's complex enough to act as a good stress test for your language, but not so complex it will take weeks to port. Don't prioritize performance over functionality When building a language, it can be tempting to focus heavily on providing a fast implementation, such as a fast and memory efficient compiler, and one can easily spend months working on this. Potential users of your language may care about performance to some degree, but what they care about more is being able to use your language, write libraries in it, and not having to reimplement every basic feature themselves because of a lacking standard library. To put it differently: the value of good performance is proportional to the amount of meaningful code (= real applications) written in a language. Recommendation: as the saying goes: first make it work, then make it fast. This doesn't mean you should not care about performance at all, rather 70-80% of your energy should be directed towards functionality, with the remaining 20-30% directed towards making the language not unreasonably slow. Building a language takes time To wrap things up, here's another observation that should be obvious but is worth bringing up regardless: building a simple language for yourself in a short amount of time is doable. Building a language meant to be used by many for many years to come is going to take a long time. To illustrate, here are some examples of a few languages and when they released their first stable release (a ? indicates no stable release is available at the time of writing): Language Started in Release of 1.0.0 Python 1989 1994 Ruby 1993 1996 Scala 2001 2004 Rust 2006 2015 Go 2007 2012 Elixir 2011 2014 Crystal 2011 2021 Vale 2012 ? Inko 2013 ? Gleam 2016 ? V 2019 ? On top of that, there can be significant time between a language becoming stable and it becoming popular. Ruby 1.0 released in 1996, but it wouldn't be until 2005 or so that Ruby became popular with the release of Ruby on Rails. Rust in turn saw a rise in popularity following its first stable release, but it would still take a few years for the language to take off. Scala released version 1.0.0 2004, but didn't see widespread adoption until some time between 2010 and 2015. Based on these patterns, I suspect that most languages will need at least 5-10 years of development before reaching their first stable release, followed by another 5 years or so before it starts to take off. That's all assuming you end up lucky enough for it to actually take off, as there are many languages that instead fade into obscurity. Recommendation: if you want your language to succeed, be prepared for it to take at least 10-15 years. If you expect it to take the world by storm in just a year, you'll be sorely disappointed.",
    "commentLink": "https://news.ycombinator.com/item?id=38261982",
    "commentBody": "A decade of developing a programming languageHacker NewspastloginA decade of developing a programming language (yorickpeterse.com) 222 points by YorickPeterse 15 hours ago| hidepastfavorite110 comments danybittel 4 hours agoYour mileage may vary. I think It&#x27;s a good article. But sometimes a PL has to go down an unconventional path to make something new. If you follow a best practice \"rulebook\", your PL will be like every other PL.I think the most important part is figuring out who is going to use the PL and what problems they are solving. As precisely as possible, best with a list of concrete Personas and Projects. This creates a \"value system\", which makes it easier to answer all questions during implementation. reply grumpyprole 3 hours agoparent> sometimes a PL has to go down an unconventional path to make something new.Absolutely, just look at Haskell as an example.> most important part is figuring out who is going to use the PL and what problems they are solving.Completely agree. Rust and Go have received a lot of criticism over the years, but I think that is often down to them not being as general purpose as many people try to claim. The creators had particular users and use cases in mind. reply rob74 17 minutes agorootparentI think Rust and Go have received that criticism also because they precisely \"dared\" to take an unconventional approach: Go is very opinionated and uses a radically simplified syntax without inheritance, exceptions and other things people are used to from other languages, which leads to many people trying to bend Go to their style of programming and getting frustrated; Rust has its focus on performance (\"zero cost abstractions\") and being memory safe without using GC, which leads to its famously steep learning curve. But I agree with OP: if you are going to start a new programming language today, you&#x27;d better try to bring some new ideas to the table, otherwise you&#x27;re just crowding an already crowded space even more. reply prmph 1 hour agorootparentprevMaybe people just like to use general purpose languages. It should not be too hard to marry the good parts of major language families reply zengid 12 hours agoprev> \"Oh, and good luck finding a book that explains how to write a type-checker, let alone one that covers more practical topics such as supporting sub-typing, generics, and so on\"I bought _Practical Foundations for Programming Languages_ by Harper and _Types and Programming Languages_ by Pierce and I just can&#x27;t get through the first few pages of either of them. I would love to see a book as gentle and fun as _Crafting Interpreters_ but about making a static ML like language without starting with hardcore theory. (Bob, if you&#x27;re listening, please make a sequel!) reply chubot 11 hours agoparentI&#x27;m in the same boat as you -- here are the two best resources I found:https:&#x2F;&#x2F;mukulrathi.com&#x2F;create-your-own-programming-language&#x2F;...https:&#x2F;&#x2F;jaked.org&#x2F;blog&#x2F;2021-09-07-Reconstructing-TypeScript-...I read through the first 10 chapters of TAPL, and skimmed the rest. The first 10 chapters were good to remind myself of the framing. But as far as I can tell, all the stuff I care about is stuffed into one chapter (chapter 11 I think), and the rest isn&#x27;t that relevant (type inference stuff that is not mainstream AFAIK)This is also good:https:&#x2F;&#x2F;github.com&#x2F;golang&#x2F;example&#x2F;blob&#x2F;master&#x2F;gotypes&#x2F;README...And yeah some of us had the same conversation on Reddit -- somebody needs to make a Crafting Interpreters for type checking :) Preferably with OOP and functional and nominal&#x2F;structural systems.---Also, it dawned on me that what makes TAPL incredibly difficult to read is that it lacks example PROGRAMS.It has the type checkers for languages, but no programs that pass and fail the type checker. You are left to kind of imagine what the language looks like from the definition of its type checker !! Look at chapter 10 for example.I mean I get that this is a math book, but there does seem to be a big hole in PL textbooks &#x2F; literature.Also I was kinda shocked that the Dragon Book doesn&#x27;t contain a type checker. For some reason I thought it would -- doesn&#x27;t everyone say it&#x27;s the authoritative compiler textbook? And IIRC there are like 10 pages on type checking out of ~500 or more. reply ww520 7 hours agorootparentAre we looking at the same Dragon book? Chapter 6 the whole chapter is about type checking. 6.2 spells out a type checker for a sample language, admittedly in pseudo code.It might not have the same lingo as the modern type checking specification but most of the ideas are there. It talks about type systems, type expressions, type rules, constructed types like array&#x2F;struct, type variables for unknown types, etc. It puts the type rules in the grammar productions. It stores the type info in the AST nodes. It uses the chained symbol tables as the type environments.It has sections on type conversion, operator&#x2F;function overloading, polymorphic functions (parameterized types), and type unification.It has all the pieces and steps to build a type checker. reply chubot 6 hours agorootparentI have the second edition, copyright 2007, and chapter 6 is \"intermediate code generation\". The whole chapter is definitely not about type checking -- is yours?https:&#x2F;&#x2F;www.amazon.com&#x2F;Compilers-Principles-Techniques-Tools...It has 11 sub-sections, 2 of which are about type checking6.3 - Types and Declarations, pages 370-3786.5 - Type Checking - pages 386 - 398So that&#x27;s about 20 pages out of 993 pages. A fraction of a chapter!---What I was referring to is the appendix \"A Complete Front End\", which contains a lexer and parser, but not a type checker! Doesn&#x27;t sound complete to me.I guess they have a pass to create the three-address code, and type checking is only in service of that. But they don&#x27;t give you the source code!---Also weirdly, when you look at the intro chapter \"The structure of a compiler\", it goes1.2.2 Syntax Analysis1.2.3 Semantic Analysis (3 paragraphs that mentions type checking and coercions)1.2.4 Intermediate Code GenerationBut when you look at the chapters, there&#x27;s NO semantic analysis chapter! It goes from (4) Syntax analysis, (5) Syntax-directed translation, to (6) intermediate code generation.I think there&#x27;s a missing chapter! (or two) reply ww520 5 hours agorootparentI have the 1st edition printed 1988. Chapter 6 is Type Checking, chapter 7 is Runtime Environments, and chapter 8 is Intermediate Code Generation.WTF. The 2nd edition is really butchered. reply chubot 5 hours agorootparentWow yeah. So in the first edition pages 343 to 388 is a full chapter on type checking. It looks much more coherent.What the heck happened ... reply zengid 11 hours agorootparentprevThank you so much! This looks like some good morning-with-coffee reading! reply orthoxerox 56 minutes agoparentprev> I would love to see a book as gentle and fun as _Crafting Interpreters_ but about making a static ML like language without starting with hardcore theory. (Bob, if you&#x27;re listening, please make a sequel!)I don&#x27;t think an ML-like language is the best option. I would start with type-checking something simple like C or Pascal: no overloading, generics, type inference, anonymous functions. Just plain old subroutines and declared types on everything, arrays and structs as the only composite data structures.Then you could add implicit conversions, type inference for locals (not that hard), subtyping, target type inference for anonymous functions, generics and maybe overloads if you&#x27;re brave enough. reply trealira 2 hours agoparentprev> about making a static ML like language without starting with hardcore theory.The book Applicative Order Programming: The Standard ML Perspective has you implement an ML-like language at the end (the book is mostly about programming in Standard ML itself). Chapter 10 covers type checking and type inference. Chapter 11 covers interpretation via graph reduction. Chapter 12 covers compilation of the ML-like language to an abstract stack machine. The code is all in Standard ML. It&#x27;s very direct and practical without getting bogged down in theory, although it does talk about theory, and it&#x27;s not as gentle and fun as Crafting Interpreters.It was published in 1991. It&#x27;s on libgen; I&#x27;d recommend downloading a PDF and reading those chapters in order to judge for yourself. reply ReleaseCandidat 2 hours agoparentprevThe bigger challenge is doing the pattern matching and mainly the exhaustiveness checking of patterns which aren&#x27;t constructors of a sum type, but for example integer intervals.Simon Peyton Jones together with others wrote a relatively easy to read book about the writing of Miranda (Haskell), which even includes a simple explanation of lambda calculus, so it includes everything you need to know about the theory, more or less: \"The Implementation of Functional Programming Languages\" 1987, https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;wp-content&#x2F;uploads&#x2F;... reply andrewcobby 6 hours agoparentprevI had a similar experience this year as I’m working on an ML-cousin to Golang. I actually found that ChatGPT (gpt4) was really good and breaking down the step of implementing a Hindley-Milner type system in a language I could understand. It could also provide follow up clarifications to my questions too. I then implemented it a small bit at a time and as the complexity grew I started to better understand the algorithm.EDIT: ChatGPT could actually demonstrate the whole process for type checking small chunks of code, including: assigning them type variables, collecting constraints and then unification. It would even then point out if there was a type error in the code snippet! reply chewxy 5 hours agorootparentI&#x27;m curious how you went. Here&#x27;s my attempt: https:&#x2F;&#x2F;github.com&#x2F;chewxy&#x2F;hm , the core of which (i.e. the interface type) is what powers most of the languages I wrote (different languages I wrote have different unification schemes) reply gilmi 2 hours agoparentprevI&#x27;ve mentioned this in a different comment, but I&#x27;ve written several articles on type inference, trying to make the topic more approachable:https:&#x2F;&#x2F;gilmi.me&#x2F;blog&#x2F;tags&#x2F;type%20inferenceIf you find that helpful, I&#x27;ve made more content on compilers (including live coding a compiler, without narration) which should be easily reachable from my website. reply ashton314 3 hours agoparentprevI&#x27;ve written a type checker or two. My experience: https:&#x2F;&#x2F;lambdaland.org&#x2F;posts&#x2F;2022-07-27_how_to_write_a_type_...I left some links&#x2F;references to books that helped me out. reply efnx 8 hours agoparentprevI highly recommend https:&#x2F;&#x2F;github.com&#x2F;sdiehl&#x2F;write-you-a-haskell as it is very developer friendly. It’s not complete, but it really gets the gears turning and will set you up for writing your own Hendley-Milner style type checker. reply PhilipRoman 12 hours agoparentprevI would love to know more about what the author found difficult regarding type checking. As long as you don&#x27;t screw up your semantics to the point of needing a full blown constraint solver there should be no issues.Edit: by \"screwing up semantics\" I mostly mean the combination of overloading and implicit conversions, which is known to cause issues in Java, C++, etc. reply YorickPeterse 11 hours agorootparentThe problem for me was that I had a rough idea of how to implement (essentially it&#x27;s similar to a recursive-descent parser), but I had a difficult time finding appropriate resources to confirm or deny whether my idea was solid, as well as tips and what not.Basically, the existing material is a bunch of existing incredibly complicated implementations, the odd blog post that just throws a bunch of code your way without really explaining the why&#x2F;thought process behind it, and books that aren&#x27;t worth the money.The result is that you can of course piece things together (as I did), but it leaves you forever wondering whether you did it in a sensible way, or if you constructed some weird monstrosity.To put it differently: you can probably build a garden by digging some holes and throwing a few plants around, but without the right resources it can be difficult to determine what the impact of your approach may be, and whether there are better ways of going about it. Oh and I&#x27;m aware there are resources on gardening, it&#x27;s just a metaphor :) reply Q6T46nT668w6i3m 11 hours agorootparentAs someone who has written many compilers and type checkers, I agree. There’s very little information online about the subject. I think part of the issue, for me, was psychological: I felt like I was missing some implementation theory that was presented alongside other compiler subjects (e.g., LALR) when the reality is that you’re mostly implementing very simple Boolean operations. reply ww520 11 hours agorootparentprevWhile helping a bit, it&#x27;s difficult to learn or reverse engineer from existing type checking code because a lot of them are mundane repetitive code implementing some high level theory and algorithms. The actual code is too far remote from the theory. You really want to start from the theory and the overall picture.The Dragon book has a chapter on type checking. It gives explanations on many topics. It has plenty of examples. It has type treatments on different areas of a language, like expression, array, struct, function, pointer, etc.Despite being really old, its ideas and explanation on the topic are still relevant. reply mabster 11 hours agorootparentprevI think the larger writings like books are generally going to be written by academics so they&#x27;ll be rigorous and hard to read.So that leaves blog posts for most developers actually implementing this stuff.But the solution here is that when you finally figure out a good strategy to deal with something muddy like this is to write that better blog post :) reply YorickPeterse 11 hours agorootparentI&#x27;m planning on doing something similar to what I did for pattern matching [1]: basically building something entirely standalone that fits in 2k LOC or so, and explains the basics (i.e. nominal typing plus basic sub-typing), hopefully such that people can then take that and extend it.As for _when_ I&#x27;ll do that, that depends on when I can convince my inner critic to actually commit to the idea :)[1]: https:&#x2F;&#x2F;github.com&#x2F;yorickpeterse&#x2F;pattern-matching-in-rust reply brundolf 10 hours agorootparentprevI&#x27;ve been implementing a language with a TypeScript-like type system, and while the core system is pretty intuitive (values combining with other values, determining whether basic value types fit into others), some of the fringes have been challenging:- Generics, especially inferring generic type params- Recursive types- Deeply mutable vs immutable types- Type refinement based on checks&#x2F;usage- Giving good error messages for deep type mismatches (though I&#x27;m not sure TypeScript itself has figured this one out yet, lol)etc. And even the stuff I&#x27;ve figured out, I figured out almost all from scratch (sometimes having to bail and take a totally new approach midway through). I would love to read a book giving me a proper bottom-up walkthrough of the current landscape of type system implementation reply nicoburns 9 hours agorootparentprevWould type-inference also \"screw up the semantics\"? My understanding is that this generally operates via some kind of constraints solving. reply orthoxerox 1 hour agorootparentDepends on how much type inference you want.If you want ML-style total type inference when you expect the compiler to deduce the parameter and the return types of your functions, then yes.Local variable type inference isn&#x27;t hard at all, unless you want to do stuff like Rust: declare a variable without an initial value and infer its type from the first assignment downcode. reply paulddraper 11 hours agorootparentprevTypeScript type-checking is quite complicated. I&#x27;m sure no human being on earth could pass a test of \"does this compile\"But that&#x27;s an exception, and deliberately decided to be complex. reply alex_lav 11 hours agorootparentprevI know nothing about how one would make a type checker, but it sort of feels like your comment is \"As long as you don&#x27;t encounter any issues, there won&#x27;t be any issues\", no? reply azdavis 10 hours agoparentprevMight this help? I wrote it: https:&#x2F;&#x2F;azdavis.net&#x2F;posts&#x2F;define-pl-01&#x2F; reply sesm 12 hours agoprevLooking at the table in the end of the article: notice how Scala and Elixir both took only 3 years, because they were targeting an existing platform. Clojure took only 2 years and was developed by a single person. reply fsckboy 11 hours agoparent> only 3 years, because they were targeting an existing platform. Clojure took only 2 years and was d̶e̶v̶e̶l̶o̶p̶e̶d̶ ̶b̶y̶ targeting a single person. reply cactusfrog 10 hours agorootparentWhat do you think about clojure? reply fsckboy 10 hours agorootparentI like it, and certainly much more than java which I just can&#x27;t. I have trouble committing to clojure fully for larger projects because it&#x27;s still a bit niche in terms of the size of the community but nice that it interfaces with java, and for smaller one-off things I just use scheme.oh, clojure also is a little bit weird being a sort of grab-bag collection of data structures that it inherits from java and then turns lisp-ish. doesn&#x27;t make it bad, each thing they add is nice, just feels a little motleyalso, i refuse to call it \"closure\", i pronounce the j reply orthoxerox 1 hour agorootparent> also, i refuse to call it \"closure\", i pronounce the jWait, it&#x27;s not \"clodger\"? reply skrebbel 3 hours agorootparentprevalso, i refuse to call it \"closure\", i pronounce the jWow so edgy reply chii 5 hours agorootparentprev> a bit niche in terms of the size of the communityi say it&#x27;s the largest lisp like community out there tbh. reply WJW 56 minutes agorootparentI like lisp but that is saying your dog is very big for a chihuahua. replybruce343434 58 minutes agoparentprevClojure is a lisp, it basically got to skip the parsing and lexing stages reply packetlost 11 hours agoprevAs someone who has worked with massive typed Python codebases, I 100% agree with the author on Gradual Typing. It&#x27;s literally the worst of both worlds. It&#x27;s actually even worse than that because it gives the illusion of some safety when there isn&#x27;t any, especially in Python where most of the tooling \"fails open\" by default and won&#x27;t tell you something is wrong despite having annotations. reply LegibleCrimson 11 hours agoparentI disagree, as somebody who has also worked with tons of type-hinted Python code. Gradual typing is obviously worse than real static typing, but it&#x27;s a step up from full dynamic typing. It has caught many bugs for me before hitting them in runtime, and provides good documentation about what I can expect a function to accept and return without forcing me to read prose.Type hints don&#x27;t provide any safety, though. That was never the goal, given that they&#x27;re strictly optional and don&#x27;t really exist at runtime anyway (though I have written some experimental monstrosities that used annotations for code generation at runtime). They&#x27;re documentation in a standard form that static analysis tools can leverage.I really can&#x27;t imagine a situation where having type hints in Python is worse than simply not having them. They&#x27;re not the worst of both worlds, they&#x27;re a compromise with some of the benefits and drawbacks of each. reply Philpax 9 hours agorootparent> I really can&#x27;t imagine a situation where having type hints in Python is worse than simply not having them.Can’t they lie &#x2F; go out of sync with what’s actually happening? IMO, an unenforced type hint is very dangerous because of this - it can give you a false sense of confidence in a scenario where you would otherwise write more defensive code. reply LegibleCrimson 6 hours agorootparentI suppose, if you&#x27;re relying on type stubs. Ordinary in-line type hints won&#x27;t lie or go out of sync without complaining somewhere though.Defensive code is good, but I&#x27;m absolutely sick of writing code that has to be defensive about the type of absolutely every variable everywhere. It&#x27;s ridiculous, verbose, and nearly impossible to do universally. THAT is the worst of both worlds. Having to manually fret about the types that every variable may hold. At the point that you&#x27;re having to write defensive code due to dynamic types, you&#x27;ve already lost the advantages of dynamic types entirely.I use type hints to say \"use and expect these types or get Undefined Behavior\", and that contract is good enough for me. reply packetlost 8 hours agorootparentprevYes, this is my whole point. Mypy by default will silently allow blatantly incorrect typing which is worse than no types IMO. reply actuallyalys 9 hours agorootparentprevYes, if you don’t run the type-checker regularly and just run the code itself, there’s nothing preventing them from going out of sync. Type hints are similar to tests in that way. reply nicoburns 9 hours agorootparentprevI suspect Python is particularly bad in this regard. For example, PHP&#x27;s gradual typing is enforced with runtime assertions. reply Q6T46nT668w6i3m 11 hours agoparentprevAny? I regularly find bugs. I don’t find every bug, but finding some bugs is better than none. Especially for the minimal cost of writing annotations. reply lmm 11 hours agorootparent> I don’t find every bug, but finding some bugs is better than none.I used to think this, but based on experience I&#x27;m now less convinced. Finding most bugs, like real static typing does, is great; you can significantly reduce your test coverage and iterate with more confidence. Finding a few bugs is pretty useless if you&#x27;re not finding enough to actually change your workflow. reply LegibleCrimson 11 hours agorootparentFinding a few bugs is very useful if they&#x27;re the kind of bugs that can cause problems in production but usually not in development or testing, like not checking an optional that is very rarely null.It&#x27;s not about workflow or finding enough bugs, but finding bugs that you might not have otherwise seen can be monumentally beneficial. reply lmm 11 hours agorootparent> Finding a few bugs is very useful if they&#x27;re the kind of bugs that can cause problems in production but usually not in development or testing, like not checking an optional that is very rarely null.There&#x27;s a kind of excluded middle here though. Either that kind of bug hits production often enough to matter - in which case a checker that catches it sometimes isn&#x27;t good enough, you need a checker that eliminates it completely. Or it doesn&#x27;t hit production often enough to matter, in which case a checker is of limited use. reply JonChesterfield 8 hours agoparentprevGradual typing is great. The parts you want precision on are statically typed, the parts that don&#x27;t matter dynamic, and they fit together sanely at the language level.Writing type annotations in comments for an optional preprocessor to pass judgement on before the actual implementation ignores them is a bad thing. But that&#x27;s on python, not on gradual typing.(An any type + static is also fine if you have a way to retrieve type information at runtime, and there&#x27;s a sense in which template instantiations of a generic function and a function taking arguments of type any are the same thing as each other) reply jampekka 1 hour agoparentprevIt&#x27;s not the gradual typing. It&#x27;s that static typing gives you the illusion of some safety where there isn&#x27;t any. reply ric2b 11 hours agoparentprevHow is it worse than no typing? If you&#x27;re not testing adequately because you think type safety is enough, you done f&#x27;d up. reply Jtsummers 10 hours agoparentprevPython&#x27;s type hints are just that, hints. They aren&#x27;t static type annotations. Cython uses gradual typing, but the mainstream Python implementation does not actually offer gradual typing. It&#x27;s still dynamically typed, but with an annotation mechanism that lets other tools do some analysis for you. reply jasonwatkinspdx 7 hours agoparentprevI know someone that was on the Dart team and they say the same thing. reply ashton314 3 hours agoprev> In reality, gradual typing ends up giving you the worst of both dynamic and static typing: you get the uncertainty and lack of safety (in dynamically typed contexts) of dynamic typing, and the cost of trying to fit your ideas into a statically typed type system.Depends on how you implement gradual typing. There&#x27;s a spectrum [1] of gradual languages, and the differences between, say TypeScript and Reticulated Python matter a great deal.It is true that you can have some serious performance hits on the migration from dynamically typed -> fully typed code; my advisor Ben Greenman is doing work examining that space of performance en route to fully-typed systems.In practice, there are several gradually typed languages that give you very good performance when all the code is typed—i.e. you don&#x27;t have to pay a cost just for having dynamic in your language. You might have to pay a cost when you use it, and the cost can vary dramatically.Note that&#x27;s just performance—sometimes performance matters less than being able to prototype something fast in a language, which is a feature I find valuable.> In fact, the few places where dynamic typing was used in the standard library was due to the type system not being powerful enough to provide a better alternative.Hey, that sounds like either a win for gradual typing, or a sign that your type system needs some serious work! ;-)[1]: https:&#x2F;&#x2F;prl.khoury.northeastern.edu&#x2F;blog&#x2F;2018&#x2F;10&#x2F;06&#x2F;a-spectr... reply orthoxerox 12 hours agoprev\"Avoid writing your own code generator, linker, etc\"There&#x27;s a certain dearth of pluggable code generators and linkers. Well, not on GNU&#x2F;Linux, where you get both as(1) and ld(1) practically out of the box, but making your compiler emit a PE&#x2F;COFF on Windows is a pain. You either bring your own homemade codegen and linker or use LLVM, using Microsoft&#x27;s ml64.exe and link.exe is incredibly impractical. reply gilmi 2 hours agoprevI really enjoyed the article.I think one way to simplify language creation is to use an existing language with somewhat similar operational semantics as a compilation target. This simplifies the backend a lot and leaves more time to explore what the language (frontend) should look like. The backend can always be rewritten at a later time. My personal choice is usually JavaScript[1].Regarding type checkers&#x2F;type inference, I&#x27;ve also ran into difficulties with this topic, and I&#x27;ve written several articles trying to make it more approachable[2].[1]: https:&#x2F;&#x2F;gilmi.me&#x2F;blog&#x2F;post&#x2F;2023&#x2F;07&#x2F;08&#x2F;js-as-a-target[2]: https:&#x2F;&#x2F;gilmi.me&#x2F;blog&#x2F;tags&#x2F;type%20inference reply matheusmoreira 7 hours agoprev> I used an S-expression syntax, instead of designing my own syntax and writing a parser for it.> This meant I was able to experiment with the semantics and virtual machine of the language, instead of worrying over what keyword to use for function definitions.Yeah. I see a lot of people asking why people are so fascinated by lisp and why there are so many lisps out there. I think this is a huge reason. It certainly was for me.I just wanted to get some ideas working as soon as possible. Lisp is the easiest language to parse that I&#x27;ve ever seen, managed to write a parser by hand. And yet it&#x27;s a fully featured programming language. It just gives you huge power for very low effort. It took a single bit to add metaprogramming to my lisp: if (function.flags.evaluate_arguments) { arguments = evaluate_all(interpreter, environment, arguments); }I see it as a little frontend for my data structures. I get to work on them endlessly and everything I do improves something. reply lolinder 4 hours agoparent> Lisp is the easiest language to parse that I&#x27;ve ever seen, managed to write a parser by hand.I&#x27;ve written a dozen or so parsers for different languages by hand: the parser is easy regardless of the syntax you choose. The complicated bit is always compilation&#x2F;interpretation.The biggest value I see to using S-expressions isn&#x27;t the time saved in writing a parser, it&#x27;s reducing the amount of bikeshedding you&#x27;ll be tempted to do fiddling with syntax. reply kazinator 2 hours agorootparentThat rings untrue in the face of how John MacCarthy specified Lisp interpretation, on paper, in Lisp itself, quite briefly, and didn&#x27;t even suspect it would be executable. Then Steve Russel comes along, and bootstraps it by hand-translating it to code. reply matheusmoreira 3 hours agorootparentprevAren&#x27;t languages like C notoriously difficult to parse? I&#x27;ve read that they&#x27;re actually context sensitive. IIRC much of Go&#x27;s syntax was designed to avoid parsing complexity. reply kazinator 2 hours agorootparentI suspect that the interpretation of C cannot be specified in a brief page of C code. For one thing, the language doesn&#x27;t supply the data structures for representing any aspec of itself; the code will have to invent those: declarations, definitions, statements, types. Then handle a ton of cases.It will not be obvious that what the C code is doing is C interpretation, because all those data structures don&#x27;t resemble the C syntax.The Lisp meta-circular interpreter side-steps that; the issue is settled elsewhere. It exists against a backdrop where the easy correspondence between the printed syntax and the data structure being handled by the interpreter is taken for granted.The C would need the same kind of backdrop; and that would be a lot of documentation. reply shaftoe444 2 hours agorootparentprevSee the clockwise&#x2F;spiral rule! reply kaba0 5 hours agoparentprevI found that a naive frontend for a language is not that big of a deal with something like ANTLR. The benefit is that you have a formal grammar syntax description, and you can iterate on that endlessly without too many code changes down the line. reply ReleaseCandidat 2 hours agorootparentIt isn&#x27;t with a handwritten lexer&#x2F;parser either, the real work starts after the parsing is complete. Of course, there are two possibilities: often changing them can either result in a cleaner, more readable result or a _real_ mess ;) But you have to rewrite it by hand anyways if you want to get better (or usable) error messages and incremental parsing for a LSP. reply danybittel 4 hours agoparentprevI see lisp as sort of \"no syntax\". It&#x27;s basically the AST. Which is a good thing, you can concentrate on the compiler and add syntax later. If you are working on a compiler for a non lisp PL, it makes sense to log the AST after parsing, as a lisp, as a readable AST. reply karmakaze 10 hours agoprevRelated post \"Inko Programming Language\" which might get some interesting comments.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38270265https:&#x2F;&#x2F;inko-lang.org&#x2F; reply otoburb 14 hours agoprev>>\"In reality, gradual typing ends up giving you the worst of both dynamic and static typing: you get the uncertainty and lack of safety (in dynamically typed contexts) of dynamic typing, and the cost of trying to fit your ideas into a statically typed type system. I also found that the use of gradual typing didn&#x27;t actually make me more productive compared to using static typing.\"The Elixir team didn’t get that memo because they are actively in the process of researching and working on a gradual type implementation.[1][1] https:&#x2F;&#x2F;elixir-lang.org&#x2F;blog&#x2F;2023&#x2F;09&#x2F;20&#x2F;strong-arrows-gradua... reply wk_end 13 hours agoparentPursuing gradual typing makes sense when you&#x27;ve got an established dynamic language and want to incorporate typing into it (forget Elixir, let&#x27;s just look at the massive success of TypeScript). But as OP&#x27;s recommendation says - \"for new languages\", gradual typing has lots of costs and fewer benefits.To put it in other terms: my informal impression is that the dynamic \"features\" of TypeScript are used grudgingly; the community strongly pushes towards strictness, eliminating anys, preferring well-typed libraries, and so on. There&#x27;s little appetite to - in a single project, unless absolutely required - mix-and-match dynamism with staticness, which is the thing that gradual typing gets you. Rather, it feels like we&#x27;re migrating from dynamic to static and gradual typing is just how we&#x27;re doing the migration. But in the case of a new language, why not just start at the destination? reply galdor 11 hours agorootparentI might be missing something, but the appeal of gradual typing to me is that I can mostly type functions, providing safe input&#x2F;output boundaries, and avoid having to type every single variable (unless I have to do so for performance reasons, as I do in Common Lisp).This approach is comfortable to me both in Erlang and in Common Lisp, I see it as a balance between safety&#x2F;performances and development speed (and I&#x27;m saying that as someone using Go for all professional development and being really happy with its full static typing). reply LegibleCrimson 11 hours agorootparent> avoid having to type every single variableMost static languages don&#x27;t make you type every single variable anymore. Java, C++, Rust, C#, and many others let you make the compiler infer types where reasonably possible. That&#x27;s still full static typing.My Python and Rust have about the same kinds of explicit type annotations in roughly the same places. My C++ has a little bit more, just because `Foo obj{a}` is more idiomatic than `auto foo = Foo{a}`. reply andyferris 7 hours agorootparentprevYou can do that with type inference and a strong, static type system. reply mikepurvis 12 hours agorootparentprevI wish Python was further down this path— I experimented with adding annotations and the mypy checker to a bunch of code at my company, and it seemed hopeless. Most libraries didn&#x27;t have annotations at all, or there were annotations being maintained by a third party in a separate pypi package, but the annotations were out of sync with the main project and thus blew up in weird ways.I feel for the people trying to develop these gradual systems but it&#x27;s truly a herculean task, especially in the Python community that is now understandably so extremely shy about major breaking changes. reply munificent 13 hours agoparentprevI think the author&#x27;s recommendation has some needed context:> Recommendation: either make your language statically typed or dynamically typed (preferably statically typed, but that&#x27;s a different topic), as gradual typing just doesn&#x27;t make sense for new languages.The \"for new languages\" part is really important. Gradual typing makes a lot of sense when you are trying to retrofit some amount of static checking onto an existing enormous corpus of dynamically typed code. That&#x27;s the case for TypeScript with JavaScript and Elixir with Elixir and Erlang. reply 7thaccount 12 hours agorootparentRaku (formerly known as Perl 6 and a radically different language than Perl 5) uses gradual typing on purpose. It&#x27;s a cool language that has a lot of advanced ideas, but has basically been in either the design or beta stage for like two decades. reply fuzztester 11 hours agorootparent>for like two decades.IOW, Raku is a gradual type of language. reply akoboldfrying 9 hours agorootparentOther puns are not as good as this pun reply YorickPeterse 11 hours agorootparentprevYou&#x27;re right about this applying to new languages. I&#x27;ve added a note to the article to (hopefully) make this more clear. reply chubot 12 hours agorootparentprevOh yeah, as far as I know Elixir has a lot of calling back and forth with Erlang code. So that makes for a super tricky type system problem.I think the macros make it even harder. Elixir appears to be done almost all with macros -- there are lots of little \"compilers\" to Erlang&#x2F;BEAM. reply nerdponx 13 hours agorootparentprevWhat about Julia? reply adgjlsfhk1 8 hours agorootparentJulia isn&#x27;t gradually typed. It&#x27;s strongly but dynamically typed. reply coldtea 12 hours agoparentprevWell, there&#x27;s no memo (real or as a manner of speaking). It&#x27;s just this person&#x27;s preference.I think gradual typing is a nice concept, and his arguments against it amount to \"gradual typing is not stating typing\", which is like the whole point. E.g. he goes on how the compiler can&#x27;t do some optimizations on functions using gradual typing, but, well, it isn&#x27;t supposed to anyway.The benefit of gradual typing is that you can make your program fully dynamic (e.g. for quick exploration), and if you want more assurances and optimizations make it fully static, or if you just want that for specific parts, do them static. And you have the option to go for any of those 3 things from the start. reply YorickPeterse 11 hours agorootparentThe problem about the whole \"it&#x27;s faster (productivity wise) than static typing\" has, as far as I know, never actually been proven (certainly not through repeated studies).Having worked with both dynamically and statically typed languages extensively, I never felt I was _more_ productive in a dynamically (or gradually) typed language compared to one that was just statically typed. For very basic programs you may spend a bit more time typing in a statically typed language due to having to add type annotations, but typing isn&#x27;t what I spend most of my time on, so it&#x27;s not a big deal.In addition, that work you need to (potentially) pay upfront will help you a lot in the long term, so I suspect that for anything but the most basic programs static typing leads to better productivity over time. reply coldtea 10 hours agorootparent>The problem about the whole \"it&#x27;s faster (productivity wise) than static typing\" has, as far as I know, never actually been proven (certainly not through repeated studies).Neither has the opposite, so there&#x27;s that.There&#x27;s an ACM paper too (\"An Experiment About Static and Dynamic Type Systems\", 2010) which found higher productivity for the same code quality with dynamic typing. Of course a few papers here and there, pro or against, are as good as none. It&#x27;s hardly a well studied area.Besides, one or the other proven better doesn&#x27;t mean much, just like you liking eggs over easy vs scrambled eggs doesn&#x27;t depend on some study. If it works for you, and you&#x27;re more productive with dynamic typing or static typing, use that.Even if a study \"proved\" that one kind is \"more productive\" based on some statistics from measuring some group, or that it has \"less bugs\" most people when given a choice would still use what they prefer and makes them, as individuals, more productive and happy coding.>Having worked with both dynamically and statically typed languages extensively, I never felt I was _more_ productive in a dynamically (or gradually) typed language compared to one that was just statically typed.Depends on the type of program, the type of programming (e.g. imperative&#x2F;declarive&#x2F;functional&#x2F;logical&#x2F;OO or some combination and so on), the program&#x27;s scale, the team size, and other aspects, including individual aptitude and preference, not to mention the language and its semantics beyond dynamic&#x2F;static (and the ecosystem too). I&#x27;d certainly be way more producting using dynamic numpy than some C equivalent, even if as a lib it had feature parity.>In addition, that work you need to (potentially) pay upfront will help you a lot in the long term, so I suspect that for anything but the most basic programs static typing leads to better productivity over time.There are problems where upfront work is not a benefit, e.g. if it means getting behind in building your MVP or getting behind a competitor adding new features faster while you \"perfect it\", and your early stage startup loses steam. Also for things where the overhead of upfront might put you off from even attempting them. It can also be a problem to have big upfront costs for exploratory programming and searching into the problem space for your design&#x2F;solution. reply kybernetikos 11 hours agorootparentprevI think it&#x27;s unfair to criticise the lack of studies for that specific question without acknowledging that there are exceedingly few studies that show a benefit for static typing despite the fact that a huge number of people feel that there must be an effect. reply mjhay 10 hours agorootparentIIRC there was a study that looked at the proportion of Github issues labeled as bugs, by language. Clojure came in with the lowest proportion. I overall prefer static typing, depending on what I&#x27;m doing, but I think avoiding ubiquitous mutable state is a lot more important than typing. reply layer8 12 hours agoparentprevWhat you can do is the opposite, integrate a “dynamic” type into a statically-types language, like C# does: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;dotnet&#x2F;csharp&#x2F;advanced-top...This lets you use dynamic typing when you want, and enables more seamless interoperability with dynamically-typed languages. reply kaba0 4 hours agorootparentIs it not literally the same on a type system level? Of course it could be very different from a user-perspective, as the ratio of `dynamic` will be very low in case of C#. reply ralmidani 10 hours agoparentprevA sibling addresses the nuance expressed in “for new languages”. But even if the original author’s opinion was that gradual typing is always bad, why does that mean we should dismiss the Elixir team’s efforts?I have opinions about Elixir, mostly around aesthetics (for context, I started programming in Python, not Ruby), but that doesn’t mean my opinions are objectively more valid than those of a genius like José Valim.I’ve actually found Elixir to be very well-designed and internally consistent, and after two years using it, it’s obvious to me that José and team are very thoughtful and deliberative. I don’t think they want to introduce gradual typing because it’s trendy. reply macintux 8 hours agoprevRelated discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38270265 reply bsder 11 hours agoprev> Avoid self-hosting your compilerNot sure this is so problematic anymore.The Zig folks targeted WASM so that they can bootstrap without needing a second compiler implementation. Compilers don&#x27;t need a lot of POSIX in order to be functional. reply zengid 11 hours agoparentOh that&#x27;s a great point! Didn&#x27;t someone give a talk about that too? reply bsder 10 hours agorootparentNo video here, but discussion: https:&#x2F;&#x2F;ziglang.org&#x2F;news&#x2F;goodbye-cpp&#x2F;Welp, video: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=MCfD7aIl-_E reply tabtab 11 hours agoprevThe only \"good\" programming language is the language I make! Everybody thinks different and wants to optimize for different things. There is no existing language I&#x27;ve found that I \"love\"; they each have features I like but none have all the features together.I&#x27;ve drafted up a language called \"Moth\" that has a lot of \"meta power\" to program block scope any way you want, as most languages hard-wire scoping rules, which I find limiting. Things like \"classes\", \"functions\", while-loops etc. would be defined by libraries, NOT the language. It&#x27;s like lambda&#x27;s on steroids and without bloated arrow syntax. But it may run slow as molasses, as scoping meta power adds lots of compiler&#x2F;interpreter indirection.However, it may turn out that only a few scoping rules are practical in most cases, and the compiler could then optimize for those. It would then only be slow if you are doing something \"weird\" with scope. Stick to a fixed known set, and things zip along. But finding that set requires R&D and road testing. reply matheusmoreira 7 hours agoparent> The only \"good\" programming language is the language I make! Everybody thinks differentYeah. There&#x27;s always one little thing or another that bothers me in every language I&#x27;ve learned. Guess I&#x27;ve just come full circle now that I&#x27;ve finally made my own. No doubt it will bother someone else too. If anyone ever uses it. reply lylejantzi3rd 13 hours agoprev\"either make your language statically typed or dynamically typed (preferably statically typed, but that&#x27;s a different topic), as gradual typing just doesn&#x27;t make sense for new languages.\"Is that because of type inference? reply marcosdumay 11 hours agoparentYour question doesn&#x27;t make a lot of sense. The entirety of gradual typing works by type inference, as does the static typing on any new language. reply Jtsummers 11 hours agorootparent> The entirety of gradual typing works by type inferenceIs not a correct statement. You can use gradual typing with explicit type annotations or via inference, it makes no difference to the concept of gradual typing. Gradual typing itself is a way of handling the case of a language being both statically and dynamically typed and handling the interaction between the two portions. reply marcosdumay 10 hours agorootparentWell, technically you can have gradual typing without inference. But you&#x27;ll need to annotate every single value to get any matching out of it. If you do any language like that, the types will be only useful for documentation purposes. reply Jtsummers 10 hours agorootparent> But you&#x27;ll need to annotate every single value to get any matching out of it.Ok, if we&#x27;re taking \"1 is an int\" as type inferencing then, yes, every statically typed language, at least every mainstream one, has at least a small amount of type inference since we don&#x27;t, in C for instance, have to annotate values. But C does not infer the types of its variables or functions, nor do you have to annotate them in every location where they are used. But that&#x27;s not inference either, that&#x27;s using the information determined by annotating the variables.----------My main point was that it&#x27;s weird to say the entirety of gradual typing works by type inference. It works by permitting mixing statically typed and dynamically typed code together in one language. Whether the statically typed portion uses type inference or more \"classical\" type annotations is orthogonal to the way it works under the hood. reply nicoburns 9 hours agorootparentprevYou can have an explicit \"any\" or \"dynamic\" type that can be used in place of a static type where you want dynamism. C# actually has this. reply cies 12 hours agoparentprevWhat part of the statement \"is\" because of type inference? reply Joel_Mckay 9 hours agoprevMost fringe languages are vanity projects that address a limited number of use-cases.There are always language specific features that may reduce a given problems implementation complexity, but it often depends how much time people are willing to commit to \"reinventing the wheel\". If you find yourself struggling with support scaffolding issues instead of the core challenge, than chances are you are using the wrong tool.https:&#x2F;&#x2F;m.xkcd.com&#x2F;927&#x2F;I am not suggesting Erlang&#x2F;Elixir, Lisp and Julia are perfect... but at least one is not trying to build a castle out of grains of sand, The only groups I see freeing themselves of C&#x2F;C++ library inertia is the Go and Julia communities to a lesser extent.Have a wonderful day, =) reply matheusmoreira 7 hours agoparentIt&#x27;s a vanity project until it isn&#x27;t. I don&#x27;t know what makes a language succeed but it&#x27;s clearly possible. I&#x27;ve seen quite a few languages rise from small to widely used. Zig is a prominent example but there are others. reply kaba0 3 hours agorootparentI wouldn’t call Zig widely used just yet. It is definitely not in the same camp as hobby languages, but it is still incredibly small.Nonetheless, it is a very welcome addition to the low-level landscape with some cool ideas. reply matheusmoreira 3 hours agorootparentI can only hope my own language will become \"incredibly small\" like that one day. :)Even if a language doesn&#x27;t take off, its ideas can make a difference and influence future designs. The creator of Zig certainly made an impression on me with this talk:https:&#x2F;&#x2F;youtube.com&#x2F;watch?t=120&v=Gv2I7qTux7gEven if Zig hadn&#x27;t been successful, the ideas it represents would&#x27;ve enriched the world. Gives me hope I&#x27;ll be able to make something out of my own ideas too one day. reply Joel_Mckay 5 hours agorootparentprevI often ponder if it is a function of the breadth of language use-cases within a problem domain, and or developer effort minimization.C&#x2F;C++ tend to be CPU bound languages that are tightly coupled to the Von Neumann architectures. Anything that is not directly isomorphic tends to not survive very long unless its for a VM, and supports wrapper libraries.Best of luck, =) reply coldtea 12 hours agoprev [–] >\"In reality, gradual typing ends up giving you the worst of both dynamic and static typing: you get the uncertainty and lack of safety (in dynamically typed contexts) of dynamic typing, and the cost of trying to fit your ideas into a statically typed type system. I also found that the use of gradual typing didn&#x27;t actually make me more productive compared to using static typing.Well, that&#x27;s like, your opinion, man... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discusses their experience and challenges in developing the Inko programming language, including the decision to switch to static typing and difficulties in self-hosting and code generation.",
      "Building a solid language and ecosystem is essential, with challenges in performance, platform support, syntax, and user acquisition.",
      "It is recommended to focus on semantics and use existing syntax when prototyping, with guidance from specific resources and communities.",
      "Growing the user base and libraries requires time and investment, prioritizing functionality over performance initially.",
      "Developing a widely used and successful language takes at least 10-15 years of development."
    ],
    "commentSummary": [
      "Understanding the target users and problems is crucial when developing a programming language.",
      "Implementing type checking can be challenging, and starting with simpler languages is recommended.",
      "There is a debate between gradual typing and static typing, with some advocating for gradual typing's bug-catching and documentation benefits, while others prefer static typing.",
      "The choice between static and dynamic typing should be based on individual preference and productivity.",
      "Integrating dynamic typing into statically typed languages is also discussed.",
      "The discussion emphasizes the various approaches to language design and type checking, the challenges they pose, and the pros and cons of different typing systems."
    ],
    "points": 222,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1699961500
  },
  {
    "id": 38262775,
    "title": "Lapce Editor 0.3.0 Released with New Features and Bug Fixes",
    "originLink": "https://github.com/lapce/lapce/releases/tag/v0.3.0",
    "originBody": "lapce / lapce Public Notifications Fork 852 Star 29.1k Code Issues 683 Pull requests 47 Discussions Actions Projects 1 Wiki Security Insights Releases v0.3.0 v0.3.0 Latest Latest Compare github-actions released this v0.3.0 c49e281 Features/Changes #2190: Rewrite with Floem UI #2425: Reimplement completion lens #2498: Show Lapce as an option when doing \"Open With...\" on Linux #2549: Implement multi-line vim-motion yank and delete (3dd, 2yy, etc.) #2553: Implement search and replace #1809: Implement debug adapter protocol Bug Fixes #2650: Inform language servers that Lapce supports LSP diagnostics Assets 12 28 10 68 6 23 12 114 people reacted",
    "commentLink": "https://news.ycombinator.com/item?id=38262775",
    "commentBody": "Lapce Editor 0.3Hacker NewspastloginLapce Editor 0.3 (github.com/lapce) 208 points by philonoist 21 hours ago| hidepastfavorite92 comments AndreasBackx 19 hours agoI wonder whether it&#x27;s achievable to have a plugin system similar to how web extensions work where you could write a plugin and have it work across different editors. I see how getting people to move to this editor might depend on people moving over once a sufficient amount of their current editor&#x27;s features are supported, some of which might be contributed by extensions. reply digitalsanctum 17 hours agoparentPerhaps using WASM via something like https:&#x2F;&#x2F;extism.org&#x2F;. That would also open it up to building plugins in multiple languages.Tangential to this I&#x27;ve wondered if it&#x27;s possible or advisable to have a utility to port VS Code plugins to a plugin that&#x27;s compatible with the JetBrains IDEs. reply yjftsjthsd-h 16 hours agorootparent> Perhaps using WASM via something like https:&#x2F;&#x2F;extism.org&#x2F;. That would also open it up to building plugins in multiple languages.https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce already says,> Plugins can be written in programming languages that can compile to the WASI format (C, Rust, AssemblyScript)so it sounds like they did that? reply elteto 19 hours agoparentprevSo an interface like LSP but for plugins… very interesting idea!Text manipulation operations are fairly simple to define and can probably be abstracted away into some generic API. Then writing generic text manipulation plugins should be very doable.The complexity comes with project management, multiple files, build actions, etc. But you could define language specific sub-interfaces and the plugins would implement them. So a C++ plugin would understand CMake files for example. reply akoboldfrying 13 hours agorootparent>So a C++ plugin would understand CMake files for example.No one understands CMake files reply PurpleRamen 18 hours agorootparentprevThe complexity comes with interaction in general. Writing plugins on their own is simple. And defining a good API for most common use cases is also not hard today, as we have enough experience now. But the hard part will be defining an independent Interface which will work everywhere on a satisfying level.I mean that&#x27;s basically the biggest selling-point of plugin-systems today. Not that you can write your own functions to do something, but how well you can integrate it into the interface and deliver your own interface to enhance the app. And another part would be safe interaction with other plugins, and necessary package-managment. reply PurpleRamen 17 hours agoparentprevThere seems to be some trend, to outsource plugins into their separate processes, so you can write them in your preferred language. So APIs are already being established here and there. All we probably need is someone carving out a standard from them, with room for editor-specific enhancement, and then convince the projects to support this.Maybe the neovim-people could get together with other projects, start some group, project, whatever to discuss this further? Of maybe something like this already exists? reply mring33621 16 hours agoparentprevMaybe we could invent a standard technology for defining interfaces between software pieces. We could call it Interface Definition Language (IDL for short). And then we could have something that brokers requests to those objects to facilitate intercommunication. We could call that an ORB! Finally a good idea! reply PH95VuimJjqBqy 16 hours agorootparentoh man, we can then build it into the OS and give it a recognizeable name like Component Object Model! reply hoistbypetard 15 hours agorootparentprevCould we define a common specification for how those brokers should interact with other objects, and call it CORBA? reply tmtvl 12 hours agorootparentOMG, why are we talking about ancient uncool technology?Jokes aside, there are so many cool standards for doing stuff it&#x27;s a shame we seem to reinvent the wheel over and over again. reply ReleaseCandidat 15 hours agorootparentprevYes, why not! But I guess that&#x27;s too complex, we&#x27;d need a simple object access protocol! reply sneed_chucker 18 hours agoprevAwesome, this looks like an editor I&#x27;ve wanted to have for a while. I&#x27;ll definitely be trying it out.Basically it seems to take the good parts from a few other editors and combines them:It&#x27;s like VSCode, but without the bloat of electron or Microsoft&#x27;s telemetry.It&#x27;s like sublime text, but it&#x27;s open source and has first class LSP support.It&#x27;s like Neovim, but it doesn&#x27;t require downloading and configuring a bunch of mystery meat plugins just to get what have been table stakes editor features for decades. reply PurpleRamen 18 hours agoparent> It&#x27;s like VSCode, but without the bloat of electron or Microsoft&#x27;s telemetry.But isn&#x27;t the bloat of electron which enables the greatest benefit of VS Code? By which I mean the plugins with their wide range of abilities and UI-integrations.> It&#x27;s like sublime text, but it&#x27;s open source and has first class LSP support.Sublime has LSP since a while now. reply sneed_chucker 18 hours agorootparent> But isn&#x27;t the bloat of electron which enables the greatest benefit of VS Code? By which I mean the plugins with their wide range of abilities and UI-integrations.Maybe for some users. I&#x27;m sparing with extensions because I find them hard to vet; I checked and I&#x27;m only using clangd and prettier right now. But I don&#x27;t think Electron is required in order to have rich extensions - just see Vim&#x2F;Neovim, Emacs, or any IDE.> Sublime has LSP since a while now.My bad then. I thought sublime&#x27;s LSP support came from a third party package. reply CrendKing 13 hours agorootparent> I don&#x27;t think Electron is required in order to have rich extensions - just see Vim&#x2F;Neovim, Emacs, or any IDEAt the cost of difficulty. Recently I wrote a VSCode extension myself with zero prior knowledge (shameless plug: https:&#x2F;&#x2F;github.com&#x2F;CrendKing&#x2F;universal-format-on-type&#x2F;), in probably one day. All I needed to study is their extension API that I cared about. No need for learning a whole new language or DSL. Also easy to debug since there is no compilation and little environment setup.There is a good reason even though everyone acknowledge about Electron&#x27;s performance cost, everyone is still using it. reply ziftface 13 hours agorootparentI haven&#x27;t made a vscode extension, but I don&#x27;t agree that neovim extensions are created with any great difficulty. The lua API is easy to use and well documented. There is a huge ecosystem of neovim extensions precisely because it is so easy to get started. reply PurpleRamen 17 hours agorootparentprev> But I don&#x27;t think Electron is required in order to have rich extensions - just see Vim&#x2F;Neovim, Emacs, or any IDE.I don&#x27;t know about IDEs, but I wouldn&#x27;t put vim & Emacs on the same level as VS Code in terms of interfaces. They go much further than your average text-editor and also grow over time, but at the end they are still text-based interfaces with some fluff. Though, It of course depends strongly on your requirements. For most developers, text with fluff is more than enough. reply doix 16 hours agorootparentI see the limitations of vim an advantage not a disadvantage. Because everything is a buffer and the layout is just windows, the usual keys just work.It&#x27;s one of the things I have about \"real\" IDEs with a vim mode. The one in vscode is pretty good, it&#x27;s pretty rare I break it when just editing code (mostly &#x27;.&#x27; not doing what I expect). But everything else doesn&#x27;t fit the vim model. I have the file manager or something open in the left and I can&#x27;t reach it with ctrl+h, because it&#x27;s own weird UI thing not just a split (like it would be in vim). reply JeremyNT 13 hours agorootparentThis is exactly what has always kept me bouncing off of these things.As a vim user I can reach a sort of uncanny valley where the core editing loop feels \"right,\" but I end up having to fight against all that muscle memory when it comes to using features outside of it. reply jokethrowaway 13 hours agorootparentprevI had the opposite experience.Every plugin in nvim required to learn some new different bs shortcut to have basic features. Plus, I ran into plenty of crashes and weird error in some plugins - and performance wasn&#x27;t always great.The editing is fine, you can get used to buffer editing - it&#x27;s everything else which is not polished and feels like a pile of half implemented features. reply doix 9 hours agorootparentI don&#x27;t really keep up with the state of the art of vim, so I don&#x27;t know what the community has shifted to doing in plugins, but in vim you have to go out of your way to override the base keys in your plugin buffer. I guess that might be more popular now, I don&#x27;t know.But the principle I&#x27;m talking about applies without any plugins at all. If just do :terminal it&#x27;ll make a split, you can now switch between whatever you were doing and that terminal the same way you would switch between a regular split.None of the proper IDEs do that. I can&#x27;t just look at what&#x27;s on my screen and do \"move focus to the thing below where I am\". You need to memorize shortcuts for each bloody tab rather than them being context aware, and I hate it. reply sneed_chucker 16 hours agorootparentprevI mean, aren&#x27;t there whole web browsers and email clients written inside of Emacs? Isn&#x27;t there a whole \"Emacs is an operating system\" joke?But in any case, I try to avoid bespoke custom editor setups unless there&#x27;s a situation that really warrants it. reply PurpleRamen 14 hours agorootparent> I mean, aren&#x27;t there whole web browsers and email clients written inside of Emacs?Depends on your definition of \"whole\". Those modes work by removing features from the webstack, or by borrowing from external GUIs. Emacs browser w3m for example, is AFAIK unable to handle JavaScript. Electron-apps on the other side offer you whole webstack, including all available components, widgets, libs, frameworks, or whatever. So it&#x27;s very easy to take some webapp or component and integrate into your workflows. Apps like Emacs are unable of doing this, and thus constantly on a game of fetching up on popular features and services. reply tmtvl 16 hours agorootparentprevOnly one web browser, I think. The w3m package exists, but that&#x27;s just a communication layer between Emacs and w3m. reply jokethrowaway 13 hours agorootparentprevVS code is reasonably fast, sane defaults and language support, that&#x27;s why it&#x27;s popular.Plugins are overrated and I try not to use them as much as I can. reply marwis 8 hours agoparentprev> It&#x27;s like VSCode, but without the bloat of electron or Microsoft&#x27;s telemetry.Just did a quick comparison on Windows 11 and vscode with hello world uses 400MB but lapce used 900MB :) reply sbt567 6 hours agorootparentI&#x27;ve tried it on Linux, load it with a Rust project, and it only consumes around 100MB. Dunno what happens on Windows though reply dang 13 hours agoprevRelated. Others?Lapce Editor, Release v0.2.0 - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32714191 - Sept 2022 (40 comments)Lapce – Fast open-source code editor - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30708505 - March 2022 (224 comments)Lapce – open-source code editor inspired by Xi-editor - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30526693 - March 2022 (2 comments)Lapce – Fast and Powerful Code Editor written in Rust - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29549173 - Dec 2021 (145 comments) reply juliangmp 17 hours agoprevDefinetly a project I&#x27;m keeping my eyes on.The \"killer feature\" will be the plugin&#x2F;extension support. Vscode got where its now mostly because of how extendable it is imo. reply ReleaseCandidat 17 hours agoparent> Vscode got where its now mostly because of how extendable it is imo.Every IDE or \"code\" editor has had some sort of extensions since decades. _Not_ being extensible is a reason to not being successful.What made VS Code was the LSP and that you can use JS (that \"everybody\" already knows) to write extensions. The need of Lapce to use a language with manual memory management (to compile to (non GC) WASM) is a problem for general adoption. reply chambored 15 hours agoprevI’ve really liked Lapce for the past 3&#x2F;4 months I’ve used it, but there’s been a persistent issue that keeps me going back to Lite XL. Pasting huge blocks of text, think 80,000 characters long, breaks it to the point of not being able to open the same file again in the editor. Lite XL doesn’t care and it’s wicked fast with edge cases like that. If Lapce could manage that better, it would be my go to. reply microflash 16 hours agoprevIt&#x27;s great but unfortunately the lack of line wrapping[1] has actively prevented me using it.[1]: https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce&#x2F;issues&#x2F;945 reply vouaobrasil 14 hours agoparentThat is absolutely right. I downloaded it to try it but luckily I saw this comment and I deleted the install file. I think if someone makes a text editor, they should have the courtesy to say \"note: there is no line wrapping in this editor\". It&#x27;s actually a basic expectation that most people have with regard to editors.Technically, like the person said in the thread \"What prevents you from doing it yourself?\"I feel like not having line wrapping is like not having a save function. Yeah technically it&#x27;s open source but it&#x27;s a little irritating not to have \"NO WRAPPING\" in big letters. reply Aerbil313 13 hours agorootparentThis is version 0.3. Calm down. reply smohare 16 hours agoparentprevThe ticket seems to be describing virtual wrapping. Interesting that this is a strict requirement for you. I’ve personally never used this feature in any editor. I disable it when I can.Personally, I just find such “long” lines too difficult to navigate. reply microflash 15 hours agorootparentFor personal projects, where I&#x27;m in control of column size, this works nicely. Not so much for \"work\" projects.> Personally, I just find such “long” lines too difficult to navigate.This is where line wrapping helps me. It wraps the lines to my preferred column size preventing the horizontal scroll. reply wvh 16 hours agoprevI&#x27;ve been using this the last year for navigating larger code bases where my usual terminal editor falls a bit short because this looks clean and minimal but has all the IDE navigation features like file system trees and LSP functionality. It&#x27;s a cool project, and it might be getting to the point where it can be used as the main code editor for a project. reply TwentyPosts 19 hours agoprevStill not sure what the \"killer feature\" here is. Is there even one? Or is it just \"something like vs code but in Rust and faster and slicker\"?I&#x27;d give it a try, but I am very happy using Helix, which is pretty novel in its own right. reply afefers 18 hours agoparentThe killer feature for me is not being an Electron app. reply snvzz 16 hours agorootparentNeither is kate.Nor vim nor emacs for that matter. reply afefers 16 hours agorootparentYup. I use Emacs since 2002 and am pretty happy with it. Never used an Electron-based app and probably never will. reply agluszak 18 hours agoparentprevNot everything has to have a killer feature. It&#x27;s an oper-source project, not a for-profit startup reply eviks 18 hours agoparentprevthis is the one> You can write a plugin for Lapce with any programing language that compiles to WASIthough faster and slicker than Electron would also be nice. A helix-like as opposed to vim-like mode as well reply zozbot234 18 hours agorootparent> A helix-like as opposed to vim-like mode as wellDiscussed here: https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce&#x2F;issues&#x2F;281 reply whywhywhywhy 17 hours agoparentprev> Or is it just \"something like vs code but in Rust and faster and slicker\"?By that logic wasn&#x27;t the \"killer feature\" of VSCode just \"something like Atom but in Typescript and faster and slicker\"? reply nicoburns 18 hours agoparentprevI&#x27;d say it&#x27;s either \"VS Code, but with better performance &#x2F; resource usage\" or \"Sublime Text, but open source\" reply Alifatisk 20 hours agoprevI could definitely give this a shot, looks clean as hell. Does it support Ruby? reply jitl 20 hours agoparentThere’s a language server for Ruby maintained by Shopify so I expect it should support Ruby okay if you have that installed. reply bobajeff 15 hours agoprevI&#x27;ve been keeping an eye on this one. This holds the most promise of replacing vscode for me. Right now, Kate is more mature and stable at the moment. But I think once it gets further along it&#x27;ll definitely kick the pants off of the other options.I&#x27;ve tried Lite XL and CudeText and they are both hard to set up and fall far short of Kate and Lapce in terms of UI. reply bogwog 14 hours agoparentI&#x27;m curious, why not Sublime Text? It is better, faster, and more mature than Lapce, LiteXL, Kate, CudaText, and VSCode (although VSCode might have more extensions).Is it just because it&#x27;s not free? I bought it, and it has been my most used piece of software by far. reply bobajeff 12 hours agorootparentI&#x27;ve heard about Sublime Text but never considered getting it.The first code editor I ever got any real use out of was vscode. Which until recently was perfect (some feature regressions and issues with crashing.) At this point the last thing I want to do is depend on a proprietary editor. reply zamalek 16 hours agoprev> Rewrote in FloemUIJust had a look at this. It&#x27;s one of the few that doesn&#x27;t use jsx-like macros, an major anti-feature in my opinion. The jsx stuff looks fancy, but it completely breaks Rust Analyzer. You might not care about code completion, but good luck resolving the complex type+lifetime issues (that UIs in Rust often have) without hover type.Kudos for showing scope discipline, that&#x27;s one I will be keeping an eye on. reply emi2k01 15 hours agoparent> The jsx stuff looks fancy, but it completely breaks Rust AnalyzerThat&#x27;s not inherent of macros, but how the parser of the macro body is written. Ideally, the jsx code would be transformed to correct Rust code even on broken input, then Rust Analyzer would map the tokens from the expansion to the tokens from the source code to have context for IDE features (rename, autocompletion, go to definition, etc).I wrote some stuff [1]. It&#x27;s a bit all over the place since I&#x27;m very bad at writing. Here&#x27;s also a thread with Rust Analyzer&#x27;s developers [2][1]: https:&#x2F;&#x2F;blog.emi0x7d1.dev&#x2F;improving-autocompletion-in-your-r... [2]: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;rust&#x2F;comments&#x2F;16x2kzi&#x2F;improving_aut... reply zamalek 14 hours agorootparentThe problem is that you also sometimes need to inspect output tokens that occur between (or that don&#x27;t logically map to tokens between) the input tokens. I ran into this with Leptos. It&#x27;s just a bad idea. Floem shows that you can have something that looks awfully similar to HTML, without the unnecessary complexity (and compile times) of JSX macros. reply zozbot234 18 hours agoprevWould it be feasible to implement a TUI&#x2F;ncurses mode over the same editor backend and feature set? Think something like the FreePascal TUI IDE, or the Oberon TUI that made HN a couple days ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38240333 reply i_am_a_peasant 9 hours agoprevTried it. Not bad actually, can&#x27;t see the type annotations from the Rust lsp though. I see one warning so I imagine annotations are at least partially supported.Also I can&#x27;t jump to my jumped-from point when pressing \"go to definition\" via Ctrl+T when using vim bindings. reply alexbezhan 17 hours agoprevCode folding is still not supported. It&#x27;s a dealbreaker for me https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce&#x2F;issues&#x2F;749 reply deckar01 12 hours agoprevThe preview image shows intellisense covering up 80% of the code with irrelevant suggestions and a compiler error verbosely complaining about code in progress. That much distraction should have a very small visual indicator at the point of focus, just enough to hint the action required to expand if it is actually relevant. reply czechdeveloper 19 hours agoprevLooks better than last time I checked. I like the idea and it&#x27;s getting there. reply norman784 15 hours agoprevIt feels extremely fast if you compare with vscode, also it looks like it is already usable for some projects, need more time to see if there&#x27;s some deal breaker for an early adoption. reply Aerbil313 13 hours agoprevI first thought “Another editor, we don’t need that…” but this one actually gets everything even more right than neovim! Plugins on WASM, written entirely in Rust, renders via wgpu, built in Remote Development. Only thing missing is an official built-in package manager, to avoid the mistake neovim did. A non-barebones VSCode like out-of-the-box configuration would be the cherry on top. reply vsskanth 16 hours agoprevweird font rendering on non-retina windows makes this unusable for me. reply littlestymaar 17 hours agoprev> #2190: Rewrite with Floem UIOh, does it means that it&#x27;s moving back to wgpu after its earlier switch to OpenGL? Does it means that the problems that were encountered earlier with wgpu has been resolved? reply yjftsjthsd-h 16 hours agoparenthttps:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce says,> Lapce (IPA: &#x2F;læps&#x2F;) is written in pure Rust with a UI in Floem. It is designed with Rope Science from the Xi-Editor which makes for lightning-fast computation, and leverages Wgpu for rendering.So I think it&#x27;s reasonable to conclude that they did move back. reply attende_domine 19 hours agoprevPromising project, but keyboard internationalization is a mess. It&#x27;s unusable for those using non-English keyboard layouts.https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce&#x2F;issues&#x2F;2244https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce&#x2F;issues&#x2F;1094https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce&#x2F;issues&#x2F;1689https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce&#x2F;issues&#x2F;2731 reply petepete 19 hours agoparentIt&#x27;s at 0.3, cut them some slack. reply Iridescent_ 19 hours agorootparentThis is kind of a problem in that it reduces the number of potential early users, testers and ultimately contributors. Sure it is still early in its development process, but it is important work to do in order to gain traction. reply golergka 16 hours agorootparentAs a non-native English speaker who speaks 4 languages, I have almost never found myself using any other language than English in a code editor or IDE. And I never seen anyone who did, except for very occasional editing of localisation data files. reply tmtvl 11 hours agorootparentI have seen used (mainly in comments): Japanese, French, German, Dutch, Chinese (don&#x27;t know if it was TC or SC; if those are even different unicode ranges or whether they are differentiated by font), and, if I remember correctly, Afrikaans. It&#x27;s also kinda funny and a little off-putting to see English keywords side-by-side with, like, Dutch variable names. reply artisin 17 hours agorootparentprevIndeed, navigating keyboard internationalization is a rabbit hole within a time-sink. Luckily, in most cases, you can just plop in a library, and it works, but it&#x27;s probably not that simple here. All the wtf edge cases, plus the quirks of different operating systems and hardware manufacturers—it&#x27;s the stuff of nightmares. reply poulpy123 12 hours agorootparentprevI&#x27;m not going to criticize them but being able to type with my keyboard is pretty high in my list of requirements for a text editor :D reply tom_ 18 hours agorootparentprevAn alternative way of looking at this: a text editor&#x27;s main job is to turn keys pressed into characters to be inserted in the document and commands to be executed. reply eyelidlessness 18 hours agorootparentSo what? It isn’t claiming to be ready, for its main job or otherwise. reply ReleaseCandidat 16 hours agorootparentI do not see any indication of its \"not-readiness\" anywhere in the README https:&#x2F;&#x2F;github.com&#x2F;lapce&#x2F;lapce or their website: https:&#x2F;&#x2F;lapce.dev&#x2F;So while it isn&#x27;t claiming to be ready, their content does not tell otherwise. Yes, the version is 0.3, but Neovim is at 0.9. reply yjftsjthsd-h 16 hours agorootparent> Yes, the version is 0.3, but Neovim is at 0.9.I would argue that that&#x27;s a fault on neovim&#x27;s side; I&#x27;m sure there&#x27;s a reason, but I would call it 1.x software. reply ReleaseCandidat 15 hours agorootparentOh, that&#x27;s to be version-comparable to Emacs, which is at version 29. Vim being at 9 is just way too optimistic!duck and run reply nwienert 16 hours agorootparentprevSays pre alpha right at the very top of the website. reply ReleaseCandidat 16 hours agorootparentOh, you&#x27;re right, it&#x27;s there right above the title \"Lightning-fast and Powerful Code Editor\". I swear they added that after my post ;) replyHelloNurse 13 hours agorootparentprevKeyboard handling is so fundamental that a text editor must deal with it, perfectly, from the beginning. The cited issues aren&#x27;t merely a severe and high impact bug cluster, but also an inexcusable design defect that disqualifies the whole project; I suppose someone decided to use Rust despite the lack of sufficiently mature input handling libraries. reply Strom 13 hours agorootparent> Keyboard handling is so fundamental that a text editor must deal with it, perfectly, from the beginning.That would be nice but in reality it&#x27;s such a tricky and subtle problem that nobody does it perfectly. All the operating systems and browsers are buggy in different ways, nevermind anything with smaller budgets. The deeper you get into proper text handling, the more you will realize that nobody has solved it. reply HelloNurse 41 minutes agorootparentPerfection is unlikely, but serious developers should aim to be less buggy than the competition and&#x2F;or limited by operating system and hardware shortcomings. reply Philpax 12 hours agorootparentprevThat&#x27;s a very strong opinion about a project that is at 0.3 and is attending to many simultaneous problems, including building their own UI solution.It&#x27;s certainly an issue, especially for non-English users, but it doesn&#x27;t \"disqualify the whole project\"; it just means that&#x27;s not what they&#x27;re prioritising at this time. reply HelloNurse 44 minutes agorootparentPrioritising glamorous and interesting aspects like plugins and \"their own UI solution\" over boring correctness is not the way to deliver a good text editor. reply bfrog 18 hours agoparentprevI&#x27;m sure contributions are welcome to solve the problem reply oneshtein 18 hours agorootparentPlease, contribute to discussion at least. reply oneshtein 17 hours agoprevI want an \"I&#x27;m adult\" switch, which will stop bombing me with popups until I press Tab (for completion) or F1 (for help).It&#x27;s unusable, because it distracts on almost every keypress with popups, type hints, and additional characters inserted. Sometimes, these popups even block the CODE I working on. It similar to browsing without an ad-blocker.I want a helpful and predictable editor, not a kaleidoscope. I don&#x27;t want to think \"hey, what happened?\" after a key press. reply billforsternz 14 hours agoparentDoes it try to help you type \"if\" once you press &#x27;i&#x27; ? I find it really weird that (some) code editors do that. Also adding )}] after I type ({ or [, and even worse a second \" or &#x27; after I type one \" or &#x27;. I realise there are ways of turning this sort of thing off (usually), but I miss the days when such things never ever happened.Edit: I just looked at the project&#x27;s main website and the auto help provided half way through typing \"self\" which they are obviously very proud of looks like my worst nightmare. A few weekends back I did some \"unplugged\" C programming just with Notepad++ rather than Visual Studio and honestly I felt it was a key step to overcoming the migraine issues I&#x27;ve been having lately. reply tmtvl 11 hours agorootparentI like autopairs (as those are called) but I really hate it when I type the terminator and the editor inserts another terminator. That&#x27;s why I gave up on VS Codium after giving it a second try. reply _a_a_a_ 17 hours agoparentprevI can&#x27;t speak for this particular editor, but Visual Studio once shipped with a mode for C# I think it was, that was like a neon light show. It was unbearable. Also SSMS it has full-on intellisense or nothing at all, and having it on; it just gets in the way. It&#x27;s like MS only know \"my way or the highway\".I realise this is tangential to the subject but I think it bears repeating: respect the user. reply ckdot2 16 hours agoprev [–] Syntax highlighting? Aaaaaand... it&#x27;s gone! reply tomtheelder 14 hours agoparent [–] Does this mean you are rejecting it because it has syntax highlighting? It has theme support so you can absolutely turn that off if you’d like. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The latest release of the lapce Public Notifications Fork on GitHub is version 0.3.0, which introduces several new features and changes.",
      "This release includes a rewrite with Floem UI, a completion lens reimplementation, and the option to use Lapce on Linux.",
      "Additionally, it implements multi-line vim-motion functionality for yank, delete, search, and replace operations, and includes bug fixes.",
      "The release has received positive feedback from 114 individuals."
    ],
    "commentSummary": [
      "The Lapce Editor is an open-source code editor that is being compared to popular editors like VSCode and Sublime Text.",
      "It is praised for its speed and functionality but some users have concerns about missing features.",
      "One potential standout feature is its plugin/extensibility support, but the project is still in early development and has room for improvement."
    ],
    "points": 208,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1699966855
  }
]
