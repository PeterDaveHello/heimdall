[
  {
    "id": 38594697,
    "title": "Omg.lol: A Quirky Oasis for Web Enthusiasts",
    "originLink": "https://blakewatson.com/journal/omg-lol-an-oasis-on-the-internet/",
    "originBody": "omg.lol: an oasis on the internet December 10, 2023 In the fall of 2022, I started using Twitter more. I don’t know why; probably a curious desire to see how bad Elon Musk would screw it up. To make it bearable from a user interface perspective I alternated between the Twitterrific1 and Tweetbot macOS apps. It was fun at first—getting back into Twitter and using a chronological timeline rather than an algorithmically generated one. But the fun died when both apps suddenly lost access to Twitter on January 12, 2023. At first we third-party app fans hoped for an announcement of a temporary glitch along with reinstated access. But it turned out to be what, deep down, we knew it was all along—Elon Musk shut down third-party API access with no warning. It was a sad day, and it’s the day that, for me, Twitter died. Over the course of 2023, Twitter fractured, with many people leaving to join Threads, Bluesky, or Mastodon/fediverse. I ultimately made the jump to Mastodon—but it was an accident. How I accidentally joined Mastodon About a week before Elon killed third-party Twitter clients, I had signed up for this quirky link-in-bio-plus-more service called omg.lol. I discovered it during what I later found out was a big wave of new users coming from Hacker News. I joined omg.lol because of the quirky, fun vibes. I didn’t even pay attention to the fact that there was an optional Mastodon instance. Mastodon, the concept, interested me but like many others I had no idea how to sign up, what instance I should pick, etc. But all of the sudden the decision was made for me—I had easy access to a smallish Mastodon instance with a lot of interesting people who, like me, had a fondness for the weird, fun internet of yore. And since it’s a paid service, I could feel like I wasn’t freeloading on some poor admin. And a paywall offers a little bit of spam protection (I’m assuming $20/yr is enough to prevent some spammy accounts from signing up). I ended up trying it out and then just a week later, Elon broke Twitter. At that point I went all-in on Mastodon and haven’t looked back. I have a third as many followers as I did on Twitter but five times the engagement on my posts.2 I have the choice of dozens of native and web clients. It’s awesome. I settled on Ivory for macOS and it’s been pretty great. The instance over at social.lol is full of interesting, friendly, folks. More on that in a bit. Cool stuff you get with omg.lol The omg.lol dashboard for my bw address The main thing you are getting with omg.lol is one or more subdomains, which are referred to as addresses. For example, my primary one is bw.omg.lol. Each one of your addresses comes with a slew of stuff: Email forwarding: You get an email address, you@omg.lol, which you can forward to any email address. Web Page: This is your link-in-bio one-pager to do whatever you want with. By default this is where your main address (eg, you.omg.lol) points. It’s the flagship feature of omg.lol. It comes with a markdown editor that has some fancy features baked into it. You get a selection of built-in themes but you also have the freedom to go wild with your own CSS. DNS: You have the ability to use your omg.lol subdomain however you wish by way of a friendly DNS panel. Now Page: This is a type of page you can use to let people know what’s going on in your life. It’s broader than a social media post but more immediately relevant than an about page. It comes with the same fancy markdown editor and you can optionally appear in omg.lol’s Now Garden. Statuslog: This is a place to post statuses. It’s really just a fun, silly alternative to other social media platforms but without follows and likes and such. These can cross-post to Mastodon if you want. Weblog: A full-fledge blogging platform. I’m not aware of all its features but it’s pretty powerful. It comes with fancy markdown support and has all the bloggy things you need like tags and RSS. A good example of a very custom blog on omg.lol is Apple Annie’s Weblog. But it’s worth noting you use it right out of the box without design customization if you want. Pastebin: It’s just a pastebin for storing text snippets. Super simple and friendly like all of the omg.lol services. Pics: It’s an image hosting service labeled as being “super-beta” as of the time of this writing. But it does what it says on the tin. You can host images there and they also show up on the some.pics image feed. PURLs: Persistent uniform resource locators. This is a URL redirection service. You get you.omg.lol/whatever and you.url.lol/whatever. You can use these the way you would use similar services and they come with a basic hit counter and way to preview the URL before following it. Switchboard: This is a powerful routing system that lets you point the variants of your address wherever you want, be it a destination on the omg.lol platform or an external website. Most omg.lol services have their own domain so you end up with a variety of options. Just as an example, you get a tilde address (ie, omg.lol/~you). Mine points to my tilde.club webpage. Keys: A place to store public keys—SSH, PGP, etc. Proofs: A service for verifying ownership or control of a particular web property at a particular moment in time. For example, here is proof that I controlled blakewatson.com as of December 10, 2023. API access: Most, if not all, omg.lol services have an API you can use to interact with them. Total nerd freedom. 🤯 Phew, we made it! Thing is, omg.lol is constantly expanding and improving its offerings—this list will probably be outdated weeks or months after I publish it. A cool community I can’t explain it but when you join omg.lol you join a community of the nicest, most interesting people. This service just seems to bring the old-internet lovers together and, I don’t know, it’s just fun and pleasant. If Mastodon is not your thing, that’s cool! There’s also an IRC with a bridge to Discord so you can chat with other members. I’m a lurker most of the time, but I will say that when I do participate in chat I feel immediately welcomed. Harnessing the power of the Adam The mastermind and architect behind omg.lol is Adam Newbold. I can’t say enough nice things about him. Adam is extremely active in this community. I’ve seen him build entire features in an afternoon just because someone in chat said “it would be cool if…” Heck, I was that person in at least one instance. Adam is just so good at listening to feedback and is always gracious about it, even if the answer is no or even if the person asking is being a little belligerent (not that this happens often). Adam’s positive energy is contagious. I think it spreads into the community and is a big reason that everyone seems so cool here. Also can we talk about what a prolific creator he is? He’s always got tons of neat (-nik 😄) stuff in the oven—a multiplayer RPG in the shell, a service dedicated to web 1, and a friendly DNS service, just to name a few. Okay, okay. Nerd crush over. The small web I’ve been enjoying a different kind of internet lately—the kind filled with personal websites, blogs that aren’t cookie-cutter marketing machines, and hypertext oddities you can only find by clicking around. And so many personal projects. This internet is a breath of fresh air if you’ve only been visiting what Google gives you on its first few pages of results ads. I, of course, think everyone should have a personal site. And if you’re worried no one will want read your stuff, just remember that Andy will. Making your own website is rewarding in a way that a corporate social media profile never will be. Need help getting started? These services, tools, and tutorials can help: omg.lol (obviously) Bear Blog – a minimalist blogging service Blot – turn a folder into a website Your first webpage with HTML and Netlify – a tutorial by yours truly HTML Dog beginner HTML tutorial – learn how to make a webpage with HTML Web design in 4 minutes – quick tips for making a website look nice How To Get Started in Web Design – Chris Coyier shows you how to put a website on the internet Twitterrific has especially historic relevance to Twitter as it is responsible for associating a bird as a mascot. Previously, Twitter sported an uninspired Web 2.0 text logo. Twitterrific introduced us to Ollie, the cute little blue bird that many people probably think Twitter itself created. Twitter would go on to incorporate a little blue bird as its logo mark. ↩ 87.5% of statistics are made up on the spot. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=38594697",
    "commentBody": "Omg.lol: An Oasis on the InternetHacker NewspastloginOmg.lol: An Oasis on the Internet (blakewatson.com) 578 points by blakewatson 13 hours ago| hidepastfavorite253 comments httpsterio 7 hours agoI just joined after reading the post. This wasn&#x27;t the first time I&#x27;ve heard of Omg.lol but I wasn&#x27;t entirely convinced earlier.For a long while, I&#x27;ve felt kinda lonely online as all of the communities and little corners online I&#x27;ve been part off have slowly died. I guess I&#x27;ve sort of been digitally homeless.I really enjoy the latest trends when it comes to indieweb and digital gardens, people creating their own space instead of living on closed platforms, so this definitely hit all the marks for me. I don&#x27;t think I&#x27;ve bought anything online faster than just now haha.Blake just cost me twenty quid, but I&#x27;m happy to vote with my feet instead of selling my data and attention to big corporations. reply crawsome 6 hours agoparent> Section 6.3 We may share personal information in connection with a corporate transaction, like a merger or sale of our company, the sale of most of our assets, or a bankruptcy.>Section 6.5 Except where explicitly stated to the contrary in this Policy, in some cases, particularly given the limited amount and type of information and data collected through omg.lol, we have not restricted contractors’ own use or disclosure of that information or data. We are not responsible for the conduct or policies of Stripe, or other contractors.INAL but that seems pretty cookie-cutter \"Company is not ruling-out selling your data to others\".https:&#x2F;&#x2F;home.omg.lol&#x2F;info&#x2F;legal reply Arch485 5 hours agorootparentAlso not a lawyer, but that sounds more like \"if another company acquires us, we will give your info to them\" and then separately \"Stripe might sell your data; we&#x27;re not responsible for them\".Which is rotally reasonable&#x2F;expected imho. reply underdeserver 1 hour agorootparentNot a lawyer so I might be reading this wrong - but to me this says \"We might sell the company to someone else, and they in turn might sell it to anyone\", and that&#x27;s a bit scarier. reply computerex 3 hours agorootparentprev>... we have not restricted contractors’ own use or disclosure of that information or data. We are not responsible for the conduct or policies of Stripe, or other contractors.I mean this seems pretty suspect for anyone privacy focused. reply rusk 1 hour agorootparentAlso not legal in Europe where you absolutely are responsible for the actions of your processors reply CaptArmchair 1 hour agorootparent> Section 8.6 GDPR> Part b. omg.lol does not believe its processing of limited personal data of those outside the United States (if any) brings it within the jurisdiction of these laws.That&#x27;s a hard disclaimer if there&#x27;s any.I read that as: if you&#x27;re a European user, we do not believe you can legally enforce us to honor your rights, even though we operate within the EEA. reply ykonstant 41 minutes agorootparentThis is very disappointing, and automatically dismisses omg.lol as an option for me as a researcher and educator. reply rusk 30 minutes agorootparentprevWorth a shot I suppose reply cderpz 1 hour agorootparentprev>omg.lol does not believe its processing of limited personal data of those outside the United States (if any) brings it within the jurisdiction of these laws.Oh dear. That is definitely not correct. The only way for omg.lol to not fall under the jurisdiction of the GDPR is to not offer their services to people living where it applies. reply amne 4 minutes agorootparentAnd how would the owner go about that? Implement expensive geo-fences and KYC processes for a market they are not interested in? If they (EU people) want to use it .. they should be able to without expecting the same protections as if the business operates in EEA.How did we get here? To where If I spin up a webserver and charge for access now I&#x27;m suddenly forced to lick your middle finger because you have laws in your country saying so?shusaku 8 hours agoprevThat’s a fun set of features, but I don’t see the connection with the community. You can browse their mastodon feed and it’s just a bunch of vaguely liberal vaguely tech posts? I’d like to see which accounts are using the services for a better community reply jim-jim-jim 7 hours agoparentThat&#x27;s the shortcoming of every alternative protocol and \"indie web\" community I&#x27;ve come across. They only attract existing techies and have a weird sheen of forced kindness about them. If you&#x27;re just chatting with other programmers under American HR communication standards, then how is it any different to work?The true magic of the early web was somebody genius but decidedly untechnical like David Bowie shitposting at his own fans. There&#x27;s no special line of code that&#x27;s going to foster that. You have to ruthlessly curate a community to avoid a critical mass of sensitive nerds, but guess who the early colonizers of these alt platforms are. None of these communities will attract today or tomorrow&#x27;s David Bowies. reply p-e-w 6 hours agorootparent> The true magic of the early web was somebody genius but decidedly untechnical like David Bowie shitposting at his own fans.No, the magic of the early web was that people treated their online identities as a secret alternative life, rather than a resume for recruiters, friends, potential partners, and other real-world acquaintances to look at.The Internet of today is little more than a (distorted) mirror of people&#x27;s offline lives. That&#x27;s why the problems of today&#x27;s Internet are the same as the problems of the real world. By contrast, the Internet of the 90s was an exciting world of its own, with rules that were dramatically different from those of everyday life. reply jl6 2 hours agorootparentThis, but also because it was something genuinely new that had never been seen before. Doubly so if you were young then and old now. Everything was novel, and therefore interesting - even the bad things. I’ve seen people expressing nostalgia for blink tags.Perhaps the medium is just a little played out. reply rudasn 4 hours agorootparentprevYeah, in the 90s and 00s I think people published just because they could. Either real identity or not. They (we?) had something to say, to express.Nowadays people just publish to be seen. There&#x27;s a huge difference on the type of content this leads to. reply paledot 4 hours agorootparentprev> The Internet of today is little more than a (distorted) mirror of people&#x27;s offline lives.Our offline lives are a distorted mirror of the Internet of today. reply hsn915 6 hours agorootparentprevAlso many of us were much younger, even teenagers, with little to no exposure to HR hell. reply jackstraw14 2 hours agorootparentMany weren&#x27;t, too. reply hitekker 5 hours agorootparentprev> attract existing techies and have a weird sheen of forced kindness about them.> If you&#x27;re just chatting with other programmers under American HR communication standards, then how is it any different to work?> There&#x27;s no special line of code that&#x27;s going to foster that.> you have to ruthlessly curate a community to avoid a critical mass of sensitive nerds, but guess who the early colonizers of these alt platforms areGreat comment. Aligns with my own observations. On the note of \"American HR Communication standards & work\" I think most of us don&#x27;t have experience participating in, let alone, organizing real communities[1]. Since most internet communities are awful, imaginary, transient etc, we default to the only actual experience we have semi-happily working with strangers: our jobs. Adding on top how internet comments are forever, cancelations is right around the corner, and careers hang in the balance, and you get a Bay Area photocopied dialogue.[1]https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Third_place reply onion2k 2 hours agorootparentprevguess who the early colonizers of these alt platforms areThe early web was mostly nerds, but not just tech nerds. I made my first site in 1997 and I linked to all sorts of things about TV shows, music and games that had been made by fans of things. If someone loved the X-Files and wanted to contribute to a site about it the only option was to get a book about HTML from the library and learn to use FTP. It thrived because it was just a group of people enthusiastic about things. Few people wanted to criticise because the only response you&#x27;d get was \"well you make a better website then!\". And when that happened people did. There were rivalries that worked like a feedback loop to improve things. That&#x27;s missing today. People just criticise and don&#x27;t try to do better. I blame the rise of guestbooks. reply ykonstant 35 minutes agorootparentMy first experience with \"social media\" was in the late 90s with a website dedicated to the Wheel of Time, www.wota.com. We had enormous fun in the forums and web chat, and I loved the design and flow. It was mostly hacked together in Perl. Rand Al&#x27;Thor, if you are reading this, where are youuu? It&#x27;s me, TrueSource! ＼(≧▽≦)／ reply robertlagrant 42 minutes agorootparentprev> guess who the early colonizers of these alt platforms areSorry - what is \"colonizer\" here? Do you mean users? reply rtpg 6 hours agorootparentprev> None of these communities will attract today or tomorrow&#x27;s David Bowies.I kind of get what you&#x27;re saying but I&#x27;m tired of people who act like \"shitposting skills\" are a useful quality trait. Similarly people who just can&#x27;t not let something be.I kind of dislike \"forced kindness\" as a community philosophy (I&#x27;ve met way too many people IRL who have a net persona of \"super kind\" and turn out to be, glibly, sociopaths), but \"please don&#x27;t be insufferable\" is a nice rule of thumb for communities. Plenty of cool stuff made by people who are merely a little annoying. Meanwhile too many places have \"those people\" who just won&#x27;t let something go. Let people keep their honor! reply jim-jim-jim 6 hours agorootparentShitposting wasn&#x27;t the best choice of words, sorry. I think you know what I&#x27;m getting at though. There are cheeky artists and those to whom cheekiness is the art. The latter cohort are just annoying trolls, but the former group can animate communities. You just don&#x27;t tend to find them among the small souled and dogmatic bitdiddlers that haunt every upstart platform. reply joeross 3 hours agoparentprev> I’d like to see which accounts are using the services for a better communityIt’s more like after you use it for a little while you look up and suddenly realize you’re in a new but familiar feeling community. It definitely skews developer&#x2F;blogger&#x2F;liberal, is openly inclusive and mindful of accessibility (not perfect, but always trying), there&#x27;s a lot of overlap with various micro.blog&#x2F;IndieWeb&#x2F;fediverse communities, a lot of folks with active GitHub accounts doing interesting stuff, a strong photographer contingent, an overarching “positive vibe” as the kids say, and a clear sense that you don’t have to remind the kind of folks who enjoy using omg.lol that there’s a person on the other side of the keyboard.Maybe that still doesn’t make much sense to you, but while I’m happy to pay for cool stuff people make on the internet, I’m paid up with omg.lol through 2030, which just isn’t something I would do anywhere else. reply proxyon 1 hour agorootparentI on the other hand would happily pay through 2030 to avoid the people you describe on omg.lol. I dislike pretentious tech positivity and HR catladies policing my online life. reply alex_lav 5 hours agoparentprevI made an account and can’t seem to figure out where this magical community actually is? It seems like I can just link other open services? And for some reason I can receive email?Not a single other person(‘s content) in sight though. reply Always_Anon 7 hours agoparentprevnext [10 more] [flagged] h0l0cube 7 hours agorootparentIf you&#x27;re looking for an oasis of &#x27;gloves off&#x27; discourse, 4chan and its ilk already fits that niche. omg.lol harks back to a time on the internet where people were at worst just &#x27;silly&#x27; to each other, and people would talk to random strangers on AOL or ICQ and the like with genuine curiosity and hope for the future of the internet. reply acdha 7 hours agorootparentprevWhere I grew up, that was called being polite and it didn’t automatically rule out anything other that liberalism. There’s a ton of room between disagreement and harassment. reply Always_Anon 6 hours agorootparent>Where I grew up, that was called being polite and it didn’t automatically rule out anything other that liberalism.Where I grew up, that was called being polite and it didn’t automatically rule out anything other that conservatism.>There’s a ton of room between disagreement and harassment.There was a time where that may have been true, but we&#x27;re far away from those times. reply Novosell 5 hours agorootparentMind you, he said liberalism and not liberal. Modern day conservatives tend to be very pro-capitalism, which liberalism loved.Not sure when things got all turned around in America. Liberalism = smaller government and a free market. Yet in the US a \"liberal\" dislikes capitalism and wants more government? It&#x27;s odd. reply teknico 3 hours agorootparentIt’s a long story.Left and Right: The Prospects for Liberty - Murray N. Rothbardhttps:&#x2F;&#x2F;mises.org&#x2F;library&#x2F;left-and-right-prospects-liberty reply dragonwriter 3 hours agorootparentprev> Yet in the US a \"liberal\" dislikes capitalism and wants more government?American liberals love capitalism; its one of the key things that differentiates the liberal (center-right) faction of the Democratic Party from the progressive (center-left) faction, which is (largely, quite mildly) critical of capitalism, and, in turn, from the actual left. reply Sophistifunk 7 hours agorootparentprev> There’s a ton of room between disagreement and harassmentUsed to be. Nowadays, if you disagree with a subjective assessment of the relative weight of two competing goods, you&#x27;re \"genociding\" somebody somewhere. reply RuleOfBirds 7 hours agorootparentprevGod. \"Freedom of speech\" and \"free of abuse?\" Sounds like a nightmare! Liberals!!Where are the spaces anymore to be outlandishly an asshole and shut other people down?! :-| reply hutzlibu 6 hours agorootparentThe thing is, what counts as \"abuse\" has considerably changed.There seems to be a shift towards, how someone perceives something, not how something actually is.That can mean, that it is no longer possible to speak about something unpleasant, as someone might consider that an insult - and that is then the opposite of freedom of speech.In other words, I don&#x27;t enjoy the assholeness of 4chan - but I also do not enjoy overly polite spaces, avoiding all controversy. I am not clear yet, what this Oasis seems to aim for, but it might be the latter. reply proxyon 2 hours agoparentprevYeah there&#x27;s no way I&#x27;m hanging out with a bunch of people salty at Musk because he stopped Twitter from being an old boys club of internet liberals. reply jay-barronville 52 minutes agorootparentI know you’re getting downvoted, but I agree with you. (I fully expect to be downvoted too after this comment—that’s totally fine!)I saw the OG post earlier, checked out the service, and I upvoted it, because I thought, “Hey, that sounds really cool!” Then I checked out their Mastodon thing and I realized it’s your typical group of virtue-signaling, outraged-at-everything, anyone-who-says-something-I-don’t-like-is-a-Nazi, I’m-inclusive-but-exclusive-to-different-ideas lefties. As a black man who used to be a leftist activist many years ago (I’m far from that now, if you couldn’t already tell), I’m so over these types of folks and their faux positivity. That said, maybe that’s for the best (mutually): They’ve done a great job at preventing someone like me from paying to join their community—they’d probably label me a Nazi the moment I commit the crime of wrongthink anyway. reply PenguinRevolver 12 hours agoprevIt&#x27;s nice, the only problem I got with omg.lol is that Wayback Machine archives are unavailable for all domains. I&#x27;m concerned that this part of the internet won&#x27;t be saved for others to see in the future. reply anjel 10 hours agoparentWorks with archive.today: https:&#x2F;&#x2F;archive.is&#x2F;zAbYO Also works with Ghost Archive: https:&#x2F;&#x2F;ghostarchive.org&#x2F;archive&#x2F;ValSPWayback Machine is arguably a more durable archive site than these other two archives, but the fact that it can be archived elsewhere would indicate that the problem is likely to be on archive.org&#x27;s end of things rather than omg.lol reply burkaman 10 hours agoparentprevThe creator&#x27;s company website is also excluded: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230000000000*&#x2F;https:&#x2F;&#x2F;neatnik..... Maybe some philosophical disagreement? reply rapnie 1 hour agorootparentThis blocking of the archiver may be philosophical, but not a disgreement. Just speculating, but on the fediverse there are quite a few people who feel their social interactions are personal and &#x27;in the moment&#x27;. Something akin to the Cozy Web [0] though not being too strict about (everything is still public after all).[0] https:&#x2F;&#x2F;maggieappleton.com&#x2F;cozy-web reply lucb1e 10 hours agorootparentprevPresumably due to https:&#x2F;&#x2F;neatnik.net&#x2F;robots.txt reply flexagoon 4 hours agorootparentWow, thanks for pointing that out, that made me never want to join omg.lol reply 77pt77 5 hours agorootparentprev> User-agent: ia_archiver> Disallow: &#x2F;Denied! reply graypegg 10 hours agoparentprevThat kind of sucks :( So much of the \"small internet\" of the past people talk about in relation to this stuff, is only really preserved in any significant scale by IA. Hope it&#x27;s not the operator making a big sweeping decision for all users. reply Grimblewald 17 minutes agorootparentSome might argue that is the magic of it. It is much easier to be happy when you miss some things, and look forwars to others. Some listen to radio, or use streaming services in a radio like way (no skipping, no targeted searches) for the same reason, sure they could keep looping their favourite song on whatever platform, but its waaay more exciting when it comes on unexpectabtly.Our interactions having a fleeting nature makes them more special and forces us to be more emotionally involved.Just an alternative take, no a statment of my personal opinion. reply politelemon 11 hours agoparentprevJust tried and I see someone else also tried after seeing your comment.> The same snapshot had been made 25 minutes ago. You can make new capture of this URL after 1 hour.But yeah it&#x27;s strange, nothing appears in the archive:https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230000000000*&#x2F;https:&#x2F;&#x2F;bw.omg.l... reply toomuchtodo 11 hours agorootparentIt’s possible the site owner has asked the Archive to dark site specific captures. Capture jobs will still run, but they won’t be available publicly (until some future date).You can always run your own crawls with grab site: https:&#x2F;&#x2F;github.com&#x2F;ArchiveTeam&#x2F;grab-site reply blakewatson 12 hours agoparentprevOh wow, you’re right. I wonder what’s up with that. reply contrarian1234 11 hours agoparentprevI think that&#x27;s great.. archiving should be opt-in not opt-outYou can read and access my work&#x2F;words as I want. And once I don&#x27;t or change my mind you can&#x27;t. Once someone posts something, you don&#x27;t have a right to it in perpetuity .. That&#x27;s how things should work - but that&#x27;s just my opinion reply TeMPOraL 11 hours agorootparentExcept for the artifice that is copyright, things don&#x27;t work like that for anything else. Reality doesn&#x27;t work like that.> Once someone posts something, you don&#x27;t have a right to it in perpetuityOn the contrary, once someone posts something, they don&#x27;t have control over it anymore. You can&#x27;t make me unsee what you wrote, or unhear what you said. You have no right to stop me from writing it down, and even if you can stop me from republishing it verbatim right now, you generally don&#x27;t have the right to do it indefinitely.> And once I don&#x27;t or change my mind you can&#x27;t.To be clear, I&#x27;m not dogmatically firm about it, but I believe that a word in which you get to distance yourself from past views, or mark them mistaken, and people accept it, would be much better than the world in which you&#x27;re free to gaslight everyone else by pretending that something never happened, even though it did.(All that on top of the usual point that it&#x27;s neither the author nor their audience that can judge what&#x27;s archive-worthy - only future people can.) reply notkaiho 10 hours agorootparentBrilliant argument&#x2F;username combo :) reply leononame 11 hours agorootparentprevI Disagree. There&#x27;s not a big difference between someone reading your stuff and saving it versus automatic archiving. Being able to delete what you said makes real discourse with a bad actor very hard if not impossible. If you change your mind, you are always free to rectify, but you shouldn&#x27;t be able to pretend you never said this or that.I know there&#x27;s a line to draw somewhere, personal blogs aren&#x27;t our countries&#x27; leaders&#x27; Twitter accounts or press conferences. Copying someone&#x27;s copyrighted work in form of an archive might some legal implications I&#x27;m not aware of. But keeping things for posteriority is important and I don&#x27;t believe people should be able to choose what part of their words and actions will be recorded and which won&#x27;t. reply johnfernow 4 hours agorootparentprevIn the UK, if you publish a book, magazine or newspaper, by law you have to send a copy to the British Library for archive. A lot of other countries have similar laws. In the UK, legal deposit has expanded to include the web (so long as the person&#x2F;group creating the content is in the UK), but since many individuals and small businesses are unaware of legal deposit, the UK Web Archive will archive a lot of the web by themselves.Tom Scott interviewed some people from the British Library, and they explain the importance of archiving:> The importance of legal deposit not being selective, and being everything, is: we can&#x27;t decide today what&#x27;s going to be important in 50 years&#x27; time. We want everything, because we don&#x27;t know what will be important.He also added his own thoughts:> I cannot overstate just how useful it is to be able to track down things that never made it online, or to research out of print, forgotten books where there are no other copies available, or to scan through every issue of an obscure local newspaper to track down one reference. This is the raw text of history, as it happened, and someone has to keep it preserved for the future.source: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ZNVuIU6UUiM reply Aeolun 9 hours agorootparentprev> archiving should be opt-in not opt-outThat’s really weird. If someone posts a sign on their store window, and I take a picture of it, should I be required to delete the picture when they remove the sign? reply porcoda 9 hours agorootparentprevTotally agree. The tech community has a massive arrogance problem where we tend towards opt-out vs opt-in for everything. Just because us tech-savvy folks understand the consequences of, say, posting something online, doesn’t mean the bulk of humanity who isn’t tech savvy also understands that and agrees with us. reply JoshuaRogers 8 hours agorootparentWhile we in the tech community are guilty of taking many things for granted as generally understood, I’m fairly certain that “consequences for past public statements” predates the bulk of our modern technology. reply bee_rider 7 hours agorootparentprevThere isn’t any way we can make being copied opt-in, rather than opt-out. We can not copy things. But we can’t prevent other people from copying things. So, it is better to set the expectation that things will be copied, otherwise people will be mislead into thinking they can delete their content, and will post things they regret.Plus, if everyone can delete their mistakes, we’ll live in a world where it looks like nobody makes mistakes, and so we’ll be less tolerant of mistakes. reply echelon 11 hours agorootparentprevVehement disagree. Many of the early communities I participated in are gone forever, and it&#x27;s a shame to think of how much more has been lost to time.In the absolute limit, I hope our future descendents reconstruct the past light cone and can replay all of our biochemical thoughts and emotions. Perhaps even simulating our existence and perception to exacting precision.Maybe they&#x27;ll get to see t-rexes in their natural habitat, visit lost 90s websites, and feel what taking the organic chemistry final was like. reply rd 9 hours agorootparentI’ve had this exact thought a million times.The first time I tripped acid - I remember writing a page of notes on how sad I felt that I would never get to experience the exact way a memory occurred to me in the past.What’s even more saddening is that with tech like Rewind, and what’ll be the future of Rewind in 10-20 years, by 2040, I fully expect all memories&#x2F;events ever produced to be logged in an almost endless database of all human experience.But - because time is linear, we wouldn’t ever fully be able to simulate the past of say everything before 2030? And that’s just so sad. reply Aeolun 8 hours agorootparentIn one way it’s sad, but if we archive everything from 2040 onward I guarantee that any pre 2040 years will always seem like a better time. reply afpx 7 hours agorootparentprevThat&#x27;s why I have alt accounts - one for each of my different personalities. reply bovermyer 10 hours agorootparentprevI disagree.If you publish something publicly, it should be available for all time.If you change your mind, it&#x27;s on you to make that known. reply yellow_lead 12 hours agoparentprevIs there a reason for that or they just haven&#x27;t been archived yet? reply Ringz 11 hours agorootparentUnlikely. Some people archive every page they visit. reply famahar 1 hour agoprevLooks fun. I&#x27;m considering signing up but I think I&#x27;d just be more happy not having a heavy online presence. Twitter falling apart made me really enjoy being offline and connecting with friends and family. Small community is key I find. omg seems like the right direction in this regard. reply cianmm 12 hours agoprevI&#x27;ve been using Omg.lol for around a year now (Cian.lol) and am really enjoying it. It&#x27;s just so simple - it feels like travelling back in time to when we wrote blog posts and made websites to share with our friends, not to Create Content. reply tambourine_man 11 hours agoparentThat internet is not dead, you know? It’s just the the other part grew so massively.There are still people writing blog posts and websites that don’t require you to dismiss 5 popups before you can interact with it. It can be done. reply lacrimacida 8 hours agorootparentIt’s just hard to find. Google returns trash reply larrysalibra 7 hours agorootparentTry out kagi...they have a filter for \"smallweb\" posts: https:&#x2F;&#x2F;kagi.com&#x2F;smallwebThe list of sites is on github: https:&#x2F;&#x2F;github.com&#x2F;kagisearch&#x2F;smallweb reply rtpg 6 hours agorootparentprevIt&#x27;s wild to think about how anybody found info back in the day. Forums were probably the big one I guess? There was always something magical about being linked to forum and finding a wealth of info there, and entire domains of knowledge.FWIW stuff linked from HN & friends is not always the best, but I am pretty agressive about sticking RSS feeds from blogs that get linked here. That gives an inflow of interesting stuff people find. It&#x27;s not a thing you can do in one go, but after a while you have a lot of neat stuff from people who cared enough to post it. reply monkeywork 17 minutes agorootparentwebrings, IRC, forums and mailing lists, etc. reply lhmiles 7 hours agorootparentprevGive me your favorite small web links reply lannisterstark 12 hours agoparentprevI skimmed OPs post, and then read yours, and I&#x27;m still a bit confused as to how it&#x27;s different than just hosting a mishmash of different but related services yourself. If you could not, yes that&#x27;s fine. But if you could, what really are the advantages? reply cianmm 12 hours agorootparentI argue with computers for my day job, I don’t want to do that after work hours too. I’m happy to pay somebody else (especially Adam who is just so active with the community) a fairly paltry sum to do it for me. reply lannisterstark 11 hours agorootparentTo be entirely fair (in my situation), what I do at work and what I find fun to do with computers are two different things :P reply alexeldeib 12 hours agorootparentprevThis is the classic Dropbox criticism, no?Moreover, the pleasure has nothing to do with self hosting or not, it’s just a pleasant and whimsical UX while being technically solid. reply soulofmischief 9 hours agorootparentprevJust because I can manage a service doesn&#x27;t mean I want to all the time. I&#x27;m a busy guy and already have client infrastructure to manage. At a point in my life where I&#x27;m trying to cut down on things I have to tend to. reply tw04 12 hours agorootparentprevPresumably the mastadon integration. Think twitter with your profile directly tied to your personal site - except not twitter. reply graypegg 12 hours agorootparentprevI think you kind of answered your question, no? Setting up web things, especially when they have a chance to get quite bursty hug-of-death traffic, is hard for most people. I&#x27;d prefer to set things up myself but I know that places me in a verrrrry small minority of folks. reply rsynnott 10 hours agorootparentprevHosting all of this stuff on your own would be a lot of fuss which most people wouldn’t want to bother with. reply stevebmark 6 hours agoprevIt takes blog posts to discover these because Mastodon micro communities aren&#x27;t discoverable and no one knows which ones to sign up for. Mastodon has no long term potential. We&#x27;re still waiting for the Twitter replacement. reply inamberclad 6 hours agoparentWhat is the long-term potential supposed to be? Is Mastodon supposed to replace Twitter, or is it supposed to enhance the lives of people? I&#x27;m a member of several small forums that just don&#x27;t grow. It&#x27;s the same people each day, and that&#x27;s fine. It&#x27;s much closer to how human interactions work in real life. You don&#x27;t join an ever-expanding pool of people where you strive to maximize your connections (or at least, I don&#x27;t). Instead, you probably have a relatively small group of people that you hang out with more often. reply dash2 4 hours agorootparentThis argument confuses “everybody hangs out with just a few people” with ”a few people hang out with a few people”. The former is a cool idea, sure, but the latter is just a description of a not-very-successful service. I mean, I like my local pub, but it isn’t HN-worthy.Social media is valuable, that’s why people use it. It would be nice if we end up coordinating on social media that aren’t toxic or addictive. Unfortunately mastodon may not make that happen, as GP said. reply smallnix 1 hour agorootparentSo a system that enables thousand if not millions of \"pubs\" would be HN worthy? From what I understand that is mastodon and this article is a success story of a single instance&#x2F;\"pub\". reply ClimaxGravely 5 hours agorootparentprevEven then I have a small fraction of the followers from twitter than I do on mastadon and I still get way more engagement. Both in numbers and quality. It&#x27;s not oldschool forums quality but it feels a lot closer. reply p-e-w 5 hours agorootparentprevWell said. It&#x27;s astonishing how much the corporate&#x2F;capitalist mantra \"if you&#x27;re not growing, you&#x27;re dying\" has taken hold in the world of open source and free culture. People not only fail to realize how unsustainable and destructive that idea is, many don&#x27;t even seem to know that alternative community models exist, and have been practiced since forever. reply Ridj48dhsnsh 5 hours agorootparentprevNot being indexed by search engines is a fatal flaw in my opinion. There might be some interesting discussions taking place on Mastodon, but I would have no way of knowing. reply frikk 5 hours agorootparentThis is an interesting thought.As an analogy, there might be some interesting discussions happening at my local Community Center, or my neighbor&#x27;s house, but I would have no way of knowing. But to discover these discussions, I would need to meet someone with a shared interest who would, in turn, share with me a place that they go to for continued discussions and to hang out with interesting people who share an interest.So maybe, if done correctly, this is a feature? The good content is one extra network connection away, but easy enough to find if an advocate chooses to highlight content, share a connection, or otherwise create an inbound reference to the community. reply lannisterstark 9 minutes agorootparentYes and wouldn&#x27;t you like to join it?If you had a way to search like \"hey there&#x27;s an interesting conversation going on at my local community center, maybe I will go and join their next session.\"wouldn&#x27;t you? reply aliasxneo 5 hours agorootparentprevI like the idea of it, but I also have no idea how one would find any of these cited discussions. It seems having an existing social network gives you a strong advantage. As a lurker, introvert, and ruralite, I think I&#x27;m going to be naturally disadvantaged on these types of platforms.Or maybe I&#x27;m just misunderstanding the whole design. reply input_sh 2 hours agorootparentprevYou have an option in user settings to allow search engines to index your profile and public posts. (It&#x27;s off by default.) reply golem14 6 hours agoparentprevMaybe that&#x27;s a feature. Early Gopher was similar, and people adapted by writing hubs&#x2F;directories.Not everyone needs their content to reach record # of visitors. reply TechSquidTV 6 hours agoparentprevMastodon isn&#x27;t meant for hosting this kind of content, for the same reason you aren&#x27;t meant to put this kind of content on Twitter. Mastodon is like a social RSS feed reader. reply 3abiton 1 hour agoparentprevI&#x27;m just curious what is the difference between Mastodon and Lemmy. I know they are a decentralized clone of Twitter and Reddit, but at their core 90% similar. Is it just the comment threads? reply jszymborski 5 hours agoparentprevJust following hashtags and using the discover page works pretty good for me reply metabagel 3 hours agorootparentIt doesn’t work for me. A lot of people don’t realize that their posts won’t show up in searches on other Mastodon instances unless they include hashtags. I found it to be a huge chore to find people posting about topics I was interested in. I pretty much gave up. reply omginternets 5 hours agoparentprevThe reason they’re good has a lot to do with how hard they are to find. reply john-radio 6 hours agoparentprevActually the trending posts I saw when I clicked through to social.lol (omg.lol&#x27;s Mastodon instance) are most of the same posts from my Explore page (the # icon) on urbanists.social, and most of these posts are not from either of these two instances but from diverse (and usually individually interesting!) ones, but please keep enjoying that haterade if you like the taste. reply scythe 5 hours agoparentprevDiscoverability doesn&#x27;t always have to be so fast. As long as the word eventually gets around, maybe a slower kind of discovery could be good for some communities.There&#x27;s also boardreader.com for finding small communities, although I don&#x27;t think it really tilts towards Mastodon very much. reply metabagel 3 hours agoparentprevDiscoverability is Mastodon’s Achilles heel. reply rconti 3 hours agorootparentTBF, a problem that twitter also had. reply hiidrew 6 hours agoparentprevhttps:&#x2F;&#x2F;www.farcaster.xyz&#x2F; is an interesting alternative that&#x27;s not bluesky reply nicbou 12 hours agoprevIf omg.lol is an oasis, this post was a stranger offering you a sip. What a refreshingly nice and personal post! reply NanoYohaneTSU 8 hours agoparentIt&#x27;s an ad bro reply beardicus 6 hours agorootparentare you saying the author was paid for this post? seems like an enthusiastic user to me. do you know what an advertisement is? reply crawsome 7 hours agorootparentprevIt certainly feels like it. reply damiante 11 hours agoprevI love the idea of such smaller communities and the \"old web\" style of interaction, but for me the issue is one of discoverability. How do I find and follow people? Does anyone still use RSS, or are we relying on Mastodon&#x2F;ActivityPub? Bavk in the day this was the purpose of search engines, but it seems that now such small pages are scarcely even indexed... reply chongli 10 hours agoparentDiscoverability and smallness are at odds. This problem isn’t specific to the internet. That quaint, beautiful postcard town does not remain so once it’s been discovered. Eternal September happens everywhere. reply mmazing 9 hours agorootparent> Discoverability and smallness are at odds.Is it really true on the internet though? omg.lol could presumably stay \"small-appearing\" and \"quaint\" and have millions of users. How could you really tell the difference?If it were all indexed you could drill down and find people who share your interests, that doesn&#x27;t necessarily ruin the website, yeah? reply moralestapia 8 hours agorootparentprevNah, you just need a (not ad oriented) search engine.Things could continue to be small and niche, we just a way to find them. reply hooby 2 hours agoparentprevMight be a tangent - but is more discoverability actually desirable in this case?Could it possibly preserve that \"old web\" style of interaction, if it becomes a global phenomenon that everyone uses? Or does this only work as long as it stays a little hidden niche, that most people don&#x27;t know about, and will never find?Or in other words - can something feel like \"the old web\" (which was early adopters and enthusiasts only) - if it&#x27;s frequented by everyone?You love the idea of smaller communities - but how can they stay small? reply culopatin 5 hours agoparentprevHow did we find forums back in the day? Someone said something somewhere and you looked it up. It was less discoverable but less… volatile, because it was just “your” kind of people there, not millions of random people who found a hashtag reply ClimaxGravely 5 hours agorootparentI wonder about that myself as someone who grew up on this.I used webcrawler at the very beginning and I&#x27;m probably looking that things through rose lenses but I found what I wanted back then. I think back then in some ways it was easier to find your community because SRO and the like wasn&#x27;t a thing back then.The years where I found my niche forums benefited me much more than my college days. reply acegopher 9 hours agoparentprevCheck out the Kagi Small Web: https:&#x2F;&#x2F;blog.kagi.com&#x2F;small-web reply mtillman 10 hours agoparentprevPlug&#x2F;thanks for https:&#x2F;&#x2F;ooh.directory&#x2F; for keeping the dream alive. reply 082349872349872 1 hour agoparentprevhave you tried https:&#x2F;&#x2F;search.marginalia.nu ? reply treyd 9 hours agoparentprevFwiw, many feeds provided by Mastodon instances are available as RSS. Same for other Fedi software, like WriteFreely. reply xhrpost 10 hours agoparentprevhttps:&#x2F;&#x2F;home.omg.lol&#x2F;directory reply tonymet 12 hours agoprevgoes to show there&#x27;s still lot of creativity left in the web. web pages, DNS, email forwarding, vanity domains -- i&#x27;m glad to see hackers tinkering and exploring what the next gen web looks like. Otherwise we&#x27;ll lose it to commercialism and walled gardens. reply maxlin 5 hours agoprevWhen a blog post starts with saying \"twitter is dead\" it doesn&#x27;t really make it worth reading further. \"Twitter is dead\" was said pretty much as numerously as \"2 more weeks\" but it&#x27;s off better than ever, with Community Notes having proved themselves and X now having proved its capability to serve its main mission by working as the town square on issues related to OpenAI, Gaza, etc, etc.Eventually, with subscriptions paying most of the bills, I hope the API access per-client is brought back without extra costs too. But even without, X does have pretty much everything it needs, and will only grow with time. You can&#x27;t put a price on Freedom of Speech. reply Timwi 4 hours agoparent> When a blog post starts with saying \"twitter is dead\"That&#x27;s not what they said. They said “it’s the day that, for me, Twitter died.” I read that as meaning “I personally don&#x27;t want to use Twitter anymore.”I personally feel the same about Reddit. I was a very regular reader and contributor, but since the big brouha about third-party apps I decided that it&#x27;s dead to me. I&#x27;m no longer using it. That doesn&#x27;t mean it has died as a platform, but it does mean that I personally have moved on from it. reply jhugo 1 hour agorootparentI wanted to move on from Reddit when all of that was going down, but it&#x27;s really the only place on the Internet where I could get (just from the memory of the last 24h) a decent range of discussion&#x2F;advice from real people about pizza stones, indoor plants, descaling a coffee machine, learning piano as an adult, and the answer to the question \"TV show where someone sings Chattanooga Choo Choo\", all from a single website that isn&#x27;t heavily polluted by ads. As long as \"[query] reddit\" makes Google so much better, I can&#x27;t really consider Reddit dead. reply enumjorge 4 hours agoparentprev> [Twitter]&#x27;s off better than everI&#x27;d agree that it&#x27;s hard to take an opinion seriously that pronounces Twitter as dead. As you pointed out, when OpenAI&#x27;s drama was unfolding, the conversations happened mainly on Twitter. But saying Twitter&#x27;s current form is the service at its best is also hard to take seriously. I tried to follow said conversation about OpenAI during Altman&#x27;s ouster and I found the site to be an inconsistently broken mess. To this day, I&#x27;m still not sure why I&#x27;m able to access certain posts without signing in, but not others. In my experience, the quality of the discussion on the site as a whole has also taken a hit.And again with the whole freedom of speech. It continues to baffle me how people associate Musk with the first amendment. He brands himself as a free speech absolutist, but his actions have continuously shown him to have no problem silencing critics and playing favorites on the platform. reply prmoustache 2 hours agoparentprevWell twitter doesn&#x27;t exist anymore so yes it is dead.Also it is funny that for a lot of people including me, slashdot, digg, twitter and reddit are already a thing of the past while we are still visiting regular old forums. reply mcintyre1994 2 hours agoparentprevI’m unconvinced that subscriptions are ever going to make enough money to pay the bills. Musk has been pretty clear he needs to get advertisers back. I don’t think his approach of specifically telling Bob Iger to fuck off is going to help with that, but he isn’t hiding that he needs to figure it out pretty quickly. reply snailmailman 3 hours agoparentprev“Better than ever” when it’s constantly promoting hate speech? And Elon is too? And advertisers are dipping out?And Twitter definitely doesn’t have free speech. People still get banned, or have their posts artificially limited, but they do allow more hate speech.Nearly everyone I followed on Twitter is on Mastodon now. It works great. Conversations still happen there on news topics.I deleted my Twitter account a while back because my feed stopped being people I followed and became people promoting conspiracy theories. The site doesn’t even work properly anymore. People link to threads of tweets but only the first tweet displays. And profiles never show latest tweets. (I think these might work when logged in? But also don’t show any errors when logged out? I don’t know as I’ll never login again) reply SalmoShalazar 5 hours agoparentprevI thought community notes already existed as bird watch before the takeover? reply patcon 4 hours agorootparentYeah, Elon just rebranded it, and pushed fwd its full deploymentI was fully bought into the premise of birdwatch due to it being based on a great tool I&#x27;ve worked with for years (Pol.is), but Elon seemed to have loved it for all the wrong reasons, in a way that irked me. He seemingly just wanted to cut the trust & safety teams, and remove onus of creating policy :&#x2F; reply TheAceOfHearts 4 hours agoparentprevTwitter is dead, long live X. Twitter as it was no longer exists. Regardless of what one may think of Elon&#x27;s leadership, he&#x27;s making big changes to the platform.It&#x27;s a Ship of Theseus argument. How much does a platformm have to change before it&#x27;s no longer what it used to be? reply twelvedogs 5 hours agoparentprevsubscriptions are most of their income, they don&#x27;t pay the bills at all reply shermantanktop 12 hours agoprev“omg.lol is unabashedly built with PHP”PHP is on my mental list of forever-security-challenged tech, but it got on that list a long time ago. It’s 2023, is that still a reasonable concern? reply jay-barronville 12 hours agoparent> It’s 2023, is that still a reasonable concern?No. A LOT has changed in the world of PHP over the years. And to be honest, I give credit to amazing frameworks like Laravel [0] for giving PHP a massive facelift (I consider Taylor Otwell one of my software heroes). Overall though, modern PHP software is much cleaner and more secure than whatever you knew from years ago.[0]: https:&#x2F;&#x2F;laravel.com reply reddalo 12 hours agorootparentI agree about Laravel and Taylor Otwell.Moreover, I&#x27;d like to point out that even if the vast majority of PHP-backed websites are based on WordPress, WordPress is not an example of good PHP practices at all. Its code-base and coding standards are old and horrible. reply joshmanders 11 hours agorootparentThat&#x27;s because it tries to not break backwards compatibility and spoiler: past web people had horrible standards. reply zlg_codes 9 hours agorootparentToday&#x27;s web people have horrible standards, too. Who ships an entire browser to ship an application? reply quickthrower2 8 hours agorootparentThat&#x27;s nothing, next they&#x27;ll ship the entire world&#x27;s knowledge to ship an application :-). Looks at LLMs. reply jay-barronville 9 hours agorootparentprevAgreed re WordPress, although I haven’t seen their code in YEARS, so maybe their codebase has evolved too.Re Taylor, if I was a billionaire (or at the very least, extremely wealthy), he’s one of those folks I’d write a no-strings-attached blank check to go build anything he wants—just a brilliant and overall great human. I used to be very active in the Laravel community many years ago, and even way back then, before Laravel was super famous (first Laracon days), I remember meeting Taylor and being thoroughly impressed. Over the years, on multiple occasions, I’ve heard folks at relatively large organizations say they adopted PHP solely because of Taylor and Laravel. Recently, when I saw someone mention in a post that Taylor has a Lambo now, I was so happy for him—it feels great to see him thrive after making the type of impact that he has. reply rchaud 7 hours agorootparentprevPHP, jQuery and W3Schools - HN&#x27;s combined kryptonite reply block_dagger 12 hours agoparentprevConcerns with PHP are less about security and more about language design, at least that’s my take after 22 years of dealing with it off and on (full-time “on” for several years). reply Retr0id 12 hours agoparentprevSpeaking as someone who has pentested a few PHP codebases over the years, rather than as a developer, It&#x27;s a bit like C. That is, it&#x27;s an absolute footgun in the wrong hands, and a lesser footgun in experienced hands.For experienced devs following best practices and using modern frameworks it&#x27;s \"mostly fine\", and that&#x27;s the side of things that&#x27;s been improved over the years, but most of the old rakes are still there to be stood on. reply wvenable 12 hours agorootparent> but most of the old rakes are still there to be stood on.I don&#x27;t think that&#x27;s necessarily true -- a lot of features have been deprecated and removed. reply kemayo 8 hours agorootparentMost notably, in 5.4.0 (in 2012!) they removed register_globals and magic_quotes. (Which had both been deprecated and off-by-default for a while before, I believe.)The former was notoriously insecure, as what it did was promote anything passed in as a cookie, GET, or POST variable into a global-scoped variable inside your script. Since PHP didn&#x27;t require any sort of declaring-your-variables-before-using-them, it was pretty easy to wind up with scripts written in a way that would allow this an unwise amount of access to the script&#x27;s internals.The latter automatically escaped special characters with backslashes in all the aforementioned user-provided variables so you could pass them straight into mysql queries. It was, however, optional and so caused errors because code got written relying on it and then ran on servers with it disabled, allowing SQL injection attacks... or double-escaping things in code written the other way around.But these days are long behind us! reply lucb1e 10 hours agorootparentprevAlso a pentester here. I find C and PHP to be quite different. Somehow, C applications always have catastrophic issues pop up, sooner or later, where you can make it execute random code at least under some circumstances. PHP applications can be the same if the team is inexperienced or doesn&#x27;t get the necessary time to apply best practices, but I&#x27;ve also seen plenty of PHP applications where we didn&#x27;t find significant issues with the server-side aspects.PHP applications are fun to test because most teams found another set of solutions to the same problems (it has so much history that wheels have been reinvented a lot), so you get to see new things. They&#x27;re also typically larger than newer and new-style services written in a shiny new language, which haven&#x27;t had time to accumulate as many features and are often written as a microservice (smaller components where one&#x2F;each dev can know all the ins and outs, allowing to have a total overview so that security controls can much more easily be implemented in a unified way). reply smsm42 4 hours agoparentprevNo it is not. Arguably, it never were. I mean yes, PHP had security bugs. So did all other platforms - including, for example, the Java one that led to Equifax compromise, which is as close as \"everybody just lost their privacy\" as any single break-in can get. I&#x27;d argue that PHP&#x27;s security stance as a platform was never substantially worse than any comparable platform.However, you get two additional factors: a) it&#x27;s easy, therefore it attracts beginners and b) it&#x27;s popular, therefore a lot of software uses it. More various software - more security issues. More software implemented by beginners - a lot more security issues. That was inevitable - any platform that was as low entry barrier and as popular and that appeared in the same time, when the web was exploding, but the understanding of how to manage security on the web was lagging behind - would have absolutely the same going on.But, blaming the tool because a lot of people didn&#x27;t use it correctly - and, also, because due to its novelty there weren&#x27;t proper education and frameworks that made it easy to do the right thing - makes little sense. There&#x27;s nothing security-challenged in PHP. It&#x27;s just that PHP was there when security-challenged programmers started to build websites. Most of them grew up now and know how to do it right. Either in PHP or in any other language. reply Aeolun 8 hours agoparentprevWell, it’s extremely backwards compatible. To the point my 15 year old websites written in it still work with some minor (+&#x2F;- 10 lines) modifications.Presumably you can still write bad code in PHP. But the mysql library that was sql injection heaven is now truly dead. reply wvenable 12 hours agoparentprevNope.PHP itself has also come along way. I don&#x27;t know if it&#x27;s because of it&#x27;s reputation that it seems to evolve faster than most languages.I recently used PHP to construct my personal site&#x2F;blog. I didn&#x27;t use any frameworks but I did use it&#x27;s statically typed&#x2F;strongly typed features that that is very different from how I would have coded in PHP years ago. reply chupapimunyenyo 8 hours agorootparentIts* https:&#x2F;&#x2F;youryoure.com&#x2F;?its reply xwowsersx 6 hours agoparentprevNot related to security, but I was quite surprised to see how far PHP has come since I used it many years ago: [PHP doesn&#x27;t suck (anymore)](https:&#x2F;&#x2F;youtu.be&#x2F;ZRV3pBuPxEQ) reply mattl 12 hours agoparentprevNo, modern PHP frameworks have come a long way. reply _heimdall 9 hours agoparentprevSecurity really was (still is?) a WordPress concern. PHP itself isn&#x27;t really a security issue, security will come from the code you write rather than the language itself reply Keyframe 12 hours agoparentprevThat crown belongs to Javascript now. reply graypegg 12 hours agorootparentThe curse of popularity. Relatively more people using something, means higher absolute amounts of garbage being made with it. I wouldn&#x27;t say modern javascript tooling gives you some obscenely high number of foot guns to target practice with, at least compared to the other web-capable options. (PHP, Python, Ruby, etc) reply Aeolun 8 hours agorootparentYeah, JS does less with it’s stdlib, which I think means a lot of people end up using mostly decent packages from npm instead of writing extra garbage themselves. reply chupapimunyenyo 8 hours agorootparentIts* https:&#x2F;&#x2F;youryoure.com&#x2F;?its reply graypegg 7 hours agorootparentFor the last few weeks you’ve been mostly posting grammar correction comments. Why? I’ll give you something to keep your interest: wensday. reply jrflowers 8 hours agorootparentprevhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;logout reply jay-barronville 12 hours agorootparentprevPlease elaborate. reply tambourine_man 11 hours agoparentprevIt was wrongly added to that list I the first place. reply contrarian1234 11 hours agoprevSeems a bit like Github pages but with more of a social angle to it. I kinda expected Github to go in this direction eventually - but keeping social elements out of Github might have been a smart move reply kvathupo 11 hours agoprevI like this.That said, I doubt we&#x27;ll ever escape towards subscription-based social media models due to the prohibitive costs of CDNs, bandwidth, and storage for video&#x2F;images. But I suppose it&#x27;s a question of ends: do we want everyone on social media? reply kibwen 10 hours agoparentIf you can live without video and images, you could comfortably host even a very large forum (on the order of the top 10% of subreddits by volume) with only a $5&#x2F;month VPS, as long as you made it serve static pages and were judicious with your tech stack. The cost of hosting text alone wasn&#x27;t prohibitive 20 years ago, and it&#x27;s even less so today. reply rglullis 7 hours agorootparentMedia is (ought) to be stored in a shared, content addressable storage system like torrent magnet links and IPFS. Backed by something like Tahoe-LAFS. reply rapnie 1 hour agorootparentOT, but via the UI design thread on HN, I just bumped into noosphere protocol, which claims to be just like what you describe here.https:&#x2F;&#x2F;subconscious.substack.com&#x2F;p&#x2F;noosphere-a-protocol-for... reply lacrimacida 8 hours agorootparentprevVideo and images could he links too reply bhasi 11 hours agoprevThe \"web design in 4 min\" linked to at the bottom of the page is very interesting. reply bbx 11 hours agoparentI didn’t realise it was linked to in this article. I built that on a whim several years ago. It’s more about what can be done in 4 min rather than what’s being done. But I’m glad it inspired people to try to style their own website themselves. reply blakewatson 8 hours agorootparentOP here. I love that little site and I link to it often! reply generic92034 11 hours agoparentprevFrom that page: \"What is the first thing you need to work on?\"I would say, a page that is usable without scripts. ;) reply 1B05H1N 11 hours agoprevSorry, why would I pay 20&#x2F;year bucks for this when I have my own website&#x2F;infra? reply p4bl0 11 hours agoparentIf you have your own infrastructure to host all of these services then you&#x27;re probably not the target audience. It&#x27;s ok, it&#x27;s my case too.But you have to admit that $20&#x2F;year is quite cheap for all of what is provided here, without having to manage it all yourself, and with a \"no trackers no bullshit\" way of doing things.It&#x27;s really the kind of services I don&#x27;t need but would almost like to need! The last time I had this feeling was about Neocities :). reply crawsome 7 hours agorootparentGithub pages is free. A .info domain is $5&#x2F;year.That&#x27;s already more than half the features you get with this, and you get to be on the actual internet, not some dude&#x27;s silo.As the post&#x27;s age goes on, I see more criticism, and less positive reactions. reply monkeywork 5 hours agorootparent>That&#x27;s already more than half the features you get with this, and you get to be on the actual internet, not some dude&#x27;s silo.\"the actual internet\" ?? reply sedatk 1 hour agorootparent> \"the actual internet\" ??Microsoft&#x27;s silo, they mean. reply rfrey 5 hours agorootparentprevHow is hosting your website using a Microsoft silo more \"on the internet\" than using this? reply slalomskiing 5 hours agorootparentprevIt’s just a fun project why are you taking it so serious reply erxam 11 hours agoparentprevEven without taking into account the time investment in maintaining your own infra, it compares favorably with everything else. Even the most dirt-cheap VPS is a few bucks more expensive on a yearly basis by itself, and you still have to buy domains and similar.Running your own infra only really works out if you either have access to great hardware for super-cheap or WANT the experience from setting everything up. reply cipheredStones 10 hours agorootparentIt consistently surprises me how much software engineers devalue the effort of software engineering when it comes to their personal lives.If you&#x27;re a SWE in an English-speaking country, you almost certainly make $20 post-tax for at most one hour of work - 30m at SV salaries, as little as 15m if you&#x27;re at a FAANG-ish company. Is it conceivable that you would spend less than an hour a year maintaining something like this if you were to do it yourself? I don&#x27;t think so.Most people can&#x27;t earn money in increments of one additional hour, of course, but it still sounds strange to hear people say \"why should I spend [the amount of money I earn in half an hour] per year when I could just do it myself [with an amount of professional effort I would expect to be paid 20x as much for]?\" reply akho 2 hours agorootparent> Is it conceivable that you would spend less than an hour a year maintaining something like this if you were to do it yourself? I don&#x27;t think so.Is it conceivable that that you would spend much more than an hour maintaining this? Including making your stuff fit the mold, working around the limitations, and, inevitably, moving your stuff to a new service when this one fails, as they do?Also: a VPS replaces quite a few of these services. Maintenance beyond initial setup and occasional update is rarely needed if you are the only user. People tend to overestimate these things. reply crims0n 6 hours agorootparentprevI get where you are coming from, but I think the answer for a lot of us is... for the experience. reply cipheredStones 6 hours agorootparentThat&#x27;s a perfectly valid motivation, but if it&#x27;s really what someone is going for, I expect to hear an objection that sounds something like \"oh, that&#x27;s cool! but I&#x27;d rather try out doing it myself\" rather than the faintly contemptuous \"why is this worth $X when I could do it myself\". reply jodrellblank 9 hours agorootparentprev> \"Even the most dirt-cheap VPS is a few bucks more expensive on a yearly basis by itself\"Not if you get a Black Friday special; here[1] was $14.95&#x2F;year for 40GB SSD, 1GB RAM, 1TB monthly bandwidth, 1CPU core.RackNerd were offering $10.28&#x2F;year[2] for 10GB SSD storage, 768MB RAM.Hudson Valley offered $8&#x2F;year[3] for 10GB SSD and 512MB RAM[1] https:&#x2F;&#x2F;lowendtalk.com&#x2F;discussion&#x2F;190984&#x2F;from-14-95-yr-10-gb...[2] https:&#x2F;&#x2F;lowendbox.com&#x2F;best-cheap-vps-hosting-updated-2020&#x2F; (sold out)[3] https:&#x2F;&#x2F;lowendbox.com&#x2F;blog&#x2F;are-you-serious-hudson-valley-hos... reply tredre3 9 hours agorootparentSo to beat omg.lol&#x27;s price you have to hunt for a bargain, then hope the price doesn&#x27;t double in the following year?Oh, and you also need to own a domain already, otherwise it&#x27;s an extra 10-20 bucks per year. reply jodrellblank 5 hours agorootparentNo, you can get a free Unix account on sdf.org with web hosting and email if you want to build for yourself the kind of thing omg.lol does and don&#x27;t want a VPS. It&#x27;s just \"Even the most dirt-cheap VPS is a few bucks more [than $20&#x2F;year]\" is outdated, they&#x27;re available less than half that price and likely only getting cheaper in future. If you really want, you can risk things like the Oracle Cloud Free Tier. If budget is what you want or need, then \"hunting\" (visiting Lowendbox.com) is something you are probably willing to do.omg.lol gives a subdomain rather than a domain, right? So do free dynamic DNS providers like noip.com or dyndns.org (not sure if they still do free ones). If you want to register a domain, you also have outdated pricing, if you want cheap don&#x27;t go for a popular TLD; .de is $4&#x2F;year after the first year at Porkbun.com, .ovh is £2.99&#x2F;year after the first year at OVH.com, internet people say .ru is available for $1&#x2F;year. reply Cyberdog 9 hours agorootparentprevAs with many other things, I&#x27;d advise against picking a VPS plan based on price alone.I&#x27;ve found Vultr to be both affordable and of consistent quality for my modest needs (personal and business web hosting plus IRC bouncing). I pay about $5&#x2F;mo or $60&#x2F;year. reply jodrellblank 4 hours agorootparentThat&#x27;s fine, but the complaint was that a VPS is \"a few dollars more [than $20&#x2F;year]\" as if that was an objectionable amount&#x2F;increase. In that case, money is the main decider and $60 is much worse, and $8 is much better. People fighting for \"a few dollars\" a year are likely to be expecting (or unhappily tolerate) lower quality.I&#x27;ve had pretty good experiences of Linux VPSs for around $20&#x2F;year from several companies. reply airstrike 11 hours agoparentprevSo that you don&#x27;t have to worry about outages, updates, bugfixes, certs, permissions, vulnerabilities, ... like you do on your own website&#x2F;infra? reply akho 2 hours agorootparentIt&#x27;s the same infrastructure, with the same outages.The other points are something for the developers of your software distribution to worry about, same as if you buy a packaged service. reply dsr_ 10 hours agoparentprevYou are not the target audience. reply james_pm 11 hours agoparentprevI happily pay $20&#x2F;year so I don&#x27;t need to worry about it. Not everyone can or wants to run their own infra. reply rglullis 8 hours agoprevI do not want to hijack the thread, but I can&#x27;t help but look at this and think at how many things I seem to have gotten wrong with communick.Both of them seem to have a similar purpose: to be a place to offer a bunch of services that can work as alternatives to the Big platforms, and to charge a modest but fair price for it. Everything else, I seem to have gotten wrong.I was convinced that issues of network effects could be mitigated by offering group packages (so that you could come and bring your friends along). Turns out that thinking was from my time working at phone companies who offer \"family and friends\" plans, which is not something that people do online. People might be online friends, but seldom they will care about sharing a package group.I thought that the people who would be geeky enough to want their own DNS would already have had their own domain, so it never occurred to me to add subdomain spaces.I thought that having separate packages for each service would let people pick whatever they want, but in the end it seems that making a single plan with a single price makes for a much more compelling product.Seeing omg.lol at the top of HN is amazing validation of the business model that I think needs to grow to help us get rid of Big Tech, but holy shit do I need help with product and biz development. reply quickthrower2 8 hours agoparentI think this stuff isn&#x27;t the easiest discover. \"Do your market research\" excludes what people might buy if presented to them. Plus the see it 7 times to buy effect. I am tempted to buy this, partly having seen on HN before, and partly for one feature - the DNS. In my case it would stop me buying domain names for toy projects, just anotherllmthing.myname.omg.lol. The silly TLD is sort of a bonus, it forces me to show it as an MVP!. Although this scratch is somewhat itched for free by Netlify and Vercel, so... reply graypegg 12 hours agoprevHey this is great! While I don&#x27;t know if it&#x27;s for me, I know tons of folks that will love this. Good find! The only thing that I think is missing is a onboarding tool to create an account from another existing mastodon instance rather than by buying a domain and getting a new masto account via that process, call it forklift.omg.lol or something. :) reply kibwen 12 hours agoprevThis is exactly what I&#x27;ve been thinking about making recently as a response to the enshittification of the web: a single site that just collects a small number of useful, simple web apps that I could share with other people who are tired of being perversely monetized by ads and VCs. Utterly brilliant, thanks for sharing! reply ghewgill 12 hours agoparentThere are various similar communities, which don&#x27;t have to compete with one another because the internet is a big place. Two that jump to mind are https:&#x2F;&#x2F;tildeverse.org and https:&#x2F;&#x2F;disroot.org. reply lannisterstark 12 hours agoparentprevI just self host stuff on my domain and link them to a Flame dashboard for family and friends.https:&#x2F;&#x2F;github.com&#x2F;pawelmalak&#x2F;flameDashboard is only accessible by my wireguard network, Which they can turn the LAN mode on on, so it doesn&#x27;t route all their traffic, just to the local domain. reply lopis 12 hours agoparentprevDo it. The more the merrier. reply unshavedyak 12 hours agoparentprevYou’ve got me thinking the same thing. Omg.lol seems as interesting as it is enticing me to build a similar thing for fun. reply 53 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their experience with Twitter and the shutdown of third-party apps by Elon Musk, highlighting the impact on the community.",
      "They accidentally joined Mastodon, a unique service called omg.lol, offering various features like email forwarding, web page creation, blogging platform, pastebin, and image hosting.",
      "The author appreciates the sense of community and active involvement of the founder, Adam Newbold, while promoting the idea of creating personal websites. The summary ends with a mention of the Twitterrific app's contribution to Twitter's mascot, the blue bird."
    ],
    "commentSummary": [
      "Omg.lol is gaining popularity as an online community, but there are concerns about its privacy policy and compliance with European privacy laws.",
      "The conversation touches on various topics including the decline of creative online communities, the changing perception of freedom of speech, and the importance of archiving online content.",
      "Discussions also revolve around the challenges of decentralized platforms like Mastodon, the future of social media platforms like Twitter, and the impact of PHP creator Taylor Otwell. Additionally, conversations explore storing media in shared storage systems and the affordability of VPS plans for web hosting."
    ],
    "points": 578,
    "commentCount": 253,
    "retryCount": 0,
    "time": 1702240002
  },
  {
    "id": 38591584,
    "title": "Emacs Users Take Matters into Their Own Hands",
    "originLink": "https://eshelyaron.com/posts/2023-12-10-bad-news.html",
    "originBody": "Bad NEWS, Emacs Troubles registered in Emacs 30 Created on [2023-12-10], last updated [2023-12-10] NOTE: the following contains several hyperboles that reflect my emotions, and should probably be taken with a grain of salt, or not at all. While I am intentionally expressing criticism, I most sincerely hope not to offend anyone. The Emacs master branch is broken—for good, it seems. Emacs maintainers accepted a heavy-handed, harmful change, disregarding concerns voiced by multiple users and developers. I’ve created a fixed, and improved, Emacs fork. I’ll be using and developing this fork, and you’re welcome to join. Let’s take a step back. What is the Emacs master branch? That’s essentially the development version of Emacs, and what will soon become Emacs version 30. Many Emacs hackers and enthusiasts track the master branch to enjoy all of the latest developments and improvements. Thanks to the tireless work of the Emacs maintainers in scrutinizing incoming patches, the Emacs master branch has been very stable in recent years. But a few weeks ago, Emacs master users got a very unpleasant surprise. commit 589e6ae1fb983bfba42f20906773555037246e45 Author: Thierry Volpiatto Date: Sun Nov 19 20:42:56 2023 +0100 Improve register-preview (bug#66394) A minibuffer is used now instead of read-key. ... This commit crippled all user interaction with Emacs registers, turning commands such as C-x r s, once smooth and frictionless, into a cumbersome and painful mess. Concretely, instead of just typing the key for the register you want to operate on, you now get a fully blown minibuffer for inserting a single key. This is nonsensical in various ways, but most staggering is probably the fact that you now need to confirm your register selection by pressing another RET, effectively doubling the work you have to do for the simplest task of specifying a single character. Emacs registers (used to) provide a perfect UX. Silky smooth, really. But no more, not in GNU Emacs 30. It’s gone, without as much as a NEWS entry to inform the unwary user about this regression. Surely, it’s not too late to revert one bad change, right? That’s what I thought, unfortunately the Emacs maintainers seem bent on dismissing the community’s outcry in an unwarranted and misguided attempt to save face. In fact, the core problems with this change were brought up already in the first technical comment regarding this patch proposal, from Michael Heerdegen: If your version is accepted, I would vote for an option to get the old behavior back. Your intended behavior is safer but requires more keys (at least confirmation with RET). Some people might prefer the old way. Well, duh! If you’re gonna change long standing Emacs behavior, it better be optional and backward compatible, people have been using this thing for decades, after all. Hmm apparently, the patch author, Thierry, doesn’t see it that way: There is only RET as additional key and it is a good thing IMO as it let the time to user to see what he is doing. This rather arrogant statement, and the approach that underlies it, led us to where we are today. After some further discussion, Eli Zaretskii applied this patch to Emacs master. I’m not sure he understood the breaking nature of this change when he did that, which I think might be even more disconcerting. Soon after, Bastien Guerry chimed in, saying: I use registers ~100 times a day, so enhancements here are very welcome, thanks! I wonder about this [change], though. It badly hinders my usual flow, where I do remember what registers I use and like to store new ones quickly. Sounds like a clear and honest testimony from an esteemed community member. But wait, no, Thierry doesn’t think so, he thinks it’s all for the better and his change is all right. At this point I got somewhat involved, and seconded Bastien’s request to restore the previous, perfectly good behavior, at least optionally. Thierry wasn’t willing to fix this damage, so Eli asked me to help out: So maybe a better way forward is for someone, perhaps you Eshel, to add whatever is needed to provide optionally the previous behavior? Would you like to work on that? Easy enough. I crafted and posted a couple of patches that add some bells and whistles from Thierry’s patch in an optional and compatible manner. It’s really not that hard to make no harm in this case. But my work was disregarded just as well, sadly. Thierry didn’t like how I reverted his change to use the minibuffer for reading a single key, which is exactly the root cause of the problems I tried to fix—and in fact I did fix it, just not in Emacs master, but in my new fork. Since then, another bug report came in from an Emacs master branch user that suffered from one of the consequences of this change (a specific regression that I spelled out days before, but was ignored, for some reason), and several users reached out to the Emacs development in request to restore the previous behavior in an ongoing thread titled “Please, Restore Previous Behavior for jump-to-register”. Astonishingly, the maintainers seem insistent not to budge, and Emacs 30 will thus likely suck. I find the willingness of the Emacs maintainers to entertain this bad behavior (of both a rouge developer and of Emacs itself) at the expense of user experience, unacceptable. This demonstrates clear disrespect for Emacs user preferences, and indeed their freedom. For me, this freedom is the number one reason for using Emacs, so I obviously won’t use an Emacs that forces on me weird breaking changes. For that reason I’ve created a new Emacs branch, the “main” branch, which was born from the master branch before this registers fiasco. If these changes will be reverted before Emacs 30, I’ll surely consider switching back to building Emacs from the upstream master branch. But in the meantime, and for the foreseeable future, I’ll be keeping my main branch up to date by cherry picking (only) good changes from the master branch. And of course, I also improve it with developments of my own. If you’re interested, feel free to checkout my main branch from here: git clone git://git.eshelyaron.com/emacs.git # or git clone https://git.sr.ht/~eshel/emacs I’ll be happy to coordinate and incorporate other’s work. Let me know if you have any suggestions, thoughts, or nice harmless patches that I should try out. © 2023 Eshel Yaron",
    "commentLink": "https://news.ycombinator.com/item?id=38591584",
    "commentBody": "Bad NEWS, EmacsHacker NewspastloginBad NEWS, Emacs (eshelyaron.com) 337 points by amiralul 19 hours ago| hidepastfavorite210 comments tarsius 17 hours agoWhen a breaking change is made on Emacs&#x27; development branch, whether intentionally or not, and some users voice concerns about that change, then the change isn&#x27;t reverted the minute those concerns are raised. The pros and cons are discussed, different solutions are implemented and improved, and finally a compromise is found.Users raising their concern started three days ago. That&#x27;s not enough for this process to have concluded already.Here&#x27;s a recent message by Eli (and the message he is responding to). > I&#x27;m hoping the old behavior stays the default and the new behaviour > is what users can opt in with a variable. > > If that is what normally happens for much less disruptive changes, > why isn&#x27;t it happening for this deep impacting one? Because the original discussion of these changes, between two people who were interested and involved, indicated that the new behavior makes much more sense than the old one. Now, that others chimed in with the opposite views, we are still discussing what should be the behavior, and once that is concluded, we can talk about the defaults.So I think this has been blown way out of proportion. IMO there are some serious issues in how Emacs is developed. I don&#x27;t have a solution but I think that us users&#x2F;package-maintainers thinking to ourselves \"gee there sure are a lot of stubborn people on emacs-devel, what&#x27;s wrong with them?\" and then the second a change is made that we strongly disagree with, we start behaving like the world is ending, that might be a problem. This is how maintainers get defensive (you might have noticed that in the projects that you maintain). reply yaantc 17 hours agoparentThank you @tarsius! Couldn&#x27;t have said it better.To add, I have issues with the attitude shown by the blog author.If you use the development branch, you can&#x27;t raise hell when there&#x27;s a breaking change: it&#x27;s to be expected!Then it&#x27;s fine to disagree on some change and discuss this. I read the email thread, and I do not see \"arrogance\". Just strong disagreement. So yes, converging will take a bit of time... Calling publicly someone \"arrogant\" for not folding back to your view, and trying to raise the crowd (a good part of whom won&#x27;t read the thread to make their own opinion) looks like bullying to me.Saying that his patch to make the change optional has been disregarded, when it was rejected because it not only made the change optional (that would have been OK, and a patch for this asked for) but removed other changes is not honest.Lastly, pointing out one person to blame when the whole discussion is done with the Emacs maintainers in the loop is also a no-go in my book.As a close to 30 years Emacs user, thank you to all its contributors! (and to Thierry, as long time Helm user) May their skin by thick, it&#x27;s unfortunately sometimes needed :-P reply Y_Y 16 hours agorootparent> Calling publicly someone \"arrogant\" for not folding back to your view, and trying to raise the crowd (a good part of whom won&#x27;t read the thread to make their own opinion) looks like bullying to me.Isn&#x27;t this a bit of a loaded take too? I&#x27;m sure the author wouldn&#x27;t agree that what they wanted was for Thierry to \"fold back\". I agree with your criticisms here in direction but not in magnitude, in fact it appears to me like you&#x27;re comitting the same sin of misrepresenting your opponent to enhance your position. reply yaantc 15 hours agorootparentThat could be, the author may not have expected nor intended the exposition of an HN post. If so, my apologies. I still find the article unfair and biased in its presentation, compared to the email list discussion. It&#x27;s unfortunate, because if one look the comments here it&#x27;s clear many take the article at face value and haven&#x27;t read the list thread. reply kazinator 9 hours agorootparentprev> If you use the development branch, you can&#x27;t raise hell when there&#x27;s a breaking change: it&#x27;s to be expected!If nobody raises hell on a development branch, the change will have a way of making it to the master branch. reply disruptiveink 16 hours agorootparentprevEh. I don&#x27;t think that argument holds water, unfortunately. Too often I&#x27;ve seen the \"it&#x27;s just a beta&#x2F;unstable version, it&#x27;s not finished, you can&#x27;t raise issues like this!\" attitude as an issue to shut down any form of discussion, or worse, to avoid having to think about the consequences of some action.Of course, nearly 100% of the times the behaviour people were complaining about will be part of the stable release. There is no magical moment where things just magically get sorted pre-release unless people voice feedback, especially if it&#x27;s intended behaviour, as it&#x27;s very much the case here. So call me jaded, when someone says \"it&#x27;s just the main branch, don&#x27;t complain!\" I just hear \"I will do whatever I want and when the new release rolls in everyone will just have to deal with it.\" Which is totally fair if that&#x27;s how you want to run your project, just don&#x27;t pretend otherwise.An extra one, which isn&#x27;t the case here but makes me roll my eyes every times it happens is the outrage of \"how dare you raise an issue that was caused by a new pre-release iOS&#x2F;MacOS version and very much seems to be an intended change in the OS behaviour, we don&#x27;t support that!!\", only for it to turn into the usual scramble \"oh no, our software is broken on the new iPhones, who could have forseen this!!?\" hours after the release version starts hitting the masses. reply yaantc 15 hours agorootparentThere&#x27;s a misunderstanding here: I&#x27;m not saying \"don&#x27;t complain\". Complaining in itself is fine, and others are complaining on this topic in a way that looks OK to me (and will probably be more efficient too). It&#x27;s the nature of this specific article complaint that I have a problem with.I agree with @tarsius on this: just give the discussion (and patches) some more time, and it&#x27;s likely to end just OK from past experience. reply achikin 12 hours agorootparentI wonder why wasn’t this patch given time before merging it? Isn’t that the whole purpose of patches and merge process? reply uranusjr 10 hours agorootparentBecause merging into main better exposes the proposal to a more diverse crowd and attracts needed feedback. Especially when you’re managing multiple proposed features, it’s not viable for a mass of users to check out and test those from their individual branches. Without merging fast, you can only gather opinions from people actively reviewing patches, which are a far more minority group and likely to be biased. reply tsimionescu 3 hours agorootparentprevWhat kind of time? It spent around two months between the first proposal and being merged. Do you think that people would have trawled through the mailing lists and found this and given their reviews if only it had been given 1 more week?Ultimately, the consequences of a patch, especially one that changes UX, can only really be evaluated after the community starts using it. People using the master branch of Emacs are basically those who wish to work as the QA engineers of Emacs. End-users use release branches or even pre-packaged releases from their distro.So, the normal process for a UX change is to merge it to master in order to get comments on the impact. Based on comments, you can either revert, move behind a flag, etc. replyeduction 17 hours agoparentprev>Users raising their concern started three days ago. That&#x27;s not enough for this process to have concluded already… So I think this has been blown way out of proportion.This reads as contradictory to me. On the one hand you’re saying that user response is a key input to making a final decision. Then you’re criticizing a negative user response as blowing things out of proportion. But if the users didn’t react, the original thing they are upset about probably would have happened, per your own description of the process.I saw a very similar process unfold with clojure-mode, where rms floated the idea of rewriting it and taking control of the name from the original longtime author, a Clojure community member. The reason this didn’t happen is people got upset and posted to the list - but those very people who caused it not to happen were told they were blowing things out of proportion. So that doesn’t seem like a very meaningful criticism. reply aeturnum 16 hours agorootparentI think the thing being criticized is the tenor of the pushback. The suggestion that, because the blog author (Eshel Yaron)&#x27;s patch wasn&#x27;t accepted and the change is still currently in, that the maintainers are \"insistent not to budge\" and that this \"demonstrates clear disrespect for Emacs user preferences, and indeed their freedom.\"The Yaron&#x27;s attitude seems to suggest that there&#x27;s an easy right answer here and that it&#x27;s the thing he wants (and Emacs does now). A lot of his upset seems not to be about the idea of an option to support the new behavior (which he wrote a patch to support!), but about the attitude with which this was introduced. In return, he&#x27;s coming at this issue with an attitude that seemed fit to match what he thinks the maintainers are bringing.So that&#x27;s why it&#x27;s important to ask if this blog post has misunderstood the arc of changes to Emacs. If the pushback will probably result in the current default staying in the editor. Because assuming the old behavior is best, even though some maintainers like the new behavior, is just as high handed as forcing the new behavior. reply achikin 12 hours agorootparentprevHere is the recap of someone is interested https:&#x2F;&#x2F;metaredux.com&#x2F;posts&#x2F;2023&#x2F;09&#x2F;09&#x2F;clojure-support-in-em... reply jbluepolarbear 17 hours agoparentprevThat’s a bad process. The review of the feature should start before it’s merged into main branch, not after. It totally reasonable for people to be upset with a breaking feature in main branch without discussing it with the community at large. Changes merged in like this are how long standing tool lose community support. reply tsimionescu 17 hours agorootparentThe review of the feature was happening in public on the mailing list. All those who contributed to that review had their concerns addressed, and the change was merged into the main branch (which is the development branch of Emacs). Only afterwards did others complain. reply jbluepolarbear 17 hours agorootparentThe article says otherwise. It says that many concerns were disregarded during that time. reply yaantc 16 hours agorootparentThe article is just his author view. Please read the emacs mailing list threads to get the full picture.GP is correct, and this is quite normal: some people using Emacs master will not follow all the mailing list discussions and commits. So they will notice a change only after it is merged. Nothing wrong with this, and nothing wrong with being unhappy about such a change. What&#x27;s wrong in my book is the nature of the reaction show in this article (see my comment in reply to @tarsius). reply kazinator 9 hours agorootparentThen there are people who don&#x27;t follow master.Some people only pick up releases, including test releases.Some people only work with final releases.All those people could find something suddenly not working well. reply Kalq 7 hours agorootparentIf you are this hypersensitive to change, it behooves you stay on a stable release and only upgrade after you&#x27;ve read the CHANGELOG and NEWS files. reply kazinator 5 hours agorootparentSomeone hypersensitive to changes track changes as early as possible, and complain quickly and loudly.Those who are not sensitive changes just use whatever Ubuntu (or whatever) provides and are unconcerned with the development. reply heyoni 16 hours agorootparentprevPlenty of comments here saying that the change did more than make the feature opt in which is why it was rejected. reply tsimionescu 16 hours agorootparentprevThat&#x27;s what the article says, but I checked the actual mailing lists. Any replies that asked for a change of this kind came in after the patch was finalized. Even this author&#x27;s own complaints. And, the author in particular didn&#x27;t understand, or seek to understand, why the patch was added in the first place. Instead, their proposed \"fix\" that they complain about in the article reverted all of the improvements the author and reviewers made. Upon being informed of this, they simply complained that those are useless changes:> Indeed, I only reimplemented the parts I saw as clearly beneficial. Most importantly, my patch improves stuff without breaking other stuff.> Perhaps you can explain your use case for the rest of the changes, and if there&#x27;s a good and compatible way to add them I&#x27;ll be happy to look into it at some point.> > - No filtering. > > - No navigation. > > - No default registers. > > - No possibility to configure a new command added to register.> If you could elaborate about these bullets, and explain their use, that&#x27;d be great.I think it&#x27;s quite obvious that engaging further with someone who came in with this attitude after a review was finalized (after ~5-10 rounds of feedback, I should point ou) and a patch applied did not seem worth it. reply Y_Y 15 hours agorootparentPerhaps you could mention what these improvements are or give a link to the messages you&#x27;re getting this information from? reply gray_-_wolf 14 hours agorootparentprev> into the main branchinto the master branch reply kazinator 9 hours agorootparentprevOn the job, I wouldn&#x27;t go implementing whatever pops into my head without input from my manager and product management.Even if the change could easily pass a technical review and get merged.First you have to determine: do we need this for the product? If this change is made, does it break user workflows? What difficulties will users have if they pick up this change? reply tsimionescu 3 hours agorootparentEmacs is a community-driven project. And the author of this patch identified several issues with the feature, created a fix, proposed it for review, debated the impact with other devs on the mailing list for weeks, addressed all concerns raised, and then finally other more senior devs merged the change to master, where all new unstable changes go.Then, people who agree to test out all changes to Emacs by using the head of master found, basically the QA department of Emacs, came back with feedback that they like some of the improvements, but that they don&#x27;t like one particular aspect (the extra RET). Thierry started addressing them, and a setting to disable the feature was added - but the initial version missed the mark. The article author also was part of this process, and even proposed a patch, but their attitude made others ignore it after the initial review (their patch reverted all changes and only implemented a tiny subset of the original, behind the discussed flag). reply Ferret7446 11 hours agoparentprevI disagree. The Emacs community has, at least historically, strongly valued backward compatibility. Breaking existing users is just about the worst thing you could do in Emacs.If we aren&#x27;t strongly considering reverting a breaking change immediately, that signals that the Emacs devs no longer value backward compatibility as highly, which means Emacs is abandoning many of its existing users.https:&#x2F;&#x2F;www.murilopereira.com&#x2F;the-values-of-emacs-the-neovim...Core values matter. The Emacs maintainers did something that violated a core value, and the community is rightfully offended. reply jeremyjh 17 hours agoparentprev> So I think this has been blown way out of proportion.If this has happened as described in the OP then I’m worried about the health of emacs. There was controversy before it was merged to master, and they merged it anyway. Breaking user workflows and not even making it optional, much less opt-in comes across as a callous disregard for user experience. reply nanny 14 hours agorootparent>If this has happened as described in the OPIt&#x27;s not. The article is bad and wrong. It preemptively tries to dodge such accusations, but most of the hyperbole and \"emotions\" are just downright lies.btw the person you responded to is the author of magit. I think his opinion should be weighted more heavily that the author of this article. reply rightbyte 16 hours agorootparentprevYe once the devs go user hostile it is all down hill from there.Especially in a project like Emacs, where every efficinado has their own really custom config.I mean, reading the Emacs docs it is written in a way that make you feel like color display and a mouse is cutting edge hardware and optional.It is a very conservative project ... reply tsimionescu 3 hours agorootparentBut that is not what happened. The author of this article is misrepresenting the attitude of everyone involved. There was a single reviewer who noticed the patch and started engaging with it, Michael, and while he originally raised the concern quoted in the article, he discussed with the patch author and they finally got to a common ground, he suggested several other improvements that the patch author worked on, and finally he is the one who approved the patch to move further - in a process that took almost two months.This is not to say that both the patch author and the reviewer didn&#x27;t misjudge the importance of a workflow change. But no one was being hostile or dismissive - they discussed the issue, and concluded (probably wrongly) that the new behavior is overall better and wouldn&#x27;t significantly impact anyone negatively.Then, two days after it became available, other users started seeing it and raised bugs on the behavior, which the patch contributor started addressing. The author of this article was the first one to raise such an issue, and it wasn&#x27;t initially clear how many others would agree with them. Still, the maintainers and the patch author agreed immediately that a flag to re-enable the old behavior would be a good thing, and asked if the article author would like to contribute it (the patch author wanted a break from this feature that they&#x27;d worked for weeks on). The article author came back with a patch that reverted all of the changes made by the original patch except for one. When told about this, they said that they only kept the changes that had any value, and that they&#x27;d require proof the other changes are also useful before going further - to which they didn&#x27;t receive any more replies, for obvious attitude reasons. reply NeutralForest 17 hours agoparentprevI&#x27;m fully with you here; let the process happen and the discussion between contributors and maintainers happen before talking about \"BAD NEWS\". reply bachmeier 17 hours agoparentprevIt would make a lot more sense to have a discussion before breaking long-used behavior. I&#x27;d use Emacs a lot more, but you honestly can&#x27;t trust it. The developers don&#x27;t see a problem with breaking things users rely on. reply rrix2 17 hours agorootparentThis is a pre-release commit not in any released version. reply Beldin 16 hours agorootparentOh please. This is not someone&#x27;s first step in an experimental new feature in its own branch. This is a commit of a fully working non-trivial patch that alters long-standing UI to the master branch. You&#x27;re not supposed to merge in patches there on a whim. reply tsimionescu 16 hours agorootparentIt was not done on a whim. The author and the reviewers felt that the improvements outweighed the cost of the workflow change, which they explicitly discussed. That doesn&#x27;t mean they got it right, but it&#x27;s completely unfair to characterize it as \"on a whim\". Of course, you would not know this from the article - it&#x27;s misrepresenting the discussion as it happened. In particular, while it correctly points out that a reviewer raised the issue and the patch author brushed it away somewhat, it fails to mention that the reviewer actually agreed with them afterwards.I think that the core problem is the article author saw a change that broke their workflow and didn&#x27;t investigate any further for why it was made. They simply assumed they knew better and got annoyed that others saw some value in the original change. The very way it is presented in the article - as a change to add a confirmation for register overwrites - is a misunderstanding. The actual purpose of the change was to make C-g, the Emacs \"cancel\" key, work with register commands. The RET for confirmation was a side-effect, one which the author felt could actually have some value in itself. reply timmytokyo 14 hours agorootparent>The RET for confirmation was a side-effect, one which the author felt could actually have some value in itself.Regardless of the reasons for the change, a single author was permitted to commit a change that broke the workflows of potentially thousands of users. And there was no way users could work around it to maintain their previous workflows. That sounds arrogant and annoying. reply tsimionescu 2 hours agorootparentThat is how all changes work, especially for an editor like Emacs where the internal APIs are public. As far as I understand, the accepted workflow for Emacs dev is:1. Raise an issue through the mailing list.2. Propose a patch on the mailing list3. Maintainers send review comments about the patch.4. The patch gets rejected or merged to master5. The few people using head of master give feedback about the change6. The change gets modified or reverted7. A release branch is created from head of master8. The bigger group of people who use un-released Emacs release branches start giving their feedback9. More fixes are made on the release branch. Some features get reverted entirely.10. A new Emacs release is decided and published11. The larger emacs community starts using the brand new release and filing bugs and giving feedback12. Release patches are sometimes issued, and other feedback is taken into consideration for the next release13. After some minor releases, major Linux distributions start packaging the Emacs release to include in their official repos14. Only now do the majority of Emacs users actually start using the new release, with all patches and feedback addressed.In the case of this change, it has just made it at step 5 three days ago, and it is now in step 6. There is a LONG way to go before it makes it to any significant number of Emacs users. I actually doubt that there are thousands of users using head of master at all - out of which only a subset probably use registers in any way to begin with.Personally, I&#x27;ve been using Emacs for 5+ years in my day to day job, and I am at a point where I build my own Emacs binaries. Even so, I&#x27;m only doing this from the latest release branch, and only close to a release, I don&#x27;t have the energy to deal with the potentially broken head of a new release, even less so head of master. reply smaudet 15 hours agorootparentprevWhat I read (did not verify) was that two users discussed and decided on the feature. That&#x27;s hardly a well deliberated change.I think for user facing features it makes more sense to provide new behaviors as opt-in, and go through a longer RFC period. Only if and when the new behavior is widely used and popular, then you flip the switch to default. reply _a_a_a_ 15 hours agorootparentCan you link to what you read please reply tptacek 16 hours agorootparentprevDo you do a lot of emacs development? I certainly don&#x27;t. I&#x27;m trying to piece together who here is talking about the norms of the Emacs development team and who&#x27;s talking about their opinions of how software engineering should work everywhere. reply ChrisMarshallNY 8 hours agoparentprev> gee there sure are a lot of stubborn people on emacs-[%]I have been watching Emacs vs Vi wars for decades. People take this stuff seriously.In my case, I tend to use GUI editors, whenever possible (or Nano&#x2F;Pico, if absolutely forced to). I know, I know, I&#x27;m a wimp. Guilty as charged. reply kazinator 9 hours agoparentprevIn my organization (embedded development in VOIP area) we revert breaking changes immediately, as soon as the breakage is identified.Also, same in every company I&#x27;ve ever worked in that had any kind of process. reply mixedmath 19 hours agoprevI&#x27;ll summarize my understanding.A commit that changes how copying (actually \"registers\", which is a bit more general than copying) works in emacs was recently accepted. Now emacs opens up a minibuffer that shows what is happening, requiring one to accept the change by hitting enter or equivalent. The OP thinks this is a terrible, breaking change as it changes default behavior (and possibly without the possibility of easily configuring this away). Further, this happened without much discussion.Let&#x27;s state a vim analogy. If I want to copy the current line into my `d` register, I can type `\"dyy`. This proposal would do something like, after typing `\"dyy`, open up a scratch buffer that tells the user the text and the buffer, requiring a keystroke to close. This is terrible for people who understand registers (the typical vim user). But I acknowledge that sometimes I don&#x27;t copy the intended text into registers and have to try again. I also know many vim people who only yank from visual mode, which has a similar show-explicitly-what-is-being-copied behavior.The rest of this article is a description of how the OP tried to raise discussion, but the commit-author, Thierry, shot it down. Implicitly the rest of the emacs dev community is at fault too. reply ekidd 17 hours agoparentAlso, registers are a relatively advanced feature mostly used for rapid edits. Registers aren&#x27;t aimed at first-time users using a mouse. They&#x27;re aimed at high-speed typists doing complex things.I used Emacs for decades, and never really got into registers. Personally, I tended to use kill&yank for copying, and to use either multiple cursors or one-off keyboard macros for complex edits. But Emacs has tons of optional, advanced editing features for people who want to rely on muscle memory.Adding a confirmation keystroke here is a bit weird. It&#x27;s a bit like taking an electric piano and adding a confirmation pedal to confirm unusual chords. It just adds one more step to a rapid, complex input operation.But the other important thing to remember is that Emacs has excellent undo. You don&#x27;t need to ask users, \"Do you want to paste register &#x27;d&#x27; containing &#x27;...&#x27;?\", because you can just paste it, and let the user undo it if they chose the wrong register.So making a breaking change here is odd, and offering no way to disable it would make a lot of users upset.Emacs predates modern GUI conventions. It&#x27;s never going to be as familiar to new users as vscodium. So I think there&#x27;s a good argument for serving power users as well as possible. That isn&#x27;t to say that Emacs should never tweak the default config or add user-friendly features like the menu bar or visible selections. But it does suggest leaving things like registers mostly alone. reply tzs 13 hours agorootparent> But the other important thing to remember is that Emacs has excellent undo. You don&#x27;t need to ask users, \"Do you want to paste register &#x27;d&#x27; containing &#x27;...&#x27;?\", because you can just paste it, and let the user undo it if they chose the wrong register.With paste you can see what got pasted, so you&#x27;ve got a chance to realize it is not what you wanted.But how about for copy? If you meant to copy into register &#x27;r&#x27; but missed by a key and typed &#x27;t&#x27; would it be noticeable right away?I don&#x27;t use Emacs so don&#x27;t know how its undo works, but when I use named registers in Vim it is often to hold something that I&#x27;m not going to paste for quite a while. By the time I notice the error it would be annoying to undo all the way back to the mis-copy.If copying into the wrong register was common enough to need to be addressed, my first thought would be something like adding a status message pups up for a short time near the cursor that says something like \"Copied to &#x27;t&#x27;\". reply medstrom 14 hours agorootparentprevGreat points! If you don&#x27;t participate in emacs development, maybe it&#x27;s time to consider it. reply oefrha 18 hours agoparentprev> A commit that changes how copying (...) works in emacsTo people who don&#x27;t use Emacs, it should be made clear that standard copying (kill-ring-save, M-w by default) isn&#x27;t affected, only more advanced saving to registers is. Registers aren&#x27;t a superset of the clipboard (kill ring).Edit: In addition, those in this thread claiming that the change won’t be configurable must have no idea of Emacs’ customizability, namely the “settings” are for convenience only, you can switch out all the code if you want to. This is in the elisp part of Emacs, even if it lands without change (doubt it, after reading the actual mailing list thread rather than this one-sided tantrum), someone will have a package within minutes changing the behavior. No, you don’t need a fork for that, the forking here is performative at best. reply jmclnx 16 hours agoparentprevThat is my understanding, and the change for v30 would very much annoy me also. I know nothing about lisp but I save items to registers all the time.I hope if this change is not reverted I can find a macro snippet that would do that extract key for me. Already in Emacs to do simple things involves lots of keys. Some I have created macros to avoid the keys, but I am not expert enough to create one for this change. reply tsimionescu 16 hours agoparentprevI think this is a good summary of the article, but not a good summary of the problem if you look at the mailing list.This whole thing seems to have started because Thierry found a few problems with the way registers work, and wanted to address them.The most important flaw was that after you hit C-x r SPC (save-to-register), whatever key you hit next, you&#x27;d save the text into the register associated with that key. In particular, the universal Emacs cancel key, C-g, would not work here: instead, the text or position would be saved to a register called ^g. Similarly, if you accidentally hit \"jump-to-register\" or \"insert-register\", you couldn&#x27;t cancel with C-g (or with any other key), you&#x27;d be forced to select a register and it&#x27;s contents would be jumped to&#x2F; inserted (if any).Secondly, registers can hold text, a position, or nothing. jump-to-register only works if a register holds a position, while insert-register expects a text. Emacs includes a preview of registers which are non-empty when invoking these commands, but it doesn&#x27;t distinguish - it will show you a register that includes strings as an option for jump-to-register, even though it knows it won&#x27;t work.So, Thierry took the time to address all of these concerns, and to address other feedback about the code. The reviewers agreed that these are important changes even though they add some extra interaction, and that the breaking change (having to hit RET after selecting a normal registry, or having to use an extra key to save to a weird register like ^g) are worth the gains. After it was done, and compiled, it was added by the Emacs maintainers to the development branch.The author of this article came along, asked for a switch to revert between the new and the old behavior, and was asked for a patch. They provided a patch that reverted all of the changes I mentioned before (so, no way to cancel the register commands, no way to get contextual help about which registers contain text VS position, nothing) and instead implemented an entirely different feature (confirmation on overwriting a non-empty register based on a flag). Thierry installed the author&#x27;s new patch and gave this simple feedback, to which the author basically replied \"you don&#x27;t need all that\".Now, as more people started using the feature, the belief by Thierry and the original reviewers that the breaking change had minimal impact was proven wrong. Thierry started working on a new improvement to create a flag that keeps both all of his new work, but allows the previous workflow too (particularly, removing the extra RET, which was ultimately a side effect, not the main point). The patch missed the mark, but it is still being worked on.Overall, it seems the process is working quite well, and it is only the author of this article that is trying to bully his way into the discussion and ignore the context, with a very anti-hacker attitude of \"if it kinda works, we shouldn&#x27;t change it in any way\", which is very much against the spirit of Emacs. reply lelanthran 2 hours agorootparent> Thierry started working on a new improvement to create a flag that keeps both all of his new work, but allows the previous workflow too (particularly, removing the extra RET, which was ultimately a side effect, not the main point). The patch missed the mark, but it is still being worked on.I think that if the author of this rant did not come along and rant, there would be no \"but is still being worked on\" done. My reading is that the maintainer (Theirry) was happy with the extra RET.NOW it is getting worked on because of the dogged and emotional criticism by the rant&#x27;s author, prior to him writing his rant.If he didn&#x27;t make such an emotionally charged and disruptive arguments and patches, the rest of us poor Emacs users would have only found out about the broken workflow (extra RET) once the bug was fully baked in, requiring waiting for the next release in order to get a fix. reply tzs 12 hours agorootparentprev> The most important flaw was that after you hit C-x r SPC (save-to-register), whatever key you hit next, you&#x27;d save the text into the register associated with that key. In particular, the universal Emacs cancel key, C-g, would not work here: instead, the text or position would be saved to a register called ^gI&#x27;m not very familiar with Emacs, so possibly stupid questions incoming.If you start to type C-x r SPC t to save to register t, does C-g work right after the C-X and the r?If so is that because the C-x, r, and SPC are part of the command sequence whereas the t is just an argument used by the command, and the C-g handling is done in the command sequence processing?Emacs is pretty famous for its flexibility in letting users bind and unbind key sequences to commands. Could people who like the old behavior fairly easily effectively restore it by unbinding the handler for C-x r SPC, and then bind handlers for C-x r SPC a, C-x r SPC b, C-X r SPC c, and so on for all registers they want to use, with each of those handlers just copying to the appropriate register?That should let them use the same keystrokes they now use, and if my guess above about C-g handling is right also make C-g work to cancel after C-x r SPC. reply domq 1 hour agorootparentprevThis is basically the only reply that everyone on this thread should read. Thanks for your research. reply Y_Y 15 hours agorootparentprevMakes me wonder why they didn&#x27;t decree, \"no using a register called ^g, that&#x27;s a special character\" and add a check when specifying a register. I bet it would break fewer workflows. reply medstrom 14 hours agorootparentWell, exceptions for C-g shouldn&#x27;t have to be hardcoded everywhere, because that makes it harder to move the C-g behavior to some other key. If going this route, I&#x27;d first implement a list of \"keys with C-g-like behavior\" that can be given more members than just C-g, via a startup flag or some such.Then commands could just reference that list if needed. reply kazinator 8 hours agorootparentprev> The author of this article came along, ...You mean, \"luckily for the entire fucking Emacs community, one of the small number of people who pick up unreleased master commits came along and happens to be a user of registers ...\" reply tsimionescu 2 hours agorootparentActually multiple people have complained, some less hyperbolically than this author. And, unless you think the people who build their own Emacs from head of master are less likely to use more obscure features of Emacs, then this feedback was always going to come before anything that actually hurts the Emacs community made it even close to a release. reply mixedmath 16 hours agorootparentprevThis extra context is very informative, thanks! reply froh 16 hours agorootparentprevthank you for this down to the facts summary. reply CoastalCoder 18 hours agoparentprevAs a perpetually novice vim user, I actually would have expected \"dyy\" to mean Delete the current line.But that probably just proves my novice status. reply addicted 18 hours agorootparentYou’re missing the double quote before the ‘d’ which means you’re talking about a register. reply CoastalCoder 18 hours agorootparentAh, thanks. You&#x27;re right, my brain parsed the gp&#x27;s comment as simply \"dyy\". reply Schiphol 18 hours agorootparentJust for completion, `dd` deletes the current line in vim. I&#x27;m not sure what `dyy` should do, if anything. reply tmtvl 18 hours agorootparentMaybe it should remove the top item from the list of copied items? Assuming that vim has one of those (in Emacs parlance it&#x27;s called the kill ring). reply duskwuff 14 hours agorootparentprev> I&#x27;m not sure what `dyy` should do, if anything.Currently, nothing. `y` isn&#x27;t a defined motion. replywokwokwok 18 hours agoparentprev> Implicitly the rest of the emacs dev community is at fault too.?I can’t tell what this comment is supposed to mean.He disagrees; other people also felt similarly about it but, they were ignored.They say, if you don’t like it, why don’t you contribute? …but he did and that wasn’t acceptable either, so he’s forked it.Do you feel it’s out of order to walk your own path with free software if you disagree with the maintainers? Isn’t that the whole point of GNU? reply Zelphyr 19 hours agoparentprevI just learned a new feature of Vim. Thank you! reply motoboi 19 hours agorootparentvimtutor is absolutely worth the time reply xhevahir 18 hours agorootparentI don&#x27;t think that feature is covered in vimtutor. As far as I can tell vimtutor mentions registers only once in passing and buffers not at all. reply davidkunz 17 hours agoparentprev> yank from visual mode, which has a similar show-explicitly-what-is-being-copied behavior.Even better: vim.highlight.on_yank reply yayitswei 18 hours agoparentprevFortunately I use evil mode so my workflow is unaffected, for now. reply kreetx 16 hours agorootparentThe way I read it, both methods will continue to be available, so I guess it&#x27;s a question of defaults. (So it doesn&#x27;t really affect anyone&#x27;s workflow) reply GavinAnderegg 18 hours agoprevReviewing the mailing list threads on this, it looks like there will be an option to revert this behaviour: https:&#x2F;&#x2F;yhetil.org&#x2F;emacs&#x2F;87h6kr9817.fsf@posteo.net&#x2F;#tIt seems like this option was mentioned before the original post was published, but perhaps the author didn&#x27;t see this? I assume this would fix the issue, but I may be missing something.--EDIT: it looks like this will still require a RETURN keypress, based on the reply from ginko below. reply addicted 17 hours agoparentFrom that thread it does appear that register-use-previews seems to be the toggle for this behavior (although it mentions a bug where even if it’s set to never a certain workflow is prompting a confirmation). That would resolve any concerns I’d imagine if it’s truly an option? reply ginko 18 hours agoparentprevSeems like that still needs an extra RET to confirm register overwrites:https:&#x2F;&#x2F;yhetil.org&#x2F;emacs&#x2F;87a5qi1vui.fsf@posteo.net&#x2F; reply GavinAnderegg 17 hours agorootparentAha! Ok, that makes sense. I had figured the original poster would have seen this fly by, so I was confused as to why it was still an issue. Thanks! reply nanny 13 hours agorootparentThierry also already offered to add another option to fully revert to the old behavior: https:&#x2F;&#x2F;yhetil.org&#x2F;emacs&#x2F;874jgq0zdh.fsf@posteo.net&#x2F;Please also take note that this (completely reasonable email) is some of the so-called \"bad behavior\" and \"arrogance\" that the author of the article is purportedly describing. reply krmboya 18 hours agoprevMuscle memory should be elevated to a first class concern when it comes to emacs. reply atticora 18 hours agoparentOnce a new version of my favorite file manager came out with remapped key bindings. I can&#x27;t even remember the name of it now. But my own anger was memorable as totally out of proportion even when I was feeling it. My investment in muscle memory had been trashed! I thought several evil thoughts before calming down. So while it sounds trivial I can understand why the reaction to the crime of Betrayal of Muscle Memory has escalated to \"fork you\". reply bradleyjg 18 hours agoparentprevNot just emacs. I don’t care about emacs at all. What makes this story important is pointing out the arrogance of devs that love to “improve” things without any regard for people’s settled expectations. That applies as much to google maps or iOS as it does to emacs. reply layer8 18 hours agoparentprevMore generally, any frequently used software should be made “muscle-memorizable”, and then should not break it in later versions without good reasons. Muscle memory has the benefit of enabling “blind” and semi-asynchronous operation of the software, without constant visual confirmation for each micro-interaction. There’s unfortunately a trend in desktop software to make operation by keyboard ever more cumbersome or outright impossible. reply runevault 18 hours agoparentprevAt a minimum switching anything that breaks muscle memory should be a toggle that defaults to \"keep my config how it used to be\" reply accelbred 16 hours agoprevThis is a bad take by the author; users wanting stability should be on the release branch, or pin commits on master. It should be expected that the development branch is used to develop in-progress features. Sometimes these take a while to hammer out. If you follow master, you will have frequent breaking changes and should be comfortable reverting to a previous commit when theres something breaking your workflow. Better yet, stay on a release. reply mtraven 17 hours agoprevSeriously, WTF?The whole point of Emacs is that it is a radically customizable platform, and if you don&#x27;t like the behavior of some feature you can modify it yourself with a few lines of Lisp. Forking the whole project over a change to one obscure feature makes zero sense.Status: Emacs user since it was implemented as TECO macros (1981 or so), but I don&#x27;t use registers. reply cratermoon 17 hours agoparentYou must have missed the part where this change does not include the ability to revert to the old behavior via any settings. reply 3836293648 12 hours agorootparentNo, they didn&#x27;t. It&#x27;s Emacs. Every thing is dynamically scoped lisp. It&#x27;s like if you could include arbitrary javascript in your vscode config and if you shadow any core function any code calling it will now use your function instead. reply morelisp 16 hours agorootparentprevAs far as I can tell this is simply not true. I don&#x27;t think the change is a good idea but the hard fork also looks like a pure political play, not a technical one.Also, as soon as someone talks about \"settings\" there is usually a profound misunderstanding of how Emacs works at play. reply JoeyBananas 10 hours agoprevThe tone of this author... Very... \"neckbeard\"-esque. This is an incredibly obscure feature only 3 nerds use, not the the Immovable Ladder of the Church of the Holy Sepulcher. The fact that some unpaid maintainer changed it slightly without consulting you first does not necessarily constitute \"breaking master.\" It&#x27;s master we&#x27;re talking about, not stable or a tagged release. Good god. reply mynegation 18 hours agoprevIt’s been 20 years since I left Emacs, but I understand that this change is pretty disruptive. What I do not understand is why Emacs that prides itself in being the “kitchen sink” did not add an option to revert to the old behaviour. reply tsimionescu 16 hours agoparentThey are adding it, most likely. The author of this article is just mad that they didn&#x27;t accept their patch (which added a flag but also did away with all the other improvements). reply binary132 18 hours agoprevObviously the only possible solution is to attempt another fork&#x2F;reimplementation of emacs. This one will definitely win and not be totally irrelevant like all the others. reply samus 17 hours agoparentThe impact of these forks is that the community around the main project is slowly eroded. But it doesn&#x27;t really matter whether the fork brings the main project to the negotiation table. The author of TA is able and is committed to carry and maintain that patch in his local repository until the end of time. What is missing to complete such a fork is publishing that local repository. reply domq 1 hour agorootparentI have a hard time putting faith in that “balkanization theory” from the &#x27;90s anymore. A fork gains traction if and only if it manages to get ahead of the mainline feature-wise, and stay there, by a sufficiently wide margin for customers to overcome their inertia. We&#x27;ll see whether this ~eshel person manages to pull that off, or not (my money is on “not”.) reply CoastalCoder 18 hours agoparentprevFrom the blog post, I couldn&#x27;t really tell what the author&#x27;s long term intentions were for his fork.Maybe just to persuade the mainline maintainers? reply domq 1 hour agorootparentForking Emacs on such grounds is a symptom of ego malfunction. The proper step (after diplomacy has failed, which it hasn&#x27;t yet in this instance) is to author an alternate implementation of the feature in Emacs Lisp and publish it. reply blantonl 18 hours agoprevSo, what&#x27;s the \"other side&#x27;s\" argument in this? Usually these opinionated changes come with some level-headed reasoning behind the changes. Or maybe not? reply lvncelot 18 hours agoparentThis is what I&#x27;ve been wondering here, as well. So far, the downsides (Change to an almost subconscious muscle-memory-task, added friction) of this are pretty apparent - what even are the upsides of this? reply vcg3rd 17 hours agorootparentI use Emacs, mostly for Org Mode, but not registers. I also understand perfectly the problem, and I also can&#x27;t understand the upside.The only clue I see is in the polite objection the author quotes who writes \"I agree it&#x27;s safer...\"But I don&#x27;t understand safer in what way or why 1 person gets to decide safer=better=forced. reply rjzzleep 18 hours agorootparentprevAttracting new users for with the old behaviour is too complex maybe? reply dromtrund 17 hours agorootparentIsn&#x27;t named registers an advanced enough feature that it shouldn&#x27;t be optimized for new users? reply samus 17 hours agorootparentprevThe new behavior doesn&#x27;t make me want to invest effort in learning registers. A notification in the status bar which register just got filled would be all I want. reply mahkoh 16 hours agorootparentprevI believe Mozilla has been successful with a similar strategy. reply lstodd 18 hours agoparentprevYeah, sure. People tried for years to extract the reasoning behind Gnome 2 to Gnome 3 changes and still no one has any idea.Almost as it&#x27;s just and only petty power trips. reply domq 1 hour agorootparentThe “or maybe not?” part of the message you are replying to, tells me that someone got told. reply nanny 13 hours agoparentprevSee: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38592984 reply morelisp 18 hours agoparentprevI imagine it&#x27;s that reading user input through something other than the minibuffer is annoying, because if you have customized your input&#x2F;editing keys, read-key generally won&#x27;t reflect that. reply devnull3 19 hours agoprevI am a Vim guy. Can someone explain what exactly is broken? reply djha-skin 19 hours agoparentIn Vim speak:Imagine you create a bunch of recorded keyboard macros that store stuff in different registers. Now this change comes and instead of typing `\"ay` you have to press `\"ay`. Indeed any time you store to a register (`\"a`) you have to add an enter key. This breaks all of your keyboard macros and all of your muscle memory made over the past twenty years or more.This is why people are upset. reply Exuma 17 hours agorootparentIve used vim for 10+ years, and I have known about yanking to different registers, but usually I only use the main register 99.9999% of the time. can you explain your use cases for using multiple registers? I should start doing this perhaps, but I can&#x27;t think of many times I want to copy more than one thing at once, maybe once a month? But I also could be not thinking of the right examples. reply ramses0 16 hours agorootparentnext [–]\"ayy - copy function signature \"byy - copy return signature \"ap \"bp - paste either... :reg - show all registers \"3p - paste 3rd \"historical\" register \"_dd - delete into the &#x2F;dev&#x2F;null buffer (so you don&#x27;t eat your `yy` register that you&#x27;d already yanked)It&#x27;s admittedly a bit of an advanced&#x2F;esoteric feature, but being able to paste \"this part\" or \"that part\" being somewhat context dependent is useful.Also useful in the context of macros... A, B, C being differing bits you might be \"lifting\", and then placing somewhere. i\" &#x2F; ia - recall (while in insert mode) the default, or the named register.Imagine that your converting `function do_something() { ... }` to: `arr[\"do_something\"] = function() { ... }`You could delete the function name into \"A\", the function body into \"B\", then go back to your marked spot, and pull out the \"A\" into the hash key, and put \"B\" as the key value.It&#x27;s reeeally awkward and complicated until you use it and it becomes a natural part of your way of thinking. Then it becomes \"simply\" two extra characters to type when working with _any_ copy&#x2F;paste task and then you have a super-power of 26 choices of holding things off to the side.`$REG` is honestly one of the best \"beginner\" uses of registers. It lets you \"inline type\" what you&#x27;ve just lifted&#x2F;cut. eg: vwy - yank visible word into default register V\"ay - yank whole line into register \"a\" I\"=\"+1 => `word = word + 1` (without having to exit insert mode!) \"ap - paste the line from register \"a\"...it&#x27;s a small thing, but an important aspect of \"vim as a live text-based programming language\", having a few \"hot\" named variables &#x2F; text strings, and being able to see them and manipulate them. It&#x27;s literally just the double-quote key and \":reg\" that gives you access to it. reply Exuma 11 hours agorootparentAh this is incredible, thank you. I didnt realize that about ... wow, that is crazy cool.Next question, do you usually add stuff to buffers in alphabetical order... a,b,c or do you pick something easier within reach like a,s,d (or something else entirely) reply ramses0 9 hours agorootparentUsually a&#x2F;b&#x2F;c but sometimes f&#x2F;function, k&#x2F;key, v&#x2F;value... just a simple mnemonic.To really blow your mind: i% V:!lsThen you start playing with marks a little bit with a similar concept (eg: ma, mb, mc, &#x27;a, &#x27;b, &#x27;c), and the good friend `gi` (go back to previous insert position)...It&#x27;s again, esoteric, but as you use it more, it becomes less esoteric and more just another part of your vim vocabulary (:help search-offset, fellow traveler). reply Tyr42 15 hours agorootparentprevUsually I yank into like p or something when I want to not just paste later, but delete something and then paste. I hate it when I delete something to make room for what I paste and it gets rid of it. Then I fumble with the numbered registers and mess it up.Or when I select something and then paste and I actually wanted to keep my paste buffer and not replace it reply pmontra 16 hours agorootparentprevFirst thing I thought about: yank a line to a, yank another line to b; move somewhere into the file; paste from a, move two lines down, paste from b; repeat N times or create a macro for it. Basically it will be macro with two inputs, the contents of the a and b registers. reply dw_arthur 17 hours agorootparentprevI do code in a gate around a few registers for macros and marks. I hate recording a macro and then accidentally overwriting it. reply hattar 19 hours agoparentprevhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38591788This person seems to explain it well reply morelisp 19 hours agoparentprevThey did the Emacs equivalent of adding a confirmation dialog to an action which some people do very frequently (like tens of times per minute). I think it also fundamentally breaks some rarer use cases I would have trouble explaining to non-Emacs users, but do seem somewhat valuable.It&#x27;s not clear to me from the messages I read why this can&#x27;t be worked around without a hard fork, although I do agree it&#x27;s an obvious bad decision to begin with, and obvious to me even as someone who barely uses this feature. reply rcthompson 16 hours agorootparentLooking at the commit diff, I don&#x27;t see anything that necessitates a hard fork. The changes are all in elisp, not compiled C code. It looks like just evaluating the old version of register.el (perhaps with a few compatibility changes) in an Emacs session would revert everything to the old behavior. reply Narishma 19 hours agoparentprevIt&#x27;s explained in the article. reply devnull3 19 hours agorootparentI could not understand and hence I asked the question. The thing I understood was that this broke some peoples flow. reply isodev 19 hours agorootparentprevI’m not an Emacs user and also failed to understand what went wrong. So they changed a default shortcut or something? reply lvncelot 18 hours agorootparentImagine copying via Ctrl-C and having to hit enter to confirm a dialog \"Did you want to copy this text\". It&#x27;s added, unavoidable friction for an action that some people use a lot.Additionally, some actions on top of the original behavior (that depended on Ctrl-C completing automatically, to keep with the analogy), now are just broken. reply Kalq 7 hours agorootparentWhile I appreciate the analogy, it&#x27;s important to note that registers are a fairly advanced feature, not quite as ubiquitous as Ctrl-C. Standard Emacs copy-paste remains unchanged and functions just as expected. reply doix 19 hours agorootparentprevI also don&#x27;t really use emacs. But by the sounds of it, when you want to use a command that involves a register, you get a mini-popup where you input the register you want.The downside is that you must push enter after you enter the register you want. reply raverbashing 19 hours agoparentprevThe Emacs development processNo, I&#x27;m not kiddingThere is no focus, there&#x27;s no objective, it seems the louder ideas win but there&#x27;s no final idea of what Emacs should look like or what&#x2F;how it should do or notThat&#x27;s why such breaking changes get in without questioning (and even why they were using such functionality as registers in the first place) reply lemper 19 hours agorootparentyea seems right. tfa says that thierry didn&#x27;t accept input at all from other maintainers. that&#x27;s not good in my eyes. reply BaculumMeumEst 18 hours agoprevI think a more effective approach to getting this reverted would be to actually describe the problem for people who don’t know what registers are, and to include the reasoning for the change, so that people can actually weigh the pros and cons and form an opinion.I also think you can leave out the names of individuals, you can discuss the idea on its merits without their identities being involved. All it accomplishes is demonizing people who donate their time to the project. Already this thread has people saying stuff like “I don’t like this person.” reply layer8 17 hours agoparentPeople who don’t know what registers are aren’t affected by the change, so it’s not clear why they should be weighing in.And criticizing a person’s decisions doesn’t amount to “demonizing”. reply BaculumMeumEst 10 hours agorootparentThis article is accusing maintainers of pushing a bad change. It was immediately obvious that there was only one side of the story being told here. And lo and behold, there was a lot of missing context:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;reply?id=38592984&goto=item%3Fi...and individuals involved have been receiving harassment:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;emacs&#x2F;comments&#x2F;18f5oi9&#x2F;comment&#x2F;kcss... reply cardanome 12 hours agoparentprevThe actual reasonable approach would be to implement it as a strictly opt-in feature for the next release and then when many people have tested it, gather some some usage data and then decide on whether to make it the new default behavior or not.Just breaking established user behavior and muscle memory just because you think something might be better without having gathered any actual evidence is insane. reply lycopodiopsida 17 hours agoprevThe \"maintainer\" is Thierry Volpiatto, mostly known as author of helm, a completion framework. reply mediumsmart 17 hours agoprevIt’s a slippery slope”it looks like you are trying to create a buffer that is not attached to a file. If that is actually what you intended please confirm with RET so that the buffer can be created but keep in mind that. . .” reply ajross 19 hours agoprevIt&#x27;s a good rant, but it probably doesn&#x27;t help its case that it doesn&#x27;t stop to explain what the feature it&#x27;s talking about actually is. Emacs registers are a really, really old abstraction. Registers are like separate clipboards, you can put stuff there and pull it out. And there are 62 of them (each associated with a core ASCII symbol: [a-zA-Z0-9]), giving you a lot of flexibilty and a very quick keyboard interface for acting on&#x2F;with them. And you can even do fun stuff like execute a keyboard macro out of a register. Some people use them heavily. I don&#x27;t.Anyway the author is peeved that they changed the default bindings such that there&#x27;s a modal UI in the way of what used to be a fully-keyboard&#x2F;home-row operation. It sounds like a reasonable complaint; I&#x27;d be annoyed too if they changed something that lived in my muscle memory like this kind of feature does. reply huggingmouth 19 hours agoparentI stopped using emacs a long while ago but I rely on x11 selection buffers (middle click paste) heavily so i can relate. I&#x27;d absolutly be miffed to find a confirmation model dialog shoved between each select and paste operation AND a lack of a setting to revert to the old behavior.I say implement the old behavior as an option. It&#x27;s literally a few extra lines of code. reply dannyfreeman 18 hours agoparentprevSounds like Eli is not married to this as the default behavior (I&#x27;m not a fan of it either).From https:&#x2F;&#x2F;yhetil.org&#x2F;emacs&#x2F;8334wawfvg.fsf@gnu.org&#x2F;Eli says:> Now, that others chimed in with the opposite views, we are still discussing what should be the behavior, and once that is concluded, we can talk about the defaults. reply morelisp 19 hours agoparentprevThere are more than 62. One of the complaints is that the minibuffer breaks access to e.g. the C-a register. One person I know uses letters for positions and C-letter for text so while this may be a very edge case for some I imagine they will be very upset... reply tinus_hn 19 hours agoparentprev‘Default bindings’ sounds like it’s configurable to be different. Is it? reply jsw97 18 hours agorootparentIt is not. That approach was tried and rejected, hence the problem. reply morelisp 18 hours agorootparentIn the interest of fairness, it should be noted a very specific solution was tried and rejected (providing configuration variables to adjust the behavior of `register-read-with-preview`). I personally don&#x27;t see anything preventing users from configuring alternate commands which call directly `set-register` and mapping those as they please, so in that sense it is still configurable. And in the extreme case you could even use advices, etc. It is very difficult to make something non-configurable in Emacs and I don&#x27;t see the register system being so ingrained as to be one of those things.Whether it&#x27;s a good idea... well, I don&#x27;t use registers but the whole thing seems like a bad idea to me. I don&#x27;t think I know anyone who uses registers and would like the new behavior. (But I also understand the desire to modernize `register-read-with-preview` a bit. A better solution seems like it would be providing multiple implementations so users can do the register equivalent of `(fset #&#x27;yes-or-no-p #&#x27;y-or-n-p)` like every old Emacs user does today.) reply jason_stelzer 18 hours agoparentprevIt’s basically global uac-mode for all buffers without the ability to turn it off. If it weren’t true it would be laughable. reply jason_stelzer 19 hours agoprevI have been using emacs since probably 94. This change, without a toggle from userspace to disable it is a kick in the pants. I will follow your fork until (if) they come around. reply pdyc 16 hours agoprevit indeed looks like a bad idea. New behavior should be optional, old behavior should be default so that it does not affects existing workflow of users. I am heavy emacs user but i use stable release, i hope this change does not makes into the stable release. reply juped 19 hours agoprevThis is really polemically overstating the case, but at least it links to the mailing list threads where you can see what&#x27;s actually going on. Not that anyone will click them. reply akarve 16 hours agoprevI was sure to read “the forces of vim have invaded our headquarters and now hold our CEO hostage.” reply philipwhiuk 17 hours agoprevIt seems like Emacs has a fairly toxic development process.Arguing that because something has been committed to the development branch it can&#x27;t be reverted, is a terrible policy. reply shadowgovt 17 hours agoprevAt the end of the day though, it&#x27;s emacs.In lisp alone, you can get the old behavior back if you want it.I think usability decisions like this are one of the harder things to decide by committee in an open source project because they are, at the end of the day, questions of taste. And people can have conflicting and equally valid tastes. Here, a swap is being made between editing speed and accuracy, with the one perhaps having the better claim for purely historical reasons (but if you let historical reasons dictate design, you end up with faster horses not cars).Me personally, I&#x27;m a committed emacs user and I don&#x27;t have skin in this game. If I don&#x27;t like the new UI, I&#x27;ll just add the necessary code to swap it out for the old UI. reply travisjungroth 17 hours agoparentThis is equivocating. It’s not better for purely historical reasons, it’s just a bad trade of speed for accuracy. If you really want to be sure, is one return click enough? Maybe it should be two. Maybe a mouse click. You could throw in a five second cool down to be really sure. Obviously some of these ideas are worse than the status quo. reply shadowgovt 16 hours agorootparentI don&#x27;t dispute that two or more clicks or a mouse press would be a bad trade-off of speed for accuracy.Why is one press of the enter key a bad trade-off of speed and accuracy besides \"It&#x27;s never been that way before?\" reply travisjungroth 13 hours agorootparentThese are high speed actions and usually all home-row. The return hit increases the time a significant percentage. They’re also easily undone, so the cost of mistakes is low.Imagine needing to hit return every time you ctrl-v. Really no point when ctrl-z is available. reply shadowgovt 8 hours agorootparentWell the good news is it should be straightforward to customize the old behavior back for people who don&#x27;t like the trade-off. reply travisjungroth 6 hours agorootparentIt should be, but the change didn’t have it as a setting. replyur-whale 19 hours agoprevOpenSource working as intended, no? reply addicted 17 hours agoparentJust because a system works doesn’t mean all outcomes out of the system when it’s working are equally good.Even the best working system will still require people to make good decisions to have the best outcomes. reply tvink 19 hours agoparentprevSure, in the sense that they can differ and co-exist? Of course it&#x27;s much better in OSS when differences can be resolved so that more people may work together on fewer forks, so discussion should come before forks most of the time. The author seems to be doing exactly the right thing - tried to contribute to the shared work, voice their concerns and thought process, and ultimately fork in protest. reply mrlonglong 19 hours agoparentprevYup. Great isn&#x27;t it when the system works as intended. reply DonHopkins 18 hours agoparentprevAbsolutely not! Free Software working as intended. Are you trying to trigger RMS by calling Emacs \"Open Source\"? reply dig1 18 hours agoprevIt may be an unpopular approach, but I&#x27;m a fan of Linus&#x27;s \"you must never break user-space&#x2F;UX.\" Some changes might be trivial for you or even an \"improvement\" (which is mostly a personal opinion, especially regarding UX, unless you prove it with many research papers or have many complaints). Still, if I hit some key combos 100 times a day in the last 20 years, that became second nature for me. Adding Enter or any other key because \"it makes things nicer\" is clearly a bug.I&#x27;m also not fond of Emacs&#x27;s many subtle UX changes in the last couple of years. Enabling eldoc by default, changing \"blink-matching-paren\" default value... For each new Emacs release, I have to revisit my init.el and revert to the old behavior (thank you, elisp!), because suddenly things start to pop out or jump around. I get it; this is maybe to please the newer&#x2F;younger crowd who are usually \"in transit\" - yesterday were on Vim, today, are on Emacs, and tomorrow, who knows, leaving us regular users with \"a big bag of odor.\"Thanks to elisp, you can bend Emacs any way you want, but don&#x27;t change default behavior just because \"it looks nice to me\". reply norir 18 hours agoparentI actually think changing the default here may have been sensible. The problem is removing the old functionality entirely. This looks like a change to benefit new users (which is good) that had the hopefully unintended consequence of burning existing power users (which is very bad). The sensible compromise is to add a config flag that restores the old behavior while keeping the new default. From the outside, it&#x27;s hard to see any reason other than pride for not doing that. reply samus 17 hours agoparentprevPleasing the newer&#x2F;younger crowd is important! The default experience of Emacs should make it possible for new users to get up to speed as quickly as possible. Else, Emacs will slowly fade to irrelevance until it is a museum piece. There are of course different views about how to approach this.Of course, for any change there should be switches or other possibilities to restore the old user experience. Power users (especially of Emacs) can be expected to be able to maintain their init.el file. Even the case of Spacebar Heating[0] could be handled that way. The legacy of a genuine technical bug should not impact the rest of the community forever.Some might point out that there are Emacs distributions out there that offer a modern experience. But these are hardly known to newcomers. Distribution shopping is a useless distraction before starting to use an editor which already has a higher-than-average learning curve.[0]: https:&#x2F;&#x2F;xkcd.com&#x2F;1172&#x2F; reply Jochim 17 hours agoparentprev> I get it; this is maybe to please the newer&#x2F;younger crowdThis seems to miss the point of these changes. If you&#x27;re only concerned with your own workflow then almost any changes that you didn&#x27;t specifically request are annoying. However, if you&#x27;re concerned with the continued development&#x2F;relevance of the program then it becomes clear that change must occur. Taking into account both existing user&#x27;s concerns and barriers to entry for new users.Many of emacs defaults are pretty awful at introducing users to the \"emacs\" way of doing things while also failing at easing unfamiliar users in. reply rightbyte 13 hours agorootparentWhy should catering to non-users even be a concern? It is like these radio stations and tv channels that blend into the sameness blurb.If the last emacs user press C-x C-c for the last time in whatever years that is not a failure. reply smarx007 17 hours agoprevI am sorry but this whole post seems like gaslighting. Looks like maintainers welcomed a patch bringing the old behaviour behind a flag [1]. Eshel Yaron then writes \"Sure. I&#x27;m attaching two patches\", while doing the exact opposite of what was asked, namely undoing the commit that has already landed on the main branch and introducing merely parts [2] of the new behaviour behind the flag.It&#x27;s totally fine to disagree with people. Agreeing with people while doing the complete opposite, is quite unacceptable, in my opinion.[1]: https:&#x2F;&#x2F;yhetil.org&#x2F;emacs&#x2F;837clv6sga.fsf@gnu.org&#x2F;[2]: https:&#x2F;&#x2F;yhetil.org&#x2F;emacs&#x2F;87r0k2pgq6.fsf@posteo.net&#x2F; reply rightbyte 9 hours agoparentI like how RMS pops up and complains about a Github link requiring non-free software and then returns to what ever he was doing. reply deng 17 hours agoprevMaybe let&#x27;s keep the drama down a bit? There was a change committed which made using registers more friendly to use for newbies. IMHO, working on making Emacs&#x27; features more accessible is a good thing. And yes, this of course annoys old-school Emacs users (including me). AFAICS the discussion is still very much in progress. There will be an option added to be able to revert to the old behavior. There&#x27;s still discussion if overwriting registers should prompt for confirmation (which it didn&#x27;t use to do). Let&#x27;s wait how that one turns out. I fail to see why one would need to write a long blog post and maintain a fork because of something like this... maybe just wait it out a bit and let people voice their opinions, it takes a bit of time... reply rollcat 11 hours agoparentContext: I&#x27;m an Emacs user for 20 years...> There was a change committed which made using registers more friendly to use for newbies....and didn&#x27;t realize this feature existed.> Maybe let&#x27;s keep the drama down a bit?Agreed, this is non-news. reply philipwhiuk 17 hours agoparentprev> fail to see why one would need to write a long blog post and maintain a fork because of something like this...Probably because, and it certainly appears the case, that the process is being ruled by fiat not by good technical&#x2F;policy decisions (and the approach of broadcast is intended to replace the fiat by overwhelming numbers) reply deng 17 hours agorootparentAnd again with the drama...The maintainer already said: he thought it was a clear improvement, so it landed on master. Now that it has landed and people use it, people react to it, and it turns out the old behavior was well beloved and it&#x27;s already decided it will stay through an option. Now further details are discussed. This has happened many times before. Just give it a bit of time, voice your opinion, and hopefully a consensus will be achieved (and if not, yes, the maintainer has the last word, that&#x27;s how it supposed to be).This is the master branch. It often takes months to flesh out these kind of things. You might say this should be done on a feature branch, but these get used much less and hence you&#x27;ll get much less feedback. reply Asooka 17 hours agoparentprevNo program feature should ever be designed to be \"friendly to newbies\".Easy to use (in general) - yes; easy to learn - yes; hard to do accidentally - yes; low friction - yes; discoverable - yes. However, designing for the person who has never learned to use the feature inevitably leads to your program having a very low skill ceiling and being a disservice to power users.Also, how do you know the new design is \"friendly to newbies\"? You&#x27;re not a newbie and neither are any of the other Emacs developers. Without lots of user studies, you have no idea what is \"friendly to newbies\".A more proper way to do this change would be to go \"Hey, I&#x27;ve been running into an issue with using registers for a while and solved it for myself with this change. Let&#x27;s add it as an option and vote on what the default should be.\" This is a much better approach, because1. It is designed based on real user feedback. Even if it&#x27;s just one user, people aren&#x27;t unique, so there must be many like that person. \"I like it this way\" is a much stronger argument than \"my imaginary newbie likes it this way\".2. It invites the rest of the community to decide on the default program behaviour. Software must serve its current users first and foremost. Thus, what the majority says should be the default is what the default should be.This is where Volpiatto and Zaretskii went wrong. A change was made and pushed to solve an imaginary problem for imaginary users without involving the people actually affected. They broke their users&#x27; trust. reply deng 15 hours agorootparent> They broke their users&#x27; trust.And now the drama is dialed to 11. I see no point in responding to such hyperbole. reply wavemode 18 hours agoprevThis thread is mostly \"I don&#x27;t use this feature of Emacs (or Emacs at all) therefore this isn&#x27;t a real problem, so stop complaining.\" Poor to see.I&#x27;d rather people who do regularly use the feature weigh in on how they feel about its new behavior. reply tmtvl 18 hours agoparentIt&#x27;ll take me a day or two to get used to it and then it&#x27;ll be fine for me. But I only really use registers to quickly jump around in buffers. If no configuration for switching to the old behaviour makes it in before Emacs 30 is released I&#x27;d be surprised if it were to take more than a week before someone makes a package that adds the old behaviour back in. reply CoastalCoder 18 hours agoparentprevFor an established tool like Emacs, it must be hard to decide when it&#x27;s okay to break the interface. Especially for a text editor, where longtime users&#x27; flow benefits from muscle memory. reply ginko 18 hours agoparentprevAs a long-time emacs user who&#x27;s never used registers I still think this is very poor form from the maintainers.Emacs should be about customization and changing such a basic feature without a way to return to the original behavior is pretty bad. reply coldtea 19 hours agoprevnext [2 more] [flagged] addicted 18 hours agoparentProblems with a large open source project are never about a single individual.Individuals will make mistakes and have wrong opinions. The problem here appears to lie with the larger eMacs development community. reply TheRoque 18 hours agoprevnext [2 more] [flagged] samus 17 hours agoparentBecause of this change, the author of TA will likely have to perform 6000 keystrokes more per day since he is a poweruser of that feature. And this is just one user. Seems entirely justified. reply Kwpolska 17 hours agoprevIs it just me, or is Emacs much more drama-prone, compared to Vim, VSCode, or any other text editor? reply entropie 19 hours agoprev> Since then, another bug report came in from a Emacs master branch user that suffered from one of the consequences of this change (a specific regression that I spelled out days before, but was ignored, for some reason), and several users reached out to the Emacs development in request to restore the previous behavior in an ongoing thread titled “Please, Restore Previous Behavior for jump-to-register”. Astonishingly, Eli and Thierry won’t seem to budge, and Emacs 30 will thus likely suck.Seriously, this is kind of funny. I use emacs for like two decades now and I knew about jump-to-register but never actively used it. But yes - emacs 30 will be bad because of this change. Barely useable anymore. This guys in their mailinglist have... very specific problems.You heard it first here. Emacs 30 will suck.Also; https:&#x2F;&#x2F;xkcd.com&#x2F;1172&#x2F; hits the spot and is - what a coincidence - about emacs and workflows. reply addicted 18 hours agoparentI’m a VIM user, not an emacs user so I don’t have a dog in this fight, but if the equivalent change was made in VIM it would pretty much be unusable for me. My muscle memory related to this feature would be completely broken.I can’t think of any such change in VIM being made without a setting to turn it off. reply tom_ 18 hours agoparentprevIf you use this stuff a lot, it is totally going to suck. Some thing has changed, that may or may not have a good technical reason, that means that after years of the program training you into behaving one way (and it&#x27;s not like this is some crazy workflow you&#x27;ve invented yourself, right - this is literally you following the documented process) you&#x27;re now being punished for it by having it not work.You wouldn&#x27;t even treat your dog like this. reply lvncelot 18 hours agoparentprevI mean, we&#x27;re talking about changing intended behavior here and not accepting any discussion regarding that. Just because you didn&#x27;t use jump-to-register, doesn&#x27;t mean no one does. Again, this is not people using unintended consequences of edge-case-behavior, this is people complaining (rightfully so) about regressions that stem from changing the intended UI. reply kyrra 19 hours agoparentprevWhile I understand your point, emacs is changing very specific behavior, that was intended, to do something new now. So I would say you&#x27;re xkcd comment is slightly off. reply entropie 19 hours agorootparent> emacs is changing very specific behavior, that was intended, to do something new nowNo? It changes how an existing command behaves.> This commit crippled all user interaction with Emacs registers, turning commands such as C-x r s, once smooth and frictionless, into a cumbersome and painful mess. Concretely, instead of just typing the key for the register you want to operate on, you now get a fully blown minibuffer for inserting a single key. reply broscillator 19 hours agorootparentwith all due respect c-x r s sounds far from smooth and frictionless to me reply monsieurbanana 18 hours agorootparentIf you&#x27;re so used to type C-x that you don&#x27;t even register it, like most emacs users, then it&#x27;s just pressing \"r s\" for \"register save\".It&#x27;s not terrible, and the mnemonic is sound. If you don&#x27;t like it, perfectly valid opinion, you&#x27;re just not made for emacs&#x27; default behavior. reply mjw1007 18 hours agorootparentprevTrue. The traditional binding was was just `C-x x`, but they&#x27;ve already done one round of making register features more inconvenient to get at. reply Finnucane 18 hours agorootparentprev‘Frictionless’ here means ‘we’ve done it this way forever and don’t have to think about it anymore.’ replyNikkiA 19 hours agoprevAssuming any particular commit in git represents the eventual state of a release seems foolhardy. reply marcinzm 19 hours agoparentThat&#x27;s not the author&#x27;s point. They did try to improve the behavior in a future commit and were shot down. Hard it seems. So were any requests from others to change this before release. reply chongli 19 hours agorootparentNot only that, but the author of the original offending change asked the author of this blog to write a patch to make the change optional within the UI and then rejected the patch that was written. That seems like bad faith to me. reply yaantc 17 hours agorootparentRead the mailing list thread: the patch did much more than making the change optional, it did revert other related changes. That&#x27;s why it was rejected. Other discussed changes were taken in, and it&#x27;s not settled yet it seems: the discussion is on-going.I find the reporting here very one sided and uselessly dramatic. I read the thread and don&#x27;t see arrogance, just (sometimes strong) differences of opinion. Calling \"arrogant\" anyone who don&#x27;t agree and fold to your view, and create drama and draw the crowd against one specific person (the initial change author, OK, but it was done with maintainers in the loop) where the crowd won&#x27;t check the details is not OK in my book. reply chongli 16 hours agorootparentThen there is bad faith, it&#x27;s just bad faith on the part of the linked blogger. replyjhoechtl 19 hours agoprevI someone is to fork Emacs its because of getting proper UI integration or multithreading.This is just a neglectable, whimsical step. Sorry.What Emacs needs is the nvim&#x2F;vim Schisma. reply PurpleRamen 17 hours agoparentThere are enough unsuccessful forks of GNU Emacs. None of them get enough attention and support for whatever reason, but I guess mainly because there have no real reason to exist. Emacs devs are working diligently and improving it regularly, and there is no good and simple solution for the problems it has.There won&#x27;t be any nvim&#x2F;vim-like Schisma happen, unless some super-dev appears who can outsmart the whole community and it&#x27;s devs. Maybe, in a decade when AI become good enough and someone feeds Emacs to electron or something like that. reply ajross 19 hours agoparentprev> What Emacs needs is the nvim&#x2F;vim Schisma.It&#x27;s already had like six, over the decades. reply PaulHoule 19 hours agoparentprevThere was Xemacs back in the day… and I guess it is still around. reply medstrom 13 hours agorootparentIt&#x27;s still around in the same sense that twm is still around. reply pilgrim0 19 hours agoprev [–] I don’t fully understand the nature&#x2F;impact of the change but this conflict seems so minor to me. Like, people on both sides display the same level of entitlement but for different reasons, and they all think they have their are “right”. Hum, no, for outsiders you all appear stubborn and lacking ability to compromise. “Oh my gosh, I’ll have to press 1 extra key now, this project is doomed!”, “no no no, this little change is the hallmark of security, it should be mandatory for every user”. Come on… reply janice1999 19 hours agoparentI disagree. As a developer I think breaking literally decades old workflows for users without notice or any level of concern is a bad thing. For a program with users so reliant on muscle memory, I think the impact is far worse. reply smitty1e 18 hours agorootparentThe friction seemed less about the change as such as in the unilateral delivery.Even good change (whether or not this is) needs a gentle transition. reply jonathanstrange 18 hours agorootparentIf I understood this correctly, the new behavior cannot be customized back to the old behavior. If that&#x27;s true, then that&#x27;s obviously very bad. Generally, as an Emacs user, I don&#x27;t just want an opt-in or a gentle transition, I need to be able to customize everything to my needs. reply smitty1e 15 hours agorootparentThis is Emacs, so one presumes that it&#x27;s a SMOeL (Simple Matter Of eLisp) problem.However, it seems tasteless to impose the burden. reply sgbeal 18 hours agoparentprev> “Oh my gosh, I’ll have to press 1 extra key now ... \"is a genuine usability tragedy for folks who have 25+ years of muscle memory involved. There are no small number of emacs users, myself included, who fall into that category.> \"... this project is doomed!”obviously it&#x27;s not quite that bad, but any change to long-standing muscle memory is going to send countless users down a rabbit hole looking to undo that change. reply domq 1 hour agorootparentThat is a correct and fair assessment of the impact, assuming the change lands in a release as-is (which apparently it won&#x27;t: https:&#x2F;&#x2F;lists.gnu.org&#x2F;archive&#x2F;html&#x2F;emacs-devel&#x2F;2023-12&#x2F;msg00... ) reply dack 19 hours agoparentprevBreaking backward compatibility should be reserved for extreme cases, and this doesn&#x27;t sound like one. It&#x27;s possible the way the author handled it was poor and caused part of the dispute, but I think the right thing would be to make such behavior opt-in or at least very easy to disable. reply potatopatch 19 hours agoparentprevInventing new things who cares, but for existing key sequences its a problem even if the new sequence would have been better. Imagine when some of these soft cars start adding power steering in an overnight update. reply praptak 18 hours agoparentprev> Oh my gosh, I’ll have to press 1 extra key nowThis key is in a context which is about as bad as Ctrl-C (\"Do you really want to copy this text into clipboard? [Y&#x2F;n]\") reply kazinator 1 hour agoparentprevWould a guitarist be wrong to reject an instrument whose string had to be plucked twice before starting to vibrate? reply ta988 18 hours agoparentprevThis is something that should just have been made as a new function not replacing an existing one. That way users have a choice between speed when they know what register to use and help when they don&#x27;t. reply binary132 18 hours agoparentprevSocial problems caused by intense and antisocial personalities? In MY free software project?It’s more common than you think. reply doubloon 18 hours agoparentprev [–] not minor at all. you do something 100 times a day, adding multiple seconds to that time is not only wasting time, it is increasing the &#x27;cognitive load&#x27; on the human brain.this is why you find in a lot of Operations centers, people have a dozen different systems where time-wasting \"improvements\" like this have been made over 20+ years, so each little time-wasting \"improvement\" has added up over time so now it takes like 10 minutes to do something that should take 10 seconds. (in other words, \"why does an Airline clerk have to spend 5 minutes typing in order to do something for my ticket&#x2F;account\", or \"how did they mess up my request so badly\", the answer is what im talking about)what it all really boils down to is a fundamental lack of respect for other people. if you respected other people, you would not break their workflow like this and then dismiss their concerns as unimportant. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author is frustrated with a recent change in the Emacs master branch that has had a negative impact on user experience.",
      "They criticize the Emacs maintainers for accepting the change despite objections from users and developers.",
      "The author has created their own fork of Emacs called the \"main\" branch, where they have reverted the problematic change and made their own improvements. They are encouraging others to join and contribute to their branch."
    ],
    "commentSummary": [
      "The Emacs community is currently experiencing a debate over a recent software change that some users find disruptive to their workflows and incompatible with previous versions.",
      "Users are discussing whether the default behavior should be reverted or if the new behavior should be optional for users to choose.",
      "The discussions delve into the development process, the value of user feedback, and the behavior of certain individuals, with suggestions including forking the software or finding a compromise to address the concerns."
    ],
    "points": 337,
    "commentCount": 210,
    "retryCount": 0,
    "time": 1702217233
  },
  {
    "id": 38591662,
    "title": "Learn to Build Your Own Retro Compiler for CP/M on the Z80 Processor",
    "originLink": "http://t3x.org/t3x/0/book.html",
    "originBody": "http://t3x.org/t3x/0/book.html WRITE YOUR OWN RETRO COMPILER Lulu Press, 2023 • 339 pages • 91 figures • 6\" x 9\" format All code from the book is in the public domain! Order a PDF copy at Lulu.com Order a paperback copy at Lulu.com View the Table of Contents (PDF) Read some sample pages (PDF) Download the Sources Enjoy old computers? New to compiler-writing? This book has you covered! Study the complete source code for a self-hosting compiler that runs on and generates code for CP/M on the Z80 processor. No prior knowledge in the field of compiler construction is required. The T3X/0 language that is discussed and implemented in the book has its roots in Pascal and BCPL and is very simple. A full 20-page manual is contained in the book. Prerequisits: The reader should know at least one procedural programming language, such as C or Pascal, and at least one assembly language, ideally the one for the Z80 CPU. They should also know the basics of the CP/M operating system. For the determined autodicact a short introduction to Z80 assembly language is also included in the book. This book cuts no corners! Everything is discussed in great details with lots of diagrams, tables, and examples. Lexical analysis Syntax analysis Code generation Simple optimizations The BDOS interface The run time library Get the code: www.t3x.org/t3x/0/ contactprivacy",
    "commentLink": "https://news.ycombinator.com/item?id=38591662",
    "commentBody": "Write your own retro compilerHacker NewspastloginWrite your own retro compiler (t3x.org) 303 points by nils-m-holm 19 hours ago| hidepastfavorite46 comments nils-m-holm 18 hours agoHere it is, my latest compiler book! Basically an expanded version of \"Write Your Own Compiler\", this time discussing code generation for CP&#x2F;M on the Z80 (instead of ELF on modern systems), which simplifies some things a lot.How much complexity do you need to self-compile a compiler in 10 minutes on a 4MHz Z80 system? Take a look and find out! The code is free (but the book is not). reply 7thaccount 16 hours agoparentAlways look forward to hearing about what you&#x27;re working on Nils! I hope your business of doing this is profitable as well! One day I&#x27;m going to finally buy a copy of everything and work through it all. It seems like there is so little time though lol.Edit: I&#x27;d also love to see you do a no-nonsense book on Forth and your take on it. reply nils-m-holm 16 hours agorootparentThank you!Books are my biggest source of income, but \"big\" is relative here: I am earning about $500 per month in revenues. This is mostly a problem visibility, I think. Most people stumble across my books by accident. Reviews, presence on the front page of HN, etc. usually increase revenues significantly for one month.Regarding FORTH, lets see. The code is already there: http:&#x2F;&#x2F;t3x.org&#x2F;t3xforth&#x2F; reply pamoroso 12 hours agorootparentNils, regarding visibility, have you considered setting up a Mastodon account? Yesterday I shared on Mastodon a link to your book and my post got 28 reshares, 42 likes, and half a dozen comments. And I&#x27;m not even the author of the book. In the Fediverse people do notice you, read what you write, and click your links. Which is mind blowing for those used to traditional social platforms. reply nils-m-holm 11 hours agorootparentThank you, but I do not even know what Mastodon or the Fediverse are. :) Could you point me to some resources that would get me started? Preferably to something for the social media-illiterate! Which software to use (on BSD), where to register (if that is a thing), etc. That would be cool! reply pamoroso 1 hour agorootparentThink of Mastodon as an open-source Twitter with multiple servers (\"instances\" in Mastodon-ese) instead of the only twitter.com The servers interoperate through a common protocol, thus allowing any user on any server to follow and engage with any other user on any other server. The protocol actually makes it possible to interoperate with platforms other than Mastodon that are part of a larger system called the Fediverse, but you can ignore it for now.The only software you need to get started is a web browser to use a web client, which is typically the Mastodon server you create an account on. Picking a server is the only potentially confusing choice, so for retrocomputing enthusiasts I recommend creating an account at https:&#x2F;&#x2F;bitbang.social or https:&#x2F;&#x2F;oldbytes.space Once you have an account on a server you can migrate to another one if needed.For more on Mastodon see https:&#x2F;&#x2F;joinmastodon.org For any other questions feel free to ask here or follow me on Mastodon at @amoroso@fosstodon.org and ask there. reply 7thaccount 4 hours agorootparentprevFediverse refers to a collection of decentralized applications that use some kind of common protocol. Like instead of centralized Twitter, you could choose Mastodon and so on. My understanding is very basic though.Long story short, I&#x27;d love to see you advertise some more. I&#x27;m not sure how you&#x27;d reach your typical audience though. reply 7thaccount 7 hours agorootparentprevAwesome Forth implementation! I&#x27;d love to read a high level book on the implementation too once you get around to it! reply Archbtw97543 15 hours agorootparentprevThanks for being so transparent reply nils-m-holm 14 hours agorootparentSure, no problem :) reply winter_blue 15 hours agorootparentprev$500 isn’t really enough to live on — do you have a job that you use to support yourself (or do you live in a low cost-of-living country)? reply nils-m-holm 14 hours agorootparentI have multiple sources of income, but book revenues are the biggest one. The area I live in is a rather expensive one, but I pay no rent, which helps. By the standard of our country I qualify as \"poor\", but I do not mind much. A little bit of safety would be nice, but I do not need much to live comfortably. reply tiberious726 12 hours agorootparentThis is great quality work---you&#x27;d crush it writing technical documentation or putting together technical internal corporate training programs. reply nils-m-holm 11 hours agorootparentI tried that some time ago, but do not seem to be compatible with all the business stuff. Thanks anyway! :) replyN3Xxus_6 8 hours agoparentprevNils just wanted to say I love your work. I have your C compiler book and one of your lisp compiler ones as well. I’ve learned a lot and your work has helped me appreciate compilers a lot more, so thanks! reply mati365 18 hours agoprevRecently I made C multipass compiler (and asembler) in typescript for such old x86 CPUshttps:&#x2F;&#x2F;github.com&#x2F;Mati365&#x2F;ts-c-compiler reply nils-m-holm 18 hours agoparentCool!The one in the book is for the Z80, which is a bit older and does not even have multiply or divide instructions. The compiler can also output code for the 8086, though. And the 386. reply vanderZwan 11 hours agorootparentDo you happen to have books that cover the latter two targets?EDIT: should have taken a look on the rest of your website first. Clearly you do, hahahahttp:&#x2F;&#x2F;t3x.org&#x2F;index.html reply scrawl 18 hours agoprevI have a physical copy of Practical Compiler Construction 2nd Ed. and like it a lot. I recommend Nils&#x27; books to anyone who may be interested. reply nils-m-holm 18 hours agoparentThank you! The 2nd Ed. was quite an endeavor, I am glad you like it! reply kqr2 16 hours agorootparentYou may need to update the thumbnail image on your index page. It still shows the first edition:https:&#x2F;&#x2F;t3x.org&#x2F;index.html reply nils-m-holm 16 hours agorootparentOops, good catch. :) Thank you! reply pests 5 hours agoprevA compiler tutorial that gets past the lexing and parsing stages? First of its kind.I kid, but it is a common stopping point. Gonna pick this up. reply amelius 16 hours agoprevThis book looks fun. But I&#x27;m still waiting for a worthy successor of The Dragon Book, discussing optimizations for modern CPUs (and perhaps GPUs), and also discusses how to design&#x2F;write a modern VM with a fast concurrent GC (something that some might say is even harder than writing the compiler!) reply tralarpa 16 hours agoparentI remember that I didn&#x27;t like the Dragon Book when I was a student. But I don&#x27;t remember anymore why :) (I think I found it poorly structured, and with too many details for some topics and not enough details for others).If you already have some base knowledge, you might like this:https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~janh&#x2F;courses&#x2F;411&#x2F;18&#x2F;schedule.htmlI particularly liked how they introduced SSA form.More advanced topics:https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~15745&#x2F;handouts.html reply kopecs 11 hours agorootparentI also disliked the Dragon Book as a student. I found it to have too much of an emphasis on lexing&#x2F;parsing and not enough discussion for optimizations&#x2F;analysis for my liking. I liked Advanced Compiler Design and Implementation by Muchnick, although it does have some warts (ICAN; less discussion of SSA than I would&#x27;ve liked) and I think it is a bit dated now.FYI: replacing the 18 with 23 in your link to 411 gets you a slightly updated version. reply tomcam 15 hours agorootparentprevFantastic resource, thank you. reply freedomben 16 hours agoparentprevSame, that&#x27;s been my hope too. Stuff is getting so complex nowadays with modern microcode&#x2F;firmware and such, and there&#x27;s so many things that seem like \"magic\" to me. Feeling like something is \"magic\" is my internal signal that somebody figured out some clever way to get around what I think is&#x2F;was the limitations, and I love discovering people&#x27;s clever hacks. Recently been reading about Fake Bass (which is how small speakers seemingly violate the laws of physics by producing bigger bass than they are capable of) and how they accomplish this by (ab)using harmonics to trick the brain into hearing deeper notes than are actually tere. Fascinating stuff! reply jdwithit 6 hours agoprevApart from the actual book content, I enjoy your appropriately retro and minimalist web design. It&#x27;s giving me a huge nostalgia hit. Fond memories of hand crafting my own sites with a 6 inch thick book titled something like \"HTML 3.2 UNLEASHED!!!!\" on my desk :) reply AlexeyBrin 18 hours agoprevThis looks really interesting, however a disadvantage is that the reader needs to know or learn a new programming language first T3X. I wonder if one could start from scratch on a CP&#x2F;M system: write and develop the compiler on a retro system that has no connection to the outside world except the keyboard and display. reply nils-m-holm 17 hours agoparent> however a disadvantage is that the reader needs to know or learn a new programming language firstThis is a good point, and I have thought about it a lot before starting the book. What finally made my choose T3X is that its compiler is much smaller[1] than my smallest C-subset compiler and (IMHO) T3X is easier to learn or understand.[1] SubC: 3815 lines, T3X&#x2F;0: 2330 lines.Of course you could start on CP&#x2F;M without any outside tools, but then you would have to write your bootstrapping compiler in assembly language. Time-consuming, but certainly manageable. I doubt that it would be an interesting reading, though. reply JonChesterfield 14 hours agorootparentYou&#x27;d have some enthusiastic readers for writing compilers in assembly. Especially if you went down the route of progressively more capable assemblers. But \"some\" might be fewer than five. reply ramilefu 8 hours agorootparentMake that six! I’m in! reply Max-q 14 hours agoprevThis comment is not ment to be negative, just some insight that might be valuable.I read the free chapter. One thing I noticed right away was that I think some things can be hard for people with not so much knowledge about the topic: under each headline, it explains a concept from the ground up, no knowledge required. Like \"the syntax of a language is...\". But just a few sentences in, advanced topics are touched, like assembly instructions, not explained. It feels a bit like \"the curse of knowledge\", where it&#x27;s hard to know what the other party knows. But if the reader needs to learn what syntax means, they will probably not understand the next sentences.So, I think more consistency could improve the product.This is of course just my meaning and interpretation of the text, it might not be relevant. But maybe something to have in mind for your next masterpiece :) reply nils-m-holm 14 hours agoparent> under each headline, it explains a concept from the ground up, no knowledge required. Like \"the syntax of a language is...\". But just a few sentences in, advanced topics are touched, like assembly instructions, not explained.As the blurb of the book states: no prior knowledge in the field of compiler construction is required, but the reader should be familiar with at least one procedural language and one assembly language. So I thought it would be OK to assume that the reader knows about things like assembly instructions.Then the appendix of the book has a short introduction to Z80 assembly (which still assumes that you know the basics of assembly language).Every books starts somewhere. It would be hard to write a compiler construction book and assuming zero knowledge about computer programming.I am not saying that the curse of knowledge is not a thing, though, so I will definitely keep this in mind! reply eterps 17 hours agoprevIt would also be interesting to have a book on writing your own CP&#x2F;M-like OS. reply retrac 16 hours agoparentThat&#x27;s Andrew Tanenbaum&#x27;s Operating Systems Design and Implementation.Now, yes, that shows you how to write a Unix-like microkernel OS; just skip everything except the file system chapter. And don&#x27;t follow the advice about tree data structures. Just use flat tables, and don&#x27;t bother to implement exact file sizes. Presto: CP&#x2F;M.(I prefer the 2nd edition. The 3rd edition needlessly complicates IMO, mostly so the demo Minix code will work on a late 1990s PC instead of a 1980s PC.) reply thinkmassive 19 hours agoprevhttps:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231210141834&#x2F;http:&#x2F;&#x2F;t3x.org&#x2F;t3... reply freedomben 16 hours agoprev [–] Why such a focus on retro computing? As an oldie I think it&#x27;s cool (though a bit impractical learning the parts that aren&#x27;t applicable to modern stuff), but my son is interested in learning operating systems, compilers, etc, and I could never get him to use something so \"outdated.\"To be clear, I&#x27;m not attempting to criticize with this question (my personal opinion is write about what interests you, even if nobody else will care), I&#x27;m assuming you choose older targets for a reason, and would like to undestand those reasons :-)I.e. Do you believe the retro targets to be a lot simpler and easier to understand, so people can iterate&#x2F;build in layers? Or do you just know the retro stuff better so it makes for a better book? reply nils-m-holm 15 hours agoparentMy personal perspective is that computing has become much more complicated than necessary in the past decades. Of course abstraction will always create complexity, and some of that can hardly be avoided, but in computing these days complexity is really off the charts.So for this book I chose a platform that is easy to understand and does not make you wade through tons of abstractions that are only loosely related to compiler construction (e.g. ELF object file format). reply 082349872349872 14 hours agorootparentHere&#x27;s one way to avoid going into detail on ELF: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38592000\"(nearly) constant\" means you can pick either (a) a constant blob, at the cost of a fixed image size, à la COM, or (b) patch up length (one or two places, iirc) if you&#x27;re feeling fancy. reply nils-m-holm 14 hours agorootparentIn one of my other compiler books (http:&#x2F;&#x2F;t3x.org&#x2F;t3x&#x2F;book.html) I just use a template for the ELF header, but I still think it adds too much complexity. One reader complained about it. reply 082349872349872 13 hours agorootparentfound it: elfheader() in https:&#x2F;&#x2F;t3x.org&#x2F;t3x&#x2F;t.t.htmlfwiw, I think you commented it very nicely; de gustibus!Did you come up with if vs. ie ... else ... independently or inherit it from BCPL? reply nils-m-holm 13 hours agorootparent> fwiw, I think you commented it very nicely;Thanks, I thought so, too, but I can also understand that the comments are not very helpful, if you know nothing about linkers, paging, and object files.I think I adopted IE&#x2F;ELSE from BCPL, but thought that IE is nicer than TEST, because it is itself short for If&#x2F;Else and because it looks almost like IF.> de gustibus!Funny, I just started to brush up my Latin! :) replyunoti 8 hours agoparentprevFor me the interest in retro computing is that the computer is so much simpler that it is indeed possible to understand. The set of assembly instructions is small enough that a person can understand every opcode fairly easily, the amount of memory is limited, and the list of things that that your installed ROM can do is relatively accessible. Contrast that to modern machines and operating systems, and no mere mortal can understand everything that is happening in the machine.Another thing to love about retro computers is how transferrable the knowledge is to modern machines. Once you know the essence of assembly language on a retro computer, you have a good basis for learning RISC-V or something else modern.Depending on where your son is at, he might enjoy this video where I explain the essence of assembly language-- it&#x27;s the video I wished someone had shown to me when I was a kid and thought I wasn&#x27;t smart enough to learn it.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ep7gcyrbutA reply Archbtw97543 15 hours agoparentprev [–] Newer compilers or other low level programs have become extremely complex. There a loads of features or optimizations which are not strictly necessary but still add a lot of complexity Attemptting to explain how modern compilers work would most likely result in lots of confusion. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The book \"Write Your Own Retro Compiler\" provides source code for a self-hosting compiler for CP/M on the Z80 processor.",
      "The language used, T3X/0, is based on Pascal and BCPL and is easy to learn.",
      "The book covers topics such as lexical analysis, syntax analysis, code generation, optimizations, BDOS interface, and the run time library."
    ],
    "commentSummary": [
      "The author has released a new book on code generation for CP/M on the Z80 processor, focusing on visibility issues.",
      "There is a suggestion to create a book on Forth and explore the use of Mastodon for increased visibility.",
      "Discussions revolve around retro computing, programming languages, and compiler construction, requiring prior knowledge in compiler construction and familiarity with procedural and assembly languages.",
      "The book highlights the simplicity and transferability of knowledge in older computer systems while acknowledging the complexity of modern compilers."
    ],
    "points": 303,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1702217905
  },
  {
    "id": 38596953,
    "title": "Greg Technology's GPT-4 Remake of Google Gemini Demo: Bringing Reality to the Fiction",
    "originLink": "https://sagittarius.greg.technology/",
    "originBody": "A Remake of the Google Gemini Fake Demo, Except Using GPT-4 and It's Real Please see below for a (real) demo. All the code is in this repo! Cheers. Made by Greg Technology.",
    "commentLink": "https://news.ycombinator.com/item?id=38596953",
    "commentBody": "I Remade the Fake Google Gemini Demo, Except Using GPT-4 and It&#x27;s RealHacker NewspastloginI Remade the Fake Google Gemini Demo, Except Using GPT-4 and It&#x27;s Real (greg.technology) 295 points by gregsadetsky 7 hours ago| hidepastfavorite70 comments phire 6 hours agoThe \"magic\" of the fake Gemini demo was the way it seemed like the LLM was continually receiving audio + video input and knew when to jump in with a response.It appeared to be able to wait until the user had finished the drawing, or even jumping in slightly before the drawing finished. At one point the LLM was halfway through a response and then saw the user was now colouring the duck in blue, and started talking about how the duck appearing to be blue. The LLM also appeared to know when a response wasn&#x27;t needed because the user was just agreeing with the LLM.I&#x27;m not sure how many people noticed that on a conscious level, but I positive everyone noticed it subconsciously, and felt the interaction was much more natural, and much more advanced than current LLMs.-----------------Checking the source code, the demo takes screenshots of the video feed every 800ms, waits until the user finishes taking and then sends the last three screenshots.While this demo is impressive, it kind of proves just how unnatural it feels to interact with an LLM in this manner when it doesn&#x27;t have continuous audio-video input. It&#x27;s been technically possible to do kind of thing for a while, but there is a good reason why nobody tried to present it as a product. reply gregsadetsky 5 hours agoparent100%.I made this demo in 2-3 hours, and I did use the \"wait until the dictation results are finalized\" technique which is safer (i.e. the dictation transcription is more robust) but slower.For another demo - https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fxS7OKh_4vc - I kept feeding the \"in progress\" transcription results into GPT and that was super super awesome & fast. It would just require more work to deal with all of the different timings going on (i.e. there&#x27;s the speech itself from the person, the time to transcribe, sending the request to GPT, \"sync&#x27;ing\" it to where the person is (mentally&#x2F;in their speech) at the point where GPT replies, etc.)But yeah. Real time&#x2F;continuous talk is absolutely where it&#x27;s at. Should GPT be available as a websocket...?! reply modeless 1 hour agorootparentI have a rough demo of real time continuous voice chat here, ~1 second response latency: https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7Basically it starts computing a response every time a word comes out of the speech recognizer, and if it is able to finish its response before it hears another word then it starts speaking. If more words come in then it stops speaking immediately; in other words, you can interrupt it. It feels so much more natural in conversation than ChatGPT&#x27;s voice mode due to the low latency and continuous listening with the ability to interrupt.There are a lot of things that need improvement. Most important is probably that the speech recognition system (Whisper) wasn&#x27;t designed for real time and is not that reliable or efficient in a real time mode. I think some more tweaking could improve reliability considerably. But also very important is that it doesn&#x27;t know when not to respond. It will always jump in if you stop speaking for a second, and it will always try to get the last word. A first pass at fixing that would be to fine tune a language model to predict whose turn it is to speak.There are also a lot of things that this architecture will never be able to do. It will never be able to correct your pronunciation (e.g. for language learning), it will never be able to identify your emotions based on vocal cues or express proper emotions in its voice, it will never be able to hear the tone of a singing voice or produce singing itself. The future is in eliminating the boundaries between speech-to-text and LLM and text-to-speech, with one unified model trained end-to-end. Such a system would be able to do everything I mentioned and more, if trained on enough data. And further integrating vision will help with conversation too, allowing it to see the emotions on your face and take conversational cues from your gaze direction and hand gestures, in addition to all the other obvious things you can do with vision such as chat about something the camera can see or something displayed on your screen. reply thom 1 hour agorootparentWhat&#x27;s the horizon after which you reset the input instead of appending to it? Does that happen if the user lets the system finish speaking? reply modeless 1 hour agorootparentGreat question. Right now that happens, somewhat arbitrarily, if the user lets the system finish speaking the first sentence of its response. If the user interrupts before that, then it&#x27;s considered a continuation of the previous input. If the user interrupts after that, it&#x27;s still an interruption (and, importantly, the language model&#x27;s response must be truncated in the conversation context because the user didn&#x27;t hear it all), but it starts a new input to the LLM. This could be handled better as well. Basically any heuristics like this that are in the system should eventually be subsumed into the AI models so that they can be responsive to the conversation context. reply TaylorAlexander 4 hours agoparentprevYeah my friend and I were just talking about continuous stream input multimodal LLMs. Does anyone know if there is a technical limitation preventing continuous stream input data? Like it’s listening to you practice guitar and then when you get to a certain point it says “okay let’s go back and practice that section again”. It seems the normal approach of next token prediction falls flat when there is a continuous stream of tokens and it only sometimes needs to produce output.What is that type of input called in the literature and what research has been done on it? Thanks! reply profile53 3 hours agorootparentAt a purely technical level, no, as long as the model can output a null token. E.g. imagine training using a transcript of two people talking. What would be a single text token is a tuple of two tokens, one per person. Each segment where a person is not talking is a series of null tokens, one per ‘tick’ of time. In an actual conversation, one token in the tuple is user input and one is GPT prediction. Just disregard the user half of the tuple when determining whether the GPT should ‘speak’.The real world challenge is threefold. First, null tokens would be massively over represented in training and by extent, in outputs. Second, at a computational level, outputting a continuous stream of tokens would be absurdly expensive. Third, there is not nearly as much training data of interspersed conversations as of monologues (e.g. research papers, this comment, etc.). reply TaylorAlexander 3 hours agorootparentYeah it seems the notion of time is sort of not built in conceptually to current systems. You could pick a fixed time constant like 0.1 seconds or 1 second, but it&#x27;s clear that it&#x27;s sort of missing something more fundamental. reply radarsat1 2 hours agorootparentI think if the same LLM were trained on audio and video input instead of text, and produced audio output, including silence tokens, then the notion of time would get \"built in\". Audio continuation without translation to text has been shown to work. Mixing it with text is also possible. But all this would require a massive network that maybe even be difficult for the world&#x27;s biggest companies to train and serve at any kind of scale. So it&#x27;s more of an engineering problem than a theoretical one imho.Also imho, I think until the context&#x2F;memory problem is fully solved we won&#x27;t really see the AI as having any kind of agency. But continuous, low latency interaction would certainly feel like a step towards that. reply ycdxvjp 38 minutes agoparentprevAs a deaf person I have been watching \"live\" speech recognition demos for 20-30 years. All look great. Using it in day to day life is crazy cause if you have 1 mistake per 10 words it builds up over time to be supremely annoying. reply og_kalu 6 hours agoparentprevI think probably training on pause tokens or something similar would be the key to something like this. Maybe it&#x27;s not even necessary. Maybe if you just tell GPT-4 to output something like .... every time it thinks it should wait for a response (you wouldn&#x27;t need to wait for the user to finish then), things would be a lot smoother. reply phire 6 hours agorootparentYes, you could probably fine-tune (or even zero-shot) a LLM to handle the \"knowing when to jump in\" use case.The real problem is that it&#x27;s simply too computationally expensive to continually feed audio and video into it one of these massive LLMs just in case it might decide to jump in.I was wondering if you could train a lightweight monitoring model that continually watching the audio&#x2F;video input and only tried to work out when the full-sized LLM might want to jump in and generate a response. reply bbarnett 4 hours agorootparentAs the human brain is a clump if regions all interconnected and interacting, for example, one may focus their attention elsewhere until their name is called, having a ight model wait for an important queue makes sense more than fiscally.One time I was so distracted, I missed an entire paragraph someone said to me, walked to my car, drove away, and 5 minutes later processed it. reply nextworddev 5 hours agoparentprevOne easy improvement would be to stop the video capture automatically via a combination of silence detection and motion detection reply godelski 6 hours agoprevI don&#x27;t get why companies lie like this. How much do they have to gain? It seems like they actually have a lot to lose.What&#x27;s crazy to me is that these tools are wildly impressive without the hype. As a ML researcher, there&#x27;s a lot of cool things we&#x27;ve done but at the same time almost everything I see is vastly over hyped from papers to products. I think there&#x27;s a kinda race to the bottom we&#x27;ve created and it&#x27;s not helpful to any of us except maybe in the short term. Playing short term games isn&#x27;t very smart, especially for companies like Google. Or maybe I completely misunderstand the environment we live in.But then again, with the discussions in this thread[0] maybe there&#x27;s a lot of people so ethically bankrupt that they don&#x27;t even know how what they&#x27;re doing is deceptive. Which is an entirely different and worse problem.[0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38559582 reply jay-barronville 21 minutes agoparent> What&#x27;s crazy to me is that these tools are wildly impressive without the hype.My wife and I were talking about this yesterday, and I made this exact point! I told her I’m convinced Google was deceptive like this for the Wall Street crowd and normies, because to techies and researchers who actually understand AI, the extra BS is unnecessary if the technology is legitimately impressive. reply CSMastermind 3 hours agoparentprevBecause the same day they released the video our CEO was messaging me saying we have to get on Google&#x27;s new stuff because it&#x27;s so much better than GPT-4.I said I was skeptical of the demo but, like all developments in the field, will try it out once they release it. reply godelski 3 hours agorootparentThis also seems like equally poor decision making. Wouldn&#x27;t you want to at least try things out before you make a hard switch? Chasing hype is costly. reply movedx 2 hours agorootparentWelcome to IT.... no seriously, this is how a lot of executives behave in IT. reply nextworddev 5 hours agoparentprevGoogle stock rallied 5% ish the after the demo (though the stock didn’t move immediately). Then it gave back about 1% once the news broke that it was faked reply lolinder 5 hours agorootparentOne question I&#x27;ve long had is why short-term changes in stock prices seem to matter so much to companies like these. Is it just that the short-term changes are seen as harbingers of longer-term trends, or is there a concrete reason to play games to get temporary boosts to stock price? reply godelski 3 hours agorootparentI&#x27;ve been using the term Goodhart&#x27;s Hell to describe what I see as a system of metric hacking and shortsightedness. I do think there&#x27;s systems encouraging people to meet metrics for metrics sake but not stopping to understand the metric or what it actually measures, and most importantly, how that differs from the actual goals. Because all metrics are proxies and far from complete, even for things that sound really simple. I think this is because we&#x27;ve gotten so good at measuring that we&#x27;ve innately forgotten about the noisiness of the system where when we weren&#x27;t good at measuring that was just forced upon us in a clear manner.But I really don&#x27;t get it either. One of the things that really sets humans apart from other animals is our capacity to perform long term planning and prediction. Why abandon that skill instead of exploiting it to its maximum potential? reply YetAnotherNick 4 hours agorootparentprevShort term changes definitely affect medium to long terms&#x27; price. Because at one level stock price is more like casino and isn&#x27;t actually related to the company&#x27;s performance. e.g. See the 5 year history of gamestop. Its price once increased due to random activity from redditors and its stock price is still increased due to that. reply godelski 5 hours agorootparentprevThat&#x27;s not a great answer. We need to know the counterfactual question of \"How much would google&#x27;s stock have rallied after a realistic demo was given?\" I would not have been surprised if the answer was also 5%. Almost certainly Google&#x27;s stock would have risen after announcing Gemini. There are other long term questions too like about how this feeds into growing dissent against Google and trust. But what the stock did is only a small part of a much bigger and far more important question.Edit: Can someone explain the downvotes? Is there a error in my response? I&#x27;m glad to learn but I&#x27;d appreciate a bit better feedback signal so that I can do so better instead of guessing. reply dkga 3 hours agorootparentEconomist here who studies exactly this type of counterfactual analysis. You are completely right: the effect of Gemini can only be estimated if we factor in what the Alphabet stock price would have been in the same time but in a world without Gemini. This is actually very standard in financial economics. This type of effect can be calculated with econometric techniques that compare before&#x2F;after for “treated” vs “untreated” units, but in instances such as these, where only one or a few units were affected, like Alphabet stock amongst hundreds of other companies, one could use techniques such as “synthetic controls”. The intuition is to use other companies’ data to estimate before Gemini how Alphabet stock prices move over time, and then use that relationship to estimate a post-Gemini version of no-Gemini Google. The difference between the actual stock price and that counterfactual is the effect of interest; whether it is a significant effect or just random noise can be established by a number of auxiliary statistical tests. For more info, see [0].[0] Abadie, A. (2021). Using synthetic controls: Feasibility, data requirements, and methodological aspects. Journal of Economic Literature, 59(2), 391-425. reply godelski 2 hours agorootparentWell I don&#x27;t mean with&#x2F;without Gemini, I mean without the deceptive marketing of Gemini vs had they counterfactually produced a non-deceptive marketing. Other than that nitpicking, I appreciate the backup and the source. Counterfactual testing is by no means easy, so good luck with your work! My gf is an economist but on the macro side. You guys have it tough. I&#x27;m a bit confused why people are disliking my comment but mostly that they are doing so without explanation lol. reply nextworddev 5 hours agorootparentprevBroad investor community was spooked by Gemini Ga being delayed to Q1 so this stunt was a good stop gap &#x2F; distraction reply yellow_postit 3 hours agorootparentAnd likely caused more long term harm since if they had to fake this they’re likely further behind reply dkga 3 hours agoparentprevRelatedly, I saw in the thread that people call these types of deceptions as “smoke and mirrors” or “dog and pony show”. What happened to “Potemkin”?! reply gwd 41 minutes agorootparentThe nice thing about \"Potemkin\" is that there&#x27;s a decent chance the video was also designed to fool their own CEOs (in response to an impossible request), just as the Potemkin Villages were used to fool the country&#x27;s own ruler. reply godelski 3 hours agorootparentprevI had never previously heard of that term but it does seem apt. I think idioms are often more cultural and can change rapidly while one might seem ubiquitous in your group it isn&#x27;t in another. Another term I think might be apt, but a bit less so, is snake oil or snake oils sells man. reply Austizzle 2 hours agorootparentprevPerhaps this is revealing my ignorance, but I&#x27;ve never even heard of potemkin before reply hackerlight 5 hours agoparentprev> Playing short term games isn&#x27;t very smart, especially for companies like Google. Or maybe I completely misunderstand the environment we live in.It could be the principal-agent problem. The agent (employee and management) is optimizing for short-term career benefits and has no loyalty to Google&#x27;s shareholders. They can quit after 3 years, so reputation damage to Google doesn&#x27;t matter that much. But the shareholders want agents to optimize for longer-term things like reputation. Aligning those incentives is difficult. Shareholders try with good governance and material incentives tied to the stock price with a vesting schedule, but you&#x27;re still going to get a level of disalignment.I suppose this is where a cult-like culture of mission alignment can deliver value. If you convince&#x2F;select your agents (employees) into actually believing in the mission, alignment follows from that. reply godelski 3 hours agorootparentYeah I think that makes some sense. But you would think the CEO and top execs of the company would be trying to balance these forces rather than letting one dominate. You need pressures for short term but you can&#x27;t abandon long term planning for short term optimization. Anyone who&#x27;s worked with basic RL systems should be keenly aware of that and I&#x27;m most certain they teach this in business school. I mean it&#x27;s not like these things don&#x27;t come up multiple times a year. reply hackerlight 2 hours agorootparentThere&#x27;s some other explanations too. Maybe they thought the deception would fly under the radar, so it was rational according to cost-benefit analysis given available information. Maybe they fell for the human psychological bias of overvaluing near-term costs&#x2F;benefits and undervaluing long-term costs&#x2F;benefits. Maybe some deception was used internally when the demo was communicated to senior execs. Maybe the ego of being second place to OpenAI was too much and the shame avoidance&#x2F;prestige seeking kicked in. reply RagnarD 5 hours agoparentprevGoogle screws up every business opportunity, including wantonly buying small successful businesses and killing them. Dishonesty is a fundamental part of the company. reply Racing0461 4 hours agoparentprevHere is the headline Business Today published just in case you wonder why businesses do this\"Google Gemini Outperforms Most Human Experts & GPT-4 I Artificial intelligence I Google’s DeepMind\".It&#x27;s all marketing. Same reason why satya publically postes sama + others are joining a new team at MSFT to continue should the openai thing not work out. reply godelski 3 hours agorootparentI&#x27;m not sure how that is really responding to my question with an explanation. I&#x27;m well aware that its marketing and I&#x27;d hope my comment makes that clear. The question is why oversell the product, and frankly by a lot. Because people are going to find out, I mean the intent is that they use it after all.I&#x27;m sure the marketing team can come up with good marketing that also isn&#x27;t deceitful. The question is why pull a con when you already got something of value that customers would legitimately buy? reply mobiuscog 2 hours agorootparent> I&#x27;m well aware that its marketing and I&#x27;d hope my comment makes that clear. The question is why oversell the product, and frankly by a lot.Most marketing sells the dream, not the reality. There are just many shades of grey (although 50 tends to sell well). reply godelski 2 hours agorootparentI&#x27;m still not sure how that is responding to my comment. Have I said something that makes me seem naive of the existence of snake oil salesmen? I&#x27;m actually operating under the assumption that my comment, and especially followup, explicitly demonstrate my awareness of this. reply parineum 5 hours agoparentprevI think it&#x27;s because, while I think these LLMs are incredibly interesting and can be very useful, they&#x27;re less than what the hype is and the valuations are based on the hype. reply sheepscreek 6 hours agoprevThank you for creating this demo. This was the point I was trying to make when the Gemini launch happened. All that hoopla for no reason.Yes - GPT-4V is a beast. I’d even encourage anyone who cares about vision or multi-modality to give LLaVA a serious shot (https:&#x2F;&#x2F;github.com&#x2F;haotian-liu&#x2F;LLaVA). I have been playing with the 7B q5_k variant last couple of days and I am seriously impressed with it. Impressed enough to build a demo app&#x2F;proof-of-concept for my employer (will have to check the license first or I might only use it for the internal demo to drive a point). reply ok_dad 6 hours agoparentI’ve been using llava via https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile which runs on any modern system. reply jart 2 hours agorootparentIt&#x27;s so great. I&#x27;ve been this vision model to rename all the files in my Pictures folder. For example, the one-liner: llamafile --temp 0 \\ --image ~&#x2F;Pictures&#x2F;lemurs.jpg \\ -m llava-v1.5-7b-Q4_K.gguf \\ --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \\ --grammar &#x27;root ::= [a-z]+ (\" \" [a-z]+)+&#x27; \\ -p $&#x27;### User: What do you see?### Assistant: &#x27; \\ --silent-prompt 2>&#x2F;dev&#x2F;nullsed -e&#x27;s&#x2F; &#x2F;_&#x2F;&#x27; -e&#x27;s&#x2F;$&#x2F;.jpg&#x2F;&#x27;Prints to standard output: a_baby_monkey_on_the_back_of_a_mother.jpgThis is something that&#x27;s coming up in the next llamafile release. You have to build from source to have the ability to use grammar and --silent-prompt on a vision model right now.Weights here: https:&#x2F;&#x2F;huggingface.co&#x2F;jartine&#x2F;llava-v1.5-7B-GGUF&#x2F;tree&#x2F;mainSauce here: https:&#x2F;&#x2F;github.com&#x2F;mozilla-Ocho&#x2F;llamafile reply swyx 6 hours agoprevhaha yes it was entirely possible with gpt4v. literally just screenshot and feed in the images and text in chat format, aka “interleaved”. made something similar at a hackathon recently. (https:&#x2F;&#x2F;x.com&#x2F;swyx&#x2F;status&#x2F;1722662234680340823). the bizarre thing is that google couldve done what you did, and we wouldve all been appropriately impressed, but instead google chose to make a misleading marketing video for the general public and leave the rest of us frustrated nerds to do the nasty work of having to explain why the technology isnt as seen on tv yet; making it seem somehow our faulti am curious about the running costs of something like this reply gregsadetsky 4 hours agoparentI made 77 requests to the GPT-vision API while developing&#x2F;demo&#x27;ing this, and that resulted in a $0.47 bill. Pretty reasonable! reply jankovicsandras 2 hours agorootparentHi Greg,Congratulations, great demo! The $0.47 bill seems reasonable for an experiment, but imagine someone doing a task of this complexity as a daily job - let&#x27;s say 100x times, or a little more than 4 hours - the bill would be $47&#x2F;day. It feels like there&#x27;s still an opportunity for a cheaper solution. Have you or someone else experimented with e.g. https:&#x2F;&#x2F;localai.io&#x2F; ? reply swyx 2 hours agorootparentif i did not have your comment history i&#x27;d have sworn you worked for localai.io reply iamleppert 1 hour agoprevI’ve recently been trying to actually use Google’s AI conversational translation app that was released awhile back and has many updates and iterations since.It’s completely unusable for real conversation. I’m actually in a situation where I could benefit from it and was excited to use it because I remember watching the demo and how natural it looked but was never able to actually try it myself.Now having used it, I went back and watched their original demo and I’m 100% convinced all or part of it was faked. There is just no way this thing ever worked. If they can’t manage to make conversational live translation work (which is a lot more useful than drawing a picture of a duck) I have high doubts about this new AI.Seems like the exact same situation to me. It’s insane to me how much nerve it must take to completely fake something like this. reply adtac 5 hours agoprev[tangential to this really cool demo] JPEG images being the only possible interface to GPT-4 feels wasteful. the human eye works the delta between \"frames\", not the image itself. I wonder if the next big step that would allow real-time video processing at high resolutions is to have the model&#x27;s internal state operate on keyframes and deltas similar to how video codecs like MPEG work. reply zwily 5 hours agoparentWhen Google talks about Gemini&#x27;s \"multi-modal\", they include \"video\" in the list of modes. It&#x27;s totally possible they don&#x27;t actually mean video, and just mean frames like in this demo. They haven&#x27;t elaborated on it anywhere that I&#x27;ve seen. reply dannyw 3 hours agorootparentTheir technical report clarifies that video is just a sequence of frames fed as images. reply dvaun 6 hours agoprevGreat demo, I laughed at the final GPT response too.Honestly: it would be fun to self-host some code hooked up to a mic and speakers to let kids, or whoever, play around with GPT4. I’m thinking of doing this on my own under an agency[0] I’m starting up on the side. Seems like a no-brainer as an application.[0]: https:&#x2F;&#x2F;www.divinatetech.com reply sibeliuss 5 hours agoprevLol at choosing the name Sagittarius, which is exactly across from Gemini in the Zodiac reply SilasX 5 hours agoparentI remember there was speculation that Facebook named their vaporware cryptocurrency Libra (later, “Diem”) as a jab at the longtime rival Winklevoss twins, who had started a crypto exchange called Gemini. I have no idea how astrologically clever that would be. reply zainhoda 6 hours agoprevWow, this is super cool! From the code it seems like the speech to text and text to speech are using the browser’s built-in features. I always forget those capabilities even exist! reply iandanforth 6 hours agoprevLooks like, again, this doesn&#x27;t have GPT-4 processing video as much as a stack of video frames, concatenated and sent as a single image. But much closer to real! reply riwsky 5 hours agoparentI just found out it gets worse: turns out GPT-4 isn&#x27;t processing images so much as arrays of pixels!And worse: turns out GPT-4 isn&#x27;t processing pixels so much as integers representing in a position in some color space like RGB!And worse! turns out GPT-4 isn&#x27;t processing integers so much as series of ones and zeroes!Now that this is public knowledge, I&#x27;m willing to bet this was the ugly \"less than candid\" truth that the board sacked Sam Altman over. reply zarzavat 5 hours agorootparentAs they say, quantity has a quality all of its own. If the framerate of a video is so slow as to be a slideshow, then it’s arguably not video anymore. Video has a connotation of appearing temporally continuous to the naked eye. reply trescenzi 6 hours agoparentprevHow does a video differ from a stack of video frames? Isn’t that all a video is? A bunch of images stuck together and played back quickly? reply og_kalu 6 hours agorootparentI&#x27;d guess you&#x27;d miss any audio that way. But otherwise, yeah a video is a stack of images. reply zwily 5 hours agorootparentprevYou could say that this demo is processing a 2.4s video that is 1.25fps. reply adtac 6 hours agoparentprevIs \"it&#x27;s just processing frames\" the new \"it&#x27;s just predicting the next token\"? reply topspin 1 hour agorootparentNothing new here at all: trivializing the intellectual achievements of machines is SOP. This will continue until machines have surpassed every conceivable benchmark. At that point we&#x27;ll be left with only our epic hubris. reply fortunefox 6 hours agoparentprevSince audio is processed separately this isn&#x27;t just close to real. it is real. After all what is video if not a stack of frames! :D reply ShamelessC 6 hours agoparentprevThe video is an actual live demo without any editing or other tricks involved and even includes reasonable mistakes and the code used. It is not close to real, it&#x27;s just real. reply ShamelessC 6 hours agoprevSad state of affairs for Google. reply paul7986 6 hours agoparentIndeed they&#x27;ve been sad starting in 2010 and on... (maybe before)... all the projects they kill.. their IP theft, them doing evil, etc reply op00to 6 hours agoprevVery cool! reply jakderrida 6 hours agoprev [–] Lmao! So, presumably, they could have hired Greg to improvise almost the exact same demonstration, but with evidence it works. I don&#x27;t know how much Greg costs, but I&#x27;ll bet my ass it&#x27;s less than the cost in investor sentiment after getting caught committing fraud. Not saying you&#x27;re cheap. Just cheaper. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A remake of the Google Gemini fake demo has been created using GPT-4, a powerful language model.",
      "The summary claims that this remake is an actual demo, with the source code available in a specific repository.",
      "The project is credited to Greg Technology."
    ],
    "commentSummary": [
      "The summary covers various topics including limitations and improvements needed for speech recognition systems, challenges of streaming input data, and skepticism towards AI technologies and deceptive marketing practices.",
      "It discusses the role of deceptive practices, Potemkin Villages, and the principal-agent problem.",
      "Users in the comment section express skepticism about Google Gemini and discuss the disconnect between marketing claims and actual product value. A company called Greg is mentioned, along with a discussion about the authenticity of a live demo."
    ],
    "points": 295,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1702261059
  },
  {
    "id": 38593616,
    "title": "Mistral AI Raises €450M, Valuation Soars to $2B",
    "originLink": "https://www.unite.ai/paris-based-startup-and-openai-competitor-mistral-ai-valued-at-2-billion/",
    "originBody": "Investments Paris-based Startup and OpenAI Competitor Mistral AI Valued at $2 Billion Published 2 days ago on December 9, 2023 By Alex McFarland In a significant development for the European artificial intelligence sector, Paris-based startup Mistral AI has achieved a noteworthy milestone. The company has successfully secured a substantial investment of €450 million, propelling its valuation to an impressive $2 billion. This funding round marks a pivotal moment, not only for Mistral AI but also for the burgeoning European AI landscape, signifying the region's increasing prominence in the global AI arena. Leading the charge in this investment round is Andreessen Horowitz, a prominent name in the venture capital world, demonstrating a strong vote of confidence in Mistral AI's potential. Joining the fray are tech giants Nvidia Corp and Salesforce, contributing an additional €120 million in convertible debt. This diverse array of investors, encompassing both traditional venture capital and major tech corporations, underscores the wide-ranging appeal and potential of Mistral AI's technology and vision. This influx of capital is a testament to Mistral AI's innovative approach and its perceived potential to disrupt the AI industry. With this substantial financial backing, Mistral AI is poised to advance its research and development, expand its reach, and further cement its position as a leading player in the AI domain. The scale of this investment round also reflects the growing recognition of the strategic importance of AI technologies and the increasing competition to lead in this transformative field. Technological Advancements and Market Impact Mistral AI stands at the forefront of innovation with its flagship product, Mistral 7B, a large language model (LLM) renowned for its efficiency and advanced capabilities. Released under the open-source Apache 2.0 license, Mistral 7B represents a significant leap in AI technology, characterized by its customized training, tuning, and data processing methods. What sets Mistral 7B apart is its ability to compress knowledge and facilitate deep reasoning capacities, even with fewer parameters compared to other models in the market. This optimized approach not only enhances the model's performance but also contributes to sustainability by reducing training time, costs, and environmental impact. The successful deployment of Mistral 7B has positioned Mistral AI as a key player in the AI market and a competitor to OpenAI. Its impact extends across various industries, offering potential transformations in fields such as healthcare, education, finance, and manufacturing. The company's ability to provide high-performance, scalable solutions is poised to impact how these sectors leverage AI for innovation and efficiency. European AI Landscape and Competitive Edge Mistral AI's recent funding round is a clear indicator of Europe's rapidly growing stature in the global AI landscape. Historically, European ventures in AI have lagged behind their counterparts in the US and Asia in terms of investment and innovation. However, Mistral AI's success, alongside other significant investments, marks a decisive shift, showcasing Europe's rising potential and commitment to AI innovation. In the competitive arena of generative AI, Mistral AI distinguishes itself with its open-source approach and focus on creating scalable and efficient models. This strategy sets it apart from established giants such as OpenAI, Google AI, and DeepMind, offering a unique value proposition to the market. By prioritizing accessibility and efficiency, Mistral AI not only contributes to the democratization of AI technology but also positions itself as a formidable competitor in the global AI race. The trajectory of Mistral AI and the burgeoning European AI sector signals a vibrant and dynamic future for AI development. With substantial investments pouring into European AI startups, the region is rapidly catching up and carving out its niche in the highly competitive and ever-evolving field of artificial intelligence. Related Topics:fundinginvestingstartup Don't Miss .AI Domain Names Skyrocket in Value With Recent Record Sales Alex McFarland Alex McFarland is a Brazil-based writer who covers the latest developments in artificial intelligence. He has worked with top AI companies and publications across the globe. You may like How an Academic Partner Can Help You Validate Your Startup’s Product AccelData Acquires Bewgle: A Major Move in AI Data Pipeline Visibility Inflection AI Secures $1.3 Billion Funding Led by Tech Titans and Industry Giants Dataloop’s New AI-Powered Tech Promotes Safer Online Environment AI in the Stock Market: Is It Better Than Humans? Cogniteam Raises $5.6 Million for AI Robotics Platform Recent Posts Paris-based Startup and OpenAI Competitor Mistral AI Valued at $2 Billion Google Accused of Misleading With Gemini Announcement Video Revolutionizing Healthcare: Exploring the Impact and Future of Large Language Models in Medicine Vivek Sharma, CEO & Co-Founder of Movable Ink – Interview Series UltraFastBERT: Exponentially Faster Language Modeling",
    "commentLink": "https://news.ycombinator.com/item?id=38593616",
    "commentBody": "Mistral AI Valued at $2BHacker NewspastloginMistral AI Valued at $2B (unite.ai) 285 points by marban 15 hours ago| hidepastfavorite215 comments mark_l_watson 15 hours agoThere is a lot of hype around LLMs, but (BUT!) Mistral well deserves the hype. I use their original 7B model, as well as some derived models, all the time. I can’t wait to see what they release next (which I expect to be a commercial product, although the MoE model set they just released is free).Another company worthy of some hype is 01.AI which released their Yi-34B model. I have been running Yi locally on my Mac (use “ ollama run yi:34b”) and it is amazing.Hype away Mistral and 01.AI, hype away… reply p1esk 14 hours agoparentHow do these small models compare to gpt4 for coding and technical questions?I noticed that gpt3.5 is practically useless to me (either wrong or too generic), while gpt4 provides a decent answer 80% of the time. reply modeless 14 hours agorootparentThey are not close to GPT-4. Yet. But the rate of improvement is higher than I expected. I think there will be open source models at GPT-4 level that can run on consumer GPUs within a year or two. Possibly requiring some new techniques that haven&#x27;t been invented yet. The rate of adoption of new techniques that work is incredibly fast.Of course, GPT-5 is expected soon, so there&#x27;s a moving target. And I can&#x27;t see myself using GPT-4 much after GPT-5 is available, if it represents a significant improvement. We are quite far from \"good enough\". reply vitorgrs 11 hours agorootparentI believe one of the problems that OSS models need to solve, is... dataset. All of them lack a good and large dataset.And this is most noticiable if you ask anything that is not in English-American-ish. reply p1esk 14 hours agorootparentprevI’m both excited and scared to think about this “significant improvement” over GPT-4.It can make our jobs a lot easier or it can take our jobs. reply rmbyrro 13 hours agorootparentI expect the demand for SWE to grow faster than productivity gains. reply Der_Einzige 7 hours agorootparentThe idea that demand scales to fill supply doesn’t work when supply becomes effectively infinite. Induction from the past is likely wrong in this case reply __loam 12 hours agorootparentprevLLMs are going to spit out a lot of broken shit that needs fixing. They&#x27;re great at small context work but full applications require more than they&#x27;re capable of imo. reply p1esk 9 hours agorootparentEven if so, the next gen model will fix it. reply stavros 13 hours agorootparentprevIsn&#x27;t that the same? At some point, your job becomes so easy that anyone can do it. reply Spivak 12 hours agorootparentIt&#x27;s weird for programmers to be worried about getting automated out of a job when my job as a programmer is basically to try as hard as I can to automate myself out of a job. reply Der_Einzige 7 hours agorootparentYou’re supposed to automate yourself out but not tell anyone. Didn’t you see that old simpsons episode from the 90s about the self driving trucks? The drivers rightfully STFU about their innovation and cashed in on great work life balance and Homer ruined it by blabbered about it to everyone, causing the drivers to try to go after him.We are trying to keep SWE salaries up, and lowering the barrier to entry will drop them. reply 0xDEF 13 hours agorootparentprev>I think there will be open source models at GPT-4 level that can run on consumer GPUs within a year or two.There is indeed already open source models rivaling ChatGPT-3.5 but GPT-4 is an order of magnitude better.The sentiment that GPT-4 is going to be surpassed by open source models soon is something I only notice on HN. Makes me suspect people here haven&#x27;t really tried the actual GPT-4 but instead the various scammy services like Bing that claim they are using GPT-4 under the hood when they are clearly not. reply rmbyrro 13 hours agorootparentMakes me suspect you don&#x27;t follow HN user base very closely. reply refulgentis 11 hours agorootparentprevYou&#x27;re 100% right and I apologize that you&#x27;re getting downvoted, in solidarity I will eat downvotes with you.HNs funny right now because LLMs are all over the front page constantly, but there&#x27;s a lot of HN \"I am an expert because I read comments sections\" type behavior. So many not even wrong comments that start from \"I know LLaMa is local and C++ is a programming language and I know LLaMa.cpp is on GitHub and software improves and I&#x27;ve heard of Mistral.\" reply OfSanguineFire 14 hours agorootparentprevCurious thought: at some point a competitor’s AI might become so advanced, you can just ask it to tell you how to create your own, analogous system. Easier than trying to catch up on your own. Corporations will have to include their own trade secrets among the things that AIs aren’t presently allowed to talk about like medical issues or sex. reply rmbyrro 13 hours agorootparentIt might work for fine-tuning an open model to a narrow use case.But creating a base model is out of reach. You need an order of probably hundreds of millions of $$ (if not billion) to get close to GPT 4. reply Xenoamorphous 13 hours agorootparentAs someone who doesn’t know much about how these models work or are created I’d love to see some kind of breakdown that shows what % of the power of GPT4 is due to how it’s modelled (layers or whatever) vs training data and the computing resources associated with it. reply mensetmanusman 7 hours agorootparentThis isn&#x27;t precisely knowable now, but it might be something academics figure out years from now. Of course, first principles of &#x27;garbage in garbage out&#x27; would put data integrity very high, the LLM code itself is supposedly not even 100k lines of code, and the HW is crazy advanced.so the ordering is probably data, HW, LLM modelThis also fits the general ordering ofdata = all human knowledge HW = integrated complexity of most technologists LLM = small teamStill requires the small team to figure out what to do with the first two, but it only happened now because the HW is good enough.LLMs would have been invented by Turing and Shannon et al. almost certainly nearly 100 years ago if they had access to the first two. reply MacsHeadroom 9 hours agorootparentprevBy % of cost it&#x27;s 99.9% compute cost and 0.01% data costs.In terms of \"secret sauce\" it&#x27;s 95% data quality and 5% architectural choices. reply Der_Einzige 7 hours agorootparentprevModel merging is easy, and a unique model merge may be hard to replicate if you don’t know the original recipe.Model merging can create truly unique models. Love to see shit from ghost in the shell turn into real lifeYes training a new model from scratch is expensive, but creating a new model that can’t be replicated by fine tuning is easy reply taneq 10 hours agorootparentprevThat’s true now, but maybe GPT6 will be able to tell you how to build GPT7 on an old laptop, and you’ll be able to summon GPT8 with a toothpick and three cc’s of mouse blood. reply p1esk 14 hours agorootparentprevHow to create my own LLM?Step 1: get a billion dollars.That’s your main trade secret. reply chongli 13 hours agorootparentWhat is inherent about AIs that requires spending a billion dollars?Humans learn a lot of things from very little input. Seems to me there&#x27;s no reason, in principle, that AIs could not do the same. We just haven&#x27;t figured out how to build them yet.What we have right now, with LLMs, is a very crude brute-force method. That suggests to me that we really don&#x27;t understand how cognition works, and much of this brute computation is actually unnecessary. reply michaelt 11 hours agorootparentMaybe not $1 billion, but you&#x27;d want quite a few million.According to [1] a 70B model needs $1.7 million of GPU time.And when you spend that - you don&#x27;t know if your model will be a damp squib like Bard&#x27;s original release. Or if you&#x27;ve scraped the wrong stuff from the internet, and you&#x27;ll get shitty results because you didn&#x27;t train on a million pirated ebooks. Or if your competitors have a multimodal model, and you really ought to be training on images too.So you&#x27;d want to be ready to spend $1.7 million more than once.You&#x27;ll also probably want $$$$ to pay a bunch of humans to choose between responses for human feedback to fine-tune the results. And you can&#x27;t use the cheapest workers for that, if you need great english language skills and want them to evaluate long responses.And if you become successful, maybe you&#x27;ll also want $$$$ for lawyers after you trained on all those pirated ebooks.And of course you&#x27;ll need employees - the kind of employees who are very much in demand right now.You might not need billions, but $10M would be a shoestring budget.[1] https:&#x2F;&#x2F;twitter.com&#x2F;moinnadeem&#x2F;status&#x2F;1681371166999707648 reply chongli 10 hours agorootparentAnd when you spend that - you don&#x27;t know if your model will be a damp squib like Bard&#x27;s original release. Or if you&#x27;ve scraped the wrong stuff from the internet, and you&#x27;ll get shitty results because you didn&#x27;t train on a million pirated ebooks.This just screams to me that we don’t have a clue what we’re doing. We know how to build various model architectures and train them, but if we can’t even roughly predict how they’ll perform then that really says a lot about our lack of understanding.Most of the people replying to my original comment seem to have dropped the “in principle” qualifier when interpreting my remarks. That’s quite frustrating because it changes the whole meaning of my comment. I think the answer is that there isn’t anything in principle stopping us from cheaply training powerful AIs. We just don’t know how to do it at this point. reply pixl97 12 hours agorootparentprev>Humans learn a lot of things from very little inputAnd also takes 8 hours of sleep per day, and are mostly worthless for the first 18 years. Oh, also they may tell you to fuck off while they go on a 3000 mile nature walk for 2 years because they like the idea of free love better.Knowing how birds fly ready doesn&#x27;t make a useful aircraft that can carry 50 tons of supplies, or one that can go over the speed of sound.This is the power of machines and bacteria. Throwing massive numbers at the problem. Being able to solve problems of cognition by throwing 1GW of power at it will absolutely solve the problem of how our brain does it with 20 watts in a faster period of time. reply nemothekid 13 hours agorootparentprevIf we knew how to build humans for cheap, then it wouldn&#x27;t require spending a billion dollars. Your reasoning is circular.It&#x27;s precisely because we don&#x27;t know how to build these LLMs cheaply that one must so spend so much money to build them. reply chongli 12 hours agorootparentThe point is that it&#x27;s not inherently necessary to spend a billion dollars. We just haven&#x27;t figured it out yet, and it&#x27;s not due to trade secrets.Transistors used to cost a billion times more than they do now [1]. Do you have any reason to suspect AIs to be different?[1] https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;how-much-did-early-transistors-cos... reply jryle70 11 hours agorootparent> Transistors used to cost a billion times more than they do nowHowever you would still need billions of dollars if you want state of the art chips today, say 3nm.Similarly, LLM may at some point not require a billion dollars, you may be able to get one, on par or surpass GPT4, easily for cheap. The state of the art AI will still require substantial investment. reply janalsncm 12 hours agorootparentprevBecause that billion dollars gets you the R&D to know how to do it?The original point was that an “AI” might become so advanced that it would be able to describe how to create a brain on a chip. This is flawed for two main reasons.1. The models we have today aren’t able to do this. We are able to model existing patterns fairly well but making new discoveries is still out of reach.2. Any company capable of creating a model which had singularity-like properties would discover them first, simply by virtue of the fact that they have first access. Then they would use their superior resources to write the algorithm and train the next-gen model before you even procured your first H100. reply janalsncm 12 hours agorootparentprevThe limiting factor isn’t knowledge of how to do it, it is GPU access and RLHF training data. reply CSMastermind 13 hours agorootparentprevMistral&#x27;s latest just released model is well below GPT-3 out of the box. I&#x27;ve seen people speculate that with fine-tuning and RLHF you could get GPT-3 like performance out of it but it&#x27;s still too early to tell.I&#x27;m in agreement with you, I&#x27;ve been following this field for a decade now and GPT-4 did seem to cross a magical threshold for me where it was finally good enough to not just be a curiosity but a real tool. I try to test every new model I can get my hands on and it remains the only one to cross that admittedly subjective threshold for me. reply espadrine 12 hours agorootparent> Mistral&#x27;s latest just released model is well below GPT-3 out of the boxThe early information I see implies it is above. Mind you, that is mostly because GPT-3 was comparatively low: for instance its 5-shot MMLU score was 43.9%, while Llama2 70B 5-shot was 68.9%[0]. Early benchmarks[1] give Mixtral scores above Llama2 70B on MMLU (and other benchmarks), thus transitively, it seems likely to be above GPT-3.Of course, GPT-3.5 has a 5-shot score of 70, and it is unclear yet whether Mixtral is above or below, and clearly it is below GPT-4’s 86.5. The dust needs to settle, and the official inference code needs to be released, before there is certainty on its exact strength.(It is also a base model, not a chat finetune; I see a lot of people saying it is worse, simply because they interact with it as if it was a chatbot.)[0]: https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;multi-task-language-understa...[1]: https:&#x2F;&#x2F;github.com&#x2F;open-compass&#x2F;MixtralKit#comparison-with-o... reply brucethemoose2 12 hours agorootparentprevHave you played with finetunes, like Cybertron? Augmented in wrappers and retrievers like GPT is?It&#x27;s not there yet, but its waaaay closer than the plain Mistral chat release. reply rmbyrro 13 hours agorootparentprevStill, for a 7B model, this is quite impressive. reply apantel 6 hours agorootparentprevOne thing people should keep in mind when reading others’ comments about how good an LLM is at coding, is that the capability of the model will vary depending on the programming language. GPT-4 is phenomenal at Java because it probably ate an absolutely enormous amount of Java in training. Also, Java is a well-managed language with good backwards-compatibility, so patterns in code written at different times are likely to be compatible with each other. Finally, Java has been designed so that it is hard for the programmer to make mistakes. GPT-4 is great for Java because Java is great for GPT-4: it provides what the LLM needs to be great. reply valval 12 hours agorootparentprevOpen source models will probably catch up at the same rate as open source search engines have caught up to Google search. reply idonotknowwhy 13 hours agorootparentprevIf you can run yi34b, you can run phind-codellama. It&#x27;s much better than yi and mistral for code questions. I use it daily. More useful than gpt3 for coding, not as good as gpt4, except that I can copy and paste secrets into it without sending them to openai. reply mark_l_watson 12 hours agorootparentThanks, I will give codellama a try. reply sharemywin 13 hours agorootparentprevwhat types of things do you ask ChatGPT to do for you regarding coding? reply yodsanklai 10 hours agorootparentTypically a few lines snippets that would require me a few minutes of thinking but that ChatGPT will provide immediately. It often works, but there are setbacks. For instance, if I&#x27;m lazy and don&#x27;t very carefully check the code, it can produce bugs and cancel the benefits.It can be useful, but I can see how it&#x27;ll generate a class of lazy coders who can&#x27;t think by themselves and just try to get the answer from ChatGPT. An amplified Stack Overflow syndrome. reply dmos62 14 hours agoparentprevHow do you use these models? If you don&#x27;t mind sharing. I use GPT-4 as an alternative to googling, haven&#x27;t yet found a reason to switch to something else. I&#x27;ll for example use it to learn about the history, architecture, cultural context, etc of a place when I&#x27;m visiting. I&#x27;ve found it very ergonomic for that. reply davidkunz 14 hours agorootparentI use them in my editor with my plugin https:&#x2F;&#x2F;github.com&#x2F;David-Kunz&#x2F;gen.nvim reply 3abiton 14 hours agorootparentInteresting use case, but the issue is wasting all this compute energy for prediction? reply HorizonXP 14 hours agorootparentCan you explain what you mean by this question? reply gdiamos 13 hours agorootparentprevI host them here: https:&#x2F;&#x2F;app.lamini.ai&#x2F;playgroundYou can play with them, tune them, and download the weightsIt isn’t exactly the same as open source because weights != source code, but it is close in the sense that it is editableIMO we just don’t have great tools for editing LLMs like we do for code, but they are getting betterPrompt engineering, RAG, and finetuning&#x2F;tuning are effective for editing LLMs. They are getting easier and better tooling is starting to emerge reply loufe 14 hours agorootparentprevIf you want to experiment Kobold.cpp is a great interface and goes a long distance to guarantee backwards compatibility of outdated model formats. reply risho 14 hours agorootparentprevlm studio is an accessible simple way to use them. that said expecting them to be anywhere near as good as gpt-4 is going to lead to disappointment. reply teaearlgraycold 14 hours agorootparentprevI’ve use lm studio. It’s not reached peak user friendliness, but it’s a nice enough GUI. You’ll need to fiddle with resource allocation settings and select an optimally quantized model for best performance. But you can do all that in the UI. reply jay-barronville 15 hours agoparentprevYou mind sharing what you find so amazing about Yi-34B? I haven’t had a chance to try it. reply mark_l_watson 12 hours agorootparentI just installed it on my 32B Mac yesterday, first impressions: it does very well reasoning, it does very well answering general common sense world knowledge questions, and so far when it generates Python code, the code works and is well documented. I know this is just subjective, but I have been running a 30B model for a while in my Mac and Yi-34B just feels much better. With 4bit quantization, I can still run Emacs, terminal windows and a web browser with a few tabs without seeing much page faulting. Anyway, please try it and share a second opinion. reply brucethemoose2 12 hours agorootparentprevThe 200K finetunes are also quite good at understanding their huge context. reply brucethemoose2 11 hours agoparentprevI concur, Yi 34B and Mistral 7B are fantastic.But you need to run the top Yi finetunes instead of the vanilla chat model. They are far better. I would recommend Xaboros&#x2F;Cybertron, or my own merge of several models on huggingface if you want the long context Yi. reply yodsanklai 12 hours agoparentprev> I use their original 7B model, as well as some derived models, all the time.How does it compare to other models? and with chatgpt in particular? reply valval 12 hours agorootparentNo comparison to be made. reply minimaxir 14 hours agoprevOf course, the reason Mistral AI got a lot of press and publicity in the first place was because they open-sourced Mistral-7B despite the not-making-money-in-the-short-term aspect of it.It&#x27;s better for the AI ecosystem as a whole to incentive AI startups to make a business through good and open software instead of building moats and lock-in ecosystems. reply sillysaurusx 14 hours agoparentI don’t think that counts as open source. They didn’t share any details about their training, making it basically impossible to replicate.It’s more akin to a SaaS company releasing a compiled binary that usually runs on their server. Better than nothing, but not exactly in the spirit of open source.This doesn’t seem like a pedantic distinction, but I suppose it’s up to the community to agree or disagree. reply minimaxir 14 hours agorootparentIt&#x27;s IMO a pedantic distinction.A compiled binary is a bad metaphor because it gives the implication that Mistral-7B is an as-is WYSIWIG project that&#x27;s not easily modifiable. In contrast, there have been a bunch of new powerful new models created by modifying or finetuning Mistral-7B such as Zephyr-7B: https:&#x2F;&#x2F;huggingface.co&#x2F;HuggingFaceH4&#x2F;zephyr-7b-betaThe better analogy to Mistral-7B is something like modding Minecraft or Skyrim: although those games are closed source themselves, it has enabled innovations which helps the open-source community directly.It would be nice to have fully open-source methodologies but lacking them isn&#x27;t an inherent disqualifier. reply hedgehog 14 hours agorootparentIt&#x27;s a big distinction, if I want to tinker with the model architecture I essentially can&#x27;t because the training pipeline is not public. reply minimaxir 14 hours agorootparentIf you want to tinker with the architecture Hugging Face has a FOSS implementation in transformers: https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;blob&#x2F;main&#x2F;src&#x2F;tr...If you want to reproduce the training pipeline, you couldn&#x27;t do that even if you wanted to because you don&#x27;t have access to thousands of A100s. reply hedgehog 13 hours agorootparentI&#x27;m well aware of the many open source architectures, and the point stands. Models like GPT-J have open code and data, and that allows using them as a baseline for architecture experiments in a way that Mistral&#x27;s models can&#x27;t be. Mistral publishes weights and code, but not the training procedure or data. Not open. reply sillysaurusx 13 hours agorootparentprevWe do, via TRC. Eleuther does too. I think it’s a bad idea to have a fatalistic attitude towards model reproduction. reply hedgehog 13 hours agorootparentExactly, nice work BTW. And no hate for Mistral, they&#x27;re doing great work, but let&#x27;s not confuse weights-available with fully open models. reply emadm 12 hours agorootparentprevWith all the new national supercomputers scale isn’t really going to be an issue, they all want large language models on 10k GH200s or whatever and the libraries are getting easier to use replymrob 14 hours agorootparentprevAccording to the Free Software Definition:\"Source code is defined as the preferred form of the program for making changes in. Thus, whatever form a developer changes to develop the program is the source code of that developer&#x27;s version.\"According to the Open Source Definition:\"The source code must be the preferred form in which a programmer would modify the program. Deliberately obfuscated source code is not allowed. Intermediate forms such as the output of a preprocessor or translator are not allowed.\"LLM models are usually modified by changing the model weights directly, instead of retraining the model from scratch. LLM weights are poorly understood, but this is an unavoidable side effect of the development methodology, not deliberate obfuscation. \"Intermediate\" implies a form must undergo further processing before it can be used, but LLM weights are typically used directly. LLMs did not exist when these definitions were written, so they aren&#x27;t a perfect fit for the terminology used, but there&#x27;s a reasonable argument to be made that LLM weights can qualify as \"source code\". reply lmm 11 hours agorootparent> LLM models are usually modified by changing the model weights directly, instead of retraining the model from scratch. LLM weights are poorly understood, but this is an unavoidable side effect of the development methodology, not deliberate obfuscation.They&#x27;re understood based on knowing the training process though, and a developer working on them would want to have the option of doing a partial or full retraining where warranted. reply seydor 12 hours agoparentprevalso because their model is unconstrained&#x2F;censored. and they are commited to that according to what they say, they build it so others can build on it. GPTs are not finished business and hopefully the open source community with surpass the early successes. reply jeron 14 hours agoparentprevThey ought to rename to “ReallyOpenAI” reply JonChesterfield 14 hours agoprevAnyone else think Nvidia giving companies money to spend on Nvidia hardware at very high profit margin is a dubious valuation scheme? reply mcmcmc 14 hours agoparentKinda like MS giving OpenAI all those Azure credits? reply SeanAnderson 14 hours agoparentprevWhy would it be a dubious valuation scheme? I guess if an investor is looking at just revenue, or only looking at one area of their business finances, maybe? Otherwise it seems like the loss in funds would be weighed against the increase in revenue and wouldn&#x27;t distort earnings. reply JonChesterfield 14 hours agorootparentSay big green gives a company $100M with the rider that it needs to spend all that on nvidia&#x27;s hardware in exchange for 10% of the company.Has Nvidia valued the company at 1B? Say their margin is 80% on the sales. So Nvidia has lost some cashflow and $20M for that 10%. Has Nvidia valued the company at $200M? reply SeanAnderson 14 hours agorootparentI see :) Thanks for clarifying. I would say that I don&#x27;t have a strong enough grasp on biz finances to do more than speculate here, but:1) Is all the money spent up front? Or does it trickle back in over a few years? Cash flow might be impacted more than implied, but I doubt this is much of an issue.2) I wonder how the 10% ownership at 2B valuation would be interpreted by investors. If it&#x27;s viewed as a fairly liquid investment with low risk of depreciation then yeah, I could see Nvidia&#x27;s strategy being quite the way to pad numbers. OTOH, the valuation could be seen as pure marketing fluff and mostly written off by the markets until regulations and profitability are firmly in place. reply wongarsu 14 hours agorootparentprevIf it was a good valuation scheme, then Nvidia giving them $100 million at a $2 billion valuation would mean that Nvidia thinks the company is worth $2 billion. But if Mistral uses that money to buy GPUs that Nvidia sells with 75% profit margin, the deal is profitable for Nvidia even if they believe the company is worth only $0.5 billion (since they effectively get 75% of the investment back). And if this deal fuels the wider LLM hype and leads other companies to spend just $50 million more at Nvidia, this investment is profitable for Nvidia even if Mistral had negative value. reply emadm 14 hours agorootparentWith convertible debt and many of these rounds investors get the first money out, so the first 450m would go to the investors. reply candiddevmike 14 hours agoparentprevIt&#x27;s the heads I win, tails you lose investment model reply raverbashing 14 hours agoparentprevYou&#x27;d be surprised how this is much more common than people realize reply simonebrunozzi 2 hours agoprevLet me say this. Whoever is going to be able to let \"normal\" Mac users to install and run a local copy of an LLM, is going to reap tons of commercial benefits. (e.g. DMG, click-install, run. No command line).It is nuts to me that we have 100M computers capable of running LLMs properly, and yet only a tiny fraction of them does.Heck, let us do p2p, and lend our computing power to others.Let us build a personalized LLM.This is, IMHO, a really interesting path forward. It seems no one is doing it. reply asim 14 hours agoprevI have realised just how meaningless valuations now are. As much as we use them as a marker of success, you can find someone to write the higher valuation ticket when it suits their agenda too e.g the markup, the status signal, or just getting the deal done ahead of your more rationale competitors in the investment landscape. Now that&#x27;s not to say Mistral isn&#x27;t a valuable company or that they aren&#x27;t doing good work. It&#x27;s just valuation markers are meaningless and most of this capital raise in the AI space is about offsetting the cloud&#x2F;GPU spend. Might get downvoted to death but watching valuation news feels like no news. reply seydor 12 hours agoparentIt&#x27;s smoke. but where there is smoke, there is some level of fire reply jack_riminton 12 hours agorootparentNot if it&#x27;s a smoke machine reply ThalesX 13 hours agoprevMy 1st thought as an European, \"YAY! EU startup to the moon\". My 2nd thought was \"n&#x27;aww, American VC\". I guess that&#x27;s the best we can do around here. reply jamesblonde 13 hours agoparentThe problem is that no European VC has that amount of capital. European VCs typically have a couple of hundred million under mgmt. SV VCs have a few billion under mgmt. reply jchonphoenix 6 hours agorootparentIndex Ventures has the money. But the truth of the matter is that even most US VCs aren&#x27;t willing to shell out 2B valuations for a company with no revenue. reply paulddraper 13 hours agoparentprevIt may feel that there are few EU startups and that&#x27;s true.But there are even fewer EU VCs. reply ThalesX 13 hours agorootparentWas CTO for some European startups. I&#x27;ll always remember one when by the time the EU VC was mid-way through its due dilligence for 500k seed, we already had some millions lined up from some US VCs no questions asked. reply bsaul 12 hours agoparentprevThere were european VCs investing in the very first round, french one in particular. Founders are french. This qualifies as european in my book (let’s not get too demanding) reply z7 15 hours agoprevMistral has a lot of potential, but there&#x27;s the obvious risk that without proper monetization strategies it might not achieve sustainable profitability in the long term. reply niemandhier 14 hours agoparentThe French have a urge to be independent, the French government will hand them some juicy contract as soon as the can provide any product that justifies that. reply fy20 7 hours agorootparentI would say most European countries have that desire. That and the fact it can easily by fine tuned to the local language could make these models very popular outside the US. reply emadm 14 hours agorootparentprevYeah they shouldn&#x27;t worry, they&#x27;ll get a big French government deal at worst reply lolive 14 hours agorootparentOne of the French tycoons will eventually buy them. reply yodsanklai 13 hours agorootparentprev> The French have a urge to be independentThey lose that fight a long time ago though. It seems they don&#x27;t even try to pretend anymore. reply digitcatphd 15 hours agoparentprevI was wondering this. What is their business model exactly? Almost seems like Europe’s attempt to say “hey, look, we are relevant too” reply lolive 14 hours agorootparentBeing acquired. reply polygamous_bat 14 hours agoparentprevCoupled with the concern that once you’re charging users money for a product, you are also liable for sketchy things they do with it. Not so much when you post a torrent link on twitter that happens to have model weights. reply stillwithit 14 hours agoparentprevWait what? If company don’t make $ it don’t survive?HN could really elevate the discourse if they flagged the submarine ads of VCs reply minimaxir 14 hours agorootparentIt is a relevant question in the AI industry specifically due to new concerns about ROI given the intense compute costs. reply lolive 14 hours agorootparentSame concern I have regarding Spotify. [Which seems to have insane recurring costs. Plus some risky expansive strategic moves] reply dharma1 13 hours agoparentprevOn their pitch deck it said they will monetise serving of their models.While it may feel like a low moat if anyone can spin up a cloud instance with the same model, it&#x27;s still a reasonable starting point. I think they will also be getting a lot of EU clients who can&#x27;t&#x2F;don&#x27;t want to use US providers. reply ukuina 3 hours agorootparentPeople forget the released version is v0.1If the commerically-served model has improved capability and is exclusive to Mistral&#x27;s service, there is a possible moat there. reply dharma1 13 minutes agorootparentthey seem pretty committed to open-source AI (from interviews I&#x27;ve heard with the founders) - but maybe if they manage to train models with truly amazing capabilities somewhere down the line, they will keep some closed source reply nothrowaways 15 hours agoparentprevNothing stops them from launching a chat app. reply quickthrower2 15 hours agorootparentThe old open source, but we&#x27;ll host it for you? I think Bezos is going to be in fits of evil laughter about that model in 5 years, as all the open source compute moves to the clouds, with dollars flowing his way.But one thing Mistral could do is have a free foundational model, and have non-free (as in beer, as in speech) \"pro\" models. I think they will have to. reply dartos 15 hours agorootparentRelease small, open, foundational models.Deploy larger, fine tuned variants and charge for them.There’s a reason we don’t have the data set or original training scripts for mistral reply behnamoh 14 hours agorootparentit’s a “mistry” ;) reply simonw 14 hours agorootparentprevThere are huge economy of scale benefits from providing hosted models.I&#x27;ve been trying out all sorts of open models, and some of them are really impressive - but for my deployed web apps I&#x27;m currently sticking with OpenAI, because the performance and price I get from their API is generally much better than I can get for open models.If Mistral offered a hosted version which didn&#x27;t have any spin-up time and was price competitive with OpenAI I would be much more likely to build against their models. reply quickthrower2 14 hours agorootparentThis only is defensible for closed models though. reply teekert 15 hours agorootparentprevHere&#x27;s to hoping such models run on dedicated chips locally, on Phones and PCs etc... reply emadm 14 hours agorootparentThey already do, we just released a model equivalent to most 40-60b base models that runs on a MacBook Air no problem.It&#x27;s like 1.6gb, ones coming are better and smaller https:&#x2F;&#x2F;x.com&#x2F;EMostaque&#x2F;status&#x2F;1732912442282312099?s=20I think the large language model paradigm is pretty much done as we move to satisficing tbh reply echelon 15 hours agorootparentprevZero moat. Everybody&#x27;s doing it.I suppose they could be the Google to everyone else&#x27;s Yahoo and Dogpile, but I expect that to be a hard game to play these days. reply skue 14 hours agoparentprevAt this valuation and given the strength of the team, it’s not hard to imagine a future acquisition yielding a significant ROI.Besides, we don’t know what future opportunities will unfold for these technologies. Clearly there’s no shortage of smart investors happy to place bets on that uncertainty. reply jsemrau 14 hours agoparentprevModel-as-a-service should work just fine. reply I_am_tiberius 15 hours agoprevI really hope that a European startup can successfully compete with the major companies. I do not want to see privacy violations, such as OpenAI&#x27;s default use of user prompts for training, become standard practice. reply quickthrower2 15 hours agoparentDoes Anthropic count as European? reply uxp8u61q 15 hours agorootparentHow on Earth would it count as European? It&#x27;s a completely American company. Founded in the US, by Americans, headquartered in the US, funded by American VCs... I genuinely don&#x27;t get how you arrived at the idea that it&#x27;s European. reply quickthrower2 14 hours agorootparentBig office and lots of jobs in UK. And with complex tax setups these days I wasn’t sure. reply uxp8u61q 14 hours agorootparentBy that measure I guess Apple is Irish...?! reply totolouis 13 hours agorootparentprevUK is not in the Europe anymore. reply denlekke 12 hours agorootparentmaybe not the distinction you meant but the UK is still in Europe (the continent) and to me, European is a word based on location not membership of the European Union (which the UK left) reply baal80spam 13 hours agorootparentprevInteresting, TIL. reply quickthrower2 11 hours agorootparentThey cut through the continental shelf as part of Brexit. replyhtrp 15 hours agorootparentprevDario is italian-american? reply quickthrower2 15 hours agorootparentThat doesn&#x27;t matter too much, the corporate structure is more interesting. reply pb7 12 hours agorootparentprevElon is South African but that doesn&#x27;t make Tesla a South African company. reply nbzso 12 hours agoprevThe old Masters have a saying: Never fall in love with your creation. The AI industry is falling into the trap of their own making (marketing). LLM&#x27;s are nice toys, but implementation is resource&#x2F;energy expensive and murky at best. There are a lot of real life problems that would be solved trough rational approach. If someone is thirsty, the water is the most important part, not the type of glass:) reply TeMPOraL 12 hours agoparentIf you compared the efficiency of steam engines during industrial revolution with the ones used today, or power generation from 100 years ago to that of now, or between just about any chemical process, manufacturing method or agricultural technique at its invention and now, you&#x27;d be amazed by the difference. In some cases, the activity of today was several orders of magnitude more wasteful just 100 years ago.Or, I guess look at how size, energy use and speed of computer hardware evolved over the past 70 years. Point is, implementation being, right now, \"resource&#x2F;energy expensive and murky at best\" is how many very powerful inventions look at the beginning.> If someone is thirsty, the water is the most important part, not the type of glass:)Sure, except here, we&#x27;re talking about one group selling a glass imbued with breakthrough nanotech, allowing it to keep the water at desired temperature indefinitely, and continuously refill itself by sucking moisture out of the air. Sometimes, the type glass may really matter, and then it&#x27;s not surprising many groups strive to be able to produce it. reply nbzso 12 hours agorootparentDon&#x27;t fall in love with your creation, is not stop creating.https:&#x2F;&#x2F;www.cell.com&#x2F;joule&#x2F;fulltext&#x2F;S2542-4351(23)00365-3 reply quickthrower2 15 hours agoprevWhat is the business model? reply hnarayanan 15 hours agoparentSshh reply quickthrower2 14 hours agorootparentSorry I forgot, in AI $2Bn is preseed reply malermeister 13 hours agoparentprevGet the French government to throw a ton of money at you for sovereignty reasons reply gnabgib 9 hours agoprevPreviously: OpenAI Rival Mistral Nears $2B Valuation with Andreessen Horowitz Backing (6 days ago, 2 points, 0 comments)[0], OpenAI Rival Mistral Nears $2B Valuation with Andreessen Horowitz Backing (5 days ago, 9 points, 1 comment)[1], French AI startup Mistral secures €2B valuation (2 days ago, 106 points, 74 comments)[2], Mistral, French A.I. Startup, Is Valued at $2B in Funding Round (6 hours ago, 15 points, 1 comment)[3][0]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38522873 [1]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38533725 [2]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38580758 [3]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38593526 reply nojvek 13 hours agoprevGotta give it to Nvidia and TSMC. In the big AI race, they’re the ones with real moat and no serious competition.No matter who wins, they’ll need those sweet GPUs and fabs. reply Yujf 12 hours agoparentIts the good old \"in a gold rush, sell shovels\" reply vinni2 3 hours agoprevDupe https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38580758 reply throwaway09799 1 hour agoprev$2B is super cheap when ChatGPT wrapper AI startups are worth $500M. reply airspresso 15 hours agoprevToo many superlatives and groundbreaking miracles reported. Probably written by AI. reply jay-barronville 15 hours agoparent> In a significant development for the European artificial intelligence sector, Paris-based startup Mistral AI has achieved a noteworthy milestone. The company has successfully secured a substantial investment of €450 million, propelling its valuation to an impressive $2 billion.I’m cracking up. I don’t need to be a rocket scientist to read this and immediately conclude it’s AI-generated. I mean, they didn’t even try to hide that. Haha. reply fidotron 14 hours agoprevThere is a lot of noise here suggesting it is too much, but relative to the supposed SV unicorns of two years ago this looks like an absolute steal. reply yreg 14 hours agoparentThe macroeconomic situation 2 years ago and now was wildly different. reply qeternity 12 hours agoprevI see a lot of comments asking what or how people are using these models for.The promise of LLMs is not in chatbots (imho). At scale, you will not even realize you are interacting with a language model.It just happens to be that the first, most boring, lowest hanging fruit products that OAI, Anthropic, et al pump out are chatbots. reply transformi 14 hours agoprevEvaluation based on what? what is the business model? reply antirez 14 hours agoparentI believe that the rationale is that if you can do an outstanding 7B model, it is likely that you are able to create, in the near future, something that may compete with OpenAI, and something that makes money, too. reply jaspa99 15 hours agoprevCurious to see how this will impact Aleph Alpha reply emadm 14 hours agoparentAleph Alpha raised even more ^_^https:&#x2F;&#x2F;sifted.eu&#x2F;articles&#x2F;ai-startup-aleph-alpha-raises-500... reply yodsanklai 12 hours agoprevNoob questions (I don&#x27;t know anything about LLM, I&#x27;m just a casual user of ChatGPT)- is what Mistral does better than Meta or OpenAI?- will LLM become eventually open-source commodities with little room for innovation or shall we expect to see a company with a competitive advantage that will make it the new Google? in other words, how much better can we expect these LLM to be in the future? should we expect significant progress or have we reached to diminished returns (after all, this is only statistical prediction of next word, maybe there&#x27;s an intrinsic limitation of this method)- are there some sorts of benchmarks to compare all these new models? reply eeasss 13 hours agoprevSome folks on this forum seem to get irritated by the prospect of a successful AI company HQed in the EU. Why the hate? reply sofixa 1 hour agoparentBecause many around here have a preconceived bias that Europe cannot be innovative, and any proof to the contrary needs to be shat upon as not good or innovative enough&#x2F;only looking for government contracts, or that they&#x27;re not the size of Meta or Alphabet or Apple so obviously they aren&#x27;t really innovative, or some other goal post shifting exercise. reply b2bsaas00 14 hours agoprevAnyone has example of products that made large use of LLM API that could make economics sense to use self-hosted model (Mistral, LLAMA)? reply sroussey 14 hours agoparentIm working on embeddings database of my personal information, and ability to query it. Just a privacy reason. reply Frummy 14 hours agoprevThat&#x27;s fair given it&#x27;s 50 times more difficult to use their model reply wholien 14 hours agoprevhow does Mistral monetize or plan to monetize? create a chat gpt-like service and charge? license to other businesses? reply VirusNewbie 15 hours agoprevA competitor to OpenAI in like, benchmarks? reply consumer451 15 hours agoparentAt least a competitor to Llama, for now.https:&#x2F;&#x2F;medium.com&#x2F;@datadrifters&#x2F;mistral-7b-beats-llama-v2-1... reply Racing0461 15 hours agoprevWith the new AI regulations the EU is going to adopt, how long will mistral be paris based? reply rolisz 15 hours agoparentMaybe the regulations will be Mistral shaped. reply kozikow 15 hours agoparentprevOr another way to put it - if you are an enterprise based in Europe that needs to stay compliant, future regulation will make it very hard to not use Mistral :P. reply Barrin92 15 hours agoparentprevthere&#x27;s nothing in the new AI regulations hindering Mistral&#x27;s work. Open Source foundation models are in no way impacted.https:&#x2F;&#x2F;x.com&#x2F;ylecun&#x2F;status&#x2F;1733481002234679685?s=20 reply Racing0461 14 hours agorootparentWe both know that&#x27;s not how regulations work. Mistral is going to have to get a legal team to understand the regulations, have a line item for each provision, verify each one doesn&#x27;t apply to them, get it signed off and continously monitor for changes both to the laws and the code to make sure it stays compliant. This will just be a mandate from HR&#x2F;Legal&#x2F;Investors.Alot of work for a company with no commercial offering off the bat. And possibly an insurmountable amount of work for new players trying to enter. reply arlort 13 hours agorootparent> Alot of work for a company with no commercial offering off the batIf you have no commercial offering it doesn&#x27;t apply to you at all in the first place reply bsaul 12 hours agorootparentIf you never have any commercial offering, you have a 0 valuation. reply sofixa 1 hour agorootparentMeta didn&#x27;t have any commercial offering until what, WhatsApp for business a few years ago, around 2018? By your logic they should have never been valued at anything or made any profit, yet they did. replyandsoitis 15 hours agoparentprevRegardless of where a company is headquartered, it has to comply with local regulations. reply Racing0461 14 hours agorootparentOnly if it wants to do business there. If a company is just headquartered there, they have to comply with regulations no matter what. reply nojvek 14 hours agoprevValuation means Jack shit for early stage startup. WeWork was valued at $50B at its peak.Until a company is consistently showing growth in revenue and a path to sustainable profitability, valuation is essentially wild speculation.OpenAI is wildly unprofitable right now. The revenue they make is through nice APIs.What is Mistral’s plan for profitability?Right now stability AI is in dumps and looking for a buyer.Only companies I see making money in AI are those who live like cockroaches and very capital efficient. Midjourney and Comma.ai come to mind.Very much applaud them for open release of models and weights. reply emadm 14 hours agoparentIt’s kinda weird thinking deep tech companies should be profitable a year in.Like it takes time to make lots of money and it’s really hard to build state of the art models.Reality is this market is huge and growing massively as it is so much more efficient to use these models than many (but not all) tasks.At stability I told team to focus on shipping models as next year is the year for generative media where we are the leader as language models go to the edge. reply mpalmer 13 hours agorootparentThey didn&#x27;t say that companies should be profitable at a year in.To my mind they just seemed to be responding to the slightly clickbait-y title, which focuses on the valuation, which has some significance but is still pretty abstract. Still, headlines love the word \"billion\".The straight-news version of the headline would probably focus more on a16z&#x27;s new round. reply nojvek 13 hours agorootparentprevI acknowledge it’s easy to be an armchair critic. You are the ones in battlefield doing real work and pushing the edge.The thing is I don’t want the pro-open-source players to fizzle out and implode because funding dried up and they have no path to self sustainability.AGI could be 6 months away or 6 decades away.E.g Cruise has a high probability of imploding. They raised too much and didn’t deliver. Now California has revoked their license for driverless cars.I’m 100% sure AGI, driverless cars and amazing robots will come. Fairly convinced the ones who get us there will be the cockroaches and not the dinosaurs. reply emadm 13 hours agorootparentI think its also tough at the early stage of the diffusion (aha) of innovation curve, we are at the point of early adopters and high churn before mass adoption of these technologies over the coming years as they are good enough, fast enough and cheap enough.AGI is a bit of a canard imo, its not really actionable on a business sense. reply segmondy 13 hours agoparentprevProfitability likewise means jack shit. You just need to be have a successful acquisition by a lazy dinosaur or go make enough income to go public. You can lose money for 10yrs straight while transferring wealth from the public to the investors&#x2F;owners. With that said, I&#x27;m short Mistral for them being French. I have absolute zero faith in EU based orgs.On profitability, For all the new comers, I don&#x27;t think anyone can wager that any of them is going to make money. Capital efficiency is overrated so long as they can survive for the next year+, they are all trying to corner the market and OpenAI is the one that seems to have found a way to milk the cow for now. I truly believe that the true hitmakers are yet to enter the scene. reply stavros 13 hours agoparentprevThis is just tangential, but I wouldn&#x27;t call their APIs \"nice\", I&#x27;d be far less charitable. I spent a few hours (because that&#x27;s how long it took to figure out the API, due to almost zero documentation) and wrote a nicer Python layer:https:&#x2F;&#x2F;github.com&#x2F;skorokithakis&#x2F;ez-openai&#x2F;With all that money, I would have thought they&#x27;d be able to design more user-friendly APIs. Maybe they could even ask an LLM for help. reply evantbyrne 14 hours agoparentprevValuation matters quite a bit for continued funding. reply hauget 14 hours agorootparentHis point is with regards to reaching & maintaining profitability, not revenue spending. reply evantbyrne 13 hours agorootparentIt&#x27;s too early for Mistral to focus on revenue. These AI companies are best thought of as moonshot projects. reply toss1 13 hours agorootparentprevYes, and it can matter in a very bad way if you need to subsequently have a \"down round\" (more funding at a lower valuation).Initial high valuations mean the founders get a lot of initial money giving up little stock. This can be awesome if they become strongly cash-flow positive before they run out of that much runway. But if not, they&#x27;ll get crammed hard in subsequent rounds.The more key question is: how much funding did they raise at that great valuation, and is it sufficient runway? Looks like €450 million plus an additional €120 million in convertible debt. Might be enough, depending on their expenses... reply evantbyrne 11 hours agorootparentI&#x27;m not saying that either of your concerns are invalid. The LLM space is just the wrong place to be for investors who are worried about cash-flow positivity this early in the game. These models are crazy expensive to develop _currently_, but they is getting cheaper to train all the time. Meaning Mistral spent a fraction of what OpenAI did on GPT-3 to train their debut model, and that companies started one year from now will be spending a fraction of what both are spending presently to train their debut models. reply toss1 8 hours agorootparentYUP. Plus, the points at the end of your post, abt how much faster and cheaper it is getting to train new models indicates that Mistral may have hit a real sweet-spot. They are getting funding at a moment where the expectations are that huge capital is needed to build these models, just when those costs are declining, so the same investment will buy them a lot more runway than it did for previous competitors... reply rmbyrro 13 hours agoparentprevGenerally agree.Instead of \"path to profitability\", I think path to ROI is more appropriate, though.WhatsApp never had a path to profitability, but it had a clear path to ROI by building a unique and massive user base that major social networks would fight for. reply wslh 13 hours agoparentprev> OpenAI is wildly unprofitable right now.Do we know some of its numbers? How many paid subscribers do they have? I pay for two subscriptions. reply vagrantJin 13 hours agoparentprevcomma.ai is a great example of a good business.But I might have a bias because I was following along as the company was built from whiteboard diagrams to what it became. reply firebot 13 hours agoprevWho comes up with these valuations? The Donald? reply mytailorisrich 14 hours agoprevPerhaps someone can answer this: this is a one year old company. Does this mean that barriers to entry are low and replication relatively simple? reply cavisne 11 hours agoparentThe part of Meta research that worked on LLaMa happened to be based in the Paris office. Then some of the leads left and started Mistral.Complex&#x2F;simple is not really the right way to think about training these models, I&#x27;d say its more arcane. Every mistake is expensive because it takes a ton of GPU time and&#x2F;or human fine tuning time. Take a look at the logbooks of some of the open source&#x2F;research training runs.So these engineers have some value as they&#x27;ve seen these mistakes (paid for by Meta&#x27;s budget). reply emadm 14 hours agoparentprevMain barrier right now is access to supercompute and how to run it, everything is standardising quickly in the space reply hn_throwaway_99 14 hours agoprevPerhaps too much off-topic, but I hate how the press (and often the startups themselves) focuses on the valuation number when a company receives funding. As we&#x27;ve seen in very recent history, those valuation numbers are at best a finger in the wind, and of course a big capital intensive project like AI requires a valuation that is at least a couple multiples of the investment, even if it&#x27;s all essentially based on hope.I think it would make much more sense to focus on the \"reality side\" of the transaction, e.g. \"Mistral AI received a €450 million investment from top tech VC firms.\" reply shrimpx 13 hours agoparentThe valuation is meaningful in the sense of \"Mistral sells 22.5% of company to VC firms.\" reply matmulbro 15 hours agoprevLLM space is so cringe so much excitement from supply side and no excitement&#x2F;cringe from supposed demand side reply rogerkirkness 15 hours agoparentMicrosoft Cloud AI revenue went $90M, $900M, $2.7B in three quarters. How much more hard dollar demand growth could there possibly be at this point? reply echelon 15 hours agorootparentThey&#x27;re selling to startups, not consumers.The good startups are building, fine tuning, and running models locally. reply matmulbro 15 hours agorootparentprevit&#x27;s shovels all the way down reply quickthrower2 15 hours agorootparentI think there are enough genuine use cases. People are saving time using AI tools. There are a lot of people in office jobs. It is a huge market. Not to say it won&#x27;t overshoot. With high interest rates valuations should be less frothy anyway. reply sjfjsjdjwvwvc 15 hours agorootparentprevshovelling what in your opinion? Or it’s just a giant house of cards? reply cgearhart 14 hours agorootparentRight now they’re shoveling “potential”. LLMs demonstrate capabilities we haven’t seen before, so there’s high uncertainty about the eventual impact. The pace of progress makes it _seem_ like an LLM “killer app” could appear any day and creating a sense of FOMO. reply shrimpx 13 hours agorootparentThere&#x27;s also the race to \"AGI\" -- companies spending tens of billions on training, hoping they&#x27;ll hit a major intelligence breakthrough. If they don&#x27;t hit anything significant that would have been money (mostly) down the drain, but Nvidia made out like a bandit. replyXenoamorphous 15 hours agoparentprevI can’t think of any software&#x2F;service that’s grown more in terms of demand over a single year than ChatGPT (in all its incarnations, like the MS Azure one). reply huytersd 15 hours agoparentprevI don’t know what you’re talking about. I use chatGPT extensively. Probably more than 50 times a day. I am extremely excited for anything that can top the already amazing thing we have now. They have a massive paying customer base. reply jay-barronville 15 hours agorootparent100%. ChatGPT is used heavily in my household (my wife and I both have paid subscriptions) and it’s absolutely worth it. One of the most interesting things for me has actually been watching my wife use it. She’s an academic in the field of education and I’ve seen her come up with so many creative uses of the technology to help with her work. I’m a power user too, but my usage, as a software engineer, is likely more predictable and typical. reply aantix 15 hours agorootparentprevIt’s replaced Google for me, for most queries.It’s just so much more efficient in getting the answers I need. And it makes a great pair programmer partner. reply 4death4 15 hours agorootparentprevWhat do you use it for? reply kozikow 15 hours agorootparentNot OP, but For me:- Writing: emails, documentation, marketing - Write a bunch of unstructured skeleton of information. Add a prompt about the intended audience and a purpose. Possibly ask it to add some detail.- Coding: Especially things like \"Is there a method for this in this library\" - a lot quicker than browsing through documentation. Some errors - copy-paste the error from the console, maybe a little bit for context, and quite often I get the solution.And API based:- Support bot- Prompt engineering of some text models that normally would require labeling, training, and evaluation for weeks or months. A couple of use cases - unstructured text as an input + prompt, JSON as an output. reply marstall 1 hour agorootparent> \"Is there a method for this in this library\"more efficient than just googling \" \"? reply sjfjsjdjwvwvc 15 hours agorootparentprevNot OP but I used it very successfully (not OpenAI but some wrapper solution) for technical&#x2F;developer support. Turns out a lot of people prefer talking to a bot that gives a direct answer than reading the docs.Support workload on our Slack was reduced by 50-75% and the output is steadily improving.I wouldn’t want to go back tbh. reply s1artibartfast 14 hours agorootparentprevI used it to write my wedding vows reply huytersd 13 hours agorootparentBased reply dartos 15 hours agorootparentprevI usually go to it before google now if I’m looking for an answer to a specific question.I know it can be wrong, but usually when it is, it’s obviously wrong reply ipaddr 14 hours agorootparentprevBash scripts reply huytersd 14 hours agorootparentprevA lot of very varied things so it’s hard to remember. Yesterday I used it extensively to determine what I need to buy for a chicken coop. Calculating the volume of concrete and cinder blocks needed, the type and number of bags of concrete I would need, calculating how many rolls of chicken wire I would need, calculating the number of shingles I would need, questions on techniques, and drying times for using those things, calculating how much mortar I would need for the cinderblocks (it took into account that I would mortar only on the edges, the thickness of mortar required for each joint, it accounted for the cores in the cinderblocks, it correctly determined I wouldn’t need mortar on the horizontal axis on the bottom row) etc. All of this, I could’ve done by hand, but I was able to sit and literally use my voice to determine all of this in under five minutes.I use DALLE3 extensively for my woodworking hobby, where I ask it to come up with ideas for different pieces of furniture, and have constructed several based on those suggestions.For work I use it to write emails, to come up with skeletons for performance reviews, look back look ahead documents, ideas for what questions to bring up during sprint reviews based on data points I provide it etc. reply itronitron 14 hours agoparentprevYeah, the demand side consists solely of those that think they will be supply side. reply 5 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mistral AI, a startup based in Paris, has secured a €450 million investment, increasing its valuation to $2 billion.",
      "Investment leaders include Andreessen Horowitz, Nvidia Corp, and Salesforce, indicating the potential of Mistral AI.",
      "Mistral AI's flagship product, Mistral 7B, is an advanced language model known for its efficiency and customization capabilities."
    ],
    "commentSummary": [
      "French startup Mistral AI has reached a valuation of $2 billion and is gaining attention for its AI models.",
      "The availability of open source AI models at the level of GPT-4 is anticipated to emerge soon, but concerns exist regarding the lack of diverse datasets for training these models.",
      "Discussions cover a range of topics including potential job automation, cost reduction possibilities, different AI models and coding assistance tools, Mistral AI's decision to open-source their model, and debates around their valuation and potential government support."
    ],
    "points": 285,
    "commentCount": 215,
    "retryCount": 0,
    "time": 1702232756
  },
  {
    "id": 38591437,
    "title": "Looking for Practical UI Design Courses? Here's a List of Down-to-Earth Tutorials, Examples, and Exercises",
    "originLink": "https://news.ycombinator.com/item?id=38591437",
    "originBody": "Hello HN,What have been some of the best UI design courses, in your experience?I&#x27;m aware of avant garde works such as Bret Victor&#x27;s \"Magic Ink\" and love it.At the same time, I&#x27;d also love to learn more about more \"down to earth\" tutorials&#x2F;examples&#x2F;exercises&#x2F;courses to build practical UI skills. Something above \"react tutorials\", but something below Victor&#x27;s \"Magic Ink\"Big plus point if there is a coherent set of principles&#x2F;system propounded, to make it easy to apply in our specific context.",
    "commentLink": "https://news.ycombinator.com/item?id=38591437",
    "commentBody": "Best UI design courses for hackers?Hacker NewspastloginBest UI design courses for hackers? 283 points by atomicnature 20 hours ago| hidepastfavorite102 comments Hello HN,What have been some of the best UI design courses, in your experience?I&#x27;m aware of avant garde works such as Bret Victor&#x27;s \"Magic Ink\" and love it.At the same time, I&#x27;d also love to learn more about more \"down to earth\" tutorials&#x2F;examples&#x2F;exercises&#x2F;courses to build practical UI skills. Something above \"react tutorials\", but something below Victor&#x27;s \"Magic Ink\"Big plus point if there is a coherent set of principles&#x2F;system propounded, to make it easy to apply in our specific context. druskacik 18 hours agoRefactoring UI is a book you can read in a couple of hours, but it helped me immensely to design my projects. It does not go to a great depth, but it&#x27;s very useful for simple and quick hacking.https:&#x2F;&#x2F;www.refactoringui.com&#x2F; reply ethanbond 17 hours agoparentI’m a fairly experienced product designer and this book has been incredibly valuable even for me. Lots of extremely practical insights packed in here. IMO a must-read for engineers and designers alike, if you’re building something where you need to care about point-and-click UI at all. reply sfRattan 11 hours agoparentprevFrom their website:>Borders are a great way to distinguish two elements from one another, but using too many of them can make your design feel busy and cluttered.>Instead, try adding a box shadow, using contrasting background colors, or simply adding more space between elements.Emphasis mine. The above may be reasonable advice if you&#x27;re building a low information density app for nonproductive content consumption... Or a touchscreen only app where all interactive elements must be at least a finger tall and wide... But it drives me absolutely batty when extra white space appears in software that I want to use to process&#x2F;analyze a lot of information and actually get stuff done. And numerous examples on the website just make things... Farther apart and&#x2F;or bigger.I still rue the day Spotify fattened up its row height for all lists in the app. It&#x27;s less readable than it was if I can&#x27;t read as many song names as before without scrolling.I don&#x27;t want to rag on the book overall without reading it, especially given how many in this thread seem to love it and the table of contents does hint at good ideas within, but that seems like a terrible set of examples to lead with. &#x27;More white space&#x27; is not universally good design advice. Give me design in the school of TMUX and Bloomberg Terminal any day over extra white space for the sake of &#x27;readability.&#x27; As much information as it is possible to present clearly on a given screen. reply druskacik 1 hour agorootparentThere&#x27;s a subchapter in the book called Dense UIs have their place in which the authors mention situations where more compact and busy design is more desirable, adding a screenshot of a sports results website as an example.The rules in the book (and on the website) should not be seen as set-in-stone, they are more principles we can follow depending on the situation. I think the use fewer borders principle is very useful in many scenarios.Saying that, I agree that today&#x27;s designs tend to too much whitespace (there was a blog here recently ranting that all product landing pages are basically a navigation bar with a shallow text over a picture background).You can check this blog if you want to see more principles from the book: https:&#x2F;&#x2F;medium.com&#x2F;refactoring-ui&#x2F;7-practical-tips-for-cheat... reply rkagerer 10 hours agorootparentprevAgreed. The trend toward wasteful whitespace is like moving from mechanical pencils to crayons. Respect my pixels, dammit. reply ImPostingOnHN 10 hours agorootparentprevI had a local entrepreneur showing me their website&#x2F;product at a meetup a while ago, and after scrolling a bit through an info page, I unconsciously switched to desktop mode to get more information density.The entrepreneur, who was watching me interact with the site, asked me what I was doing, and was confused at why I&#x27;d go to desktop mode. That, in his mind, was too information dense. But I wanted to see all the information at once without scrolling around, my eyes can scroll fine.I guess it goes to show it should be a consideration, given its variance. reply sph 10 hours agorootparentprevThere are no universal rule of design. You ragging about a mention of whitespace, as if whitespace is universally a bad idea, means you are just judging a book by its cover and only have the cursory understanding of the matter that is typical of most programmers.Remember, good design is not only what the cool kids do or what FAANG might believe it is. Good design is timeless. Good design is a conversation with the product. Sometimes whitespace is good, sometimes it is not. You saying whitespace is a bad idea is as misinformed as a naive designer saying whitespace is king.That said, I recommend Refactoring UI, but I recommend more anything on Dieter Rams or the book \"The Design of Everyday Things\" by Donald A. Norman. reply rcxdude 9 hours agorootparentGood design is also subjective. More whitespace is trendy and it annoys people who prefer a higher information density (I see the same with code formatting preferences: some people really like to space out code, others like to make it as compact as possible. Most people are somewhere in the middle). reply omnimus 1 hour agorootparentThere is a thing called user testing. I would say if big company makes design with a lot of whitespace its outcome of a process that had many variations and many tests.So yes its subjective and people probably like it. reply Sakos 8 hours agorootparentprevOn the flipside, probably the most common software found and used in businesses everywhere is spreadsheet related, and it&#x27;s common even in non-business contexts. How many of these applications are adding senseless whitespace everywhere?In the cases I&#x27;ve seen where whitespace is added, it&#x27;s purely an aesthetic choice and not a UI design choice. reply aleph_minus_one 15 hours agoparentprevWith its USD 99 price (or USD 149 for a version with additional material), this e-book seems quite expensive to me. reply adamzerner 12 hours agorootparentI strongly disagree. Relative to other books, maybe it is on the expensive side. But relative to the value you get from it, I think it is incredibly low.One way to think about it is in terms of how much you value your time. If you value your time at $25&#x2F;hr and this book saves you more than 4 hours of time learning UI design, it is worth it.Another way to think about it is in terms of ROI. As someone in the tech industry I think that having these skills is likely to pay off way more than $99. Not in a legible way -- it&#x27;s not like anyone will ever say to you \"I see you have these UI design skills, here&#x27;s a $5,000 raise.\" But I believe that the skills will ultimately shine through and improve your ability to get jobs and make more money.Also, in practice, if you&#x27;re in the tech industry, there&#x27;s probably a good chance that you can get your employer to pay for it. reply aleph_minus_one 10 hours agorootparentIn other countries, \"tech industry\" does not pay that well. Also keep in mind that there exist a lot of people who do programming (even often quite complicated programming) as part of their job, but work in a very different branch of industry than tech industry - also with a much smaller salary than what is common in the tech sector in the USA. reply fauigerzigerk 1 hour agorootparentThe example given was $25 per hour though, which is not some outrageous Silicon Valley salary.I think the idea may well be that there are two groups of people in the world. Those who pay for books and those who never pay for books, especially if they can find a free download somewhere.Students, young tech workers from low wage countries, people who just want to have a look or are horders rather than readers won&#x27;t pay regardless of price.So the only question remaining is what effect price has on those who do buy tech books. Will they buy a cheaper book instead?I don&#x27;t know the answer to that, but I can say that for me there is a psychological £49.99 threshold that makes me start thinking and looking at other options rather than making an impulse purchase.This threshold is completely irrational. If you double my salary today, the threshold won&#x27;t change. It only changes gradually over time. reply aleph_minus_one 30 minutes agorootparent> I think the idea may well be that there are two groups of people in the world. Those who pay for books and those who never pay for books, especially if they can find a free download somewhere.I claim that I belong to a third group: I do spend a lot of money on books, but have to be somewhat careful with my spending. I can also claim that the knowledge that I get from the mentioned book will in all likelihood not increase my salary, so I am interested in this book solely because I am very interested in this topic. But since I am interested in a lot of topics (and tend to avoid illegal downloading of e-books if possible), I have to concentrate the huge book spendings on those books that are insanely good. reply squarefoot 5 hours agorootparentprev> As someone in the tech industry I think that having these skills is likely to pay off way more than $99.That is certainly true, however many young programmers could benefit from books like this one when they are in the early learning phase, to avoid developing bad habits that can only become harder to forget with age and (bad) experience, especially if they land the first job after creating or contributing to a dozen projects. reply shinycode 12 hours agorootparentprevI totally agree, quality knowledge that will last years and saves time is worth it. Amazing book reply charlie0 4 hours agorootparentprevYes, it seems expensive. However, you will save sooo much time by getting it. One of my regrets was not spending money at the beginning of my journey into software. It&#x27;s easy to justify not spending the money when there&#x27;s so much free content on YT and other places, but this book is really good. Adam and team has created Tailwind, so he knows what he&#x27;s talking about. reply windowshopping 15 hours agorootparentprevnext [7 more] [flagged] replwoacause 15 hours agorootparentPretty sure this isn’t allowed here reply windowshopping 15 hours agorootparentI mean, you can just Google RefactoringUi PDF and it&#x27;s the top result. Me linking the url makes little difference. reply replwoacause 15 hours agorootparentWell, there is a difference. It’s you doing the piracy legwork in a community full of the types of people who make this kind of content (hackers, designers, indie devs, writers, etc.), versus someone doing it themselves outside of the community. It’s a promotion of it which I think won’t be well received here. reply systemvoltage 15 hours agorootparentprevSo that justifies it? reply windowshopping 11 hours agorootparentYes. reply quickthrower2 15 hours agorootparentprevLooks like a trojan horse site reply tiffanyh 17 hours agoparentprevShould be noted, this booked was created by the Tailwind CSS folks (before Tailwind existed IIRC). reply mattfrommars 9 hours agoparentprevIs this a book someone like me who will benefit if I can&#x27;t figure out the basic of layout and UI of a react website? I developed a React App to track expense b&#x2F;w me and my wife. I sort of thought as I developed, the layout and structure will automatically work out of the box b&#x2F;w mobile and desktop. The end result is it is not. The layout do not intelligently work themselves out in different screen resolution.With that said, I have read about &#x27;flexbox&#x27; or sort tech that React has but I have procrastinated forever to revamp the UI using flexbox. I didn&#x27;t quiet understand flexbox or react layout general and will need to take another jab at it.On the other hand, will this book benefit me? Sort of mental model to have when mocking up UI for website? I understand the whole idea of wireframe and sketches, but I want think beyond that. For example, I am always impressed by https:&#x2F;&#x2F;www.hims.com&#x2F; UI&#x2F;UX.Second, how long should it generally take to read through the book? In my mind, it will take months or so to go through it... reply hutzlibu 8 hours agorootparentI do not know that book, but it seems you are mixing up and asking for 2 different things:- how to design a good UI- how to implement a given UI design (in your case with react)In general, I would start with visualizing and sketching the desired end result. Then choose a UI framework, that can make that happen. (and I don&#x27;t know of a magic bullet UI framework, that just works automatically for different screenresolutions)I would suggest with keeping it as simple as possible. Less is usually more. reply mattfrommars 6 hours agorootparentThank you. Is it correct to say the book parent comment suggested solves item one in your list> how to design a good UIAnd my issue is exactly your second point - how to implement a given UI design. I came across multiple React UI Framework but haven&#x27;t been able to fully understand the &#x27;big picture&#x27; per say on using them. reply lxchase 10 hours agoparentprevMy personal opinion based on the examples provided, is that while it may provide a good baseline, I disagree with some of the examples as being the correct thing to do. Information density intention and audience is important.For example, I don&#x27;t know if I agree with the removal of borders. Old reddit looks better for many folks because of the information density even if its an \"uglier reddit\".Another example for a good UI but would not meet the \"recommendations\" of this book is https:&#x2F;&#x2F;www.mcmaster.com&#x2F; reply strontian 16 hours agoparentprevJust seconding this comment. RefactoringUI is a far outlier in how _useful_ it is compared to other books. reply JCharante 16 hours agoparentprevThe first two chapters are free and I&#x27;ve read them and they&#x27;re very very good reply zengid 15 hours agoparentprevIsn&#x27;t that by the folks behind Tailwind? reply khaldiameur 14 hours agorootparentYes it is. I don&#x27;t like Tailwind but the book is great for devs. reply 3abiton 12 hours agoparentprevI will take a look at this! reply BasilPH 17 hours agoparentprevCame here to mention Refactoring UI. I think the structure makes it very valuable: Each one is focused on a particular problem or topic. So if you need help with font weights or spacing or shadows there is a chapter for that and I often go back and look at a specific chapter when I encounter a problem. reply zer0tonin 14 hours agoprevWhat helped me go from being completely unable to design a simple form to doing okay looking UI (not great yet) was simply to learn to draw. The best UX designers I&#x27;ve had the occasion to work with came from art school backgrounds, so I think there&#x27;s definitely something to it.After grinding drawing for a while, it becomes very easy to simply see what works (and what you like) in other software UI-wise, and re-use that on your own projects.One recommendation I have is not to try to over-focus on the design of single components (ie. buttons or form inputs). It is fine to reuse a lib for that. What&#x27;s really going to make the difference is how you organize them across the page, and the colors you pick.Edit: since I haven&#x27;t really mentioned any courses: proko.com has great drawing courses. For books, check out Andrew Loomis&#x27; Fun with a pencil and Betty Edwards&#x27;s Drawing on the Right Side of the Brain. reply fzeindl 13 hours agoprevApples Human interface guidelines helped me a lot in improving the windows gui application, apps and websites I was developing in my career. After reading them I was definitely able to create much simpler UIs. (Example: use verbs in message-boxes. Not: Delete? Ok&#x2F;Cancel, But instead: Delete XYZ. Delete&#x2F;Keep.) https:&#x2F;&#x2F;developer.apple.com&#x2F;design&#x2F;human-interface-guideline... reply hypertexthero 18 hours agoprevThe following will give you a good understanding of UI design.Number 3, Designing Interfaces, has a coherent set of principles.1. [Don’t Make Me Think](https:&#x2F;&#x2F;sensible.com&#x2F;dont-make-me-think&#x2F;)2. [The Design of Everyday Things](https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Design_of_Everyday_Things)3. [Designing Interfaces, 3rd Edition](https:&#x2F;&#x2F;www.oreilly.com&#x2F;library&#x2F;view&#x2F;designing-interfaces-3r...)4. [Nielsen Norman Group Interaction Design: 3-Day Course](https:&#x2F;&#x2F;www.nngroup.com&#x2F;courses&#x2F;interaction-design-3-day-cou...)5. [Apple Human Interface Guidelines](https:&#x2F;&#x2F;developer.apple.com&#x2F;design&#x2F;human-interface-guideline...)6. [A Dao of Web Design](https:&#x2F;&#x2F;alistapart.com&#x2F;article&#x2F;dao&#x2F;)7. [Usability Testing](https:&#x2F;&#x2F;www.nngroup.com&#x2F;courses&#x2F;usability-testing&#x2F;)It’s important to practice, not just read. The term is “dogfooding”.https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Learn&#x2F;Getting_start...Finally, probably first of all, it’s worth thinking about whether your user interface is harmful to the people using it, and changing if so:https:&#x2F;&#x2F;www.deceptive.design&#x2F;https:&#x2F;&#x2F;www.humanetech.com&#x2F;key-issues reply preommr 18 hours agoparentWhenever this question comes up, the usual suspects like \"The Design of Everyday things\" get mentioned and I can&#x27;t help wonder what it would be like if someone asked for books about learning to program [websites] and kept getting \"Godel, Escher Bach\", \"Zen and Art of Motorcycle Maintenance\" or \"Meditations\" by Marcus Aurelius. reply imjonse 17 hours agorootparentUnless you read The art of war you cannot possibly become a 10x ninja developer. &#x2F;sMore seriously, I have read the design of everyday things about 10 years ago and it was one of the most boring books I have ever had to go through. I only remember doorknobs and something about affordances. Read refactoringUI as well, some vague shiny UI tips of which can&#x27;t remember any but &#x27;have decent spacing&#x27;. I still can&#x27;t design even a simple form. I am starting to suspect that if one wants to be good at web design one needs to start doing lots of web design until one gets better. Reading books may come later to place that practical knowledge in some coherent mental framework. reply _trackno5 17 hours agorootparentThat’s interesting. I went through that book not that long ago and I found it fascinating. I had a hard time putting it down.I found that the concepts he covers in that book can even be applied for good software API design.Also, he did spoil doors for me. Pretty much every building I go into now annoys me because of the stupid door handles they pick. reply brailsafe 12 hours agorootparentprev> I am starting to suspect that if one wants to be good at web design one needs to start doing lots of web design until one gets better. Reading books may come later to place that practical knowledge in some coherent mental framework.This is true, but it&#x27;s true for anything. Reading a programming book won&#x27;t teach you to program unless it presents problems you can build on actively throughout the reading. You need applied reps, otherwise you&#x27;ll at most get some vague inspiration or enjoy&#x2F;hate it.This is why I don&#x27;t read practical books anymore unless I&#x27;m prepared to practice what I&#x27;m supposed to be learning about during the course of my reading. This is also true for videos. reply charlie0 4 hours agorootparentprevI can&#x27;t remember most of the stuff I read. Doesn&#x27;t mean having it on the bookshelf as reference is a bad idea. I think most books are like that anyway. reply neeleshs 13 hours agorootparentprevThat&#x27;s quite interesting. I&#x27;m reading design of everyday things right now, and has been very valuable for my product (SaaS,drag&#x2F;drop). Not actually to build it, but designing parts.(mockups etc)A lot of it is common sense, but only in hindsight. reply jdgoesmarching 16 hours agorootparentprevThese books are a solid foundation for design education, but also design is its own deep field of expertise that you’re not going to learn after a a book or two. Both things can be true. reply dutchCourage 15 hours agorootparentprev\"Refactoring UI\" won&#x27;t make you a good UI designer on its own, but it&#x27;s a great place to start. It&#x27;s the perfect book if after making some software you feel like the design is amateurish and want to know how to start improving it.And then if you&#x27;re interested you can dig deeper into colors, layouts, types... reply flappyeagle 16 hours agorootparentprevDOET is very helpful in getting you to understand how to think from a user perspective.I found it to be one of the most useful books in my development as an application programmer.It teaches you how to see UI reply chiefalchemist 14 hours agorootparentYes. But he also spends plenty of time on the idea that behind the scenes there are always trade-offs, compromises, etc. That is, organization and personel impact design, often in ways that aren&#x27;t obvious from the outside. reply hypertexthero 17 hours agorootparentprevFor websites at this point in time I’d recommend Mozilla’s Getting started with the web: https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Learn&#x2F;Getting_start...I couldn’t get through Godel, Escher Bach, but can recommend I Am A Strange Loop by the same author! https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;I_Am_a_Strange_Loop reply fernirello 15 hours agorootparentprevThis has to be the best HN comment in a long time. reply cm11 12 hours agorootparentprevI agree, but sorta in both directions—which paints a great contrast between product (design or management) and UI design.I really enjoyed the book and think it&#x27;s great entry to a design mindset. This is a mindset for how to make products that work well and people like or would actually use. Is that what we expect of a designer or a product manager? Important, but just a bit downstream of that is how it works and where the buttons&#x2F;interactions are and whether people think its attractive. Most designers (we can confine this to tech [1]) consider themselves product designers rather than UI (or even UX) and often have that title and the bullet points in the job description to reflect it. That is, they want and are hired to understand users and design _products_. And then later, design _interfaces_—but only later because ofc the interfaces should derive from those things.You can argue, for various reasons [2], a majority of today&#x27;s tech designers would not be good at making product decisions (which includes the processes of understanding users or designing products); to which I would mildly agree, but find more pertinent to then immediately question why recruit and hire someone with that offer. You could hire directly a UI or visual designer.In practice, because of power dynamics, most designers design UIs and some UX. There is indeed time spent on the product work that the designers feel they should be doing as part of a \"real\" process, but doesn&#x27;t ever land in the product. That work is done genuinely, but frequently amounts to design theater—both internally to their own teams&#x2F;companies (which \"storytells\" the work) and externally when they present their portfolio to others [3]. Most product (design) decisions are not made by designers because they&#x27;re either not present for or can&#x27;t win any of the upstream arguments. So even at the UI level, where there is more control, the design is based on \"received\" constraints.Anyways, I think your comment is a real good provocation for what do we mean by designer or what should a designer be.[1] I think this applies to plenty of folks called a designer pre-tech, say, industrial designers or architects.[2] There are many, but one is simply that designers who are capable at this, find that design is not the place to impact them. They should perhaps become a PM (which is full of its own pitfalls) or found their own company. Or more commonly become a disinterested designer. I&#x27;m not sure either role, PM or design, is able to really do this well in the way most tech companies are organized, but the PM is in many cases the designer&#x27;s real informal manager. And the informality is the problem. They have more power, but not any hard responsibility for&#x2F;to the designer. And so you get a junior and&#x2F;or apathetic pool of designers.[3] Most portfolios, if you&#x27;re not familiar, present designs as designed (what was made in the design tool) not as implemented (screenshots or the actual live app) for a reason. And further, most reply prox 13 hours agoparentprevGreat list! You are missing the big one (imho) : About Face, Interaction Design by Alan Cooper reply lstamour 13 hours agorootparentIn particular, the third edition focuses heavily on desktop while the fourth edition strays into mobile. Love em.That said, more practical and recent resources include https:&#x2F;&#x2F;www.interaction-design.org&#x2F;master-classes and https:&#x2F;&#x2F;designcode.io&#x2F; for different topics. I’m sure there are others, but these are just off the top of my head. reply atomicnature 18 hours agoparentprevWoah, that&#x27;s a long list of resources, thanks. I&#x27;ve read a few of those ((1), (2), (7)).(3) is new to me. Will give it a read for sure. reply maroonblazer 18 hours agoparentprevAnyone heard from Tog (#4 above)? The cert on his website, asktog.com, appears to have expired and no new content since 2014. reply coldbrewed 16 hours agoprevI found Design for Hackers[1] to be an incredibly informative book; it provides a great deal of insight into UI patterns, color schemes and selections, and overall UI design. It&#x27;s definitely more oriented towards graphical UIs but provides enough general insight into design considerations that you could generalize it for TUIs and CLIs if needed.[1]: https:&#x2F;&#x2F;designforhackers.com&#x2F; reply brainbag 17 hours agoprevI am a longtime developer but I&#x27;m passionate about design and UX. I&#x27;m always on the lookout for materials that I can give my team and other developers to help them get better at design. It&#x27;s not a course, but \"The Non-Designer&#x27;s Design Book\" (ISBN 978-0133966152, Robin Williams) is the best material for design fundamentals I&#x27;ve found. It&#x27;s very approachable for anyone and it&#x27;s broadly applicable across all kinds of design. Everyone I have convinced to read it has loved it, and I&#x27;ve seen an improvement in output and understanding. I highly recommend this if you have an interest in design.Refactoring UI is also valuable and can be impactful, though it&#x27;s heavily web focused and is more like a Web Component Design Cookbook rather than foundational knowledge. reply ironskull 18 hours agoprevIt&#x27;s definitely not cheap, but I would be surprised if Erik Kennedy&#x27;s https:&#x2F;&#x2F;www.learnui.design isn&#x27;t the best course out there. In fact, he has three courses:1. UI design2. UX design3. Landing Page designI own all 3 and they are among the best purchases I&#x27;ve ever made, even at the cost. Erik is a former programmer who has taken the engineers mindset and systematically analyzed and broken down the various parts of UI design. It is very practical, which was something that was lacking in most resources I found when I was in your position.If anyone is interested, I would recommend starting with the UI course, which probably runs around $1000. Unfortunately It is only available at certain intervals, probably every 6-8 weeks.If the cost is intimidating, you can get a lot out of his blog, which will also give you a taste of how he thinks about design: https:&#x2F;&#x2F;www.learnui.design&#x2F;blog&#x2F;. reply davidivadavid 17 hours agoparentWith RefactoringUI and Erik&#x27;s course, I think the 3rd I would use to complete my top 3 is Shift Nudge (https:&#x2F;&#x2F;shiftnudge.com&#x2F;).There&#x27;s a fair amount of overlap between all of them, so if you want to Pareto minmax it, I would recommend starting with Refactoring UI, which should help provide practical solutions to many situations and get rid of the most egregious horrors you might commit.Of course, UI design goes much deeper than anything those courses can teach, but they&#x27;re a great start. reply switch007 16 hours agorootparent> https:&#x2F;&#x2F;shiftnudge.com&#x2F;Only commenting about it as that site is a course about designing interfaces: to me the font on that site borderline is ridiculous, with the flat lines on the t&#x2F;f&#x2F;g, narrow L, an ampersand that looks like an &#x27;e&#x27; with a long tail. Makes it hard to read reply davidivadavid 15 hours agorootparentAnd, appropriately, that font is not used for an application&#x27;s interface but for a display title.Legibility sometimes needs to be traded off for impact or other variable that matters more depending on the purpose.He could have used Inter, like every other landing page on the internet these days, but then it wouldn&#x27;t stand out.These are the kinds of things that have more to do with graphic design and that incorporate other considerations than typical UI stuff. reply sanitycheck 14 hours agorootparentContrast is terrible on the body text, especially compared to those brightly coloured highlighted sections... It&#x27;s like he&#x27;s shining a torch in my face while asking me to read grey chalk on a blackboard in a darkened room.There are big weird gaps everywhere, some of them have mouseovers which change my mouse cursor into something intended to be amusing but have no link.The \"This will&#x2F;won&#x27;t work if...\" items seem to be links but clicking them just jumps me back to the top of the page.The weird animation under \"Pro\", no idea what that&#x27;s supposed to be telling me.Company logos at the top. Used with permission, I wonder? They have mouseovers but don&#x27;t seem to be links.\"5-star rating\"? Really? Who from? I need more information for that to be remotely useful, should be a link. Let&#x27;s say my HN post has a 5-star rating too * * * * *.As a \"beautiful and functional interface\" I&#x27;m afraid the web page scores a 0&#x2F;2, for me. reply davidivadavid 11 hours agorootparentFeel free to give everyone your suggestions to learn UI design. reply ironskull 13 hours agorootparentprevI also started with RefactoringUI and agree that is a good starting point for a newbie. reply funksta 16 hours agoparentprevSeconding this. Not cheap, but so worth the money. Learn UI design and RefactoringUI are the best resources I&#x27;ve found for engineers who want to learn design. I&#x27;m just starting Erik&#x27;s Landing Page course now, and so far it&#x27;s also really good. reply atomicnature 17 hours agoparentprevSeems like a gem of a resource; took a look at the blog, and already found a great principle (\"rule of locality; place button where the data is\") which I had violated just today. Thanks for the recommendation. reply dinkleberg 11 hours agoparentprevAgreed, Erik’s courses are great. reply chiefalchemist 14 hours agoparentprevLanding page design? Well...I had to scroll all the way to the bottom of the home page to find out the course is currently closed. There&#x27;s also no mention of price. reply nprateem 15 hours agoparentprevMaybe those walls of text with testimonials AND THAT&#x27;S NOT ALL!!! work for some.I lost the will to wade through to find out how much the courses are - how much are they roughly? reply pc86 12 hours agorootparentThis, combined with the fact that it&#x27;s an asynchronous video course but somehow \"closed for enrollment\" also left a bad taste in my mouth. I&#x27;m sure they&#x27;re great courses but I&#x27;m not really interested in supporting the artificial scarcity so someone can charge a few hundred dollars more per person. reply ayhanfuat 14 hours agorootparentprevAfter all the praise, I powered through and scrolled all the way but apparently it is closed for enrollment. No mention of the price either. reply ironskull 13 hours agorootparentErik opens the courses up regularly (I would guess every 6-8 weeks). If you sign up for his newsletter (which is also quite good, almost completely original substance like his blog) he will let you know when registration is opening.The approach is definitely unconventional - I would guess the main reason is that selling a high ticket item like this benefits from a longer sales cycle. The other reason he might do this is he has a Slack group where students can post their homework assignments and get feedback. So having a cohort start at the same time might be desirable. As a consumer I would agree with the comments that I would prefer to have them always available for purchase. If you do buy one of them, he makes all of the other courses available at any time.As I mentioned in my post, the price is probably around $1000 for the UI course; the other courses are typically a few hundred dollars cheaper (they aren&#x27;t quite as long). The course prices do tend to increase over time, and I think Erik is very averse to offering discounts. reply j3d 18 hours agoprevNot sure if this fits your goal of a UI design course, but I found Josh Comeau&#x27;s CSS for JS Devs course to be a great way to learn the fundamentals of CSS in a way that resonated with my developer mindset.https:&#x2F;&#x2F;css-for-js.dev&#x2F; reply atomicnature 18 hours agoparentLove the sharp focus on css, looks interesting. Thanks for the rec. reply no_wizard 15 hours agoprevIn addition to the many great recommendations already posted in replies I recommend reading and watching videos about how Steve Jobs approached product design, and watch some infamous Apple keynotes.A part of this is developing a sense of “taste”, which I strongly believe is possible to do but you have to have a certain mindset to do it. This helped me immensely once I realized it.One great website with great tidbits around the creation of the original Macintosh is Folklore[0][0]: https:&#x2F;&#x2F;www.folklore.org&#x2F;index.py reply karaterobot 12 hours agoprevAs a designer, I think the two best exercises you could do would be:1. Look at designs that work or do not work, and ask what makes them work or not work. You may have a gut reaction: examine that reaction in cold blood.But that&#x27;s basic stuff. After you do that, you should ask what the designer had to trade off in order to arrive at that solution. Design is how you solve a problem given a set of goals, requirements, and constraints. If you understand the problem at that level, it&#x27;s a very short path to the design. It&#x27;s trivial to say \"this designer was bad at their job\" if you see a bad product, but it&#x27;s more instructive to understand all the inputs into that bad decision, rather than just judge the output.2. Give a shit. This is what makes someone good at their job—any job. Sweat the details. Do not trust a checklist of steps for \"how to do design good\" any more than you&#x27;d trust a corresponding recipe for \"how to do programming good\".The reason I went from front end development to design is that I found I cared more about getting it right than the original designer who handed me the mockups did, and realized I should be sitting upstream of where I was. If you don&#x27;t give a shit, no course is going to make you a good designer, and if you do give a shit, you won&#x27;t need a course. Along the way, sure, you have to pick up some basic skills, but that&#x27;s trivial, and ought to be second nature for a hacker. reply atomicnature 6 hours agoparentYes that makes. The problem is how does one raise the bar within teams and orgs? There, I think principles do help. Also, for teaching others, it is best if we have a baseline of principles.One can always break the principles once advanced enough reply jokab 12 hours agoparentprev> give a shit.This is probably the best advice I heard in a while.On a slightly different note this is easier said than done specially if you have suspected adhd and live in the UK where you have to wait years before getting proper treatment. reply ndiddy 7 hours agoprevNot a course, but the best resource I&#x27;ve found for UI design is the Windows 2000 interface guidelines book: https:&#x2F;&#x2F;www.amazon.com&#x2F;Microsoft-Windows-Experience-Professi... . It lays out everything (big and small) that made older Windows versions nice to use in an easy to digest format. reply hackermailman 17 hours agoprevDaniel Jackson&#x27;s book https:&#x2F;&#x2F;subconscious.substack.com&#x2F;p&#x2F;concept-design-in-three-...I use it all the time after reading it for example they redesigned git into gitless using these methods to audit and find redundant or confusing features. reply photon_collider 14 hours agoprevI found the book Practical UI (https:&#x2F;&#x2F;www.practical-ui.com) useful for learning practical design tips. I still reference it from time to time in my work. reply WillAdams 13 hours agoprevAn early text on this was:https:&#x2F;&#x2F;www.goodreads.com&#x2F;book&#x2F;show&#x2F;344729.Designing_Visual_...(if it&#x27;s at all possible, get a first edition which has good quality reproductions of the screen grabs)A list of the chapters gives a good idea of the content:- Introduction- Elegance and Simplicity- Scale, Contrast, and Proportion- Organization and Visual Structure- Module and Program --- \"The module is a scale of proportions that makes the bad difficult and the good easy\" Albert Einstein (to Le Corbusier, 1964)- Image and Representation- So What About Style? reply panic 9 hours agoprevThere’s no coherent set of principles or system for UI design. It’s all about what will work or not work for real people. Paying attention to your own feelings about what works or not is the first step—listening to your users and understanding their feelings will take you the rest of the way. reply patcon 17 hours agoprevI recall the HackDesign website&#x2F;course being great a few years ago! Not sure about now, but used to be free...!https:&#x2F;&#x2F;hackdesign.org&#x2F; reply abhinavsharma 15 hours agoprevI&#x27;d like to add https:&#x2F;&#x2F;growth.design&#x2F;psychology to an already great list that&#x27;s building here. reply esafak 15 hours agoprevCan anyone recommend any online communities to have designs critiqued? reply gjsman-1000 18 hours agoprevI wouldn’t say it’s a course as much as a book, but I found Refactoring UI to be pretty helpful. It’s done by the folks that brought us Tailwind.https:&#x2F;&#x2F;www.refactoringui.com&#x2F; reply sxg 18 hours agoparentThe Refactoring UI YouTube videos are super useful too! https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5gdYHlYAKDY&list=PLDVpvW8ghD... reply atomicnature 18 hours agoparentprevLooks like they have lots of examples; will give this a try! reply robertlagrant 12 hours agoprevI think the best practical approach for designing UIs is to download (and buy) Balsamic[0] and use that to design UIs. Cut through the nonsense of colours and pixels in the first instance and just lay things out logically and simply.[0] https:&#x2F;&#x2F;balsamiq.com reply kadomony 12 hours agoparentHeaven and stars, my soul hurts just reading this. reply robertlagrant 12 hours agorootparentWhy&#x27;s that? reply andreasmueller 17 hours agoprevui design should be part of sophisticated ux (human centered design). https:&#x2F;&#x2F;uxqb.org&#x2F;en&#x2F;documents&#x2F;cpux-f-en-curriculum&#x2F; contains valuable ux know-how which meets professional needs. reply ilrwbwrkhv 17 hours agoprevNone. The thing is for UX you have something like don&#x27;t make me think but UI is a matter of taste so everyone has their own rules which might not appeal to you. You can check out the windows 95 design docs as that might give you some ideas and then I would suggest using an uncommon UI component library and let that do the heavy lifting till you can hire a designer. reply ChicagoDave 17 hours agoprevI’d add an additional request. Are there any materials specifically about touch interfaces? reply civilitty 17 hours agoprev> At the same time, I&#x27;d also love to learn more about more \"down to earth\" tutorials&#x2F;examples&#x2F;exercises&#x2F;courses to build practical UI skills. Something above \"react tutorials\", but something below Victor&#x27;s \"Magic Ink\"I have no recommendations for UI in general but for practical UI skills I really like Every Layout [1] which covers common page layouts and how to make them responsive beyond just media queries.[1] https:&#x2F;&#x2F;every-layout.dev&#x2F; reply couchand 12 hours agoparentStrong recommendation for Every Layout, and any other project from creators Andy Bell and Heydon Pickering.What I find particularly compelling about that resource is its structure. It&#x27;s not just a list of recipes (despite what the landing page suggests). They build it all up from a foundation of general principles which provide guidance in visual design far beyond the enumerated examples [0]. Then by showing their work developing each example, they show how to apply the system of design thinking. It&#x27;s really quite elegant! reply rahoulb 13 hours agoparentprevI found Every Layout incredibly useful and, even when I’m not using their components I rarely find myself writing a piece of CSS that just won’t do what I want (an all to common occurrence before the book). reply waprin 16 hours agoprevI am a long-time developer who&#x27;s always dreamed of building an indie software business but design skills hold me back.I recognize this and get plenty of feedback around it. So this year I set out to improve to at least try to get to \"mediocre\" instead of \"terrible\".Refactoring UI and Erik Kennedys blog &#x2F; class are mentioned and are great resources and I own both.I did Dribbble&#x27;s Figma UI design class which was $600. It&#x27;s biggest strength is that its a cohort based class, and cohort classes tend to have much higher finishing rates than self-paced classes. Their instructor will review your Figma designs but only if you finish in time so if you want to get your $600 worth you better open up Figma, so I recommend it for that reason. Kennedy&#x27;s is self-paced and while it&#x27;s extremely high quality, I haven&#x27;t even worked through most of it for this reason.Of course, the single most important thing you can do is build lots of UIs. If you&#x27;re like me, your UIs will suck, but if you do it more regularly, you will also notice more UI&#x2F;UX techniques on other websites. I save all those in a Notion database organized by category and refer to them.One thing I almost never see mentioned but it was a really good piece of advice. I told someone that I was between hiring contractor designers for my project, and trying to improve at design and do it myself. One person told me, it&#x27;s not mutually exclusive. So you can design an app, and it will probably look bad. Then hire an experienced UI&#x2F;UX designer off Upwork to do a better job. And pay attention to the decisions they made and the decsions you made and compare the difference. Figma is a great tool these days because it&#x27;s much more collaborative than just getting a big stack of PNGs or SVGs at the end, you can discuss design choices in Figma comments as the designer works.Another thing worth noting - professional designers will make several versions and iterations of everything, each screen and each component on that screen. And then pick the best one. The Dribbble instructor said, the best design is almost never the first one. This is time consuming and tedious if you don&#x27;t love design but it&#x27;s how you get the best results.If you just have a one-off project and don&#x27;t truly care about improving at design, the simplest option is to hire a contractor. UI&#x2F;UX is not something you learn in a weekend and then you&#x27;re good to go, it&#x27;s more like learning a language or an instrument in that you&#x27;re either going to invest a lot of time to learn it well or you&#x27;re going to suck. It&#x27;s pretty affordable to hire-out because it&#x27;s mostly up-front work.Hiring contractors and spending for classes is the expensive route but spending money can expedite the process. But, there&#x27;s lots of free resources if you&#x27;re broke. The single most important thing is design a lot, and pay attention to other people&#x27;s designs and what they&#x27;re doing. reply atomicnature 5 hours agoparentThanks for sharing your experience and wisdom. The dribble figma course sounds interesting, will check it out! reply b2bsaas00 13 hours agoprevRefactoring UI reply adriangrigore 14 hours agoprev [–] Best rule I believe is just iterate until satisfied. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The writer is looking for UI design courses that are practical and provide clear principles to apply in their specific context.",
      "They are aware of avant-garde works like Bret Victor's \"Magic Ink\" but are seeking more \"down to earth\" tutorials, examples, exercises, or courses.",
      "The writer wants recommendations for UI design resources that are more grounded and applicable."
    ],
    "commentSummary": [
      "The article is a collection of discussions and recommendations on UI design resources, covering books, courses, and websites.",
      "Topics discussed include the value of whitespace in design, the effectiveness of reading books for design skills, and the role and expertise of designers in tech companies.",
      "The article emphasizes the importance of practical application and practice in gaining design skills and mentions both paid and free resources for learning UI design."
    ],
    "points": 284,
    "commentCount": 102,
    "retryCount": 0,
    "time": 1702215463
  },
  {
    "id": 38590984,
    "title": "AST-grep: CLI Tool for Code Structural Search, Linting, and Rewriting",
    "originLink": "https://github.com/ast-grep/ast-grep",
    "originBody": "ast-grep(sg) ast-grep(sg) is a CLI tool for code structural search, lint, and rewriting. Introduction ast-grep is a AST-based tool to search code by pattern code. Think it as your old-friend grep but it matches AST nodes instead of text. You can write patterns as if you are writing ordinary code. It will match all code that has the same syntactical structure. You can use $ sign + upper case letters as wildcard, e.g. $MATCH, to match any single AST node. Think it as REGEX dot ., except it is not textual. Try the online playground for a taste! Demo Installation You can install it from npm, pip, cargo, homebrew or scoop! npm install --global @ast-grep/cli pip install ast-grep-cli cargo install ast-grep # install via homebrew, thank @henryhchchc brew install ast-grep # install via scoop, thank @brian6932 scoop install main/ast-grep # install via MacPorts sudo port install ast-grep Or you can build ast-grep from source. You need install rustup, clone the repository and then cargo install --path ./crates/cli Packages are available on other platforms too. Command line usage example ast-grep has following form. sg --pattern 'var code = $PATTERN' --rewrite 'let code = new $PATTERN' --lang ts Example Rewrite code in null coalescing operator sg -p '$A && $A()' -l ts -r '$A?.()' Rewrite Zodios sg -p 'new Zodios($URL, $CONF as const,)' -l ts -r 'new Zodios($URL, $CONF)' -i Implement eslint rule using YAML. Sponsor If you find ast-grep interesting and useful for your work, please buy me a coffee so I can spend more time on the project! Feature Highlight ast-grep's core is an algorithm to search and replace code based on abstract syntax tree produced by tree-sitter. It can help you to do lightweight static analysis and massive scale code manipulation in an intuitive way. Key highlights: An intuitive pattern to find and replace AST. ast-grep's pattern looks like ordinary code you would write every day. (You can call the pattern is isomorphic to code). jQuery like API for AST traversal and manipulation. YAML configuration to write new linting rules or code modification. Written in compiled language, with tree-sitter based parsing and utilizing multiple cores. Beautiful command line interface :) ast-grep's vision is to democratize abstract syntax tree magic and to liberate one from cumbersome AST programming! If you are an open source library author, ast-grep can help your library users adopt breaking changes more easily. if you are a tech lead in your team, ast-grep can help you enforce code best practice tailored to your business need. If you are a security researcher, ast-grep can help you write rules much faster. CLI Screenshot Search Feature Command Screenshot Search sg -p 'Some($A)' -l rsRewrite sg -p '$F && $F($$$ARGS)' -r '$F?.($$$ARGS)' -l tsReport sg scan",
    "commentLink": "https://news.ycombinator.com/item?id=38590984",
    "commentBody": "AST-grep(sg) is a CLI tool for code structural search, lint, and rewritingHacker NewspastloginAST-grep(sg) is a CLI tool for code structural search, lint, and rewriting (github.com/ast-grep) 259 points by methou 21 hours ago| hidepastfavorite70 comments jgke 0 minutes agoCool! Like many others here, I also relatively recently created a similar tool called syntax-searcher or syns[1] which is focused on searching, and doesn&#x27;t support replacing. It&#x27;s implemented with a hand-written tokenizer&#x2F;parser rather than using eg. tree-sitter. It works best with (mostly) context-free languages, but it doesn&#x27;t crash or anything with Python and the like. At least I find its query syntax better than the alternatives, but it&#x27;s probably because I wrote it :)I find the best uses for it being answering questions like &#x27;how is this function called&#x27; and &#x27;what is this struct&#x27;s definition&#x27;:# find struct definition $ syns &#x27;struct Span {}&#x27; [.&#x2F;src&#x2F;psi.rs:21-26] pub struct Span { &#x2F;&#x2F;&#x2F; Starting byte index of the span. pub lo: usize, &#x2F;&#x2F;&#x2F; End byte index of the span. pub hi: usize, }# How is this function called? $ syns &#x27;ast_match()&#x27; [.&#x2F;src&#x2F;query.rs:91] if &op.ty == op1 && self.ast_match(content1, &[*start]).is_some() { [.&#x2F;src&#x2F;query.rs:125] .flat_map(move |tts| self.ast_match(tts, &[self.machine.initial]))It&#x27;s also pretty fast for most repositories, as an extreme case with the kernel source: time syns &#x27;kmalloc()&#x27; > &#x2F;dev&#x2F;null syns &#x27;kmalloc()&#x27; > &#x2F;dev&#x2F;null 61,82s user 0,90s system 99% cpu 1:02,73 total Most other repositories print all results pretty much instantly.[1] https:&#x2F;&#x2F;github.com&#x2F;jgke&#x2F;syntax-searcher reply anotherpaulg 18 hours agoprevI&#x27;ll share my similarly named tool `grep-ast` [0], which sort of does the opposite of the OP&#x27;s `ast-grep`. The OP&#x27;s tool lets you specify your search as a chunk of code&#x2F;AST (and then do AST transforms on matches).My tool let&#x27;s you grep a regex as usual, but shows you the matches in a helpful AST aware way. It works with most popular languages, thanks to tree-sitter.It uses the abstract syntax tree (AST) of the source code to show how the matching lines fit into the code structure. It shows relevant code from every layer of the AST, above and below the matches. It&#x27;s useful when you&#x27;re grepping to understand how functions, classes, variables etc are used within a non-trivial codebase.Here&#x27;s a snippet that shows grep-ast searching the django repo. Notice that it finds `ROOT_URLCONF` and then shows you the method and class that contain the matching line, including a helpful part of the docstring. If you ran this in the terminal, it would also colorize the matches. django$ grep-ast ROOT_URLCONF middleware&#x2F;locale.py: │from django.conf import settings │from django.conf.urls.i18n import is_language_prefix_patterns_used │from django.http import HttpResponseRedirect ⋮... │class LocaleMiddleware(MiddlewareMixin): │ \"\"\" │ Parse a request and decide what translation │ object to install in the current thread context. ⋮... │ def process_request(self, request): ▶ urlconf = getattr(request, \"urlconf\", settings.ROOT_URLCONF)[0] https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;grep-ast reply herrington_d 15 hours agoparentHey paulg, ast-grep author here! This is something I also want to do in ast-grep! ast-grep prints the surrounding lines around matches but they are not aware of which function&#x2F;scope the matches are in. May I ask how you do the scope detection in a general fashion? (say language agnostic) https:&#x2F;&#x2F;github.com&#x2F;ast-grep&#x2F;ast-grep&#x2F;issues&#x2F;155 reply anotherpaulg 12 hours agorootparentNice, thanks for checking out grep-ast.The command line tool is a thin wrapper around the `TreeContext` class, whose purpose is show you a set of \"lines of interest\" in the context of the entire AST. This all exists because my other project aider [0] uses TreeContext to display a repository map [1] so that GPT-4 can understand how the most important classes, methods, functions, etc fit into the entire code base of a git repository.But it was easy to make a CLI interface to grep lines of interest and display them with TreeContext, and it turned out to be quite useful.The TreeContext class is line-oriented, and is mainly interested in tracking language constructs whose scope spans multiple lines. Typically these are things like classes, methods, functions, loops, if&#x2F;else constructs, etc. Given a line of interest, we look at all the multi-line scopes which contain it. For each such multi-line scope, we want to display some \"header\" lines to provide context.In this example, the match for \"two\" is contained in the multi-line scopes of a method and a class. So we print their headers. $ grep-ast two example.py ⋮... │class MyClass: │ \"MyClass is great\" ⋮... │ def print2(self): ▶ print(\"two\") ⋮...The trick is how to determine the header for each multi-line scope? It&#x27;s not ideal to just use the first line. For example, it&#x27;s nice that the header for the class included the docstring as well as the bare `class MyClass:` line.For any multi-line scope, we look at all the other AST scopes which start on the same line. We take the smallest such co-occurring scope, and declare the header to be the lines that it spans. For a simple method like `def print2(self):`, that&#x27;s all that gets picked up.But a complex method like `print1()` below picks up all the lines which are part of its full function signature: $ grep-ast one example.py ⋮... │class MyClass: │ \"MyClass is great\" ⋮... │ def print1( │ self, │ prefix, │ suffix, │ ): ⋮... ▶ print(f\"{prefix} one {suffix}\") ⋮...It&#x27;s a heuristic, but it seems to work well in practice.[0] https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider[1] https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;repomap.html reply mingodad 9 hours agoparentprevThere is also gram_grep[0] \"Search text using a grammar, lexer, or straight regex. Chain searches for greater refinement.\"See also parsertl-playground[1] for online edit&#x2F;test grammars.[0]https:&#x2F;&#x2F;github.com&#x2F;BenHanson&#x2F;gram_grep[1]https:&#x2F;&#x2F;mingodad.github.io&#x2F;parsertl-playground&#x2F;playground&#x2F; reply alexpovel 20 hours agoprevWow! What a coincidence. Just the other day I finished \"v1\" of a similar tool: https:&#x2F;&#x2F;github.com&#x2F;alexpovel&#x2F;srgn , calling it a combination of tr&#x2F;sed, ripgrep and tree-sitter. It&#x27;s more about editing code in-place, not finding matches.I&#x27;ve spent a lot of time trying to find similar tools, and even list them in the README, but `AST-grep` did not come up! I was a bit confused, as I was sure such a thing must exist already. AST-grep looks much more capable and dynamic, great work, especially around the variable syntax. reply tekacs 19 hours agoparentThis looks really interesting, thank you for putting this together! I’ll likely give it a go today. I say that as someone who has explored quite a few of these and found them mostly quite basic. srgn looks like more than the usual.One minor comment: I personally found the first Python example involving a docstring a little hard to parse (ha). It may show a variety of features, but in particular I found that it was hard to spot at a glance what had changed.If you could use diff formatting or a screenshot with color to show the differences it would make it much easier to follow. If I get around to using it later today, I might submit a PR for that. :) reply alexpovel 18 hours agorootparent> diff formattingThank you for the feedback! That sounds good, I&#x27;ll add that. reply larve 9 hours agoparentprevI&#x27;ll post my own crappy one called oak which uses templates to render the result of tree-sitter queries.https:&#x2F;&#x2F;github.com&#x2F;go-go-golems&#x2F;oakI initially hope the queries would be more powerful, but they are really not. You can write queries and a resulting template in a yaml file. The program will scan a list of repositories for all these YAML files, and expose them as command line verbs.Here is one to find go definitions:https:&#x2F;&#x2F;github.com&#x2F;go-go-golems&#x2F;oak&#x2F;blob&#x2F;main&#x2F;cmd&#x2F;oak&#x2F;querie...This can then be run as: oak go definitions &#x2F;home&#x2F;manuel&#x2F;code&#x2F;wesen&#x2F;corporate-headquarters&#x2F;geppetto&#x2F;pkg&#x2F;cmds&#x2F;cmd.go type GeppettoCommandDescription struct { Name string `yaml:\"name\"` Short string `yaml:\"short\"` Long string `yaml:\"long,omitempty\"` Flags []*parameters.ParameterDefinition `yaml:\"flags,omitempty\"` Arguments []*parameters.ParameterDefinition `yaml:\"arguments,omitempty\"` Layers []layers.ParameterLayer `yaml:\"layers,omitempty\"` Prompt string `yaml:\"prompt,omitempty\"` Messages []*geppetto_context.Message `yaml:\"messages,omitempty\"` SystemPrompt string `yaml:\"system-prompt,omitempty\"` } type GeppettoCommand struct { *glazedcmds.CommandDescription StepSettings *settings.StepSettings Prompt string Messages []*geppetto_context.Message SystemPrompt string }While I can use it for good effect for LLM prompting as is, I really would like to add a unification algorithm (like the one in Peter Norvig&#x27;s Prolog compiler) to get better queries, and connect it to LSP as well. reply alchemist1e9 18 hours agoparentprevSuch an awesome idea and useful tool!Do you use tree-sitter for the AST part also? reply alexpovel 18 hours agorootparentExactly, all the parsing is done by tree-sitter. The Rust bindings to the tree-sitter C lib are a \"first-class consumer\". reply simonw 14 hours agoprevSomething I find really interesting about this is the way the tool is packaged.You can install the CLI utility in four different ways: https:&#x2F;&#x2F;ast-grep.github.io&#x2F;guide&#x2F;quick-start.html#installati... # via Homebrew brew install ast-grep # via Cargo cargo install ast-grep # via npm npm i @ast-grep&#x2F;cli -g # via pip pip install ast-grep-cli # I tested and pipx works too: pipx install ast-grep-cliI really like this - it means the tool is available to people with familiarity of any of those four distribution mechanisms.You can also download pre-built binaries from their releases page: https:&#x2F;&#x2F;github.com&#x2F;ast-grep&#x2F;ast-grep&#x2F;releases&#x2F;tag&#x2F;0.14.2On top of that, they offer API bindings for it in three different languages:- Rust (not yet stable): https:&#x2F;&#x2F;docs.rs&#x2F;ast-grep-core&#x2F;latest&#x2F;ast_grep_core&#x2F;- JavaScript&#x2F;TypeScript: https:&#x2F;&#x2F;ast-grep.github.io&#x2F;guide&#x2F;api-usage&#x2F;js-api.html- Python: https:&#x2F;&#x2F;ast-grep.github.io&#x2F;guide&#x2F;api-usage&#x2F;py-api.htmlIt&#x27;s rare to see a tool&#x2F;library offer this depth of language support out of the box. reply simonw 13 hours agoparentI was curious so I had a look at how the \"pip install ast-grep-cli\" command works. It downloads a wheel for the correct platform from https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;ast-grep-cli&#x2F;#filesThe wheel just contains the two binaries (sg and ast-grep) and no Python code: $ unzip -l ast_grep_cli-0.14.2-py3-none-macosx_10_7_x86_64.whl Archive: ast_grep_cli-0.14.2-py3-none-macosx_10_7_x86_64.whl Length Date Time Name --------- ---------- ----- ---- 6207 12-03-2023 07:34 ast_grep_cli-0.14.2.dist-info&#x2F;METADATA 102 12-03-2023 07:34 ast_grep_cli-0.14.2.dist-info&#x2F;WHEEL 1077 12-03-2023 07:34 ast_grep_cli-0.14.2.dist-info&#x2F;license_files&#x2F;LICENSE 1077 12-03-2023 07:34 ast_grep_cli-0.14.2.dist-info&#x2F;license_files&#x2F;LICENSE 32865880 12-03-2023 07:34 ast_grep_cli-0.14.2.data&#x2F;scripts&#x2F;sg 32865880 12-03-2023 07:34 ast_grep_cli-0.14.2.data&#x2F;scripts&#x2F;ast-grep 639 12-03-2023 07:34 ast_grep_cli-0.14.2.dist-info&#x2F;RECORD --------- ------- 65740862 7 filesI haven&#x27;t seen pip and wheels used to distribute a purely binary tool like this before. reply charliermarsh 13 hours agorootparentThis is how Ruff works too! (Ruff is also a standalone binary with no Python dependency.) If you&#x27;re interested, I recommend checking out Maturin, which makes this pretty easy -- you can ship any standalone Rust binary as a Python package by zipping it into a wheel. reply herrington_d 13 hours agorootparentI confess I stole the pip recipe from Charlie :Dhttps:&#x2F;&#x2F;github.com&#x2F;astral-sh&#x2F;ruff&#x2F;blob&#x2F;main&#x2F;.github&#x2F;workflow... reply morgante 16 hours agoprevAST-grep is well done - the speed is particularly impressive and it&#x27;s quite easy to get started with.One of the downsides of the simplicity is that rules are written in yaml. This works nicely for simple rules, but if you try to save a complex migration as a rule, you end up programming in YAML (which is very hard).For my similar tool we decided to build a full query language for matching code, called GritQL: https:&#x2F;&#x2F;docs.grit.io&#x2F;tutorials&#x2F;gritql reply herrington_d 14 hours agoparentHey morgante, nice to meet you again! Indeed YAML is a compromise between expressiveness and easy-learning. Grit did a great job for providing advanced code manipulation! reply eloh 19 hours agoprevThere is also a neovim plugin doing structural search&#x2F;replace, also based on treesitter: https:&#x2F;&#x2F;github.com&#x2F;cshuaimin&#x2F;ssr.nvim reply xwowsersx 6 hours agoparentAwesome, needed this! reply zX41ZdbW 5 hours agoprevIt&#x27;s strange, but I cannot make it work.I did this: cargo install ast-grepThen I&#x27;m searching in my code with: $ sg --pattern &#x27;catch (...) { $$$ARGS }&#x27; --lang c++ .&#x2F;Server&#x2F;TCPHandler.cpp 657│ catch (...) 658│ { 659│ state.io.onException(); 660│ exception = std::make_unique(ErrorCodes::UNKNOWN_EXCEPTION, \"Unknown exception\"); 661│ }But there should be more than one result.It also does not work in Playground: https:&#x2F;&#x2F;ast-grep.github.io&#x2F;playground.html#eyJtb2RlIjoiUGF0Y... reply tedunangst 13 hours agoprevA looping gif is an unfortunate choice for a demo. It looks cool to start, but then I&#x27;m trying to see what it&#x27;s done when it restarts and I have to sit through it again. Some before and after still screenshots would help. reply adr1an 1 hour agoparentIndeed, asciinema embed would be better suited. I have seen so many of these gifs in readmes..! :( reply mayama 5 hours agoparentprevIf you have mpv or avconv&#x2F;ffmpeg based video player, you can play and seek the in the gif video file. You can use `mpv http-gif-url` to play it directly. reply eviks 13 hours agoparentprevindeed, this is purely text demo, and it wastes too much time with slow typing in the video while also preventing you from using search reply elanning 13 hours agoprevAlso plugging my related project: https:&#x2F;&#x2F;github.com&#x2F;Ichigo-Labs&#x2F;cgrep From the comments in this thread, it seems a lot of people have built or needed an easy way to quickly create static analysis checks, without a bunch of hassle. I think extended regex is a great way to do this. reply cglong 12 hours agoprevI was hoping this could be a local replacement for Azure DevOps&#x27;s functional code search[1], but this seems lower-level than that. Basically, I want a tool where I can write something like `class:Logger` and it&#x27;ll show me which file(s) define a class with that name, or `ref:Logger` to find all usages of that&#x2F;those class(es).[1]: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;devops&#x2F;project&#x2F;searc... reply hprotagonist 20 hours agoprevNice to see treesitter showing up in tools that aren’t just syntax highlighting. reply teo_zero 12 hours agoparentBesides, it doesn&#x27;t shine at syntax highlighting, either! In the sense that it doesn&#x27;t add anything that the traditional text-based algorithms embedded in practically any text editor can&#x27;t already do. For example, if I declare a variable called \"something\", it should highlight all successive occurrences of \"something\" in a remarkably different style than \"somethink\". And the \"a\" in \"sizeof(a)\" should be rendered differently when it&#x27;s a variable and when it&#x27;s a type. reply herrington_d 14 hours agoparentprevtreesitter gives us a uniform interface to parse and manipulate code, which is awe-inspiring work. I wish tree-sitter could have more contributors to the core library. It still has a lot of improvement space.Say, like performance. tree-sitter&#x27;s initial parsing speed can be easily beaten by a carefully hand-crafted parser. Tree-sitter, written in C, has a similar JavaScript parsing speed as Babel, a JS-based parser. See the benchmark https:&#x2F;&#x2F;dev.to&#x2F;herrington_darkholme&#x2F;benchmark-typescript-par... reply norir 18 hours agoprevThe problem with any tree-sitter based tool is that there will typically be edge cases where the tree-sitter parser is wrong. Probably not a big deal most of the time, but it makes me wary of using it for security. reply Noumenon72 14 hours agoparentWhat does it mean to use grep \"for security\"? reply richbell 13 hours agorootparentE.g., \"I just read about CVE-2007-4559 being exploited in the wild. Are we using this vulnerable method?\" reply beardedwizard 20 hours agoprevIs this meant to compliment or compete with semgrep? reply ekidd 20 hours agoparentWell, when I seach for \"semgrep\", I get a very nice corporate landing page with a \"Book Demo\" button. Which is a level of hassle that just isn&#x27;t worth it for smaller teams, because \"Book Demo\" usually means \"We&#x27;re going to do a dance to see how much money we can extract from you.\" Which smaller teams may only want to do for a handful of key tools.(4 years ago, I was more willing to put up with enterprise licensing. But in the last two years, I&#x27;ve seen way too many enterprise vendors try to squeeze every penny they can get from existing clients. An enterprise sales process now often means \"Expect 30% annual price hikes once you&#x27;re in too deep to back out.\" The lack of easy VC money seems to have made some enterprise vendors pretty desperate.)There&#x27;s also an open source \"semgrep\" project here: https:&#x2F;&#x2F;github.com&#x2F;semgrep&#x2F;semgrep. But this seems to be basically a vulernability scanner, going by the README.Whereas AST-grep seems to focus heavily on things like:1. One-off searching: \"Search my tree for this pattern.\"2. Refactoring: \"Replace this pattern with this other pattern.\"AST-grep also includes a vulnerability scanning mode like semgrep.It&#x27;s possible that semgrep also has nice support for (1) and (2), but it isn&#x27;t clearly visible on their corporate landing page or the first open source README I found. reply icholy 19 hours agorootparentSemgrep is capable of one-off searching and refactoring. I agreed that the docs are a little hard to navigate. reply herrington_d 14 hours agorootparentprevThank ekidd for your kind words! ast-grep author here. This is a hobby project and mainly focuses on developers&#x27; daily job like search and linting. Appreciate you like it!Semgrep&#x27;s vulnerability scanning is much more advanced, mostly for enterprise security usage. reply andrewshadura 20 hours agoparentprevWell, it is semgrep (hence sg). reply beardedwizard 17 hours agorootparentyeah I had this feeling a bit, I guess im curious what problems they solve differently (if any). My sense it that semgrep is an enterprise managed solution of the same kind (and btw, is still itself OSS) reply icholy 19 hours agoparentprevLooks like a competitor to me. reply herrington_d 14 hours agoparentprevHi, ast-grep author here. This is a great question and I asked this in the first place before I started the hobby project.TLDR; I designed ast-grep to be on different tracks than semgrep.Semgrep is for security and ast-grep is for development.First and foremost, I have always been in awe of semgrep. Semgrep&#x27;s documentation, product sites and Padioleau&#x27;s podcast all gave me a lot of inspiration. Using code to find code is such a cool idea that I never need to craft an intricate regex or write a lengthy AST program. sgrep and patch from https:&#x2F;&#x2F;github.com&#x2F;facebookarchive&#x2F;pfff&#x2F;wiki&#x2F;Sgrep have helped me a lot in real large codebases.When I used semgrep as a software engineer, instead of a security researcher, I found semgrep has not touched too much on routine development works. I can use `semgrep -e PATTERN` but the Python wrapper is not too fast compared to grep. While pattern is cool, it cannot precisely match some syntax nodes. (example, selecting generator expression in Semgrep is very hard). It also does not have API to find code programmatically.I have also a short summary for tool comparison. https:&#x2F;&#x2F;ast-grep.github.io&#x2F;advanced&#x2F;tool-comparison.html reply herrington_d 13 hours agorootparentWhy I think semgrep is a security tool different from ast-grep:* Semgrep is security focused. It has many advanced static analysis features in its core product, such as dataflow analysis, symbolic propagation, and semantic equivalence, all of which are useful for security analysis. They are not available in ast-grep. * Semgrep’s pattern syntax also prefers matching more potentially vulnerable semantics than matching precise syntax. Semantic level information is the better level of abstraction for security model. ast-grep, on the other hand, sticks to faithfully translating users&#x27; queries syntactically. * Semgrep has a one-off search and rewrite feature, but it is not its primary focus. The CLI is a bit slow compared to other tools. ast-grep strives to be a fast CLI tool. * Semgrep has a product matrix for vulnerability detection: detecting secrets, supply chain vulnerabilities, and cross-file detection. It also has a plethora of security rules in the registry. These features will not be included in ast-grep. reply gillh 11 hours agoprevInteresting use-case: We recently started using ast-grep at CodeRabbit[0] to review pull request changes with AI.We use gpt-4 to generate ast-grep patterns to deep-dive and verify pull-request integrity. We just rolled this feature out 3 days back and are getting excellent results!Comments such as these are powered by AI-generate ast-grep queries: https:&#x2F;&#x2F;github.com&#x2F;amorphie&#x2F;contract&#x2F;pull&#x2F;100#discussion_r14...[0]: https:&#x2F;&#x2F;coderabbit.ai reply Conscat 13 hours agoprevI&#x27;ve tried using this, but the documentation and learning resources weren&#x27;t very good (at least at the time ~6 months ago) and structuring refactors with YAML made it very cumbersome for me to write and edit one-off commands.Tree Sitter also leaves a lot to be desired for C++ editing, but that&#x27;s a special problem. reply simonw 13 hours agoparentLooks like the project is only about 12 months old, so if you last checked it out 6 months ago it&#x27;s worth taking another look.Was it possible to use it entirely as a CLI tool without any YAML 6 months ago? reply Conscat 11 hours agorootparentUnless the search&#x2F;replace is super simple, you need the YAML as far as I can tell. The refactor I gave up on automating had to do with changing variadic C++ macros into arithmetic expressions, which wasn&#x27;t conceptually very complicated, but felt almost impossible while constantly tripping over YAML syntax errors. reply simonw 11 hours agorootparentThe YAML syntax I find most useful for this kind of thing is this: something: subkey:I can put any characters I like in here And they \"won&#x27;t be messed up\" by anything Because they are part of a multi-line string reply gushogg-blake 18 hours agoprevI came up with a similar concept for in-editor SSR as an extension to existing find&#x2F;replace functionality: https:&#x2F;&#x2F;codepatterns.io&#x2F;It worked great for the use case I built it around initially but I think it would need a scripting&#x2F;logic component to generalise to any conceivable refactoring. reply gpuhacker 20 hours agoprevDoes anyone happen to know of a similar tool that can compare two codes for semantic similarity? reply LelouBil 20 hours agoparentMaybe look here (never used it though)https:&#x2F;&#x2F;github.com&#x2F;Wilfred&#x2F;difftastic reply dorian-graph 19 hours agorootparentOr https:&#x2F;&#x2F;github.com&#x2F;afnanenayet&#x2F;diffsitter. I&#x27;ve tried both and I like them. No preference or notable opinions on them yet! reply benmanns 19 hours agoparentprevYou could try embedding the two codes with an LLM and run any number of similarity measures on the output vectors. reply _a_a_a_ 20 hours agoparentprevdefine &#x27;semantic similarity&#x27;would your hoped-for tool recognise that 1and sin(x)^2 + cos(x)^2are the same? (I think that identity holds, but if not you get the picture) reply mst 18 hours agorootparentThat looks like a case where \"analyse the AST after constant folding\" might be a theoretical path if you had a language frontend that could emit the AST at that point.I suspect that things like \"these two functions both start with the same conditional+early return\" would be more useful to -me- given the sort of things I tend to be working on. Also a &#x27;fuzzy possible copy+paste detector&#x27; in general to help identify refactoring targets.It also strikes me that something that was mostly &#x27;just&#x27; a structure-aware diff so e.g. you got diffs within-if-body and similar but I&#x27;m now into vigorous hand waving because it&#x27;s been ages since I&#x27;ve thought about this and I probably need more coffee. I -did- do a pure maths degree many years ago but I don&#x27;t generally seem to end up working on computational code reply thfuran 15 hours agorootparentprevNot with floats it isn&#x27;t. reply _a_a_a_ 14 hours agorootparentumm, touche reply _a_a_a_ 19 hours agorootparentprevto the downvoter: I thought this was a reasonable question? Semantic equivalence is IIRC undecidable in general. Some languages (Backus&#x27; FL?) try to deal with that but I dunno. reply tyingq 18 hours agorootparent> Semantic equivalence is IIRC undecidable in general.They did mention code, and said \"similarity\" rather than equivalence.But, as a trivial example, two different pieces of code can compile down to the same AST, or bytecode, or assembler. reply elric 18 hours agoprevIf you&#x27;re into this sort of thing, there&#x27;s OpenRewrite[1] for the Java ecosystem.[1] https:&#x2F;&#x2F;docs.openrewrite.org&#x2F; reply Phelinofist 17 hours agoprevSo this is like a more general Coccinelle? reply wslh 19 hours agoprevELI5: should you specify the target language? The example is in TS, how we expand it to other programming languages? reply simonw 17 hours agoparentThere is a list of supported languages here: https:&#x2F;&#x2F;ast-grep.github.io&#x2F;guide&#x2F;introduction.html#supported...If you leave off the language command line option it detects the language from the extension on your files. reply lyjackal 18 hours agoparentprevI see an -l tsAnd an -l rsIn the examples. Those target typescript and rust. Looks like it’s built in tree-sitter, so presumably any language that supports that should work reply wslh 18 hours agorootparentI understand this approach is different from Semmle [1] (has queries and states). Do you know if they are modern alternatives to it?[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Semmle reply svilen_dobrev 18 hours agoprevhey.. are these tools (or combination there of) capable of converting parts of code in one language to another? Given no (or minimum) idiosyncracies... e.g. python to javascript or other way around? (And no, ML is not the answer, i need provable correctness) reply morgante 16 hours agoparentI&#x27;ve done a lot of work in this space, and unfortunately the answer is largely no.These provide a nice frontend for writing simple rules, but I would not want to (essentially) write an entire transpiler in yaml.For Python->JavaScript, you likely want a transpiler focused specifically on that.Unfortunately, every such effort eventually hits serious limits in the emergent complexity for languages. There&#x27;s a reason most of the SOTA techniques ML-based. reply herrington_d 13 hours agoparentprevProvable correctness means you have to model your source and target languages. And then translate the source model to the target model. It is theoretically possible, but in practice, modeling an industry language is way too much work. Some languages do not even have a spec :&#x2F; reply da39a3ee 16 hours agoprev [–] This looks exciting. One thing I&#x27;ve always wanted to do is search Rust code but excluding code in tests (marked by a #[cfg(test)] annotation). Can it do that?I certainly hope some excellent AST-based CLI code search tools come to exist; hopefully this is one of them. reply herrington_d 13 hours agoparent [–] Of course, it gets you covered.https:&#x2F;&#x2F;ast-grep.github.io&#x2F;playground.html#eyJtb2RlIjoiQ29uZ...I have the same problem also, haha, https:&#x2F;&#x2F;x.com&#x2F;hd_nvim&#x2F;status&#x2F;1667059966111547392 reply da39a3ee 13 hours agorootparent [–] Thanks! How would you do that for a #[cfg(test)] attribute in Rust? (I believe that the true identifier of test code; `mod test {}` is just a convention). I assume Rust attributes \"wrap\" the AST node rooted at the node that follows them? reply herrington_d 4 hours agorootparent [–] Yes you can!https:&#x2F;&#x2F;ast-grep.github.io&#x2F;playground.html#eyJtb2RlIjoiQ29uZ... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AST-grep is a command-line tool that enables code structural search, linting, and rewriting by leveraging abstract syntax trees (AST).",
      "It functions similar to grep, but instead of matching text, it matches AST nodes.",
      "Users can write code-like patterns to match code with the same syntactical structure.",
      "AST-grep provides advanced code manipulation, intuitive pattern matching, and a jQuery-like API for AST traversal.",
      "It can be installed through package managers or built from source.",
      "The tool aims to simplify AST programming and is particularly useful for open source library authors, tech leads, and security researchers."
    ],
    "commentSummary": [
      "Ast-grep, grep-ast, and syntax-searcher are highlighted as effective tools for code structural search, linting, and rewriting, offering functionality in finding and displaying code matches.",
      "Concerns are raised about the documentation and complexity of these tools, indicating areas that could be improved.",
      "Other tools such as semgrep, Conscat, and OpenRewrite are mentioned as options for code refactoring and identifying semantic similarity, expanding the range of available tools for developers.",
      "The limitations of language modeling and the challenges of determining semantic equivalence are acknowledged, highlighting the complexity of these tasks.",
      "Potential enhancements and improvements for these tools are discussed, showcasing a commitment to ongoing development and innovation in the field of code analysis and refinement."
    ],
    "points": 259,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1702209798
  },
  {
    "id": 38590827,
    "title": "Krasue: Stealthy Linux Rootkit Invades Thai Telecoms Undetected for 2 Years",
    "originLink": "https://arstechnica.com/security/2023/12/stealthy-linux-rootkit-found-in-the-wild-after-going-undetected-for-2-years/",
    "originBody": "MORE FUN WITH ROOTKITS — Stealthy Linux rootkit found in the wild after going undetected for 2 years Krasue infects telecom firms in Thailand using techniques for staying under the radar. Dan Goodin - 12/8/2023, 8:54 PM Enlarge reader comments 32 Stealthy and multifunctional Linux malware that has been infecting telecommunications companies went largely unnoticed for two years until being documented for the first time by researchers on Thursday. Researchers from security firm Group-IB have named the remote access trojan “Krasue,” after a nocturnal spirit depicted in Southeast Asian folklore “floating in mid-air, with no torso, just her intestines hanging from below her chin.” The researchers chose the name because evidence to date shows it almost exclusively targets victims in Thailand and “poses a severe risk to critical systems and sensitive data given that it is able to grant attackers remote access to the targeted network. According to the researchers: Krasue is a Linux Remote Access Trojan that has been active since 20 and predominantly targets organizations in Thailand. Group-IB can confirm that telecommunications companies were targeted by Krasue. The malware contains several embedded rootkits to support different Linux kernel versions. Krasue’s rootkit is drawn from public sources (3 open-source Linux Kernel Module rootkits), as is the case with many Linux rootkits. The rootkit can hook the `kill()` syscall, network-related functions, and file listing operations in order to hide its activities and evade detection. Notably, Krasue uses RTSP (Real-Time Streaming Protocol) messages to serve as a disguised “alive ping,” a tactic rarely seen in the wild. This Linux malware, Group-IB researchers presume, is deployed during the later stages of an attack chain in order to maintain access to a victim host. Krasue is likely to either be deployed as part of a botnet or sold by initial access brokers to other cybercriminals. Group-IB researchers believe that Krasue was created by the same author as the XorDdos Linux Trojan, documented by Microsoft in a March 2022 blog post, or someone who had access to the latter’s source code. During the initialization phase, the rootkit conceals its own presence. It then proceeds to hook the `kill()` syscall, network-related functions, and file listing operations, thereby obscuring its activities and evading detection. Advertisement The researchers have so far been unable to determine precisely how Krasue gets installed. Possible infection vectors include through vulnerability exploitation, credential-stealing or -guessing attacks, or by unwittingly being installed as trojan stashed in an installation file or update masquerading as legitimate software. The three open source rootkit packages incorporated into Krasue are: Diamorphine Suterusu Rooty Enlarge / An image showing salient research points of Krasue. Group-IB Rootkits are a type of malware that hides directories, files, processes, and other evidence of its presence to the operating system it’s installed on. By hooking legitimate Linux processes, the malware is able to suspend them at select points and interject functions that conceal its presence. Specifically, it hides files and directories beginning with the names “auwd” and “vmware_helper” from directory listings and hides ports 52695 and 52699, where communications to attacker-controlled servers occur. Intercepting the kill() syscall also allows the trojan to survive Linux commands attempting to abort the program and shut it down. Page: 1 2 Next → reader comments 32 Dan Goodin Dan Goodin is Senior Security Editor at Ars Technica, where he oversees coverage of malware, computer espionage, botnets, hardware hacking, encryption, and passwords. In his spare time, he enjoys gardening, cooking, and following the independent music scene. Advertisement Channel Ars Technica Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario Today \"Quantum Leap\" series creator Donald P. Bellisario joins Ars Technica to answer once and for all the lingering questions we have about his enduringly popular show. Was Dr. Sam Beckett really leaping between all those time periods and people or did he simply imagine it all? What do people in the waiting room do while Sam is in their bodies? What happens to Sam's loyal ally Al? 30 years following the series finale, answers to these mysteries and more await. Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario Unsolved Mysteries Of Warhammer 40K With Author Dan Abnett SITREP: F-16 replacement search a signal of F-35 fail? Sitrep: Boeing 707 Steve Burke of GamersNexus Reacts To Their Top 1000 Comments On YouTube Modern Vintage Gamer Reacts To His Top 1000 Comments On YouTube How The NES Conquered A Skeptical America In 1985 Scott Manley Reacts To His Top 1000 YouTube Comments How Horror Works in Amnesia: Rebirth, Soma and Amnesia: The Dark Descent LGR's Clint Basinger Reacts To His Top 1000 YouTube Comments The F-35's next tech upgrade How One Gameplay Decision Changed Diablo Forever Unsolved Mortal Kombat Mysteries With Dominic Cianciolo From NetherRealm Studios US Navy Gets an Italian Accent How Amazon’s “Undone” Animates Dreams With Rotoscoping And Oil Paints Fighter Pilot Breaks Down Every Button in an F-15 Cockpit How NBA JAM Became A Billion-Dollar Slam Dunk Linus \"Tech Tips\" Sebastian Reacts to His Top 1000 YouTube Comments How Alan Wake Was Rebuilt 3 Years Into Development How Prince of Persia Defeated Apple II's Memory Limitations How Crash Bandicoot Hacked The Original Playstation Myst: The challenges of CD-ROMWar Stories Markiplier Reacts To His Top 1000 YouTube Comments How Mind Control Saved Oddworld: Abe's Oddysee Bioware answers unsolved mysteries of the Mass Effect universe Civilization: It's good to take turnsWar Stories SITREP: DOD Resets Ballistic Missile Interceptor program Warframe's Rebecca Ford reviews your characters Subnautica: A world without gunsWar Stories How Slay the Spire’s Original Interface Almost Killed the GameWar Stories Amnesia: The Dark Descent - The horror facadeWar Stories Command & Conquer: Tiberian SunWar Stories Blade Runner: Skinjobs, voxels, and future noirWar Stories Dead Space: The Drag TentacleWar Stories Teach the Controversy: Flat Earthers Delta V: The Burgeoning World of Small Rockets, Paul Allen's Huge Plane, and SpaceX Gets a Crucial Green-light Chris Hadfield explains his 'Space Oddity' video The Greatest Leap, Episode 1: Risk Ultima Online: The Virtual EcologyWar Stories More videos ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=38590827",
    "commentBody": "Stealthy Linux rootkit found in the wild after going undetected for 2 yearsHacker NewspastloginStealthy Linux rootkit found in the wild after going undetected for 2 years (arstechnica.com) 238 points by webmaven 22 hours ago| hidepastfavorite106 comments milliams 16 hours agonext [–]Krasue is a Linux Remote Access Trojan that has been active since 20 and predominantly targets organizations in Thailand.Off-topic, but this is the first time I&#x27;ve seen post-1999 year denoted with just two digits without the use of an abbreviating apostrophe. reply Luc 16 hours agoparentIt&#x27;s written &#x27;2021&#x27; on Group-IB&#x27;s page. Perhaps a typo they corrected. https:&#x2F;&#x2F;www.group-ib.com&#x2F;blog&#x2F;krasue-rat&#x2F; reply juunpp 12 hours agoparentprevAka, we&#x27;re getting old. reply consumer451 15 hours agoprevIn a Darknet Diaries episode from a while back, I recall a three letter agency person saying something along the lines of \"oh, haha they are running desktop Linux, those guys never run anti-virus software, this will be easy...\"Can anyone give me any more depth on this? Is a typical desktop Linux distro install more likely to be susceptible to attack by a nation-state level actor? reply throwaway09223 14 hours agoparentAnti-virus software is utterly worthless in detecting previously-unknown rootkits. Not a factor.The typical attack vector for any system would be compromising the inbound software. This is really easy to do when people are downloading and installing random programs from websites. This used to be less common, but popular cutting edge tools have popularized it recently \"distro packaging is too slow, etc, curl|bash to install\"On linux, most base software comes from the distro repositories. The question there is how hard it would be to compromise these systems (including one of the mirrors). This includes ancillary packaging systems (pip, cpan, gems, conda, etc) reply heavyset_go 12 hours agorootparent> On linux, most base software comes from the distro repositories. The question there is how hard it would be to compromise these systems (including one of the mirrors). This includes ancillary packaging systems (pip, cpan, gems, conda, etc)IMO if you&#x27;re worried about the NSA, I assume they have access to root certificates and your package manager that uses TLS would be vulnerable. Wouldn&#x27;t have to compromise any system or mirror to do so. reply midasuni 9 hours agorootparentTLS isn’t a major pet of the security of say APT, the key which signs the Release&#x2F;Pacakges files is, and the process that provides the checksums to those files, and if you’ve compromised the developer it’s over anyway.With an NSA able to generate a root cert as you say, I’d trust an apt package from a known provider way before the curl|bash stuff or YetAnotherPackageManagerBecause13ArentEnough that people prefer today. reply lmm 11 hours agorootparentprev> IMO if you&#x27;re worried about the NSA, I assume they have access to root certificates and your package manager that uses TLS would be vulnerable. Wouldn&#x27;t have to compromise any system or mirror to do so.Debian-derived systems use GPG signing so that wouldn&#x27;t be enough, there&#x27;s no central authority in the first place. There are hundreds of Debian developers, and most likely they could find one with poor opsec (or straight-up send a National Security Letter), but they would pretty much be burning that specific individual.(Do distro package managers use Certificate Transparency? That would be the hope on the TLS-based side) reply consumer451 3 hours agorootparentprevNow that I have had a chance to recall this story a bit more, I should not have mentioned the nation state actor aspect, it was distracting.I believe the point was that they wouldn&#x27;t have to do the paperwork, and wait to run it up the chain for the big guns.Also, maybe the big guns would not be authorized for this particular op. I think what they were getting at was that there was probably no need for all that as there was no end point protection agent.And, now I am inferring, that on desktop Linux, with all of the additionally installed attack surface combined with how often people actually run apt update && apt uprgade && apt dist-upgrade... \"it was going to be (relatively) easy.\" reply sp1rit 12 hours agorootparentprev> your package manager that uses TLS would be vulnerable most distro package managers (dpkg, rpm, etc.) tend to use gpg which shouldn&#x27;t suffer from those issues (but they could obviously still have some sort of other backdoor for gpg).Still, I feel like distro packages are really secured compared to stuff you install via pip&#x2F;npm&#x2F;... as I don&#x27;t believe they do anything beyond protecting downloads with TLS. reply consumer451 4 hours agorootparentThis seems like an extremely important point, which applies not just to desktop Linux (my OP,) but especially server Linux.Since I am very much a Linux & infosec muggle, please indulge me with these possibly dumb questions:What are the mitigations for the lack of provenance in pip&#x2F;npm?Does properly configured SELinux do enough? Or is the fact that many of these packages use 80&#x2F;443 negate that? Or is the fact that a pip&#x2F;npm package could be comprised after install, during update the main problem?I always think of that left-pad NPM drama, could that single dev have comprised thousands of systems by changing his update to something much more nefarious, instead of just deleting the package?Again, sorry if these are dumb questions. reply throwaway09223 12 hours agorootparentprevI mean yeah, I would say that&#x27;s one way to compromise those systems. reply consumer451 14 hours agorootparentprev> Anti-virus software is utterly worthless in detecting previously-unknown rootkits.Maybe the \"this will be easy\" part was that without AV software, there was a greater chance that they could just spin up Metasploit, and wouldn&#x27;t even have to use one their stockpiled zero-days? reply throwaway09223 12 hours agorootparentAV doesn&#x27;t detect the exploit mechanism -- that&#x27;s not how it works. It&#x27;s only pattern matching existing binaries.The difficulty level is \"recompile with small changes and test that it doesn&#x27;t match\" not \"develop a new attack vector\"It really is stupid and pointless against an attacker with even a bare minimum of competence (able to run a software build vs downloading binaries) reply consumer451 9 hours agorootparentBut AV (EPP) could detect a binary which was passed through a traditional vector, like phishing or gaining access to a \"trusted\" email account and attaching a compromised PDF, as a random example.Just a reminder that I was originally talking about desktop Linux, and all of the additionally installed attack surface which that entails.It would seem to me that it does not matter how \"technical\" a user is, they are still human, and some of the time anyone could fall for a well formed phishing attack. We all get tired, overworked, or click too fast, etc.Not running AV on a desktop OS and relying on one&#x27;s own superhuman technical ability seems like the exact type of hubris that would be ripe for attack. reply Jenda_ 8 hours agorootparent> attaching a compromised PDFThis means either:- a 0day, which would require the AV to have a PDF parser better than the standard document viewer, and the ability to sense that this PDF is \"weird\" -- I would expect AV companies to publish ads \"our AV has detected a 0day in XXX\"- a vulnerability was recently discovered in a PDF viewer, and the AV company can push their definitions earlier than the standard \"package the fixed version - send to debian-security - let users upgrade\" route. This would shorten the attack window by a few hours. Again, I would expect AV companies to boast \"we were X hours earlier than the official fix\".Which one is the case? Or is there another option?Actually, this whole \"buggy PDF parser\" thing should be solved by application sandboxing -- there is no need that document viewer needs any other access to my system. Unfortunately, Linux is lagging behind. There are some AppArmor experiments with not so great UX, and then there is QubesOS, which is difficult to use. The average Linux desktop is AFAIK almost unsandboxed. reply consumer451 7 hours agorootparentThank you for replying.I am now at the limits of my understanding...I only ran Ubuntu as a desktop daily driver for a year or so, and I&#x27;m a muggle, so my understanding is limited. But, is there any real-world data on how often desktop Linux users run the equivalent of: sudo apt update sudo apt upgrade sudo apt dist-upgradeversus the more automated update systems MacOS or Windows ?I am genuinely curious which ecosystem is more likely to be up to date. In my limited experience, I ran into issues updating on Ubuntu, and have not on MacOS and Windows. It seems like MacOS does it best as most applications come via the App Store, and on Windows that&#x27;s in the future leaving most apps to take care of their own updates. However, Windows makes up for that a little bit with excellent, and auto-updated EPP, so that&#x27;s something at least.In your view, which desktop OS is most likely to be up to date for OS and apps? reply Jenda_ 7 hours agorootparentI think Ubuntu has some kind of notifier in \"tray\". For me, I am subscribed to debian-security mailing list and update when something that is running on systems that I manage seems to be affected.> versus the more automated update systems MacOS or WindowsI&#x27;m not sure -- it is better with Microsoft Store, but other apps solve updates on their own, with various success. I have little experience with Windows and no with mac OS, so I cannot comment. reply midasuni 9 hours agorootparentprevHow many genuine issues (not nonsense like “blah.con has stored a cookie” does your av pick up? reply consumer451 8 hours agorootparentIt is very likely that a modern AV (I should have been saying EPP) will pick up your average re-encoded payload, or Metasploit fun time. Certainly they will pickup most known vulnerabilities. Endpoint Protection (EPP) orgs use the same CVE DBs that adversaries use.See another user&#x27;s comment: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38594247Security is an onion, perfect is the enemy of good, etc...Why make it easier for the adversaries? While annoying, running EPP on your desktop OS is not exactly neuroscience.Ideally, it&#x27;s furnished by the OS provider so that there are fewer parties to trust. I hate to say it, but Microsoft is now teaching by example. MS has factually one of the best-in-class Endpoint Protection agents on the market.If resources allowed, all other desktop OS providers should follow suit, otherwise they are just shirking responsibility. reply EricLeer 2 hours agorootparentAny recommendations for a good AV&#x2F;EPP for desktop linux (ubuntu)? reply consumer451 2 hours agorootparentYou have asked the most important question. I should have asked it as well.Would love other input as I’m not an Ubuntu daily driver anymore.It turns out 22.04 has ClamAV “provided and supported,” but not installed by default. So it should easily install manually. That’s what I would go with first, as it’s officially supported. An OS vendor supporting a specific AV vendor is a really big deal imho.If that is not satisfactory by some metric, and this is just my personal and dated opinion… I used to like Eset.I was naive before, but next time I install desktop Ubuntu, I will install ClamAV immediately. replyautoexec 12 hours agorootparentprevWhy worry about stockpiled zero-days even for non-linux systems when they can just force MS or Apple to hand over data or inject whatever they want into an OS \"update\". reply consumer451 11 hours agorootparentIf I&#x27;m beginning to recall the story correctly, it&#x27;s that your suggested route would involve more time, paperwork, and this particular op might not be important enough to get the big guns authorized by the higher-ups. Hence, \"oh, this will be (relatively) easy...\" reply CAP_NET_ADMIN 14 hours agorootparentprevMost of the currently used AV software contains heuristic&#x2F;ML components that are able to gather and analyze various indicators of compromise. Without something like that running, you&#x27;re basically tied to manual review of systems running at the moment. Making malware for such scenarios is basically making a Base64-encoded script in the language of your choice and then exploiting something(either the user or some software) to get it to execute.I know, because I&#x27;ve been writing small malware toys and it got blasted by both Bitdefender and ESET. reply hulitu 12 hours agorootparent> Most of the currently used AV software contains heuristic&#x2F;ML components that are able to gather and analyze various indicators of compromiseRansomware runs just fine on corporate computers with the latest and greatest \"antivirus\" software.I really lost hope some 10 years ago, when i saw that i have to manually remove malware from memory sticks, because macaffe was clueless. After more than a year they released an update that will detect and remove the malware (it was a virus speading through autorun.inf with exe names which did not have any sense like jhghjjj.exe) reply consumer451 12 hours agorootparentI can&#x27;t speak to that particular attack vector today, but it&#x27;s interesting that McAfee isn&#x27;t even on this Gartner chart, and that Microsoft is now a leader. Things have really changed in that space.https:&#x2F;&#x2F;www.cybereason.com&#x2F;blog&#x2F;cybereason-named-a-leader-in... reply seeknotfind 14 hours agoparentprevOne big example is home folder access. Linux has a variety of file access control systems (the most prominent are Linux file permissions and SELinux). However, typically any application you run will be run under your own user and have access to your home folder.In macOS&#x2F;Windows, there are runtime permissions these days which allow you to control which folders can be given to which application.Linux needs to mainstream a solution like this. One of the most popular way Linux users can handle this today is by associating a user with a application and granting various permissions to that user. However this is a far cry from safe by default, and it requires deep system knowledge. reply foobiekr 13 hours agorootparentOnce you own the home folder you can drop aliases for anything you want in the bashrc with almost no one noticing. At that moment point you are essentially done in almost all cases even in the absence of a local root exploit which Linux continues to be rife with. You will eventually catch a sudo password (using a trivial sudo wrapper alias) and all the ssh keys and jump host or password manager config. reply Jenda_ 8 hours agorootparentOf course - the point is not to get the home folder owned.There is no reason why Evince&#x2F;Okular or mpv (to name a few apps which handle files with complex formats from untrusted sources) should have the right to access anything beside their ~&#x2F;.config&#x2F; and the file they are currently viewing&#x2F;playing, or maybe read-only ~&#x2F;Music. If you want to do a \"Save as\", you will do this through a OS-controlled file dialog, or save it to &#x2F;tmp and copy it.This can be achieved with AppArmor, with a caveat that in X, applications can steal each other&#x27;s windows, but unfortunately this is not the default and easy configuration on most distros. reply dpkirchner 8 hours agorootparentprevI guess it would make sense to make bashrc, zshrc, whatever immutable to all but root. Never really considered that before. Allowing them to be edited is like allowing you to install or edit binaries anywhere in your $PATH. reply Jenda_ 8 hours agorootparentThere are many more files that will need this protection:- autorun and keyboard shortcuts of your window manager -- one can hook an evil command to Ctrl+C- ~&#x2F;.mozilla -- you can add arbitrary javascript to your profile or extensions- any application which does not expect to have its config externally tampered with and this may result in various errors including RCE- ~&#x2F;work&#x2F;FooProject&#x2F;Makefile, configuration of your IDE (which contains list of commands that shall be executed to compile)etc.An explicit allowlist would be a better option, IMHO perfectly manageable - with a popup window \"the app wants to access , allow onceallow permanentlydeny\". reply medstrom 12 hours agorootparentprevMoreover, even if the user manages to deny you root access through excellent hygiene, that just means you can&#x27;t include this machine in your botnet. You still have full view of the home folder! Upload to a data broker, get $. reply pca006132 8 hours agorootparentprevI think there is also bubblewrap which flatpak uses, which can limit access to folders, and also XDG desktop portal which allows GUI applications to access files that are not available to them. Not sure if these are widely used though, iirc a lot of flatpak packages just have \"normal\" permission. Also, these are not very helpful for CLI applications. reply vladvasiliu 9 hours agorootparentprev> In macOS&#x2F;Windows, there are runtime permissions these days which allow you to control which folders can be given to which application.I know about this feature on macOS, but how do you configure this on windows?I’ve seen the protected folders feature or something like that. But that’s not “which app can access which folder”. Rather, “only this list of apps can access this list of folders”. reply dharmab 14 hours agoparentprevThe default settings of many Linux distros don&#x27;t enable major security features. For example, on Arch Linux, microcode updates must be configured separately and most users likely skipped that step.Also, X11 has really poor separation between processes. Any desktop application running in Xorg can keylog all user inputs.It is possible to build a secure Linux installation but it requires a lot of careful OS knowledge and the discipline to only use a small set of software with established provenance. And probably disabling Javascript. reply enasterosophes 11 hours agoparentprevMalware requires propagation vectors. Computers don&#x27;t work by magic, instead when thinking about security you should think about your threat model. What are you protecting and from whom, and what is the attack surface you are exposing which would allow them to get prize?On Linux, the attack surface may not be non-existent, but by default it is smaller than some other operating systems. Any halfway competent Linux sysadmin can configure iptables or some other firewall to block all network traffic except what is specifically permitted. Even when something is exposed, it usually only allows access to specific user accounts (including system service accounts.) Once access is granted to an account, you are then constrained by discretionary access controls (DAC) (i.e. POSIX users + groups + file permission modes), and, for the paranoid, by mandatory access control (MAC) (eg AppArmor or SELinux) as well.As others have said, such access could theoretically allow bootstrapping to greater privilege escalation, especially if you only have DAC and no MAC, but in practice it&#x27;s harder than it sounds.I have seen plenty of publicly-exposed Linux VMs compromized over the years. They were always compromized via one of two methods: the admin user explicitly enabled password authentication to ssh instead of adhering to keypairs (which we enforce by default); or, they opened up a vulnerable service such as a database to the public internet. It was never via some virus.You hear people saying dumb stuff like, oh no one uses Linux and that&#x27;s why there are no viruses. To that, I say our Linux server farms have a vast amount of highly valuable compute power and research data. If it were possible to infect these machines with a virus, any halfway competent nation state, ransomware group or bitcoin mining conglomerate would very much consider it a worthwhile investment to develop such a virus. reply Jenda_ 8 hours agorootparentThe GP was talking about Linux desktop, you seem to be talking about Linux server. reply enasterosophes 6 hours agorootparentI&#x27;m talking about Linux.I&#x27;ve been using Linux desktop for 15 years. No viruses so far.But if I just talked about my personal machines, would anyone listen? I&#x27;ve had a lot more opportunity to see what it looks like when a VM is compromized, compared to a desktop, because my desktop has never been compromized. reply akyuu 12 hours agoparentprevThe Linux desktop technology stack lags behind Windows and macOS when it comes to security. The causes are both technical (see this comment [1] for an overview) and non-technical, often stemming from a fragmented development model where there are no clearly defined security boundaries. For example:- There is no real concept of base system because distros are usually a patchwork of software from diverse sources. This means stuff like proper secure boot is not really feasible on any distro (although AFAIK the systemd&#x2F;Fedora people are working on it with signed UKIs and immutable OS images).- Some features that could live in userland for improved security are instead implemented in the kernel, while both Windows and macOS generally keep moving exploitable features like font rendering to userland.- Distros often disable or disregard security features such as SELinux or mitigations like CFI.Here [2] is a more detailed article examining the lack of security of Linux desktops in case you&#x27;re interested.[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37502088[2] https:&#x2F;&#x2F;madaidans-insecurities.github.io&#x2F;linux.html reply akira2501 12 hours agoparentprevIn order for AV to work, it has to have the highest level of permissions on the system, high enough to analyze all actions and potentially deny or alert on them.I don&#x27;t trust that an AV vendor can _add_ security to a system with a _new_ and obscure binary looking over the kernels shoulder. I don&#x27;t think the properties of the universe allow for this to ever be true. reply consumer451 7 hours agorootparentThis is why.. shudder.. Microsoft appears to be the leader in AV (EPP). I also don&#x27;t want an extra party involved, so ideally, the OS provider should also provide the EPP.I do see how other OSes address this problem differently, such as everything going through a package manager.But, see my question here regarding how often some is going to run apt update && apt upgrade && apt dist-upgrade, vs. the more automated update systems on MacOS and Windows.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38596966 reply sspiff 4 hours agoparentprevDepends. Most Linux users will run the latest security patches for all their software, not just the OS, because it all updates automatically.Additionally, a lot of Linix users run most attack vectors (like browsers or email clients) inside a sandbox using Snap or FlatPak).None of these things make it impossible to get through to the system, but they are additional barriers that an attacker needs to deal with. reply marcosdumay 14 hours agoparentprevYeah, that&#x27;s bullshit. None of the mainstream OSes will help you protect you from a nation-state actor. reply tsujamin 12 hours agorootparentThere’s a lot more footguns for attackers on modern windows than other operating systems. Most can be bypassed (EDR, AV, AMSI, ETW, UAC, controlled folders, App Control, block at first site etc), but it definitely raises the bar if your end goal is very low likelihoods of detection.Put differently: the gauntlet you have to run on Windows (and to a lesser extent macOS I imagine) is much longer reply consumer451 14 hours agorootparentprevAs I wrote in another comment:> Maybe the \"this will be easy\" part was that without AV software, there was a greater chance that they could just spin up Metasploit, and wouldn&#x27;t even have to use one their stockpiled zero-days? reply steve1977 14 hours agoparentprevI mean there’s people installing stuff with curlsudo bash and friends, so go figure… reply le-mark 21 hours agoprev> Specifically, it hides files and directories beginning with the names “auwd” and “vmware_helper” from directory listings and hides ports 52695 and 52699, where communications to attacker-controlled servers occur.Do people out there really “allow any” ports on their firewalls? Or are those ports for control inside the firewall only? reply BLKNSLVR 21 hours agoparentOutgoing, which I believe this would be, is generally far less restrictive than incoming.I&#x27;m playing around with IP address level detecting and blocking using incoming ports as indicators. I&#x27;m going to set aside some time to think more about restrictions on outgoing ports as a result of this and your comment.Do some logging of frequently used remote ports over a couple of weeks and create a baseline set of allowed ports, block everything else and see what breaks.I already use the limited Feodo Tracker[0] lists to flag in my firewall logs whether any device on the network has attempted to contact a known C&C IP address.Thanks for push.[0]:https:&#x2F;&#x2F;feodotracker.abuse.ch reply vitus 18 hours agoparentprev> Or are those ports for control inside the firewall only?It seems so. This traffic likely will not even traverse a firewall -- the article lists the IP addresses used with port 52699, which are all RFC1918 space (172.16.0.0&#x2F;12). reply peblos 21 hours agoparentprevLog4shell would have been a much smaller problem if so reply goodpoint 21 hours agoparentprevYes. This is some very basic malware, but real ones will use HTTPS on port 443. reply BLKNSLVR 21 hours agorootparentOnly allow HTTPS out to FAANG (whatever it is now) + Cloudflare IP address ranges reply fullspectrumdev 17 hours agorootparentHiding your C&C behind clownflare is basically the default mode for a lot of actors.CF are terribly slow to respond to abuse, their ranges are allowlisted everywhere, and it’s not terribly hard to hide your backend infrastructure (origin) even from cloudflare itself.Sure you have to deal with the eventual takedown of your domain, by CF or more likely the domain registrar, but it’s trivial to work around that (backup domains, frequently rotating them, etc).What’s funny is how Namecheap are now absolutely god tier at doing takedowns on malicious domains - they used to have a very poor reputation and now will process a takedown (provided evidence) within the hour usually. reply miloudi 7 hours agorootparentwhat&#x27;s C&C? sorry for hijacking the thread. reply e63f67dd-065b 6 hours agorootparentCommand and Control. The server that issues instructions to the malware. reply zelphirkalt 15 hours agorootparentprevWondering if typo, auto-correct, or intentional ... anyway, I like :D reply averageRoyalty 13 hours agorootparentGiven the critical tone, I&#x27;d say probably intentional. reply Kubuxu 20 hours agorootparentprevNothing prevents C&C from being fronted by cloudflare. reply viraptor 20 hours agorootparentAnd CloudFlare will be extremely happy to let them hide there. They don&#x27;t care about malware abusing their workers infra.But also \"Only allow HTTPS out to FAANG\" - ok, so now you&#x27;re allowing encrypted connections to 2 largest cloud providers. At that point what&#x27;s the benefit at all? reply llamaInSouth 15 hours agorootparentIm pretty sure he was joking anyways... Either way, I thought it was funny. reply jay-barronville 18 hours agorootparentprev> They [Cloudflare] don&#x27;t care about malware abusing their workers infra.This is a serious allegation to make. Do you happen to have some supporting evidence? reply fullspectrumdev 17 hours agorootparentIt’s widely known and talked about in the infosec community - getting CF to do anything about malicious content on their services is an absolute nightmare.I do understand they don’t want to set any bad precedents internally, but their abuse reporting is abysmal. reply jay-barronville 17 hours agorootparentDo you have some supporting evidence? reply averageRoyalty 13 hours agorootparentIs it needed? Most hosting companies and CDNs are difficult to get to remove malicious content, this is pretty well known. I don&#x27;t know why we&#x27;d require some form of irrefutable evidence to believe Cloudflare isn&#x27;t the same. reply viraptor 12 hours agorootparentprevJust search for CloudFlare worker abuse. There&#x27;s lots of stories like this https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37064311 , larger campaigns like https:&#x2F;&#x2F;www.bleepingcomputer.com&#x2F;news&#x2F;security&#x2F;blackwater-ma... , DDoS campaigns originating from workers https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38496499It&#x27;s kinda hard to point to someone saying how big the problem is, because most posts are just \"sigh, here&#x27;s yet another example, anyway...\". I wouldn&#x27;t go as far as saying they&#x27;re doing this as an intended strategy, but it&#x27;s definitely a case of it being hard for CloudFlare to care about an issue when they&#x27;re in business of selling protection from that issue. reply jay-barronville 10 hours agorootparentThank you for providing links instead of giving me a “just believe me, bro” type of response. I’m about to take a look at them.As a side note, I’m actually amused that I got downvoted for simply asking for evidence. I think it’s a pretty reasonable to ask folks to substantiate serious allegations made against a company like Cloudflare (a company, I might add, I’m not even a fan of for my own personal&#x2F;philosophical reasons). I find this behavior very cult-like, but anyway, I digress… reply sambull 16 hours agorootparentprevOne mans C&C server is anothers RMM. reply mort96 18 hours agorootparentprevCreate a bunch of burner Google accounts and use Google Drive or Docs or Calendar as C&C? reply franga2000 17 hours agorootparentI&#x27;ve used Google App Script for C&C in a demo before. It&#x27;s HTTPS traffic going to a Google IP and the domain is script.google.com - what looks more legit than that? reply pyinstallwoes 19 hours agorootparentprevBetter to limit per process reply goodpoint 17 hours agorootparentprevYes, and with domain fronting and doing traffic patterns that look legitimate.A good solution is to block all outgoing traffic on production servers. reply 3np 21 hours agoparentprevOutgoing? You bet. Have you been out there? reply ninkendo 11 hours agoprevQuestion to HN, is chkrootkit still useful to use at all? I remember using that tool decades ago thinking it gave me some semblance of peace of mind, but I’ve also kind of assumed that sophisticated malware would likely be able to evade it. reply halJordan 10 hours agoparentProbably not. It&#x27;s probably similar to windows AVs saying they can catch empire or meterpretor. They can, but it&#x27;s also not too hard to change the signature away from default reply Pixie_Dust 19 hours agoprevHow does this “stealthy Linux rootkit” get onto the system in the first place. Without opening a malicious email attachment or clicking on a malicious weblink. reply ufmace 17 hours agoparentSeems to me this is probably a later stage thing. Somebody got initial access to a company&#x27;s systems via such a mechanism to some individual&#x27;s system. A few more cycles of recon, exploitation, and pivoting later, they may be in a position to install something like this on some actually important server. Use it to maintain access to the things they really want, then delete evidence of the previous steps to cover their tracks.Now that it&#x27;s at least 2 years after the initial intrusion, it could be pretty tough to determine how that happened and what path the attacker took. reply Jenda_ 7 hours agoparentprevThe malicious weblink can be an advertisement, or a legit webpage that got compromised&#x2F;XSS&#x27;d, or a formerly legit webpage whose domain has expired. (AFAIK this is pretty common)The email attachment may come from your friend&#x2F;business partner which themselves got infected and the malware is now attaching itself to their legit emails. (AFAIK not very common) reply fulafel 17 hours agoparentprevThe posing as a VMware helper process and timeframe hints this may be associated with the recent VMware compromise epidemic(s). reply 0xDEF 17 hours agoparentprevToday most (by volume) Linux attacks are against IoT devices that run Linux and SSH with weak&#x2F;no auth.Behind that are attacks on Linux web servers where exploits in the web application (e.g. WordPress) or the web framework (e.g. Rails) are the attack vector. reply dspillett 19 hours agoparentprevThose methods would work. Could also be included in pirate content in a torrent or similar (this was a significant vector for windows malware in the 2010s). Some instances could also have been manually placed. Or the creator could have bought the services if a bonnet, installing the seeds on machines already backdoored and open. There are a fair few ways to get new rootkits out there, a number of them difficult to trace back to the true source.EDIT: from the article: The researchers have so far been unable to determine precisely how Krasue gets installed. Possible infection vectors include through vulnerability exploitation, credential-stealing or -guessing attacks, or by unwittingly being installed as trojan stashed in an installation file or update masquerading as legitimate software reply belltaco 18 hours agoprevWould SecureBoot have prevented the rootkit parts of this? reply fullspectrumdev 17 hours agoparentProbably not, depending on configuration. You can do very clever things with secure boot on Linux to have all kernel modules signed, etc, but if the attacker has ring0 code execution anyway that can be evaded without a huge amount of effort. reply rs_rs_rs_rs_rs 17 hours agoprevLinux rootkits meta really did not change much since 1999. reply fullspectrumdev 17 hours agoparentYep. They come in basically the same flavours.Binary replacement (replacing top, ps, netstat, etc with patched versions) is the oldest trick.Syscall hooking kernel root kits largely only vary in how exactly they hook the syscalls - there’s a few methods, but the underlying principle is the same.And then you have userland hooks which usually use LD_PRELOAD to intercept system calls&#x2F;libc calls to get the job done.Honestly most of the time a full blown root kit is overkill, just name your binary something innocuous and have it not do anything too fucking obvious and nobody will notice. reply CableNinja 17 hours agorootparentItd be obvious if you really look, but i once discovered in Perl you can make your command look however you want when you run `ps`. I was able to make a process appear to be a kernel process (one with `[]`). It was pretty hard to discover unless you were very familiar with what every kernel process did. Ive discovered other fun ways of hiding things too, such as creating `...` as a directory. The first time i encountered that, it took me a whole day to find that dir. reply matheusmoreira 11 hours agorootparentNot just Perl. Any Linux process can do it.https:&#x2F;&#x2F;www.man7.org&#x2F;linux&#x2F;man-pages&#x2F;man2&#x2F;prctl.2.html> PR_SET_NAME> Set the name of the calling thread reply CableNinja 10 hours agorootparentYeah, it was just that id discovered it when i was using perl. reply Jenda_ 7 hours agorootparentprev> Syscall hooking kernel root kits largely only vary in how exactly they hook the syscallsI don&#x27;t understand how this survives major kernel upgrades. I have problems keeping out-of-tree modules working, intentionally. Do rootkits ship with fancy DKMS these days? Do the authors test with upcoming versions of major distros and push an upgrade to support the new release? reply RCitronsBroker 21 hours agoprevattacking telecom infrastructure smells like possibly state actor related to me reply BLKNSLVR 21 hours agoparentNovel protocol usage gives off that kind of smell too. reply RCitronsBroker 20 hours agorootparentoh yes, it does. Good catch. False-Flagging your actions as commercially motivated doesn’t strike me as a hard to construct alibi, either. Certainly not beyond the scope of sophisticated, state aligned actors, except maybe the north-Koreans, that guardians of peace Spiel was entertaining tho.This awakens memories of GCHQ attacks on Belgian(?) telecom providers; they really are inherent gold mines for dragnet surveillance, stealing database, offensive cyberattacks and probably lots more. There’s a reason Huawei is subsidized for offering cheap 5g infrastructure to international clients, it’s a real nice thing to have backdoors to. Not saying the west is much better in this regard, Cisco has been more than compliant in similar shenanigans just to name one. reply fullspectrumdev 17 hours agorootparentprev“Using weird protocols” for C&C is old hat for hackers tbh. Well over a decade ago after reading an RFC I switched all my backdoors to use SCTP - a protocol many of my peers had never fucking heard of - and found it evaded a shit tonne of IDS&#x2F;firewall products. reply foobarian 19 hours agoparentprevOr is it low hanging fruit? :-) reply RCitronsBroker 18 hours agorootparenttouché lol, narrowing it further down to thai telecom is not really helping to raise the metaphorical fruit either. reply TheLoafOfBread 19 hours agoprevnext [10 more] [flagged] hollerith 19 hours agoparent>people will disable SELinux the moment their distro boots first time, because that thing is just massive pain to use Linux with.I think you mean secure boot: the only distros that even try to enable SELinux are Fedora and Red Hat, and on those distros SELinux gets in the way only very rarely. (Source: I am a user of Fedora and Red Hat.) reply izzdrasil 18 hours agorootparentThey definitely mean selinux. There only needs to be a single issue for an average user to disable selinux. reply nurettin 12 hours agorootparentThat&#x27;s it! I was just having trouble connecting to my local rmq server on a fresh fedora 39 install. Forgot to disable selinux! reply hollerith 17 hours agorootparentprevI always thought that admins of Linux installs other than Fedora and Red Hat installs rarely bothered with selinux and that it is definitely not enabled on an initial install.I thought selinux was available as a package on non-RH distros, but install it and enabling was very painful unless you are an selinux expert, so almost no one on those distros does.Are you sure you don&#x27;t mean AppArmor? reply freedomben 16 hours agorootparentI&#x27;ve been a fedora user and Red hat user since 2010, and worked at Red hat for a few years, though not on RHEL.I think they did mean selinux. You are absolutely right that nowadays it just works for the most part. The big exception is for people deploying applications that don&#x27;t have selinux config shipped by Red hat. Those people have to configure things properly, and rarely do people bother. If you do things like change the default ports for services, that can also get you in selinux trouble. reply TheLoafOfBread 14 hours agorootparentprevNo, I mean selinux. And when I was using Linux, selinux got in the way basically immediately. reply hollerith 13 hours agorootparentInteresting. On which distro? reply TheLoafOfBread 11 hours agorootparentFedora if I remember correctly. It is few years back. reply codedokode 18 hours agoparentprevselinux should come with a GUI prompting for every operation. As I am aware, it cannot work in this mode. reply spacecadet 20 hours agoprev [–] 2 years and they don&#x27;t know how it starts? Well, I bet we can assume at first it was People. It is probably still largely people, but 2 years of possible persistent C2... I would think by now it&#x27;s possibly using many other areas of ingress without any people involved. reply halJordan 10 hours agoparent [–] Oh man great insight. Do wealth inequality next? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Linux malware named Krasue has secretly infected Thai telecommunications companies for two years undetected.",
      "Krasue is a remote access trojan that allows attackers to control targeted networks remotely.",
      "Krasue uses various rootkits to hide its activities, disguises \"alive pings\" as RTSP messages, and contains a rootkit that intercepts the kill() syscall to survive shutdown attempts. Possible installation vectors include vulnerability exploitation, credential theft, or trojanized software updates."
    ],
    "commentSummary": [
      "The conversation discusses various topics related to Linux security, including the discovery of the Krasue rootkit and concerns about the vulnerability of Linux systems.",
      "It explores the effectiveness of antivirus software and the need for enhanced security measures on Linux desktops.",
      "The conversation also examines the potential implementation of a runtime permission system on Linux and compares the security features of Linux to other operating systems."
    ],
    "points": 238,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1702207604
  },
  {
    "id": 38595855,
    "title": "Improve Web Accessibility: Encourage RSS Integration for Websites",
    "originLink": "https://rknight.me/please-expose-your-rss/",
    "originBody": "Earlier this week I had a need to manually find a bunch of people's RSS feed links. It seemed simple enough: go to their website and look for an RSS/Subscribe link but I was surprised to find that a lot of people don't have a link anywhere to their feed. Even if people only ever add your website into their feed reader and let the app find the RSS feed (see below for more info on this), showing an RSS link reminds people that RSS exists, a win for the open web. My second step when finding a link failed was to use this handy JS snippet from my Podcast Duration project: return Array.from(document.getElementsByTagName('link')).find(l => l.type.includes('application/rss+xml'))?.href This looks for atag on the website that has a type of application/rss+xml. This is called RSS auto-discovery and is a standard way to expose RSS feeds to help browsers and other software to automatically find a site's RSS feed. Like the standard link, a lot of sites were also missing this. This is (at least as a first step) what feed reeders like NetNewsWire will use to automatically find a feed when you paste in a URL. If you have an RSS feed, you should have the following in the head of your website:If you have multiple feeds, you can have more than one link tag that links to those feeds as well. For example, say you have a JSON feed and a podcast feed you want to link to: Please, expose your RSS. Update 2023-12-09 via James: if you're going to add an RSS button, please ensure it looks like an RSS button and is in RSS orange This is an excellent idea and I have done so here. Follow me on Mastodon or Subscribe with RSS If you like this post or one of my projects you can buy me a coffee 💙 LikesBoosts 31 Comments Popular Posts App Defaults Using the Johnny Decimal System Smart Speed Broke My Brain So Many Default Apps Using PageFind with Eleventy for Search Analytics powered by Fathom",
    "commentLink": "https://news.ycombinator.com/item?id=38595855",
    "commentBody": "Please, expose your RSSHacker NewspastloginPlease, expose your RSS (rknight.me) 237 points by rknightuk 11 hours ago| hidepastfavorite84 comments fmajid 4 minutes agoAs autodiscovery is often broken or missing on many sites, my feed reader Temboz falls back to testing these suffixes in the forlorn hope there may be a RSS or Atom feed hiding somewhere: &#x27;feed&#x27;, &#x27;feed&#x2F;&#x27;, &#x27;rss&#x27;, &#x27;atom&#x27;, &#x27;feed.xml&#x27;, &#x27;&#x2F;feed&#x27;, &#x27;&#x2F;feed&#x2F;&#x27;, &#x27;&#x2F;rss&#x27;, &#x27;&#x2F;atom&#x27;, &#x27;&#x2F;feed.xml&#x27;, &#x27;index.atom&#x27;, &#x27;index.rss&#x27;, &#x27;index.xml&#x27;, &#x27;atom.xml&#x27;, &#x27;rss.xml&#x27;, &#x27;&#x2F;index.atom&#x27;, &#x27;&#x2F;index.rss&#x27;, &#x27;&#x2F;index.xml&#x27;, &#x27;&#x2F;atom.xml&#x27;, &#x27;&#x2F;rss.xml&#x27;, &#x27;.rss&#x27;, &#x27;&#x2F;.rss&#x27;, &#x27;?rss=1&#x27;, &#x27;?feed=rss2&#x27;, reply rollcat 1 hour agoprevHot tip: YouTube channels do expose an RSS feed; you can subscribe to a channel by just pasting its URL in your news reader.You might find RSS feeds in other unexpected places! Use an extension to put the \"subscribe via RSS\" button back in your browser.reply zlg_codes 8 hours agoprevA lot of RSS&#x2F;Atom is left unexposed for two key reasons:1) Google Reader and friends disappeared, and 2) Browsers stopped supporting RSS feeds natively, including discovery of a feed.Put the functionality back in browsers where it belongs, it could at least be used to find feeds and then you can use newsbeuter or something similar to subscribe with. reply ramijames 8 hours agoparentIt was intentional. RSS is a way to sidestep advertising.Guess who makes the most widely used browser today. reply dugite-code 7 hours agorootparent> It was intentional. RSS is a way to sidestep advertising.I would argue that&#x27;s only a part of the story. After all RSS quickly morphed into Link with a short blurb and image just like any modern link aggregation site rather than a full page of content.The big issue was RSS was it prevents site lock-in like Facebook ect. Even Reddit pushes people to the comments to try keep you on the site as much as possible in order to have you see the link aggregator&#x27;s Advertisements and more recently, to provide more robust data collection for later sale. reply rambambram 3 hours agorootparentI&#x27;m willing to advocate in favor of the short blurbs. I follow a lot of &#x27;dev blogs&#x27; by RSS and my homegrown feed reader also only shows the short blurbs. I do this because all these people crafted their (personal) sites, made it beautiful and personal, and I&#x27;m wanting to visit their site to pay hommage and look at what they made (whether that&#x27;s a cool logo, a nice color scheme, etc.) exactly in the way they made it. My ad blocker already blocks the ads I don&#x27;t want to see (if there are any externally loaded ads).It also has the benefit of the site owners now being able to see somebody is subscribed to the feed. I&#x27;m visiting their website and they might see my traffic in their visitor statistics. To make this even more clear to the site owner, I add an URL parameter called &#x2F;?rss_ref=heyhomepage.com reply politelemon 2 hours agorootparentprevFirefox dropped RSS&#x2F;Atom support too reply mdp2021 2 hours agoparentprevThat some corporate services stopped existing does not imply one cannot have an \"application&#x2F;rss+xml\" link in the html page source - in fact, it is not normal to depend on perishable services when \"unrevokable\" desktop software applications can be available, and in the case of RSS readers there still exists a large number (and writing one is pretty trivial).To discover a feed, just check the html source. The problem is when the feed exists but it is not mentioned at all in the website. reply orbital-decay 1 hour agoparentprevNative RSS support never made sense as a core feature for browsers in the first place; extensions are much better suited for this. There are plenty of replacements for Google Reader as well, which also provide much more than just RSS.In the media optimized for engagement&#x2F;addiction, RSS provides a step off the platform, so of course they don&#x27;t use it. Even many personal blogs the article mostly talks about might want to keep the reader on the site for clicks.It&#x27;s a conflict of interest between content providers and consumers (mostly advertisement). It&#x27;s not an implementation issue. reply rambambram 3 hours agoprevPlease, also expose that you&#x27;re subscribed to a blog&#x27;s feed. I sometimes sent an email to site owners to compliment them with a nice article I just read. Sometimes I leave a short comment here on HN to let somebody know I subscribed to their feed. I hope it encourages people that put energy and heart into quality content to keep making that content. I try to not overdo that here on HN, because it adds little to the general discussion.What I also do with my homegrown RSS reader (see https:&#x2F;&#x2F;www.heyhomepage.com if you&#x27;re interested) is adding a parameter to the links I click, like &#x2F;?rss_ref=heyhomepage.com Site owners get my traffic and they can easily see where it comes from if they happen to look at their visitor statistics. reply darekkay 2 hours agoparent> Site owners get my traffic and they can easily see where it comes from if they happen to look at their visitor statistics.I suggest to also put the name of your reader and the total number of subscribers into the HTTP request (as user agent), if possible.https:&#x2F;&#x2F;darekkay.com&#x2F;blog&#x2F;rss-subscriber-count&#x2F; reply seeknotfind 10 hours agoprevRSS can also be a bit inconsistent. Sometimes people put content in the feed. Sometimes it&#x27;s a link. Of course, if it&#x27;s in a link, programmatically scraping the content usually requires specialized code per site, unless you are okay with various random garbage in your data (e.g. generalized scrapers exist, but they might include header text from the site or similar).I suspect with the advent of LLM, there&#x27;s finally a market for this kind of stuff. People can sell data, including articles, as data source for people&#x27;s ML pipelines. It&#x27;s a possible path we can detox from ads.So yes, please start with making RSS better! It&#x27;s a beacon of light in a sea of darkness. reply kazinator 10 hours agoparentI don&#x27;t think RSS needs to \"improve\" for the benefit of scrapers. reply seeknotfind 9 hours agorootparentI want to run&#x2F;build my own clients. Scrapers can get data no matter what. reply kazinator 5 hours agorootparentClients for what? There are RSS readers; they mostly don&#x27;t scrape into RSS, beyond rendering the content. reply p4bl0 9 hours agoparentprevNewsBlur has a switch to choose between two views : feed or text. The text view is generally able to fetch the full article on the website even if the feed only contains an abstract. reply stanislavb 9 hours agoparentprevI&#x27;ve built an opinionated RSS reader&#x2F;tracker that was built around this - It works with links only. This way, you have consistent experience, and the author gets visits to his website.You can give it a go here: https:&#x2F;&#x2F;lenns.io. I&#x27;d be happy to get some feedback. Thanks. reply bluish29 9 hours agorootparentSeems interesting, but can you provide more information on the pricing or your business model. Investing in an rss web client is about giving up some control vs local client (self hosted). So it would be good to plan and decide from the beginning. reply cratermoon 10 hours agoparentprev> I suspect with the advent of LLM, there&#x27;s finally a market for this kind of stuff. People can sell data, including articles, as data source for people&#x27;s ML pipelines.You mean RSS will improve copyright laundering? reply diego_sandoval 10 hours agoparentprev> Sometimes people put content in the feed. Sometimes it&#x27;s a link.This is the reason I gave up on trying to use RSS.If I&#x27;m going to have to follow a link anyway, I&#x27;ll just save myself the time and go directly to the websites I want to read. reply kazinator 9 hours agorootparentRSS solves the problem that web portals with updating content are web pages that you have to grok in their rendered form. RSS allows such a web page to offer a summary of the updating items in a more structured form that users can consume via a feed reader.The idea is that checking an Inbox-like reader dashboard with 7 feeds is less time consuming than separately going to 7 websites and scrolling around.RSS readers keep track of which items are new. They can filter items, mark them \"read\" and delete them. They offer searches through the items.Using the original site instead of its feed doesn&#x27;t guarantee you don&#x27;t have to follow any links.Look at this HackerNews: you have to click on links in the front page to get to the submissions, which are in different sites. Gee, how useless; if I have to click, I might as well just go directly to those websites and skip this HackerNews thing. reply joshmanders 9 hours agorootparent> Look at this HackerNews: you have to click on links in the front page to get to the submissions, which are in different sites. Gee, how useless; if I have to click, I might as well just go directly to those websites and skip this HackerNews thing.This analogy breaks down when you take into context what the purpose is for both sites.HN: I want to discover new content that likeminded people find interesting.RSS: I want to read what this person has to say specifically. reply kazinator 5 hours agorootparentIt&#x27;s not just an analogy, because you can have an RSS feed of HackerNews. You&#x27;re no worse off clicking on links in a HackerNews feed, than clicking on links in HackerNews. reply rchaud 8 hours agorootparentprev> if I have to click, I might as well just go directly to those websites and skip this HackerNews thing.You accidentally arrived at the root of the anti-RSS rationale for many HN readers.They don&#x27;t want to actually go on the web. They want to participate in the comments section, where it&#x27;s safe and sanitized; no paywalls, no &#x27;sign up for my email list&#x27; popups, no analytics scripts, no strange site layouts.They feel RSS should deliver exactly what they want: full-text article and images, nothing more. reply Xymist 2 hours agorootparentI _don&#x27;t_ particularly want to participate in the comments section on most sites. Full text and image content of the article and nothing further is absolutely what I want out of an RSS feed - I should not have to leave my reader or deal with the internet at large to read the things that interest me. reply kazinator 5 hours agorootparentprevWith just article text and images, you have no UI to reply to it.Maybe RSS could be extended to FRSS (Forum RSS). Forums export posts in FRSS format, which includes elements that tell the reader how to submit replies, as well as up&#x2F;down votes. The RSS reader would render its own UI for doing these things. reply jraph 9 hours agorootparentprevIsn&#x27;t it throwing the baby with the bathwater? In such a case RSS already tells you if there is new stuff and what already. If you follow a dozen of websites, not having to visit all them each time is already useful.Or are all your followed website producing new content such that there&#x27;s always something new when you check it out? reply mdp2021 1 hour agorootparentprevRSS is there to inform you of new content being released - it is a \"push\" notification. reply flir 4 hours agorootparentprevI&#x27;ve been meaning to get around to an RSS pipeline tool that runs each link through a chatbot summarizer and stuffs the summary in the description field.Might get around to it this Christmas, but I&#x27;m throwing it out there in case I can lazyweb it into existence. reply rldjbpin 23 minutes agoprevi have been using rss to get all my tech news (other than here perhaps) for over a decade.however since past at least 5 years, many tech sites have implemented \"dark patterns\" to hinder the experience and to nudge you to open their website. the \"click here to read more\" or just giving the summary outright does not help with the goal of having a feed.i get that in today&#x27;s age, where everyone wants to gatekeep their \"content\", it is an easy way to scrape data (i truly hope this does not become a trend). at the same time, it is so cheap to maintain that even static site running on solar can manage it (https:&#x2F;&#x2F;solar.lowtechmagazine.com&#x2F;feeds) reply jmduke 10 hours agoprevOne hackneyed reason that some folks give for not wanting to expose their RSS is wanting some level of legibility into their subscribers. I recently learned that many large RSS readers actually expose high-level analytics numbers [^1] so you can get an estimate of RSS readership that way, too. Would love for more readers to support this functionality: as far as I can tell across all of the RSS feeds my product exposes, the only supporting clients of this faux protocol are NewsBlur, Feedly, Feedbin, and inoreader.[^1]: ht to Darek Kay&#x27;s blog post, https:&#x2F;&#x2F;darekkay.com&#x2F;blog&#x2F;rss-subscriber-count&#x2F;, for alerting me to this fact! reply rambambram 3 hours agoparentExposing the number of subscribers from large RSS readers is a nice addition, but it also slightly nudges one to centralization.I try to let site owners know I&#x27;m subscribed to their feed by adding an URL parameter like &#x2F;?rss_ref=heyhomepage.com They might or might not see this RSS referrer in their own visitor statistics. I also do not consume all the article&#x27;s content solely in my reader, but I show a short summary and then click the link to the article. This way I can enjoy their (personal) site and they can see my traffic more clearly. reply cqqxo4zV46cp 10 hours agoparentprevThis is also common for podcast clients with server-side crawling, which is basically all of them. reply FerretFred 1 hour agoprevFortunately I found an iOS reader that will go and hunt for reclusive RSS&#x2F;ATOM feeds, but the author&#x27;s right - why make it difficult - just put a real link up! reply AnonHP 5 hours agoprevTangential question, since I use feeds but am not an expert in this: why do some feeds lose the articles after sometime while others don’t?Say I have two feeds from two different sites. The newsreader shows only the most recent (10 or 20 or whatever) articles for one feed whereas it shows all the articles from the time I subscribed to that feed for the second one. I know that the feed XML itself contains only a limited number of articles in both the cases whenever the reader requests for a refresh.How can I make sure that the newsreader preserves (does not purge) the older articles in the first case above (I use NetNewsWire on macOS and Mozilla Thunderbird on Windows)? reply defrost 5 hours agoparentThat&#x27;d be a per Reader, per Feed \"Max Entries to Save\" setting - which may have default values that differ from reader to reader, from \"feed collection\" to \"feed collection\" (if you create tree heirarchies in your reader) and might (I don&#x27;t recall) have default values (unless over ridden in reader) from individual feed values.It&#x27;s all very implementaton dependant.I mainly use FeedBro as a browser extension - everything is a per feed value that can be set to over ride FeedBro defaults ; in FeedBro I can bundle all different news sites (BBC, ABC(AU), Fox, CNN, etc) rss feeds under a \"News\" heading but cannot set a single Max Entries value that is applied to all the sub feeds - other implementations (different readers) can. reply SuperNinKenDo 5 hours agoparentprevWell, to answer your initial question, RSS is XML, meaning for the most part it relies on serving the entire document every time to create a valid feed file. As such, as the number of articles grows, every request for the feed grows in size. So clipping it to the latest stuff helps relieve load on the server.RSS is also commonly used to serve podcasts, and I&#x27;ve been told by a number of podcast hists that they like this because it prevents someone from sticking the RSS in their podcatcher and blasting the server with requests for every episode all at once.I&#x27;m guessing that a lot of RSS generators have this as a default, and many of the people with their RSS set up like this simply never change the default. reply GavinAnderegg 9 hours agoprevI felt like this was aimed at me[1], so I added an RSS icon in my page&#x27;s footer.https:&#x2F;&#x2F;github.com&#x2F;gavinanderegg&#x2F;gavinanderegg.github.io&#x2F;com...I had assumed that an \"application&#x2F;rss+xml\" link would be sufficient, but I get that folks would likely not assume that exists on all sites these days. As someone who reads blog posts mostly through RSS, I&#x27;m very happy to make this more explicit![1] https:&#x2F;&#x2F;mastodon.social&#x2F;@gavinanderegg&#x2F;111362850402497489 reply JKCalhoun 6 hours agoparentThanks, my page also was lacking in exposing my RSS feed. I&#x27;m htmlstupid and so massaged what you did for your site.That&#x27;s two of us now that have exposed our RSS. reply loughnane 10 hours agoprevI use miniflux and if im looking for a feed I just toss in the domain. 3 out of 4 times it find something at &#x2F;rss, feed, or something else.Would be great if it was exposed. I presume it’s not because it’s baked into frameworks and comes along for free, but maybe that’s naive. reply joshbode 2 hours agoprevShoutout for The Old Reader (https:&#x2F;&#x2F;theoldreader.com&#x2F;) as a great online RSS reader ($20 per year) which sprung up in response to Google Reader shutting down and is still going strong. reply robdelacruz 3 hours agoprevCheck out FreeRSS, it&#x27;s an iGoogle clone, just Add Widget, paste in the url of the website, and it auto-detects any RSS feeds in the site.https:&#x2F;&#x2F;freerss.robdelacruz.xyz&#x2F; reply ramijames 8 hours agoprevThe fact that RSS was suppressed will never cease to make me angry. reply rambambram 3 hours agoparentSuppressed by who? Google might have killed their newsreader, but RSS never stopped, because it&#x27;s not dependent on Google (hurray!). I&#x27;m very happy Wordpress automatically adds a feed to every site on the internet.I see it as a filter. All the stupid clickbait content goes somewhere else, and all the quality content - where the individuals who made it have skin in the game - sits nicely in my feed reader. It&#x27;s a blessing in disguise, if you ask me.Yes, you have to put extra energy in curating a nice collection of feeds. But isn&#x27;t that valid for everything good in life? This energy compounds and pays back in no-time.Long live RSS, long live the open web! reply outcoldman 7 hours agoprevWhen I was building this https:&#x2F;&#x2F;github.com&#x2F;outcoldman&#x2F;hackernews-personal-blogs it was crazy how inconsistent people are with their links to rss feeds reply bastawhiz 5 hours agoprev>> if you&#x27;re going to add an RSS button, please ensure it looks like an RSS button and is in RSS orange> This is an excellent idea and I have done so here.I don&#x27;t know if this was sarcasm or whether it&#x27;s just a bit silly that literally the next line of text on the page is a link (the link?) to subscribe—with the RSS icon—in white on magenta. reply rambambram 3 hours agoparentI also chuckled. The orange does something extra for visibility, but I can&#x27;t stand that color (even though I&#x27;m Dutch). The &#x27;Wifi icon on it&#x27;s side&#x27; is enough for me. reply Hamuko 2 hours agorootparentYeah, I don&#x27;t like the colour either. I also can&#x27;t really justify adding an orange logo on my site when it&#x27;s almost entirely in black and white otherwise. reply tkellogg 8 hours agoprevanyone out there generating fediverse feeds for their static site? reply boyter 7 hours agoparentI don&#x27;t think this would be practical for a static site. You still need to maintain a list of followers of your account somewhere and that needs to be dynamic if you want it to work the way people expect it to where they follow you from other instances.Assuming you kept the @ list of accounts through some other means, if you had your webfinger setup with your public key, you could after creating new content to push up sign the publish events and push them to those followers.I don&#x27;t know of anyone doing this though. reply dugite-code 7 hours agoparentprevI think it was Mark Johnson of the linux matters podcast that was talking about how he&#x27;s experimenting with integrating his blog with the fedi-verse using aws lambda for the dynamic parts https:&#x2F;&#x2F;linuxmatters.sh&#x2F;16&#x2F;I also read another blog about it a while back, but essentially you have the same issue as comment systems where it can&#x27;t be static, there needs to be some server component handling the back and forth. reply blacklight 1 hour agoprevI have made an extension that parses thetags and puts back the feed icon where it&#x27;s supposed to be (on the right side of the URL bar): https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;rss-viewer&#x2F;This is how the web used to work 10 years ago. Thetag has a purpose: it instructs browsers that the current page has a feed, so the browser can parse it and show a feed button to the user. Both Firefox and Chrome used to work this way 10 years ago.Then motherfucking evil Google began its war against feeds. It killed Google Reader first, then it removed the feed icon from the browser. Firefox quickly complied too.Let&#x27;s make it clear once and for all: yes, I, can add an RSS URL to my website to make discoverability easier, but it&#x27;s not my job as a web developer to make feed discoverability easier. That&#x27;s just a workaround. My job as a web developer should be to provide the rightelement, and then the browser should know what to do with it. I shouldn&#x27;t write my custom JavaScript to parse it from the DOM. If the browsers refuse to do that, then it&#x27;s a browser problem, and extensions should be used by anyone who still cares about feeds to mitigate the impact of feed-hostile browser politics. reply yawnxyz 10 hours agoprevare there any guides on how to do RSS correctly?I just dump all of our blog content in there (250 posts) and it&#x27;s a lot of content haha. I don&#x27;t know how you&#x27;re supposed to set it up and there are so few guides on it reply kevincox 9 hours agoparentI wrote one a while ago and have been maintaining it since: https:&#x2F;&#x2F;kevincox.ca&#x2F;2022&#x2F;05&#x2F;06&#x2F;rss-feed-best-practices&#x2F;It was fairly popular on HN when it first went live: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31293488It has a section on discovery: https:&#x2F;&#x2F;kevincox.ca&#x2F;2022&#x2F;05&#x2F;06&#x2F;rss-feed-best-practices&#x2F;#disc... reply disposition2 30 minutes agorootparentThanks for sharing reply drkstr 7 hours agorootparentprevI thought this might be an interesting read but was super put off to be greeted with a long screed moralizing over my choice of browser. I have 3 different browsers installed that use for various situations and reasons. Getting shamed by an internet stranger is not one of them. reply mdp2021 1 hour agoparentprevRSS works in the framework of users periodically checking for new content (maybe daily, maybe weekly...): you have to decide a window that has the users receive a reasonable number of new and old posts. reply phyzome 9 hours agoparentprevOther reply has good advice. But beyond that I&#x27;d advise 1) specifically using the Atom format, which is more likely to display correctly, and 2) just include the latest 10 posts. reply ericjmorey 7 hours agorootparentWhy only 10? Why limit it at all? reply equinoxnemesis 6 hours agorootparentIt&#x27;s a good idea to limit it somewhere so you don&#x27;t end up sending 10 MB every time it&#x27;s fetched. The feed will be re-fetched to check for updates so the cost isn&#x27;t paid just once. reply charcircuit 2 hours agorootparentIt is paid only once since that 10 MiB gets cached by the reader. In future fetches the reader asks for entries newer than the date it last checked which means that the items already requested won&#x27;t be sent again. replyintrasight 5 hours agoprevI used to generate RSS feeds for all the sites I build for clients. Those were good times - 2003. reply teeray 5 hours agoprevIs there any service that scrapes arbitrary sites into an RSS feed? reply leeeeeepw 9 hours agoprevNoice...Checkout my RSS feed of AI characters and Chatbots getting created netwrck.com&#x2F;rss.xml reply weijarz 7 hours agoprevI developed a new web RSS Reader (PWA): https:&#x2F;&#x2F;www.qireader.com Welcome to use. reply jrhey 1 hour agoparentThis looks really nice reply KoftaBob 1 hour agoparentprevThat looks great! Is it open source? reply forrestthewoods 9 hours agoprev [–] RSS doesn’t move the needle. Only a super tiny, insignificant number of people care about RSS. Social aggregators like HN and Reddit won. RSS lost.I have a blog and try my best to maintain the RSS feed. But if we’re being honest it’s a waste of time.I stopped using RSS readers when half my subs required me to open a browser tab to go to the actual website to view the actual content. Webcomics were particularly annoying about this.I mean I get it. The ideal RSS feed has no ads which means no revenue for people working hard to create interesting things I want to read! Alas. reply enasterosophes 9 hours agoparentI never actually expected my rss reader to provide a place for viewing the content. I treat it as a notification service. There is usually enough info in the subject line for me to decide whether I want to open the notification in a browser or dismiss it as read.In addition to a few blogs, I use rss for news pages (including hacker news front page) and to get notified about new package releases in pypi and rubygems. For the package releases, I just mark them as read as soon as I upgrade my virtualenvs. reply kevincox 9 hours agoparentprev> Social aggregators like HN and Reddit won. RSS lost.Social aggregators are great for discovery, they can let the community surface good items, but they suck for subscription. Once I have found authors that I like I want a way to reliably see their content, not hoping that it happens to be popular on the site at around the time that I happen to visit. Unless you are refreshing your feed hourly you are going to miss a lot of content from your favourite authors, and even the most addicted users won&#x27;t see it all. As an author I also appreciate loyalty of subscribers more than viral surges, but I understand that the money is probably better with the viral surges. reply phyzome 9 hours agoparentprev\"Expose\" was the key word here. A ton of sites already have feeds and just don&#x27;t make them easy to get to. It&#x27;s honestly not hard.It&#x27;s also a category error to compare RSS and social aggregators. reply forrestthewoods 9 hours agorootparentI&#x27;m saying that social aggregators killed RSS. Not that they&#x27;re equal in use case. Where \"killed\" means \"some minuscule percentage that is large than 0 but less than an arbitrary threshold of my choosing\". reply panic 9 hours agoparentprevWhere do you think we find the posts we submit to HN? reply rambambram 3 hours agoparentprevTalking about RSS in terms of &#x27;winning and losing&#x27; is exactly why I use RSS over the mainstream social media. Not everything is a competition. reply schemescape 9 hours agoparentprevIf the target audience for a blog is HN readers, then RSS is probably worth it (since HN readers use RSS, myself included).The biggest issue I have with RSS is that by the time I find interesting blogs, they’ve stopped posting, so I never see updates in their feeds. But that’s an issue with blogs, not RSS. reply microflash 4 hours agoparentprev> I stopped using RSS readers when half my subs required me to open a browser tab to go to the actual website to view the actual content.The RSS client that I use, InoReader, has an option to fetch full content if it is just a blurb. So, I have to never deal such annoyances. reply charles_f 4 hours agorootparentFreshRSS and Miniflux both have the option. The former can even let you customize some css filters to cleanup the stuff it retrieves. reply cromka 9 hours agoparentprevI don’t agree — RSS was only ever meant really to show the title and some few paragraphs, maybe a photo — exactly like Reddit does. I for one would hate to scroll through that much content. reply remram 8 hours agoparentprev> The ideal RSS feed has no adIsn&#x27;t that like the ideal anything-else? What&#x27;s specific about RSS on ads? reply stefan_ 7 hours agoparentprevRSS users are the Linux gamers of the podcast & blog world - tiny amount of people but all the problems in the world. reply fLaMEd 4 hours agorootparentOk, as both of these… this made me laugh reply colesantiago 9 hours agoparentprev [–] > The ideal RSS feed has no ads which means no revenue for people working hard to create interesting things I want to read!I don&#x27;t see a problem with this at all, the best things on the internet are free and open source.Everyone gets content for free and the author gets more views everybody wins. reply jen729w 8 hours agorootparentAs an independent content creator I just called my rental agency and asked them if they would take ‘views’ as partial payment and I can confirm that they will not.Bummer! reply eropple 8 hours agorootparentprev [–] And what quality content are you creating to contribute to this? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author encountered difficulty finding RSS feed links on different websites.",
      "Many sites were found to be missing this feature, hindering web accessibility.",
      "The author advocates for website owners to include RSS links to support RSS and enhance web accessibility. They suggest using standard link and RSS auto-discovery methods to make it easier for browsers and software to locate a site's RSS feed. The author also recommends including multiple link tags for multiple feeds and emphasizes the significance of displaying a recognizable RSS button."
    ],
    "commentSummary": [
      "Issues with discovering and accessing RSS feeds on websites, due to lack of visibility and support from browsers.",
      "The disappearance of Google Reader has impacted the availability and popularity of RSS feeds.",
      "Lack of native support for advertising in RSS feeds, making them a preferred choice for those who prefer an ad-free experience.",
      "Inconsistency of content in RSS feeds, which can vary in format and quality.",
      "Advantages of using an RSS reader for convenience and personalized content aggregation.",
      "Debates on scraping content programmatically, selling data from RSS feeds, and exposing subscriber counts.",
      "Suggestions to extend RSS to include forum posts and integrate static sites with fediverse feeds.",
      "Discussions on website design, setting up and maintaining RSS feeds, and comparisons between RSS readers and social aggregators.",
      "The presence of ads in RSS feeds is a topic of debate."
    ],
    "points": 237,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1702248775
  },
  {
    "id": 38595245,
    "title": "The Impact of After-Hours Work on Productivity and Well-being",
    "originLink": "https://slack.com/intl/en-gb/blog/news/the-surprising-connection-between-after-hours-work-and-decreased-productivity?nojsmode=1",
    "originBody": "News The surprising connection between after-hours work and decreased productivity Slack’s Workforce Index uncovers new findings on how to structure the workday to maximize employee productivity, well-being and satisfaction By the team at Slack5th December 2023 Table of contents Table of contents When it comes to productivity, it’s not quantity of time spent working, it’s quality Prime productivity hours: Whether you’re a morning person or a night owl, the afternoon slump is real The “Goldilocks Zone” for work: How to balance your workday to optimize your productivity What do desk workers most want from AI? Assistance and automation to rightsize the meeting load and free up time Methodology Download PDF 7 min read Quick take: How do you spend your time at work and what is it costing you? Slack’s Workforce Index, based on survey responses from more than 10,000 desk workers around the globe, uncovers new findings on how to structure the workday to maximize productivity and strengthen employee well-being and satisfaction. Key learnings include: Employees who log off at the end of the workday register 20% higher productivity scores than those who feel obligated to work after hours. Making time for breaks during the workday improves employee productivity and well-being, and yet half of all desk workers say they rarely or never take breaks. On average, desk workers say that the ideal amount of focus time is around four hours a day, and more than two hours a day in meetings is the tipping point at which a majority of workers feel overburdened by meetings. Three out of every four desk workers report working in the 3 to 6pm timeframe, but of those, only one in four consider these hours highly productive. For decades, putting in extra hours at the office was seen by many as a sign of hard work and productivity, even a badge of honor. But new research from Slack shows despite that longstanding perception, working after hours is more often associated with lower levels of productivity—and could be a red flag that an employee is juggling too many tasks and needs help prioritizing and balancing their time. The latest results from the Workforce Index, Slack’s survey of more than 10,000 desk workers, show the productivity gap depends on what’s driving workers to burn the midnight (or early morning) oil. About two out of every five desk workers (37%) are logging on outside of their company’s standard hours at least weekly, and more than half (54%) of these workers say it’s because they feel pressured to, not because they choose to. Employees who feel obligated to work after-hours register 20% lower productivity scores than those who log off at the end of the standard workday. They also report: 2.1x worse work-related stress 1.7x times lower satisfaction with their overall working environment 2x greater burnout Both groups say around 70% of their time spent working is productive—a sign that those working extra hours are putting in as much effort as their colleagues—but those who work after hours are 50% more likely to say their productivity is blocked by competing priorities compared to those who log a standard workday. On the flip side, employees who work outside of standard hours by choice, to better suit their schedule or to pursue personal ambitions, report no negative impacts and even a slight uptick in their wellness and productivity scores. “We’ve long seen a focus on quantity over quality across many aspects of work, from how we spend our time to how we define productivity. Constantly feeling like you need to catch up is hurting employees and businesses. This underscores the importance of building a culture of trust where employees feel safe enough to speak up when they need help prioritizing and have the right balance of time in the work day to get work done.” Christina JanzerSVP of Research & Analytics and Head of the Workforce Lab, SlackLearn more about Slack's Workforce Lab When it comes to productivity, it’s not quantity of time spent working, it’s quality Results from the Workforce Index show that a significant portion of desk workers across the globe are struggling to balance their time at work, with different job tiers experiencing this problem in different ways. More than one in four desk workers (27%), including more than half (55%) of executives, say they spend too much time in meetings. A similar share (25%) of all desk workers, including 43% of executives, say they spend too much time in email. One in five (20%) don’t have enough time to connect with coworkers, and this problem is most pronounced among more junior employees. Alarmingly, the data shows that many workers across all levels are plowing through their daily tasks without any down time: Half of desk workers surveyed (50%) say they rarely or never take breaks during the workday. These workers are 1.7x more likely to experience burnout. Their break-taking counterparts, on the other hand, show 62% higher scores for work-life balance, 43% greater ability to manage stress and anxiety, 43% greater overall satisfaction, and—perhaps surprisingly—13% higher scores for productivity. “Why did we all come to believe that we are more productive if we are always on and that we need to burn out in order to succeed? It goes back to the first Industrial Revolution, when we started revering machines. The goal of machines is to minimize downtime. But for the human operating system, downtime is not a bug, it’s a feature. Elite athletes know that recovery is part of peak performance. Downtime is a productivity multiplier.” Arianna HuffingtonFounder and CEO, Thrive Global Prime productivity hours: Whether you’re a morning person or a night owl, the afternoon slump is real On average, desk workers say that 70% of their time at work is productive. When asked about prime hours for productivity, answers vary widely, with some desk workers preferring the morning and others preferring the evening. But no matter their preference, a majority (71%) of desk workers agree that the late afternoon is the worst time for work, with productivity plummeting between the hours of 3 and 6pm. While three out of every four desk workers report working in the 3-6pm timeframe, only one in four consider these hours highly productive. “This goes to show that productivity isn’t linear. Productivity happens in bursts, on and off throughout a day, not necessarily in prescribed windows of time, and definitely not for eight consecutive hours. The ‘afternoon slump’ shouldn’t be seen as a bad thing; for many workers this could be an ideal time to take that break that will boost their overall productivity for the day.” Christina JanzerSVP of Research and Analytics and Head of the Workforce Lab, Slack The most productive people use time management strategies. They are 1.6x more likely to block time to complete specific tasks, 1.7x more likely to only check email at specific times, and 2.2x more likely to set focus timers. The “Goldilocks Zone” for work: How to balance your workday to optimize your productivity While there’s no one-size-fits-all schedule that applies across all industries, roles, and job levels, a close examination of the data reveals a formula emerging to set employees up for success. Regardless of job tier, the research shows a “Goldilocks Zone” for the ideal balance of focus time, collaboration time, social connection, and downtime. On average, desk workers say the ideal amount of focus time is around four hours a day. More than two hours a day in meetings is the tipping point at which a majority of workers say they’re spending “too much time” in meetings, with a similar pattern emerging across all job levels. People who say they spend too much time in meetings are more than twice as likely to say they don’t have enough time to focus. In contrast, about 10% of desk workers, most common among employees with less than one year at a company and those under 30, say they spend too little time in meetings, and this is also associated with decreased sense of belonging and productivity. “Focus time, collaboration time, connection, and rest are like the macronutrients of a workday. The right balance gives you the energy you need to work your best. We cannot consider these critical components of our work in silos. To be our most effective, we must create the space for collaborative work and for focused work.” David ArdSenior Vice President of Employee Success, Slack and Salesforce What do desk workers most want from AI? Assistance and automation to rightsize the meeting load and free up time At the same time that desk workers are struggling with time management, many are also excited about the potential of AI tools to give them more command over balancing their time. An overwhelming majority of executives—94%—feel some urgency to incorporate AI into their organizations, with half of executives saying they feel a strong sense of urgency. And yet, our survey shows that adoption of AI is still in its infancy, with only one in five desk workers reporting that they have used AI tools for work. Given the low adoption, it’s not surprising that most desk workers (more than 80%) say that AI tools are not improving their productivity at work—yet. But they’re anticipating that AI will assist with one of the biggest struggles of the workday: meetings. The top three activities that employees expect AI will provide the most value in the future are 1) meeting notes and recaps 2) writing assistance and 3) automation of workflows. “People at every job level may be shocked to see that more than two hours of meetings a day reduces productivity. It may feel unrealistic to many team leaders to try to hit that target today. But that’s where the newest generation of AI tools could be a lifesaver. An AI assistant that could accurately summarize meeting notes and automate common workflows could be the key that frees up our time and helps us to unlock the balance we need to set ourselves up for success.” Christina JanzerSVP of Research and Analytics and Head of the Workforce Lab, Slack Are you working hard or working smart? Dive deeper into what the data shows about how to optimize your time at work in our webinar New research uncovers the secret to a productive workday. Methodology The Workforce Index surveyed 10,333 workers in the U.S., Australia, France, Germany, Japan and the U.K. between August 24 and September 15, 2023. The survey was administered by Qualtrics and did not target Slack or Salesforce employees or customers. Respondents were all desk workers, defined as employed full-time (30 or more hours per week) and either having one of the roles listed below or saying they “work with data, analyze information or think creatively”: executive management (e.g. president/partner, CEO, CFO, C-suite), senior management (e.g. executive VP, senior VP), middle management (e.g. department/group manager, VP), junior management (e.g. manager, team leader), senior staff (i.e. non-management), skilled office worker (e.g. analyst, graphic designer). For brevity, we refer to the survey population as “desk-based” or “desk workers.” Productivity Tips and tricks Workplace culture Employee experience Was this post useful? Yes, thanks!Not really 0/600Submit feedback Nice one! Thanks a lot for your feedback! Got it! Thanks for your feedback. Whoops! We’re having some problems. Please try again later.",
    "commentLink": "https://news.ycombinator.com/item?id=38595245",
    "commentBody": "The surprising connection between after-hours work and decreased productivityHacker NewspastloginThe surprising connection between after-hours work and decreased productivity (slack.com) 225 points by tchalla 12 hours ago| hidepastfavorite124 comments sublinear 8 hours agoMy own experience is that hours worked are irrelevant. It can vary. Deal with it. To believe otherwise is to think management fully understands the work ahead of time. This is not usually the case. To a lesser extent, even the devs might not know. Overscheduling and underscheduling happen all the time. Things catch on fire. You have to work accordingly. Some days you can stop early. It&#x27;s not entirely in the hands of the workers. You won&#x27;t know until you start.What matters more is a reliable result done on time and everyone is happy with the effort required to achieve that. That&#x27;s a complex balance to achieve across the whole team. You need everyone recognizing the long term benefits of a job well done and they need to feel comfortable with their part of it.Working slowly after hours with no promise of getting much if anything concrete done is a deep joy for some of us. I sometimes need to play with my work to know what I&#x27;m doing tomorrow. It&#x27;s often outside the scope of what I&#x27;m being asked to do, yet vital to a successful project instead of a mediocre one full of garbage decisions smoothed over by management lies and stress on the whole team to maintain.What world does one want to live in? Build it and enjoy. That&#x27;s where productivity, happiness, and ease come from. There are so many heroes out there casually looking at work while eating cereal in their underwear at 1am. They&#x27;re no more stressed than anyone else. reply matrix87 7 hours agoparent> There are so many heroes out there casually looking at work while eating cereal in their underwear at 1am. They&#x27;re no more stressed than anyone else.I personally know several people from amazon, pretty much all of them acknowledged the crazy oncall expectations there. Combine that with the pip culture and visa issues, and your statement above about being \"no more stressed than anyone else\" is just ridiculousExpecting people to work at 1AM, romanticizing them as \"heroes\"... that&#x27;s not a normal culture. That&#x27;s not a healthy lifestyle for the employee and can lead to long term health consequences. And it never applies to everyone equally, because why would you call them \"heroes\" in the first placeIt&#x27;s one thing if management expects it, it&#x27;s another thing if I as an employee choose to trade 4 daytime hours for 4 nighttime hours because I know I can be more productive at that time. But building that type of management expectation into a work culture is a red flag reply sublinear 6 hours agorootparentYeah I&#x27;m not talking about being \"on call\". I don&#x27;t know anything about Amazon other than it&#x27;s probably full of bad culture. Nowhere I&#x27;ve worked has ever been like that except a handful of emergencies.I&#x27;m talking about working at a relaxed place because everyone actually gets their stuff done 9 to 5. A place that&#x27;s fun and interesting enough that you feel happy to randomly think about work in the middle of the night for an hour or two or whenever else motivation strikes to keep it that way.I&#x27;m saying that these little nudges from the team where extra hours are silently worked prevents being forced to pick up the slack that shouldn&#x27;t be there in the first place. Slack comes from miserable people regardless of how many hours they do or don&#x27;t work. reply indigochill 2 hours agorootparent> A place that&#x27;s fun and interesting enough that you feel happy to randomly think about work in the middle of the night for an hour or twoI feel like in my experience it&#x27;s actually less about fun and interesting and more about do you feel like the people running the company see you as a compatriot or a cog? I worked at a pretty tiny startup in college where I regularly talked with the CEO and worked directly for one of the VPs. They were great and from start to finish it&#x27;s been my most positive work experience, since I felt like one of the team. Dare I say it even felt a bit like a family (well, the CEO and VP were a husband-wife team and I was a college kid at the time).Then I switched to a company that billed itself as having a startup culture but really couldn&#x27;t because of turf wars between layers of irrelevant middle management, never mind regular tone-deaf decisions from upper management and repeated bad bets from them that lead to multiple layoffs. The work itself is actually more interesting than what I was doing at the real startup, but the mismanagement has pushed me into a mindset of mostly only giving them what I&#x27;m contractually obligated to, with rare exceptions for smaller projects that I feel I can have a greater direct impact on. I would hazard a guess that management in that company is actually a net negative because if it wasn&#x27;t for all the bad politics there, I&#x27;d probably be just as happy to give them extra time because like you say the work is fun and interesting. And I know a couple other guys there who -were- regularly giving extra time until they got repeatedly personally burned by management. reply watwut 1 hour agorootparent> a company that billed itself as having a startup culture but really couldn&#x27;t because of turf wars between layers of irrelevant middle management, never mind regular tone-deaf decisions from upper management and repeated bad betsHonestly, that sounds like typical startup to me. reply matrix87 6 hours agorootparentprevI initially thought above comment was about on call because of the \"things catch on fire part\", I guess not> where extra hours are silently worked prevents being forced to pick up the slack that shouldn&#x27;t be there in the first place. Slack comes from miserable people regardless of how many hours they do or don&#x27;t workThis is true except from my experience the miserable people who leave slack for others to pick up aren&#x27;t normally all that miserable, just not really motivated. At least, they&#x27;re not miserable until management does something about it reply zztop44 3 hours agorootparentI think it depends. I’ve encountered some people that definitely seen that way. But I also know that during every period of prolonged underperformance in my career (there have been a couple) I have felt awful about it despite having a strong incentive to try to convince myself it’s my fault. reply strken 3 hours agorootparentI think a lot of underperformers end up in a hole where they&#x27;re burnt out to such an extent that they don&#x27;t have the energy to leave. It takes clarity, introspection, positivity, and extra work to find a new job; those are exactly the things that burnout saps.Of course, most people are really good at hiding their emotional state, since that&#x27;s a requirement to fit into a professional workplace. The facade of happiness stays up well after the real thing is gone. reply quickthrower2 6 hours agorootparentprevIt is not about the time on the clock, but who controls that clock. If it is the employee they are happy!Get the 1am lover to do 9-5 they will be as miserable as the 7-3 guy being woken up a 1am reply water-your-self 6 hours agorootparentMy best hours worked start at 7pm when the world is quieter and my peers arent around to disrupt me.Plus it feels good that the next time I see someone I have conjured a deliverable.Meanwhile, if you give me daily 9am meetings I am not functional at any point in the day reply mewpmewp2 2 hours agorootparentprevThis sounds more like a Visa problem though. I think the best stress reliever is when you know you are safe losing your job or walking away by yourself, you have enough money saved that you can spend more than a year doing whatever you want even if you were to lose your job and without having to worry about it influencing your life. Then the motivator to do long hours and work at odd times is passion, desire to climb and feeling of ownership rather than fear of losing your job. reply matwood 2 hours agorootparentprevAmazon is known to be a bad place to work for the exact issues you list. The OP was talking about generally, which I agree with.This conversation feels very similar to comp conversations where outliers drive the discussion. reply hn_throwaway_99 7 hours agorootparentprev\"Crazy on-call expectations that require you to be available and potentially working at any hour of the day and night\" is something very different than the topic under discussion, and I didn&#x27;t interpret sublinear&#x27;s comment at all to be endorsing that viewpoint. reply lannisterstark 4 hours agorootparentprevthis isn&#x27;t about &#x27;on-call&#x27; or &#x27;expectations.&#x27; Eg, I&#x27;m currently looking at a work problem casually (it&#x27;s about 11 PM) because I have some time and I&#x27;m curious, so I can slack off tomorrow at work. Win win. reply atoav 4 hours agoparentprevI have seen this repeated so often, but let me put something clear: If your work constantly requires weird hours when it could realistically be a 9-to-5 job if they just hired enough&#x2F;the right people or had their marbles together in terms of organization — then the only reason you need to do this is mis-management.I worked in the film industry where you are sometimes for actual physical reasons (the sun being the sun) required to work weird hours. Excuse my french here, but only idiots wear bad work schedules as a badge of honor. Bonus points for when you confuse the thin bond you form with the other victims of such abuse with friendship. If you constantly need to overschedule people, you did a bad job at resource allocation. If your work load is constantly on the edge and things are always \"exciting\" for all the wrong reasons, you are managed by incapable fools. In a smooth operation the content of the work is exciting, the schedule and the work hours boring.You can call yourself a \"hero\", but it is your life (that all of us only get one of) you&#x27;re wasting. If you ask me (and you didn&#x27;t) there is better a good reason each time it is wasted like that.Don&#x27;t get me wrong, I am willing to step in to do the extra mile when there is a good reason, but in my experience it is very rare that there truly is. If the content of my work is exciting I can easily go for the whole day, but it would be in the interest of my employer to stop me, so I can do this for months without burnout and fatigue. reply mavamaarten 2 hours agorootparentFully agreed.Especially in IT, most deadlines are arbitrary moments in time, pulled out of someone&#x27;s ass. If the deadline is so tight that people need to work more hours, either there has to be a very good reason which there very well might be, or someone&#x27;s shit at making deadlines. Then admit you did poorly at planning, move the deadline, and only then can you start thinking about making your people work overtime. reply jader201 2 hours agoparentprev> There are so many heroes out there casually looking at work while eating cereal in their underwear at 1am.Please don’t call these people “heroes”, and if you’re doing this, please stop. You’re setting expectations that everyone else should also be up at 1am “casually looking at work”, and if they’re not, they’re not a true “hero”.This is how unhealthy work&#x2F;life balance starts at companies. People that work after hours for fun is is called workaholism. Meanwhile, the people that are also having fun from 9-5 are suddenly not doing enough, and are no longer having fun.And of course the “heroes” are not stressed — they’re able to enjoy working long hours, and making their way up the ranks by being a “hero”. reply Lutger 20 minutes agorootparentFraming abuse and alienation as heroism is how capitalism extracts value from the most precious resource in our decadent lives: time and energy.Within the confines of my insignificant job, I appreciate more the professionalism of a properly work-life balanced management, which should provide more than enough wiggle room for the weird hours people without fetishizing their volunteer work. reply fransje26 52 minutes agoparentprev> Working slowly after hours with no promise of getting much if anything concrete done is a deep joy for some of us. I sometimes need to play with my work to know what I&#x27;m doing tomorrow.It could be argued that code exploration at the pace you need should be part of your regular working hours, if that is what is needed to do your job efficiently. In the extreme, not allowing your employees to do so is failed management.Of course, the knowledge and efficiency you gain by doing so is not measurable by any quantifiable metric, so it&#x27;s easy to see how it will be conveniently discarded as unnecessarily.Even better, by having you convinced that it is something you should do in your own free time, they can reap the benefits without the costs. Full profit. reply dannycastonguay 7 hours agoparentprevAgreed. It&#x27;s important to put enough effort that you find meaning in your work, but not so much that it ruins your wellbeing. Here&#x27;s a rough algorithm that works for me:1. Estimate the hours you think it will take to complete a task.2. Double it and let the team know you did that.3. Do the work well including good documentation.4. Assess your progress when you&#x27;ve spent 50% of the planned hours. If you&#x27;re not at least halfway done, avoid overworking. Instead, seek help within the team and descope.5. Utilise any extra time for learning new and useful skills, if you finish ahead of schedule.Cheers reply quickthrower2 6 hours agorootparentI agree. This requires a healthy workplace though.I worked somewhere, well two places where I was literally taken to task about how long something took. Repeatedly. They didn’t care about why, just that it wont happen again.It didn’t: in both cases it’s time to fire up Word again and edit my CV (pretty much the one reason I use that program!) reply lokar 4 hours agorootparentIt’s reasonably straightforward to produce a very nice resume&#x2F;CV in LaTeX. reply crossroadsguy 3 hours agorootparentMaybe. But that’s like learning to build a car and then building one and fine tuning one because I had to go to an office 200 meters away once in a few years.Yeah, I did Texin’ in college and tried after that as well. No body gave a shit and now when I look at CVs for hiring purposes I don’t give a shit either. Now my CV is on a live.com free throwaway account — that’s where it resides and gets worked upon and converted to PDF when needed. reply quickthrower2 2 hours agorootparentIf I move away from Doc, it&#x27;ll be to Markdown most likely, or some kind of paid generator thing. replycrossroadsguy 4 hours agoparentprevIn my humble opinion you have started totally on something that a study&#x2F;poll like this doesn’t even point to.You are talking about I&#x2F;we&#x2F;people “wanting&#x2F;loving&#x2F;preferring” to work at certain hours or for certain length of hours (usually not often or not regularly&#x2F;daily) compared to being “asked&#x2F;made&#x2F;forced&#x2F;expected” to work that way often&#x2F;regularly&#x2F;daily.> There are so many heroesHeroes? Really? Okay. reply pawelwentpawel 1 hour agoparentprevForm the point of view of a maker &#x2F; contributor it&#x27;s common sense that the more tired you are, the worse your output will be. For me, there is a cut off point where the time spent working \"tired\" or after hours is just not worth the return anymore.That said, not every work is directly \"building\" - some aspects of a job might involve collaboration, communication and helping others out when they&#x27;re stuck. Spending some time after hours to help a colleague who got stuck might have a result which is disproportionately larger to the input that a tired person had. reply quickthrower2 4 hours agoparentprev> Overscheduling and underscheduling happen all the time. Things catch on fire. You have to work accordingly.You need more predictability than that in most things. Even start ups. The picture you are painting is of a badly organized company that needs to slow down a bit and ... think.I have worked for that kind of company and the more boring sort. The more boring sort tends to make more profit.Startups might be different, but it must be intentful chaos - not getting caught up in pointless busywork.> Some days you can stop early.Rarely. The only companies that allow that are probably the ones that made you do a 24h stint the day before. reply drBonkers 2 hours agoparentprev> There are so many heroes out there casually looking at work while eating cereal in their underwear at 1am.You’re insane. reply Muromec 1 hour agorootparentThere is a difference between being excited about learning this fancy technology thing in your 20ies and having to deal with the same basic shit to afford desirable lifestyle in 30ies. reply rightbyte 1 hour agoparentprev> My own experience is that hours worked are irrelevant. It can vary. Deal with it.Why? Just work your hours.Once I grew out of the initial imposter syndrom in my first job, I would never work late for some arbitrary deadline.> There are so many heroes out there casually looking at work while eating cereal in their underwear at 1am. They&#x27;re no more stressed than anyone else.Oh please. They are losers getting abused, not heroes. And they wage dump the rest of us.You can get away with romantizing that for hackers or business owners in the startup phase. That is like 0,5% of us. reply deebosong 6 hours agoparentprevI&#x27;m in the \"creative\" field (aka, visual time-based arts, so to speak, but honestly, it&#x27;s \"work,\" and in the same vein, every other job is teeming with much-needed creativity that often doesn&#x27;t get translated to visual arts, but that&#x27;s an aside). But this is how I approach my work as well. All that is to say, I really appreciate this post, and it&#x27;s very validating to whatever I&#x27;ve experienced in the \"creative\" visual arts field. Which to me kinda points to larger meta patterns that emerge from work in general. reply saulrh 7 hours agoparentprevYou only see people going on-call with no stress because everyone else burned out and quit. Very simple selection effect. Declaring that things can never be better and that this is fine prevents us from finding useful and meaningful improvements that will make our teams happier, healthier, more productive, more stable, and more inclusive. reply cedws 11 hours agoprev>Employees who log off at the end of the workday register 20% higher productivity scores than those who feel obligated to work after hours>Slack’s Workforce Index, based on survey responses from more than 10,000 desk workersOkay, so this \"productivity\" data is self-reported. How do you know that the after-hours workers aren&#x27;t simply rating their own productivity lower than actual, or that the 9-5 workers aren&#x27;t rating their productivity higher than actual? This data is useless. reply christophilus 10 hours agoparentTo me, it reads like this: “those who log off of Slack are more productive” which matches my experience perfectly. My productivity shoots up as soon as I close Slack. reply antisthenes 10 hours agorootparentDepending on your role, communication on Slack could be as much as 50% of your job responsibilities.Not everyone is a line dev who codes 7 hours after a 10 minute standup. reply groestl 10 hours agorootparent> Not everyone is a line dev who codes 7 hours after a 10 minute standup.Funny enough, those often answer fastest. The busiest communicaters sometime don&#x27;t answer for days. reply cqqxo4zV46cp 10 hours agorootparentprevnext [2 more] [flagged] TeMPOraL 10 hours agorootparentCorrect. Managers gotta manage, but code doesn&#x27;t write itself either, and the constant push to turn software engineers into part-time faux-managers (all the responsibilities, none of control or status) isn&#x27;t helping. reply acchow 9 hours agorootparentprevAnd how does that affect the productivity of those who are trying to reach you? reply jabroni_salad 6 hours agorootparentIf it wasn&#x27;t important enough to warrant a phone call, probably not that much.I was getting a couple hundred IMs a day and I would say that less than five of them were actually worth an interruption. reply x0x0 8 hours agorootparentprevIt forces them to structure their work so they don&#x27;t need constant realtime questions from teammates generating constant interruptions. reply taneq 8 hours agorootparentprevHow does it affect the overall long term productivity of the team, I think is the real question. reply tremon 9 hours agoparentprevIsn&#x27;t this just a variation of \"those who feel more productive during the day log off earlier\"? Or reversing the statement, \"those who feel less productive during the day feel compelled to work longer hours\"? reply acchow 9 hours agorootparentThose who are in a lower productivity state (stress, sleep quality, etc) are more likely to work after hours? reply taneq 8 hours agorootparentThose who are under the pump are more likely to be stressed, sleep poorly, AND work longer hours. Zounds! reply mvdtnz 4 hours agorootparentprevSure but is it a particularly long bow to draw to say those who feel less productive often are in fact less productive? reply kqr 1 hour agorootparentNot at all, but then all the survey says is that less productive people make up for it by spending more time. That hardly surprises me -- desk workers usually feel an obligation to complete tasks even on days they aren&#x27;t running hot. reply autoexec 10 hours agoparentprevA lot of this stuff seems obvious and self-evident, and I wouldn&#x27;t be surprised if much of it was supported by actual research, but you&#x27;re absolutely right that self-reported data gathered from internet surveys is questionable at best.Every person has their own definition of what their \"productivity\" even means, they each have only their own subjective measures of their own productivity, and there&#x27;s no verification to check who is providing the data, to catch those who simply outright lie or to catch people who click through the survey without even reading&#x2F;understanding what is asked. Two people could both report 10% higher productivity when in fact there is nothing similar about their productivity levels at all.Internet polls and surveys are cheap and easy which is why so many people use them, and they can be pretty fun when all you&#x27;re trying to find out is which celebrities are popular or which harry potter character you are, but they are generally terrible for getting data useful for actual science or for anything where accuracy matters. It&#x27;s no wonder that there&#x27;s such a huge reproducibility problem in social sciences considering the heavy reliance on self-reported data through internet surveys. reply polishdude20 8 hours agoparentprevYeah or if you&#x27;re the type of person who THINKS you&#x27;re not productive and therefore that causes you to work more hours. reply seanmcdirmid 10 hours agoparentprevWelcome to evidence in the soft sciences. I can&#x27;t really blame them though, the kind of experiments needed to really measure these kinds of things would never past muster with an ethics board. reply bazoom42 9 hours agorootparentIt is difficult to measure productivity, but what would be unethical about it? reply CoastalCoder 9 hours agorootparent> what would be unethical about it?Introducing Jira to the experiment. reply butterlesstoast 11 hours agoparentprevYou could say the opposite is true as well. How do you know after hours workers are not rating their productivity higher than actual? How do you know that the 9-5 workers aren’t rating their productivity as lower than usual?All data in of itself is useless. A sample pool of 10,000 volunteers is pretty good in the realm of statistics. reply kortilla 9 hours agorootparent> A sample pool of 10,000 volunteers is pretty good in the realm of statistics.No it’s not. No sample size can make up for other biases. reply mym1990 10 hours agorootparentprev“All data in of itself is useless”…what does that even mean? That’s like saying a chair in of itself is useless because no one is sitting in it. And this could be extrapolated to saying everything is useless.Data’s usefulness stems first from its many defining factors, which validates it, and then opens doors to using that data to explore insights. reply richk449 8 hours agorootparent> And this could be extrapolated to saying everything is useless.Now we are getting somewhere. reply halfcat 10 hours agorootparentprevIt’s the nature of the study that parent comment is referring to.If a study is self-reporting, it’s an observational study which can only establish correlation, meaning the study can only say, “there might be something here that warrants a further research”.A clinical trial is needed to establish causation.So while 10k sample size reduces the error bars, it only increases the confidence that there might be something here worth doing a more rigorous study later. reply bofaGuy 8 hours agoprev> “Employees who log off at the end of the workday register 20% higher productivity scores than those who feel obligated to work after hours.”Maybe people who are less productive end up having to work more hours. reply paulddraper 7 hours agoparentThis reminds of the Dilbert about marriage + happiness:https:&#x2F;&#x2F;nonconformist1.files.wordpress.com&#x2F;2012&#x2F;07&#x2F;dilbert_r... reply zmmmmm 5 hours agoparentprevI agree, I think it&#x27;s mixing up cause and effect.Especially how it is phrased with \"feel obligated\", it&#x27;s clear these are people who are conscientious about their productivity and most likely set a higher bar for their output. It&#x27;s hard to think their out of hours work is causing their loss of productivity, rather their work environment is causing it and only the people who feel bad about it (ie: would report that their productivity wasn&#x27;t adequate) actually work longer hours to compensate. reply lll-o-lll 2 hours agoparentprevOr equally. Maybe people who are well organised are more productive. Which, personally, I think is self evident. reply lexandstuff 10 hours agoprevFor those that only read the headline, this is an important caveat:\"On the flip side, employees who work outside of standard hours by choice, to better suit their schedule or to pursue personal ambitions, report no negative impacts and even a slight uptick in their wellness and productivity scores.\" reply LesZedCB 2 hours agoparentI read this as \"who prefers to work non-standard workday\" e.g. 11-7 which I used to do to avoid traffic. I really enjoyed it and the slower morning was really helpful for me. early morning stand-ups post-pandemic really killed my morning happiness. reply gsuuon 5 hours agoparentprevI wonder if that means pursuing work-related personal ambitions (i.e. promotion), or outside-work personal ambitions (i.e. side projects)? Could side projects actually reduce burnout? reply groby_b 3 hours agorootparentYes. If you want to pursue them, but not if you add a side project just to battle your burnout.A core part of burnout is lack of control, and a side project can give you that feeling back - if you do it for the joy of it. reply MattGaiser 8 hours agoparentprevI personally do this a great deal. So many things that are not coding in life (from visiting museums to getting lunch with a friend to going to a government office to getting medical tests done) are far easier during normal work hours.Coding can be done at midnight, when none of those things can be done. reply tomcam 8 hours agorootparentI&#x27;m pretty much retired but I have zero sense of holidays. I always worked much more efficiently when everyone left the office, and 3 or 4 day weekends were always pure heaven. (I liked my work though.) reply dijit 7 hours agoprevBefore I thought I had ADHD I would sit around the office not doing much of anything in a sort of \"ready\" state, ready to jump on the next thing someone would try to distract me with or \"ready\" for the next meeting that is sometimes more than an hour away.It&#x27;s only after everyone else went home and I felt relaxed that I finally got any of my actual work done. :( reply cheema33 3 hours agoparentSame for me. I struggle to get much done during the day. Way too many distractions. I become quite productive around 4pm and onwards. And yes, I have been officially diagnosed with ADHD. I sometimes do take low-dose meds. And they appear to help. reply ianmcgowan 4 hours agoparentprevThat sounds like my self-diagnosed ADHD - I have exactly the same thing. What changed your mind about having ADHD? reply goalonetwo 4 hours agorootparentThat scenario speaks to me as well but it seems to be more an issue with modern society than anything else.Isn&#x27;t this \"ADHD\" everyone that has a hard time getting things done because we expect constant interruption and we got used to it? reply cromulent 1 hour agorootparent> Isn&#x27;t this \"ADHD\" everyone that has a hard time getting things done because we expect constant interruption and we got used to it?I would have agreed a couple of years ago but since then I have become considerably enlightened about neurodiversity and how differently brains can work. reply kortilla 11 hours agoprevHow do they control for the fact that people who are falling behind frequently work more after hours?Is it that after hours works causes loss of productivity or is that slow people work after hours to try to make up for falling behind? reply matrix87 7 hours agoparent> people who are falling behind frequently work more after hours?From my experience it&#x27;s the opposite, the people who are falling behind aren&#x27;t the ones working irregular hours reply balderdash 10 hours agoparentprev100% I feel like a lot of the time after hours work is driving by lack of productivity &#x2F; prioritization &#x2F; understanding “good enough” reply caminante 9 hours agorootparentI smile out of respect on calls when a subject matter lead saysa. they&#x27;ve gotten as far as they can given incomplete inputs&#x2F;scopeb. and they&#x27;re not going to prioritize further meetings until the lead provides what they needNot being a \"team player\" in a fuzzy sense is actually \"helping the team\" prioritize efficiently! reply ImaCake 10 hours agoparentprevThey do mention it in passing, but there is no way to statistically control for that without a randomised controlled trial (or Mendelian Randomisation).There is no reason to think that working outside hours is the cause of the problem. It could be a consequence of an individual struggling outside of work that then leads to having to try to catch up. reply __MatrixMan__ 6 hours agoparentprevI&#x27;ll give them the benefit of the doubt and say that the marketers who wrote this have probably heard of a control variable at some point in their life. reply halfcat 10 hours agoparentprev> How do they control for…They don’t. It’s an observational, self-reporting study, which can only establish correlation, and not causation. reply greenyoda 11 hours agoprevBig discussion from a couple of days ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38579890 reply moaf 10 hours agoprevThe key point here is feeling obligated to work outside regular hours. My most productive time is early in the morning or late at night. There’s just less happening at that time, both professionally and personally. There are fewer interruptions and less need to context switch, making it easier to focus and get things done.Knowing that I’m more creative and productive outside normal business hours, I try to take brakes during the day because I know I’ll make up the time later. I’d much rather get out of the house and take an hour walk when the sun is up and work at night. reply hasoleju 3 hours agoprevI have been there. I was working long hours because work was exciting. And also I wanted to show my boss that I put in a lot of effort. I voluntarily spent a lot of time in the office to finish tasks more early than expected. Sometimes this extra time was completely unproductive and the things I did turned out to be irrelevant a few days later. Sometimes it was very important for the business that things got finished and the extra time was therefor productive. I learned for myself, that I cannot sustain working long hours regularly without loosing interest in the job.Right now my mantra is: Keep showing up.For me this means get work done on a regular basis. My focus is to work on this topic for years without loosing interest, instead of prioritizing to finish the current workload as fast as possible.Real productivity is achieved when you are an expert in the domain and you have built a network in the company or market segment. Expertise and personal network are the main levers to get thins done. reply dutchCourage 1 hour agoparentI noticed this as well. When I was freelancing I would sometimes get inspired and motivated and work til 10pm or later, not because of a deadline but because I felt like it.Each time, I noticed the next day was rather slow. I was more tired and less inspired.Forcing myself to stop after 6pm worked wonders, I would wake up the next day easily, my brain overflowing with new ideas. On top of that I&#x27;d manage to sustain that motivation for several days or even weeks in a row. reply jvans 10 hours agoprevLess is more work posts are so trendy now. The comments invariably point out unexplored confounding variables and the posts lack sufficient substance to be interesting. At the end of the day you can reason that hours != productivity, which everyone knows by now reply gnabgib 10 hours agoprevPosted yesterday:[0] (190pts, 128 comments). Slack either changed the URL (fooling the dupe detector?), or mods made yesterday&#x27;s title much less clickbaity.[0]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38579890 reply seeknotfind 10 hours agoprevFor me, I&#x27;m most productive after hours because during the day, I get too many pings and escalations. After hours is the only time I can focus and get deep work done myself. This is something I&#x27;d really like to improve, but I&#x27;m really not sure how. I&#x27;ve tried creating discussion groups and many other approaches to make knowledge transfer more efficient. However, the real issue is company growth. A huge recent period of growth has meant a focus on training people. This is only the kind of thing I could defend against if I were 1-2 ranks higher in the company, and honestly, this lack of longterm growth planning is one of the reasons I would leave my company.One approach I&#x27;ve been thinking of experimenting with is saving everything I say, and using an LLM with my custom knowledge base to answer a question. I actually think a borg-like communication structure in a company could be really efficient. If every communication was publicly available, and we used ML to make that information consistent and to answer queries, I think you could achieve coherence with a company much faster, move as a unit much faster, and get a lot more done.However, it&#x27;s still a pipe dream at this point due to the context length limits. The actual amount of important context at a company is many times what a model can keep in mind now. As models get bigger, or if a group can run multiple models many times to make the company info widely consistent, it would be much better.For instance imagine now, 30 people at your company work on one project, and 50 people work on another. They might take a slightly different definition for a term, and do work which is ultimately counterproductive to each other. Usually, it takes a conflict arising to resolve this, and people are long stuck with the mutually-incorrect definition. If an ML algorithm can look at everything both groups are saying and point out inefficiencies, I think you could get more cohesion, especially before problems get big enough to be a drain.That being said, this type of system is more of a pipe dream, and I don&#x27;t think it will be feasible for a few more years. In the meantime, I&#x27;m struggling how to balance communication with work. Advice appreciated. reply twodave 10 hours agoparentI ran into this problem after a promotion and some company growth left me over a much larger footprint of our engineering department than I was used to. I ended up making a couple rules for myself and establishing a couple new routines, which all have helped:“Handbooking” - this is something I learned from reading about Gitlab and ended up adopting for my department. If it’s important, it goes into the handbook. If it’s not, then I probably don’t need to know it anyway. This alone took a lot of burden off me (people only ping me now if something is missing from the handbook or if they didn’t understand it, both of which are signals that the handbook needs work).Office hours - I don’t do this consistently, but when I feel like the interruptions are still too frequent I’ll just post some office ours in our dev channel and tell people they can bother me in a few 2-hour blocks that week. Otherwise I sit there and livestream whatever work it is that I’m doing for whomever is interested in that sort of thing.Trusting - I found much of my increased stress was a lack of faith in others to do a passable job of things, so I’d burden myself with them. In a lot of cases things went as badly as I expected by letting go, but my personal stress was reduced nonetheless. reply seeknotfind 8 hours agorootparentThanks for the tips. I tried live streaming work a few times. I think it can work as long as people don&#x27;t ask questions too much, but I feel a bit too egotistical to do it. It doesn&#x27;t feel great. reply twodave 7 hours agorootparentYeah, I feel that. Usually I have like a 2 hour coding block planned with something new&#x2F;interesting, so it would end up being more of a workshop, like e.g. here’s how to build a really quick cron job for our k8s cluster or whatever. I tend to keep an eye out for little ~2 hour tasks that would be interesting and just save them for those blocks.Aside from that, ego is a lot more about attitude than action. If you remain inquisitive&#x2F;curious&#x2F;accepting&#x2F;etc. then you won’t be seen as cocky.Also, just a reminder if you’re in an advanced position then showing off your coding skills is less about ego and more about helping. I had a boss early on point out to me (much to my surprise) that the programmers sitting around me were drowning trying to put together even simple solutions to their problems, and I had the ability to save them hours with just a few minutes of attention. Obviously you can only take that so far, but it gave me a lot of confidence to know I could help folks out. reply twhitmore 4 hours agoparentprevYes, there&#x27;s this amazing thing called \"documentation\".Confluence works well if you KISS - write just what the audience needs to know, in terms they can read & find.Avoid writing grand tracts of wanna-be architecture, policy, theory or planning; just write useful actionable material on topics your audience needs. You will find this works.Source: I led 400 devs and this was the only way that could scale.http:&#x2F;&#x2F;literatejava.com&#x2F;documentation&#x2F;how-to-write-good-wiki... reply killjoywashere 8 hours agoprevI saw a sign in a cubicle farm one time: \"In every group, someone&#x27;s the weakest. If you don&#x27;t know who that is, it&#x27;s you.\" Fuck that. reply goodpoint 2 hours agoparentThe person who put up that sign is desperate for reassurance. reply aussieguy1234 6 hours agoprev\"employees who work outside of standard hours by choice, to better suit their schedule or to pursue personal ambitions, report no negative impacts and even a slight uptick in their wellness and productivity scores.\"So, if, like alot of Hacker News readers, you&#x27;re hacking away on your next startup idea after hours, you should be fine. reply what 9 hours agoprevDupe from yesterday (128 comments): https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38579890 reply lolive 7 hours agoprev47 here. And 20 years of dev experience. I am now hired by a big company. As far as I understand, it is to become a “manager” (==a politician). So now I spend my days having pointless meetings with bozos.Because of the poor management, we have had reorganizations and budget cuts all along the year. Making proper work impossible.So now, I simply code what we need, after hours. This is exhausting, as it is double days. But extremely valuable in the end. reply robertlagrant 21 minutes agoparentIt&#x27;s good that you have ownership of the problem, but make sure you get something out of it. If someone else will get credit for your hard work, you will experience explosive demotivation. reply deviantbit 10 hours agoprevInteresting. I found I wrote more, and better code once everyone left me alone. reply darth_avocado 10 hours agoprevThe only people surprised would be the “MBA class” of management. Anyone who has ever worked any job would gladly tell you how productivity decreases if you work after hours. It’s not a revolutionary idea. reply 0xbadcafebee 7 hours agoprevThe 8 hour work day: created for physical laborers, extended to paper-pushers, and forced onto creative workers along with productivity-sapping random meetings.The current white collar worker paradigm is proof that capitalism is inherently inefficient. The theory that a business will naturally optimize for efficiency does not survive a meeting with human culture, which has nothing at all to do with organizational efficiency, and everything to do with repetition, imitation, resistance to change, and the easiest short-term path. If you want to get more productivity out of a human, you need to do more than just tell them what time to arrive and go home. reply robertlagrant 19 minutes agoparent> The current white collar worker paradigm is proof that capitalism is inherently inefficientOf course it&#x27;s inefficient. If you don&#x27;t have a dictator saying what to do, then people deciding between themselves to try stuff won&#x27;t always get it right. But it seems to work better than everything else in the long term.> If you want to get more productivity out of a human, you need to do more than just tell them what time to arrive and go home.No one just does this, and lots of companies don&#x27;t even do it. Working times are mostly so that there are work overlap times so people can communicate and not be blocked on things. reply pk-protect-ai 2 hours agoprevWow, slaves perform worse when tired! What a discovery... Fck any employer with required \"off hours\" and wage formulas for compensation. Actually ... fck off hours. reply fma 9 hours agoprevPeople who feel obligated to work after work hours also are probably in a poor work culture company.I log off when I went to log off and if I have an idea or a reason to work at night I do it. If I feel it&#x27;s not productive I sleep early and do it in the morning.Those who feel obligated could also be doing it to look good to their boss. reply intrepidsoldier 3 hours agoprevHave they considered how Slack contributes to decreased productivity? reply teaearlgraycold 5 hours agoprevMy strategy:* Work for a small company* Show up at 10am* Leave at 4pmIf I&#x27;m really into what I&#x27;m doing I&#x27;ll happily do some work after hours, or on the weekend. No one ever expects it and honestly I try to keep the fact that I&#x27;m doing it a secret (don&#x27;t push commits, just keep changes local). reply nikau 9 hours agoprevSeems this is very cultural, eg in Japan there is an expectation that you must always work back late, so people don&#x27;t give 100% for 8 hrs as they know regardless of performance they will need to put in more hours of overtime. reply simplypeter 11 hours agoprevThere is no way that climbing the stairs to the 10th floor will be as fast as climbing to the 1st or 8th floor, let alone the 10th floor. The same applies to working hours. There&#x27;s no way you&#x27;ll be as productive in the 10th hour as you were in your first hour. reply FredPret 11 hours agoparentBrains aren’t like quadriceps. I get more productive as I work longer and build momentum. More and more concepts are loaded in memory, so to speak.To me, it’s more like speeding up until I hit the wall on the tenth floor. reply balderdash 11 hours agorootparentI’m of two minds on this a. It depends if we are building or thinking - building definitely benefits from more time, and feel like I get momentum (as you described), implementing something over 12+hr sprint is hugely productive for me. On the other hand if I have a problem to solve I’m completely unproductive trying to do it at 3am. reply LesZedCB 2 hours agorootparentprevsure but cache warming works on the order of 30 minutes reply fleischhauf 11 hours agoparentprevI don&#x27;t fully get the analogy. 10th floor is higher than 1st or 8th, so then you would get more work done? You can certainly climb 10 floors with the same speed as 1 or 8.. reply nikau 9 hours agorootparentReplace 10 with the highest floor your muscles can go to before they are exhausted reply osigurdson 6 hours agoprevWe all know that working your ass off is the way to get things done. The problem is, in most companies, no one can tell the difference between hard working people and folks who take it easy. If a project takes 6 months it must have been hard if it takes 6 weeks on the other hand it must have been easy. No one is going to run the experiment twice. reply makeitdouble 8 hours agoprevThe subtitle of this piece:> Slack’s Workforce Index uncovers new findings on how to structure the workday to maximize employee productivity, well-being and satisfactionThe base assumption of the employer structuring and managing the employee&#x27;s well-being and satisfaction feels so 19th century. I know it&#x27;s still the prevalent paradigm in HR in most places, but it&#x27;s crazy how hard that idea persists. reply kazinator 9 hours agoprevThose who feel on top of their work don&#x27;t feel they have to catch up on work after hours?Where is the surprise? reply tglobs 7 hours agoprevAnyone have a link to the wordings of the original questions asked? reply guhcampos 10 hours agoprevHow is that still surprising? reply danaris 10 hours agoparentBecause far too many people still think that more hours worked directly and linearly translates to more productivity, with no upper bound. reply PedroBatista 11 hours agoprevSurprising huh?If the wording and subjects presented in the \"article\" sound a bit like Bill Lumbergh with a polo shirt was the author, don&#x27;t forget Slack is Salesforce now. reply goodpoint 2 hours agoprev\"surprising\"? It&#x27;s very well known and nowadays well proven by neuroscience. reply stochastimus 6 hours agoprev> working less is good > blog by “Slack”True or false, the jokes write themselves sometimes reply Eumenes 7 hours agoprevI have alot of colleagues that avoid their actual work by taking on pet projects or some cross functional nonsense. I have a colleague that takes calls with vendors almost weekly, knowing there&#x27;s no budget for it. Someone who spends hours with HR or recruiting on some DEI stuff. Someone who wants to be a professional conference speaker. Almost anything to avoid coding. Meanwhile, those of us that simply log on to do work have to pick up their slack. Management doesn&#x27;t care. I understand this isn&#x27;t the case everywhere but happiness equates to doing less for some. I don&#x27;t engage in those projects and I&#x27;m very productive. reply hackerlight 8 hours agoprev> Employees who feel obligated to work after-hours register 20% lower productivity scoresVery notably, this excludes people who want to work after-hours. A rare person but they do exist in some fields. reply 7e 8 hours agoprevSimplest explanation: the lowest performing employees feel the need to work after hours to keep up. Few people start out working more hours than necessary. reply akomtu 10 hours agoprev [–] High performers don&#x27;t need to put in extra hours, in other words. Big surprise. reply ascotan 9 hours agoparent [–] I don&#x27;t think this is a reflection on performance. seems like the key word here is \"obligated\" to work after hours. this is due to managers pushing deadlines on their reports. honestly this isn&#x27;t much of a surprise. If your boss is obligating you to work after hours you are unhappy. one issue here asking engineers to work after hours in a &#x27;you build it you own it&#x27; culture. in my mind this qualifies for being \"obligated\" to work outside of business hours.A more interesting item in the slide deck is that anyone that has more than 2 hours of meetings in a day has a loss of focus. I think it&#x27;s conceivable to say that a heavy meeting culture may also force people to be \"obligated\" to work after hours because they have lost focus time in the day. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Employees who log off at the end of the workday are 20% more productive than those who continue working after hours, according to a survey conducted by Slack's Workforce Index.",
      "Taking breaks during the workday is crucial for productivity and well-being, yet half of desk workers rarely or never take breaks.",
      "The study suggests that around four hours of focused work time is ideal, and spending more than two hours in meetings is considered excessive. Additionally, the late afternoon (3-6pm) is the least productive time for work."
    ],
    "commentSummary": [
      "The article examines the connection between after-hours work and productivity, emphasizing the importance of timely and reliable results over the number of hours worked.",
      "It highlights the negative effects of working late on health and well-being, using Amazon's culture and its unrealistic after-hours work expectations as an example.",
      "The debate also explores the impact of fatigue on productivity and discusses various programs for creating resumes and CVs. Overall, it delves into the complexities of productivity and the importance of work-life balance."
    ],
    "points": 225,
    "commentCount": 124,
    "retryCount": 0,
    "time": 1702243796
  },
  {
    "id": 38596634,
    "title": "John Carmack and John Romero Reflect on Doom's 30th Anniversary",
    "originLink": "https://www.pcgamer.com/for-dooms-30th-anniversary-the-johns-romero-and-carmack-reunited-to-celebrate-the-fps-that-changed-everything-i-want-to-thank-everybody-in-the-doom-community-for-keeping-this-game-alive/",
    "originBody": "News FPS Doom For Doom's 30th anniversary, the Johns Romero and Carmack reunited to celebrate the FPS that changed everything: 'I want to thank everybody in the Doom community for keeping this game alive' By Ted Litchfield published 10 December 2023 The two reminisced about the shooter's development and how it differed from id's games up to that point. COMMENTS (Image credit: John Romero (Twitter)) To celebrate the 30th anniversary of the launch of Doom, id Software co-founders John Carmack and John Romero reunited to talk about the legendary FPS. The discussion was moderated by David Craddock (The FPS Documentary, Long Live Mortal Kombat), with interview questions from Craddock and the Twitch chat. The conversation was understandably warm and celebratory, but I was also surprised at how critical the two were of their own work. Carmack alluded to \"flashier\" (and potentially technically riskier) graphical effects he wishes he had built into Doom's engine, and he noted that he thinks the more grounded, military sci-fi aesthetic of Episode One has aged better than the abstract hellscapes later in the game. Romero, meanwhile, contrasted Doom with the id games before and after, arguing it represented a technical \"sweet spot\" before Quake and full 3D acceleration started to seriously complicate development and limit how many enemies they could fit on screen. The developer praised Doom's engine for allowing more complex maps than Wolfenstein though, ruefully remarking that \"Making levels for Wolfenstein had to be the most boring level design job ever.\" The two also fondly reminisced about the technical limitations of the time. Carmack remarked that, although he thought id could \"just sell [Doom] in a brown paper bag\" off its quality alone, he was glad they went the extra mile with its iconic box art and marketing. Both devs expressed an appreciation for '90s PC big box packaging and accompanying \"feelies\" like cloth maps, and I'm 100% with them on that. As far as development stories, I was struck by Romero's recollection of getting multiplayer working for the first time shortly before Doom's release: \"I went into my office—I was making E1M7 at the time—I'm looking out the window and I'm seeing two characters fighting, rockets are flying up at a high window and someone is plasma gunning the other guy. And I'm like, this is going to be the coolest fucking game the planet has ever seen, I can't wait to play that.\" \"I've said before that I'm not a very sentimental person, that I don't spend a lot of time reminiscing about the good old days,\" Carmack confided as a means of farewell, \"But they were really quite good. I'm very proud of the things that we built back then and that they have this legacy that's lasted to this day.\" Romero echoed the sentiment, thanking Carmack for the years they spent working together, and also extending his appreciation to the players who keep coming back to Doom: \"I want to thank everybody in the Doom community for keeping this game alive. And really, just thank you for playing our games everybody.\" You can check out the conversation in its entirety on John Romero's Twitch channel, and it's also a perfect time to dive into Sigil 2, the sequel to Romero's 2019 Doom megawad and subject of PC Gamer's latest print cover story. While you can pay for a full-on classico big box with all those feelies we love, both Sigil megawads are free to download. If that's not enough WAD action for you, the megawad Eviternity also just got a sequel campaign to celebrate Doom's 30th birthday, and you can peruse the list of this year's Cacowards for more quality creations. PC Gamer Newsletter Sign up to get the best content of the week, and great gaming deals, as picked by the editors. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Ted Litchfield Associate Editor Ted has been thinking about PC games and bothering anyone who would listen with his thoughts on them ever since he booted up his sister's copy of Neverwinter Nights on the family computer. He is obsessed with all things CRPG and CRPG-adjacent, but has also covered esports, modding, and rare game collecting. When he's not playing or writing about games, you can find Ted lifting weights on his back porch. MORE NEWS People are exceptionally thirsty about this free game where you flirt with the grim reaper on text chat Superb survival city-builder Against the Storm is finally finished and released LATEST Train people are dang pleased with this line operations sim because it's not too hardcore, nor too simple SEE MORE LATEST ► See comments MOST POPULAR That game where you farm with your mech finally has a release date By Jonathan BoldingDecember 10, 2023 There's a change buried in Baldur's Gate 3's latest patch that makes it easier than ever to recruit both of its mutually exclusive companions⁠—without breaking the game By Ted LitchfieldDecember 09, 2023 Dave the Diver is teaming up with our other favorite nautical adventure of 2023, Dredge, for a spooky free DLC By Ted LitchfieldDecember 09, 2023 Wordle today: Hint and answer #903 for Saturday, December 9 By Kerry BrunskillDecember 09, 2023 The Finals is getting blasted on Steam for slowing the game down, except the developer says it didn't change speeds at all By Morgan ParkDecember 09, 2023 Suicide Squad: Kill the Justice League is getting an offline mode after all, but not until sometime after it's out By Andy ChalkDecember 08, 2023 Space Marine 2 gets a new release date that's only 276 days away By Andy ChalkDecember 08, 2023 Diablo 4 will get a hotfix today to sort out a major issue: its hardest dungeon is too hard By Harvey RandallDecember 08, 2023 Creator behind hugely popular Skyrim co-op mod gives up on the Starfield version of it because, drum roll please, 'this game is f***ing trash' By Rich StantonDecember 08, 2023 Developers blast the celeb-laden Game Awards as 'an embarrassing indictment of a segment of the industry desperate for validation… with little respect for the devs' By Joshua WolensDecember 08, 2023 Cyberpunk 2077 comes full circle with Sad Keanu meme By Rich StantonDecember 08, 2023 LOAD COMMENTS",
    "commentLink": "https://news.ycombinator.com/item?id=38596634",
    "commentBody": "John Carmack and John Romero reunited to talk DOOM on its 30th AnniversaryHacker NewspastloginJohn Carmack and John Romero reunited to talk DOOM on its 30th Anniversary (pcgamer.com) 220 points by ChrisArchitect 8 hours ago| hidepastfavorite67 comments tcmb 2 hours agoIt&#x27;s also worthwhile to remember where the team honed their craft, as was also mentioned in this session: The early id team worked for a publisher called Softdisk that provided a game subscription where customers received a new game every month. This was basically a way to iterate on the practice and process of game development in one month cycles. The shareware relase of Doom had four months of development, which sounds crazy short by today&#x27;s standards, but for them it was unusual to have so much time.There&#x27;s several talks on Youtube by John Romero where he tells the story of the early id software, e.g. https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=E2MIpi8pIvY reply tigger0jk 1 hour agoparentSokpop is running a similar approach on Patreon. They have 4 devs and have released over 100 games since 2018 https:&#x2F;&#x2F;www.patreon.com&#x2F;sokpop&#x2F;about reply milchek 7 hours agoprevFor those interested in learning more about DOOM, the book \"Masters of Doom\" is an entertaining recollection of how id Software started and how John Romero and John Carmack started their careers.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Masters_of_Doom reply roland35 7 hours agoparentGame Engine Black Book: Doom is also a fantastic read!https:&#x2F;&#x2F;fabiensanglard.net&#x2F;gebbdoom&#x2F; reply globuous 7 hours agorootparentLooks very interesting, thanks for sharing! The pricing section is very informative btw, congrats to the author for sharing it reply soulofmischief 6 hours agorootparentFabien is a Hacker News darling, I&#x27;d highly recommend both the DOOM black book and Wolfenstein black book, I have them and love every page. Give him your support!Masters of Doom is also required reading for any hacker. reply stathibus 5 hours agoparentprevIf you&#x27;re interested in this topic, there is probably no better source now than the recent John Romero autobiography Doom Guy reply nomilk 7 hours agoprevWhat a treat it was listening to two legends talk with great passion about software they made 30 years ago.I absolutely loved their answer to a question regarding better enemy AI, which boiled down to “better enemy AI doesn’t automatically make for a more fun game”. (they put fun and player experience first and aren’t tricked by shiny new toys)Was also interesting hearing Carmack lament how so few much loved games release their source code (he attributed doom’s longevity partly to the release of its source code) reply noirscape 1 hour agoparentThe original F.E.A.R. always stuck out to me for its AI. Not because the actual AI is very smart - most enemies still are dumb lugs that try to get the player and have basic \"is seen, rush down\" AI, but because the player gets a lot of feedback for their actions.The main way to trick a player into thinking an AI is smarter than it actually is? Have their voicelines \"cheat\" a bit and respond very vocally to things the player does near them; even if the enemy can&#x27;t see the player, the idea of \"oh shit, they heard that\" can really enhance the seeming intelligence of the enemies.Given that F.E.A.R. is a horror themed shooter, that&#x27;s a big boon. reply Cthulhu_ 40 minutes agorootparentIt&#x27;s a testament to the game that its AI is still this memorable. Have there been any advances in that area since then? I&#x27;m not at home in the singleplayer FPS genre anymore, how do the SP campaigns of the newer Halo or CoD games hold up? reply badsectoracula 5 hours agoparentprev> Was also interesting hearing Carmack lament how so few much loved games release their source code (he attributed doom’s longevity partly to the release of its source code)I blame middleware. Hell, middleware makes even releasing the mod tools harder, i remember when i worked at some gamedev company and we wanted to release the editor for our game and the company behind some middleware we used wanted us to buy a separate license for it. reply jamesfinlayson 4 hours agorootparent> I blame middlewareI think John Carmack does too - from memory Doom&#x27;s source release had no sound code because it was done externally. reply jamesfinlayson 4 hours agoparentprev> Was also interesting hearing Carmack lament how so few much loved games release their source codeCouldn&#x27;t agree more - it&#x27;s sad that id is&#x2F;was one of the exceptions.I&#x27;m still waiting for the GoldSource engine to be open sourced... reply Cthulhu_ 38 minutes agorootparentWasn&#x27;t GoldSource based on the Quake engine? I&#x27;m reading that it&#x27;s heavily modified, mainly in terms of enemy AI. reply rvba 7 hours agoparentprevMonster infighting added so much to the game and was relatively \"cheap\" to program.What other games even use it apart from Doom, Quake and Half Life? (HL 1 had some impressive AI) reply wsc981 10 minutes agorootparentMarathon had monster infighting.Marathon is a game series from Bungie, the first version released shortly after Doom, but only on Macintosh.There are defense drones [0] that aid the player and later on enslaved cybernetic aliens [1] that revolt against their alien overlords.As in Doom, if an enemy would accidentally hit another enemy, they would fight one another.The Marathon series can be played on modern computers using Aleph One [2]---[0]: https:&#x2F;&#x2F;marathongame.fandom.com&#x2F;wiki&#x2F;Marathon_Automated_Defe...[1]: https:&#x2F;&#x2F;marathongame.fandom.com&#x2F;wiki&#x2F;S%27pht[2]: https:&#x2F;&#x2F;alephone.lhowon.org&#x2F; reply nomilk 7 hours agorootparentprevThey gave a couple examples of the kind of AIs that don’t make games fun. One was some game (real or hypothetical) where you fight a small drone that’s very smart and fast, so it’s hard to shoot at. Another was where the enemy goes off and concocts a sophisticated plan to beat you.They described how the player wants to feel like the game play is about themselves, not about some ultra smart enemy. So big and dumb enemies tend to work best, even though they don’t use sophisticated AI by today’s standards.It was like the the Jurassic Park meme: just because we can doesn’t mean we should.(Also very strong agree on monster infighting, it added so much depth, as a kid I didn’t even know monsters could infight until the level with cyberdemon and arachnotron in the same room) reply Macha 6 hours agorootparentConsider an FPS AI with perfect tracking and aim, or a starcraft AI with perfect blink stalker micro. It&#x27;s a smart play, but not fun for the player reply bombcar 6 hours agorootparentOr the Deus Ex “ai” which people praised as feeling very intelligent- but was just programmed scripted sequences based on what the player most likely would do. reply atoav 4 hours agorootparentSo a state machine? My unchecked suspicion would be that 99% of all enemy AI in gaming history are state machines of some sort. reply flohofwoe 1 hour agorootparentYeah, in the end it&#x27;s all if-this-happens-do-that under the hood. And that&#x27;s important because game AI must be deterministic. Otherwise reproducing bugs would be impossible. reply Cthulhu_ 38 minutes agorootparentprevAren&#x27;t most elements in video games, including the player characters, state machines? reply sho_hn 5 hours agorootparentprevDo you have a source? I had never heard this. reply beckhamc 3 hours agorootparentprevsophisticated AI != aimbot accuracy reply Cthulhu_ 36 minutes agorootparentprevMonster Hunter has it, it&#x27;s pretty epic seeing massive creatures fight each other and you, a tiny human with an oversized weapon, running for your life and &#x2F; or awaiting your opportunity to kill them and use their corpses to make new weapons&#x2F;armor. reply Al-Khwarizmi 6 hours agorootparentprevOff the top of my head, several roguelikes (e.g. ADOM, Caves of Qud, ToME) and several entries in the TES series (e.g. Daggerfall, Skyrim) support it.I also remember at some point playing an RPG where you found an ongoing even fight between two armies and you could join one to make the fight go your way, but I don&#x27;t remember what it was. reply basilgohar 5 hours agorootparentSounds like Skyrim and the fight between the Empire and the Stormcloaks. reply pseudocomposer 4 hours agorootparentprevThe original Halo trilogy, Breath of the Wild&#x2F;Tears of the Kingdom, The Last of Us, and Horizon: Zero Dawn, to name a few. A great mechanic! reply a_bonobo 3 hours agorootparentprevThe original Turok on the N64 had this too :) It just was very hard to trigger for obvious reasons, as humans and dinosaurs spawned far apart reply Macha 6 hours agorootparentprevMinecraft. Triggering it so skeletons kill creepers was the primary way to get music discs reply fipar 6 hours agorootparentprevIn Tears of the kingdom you can shoot muddle buds to monsters and they’ll start fighting each other. reply HideousKojima 7 hours agorootparentprevHalo, you can let the Covenant and Flood fight each other and clean up whoever is left. reply spiritplumber 7 hours agoprevDoom helped me out of a crippling fear of hell&#x2F;damnation that was beat into me by a religious upbringing and I will forever be grateful. reply soulofmischief 6 hours agoparentThat&#x27;s beautiful. I&#x27;m deeply sorry you had to go through that, from someone raised by a deacon. Hope everything is going OK. reply oobuffet 1 hour agoparentprevRip and tear reply intrasight 6 hours agoprevI remember when, in 1993, my boss bought a $6000 Windows machine at work - to play Doom.Edit: He had already mentally checked out of this job and was transitioning (already working at) a different company. As a comp-sci computer graphics specialist, he dug Doom at multiple levels. Me - I was not impressed by Doom - probably because I didn&#x27;t have his background to understand what a feat of engineering it was to get such performance out of a 1992-era CPU. reply pizza234 48 minutes agoparentIt wasn&#x27;t just the engineering. The immersion was unprecedented in videogames, which was supported, besides the obvious rendering improvements, by lighting and sound.Experiencing for example, lights suddenly turning off and monsters appearing and making noises behind the player, was very intense and novel.(There were also other revolutionary aspects in Doom, notably, multiplayer) reply jonhohle 5 hours agoparentprevIf you played other games at the time, there wasn’t really anything else like it. Wolfenstein and similar existed, but Doom was a significant advancement. Us lowly console players were many years from anything similar. reply pizza234 51 minutes agorootparentMost of the Id games were significant advancements (up to Quake at least); in addition to Doom, the other major revolutions were:- Commander Keen (smooth scrolling)- Catacomb 3D&#x2F;Wolfenstein 3d (pioneered ray casting on PC)- Quake (introduced full 3d worlds) reply jasonwatkinspdx 1 hour agorootparentprevYeah, I can vividly remember when my friends and I booted up a copy of shareware Doom the first time. We&#x27;d played the heck out of Wolf3D and so on.Nothing was like Doom when it came out. It was shockingly, impossibly smooth. reply tverbeure 4 hours agorootparentprevPlaying the limited shareware release of Doom the very first time in single player mode was already incredible, but it was the multi-player mode that was mind blowing.It would be fun to see a graph with sales stats of NE2000 clones around that time. They were the cheapest Ethernet expansions boards at the time, and one of the few that Doom supported. reply zengid 3 hours agoprevSeeing the picture of them holding swords when they were young is blowing my mind. I only picture Carmack as an old sage, not as a young ranger. reply rconti 3 hours agoprevI see John Romero uses the same logitech webcam I do that makes me channel AvE every time I have to yell \"focus, you fuck!\" reply italophil 6 hours agoprevHalf-Life just had its 25th anniversary. It’s crazy to think there were only 5 years between those two titles. reply soulofmischief 6 hours agoparentWhen I think about that period in gaming I just trip out.We are as far away from that period now as that period was from the birth of video games. Tennis for Two, Space Wars, Pong, Super Mario, DOOM, Quake, Half-Life.Compare that 20-year evolution to the evolution of gaming in the last 20 years. You can see areas where games have gotten vastly better, not just graphically or cinematically, but with level design, mission design, characterization, contextual integration, procedural elements.But still nowhere near the explosion of the first 20 years as innovation poured in, and all of the main players were scrappy young companies, ready to innovate in an atmosphere where it still meant something to create entirely novel experiences that challenged the player.You can also very easily see an industry-wide shift in game design after both GTA and Minecraft, which seemed so far apart at the time but really were about a decade apart. What comes next? reply Daishiman 4 hours agorootparentAs a teenager during that time it feels easy to say that we idealize those periods of youth, but objectively speaking there was never a period in gaming history with such drastic speed of innovation between the late 90s and early 200s. reply jamesfinlayson 3 hours agorootparentAgreed! Not as stark a contrast as Doom to Half-Life, but even Quake (1996) to Quake III Arena (1999) was an insane leap technologically. reply Cthulhu_ 30 minutes agorootparentFor sure; if you look at 15, 20 year old games now vs today&#x27;s, you can see it&#x27;s more evolution than revolution, in terms of gameplay mechanics, graphics, scale, etc.Not that it&#x27;s not impressive; look at the PS5 tech demos (large scale areas that players can go through fast without loading times thanks to fast storage), or the Unreal Engine demos, e.g. the Matrix one you can download to the PS5 or the one where they highlight generative level creation.But it doesn&#x27;t feel as revolutionary anymore. Mind you, that may just be fatigue, or \"getting used to\" things quickly. reply Cthulhu_ 34 minutes agoparentprevIt was a wild time; as they said, Doom was made in (just) one year; while in a sense the games of back then are comparable to indie games of today, even indie games take longer to build these days.That said, it&#x27;s a work&#x2F;life balance thing too; the Doom team did pretty much a year of grinding, pizza and coffee fueled 14+ hour workdays. reply ChrisArchitect 5 hours agoprevBesides the legendary Masters of Doom book, Romero released his autobiography this year:The birth of id software – Excerpt from John Romero&#x27;s autobiographyhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36808939 reply zzanz 7 hours agoprevActual interview is a Twitch vod on John Romero&#x27;s channel: https:&#x2F;&#x2F;www.twitch.tv&#x2F;videos&#x2F;2000693432 reply ChrisArchitect 5 hours agoparentYouTube archive also: https:&#x2F;&#x2F;youtu.be&#x2F;QvAkaJsvAXs reply bombcar 6 hours agoprevYou can run DooM on anything - including DooM.https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=c6hnQ1RKhbo reply donatj 7 hours agoprevIt was fun to watch live. Had it on my calendar for a while. I would have liked it more without the moderator I suspect, just a discussion between the Johns. reply ChrisArchitect 8 hours agoprevMore Doom 30-year chat:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38594405 reply ChrisArchitect 2 hours agoparentSome more anecdotes and stuff from earlier today:Sigil II, a Doom WAD from J.Romero, has been releasedhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38593248 reply MisterBastahrd 5 hours agoprevAnyone know if anyone ever asked Carmack about how he felt about Descent? That was the first 3d game I was exposed to that ever felt like a true 3d experience. reply freitzkriesler2 7 hours agoprevI find it impressive that the doom modding community is as vast and varied as it is.I could easily keep myself entertained with doom wads for quick a long time.Is there another game that has that level of dedication? I think sim city 4 does but doom is much much older. reply Cthulhu_ 28 minutes agoparentModern-day I&#x27;d look at Minecraft or even Roblox; mind you, the latter is less modding and more building things in the tools that the game &#x2F; dev tools give you. reply paulryanrogers 6 hours agoparentprevPart of this comes from its simplicity. Maps are relatively fast to make. DoomBuilder and TrenchBroom have made more 3D maps practical. Duke3D and its engine has a smaller yet continuously active community. There are others. reply iforgotpassword 2 hours agorootparentAgree. I got into duke3d and shadow warrior mapping back in the day. The fact that these games weren&#x27;t truly 3d mandated that you designed the maps 2d first, ie drawing the layout of the map top-down, and then switching to 3d mode and assign textures to walls and floor&#x2F;ceiling, as well as adjusting their height. That forced you to approach level design in a specific way, it was easy to get into.When unreal was released I tried to create some maps for it, but was quickly overwhelmed. You&#x27;re basically thrown into blender and supposed to construct a 3d world from cubes and other simple 3d shapes. I ended up with I think one and a half somewhat decent levels, but they still felt clearly inferior to any of the official maps. reply jamesfinlayson 4 hours agoparentprev> Is there another game that has that level of dedication?Quake is more difficult to mod (map building is more complicated and you need to build models rather than sprites) but it seems to have a few active source ports. reply ChickeNES 2 hours agoparentprevSuper Mario World and Super Mario 64 arguably reply vkou 4 hours agoparentprevThief 2. There&#x27;s an incredible amount of fan mission content for it, ranging from relatively simple levels, to one-off-masterpieces, to incredibly elaborate full-game-length mission packs.Warcraft 3. It shipped with a powerful but approachable map editor, had thousands of custom maps and game modes, and spawned two separate gaming genres (Tower Defense, MOBAs).Morrowind and Skyrim. Both were solid base games to build on top of, both shipped with a powerful creation kit, and both have an incredible amount of modded content available for them.Technically, Neverwinter Nights would also qualify, but that was less of a game and more of a DnD dungeon master sandbox. reply soulofmischief 4 hours agorootparentWhat are some of your favorite Thief fan levels? reply vkou 3 hours agorootparentIf you&#x27;re looking for any unusual recommendations, I&#x27;m not the best person to ask! I&#x27;ve only tried the more popular ones, but The Seventh Crystal, Gathering at the Inn, The Inverted Manse are my favorite stand-alone missions. reply 29athrowaway 7 hours agoprev [–] id Software has an incredibly important legacy.Not only they revolutionized games, but their decision to open source their game engines has so many implications that it is hard to even begin quantifying their impact.And it&#x27;s not only the FPS games that left a mark in history, the 2D platformers were awesome too. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Co-founders John Carmack and John Romero commemorated Doom's 30th anniversary with a discussion about the legendary FPS game.",
      "They reflected on the game's development, acknowledging their own criticisms and desires for flashier graphical effects.",
      "The developers also discussed the multiplayer experience and expressed gratitude to the Doom community for their support in keeping the game alive."
    ],
    "commentSummary": [
      "The summary discusses discussions and forum threads about the iconic game DOOM and its impact on the gaming industry.",
      "It covers topics such as the development process, prioritizing fun and player experience, the release of the source code, and DOOM's influence on other games.",
      "The discussions also touch on AI in games, monster infighting, AI-controlled allies, the legacy of DOOM, its impact on the indie game industry and modding community, and technological advancements in game design."
    ],
    "points": 220,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1702257253
  },
  {
    "id": 38593638,
    "title": "Ratatui: A Lightweight Rust Library for Terminal User Interfaces",
    "originLink": "https://github.com/ratatui-org/ratatui",
    "originBody": "Table of Contents Documentation · Ratatui Website · Examples · Report a bug · Request a Feature · Send a Pull Request Ratatui Ratatui is a crate for cooking up terminal user interfaces in Rust. It is a lightweight library that provides a set of widgets and utilities to build complex Rust TUIs. Ratatui was forked from the tui-rs crate in 2023 in order to continue its development. Installation Add ratatui and crossterm as dependencies to your cargo.toml: cargo add ratatui crossterm Ratatui uses Crossterm by default as it works on most platforms. See the Installation section of the Ratatui Website for more details on how to use other backends (Termion / Termwiz). Introduction Ratatui is based on the principle of immediate rendering with intermediate buffers. This means that for each frame, your app must render all widgets that are supposed to be part of the UI. This is in contrast to the retained mode style of rendering where widgets are updated and then automatically redrawn on the next frame. See the Rendering section of the Ratatui Website for more info. Other documentation Ratatui Website - explains the library's concepts and provides step-by-step tutorials Examples - a collection of examples that demonstrate how to use the library. API Documentation - the full API documentation for the library on docs.rs. Changelog - generated by git-cliff utilizing Conventional Commits. Contributing - Please read this if you are interested in contributing to the project. Breaking Changes - a list of breaking changes in the library. Quickstart The following example demonstrates the minimal amount of code necessary to setup a terminal and render \"Hello World!\". The full code for this example which contains a little more detail is in hello_world.rs. For more guidance on different ways to structure your application see the Application Patterns and Hello World tutorial sections in the Ratatui Website and the various Examples. There are also several starter templates available: ratatui-template ratatui-async-template (book and template) Every application built with ratatui needs to implement the following steps: Initialize the terminal A main loop to: Handle input events Draw the UI Restore the terminal state The library contains a [prelude] module that re-exports the most commonly used traits and types for convenience. Most examples in the documentation will use this instead of showing the full path of each type. Initialize and restore the terminal The [Terminal] type is the main entry point for any Ratatui application. It is a light abstraction over a choice of Backend implementations that provides functionality to draw each frame, clear the screen, hide the cursor, etc. It is parametrized over any type that implements the Backend trait which has implementations for Crossterm, Termion and Termwiz. Most applications should enter the Alternate Screen when starting and leave it when exiting and also enable raw mode to disable line buffering and enable reading key events. See the backend module and the Backends section of the Ratatui Website for more info. Drawing the UI The drawing logic is delegated to a closure that takes a Frame instance as argument. The Frame provides the size of the area to draw to and allows the app to render any Widget using the provided render_widget method. See the Widgets section of the Ratatui Website for more info. Handling events Ratatui does not include any input handling. Instead event handling can be implemented by calling backend library methods directly. See the Handling Events section of the Ratatui Website for more info. For example, if you are using Crossterm, you can use the crossterm::event module to handle events. Example use std::io::{self, stdout}; use crossterm::{ event::{self, Event, KeyCode}, ExecutableCommand, terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen} }; use ratatui::{prelude::*, widgets::*}; fn main() -> io::Result { enable_raw_mode()?; stdout().execute(EnterAlternateScreen)?; let mut terminal = Terminal::new(CrosstermBackend::new(stdout()))?; let mut should_quit = false; while !should_quit { terminal.draw(ui)?; should_quit = handle_events()?; } disable_raw_mode()?; stdout().execute(LeaveAlternateScreen)?; Ok(()) } fn handle_events() -> io::Result { if event::poll(std::time::Duration::from_millis(50))? { if let Event::Key(key) = event::read()? { if key.kind == event::KeyEventKind::Press && key.code == KeyCode::Char('q') { return Ok(true); } } } Ok(false) } fn ui(frame: &mut Frame) { frame.render_widget( Paragraph::new(\"Hello World!\") .block(Block::default().title(\"Greeting\").borders(Borders::ALL)), frame.size(), ); } Running this example produces the following output: Layout The library comes with a basic yet useful layout management object called Layout which allows you to split the available space into multiple areas and then render widgets in each area. This lets you describe a responsive terminal UI by nesting layouts. See the Layout section of the Ratatui Website for more info. use ratatui::{prelude::*, widgets::*}; fn ui(frame: &mut Frame) { let main_layout = Layout::new( Direction::Vertical, [ Constraint::Length(1), Constraint::Min(0), Constraint::Length(1), ] ) .split(frame.size()); frame.render_widget( Block::new().borders(Borders::TOP).title(\"Title Bar\"), main_layout[0], ); frame.render_widget( Block::new().borders(Borders::TOP).title(\"Status Bar\"), main_layout[2], ); let inner_layout = Layout::new( Direction::Horizontal, [Constraint::Percentage(50), Constraint::Percentage(50)] ) .split(main_layout[1]); frame.render_widget( Block::default().borders(Borders::ALL).title(\"Left\"), inner_layout[0], ); frame.render_widget( Block::default().borders(Borders::ALL).title(\"Right\"), inner_layout[1], ); } Running this example produces the following output: Text and styling The Text, Line and Span types are the building blocks of the library and are used in many places. Text is a list of Lines and a Line is a list of Spans. A Span is a string with a specific style. The style module provides types that represent the various styling options. The most important one is Style which represents the foreground and background colors and the text attributes of a Span. The style module also provides a Stylize trait that allows short-hand syntax to apply a style to widgets and text. See the Styling Text section of the Ratatui Website for more info. use ratatui::{prelude::*, widgets::*}; fn ui(frame: &mut Frame) { let areas = Layout::new( Direction::Vertical, [ Constraint::Length(1), Constraint::Length(1), Constraint::Length(1), Constraint::Length(1), Constraint::Min(0), ] ) .split(frame.size()); let span1 = Span::raw(\"Hello \"); let span2 = Span::styled( \"World\", Style::new() .fg(Color::Green) .bg(Color::White) .add_modifier(Modifier::BOLD), ); let span3 = \"!\".red().on_light_yellow().italic(); let line = Line::from(vec![span1, span2, span3]); let text: Text = Text::from(vec![line]); frame.render_widget(Paragraph::new(text), areas[0]); // or using the short-hand syntax and implicit conversions frame.render_widget( Paragraph::new(\"Hello World!\".red().on_white().bold()), areas[1], ); // to style the whole widget instead of just the text frame.render_widget( Paragraph::new(\"Hello World!\").style(Style::new().red().on_white()), areas[2], ); // or using the short-hand syntax frame.render_widget(Paragraph::new(\"Hello World!\").blue().on_yellow(), areas[3]); } Running this example produces the following output: Status of this fork In response to the original maintainer Florian Dehau's issue regarding the future of tui-rs, several members of the community forked the project and created this crate. We look forward to continuing the work started by Florian 🚀 In order to organize ourselves, we currently use a Discord server, feel free to join and come chat! There is also a Matrix bridge available at #ratatui:matrix.org. While we do utilize Discord for coordinating, it's not essential for contributing. Our primary open-source workflow is centered around GitHub. For significant discussions, we rely on GitHub — please open an issue, a discussion or a PR. Please make sure you read the updated contributing guidelines, especially if you are interested in working on a PR or issue opened in the previous repository. Rust version requirements Since version 0.23.0, The Minimum Supported Rust Version (MSRV) of ratatui is 1.67.0. Widgets Built in The library comes with the following widgets: BarChart Block Calendar Canvas which allows rendering points, lines, shapes and a world map Chart Clear Gauge List Paragraph Scrollbar Sparkline Table Tabs Each widget has an associated example which can be found in the examples folder. Run each examples with cargo (e.g. to run the gauge example cargo run --example gauge), and quit by pressing q. You can also run all examples by running cargo make run-examples (requires cargo-make that can be installed with cargo install cargo-make). Third-party libraries, bootstrapping templates and widgets ansi-to-tui — Convert ansi colored text to ratatui::text::Text color-to-tui — Parse hex colors to ratatui::style::Color rust-tui-template — A template for bootstrapping a Rust TUI application with Tui-rs & crossterm tui-builder — Batteries-included MVC framework for Tui-rs + Crossterm apps tui-clap — Use clap-rs together with Tui-rs tui-log — Example of how to use logging with Tui-rs tui-logger — Logger and Widget for Tui-rs tui-realm — Tui-rs framework to build stateful applications with a React/Elm inspired approach tui-realm-treeview — Treeview component for Tui-realm tui-rs-tree-widgets — Widget for tree data structures. tui-windows — Tui-rs abstraction to handle multiple windows and their rendering tui-textarea — Simple yet powerful multi-line text editor widget supporting several key shortcuts, undo/redo, text search, etc. tui-input — TUI input library supporting multiple backends and tui-rs. tui-term — A pseudoterminal widget library that enables the rendering of terminal applications as ratatui widgets. Apps Check out the list of more than 50 Apps using Ratatui! Alternatives You might want to checkout Cursive for an alternative solution to build text user interfaces in Rust. Acknowledgments Special thanks to Pavel Fomchenkov for his work in designing an awesome logo for the ratatui project and ratatui-org organization. License MIT",
    "commentLink": "https://news.ycombinator.com/item?id=38593638",
    "commentBody": "RatatuiHacker NewspastloginRatatui (github.com/ratatui-org) 219 points by tosh 15 hours ago| hidepastfavorite57 comments treesciencebot 14 hours agoSeems like especially for the last year or so, there have been a significant amount of interest in single-language oriented (instead of a single core library w&#x2F;N language bindings, winking at a particular one) TUI libraries that are getting better and better (potentially because some of them were able to attract VC money). Two of them off top of my head is Textual (by textualize.io) for Python and BubbleTea (by charm.sh) for Go. reply pjmlp 14 hours agoparentFor me all of them have Turbo Vision for Turbo Pascal 6.0 in MS-DOS, circa 1990, as baseline to beat.Or the various Clipper based TUI for business data entry, as another example.Since they are catching up with the past, they should improve upon it, not just revisit it. reply kragen 13 hours agorootparentis there a good publicly available video that demonstrates what is good about these user interfaces, or alternatively, something that a non-expert user could run in dosbox to get a feeling? is the notably excellent part the user experience of the things that people built, or the library calling interface, or both?business data entry seems like something that probably requires an experienced user running the application to show what&#x27;s good and bad about itmy own &#x27;baseline to beat&#x27; is fractint, which is pretty much unchanged in xfractint, which you can install from apt on debian. you just have to imagine it running fullscreen on a svga reply mixmastamyk 13 hours agorootparentWhy a video? Here&#x27;s a recent port: https:&#x2F;&#x2F;github.com&#x2F;magiblot&#x2F;tvision reply jrumbut 13 hours agorootparentThe video is to display what an expert can do with it, if I understand correctly.In undergrad, I worked nights in a warehouse where most things were managed by a very complicated TUI program.The learning curve was quite steep. What someone could do after 6 months of using it was completely different to what a beginner without a trainer could do. You wouldn&#x27;t be able to see what is good about it by just setting it up on your own computer and playing with it (and of course you probably don&#x27;t have a busy warehouse to manage). reply mixmastamyk 13 hours agorootparentMay be so, but Turbo Vision was not a unique interface paradigm. It worked just like any CUA application of the era, with dialogs and widgets, tab&#x2F;funcion keys etc. Its \"raison d&#x27;être.\" reply kragen 12 hours agorootparentperhaps you intend to assert that turbo vision implemented the https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IBM_Common_User_Access standard, or made it easy for applications to do soif so, that&#x27;s news to me reply mixmastamyk 12 hours agorootparentYes it did to a large extent, although I think some of the function key defaults were a bit different, probably from older Borland apps.But by and large converging on a singular CUA&#x2F;MDI interface from Mac OS, Windows, IBM, Motif, QBasic, and TVision, in the late 80s.While not everyone got there 100% it did help tremendously that one could sit at a Mac&#x2F;Win&#x2F;Dos PC, Unix Workstation, or IBM mainframe and understand the application level interface immediately.We&#x27;re sadly losing that as \"smart\" people decide to hide things from \"dumb\" people in terms of interface design. reply kragen 12 hours agorootparentprevcompare these alternatives:- spend an hour comparing recent turbovision-like libraries. spend twenty minutes building one. spend two hours working through the tutorial. spend sixteen hours building a new application using the library. use it. get a feeling for what an application using the library written in a weekend by someone with no experience is like. (this is my understanding of the alternative you&#x27;re suggesting)- spend eight minutes watching a video of a domain expert using a user interface built in turbovision by programmers experienced in turbovision, who refined that user interface over years. (this is what i was asking for)which one do you think provides more insights into the textual user interface design spacewhich one provides more insights per minute reply mixmastamyk 12 hours agorootparentWell, maybe you could spend a minute looking at the screen shots at the link and realize it works like every typical desktop app anyone has used since 1988.TV is a classic interface in every sense of the word—it isn&#x27;t pushing boundaries as perhaps you are looking for. reply kragen 12 hours agorootparenti notice that you haven&#x27;t answered any of the four questions i&#x27;ve asked in my previous two comments, instead telling me a lot of things i already know (and also linking tvision, which is an interesting project, thank you)to me this seems surprisingly discourteouswhat do you hope to accomplish by such an abrasive interaction style reply mixmastamyk 10 hours agorootparentI see no question marks outside of the first post asking for a video. You also implied looking at TV would take an hour or more, which yes was mildly annoying.Though I see the grandparent of this post reads more passive-aggressive than I was intending when I wrote it. replykarmakaze 13 hours agorootparentprevWhat more is there? It seems like a feature-complete set once there&#x27;s mouse support. I suppose adding graphics for iTerm and such could be extras. reply joshka 3 hours agorootparentThere are apps that are built on ratatui that support mouse already including an example in the repo[1], and crates (and some internal changes to the buffer) to support iterm&#x2F;kitty&#x2F;sixel based images.[2][1]: https:&#x2F;&#x2F;github.com&#x2F;ratatui-org&#x2F;ratatui&#x2F;tree&#x2F;main&#x2F;examples#cu...[2]: https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;ratatui-imageCompared to TurboVision, Ratatui has a lot of missing things:- Containers- Dialog types (I&#x27;m working on this in https:&#x2F;&#x2F;github.com&#x2F;joshka&#x2F;tui-prompts)- Higher order combinations of widgets (e.g. combine the scrollbar and paragraph)- Menus- Any event system (apps bring their own - we just handle display)- etc.- There&#x27;s lots of things in TV that are provided as external crates (like editors, treeview, etc.)The main thing is that Ratatui is at least right now, just the display side of things. Things to do with events or application shell aren&#x27;t built-in. This somewhat stems from the immediate vs retained mode approach to the library, but this may change in the future. reply deadbabe 12 hours agorootparentprevI think mouse support would defeat the purpose of a terminal UI. reply joshka 8 hours agorootparentCore Ratatui dev here.Mouse events are supported by the underlying backend libraries (e.g. crossterm). There&#x27;s an example custom widget that shows how to handle these in Ratatui at https:&#x2F;&#x2F;github.com&#x2F;ratatui-org&#x2F;ratatui&#x2F;blob&#x2F;f767ea7d3766887c...Note the code is fairly manual as Ratatui doesn&#x27;t provide any abstractions over the input side of UIs - just the output. (yet?) reply joshka 8 hours agorootparentprevCore Ratatui dev here. Fun fact, I played a lot with these libraries when I was in a kid in the 80s&#x2F;90s learning Pascal and C++. I never played with clipper though. reply joshka 8 hours agoparentprevTextualize and BubbleTea are both things I look to for inspiration in what Ratatui could be. Eventually, I&#x27;d like to make widgets look great with sensible defaults and support themes that allow for customization.We use Charm&#x27;s VHS for scripting the generation of images for all our examples, so definitely some thanks to distribute there in addition to the inspo. reply lc9er 14 hours agoparentprevSpectre.Console for dotnet. reply goodpoint 3 hours agoparentprev>able to attract VC moneypass reply kdheepak 8 hours agoprevThanks tosh, for sharing the project here! I&#x27;m one of the maintainers of Ratatui, and was pleasantly surprised to see this on HN.I just wanted to add that we are welcoming contributions, so if you are interested please join us on GitHub for more discussions, feature requests or bug reports.If you&#x27;ve built something cool with Ratatui, we&#x27;d love to hear about it on Discord or make it part of our showcase pages.And if you are wondering what Ratatui is, check out our (fairly new) website for tutorials for getting started: https:&#x2F;&#x2F;ratatui.rs&#x2F;Happy to answer any questions here too! reply neverrroot 13 hours agoprevI love TUI. Nothing beats its clean UI, focus, speed and the resulting productivity. Learn a few shortcuts and fly. We need more TUI apps. reply baq 13 hours agoparentThe worst part is there&#x27;s absolutely nothing stopping anyone from making the same principles work in the browser. Zero. Nada. It&#x27;s trivially proven by compiling apps to wasm and running in a terminal emulator but there&#x27;s nothing stopping anyone from building react-terminal-like or whatever except that... I don&#x27;t even know what since we&#x27;ve got https:&#x2F;&#x2F;github.com&#x2F;Textualize&#x2F;textual-web.People have thrown out decades of UX research and engineering out of the window because it isn&#x27;t cool anymore. Makes me sick. reply Aurornis 13 hours agorootparent> People have thrown out decades of UX research and engineering out of the window because it isn&#x27;t cool anymore. Makes me sick.Most people don’t want to browse the web with a keyboard.Traffic to many websites primarily comes from mobile devices with touchscreens.It has nothing to do with being “cool”. Terminal interfaces are great for those of us who spend a lot of time attached to the keyboard, but most people don’t operate like that. reply sshine 11 hours agorootparent> Most people don’t want to browse the web with a keyboard.You can easily add mouse support for the equivalent of a TUI.Unlike CLIs (and shells) which requires learning a language and memorizing, TUIs give you relevant options.> most people don’t operate like thatI&#x27;m convinced that if more UIs were simpler state machines, computers would be easier to use for non-programmers. reply zozbot234 10 hours agorootparentprev> Most people don&#x27;t want to browse the web with a keyboard.Site-specific keyboard shortcuts are a thing, they&#x27;re important for accessibility. Unfortunately most sites either don&#x27;t support them or do a poor job of documenting that support in a way that can be surfaced by users. reply speed_spread 13 hours agorootparentprevIt&#x27;s not about the keyboard, it&#x27;s about discoverability and standards. Designers think their ideas are sooo good that they warrant overthrowing previously established norms of computer interaction. You now need to re-learn where things are for every new \"application\". reply 12_throw_away 13 hours agoparentprevI also love TUI, and ratatui in particular, but ... are there any TUI frameworks with accessibility features? Is that even possible?I worry that we&#x27;ve taken the TTY, a foundational accessibility device, and found a way to slap an inaccessible technology layer on top of it. reply joshka 8 hours agorootparentIt&#x27;s something I&#x27;ve thought about a little, but I haven&#x27;t seen anything that does it. There&#x27;s some pretty large blockers though. TUIs get by on effectively rendering text all over the screen, potentially out of order, and potentially without respect to whether that text makes any logical sense. AFAIK, There&#x27;s no way to attach metadata to screen characters or regions that would mark up things as buttons &#x2F; inputs &#x2F; outputs &#x2F; etc.That said, I suspect that the right modality for accessibile command line apps is repls &#x2F; cli apps rather than TUI apps. reply 082349872349872 49 minutes agorootparentIn particular, curses(3) was the \"virtual DOM\" of its day, so its output can be arbitrary pieces of TUI elements, and it may and will re-render an element with different terminal control strings if it can save bytes on the wire (recall: this was at ~300 baud) by grouping neighbouring updates. reply davidcox143 12 hours agoparentprevWe&#x27;re working on a new TUI for managing local and remote executions of optimization solvers like CBC, HiGHS, and our own hardware-accelerated solvers [2].Ratatui is a delight to work with. It uses immediate mode rendering [3] which feels very intuitive compared to other TUI frameworks.[1] https:&#x2F;&#x2F;github.com&#x2F;integrated-reasoning&#x2F;napali [2] https:&#x2F;&#x2F;reason.ing [3] https:&#x2F;&#x2F;ratatui.rs&#x2F;concepts&#x2F;rendering&#x2F; reply eviks 2 hours agoparentprevWhat beats that is a good GUI with all those features but without being limited by the clunky terminal host, which, for example, doesn&#x27;t support all the power of the keyboard shortcuts reply zozbot234 2 hours agorootparentIs that a serious limitation, though? Sure, the terminal cannot support all the keyboard interactions that an old-style DOS TUI could use, but it can get pretty close for most purposes. reply eviks 29 minutes agorootparentIf you want to fly with a few shortcuts it is, otherwise no: most of the apps illustrate that \"most purposes\" can be served with pretty bad keybinding support reply qudat 10 hours agoparentprevI built https:&#x2F;&#x2F;prose.sh off of Charm and while it was a lot of fun, nothing beats what you can build in the browser.Having said that, SSH apps are very ergonomic for the demo on this site. We have been able to build a lot of cool features on top of SSH and Charm.We are working on a static hosting site that provides a zero install CLI. It’s sick and everyone is going to want to use it. https:&#x2F;&#x2F;pgs.sh reply ku1ik 31 minutes agoprevDiscord though...? :&#x2F; reply varbhat 13 hours agoprevratatui is very good Rust TUI toolkit right now. But, it doesn&#x27;t support mouse click events yet[0]. I hope that they implement this feature as i think it&#x27;s critical to some TUI applications.[0]: https:&#x2F;&#x2F;github.com&#x2F;ratatui-org&#x2F;ratatui&#x2F;issues&#x2F;273 reply joshka 8 hours agoparentIt&#x27;s a bit broader than this - Ratatui doesn&#x27;t support events at all. It&#x27;s not a UI framework, it&#x27;s a display library (i.e. you call us, not we call you). That doesn&#x27;t mean you can&#x27;t do Mouse events (there&#x27;s an example of this in https:&#x2F;&#x2F;github.com&#x2F;ratatui-org&#x2F;ratatui&#x2F;blob&#x2F;f767ea7d3766887c...), it just means there&#x27;s not deep support for events in any way in Ratatui. reply Varimpls 13 hours agoparentprevI think this application uses click eventshttps:&#x2F;&#x2F;www.github.com&#x2F;mrjackwills&#x2F;oxker reply fnordpiglet 14 hours agoprevUse ratatui in all my tooling, it’s great. Very simple immediate mode api that eschews most of the complex framework insanity. reply enricozb 14 hours agoprevI really wish there was a more react&#x2F;swiftui-esque TUI library. One day I&#x27;ll get around to fixing the fundamental flaws in my crate [0][0]: https:&#x2F;&#x2F;docs.rs&#x2F;intuitive&#x2F;latest&#x2F;intuitive&#x2F; reply joshka 8 hours agoparentI think a lot of Ratatui apps will tend to land on similar concepts for your app. There&#x27;s a few good examples of apps using a component approach rather than just widgets that I&#x27;m aware of:- https:&#x2F;&#x2F;github.com&#x2F;sxyazi&#x2F;yazi- https:&#x2F;&#x2F;github.com&#x2F;TaKO8Ki&#x2F;gobang- https:&#x2F;&#x2F;github.com&#x2F;nomadiz&#x2F;edmaPerhaps the intuitive crate would make a good abstraction on top of Ratatui?There&#x27;s also https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;tui-react reply withinboredom 13 hours agoparentprevIf you like php, there’s termwind that combines html + tailwind. Combine that with something like the swytch framework, and you’ve got a pretty similar to react workflow (components, still working on hooks, tui, etc).The things we work on when we are bored… reply srott 13 hours agoparentprevReact & tuihttps:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;react-blessed reply airstrike 13 hours agoparentprevLove the SwiftUI inspiration. Swift has its warts but after spending a couple months deep in SwiftUI I miss it dearly when I&#x27;m writing GUIs anywhere else reply nicoburns 13 hours agoparentprevYou should check out dioxus’s TUI support reply binary132 12 hours agoprevStuff like this is cool, and I like a nice TUI, but it’s really not for everyone. It always makes me think about how I’d like GUIs to be more like the TUIs I know and love, and what a pain it seems to be for the world to converge on good, simple GUI toolkits that aren’t HMTL+JS. Please don’t say QT or GTK+.... reply zozbot234 10 hours agoparentTUI&#x27;s are closer to the HTML+JS paradigm than most GUI&#x27;s are. There&#x27;s nothing inherently wrong with that, especially when styled via CSS. reply arcanemachiner 10 hours agoparentprevBest I can do is Flutter :&#x2F; reply jordanreger 14 hours agoprevExtra points for the awesome demo GIF! reply joshka 8 hours agoparentThanks :)That was the result of wanting to do something to celebrate PR 500[1] and being repeatedly nerd-sniped by one of the other maintainers a while back. The source code for the demo[2] is in the repo btw.[1]: https:&#x2F;&#x2F;github.com&#x2F;ratatui-org&#x2F;ratatui&#x2F;pull&#x2F;500[2]: https:&#x2F;&#x2F;github.com&#x2F;ratatui-org&#x2F;ratatui&#x2F;tree&#x2F;main&#x2F;examples&#x2F;de... reply lambdatronics 4 hours agorootparentI see what you did there with the Dr. Horrible reference! :D reply joshka 3 hours agorootparentHehe - yeah, I needed an example for the table, and figured traceroute was a good easy one. Then I remembered what happens when you traceroute bad.horse... reply ShadowBanThis01 2 hours agoprevIs what? reply chaosprint 13 hours agoprevgreat to see tui-rs can be continued in this way!should try ratatui for glicol-cli at some point.you can have a look on how I use tui for music live coding:https:&#x2F;&#x2F;github.com&#x2F;glicol&#x2F;glicol-cli reply joshka 8 hours agoparentI&#x27;ve actually got a ratatui update for glicol that I haven&#x27;t pushed because I was stuck on a bug in the way the background thread continues after the UI exits. I&#x27;ll flip you a PR. reply deafpolygon 4 hours agoprev [–] I got me a ratatui...https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XWAWgEsbrXI replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ratatui is a lightweight library in Rust for creating terminal user interfaces (TUIs) and is a fork of the tui-rs crate for ongoing development.",
      "It uses immediate rendering with intermediate buffers and requires terminal initialization and restoration.",
      "Ratatui supports multiple backends, such as Crossterm, Termion, and Termwiz, and offers features for UI drawing, event handling, layout creation, text styling, and various built-in widgets. It has an active community with a Discord server and a GitHub workflow for contributions."
    ],
    "commentSummary": [
      "The discussion revolves around TUI (Text User Interface) libraries like Textual for Python and BubbleTea for Go, comparing them to the Turbo Vision library from the 1990s.",
      "The advantages and limitations of TUIs are examined, with a focus on user experience and accessibility issues.",
      "Ratatui, a terminal UI library, is praised, but it currently lacks certain features and event support."
    ],
    "points": 219,
    "commentCount": 57,
    "retryCount": 0,
    "time": 1702232930
  },
  {
    "id": 38592243,
    "title": "U.S. government demands push notification data, raising surveillance concerns",
    "originLink": "https://www.404media.co/us-government-warrant-monitoring-push-notifications-apple-google-yahoo/",
    "originBody": "This article was produced in collaboration with Court Watch, an independent outlet that unearths overlooked court records. The U.S. government is demanding that tech companies provide information related to push notifications in order to identify a target’s specific device, according to a court record reviewed by 404 Media. The finding comes as Senator Ron Wyden published a letter on Wednesday warning that the U.S. and foreign governments are making such surveillance demands around push notifications to Apple and Google. It is not totally clear if the demand for data related to push notifications mentioned in the court record is one and the same as that described at a high level in Wyden’s letter. Regardless, the court record provides more clarity on the legal mechanisms being used in at least some cases to request information related to push notifications, and what sort of crimes this novel surveillance technique is being used against. “In the spring of 2022, my office received a tip that government agencies in foreign countries were demanding smartphone ‘push’ notification records from Google and Apple. My staff have been investigating this tip for the past year, which included contacting Apple and Google,” the letter from Senator Wyden to Attorney General Merrick B. Garland reads. Reuters was first to report the letter. 📱 Do you know anything else about push notification surveillance? I would love to hear from you. Using a non-work device, you can message me securely on Signal at +44 20 8133 5190. Otherwise, send me an email at joseph@404media.co. As the letter explains, push notifications are not sent directly from an app provider to a user’s smartphone. Instead, “they pass through a kind of digital post office run by the phone’s operating system provider,” typically meaning Apple or Google. When a user gets a push notification on their phone, Apple or Google receive a lot of information, including metadata that shows which app received a notification, and which phone and associated Google or Apple account it was to be sent to, Wyden says in his letter. In some cases, unencrypted content like the actual text displayed in the notification may be included too, Wyden adds. The letter does not disclose the legal mechanism used by governments to demand this data from Apple or Google. But the court record reviewed by 404 Media does include some specifics around push notification demands. Court Watch shared the record with 404 Media. The record is a search warrant application from May 2020 related to the investigation of a person suspected of theft or bribery concerning programs receiving federal funds. In the search warrant application for information associated with a specific Yahoo email account, an FBI Special Agent writes under a section of the record entitled “Background Information Regarding Provider Services” that when a user of a mobile app installs and launches an app, the app will direct the device to obtain a “Push Token.” This is “a unique identifier that allows the provider associated with the application [...] to locate the device on which the application is installed.” Screenshots from the search warrant application. The court record then points specifically to Apple and Google, adding “After the applicable push notification (e.g., Apple Push Notifications (APN) or Google Cloud Messaging) sends a Push Token to the device, the Token is then sent to the application, which in turn sends the Push Token to the application’s server/provider.” When a company then sends push notifications, it sends both the token itself and what the court record describes as the “payload” associated with the notification, “the substance of what needs to be sent by the application to the device.” The Special Agent adds that these Push Tokens are stored on the relevant tech company’s servers, and that these may help identify a specific phone or computer used by the target. Or, as the Special Agent puts it, “Accordingly, the computers of PROVIDER are likely to contain useful information that may help to identify the specific device(s) used by a particular subscriber to access the subscriber’s PROVIDER account via the mobile application.” It is not clear if this is boilerplate language that has been included in the search warrant application or whether the agent was specifically seeking this information from Yahoo. As Wyden’s letter continues, “as with all of the other information these companies store for or about their users, because Apple and Google deliver push notification data, they can be secretly compelled by governments to hand over this information.” Wyden concludes the letter by saying that Apple and Google should be permitted to “generally reveal whether they have been compelled to facilitate this surveillance practice,” and to publish aggregate data on the number of demands they have received. When Reuters contacted Apple for comment, the company told the outlet that Wyden’s letter gave them the opening needed to share more details about how governments monitor push notifications. “In this case, the federal government prohibited us from sharing any information,” Apple told Reuters in a statement. “Now that this method has become public we are updating our transparency reporting to detail these kinds of requests.” Reuters cited a source familiar with the matter who described the foreign governments involved in making the requests as democracies allied to the U.S. government. Apple did not immediately respond to 404 Media’s request for comment on the legal mechanisms used against push notification-related data. A Google spokesperson told 404 Media in an emailed statement that “We were the first major company to publish a public transparency report sharing the number and types of government requests for user data we receive, including the requests referred to by Senator Wyden. We share the Senator’s commitment to keeping users informed about these requests.” The full section of the court record discussing push notifications is included below. PROVIDER also allows its subscribers to access its various services through an application that can be installed on and accessed via cellular telephones and other mobile devices. This application is associated with the subscriber’s PROVIDER account. In my training and experience, I have learned that when the user of a mobile application installs and launches the application on a device (such as a cellular telephone), the application directs the device in question to obtain a Push Token, a unique identifier that allows the provider associated with the application (such as PROVIDER) to locate the device on which the application is installed. After the applicable push notification (e.g., Apple Push Notifications (APN) or Google Cloud Messaging) sends a Push Token to the device, the Token is then sent to the application, which in turn sends the Push Token to the application’s server/provider. Thereafter, whenever the provider needs to send notifications to the user’s device, it sends both the Push Token and the payload associated with the notification (i.e., the substance of what needs to be sent by the application to the device). To ensure this process works, Push Tokens associated with a subscriber’s account are stored on the provider’s server(s). Accordingly, the computers of PROVIDER are likely to contain useful information that may help to identify the specific device(s) used by a particular subscriber to access the subscriber’s PROVIDER account via the mobile application. About the author Joseph is an award-winning investigative journalist focused on generating impact. His work has triggered hundreds of millions of dollars worth of fines, shut down tech companies, and much more. More from Joseph Cox",
    "commentLink": "https://news.ycombinator.com/item?id=38592243",
    "commentBody": "A warrant showing the U.S. government is monitoring push notificationsHacker NewspastloginA warrant showing the U.S. government is monitoring push notifications (404media.co) 206 points by PaulHoule 18 hours ago| hidepastfavorite75 comments macintux 17 hours agoExtensive discussion (>600 comments) last week: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38543155 reply losteric 13 hours agoparentThis has news. Last week just reported on Senator Ron Wyden&#x27;s letter, this article has concrete data from court orders and warrants. reply macintux 12 hours agorootparentI didn&#x27;t flag it as a dupe, just helpful to have links between discussion threads (and try to avoid beating dead horses, although that&#x27;s always a lost cause). reply codethief 12 hours agoprevThis is the third time in a week that I read about this and, to me, the most important question has remained unanswered: If a push notification&#x27;s payload is E2E-encrypted (consider, e.g., push notifications for Signal running on GrapheneOS with sandboxed Google Play Services), is there still a data leak? Like, what metadata are people referring to? The fact that I use Signal at all?Of course, depending on the app, it coupd be possible to correlate even E2E-encryped push notifications with other data on that app&#x27;s backend server etc. But beyond specific apps is there a generic vulnerability here? reply halJordan 10 hours agoparentThe data leak, more of an oracle than anything else, is that you have an active account with the business sending the push. And it&#x27;s not really even a data leak because it&#x27;s data generated in the course of activity the user has requested. reply codethief 3 hours agorootparent> is that you have an active account with the business sending the pushBut they wouldn&#x27;t need to analyze push notifications for that, they could simply ask Google&#x2F;Apple who installed a given app. reply 0xy 10 hours agoparentprevThe approximate size of the payload, the application, the time, maybe where you received it (if you were on a cellular network). All of this has a lot of value. You don&#x27;t exactly need the message or the recipient. reply codethief 3 hours agorootparentWhat value does it have?The payload of Signal&#x27;s push notifications should have roughly the same length every time as they are only used to wake up the app. The time I receive a push notification doesn&#x27;t depend on me, but on when my contacts send messages. (Yes, you could use this to narrow down someone&#x27;s time zone but not much more than that. Plus there are a million other ways to determine the time zone.) reply walrus01 9 hours agoparentprev> Like, what metadata are people referring to?Time of day that you&#x27;re awake&#x2F;active, the frequency of message push notifications vs your travel patterns when you might be expected to be offline, weekday vs weekend message traffic volumes per day, traffic volumes on holidays, all these sorts of things are valuable metadata for an NSA-like organization. reply codethief 3 hours agorootparent> Time of day that you&#x27;re awake&#x2F;active, […] your travel patterns when you might be expected to be offlineBut the push notifications I receive through Signal don&#x27;t tell you any of those things? reply traceroute66 13 hours agoprevGood to see Threema are ahead of the game, they anticipated the scenario and have been using encrypted notifications for some time now[1].[1]https:&#x2F;&#x2F;threema.ch&#x2F;en&#x2F;faq&#x2F;privacy_push reply afroboy 13 hours agoparentSignal doesn&#x27;t even send the message via push notifications. reply seeknotfind 15 hours agoprevHow to protect yourself? Is disabling notifications locally a good countermeasure? reply _heimdall 14 hours agoparentDepends on how deeply you want to protect yourself.Disabling push notifications would help, especially if you disable notifications through individually app settings first. That should make sure an app doesn&#x27;t continue trying to send notifications entirely, if you just disable notifications globally Apple or Google may still see notifications that they just don&#x27;t route to your device.If you really want better protection, use GrapheneOS or a similar de-Googled android device and don&#x27;t install any Google services. That&#x27;s the best way to still have a modern smartphone with limited risk that Apple or Google is somehow tracking most use. reply devwastaken 2 hours agorootparentI&#x27;m unconvinced that graphene is safe with the behavioral problems it&#x27;s leadership has shown. reply TheCraiggers 14 hours agoparentprevDepends. Since Apple&#x2F;Google are monitoring it on their end, and I believe (at least on Android) turning off notifications at the OS level just blocks it from showing on your phone, these would still be sniffable.If the application in question has the ability to disable notifications inside the application itself, that should work. reply Syonyk 13 hours agoparentprevYou stop using your cell phone for anything important, or anything that can&#x27;t be gotten through other means trivially.Practically, this means you use your cell phone for phone calls (the metadata is public, and I assume anyone who wants to listen in can already do so), and for SMS&#x2F;MMS messages (see above, except I don&#x27;t think the contents are quite as protected as voice).You disable location services, you don&#x27;t install anything of any interest on your \"daily carry\" phone, and you regularly shut it down for periods of time to build the expectation that your device is regularly offline. Let it run out of battery. Cultivate the \"senile old senior\" approach to using your phone. Leave it behind.And then carry a small laptop, preferably running Qubes, for \"everything else,\" and either use Tor or your own VPN infrastructure (ideally shared with friends) for access.... and start cultivating ways of life that are offline first, that don&#x27;t rely on consumer electronics (or the upstream companies) to behave as anything other than the data-grubbing, data-selling sorts they&#x27;ve reliably proven to be.Yeah. It sucks. The past 20 years of consumer electronics turn out to have been rotten on the vine, actively working against your own interests, comically insecure (so even if they&#x27;re not just streaming your data off to whoever pays&#x2F;demands, it&#x27;s not hard to extract), etc.I don&#x27;t have any better answers. I&#x27;ve been trying for about the last 5 years to figure out a solution, and I just can&#x27;t come up with anything reasonable that still involves using consumer electronics for much more than toy uses. Apple looked better for a while, but then lost their head with the on-device CSAM scanning stuff and, while I like it, Lockdown is a simple admission that they cannot build secure software against nation-state level adversaries. Plus, most of their updates are \"Oh, yeah, so, update this now, we have reason to believe [solid proof and won&#x27;t say it, usually...] that this fixes things under active exploit.\" But, hey, we&#x27;ve got MeMojis and such now!We have built too much complexity into our systems (see all the uarch vulns that are fundamentally a result of chasing performance over everything) to understand, to reason about, and we can&#x27;t fix the problems of complexity with yet more complexity (as the last 5 years of papers demonstrate, often to comedic effect, about how the uarch vuln mitigations open up this other channel). And the software isn&#x27;t any better.I don&#x27;t see the path forward other than simply opting out, and building systems that no longer rely on vulnerable pocket computers that leak literally everything you&#x27;re doing to whoever might care. reply aaomidi 14 hours agoprevApple and Google have been recommending that actual data not be sent through these push notifications and only a “ping” for the app to go check the source of truth.Maybe it’s time to actually enforce this and remove the ability for arbitrary content to be sent? reply wkat4242 14 hours agoparentAh I wonder if that&#x27;s why notifications don&#x27;t work when I force stopped an app reply anonymouse008 13 hours agorootparentIn effect, yes. The notification delegates in Swift only call the notification callbacks if you tap the specific notification (if you just go to the App the notification callbacks are not fired) reply gsuuon 13 hours agoparentprevYou&#x27;d need some amount of arbitrary data (the copy) so the user knows what kind of content they can expect. reply aaomidi 12 hours agorootparentNope, these notifications don’t always turn into a user-visible notification. reply TheCraiggers 14 hours agoprevLooks like it&#x27;s time to finally dive into setting up ntfy and UnifiedPush for my stuff. reply jhot 6 hours agoparentAlready had ntfy set up and took the time to setup MollySocket so I can get signal notifications without firebase or a dedicated websocket connection (even if no message data is included). Here&#x27;s to hoping more apps are brought on board.I also love ntfy for its general handiness. reply jmnicolas 14 hours agoparentprevI don&#x27;t know UnifiedPush so I can&#x27;t compare, but ntfy is an absolute gem.I have it on my personal server, configuration is easy and the app is available on degoogled phones and works perfectly.Just look at the doc on Github, most professional software don&#x27;t have such a well done doc. reply wkat4242 14 hours agoparentprevYes but good apps do encrypt it already reply jonplackett 14 hours agoprevAre push notifications sent in plain text? reply yellow_lead 14 hours agoparentFor most applications, yes. They are only encrypted in transit via HTTPS, but they are readable to Google&#x2F;Apple.It&#x27;s possible to E2EE push notifications, but you need custom application logic. reply jonplackett 14 hours agorootparentWouldn’t it be pretty simple to do that though if you wanted to?Send the push without content, or with just an identifier, and then have the app go get that message from the database and show it. reply toast0 14 hours agorootparentOn iOS historically, you needed a special dispensation for your app to run when a push is received. So your choice was to let Apple see the content, or to have a push like &#x27;you&#x27;ve got a new message&#x27;. I don&#x27;t know the current status, but this used to be called a voip push or silent push; and Apple kept track to make sure you were at least posting notifications, otherwise future notifications would be dropped&#x2F;delayed.A lot of apps clearly do notifications separately from content though: you&#x27;ll get a notification, but when you tap on it, the content has to load. reply tiahura 16 hours agoprevMaybe I’m confused but the warrant seems to suggest they’re not monitoring them. It’s asking for the notifications. If they were monitoring them, they wouldn’t need to subpoena them. reply NegativeLatency 16 hours agoparentParallel construction? reply jjtheblunt 16 hours agorootparentWhat does that mean? reply tofof 14 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Parallel_constructionIn the US, a particular form is evidence laundering, where one police officer obtains evidence via means that are in violation of the Fourth Amendment&#x27;s protection against unreasonable searches and seizures, and then passes it on to another officer, who builds on it and gets it accepted by the court under the good-faith exception as applied to the second officer. This practice gained support after the Supreme Court&#x27;s 2009 Herring v. United States decision.See also the sibling about Fruit of the Poisonous Tree, the principle of law that Parallel Construction has rendered moot. reply jjtheblunt 14 hours agorootparentMath nerd observation:reminds me of algebra equation solving encountering square root of -1, then naming it an introduced variable “i”, rather than being stuck, and moving on, in hopes “i” vanishes later in the set of equations being solved or simplified. reply tharkun__ 16 hours agorootparentprevThey already know through either inadmissible means or outright illegal ones or they don&#x27;t want you to know their capabilities.So now they go the official way. They already know exactly where to look and what to look for and what to ask for. reply temp0826 16 hours agorootparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fruit_of_the_poisonous_treeIllegally obtained evidence can&#x27;t be used, so they must build the story using only legal means, which can be difficult and take longer or not possible at all sometimes. reply nullc 13 hours agorootparenthahaThat rule been so undermined in so many respects that is has little effect.When the government illegally spies on the public it goes in knowing that it has to cover for its actions.The evidence rules tend to only catch genuine errors where they failed to do the required parallel construction or set things up for the inevitable discovery doctrine because the unlawful search was inadvertent rather than intentional. reply wharvle 16 hours agorootparentprevLearn something from illegal&#x2F;inadmissible&#x2F;secret source, use that info to find other evidence you can actually present in a public court, that you otherwise might not have found. reply llamaInSouth 14 hours agorootparentprevit means that they are trying to hide being criminals... (the government)... look it up on wikipediahttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Parallel_constructionAGABAll Governements Are Bastards reply cyberpunk 16 hours agorootparentprevIf they request a warrant also, then they can actually use the results in a court process? reply jacobsenscott 14 hours agorootparentprevStandard tinfoil hatter stuff - it is impossible to prove the government is not monitoring everything. From there you can build any theory - one being the government finds out you are doing something illegal, and then they go back and find legal ways to prove it.Has this happened - probably. Does it happen a lot - probably not. But none of that matters - the uncertainty is enough to build a whole online community that believes hard. reply bastard_op 16 hours agoprevShow me any large US ISP, and I&#x27;ll find you a locked room few know about with government network sniffers that sit at the head of all regional traffic to get a copy of everything going in and out there. Everyone does it, but like fight club, no one talks about it. If they&#x27;re not in yours, it&#x27;s because they&#x27;ve already gotten upstream of traffic to see it all anyways.The problem comes with sifting through the data, but now that you have tireless AI doing that work for tired humans, who&#x27;s to say what they actually don&#x27;t see. reply jasonwatkinspdx 16 hours agoparentDoesn&#x27;t even have to be big.Years back I was touring a local datacenter that was more than a bit quirky, but their offer was basically that they had fiber loops into the main carrier hotel a few blocks away. This was useful because the guy than ran the carrier hotel wouldn&#x27;t even return your email unless you were from BigCo.But anyhow, walking around he pointed out one cage and said something like \"And that&#x27;s the NSA&#x27;s cage, we don&#x27;t ask what they do haw haw.\" At the time I mostly thought he was just exaggerating or joking around. But later after revelations of the scale of bulk collection I had to wonder if it really was true and simply banally that much in the open. reply fragmede 15 hours agorootparentRoom 641A was known to the security community long before the Snowden leaks. reply OfSanguineFire 14 hours agorootparentSo much was revealed in the European Parliament&#x27;s ECHELON report back in 2000 that I found it hard to understand why Snowden made the big splash that he did. It all seemed pretty old hat to me. reply 0xDEF 13 hours agorootparentThe chattering classes love counter-cultural packaging. That is why they embraced Greta Thunberg much more than they embraced Al Gore despite the messaging being the same.The ECHELON report revelations were packaged into a formal (boring) European Parliament report. Meanwhile Edward Snowden had the counter-cultural packaging of a cool dissident hacker. reply gscott 14 hours agorootparentprevAnd the Utah data center that stores days worth of the entire Internet and then they just keep the most interesting parts and the parts they can&#x27;t hack into for later analysis.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Utah_Data_Center reply wayfinder 14 hours agorootparentprevIn 2006, Mark Klein working for AT&T leaked it. It was in the news.Snowden did his leak way later in 2013. reply formerly_proven 15 hours agorootparentprevIn fact it even had a Wikipedia article a number of years before them: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;w&#x2F;index.php?title=Room_641A&oldid=6... reply bastard_op 15 hours agorootparentprevHah, I only say large as anything smaller they&#x27;re already getting you somewhere at your provider&#x27;s provider. reply _heimdall 14 hours agoparentprevAt least as of a few years ago, AT&T still owned most of the core network in the US and leased it out to other ISPs. The government has a direct pipe into AT&T which allowed (still does?) them to sniff everything regardless of ISP since AT&T almost certainly owned the underlying pipe. reply dirtyhippiefree 15 hours agoparentprevThe government spooks aren’t at the ISP level…remember the AT&T whistleblower…https:&#x2F;&#x2F;www.wired.com&#x2F;2006&#x2F;05&#x2F;att-whistle-blowers-evidence&#x2F; reply mdgrech23 15 hours agoparentprevcould see these \"statistics\" being used to gauge to public response to political decisions. That&#x27;s pretty dystopian. \"President Biden, the &#x27;data&#x27; shows your response to Palestine was not very popular\". reply bathtub365 15 hours agorootparentThis world where the people running the government care what the populace thinks about their decisions reads like a utopian fantasy. reply 2OEH8eoCRo0 9 hours agorootparentprevIt&#x27;s not popular with me. We should be providing Israel with far more JDAMs. reply badrabbit 14 hours agoprevWhere are the popular personalities telling people to not use VPNs? I swear sometimes I theorize about gov agencies using people to spread insecurities like that.VPN providers that are run by reputable people&#x2F;orgs and make security promises are liable to lawsuits and criminal prosecution if they sniff your traffic or sell info about you, unlike ISPs complying with gov requests&#x2F;partnerships and who want another revenue stream by selling your info to the highest bidder with no specific privacy guarantees. reply jeffbee 16 hours agoprevI renew my question of why this is surprising or objectionable. \"Pen register\" surveillance has been a thing that applies to actual mail, email, telephone networks, IP networks, and any other thing with a real or metaphorical envelope. reply karaterobot 16 hours agoparent> I renew my question of why this is surprising or objectionable.It&#x27;s surprising because the government doesn&#x27;t exactly talk about it a lot. Thus, most people who don&#x27;t follow security issues don&#x27;t hear about it very often. It&#x27;s not like the government advertises these activities with billboards and TV spots. The reason they don&#x27;t is because this broad interpretation of their responsibilities makes them look pretty bad without having a long discussion with a lot of context. As it is, people might just ask them to stop reading their emails. So, if the people doing it don&#x27;t talk about it, why would you be surprised that other people don&#x27;t know it&#x27;s happening? It sounds like you&#x27;re saying \"I&#x27;m surprised that not everybody pays careful attention to the specific domains I pay attention to\", but remember that it takes all kinds of people to make the world go round. reply tastyfreeze 14 hours agorootparentIt shouldn&#x27;t be surprising to anybody that has been paying even a small amount of attention to the results of Snowden releases.They might not talk about it but one agency or another has been consuming all digital traffic since at least the late 90s.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Carnivore_%28software%29?wprov... reply jeffbee 15 hours agorootparentprevThis isn&#x27;t the police reading your emails. It is envelope metadata (to&#x2F;from). This type of surveillance is not even a search under the 4th Amendment (Smith v. Maryland). Having a warrant makes the activities described in the article completely legitimate. It is the definition of \"due process\". reply nulbyte 15 hours agorootparentIt may not be a search under that case, but plenty of folks disagree with that assessment. It&#x27;s not like the telcos publish metadata for anyone to read. It is private data, and the fact that government officers think they have carte blanche to get it, and worse, can find judges that agree with them, is disagreeable to a significant portion of the population.Is it warranted (figuratively, not literally) in this instance? Perhaps. But nonetheless, it re-opens the conversation about the wider implications about warrantless searches. reply jeffbee 13 hours agorootparentThat isn&#x27;t the discussion that HN seems to be having though. The idea that a court cannot order a search is an extreme, fringe position. reply karaterobot 14 hours agorootparentprev> It is envelope metadata (to&#x2F;from).Right, but!That&#x27;s what the conversation would be about, without a long discussion and a lot of context. That&#x27;s why we don&#x27;t have the conversation: it&#x27;s too hard, and people would just say \"stop listening to my phone calls, you government perverts\". And because we don&#x27;t have that conversation, it&#x27;s a surprise when things like this come up. reply rightbyte 16 hours agoparentprevAutomatization has made it possible to do all these stuff at such a scale that Google can spy on everyone all the time.Stasi was limited by that the whole of DDR can&#x27;t work at Stasi.Instead of the government joining in on the fun, maybe it would be good to e.g. close down Google (split up the spy and search parts, which essentially is closing down Google since they have relatively nothing without the spying). reply jeffbee 16 hours agorootparentYou obviously do not have a realistic appreciation for the scale or centralization of the post office or AT&T, nor any understanding of American 4th Amendment jurisprudence. If the only thing you bring to the discussion is an equivalation of Google and Apple with the Stasi, that&#x27;s nothing more than fulfilling a variant of Godwin&#x27;s Law. reply feedforward 16 hours agorootparentHe said Stasi was limited so it is not equivalation. The US government and its infrastructure oligopolies monitor citizens far more than the Stasi ever did. We are pleading to a reduction down to a Stasi-like intelligence service.Tangentially, J. Edgar Hoover was politically opposed to feminism, and COINTELPRO had a massive secret police action against feminists and feminist groups in the 1970s. Some of us want these issues hashed out at the ballot box, not by some giant secret political police force. reply jeffbee 16 hours agorootparentIn what way is COINTELPRO related to a court-ordered pen register? reply feedforward 14 hours agorootparentThe answer can be found in the first word of the statement. reply dirtyhippiefree 15 hours agorootparentprev…in the way governments want the data to extend control…SMH replystaplers 16 hours agoparentprevBenevolent surveillance is a waiting period before malevolent manipulation.Study history. reply jtbayly 16 hours agoparentprevReally?!The US Gov kept a record of all the metadata of every letter sent through the USPS? reply paulmd 13 hours agorootparenthttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Mail_cover> Since 2001, the Postal Service has been effectively conducting mail covers on all American postal mail as part of the Mail Isolation Control and Tracking program.[1]https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Mail_Isolation_Control_and_T... reply jeffbee 16 hours agorootparentprevNo, and they don&#x27;t keep a record of all APNS traffic, either. They get a court to order the push operator to log the envelope metadata of messages to and from enumerated parties who are the subject of the warrant. reply seydor 15 hours agoprev [–] Nobody should be surprised. I think most people are familiar with Murphy&#x27;s law replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The U.S. government is requesting information from tech companies related to push notifications to identify specific devices, as revealed in a court record and a letter from Senator Ron Wyden.",
      "Apple and Google receive various types of information, including metadata and sometimes unencrypted content, when push notifications are sent to users' phones.",
      "Senator Wyden is advocating for transparency from tech companies in regards to government surveillance requests, and Apple and Google have responded by updating their transparency reporting and committing to keeping users informed."
    ],
    "commentSummary": [
      "The conversation discusses government surveillance and privacy concerns in several areas, such as push notification monitoring, encryption, parallel construction, ISP network sniffers, and artificial intelligence analysis.",
      "Participants express worries about privacy, potential abuse of power, and the importance of public awareness and due process.",
      "Countermeasures suggested include disabling notifications, using encrypted services or VPNs, and reducing reliance on consumer electronics."
    ],
    "points": 206,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1702222832
  },
  {
    "id": 38597744,
    "title": "Improving Internet Performance: Introducing L4S Architecture for Low Latency, Low Loss, and Scalable Throughput",
    "originLink": "https://datatracker.ietf.org/doc/rfc9330/",
    "originBody": "Skip to main content Datatracker Groups By area/parent Apps & Realtime General Internet Ops & Management Routing Security Transport IAB IRTF IETF LLC RFC Editor Other Active AGs Active Areas Active Directorates Active IAB Workshops Active Programs Active RAGs Active Teams New work Chartering groups BOFs BOF Requests Other groups Concluded groups Non-WG lists Documents Search Recent I-Ds I-D submission IESG dashboard RFC streams IAB IRTF ISE Editorial Meetings Agenda Materials Floor plan Registration Important dates Request a session Session requests Upcoming meetings Upcoming meetings Past meetings Past meetings Meeting proceedings Other IPR disclosures Liaison statements IESG agenda NomComs Downref registry Statistics I-Ds/RFCs Meetings API Help Release notes Report a bug User Sign in Password reset Preferences New account Report a bug Sign in Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service: Architecture RFC 9330 Status IESG evaluation record IESG writeups Email expansions History Versions: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 RFC 9330 Document TypeRFC - Informational (January 2023) Was draft-ietf-tsvwg-l4s-arch (tsvwg WG)AuthorsBob Briscoe , Koen De Schepper , Marcelo Bagnulo , Greg WhiteLast updated2023-01-19Replacesdraft-briscoe-tsvwg-l4s-archRFC streamInternet Engineering Task Force (IETF)Formats txt html xml pdf htmlized bibtexReviews SECDIR Last Call review by Brian Weis Ready ARTART Last Call review (of -18) by Marco Tiloca Ready w/nits OPSDIR Last Call Review due 2022-07-21 IncompleteAdditional resourcesOther Repository Mailing list discussion Stream WG stateSubmitted to IESG for PublicationAssociated WG milestone Jun 2022 Submit \"Low Latency, Low Loss, Scalable Throughput (L4S) Internet Service: Architecture\" as an Informational RFCDocument shepherdWesley EddyShepherd write-upShow Last changed 2022-03-08 IESG IESG stateRFC 9330 (Informational)Action Holders (None)Consensus boilerplateYesTelechat date(None)Responsible ADMartin DukeSend notices toWesley EddyIANA IANA review stateIANA OK - No Actions NeededIANA action stateNo IANA Actions Email authors Email WG IPR References Referenced by Nits Search lists IETF Mail Archive Google RFC 9330 Internet Engineering Task Force (IETF) B. Briscoe, Ed. Request for Comments: 9330Independent Category: InformationalK. De Schepper ISSN: 2070-1721Nokia Bell LabsM. BagnuloUniversidad Carlos III de Madrid G. WhiteCableLabsJanuary 2023 Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service: Architecture Abstract This document describes the L4S architecture, which enables Internet applications to achieve low queuing latency, low congestion loss, and scalable throughput control. L4S is based on the insight that the root cause of queuing delay is in the capacity-seeking congestion controllers of senders, not in the queue itself. With the L4S architecture, all Internet applications could (but do not have to) transition away from congestion control algorithms that cause substantial queuing delay and instead adopt a new class of congestion controls that can seek capacity with very little queuing. These are aided by a modified form of Explicit Congestion Notification (ECN) from the network. With this new architecture, applications can have both low latency and high throughput. The architecture primarily concerns incremental deployment. It defines mechanisms that allow the new class of L4S congestion controls to coexist with 'Classic' congestion controls in a shared network. The aim is for L4S latency and throughput to be usually much better (and rarely worse) while typically not impacting Classic performance. Status of This Memo This document is not an Internet Standards Track specification; it is published for informational purposes. This document is a product of the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by the Internet Engineering Steering Group (IESG). Not all documents approved by the IESG are candidates for any level of Internet Standard; see Section 2 of RFC 7841. Information about the current status of this document, any errata, and how to provide feedback on it may be obtained at https://www.rfc-editor.org/info/rfc9330. Copyright Notice Copyright (c) 2023 IETF Trust and the persons identified as the document authors. All rights reserved. This document is subject to BCP 78 and the IETF Trust's Legal Provisions Relating to IETF Documents (https://trustee.ietf.org/license-info) in effect on the date of publication of this document. Please review these documents carefully, as they describe your rights and restrictions with respect to this document. Code Components extracted from this document must include Revised BSD License text as described in Section 4.e of the Trust Legal Provisions and are provided without warranty as described in the Revised BSD License. Table of Contents 1. Introduction 1.1. Document Roadmap 2. L4S Architecture Overview 3. Terminology 4. L4S Architecture Components 4.1. Protocol Mechanisms 4.2. Network Components 4.3. Host Mechanisms 5. Rationale 5.1. Why These Primary Components? 5.2. What L4S Adds to Existing Approaches 6. Applicability 6.1. Applications 6.2. Use Cases 6.3. Applicability with Specific Link Technologies 6.4. Deployment Considerations 6.4.1. Deployment Topology 6.4.2. Deployment Sequences 6.4.3. L4S Flow but Non-ECN Bottleneck 6.4.4. L4S Flow but Classic ECN Bottleneck 6.4.5. L4S AQM Deployment within Tunnels 7. IANA Considerations 8. Security Considerations 8.1. Traffic Rate (Non-)Policing 8.1.1. (Non-)Policing Rate per Flow 8.1.2. (Non-)Policing L4S Service Rate 8.2. 'Latency Friendliness' 8.3. Interaction between Rate Policing and L4S 8.4. ECN Integrity 8.5. Privacy Considerations 9. Informative References Acknowledgements Authors' Addresses 1. Introduction At any one time, it is increasingly common for all of the traffic in a bottleneck link (e.g., a household's Internet access or Wi-Fi) to come from applications that prefer low delay: interactive web, web services, voice, conversational video, interactive video, interactive remote presence, instant messaging, online and cloud-rendered gaming, remote desktop, cloud-based applications, cloud-rendered virtual reality or augmented reality, and video-assisted remote control of machinery and industrial processes. In the last decade or so, much has been done to reduce propagation delay by placing caches or servers closer to users. However, queuing remains a major, albeit intermittent, component of latency. For instance, spikes of hundreds of milliseconds are not uncommon, even with state-of-the-art Active Queue Management (AQM) [COBALT] [DOCSIS3AQM]. A Classic AQM in an access network bottleneck is typically configured to buffer the sawteeth of lone flows, which can cause peak overall network delay to roughly double during a long-running flow, relative to expected base (unloaded) path delay [BufferSize]. Low loss is also important because, for interactive applications, losses translate into even longer retransmission delays. It has been demonstrated that, once access network bit rates reach levels now common in the developed world, increasing link capacity offers diminishing returns if latency (delay) is not addressed [Dukkipati06] [Rajiullah15]. Therefore, the goal is an Internet service with very low queuing latency, very low loss, and scalable throughput. Very low queuing latency means less than 1 millisecond (ms) on average and less than about 2 ms at the 99th percentile. End-to-end delay above 50 ms [Raaen14], or even above 20 ms [NASA04], starts to feel unnatural for more demanding interactive applications. Therefore, removing unnecessary delay variability increases the reach of these applications (the distance over which they are comfortable to use) and/or provides additional latency budget that can be used for enhanced processing. This document describes the L4S architecture for achieving these goals. Differentiated services (Diffserv) offers Expedited Forwarding (EF) [RFC3246] for some packets at the expense of others, but this makes no difference when all (or most) of the traffic at a bottleneck at any one time requires low latency. In contrast, L4S still works well when all traffic is L4S -- a service that gives without taking needs none of the configuration or management baggage (traffic policing or traffic contracts) associated with favouring some traffic flows over others. Queuing delay degrades performance intermittently [Hohlfeld14]. It occurs i) when a large enough capacity-seeking (e.g., TCP) flow is running alongside the user's traffic in the bottleneck link, which is typically in the access network, or ii) when the low latency application is itself a large capacity-seeking or adaptive rate flow (e.g., interactive video). At these times, the performance improvement from L4S must be sufficient for network operators to be motivated to deploy it. Active Queue Management (AQM) is part of the solution to queuing under load. AQM improves performance for all traffic, but there is a limit to how much queuing delay can be reduced by solely changing the network without addressing the root of the problem. The root of the problem is the presence of standard congestion control (Reno [RFC5681]) or compatible variants (e.g., CUBIC [RFC8312]) that are used in TCP and in other transports, such as QUIC [RFC9000]. We shall use the term 'Classic' for these Reno-friendly congestion controls. Classic congestion controls induce relatively large sawtooth-shaped excursions of queue occupancy. So if a network operator naively attempts to reduce queuing delay by configuring an AQM to operate at a shallower queue, a Classic congestion control will significantly underutilize the link at the bottom of every sawtooth. These sawteeth have also been growing in duration as flow rate scales (see Section 5.1 and [RFC3649]). It has been demonstrated that, if the sending host replaces a Classic congestion control with a 'Scalable' alternative, the performance under load of all the above interactive applications can be significantly improved once a suitable AQM is deployed in the network. Taking the example solution cited below that uses Data Center TCP (DCTCP) [RFC8257] and a Dual-Queue Coupled AQM [RFC9332] on a DSL or Ethernet link, queuing delay under heavy load is roughly 1-2 ms at the 99th percentile without losing link utilization [L4Seval22] [DualPI2Linux] (for other link types, see Section 6.3). This compares with 5-20 ms on _average_ with a Classic congestion control and current state-of-the-art AQMs, such as Flow Queue CoDel [RFC8290], Proportional Integral controller Enhanced (PIE) [RFC8033], or DOCSIS PIE [RFC8034] and about 20-30 ms at the 99th percentile [DualPI2Linux]. L4S is designed for incremental deployment. It is possible to deploy the L4S service at a bottleneck link alongside the existing best efforts service [DualPI2Linux] so that unmodified applications can start using it as soon as the sender's stack is updated. Access networks are typically designed with one link as the bottleneck for each site (which might be a home, small enterprise, or mobile device), so deployment at either or both ends of this link should give nearly all the benefit in the respective direction. With some transport protocols, namely TCP [ACCECN], the sender has to check that the receiver has been suitably updated to give more accurate feedback, whereas with more recent transport protocols, such as QUIC [RFC9000] and Datagram Congestion Control Protocol (DCCP) [RFC4340], all receivers have always been suitable. This document presents the L4S architecture. It consists of three components: network support to isolate L4S traffic from Classic traffic; protocol features that allow network elements to identify L4S traffic; and host support for L4S congestion controls. The protocol is defined separately in [RFC9331] as an experimental change to Explicit Congestion Notification (ECN). This document describes and justifies the component parts and how they interact to provide the low latency, low loss, and scalable Internet service. It also details the approach to incremental deployment, as briefly summarized above. 1.1. Document Roadmap This document describes the L4S architecture in three passes. First, the brief overview in Section 2 gives the very high-level idea and states the main components with minimal rationale. This is only intended to give some context for the terminology definitions that follow in Section 3 and to explain the structure of the rest of the document. Then, Section 4 goes into more detail on each component with some rationale but still mostly stating what the architecture is, rather than why. Finally, Section 5 justifies why each element of the solution was chosen (Section 5.1) and why these choices were different from other solutions (Section 5.2). After the architecture has been described, Section 6 clarifies its applicability by describing the applications and use cases that motivated the design, the challenges applying the architecture to various link technologies, and various incremental deployment models (including the two main deployment topologies, different sequences for incremental deployment, and various interactions with preexisting approaches). The document ends with the usual tailpieces, including extensive discussion of traffic policing and other security considerations in Section 8. 2. L4S Architecture Overview Below, we outline the three main components to the L4S architecture: 1) the Scalable congestion control on the sending host; 2) the AQM at the network bottleneck; and 3) the protocol between them. But first, the main point to grasp is that low latency is not provided by the network; low latency results from the careful behaviour of the Scalable congestion controllers used by L4S senders. The network does have a role, primarily to isolate the low latency of the carefully behaving L4S traffic from the higher queuing delay needed by traffic with preexisting Classic behaviour. The network also alters the way it signals queue growth to the transport. It uses the Explicit Congestion Notification (ECN) protocol, but it signals the very start of queue growth immediately, without the smoothing delay typical of Classic AQMs. Because ECN support is essential for L4S, senders use the ECN field as the protocol that allows the network to identify which packets are L4S and which are Classic. 1) Host: Scalable congestion controls already exist. They solve the scaling problem with Classic congestion controls, such as Reno or CUBIC. Because flow rate has scaled since TCP congestion control was first designed in 1988, assuming the flow lasts long enough, it now takes hundreds of round trips (and growing) to recover after a congestion signal (whether a loss or an ECN mark), as shown in the examples in Section 5.1 and [RFC3649]. Therefore, control of queuing and utilization becomes very slack, and the slightest disturbances (e.g., from new flows starting) prevent a high rate from being attained. With a Scalable congestion control, the average time from one congestion signal to the next (the recovery time) remains invariant as flow rate scales, all other factors being equal. This maintains the same degree of control over queuing and utilization, whatever the flow rate, as well as ensuring that high throughput is more robust to disturbances. The Scalable control used most widely (in controlled environments) is DCTCP [RFC8257], which has been implemented and deployed in Windows Server Editions (since 2012), in Linux, and in FreeBSD. Although DCTCP as-is functions well over wide-area round-trip times (RTTs), most implementations lack certain safety features that would be necessary for use outside controlled environments, like data centres (see Section 6.4.3). Therefore, Scalable congestion control needs to be implemented in TCP and other transport protocols (QUIC, Stream Control Transmission Protocol (SCTP), RTP/RTCP, RTP Media Congestion Avoidance Techniques (RMCAT), etc.). Indeed, between the present document being drafted and published, the following Scalable congestion controls were implemented: Prague over TCP and QUIC [PRAGUE-CC] [PragueLinux], an L4S variant of the RMCAT SCReAM controller [SCReAM-L4S], and the L4S ECN part of Bottleneck Bandwidth and Round-trip propagation time (BBRv2) [BBRv2] intended for TCP and QUIC transports. 2) Network: L4S traffic needs to be isolated from the queuing latency of Classic traffic. One queue per application flow (FQ) is one way to achieve this, e.g., FQ-CoDel [RFC8290]. However, using just two queues is sufficient and does not require inspection of transport layer headers in the network, which is not always possible (see Section 5.2). With just two queues, it might seem impossible to know how much capacity to schedule for each queue without inspecting how many flows at any one time are using each. And it would be undesirable to arbitrarily divide access network capacity into two partitions. The Dual-Queue Coupled AQM was developed as a minimal complexity solution to this problem. It acts like a 'semi-permeable' membrane that partitions latency but not bandwidth. As such, the two queues are for transitioning from Classic to L4S behaviour, not bandwidth prioritization. Section 4 gives a high-level explanation of how the per-flow queue (FQ) and DualQ variants of L4S work, and [RFC9332] gives a full explanation of the DualQ Coupled AQM framework. A specific marking algorithm is not mandated for L4S AQMs. Appendices of [RFC9332] give non-normative examples that have been implemented and evaluated and give recommended default parameter settings. It is expected that L4S experiments will improve knowledge of parameter settings and whether the set of marking algorithms needs to be limited. 3) Protocol: A sending host needs to distinguish L4S and Classic packets with an identifier so that the network can classify them into their separate treatments. The L4S identifier spec [RFC9331] concludes that all alternatives involve compromises, but the ECT(1) and Congestion Experienced (CE) codepoints of the ECN field represent a workable solution. As already explained, the network also uses ECN to immediately signal the very start of queue growth to the transport. 3. Terminology Classic Congestion Control: A congestion control behaviour that can coexist with standard Reno [RFC5681] without causing significantly negative impact on its flow rate [RFC5033]. The scaling problem with Classic congestion control is explained, with examples, in Section 5.1 and in [RFC3649]. Scalable Congestion Control: A congestion control where the average time from one congestion signal to the next (the recovery time) remains invariant as flow rate scales, all other factors being equal. For instance, DCTCP averages 2 congestion signals per round trip, whatever the flow rate, as do other recently developed Scalable congestion controls, e.g., Relentless TCP [RELENTLESS], Prague for TCP and QUIC [PRAGUE-CC] [PragueLinux], BBRv2 [BBRv2] [BBR-CC], and the L4S variant of SCReAM for real-time media [SCReAM-L4S] [RFC8298]. See Section 4.3 of [RFC9331] for more explanation. Classic Service: The Classic service is intended for all the congestion control behaviours that coexist with Reno [RFC5681] (e.g., Reno itself, CUBIC [RFC8312], Compound [CTCP], and TFRC [RFC5348]). The term 'Classic queue' means a queue providing the Classic service. Low Latency, Low Loss, and Scalable throughput (L4S) service: The 'L4S' service is intended for traffic from Scalable congestion control algorithms, such as the Prague congestion control [PRAGUE-CC], which was derived from DCTCP [RFC8257]. The L4S service is for more general traffic than just Prague -- it allows the set of congestion controls with similar scaling properties to Prague to evolve, such as the examples listed above (Relentless, SCReAM, etc.). The term 'L4S queue' means a queue providing the L4S service. The terms Classic or L4S can also qualify other nouns, such as 'queue', 'codepoint', 'identifier', 'classification', 'packet', and 'flow'. For example, an L4S packet means a packet with an L4S identifier sent from an L4S congestion control. Both Classic and L4S services can cope with a proportion of unresponsive or less-responsive traffic as well but, in the L4S case, its rate has to be smooth enough or low enough to not build a queue (e.g., DNS, Voice over IP (VoIP), game sync datagrams, etc.). Reno-friendly: The subset of Classic traffic that is friendly to the standard Reno congestion control defined for TCP in [RFC5681]. The TFRC spec [RFC5348] indirectly implies that 'friendly' is defined as \"generally within a factor of two of the sending rate of a TCP flow under the same conditions\". Reno-friendly is used here in place of 'TCP-friendly', given the latter has become imprecise, because the TCP protocol is now used with so many different congestion control behaviours, and Reno is used in non- TCP transports, such as QUIC [RFC9000]. Classic ECN: The original Explicit Congestion Notification (ECN) protocol [RFC3168] that requires ECN signals to be treated as equivalent to drops, both when generated in the network and when responded to by the sender. For L4S, the names used for the four codepoints of the 2-bit IP- ECN field are unchanged from those defined in the ECN spec [RFC3168], i.e., Not-ECT, ECT(0), ECT(1), and CE, where ECT stands for ECN-Capable Transport and CE stands for Congestion Experienced. A packet marked with the CE codepoint is termed 'ECN-marked' or sometimes just 'marked' where the context makes ECN obvious. Site: A home, mobile device, small enterprise, or campus where the network bottleneck is typically the access link to the site. Not all network arrangements fit this model, but it is a useful, widely applicable generalization. Traffic Policing: Limiting traffic by dropping packets or shifting them to a lower service class (as opposed to introducing delay, which is termed 'traffic shaping'). Policing can involve limiting the average rate and/or burst size. Policing focused on limiting queuing but not the average flow rate is termed 'congestion policing', 'latency policing', 'burst policing', or 'queue protection' in this document. Otherwise, the term rate policing is used. 4. L4S Architecture Components The L4S architecture is composed of the elements in the following three subsections. 4.1. Protocol Mechanisms The L4S architecture involves: a) unassignment of the previous use of the identifier; b) reassignment of the same identifier; and c) optional further identifiers: a. An essential aspect of a Scalable congestion control is the use of explicit congestion signals. Classic ECN [RFC3168] requires an ECN signal to be treated as equivalent to drop, both when it is generated in the network and when it is responded to by hosts. L4S needs networks and hosts to support a more fine-grained meaning for each ECN signal that is less severe than a drop, so that the L4S signals: * can be much more frequent and * can be signalled immediately, without the significant delay required to smooth out fluctuations in the queue. To enable L4S, the Standards Track Classic ECN spec [RFC3168] has had to be updated to allow L4S packets to depart from the 'equivalent-to-drop' constraint. [RFC8311] is a Standards Track update to relax specific requirements in [RFC3168] (and certain other Standards Track RFCs), which clears the way for the experimental changes proposed for L4S. Also, the ECT(1) codepoint was previously assigned as the experimental ECN nonce [RFC3540], which [RFC8311] recategorizes as historic to make the codepoint available again. b. [RFC9331] specifies that ECT(1) is used as the identifier to classify L4S packets into a separate treatment from Classic packets. This satisfies the requirement for identifying an alternative ECN treatment in [RFC4774]. The CE codepoint is used to indicate Congestion Experienced by both L4S and Classic treatments. This raises the concern that a Classic AQM earlier on the path might have marked some ECT(0) packets as CE. Then, these packets will be erroneously classified into the L4S queue. Appendix B of [RFC9331] explains why five unlikely eventualities all have to coincide for this to have any detrimental effect, which even then would only involve a vanishingly small likelihood of a spurious retransmission. c. A network operator might wish to include certain unresponsive, non-L4S traffic in the L4S queue if it is deemed to be paced smoothly enough and at a low enough rate not to build a queue, for instance, VoIP, low rate datagrams to sync online games, relatively low rate application-limited traffic, DNS, Lightweight Directory Access Protocol (LDAP), etc. This traffic would need to be tagged with specific identifiers, e.g., a low-latency Diffserv codepoint such as Expedited Forwarding (EF) [RFC3246], Non-Queue-Building (NQB) [NQB-PHB], or operator-specific identifiers. 4.2. Network Components The L4S architecture aims to provide low latency without the _need_ for per-flow operations in network components. Nonetheless, the architecture does not preclude per-flow solutions. The following bullets describe the known arrangements: a) the DualQ Coupled AQM with an L4S AQM in one queue coupled from a Classic AQM in the other; b) per-flow queues with an instance of a Classic and an L4S AQM in each queue; and c) Dual queues with per-flow AQMs but no per-flow queues: a. The Dual-Queue Coupled AQM (illustrated in Figure 1) achieves the 'semi-permeable' membrane property mentioned earlier as follows: * Latency isolation: Two separate queues are used to isolate L4S queuing delay from the larger queue that Classic traffic needs to maintain full utilization. * Bandwidth pooling: The two queues act as if they are a single pool of bandwidth in which flows of either type get roughly equal throughput without the scheduler needing to identify any flows. This is achieved by having an AQM in each queue, but the Classic AQM provides a congestion signal to both queues in a manner that ensures a consistent response from the two classes of congestion control. Specifically, the Classic AQM generates a drop/mark probability based on congestion in its own queue, which it uses both to drop/mark packets in its own queue and to affect the marking probability in the L4S queue. The strength of the coupling of the congestion signalling between the two queues is enough to make the L4S flows slow down to leave the right amount of capacity for the Classic flows (as they would if they were the same type of traffic sharing the same queue). Then, the scheduler can serve the L4S queue with priority (denoted by the '1' on the higher priority input), because the L4S traffic isn't offering up enough traffic to use all the priority that it is given. Therefore: * for latency isolation on short timescales (sub-round-trip), the prioritization of the L4S queue protects its low latency by allowing bursts to dissipate quickly; * but for bandwidth pooling on longer timescales (round-trip and longer), the Classic queue creates an equal and opposite pressure against the L4S traffic to ensure that neither has priority when it comes to bandwidth -- the tension between prioritizing L4S and coupling the marking from the Classic AQM results in approximate per-flow fairness. To protect against the prioritization of persistent L4S traffic deadlocking the Classic queue for a while in some implementations, it is advisable for the priority to be conditional, not strict (see Appendix A of the DualQ spec [RFC9332]). When there is no Classic traffic, the L4S queue's own AQM comes into play. It starts congestion marking with a very shallow queue, so L4S traffic maintains very low queuing delay. If either queue becomes persistently overloaded, drop of some ECN-capable packets is introduced, as recommended in Section 7 of the ECN spec [RFC3168] and Section 4.2.1 of the AQM recommendations [RFC7567]. The trade-offs with different approaches are discussed in Section 4.2.3 of the DualQ spec [RFC9332] (not shown in the figure here). The Dual-Queue Coupled AQM has been specified as generically as possible [RFC9332] without specifying the particular AQMs to use in the two queues so that designers are free to implement diverse ideas. Informational appendices in that document give pseudocode examples of two different specific AQM approaches: one called DualPI2 (pronounced Dual PI Squared) [DualPI2Linux] that uses the PI2 variant of PIE and a zero-config variant of Random Early Detection (RED) called Curvy RED. A DualQ Coupled AQM based on PIE has also been specified and implemented for Low Latency DOCSIS [DOCSIS3.1]. (3) (2) .-------^------..------------^------------------. ,-(1)-----. _____ ; ________ : L4S -------.| :|Scalable| : _\\ ||__\\_|mark:| sender: __________ / / || / |_____|\\ _________ :|________|\\;|/ ------ ^ \\1|condit'nl| `--------\\_| IP-ECNCoupling : \\|priority |_\\ ________ / |Classifier| : /|scheduler| / |Classic |/ |__________|\\ -------. __:__ / |_________|sender\\_\\ ||||__\\_|mark/|/ |________| / |||| / |dropClassic ------ |_____| (1) Scalable sending host (2) Isolation in separate network queues (3) Packet identification protocol Figure 1: Components of an L4S DualQ Coupled AQM Solution b. Per-Flow Queues and AQMs: A scheduler with per-flow queues, such as FQ-CoDel or FQ-PIE, can be used for L4S. For instance, within each queue of an FQ-CoDel system, as well as a CoDel AQM, there is typically also the option of ECN marking at an immediate (unsmoothed) shallow threshold to support use in data centres (see Section 5.2.7 of the FQ-CoDel spec [RFC8290]). In Linux, this has been modified so that the shallow threshold can be solely applied to ECT(1) packets [FQ_CoDel_Thresh]. Then, if there is a flow of Not-ECT or ECT(0) packets in the per-flow queue, the Classic AQM (e.g., CoDel) is applied; whereas, if there is a flow of ECT(1) packets in the queue, the shallower (typically sub-millisecond) threshold is applied. In addition, ECT(0) and Not-ECT packets could potentially be classified into a separate flow queue from ECT(1) and CE packets to avoid them mixing if they share a common flow identifier (e.g., in a VPN). c. Dual queues but per-flow AQMs: It should also be possible to use dual queues for isolation but with per-flow marking to control flow rates (instead of the coupled per-queue marking of the Dual- Queue Coupled AQM). One of the two queues would be for isolating L4S packets, which would be classified by the ECN codepoint. Flow rates could be controlled by flow-specific marking. The policy goal of the marking could be to differentiate flow rates (e.g., [Nadas20], which requires additional signalling of a per- flow 'value') or to equalize flow rates (perhaps in a similar way to Approx Fair CoDel [AFCD] [CODEL-APPROX-FAIR] but with two queues not one). Note that, whenever the term 'DualQ' is used loosely without saying whether marking is per queue or per flow, it means a dual- queue AQM with per-queue marking. 4.3. Host Mechanisms The L4S architecture includes two main mechanisms in the end host that we enumerate next: a. Scalable congestion control at the sender: Section 2 defines a Scalable congestion control as one where the average time from one congestion signal to the next (the recovery time) remains invariant as flow rate scales, all other factors being equal. DCTCP is the most widely used example. It has been documented as an informational record of the protocol currently in use in controlled environments [RFC8257]. A list of safety and performance improvements for a Scalable congestion control to be usable on the public Internet has been drawn up (see the so- called 'Prague L4S requirements' in Appendix A of [RFC9331]). The subset that involve risk of harm to others have been captured as normative requirements in Section 4 of [RFC9331]. TCP Prague [PRAGUE-CC] has been implemented in Linux as a reference implementation to address these requirements [PragueLinux]. Transport protocols other than TCP use various congestion controls that are designed to be friendly with Reno. Before they can use the L4S service, they will need to be updated to implement a Scalable congestion response, which they will have to indicate by using the ECT(1) codepoint. Scalable variants are under consideration for more recent transport protocols (e.g., QUIC), and the L4S ECN part of BBRv2 [BBRv2] [BBR-CC] is a Scalable congestion control intended for the TCP and QUIC transports, amongst others. Also, an L4S variant of the RMCAT SCReAM controller [RFC8298] has been implemented [SCReAM-L4S] for media transported over RTP. Section 4.3 of the L4S ECN spec [RFC9331] defines Scalable congestion control in more detail and specifies the requirements that an L4S Scalable congestion control has to comply with. b. The ECN feedback in some transport protocols is already sufficiently fine-grained for L4S (specifically DCCP [RFC4340] and QUIC [RFC9000]). But others either require updates or are in the process of being updated: * For the case of TCP, the feedback protocol for ECN embeds the assumption from Classic ECN [RFC3168] that an ECN mark is equivalent to a drop, making it unusable for a Scalable TCP. Therefore, the implementation of TCP receivers will have to be upgraded [RFC7560]. Work to standardize and implement more accurate ECN feedback for TCP (AccECN) is in progress [ACCECN] [PragueLinux]. * ECN feedback was only roughly sketched in the appendix of the now obsoleted second specification of SCTP [RFC4960], while a fuller specification was proposed in a long-expired document [ECN-SCTP]. A new design would need to be implemented and deployed before SCTP could support L4S. * For RTP, sufficient ECN feedback was defined in [RFC6679], but [RFC8888] defines the latest Standards Track improvements. 5. Rationale 5.1. Why These Primary Components? Explicit congestion signalling (protocol): Explicit congestion signalling is a key part of the L4S approach. In contrast, use of drop as a congestion signal creates tension because drop is both an impairment (less would be better) and a useful signal (more would be better): * Explicit congestion signals can be used many times per round trip to keep tight control without any impairment. Under heavy load, even more explicit signals can be applied so that the queue can be kept short whatever the load. In contrast, Classic AQMs have to introduce very high packet drop at high load to keep the queue short. By using ECN, an L4S congestion control's sawtooth reduction can be smaller and therefore return to the operating point more often, without worrying that more sawteeth will cause more signals. The consequent smaller amplitude sawteeth fit between an empty queue and a very shallow marking threshold (~1 ms in the public Internet), so queue delay variation can be very low, without risk of underutilization. * Explicit congestion signals can be emitted immediately to track fluctuations of the queue. L4S shifts smoothing from the network to the host. The network doesn't know the round-trip times (RTTs) of any of the flows. So if the network is responsible for smoothing (as in the Classic approach), it has to assume a worst case RTT, otherwise long RTT flows would become unstable. This delays Classic congestion signals by 100-200 ms. In contrast, each host knows its own RTT. So, in the L4S approach, the host can smooth each flow over its own RTT, introducing no more smoothing delay than strictly necessary (usually only a few milliseconds). A host can also choose not to introduce any smoothing delay if appropriate, e.g., during flow start-up. Neither of the above are feasible if explicit congestion signalling has to be considered 'equivalent to drop' (as was required with Classic ECN [RFC3168]), because drop is an impairment as well as a signal. So drop cannot be excessively frequent, and drop cannot be immediate; otherwise, too many drops would turn out to have been due to only a transient fluctuation in the queue that would not have warranted dropping a packet in hindsight. Therefore, in an L4S AQM, the L4S queue uses a new L4S variant of ECN that is not equivalent to drop (see Section 5.2 of the L4S ECN spec [RFC9331]), while the Classic queue uses either Classic ECN [RFC3168] or drop, which are still equivalent to each other. Before Classic ECN was standardized, there were various proposals to give an ECN mark a different meaning from drop. However, there was no particular reason to agree on any one of the alternative meanings, so 'equivalent to drop' was the only compromise that could be reached. [RFC3168] contains a statement that: An environment where all end nodes were ECN-Capable could allow new criteria to be developed for setting the CE codepoint, and new congestion control mechanisms for end-node reaction to CE packets. However, this is a research issue, and as such is not addressed in this document. Latency isolation (network): L4S congestion controls keep queue delay low, whereas Classic congestion controls need a queue of the order of the RTT to avoid underutilization. One queue cannot have two lengths; therefore, L4S traffic needs to be isolated in a separate queue (e.g., DualQ) or queues (e.g., FQ). Coupled congestion notification: Coupling the congestion notification between two queues as in the DualQ Coupled AQM is not necessarily essential, but it is a simple way to allow senders to determine their rate packet by packet, rather than be overridden by a network scheduler. An alternative is for a network scheduler to control the rate of each application flow (see the discussion in Section 5.2). L4S packet identifier (protocol): Once there are at least two treatments in the network, hosts need an identifier at the IP layer to distinguish which treatment they intend to use. Scalable congestion notification: A Scalable congestion control in the host keeps the signalling frequency from the network high, whatever the flow rate, so that queue delay variations can be small when conditions are stable, and rate can track variations in available capacity as rapidly as possible otherwise. Low loss: Latency is not the only concern of L4S. The 'Low Loss' part of the name denotes that L4S generally achieves zero congestion loss due to its use of ECN. Otherwise, loss would itself cause delay, particularly for short flows, due to retransmission delay [RFC2884]. Scalable throughput: The 'Scalable throughput' part of the name denotes that the per-flow throughput of Scalable congestion controls should scale indefinitely, avoiding the imminent scaling problems with Reno-friendly congestion control algorithms [RFC3649]. It was known when TCP congestion avoidance was first developed in 1988 that it would not scale to high bandwidth-delay products (see footnote 6 in [TCP-CA]). Today, regular broadband flow rates over WAN distances are already beyond the scaling range of Classic Reno congestion control. So 'less unscalable' CUBIC [RFC8312] and Compound [CTCP] variants of TCP have been successfully deployed. However, these are now approaching their scaling limits. For instance, we will consider a scenario with a maximum RTT of 30 ms at the peak of each sawtooth. As Reno packet rate scales 8 times from 1,250 to 10,000 packet/s (from 15 to 120 Mb/s with 1500 B packets), the time to recover from a congestion event rises proportionately by 8 times as well, from 422 ms to 3.38 s. It is clearly problematic for a congestion control to take multiple seconds to recover from each congestion event. CUBIC [RFC8312] was developed to be less unscalable, but it is approaching its scaling limit; with the same max RTT of 30 ms, at 120 Mb/s, CUBIC is still fully in its Reno-friendly mode, so it takes about 4.3 s to recover. However, once flow rate scales by 8 times again to 960 Mb/s it enters true CUBIC mode, with a recovery time of 12.2 s. From then on, each further scaling by 8 times doubles CUBIC's recovery time (because the cube root of 8 is 2), e.g., at 7.68 Gb/ s, the recovery time is 24.3 s. In contrast, a Scalable congestion control like DCTCP or Prague induces 2 congestion signals per round trip on average, which remains invariant for any flow rate, keeping dynamic control very tight. For a feel of where the global average lone-flow download sits on this scale at the time of writing (2021), according to [BDPdata], the global average fixed access capacity was 103 Mb/s in 2020 and the average base RTT to a CDN was 25 to 34 ms in 2019. Averaging of per-country data was weighted by Internet user population (data collected globally is necessarily of variable quality, but the paper does double-check that the outcome compares well against a second source). So a lone CUBIC flow would at best take about 200 round trips (5 s) to recover from each of its sawtooth reductions, if the flow even lasted that long. This is described as 'at best' because it assumes everyone uses an AQM, whereas in reality, most users still have a (probably bloated) tail-drop buffer. In the tail-drop case, the likely average recovery time would be at least 4 times 5 s, if not more, because RTT under load would be at least double that of an AQM, and the recovery time of Reno-friendly flows depends on the square of RTT. Although work on scaling congestion controls tends to start with TCP as the transport, the above is not intended to exclude other transports (e.g., SCTP and QUIC) or less elastic algorithms (e.g., RMCAT), which all tend to adopt the same or similar developments. 5.2. What L4S Adds to Existing Approaches All the following approaches address some part of the same problem space as L4S. In each case, it is shown that L4S complements them or improves on them, rather than being a mutually exclusive alternative: Diffserv: Diffserv addresses the problem of bandwidth apportionment for important traffic as well as queuing latency for delay- sensitive traffic. Of these, L4S solely addresses the problem of queuing latency. Diffserv will still be necessary where important traffic requires priority (e.g., for commercial reasons or for protection of critical infrastructure traffic) -- see [L4S-DIFFSERV]. Nonetheless, the L4S approach can provide low latency for all traffic within each Diffserv class (including the case where there is only the one default Diffserv class). Also, Diffserv can only provide a latency benefit if a small subset of the traffic on a bottleneck link requests low latency. As already explained, it has no effect when all the applications in use at one time at a single site (e.g., a home, small business, or mobile device) require low latency. In contrast, because L4S works for all traffic, it needs none of the management baggage (traffic policing or traffic contracts) associated with favouring some packets over others. This lack of management baggage ought to give L4S a better chance of end-to-end deployment. In particular, if networks do not trust end systems to identify which packets should be favoured, they assign packets to Diffserv classes themselves. However, the techniques available to such networks, like inspection of flow identifiers or deeper inspection of application signatures, do not always sit well with encryption of the layers above IP [RFC8404]. In these cases, users can have either privacy or Quality of Service (QoS), but not both. As with Diffserv, the L4S identifier is in the IP header. But, in contrast to Diffserv, the L4S identifier does not convey a want or a need for a certain level of quality. Rather, it promises a certain behaviour (Scalable congestion response), which networks can objectively verify if they need to. This is because low delay depends on collective host behaviour, whereas bandwidth priority depends on network behaviour. State-of-the-art AQMs: AQMs for Classic traffic, such as PIE and FQ- CoDel, give a significant reduction in queuing delay relative to no AQM at all. L4S is intended to complement these AQMs and should not distract from the need to deploy them as widely as possible. Nonetheless, AQMs alone cannot reduce queuing delay too far without significantly reducing link utilization, because the root cause of the problem is on the host -- where Classic congestion controls use large sawtoothing rate variations. The L4S approach resolves this tension between delay and utilization by enabling hosts to minimize the amplitude of their sawteeth. A single-queue Classic AQM is not sufficient to allow hosts to use small sawteeth for two reasons: i) smaller sawteeth would not get lower delay in an AQM designed for larger amplitude Classic sawteeth, because a queue can only have one length at a time and ii) much smaller sawteeth implies much more frequent sawteeth, so L4S flows would drive a Classic AQM into a high level of ECN- marking, which would appear as heavy congestion to Classic flows, which in turn would greatly reduce their rate as a result (see Section 6.4.4). Per-flow queuing or marking: Similarly, per-flow approaches, such as FQ-CoDel or Approx Fair CoDel [AFCD], are not incompatible with the L4S approach. However, per-flow queuing alone is not enough -- it only isolates the queuing of one flow from others, not from itself. Per-flow implementations need to have support for Scalable congestion control added, which has already been done for FQ-CoDel in Linux (see Section 5.2.7 of [RFC8290] and [FQ_CoDel_Thresh]). Without this simple modification, per-flow AQMs, like FQ-CoDel, would still not be able to support applications that need both very low delay and high bandwidth, e.g., video-based control of remote procedures or interactive cloud-based video (see Note 1 below). Although per-flow techniques are not incompatible with L4S, it is important to have the DualQ alternative. This is because handling end-to-end (layer 4) flows in the network (layer 3 or 2) precludes some important end-to-end functions. For instance: a. Per-flow forms of L4S, like FQ-CoDel, are incompatible with full end-to-end encryption of transport layer identifiers for privacy and confidentiality (e.g., IPsec or encrypted VPN tunnels, as opposed to DTLS over UDP), because they require packet inspection to access the end-to-end transport flow identifiers. In contrast, the DualQ form of L4S requires no deeper inspection than the IP layer. So as long as operators take the DualQ approach, their users can have both very low queuing delay and full end-to-end encryption [RFC8404]. b. With per-flow forms of L4S, the network takes over control of the relative rates of each application flow. Some see it as an advantage that the network will prevent some flows running faster than others. Others consider it an inherent part of the Internet's appeal that applications can control their rate while taking account of the needs of others via congestion signals. They maintain that this has allowed applications with interesting rate behaviours to evolve, for instance: i) a variable bit-rate video that varies around an equal share, rather than being forced to remain equal at every instant or ii) end-to-end scavenger behaviours [RFC6817] that use less than an equal share of capacity [LEDBAT_AQM]. The L4S architecture does not require the IETF to commit to one approach over the other, because it supports both so that the 'market' can decide. Nonetheless, in the spirit of 'Do one thing and do it well' [McIlroy78], the DualQ option provides low delay without prejudging the issue of flow-rate control. Then, flow rate policing can be added separately if desired. In contrast to scheduling, a policer would allow application control up to a point, but the network would still be able to set the point at which it intervened to prevent one flow completely starving another. Note: 1. It might seem that self-inflicted queuing delay within a per- flow queue should not be counted, because if the delay wasn't in the network, it would just shift to the sender. However, modern adaptive applications, e.g., HTTP/2 [RFC9113] or some interactive media applications (see Section 6.1), can keep low latency objects at the front of their local send queue by shuffling priorities of other objects dependent on the progress of other transfers (for example, see [lowat]). They cannot shuffle objects once they have released them into the network. Alternative Back-off ECN (ABE): Here again, L4S is not an alternative to ABE but a complement that introduces much lower queuing delay. ABE [RFC8511] alters the host behaviour in response to ECN marking to utilize a link better and give ECN flows faster throughput. It uses ECT(0) and assumes the network still treats ECN and drop the same. Therefore, ABE exploits any lower queuing delay that AQMs can provide. But, as explained above, AQMs still cannot reduce queuing delay too much without losing link utilization (to allow for other, non-ABE, flows). BBR: Bottleneck Bandwidth and Round-trip propagation time (BBR) [BBR-CC] controls queuing delay end-to-end without needing any special logic in the network, such as an AQM. So it works pretty much on any path. BBR keeps queuing delay reasonably low, but perhaps not quite as low as with state-of-the-art AQMs, such as PIE or FQ-CoDel, and certainly nowhere near as low as with L4S. Queuing delay is also not consistently low, due to BBR's regular bandwidth probing spikes and its aggressive flow start-up phase. L4S complements BBR. Indeed, BBRv2 can use L4S ECN where available and a Scalable L4S congestion control behaviour in response to any ECN signalling from the path [BBRv2]. The L4S ECN signal complements the delay-based congestion control aspects of BBR with an explicit indication that hosts can use, both to converge on a fair rate and to keep below a shallow queue target set by the network. Without L4S ECN, both these aspects need to be assumed or estimated. 6. Applicability 6.1. Applications A transport layer that solves the current latency issues will provide new service, product, and application opportunities. With the L4S approach, the following existing applications also experience significantly better quality of experience under load: * gaming, including cloud-based gaming; * VoIP; * video conferencing; * web browsing; * (adaptive) video streaming; and * instant messaging. The significantly lower queuing latency also enables some interactive application functions to be offloaded to the cloud that would hardly even be usable today, including: * cloud-based interactive video and * cloud-based virtual and augmented reality. The above two applications have been successfully demonstrated with L4S, both running together over a 40 Mb/s broadband access link loaded up with the numerous other latency-sensitive applications in the previous list, as well as numerous downloads, with all sharing the same bottleneck queue simultaneously [L4Sdemo16] [L4Sdemo16-Video]. For the former, a panoramic video of a football stadium could be swiped and pinched so that, on the fly, a proxy in the cloud could generate a sub-window of the match video under the finger-gesture control of each user. For the latter, a virtual reality headset displayed a viewport taken from a 360-degree camera in a racing car. The user's head movements controlled the viewport extracted by a cloud-based proxy. In both cases, with a 7 ms end-to- end base delay, the additional queuing delay of roughly 1 ms was so low that it seemed the video was generated locally. Using a swiping finger gesture or head movement to pan a video are extremely latency-demanding actions -- far more demanding than VoIP -- because human vision can detect extremely low delays of the order of single milliseconds when delay is translated into a visual lag between a video and a reference point (the finger or the orientation of the head sensed by the balance system in the inner ear, i.e., the vestibular system). With an alternative AQM, the video noticeably lagged behind the finger gestures and head movements. Without the low queuing delay of L4S, cloud-based applications like these would not be credible without significantly more access-network bandwidth (to deliver all possible areas of the video that might be viewed) and more local processing, which would increase the weight and power consumption of head-mounted displays. When all interactive processing can be done in the cloud, only the data to be rendered for the end user needs to be sent. Other low latency high bandwidth applications, such as: * interactive remote presence and * video-assisted remote control of machinery or industrial processes are not credible at all without very low queuing delay. No amount of extra access bandwidth or local processing can make up for lost time. 6.2. Use Cases The following use cases for L4S are being considered by various interested parties: * where the bottleneck is one of various types of access network, e.g., DSL, Passive Optical Networks (PONs), DOCSIS cable, mobile, satellite; or where it's a Wi-Fi link (see Section 6.3 for some technology-specific details) * private networks of heterogeneous data centres, where there is no single administrator that can arrange for all the simultaneous changes to senders, receivers, and networks needed to deploy DCTCP: - a set of private data centres interconnected over a wide area with separate administrations but within the same company - a set of data centres operated by separate companies interconnected by a community of interest network (e.g., for the finance sector) - multi-tenant (cloud) data centres where tenants choose their operating system stack (Infrastructure as a Service (IaaS)) * different types of transport (or application) congestion control: - elastic (TCP/SCTP); - real-time (RTP, RMCAT); and - query-response (DNS/LDAP). * where low delay QoS is required but without inspecting or intervening above the IP layer [RFC8404]: - Mobile and other networks have tended to inspect higher layers in order to guess application QoS requirements. However, with growing demand for support of privacy and encryption, L4S offers an alternative. There is no need to select which traffic to favour for queuing when L4S can give favourable queuing to all traffic. * If queuing delay is minimized, applications with a fixed delay budget can communicate over longer distances or via more circuitous paths, e.g., longer chains of service functions [RFC7665] or of onion routers. * If delay jitter is minimized, it is possible to reduce the dejitter buffers on the receiving end of video streaming, which should improve the interactive experience. 6.3. Applicability with Specific Link Technologies Certain link technologies aggregate data from multiple packets into bursts and buffer incoming packets while building each burst. Wi-Fi, PON, and cable all involve such packet aggregation, whereas fixed Ethernet and DSL do not. No sender, whether L4S or not, can do anything to reduce the buffering needed for packet aggregation. So an AQM should not count this buffering as part of the queue that it controls, given no amount of congestion signals will reduce it. Certain link technologies also add buffering for other reasons, specifically: * Radio links (cellular, Wi-Fi, or satellite) that are distant from the source are particularly challenging. The radio link capacity can vary rapidly by orders of magnitude, so it is considered desirable to hold a standing queue that can utilize sudden increases of capacity. * Cellular networks are further complicated by a perceived need to buffer in order to make hand-overs imperceptible. L4S cannot remove the need for all these different forms of buffering. However, by removing 'the longest pole in the tent' (buffering for the large sawteeth of Classic congestion controls), L4S exposes all these 'shorter poles' to greater scrutiny. Until now, the buffering needed for these additional reasons tended to be over-specified -- with the excuse that none were 'the longest pole in the tent'. But having removed the 'longest pole', it becomes worthwhile to minimize them, for instance, reducing packet aggregation burst sizes and MAC scheduling intervals. Also, certain link types, particularly radio-based links, are far more prone to transmission losses. Section 6.4.3 explains how an L4S response to loss has to be as drastic as a Classic response. Nonetheless, research referred to in the same section has demonstrated potential for considerably more effective loss repair at the link layer, due to the relaxed ordering constraints of L4S packets. 6.4. Deployment Considerations L4S AQMs, whether DualQ [RFC9332] or FQ [RFC8290], are in themselves an incremental deployment mechanism for L4S -- so that L4S traffic can coexist with existing Classic (Reno-friendly) traffic. Section 6.4.1 explains why only deploying an L4S AQM in one node at each end of the access link will realize nearly all the benefit of L4S. L4S involves both the network and end systems, so Section 6.4.2 suggests some typical sequences to deploy each part and why there will be an immediate and significant benefit after deploying just one part. Sections 6.4.3 and 6.4.4 describe the converse incremental deployment case where there is no L4S AQM at the network bottleneck, so any L4S flow traversing this bottleneck has to take care in case it is competing with Classic traffic. 6.4.1. Deployment Topology L4S AQMs will not have to be deployed throughout the Internet before L4S can benefit anyone. Operators of public Internet access networks typically design their networks so that the bottleneck will nearly always occur at one known (logical) link. This confines the cost of queue management technology to one place. The case of mesh networks is different and will be discussed later in this section. However, the known-bottleneck case is generally true for Internet access to all sorts of different 'sites', where the word 'site' includes home networks, small- to medium-sized campus or enterprise networks and even cellular devices (Figure 2). Also, this known-bottleneck case tends to be applicable whatever the access link technology, whether xDSL, cable, PON, cellular, line of sight wireless, or satellite. Therefore, the full benefit of the L4S service should be available in the downstream direction when an L4S AQM is deployed at the ingress to this bottleneck link. And similarly, the full upstream service will typically be available once an L4S AQM is deployed at the ingress into the upstream link. (Of course, multihomed sites would only see the full benefit once all their access links were covered.)______( ) __ __ ( ) |DQ\\________/DQ|( enterprise ) ___ |__/ \\__| ( /campus ) ( ) (______) ( ) ___||_ +----+ ( ) __ __ / \\DC |-----( Core )|DQ\\_______________/DQ|| home+----+ ( ) |__/ \\__||______| (_____) __ |DQ\\__/\\ __ ,===. |__/ \\ ____/DQ||| ||mobile\\/ \\__|||_||device| o |`-- Figure 2: Likely Location of DualQ (DQ) Deployments in Common Access Topologies Deployment in mesh topologies depends on how overbooked the core is. If the core is non-blocking, or at least generously provisioned so that the edges are nearly always the bottlenecks, it would only be necessary to deploy an L4S AQM at the edge bottlenecks. For example, some data-centre networks are designed with the bottleneck in the hypervisor or host Network Interface Controllers (NICs), while others bottleneck at the top-of-rack switch (both the output ports facing hosts and those facing the core). An L4S AQM would often next be needed where the Wi-Fi links in a home sometimes become the bottleneck. Also an L4S AQM would eventually need to be deployed at any other persistent bottlenecks, such as network interconnections, e.g., some public Internet exchange points and the ingress and egress to WAN links interconnecting data centres. 6.4.2. Deployment Sequences For any one L4S flow to provide benefit, it requires three (or sometimes two) parts to have been deployed: i) the congestion control at the sender; ii) the AQM at the bottleneck; and iii) older transports (namely TCP) need upgraded receiver feedback too. This was the same deployment problem that ECN faced [RFC8170], so we have learned from that experience. Firstly, L4S deployment exploits the fact that DCTCP already exists on many Internet hosts (e.g., Windows, FreeBSD, and Linux), both servers and clients. Therefore, an L4S AQM can be deployed at a network bottleneck to immediately give a working deployment of all the L4S parts for testing, as long as the ECT(0) codepoint is switched to ECT(1). DCTCP needs some safety concerns to be fixed for general use over the public Internet (see Section 4.3 of the L4S ECN spec [RFC9331]), but DCTCP is not on by default, so these issues can be managed within controlled deployments or controlled trials. Secondly, the performance improvement with L4S is so significant that it enables new interactive services and products that were not previously possible. It is much easier for companies to initiate new work on deployment if there is budget for a new product trial. In contrast, if there were only an incremental performance improvement (as with Classic ECN), spending on deployment tends to be much harder to justify. Thirdly, the L4S identifier is defined so that network operators can initially enable L4S exclusively for certain customers or certain applications. However, this is carefully defined so that it does not compromise future evolution towards L4S as an Internet-wide service. This is because the L4S identifier is defined not only as the end-to- end ECN field, but it can also optionally be combined with any other packet header or some status of a customer or their access link (see Section 5.4 of [RFC9331]). Operators could do this anyway, even if it were not blessed by the IETF. However, it is best for the IETF to specify that, if they use their own local identifier, it must be in combination with the IETF's identifier, ECT(1). Then, if an operator has opted for an exclusive local-use approach, they only have to remove this extra rule later to make the service work across the Internet -- it will already traverse middleboxes, peerings, etc. +-+--------------------+----------------------+---------------------+| Servers or proxiesAccess linkClients+-+--------------------+----------------------+---------------------+ |0| DCTCP (existing)| DCTCP (existing)+-+--------------------+----------------------+---------------------+ |1| |Add L4S AQM downstream||WORKS DOWNSTREAM FOR CONTROLLED DEPLOYMENTS/TRIALS+-+--------------------+----------------------+---------------------+ |2| Upgrade DCTCP to|Replace DCTCP feedb'k|| TCP Prague| with AccECN|FULLY WORKS DOWNSTREAM+-+--------------------+----------------------+---------------------+|| Upgrade DCTCP to|3|Add L4S AQM upstreamTCP Prague|||| FULLY WORKS UPSTREAM AND DOWNSTREAM+-+--------------------+----------------------+---------------------+ Figure 3: Example L4S Deployment Sequence Figure 3 illustrates some example sequences in which the parts of L4S might be deployed. It consists of the following stages, preceded by a presumption that DCTCP is already installed at both ends: 1. DCTCP is not applicable for use over the public Internet, so it is emphasized here that any DCTCP flow has to be completely contained within a controlled trial environment. Within this trial environment, once an L4S AQM has been deployed, the trial DCTCP flow will experience immediate benefit, without any other deployment being needed. In this example, downstream deployment is first, but in other scenarios, the upstream might be deployed first. If no AQM at all was previously deployed for the downstream access, an L4S AQM greatly improves the Classic service (as well as adding the L4S service). If an AQM was already deployed, the Classic service will be unchanged (and L4S will add an improvement on top). 2. In this stage, the name 'TCP Prague' [PRAGUE-CC] is used to represent a variant of DCTCP that is designed to be used in a production Internet environment (that is, it has to comply with all the requirements in Section 4 of the L4S ECN spec [RFC9331], which then means it can be used over the public Internet). If the application is primarily unidirectional, 'TCP Prague' at the sending end will provide all the benefit needed, as long as the receiving end supports Accurate ECN (AccECN) feedback [ACCECN]. For TCP transports, AccECN feedback is needed at the other end, but it is a generic ECN feedback facility that is already planned to be deployed for other purposes, e.g., DCTCP and BBR. The two ends can be deployed in either order because, in TCP, an L4S congestion control only enables itself if it has negotiated the use of AccECN feedback with the other end during the connection handshake. Thus, deployment of TCP Prague on a server enables L4S trials to move to a production service in one direction, wherever AccECN is deployed at the other end. This stage might be further motivated by the performance improvements of TCP Prague relative to DCTCP (see Appendix A.2 of the L4S ECN spec [RFC9331]). Unlike TCP, from the outset, QUIC ECN feedback [RFC9000] has supported L4S. Therefore, if the transport is QUIC, one-ended deployment of a Prague congestion control at this stage is simple and sufficient. For QUIC, if a proxy sits in the path between multiple origin servers and the access bottlenecks to multiple clients, then upgrading the proxy with a Scalable congestion control would provide the benefits of L4S over all the clients' downstream bottlenecks in one go -- whether or not all the origin servers were upgraded. Conversely, where a proxy has not been upgraded, the clients served by it will not benefit from L4S at all in the downstream, even when any origin server behind the proxy has been upgraded to support L4S. For TCP, a proxy upgraded to support 'TCP Prague' would provide the benefits of L4S downstream to all clients that support AccECN (whether or not they support L4S as well). And in the upstream, the proxy would also support AccECN as a receiver, so that any client deploying its own L4S support would benefit in the upstream direction, irrespective of whether any origin server beyond the proxy supported AccECN. 3. This is a two-move stage to enable L4S upstream. An L4S AQM or TCP Prague can be deployed in either order as already explained. To motivate the first of two independent moves, the deferred benefit of enabling new services after the second move has to be worth it to cover the first mover's investment risk. As explained already, the potential for new interactive services provides this motivation. An L4S AQM also improves the upstream Classic service significantly if no other AQM has already been deployed. Note that other deployment sequences might occur. For instance, the upstream might be deployed first; a non-TCP protocol might be used end to end, e.g., QUIC and RTP; a body, such as the 3GPP, might require L4S to be implemented in 5G user equipment; or other random acts of kindness might arise. 6.4.3. L4S Flow but Non-ECN Bottleneck If L4S is enabled between two hosts, the L4S sender is required to coexist safely with Reno in response to any drop (see Section 4.3 of the L4S ECN spec [RFC9331]). Unfortunately, as well as protecting Classic traffic, this rule degrades the L4S service whenever there is any loss, even if the cause is not persistent congestion at a bottleneck, for example: * congestion loss at other transient bottlenecks, e.g., due to bursts in shallower queues; * transmission errors, e.g., due to electrical interference; and * rate policing. Three complementary approaches are in progress to address this issue, but they are all currently research: * In Prague congestion control, ignore certain losses deemed unlikely to be due to congestion (using some ideas from BBR [BBR-CC] regarding isolated losses). This could mask any of the above types of loss while still coexisting with drop-based congestion controls. * A combination of Recent Acknowledgement (RACK) [RFC8985], L4S, and link retransmission without resequencing could repair transmission errors without the head of line blocking delay usually associated with link-layer retransmission [UnorderedLTE] [RFC9331]. * Hybrid ECN/drop rate policers (see Section 8.3). L4S deployment scenarios that minimize these issues (e.g., over wireline networks) can proceed in parallel to this research, in the expectation that research success could continually widen L4S applicability. 6.4.4. L4S Flow but Classic ECN Bottleneck Classic ECN support is starting to materialize on the Internet as an increased level of CE marking. It is hard to detect whether this is all due to the addition of support for ECN in implementations of FQ- CoDel and/or FQ-COBALT, which is not generally problematic, because flow queue (FQ) scheduling inherently prevents a flow from exceeding the 'fair' rate irrespective of its aggressiveness. However, some of this Classic ECN marking might be due to single-queue ECN deployment. This case is discussed in Section 4.3 of the L4S ECN spec [RFC9331]. 6.4.5. L4S AQM Deployment within Tunnels An L4S AQM uses the ECN field to signal congestion. So in common with Classic ECN, if the AQM is within a tunnel or at a lower layer, correct functioning of ECN signalling requires standards-compliant propagation of the ECN field up the layers [RFC6040] [ECN-SHIM] [ECN-ENCAP]. 7. IANA Considerations This document has no IANA actions. 8. Security Considerations 8.1. Traffic Rate (Non-)Policing 8.1.1. (Non-)Policing Rate per Flow In the current Internet, ISPs usually enforce separation between the capacity of shared links assigned to different 'sites' (e.g., households, businesses, or mobile users -- see terminology in Section 3) using some form of scheduler [RFC0970]. And they use various techniques, like redirection to traffic scrubbing facilities, to deal with flooding attacks. However, there has never been a universal need to police the rate of individual application flows -- the Internet has generally always relied on self-restraint of congestion controls at senders for sharing intrasite' capacity. L4S has been designed not to upset this status quo. If a DualQ is used to provide L4S service, Section 4.2 of [RFC9332] explains how it is designed to give no more rate advantage to unresponsive flows than a single-queue AQM would, whether or not there is traffic overload. Also, in case per-flow rate policing is ever required, it can be added because it is orthogonal to the distinction between L4S and Classic. As explained in Section 5.2, the DualQ variant of L4S provides low delay without prejudging the issue of flow-rate control. So if flow-rate control is needed, per-flow queuing (FQ) with L4S support can be used instead, or flow rate policing can be added as a modular addition to a DualQ. However, per-flow rate control is not usually deployed as a security mechanism, because an active attacker can just shard its traffic over more flow identifiers if the rate of each is restricted. 8.1.2. (Non-)Policing L4S Service Rate Section 5.2 explains how Diffserv only makes a difference if some packets get less favourable treatment than others, which typically requires traffic rate policing for a low latency class. In contrast, it should not be necessary to rate-police access to the L4S service to protect the Classic service, because L4S is designed to reduce delay without harming the delay or rate of any Classic traffic. During early deployment (and perhaps always), some networks will not offer the L4S service. In general, these networks should not need to police L4S traffic. They are required (by both the ECN spec [RFC3168] and the L4S ECN spec [RFC9331]) not to change the L4S identifier, which would interfere with end-to-end congestion control. If they already treat ECN traffic as Not-ECT, they can merely treat L4S traffic as Not-ECT too. At a bottleneck, such networks will introduce some queuing and dropping. When a Scalable congestion control detects a drop, it will have to respond safely with respect to Classic congestion controls (as required in Section 4.3 of [RFC9331]). This will degrade the L4S service to be no better (but never worse) than Classic best efforts whenever a non-ECN bottleneck is encountered on a path (see Section 6.4.3). In cases that are expected to be rare, networks that solely support Classic ECN [RFC3168] in a single queue bottleneck might opt to police L4S traffic so as to protect competing Classic ECN traffic (for instance, see Section 6.1.3 of the L4S operational guidance [L4SOPS]). However, Section 4.3 of the L4S ECN spec [RFC9331] recommends that the sender adapts its congestion response to properly coexist with Classic ECN flows, i.e., reverting to the self-restraint approach. Certain network operators might choose to restrict access to the L4S service, perhaps only to selected premium customers as a value-added service. Their packet classifier (item 2 in Figure 1) could identify such customers against some other field (e.g., source address range), as well as classifying on the ECN field. If only the ECN L4S identifier matched, but not (say) the source address, the classifier could direct these packets (from non-premium customers) into the Classic queue. Explaining clearly how operators can use additional local classifiers (see Section 5.4 of [RFC9331]) is intended to remove any motivation to clear the L4S identifier. Then at least the L4S ECN identifier will be more likely to survive end to end, even though the service may not be supported at every hop. Such local arrangements would only require simple registered/not-registered packet classification, rather than the managed, application-specific traffic policing against customer-specific traffic contracts that Diffserv uses. 8.2. 'Latency Friendliness' Like the Classic service, the L4S service relies on self-restraint to limit the rate in response to congestion. In addition, the L4S service requires self-restraint in terms of limiting latency (burstiness). It is hoped that self-interest and guidance on dynamic behaviour (especially flow start-up, which might need to be standardized) will be sufficient to prevent transports from sending excessive bursts of L4S traffic, given the application's own latency will suffer most from such behaviour. Because the L4S service can reduce delay without discernibly increasing the delay of any Classic traffic, it should not be necessary to police L4S traffic to protect the delay of Classic traffic. However, whether burst policing becomes necessary to protect other L4S traffic remains to be seen. Without it, there will be potential for attacks on the low latency of the L4S service. If needed, various arrangements could be used to address this concern: Local bottleneck queue protection: A per-flow (5-tuple) queue protection function [DOCSIS-Q-PROT] has been developed for the low latency queue in DOCSIS, which has adopted the DualQ L4S architecture. It protects the low latency service from any queue- building flows that accidentally or maliciously classify themselves into the low latency queue. It is designed to score flows based solely on their contribution to queuing (not flow rate in itself). Then, if the shared low latency queue is at risk of exceeding a threshold, the function redirects enough packets of the highest scoring flow(s) into the Classic queue to preserve low latency. Distributed traffic scrubbing: Rather than policing locally at each bottleneck, it may only be necessary to address problems reactively, e.g., punitively target any deployments of new bursty malware, in a similar way to how traffic from flooding attack sources is rerouted via scrubbing facilities. Local bottleneck per-flow scheduling: Per-flow scheduling should inherently isolate non-bursty flows from bursty flows (see Section 5.2 for discussion of the merits of per-flow scheduling relative to per-flow policing). Distributed access subnet queue protection: Per-flow queue protection could be arranged for a queue structure distributed across a subnet intercommunicating using lower layer control messages (see Section 2.1.4 of [QDyn]). For instance, in a radio access network, user equipment already sends regular buffer status reports to a radio network controller, which could use this information to remotely police individual flows. Distributed Congestion Exposure to ingress policers: The Congestion Exposure (ConEx) architecture [RFC7713] uses an egress audit to motivate senders to truthfully signal path congestion in-band, where it can be used by ingress policers. An edge-to-edge variant of this architecture is also possible. Distributed domain-edge traffic conditioning: An architecture similar to Diffserv [RFC2475] may be preferred, where traffic is proactively conditioned on entry to a domain, rather than reactively policed only if it leads to queuing once combined with other traffic at a bottleneck. Distributed core network queue protection: The policing function could be divided between per-flow mechanisms at the network ingress that characterize the burstiness of each flow into a signal carried with the traffic and per-class mechanisms at bottlenecks that act on these signals if queuing actually occurs once the traffic converges. This would be somewhat similar to [Nadas20], which is in turn similar to the idea behind core stateless fair queuing. No single one of these possible queue protection capabilities is considered an essential part of the L4S architecture, which works without any of them under non-attack conditions (much as the Internet normally works without per-flow rate policing). Indeed, even where latency policers are deployed, under normal circumstances, they would not intervene, and if operators found they were not necessary, they could disable them. Part of the L4S experiment will be to see whether such a function is necessary and which arrangements are most appropriate to the size of the problem. 8.3. Interaction between Rate Policing and L4S As mentioned in Section 5.2, L4S should remove the need for low latency Diffserv classes. However, those Diffserv classes that give certain applications or users priority over capacity would still be applicable in certain scenarios (e.g., corporate networks). Then, within such Diffserv classes, L4S would often be applicable to give traffic low latency and low loss as well. Within such a Diffserv class, the bandwidth available to a user or application is often limited by a rate policer. Similarly, in the default Diffserv class, rate policers are sometimes used to partition shared capacity. A Classic rate policer drops any packets exceeding a set rate, usually also giving a burst allowance (variants exist where the policer re-marks noncompliant traffic to a discard-eligible Diffserv codepoint, so they can be dropped elsewhere during contention). Whenever L4S traffic encounters one of these rate policers, it will experience drops and the source will have to fall back to a Classic congestion control, thus losing the benefits of L4S (Section 6.4.3). So in networks that already use rate policers and plan to deploy L4S, it will be preferable to redesign these rate policers to be more friendly to the L4S service. L4S-friendly rate policing is currently a research area (note that this is not the same as latency policing). It might be achieved by setting a threshold where ECN marking is introduced, such that it is just under the policed rate or just under the burst allowance where drop is introduced. For instance, the two-rate, three-colour marker [RFC2698] or a PCN threshold and excess-rate marker [RFC5670] could mark ECN at the lower rate and drop at the higher. Or an existing rate policer could have congestion-rate policing added, e.g., using the 'local' (non-ConEx) variant of the ConEx aggregate congestion policer [CONG-POLICING]. It might also be possible to design Scalable congestion controls to respond less catastrophically to loss that has not been preceded by a period of increasing delay. The design of L4S-friendly rate policers will require a separate, dedicated document. For further discussion of the interaction between L4S and Diffserv, see [L4S-DIFFSERV]. 8.4. ECN Integrity Various ways have been developed to protect the integrity of the congestion feedback loop (whether signalled by loss, Classic ECN, or L4S ECN) against misbehaviour by the receiver, sender, or network (or all three). Brief details of each, including applicability, pros, and cons, are given in Appendix C.1 of the L4S ECN spec [RFC9331]. 8.5. Privacy Considerations As discussed in Section 5.2, the L4S architecture does not preclude approaches that inspect end-to-end transport layer identifiers. For instance, L4S support has been added to FQ-CoDel, which classifies by application flow identifier in the network. However, the main innovation of L4S is the DualQ AQM framework that does not need to inspect any deeper than the outermost IP header, because the L4S identifier is in the IP-ECN field. Thus, the L4S architecture enables very low queuing delay without _requiring_ inspection of information above the IP layer. This means that users who want to encrypt application flow identifiers, e.g., in IPsec or other encrypted VPN tunnels, don't have to sacrifice low delay [RFC8404]. Because L4S can provide low delay for a broad set of applications that choose to use it, there is no need for individual applications or classes within that broad set to be distinguishable in any way while traversing networks. This removes much of the ability to correlate between the delay requirements of traffic and other identifying features [RFC6973]. There may be some types of traffic that prefer not to use L4S, but the coarse binary categorization of traffic reveals very little that could be exploited to compromise privacy. 9. Informative References [ACCECN] Briscoe, B., Kühlewind, M., and R. Scheffenegger, \"More Accurate ECN Feedback in TCP\", Work in Progress, Internet- Draft, draft-ietf-tcpm-accurate-ecn-22, 9 November 2022, . [AFCD] Xue, L., Kumar, S., Cui, C., Kondikoppa, P., Chiu, C-H., and S-J. Park, \"Towards fair and low latency next generation high speed networks: AFCD queuing\", Journal of Network and Computer Applications, Volume 70, pp. 183-193, DOI 10.1016/j.jnca.2016.03.021, July 2016, . [BBR-CC] Cardwell, N., Cheng, Y., Hassas Yeganeh, S., Swett, I., and V. Jacobson, \"BBR Congestion Control\", Work in Progress, Internet-Draft, draft-cardwell-iccrg-bbr- congestion-control-02, 7 March 2022, . [BBRv2] \"TCP BBR v2 Alpha/Preview Release\", commit 17700ca, June 2022, . [BDPdata] Briscoe, B., \"PI2 Parameters\", TR-BB-2021-001, arXiv:2107.01003 [cs.NI], DOI 10.48550/arXiv.2107.01003, October 2021, . [BufferSize] Appenzeller, G., Keslassy, I., and N. McKeown, \"Sizing Router Buffers\", SIGCOMM '04: Proceedings of the 2004 conference on Applications, technologies, architectures, and protocols for computer communications, pp. 281-292, DOI 10.1145/1015467.1015499, October 2004, . [COBALT] Palmei, J., Gupta, S., Imputato, P., Morton, J., Tahiliani, M. P., Avallone, S., and D. Täht, \"Design and Evaluation of COBALT Queue Discipline\", IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN), DOI 10.1109/LANMAN.2019.8847054, July 2019, . [CODEL-APPROX-FAIR] Morton, J. and P. Heist, \"Controlled Delay Approximate Fairness AQM\", Work in Progress, Internet-Draft, draft- morton-tsvwg-codel-approx-fair-01, 9 March 2020, . [CONG-POLICING] Briscoe, B., \"Network Performance Isolation using Congestion Policing\", Work in Progress, Internet-Draft, draft-briscoe-conex-policing-01, 14 February 2014, . [CTCP] Sridharan, M., Tan, K., Bansal, D., and D. Thaler, \"Compound TCP: A New TCP Congestion Control for High-Speed and Long Distance Networks\", Work in Progress, Internet- Draft, draft-sridharan-tcpm-ctcp-02, 11 November 2008, . [DOCSIS-Q-PROT] Briscoe, B., Ed. and G. White, \"The DOCSIS® Queue Protection Algorithm to Preserve Low Latency\", Work in Progress, Internet-Draft, draft-briscoe-docsis-q- protection-06, 13 May 2022, . [DOCSIS3.1] CableLabs, \"MAC and Upper Layer Protocols Interface (MULPI) Specification, CM-SP-MULPIv3.1\", Data-Over-Cable Service Interface Specifications DOCSIS 3.1 Version i17 or later, 21 January 2019, . [DOCSIS3AQM] White, G., \"Active Queue Management Algorithms for DOCSIS 3.0: A Simulation Study of CoDel, SFQ-CoDel and PIE in DOCSIS 3.0 Networks\", CableLabs Technical Report, April 2013, . [DualPI2Linux] Albisser, O., De Schepper, K., Briscoe, B., Tilmans, O., and H. Steen, \"DUALPI2 - Low Latency, Low Loss and Scalable (L4S) AQM\", Proceedings of Linux Netdev 0x13, March 2019, . [Dukkipati06] Dukkipati, N. and N. McKeown, \"Why Flow-Completion Time is the Right Metric for Congestion Control\", ACM SIGCOMM Computer Communication Review, Volume 36, Issue 1, pp. 59-62, DOI 10.1145/1111322.1111336, January 2006, . [ECN-ENCAP] Briscoe, B. and J. Kaippallimalil, \"Guidelines for Adding Congestion Notification to Protocols that Encapsulate IP\", Work in Progress, Internet-Draft, draft-ietf-tsvwg-ecn- encap-guidelines-17, 11 July 2022, . [ECN-SCTP] Stewart, R., Tuexen, M., and X. Dong, \"ECN for Stream Control Transmission Protocol (SCTP)\", Work in Progress, Internet-Draft, draft-stewart-tsvwg-sctpecn-05, 15 January 2014, . [ECN-SHIM] Briscoe, B., \"Propagating Explicit Congestion Notification Across IP Tunnel Headers Separated by a Shim\", Work in Progress, Internet-Draft, draft-ietf-tsvwg-rfc6040update- shim-15, 11 July 2022, . [FQ_CoDel_Thresh] \"fq_codel: generalise ce_threshold marking for subset of traffic\", commit dfcb63ce1de6b10b, October 2021, . [Hohlfeld14] Hohlfeld, O., Pujol, E., Ciucu, F., Feldmann, A., and P. Barford, \"A QoE Perspective on Sizing Network Buffers\", IMC '14: Proceedings of the 2014 Conference on Internet Measurement, pp. 333-346, DOI 10.1145/2663716.2663730, November 2014, . [L4S-DIFFSERV] Briscoe, B., \"Interactions between Low Latency, Low Loss, Scalable Throughput (L4S) and Differentiated Services\", Work in Progress, Internet-Draft, draft-briscoe-tsvwg-l4s- diffserv-02, 4 November 2018, . [L4Sdemo16] Bondarenko, O., De Schepper, K., Tsang, I., Briscoe, B., Petlund, A., and C. Griwodz, \"Ultra-Low Delay for All: Live Experience, Live Analysis\", Proceedings of the 7th International Conference on Multimedia Systems, Article No. 33, pp. 1-4, DOI 10.1145/2910017.2910633, May 2016, . [L4Sdemo16-Video] \"Videos used in IETF dispatch WG 'Ultra-Low Queuing Delay for All Apps' slot\", . [L4Seval22] De Schepper, K., Albisser, O., Tilmans, O., and B. Briscoe, \"Dual Queue Coupled AQM: Deployable Very Low Queuing Delay for All\", TR-BB-2022-001, arXiv:2209.01078 [cs.NI], DOI 10.48550/arXiv.2209.01078, September 2022, . [L4SOPS] White, G., Ed., \"Operational Guidance for Deployment of L4S in the Internet\", Work in Progress, Internet-Draft, draft-ietf-tsvwg-l4sops-03, 28 April 2022, . [LEDBAT_AQM] Al-Saadi, R., Armitage, G., and J. But, \"Characterising LEDBAT Performance Through Bottlenecks Using PIE, FQ-CoDel and FQ-PIE Active Queue Management\", IEEE 42nd Conference on Local Computer Networks (LCN), DOI 10.1109/LCN.2017.22, October 2017, . [lowat] Meenan, P., \"Optimizing HTTP/2 prioritization with BBR and tcp_notsent_lowat\", Cloudflare Blog, October 2018, . [McIlroy78] McIlroy, M.D., Pinson, E. N., and B. A. Tague, \"UNIX Time- Sharing System: Foreword\", The Bell System Technical Journal 57: 6, pp. 1899-1904, DOI 10.1002/j.1538-7305.1978.tb02135.x, July 1978, . [Nadas20] Nádas, S., Gombos, G., Fejes, F., and S. Laki, \"A Congestion Control Independent L4S Scheduler\", ANRW '20: Proceedings of the Applied Networking Research Workshop, pp. 45-51, DOI 10.1145/3404868.3406669, July 2020, . [NASA04] Bailey, R., Trey Arthur III, J., and S. Williams, \"Latency Requirements for Head-Worn Display S/EVS Applications\", Proceedings of SPIE 5424, DOI 10.1117/12.554462, April 2004, . [NQB-PHB] White, G. and T. Fossati, \"A Non-Queue-Building Per-Hop Behavior (NQB PHB) for Differentiated Services\", Work in Progress, Internet-Draft, draft-ietf-tsvwg-nqb-15, 11 January 2023, . [PRAGUE-CC] De Schepper, K., Tilmans, O., and B. Briscoe, Ed., \"Prague Congestion Control\", Work in Progress, Internet-Draft, draft-briscoe-iccrg-prague-congestion-control-01, 11 July 2022, . [PragueLinux] Briscoe, B., De Schepper, K., Albisser, O., Misund, J., Tilmans, O., Kühlewind, M., and A.S. Ahmed, \"Implementing the 'TCP Prague' Requirements for Low Latency Low Loss Scalable Throughput (L4S)\", Proceedings Linux Netdev 0x13, March 2019, . [QDyn] Briscoe, B., \"Rapid Signalling of Queue Dynamics\", TR-BB- 2017-001, arXiv:1904.07044 [cs.NI], DOI 10.48550/arXiv.1904.07044, April 2019, . [Raaen14] Raaen, K. and T-M. Grønli, \"Latency Thresholds for Usability in Games: A Survey\", Norsk IKT-konferanse for forskning og utdanning (Norwegian ICT conference for research and education), 2014, . [Rajiullah15] Rajiullah, M., \"Towards a Low Latency Internet: Understanding and Solutions\", Dissertation, Karlstad University, 2015, . [RELENTLESS] Mathis, M., \"Relentless Congestion Control\", Work in Progress, Internet-Draft, draft-mathis-iccrg-relentless- tcp-00, 4 March 2009, . [RFC0970] Nagle, J., \"On Packet Switches With Infinite Storage\", RFC 970, DOI 10.17487/RFC0970, December 1985, . [RFC2475] Blake, S., Black, D., Carlson, M., Davies, E., Wang, Z., and W. Weiss, \"An Architecture for Differentiated Services\", RFC 2475, DOI 10.17487/RFC2475, December 1998, . [RFC2698] Heinanen, J. and R. Guerin, \"A Two Rate Three Color Marker\", RFC 2698, DOI 10.17487/RFC2698, September 1999, . [RFC2884] Hadi Salim, J. and U. Ahmed, \"Performance Evaluation of Explicit Congestion Notification (ECN) in IP Networks\", RFC 2884, DOI 10.17487/RFC2884, July 2000, . [RFC3168] Ramakrishnan, K., Floyd, S., and D. Black, \"The Addition of Explicit Congestion Notification (ECN) to IP\", RFC 3168, DOI 10.17487/RFC3168, September 2001, . [RFC3246] Davie, B., Charny, A., Bennet, J.C.R., Benson, K., Le Boudec, J.Y., Courtney, W., Davari, S., Firoiu, V., and D. Stiliadis, \"An Expedited Forwarding PHB (Per-Hop Behavior)\", RFC 3246, DOI 10.17487/RFC3246, March 2002, . [RFC3540] Spring, N., Wetherall, D., and D. Ely, \"Robust Explicit Congestion Notification (ECN) Signaling with Nonces\", RFC 3540, DOI 10.17487/RFC3540, June 2003, . [RFC3649] Floyd, S., \"HighSpeed TCP for Large Congestion Windows\", RFC 3649, DOI 10.17487/RFC3649, December 2003, . [RFC4340] Kohler, E., Handley, M., and S. Floyd, \"Datagram Congestion Control Protocol (DCCP)\", RFC 4340, DOI 10.17487/RFC4340, March 2006, . [RFC4774] Floyd, S., \"Specifying Alternate Semantics for the Explicit Congestion Notification (ECN) Field\", BCP 124, RFC 4774, DOI 10.17487/RFC4774, November 2006, . [RFC4960] Stewart, R., Ed., \"Stream Control Transmission Protocol\", RFC 4960, DOI 10.17487/RFC4960, September 2007, . [RFC5033] Floyd, S. and M. Allman, \"Specifying New Congestion Control Algorithms\", BCP 133, RFC 5033, DOI 10.17487/RFC5033, August 2007, . [RFC5348] Floyd, S., Handley, M., Padhye, J., and J. Widmer, \"TCP Friendly Rate Control (TFRC): Protocol Specification\", RFC 5348, DOI 10.17487/RFC5348, September 2008, . [RFC5670] Eardley, P., Ed., \"Metering and Marking Behaviour of PCN- Nodes\", RFC 5670, DOI 10.17487/RFC5670, November 2009, . [RFC5681] Allman, M., Paxson, V., and E. Blanton, \"TCP Congestion Control\", RFC 5681, DOI 10.17487/RFC5681, September 2009, . [RFC6040] Briscoe, B., \"Tunnelling of Explicit Congestion Notification\", RFC 6040, DOI 10.17487/RFC6040, November 2010, . [RFC6679] Westerlund, M., Johansson, I., Perkins, C., O'Hanlon, P., and K. Carlberg, \"Explicit Congestion Notification (ECN) for RTP over UDP\", RFC 6679, DOI 10.17487/RFC6679, August 2012, . [RFC6817] Shalunov, S., Hazel, G., Iyengar, J., and M. Kuehlewind, \"Low Extra Delay Background Transport (LEDBAT)\", RFC 6817, DOI 10.17487/RFC6817, December 2012, . [RFC6973] Cooper, A., Tschofenig, H., Aboba, B., Peterson, J., Morris, J., Hansen, M., and R. Smith, \"Privacy Considerations for Internet Protocols\", RFC 6973, DOI 10.17487/RFC6973, July 2013, . [RFC7560] Kuehlewind, M., Ed., Scheffenegger, R., and B. Briscoe, \"Problem Statement and Requirements for Increased Accuracy in Explicit Congestion Notification (ECN) Feedback\", RFC 7560, DOI 10.17487/RFC7560, August 2015, . [RFC7567] Baker, F., Ed. and G. Fairhurst, Ed., \"IETF Recommendations Regarding Active Queue Management\", BCP 197, RFC 7567, DOI 10.17487/RFC7567, July 2015, . [RFC7665] Halpern, J., Ed. and C. Pignataro, Ed., \"Service Function Chaining (SFC) Architecture\", RFC 7665, DOI 10.17487/RFC7665, October 2015, . [RFC7713] Mathis, M. and B. Briscoe, \"Congestion Exposure (ConEx) Concepts, Abstract Mechanism, and Requirements\", RFC 7713, DOI 10.17487/RFC7713, December 2015, . [RFC8033] Pan, R., Natarajan, P., Baker, F., and G. White, \"Proportional Integral Controller Enhanced (PIE): A Lightweight Control Scheme to Address the Bufferbloat Problem\", RFC 8033, DOI 10.17487/RFC8033, February 2017, . [RFC8034] White, G. and R. Pan, \"Active Queue Management (AQM) Based on Proportional Integral Controller Enhanced (PIE) for Data-Over-Cable Service Interface Specifications (DOCSIS) Cable Modems\", RFC 8034, DOI 10.17487/RFC8034, February 2017, . [RFC8170] Thaler, D., Ed., \"Planning for Protocol Adoption and Subsequent Transitions\", RFC 8170, DOI 10.17487/RFC8170, May 2017, . [RFC8257] Bensley, S., Thaler, D., Balasubramanian, P., Eggert, L., and G. Judd, \"Data Center TCP (DCTCP): TCP Congestion Control for Data Centers\", RFC 8257, DOI 10.17487/RFC8257, October 2017, . [RFC8290] Hoeiland-Joergensen, T., McKenney, P., Taht, D., Gettys, J., and E. Dumazet, \"The Flow Queue CoDel Packet Scheduler and Active Queue Management Algorithm\", RFC 8290, DOI 10.17487/RFC8290, January 2018, . [RFC8298] Johansson, I. and Z. Sarker, \"Self-Clocked Rate Adaptation for Multimedia\", RFC 8298, DOI 10.17487/RFC8298, December 2017, . [RFC8311] Black, D., \"Relaxing Restrictions on Explicit Congestion Notification (ECN) Experimentation\", RFC 8311, DOI 10.17487/RFC8311, January 2018, . [RFC8312] Rhee, I., Xu, L., Ha, S., Zimmermann, A., Eggert, L., and R. Scheffenegger, \"CUBIC for Fast Long-Distance Networks\", RFC 8312, DOI 10.17487/RFC8312, February 2018, . [RFC8404] Moriarty, K., Ed. and A. Morton, Ed., \"Effects of Pervasive Encryption on Operators\", RFC 8404, DOI 10.17487/RFC8404, July 2018, . [RFC8511] Khademi, N., Welzl, M., Armitage, G., and G. Fairhurst, \"TCP Alternative Backoff with ECN (ABE)\", RFC 8511, DOI 10.17487/RFC8511, December 2018, . [RFC8888] Sarker, Z., Perkins, C., Singh, V., and M. Ramalho, \"RTP Control Protocol (RTCP) Feedback for Congestion Control\", RFC 8888, DOI 10.17487/RFC8888, January 2021, . [RFC8985] Cheng, Y., Cardwell, N., Dukkipati, N., and P. Jha, \"The RACK-TLP Loss Detection Algorithm for TCP\", RFC 8985, DOI 10.17487/RFC8985, February 2021, . [RFC9000] Iyengar, J., Ed. and M. Thomson, Ed., \"QUIC: A UDP-Based Multiplexed and Secure Transport\", RFC 9000, DOI 10.17487/RFC9000, May 2021, . [RFC9113] Thomson, M., Ed. and C. Benfield, Ed., \"HTTP/2\", RFC 9113, DOI 10.17487/RFC9113, June 2022, . [RFC9331] De Schepper, K. and B. Briscoe, Ed., \"The Explicit Congestion Notification (ECN) Protocol for Low Latency, Low Loss, and Scalable Throughput (L4S)\", RFC 9331, DOI 10.17487/RFC9331, January 2023, . [RFC9332] De Schepper, K., Briscoe, B., Ed., and G. White, \"Dual- Queue Coupled Active Queue Management (AQM) for Low Latency, Low Loss, and Scalable Throughput (L4S)\", RFC 9332, DOI 10.17487/RFC9332, January 2023, . [SCReAM-L4S] \"SCReAM\", commit fda6c53, June 2022, . [TCP-CA] Jacobson, V. and M. Karels, \"Congestion Avoidance and Control\", Laurence Berkeley Labs Technical Report , November 1988, . [UnorderedLTE] Austrheim, M., \"Implementing immediate forwarding for 4G in a network simulator\", Master's Thesis, University of Oslo, 2018. Acknowledgements Thanks to Richard Scheffenegger, Wes Eddy, Karen Nielsen, David Black, Jake Holland, Vidhi Goel, Ermin Sakic, Praveen Balasubramanian, Gorry Fairhurst, Mirja Kuehlewind, Philip Eardley, Neal Cardwell, Pete Heist, and Martin Duke for their useful review comments. Thanks also to the area reviewers: Marco Tiloca, Lars Eggert, Roman Danyliw, and Éric Vyncke. Bob Briscoe and Koen De Schepper were partly funded by the European Community under its Seventh Framework Programme through the Reducing Internet Transport Latency (RITE) project (ICT-317700). The contribution of Koen De Schepper was also partly funded by the 5Growth and DAEMON EU H2020 projects. Bob Briscoe was also partly funded by the Research Council of Norway through the TimeIn project, partly by CableLabs, and partly by the Comcast Innovation Fund. The views expressed here are solely those of the authors. Authors' Addresses Bob Briscoe (editor) Independent United Kingdom Email: ietf@bobbriscoe.net URI: https://bobbriscoe.net/ Koen De Schepper Nokia Bell Labs Antwerp Belgium Email: koen.de_schepper@nokia.com URI: https://www.bell-labs.com/about/researcher-profiles/ koende_schepper/ Marcelo Bagnulo Universidad Carlos III de Madrid Av. Universidad 30 28911 Madrid Spain Phone: 34 91 6249500 Email: marcelo@it.uc3m.es URI: https://www.it.uc3m.es Greg White CableLabs United States of America Email: G.White@CableLabs.com IETF IESG IAB IRTF IETF LLC IETF Trust RFC Editor IANA Privacy Statement About IETF Datatracker Version 11.16.0 (release - 7d5afaf) Report a bug: GitHub Email",
    "commentLink": "https://news.ycombinator.com/item?id=38597744",
    "commentBody": "Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service: RFC 9330Hacker NewspastloginLow Latency, Low Loss, and Scalable Throughput (L4S) Internet Service: RFC 9330 (ietf.org) 201 points by monkburger 5 hours ago| hidepastfavorite41 comments dozaa 4 minutes agoWhat does this mean in practicality as a user? Will e.g. video calls be closer to real-time? There&#x27;s usually about 0.5-1 second delay which leads to a lot of hiccups and interruptions when speaking with each other. What other application uses will be significantly improved? reply toomim 2 hours agoprevThis thing is cool. I saw a live demo at IETF 118 in Prague last month. It totally eliminates buffer bloat, which makes it awesome for video chat. I saw the demo and was like \"woah... I didn&#x27;t think this would ever be possible.\"It requires an additional bit to be inserted into IP packets, to carry information about when buffers are full (I think?), but it actually works. It feels like living in the future! reply ajb 1 hour agoparentThat bit is already there. L4S changes the meaning of the bit to allow a more accurate signal. reply fragmede 45 minutes agorootparentMore particularly, L4S is an advancement to the existing ECN (Explicit Congestion Notification) extension to TCP&#x2F;IP, allowing for more advanced algorithms to cut down latency further. reply toomim 49 minutes agorootparentprevYes, thanks for the clarification. IIRC was explained to me as \"we put the last unused bit in IP packets to use, and get this great feature from it.\" reply ajb 40 minutes agorootparentYeah. It being the last bit (really the last codepoint in a 2 bit field) there was a big argument over it:https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;meeting&#x2F;interim-2020-tsvwg-01&#x2F;s...https:&#x2F;&#x2F;mailarchive.ietf.org&#x2F;arch&#x2F;msg&#x2F;tsvwg&#x2F;rXWRHAyGOuu_qOGM... reply barathr 4 hours agoprevBob Briscoe has been on this line of thought for a long time. I&#x27;d recommend reading a couple of his classics on the topic, including:http:&#x2F;&#x2F;www.sigcomm.org&#x2F;sites&#x2F;default&#x2F;files&#x2F;ccr&#x2F;papers&#x2F;2007&#x2F;A...https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;1080091.1080124 reply monkburger 27 minutes agoparentThank you for the links. I will read over them. reply cepholdapod 1 hour agoprevIf you are interesting in learning more on L4S, there is a webinar series starting today on understandinglatency.com. Some of the authors of L4S, the head of Comcasts L4S field trail and some critical voices are speaking reply vkdelta 3 hours agoprevSome tests were done on Comcast networks on cable plant.Slide deck below explains it:https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;meeting&#x2F;118&#x2F;materials&#x2F;slides-11...Not sure where this leads but I guess ISPs will start charging toll for express lanes reply jesperwe 3 hours agoparentL4S is not really an express lane. It is a way for applications to know when their traffic is congested, enabling them to scale DOWN their traffic to alleviate the congestion. Less congestion means less latency. reply Phelinofist 2 hours agorootparentHow is that different to TCP congestion control? reply ajb 45 minutes agorootparentTCP congestion control can use this new signal, if present. An update to the TCP protocol which allows it to do is going through IETF at present: https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;draft-ietf-tcpm-accurate-ec... reply toomim 43 minutes agorootparentprevTCP congestion control relies on packets being dropped to signal that a link is congested.L4S actually includes an extra bit of information in IP packets that routers can mutate to explicitly say when they are congested.This means that you (a) don&#x27;t need to play exponential backoff games, (b), don&#x27;t need to re-send redundant packets, and (c) don&#x27;t need big buffers in routers.You need big buffers in routers because otherwise exponential backoff goes crazy. But when you add big buffers, you get latency, which is another kind of suck.In order to avoid latency, you need to avoid buffers, which is hard unless you avoid exponential backoff. To avoid exponential backoff, you need routers to actually communicate their congestion, by sending more information. L4S does that by using an unallocated bit in IP packets. reply sznio 1 hour agorootparentprevThis signals congestion explicitly, by a device declaring the link congested and asking others to slow down. TCP congestion control works by detecting when packets are dropped because devices can&#x27;t keep up.Also, when the congestion signal disappears you can try to push the transfer speed up immediately, rather than slowly ramping back up like with TCP. reply flumpcakes 1 hour agorootparentprevCongestion control with TCP will eventually still need to send the same number of bytes down a pipe, albeit with added latency. After a while an application could notice and make a change, but it would be long enough for a user to notice poor service. reply polonbike 2 hours agorootparentprevI guess ISPs will start charging toll for congestionless lanes... reply fotta 3 hours agoparentprevOh wow, I did not know this is what’s behind their low latency trials I’ve seen on dslreports. reply ncruces 1 hour agoprevHow does it compare to μTP (Micro Transport Protocol)?https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Micro_Transport_Protocol reply ajb 56 minutes agoparentIt is independent of that. The L4S standards change the IP layer to provide a more accurate ECN congestion signal , any transport protocol can then take advantage of it. There are versions of TCP and QUIC that do so, in theory a version of uTP could be made to do so as well.However, from a brief look, uTP is designed for background transfers for which latency is not important, so there is no particular need to do so. reply danr4 2 hours agoprevI just hope they pronounce it \"L-Force\" reply tamarlikesdata 2 hours agoprevHow can you differentiate between L4S and non-L4S traffic at the network level, especially in mixed traffic environments? reply ksjskskskkk 1 hour agoparentyou&#x27;d never guess: a new heater bit reply fragmede 50 minutes agorootparentit reuses an existing header bit, but yeah reply fabrixxm 49 minutes agorootparentprevso you know when things get hot... reply eru 17 minutes agoprevHow does this interact with eg BBR? reply ksjskskskkk 1 hour agopreva rfc which simply sells two others rfc... sigh> Center TCP (DCTCP) [RFC8257] and a Dual-Queue Coupled AQM [RFC9332]this only exists to ask that cable modems (and maybe mobile phones?) use that too reply c54 4 hours agoprevnext [13 more] [flagged] MrBuddyCasino 3 hours agoparentI found this useful and don’t understand the downvotes. reply pocketarc 2 hours agorootparentI would assume the downvotes are because the comment is GPT-generated. People come here for the community&#x27;s comments and insights, not for GPT&#x27;s summarisations, even if you yourself find them useful.I personally agree with the downvotes - I don&#x27;t want to see every HN post littered with \"this is what GPT-4 has to say about this\". reply Villodre 2 hours agorootparentAn excellent answer - the very same kind of post I come here to read and not the very same machine-generated content that&#x27;s everywhere in the Net ;-) reply MrBuddyCasino 2 hours agorootparentprevI think an AI summary, clearly marked as such, can be just as useful as the obligatory Archive link.I understand the aversion against „AI spam“, but I find it weird that a supposed tech community rejects AI tech wholesale. reply valvar 2 hours agorootparentNothing&#x27;s stopping users who want an AI summary from feeding the content into their favourite GPT. But it&#x27;s not contributing anything meaningful to a HN discussion. reply Closi 1 hour agorootparentprevAgree - possibly this is even an example of how to do it?Clearly labelled as LLM generated and used to summarise a long RFC (where I personally didn’t find the abstract as clear as GPTs summary).But everyone will have their own views on this. There is definitely a lot of anti-AI or anti-LLM scepticism or denial on HN. reply viraptor 2 hours agorootparentprevThe archive link is useful, because it provides you a way to access the page itself. Nobody here has subscriptions to every single service, so if there&#x27;s something paywalled, that link is useful to almost everyone person. (And I wish HN did it automatically) A lazy gpt summary is not that. reply rwiggins 2 hours agorootparentprevIMO, an LLM-generated summary is almost never a useful post without further comment.I don&#x27;t trust current LLMs to correctly summarize complicated and nuanced text. Now, if someone with the relevant expertise wanted to carefully read an article, feed it into an LLM for a summary, verify its correctness, and post that, I&#x27;d be alright with it.Or if the summary is interesting in some other way - like is it super wrong? or does it make interesting leaps? or maybe it is startlingly correct? - then sure, share it, but also share why it&#x27;s interesting. reply viraptor 2 hours agorootparentprevThere&#x27;s literally an abstract at the top of this document which provides a summary. If you want more, you can feed it into chatgpt (or many other services) yourself, same as everyone here. There&#x27;s no reason to post a summary as an unsolicited comment. reply mkmk3 1 hour agorootparentI&#x27;m pretty sure we often see the abstract posted as a comment for a quick summary, and I don&#x27;t see it downvoted as hard. Not everyone goes through the link.But sure, they could have just posted that instead of going through GPT. Doesn&#x27;t really matter much imo. reply Dylan16807 58 minutes agorootparentIf you use the abstract from the document, then you know multiple people attempted to make it accurate, rather than zero people. reply Pixie_Dust 3 hours agorootparentprev> I found this useful and don’t understand the downvotes.It&#x27;s easier to downvote than post a refutation &#x2F;s reply 1123581321 2 hours agoprev [–] I’m having trouble determining if my 3.1 cable modem supports the draft spec. Is there a way to tell based on serial number? Are there hardware limitations that would prevent older 3.1 modems from receiving a software update to enable support? reply evilmonkey19 19 minutes agoparent [–] It is quite rare that a modem, or home router have support for draft specs. I&#x27;m sorry to disappoint you replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "RFC 9330 introduces the L4S architecture, which aims to enhance internet applications by reducing queuing delay and enabling low latency and scalable throughput.",
      "The L4S architecture proposes new congestion control mechanisms that work alongside existing controls and use a modified version of Explicit Congestion Notification (ECN).",
      "The goal is to provide a high-quality internet service with improved performance for interactive applications, such as gaming, streaming, and video conferencing.",
      "The architecture can be incrementally deployed and is compatible with different network types.",
      "The text includes discussions on deployment considerations, traffic policing, and security considerations.",
      "It emphasizes the importance of accurate ECN feedback and scalability in congestion control algorithms.",
      "The document provides references to additional resources on congestion control, network protocols, and differentiated services."
    ],
    "commentSummary": [
      "The Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service aims to reduce latency and improve performance for applications like video calls.",
      "L4S eliminates buffer bloat and improves congestion signaling to alleviate delays and interruptions in data transmission.",
      "L4S can be implemented by various transport protocols, including TCP and QUIC, and has the potential to improve network performance when adopted by ISPs."
    ],
    "points": 201,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1702270232
  },
  {
    "id": 38593008,
    "title": "Inert Ingredients in Pesticides Could Pose Greater Threat to Bees",
    "originLink": "https://theconversation.com/inert-ingredients-in-pesticides-may-be-more-toxic-to-bees-than-scientists-thought-218005",
    "originBody": "A honeybee approaches a sunflower at Wards Berry Farm in Sharon, Mass. John Tlumacki/The Boston Globe via Getty Images ‘Inert’ ingredients in pesticides may be more toxic to bees than scientists thought Published: December 5, 2023 8:19am EST Author Jennie L. Durant Research Affiliate in Human Ecology, University of California, Davis Disclosure statement Jennie L. Durant has worked as a Science and Technology Policy Fellow with the Association for the Advancement of Science (AAAS) in USDA's Office of Pest Management Policy. Partners University of California, Davis provides funding as a member of The Conversation US. View all partners Email Twitter Facebook LinkedIn Bees help pollinate over a third of the world’s crops, contributing an estimated US$235 billion to $577 billion in value to global agriculture. They also face a myriad of stresses, including pathogens and parasites, loss of suitable food sources and habitat, air pollution and climate-driven weather extremes. A recent study has identified another important but understudied pressure on bees: “inert” ingredients in pesticides. All pesticide products in the U.S. contain active and inert ingredients. Active ingredients are designed to kill or control a specific insect, weed or fungus and are listed on product labels. All other ingredients – emulsifiers, solvents, carriers, aerosol propellants, fragrances, dyes and such – are considered inert. The new study exposed honeybees to two treatments: the isolated active ingredients in the fungicide Pristine, which is used to control fungal diseases in almonds and other crops, and the whole Pristine formulation, including inert ingredients. The results were quite surprising: The whole formulation impaired honeybees’ memory, while the active ingredients alone did not. This suggests that the inert ingredients in the formula were actually what made Pristine toxic to bees – either because the inerts were toxic on their own or because combining them with the active ingredients made the active ingredients more toxic. As a social scientist focusing on bee declines, I believe that either way, these findings have important implications for pesticide regulation and bee health. Threats to bees include single-crop agriculture, habitat loss, air pollution and pesticide exposure. What are inert ingredients? Inert ingredients have a variety of functions. They may extend a pesticide’s shelf life, reduce risks for people who apply the pesticides or help a pesticide work better. Some inerts, called adjuvants, help pesticides stick to plant surfaces, reduce pesticide drift or help active ingredients better penetrate a plant’s surface. The “inert” label is a colloquial misnomer, though. As the U.S. Environmental Protection Agency notes, inerts aren’t necessarily inactive or even nontoxic. In fact, pesticide users sometimes know very little about how inerts function in a pesticide formula. That’s partly because they are regulated very differently than active ingredients. Measuring bee effects Under the Federal Insecticide, Fungicide, and Rodenticide Act, or FIFRA, the EPA oversees pesticide regulation in the U.S. To register a pesticide product for outdoor use, chemical companies must provide reliable risk assessment data on the active ingredients’ toxicity for bees, including the results of an acute honeybee contact test. The acute contact test tracks how honeybees react to a pesticide application over a short period of time. It also aims to establish the dose of a pesticide that will kill 50% of a group of honeybees, a value known as the LD50. To determine the LD50, scientists apply the pesticide to bees’ midsections and then observe the bees for 48 to 96 hours for signs of poisoning. In 2016, the EPA expanded its data requirements by requiring an acute honeybee oral toxicity test, in which adult bees are fed a chemical, as well as a 21-day honeybee larval test that tracks larval reaction to an agrochemical from the egg to their emergence as adult bees. These tests all help the agency determine what potential risk an active ingredient may pose for honeybees, along with other data. Based on the information from these varied tests, pesticides are labeled as nontoxic, moderately toxic or highly toxic. A chemical black box Despite this rigorous testing, much remains unknown about how safe pesticides are for bees. This is particularly true for pesticides that have sublethal or chronic toxicities – in other words, pesticides that don’t cause immediate death or obvious signs of poisoning but have other significant effects. This lack of knowledge about sublethal and chronic effects is problematic, because bees can be repeatedly exposed over long time spans to pesticides on floral nectar or pollen, or to pesticide contamination that builds up in beehives. They even may be exposed through miticides that beekeepers use to control Varroa mites, a devastating bee parasite. Complicating the issue, symptoms of sublethal exposure are often more subtle or take longer to become apparent than acute or lethal toxicity. Symptoms might include abnormal foraging and learning ability, decreased egg laying by the queen, wing deformation, stunted growth or decreased colony survival. The EPA doesn’t always require chemical companies to perform the tests that could detect these symptoms. Inert ingredients add another level of mystery. While the EPA reviews and must approve all inert ingredients, it does not require the same toxicity testing as for active ingredients. This is because under FIFRA, inert ingredients are protected as trade secrets, or confidential business information. Only the total percentage of inert ingredients is required on the label, often lumped together and described as “other ingredients.” Sample pesticide ingredient label from an EPA training guide, showing that just 0.375% of ingredients are disclosed and tested for bee safety. EPA Sublethal weapons A growing body of evidence suggests that inerts are not as harmless as the name suggests. For example, exposure to two types of adjuvants – organosilicone and nonionic surfactants – can impair honeybees’ learning performance. Bees rely on learning and memory functions to gather food and return to the hive, so losing these crucial skills can endanger a colony’s survival. Inerts can also affect bumblebees. In a 2021 study, exposure to alcohol ethoxylates, a coformulant in the fungicide Amistar, killed 30% of the bees exposed to it and caused a number of sublethal effects. While some inerts may be nontoxic on their own, it’s hard to predict what will happen when they are combined with active ingredients. Research has shown that when two or more agrochemicals are combined, they can become more toxic for bees than when applied on their own. This is known as synergistic toxicity. Synergism can also occur when inerts are combined with pesticides. Another 2021 study showed that adjuvants that were nontoxic on their own caused increased colony mortality when combined with insecticides. A sweat bee (Halictus ligatus) covered with pollen. Sam Droege, USGS/Flickr A better testing strategy Mounting evidence on the toxicity of inerts points to three key changes that could better support bee health and minimize bees’ exposure to potential stressors. First, environmental risk assessments for pesticides could test the whole pesticide formulation, including inert ingredients, to provide a more complete picture of a pesticide’s toxicity to bees. This is already done in some cases but could be required for all outdoor uses where bees are at risk of exposure. Second, inerts could be identified on product labels to enable independent research and risk assessment. Third, more testing could be required on pesticides’ long-term sublethal effects on bees, such as learning impairment. Such research would be especially relevant for pesticides that are applied to blooming crops or flowers that attract bees. Researchers and environmental groups have been arguing for changes like these since at least 2006. However, because pesticide regulation is dictated by federal law, changes require congressional action. This would be challenging politically, since it would increase the regulatory burden on the chemical industry. Nonetheless, rising concerns about bumblebee declines and beekeepers’ significant annual colony losses make a strong case for a more precautionary approach to pesticide regulation. With a growing world population and food supplies under increasing stress, supporting bees’ contribution to agriculture is more important then ever. Bees Regulation Pesticides Pollinators Testing Toxicity Honeybees Bumblebees Bee conservation US agriculture agrochemicals",
    "commentLink": "https://news.ycombinator.com/item?id=38593008",
    "commentBody": "‘Inert’ ingredients in pesticides may be more toxic to bees than thoughtHacker Newspastlogin‘Inert’ ingredients in pesticides may be more toxic to bees than thought (theconversation.com) 174 points by PaulHoule 16 hours ago| hidepastfavorite36 comments calibas 14 hours agoHerbicides and pesticides are actively harmful to bees. Even if chemicals in the herbicides aren&#x27;t toxic to bees, you&#x27;re still killing off the \"weeds\" that the bees feed upon. When there&#x27;s less food and the bees have to travel further, they&#x27;re more susceptible to parasites and disease.We can&#x27;t just casually sever links in the food chain, then wonder why ecosystems are collapsing. reply JoshTko 8 hours agoparentI have no doubt we&#x27;ll find out eventually about how they are harmful to humans as well. Companies indiscriminately puring millions of gallons of chemicals with unknown long term health impacts is an insane policy for an advanced nation. I don&#x27;t know why we don&#x27;t focus on mechanical treatments. We have the technology. reply kurthr 16 hours agoprevThe key is this: The “inert” label is a colloquial misnomer, though. As the U.S. Environmental Protection Agency notes, inerts aren’t necessarily inactive or even nontoxic. In fact, pesticide users sometimes know very little about how inerts function in a pesticide formula. That’s partly because they are regulated very differently than active ingredients. reply constantly 16 hours agoparentSomewhat worse, depending on your perspective, is that inert ingredients are confidential business information so while EPA has a list of what inerts are in what pesticides, that list is not publicly shareable. So, you as a consumer or citizen don’t even know what inerts are in the pesticides being used. Unless the manufacturer shares them, but they don’t typically, because they’re the proprietary information that separates Company A’s generic pesticide using active ingredient X (plus confidential inerts) and Company B’s generic pesticide using active ingredient X (plus their I’m sure very different inerts). reply mock-possum 14 hours agoparentprevThat’s a good (if frustrating) clarification - I was immediately thinking, well they can’t be all that inert if they’re having an effect! reply CrzyLngPwd 16 hours agoprevIf you bathe your land in pesticides, don&#x27;t be surprised to find it eventually kills everything. reply derpiederpie 14 hours agoprevIts time to prosecute the Scientists & Business people who privately gain from the public harm they do.It&#x27;s insane to me that such actors are let off the hook regarding the millions or billions of damages they are liable for.Frankly, I understand why China executes white collar criminals--I wish more scientists responsible for developing toxic chemicals, and the businesspeople who pay them-- were prosecuted and handed capital sentences for their crimes against wildlife & humanity.Perhaps then-- by holding them responsible and making examples of them-- their future ilk would be responsible actors. reply parineum 13 hours agoparentIf \"Scientists & Business\" develop solutions that unknowingly do harm amd were approved by the FDA, who should be held responsible? reply autoexec 12 hours agorootparentWe&#x27;ve seen example after example of \"Scientists & Business\" knowing full well that their products are harmful and doing everything they can to cover that fact up and prevent the public from learning the truth, and even examples where the FDA itself knew full well that the products are harmful and yet the products are allowed to continue to be sold.The entire system is broken and part of what is missing, perhaps the most important part, is accountability and meaningful consequences.Obviously, companies who were genuinely unaware that their products were harmful and who immediately recalled and ceased production of those products after finding out aren&#x27;t the biggest problem, but they still show that the product safety testing practices of both the company and the FDA are inadequate. reply londons_explore 12 hours agorootparentIt frustrates me when privacy prevents anyone discovering about a companies bad product.If a company has a list of buyers of their product, they should be able to hand that to healthcare providers and have those healthcare providers check medical records to see if, for example, everyone who uses this brand of toilet cleaner ends up getting arthritis 10 years later. reply rqtwteye 12 hours agorootparentprevIf it’s really unknowingly they shouldn’t be held responsible. But we have plenty of examples where companies knew exactly what’s going on and they covered it up for decades. See lead gas or tobacco. Or the food industry who pours sugar into everything. They know exactly that they are generating millions of diabetics. reply girvo 11 hours agorootparentprevThe thing is, it seems they usually do know. And we don’t prosecute and handle that case yet. So the “unknowingly” seems to be the least of our worries. reply parineum 11 hours agorootparentDo they usually know or are there just a handful of very high profile cases where they did?It seems like everyone&#x27;s go to example of this is the oil industry and global warming and tobacco and cancer. reply Zigurd 8 hours agorootparentThat only scratches the surface. In every high stakes business, they have prepared the ground for themselves through lobbying and regulatory capture to minimize the level of necessary care while maximizing protection against liability. reply brightstep 8 hours agorootparentprevBoth. Unknowingly usually means uncaringly. And the FDA clearly has duty to thoroughly test products. reply senderista 15 hours agoprevThe real shocker here is that whole-pesticide testing wasn&#x27;t required in the first place. reply zahma 11 hours agoparentThe biggest shock is that we still put the burden of proof on science to demonstrate the extent to which food chains are disrupted by isolated uses of pesticides, as if habitat fragmentation, global warming, land use change, pollution, water table pollution and change, overpopulation, etc. doesn’t all tie in. We are so, so myopic.It’s about time the burden of proof falls on chemical companies to show their products don’t do such tremendous damage, instead of leaving it to be discovered and reported in already obliterated ecosystems. reply richbell 10 hours agorootparentIt&#x27;s even worse: the chemical companies also get to dictate what science is allowed to be considered.https:&#x2F;&#x2F;youtu.be&#x2F;i5uSbp0YDhc (this whole video is a gem, but ~13:30 is the relevant bit) reply senderista 10 hours agorootparentWow, so they get to fund their own studies, formulate their own protocols, and their studies don&#x27;t even have to be independently replicated! reply Zigurd 8 hours agorootparentIt. wouldn&#x27;t be called regulatory capture if it wasn&#x27;t executed like a well-run combined arms military operation. replyadrr 14 hours agoprevArticle is talking about a fungicide. Not an insecticide.> The new study exposed honeybees to two treatments: the isolated active ingredients in the fungicide Pristine, which is used to control fungal diseases in almonds and other crops, and the whole Pristine formulation, including inert ingredients. The results were quite surprising: The whole formulation impaired honeybees’ memory, while the active ingredients alone did not. reply seattle_spring 14 hours agoparentTitle currently says \"pesticide\", which includes both insecticides and fungicides. Did the title change from \"insecticide\"? reply staplers 16 hours agoprevWhy would anyone believe a \"pesticide\" would be anything but harmful to almost all insects? reply computerdork 12 hours agoparentAgreed. And while the articles focuses on bees, it&#x27;s important to know that the insect population as a whole have been severely hit over the past two decades (especially flying insects).https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Decline_in_insect_populationsIt&#x27;s shocking and really disturbing how big the decline is, and there is some evidence that this decline at least partially effects other parts of the ecosystem, as freshwater fish populations have great declined too. reply tredre3 15 hours agoparentprevPeople will believe it isn&#x27;t harmful to all insects because the manufacturer says it&#x27;s only harmful to specific classes of insects.(Carefully crafted) Studies will confirm that.And the government tacitly endorses the claims (by the EPA approving its sale).What is the customer supposed to do? Doubt everything the manufacturer, scientists, and the government say? Ok, some doubt is healthy. But then then what?Should the customer test the chemical on all classes of insects himself? And what if the effects aren&#x27;t immediately obvious (as is the case here)?I guess you&#x27;re arguing for pesticide-free farming here, which is unsustainable for almost all farms. reply JeffSnazz 11 hours agorootparent> But then what?Presumably you could use techniques for discouraging pests we (the public) understand better. You can do this for any class of substance, though this isn&#x27;t always possible (e.g. good luck replacing cancer medications with something over the counter). We&#x27;ve been farming for thousands of years; it&#x27;s a little ridiculous to suggest there&#x27;s no alternative to a chemical developed in the last century.Hell, just off the top of my head you can spray the plants with narrowly-targeted substances that are known to be human safe specifically tested against pollinators (e.g. you might use capsacin to discourage mammal consumption). You could also use natural predators to groom the crops. This is a well-documented and ancient technique, although I&#x27;m sure it&#x27;s much more difficult to scale and has a lot of externalities. I.e. lean into the existing ecological web rather than trying to make our own emaciated one which evidently isn&#x27;t self sufficient. And maybe we just use too many pesticides—we certainly produce far more food than we consume, even if we&#x27;re not great at distributing it, perhaps taking some loss in the short term will prevent a catastrophic loss in the long term (I know, literally unimaginable to quarterly-oriented individuals).Ultimately, without some known alternative there&#x27;s not much you can do aside from calling your representative to complain that we need our agencies to be more skeptical and to mandate making the production process public. reply mistrial9 16 hours agoparentprevin chemical engineering, there is a long and rigorous body of science on toxic and poisonous properties. When a chemical result gets close to a product, things diverge. When the products are profitable in some markets, things diverge even more.All that means to say -- \"harmful\" is very well studied. The design of the product on the market is not the same.People have blocked or regulated all kinds of new chemical products over centuries.. Product liability is a \"third rail\" of commerce politics. There are huge incentives to bury publications, news items, science studies and other things, that might bring financial liability to the makers of products on the market. Its a systemic property. Incentives of reward to discover, produce, distribute and market products is also a systemic property.There are multiple serious, moving works of popular science writing that do cross that third rail - Silent Spring by Rachel Carson is often cited.. there are more. reply staplers 16 hours agorootparent\"People lie to make money\" would have been more concise.Still doesn&#x27;t address how adults can genuinely believe pesticides wouldn&#x27;t be harmful to insects. reply parineum 13 hours agorootparentSo, if I can identify one pesticide that doesn&#x27;t harm all insects, we can answer why an adult could genuinely believe that.Is it your assertion that there are zero pesticides that don&#x27;t harm all insects? reply ChoGGi 16 hours agoprevI think someone messed up copying the title? reply PaulHoule 16 hours agoparentFixed reply RadixDLT 8 hours agoprevyou think? pesticides are designed to kill insects, bees are insects reply beebeepka 13 hours agoprev [–] Literal poisons, designed as such, may be harmful? I cannot sanction stupid newspeak hogwash like this. Is it not possible to openly say the truth anymore? reply trallnag 10 hours agoparent [–] Xylitol is a \"literal\" poison for dogs, but perfectly safe for humans. reply beebeepka 2 hours agorootparentHa, this appears to be sold as sugar substitute. Of course it would be promoted on here. Very profound, thank you. Any other suggestions popular with fat people? reply Spivak 10 hours agorootparentprev [–] Xanthan Gum is also poisonous to newborn infant humans but is perfectly safe for adults. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Inert ingredients in pesticides, previously considered non-toxic, may be more harmful to bees than thought, according to new research.",
      "A study exposed honeybees to a fungicide containing both active and inert ingredients and found that the whole formulation impaired the bees' memory, while active ingredients alone did not.",
      "Inert ingredients have different functions and are regulated differently than active ingredients, making it hard to determine their toxicity. Comprehensive testing of pesticide formulations is needed to understand their impact on bee health. Changes to pesticide regulation could help protect bees by identifying inert ingredients on labels and requiring more testing on long-term effects."
    ],
    "commentSummary": [
      "\"Inert\" ingredients in pesticides, which are typically considered non-toxic, may be more harmful to bees than previously believed.",
      "A study reveals that the complete formulation of a fungicide impairs bees' memory, while the active ingredients alone do not.",
      "The use of pesticides has direct negative effects on bees and indirectly affects their food sources, making them more vulnerable to parasites and disease. The regulation and transparency surrounding inert ingredients in pesticides raise concerns about potential harm."
    ],
    "points": 174,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1702228030
  }
]
