[
  {
    "id": 38415252,
    "title": "Reevaluating the Dunning-Kruger Effect: Autocorrelation as an Explanation",
    "originLink": "https://economicsfromthetopdown.com/2022/04/08/the-dunning-kruger-effect-is-autocorrelation/",
    "originBody": "April 8, 2022May 13, 2022 Blair Fix The Dunning-Kruger Effect is Autocorrelation Have you heard of the ‘Dunning-Kruger effect’? It’s the (apparent) tendency for unskilled people to overestimate their competence. Discovered in 1999 by psychologists Justin Kruger and David Dunning, the effect has since become famous. And you can see why. It’s the kind of idea that is too juicy to not be true. Everyone ‘knows’ that idiots tend to be unaware of their own idiocy. Or as John Cleese puts it: If you’re very very stupid, how can you possibly realize that you’re very very stupid? Of course, psychologists have been careful to make sure that the evidence replicates. But sure enough, every time you look for it, the Dunning-Kruger effect leaps out of the data. So it would seem that everything’s on sound footing. Except there’s a problem. The Dunning-Kruger effect also emerges from data in which it shouldn’t. For instance, if you carefully craft random data so that it does not contain a Dunning-Kruger effect, you will still find the effect. The reason turns out to be embarrassingly simple: the Dunning-Kruger effect has nothing to do with human psychology.1 It is a statistical artifact — a stunning example of autocorrelation. What is autocorrelation? Autocorrelation occurs when you correlate a variable with itself. For instance, if I measure the height of 10 people, I’ll find that each person’s height correlates perfectly with itself. If this sounds like circular reasoning, that’s because it is. Autocorrelation is the statistical equivalent of stating that 5 = 5. When framed this way, the idea of autocorrelation sounds absurd. No competent scientist would correlate a variable with itself. And that’s true for the pure form of autocorrelation. But what if a variable gets mixed into both sides of an equation, where it is forgotten? In that case, autocorrelation is more difficult to spot. Here’s an example. Suppose I am working with two variables, x and y. I find that these variables are completely uncorrelated, as shown in the left panel of Figure 1. So far so good. Figure 1: Generating autocorrelation. The left panel plots the random variables x and y, which are uncorrelated. The right panel shows how this non-correlation can be transformed into an autocorrelation. We define a variable called z, which is correlated strongly with x. The problem is that z happens to be the sum x + y. So we are correlating x with itself. The variable y adds statistical noise. Next, I start to play with the data. After a bit of manipulation, I come up with a quantity that I call z. I save my work and forget about it. Months later, my colleague revisits my dataset and discovers that z strongly correlates with x (Figure 1, right). We’ve discovered something interesting! Actually, we’ve discovered autocorrelation. You see, unbeknownst to my colleague, I’ve defined the variable z to be the sum of x + y. As a result, when we correlate z with x, we are actually correlating x with itself. (The variable y comes along for the ride, providing statistical noise.) That’s how autocorrelation happens — forgetting that you’ve got the same variable on both sides of a correlation. The Dunning-Kruger effect Now that you understand autocorrelation, let’s talk about the Dunning-Kruger effect. Much like the example in Figure 1, the Dunning-Kruger effect amounts to autocorrelation. But instead of lurking within a relabeled variable, the Dunning-Kruger autocorrelation hides beneath a deceptive chart.2 Let’s have a look. In 1999, Dunning and Kruger reported the results of a simple experiment. They got a bunch of people to complete a skills test. (Actually, Dunning and Kruger used several tests, but that’s irrelevant for my discussion.) Then they asked each person to assess their own ability. What Dunning and Kruger (thought they) found was that the people who did poorly on the skills test also tended to overestimate their ability. That’s the ‘Dunning-Kruger effect’. Dunning and Kruger visualized their results as shown in Figure 2. It’s a simple chart that draws the eye to the difference between two curves. On the horizontal axis, Dunning and Kruger have placed people into four groups (quartiles) according to their test scores. In the plot, the two lines show the results within each group. The grey line indicates people’s average results on the skills test. The black line indicates their average ‘perceived ability’. Clearly, people who scored poorly on the skills test are overconfident in their abilities. (Or so it appears.) Figure 2: The Dunning-Kruger chart. From Dunning and Kruger (1999). This figure shows how Dunning and Kruger reported their original findings. Dunning and Kruger gave a skills test to individuals, and also asked each person to estimate their ability. Dunning and Kruger then placed people into four groups based on their ranked test scores. This figure contrasts the (average) percentile of the ‘actual test score’ within each group (grey line) with the (average) percentile of ‘perceived ability’. The Dunning-Kruger ‘effect’ is the difference between the two curves — the (apparent) fact that unskilled people overestimate their ability. On its own, the Dunning-Kruger chart seems convincing. Add in the fact that Dunning and Kruger are excellent writers, and you have the recipe for a hit paper. On that note, I recommend that you read their article, because it reminds us that good rhetoric is not the same as good science. Deconstructing Dunning-Kruger Now that you’ve seen the Dunning-Kruger chart, let’s show how it hides autocorrelation. To make things clear, I’ll annotate the chart as we go. We’ll start with the horizontal axis. In the Dunning-Kruger chart, the horizontal axis is ‘categorical’, meaning it shows ‘categories’ rather than numerical values. Of course, there’s nothing wrong with plotting categories. But in this case, the categories are actually numerical. Dunning and Kruger take people’s test scores and place them into 4 ranked groups. (Statisticians call these groups ‘quartiles’.) What this ranking means is that the horizontal axis effectively plots test score. Let’s call this score x. Figure 3: Deconstructing the Dunning-Kruger chart. In the Dunning-Kruger chart, the horizontal axis ranks ‘actual test score’, which I’ll call x. Next, let’s look at the vertical axis, which is marked ‘percentile’. What this means is that instead of plotting actual test scores, Dunning and Kruger plot the score’s ranking on a 100-point scale.3 Now let’s look at the curves. The line labeled ‘actual test score’ plots the average percentile of each quartile’s test score (a mouthful, I know). Things seems fine, until we realize that Dunning and Kruger are essentially plotting test score (x) against itself.4 Noticing this fact, let’s relabel the grey line. It effectively plots x vs. x. Figure 3: Deconstructing the Dunning-Kruger chart. In the Dunning-Kruger chart, the line marked ‘actual test score’ is plotting test score (x) against itself. In my notation, that’s x vs. x. Moving on, let’s look at the line labeled ‘perceived ability’. This line measures the average percentile for each group’s self assessment. Let’s call this self-assessment y. Recalling that we’ve labeled ‘actual test score’ as x, we see that the black line plots y vs. x. Figure 3: Deconstructing the Dunning-Kruger chart. In the Dunning-Kruger chart, the line marked ‘perceived ability’ is plotting ‘perceived ability’ y against actual test score x. So far, nothing jumps out as obviously wrong. Yes, it’s a bit weird to plot x vs. x. But Dunning and Kruger are not claiming that this line alone is important. What’s important is the difference between the two lines (‘perceived ability’ vs. ‘actual test score’). It’s in this difference that the autocorrelation appears. In mathematical terms, a ‘difference’ means ‘subtract’. So by showing us two diverging lines, Dunning and Kruger are (implicitly) asking us to subtract one from the other: take ‘perceived ability’ and subtract ‘actual test score’. In my notation, that corresponds to y – x. Figure 3: Deconstructing the Dunning-Kruger chart. To interpret the Dunning-Kruger chart, we (implicitly) look at the difference between the two curves. That corresponds to taking ‘perceived ability’ and subtracting from it ‘actual test score’. In my notation, that difference is y – x (indicated by the double-headed arrow). When we judge this difference as a function of the horizontal axis, we are implicitly comparing y – x to x. Since x is on both sides of the comparison, the result will be an autocorrelation. Subtracting y – x seems fine, until we realize that we’re supposed to interpret this difference as a function of the horizontal axis. But the horizontal axis plots test score x. So we are (implicitly) asked to compare y – x to x: ( 𝑦 − 𝑥 ) ∼ 𝑥 (y−x)∼x Do you see the problem? We’re comparing x with the negative version of itself. That is textbook autocorrelation. It means that we can throw random numbers into x and y — numbers which could not possibly contain the Dunning-Kruger effect — and yet out the other end, the effect will still emerge. Replicating Dunning-Kruger To be honest, I’m not particularly convinced by the analytic arguments above. It’s only by using real data that I can understand the problem with the Dunning-Kruger effect. So let’s have a look at some real numbers. Suppose we are psychologists who get a big grant to replicate the Dunning-Kruger experiment. We recruit 1000 people, give them each a skills test, and ask them to report a self-assessment. When the results are in, we have a look at the data. It doesn’t look good. When we plot individuals’ test score against their self assessment, the data appear completely random. Figure 7 shows the pattern. It seems that people of all abilities are equally terrible at predicting their skill. There is no hint of a Dunning-Kruger effect. Figure 7: A failed replication. This figure shows the results of a thought experiment in which we try to replicate the Dunning-Kruger effect. We get 1000 people to take a skills test and to estimate their own ability. Here, we plot the raw data. Each point represents an individual’s result, with ‘actual test score’ on the horizontal axis, and ‘self assessment’ on the vertical axis. There is no hint of a Dunning-Kruger effect. After looking at our raw data, we’re worried that we did something wrong. Many other researchers have replicated the Dunning-Kruger effect. Did we make a mistake in our experiment? Unfortunately, we can’t collect more data. (We’ve run out of money.) But we can play with the analysis. A colleague suggests that instead of plotting the raw data, we calculate each person’s ‘self-assessment error’. This error is the difference between a person’s self assessment and their test score. Perhaps this assessment error relates to actual test score? We run the numbers and, to our amazement, find an enormous effect. Figure 8 shows the results. It seems that unskilled people are massively overconfident, while skilled people are overly modest. (Our lab techs points out that the correlation is surprisingly tight, almost as if the numbers were picked by hand. But we push this observation out of mind and forge ahead.) Figure 8: Maybe the experiment was successful? Using the raw data from Figure 7, this figure calculates the ‘self-assessment error’ — the difference between an individual’s self assessment and their actual test score. This assessment error (vertical axis) correlates strongly with actual test score (horizontal) axis. Buoyed by our success in Figure 8, we decide that the results may not be ‘bad’ after all. So we throw the data into the Dunning-Kruger chart to see what happens. We find that despite our misgivings about the data, the Dunning-Kruger effect was there all along. In fact, as Figure 9 shows, our effect is even bigger than the original (from Figure 2). Figure 9: Recovering Dunning and Kruger. Despite the apparent lack of effect in our raw data (Figure 7), when we plug this data into the Dunning-Kruger chart, we get a massive effect. People who are unskilled over-estimate their abilities. And people who are skilled are too modest. Things fall apart Pleased with our successful replication, we start to write up our results. Then things fall apart. Riddled with guilt, our data curator comes clean: he lost the data from our experiment and, in a fit of panic, replaced it with random numbers. Our results, he confides, are based on statistical noise. Devastated, we return to our data to make sense of what went wrong. If we have been working with random numbers, how could we possibly have replicated the Dunning-Kruger effect? To figure out what happened, we drop the pretense that we’re working with psychological data. We relabel our charts in terms of abstract variables x and y. By doing so, we discover that our apparent ‘effect’ is actually autocorrelation. Figure 10 breaks it down. Our dataset is comprised of statistical noise — two random variables, x and y, that are completely unrelated (Figure 10A). When we calculated the ‘self-assessment error’, we took the difference between y and x. Unsurprisingly, we find that this difference correlates with x (Figure 10B). But that’s because x is autocorrelating with itself. Finally, we break down the Dunning-Kruger chart and realize that it too is based on autocorrelation (Figure 10C). It asks us to interpret the difference between y and x as a function of x. It’s the autocorrelation from panel B, wrapped in a more deceptive veneer. Figure 10: Dropping the psychological pretense. This figure repeats the analysis shown in Figures 7 – 9, but drops the pretense that we’re dealing with human psychology. We’re working with random variables x and y that are drawn from a uniform distribution. Panel A shows that the variables are completely uncorrelated. Panel B shows that when we plot y – x against x, we get a strong correlation. But that’s because we have correlated x with itself. In panel C, we input these variables into the Dunning-Kruger chart. Again, the apparent effect amounts to autocorrelation — interpreting y – x as a function of x. The point of this story is to illustrate that the Dunning-Kruger effect has nothing to do with human psychology. It is a statistical artifact — an example of autocorrelation hiding in plain sight. What’s interesting is how long it took for researchers to realize the flaw in Dunning and Kruger’s analysis. Dunning and Kruger published their results in 1999. But it took until 2016 for the mistake to be fully understood. To my knowledge, Edward Nuhfer and colleagues were the first to exhaustively debunk the Dunning-Kruger effect. (See their joint papers in 2016 and 2017.) In 2020, Gilles Gignac and Marcin Zajenkowski published a similar critique. Once you read these critiques, it becomes painfully obvious that the Dunning-Kruger effect is a statistical artifact. But to date, very few people know this fact. Collectively, the three critique papers have about 90 times fewer citations than the original Dunning-Kruger article.5 So it appears that most scientists still think that the Dunning-Kruger effect is a robust aspect of human psychology.6 No sign of Dunning Kruger The problem with the Dunning-Kruger chart is that it violates a fundamental principle in statistics. If you’re going to correlate two sets of data, they must be measured independently. In the Dunning-Kruger chart, this principle gets violated. The chart mixes test score into both axes, giving rise to autocorrelation. Realizing this mistake, Edward Nuhfer and colleagues asked an interesting question: what happens to the Dunning-Kruger effect if it is measured in a way that is statistically valid? According to Nuhfer’s evidence, the answer is that the effect disappears. Figure 11 shows their results. What’s important here is that people’s ‘skill’ is measured independently from their test performance and self assessment. To measure ‘skill’, Nuhfer groups individuals by their education level, shown on the horizontal axis. The vertical axis then plots the error in people’s self assessment. Each point represents an individual. Figure 11: A statistically valid test of the Dunning-Kruger effect. This figure shows Nuhfer and colleagues’ 2017 test of the Dunning-Kruger effect. Similar to Figure 8, this chart plots people’s skill against their error in self assessment. But unlike Figure 8, here the variables are statistically independent. The horizontal axis measures skill using academic rank. The vertical axis measures self-assessment error as follows. Nuhfer takes a person’s score on the SLCI test (science literacy concept inventory test) and subtracts it from the person’s self assessment, called KSSLCI (knowledge survey of the SLCI test). Each black point indicates the self-assessment error of an individual. Green bubbles indicate means within each group, with the associated confidence interval. The fact that the green bubbles overlap the zero-effect line indicates that within each group, the averages are not statistically different from 0. In other words, there is no evidence for a Dunning-Kruger effect. If the Dunning-Kruger effect were present, it would show up in Figure 11 as a downward trend in the data (similar to the trend in Figure 7). Such a trend would indicate that unskilled people overestimate their ability, and that this overestimate decreases with skill. Looking at Figure 11, there is no hint of a trend. Instead, the average assessment error (indicated by the green bubbles) hovers around zero. In other words, assessment bias is trivially small. Although there is no hint of a Dunning-Kruger effect, Figure 11 does show an interesting pattern. Moving from left to right, the spread in self-assessment error tends to decrease with more education. In other words, professors are generally better at assessing their ability than are freshmen. That makes sense. Notice, though, that this increasing accuracy is different than the Dunning-Kruger effect, which is about systemic bias in the average assessment. No such bias exists in Nuhfer’s data. Unskilled and unaware of it Mistakes happen. So in that sense, we should not fault Dunning and Kruger for having erred. However, there is a delightful irony to the circumstances of their blunder. Here are two Ivy League professors7 arguing that unskilled people have a ‘dual burden’: not only are unskilled people ‘incompetent’ … they are unaware of their own incompetence. The irony is that the situation is actually reversed. In their seminal paper, Dunning and Kruger are the ones broadcasting their (statistical) incompetence by conflating autocorrelation for a psychological effect. In this light, the paper’s title may still be appropriate. It’s just that it was the authors (not the test subjects) who were ‘unskilled and unaware of it’. Support this blog Economics from the Top Down is where I share my ideas for how to create a better economics. If you liked this post, consider becoming a patron. You’ll help me continue my research, and continue to share it with readers like you. Stay updated Sign up to get email updates from this blog. Email Address Keep me up to date This work is licensed under a Creative Commons Attribution 4.0 License. You can use/share it anyway you want, provided you attribute it to me (Blair Fix) and link to Economics from the Top Down. Notes Cover image: Nevit Dilmen, altered. The Dunning-Kruger effect tells us nothing about the people it purports to measure. But it does tell us about the psychology of social scientists, who apparently struggle with statistics.↩︎ It seems clear that Dunning and Kruger didn’t mean to be deceptive. Instead, it appears that they fooled themselves (and many others). On that note, I’m ashamed to say that I read Dunning and Kruger’s paper a few years ago and didn’t spot anything wrong. It was only after reading Jonathan Jarry’s blog post that I clued in. That’s embarrassing, because a major theme of this blog has been me pointing out how economists appeal to autocorrelation when they test their theories of value. (Examples here, here, here, here, and here.) I take solace in the fact that many scientists were similarly hoodwinked by the Dunning-Kruger chart.↩︎ The conversion to percentiles introduces a second bias (in addition to the problem of autocorrelation). By definition, percentiles have a floor (0) and a ceiling (100), and are uniformly distributed between these bounds. If you are close the floor, it is impossible for you to underestimate your rank. Therefore, the ‘unskilled’ will appear overconfident. And if you are close to the ceiling, you cannot overestimate your rank. Therefore, the ‘skilled’ will appear too modest. See Nuhfer et al (2016) for more details.↩︎ In technical terms, Dunning and Kruger are plotting two different forms of ranking against each other — test-score ‘percentile’ against test-score ‘quartile’. What is not obvious is that this type of plot is data independent. By definition, each quartile contains 25 percentiles whose average corresponds to the midpoint of the quartile. The consequence of this truism is that the line labeled ‘actual test score’ tells us (paradoxically) nothing about people’s actual test score.↩︎ According to Google scholar, the three critique papers (Nuhfer 2016, 2017 and Gignac and Zajenkowski 2020) have 88 citations collectively. In contrast, Dunning and Kruger (1999) has 7893 citations.↩︎ The slow dissemination of ‘debunkings’ is a common problem in science. Even when the original (flawed) papers are retracted, they often continue to accumulate citations. And then there’s the fact that critique papers are rarely published in the same journal that hosted the original paper. So a flawed article in Nature is likely to be debunked in a more obscure journal. This asymmetry is partially why I’m writing about the Dunning-Kruger effect here. I think the critique raised by Nuhfer et al. (and Gignac and Zajenkowski) deserves to be well known.↩︎ When Dunning and Kruger published their 1999 paper, they both worked at Cornell University.↩︎ Further reading Gignac, G. E., & Zajenkowski, M. (2020). The Dunning-Kruger effect is (mostly) a statistical artefact: Valid approaches to testing the hypothesis with individual differences data. Intelligence, 80, 101449. Kruger, J., & Dunning, D. (1999). Unskilled and unaware of it: How difficulties in recognizing one’s own incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77(6), 1121. Nuhfer, E., Cogan, C., Fleisher, S., Gaze, E., & Wirth, K. (2016). Random number simulations reveal how random noise affects the measurements and graphical portrayals of self-assessed competency. Numeracy: Advancing Education in Quantitative Literacy, 9(1). Nuhfer, E., Fleisher, S., Cogan, C., Wirth, K., & Gaze, E. (2017). How random noise and a graphical convention subverted behavioral scientists’ explanations of self-assessment data: Numeracy underlies better alternatives. Numeracy: Advancing Education in Quantitative Literacy, 10(1). Share this: Twitter Facebook Like this: Like Loading... Uncategorized autocorrelation Dunning-Kruger effect incompetence misleading charts Open Science psychology replication skill assessment statistical independence statistics Published by Blair Fix Political economist. Blogger. Muckraker. Foe of neoclassical economics. View all posts by Blair Fix Post navigation Previous The Voldemort Index Next Weird Consilience: A Review of Joseph Henrich’s ‘The WEIRDest People in the World’",
    "commentLink": "https://news.ycombinator.com/item?id=38415252",
    "commentBody": "The Dunning-Kruger effect is autocorrelationHacker NewspastloginThe Dunning-Kruger effect is autocorrelation (economicsfromthetopdown.com) 451 points by ljosifov 15 hours ago| hidepastfavorite155 comments tempestn 14 hours agoI don&#x27;t buy this take, and this rebuttal does a better job than I could of explaining why: https:&#x2F;&#x2F;andersource.dev&#x2F;2022&#x2F;04&#x2F;19&#x2F;dk-autocorrelation.htmlBasically, this autocorrelation take shows that if performance and evaluation of performance were random and independent, you would get a graph like the D-K one, and therefore it states that the effect is just autocorrelation. But in reality, it would be very surprising if performance and evaluation of performance were independent. We expect people to be able to accurately rate their own ability. And D-K did indeed show a correlation between the two, just not as strong of one as we would expect. Rather, they showed a consistent bias. That&#x27;s the interesting result. They then posit reasons for this. One could certainly debate those reasons. But to say the whole effect is just a statistical artifact because random, independent variables would act in a similar way ignores the fact that these variables aren&#x27;t expected to be independent. reply crazygringo 12 hours agoparentYup. Assuming the sample sizes are statistically significant, the original paper clearly shows:- On average, people estimate their ability around the 65th percentile (actual results) rather than the 50th (simulated random results) -- a significant difference- That people&#x27;s self-estimation increases with their actual ability, but only by a surprisingly small degree (actual results show a slight upwards trend, simulated random results are flat) -- another significant differenceThe author&#x27;s entire discussion of \"autocorrelation\" is a red herring that has nothing to do with anything. Their randomly-generated results do not match what the original paper shows.None of this really sheds much light on to what degree the results can be or have been robustly replicated, of course. But there&#x27;s nothing inherently problematic whatsoever about the way it&#x27;s visualized. (It would be nice to see bars for variance, though.) reply cortesoft 5 hours agorootparent> That people&#x27;s self-estimation increases with their actual ability, but only by a surprisingly small degree (actual results show a slight upwards trend, simulated random results are flat) -- another significant differenceIf everyone thinks they are slightly above average, isn&#x27;t this inevitable? If everyone thinks they are slightly above average, people who are slightly above average are going to be the most accurate at predicting where they land? reply cycomanic 54 minutes agorootparentHave you not just summarized the Dunning-Kruger effect in other words?That essentially follows from everyone assume they are slightly above average. That&#x27;s also the crux of the refutation and why the whole autocorrelation is a red hering, even if we all would just self assess completely randomly, that actually confirms the Dunning-Kruger effect is real (because if we self assess randomly worse performance are more likely to overestimate).We could argue that this is not surprising, but the \"surprising\" bit is that the curves show that better performers are actually more skilled at assessing their performance, which incidentally was also confirmed by the followup studies. reply quetzthecoatl 21 minutes agorootparentIs it though? Everyone overestimating their ability a bit isn&#x27;t DK effect. It&#x27;s when people with less knowledge and ability vastly over estimate their ability (because they don&#x27;t know how little they know - while others do), and the opposite for those who are truly more able and knowledgeable (again because they understand how vast the topic is and though they know more and are capable more than the average person, they also understand how little they truly know compared to what they don&#x27;t know) reply zuminator 4 hours agorootparentprevEven if \"people tend to slightly overrate their own ability,\" was the only takeaway, it would still refute the author&#x27;s conclusion that DK has nothing to do with human psychology. reply IanCal 1 hour agorootparentprevYes but then you&#x27;d see a flat line for people&#x27;s estimates, which wasn&#x27;t the result. reply ketozhang 8 hours agorootparentprevThe autocorrelation is important to show that it&#x27;s transformation to D-K plot will always give you the D-K affect for independent variables.However, the focus on autocorrelation is not very illuminating. We can explain the behaviors found quite easily:- If everyone&#x27;s self-assessment score are (uniformally) random guesses, then the average self-assessment score for any quantile is 50%. Then of course those of lower quantile (less skilled) are overestimating.- If self-assessment score vs actual score are dependent proportionally, then the average of each quantile is always at least it&#x27;s quantile value. This is the D-K effect, which is weaker as the correlation grows.-The opposite is true for disproportional relation.So, the D-K plot is extremely sensitive to correlations and can easily over-exaggerate the weakest of correlations. reply raincole 10 hours agoparentprev> And D-K did indeed show a correlation between the two, just not as strong of one as we would expect. Rather, they showed a consistent bias. That&#x27;s the interesting result.\"D-K effect in its original form\" vs \"D-K effect in pop culture\" is the biggest D-K effect live example. Of course I mean D-K effect in pop culture here.Interestingly, the \"interesting\" part of the original result is that the correlation between actual performance and perceived performance is less than people intuitively think.But as the \"D-K effect in pop culture\" spreads, people&#x27;s collective intuition changes. Today if you explained the original D-K effect to a random person on the internet, they might find it interesting because the correlation is greater than they thought: they thought the correlation would be negative! reply hoosieree 10 hours agorootparentD-K effect effect is almost as entertaining as the Butterfly effect effect[1].[1]: Which is the far-away effect attributed to having watched the movie The Butterfly Effect. reply svnt 13 hours agoparentprevThe author of this assumes the conclusion in order to decide how to analyze his data.He cannot reasonably say both:> we have a decision to make: what are we going to assume? How are we going to quantify our surprise from the results?> The first option is, as in the case of the state census, to assume dependence between X and Y. I.e. to assume that, generally, people are capable of self-assessing their performance.> The second option conforms with the Research Methods 101 rule-of-thumb “always assume independence.” Until proven otherwise, we should assume people have no ability to self-assess their performance.> It seems to me glaringly obvious that the first option is much, much more reasonable than the second.— and -> most notably the claim that the more skilled people are, the better they are at self-assessing their performance. This result is supported by their plot, but in any case, my issue is not with objections to this claimand then expect to carry any credibility.The author of this piece both suggests that a key variable is fixed and later admits it varies within the same dataset.I guess at least they admit it, but this lacks basic self-consistency. reply contravariant 8 hours agorootparentI&#x27;m utterly confused. The latter statements it just the author explaining which parts they didn&#x27;t discuss in their article; it has no bearing whatsoever on the section before it. reply Jensson 13 hours agorootparentprev> The author of this piece both suggests that a key variable is fixed and later admits it varies within the same dataset.I don&#x27;t see how that variable changes, here is an example how the error variable can be exactly the same for everyone and reproduce the results:Lets say the overconfidence is always that you feel 50% of those better than you are actually worse than you. So everyone is equally overconfident, just that the top wont move their own placings as much as the bottom since there are much fewer people that they can mistake being worse than them. Then apply noise to this and you get the graph Dunning-Kruger got.You could say \"But they are better at estimating their rank!\", but that is just a mathematical artefact, it isn&#x27;t a psychological result. Even if everyone always guessed that they are number 1, the better you are the better your guess will be, but in that case it is easy to see that everyone overestimates their skill in the same way instead of the better people having a fundamentally different way of evaluating themselves. reply svnt 9 hours agorootparentBoth analyses seem to agree on one finding: people’s skill at estimating their own ability increases with that skill. It can’t be a purely mathematical artifact because you would see a tapering at either end, or a narrowing distribution of errors at the bottom end, not just a narrowing toward the top end.This should be unsurprising for anyone who has become sufficiently skilled at something. Beginners can’t even discern the differences the experts are discussing, and frequently make errors in classes they don’t even understand. reply chiefalchemist 7 hours agorootparentBeginners, by definition, are guessing 100%. Some will guess high, others low, and the rest in between. But they are all guessing. Perhaps There&#x27;s a cultural bias to over-estimate their skill? Perhaps there&#x27;s a nudge in the process of the study that led them to overestimate?The lede isn&#x27;t that people over-estimate their skill level. The lede is, why would that be as they have nothing else to go on. That is the trigger or triggers? And to say, the more experienced estimate better? Well, duh. reply raincole 10 hours agorootparentprev> Lets say the overconfidence is always that you feel 50% of those better than you are actually worse than you. So everyone is equally overconfident, just that the top wont move their own placings as much as the bottom since there are much fewer people that they can mistake being worse than them. Then apply noise to this and you get the graph Dunning-Kruger got.But the data of original D-K paper shows that the top 25% people underestimate their placings. So this whole paragraph, while logically true, has little to do with the original D-K effect.> You could say \"But they are better at estimating their rank!\", but that is just a mathematical artefact, it isn&#x27;t a psychological result. Even if everyone always guessed that they are number 1...If everyone always guessed that they are number 1, it&#x27;s a huge psychological result: it means people are extremely irrational when it comes to self-evaluation. reply Jensson 9 hours agorootparent> But the data of original D-K paper shows that the top 25% people underestimate their placings. So this whole paragraph, while logically true, has little to do with the original D-K effect.That is what you would expect under my model, due to the randomness being limited upwards for the high placings but still go downwards. That is the effect the article we are talking about refers to when they say \"Autocorrelation\". reply xpe 6 hours agoparentprevThe rebuttal by Daniel (andersource.dev) is useful, generally. However, when he writes ...> The history of statistics is well out of scope for this post, but very succinctly, my answer is that statistics is an attempt to objectively quantify surprise.... I cannot agree. Statistics is not this; it is much broader. One may or may not be surprised by particular statistics, sure, but there are _specific_ concepts that map more directly to surprise, such as entropy from information theory. reply vasco 1 hour agorootparentIf entropy is defined as statistical disorder than I think the definition of \"quantifying surprise\" is great. reply atleastoptimal 13 hours agoparentprevThe issue is people have differing personal definitions of Dunning Kruger. The generally demonstrated effect in the sample of people Dunning and Kruger analyzed was \"people tend to estimate the percentile of their own skill as closer to the average than it really is, with a slight bias towards an above-average mean. This leads to overestimation of relative ability by those in lower percentiles, and the opposite for those in higher percentiles\"However when people cite Dunning Kruger in popular culture they mean \"below average people think they&#x27;re above average, and above average people assume they&#x27;re below average\", which was not shown in the original study, and wouldn&#x27;t show up in an analysis attempting to justify it via a misunderstanding of autocorrelation.The general point in the rebuttal is correct. A completely noisy graph of people&#x27;s estimations of their own ability would show a Dunning-Kruger resembling residual graph (x-y vs x). However, one wouldn&#x27;t expect people in the 1st percentile to have an equal distribution of perceived skill as people in the 50th or 99th percentile. If that were true, it would be worth reporting. reply ShamelessC 12 hours agorootparent> \"below average people think they&#x27;re above average, and above average people assume they&#x27;re below average\"There’s no way to know if you’re wrong, but when I see it used it seems to be pointing out - “some (not all) under qualified people tend to defer to their own beliefs rather than the views&#x2F;statements from experts, even when that is demonstrably silly.”^ Referring to the pop-sci interpretation, not in disagreement with the general point. reply staunton 11 hours agorootparentWhich also has nothing at all to do with this study by Dunning and Kruger. So you agree with the general point of parent. reply ShamelessC 11 hours agorootparentYes. Just clarifying a small disagreement about the pop-sci interpretation of the phrase. reply bradley13 2 hours agoparentprevI have to agree. You cannot separate the statistical analysis from the meaning of the study. In the article, the author&#x27;s random data is exactly an extreme replication of Dunning-Kruger. Why? Because, in his random data, people with low test scores almost always overestimate their ability, while people with high test scores almost always underestimate.That is precisely the premise of the Dunning-Kruger effect. The fact that the original Dunning-Kruger paper shows a less extreme effect? That just shows that people are slightly better than random at estimating their own abilities - but still nowhere accurate. reply jgilias 1 hour agorootparentSo that’s what the Dunning-Kruger effect basically boils down to, right? That people in general are just bad at assessing their skills. reply t_mann 11 hours agoparentprevI was surprised by the figure from the original article, imho that&#x27;s the strongest rebuttal: perceived ability grows strictly mononotonically with actual ability, no sign of the famous non-monotonic U-curve. Yeah, the slope is less than one, and it grows a bit faster from the second to the third quartile than from the first to the second, but none of that changes the fact that people tend to slot themselves correctly. The chart is interesting in that it confirms that everyone perceives themselves to be slightly above average in terms of ability, which of course can&#x27;t be true in practice. But what it also shows is that when they think they&#x27;ll be below or above that (false) baseline, they&#x27;re actually correct about it. So pretty much the exact opposite of what the Dunning-Kruger effect claims. reply tempestn 10 hours agorootparent> The chart is interesting in that it confirms that everyone perceives themselves to be slightly above average in terms of ability, which of course can&#x27;t be true in practice.No, everyone biases their self-assessments toward a point slightly above the mean. That&#x27;s not the same as saying everyone thinks they&#x27;re slightly above average, nor that people&#x27;s self-assessments have no predictive power whatsoever. The lowest performers still think they&#x27;re below average, just not as much as they should. The highest performers still think they&#x27;re considerably above average. But they all have a bias toward (slightly above) the middle.So yes, people are generally correct in the direction that they deviate from that median self-assessment, but that just shows that people&#x27;s self-assessments aren&#x27;t completely without basis. Which D-K certainly didn&#x27;t claim. reply t_mann 10 hours agorootparentD-K claim a non-monotonic relationship, which simply isn&#x27;t supported by that data, as you yourself point out: people rank themselves correctly (ordinally). I didn&#x27;t mean to say that all self-assessments are the same, if that was the misunderstanding. My point is that the self-assessments indeed are meaningful, even more so than D-K claim. reply RevEng 7 hours agorootparentCheck the original paper by D-K. Fix only focused on the first plot which has a monotonically increasing trend. The later plots show varying degrees of nonmonotonicity, though sadly they don&#x27;t include error bars to indicate how statistically significant the differences between groups is. reply zeroonetwothree 6 hours agorootparentprevBut we don’t know their true ability, only the results on one test. It could be they accurately predicted their ability but because of random chance they did better&#x2F;worse than their guess. Then you would get the exact data that is observed. reply lokar 4 hours agorootparentI thought they were estimating their performance on the test relative to others. There was no “real world” element. reply jampekka 10 hours agorootparentprevThe slope will be less than one if there&#x27;s e.g. any random guessing in the test even if the self-assesment is perfect (apart from whether they know if their guess is right or wrong of course) [1].I think this is the effect that the post is dancing around, but doesn&#x27;t seem to really understand (and how \"autocorrelation\" and indepence are discussed is very nonstandard to be charitable).[1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Regression_dilution reply t_mann 10 hours agorootparentI agree, the statistical analysis in the original post makes me very uneasy. I think it could be a case where the conclusion is correct, even though argument isn&#x27;t necessarily.And yes, the fact that the slope is less than one is fairly uninteresting.The real problem here is that the Dunning-Kruger effect, as it&#x27;s classically stated, claims that if you asked four people to rank themselves in terms of ability, the result would be 1-3-2-4, ie the people who know a little would put themselves above the people who know a lot but aren&#x27;t quite experts. The problem is the data shows that they&#x27;d actually rank themselves correctly 1-2-3-4. But such a boring finding probably wouldn&#x27;t have made the authors quite as famous, which might be why they tried bit of data mangling, and they found this really cool story that everyone would secretly love to be true.Which is a shame, because I think the fact that the mean of perceived ability is too high (and the variance too low) is really interesting too, and perfectly supported by the raw data. reply jampekka 10 hours agorootparentYes. The methodology in the original D&K is quite shoddy, and vulnerable to e.g. good old regression to the mean, and the interpretations are too strong. This is sadly very common in psychology (and many other fields I&#x27;d guess) and even researchers don&#x27;t care so much if the story is juicy enough.The pop version of the DK effect seems to be something like a 4-3-2-1 ranking, which is obviously not supported by the data. reply tempestn 8 hours agorootparentprevBut they wouldn&#x27;t. They&#x27;d rank themselves something like 1,2,2,3. We&#x27;re not dealing with a population collaborating to all rank themselves in order, but rather each person individually estimating where their abilities lie in the population.The point is that if you ask someone in the, say, 5th percentile of ability what their ability is compared to the population, they might say 25th percentile. Ask someone at the 25th,and they might say 40th. At the 40th they could say 55th. And at the 90th, maybe they&#x27;ll say 80th. So yes, if you order their guesses, they will be in roughly the correct order. But, crucially, that doesn&#x27;t mean that they are ranking themselves correctly! reply zeroonetwothree 6 hours agoparentprevThis rebuttal seems weak because it’s using unbounded datasets (population). A big issue with the DK research is using bounded data (test scores). For example if I get 100% right it’s mathematically impossible to have overestimated. reply cool_dude85 10 hours agoparentprevYeah this must be some high end satire where the guy Dunning-Krugers up an explanation of Dunning-Kruger. Since even an economist is supposed to understand ANOVA I have to conclude that this article is a joke. reply nickelpro 9 hours agorootparentThe incorrect usage of \"autocorrelation\" made me double take and wonder if this was satire the first time it was posted. reply IAmGraydon 11 hours agoparentprevSo what we have here is some scientists trying to prove that the Dunning-Kruger effect doesn’t exist and instead they give us a perfect example of the Dunning-Kruger effect. reply wyldfire 11 hours agorootparent> The irony is that the situation is actually reversed. In their seminal paper, Dunning and Kruger are the ones broadcasting their (statistical) incompetence by conflating autocorrelation for a psychological effect. In this light, the paper’s title may still be appropriate. It’s just that it was the authors (not the test subjects) who were ‘unskilled and unaware of it’. reply expazl 10 hours agoparentprev> But in reality, it would be very surprising if performance and evaluation of performance were independent. We expect people to be able to accurately rate their own ability.This seems to be attacking an irrelevant point in the analysis. The argument goes as such: Researcher carries out all the studies needed to prove the Dunning-Kruger effect, then trips and drops all the results into a vat of acid. But he&#x27;s ashamed and quickly generates random numbers for the results, and somehow the data still proves the Dunning-Kruger effect. Not just that, repeating the same exercise again and again with completely random data leads to the same result, the effect is always present. So is the Dunning-kruger effect so powerful that it exists in the very fabric of the universe devoid of any human interaction, or is something amiss?In this situation we are forced to look at the test we have that concluded from the data that the Dunning-Kruger effect exists and conclude that it&#x27;s a bad test, we need something different.You seem to be arguing \"oh no, you can&#x27;t look at random data, because we wouldn&#x27;t expect the experiment to yield random data!\". But that doesn&#x27;t work as an argument for why the test should still be considered good. If it&#x27;s supposed to have any worth, then the test has to be able to come to one of two conclusions: The Dunning-Kruger effect exists or the Dunning-Kruger effect doesn&#x27;t exist. And if the test is set up such that for positive experimental results, or just random noise, it comes out in the positive, and only in extremely unlikely and a narrow band of the possible outcome space come out negative, then the test is bad.If we want to try to rephrase everything a bit to make the issue much clearer. Lets set up a coin-toss competition between ChatGPT and a group of 100 people. Each participant goes 1:1 against ChatGPT where both parties toss a coin and whoever has the most heads wins, on draws toss again, in case a pair goes into an infinite loop that doesn&#x27;t end before our allotted trial time, they get removed from the study. A human assistant tosses on the behalf of ChatGPT on account of it not having arms yet.Now we ask each person how they would rate their ability vs. ChatGPT in a coin-toss, everyone answers 50&#x2F;50, for obvious reasons.So we run the experiment, the line for \"ability plotted against ability\" is a straight diagonal line. The line for estimated ability vs actual ability is a a straight flat line at 50%.Eureka! To the presses! we have just proven the Dunning-Coin-Kruger effect! People who are worse at throwing coins tend to over estimate their ability, and people who are better at throwing coins underestimate their ability! What a marvelous bit of psychological insight, it really tells us something about how the human mind works, and has broader insights about our society! But naturally we always expected this outcome, people who are bad a tossing coins are dumb and of cause they are overconfident, not like people who are good at tossing coins who have a remarkable Intellect about themselves and are therefore humble in their self estimation... and so on and on about preconceived biases that have nothing to do with the actual test we performed. reply Jensson 14 hours agoparentprevThe effect that the worst overestimate their skill is known since before, that wasn&#x27;t the main result of Dunning-Kruger. The effect that the best underestimate their skill can be chalked up to auto-correlation. reply tempestn 13 hours agorootparentThe best don&#x27;t tend to overestimate their skill; they underestimate it. The D-K results show a consistent bias in estimates toward (somewhere near) the mean. Hence an overestimate at the bottom and an underestimate at the top. reply Jensson 13 hours agorootparent> The best don&#x27;t tend to overestimate their skill; they underestimateI wrote the wrong word, I fixed it. The best can&#x27;t overestimate their rank, so of course that wasn&#x27;t what I meant. reply anonymouskimmer 13 hours agorootparentprevDunning-Kruger posits this as a psychological effect, yes? On the top half psychological effects such as imposter syndrome could come in to play.Have sociological factors such as being kind or big fish little pond been considered as likely causes of the misestimates? reply chiefalchemist 7 hours agorootparentI have the same question...why do some get it so wrong? Was there a nudge in the process of the study that caused some to answer what they did?Heck, I&#x27;m wondering if \"Honestly, I can&#x27;t say\" was an allowed response. Or were they forced to pick a number? If so, then I&#x27;d want to know what happens when you ask 100 ppl to pick a number between 0 and 100. I bet it&#x27;s not evenly distributed. Maybe the beginners give a \"discounted\" version of the distribution?Even if the autocorrection explanation is off, there does now seem to be flaws in DK, at least from the perspective of pure and proper science replybitshiftfaced 13 hours agoprevThe authors did \"X - Y vs X,\" but that&#x27;s not even the biggest problem. The authors subtracted two measures that had been transformed and bounded from 0 to 1 (think percentiles). What happens at the extremes of those bounds? How much can your top performers overestimate their performance? They&#x27;re almost at 1 already, so not much. If they were to overestimate and underestimate at the same rate and by the same magnitude in terms of raw values, the ceiling effect on the transformed values means that the graph will make it look like they underestimate more often. The opposite problem happens for the worst performers.See \"Random Number Simulations Reveal How Random Noise Affects the Measurements and Graphical Portrayals of Self-Assessed Competency.\" Numeracy 9, Iss. 1 (2016), particularly figures 7, 8, and 9. reply SamBam 11 hours agoparentExactly, that was my thought. How would it be possible to get anything other than the D-K effect, even if it wasn&#x27;t just averaging to the mean?The lowest quartile can&#x27;t say they&#x27;re below the lowest quartile, so any error at all will be counted as \"overconfidence.\" The top quartile can&#x27;t say they&#x27;re above the top quartile, so any error at all will be counted as \"underconfidance.\" reply anonymouskimmer 8 hours agorootparent> Exactly, that was my thought. How would it be possible to get anything other than the D-K effect, even if it wasn&#x27;t just averaging to the mean?Quite easily with the method they demonstrate in the study in figure 11. In that study test participants are not rating themselves in terms of population percentages, but in terms of the percentage correct they got on the test. In such a case the test could be designed to have a huge ceiling that even the most knowledgeable participants would have trouble reaching. And could have such a low floor that even the least knowledgeable participants would still get some answers correct (unless they weren&#x27;t even trying, which would allow throwing out their data points).With 20 questions you could have four gimmes and four impossible questions, bounding the worst participants to about 20% and the best to about 80%. reply SamBam 7 hours agorootparentRight. To clarify, I meant: with the original study design, how could they not have gotten the result they did? (And that&#x27;s rhetorical.) reply anonymouskimmer 7 hours agorootparentIt would have been noteworthy in the original design if more than one group of participants were, on average, within their quartiles on the guessing. I also find it noteworthy that the average guess of the lowest quartile is lower than the average guess of the second lowest quartile, and on up the quartiles. On one hand this shows some awareness of relative ability along a massively smooshed logarithmic scale. On the other hand I wonder if this laddering follows as the averages are split into quintiles and deciles. reply jmpeax 6 hours agorootparentprevI wonder if estimating on the logit scale would solve this problem. reply anonymouskimmer 12 hours agoparentprevThis can be dealt with to an extent by truncating the extreme ends. Even the middle quartiles in the graphs in the linked article show the same trends. reply dimask 5 hours agoparentprevThe boundedness of the data is also the main argument here https:&#x2F;&#x2F;www.frontiersin.org&#x2F;articles&#x2F;10.3389&#x2F;fpsyg.2022.8401... reply ImaCake 12 hours agoparentprevThanks for stating just how much of a statistical minefield this is. The reference does a great job showing just how wrong the DK studies are. Unfortunately, most people have already made up their minds and are happy to link conflicting blog posts as evidence. reply concordDance 1 hour agorootparent> wrong the DK studies areThe DK studies are not wrong, they are misinterpreted by people who don&#x27;t know what they&#x27;re talking about (e.g. what tge DK effect actually is), like this blogger.\"People have worse self assessment ability as their real ability declines\" would be a valid interpretation of the DK data and notably would NOT be a valid conclusion from the random data in the blog post. reply Probiotic6081 12 hours agorootparentprevProbably in another year or two they&#x27;ll find another statistic that will render the old one moot like again and again. reply dclowd9901 12 hours agoparentprevI think if people at all levels of skill were reasonably good at measuring their own ability, we would see two curves that roughly overlap. Instead we see the graph given.The fact that random noise can generate a mean curve on the Y axis doesn’t mean DK doesn’t exist. It just means DK’s mean self analysis resembles a middling random mean, which if you think about it, makes sense. Most people will probably self evaluate as average, regardless of their actual skill. This means DK is right as rain. reply expazl 9 hours agorootparent> I think if people at all levels of skill were reasonably good at measuring their own ability, we would see two curves that roughly overlap. Instead we see the graph given.Actually, due to the construction of the test, the ability to evaluate your own absolute ability in a subject isn&#x27;t sufficient for the two lines to be able to overlap.It&#x27;s a percentile axis, so you need to be able to reasonably accurately estimate the ability of everyone taking the test, and where you fall in the quartile range of those participants. reply r0uv3n 13 hours agoprevThe discussion between Nicolas Boneel and the author in the comments of the article is interesting and Nicolas expresses the doubts I had when reading this. The whole point of the DK effect is that people are bad at estimating their skill, so if you assume that they randomly guess their skill level then of course you will replicate the results.The correct model for a world without DK should be something like (estimated test scores)=(actual test scores)+noise, and then the only form of spurious DK you&#x27;d expect is caused by the fact that there&#x27;s a minimum and maximum test score. But this effect would be proportional to the variance of the noise, and I assume the variance on the additional dataset is too low to fully understand the effect seen there.Also, in this model on average everyone should still guess correctly in which half of the distribution they are, but even the bottom quartile seemed to estimate their abilities as above the 50th percentile reply jampekka 10 hours agoparentThe correct model is probably (estimated test score + estimation noise) = (actual test score + test noise). The test contains a random element, e.g. guessing, that the person can&#x27;t estimate.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Regression_dilutionhttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Errors-in-variables_models reply svnt 13 hours agoparentprevJust because the data appear random doesn’t mean you’ve gotten at the cause though.From those charts it could equally be low skill throughout, or something nuanced like lack of skill at estimating at the bottom, improving skill in estimating through the middle, and high skill and learned modesty at the top. reply Jensson 13 hours agoparentprev> Also, in this model on average everyone should still guess correctly in which half of the distribution they are, but even the bottom quartile seemed to estimate their abilities as above the 50th percentileDepends on the noise applied. If the noise is -10% to +100% for everyone then you get roughly the graph Dunning-Kruger got. So there is no reason to believe that the best are better at estimating their abilities, just that you can&#x27;t estimate your own rank as better than the best. reply tempestn 12 hours agorootparentThat&#x27;s a great observation. For what it&#x27;s worth though, it does seem logical to me that the best would also be best at estimating their skill. Not necessarily because they&#x27;re better at it per se (though there&#x27;s likely some of that too, for the reasons originally posited by D-K), but also because they have an easier problem to solve. When you know something well, it&#x27;s fairly obvious that that&#x27;s the case. (Think of the experience of acing a math test. It&#x27;s entirely possible you&#x27;d know you answered everything correctly.) When you struggle somewhat though, it&#x27;s much more difficult to estimate how much you&#x27;re struggling compared to how others would fare. reply snarkconjecture 13 hours agoprevNonstandard terminology warning: the author is using \"autocorrelation\" in a way I&#x27;ve never seen before. There is a much more common usage of \"autocorrelation\" to refer to the correlation of a timeseries with itself (shifted by some amount).If you use autocorrelation to refer to the thing in OP, you&#x27;ll probably confuse people who know statistics, and vice versa. reply ketozhang 9 hours agoparentThe more common experience with autocorrelations are with time series, but what the author said is correct even in that context. A time series autocorrelation relates the same time series function at different times. At the simplest you plot the arrays X vs X where X[i] = f(t[i]). You then may complicate it further by some transformation g(X) vs X (e.g., moving average). reply xpe 6 hours agoparentprev> Nonstandard terminology warning: the author is using \"autocorrelation\" in a way I&#x27;ve never seen before.That&#x27;s a nice way of putting it. A more accurate description would be: the author is butchering the key essence of autocorrelation, since they don&#x27;t clearly mention that it is a temporal relationship!> What is autocorrelation?> Autocorrelation occurs when you correlate a variable with itself.Groan.A standard definition is:> Autocorrelation refers to the degree of correlation of the same variables between two successive time intervals. It measures how the lagged version of the value of a variable is related to the original version of it in a time series. Autocorrelation, as a statistical concept, is also known as serial correlation. reply gnicholas 4 hours agoparentprevWhat term is appropriate to describe what the author is referring to? reply epigramx 6 hours agoparentprevyou might say the article author might have some ..dunning-kruger on what autocorrelation is. reply nothrowaways 2 hours agorootparentL2 of dk reply randomizedalgs 12 hours agoprevConsider the imaginary world that the author describes, in which people&#x27;s estimate of their score is independent of their actual score. Wouldn&#x27;t it be fair to say that, in this imaginary world, the DK effect is real?The point of the effect is that people who score low tend to overestimate their score and people who score high tend to underestimate. Of course there are lots of rational reasons why this could occur (including the toy example the author gave, where nobody has any good sense of what their score will be), but the phenomenon appears to me to be correct. reply CalChris 8 hours agoprevThe article&#x27;s definition of autocorrelation: Autocorrelation occurs when you correlate a variable with itself.Wikipedia&#x27;s definition of autocorrelation: Autocorrelation, sometimes known as serial correlation in the discrete time case, is the correlation of a signal with a delayed copy of itself as a function of delay.Of course, 0 delay is the trivial case of time delay but really, the article&#x27;s definition is at best inaccurate. D-K has nothing to do with time delay and calling it autocorrelation seems like a weird pun that doesn&#x27;t quite land. reply James_K 11 hours agoprevI think the issue here is a confusion about what \"bias\" means. If they are self-assessing at random, then the high performers will all underestimate themselves, but this is not a bias towards underestimation as they are choosing randomly.That said, the chart from D-K seems to show a different bias and line up roughly with what you would expect. Someone with no knowledge assumes they are average skill and hence inflates their position, someone who is very good doesn&#x27;t want to rate themselves the best because they assume others know as much as they do. The assumption underlying both groups is that you are normal and others are similar to you.I hypothesise that most people think they&#x27;re average, which is something you could easily test by asking them to rate how well they think the average person would do on a test and comparing it to that individual&#x27;s test score. I&#x27;m almost certain that high performers will overestimate the average, and low performers underestimate it. reply abnry 13 hours agoprevIf there is a linear relationship between test score (X, ability) and test score self-assessment (Y, self-perception), then the random variables are modeled as:$$ Y \\sim aX+b+N $$Where N is some statistically independent noise, mean zero.This means the covariance between them is$$ Cov(Y-X,X) = E[ ((a-1)X+b+N -(a-1)E[X]-b) (X - E[X]) ] $$Which is$$ Cov(Y-X,X) = E[(a-1)(X-E[X])(X-E[X])] + E[N(X-E[X])]= (a-1) Var[X] $$To get a \"DK effect\" we need (a-1)1, then we&#x27;d have a whole new effect about arrogant experts.So the only thing that matters from this \"auto-correlation perspective\" is the rate at which an individual&#x27;s self-assessment increases with their ability. As long as they underestimate the increase, a \"DK effect\" will occur.However, in the above analysis, we ignored the variable b. If a = 0.8 and b=0, we&#x27;d never have the so-called \"DK effect\" even though it matches the \"auto-correlation perspective\" because everyone would underestimate their ability.This tells me that the value of b matters. It is sort of like the prior ability everyone assumes they have. What the DK papers shows is that b > .5, which I think is in line with the spirit of the popular interpretation of the \"DK effect\". People should not be assuming they have, at a minimum, a capacity higher than the average.At the same time, the value b isn&#x27;t insanely higher than .5, which also makes me want to cut those unskilled and unaware some slack. It \"seems reasonable\" to assume your baseline is average. That can&#x27;t be the case, but it feels intuitive. reply 19f191ty 5 hours agoprevThat is not an autocorrelation. The OP is equating linear dependence with autocorrelation, which not how we use that term. Autocorrelation is when a random process is correlated with time lagged version of itself. reply PeterStuer 2 hours agoprevYou can take out the x from both sides, and the y would still not be a horizontal line.In their eagerness to &#x27;deconstruct&#x27; the narrative, do the authors merely provide another example of Dunning-Kuger by overestimating their own cleverness? reply zw123456 6 hours agoprevI know I&#x27;m not smart enough on statistics or psychology to evaluate the article but it always struck me that D&K seemed to say something similar to what my grandpa said when I was a wee lad, \"The more you know, the more you realize how much you don&#x27;t know\", I know he wasn&#x27;t the first person to say that, but he was the first person to say it to me. I don&#x27;t know if D&K is autocorrelation or not, but I know that an awful lot of people seem to think they know more than maybe they actually do, probably me included. Hmmm, maybe the author of that article as well? I wonder if that occurred to him, seems like a glaring oversight not to at least recognize that possible irony. reply Arch485 5 hours agoparentIn the article, a real study was used as a counterexample to the DK effect.Part of the results was a correlation that people who were \"less capable\" were also worse at predicting their own skill, and people who were \"more capable\" were better at predicting their own skill.While similar to the DK effect, this is different, as the DK effect states that \"less capable\" individuals specifically _overestimate_ their skill, as opposed to simply being wrong (both over and under -estimating).With relation to some people \"seeming to think they know more than they actually know\", this is likely confirmation bias in the sense that there are an equal number of people who don&#x27;t know much, and know that they don&#x27;t know much. reply mattbit 50 minutes agoprevThis is not ‘autocorrelation’, it is regression to the mean. I find the article unclear and imprecise. For those interested in a better overview of the Dunning–Kruger effect, I recommend this short article by McIntosh & Della Sala instead:https:&#x2F;&#x2F;www.bps.org.uk&#x2F;psychologist&#x2F;persistent-irony-dunning... reply mattbit 44 minutes agoparentThis is how McIntosh & Della Sala put it:> in the academic literature, it has been suggested that the signature pattern of the DKE (Figure 1A) might be nothing more than a statistical artefact. In a typical study, people’s tendencies to under- or overestimation are analysed as a function of their ability for the task. This involves a ‘double dipping’ into the data because the task performance score is used once to rank people for ability, and then again to determine whether the self-estimate is an under- or over-estimate. This dubious double-dipping makes the analysis prone to a slippery statistical phenomenon called ‘regression to the mean’. reply hn_throwaway_99 13 hours agoprevPrevious discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31036800 reply golol 8 hours agoprevI disagree. Dunning Kruger is not a statement about predicted score correlating with actual score in some way. It states that predicted score does not correlate well with actual score. This can be rephrased as the prediction error having a negative correlation with the actual score. The article then claims that this negative correlation is autocorrelation. That is true but the correlation still exist. The thing is that ideally we EXPECT there to be no correlation of the prediction error with the actual score, but we find autocorrelation. Going back to variables where this autocorrelation is not there, we EXPECTED to find a 1:1 positive correlation between predicted score and actual score but find no correlation, or a weak correlation.So finding autocorrelation when you expected to find no correlation is pretty much the Dunning-Kruger effect here.In fact their example with the random data totally makes sense: Suppose people uniformly randomly estimate their performance. Then the people who are low skilled will consistently over-estimate and the people who are high-skilled will consistently underestimate. Of course there is no causation here, as the people choose randomly, but there is an undeniable correlation. I guess the question is if you view the Dunning-Kruger effect as a claim to low skill CAUSING positive prediction error, or just correlating with it. reply chmod600 7 hours agoprevA related effect that I&#x27;ve wondered about is: perhaps lower-skilled people compare themselves to the general public, while perhaps skilled people compare themselves to a smaller group of skilled peers.In other words, if you asked me if I&#x27;m good at riding a bicycle, I&#x27;d compare myself to others in the general population and say \"yes\". But if you ask a weekend bicyclist, they&#x27;d be better than me but perhaps compare themselves to weekend bicyclists, and rate themselves lower. And the effect might repeat for competitive bicyclists.If true, this could explain why we intuitively believe the DK effect. reply dimask 5 hours agoprevI would call this type of argument a case of regression to the mean rather than \"autocorrelation\". That, of course, in principle requires independence between performance and assessment of performance. In many cases, it would make little sense to assume that the performance and assessment of performance are independent. But even then, one can simulate random data with some correlation, and still get a DK effect merely as statistical artifact. An overview of similar critiques, and a similar argument in https:&#x2F;&#x2F;www.frontiersin.org&#x2F;articles&#x2F;10.3389&#x2F;fpsyg.2022.8401... . reply civilized 6 hours agoprevWe discussed this in a previous thread. The author is basically hypothesizing that perhaps people are so universally terrible at predicting their ability, their self-rating is like an unconditional random variable - just a random draw that is not influenced by their actual ability level at all.If this is true, then when your actual ability is high, your self-rating is likely to be lower than your ability simply by random chance. For example, if ability ranges from 0-100, your actual ability is 99, and your self-rating is a uniform random number from 0-100, your self-rating is 99% likely to be lower than your actual ability. Conversely, if your actual ability is low, your self-rating is likely to exceed your actual ability level.When it&#x27;s explained clearly and simply, the criticism raises a lot of questions. Are people actually that bad at rating their own ability? I doubt it. reply hyperthesis 7 hours agoprevIf unskilled and skilled self-assessed themselves the same on average, then unskilled overestimate, and skilled underestimate.That would be a significant result alone - that no one had any idea. (but as https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38416100 notes, there is a correlation). reply dmbche 14 hours agoprevIsn&#x27;t it ironic that they fooled themselves? reply ulizzle 13 hours agoparentIt was actually hilarious but I don’t think many people here got the irony reply DangitBobby 8 hours agorootparentLiterally the closing paragraph of TFA is about that exact irony. reply xanderlewis 13 hours agoprevNaïve take: I’ve always felt like Dunning-Kruger is just the result of the fact that when guessing the value of anything people tend towards some common mean, and so if the true value is low your guess tends to be high, and vice versa. This assumes nothing about what is being guessed, but does assume (perhaps wrongly) that there is a commonly believed mean value and that people tend to imagine they are close to it. reply wavemode 12 hours agoparentThat&#x27;s essentially the plain-language interpretation of what the author of this article is pointing out - when you plot (actual score) against (difference between test score and actual score), you will always find a trend that underperformers overestimate and overperformers underestimate - for the exact reason you state. reply mewpmewp2 14 hours agoprevMy take on Dunning Kruger:1. People really like the idea of smart people being humble and arrogance meaning stupidity, so they like to believe that DK is true, and they like to repeat this.2. Some smart&#x2F;skilled people are humble, some are arrogant.3. Some smart&#x2F;skilled people underestimate their skills, some overestimate.4. Some stupid people are humble, some are arrogant.5. Some stupid people underestimate their skills, some overestimate.Overall, even if there is a correlation, you can&#x27;t tell by just arrogance of a person whether we are dealing with DK or whether it&#x27;s an effect at all. People&#x27;s personalities, skills and everything are a bit more complex than that.Overall bringing DK up seems like some sort of social justice&#x2F;fairness effort rather than something that is actually true given any situation where someone is arrogant. reply spacebacon 13 hours agoparentMaybe this shows how effective dumb people are at keeping smart people hammered down with thought stopping arguments. reply concordDance 12 hours agoprevThe author fails to make his point quite badly. Of course if everyone&#x27;s self assessment was random the bottom quartile would overrate themselves! And that would be half of the Dunning-Kruger effect and we could truthfully say \"the bottom quartile of people overrate themselves\"!The other part where those at the top have a better idea or where they rank noticeably does not come out in his toy example.Honestly, he comes across as not having the slightest understanding of how people interpet those graphs... reply dilawar 2 hours agoprevDavid Dunning response (2022): https:&#x2F;&#x2F;www.bps.org.uk&#x2F;psychologist&#x2F;dunning-kruger-effect-an... reply vismwasm 13 hours agoprevThe author measures the Dunning Kruger effect on his random data exactly because he assumes it when generating his random data.By modelling skill and perceived skill as uniform draws between 0 and 100, the unskilled (e.g. skill=0) will over-estimate their skills (estimated skill = 50, the mean on the uniform random variable) and the skilled (e.g. skill=100) will underestimate it (as 50 as well, again the mean of the same random variable). The only ones who will be correct (on average) are the average skilled ones (skill=50). reply ezekiel68 12 hours agoprev> However, there is a delightful irony to the circumstances of their blunder.Indeed. And I find the tendency of people in this comment section to defend the flawed theory is further confirmation of another scientific finding: that we decide based on emotion and then justify our decision using rationality. reply stubish 6 hours agoparentEven when the article cites the 3 papers it is based on, no refutations of the published science by people who grok it. reply joefourier 13 hours agoprevSo from my understanding, the Dunning-Kruger Effect paper doesn’t show the distribution of the perceived test scores nor the standard deviation, only an average, which rises with actual test score level.If they showed the spread bar in each bin, you could form very different conclusions. Do low skilled people consistently estimate their score at around 60, or do they give effectively random results centred around 60?Assuming the latter, it could mean that low skilled individuals are completely unable to evaluate their performance while higher skilled people are slightly better at it but still not very good, giving a slightly positive correlation which… is very distinct from what the DK effect implied. reply glitchc 14 hours agoprevGeez, this is eye-opening. Thank you for sharing this. reply anonymouskimmer 13 hours agoprev> If the Dunning-Kruger effect were present, it would show up in Figure 11 as a downward trend in the data (similar to the trend in Figure 7). Such a trend would indicate that unskilled people overestimate their ability, and that this overestimate decreases with skill. Looking at Figure 11, there is no hint of a trend.There certainly is a hint of a trend. Why do people, when visualizing data with a distinct trend, say that because the \"error bars\" from a particular statistical test overlap zero that no trend exists!?Freshman trend to over-confidence. Grad students trend to under-confidence. Undergrads in general trend to over-confidence (though this trend decreases as year in school increases), and post-graduates, whether grad students or professors, trend to under-confidence.These \"trends\" are not statistically significant, but they certainly are a trend!Also, the random data distribution in figure 9 doesn&#x27;t show the same trends as Dunning-Kruger&#x27;s curve in figure 2. Perhaps there is at least one psycho-social mechanism here worth investigating? reply Dylan16807 12 hours agoparentIf they&#x27;re actually error bars, you can shrink them with more data. That will turn the hint of a trend into an observation of a trend. If it wasn&#x27;t random noise giving a fake hint. reply anonymouskimmer 12 hours agorootparent> If they&#x27;re actually error bars, you can shrink them with more data.Assuming the new data has the same systemic or instrumental bias as the old data. Even using a different test date could skew results enough to widen the error bars. reply mrkeen 12 hours agoparentprev> These \"trends\" are not statistically significant, but they certainly are a trend!This is an oxymoron. reply anonymouskimmer 12 hours agorootparentShow how.I place mechanistic theory prior to statistics in science. Mechanistic theory can be tested, statistics are a kind of test.If a statistically-insignificant result shows consistent, though non-significant deviations, such as the kind seen in Figure 11, then it tells me it&#x27;s worth investigating whether mechanism(s) are explaining a very small portion of the variation that will not, in itself, show up as statistically significant, as it&#x27;s being swamped by variation in other parameters. reply Dylan16807 12 hours agorootparentConsistency is a synonym for statistical significance. If there&#x27;s consistency beyond random alignment, then there should be a statistical test you can apply over your data to extract the signal.You can extract surprisingly small signals relative to variation in other parameters. But if it&#x27;s actually swamped, then it might not be real, so go get more data. reply anonymouskimmer 11 hours agorootparent> Consistency is a synonym for statistical significance.So basically you&#x27;re telling me that if I can visually see a consistency that does not show up in their statistical test, then they aren&#x27;t running an appropriate statistical test on what I&#x27;m seeing.> But if it&#x27;s actually swamped, then it might not be real, so go get more data.Even better to design other experiments. reply Dylan16807 11 hours agorootparent> So basically you&#x27;re telling me that if I can visually see a consistency that does not show up in their statistical test, then they aren&#x27;t running an appropriate statistical test on what I&#x27;m seeing.Either they&#x27;re not doing the right statistics, or it&#x27;s a \"consistency\" that is much more likely to show up randomly than you naively expect, and the study needs to be repeated or enhanced.Sometimes you can see a pattern that&#x27;s just a figment of chance. See also: numerology, jelly bean xkcd reply Dylan16807 12 hours agorootparentprevOxymorons only sound contradictory on a surface level.Something \"certainly\" being a \"trend\" is the definition of statistical significance, so this is a straight up contradiction. reply anonymouskimmer 12 hours agorootparentSee here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38416858\"Trend\" has multiple meanings. Statistics doesn&#x27;t get to claim all of the meaning. reply lencastre 14 hours agoprevWasn’t this DK effect already debunked? reply mrkeen 13 hours agoparentYes but some claim to have debunked the debunking also. [1]This paper (2023) claims \"the magnitude of the effect was minimal; bringing its meaningfulness into question.\" [2][1] https:&#x2F;&#x2F;andersource.dev&#x2F;2022&#x2F;04&#x2F;19&#x2F;dk-autocorrelation.html[2] https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S01602... reply xbar 13 hours agoparentprevYes. This article highlights the 2016, 2017 and 2020 debunkings of DK. But it hangs on as an oft repeated scientific fallacy.The fact that anyone has to ask if it has debunked shows how desirable some people find the DK myth. Even in the comments here, people are not willing to be skeptical of DK. That&#x27;s interesting psychology. reply hasch 13 hours agoparentprevArticle mentions 2016 somewhere. They explain a bit on top of that, with more depth ... at least my rough take on this reply jahewson 13 hours agoparentprevI don’t know much about it but I’m sure you’re right. reply BrenBarn 2 hours agoprevYeah I don&#x27;t buy this either.I do think the original Dunning-Kruger plot is a bit of an odd presentation. The way I look at it is just to say that people&#x27;s self-estimates of their ability fall into a relatively narrow range (e.g., 55-75th percentile on the graph), whereas their actual abilities of course cover the whole range from 0-100th percentile. You don&#x27;t really need the plot of \"x versus x\" (average score in each quartile). You just need to say \"people&#x27;s self-assessments seem to start unrealistically high and only go up a little, even as their ability goes up a lot\". reply nitwit005 10 hours agoprevIf self evaluations are random, and you group a bunch of them together, then you&#x27;ll see values around the 50th percentile. That&#x27;s why their self evaluation line is nearly flat.In the actual data though, the line clearly trends upward. The people who did well appear to be scoring themselves non-randomly. reply austin-cheney 11 hours agoprevThe best way to differentiate DK from autocorrection is motive. Low performance people will focus on motives that reinforce the perception of their competence, for example preferring code style over code delivery because while both may be arguably important one requires less effort and risk to attain.There is research to qualify this out of Stanford. People will shift motives to attain complements and the types of compliments received will dictate the challenges they are willing to accept. When a compliment is specific to an action and measurable people will strive for continuously more challenging tasks to continually receive specific compliments. When compliments are generic and directed to the person they will tend to preference progressively less challenging tasks so that they continue to shine relative to the attempted effort. The differences in behavior produces a natural Dunning-Kruger effect wherein people seeking less qualified activities are more likely to over estimate their potential and degree of success.This also statistically verified in research that correlates predictions to confidence. The more confidence a person is in their predictions, such as political talk radio hosts, the less accurate their predictions tend to be. reply eterevsky 1 hour agoprevI think this article would&#x27;ve made more sense if it had a title \"The Dunning-Kruger effect is regression toward the mean\", because that&#x27;s what the author is actually showing. reply tgv 56 minutes agoparentI think your description is the most apt.OP&#x27;s own analysis shows that using random data (two variables uniformly distributed over the same range!) for both skill and self-assessment results in a different graph. The original comparison therefor implies another effect on the second dimension, which could be interpreted as: people don&#x27;t estimate their skills correctly, but drift towards the mean.But then the question becomes: what did they really ask their subjects? To pick the percentile or a true test score? reply beltsazar 13 hours agoprevI don&#x27;t know if I agree that it&#x27;s an autocorrelation, but one way to explain The Dunning-Krugger Effect is by acknowledging this simple fact:Most people think that they are an average person, but they can&#x27;t be all average—there must be some people substantially below the median. Therefore, those people must overestimate their abilities.This also applies to other aspects, such as attractiveness. Less attractive people would overestimate their attractiveness. reply anonymouskimmer 12 hours agoparentFor all of the tests and rebuttals of the Dunning-Kruger effect the people tested are not drawing from the totality of other people, but trying to compare themselves solely to those who also took the same test.Anyone in a position to take such a test is almost guaranteed to be above average compared to the general population (which includes babies for intellectual tests, or the extremely old for attractiveness tests).I think this complicates personal evaluation. reply TrackerFF 11 hours agoprevThe DK effect has gotten WAY more cred than it should. Today, it is just anoter feel-good piece that people use to justify their feeling that they&#x27;re (ironically) surrounded by loud idiots. reply markhahn 11 hours agoprevthe numeric experiment does not produce a line identical to what DK report. if DK&#x27;s line where horizontal at 50%, it would indeed be nothing but autocorrelation. reply salty_biscuits 13 hours agoprevIt&#x27;s just correlation, why do they keep calling it autocorrelation. reply stubish 6 hours agoparentauto correlation, or self correlation. A correlation between different things may indicate an actual relation (smoking is correlated with early mortality). A self correlation is a tautology. reply Jensson 14 hours agoprevPsychologists using their pet theories to explain results and then people taking that explanation as the truth when they should really just look at the data is probably an as large problem as the replication crisis. reply lopatin 12 hours agoprevOh I read about the about the DK effect a while ago. I&#x27;m pretty much an expert in Psychology now, AMA. reply a-dub 12 hours agoprevi think of acf as a measure of repeating temporal structure and how \"strong\" and \"long\" it is, if it exists.that is, it gives you a notion of if and what order of an ar model should fit any repeating structure in the data. reply resource0x 10 hours agoprevCan someone explain the difference between Dunning-Kruger effect and \"illusory superiority\" effect (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Illusory_superiority)? reply zeroonetwothree 6 hours agoparentDK says that skilled people tend to underestimate their skill while unskilled people tend to overestimate their skill. This is likely a statistical artifact.IS says that people tend to overestimate their own skill compared to how other people estimate their skill. This seems likely true on average but not necessarily in all cases. reply thewanderer1983 13 hours agoprevThe Dunning-Kruger effect isn&#x27;t as the article first quotes. It&#x27;s an effect that everyone experiences. We as humans tend to over simplify things we don&#x27;t understand well or at all. Therefore we over estimate our expertise on these subjects. We also tend to under estimate how much an expert on subjects we do know well. Everyone does this. It&#x27;s not just dumb people. reply Jensson 13 hours agoparent> We also tend to under estimate how much an expert on subjects we do know wellAny evidence for this, except Dunning-Kruger? To me it looks like everyone overestimates themselves. There are a lot of professionals who think they are undervalued and that people worse than them gets all the rewards and fame. reply dahart 11 hours agoprevMost people, even here on HN, do not know what the DK effect actually claimed to show. It does not show that confident people are more likely to be incompetent. Their primary result shows a positive correlation between confidence and supposed skill. (What skill, you ask?*)This article suggests DK is even simpler than autocorrelation, that it’s just regression toward the mean. https:&#x2F;&#x2F;www.talyarkoni.org&#x2F;blog&#x2F;2010&#x2F;07&#x2F;07&#x2F;what-the-dunning-...I don’t know which statistical artifact it is, but I am quite convinced that the so-called DK effect is not demonstrating something interesting about human psychology, I don’t buy that this is a real cognitive bias. I’ve read the paper several times, and the methodology seems to be lacking rigor. They tested a small handful of Cornell undergrads volunteering for extra credit, not a large sample, not the general population, and tested nobody who actually fits the description of ‘incompetent’ in a meaningful way. They primarily measured how people rank each other, not what their absolute skill was - and ranking each other requires speculating on the skills of others. There are obvious bias problems with asking a group of pampered Ivy League kids how well they think they rank.* One of the four “skills” they measured was ability to get a joke - “appreciation of humor” - Huh? This is subjective! The jokes used aren’t given in the paper, either. Another was ‘grammar’ tests. reply eagerpace 11 hours agoprevIs this the opposite of imposter syndrome? reply lifeisstillgood 8 hours agoprevThe Dunning Kruger effect is simply the same reason expensive projects are undertaken and never hit budget - not because we cannot estimate costs but because if we did we would never do anything. reply epigramx 6 hours agoprev\"Autocorrelation is the statistical equivalent of stating that 5 = 5.\" no sure if the author has some ..dunning-kruger there. reply riazrizvi 10 hours agoprevA general problem with Dunning Kruger is the assumption that if you score low on a test then you are bad at the subject it is evaluating. I’ve taken enough bad quizzes that purportedly evaluate skills that I am an expert in, to know that that is a leap. reply badrabbit 9 hours agoprevIn my experience, people abuse flattery too much so it is hard to tell if their positive opinions of me are genuine and with merit. Generally speaking, I try to see the big picture and realize no matter how well I do, in a more global sense at best I am too 50th percentile, slightly above average. It is chance,relationships and supply&#x2F;demand economics that ultimately decide our ability to apply our talents effectively.When it comes to others, I wish more people experienced the D&R effect. It gets frustrating sometimes dealing with smart and talented people who think they are revolutionary rockstars. You know the kind, they see other people&#x27;s work and they are shocked how bad everything is, but never fear, they, our heroes are here to refactor everything until they leave and another hero looks at their work and rescues metropolis from it again. Patience and humility are a rare virtue for all of us. reply dclowd9901 12 hours agoprevI think what this article is missing is “the chart DK should have used.”Instead we get a spurious explanation that doesn’t make a lot of sense based on completely fabricated data. It’s entirely natural for something that looks like DK to emerge from randomized data, especially when the Y axis is represented by some number of the mean (actually 50ish in this case). reply toasted-subs 12 hours agoprevIdk I genuinely feel like after having to deal with 10+ doctors who all had different opinions. The last doctor finally made the same conclusion as me and he was the last person I had to see.There&#x27;s always exceptions. And sometimes reading publications pertaining to a very specific thing should give you more say on a subject.I just feel bad American tax payer money and the best years of my life was spent on telling medical professionals they don&#x27;t know what they are talking about. reply im3w1l 12 hours agoprevIt&#x27;s fascinating how great Elo and similar ranking systems are at curbing DK. You just get a number, and that&#x27;s how good (bad) you are. It&#x27;s incredibly precise too, there&#x27;s just no arguing with it.Also since the topic is D-K I&#x27;m a bit scared that I&#x27;m the fool here, but isn&#x27;t he misusing the term autocorrelation? What he describes sounds like just normal correlation? reply fnord77 10 hours agoprevwikipedia&#x27;s article intro on this doesn&#x27;t state it is invalid :&#x2F; reply hyperthesis 7 hours agoprevIt&#x27;s Dunning-Krugers all the way down - including this self-referential smugness. reply chiefalchemist 13 hours agoprevDK for me is simply: \"You don&#x27;t know what you don&#x27;t know.\" When that happens, it&#x27;s easy - surprise, surprise! - to misjudge your skill level. In a way, it almost feels cruel to ask someone with too few points of reference to say how much they know. The fact is whether high, low, or in the middle...they are guessing.On the other hand, with enough experience the depth and breadth of your context improves, as it should. At that point, mis-self-assessment is the result of arrogance, bravado, etc. That&#x27;s a different problem than simply not knowing.If nothing else, DK has a case of apple v oranges. reply 6510 6 hours agoprevI was curious if the self assessment is done before or after the test.Bing chat gave me this wild answer:> The effect is usually measured by comparing self-assessment with objective performance. For example, participants may take a quiz and estimate their performance afterward, which is then compared to their actual results 1. Therefore, people estimate their ability before the test by Dunning-Kruger.In the case estimation is done before: If you&#x27;ve had training, like a soup of ingredients, that matches the priorities and biases of the test it would be strange if no measurable effect remained.If it&#x27;s done after: You can create trick questions specifically designed to test if someone learned a specific thing. A good test would test for that. If someone didn&#x27;t learn the specific thing they could give&#x2F;guess the wrong answer with some confidence.The design of the test has great influence on how poorly you&#x27;ll think you&#x27;ve done. I would argue that the superior test is the one designed to fool you. Hans Rosling famously created a multiple choice test with 4 answers per question with average results below 25%.On a more fascinating note, unskilled means all areas of expertise outside your own.People who are universally unskilled in all areas are of course more likely to think they are unskilled. In reality these people know little bits about many things.This in contrast with people who spend all day, every day, for their entire lives pondering topics inside their area of expertise. If you are doing one thing you aren&#x27;t doing all of the other things.Wikipedia had hilarious instances of experts contributing to countless articles accidentally ending up on the wrong page. Suddenly they have no patience, think they know everything and act like children. It&#x27;s funny because you cant just ban valuable contributors.I would love to see this DK test done with professors furthest removed from the area of expertise. reply RevEng 7 hours agoprevWhat Blair Fix&#x27;s article gets wrong is that there are two stark differences between what Fix generated with random data and what Dunning and Kruger observed in theirs.Fix has each person guess randomly between 0 and 99 where they will lie in the percentiles. They simulate every person having no idea and giving equal probability to being the best or the worst. If we then sort them by how well they really did into quartiles and then evaluate the average of how well they thought they would do, we get what we would expect: each quartile has an equal chance of predicting that they will do well or do poorly, with an average expected percentile of 50, which is what you would expect by a random guess.Note two key things about this: - All quartiles guessed the same - there was no correlation between what they guessed and how well they actually did - All quartiles guessed the expected average percentile - 50%. This means they were unbiased in how well they thought they would do.If people were unbiased but also unaware, this is the null hypothesis we would expect: on average people predict themselves to be average and there&#x27;s no correlation between how well they predicted they would do and how well they actually did.Now compare that to what Dunning and Kruger observed: - The quartiles did NOT guess the same. There was a bit of an upwards trend, which suggests that people at least somewhat were able to determine their actual percentiles, even if only weakly on average. - The predictions were biased. All groups estimated they would do better than the expected average. That is to say, on average, they thought they were above average. This is an important bias. - The differentials between quartiles are not equal. The first and second quartile typically predicted the same, over-estimated value, implying that neither group had any idea they were better or worse than each other. However, the upper quartile consistently estimates a higher average. That is to say, people who perform well, on average, believe they are performing even better than those who don&#x27;t perform well. And perhaps most surprisingly, there was often a statistically significant dip at the third quantile. Comparing their beliefs, people who did well believed they had done worse than the people who actually did worse.Fix also fails to go beyond the first figure of the paper. After seeing this inconsistent behaviour between the quartiles, Dunning and Kruger then test what happens if the respondents are given an opportunity to grade each other - therefore getting an idea of what the percentiles actually look like - and to have their skills improved - thereby possibly making them better able to judge their own and each other&#x27;s abilities. Again, if Fix&#x27;s premise that this is all just a result of manipulating the autocorrelation of an otherwise unbiased random sequence, then these interventions should have no discernable effect. Yet, Dunning and Kruger find markedly significant changes after these interventions, and those changes are different within the different quantiles.It is precisely this difference between quantiles which is the Dunning-Kruger effect. Fix effectively makes their point for them by building a null model and showing what would happen if there were no Dunning-Kruger effect - if people were fully unaware and unbiased. Instead, it is the way in which Dunning and Kruger&#x27;s observations deviate from this model that is the very effect that bears their name.Instead, all that Fix manages to do is point out how confusing the plot is that Dunning and Kruger produced. The plot can easily be misinterpreted to suggest that it&#x27;s the difference between y and y-x that is important. Instead, in their writing, Dunning and Kruger actually focus on the differences in how y-x changes when the situation changes, demonstrating that it&#x27;s actually dependent on knowledge and how different people respond to that knowledge. What they actually show is that delta(y-x) vs x has a nonzero relationship and this is particularly interesting.Perhaps if Dunning and Kruger had not included the example of perfect knowledge as a comparison, but instead included the example of unbiased and unknowledgeable that Fix produced as the thing to compare against, the Dunning-Kruger effect would be much better understood.Further, both could benefit greatly from plotting and tabulating not just an average, but the overall distribution within each group. Fix should know that variance is just as important as bias. Even if all groups are biased in their prediction, differences in variance between each group indicates their confidence in their belief. Knowledge should help to reduce both bias and variance. A guess with high variance tells us little, while a guess with low variance tells us quite a bit. Even if all quartiles predicted the same average, we wouldn&#x27;t fault those with little ability for guessing a high number if they did so with low confidence. On the contrary, we would expect people with high ability to be more confident (and correct) in the assessment of their ability. reply notShabu 12 hours agoprevevery domain of expertise has two \"elo\" systems, the niche one and the broader one.e.g. you can learn basic juggling in 30 minutes that you are top 10% of your friends&#x2F;colleagues etc...however within the juggling community itself this is known as the \"3 ball cascade\" a really simple trick relative to the ones that requires years to master. an outsider may not be able to tell the difference between the 1 year expert and the 10 year master.a lot dunning-kruger can be explained by people in one or the other not understanding the other system reply jongjong 10 hours agoprevThis makes sense. IMO, the reason why Dunning-Kruger effect is so popular among the upper classes (along with Impostor Syndrome) is that it helps to provide justification for social inequalities as it corrects inner monologues.\"How come I have so much given that I&#x27;m not as skilled as these other people? I must suffer from impostor syndrome.\"\"Look at all these people complaining instead of taking responsibility for their own failures, they probably suffer from Dunning-Kruger effect. Their work must not be good enough.\"But of course this requires a certain detachment from reality (hence why many upper class people have blind spots). If they actually took a look at the evidence, they may find that some of these &#x27;Dunning-Kruger people&#x27; are actually far more skilled than they imagine. I think it explains why people like Jürgen Schmidhuber who made significant contributions to AI tend to be ignored. Then because people are ignoring them, they are compelled to promote themselves harder to try to get their fair share of attention but they are then put in the &#x27;Dunning-Kruger basket&#x27; until someone with a very good reputation like Elon Musk comes along and gives them credit. I think the same could be said about the mathematician Srinivasa Ramanujan; many mathematicians ignored his work or assumed he was a fraud because he seemed too sure of himself for someone who was completely unknown at the time. If such gross injustice can happen in a perfectly-quantifiable field like math, you can be sure it can happen in any field. reply greenthrow 13 hours agoprevLmao this article is an example of Dunning-Kruger at work. The author thinks they have found and are revealing something but they are just failing to fully understand the subject. Amazing. reply flappyeagle 13 hours agoparentTry reading the article again and understanding the argument. reply greenthrow 12 hours agorootparentOh I did. Completely. reply mattxxx 10 hours agorootparentWait... but what if this is DK? What if my comment is DK?? reply pie_flavor 14 hours agoprev [–] This take is a perfect example of Dunning-Kruger itself, ironically. https:&#x2F;&#x2F;andersource.dev&#x2F;2022&#x2F;04&#x2F;19&#x2F;dk-autocorrelation.html reply dahart 11 hours agoparent [–] How so? DK shows a positive correlation between confidence and competence. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Dunning-Kruger effect, which suggests that unskilled individuals overestimate their competence, is being reevaluated as a statistical artifact caused by autocorrelation.",
      "Replication studies have found no evidence of the effect, questioning its existence as a psychological phenomenon.",
      "Criticisms of the original study highlight statistical errors and biases, emphasizing the need for better scientific analysis and awareness of flaws in the Dunning-Kruger effect."
    ],
    "commentSummary": [
      "The Dunning-Kruger effect is a psychological phenomenon where people with lower abilities tend to overestimate their competence, while those with higher abilities tend to underestimate themselves.",
      "There is debate and criticism surrounding the accuracy and interpretation of the effect, including discussions on methodological flaws, replication studies, autocorrelation, and societal factors.",
      "The complexity of self-assessment and the presence of Impostor Syndrome are mentioned, which is related to the Dunning-Kruger effect."
    ],
    "points": 451,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1700936045
  },
  {
    "id": 38413660,
    "title": "Communicating Browser Window Dimensions and Position Using Socket Clients: A Fun and Shareable Experiment",
    "originLink": "https://github.com/Momciloo/fun-with-sockets",
    "originBody": "Two browser windows (acting as socket clients) communicate their:- Screen dimensions - (screen.width, screen.height)- Window dimensions - (window.innerWidth, window.innerHeight)- Window X&#x2F;Y position - (window.screenX, window.screenY)...or whichever calculation works best for you.The original work by Bjorn Staal https:&#x2F;&#x2F;twitter.com&#x2F;_nonfigurativ_&#x2F;status&#x2F;172732259457002734 used localStorage, but I found sockets more fun, because if tweaked a bit, this can be shared with friends.Here&#x27;s a demo of how it works and the codebase: https:&#x2F;&#x2F;github.com&#x2F;Momciloo&#x2F;fun-with-sockets&#x2F;",
    "commentLink": "https://news.ycombinator.com/item?id=38413660",
    "commentBody": "I saw this mind-blowing experiment, so I made a simple version of itHacker NewspastloginI saw this mind-blowing experiment, so I made a simple version of it (github.com/momciloo) 315 points by momciloo 17 hours ago| hidepastfavorite104 comments Two browser windows (acting as socket clients) communicate their:- Screen dimensions - (screen.width, screen.height)- Window dimensions - (window.innerWidth, window.innerHeight)- Window X&#x2F;Y position - (window.screenX, window.screenY)...or whichever calculation works best for you.The original work by Bjorn Staal https:&#x2F;&#x2F;twitter.com&#x2F;_nonfigurativ_&#x2F;status&#x2F;172732259457002734 used localStorage, but I found sockets more fun, because if tweaked a bit, this can be shared with friends.Here&#x27;s a demo of how it works and the codebase: https:&#x2F;&#x2F;github.com&#x2F;Momciloo&#x2F;fun-with-sockets&#x2F; vunderba 11 hours agoThat&#x27;s a great demo! I&#x27;d be curious to see how this would work with multiple monitors.Also, thank you for voluntarily acknowledging that you were directly inspired by somebody else and giving them credit - the software industry could use more people like you. reply stavros 9 hours agoparentWhat, thieves?Make your stuff from zero, like the rest of us! reply zxexz 7 hours agorootparentPretty sure this is tongue-in-cheek, given the commenter (pretty high profile dev who creates a lot of cool stuff and does not shy away from using other open source code!) :) reply stavros 30 minutes agorootparentAw I thought it would be obvious :( Thanks for clarifying, of course nobody makes anything without being inspired from something that already existed, at least to a degree.I guess it&#x27;s hard to accurately convey sarcasm online, thanks again! reply kh0st 5 hours agorootparentprevthat was super cool of you. Before letting people make the incorrect assumption, you hit em off at the pass. good people right there reply heavyset_go 2 hours agoprev> The original work by Bjorn Staal https:&#x2F;&#x2F;twitter.com&#x2F;_nonfigurativ_&#x2F;status&#x2F;172732259457002734 used localStorage, but I found sockets more fun, because if tweaked a bit, this can be shared with friends.Anyone have a link to what this was? The tweet is gone. reply iamateapot 1 hour agoparentIt&#x27;s missing a digit at the end, original tweet: https:&#x2F;&#x2F;twitter.com&#x2F;_nonfigurativ_&#x2F;status&#x2F;172732259457002734... &#x2F; https:&#x2F;&#x2F;nitter.net&#x2F;_nonfigurativ_&#x2F;status&#x2F;1727322594570027343 reply kpandit 2 hours agoparentprevHe gives the link on GitHub but here it is (video) https:&#x2F;&#x2F;www.linkedin.com&#x2F;feed&#x2F;update&#x2F;urn:li:activity:7133171... reply oniony 24 minutes agoprevI love stollen. It&#x27;s one of the few aspects of Christmas I actually look forward to. reply aylmao 5 hours agoprevI&#x27;m old enough to remember a bunch of demos using window position&#x2F;dimensions from back in the day. I remember one with a physics simulation. I can&#x27;t remember if it was liquid or just a bunch of solids, but you could drop things from one window to the other.IIRC you don&#x27;t even need the sockets. One can simply use message channels between the windows. If a window opens child window, iirc it has special access over it (I say special, because usually different tabs&#x2F;windows are isolated from one another) which might make a local-only version easy to implement too. reply unfunco 4 hours agoparentBrowser Ball maybe? https:&#x2F;&#x2F;experiments.withgoogle.com&#x2F;browser-ball reply aylmao 3 hours agorootparentAh yeah! Browser ball is a classic! reply Simran-B 53 minutes agoprevIf you like this, you might also enjoy WindowKill, a video game similar to Asteroids that cleverly uses windows that can overlap and interact with each other.You need to shoot at the window boundaries, too, or the window shrinks. Additional windows appear later in the game with boss enemies.Gameplay video: https:&#x2F;&#x2F;youtu.be&#x2F;7iP68FZWVxM reply itronitron 13 hours agoprevThis, or something similar, would be nice for managing layers in a paint program such as Krita, Inkscape, or Gimp. Could be implemented simply as tabbed panes within the encompassing application window, and whatever tab is selected is the layer that is active for the editing actions. reply arshxyz 1 hour agoprevThe devs behind tldraw also played with this to create something cool - https:&#x2F;&#x2F;twitter.com&#x2F;steveruizok&#x2F;status&#x2F;1727625036159234555 reply sshh12 11 hours agoprevReminds me of a cool demo of this for playing pong with browser windows http:&#x2F;&#x2F;stewd.io&#x2F;pong&#x2F; reply jakegmaths 2 hours agoprevHere&#x27;s something similar I made yesterday. Just view source of the controlling page and the pop-up pages for how it works (single page with inline vanilla JavaScript and no npm install nonsense). Just uses postMessage(). https:&#x2F;&#x2F;cms-compsci.deno.dev&#x2F;mrgordon&#x2F;window&#x2F; reply Filligree 11 hours agoprevBut… why does it lag? Shouldn’t this be trivial enough to be instant? reply Geee 2 hours agoparentQuerying window position is laggy unfortunately. It&#x27;s a browser feature. reply akdor1154 5 hours agoparentprevBackground windows&#x2F;tabs getting a lower priority maybe a factor. reply bawolff 10 hours agoparentprevI wonder if it would be more instant if they used postMessage api instead of going over network. reply 4death4 6 hours agorootparentAny local network usage should be nearly instantaneous.My personal theory about the lag is that positioning the window is not synchronized to the JS thread, so the window moves before the JS thread is aware, which means you see the square move after the window has already moved. reply crabmusket 4 hours agorootparentprevI assume you mean BroadcastChannel?https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;BroadcastCh... reply rco8786 6 hours agorootparentprevGoing over local network is more or less instant reply taberiand 9 hours agoparentprevProbably because of websocket network lag and also, in my anecdotal experience, window position updates are sometimes strangely jittery reply mrits 9 hours agorootparentWhy is it over a websocket? Can&#x27;t it just share local data? reply trevor-e 6 hours agorootparentIt says so right in the OP> The original work by Bjorn Staal used localStorage, but I found sockets more fun, because if tweaked a bit, this can be shared with friends. reply raincole 8 hours agorootparentprevI might be wrong, but I think the point is to simulate a server-client architecture (to show that it works even for an actual web app). reply EGreg 8 hours agorootparentprevthose were my thoughts tooYou can use Broadcast Channels as well as Service Workers, even WebRTC would use a loopback interface! reply Agingcoder 11 hours agoprevCould someone explain what this means ? ( I’m not even sure I understand the gif on the GitHub page ) I just see windows seemingly sharing data. reply et-al 8 hours agoparentThe core of the trick isn&#x27;t the window-window communication as much as using browser APIs to expose the window coordinates. reply NL807 10 hours agoparentprevI think it&#x27;s some kind of multiple client, single server setup. The windows are the individual clients, which pass screen geometry info to the server. The server then serves content to clients with different layout that makes content appear as a single object across multiple windows. reply ww520 12 hours agoprevThat&#x27;s a fun use of LocalStorage.I used the same LocalStorage sharing technique to update the target window when the setting had been changed on a separate browser window. Just listen for the storage.onChanged event for the update. reply abid786 9 hours agoparentI don’t think this is using local storage. There’s a web sockets based server that is sending clients the location of the other boxes as they are moving reply doctorhandshake 15 hours agoprevCool! Feels like whichever window has focus, that window’s rectangle should draw on top. reply benj111 12 hours agoparentIsn&#x27;t that just to illustrate that it isn&#x27;t just transparency? reply faloppad 3 hours agoprevReminds me of http:&#x2F;&#x2F;www.thewildernessdowntown.com&#x2F; [desktop] reply hoosieree 10 hours agoprevwhat fresh pop-up&#x2F;pop-under hell even is this?!? I specifically asked for not that reply hot_gril 13 hours agoprevThis reminds me of a project I did in school, a multi-user web browser. You could both control the same window&#x27;s size, scroll position, and other state on each other&#x27;s screens. The intended usage was having 2+ windows side by side. reply jocaal 14 hours agoprevThe available API&#x27;s on the web are very questionable. Feross has a web security lecture series on youtube where he showed off the following abomination. Enter at own risk.https:&#x2F;&#x2F;theannoyingsite.com&#x2F; reply lgats 14 hours agoparentwarning, this is really annoying. imagine you&#x27;re infected with a ton of adware in the early 2000s you will be logged out of many accountshere&#x27;s someone testing it out on Windows a few years ago, but I assure you even in the latest chrome on mac, it&#x27;s at least as bad https:&#x2F;&#x2F;youtu.be&#x2F;estvbRSj4LU?si=gq6-l4pWLC9yXqap&t=109 reply dylan604 13 hours agorootparentWhy would you keep minimizing things instead of closing them? The video just looks like one of those meant to frustrate the viewer into wanting to do it themselves. I blame the creator of that video as much as the site itself. The site itself served a purpose. The person running that video has a much less clear motive reply bbarnett 12 hours agorootparentI think if you close a window, another pops up. Or two.You should go to this site and test this. reply dylan604 10 hours agorootparentIt spawned an app outside of the browser. That&#x27;s the window being minimized that I meant. I&#x27;m not an unawares person to the site&#x27;s purpose. You&#x27;re being a bit disingenuous with your reading of my comment reply PakG1 12 hours agorootparentprevI tried it on Safari private window. It was absolutely horrible. I hesitate to guess what would have happened if I didn&#x27;t keep clicking cancel whenever it asked if I wanted to allow downloads, etc. reply CrimsonRain 4 hours agoparentprevIt lived up to its name for sure!! reply 38 10 hours agoparentprevuBlock Origin has prevented the following page from loading: https:&#x2F;&#x2F;theannoyingsite.com&#x2F;Because of the following filter: ||theannoyingsite.com^$documentFound in: uBlock filters – Badware risks reply derangedHorse 5 hours agoparentprevholy shit, I was not expecting that LOL reply mholt 15 hours agoprevMore like this:- https:&#x2F;&#x2F;twitter.com&#x2F;steveruizok&#x2F;status&#x2F;1727625036159234555- https:&#x2F;&#x2F;twitter.com&#x2F;_nonfigurativ_&#x2F;status&#x2F;172732259457002734...- https:&#x2F;&#x2F;twitter.com&#x2F;wesbos&#x2F;status&#x2F;1727755227766350031- https:&#x2F;&#x2F;twitter.com&#x2F;wesbos&#x2F;status&#x2F;1727722358465523721 reply burcs 11 hours agoparentI had some fun doing some SQL joins with this:https:&#x2F;&#x2F;twitter.com&#x2F;burcs&#x2F;status&#x2F;1728081692273823863 reply worldsayshi 9 hours agorootparentOkay this one is actually cool. I&#x27;m hoping it&#x27;s not totally one-off scripted but it&#x27;s fun either way. :) reply Kiro 13 hours agoparentprevVideo of last one: https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=3Hye_47c0Pc reply gmueckl 14 hours agoprevDoes this work on Wayland? reply HellsMaddy 10 hours agoparentTested on Sway with both Firefox and Chromium. Resizing works (no surprise) but the position doesn&#x27;t work. window.screenX&#x2F;Y always return 0. reply nikeee 7 hours agoprevYou can use a BroadcastChannel to send messages between the Windows directly. This should reduce latency when working locally. reply kaycebasques 14 hours agoprevVery cool. What&#x27;s the working title for this interface &#x2F; experience? reply system2 10 hours agoparentLocalStorage API. reply bdcravens 6 hours agoprevLove how simple the code is, with almost no external dependencies reply aantix 15 hours agoprevOr a postMessage could be used if one tab is opened from the other. reply placebo 14 hours agoprevmade something similar 14 years ago...https:&#x2F;&#x2F;youtu.be&#x2F;bEwsuWNxdnodomain isn&#x27;t mine anymore... reply HatchedLake721 13 hours agoparentOh wow, from December 2008.Here&#x27;s to no one trying to patent it now! reply whenlambo 12 hours agoparentprevWhat API allowed that back then? reply placebo 12 hours agorootparentIt was an API I developed using ActiveX on Microsoft&#x27;s Internet explorer and NPAPI on other browsers reply worldsayshi 9 hours agorootparentprevI don&#x27;t see what is technically new about this? What API has been missing to do this?Tcp and window.screenTop and screenLeft should get you there right? Hmm, yeah, 2008 is a long time ago though.. reply awei 10 hours agoprevI love the concept and the demo! Amazing creative work reply adroitboss 14 hours agoprevTwo more examples of multiple windows as a single unit. This time on the video game side.(Ludum Dare 35 Overall Winner) WindowFrame: https:&#x2F;&#x2F;youtu.be&#x2F;CY7NUHn6GQg?t=273Window Kill: https:&#x2F;&#x2F;youtu.be&#x2F;7iP68FZWVxM?t=160 reply throwaway7679 10 hours agoparentMetal Marines, a 1993 game with multiple windows that interact with each other: https:&#x2F;&#x2F;youtu.be&#x2F;KiiAUP1fG14?t=297 reply butz 13 hours agoprevI&#x27;m wondering about practical application of this demo... reply bartvk 1 hour agoparentIt would not surprise me if the original practical application is long gone, but it&#x27;s now used to fingerprint browsers for ad serving purposes. reply sntran 8 hours agoparentprevI have always been fascinated by those sci-fi movies in which a person would swipe up on their tablet, and the current window would fly to another monitor.This maybe be something that can achieve that. reply worldsayshi 9 hours agoparentprevMaybe making colour mixing ven diagrams? reply p-nerd 11 hours agoprevReally great. The web is really going forward reply bottled_poe 5 hours agoparentSeems like this was all possible 10+ years ago with existing APIs at that time. I don’t see what’s so special tbh. reply 38 15 hours agoprevcorrect URLs:https:&#x2F;&#x2F;twitter.com&#x2F;_nonfigurativ_&#x2F;status&#x2F;172732259457002734...http:&#x2F;&#x2F;farside.link&#x2F;twitter.com&#x2F;_nonfigurativ_&#x2F;status&#x2F;172732... reply no_time 14 hours agoprevW-why is window LOCATION exposed to websites? Apparently even in FF private browsing...At first I thought this is an electron only thing. I get it that we gave up on fingerprinting resistance but this is like pissing on it&#x27;s grave.EDIT: privacy.resistFingerprinting set to true fixes the coords to 0 reply thayne 10 hours agoparentOn Wayland, a native window can&#x27;t even get that information. At least not in a way that works across composiotors. reply a_wild_dandan 14 hours agoparentprevI thought you meant literal `window.location` and was confused, but yeah `window.screenX&#x2F;Y` is a pretty suspect feature, now that you mention it. reply crazygringo 14 hours agoparentprevI&#x27;m as surprised as well.I&#x27;m trying to guess why, and can only imagine that at some point the idea was to support multi-window webapps, like maybe you&#x27;d have separate windows for documents and different tool palettes, and they&#x27;d want to know each other&#x27;s locations to try to prevent overlaps? I mean, back in the day, framesets were another idea that experienced some popularity before being totally eclipsed by-- all of this back in the era when everything was based on the multiple-windows paradigm rather than the multiple-tabs paradigm. Window.screenLeft apparently originated in IE [1].Just a guess though. Very curious if there&#x27;s any legitimate reason for it to continue to exist, or whether W3 ought to deprecate it.[1] https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;Window&#x2F;scre... reply munch117 14 hours agoprevAm I the only one that&#x27;s horrified that window.screenX and window.screenY exist? This is not information that I want to provide. reply progbits 14 hours agoparentUsing the demo in https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;MouseEvent&#x2F;... it seems Firefox pretends the window is always at (0,0). So that&#x27;s nice. reply Cyphase 11 hours agorootparentYou likely have `privacy.resistFingerprinting` set to true. reply fyokdrigd 6 hours agorootparentas everyone should!people forget lesson from recent past. i don&#x27;t know when google strong armed w3c to include this back... but we had this since ie4 days and then disabled everywhere since it was abused left and right for click jacking attacks. reply Andrews54757 14 hours agoparentprevYep. There are some ridiculous web APIs out there that expose all sorts of things you&#x27;d think should remain private. Take the Battery Status API for example [1].[1] https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;Battery_Sta... reply geoelectric 13 hours agorootparentIIRC, that one and a number of other lower-level “WTF” APIs were introduced for Firefox OS. They made a little more sense in the context of providing an interface for mobile phones using the browser stack. reply david_allison 14 hours agorootparentprevThis would be useful in WebXR (given opt-in) reply zer0kool 13 hours agoparentprevSorry, i&#x27;m a little lost. I tried thinking about it, but I can&#x27;t think of any nefarious attacks someone could do with screenX&#x2F;Y. Why is that such a risk? What could they possibly learn from that other than screen size? reply munch117 12 hours agorootparentThe fingerprinting potential may well be limited, but it&#x27;s the principle of the thing: The browser should not provide unnecessary information about its surroundings.It would be another matter if this was a very useful feature, but I don&#x27;t think that it is.Here&#x27;s one use for it: Ads could sense that they are off-screen and withhold content until the ad is moved back in view. I could easily see screenX&#x2F;Y being used for that. I mean, I hope not, but how am I going to stop it? reply jackewiehose 12 hours agorootparentprevBecause fingerprinting. Shit like this is the reason you are supposed to use the Tor browser only with a tiny window. reply spiderice 11 hours agorootparent> you are supposed to use the Tor browser only with a tiny windowI hadn&#x27;t ever heard of this. Here is a statement from the Tor project for anyone else wondering what this is about:> We automatically resize new browser windows to a 200x100 pixel multiple based on desktop resolution which is provided by a Firefox patch. To minimize the effect of the long tail of large monitor sizes, we also cap the window size at 1000 pixels in each direction. In addition to that we set privacy.resistFingerprinting to true to use the client content window size for window.screen, and to report a window.devicePixelRatio of 1.0. Similarly, we use that preference to return content window relative points for DOM events. We also force popups to open in new tabs to avoid full-screen popups inferring information about the browser resolution. In addition, we prevent auto-maximizing on browser start, and inform users that maximized windows are detrimental to privacy in this mode reply pixel8account 38 minutes agorootparentFingerprinting by screen size is real. I&#x27;ve checked one day, and it turned out my browser viewport size when maximized is super unique (like The original work by Bjorn Staal used localStorage, but I found sockets more fun, because if tweaked a bit, this can be shared with friends.Who among us hasn&#x27;t overcomplicated things for fun? :) reply sprobertson 12 hours agoprevMight as well link to the original &#x2F; \"stolen from\" author&#x27;s equivalent demo with Three.js (open this in multiple overlapping windows) - https:&#x2F;&#x2F;bgstaal.github.io&#x2F;multipleWindow3dScene&#x2F; - and repo - https:&#x2F;&#x2F;github.com&#x2F;bgstaal&#x2F;multipleWindow3dScene&#x2F; reply andybak 14 hours agoprevThe original seems to be on LinkedIn. Does anyone else find it weird when geek content is posted to LinkedIn? It&#x27;s the last place I&#x27;d think to look. Am I missing out or is this an outlier? reply pixel8account 34 minutes agoparentI&#x27;ve recently started posting my content on LinkedIn (usually just linking to my blog or publication with a short explanation). Main reason is that I&#x27;m a bit unhappy with the new Twitter (a place where I used to share the most), approximately nobody reads Mastodon so I can&#x27;t stop there, and all least I already have LinkedIn so it&#x27;s low cost to share there. reply zerojames 14 hours agoparentprevI work in computer vision and like hearing about the latest thing. I have been finding LinkedIn increasingly useful as a way to learn about machine learning.There are _a lot_ of posts that I have to scroll through of little to no value, but every so often I see a post by Yann LeCun or the lead developer advocate at TensorFlow or a paper someone has liked that catches my eye or a new model release.\"Machine learning LinkedIn\", as it were, is a real thing; not sure whether this applies to other disciplines. reply Geee 2 hours agorootparentYann posts mostly on X. reply cpach 14 hours agoparentprevFor sure. It’s approximately the first time I’ve seen something like that on LinkedIn. reply bionhoward 15 hours agoprev [–] Wow this seems profound, I didn’t even know you could do that, that could be the future of augmented reality if you think about it, layers! Great job! reply jonhohle 14 hours agoparentIs it really different from any multiplayer game? Two view ports, shared coordinates, independent renderers. The “trick” is that the view ports are overlapping, but I would imagine sendMessage might be faster and not require any server round trip. reply simbolit 15 hours agoparentprev [–] Could you please expand on your \"if you think about it\"? reply Takennickname 14 hours agorootparent [–] Spoiler: he didn&#x27;t. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores how two browser windows can share screen dimensions, window size, and position using socket clients.",
      "The author prefers using socket clients over localStorage due to the entertainment value and ease of sharing with friends.",
      "A demo and codebase are available for reference."
    ],
    "commentSummary": [],
    "points": 315,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1700921809
  },
  {
    "id": 38415875,
    "title": "NVIDIA sued for trade secret theft: employee's blunder exposes rival's code",
    "originLink": "https://www.engadget.com/nvidia-sued-for-stealing-trade-secrets-after-screensharing-blunder-showed-rival-companys-code-063009605.html",
    "originBody": "Read full article NVIDIA sued for stealing trade secrets after screensharing blunder showed rival company's code Valeo said NVIDIA saved millions of dollars by stealing its trade secrets. Mariella Moon Contributing Reporter Thu, Nov 23, 2023, 11:30 PM MST·2 min read 0 I-HWA CHENG via Getty Images NVIDIA is facing a lawsuit filed by French automotive company Valeo after a screensharing blunder by one of its employees. According to Valeo's complaint, Mohammad Moniruzzaman, an engineer for NVIDIA who used to work for its company, had mistakenly showed its source code files on his computer as he was sharing his screen during a meeting with both firms in 2022. Valeo's employees quickly recognized the code and took screenshots before Moniruzzaman was notified of his mistake. To note, Valeo and NVIDIA are working together on an advanced parking and driving assistance technology offered by a manufacturer to its customers. Valeo used to be in charge of both software and hardware sides of the manufacturer's parking assistance tech. In 2021, however, the the bigger corporation won the contract to develop its parking assistance software. Valeo wrote in its lawsuit that its former employee, who helped it develop its parking and driving assistance systems, had realized that his exposure and access to its proprietary technologies would make him \"exceedingly valuable\" to NVIDIA. Moniruzzaman allegedly gave his personal email unauthorized access to Valeo's systems to steal \"tens of thousands of files\" and 6GB of source code shortly after that development. He then left Valeo a few months later and took the stolen information with him when he was given a senior position at NVIDIA, the complaint reads. He also worked on the very same project he was involved in for Valeo, which is why he was present at that video conference. Valeo said its former employee admitted to stealing its software and that German police found its documentation and hardware pinned on Moniruzzaman's walls when his home was raided. According to Bloomberg, he was already convicted of infringement of business secrets in a German court and was ordered to pay €14,400 ($15,750) in September. In a letter dated June 2022, NVIDIA's lawyers told the plaintiff's counsel that the company \"has no interest in Valeo's code or its alleged trade secrets and has taken prompt concrete steps to protect [its] client’s asserted rights.\" Valeo still sued the company earlier this month, however, and said that NVIDIA has \"saved millions, perhaps hundreds of millions, of dollars in development costs, and generated profits that it did not properly earn and to which it was not entitled\" by stealing its trade secrets. This is but another proof that competition continues to heat up in the autonomous driving market. Back in 2017, Waymo accused Uber of colluding with its former employee, Anthony Levandowski, to steal over 14,000 confidential and proprietary design files. Levandowski was sentenced to 18 months in prison, but he was pardoned six months later by then President Donald Trump.",
    "commentLink": "https://news.ycombinator.com/item?id=38415875",
    "commentBody": "Nvidia sued for stealing trade secrets: blunder showed rival company&#x27;s codeHacker NewspastloginNvidia sued for stealing trade secrets: blunder showed rival company&#x27;s code (engadget.com) 311 points by bookofjoe 14 hours ago| hidepastfavorite188 comments crazygringo 9 hours agoI seriously do not understand why any employee would steal their previous employers&#x27; code to use at a new employer.There&#x27;s little-to-no personal upside, and only horrible downside if you get caught.I mean, this guy:> Moniruzzaman allegedly gave his personal email unauthorized access to Valeo&#x27;s systems to steal \"tens of thousands of files\" and 6GB of source code shortly after that development... Valeo said its former employee admitted to stealing its software and that German police found its documentation and hardware pinned on Moniruzzaman&#x27;s walls when his home was raided.And while Nvidia presumably hired him for his expertise, they certainly didn&#x27;t expect him to be stealing code, not even wink-wink-nudge-nudge. Corporate lawyers at trillion-dollar-companies take this stuff super seriously.So this guy puts himself at massive legal risk... for what? So he can slack off for a few months while he pretends to write code that&#x27;s already been written -- and gets to browse Reddit? Or so he can deliver code extra-fast in hopes of a quicker promotion -- that may or may not come? Is that really worth it?It&#x27;s crazy to me. Why would you risk that? reply yumraj 8 hours agoparentYou’re assuming all&#x2F;large portion of the stolen code was developed by him and he could have even written all of it at the NVIDIA.Maybe he needed all that to actually perform in the new job. reply fbdab103 3 hours agoparentprevIf nothing else, it seems pretty silly to have the stolen source accessible from the new work computer. That&#x27;s just asking for the ruse to be detected too easily (automated backups, virus scanning, etc which could fingerprint the data). Keep the illicit goods on an air-gapped, encrypted, personal computer that you can reference as required.As far as I know, they were never able to find the Waymo files that Anthony Levandowski stole, because he was at least crafty enough to not load everything directly onto Uber hardware. reply tremere 9 hours agoparentprevThere is tremendous upside. You can look like a rockstar at the new company and propel yourself upwards with that momentum. reply Blammar 9 hours agorootparentMore like a shooting star once your perfidy is discovered. reply TeMPOraL 9 hours agorootparentAs with any other crime, the trick is to not get caught! reply justrealist 7 hours agorootparentprevYou could easily milk it for 4-5 years and then transition into management before you have to pull your own weight as an IC. reply Cacti 2 hours agorootparenthaha god it’s so true it hurts reply jakobson14 8 hours agorootparentprevPure mythology. reply feelandcoffee 9 hours agoparentprevI wonder if this would become more common with things like ChatGPT.Let&#x27;s say you&#x27;ve been working in place A, you show your code to an LLM service (like the dozen or so Copilot-like services) and tell them to refactor. And for the sake of argument, let&#x27;s say the LLM uses your code and questions for its next training dataset.A few years pass, then you go to work at Place B, and ask a question that happens to be related to the problem that Place A&#x27;s code solved, and they give you Place A&#x27;s code as is. reply sircastor 8 hours agorootparentFor this reason, and a few others, my workplace simply put a blanket ban on these kinds of tools. If our code is never exposed to the learning tool, it’s never in danger of being showing up somewhere else.Incidental to that, I feel like these tools expose the reality behind “copyrighting code&#x2F;math” and how fallacious it is. If the tool can generate the efficient methods of achieving a result, I think it becomes obvious that one shouldn’t be able to protect it via IP law. reply dylan604 4 hours agorootparentJust like with social media, all it takes is one person to not honor that request, and boom! your shit is out there. Sure, you can fire the offending party, but you can&#x27;t just ask Co-pilot to not use your contributions. That&#x27;s like asking the internet to give those pictures back. It ain&#x27;t gonna happen. reply ludston 32 minutes agorootparentIt&#x27;s quite a different from the analogy you suggest, as copilot is controlled by a single organisation and we know the address. reply treprinum 7 hours agorootparentprevIf you use GitHub, you feed OpenAI with your code as training data already, with GitLab you do the same for Google. reply galangalalgol 6 hours agorootparentIf you use on-prem gitlab, presumably that is not the case. reply thfuran 3 hours agorootparentprev>If the tool can generate the efficient methods of achieving a result, I think it becomes obvious that one shouldn’t be able to protect it via IP law.Why does that only hold when the result in question is in software? Machines are just tools for achieving results. reply nvy 3 hours agorootparentBecause you can patent a machine. The argument is that software is \"just math\" (because it literally is just doing binary arithmetic) and mathematics cannot be patented. reply bad_user 2 hours agorootparentMath should be patentable, too. I see no reason for why not.The old argument that it&#x27;s discovered rather than invented is bullshit. Multiple people can always have the same idea for an invention because we think alike and live in the same environment.Or just ban patents altogether. Of course, this may discourage companies from investing in R&D and that&#x27;s the real problem: how expensive is it to invent something, and does it justify a 20-year monopoly? But there are no good answers here, and trying to draw a line between math and non-math is bollocks. reply zarzavat 2 hours agorootparentThere’s just something obscene about patenting mathematics. The universe gifts us these truths and our first instinct is that it should be the property of a human.Patents exist to incentivize invention. As long as mathematicians are content to do mathematics for the love of it, and they certainly are, there’s no need for mathematical patents.Practically speaking, mathematical ideas are building blocks not products. Patents on mathematical ideas discourage invention rather than encouraging it because they prevent use of that idea in new products - an idea that would have been discovered anyway. For example the parents of elliptic curve cryptography and arithmetic coding were hugely damaging to invention overall. Patenting a new kind of cork screw doesn’t have this problem, it’s a destination, not an intermediate. reply bad_user 1 hour agorootparentThe universe is not based on math, says math.Math can be viewed as a product of how our minds work. We use abstractions to understand and predict the universe, but it&#x27;s always imperfect, and the theories always incomplete.E g., you&#x27;d think 1+1=2 is some universal truth, except integers don&#x27;t exist in nature, being just another abstraction that we came up with. And of course, people can rediscover integers repeatedly, but that just says more about how our mind works.And yes, math is a building block, but so is software. If math theories aren&#x27;t patentable, that should happen based on them being trivial or perhaps being too useful to society, and not due to some romantic notions of discovery and the universe. Software, too. reply thfuran 2 hours agorootparentprev>Practically speaking, mathematical ideas are building blocksSo are technological ones. reply thfuran 3 hours agorootparentprevBut a machine can also be mathematically described. Should that render it unpatentable, or will that have to wait until the grand unified theory of everything is sorted out? reply bennyg 8 hours agorootparentprevSelf-hosted LLM is really the only way to do this. reply Silhouette 8 hours agorootparentprevIf the tool can generate the efficient methods of achieving a result, I think it becomes obvious that one shouldn’t be able to protect it via IP law.But these kinds of tools can only do that because someone else already put in the work to write the solutions that are used to train their models. Isn&#x27;t this exactly the kind of situation when copyright is supposed to apply? reply kaliqt 8 hours agorootparentBut with enough training data, it&#x27;s not generating it because it remembers the exact code line for line, it does it because it knows that to be a good method. Especially if you ask it to refactor it, that&#x27;s a whole new creation even if it&#x27;s been done before by some engineer somewhere. reply bad_user 4 hours agorootparentIt&#x27;s still parroting what other people did, it&#x27;s not doing any math reasoning, and it&#x27;s not any different to LLMs seemingly able to compose prose or poetry.If you want to make an argument that math or software shouldn&#x27;t be copyrighted, LLMs actually make the case for stronger copyright protections. reply ekianjo 6 hours agorootparentprevYou can self host LLMs you know reply two_in_one 4 hours agorootparentprevfor this ChatGPT has a &#x27;private&#x27; mode in which your conversation exists only while you keep it open. It&#x27;s not used for training, an no human see it (presumably). The negative side is it disappears with no history, so you can&#x27;t continue next day. That was introduced after complains similar to yours. Some companies put a total ban. reply sillysaurusx 4 hours agorootparent. reply missedthecue 8 hours agoparentprevYou can be lazy and get lots of promotions and bonuses. That&#x27;s why. People risk major felonies to steal $5k from a bank. Getting fired on the (let&#x27;s be real) low risk you get caught is nothing reply dylan604 4 hours agoparentprevIs printing out and pinning the prints to the wall still a thing done IRL and not just a movie thing? We had to do prints to green-bar back in school days when we only had shared time at the school&#x27;s computer lab. But I haven&#x27;t considered printing code out since the early 90s. It seems so out of place in today&#x27;s time reply nvy 3 hours agorootparent>But I haven&#x27;t considered printing code out since the early 90s.TFA says he had documentation printouts, not code listings. That I can relate to; when learning something new or unfamiliar it&#x27;s nice to be able to flip back and forth through the physical copy, make annotations, etc. reply blt 6 hours agoparentprevMaybe he was offered the job at Nvidia on the condition that he arrives with Valeo&#x27;s code. reply dylan604 4 hours agorootparentProve that conversation didn&#x27;t happen. Of course it&#x27;s not going to be in writing. Of course they are denying it now. Otherwise, there&#x27;s literally no defense. So you either fall on the sword, or blame someone else. reply YetAnotherNick 8 hours agoparentprev> There&#x27;s little-to-no personal upside, and only horrible downside if you get caught.Here you just mention upsides with very small chance of small downside. See the case for Anthony Levandowski which was much more serious crime as he knowingly created and sold a company with only moat being Wyamo docs and everything bad that could happen to him did happen. He spent 6 months in jail and now he rejoined as CEO in Pronto. The much more probable case is he got to enjoy $680M that Uber gave him and not have to worry about money again. reply choppaface 1 hour agoparentprevHere&#x27;s a good example of an ex-Uber employee who stole &#x2F; took his excel-in-browser to his next employer: https:&#x2F;&#x2F;basta.substack.com&#x2F;p&#x2F;no-sacred-masterpieces\"but I took the code and shoved it into my back pocket for a rainy day.\"Discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37527720 reply Symmetry 13 hours agoprevOne of the generally accepted reasons why its hard to get graphics companies to open source their drives has always been that everybody is violating everybody else&#x27;s patents. And while everybody knows this making it too obvious is a legal disadvantage. But I hadn&#x27;t expected it was also true of copyright. reply justinclift 9 hours agoparentFor Nvidia, the most likely reason they&#x27;ve strongly avoided Open Sourcing their drivers isn&#x27;t anything like that.It&#x27;s simply a function of their history. They used to have professional level graphics cards (\"Nvidia Quadro\") using exactly the same chips as their consumer graphics cards.The BIOS of the cards was different, enabling different features. So people wanting those features cheaply would buy the consumer graphics cards and flash the matching Quadro BIOS to them. Worked perfectly fine.Nvidia naturally wasn&#x27;t happy about those \"lost sales\", so began a game of whack-a-mole to stop BIOS flashing from working. They did stuff like adding resistors to the boards to tell the card whether it was a Geforce or Quadro card, and when that was promptly reverse engineered they started getting creative in other ways.Meanwhile, they couldn&#x27;t really Open Source their drivers because then people could see what the \"Geforce vs Quadro\" software checks were. That would open up software countermeasures being developed.---In the most recent few years the professional cards and gaming cards now use different chips. So the BIOS tricks are no longer relevant.Which means Nvidia can \"safely\" Open Source their drivers now, and they&#x27;ve begun doing so. reply chii 8 hours agorootparentCustomers really, really hate segmentation of product [1], esp. if the product end up being exactly the same!I think companies should stop trying to segment customers. It&#x27;s a long term loss, despite a short term gain.[1] https:&#x2F;&#x2F;www.joelonsoftware.com&#x2F;2004&#x2F;12&#x2F;15&#x2F;camels-and-rubber-... reply justinclift 6 hours agorootparent> It&#x27;s a long term loss, despite a short term gain.While I dislike market segmentation as much as anyone, it seems like it worked out ok for Nvidia.It let them keep a (very) profitable segment of the business, which in turn financed other developments and let them become the GPU leader for a very long time.No idea how it&#x27;ll play out in the end of course, but we&#x27;re definite past the whole \"short term\" time frame. reply blagie 8 hours agorootparentprevI suspect there are a few issues:1) NVidia drivers had a lot of secret sauce to give high performance.2) NVidia for machine learning still has a lot of platform lock-in (although fading gradually), and cross-compatibility doesn&#x27;t help them3) Quite often, if you&#x27;ve licensed something from a third-party, you can&#x27;t legally open-source. Proprietary codebases sometimes get... messy.I&#x27;m jumping ship as soon as Intel drivers are good enough. I don&#x27;t trust AMD to have anything working -- too many bad experiences -- but Intel has a good track record. Arc A770 gives 16gb for $1000 NVidia card. I don&#x27;t need maximum FLOPS. So long as deep learning models run, and 3d apps are accelerated, I&#x27;m happy. reply FirmwareBurner 12 hours agoparentprev> everybody is violating everybody else&#x27;s patentsViolating patents is one thing, as you&#x27;re only violating the concept&#x2F;idea, but the implementation is still up to you meaning it will still be clean room design, whereas this guy also blatantly copied the source code and design files which is a slam dunk lawsuit, hence why no company ever wants to have competitors&#x27; IP on their systems. reply throwaway54_56 12 hours agorootparent> Violating patents is one thing, as you&#x27;re only violating the concept&#x2F;idea, but the implementation is still up to youThe concept&#x2F;idea is not what is patented. The patent is (or should be) for the specific execution of the idea. Competitors are free to implement their feature using methods other that what is covered by the patent, even if the end result gives the exact same functionality. reply dctoedt 11 hours agorootparent> The concept&#x2F;idea is not what is patented. The patent is (or should be) for the specific execution of the idea. Competitors are free to implement their feature using methods other that what is covered by the patent, even if the end result gives the exact same functionality.IP lawyer here (EDIT: not yours, of course): That&#x27;s a considerable (and potentially-dangerous) oversimplification. What matters is whether what you do comes within the claims of the patent.(For a more-detailed explanation, written in pseudocode-like terms, see a 2010 post I did: https:&#x2F;&#x2F;www.oncontracts.com&#x2F;how-patent-claims-work-a-variety....) reply dmoy 10 hours agorootparentYea, this is a more correct explanation. Not a patent lawyer, but raised by one lol.Tangentially, it gets difficult in software because a lot of patents are .... maybe overbroad in their wording of claims. Lot of ambiguous looking landmines.This is somewhat similar to business method patents (which were curtailed a little by the SC a decade ago, but were already known to be kinda sketchy for decade+ before that). Can&#x27;t patent a pure algorithm, for example. reply david-gpu 10 hours agorootparentprevAt some point I was told to never ever look at a competitor&#x27;s patents, because doing so would worsen the penalties if it turned out that our design infringed upon them. Can you confirm that&#x27;s true?Doesn&#x27;t that mean that in general it is also a really bad idea to ask an engineer questions about a particular piece of tech that they patented at a previous employer, even though the specific information is a matter of public record by virtue of being explained in the patent? reply toast0 10 hours agorootparentWillful infringement allows for up to triple damages. The expectation is you can&#x27;t do willful infringement if you&#x27;re not aware of competitor&#x27;s patents, and you can&#x27;t be aware of them if your policy is to never look at patent documents. Or that&#x27;s the idea anyway. reply dctoedt 6 hours agorootparent> The expectation is you can&#x27;t do willful infringement if you&#x27;re not aware of competitor&#x27;s patents, and you can&#x27;t be aware of them if your policy is to never look at patent documents. Or that&#x27;s the idea anyway.\"Willful blindness\" can be a danger (according to the Supreme Court, albeit in a different context).Possibly a bigger danger: Your product gets kicked out of the market by an injunction (a court order to stop making, using, selling, etc.) reply pierat 9 hours agorootparentprevJust search it in Yandex with a VPN, and in a Tails VM just for paranoid-icity.I did similar for medical self-symptom before ACA prevented \"pre-existing condition\" scam. reply teddyh 6 hours agorootparentThat stuff is exactly what the Tor Browser is for: . No need for any of that other stuff. reply IG_Semmelweiss 8 hours agorootparentprevQuestion, in this case, is it a violation of patents + trade secrets, or just the patents? reply dmoy 6 hours agorootparentThe case in the original article is not patents at all. Closer to copyright? Idk if it&#x27;s actually copyright or some other trade secret law (? sorry, don&#x27;t know much about non-patent IP law) reply FirmwareBurner 12 hours agorootparentprev>The concept&#x2F;idea is not what is patented. The patent is (or should be) for the specific execution of the idea.Have you ever seen patents? They rarely cover the implementation details, or at most they&#x27;re intentionally super vague about that, most of the time it&#x27;s just the general idea on how the widget would work and what it does, but not how to implement it technically. reply throwaway54_56 12 hours agorootparentI have seen patents. The whole point is to share a method of doing something, in return for exclusive use of that method for a period of time. That&#x27;s the theory, anyway. reply anon373839 11 hours agorootparentI’m sure we’ll all be glad one day that Apple shared this research breakthrough:https:&#x2F;&#x2F;www.theverge.com&#x2F;2017&#x2F;11&#x2F;6&#x2F;16614038&#x2F;apple-samsung-sl... reply criddell 10 hours agorootparentA good example of a patent that was challenged in court and wasn’t totally invalidated is Amazon’s 1-click ordering. They patented storing customer shipping and payment details in a database so they could purchase something with a single click.It expired in 2017 but for the period it was in force, Amazon collected millions in licensing fees. reply SoftTalker 6 hours agorootparentPatents really shouldn&#x27;t be granted when any competent junior engineer could have designed and implemented the feature. This method is doesn&#x27;t pass the \"nonobvious\" test. reply User23 5 hours agorootparentBatteries used to have cardboard instead of metal shells. Because of this batteries used to leak prolifically. Then an inventor patented the modern metal shelled battery. His competitors all started infringing so he sued. They claimed that the invention was \"obvious.\" The judge ruled that it clearly wasn&#x27;t obvious, because if it had been they wouldn&#x27;t have been making the obnoxiously stupid cardboard batteries for so many years. reply objclxt 11 hours agorootparentprevYou realize the patent you’re referring to was a design patent, not a utility patent? They are very different, the former only covers look and feel, not method. reply anon373839 9 hours agorootparentI wasn’t aware of that. But it’s even more baffling: how did they qualify for a patent on the design of a basic sliding latch? reply FirmwareBurner 11 hours agorootparentprevThen you misunderstood or saw too few patents. reply BaculumMeumEst 9 hours agorootparentprev> They rarely cover the implementation detailsif you can’t figure out how its implemented you’re looking at an invalid patent or an application> they&#x27;re intentionally super vague about thatyes that’s how claims work> most of the time it&#x27;s just the general idea on how the widget would work and what it does, but not how to implement it technicallyyou’re looking at an invalid patent or an application reply atq2119 12 hours agorootparentprevIdea is a pretty general term. I have a bunch of patents and I would describe them all as patenting an idea (for how to achieve some goal).The implementation or execution of the idea usually takes the form of some Verilog or some C++. That is covered by copyright.The patent is for the idea. Which is part of why I&#x27;m so opposed to patents, not just in software. In other fields, like medicine, patents are perhaps for discoveries, which are IMHO similarly valuable as the execution. But ideas aren&#x27;t that valuable, or shouldn&#x27;t be. reply conradev 11 hours agorootparentprevThe specific execution of an idea is also an idea, thoughI feel like the granularity of patents is defined more so by where the frontier of knowledge is for a given domain than the patent office (i.e. what is hard but also valuable). But, I also haven’t spent a lot of time with patents reply ric2b 9 hours agoparentprevAMD has open source drivers so it&#x27;s basically just NVIDIA that doesn&#x27;t.Unless you already want to count intel, not sure if they count as actual products or are still in early access. reply skavi 9 hours agorootparentIntel’s Linux drivers are also open source IIRC. reply SushiHippie 4 hours agorootparentprevhttps:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;open-gpu-kernel-modules https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;nvidia-releases-open-sourc... reply ip26 12 hours agoparentprevGiven independent invention is apparently not a defense against infringement, that makes a lot of sense. I can’t even imagine trying to screen the codebase for that. reply sokoloff 12 hours agorootparent\"Infringement\" is not specific enough there. Independent invention is not a defense against patent infringement, but is a defense against copyright infringement. reply chii 8 hours agorootparentIt&#x27;s not easy to prove independent invention as a defense, because you have to have set it up to do so in the first place, not after the fact. reply kahnclusions 12 hours agorootparentprevNo, but a patent can be invalidated if you can show that the idea is obvious to practitioners of the trade, i.e. given the same problem most software engineers would arrive at the same solution. reply randombits0 8 hours agorootparentSame with the copyright. Purely functional code is not creative expression and cannot be copyrighted. reply cyanydeez 11 hours agoparentprevand now we have AI violating everything. reply pritambaral 13 hours agoprevTFA makes it sound as if the entirety of the blame can be placed on one employee. Sure, his actions do seem to support that view, but then again, Nvidia did hire him precisely for his previous experience at this rival company, on the very same project that the two companies were partnered on, which is the same project that Nvidia hired him for.There is no argument to be made that Nvidia wasn&#x27;t aware he&#x27;d be coming with secrets. The argument that that&#x27;s precisely why he was hired, OTOH, is looking very strong. reply juunpp 13 hours agoparent> Nvidia did hire him precisely for his previous experience at this rival company, on the very same project that the two companies were partnered on, which is the same project that Nvidia hired him for.Yes, this happens all the time.> There is no argument to be made that Nvidia wasn&#x27;t aware he&#x27;d be coming with secrets.This is not a logical conclusion from the above. Hiring for the exp is fine. Hiring for the trade secrets obviously is not. No serious company would do the latter, esp a company the size of Nvidia. reply zik 12 hours agorootparent> No serious company would do the latterIt happens literally all the time:[1] https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2019&#x2F;aug&#x2F;27&#x2F;anthony-l...[2] https:&#x2F;&#x2F;www.theverge.com&#x2F;2022&#x2F;8&#x2F;22&#x2F;23317502&#x2F;xiaolang-zhang-p...[3] https:&#x2F;&#x2F;www.justice.gov&#x2F;usao-ma&#x2F;pr&#x2F;former-engineer-sentenced...[4] https:&#x2F;&#x2F;apnews.com&#x2F;article&#x2F;korea-samsung-china-copycat-semic...[5] https:&#x2F;&#x2F;www.reuters.com&#x2F;technology&#x2F;twitter-may-face-difficul... reply beardedwizard 11 hours agorootparentThis proves individual employees routinely steal trade secrets; yes, they do. It does not prove that the companies they join (US public companies in the United States anyway) willfully use it.Think about the large scale conspiracy required to keep something like that secret from an entire company. reply mcpackieh 10 hours agorootparentThere&#x27;s no need to rope conspiracies into this. The structure of incentives in corporate environments have people in the company not looking for something they don&#x27;t want to find. Managers would rather not know about violations and be able to tell their boss that their team are pros who got the job done fast, than investigate their reports for violations and quite possibly find a big mess that needs to be cleaned up.Point is, shit that shouldn&#x27;t happen routinely does happen anyway. \"That wouldn&#x27;t happen because it would be illegal\" is generally bullshit. reply juunpp 7 hours agorootparentCompanies of this size have routine code audits that would find copyright infringements of this sort. If the employee literally brought in source code files and not even change the name of the directory like it was highlighted here, it&#x27;d have been caught. The audit is also not conducted by the manager, but a third party. There is no possible way this would fly unless the individual intentionally stripped the source of copyright notices (the individual&#x27;s fault), or the source code was just sitting on their laptop and never acted upon, etc.So I think the question (and I&#x27;m not a lawyer...) is whether Nvidia conducted such audit and to what extent (if any) the other company&#x27;s source code was merged into theirs (versus just sitting on the laptop). reply winocm 13 hours agorootparentprevA thought that came to me recently in the shower: Isn&#x27;t all knowledge effectively based on previous knowledge, and by extension, experience?i.e: A programmer knows how to do X, leaves a company to do Y, where Y is in the same field of work as X. Doesn&#x27;t X still affect the programmer on a subconscious level and henceforth, their thoughts indirectly? reply kmeisthax 12 hours agorootparentThis is the \"inevitable disclosure\" argument - AKA the idea that the experience is the secret, and thus nobody should ever be allowed to switch employers ever again.For various reasons (notably, the fact that slavery is illegal), we don&#x27;t accept this in general. You have to show that secrets were copied in full. Employees cannot memorize millions of lines of source, they can only memorize vague architectural details that would be easily reverse engineered by competitors. If you want to own those vague details, get a patent, or shut up. reply throwaway914 11 hours agorootparentIn theory, one can memorize a great deal.. I do agree with you, though. reply whatshisface 11 hours agorootparentMemorizing a sizable fraction of Windows source code so you can take it to Apple sounds like the most pointless and difficult heist imaginable! reply zer00eyz 9 hours agorootparentThis sounds like the plot to a Netflix movie... reply jzombie 13 minutes agorootparent\"Windowed Gardens: The Source Code Heist\" featuring Rainman reply juunpp 7 hours agorootparentprevhttps:&#x2F;&#x2F;wiki.winehq.org&#x2F;Developer_FAQ#Who_can.27t_contribute...Wine does not allow people who have worked on Windows to contribute precisely for this reason, with some nuances as noted in the link above. replyjfim 13 hours agorootparentprevYou can bring your expertise in X without the source code that does X. The former is legal, the latter is not. reply FirmwareBurner 12 hours agorootparentprevBringing knowledge is one thing, which is legal, but stealing source code and design files from your employer to copy it to the systems of their competitor where you now work is a completely different thing which is illegal.Companies want your knowledge, not you bringing proprietary IP from their competitor to work, as they know that&#x27;s a very expensive lawsuit waiting to happen. reply wombatpm 12 hours agorootparentBut what if I have an idemic memory? reply FirmwareBurner 12 hours agorootparentThen you&#x27;d be able to draw and type out everything from scratch directly on your employer&#x27;s PC and not have to download it via USB drives or email, like this guy did. reply chii 8 hours agorootparentSo if you obtained an idemic memory via installing a camera, flash drive and io ports into your brain?How or why is the mere transport method of information the distinction between infringement of IP vs not? reply themoonisachees 10 hours agorootparentprevIdemic or \"photographic\" memory as depicted in popular culture does not exist. most of the people who can remember a lot of information very quickly are actually on some level of deliberately using mnemotechnique methods, such as nursery ryhming (like that one released soldier in vietnam) or graph traversal. it&#x27;s preposterous to assume that anyone could recite fragmented data such as source code simply by being exposed to it (ie not by purposefully memorizing it) reply anonymouskimmer 11 hours agorootparentprevThen copyright comes in to play. You should have been taught how to avoid plagiarism in school. Use those techniques to reimplement. These techniques involve more than just using synonyms and rephrasing. reply hobs 7 hours agorootparentprevThen it&#x27;s fine in the United States at least - all the laws are about copying data&#x2F;files&#x2F;assets not about using your mind. reply unethical_ban 11 hours agorootparentprevThen congratulations, you&#x27;re able to ~~excel~~ exfil more data.We live in an imperfect world and solve problems as best we can. reply SoftTalker 6 hours agorootparentprevThis is why companies want employees to sign noncompete agreements (in jurisdictions where they are not illegal). reply fnord77 10 hours agorootparentprevAnd nobody at NVDA noticed when the employee pushed several GB of code into NVDA&#x27;s repos? reply juunpp 7 hours agorootparentWhere is the proof that the source code was merged? The article only mentions that former employees caught an eye of the source on the individual&#x27;s computer; it does not mean the source code was merged or acted upon. reply Guthur 12 hours agorootparentprevnext [2 more] [flagged] bsder 10 hours agorootparentAgreed.NVIDIA has always been a no holds barred competitor willing to push the boundaries of both truth and legality. reply devmor 13 hours agorootparentprevnext [6 more] [flagged] takinola 13 hours agorootparentRisk is always balanced against reward. I doubt Nvidia is culpable here not because I have some strong belief in their morality but because I trust they would not take stupid tradeoffs. reply margalabargala 13 hours agorootparentIMO this is more of a \"don&#x27;t ask don&#x27;t tell\" thing. I&#x27;m sure there was never an explicit agreement that the employee would bring trade secrets, but they can promise to be able to build for Nvidia what was built at the last company, and Nvidia could say \"yes I want that\", and not audit the new employee&#x27;s dev environment. reply FirmwareBurner 12 hours agorootparent>IMO this is more of a \"don&#x27;t ask don&#x27;t tell\" thing.I assure you isn&#x27;t. If your company (in the law abiding west) has any suspicion you&#x27;re using another company&#x27;s illegally obtained proprietary IP, they won&#x27;t see you as some hero doing God&#x27;s work and put you on the promotion track wile closing a blind eye to what you&#x27;re doing, but they immediately ask you to delete everything and every trace related to that.Foreign IP is radioactive and they don&#x27;t want to get sued because you&#x27;re bringing some source code and PDFs from their competitor, which might not even be that useful for them anyway.There were even cases of companies ratting out their employees they found using IP they stolen from their previous employers and getting them arrested, because if you stole IP from their competitor what&#x27;s stopping you from also stealing from them?>not audit the new employee&#x27;s dev environmentAudit how? Against what? Stolen foreign source code you don&#x27;t have? That&#x27;s just not realistically possible to audit every employees work and accurately determine if they are or not reusing source code they stolen form a competitor, especially if the employee doing this is careful to change or redact what he&#x27;s checking in.Only thing you can audit is against FOSS code that is public, but not if it&#x27;s stolen proprietary code and the employee made sure to not check-in anything giving away the origin of the original IP holder. They didn&#x27;t catch this guy until he got sloppy and made this huge blunder.You can never secure everything and audit everyone, especially if you want people to get any work done and not feel violated, so everything boils down to trusting employees they won&#x27;t steal from you, and trusting the legal framework and law enforcement they&#x27;ll do their job when in need, so you just have everyone sign NDAs and hope for the best. reply Retric 13 hours agorootparentprevThe exposure was unexpected so the risk was likely perceived as low.That said, the discovery process could clear NVIDA. reply actionfromafar 13 hours agorootparentprevThat puts a lot of faith in a lot employees. reply takinola 13 hours agoparentprevI don&#x27;t have direct knowledge of this company or the parties involved but I would be highly doubtful that Nvidia would want to have an employee steal the secrets of a competitor&#x2F;partner. In my experience, companies of this size would aggressively not want tainted IP inside their companies. He would need to be bringing across something as valuable as AGI, cure for cancer, etc for it to be even worth considering. There are numerous examples of companies being offered trade secrets of their competitors and reporting it back to the FBI just so they can avoid even the suspicion of stealing corporate secrets.If you think about it for a second, it is kind of obvious. Pretty much every technology is reproducible with the right amount of talent, funding and time. Why commit a crime when you can simply throw money (of which you have a lot) at the problem? Responsible corporate officers know this and act accordingly. reply ChrisMarshallNY 13 hours agorootparentThe company I worked for, was paranoid as hell about IP in the code. They hired some source scanning firm, for a lot of money, to continually scan our codebase.They were mostly looking for GPL (nasty, naasssssty GPL!) code, but they also scanned for code that couldn&#x27;t be accounted for in our \"clean\" repos. Not exactly sure how that worked (or even, if it worked at all. I think they brought smoke[0]).[0] https:&#x2F;&#x2F;www.tell-a-tale.com&#x2F;nasreddin-hodja-story-smoke-sell... reply andy99 12 hours agorootparent> nasty, naasssssty GPL!What does that mean? Why would scanning for gpl code be looked at badly? It presumably means a company is proactively abiding by gpl licensing. The only thing better would be to use gpl and share their source as well. But of course it&#x27;s a legit choice to just not use any gpl&#x27;d code.It&#x27;s probably more common to just turn a blind eye to gpl code, so it&#x27;s good to see companies making sure they&#x27;re on the right side of it. reply ChrisMarshallNY 12 hours agorootparentIt was a joke.I&#x27;m not a fan of \"viral\" licenses, and agree that, if a company doesn&#x27;t want to abide by the license, they should not include them, but I am also not a fan of trying to force others to force others, to force others, etc., ad nauseam.I tend to use MIT, which isn&#x27;t always everyone&#x27;s cup of tea, but means that you can use my code, and it would be nice to be credited, but I won&#x27;t cry myself to sleep, if you don&#x27;t. reply Thorrez 10 hours agorootparentThe MIT license requires giving credit. The difference between MIT and GPL is that GPL requires sharing the modified source code and licensing it the same. reply ChrisMarshallNY 9 hours agorootparentYeaaahhh... I&#x27;m not going to argue about this. It&#x27;s basically \"Religion and politics,\" in this crowd, and discussions don&#x27;t end well.I apologize for my joke. reply Thorrez 4 hours agorootparentHuh, I wasn&#x27;t trying to argue one license is better than the other. I was just trying to clarify what the licenses require. I didn&#x27;t want someone to see your comment and think that it would be ok to use someone else&#x27;s MIT-licensed code in a product without giving credit. replymangamadaiyan 9 hours agorootparentprevFunny, I too worked for a company that did exactly this. The scanner was called \"Black Duck\" or some such. reply ChrisMarshallNY 8 hours agorootparentI don’t remember the name of the company, but it was a single word that began with “P,” (I think). reply MertsA 12 hours agorootparentprevWould Nvidia the company want to engage in this? No. Would some middle manager involved in poaching this guy from the competitor want to do it? I have my suspicions. It takes two to tango and Nvidia didn&#x27;t catch this themselves which raises some red flags. How was he hired? What kind of compensation was he able to negotiate? Was it well above the compensation Nvidia would ordinarily pay for an engineer of his level? How did he introduce the code into Nvidia&#x27;s version control? Were there obvious red flags about the \"development\" pace that should have raised eyebrows during peer review?I work at a big tech company and if I tried something similar, I&#x27;m pretty sure it would be caught internally. Even if I managed to pull it off, all it could realistically give me is a foot in the door. Some sketchy hiring manager isn&#x27;t going to be able to just sweep some $500,000 signing bonus under the rug and $100k isn&#x27;t unheard of for regular engineers here anyways. As far as compensation and promotion opportunities afterwards it stands little chance of mattering for that either. For the first few months nothing I did was even used performance reviews and it&#x27;s a peer driven process to rate&#x2F;promote engineers.Combined that means that even if I wanted to do this, and I found a corrupt hiring manager that wanted to play ball, I&#x27;d have to sit on that IP for a few months after being hired, slowly introduce it into the codebase, alter it in response to peer review and to fit the new code base&#x27;s coding styles, etc. In the end, that would prove useful for a grand total of one peer review cycle and then it&#x27;s sink or swim on my own merits from that point forward.All that to say, yes Nvidia doesn&#x27;t want this kind of thing as a company, but there are still individuals who potentially stand to benefit and there&#x27;s a lot of opportunities for Nvidia to catch this before it&#x27;s accidentally shown on screen to the competitor it was stolen from this far down the line. I don&#x27;t know much about Nvidia&#x27;s corporate structure but it kinda seems like they&#x27;re trying to avoid finding out about it rather than trying to actually prevent it. reply FirmwareBurner 10 hours agorootparent>Would some middle manager involved in poaching this guy from the competitor want to do it? I have my suspicions.No company or manager I ever worked at, at both good and bad companies, would even think you&#x27;d be bringing stolen proprietary IP from your old job let alone allow something like this to happen under their nose with their knowledge.They&#x27;re far too afraid of IP lawsuits, as knowledge of the use of stolen IP can easily leak, and you then rating out that manager making them an accomplice, for anyone to allow for something like this to happen with their blessing. And plus, you never want to hire IP thieves, if they stole source code from their old job they&#x27;ll steal from you as well.>How was he hired? Most likely Nvidia poached the guy on the premise he&#x27;s gonna build form them something very similar to what he was working on at Valeo. The guy probably sold himself well to get the senior job at Nvidia but most likely knew he overpromised and would underdeliver, so to make his life easy at his new job, he took all the sourcecode and documents from his old job to use at is next job.>How did he introduce the code into Nvidia&#x27;s version control?Well it&#x27;s not like he was dumb enough to just dump in git all the stolen source code from Valeo with all the headers, variable names and copyright notices and nobody would notice. Most likely he kept the code on the laptop as an offline copy and only used it as inspiration for the code he wrote for Nvidia or maybe he even bluntly took Valeo&#x27;s source code then pruned, redacted or renamed any and all references to Valeo and checked it in as Nvidia&#x27;s project so nobody was the wiser that the code was not originally written by him. reply aaomidi 13 hours agorootparentprev> Why commit a crime when you can simply throw money (of which you have a lot) at the problem?Indeed. But the stupidity of committing a crime does not actually stop companies from doing so. Mainly because the penalties for it are never harsh enough. reply szundi 13 hours agorootparentStupidity in this case probably means low level employees cheating for benefits and careerNvidia can get out of this fairly low cost thenWho knows reply FirmwareBurner 12 hours agorootparent>Stupidity in this case probably means low level employees cheating for benefits and careerThat&#x27;s most likely the case here. FFS, the guy stole 6GB of proprietary data and police found the stolen design files pinned on his wall at home, so the guy was fully committed to his scammer role, and not just an accidental \"oopsie I walked out with some proprietary IP by mistake, better discard it and keep this low key so nobody finds out\".By the looks of it, this guy, most likely a Bluecard(German equivalent of H1B) was just cheating and stealing his way up the career chain through the revolving door of the blue-chip automotive sector, until he got caught.Companies both big and small, never ever encourage you to bring to work proprietary files and data from your previous workplaces, since that&#x27;s a guaranteed lawsuit as these things always get out eventually. reply jampekka 9 hours agorootparentWhy do you deem the guy is most likely a Bluecard and how is it relevant? replyAshamedCaptain 12 hours agoparentprevA problem here is that while the companies will generally make very clear that they don&#x27;t want you to have any single line of code, any schematic, any drawing, anything at all from your previous company; they may also expect you to bring \"experience\" from the previous company, thereby pressuring more junior employees into doing exactly that - bringing some docs from the previous company - but not telling about it. Through my career I have been to several meetings where everyone was, notebook in hand, expecting to hear the \"experience\" from the new guy. That is obviously as legal as it gets, but the pressure for the junior employee to have kept a couple of notes is there, and you&#x27;d never know.i.e. I suppose no one was aware that he had this code, and it&#x27;s unlikely it went into nvidia&#x27;s codebases or that nvidia wanted it; but it also doesn&#x27;t mean nvidia did not pressure the guy into doing that. reply FirmwareBurner 10 hours agorootparentThat sounds wrong in so many ways. Do you live in China or something where this is expected?I&#x27;ve worked at about 10 or so companies in 3 countries and it was never expected for the juniors to ever \"bring documents from previous workplaces as knowledge and not tell about it\".Bringing your \"experience\" means only the experience and problem solving skills that are in your head as we&#x27;re in the knowledge work business. Bringing documents to regurgitate just means IP theft, not knowledge work and is no guarantee to make you a productive employee, and no company would ever touch you for ever doing that. reply chii 8 hours agorootparent> Bringing your \"experience\" means only the experience and problem solving skills that are in your headit&#x27;s hard to differentiate experience with IP. For example, there might be a tricky problem (say, in manufacturing), and the solution is a trade secret. The \"experience\" from said employee is really just relaying that trade secret. reply userbinator 6 hours agorootparentprevSome people have very, very good memories and can bring all of that in their brain. Does that make it legal or not, just because it&#x27;s in someone&#x27;s brain and can be easily regurgigated at will? It&#x27;s a subtle question. reply tedunangst 9 hours agoparentprevSo... noncompetes, except for the hiring company? reply clnq 13 hours agoprevThis often happens ingenuously, not out of calculated ill intent. Coders will keep code snippets and thoughts in personal knowledge tools like Notion, and then reuse them in different companies. Or contractors will straight up copy and paste code from source files of projects they worked on for different companies, thinking \"I wrote it, so I could write it again, but why bother?\", or something along those lines. People don&#x27;t usually brag about these things, but they do come to light in random conversations.This is exactly what happened here:> According to Valeo&#x27;s complaint, Mohammad Moniruzzaman, an engineer for NVIDIA who used to work for its company, had mistakenly showed its source code files on his computer as he was sharing his screen during a meeting with both firms in 2022In most cases, these people are asked to remove all such code from the codebase and never do it again, but news about this rarely reaches the executive level. Usually, there aren&#x27;t clear rules that it&#x27;s supposed to be reported, so low level managers handle it the best they can. Of course, this guy got caught in very unusual circumstances.It is also very unfortunate to be the software engineer who notices others doing this, because it puts you in a whistleblower&#x27;s dilemma. The upper management does not want to be implicated in this and they do not want to know. Besides, informing them would definitely lead to the coder&#x27;s firing. What is worse, many programmers see liberal use of IP as \"not a big deal\". So you would be perceived as causing problems for upper management, and getting people fired for \"petty\" reasons. It can sink your career in most companies if you witness this and it gets out. There are laws that protect whistleblowers from being let go sometimes, but it&#x27;s not conducive to anyone&#x27;s career growth to remain in the company because they cannot be fired. reply mkl 10 hours agoparentThe screen sharing incident is not the important thing that happened here. From the article:> Moniruzzaman allegedly gave his personal email unauthorized access to Valeo&#x27;s systems to steal \"tens of thousands of files\" and 6GB of source code shortly after that development. He then left Valeo a few months later and took the stolen information with him when he was given a senior position at NVIDIA, the complaint reads. reply asddubs 13 hours agoparentprevwhat about this:>Valeo said its former employee admitted to stealing its software and that German police found its documentation and hardware pinned on Moniruzzaman&#x27;s walls when his home was raided. According to Bloomberg, he was already convicted of infringement of business secrets in a German court and was ordered to pay €14,400 ($15,750) in September. reply kuroguro 13 hours agoparentprev> paste code from source files of projects they worked on for different companiesI wonder what would happen if legal action started between two companies and it turned out a coder pasted code from personal projects that predates both. reply FirmwareBurner 12 hours agorootparent>I wonder what would happen if legal action started between two companies and it turned out a coder pasted code from personal projects that predates both.Highly unlikely. This was no FOSS web library he was working on, but some relatively cutting edge embedded automotive stuff, which few people do in their free time as a side project to put on github.And anyway, according to most industry contracts and work laws, whatever code you check in your employer&#x27;s systems during work hours and using work equipment, automatically now becomes your employer&#x27;s code which you now can&#x27;t share anymore. reply Crosseye_Jack 12 hours agorootparentprevINAL (so take this with the pinch of salt that it comes from someone just thinking out loud), but I think it would depend on a number of factors such as how novel the code was, and how integral the code is (just to name two factors)If the code was something as simple as let’s say leftpad for a simple example, it could be argued that it’s not the “meat” of the application so those few lines can not by themselves be copyrighted but the whole work (or even larger portions of it) can be.If it was some special sauce algorithm, it could be argued under their work contract that the employee assigned copyright of the code of the personal project to the first employer they did the work for.It also depends on the status of the employee, the contract of the employee, and the jurisdiction of both employee&#x2F;employee.A “full fledged” employee work is often deemed as the companies property if done under the course of their employment. A contractor in the US is about the same, however in the UK a contractor by default can retain the copyright of the “work product” unless stipulated otherwise in the work contract (so most contracts will state that you as a contractor are assigning copyright for the work you do to the company).So in that last case it could be argued that the coder still owns the copyright but licenses the use to both parties. It would then be a case of the two companies maybe suing the coder for selling code they may have represented as given them an exclusive license to it, but obv didn’t because it was licensed to multiple companies. reply jandrewrogers 8 hours agorootparentprevYou can easily add terms to employment contracts that grant non-exclusive license to code you own that you use in the course of employment. I&#x27;ve done it many times. The only issue that has come up out of this is when someone wants a warrant of exclusivity downstream of that, but that has never been a showstopper. reply __turbobrew__ 12 hours agorootparentprevFor most contractual agreements you assign copy-write of your work to your employer. So if you used your personal project in work for your employer the copy-write becomes theirs.My guess is that in your hypothetical scenario the first company would own the IP and could sue the worker or other company for infringement. reply edgyquant 9 hours agoparentprev>Coders will keep code snippets and thoughts in personal knowledge tools like Notion,Do they? I’ve never personally done such a thing, though I may keep some code in public GitHub repos. I’ve rewritten quite a bit of the same logic at most places I’ve worked over the years. reply ohyes 12 hours agoparentprevOr worse, they may just remember how they coded it last time and code it the same, or only remember subconsciously and code it the same without knowing.Someday there will be technology to erase all memory of work you did in service of your corporate overlords and you’ll be able to start with a true clean slate at every job. reply kennethrc 12 hours agorootparent> Someday there will be technology to erase all memory of work you did in service of your corporate overlords and you’ll be able to start with a true clean slate at every job.https:&#x2F;&#x2F;www.imdb.com&#x2F;title&#x2F;tt11280740 reply muhehe 12 hours agorootparentOrhttps:&#x2F;&#x2F;m.imdb.com&#x2F;title&#x2F;tt0338337&#x2F; reply teddyh 6 hours agorootparentBased on a Philip K. Dick short story, available here:. replyUberFly 14 hours agoprevOne of the employees had his previous employers&#x27; code. There will have to be proof that Nvidia even knew about it for this to go anywhere beyond the employee. reply juunpp 13 hours agoparentI am positive Nvidia has no part in this. A corporation of this size is severely allergic to foreign proprietary code and would not risk the lawsuit. Sounds more like this individual forgot to read the memo on IP, and the article says he was already sued by the German courts for prior misconduct anyway.Kind of reminds me of the guys who do personal stuff on their work laptop and then get the entire company pwned. Do people not read the fucking manual anymore? reply alistairSH 13 hours agorootparentAnymore? People have never RTFM. reply mcpackieh 13 hours agoparentprevWhat does it mean for \"Nvidia to know\" something? Does it never count as a \"corporation knowing\" unless the executives are aware of it? Obviously this cannot be the standard by which companies are held accountable. reply IshKebab 13 hours agorootparentIt&#x27;s pretty clear what he means. He means that the managers and executives are ok with stealing code. reply mcpackieh 12 hours agorootparentHow many managers need to be in-the-loop with the theft before we can fairly say \"the corporation knows\"? As far as I&#x27;m concerned, even if no managers are aware of it, they should have been aware of it (it&#x27;s their job to know what their reports are doing) so the corporation should be liable for the theft. Otherwise it&#x27;s trivial for everybody to play dumb and turn a blind eye to what&#x27;s going on. reply Xelynega 12 hours agorootparentYea \"they were too big to know something illegal was going on\" is a narrative that benefits these large companies while ensuring any individuals or small companies that do the same see the full force of the law. reply ohyes 12 hours agorootparentprevThe magic number is 1 manager. reply IshKebab 11 hours agorootparentprev2 replyalmost_usual 13 hours agoprev> Moniruzzaman allegedly gave his personal email unauthorized access to Valeo&#x27;s systems to steal \"tens of thousands of files\" and 6GB of source code shortly after that development. reply lsllc 7 hours agoprevCompare Moniruzzaman with Sergey Aleynikov [0] who allegedly \"stole\" [open source] Erlang code from Goldman Sachs, was arrested by the FBI at Newark airport, found guilty in Federal court and was initially sentenced to 8 years in jail although it was overturned on appeal. He was then tried again on the same charges this time in NY state courts and again the conviction was overturned but then reinstated on appeal by the NYC DA, he was ultimately sentenced to time served (1 year) while waiting for the federal trial as he was deemed a \"flight risk\".So I think with \"only\" a €14,400 fine, Moniruzzaman did better than Sergey despite apparently committing an actual crime.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sergey_Aleynikov reply slow_numbnut 41 minutes agoparentJudges and prosecutors should lose their immunity and be held accountable for their negligent and reckless actions. Legal systems can never be just until bad actors face repercussions, irrespective of what role they serve. reply happytiger 8 hours agoprevTertiary point but one that been on my mind…What’s the difference between one person doing it individually and a company doing it to train an AI? I mean isn’t this exactly what a lot of LLm training data is built on as well?I get there might be different legalities, but morally isn’t it all basically merely degrees of theft? Like this is trade secret theft, but training an AI on the code isn’t?You can download a car… reply mattnewton 8 hours agoparentHow is this morally theft anymore than hiring someone who has experience solving the problem to solve the problem again? As long as they aren’t reproducing the solution verbatim it’s already morally acceptable to hire experts for their experience. Why would “hiring” software for its “experience” be different? reply happytiger 7 hours agorootparentBecause he copied the source code or at least had it for reference all over the room he was coding in? reply mattnewton 4 hours agorootparentRight, and if you do that, it’s theft under the law because we want to reward people for doing novel work. In the same way if I could get GitHub to send me a copy of your private proprietary repository without your consent, that would be called theft by the law, because it removes the incentive to invest in proprietary software. If I hire one of the engineers who worked in that repository and spent years on the problem so she could code solutions to it in their sleep that’s not theft in the law, because we value labor rights, the free exchange of ideas, and want to incentivize building up human capital. Even as it creates a new risk people investing in proprietary software have to deal with now.If I hire “software” that learned from a copy written source, but doesn’t reproduce the copywritten code directly, why is that different from hiring someone who worked on that project before?I think morality isn’t a useful compass here, it isn’t a moral problem. It’s a problem of what kinds of rules you want for society to increase utility for everyone around these tools. If you restrict learning from each other too much you stifle progress. If you make it too easy to copy the leader in a field you disincentivize anyone doing novel research first. reply smallstepforman 4 hours agoprevMost professional software engineers develop a coding style, with consistant function headers, names, case, variable naming etc. Rewriting a feature from scratch may look very similar to your previous work in a screenshot.Not only do engineers face this problem, but so do hair dressers, architects, pizza masters, soccer players, etc when they switch employers. reply dontupvoteme 13 hours agoprev_always_ refactor, rename and relib when you reuse ! reply gumballindie 9 hours agoparentOr simply use procedural generators, such as llms. Those will rinse ip like it’s nothing. reply siva7 12 hours agoprevNever share your screen, only a window! reply RadixDLT 10 hours agoparentnever steal reply jonplackett 12 hours agoprevWas hoping they stolen CUDA from AMD reply mnd999 13 hours agoprevNvidia definitely don’t have form for stealing trade secrets. They never stole anything from SGI. reply mcpackieh 13 hours agoparentHow can you know that? reply aunty_helen 12 hours agoprev> Too many requests -- error 999.Did engadget get the hug of death??? reply latchkey 13 hours agoprevdupe: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38363361 reply gremlinsinc 13 hours agoparentit is... but there&#x27;s apparently a lot more discussion on this thread so maybe that one should be rolled into this one... reply gremlinsinc 13 hours agoprev> German police found its documentation and hardware pinned on Moniruzzaman&#x27;s walls when his home was raidedWTF actually prints out documentation? Let alone pins it to their walls? I mean....seriously does anybody actually do this? reply rightbyte 13 hours agoparentI do. I can&#x27;t read any length of text on a screen. I pin stuff to the wall too, of quick sheet character. Like Emacs hotkeys, C operator precedence, pinouts, general specs etc. reply filchermcurr 13 hours agorootparentSame. I recently bought a Kindle Scribe (would have preferred Kobo, but enh) to see if that would make it easier than constant printing and shuffling papers. It&#x27;s alright. Better than a monitor, slightly less good than paper. reply FirmwareBurner 13 hours agoparentprev>WTF actually prints out documentation?Probably easier to exfiltrate confidential documents when printed, rather than digitally through the company internet which is logged and points straight to you.If I print something confidential and take it home there&#x27;s only the printer logs as proof that I printed it, but no proof that I also took it home (unless there&#x27;s surveillance footage).>Let alone pins it to their walls?The man is proud of his work, wants to see it daily for motivation. reply jbverschoor 13 hours agoparentprevI&#x27;d love to have a 100 inch hidpi eink wall. reply WalterBright 12 hours agorootparentSo would I. But I settle for an extra monitor in portrait mode, and will stick the pdf documentation in it when I&#x27;m following a spec. reply AndroTux 12 hours agoparentprevGermans. We love printing, faxing and scanning stuff. reply FirmwareBurner 12 hours agorootparent>Germans. >Mohammad MoniruzzamanVery German. reply krmboya 10 hours agorootparentI hope you don&#x27;t write software that flags people by their name reply FirmwareBurner 10 hours agorootparentFYI, a lot of other nationalities besides German work in Germany. It&#x27;s natural to find immigrants in almost every company, especially the big international ones. reply jampekka 9 hours agorootparentprevAdolf would be more German indeed. reply anthk 10 hours agorootparentprevBy that name it could be a modern Spaniard, French or German fully assimilated to the culture. Specially if he was born there. reply FirmwareBurner 10 hours agorootparent>fully assimilated to the cultureThe culture of stealing IP from your employer? reply jampekka 9 hours agorootparentWould be relatively tame compared to what kinds of actions the German culture is quite easy to associate with. reply anthk 14 minutes agorootparentWolkswagen&#x27;s data mangling? reply FirmwareBurner 9 hours agorootparentprevLike? Socks in sandals? reply jampekka 9 hours agorootparentSauerkraut and such of course. replysteponlego 13 hours agoprevI just avoid NVIDIA because they hate Free Software, not for any other reason. reply kkielhofner 11 hours agoparent400 GH repos:https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;NVIDIA&#x2F;repositoriesNot to mention tons of other projects that live outside of the main org, contributions to other projects all over the place, etc.So much hate for free software.The “I hate Nvidia because all I know is their driver is proprietary” schtick is old.They crossed the $1T mark in value solely because of the almost completely open source ecosystem (a large portion of which they directly develop and contribute to) that runs on top of their hardware and (yes, proprietary) driver.They’re not angels but this position is something out of Slashdot circa 2005. reply anthk 10 hours agorootparentCUDA. Non-free firmware. Still bad. And Radeon the same, propietary firmware run by the kernel is needed sometimes to even boot the GPU, it just happens Linux-Libre patches it and the Modesetting driver will work fine until you call 3D accelerated calls, when that happens the system may either panic or crash X entirely. That can be fixed by setting RenderAccel to none in the X.org config file. reply kkielhofner 10 hours agorootparentThese plus Raspberry Pi firmware, Intel ME, the list goes on and on.RISC-V is so exciting and interesting because it’s practically the first time in modern history 100% open source meets reality.People that act as though Nvidia is the exception here should just say they hate Nvidia for personal reasons - and that’s fair and fine.Again, they’re not angels and they do all kinds of shady things but to single them out vs practically every company in history is just bizarre. reply redder23 13 hours agoprev [–] And they will get a slap on the wrist at best. They made billions this is just peanuts. reply tmtvl 11 hours agoparentShould just give the board of directors 100 lashes. Except of course if it turns out they didn&#x27;t know about it, then they should get 200 lashes, have their genitals chopped off, and their immediate family be given 10 lashes. Just like the good old days. reply redder23 11 hours agoparentprev [2 more] [flagged] trealira 7 hours agorootparent [–] It&#x27;s not a fact, it&#x27;s your prediction of the future. I didn&#x27;t downvote you, but don&#x27;t act like what you personally think is a fact. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NVIDIA is being sued by French automotive company Valeo over allegations of trade secret theft.",
      "The lawsuit was filed after an NVIDIA employee accidentally displayed Valeo's source code files in a screensharing meeting.",
      "NVIDIA denies the allegations and asserts that they have implemented measures to safeguard their clients' rights."
    ],
    "commentSummary": [
      "Nvidia is facing a lawsuit for allegedly stealing trade secrets from a competitor, sparking debates on the ethics of AI-based code generation tools and the patentability of software and mathematical ideas.",
      "Discussions also cover topics such as Nvidia's drivers and proprietary code, the impact on the company's success, and the importance of intellectual property protection.",
      "Other areas of discussion include hiring employees from rival companies, distinguishing between IP infringement and not, ownership of code in employment contracts, consequences of code theft, legal implications of using personal code, and debate on learning from others' projects without reproducing copyrighted code."
    ],
    "points": 313,
    "commentCount": 188,
    "retryCount": 0,
    "time": 1700940356
  },
  {
    "id": 38418262,
    "title": "Naev: Embark on an Open-Source Space Adventure of Trade and Combat",
    "originLink": "https://naev.org/",
    "originBody": "Naev 0.10.0 Trailer Naev is a game about space exploration, trade and combat. Players travel the galaxy and earn money by trading, fighting and performing missions. Want to keep up with development? Check out the Blarg to see new posts about the development of Naev! You can also add this page to your favourite RSS reader to get notifications every time we post something on the Blarg. Recent posts are shown below: Sporadic Naev Newsletter Vol. 3 on September 5th, 2023 New Autonav on July 30th, 2023 Autonav now works on patrol lanes and is also customizable. Ships Go Boom on July 18th, 2023 Ships now have different types of explosions that can vary drastically. Main features: A large galaxy with hundreds of planets and moons to explore. Real-time, semi-Newtonian gameplay reminiscent of the Escape Velocity games. Many different ships to buy and pilot, and many ways to customize each of them. Multiple factions, each with unique ship designs and personalities. Open-source, meaning ultimate freedom of modding, although plugins are also supported. Currently Naev is a playable game, but it is far from being a complete one. To further mature the game, we, the developers, need you. Yes, you. All it takes is the motivation to be creative in some way! Probably the easiest way to help, as well as one of the most valuable ones, is by coming up with new stories for the game, and by shaping those stories into playable missions. Other ways to help include making art or sounds, improving gameplay via balancing work, or (if you have the ability) even a bit of coding on the main program. A few things to keep in mind: This game is a hobby project, not an indie development project. There is no final plan, no time frame for the “finished game”. The dev team does not accept monetary donations, only direct contributions to the game itself. The game engine is written in C. Missions are written in Lua. There is an experimental multiplayer plugin, although it is quite limited. Where can I get Naev?: You can find downloads at the downloads page and also on popular services such as steam or itch.io. If you’re a Linux user, you can also get Naev from a few other sources, see the downloads page for more information. How can I get in touch?: You can join the discussion via the links on the contact page, or from the website header bar above.",
    "commentLink": "https://news.ycombinator.com/item?id=38418262",
    "commentBody": "Naev – open-source game about space exploration, trade and combatHacker NewspastloginNaev – open-source game about space exploration, trade and combat (naev.org) 299 points by pabs3 9 hours ago| hidepastfavorite37 comments maxbond 7 hours agoA few years ago I played Naev for a few days. It was a lovely nostalgia trip, and I recommended it to friends who had also played Escape Velocity, but there wasn&#x27;t much game there and it was a bit clunky.A few weeks ago I got the itch again, so I downloaded Naev. To my pleasant surprise, development had not only continued but it has - dare I say it - surpassed Escape Velocity. There&#x27;s a very cool stealth mechanic, the campaigns have been fleshed out, the AIs are more sophisticated than I remember in EV; it stands on it&#x27;s own as a fantastic entry in the \"Elite like\" genre.ETA: Some advice for new players, do the Empire Shipping missions (by talking to an Empire official in the spaceport bar in Empire worlds). It&#x27;s sort of the second tutorial, they&#x27;ll have you go to meet all the different factions, and you&#x27;ll get your Heavy Weapons and Heavy Combat Ship licenses. reply calf 5 hours agoparentI absolutely LOVED playing Escape Velocity in high school! AmbrosiaSW made such great shareware games, with appealing graphic design and replayability--the modular system. I think all 3 EV games are good examples of the concept of \"aesthetic unity\" in game design. reply dguest 1 hour agorootparentSpeaking of shareware: was I the only person who modded Captain Hector to take away his more powerful weapons?I feel bad now for not scraping together the 10 bucks or whatever they were asking for, but mom would have just told me to go play outside if I&#x27;d asked. reply jwithington 1 hour agoparentprevBetter than Escape Velocity?? Gotta try it then. reply EdwardDiego 7 hours agoparentprevHow does the stealth mechanic work? reply maxbond 7 hours agorootparentIt&#x27;s not dissimilar from mechanics in other games, implicit in my comment is that none of the 3 EV games had any sort of stealth mechanic. (ETA: Actually, they had cloaking devices, but they were special unique items that you couldn&#x27;t transfer if you upgraded your ship. Not sure if that exists in Naev, I haven&#x27;t gotten very far into the main story.)You can go into stealth, and other ships have a bubble around you can see on your radar (the radar is also much better than in EV). If you get inside that bubble you&#x27;ll fall out of stealth. The size of the bubbles scales with the mass of your ship, so a shuttle or an interceptor can hide very effectively, but massive cargo ships simply cannot.This makes being a criminal or smuggler much more interesting. In EV there was nothing more to it than being super fast and staying ahead of the patrols trying to scan you. In Naev it&#x27;s the similar, but stealth gives you more options. It feels very cool to be drifting silently in space, waiting for patrols to move away so you can reach the jump point undetected. Feels more deliberate and clever.Another cool thing is that the AIs use it too. So it feels much more like you&#x27;ve been accosted by a highwayman when you&#x27;re a little Terrapin trying to finish a mission, and suddenly pirates appear out of the blue (black?). reply EdwardDiego 6 hours agorootparentSo does stealth involve shutting down systems &#x2F; power output etc.?I always like a stealth system in space that involved not radiating, as well as avoiding active radar, too many sub sims in my youth :DMy favourite thing about The Expanse was when incoming radar stealth ships were finally identified by their bright drive plumes as they slowed for an attack. reply maxbond 6 hours agorootparentNo unfortunately (or not that I&#x27;ve noticed), though you drop out of stealth if you open fire or use your afterburners. It would be cool if you came out of stealth at half power; that would make it pretty costly, since power and power regen are the stats that dictate how long you can sustain your weapons in a firefight (missiles not withstanding).That&#x27;s very cool re: The Expanse. Alastair Reynolds, an astronomer turned sci-fi author, explores \"weaponized astronomy\" a lot in his books, especially the opening chapters of \"Inhibitor Phase\". Without too many spoilers, there&#x27;s a ship that cools it&#x27;s hull close to absolute zero and uses cryogenic gasses for thrust in order to maneuver, so that it isn&#x27;t radiating IR. And they have to time their operations so that they aren&#x27;t seen occluding the sun. reply EdwardDiego 4 hours agorootparentOh, I definitely need to read those books, cheers! reply maxbond 3 hours agorootparentThe Revelation Space series does start slow, he was a junior writer for the first part and he&#x27;s, but I enjoy it a lot. replyrauljara 6 hours agoprevSeems like HN broke the trailer on their site. Fortunately, it also lives on YouTube: https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=CNBk1DK046k&pp=ygUMTmFldiB0cmF...Glad they aren’t shy about their escape velocity roots. I swear, the ship in their logo looks just like a kestrel. reply maxbond 6 hours agoparentIt&#x27;s the Kestrel from EV: Nova (which as we all know is the ship that singlehandedly put Krain Industries on the map), with a little bit of red added.They&#x27;re pretty shameless! Lots of the ships are pulled from EV: Nova with cosmetic changes (no idea about the legality, I suspect they got permission but I haven&#x27;t looked into it), and there&#x27;s a setting that skins the UI to look like EV classic. reply exo-pla-net 6 hours agoprevFor anyone into this sort of game, I always recommend Starsector [1]. It&#x27;s a beautifully made game that is in active development and already very fun to play.[1] https:&#x2F;&#x2F;fractalsoftworks.com&#x2F; reply hschne 35 minutes agoparentI don&#x27;t play much anymore, but there&#x27;s basically two games I keep coming back to. Starsector and Stellaris.Both Games get updates every couple of months. Which is perfect, because you get something new every time you pick one of them up again. reply ajvs 53 minutes agoparentprevYeah I have to say Starsector way of less arcadey combat works really really good, wish more of these space sims would copy it. reply Aeolun 2 hours agoparentprevI’m not sure if I’d call one update every half year&#x2F;year ‘active’ development, but it has certainly been consistently developed for 10 years or more at this point. reply Der_Einzige 1 hour agorootparentMods fix that issue, and the modding scene is active. reply Loughla 5 hours agoparentprevWow. That hits on exactly what I&#x27;m looking for. reply 83457 5 hours agoparentprevReminds me of Armada reply jauntywundrkind 4 hours agorootparentI really really miss having games like Armada to couch-play. reply dbingham 8 hours agoprevSounds very similar to Endless Sky[1] - another open source game drawing on the old Escape Velocity games. I really enjoyed both Endless Sky and the Escape Velocity games, so I&#x27;ll have to check out Naev at some point![1] https:&#x2F;&#x2F;endless-sky.github.io&#x2F; reply whartung 6 hours agoparentI found Endless Sky to be ok, but it didn’t hold my interest in the end. The early part of exploring a new game was fun, just going through the mechanics of trade and navigation and such.My one combat was decidedly one sided and a disaster. I honestly don’t remember the game play of it much. Most of the navigation and movement is automated.By the time I got enough money for another ship, I had become disenchanted with the mechanics. Trying to min&#x2F;max sales and trade and such. And was never strong enough to play Galactic Marshal or anything like that.That’s when I put it down. Just may be the game doesn’t suit me. Or I never got into some sweet spot of challenge to engage me. And since the galaxy is fixed (at least I think it’s fixed), I didn’t see much opportunity in replay of it. reply cwillu 5 hours agorootparentThe actual storyline develops after a significant amount of in-game time, and is actually kinda easy to miss the initial two hooks if you click through things too quick. Basically, a planet gets nuked, and a while after _that_, somebody approaches you once you end up in a certain area of the map for a little while.(And that&#x27;s before you discover [censored] and [censored] and the whole [censored] storyline that happens after the “main”&#x2F;initial plot ends) reply Widdershin 4 hours agorootparentProps to the writing in Endless Sky, probably has the most nuanced and interesting plot of any open source community built game I’ve played. reply cwillu 3 hours agorootparent‹tries that karate move now, just for kicks› reply o11c 4 hours agorootparentprevNote that trade missions generally give about 10x value compared to trading yourself. Going into dead-end system chains can help stack multiple missions with the same destination.Some brief notes about combat in Endless Sky (with some differences from EV:N):* The population of ships is much higher in ES; it&#x27;s much easier to get overwhelmed. Note that when you initially enter a system it starts with 5 seconds (I think) worth of ships, and they keep randomly spawning at a continuous rate (in the game files, the numbers mean \"expected delay between spawns in milliseconds\" I think). If there is a hostile enemy ship, chances are its random delay was pretty short, so more will come. Instead you should try to arrange your fights to be in a system with low traffic.* The easiest way to get started with combat is to take escort missions, which spawn fixed enemies, and have allies that you can use as shields. They also persist if you disable them and land, so you can capture them easily (sell your fighter and buy a shuttle which can hire more crew). This is by far the fastest way to make money (as in, pay off your debt on day 3).* Ships do persist between systems, so if they jump out you can follow them* You absolutely can play the game without ever buying anything or doing missions, slowing capturing more powerful ships until you get a Bastion. This isn&#x27;t a reasonable thing to do, but I did it anyway just to prove it can be done.* The Monty Python maneuver has been heavily nerfed, but still has an important place.* There are 4 different approaches for weapons on your ship&#x2F;fleet; each ship should largely pick one and stick to it (and you should ground ships with the wrong loadout for your current task): 1. do as much damage as possible, 2. reliably stop shooting when target is disabled (that is, beam or weapons or at least high-velocity ones (though this is less important against slow and tanky enemies); it won&#x27;t help if your allies see another target behind it), 3. overload enemy heatsinks (very useful against certain ships but useless against others), and 4. drain enemy energy (I&#x27;ve never found this useful, but have been affected by enemies using it on me). Also a couple special cases: 5. the player ship should be somewhat optimized for capturing, which may be at the cost of weapons, 6. your main ship might also want extra fuel, for exploring or just avoiding deadlines (escorts can land&#x2F;takeoff to refuel without taking an extra day ... they can share with you, but they are dumb if multiple uninhabited systems exist on your path), 7. anti-missile also takes away from direct damage but is usually worth it (and without the tough energy&#x2F;heat costs), 8. it does matter which gun slot you put your weapons in; use the I(nfo) window, 9. weapons that use ammo get expensive for the player&#x27;s fleet, unlike enemy ships (and I guess mission-based escort ships, but those are rarely really combat-capable) which get everything for free; unlike EV:N your permanent escorts don&#x27;t refill for free (but you do get to choose their loadout, so it&#x27;s a net win I guess).* Regarding engines, steering is generally more important than thrust for both combat and escaping to another system, but thrust matters for escaping&#x2F;dodging within a system. A full guide to optimizing your ship would not fit in this margin, but it is certainly possible to beat the default ships, especially by mixing outfits between factions.* Many factions will forgive you easily if you just attack pirates or something, but some factions will never forgive you. If you accidentally draw aggro merely jumping between systems should suffice.* There are some outfits that you can only get by boarding or capturing a ship, and some that you can only get by capturing the whole ship. This includes outfits never buyable, outfits only be unlocked later in the main plot, and (if you use 0.9.8 which is still widely deployed) outfits that were supposed to be removed for balance reasons. reply tsujamin 8 hours agoparentprev+1 for endless sky, has that “just 30 more minutes” hook reply ktbwrestler 6 hours agoparentprevthis is awesome, do you know if it supports multiplayer? I might help out with Naev since that also apparently has a multiplayer plugin... reply thot_experiment 4 hours agoprevIs the logo the Kestrel? Love it. Played the shit out of EV:N on the computer lab computers. reply walteweiss 2 hours agoprevThere is a Russian game Space Rangers ¹ I played in the early ’00s, and I quite enjoyed it, was one of my favourite game.It’s a 2D game with quests and space exploration, battles and some arcade mode in hyper-space. I remember it very well, as it worked very snappy on my poor computer (even by that time standards).Now as I remembered that game, I wonder if Naev is something similar. I would rather not promote a Russian game these days, and I would rather play open source game. But from user interface perspective Naev looks quite bad when compared to that Rangers game made two decades ago. I judge Naev by its screenshots, so maybe the game looks better.[1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Space_Rangers_(video_game) reply rkagerer 2 hours agoparentDespite its bugs Space Rangers was incredible. The diversity of gameplay was great. Fly around space, fight people, solve puzzles and manage ski resorts. reply lenerdenator 4 hours agoprevNever did play Escape Velocity but I like this one so far. It&#x27;s a charming little game with some decent story development. And it even runs natively on Mac. reply EamonnMR 5 hours agoprevAlways love a good EV clone. I loved this one back when it was much more Nova-like. reply Waterluvian 5 hours agoprevThis game looks pretty exciting to follow. Escape Velocity was a huge part of my childhood. My brother and I completely scraped that game clean of secrets. I learned how to use ResEdit because of it. We would daisy chain ADB keyboards and he’d fly while I controlled weapon selection and escorts. reply prawn 5 hours agoprevReminds me a bit of Star Control II and its Melee combat mode. reply stevefan1999 4 hours agoprevThis reminds me of Spore a lot reply aunglin 1 hour agoprev [–] pubg replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Naev is an open-source space exploration, trade, and combat game with real-time gameplay and a large galaxy to explore.",
      "Players can earn money by completing missions and can customize their ships.",
      "Developers are actively seeking contributions in areas like story creation, art, sound, gameplay balancing, and coding, offering an opportunity for players to get involved and make an impact on the game's development."
    ],
    "commentSummary": [
      "Naev is a space exploration and combat game that offers a stealth mechanic and improved AI, surpassing Escape Velocity in terms of gameplay and mechanics.",
      "Starsector is recommended for its enjoyable gameplay and beautiful design.",
      "Endless Sky receives praise for its nuanced storyline, and helpful combat tips are provided. Space Rangers is highly regarded for its diverse gameplay and story development."
    ],
    "points": 301,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1700960031
  },
  {
    "id": 38413727,
    "title": "Why 56k is the Fastest Dialup Modem Speed",
    "originLink": "https://www.10stripe.com/articles/why-is-56k-the-fastest-dialup-modem-speed.php",
    "originBody": "Sites 10stripe 10blog Browse By Topic Processors Power Supplies Software Greatest Hits CPU Guide PSU Makers PCI-Express 2.0 Toolbox Printer-Friendly Bookshelf Why is 56k the fastest dialup modem speed? by Alex Freeman If you've ever had dialup internet service, or still do, or just know someone that does, you have probably heard terms like \"56k modem\". \"56k\" has become almost synonymous with dialup Internet access. But it's such an arbitrary number. It's not divisible by ten, it's not a power of two... so why was it chosen as the fastest dialup speed? For the answer, we will have to travel back in time quite a while. Our visitors from Google should be warned that this is not a \"stripped down\" explanation; it is intended for relatively technical readers. But if you really want to know where this magic number comes from, you need to understand some of the technical background. As we shall see, \"56k\" was not just pulled out of a hat. Contents Setting the baseline Long, long ago Digitization We're with the band Petty theft Enter the modem So why can't I actually connect at 56 kbit/s? About the Author Setting the baseline First, let's be specific about what we mean. A \"56k\" (sometimes the 'k' is capitalized and sometimes not; the reasons are not really relevant here) dialup modem can download data at up to 56 kilobits per second. That is a theoretical maximum of the hardware; for various reasons (and yes, we'll get there) this speed can never actually be achieved. Upload speeds are considerably slower. This was all codified in 1998 in the ITU V.90 recommendation, which merged two competing standards (X2 from 3COM and K56flex from Rockwell Semiconductor). V.92 came slightly later, and introduced various tweaks; mostly, it increased the maximum upload speed. Any modern dialup modem will conform to one of these standards. Got all that? Long, long ago Bur story starts much earlier. Long ago, when the phone networks were still completely analog, when the first real phone networks were being built, the engineers that were designing them faced a decision. They needed to determine how much bandwidth their networks had to have in order to ensure adequate voice quality. Too little bandwidth, and users wouldn't be able to recognize the voice of the person on the other end. But the more bandwidth the system was specified for, the more difficult (and expensive) the network was to build. Ultimately, they settled on approximately 3.2 kHz. The implementation for this is pretty simple: the phone lines in individual houses run to boxes owned by the phone company, which pass all voice traffic through a lowpass filter. This filter has a passband of 3.2 kHz, and a stopband starting at 4 kHz. The entire signal at 3.2 kHz and below is preserved, everything above 4 kHz is attenuated heavily, and as for the frequencies in between, well, we just don't worry about those. Problem solved. Digitization Fast-forward to 1962. Ma Bell begins using T-1s to connect its switching centers. T-1s are a digital communications link, not analog. The entire phone network goes digital. In a modern phone network, the only analog communication is between an individual house's phone line and the phone company's box. T-1s are a particular form of DS-1 (Digital Signal 1). They implement the DS-1 protocol, with an AMI line code (alternate mark inversion; a \"0\" is 0 volts, \"1\"s alternate between logic high and logic low), over twisted pair cable, for a distance of up to 6000 feet (very roughly 1 mile). Now that we've gotten all precise, I'm going to play a bit loose with terminology and call this \"a DS-1\" because it makes what follows easier to understand. And it's very nearly correct. A DS-1 is made by multiplexing 24 DS-0 streams. Each DS-0 corresponds to a single phone line; that is, a typical house gets one DS-0. Why 24 of them in each DS-1? No idea. Most likely that was that was the maximum possible without overstressing the equipment that was available at the time. The most comparable European standard, E-1, multiplexes 32 channels together (1 is used for internal signaling purposes). In order to get your voice data onto this digital network, it must first be digitized. So now instead of just going through a lowpass filter, it goes through a lowpass filter, then is sampled, and then is run into an analog-to-digital converter that assigns a numerical value to the amplitude of the signal at the instant it is sampled. Simple, right? As we've said, the lowpass filter cuts off all frequencies above 4 kHz. This means that the filtered signal can be sampled at 8 kHz (which is to say, it is sampled 8000 times per second) with no fidelity loss. This is the Nyquist Rate; any signal sampled at a rate equal to twice its highest frequency component can be completely reconstructed. So now we have turned our analog signal into a discrete-time signal running along at 8000 samples/second. Next we want to quantize our signal so that we have a purely digital signal. For this, an 8-bit analog-to-digital converter (ADC) is used. Here there is some fidelity loss, but not enough to be detectable to human ears. Fidelity could be improved by using, say, a 16-bit ADC. But that would cost more to manufacture (at the time, a lot more) , and would mean that all the other hardware has to be built to transmit twice as much data. So 8-bit it is. For those of you keeping score at home, that means we now have a signal that transmits data 8000 times each second, and transmits 8 bits of data for each of those samples. 8 bits/sample * 8000 samples/second yields 64 kbit/s (64 kilobits per second). So why can't dialup modems go up to 64 kbit/s? The short answer would be \"historical reasons\". We're with the band The more complete answer is \"control data\". Most communications networks need to transmit two kinds of information. The first is data; for phone networks, this is voice data. The second is signaling information, the control data that lets the network communicate status information with itself. There are two ways to send control data: \"in band\" (where it is packed in with the data) and \"out of band\" (where it gets its own separate channel, possibly in an entirely different physical medium). After we've created the 64 kbit/s data stream that fills up our DS-0, it needs to be multiplexed into a DS-1 for longer-distance transmission to the phone company's switching station. And the DS-1 has control data to carry. The Bell engineers decided to send that information in band. But how do we go about packing this data in? One way is to \"steal\" some data bits and replace them with control data. And that is exactly what the Bell engineers did. Petty theft A DS-1 transmits data in frames. Each frame contains one complete sample from each DS-0, plus a framing bit. Since there are 24 DS-0s multiplexed to make each DS-1, and a single sample from a DS-0 is 8 bits of data. 24 * 8 makes 192 bits. Toss in the framing bit and you have 193 bits. So we have our 193-bit frame, and we want to pack some control data into it. How? Every 6 frames, the least significant bit of each voice channel is \"stolen\". The data bit is thrown out, and control data is sent instead. Because it is the least significant bit that is lost, this very slight alteration to the voice signal is not detectable by human ears. Everybody's happy. Enter the modem That system worked fine until people wanted to transmit more than just voice on these networks. If you're sending voice, losing the least significant bit of every sixth byte is no big deal. If you are sending data, it is a very big deal. Neither modem has any way to know which bytes will be affected. Maybe this sample, maybe the next one. This is a problem. The solution is simple enough: assume you are always going to lose that least significant bit. Assume that you can only reliably send or receive 7 bits with each sample. When the receiver gets its data, it simply throws away the least significant bit of every byte. That yields a maximum data rate of 7 bit/sample * 8000 samples/s, or 56 kbit/s. And that is where the term \"56k\" comes from. So why can't I actually connect at 56 kbit/s? Anyone that has ever used a dialup modem knows full well that they don't actually get to connect at that speed, though. And that their connection speed varies each time they dial in. There are two factors at work here. The first is the FCC. If you are in the United States, the FCC places a restriction on the power output of devices connected to the phone network. The result is that you will never be able to connect at a speed faster than 53.3 kbit/s. The second is the overall complexity of the phone network. 56 kbit/s (or 53.3 kbit/s) requires very good operating conditions, as it is really operating beyond the paramaters of what the phone network is required to be capable of. Operating at these speeds requires that there only be one ADC between the user and their ISP (which is not guaranteed to be true, but typically is), and that the copper wiring in the user's \"local loop\" have very good electrical properties. Part of the dialup process that is used to initiate a connection is an evaluation of the overall quality of the connection; if it is determined to be lacking, the modem will automatically drop down to a lower data rate. About the Author Alex Freeman is the creator of 10stripe.com, and has entirely too much free time. Born: August 9, 2007; Cleaned: June 28, 2008 ©2004-2008 10stripe.com Home Legal Contact About",
    "commentLink": "https://news.ycombinator.com/item?id=38413727",
    "commentBody": "Why 56k is the fastest dial up modem speedHacker NewspastloginWhy 56k is the fastest dial up modem speed (10stripe.com) 223 points by marcodiego 19 hours ago| hidepastfavorite114 comments bwann 18 hours agoTo nitpick, the 53.3k limit only affected US Robotics&#x27; pre-V.90 standard called X2, due to how much power they would require, which violated FCC rules. K56Flex (the competing 56k protocol), and later the ratified V.90 standard didn&#x27;t have this problem and you could achieve carrier speeds of 56,000 bps.This distinction was getting lost to time even back then with everyone just assuming it was some arbitrary FCC limit. This post from NANOG in 1998 explains it: https:&#x2F;&#x2F;archive.nanog.org&#x2F;mailinglist&#x2F;mailarchives&#x2F;old_archi... reply qingcharles 17 hours agoparentWhat&#x27;s crazy is how badly I was waiting to upgrade from X2 to K56Flex just to get the 3kbps, which was like a life-altering speed difference back then! reply guidedlight 12 hours agorootparent3.2kbps is the maximum rate of symbols (i.e. 3200 baud). So K56Flex squeezes one extra bit per symbol. reply philistine 13 hours agorootparentprevI’ll go to my grave thinking wow 3.2 kilobytes is a good connection! reply rascul 17 hours agoparentprevI don&#x27;t know how it worked but I never saw better than 53&#x2F;54k with v.90 or v.92. Maybe there was more going on. reply MBCook 14 hours agorootparentOnce, for a short while, I was getting around 110kbps reported from my modem (obviously it was doing compression).It couldn’t do it for binary, but it still was noticeable with HTML and email.It didn’t last long. Whatever accidental phone line + ISP magic enabled it was quickly “fixed”. reply Aloha 16 hours agorootparentprevIt really depended a lot of how clean your pair was, and sometimes even what kind of switch you were homed on.Like channel banks prior to the D4 and a 1AESS would very likely have a marked difference to your speeds. reply hypercube33 12 hours agoparentprevI&#x27;m pretty sure my external modem had a firmware update that took it from 53kbps to 56k flex reply tibbydudeza 14 hours agoparentprevAhh the 56K modem wars - each group swore either by USR or K56Flex and were incompatible with each other until V90 standard came along - some modems could be firmware upgraded (USR) others not that used discrete components. reply chriscjcj 17 hours agoprevI was never able to exceed 24kb&#x2F;s with my \"56k\" modem. I came to find out that the phone company (in Austin TX) was using a single pair to service more than one customer in my apartment complex. Naturally, whatever piece of equipment they used to accomplish this stymied faster modem speeds.I finally installed an ISDN. That was delightful. Not only did it allow me to have two lines - one for voice and one for data - the netgear RT-338 ISDN router automatically dialed my ISP instantaneously. It felt like I was always connected because I didn&#x27;t have to wait for a modem handshake. Speeds were more than twice as fast as what I could get with my analog modem. However, by far the MOST IMPORTANT thing (and mostly the reason I installed the ISDN in the first place) is that my latency took a nosedive allowing me to become an LPB with a 50ms ping time on QuakeWorld. >:-D(Shoutout to Charlie S. from OuterNet, Austin&#x27;s best ISP in the 90&#x27;s) reply bcrl 16 hours agoparentI worked on ISDN drivers and firmware at a job back in the late 1990s. One of the optimizations I implemented eliminated a memory copy by the 68302 which improved ping times from ~30ms to ~16ms. The good old 68000 is not exactly the fastest at memory copies... reply mips_r4300i 15 hours agorootparentSeems like an oversight that the extra stuff the 68302 added wouldn&#x27;t have been a DMA block copy or something. reply Aloha 16 hours agoparentprevYou likely had something called a &#x27;pair gain&#x27; installed - which worked much like DSL, by using a high frequency FM carrier over the audio frequencies to provide a second phone line on the same pair. These things were largely gone by the mid 90&#x27;s - and were not that common in the first place. reply IntelMiner 15 hours agorootparentThey plagued Australian homes up through the late 2000&#x27;s unfortunatelyFucking Telstra reply throwaheyy 12 hours agorootparentYup, we built a new house in Australia in 2001 and thought we were being so forward thinking by getting a 2nd phone line for the modem&#x2F;fax. Of course it ended up being done using a pair gain and we were stuck at 33.6k. reply bigtex 14 hours agoparentprevBack in the late 90s friend had ISDN installed at his apartment and it was glorious. Compared to dialup it was like high speed internet. We used some Mac app that basically gave you access to all the cracked software you could have ever wanted. reply Lammy 10 hours agorootparent> some Mac appHotline? https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hotline_Communications reply queuebert 16 hours agoparentprevWhat is an LPB? reply zain 16 hours agorootparentLow Ping Bastard reply hypercube33 12 hours agorootparentBack in the day LPB was someone with low pings (remember dial up was 150 to 300ms sometimes) and they&#x27;d have an upper hand on games before they were \"unlagged\" with prediction code. reply oh_sigh 13 hours agorootparentprevA common lament among the dead 15 years ago in real time online gaming...low ping bastard&#x2F;bitch, basically just saying the only reason you got the kill is because your ping is 60ms and mine is 250ms. reply tibbydudeza 14 hours agoparentprevYou could also bond the two channels together for a 128K connection - installed Linux to get the bonding to work on my ISDN card as Windows 3.1 For Workgroups knew nothing about it. reply Syzygies 18 hours agoprevI mentioned this article to my retired AT&T engineer wife, and she blurted out \"Robbed-bit signaling!\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Robbed-bit_signaling reply hlandau 17 hours agoparentUnrelated, but I highly recommend anyone visiting Seattle check out the Connections Museum, open on Sundays only. It has several different genuine analogue and early digital telephone exchange frames saved from being scrapped from genuine telephone exchanges. They turn them on and you can actually call from one phone to another inside the museum. The museum itself actually shares a building with a real telephone exchange. Lots of old books, technical journals and schematics too.I just thought to post this since when I visited in September, some actual retired telecoms engineers were part of the tour group. One had been involved in the development of submarine cable technology. reply Aloha 16 hours agorootparentTechnically only the not yet functional DMS-10 is digital, everything else is analog. The 3ESS is a stored program, computer controlled analog switch - it uses the same remreed technology the 1AESS used to set up call links.Out of all of those however, panel is my favorite however, it sounds amazing to listen to, both the equipment and what the line sounds like. A call between a panel and the number 1 xbar sounds amazing, nothing quite sounds like revertive pulse signaling.Interestingly enough, there were three switches made that used RP internally - Panel, 7A Rotary and 1XBAR - and if you go find video of the 7A Rotary, you&#x27;ll see it sounds almost exactly like panel, because the unique sounds of the rotary sequence switches both exchanges use. reply 93po 10 hours agorootparentprevI will second this. It&#x27;s super cool and the volunteers there love showing people stuff and have lots of enthusiasm for it reply jonathankoren 17 hours agorootparentprevI recently discovered their YouTube channel, and it is absolutely fascinating.https:&#x2F;&#x2F;youtube.com&#x2F;@ConnectionsMuseum reply Solvency 15 hours agorootparentprevIs the fact that the museum isn&#x27;t available 85% or the week intentional irony or something? reply 93po 10 hours agorootparentIt&#x27;s all unpaid volunteers, and I think the museum is free if i recall correctly reply marcus0x62 17 hours agoprevThis article glances at, then misses the fundamental technical reason why modems topped out at ~ 56kbps: that’s really all the bandwidth that is available on a POTS line, due to the bandpass filter, the ADC on the other end, companding, and engineering practices that led the Bells to maintain their lines to the standards of an analog voice conversation.The robbed-bit signaling thing is really a red herring: it was a problem, but not THE problem. At the time analog 56kbps modems were popular, it was already possible to negotiate 64kbps&#x2F;8-bit clean DS0 channels over T1 — you just needed a PRI or an inter-machine trunk to do so.In fact, if you were an ISP at the time, you generally needed those modems connected to T1 PRIs or IMTs anyway to support your ISDN customers economically. (My memory may be getting fuzzy on this, but I also remember that certain access concentrators with k56&#x2F;v.90 digital modems would only negotiate the higher analog speeds on a PRI or IMT bearer channel.)What there was absolutely no appetite to do, on the part of the voice carriers, was beef up the bandwidth available on POTS lines by developing new ADCs, getting rid of companding, or engaging in labor-intensive “facilities grooming”[0] for a POTS subscriber, in the way they routinely did for T1[1] circuits.0 - Telcos call their lines and equipment “facilities.” Grooming, in the context of outside plant (telco terminology for the physical cabling that goes to subscribers) means to select wire pairs that exhibit good noise and echo characteristics and to remove elements, like poor splices and bridge taps, that impede performance.1 - T1s, unless delivered to a customer via an optical transport network, use the same copper lines as analog POTS lines. Originally, T1s required two pairs (four wires). This eventually was replaced with HDSL4 (two pairs, but better distance and repeater characteristics,) then HDSL2 (single pair.) As the names suggest, the encoding used was a DSL variant. reply Sesse__ 17 hours agoparentAnd even for 56kbps, you need to have the ISP end talk digitally to the telco, so that you don&#x27;t get the noise (including from the DAC) of the local loop on the ISP end. That&#x27;s really what pushes the SNR far enough down to make it work (and why the uplink is only 33.6kbps; I guess they didn&#x27;t want to try to make a separate modulation for that direction). reply marcus0x62 16 hours agorootparentV.92 supported up to 48kbps uplinks, but that standard caught on about as well as you might imagine an analog modem standard from the year 2000&#x2F;2001 to.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;ITU-T_V.92 reply 13of40 15 hours agoparentprevWhen I was in highschool I lived in a rural town of about 7000 people that had an analog telephone exchange. I read about the bandwidth limitations, and a friend of mine and I decided to test them over a local and long distance call. The long distance one cut out pretty much exactly at the frequency the spec said it should, but the local one could transmit and receive much higher frequencies, presumably because it had its own circuit and wasn&#x27;t multiplexed with other ones. (That led to a lot of unrealized fantasies about creating a homemade super modem for local connections.) reply marcus0x62 14 hours agorootparentAssuming the local call wasn’t on a party line, your lines were probably served by an analog switch or maybe on the same line card of something like a DMS-100. What you described wouldn’t work on “modern” digital switch. reply Sembiance 15 hours agoparentprevThe article mentioned using an 8bit ADC. Could a 16bit ADC be used on BOTH sides and achieve 112kbps? reply marcus0x62 15 hours agorootparentSure, with other changes. You would still have to:* Select (or repair) “good” pairs that didn’t have poor echo, impedance, noise, etc. characteristics.* Not compand the resulting PCM sample with mu&#x2F;alaw companding.* Provide an atomic bearer channel from the telephone switch to the ISP that was at least 112kbps.* Develop a modem encoding scheme that could take advantage of the extra bandwidth.At this point, you’ve reinvented a really crappy version of DSL that could be user-signaled to call different networks (i.e., phone numbers.)The other option, the one that people selected, was to use the copper facilities but bypass the phone switch and its associated audio and in-band signaling shenanigans. That was&#x2F;is xDSL. reply dreamcompiler 15 hours agorootparentprevNo because you still have to modulate the signal to get it to&#x2F;from a house, and modulation schemes are where you squeeze maximum bits per second into available analog bandwidth within the limits imposed by Claude Shannon.The article didn&#x27;t talk about modulation schemes and I wish it had. reply TonyTrapp 18 hours agoprevTIL: Phone networks already carried digital signals in the 60s. I absolutely did not expect that. reply p3n1s 18 hours agoparent\"Fast-forward to 1962. Ma Bell begins using T-1s to connect its switching centers. T-1s are a digital communications link, not analog. The entire phone network goes digital. In a modern phone network, the only analog communication is between an individual house&#x27;s phone line and the phone company&#x27;s box.\"This is misleading. The phone network used analog links (typically microwave relay) for long distance almost exclusively for many years (into the 80s) after the introduction of T1. T1 itself is not terribly well suited to very long distance links though and can only be pressed into it with repeaters every mile or so. reply h2odragon 18 hours agorootparent130VDC on the copper, apparently.Had \"the second longest T1 in the service area\" for a while there. BellSouth declined to renew the contract in 2008, the business folded, etc. In 2013 I was digging through the old building, and the T1 line card in the wiring closet was still powered.Nowadays the relay boxes all the way into town are mostly hanging open or have been repeatedly run over. I wonder how much of the rest of that infrastructure they just abandoned in place. reply Aloha 15 hours agorootparentthe 130V was to power either repeaters or the smart jack. reply throwaway167 18 hours agorootparentprevHence towers with dishes on the outside. Long perplexed why they had canvas stretched across, and didn&#x27;t point up: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Microwave#&#x2F;media&#x2F;File%3AFraz... reply jjjjmoney 17 hours agorootparentOur Charter Spectrum “cable” connection goes over a microwave link like this.How do I know? Well, we had a pretty big wildfire a couple years ago that burned a microwave tower a couple hours away. The official response was that they couldn’t get internet back in town until they rebuilt the tower. Our internet was down for a couple weeks until it was safe enough for them to get back to the tower site. reply ac29 12 hours agorootparentprev> Long perplexed why they had canvas stretched acrossIts a radome, used to protect the antenna from weather. reply Sharlin 15 hours agorootparentprevMicrowave relay links predate spaceflight and even when comms satellites became a thing, it made (and still makes) little sense to beam regional TV&#x2F;radio&#x2F;phone up to geosynchronous and back, due to cost and latency. There was no good replacement to terrestrial microwave until fiber optic became a thing.In a very real sense microwave relays are the direct descendant of the optical telegraph (aka \"clacks\" in Discworld lingo). reply dylan604 16 hours agorootparentprev>The phone network used analog links (typically microwave relay) for long distance almost exclusively for many years (into the 80s) after the introduction of T1The reason should be obvious. Digging long distance trenches and all of the property buying&#x2F;leasing and right of ways&#x2F;easements and all of the other fun stuff makes it a total nightmare. Radio broadcast just makes much more financial sense as well as logistically. reply brookst 16 hours agorootparentprevWere the microwave links really analog signaling? reply NovemberWhiskey 16 hours agorootparentThe Long Lines network used frequency modulation and frequency division multiplexing and was completely analog. It was really only advancements in semiconductor lasers and optical fiber manufacturing that drove its abandonment. reply the_third_wave 16 hours agorootparentprev> The phone network used analog links (typically microwave relay) for long distanceThose microwave links represent layer 0 in the connection which is by definition always analogue, the same is&#x2F;was true for T-1 which used alternate mark inversion [1] signalling over a 4-wire circuit. It is the type of signal carried over the link which decides whether it is considered \"digital\" or \"analogue\". T-1 carried 24 time-divided PCM channels (\"digital\"), microwave links could and can carry both frequency-divided multiplexed analogue channels (\"analogue\") as well as time-divided PCM or packet-switched channels (\"digital\").[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bipolar_encoding#Alternate_mar... reply fragmede 18 hours agoparentprevComputers are old. The future is here, it&#x27;s just unevenly distributed. I take a self-driving car multiple times a week yet anyone outside my bubble hasn&#x27;t ever ridden one and thinks that technology doesn&#x27;t work yet. I have conversations with an LLM, but people on the other side of the world don&#x27;t even have laptops or reliable Internet access. Military people have jetpacks to move around on. Inequality is as old as dirt, unfortunately. reply flyinghamster 17 hours agorootparentIt also strikes me how much the old phone network, and its cultural aspects, have faded into the past - particularly when I drive past some derelict bit of telecom infrastructure. Even 30 years ago, it was amusing to watch kids try to deal with a rotary-dial phone. Now even landlines are an endangered species, and area codes are barely relevant. Finding a working telco pay phone these days would be like finding a unicorn.Meanwhile, on a random roadtrip through rural Illinois yesterday, I was seeing \"5G UC\" on my cell phone, out in the middle of nowhere. Unevenly distributed, indeed! reply dylan604 16 hours agorootparent>Finding a working telco pay phone these days would be like finding a unicorn.Even if I found one, I would only be able to make a collect call. I never carry loose change on me, and I haven&#x27;t had a calling card in decades. reply actionfromafar 14 hours agorootparentThere used to be a way to pay with credit card IIRC? reply dylan604 13 hours agorootparentNot that I was ever exposed to. Calling cards were just account numbers with a monthly statement. If you used a credit card for that, then maybe, but to me paying with a credit card suggests a per call transaction. That I have not experienced. But the world is a bigger place than just me, so maybe it was available in other areas? reply the_third_wave 15 hours agorootparentprev> Inequality is as old as dirtYes> unfortunatelyNo. Perfect equality means that nobody can excel in anything, anywhere and any time. With perfect equality comes stagnation. If you happen to know Rush&#x27; song \"The Trees\" you&#x27;ll know what I&#x27;m talking about:Now there&#x27;s no more oak oppression For they passed a noble law And the trees are all kept equal By hatchet, axe, and sawIt is not so much inequality which can be a problem but the fact that the lower bounds can be too low for an acceptable quality of life. As long as the lower bounds are high enough - above the \"poverty line\" - there can be inequality without that being a problem in and of itself.Another thing which can be a problem is the reason for there being inequality, e.g. when some rule or custom creates artificial limits for some people but not for others (\"discrimination\").For a society to thrive you want people to make the best of their possibilities and to use their capabilities to the fullest extent. With that you automatically get inequality because not everyone has the same possibilities and capabilities, e.g. a farmer on a rock-infested piece of glacial detritus will not be able to produce bumper crops even if he is at least as capable as the farmer in the river delta who can not leave his wooden shoes outside without them sprouting roots and growing leaves. On that same river delta a capable farmer will be able to more reliably produce good crops than one who has never heard of crop rotation. If you take the most productive developer in your company and provide him with a fully equipped tool shop he will not be able to produce as good a widget as would the most experienced mechanic who in his turn would not be as productive a developer. Both can do their best to reach their local optimum but the developer will most likely see that result in higher earnings than the mechanic will.Inequality is not a problem. Equity - forced equality as personified by the \"they\" in the lyrics to \"The Trees\" - is a problem. reply fragmede 14 hours agorootparentGreat point. The distinction between equality and equity is an important one, with the canonical image for it for me being the one with the fence*. Where inequality today looks like billionaires throwing parties and doing drugs on their own private island at one end, and people living in tents and going hungry, also doing drugs, but with no hope at the other end, it&#x27;s not the inequality itself that&#x27;s the problem, it&#x27;s that the lowest end of the spectrum is so utterly bleak that people have to steal and commit crimes to survive that&#x27;s the problem.* https:&#x2F;&#x2F;www.researchgate.net&#x2F;profile&#x2F;Shrehan-Lynch&#x2F;publicati... reply retrac 17 hours agoparentprevTelecommunications were the first application for nearly every principle and technology now used in computers.In the 1930s, Claude Shannon was trying to optimize the phone network&#x27;s automatic switching fabric built out of millions and millions of relays when he, more or less by accident, proved that switching circuits are equivalent to Boolean algebra, in a deep sense, and so the appropriate arrangement of switches, can compute anything that can be computed.The first major commercial application for the vacuum tube was not in radio, but in amplifying long-distance telephone links. They would quickly find use in frequency multiplexing telephone lines, too. [1]Similarly, one of the first applications of vacuum tubes as a high-speed electronic switch was for multiplexing telegraph lines, and much of the initial research on high-reliability low-power tubes able to switched fully on without damage began, with the telephone and telegraph companies.The first computers were built out of relays and vacuum tubes specially designed for telecommunications.The transistor was invented at Bell Labs. The planned application was the telephone network. The very same year, the theory behind modern error correcting codes was developed at Bell Labs too. These find extensive use today in data storage in computers, but the original goal was arbitrarily accurate transfer of telegraph data over noisy links.Digital transmission of photographs dates to the late 1920s too; they wanted to send images over extremely noisy long-distance telegraph links. So record the intensity of light over each spot of the image on paper tape. Then send the tape over the wire. And reverse on the other end. [2]I&#x27;ve come to understand the computer revolution as really the story of the telecommunication revolution. Computers serendipitously came about with advances in telecommunications, enabled by the same technologies, and indeed, often driven by the needs of telecommunications. (One of the very first applications for a computer in an embedded context - dating to the late 1950s in experiment - the telephone exchange, of course.)[1] https:&#x2F;&#x2F;long-lines.net&#x2F;tech-equip&#x2F;misc&#x2F;Graybar103-0693.jpg[2] http:&#x2F;&#x2F;www.hffax.de&#x2F;history&#x2F;html&#x2F;bartlane.html reply trealira 16 hours agorootparentI&#x27;d argue that they were also invented specifically to do specific mathematical calculations. Many predecessors of the modern computer were designed to do math. Some examples:The ENIAC was invented to do mathematical calculations for the army.Blaise Pascal invented a mechanical calculator to do arithmetic in 1642. Leibniz improved on his design and made his own version in 1673. However, not many of either version were made, as it was still cheaper to just do the addition, subtraction, multiplication, and division yourself.Herman Hollerith invented a punch card tabulating machine and patented it in 1884. It was used to process the 1890 US Census. It could increment counters to do addition and multiplication, but its main purpose was to assist with processing large amounts of data using punch cards, the way SQL databases are used today.In 1930, Vannevar Bush invented a machine called the Differential Analyzer, which was an analogue computer that could solve differential equations by integration. You could store functions in it by adjusting gears. The purpose was to be able to automatically calculate things like the position of a projectile moving through the air after n seconds without having to calculate it by hand.Most of this I got from the book ENIAC, The Triumphs and Tragedies of the World&#x27;s First Computer, which touches on all these predecessors to the computer. reply Merrill 16 hours agorootparentprevAt one time teletype switching was done in centers where the incoming line was connected to a tape punch. The operator tore the tape off the punch, read the destination address punched at the beginning of the tape, and put the tape on a tape reader connected to an outgoing line to another center closer to the destination. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Tape_relayIP routers now do this faster. reply hinkley 9 hours agorootparentprevThe idea of the Semantic Web dates to about 1945 and Vannevar Bush, in As We May Think. Douglas Engelbart (Mother of All Demos) cites Bush as a major influence.Bush was Claude Shannon&#x27;s graduate advisor.I sometimes wonder how much crazy stuff these people didn&#x27;t write down because they only knew how to make little bits of it happen with current tech. reply queuebert 16 hours agoparentprevIsn&#x27;t Morse code technically a digital signal, albeit with an extremely low bit rate? reply dreamcompiler 14 hours agorootparentYes. It&#x27;s sloppy PWM with a non-ASCII character code. It&#x27;s optimized for human encoding&#x2F;decoding rather than machine; the pulse widths are not globally uniform like they would be in a machine-oriented code, which is what I mean by \"sloppy.\" reply dylan604 18 hours agoprevGrowing up, I lived in the boonies. Even with a 56k modem, the fastest it could go was 33.6k. I got to learn all of the info from the TFA when I was a trying to find out why. That’s also when I learned that some companies had full T1 lines all to themselves for a whopping ~1.5Mbps. Until some makes a call or sends a fax when your top speed drops 64k per phone line being used. It’s also why ISDN didn’t connect at 128k but at 112k since it just used two “bonded” lines of the T1 reply marcus0x62 17 hours agoparent> Until some makes a call or sends a fax when your top speed drops 64k per phone line being used.That wasn’t how it worked. You could have a T1 with certain channels configured for voice and others for data, but the configuration was static — you didn’t get to use the bandwidth from the voice channels when they were idle.> It’s also why ISDN didn’t connect at 128k but at 112k since it just used two “bonded” lines of the T1ISDN bearer channels were 8-bit clean. If you bonded both channels of a BRI, you had a 128kbps data path. What you typically couldn’t use was the 16kbps D channel, which was used for signaling. reply tbyehl 17 hours agorootparentSome ISDN TAs supported data-over-voice for 56kbps per channel. Useful in areas where data calls has per-minute charges but unlimited local \"voice\" calls were included in the monthly charges. reply marcus0x62 16 hours agorootparentRight, and some TAs supported data notifications over the D channel. There was also 144kbps IDSL which made use of the D channel bandwidth. And a bunch of other variations.But, the fundamental channel width was 64kbps and ISDN data calls were not routinely connecting at 56kbps due to robbed-bit signaling, which the GP comment claimed. reply InvaderFizz 17 hours agorootparentprevCorrect. And IIRC, IDSL was just ISDN with all three channels dedicated to data. Yielding 144kbps at phenomenal distances from the CO. reply Aloha 15 hours agorootparentIDSL was an ISDN bearer with Frame Relay running on top, it basically didn&#x27;t use the channelization scheme at all - and IIRC had different framing than a real BRI line. reply marcus0x62 15 hours agorootparentIDSL used HDLC natively as a frame format. You could run Frame Relay on top of that (or PPP.) Line coding was standard 2B1Q in North America (same as a “regular” ISDN BRI.) reply reactordev 18 hours agoparentprevTwin bonded ISDN lines were my only option for anything faster than 33.6k. “You too could have had 128k for the low monthly price of $250 + cost of two phone lines + cost of equipment + labor”. When I finally moved to somewhere that had cable (and cable internet at 1.5mbps) I vowed never to return to the sticks.Then Elon came along… Starlink is probably one of the most useful inventions next to the lightbulb, DC current, Solar Cells, Toilets, desalination&#x2F;ro and the WWW. I’ve gotten 100mb+ out in places where you have no cell service, no humans, no life, and no way of contacting anyone if in trouble. Starlink kept the connection no problem.I’m excited for the future if we can just get over our differences. XR, Starlink, EV’s, Highspeed rail, Electric planes, Mars… reply Aloha 15 hours agorootparentI just dont see how starlink makes money in the long term, without massive government subsidy like Iridium gets. Elon&#x27;s seeming unwillingness to play nice with the federal government in regards to this harms Starlink&#x27;s ongoing viability.Technically, it&#x27;s a pretty neat trick - architecturally, it works just like any cellular network, except the power is moving, not the subscriber. reply inemesitaffia 56 minutes agorootparentThe same government that refused to pay up until they had their arms twisted?The unwillingness is from the government reply charcircuit 15 hours agorootparentprevThe government has been trying to subsidize ISPs to expand broadband to more rural areas, but that has been a failure compared to Starlink which actually exists. reply hinkley 9 hours agorootparentprevT1&#x27;s were so mind numbingly expensive and ISDNs always seemed like a cheaper but insulting value. Same bandwidth as two modems, I ended up holding out for &#x27;broadband&#x27; and making due with two phone lines for a few years. reply toast0 15 hours agorootparentprev> You too could have had 128k for the low monthly price of $250 + cost of two phone lines + cost of equipment + labor”.+ per minute billing from your telco and your ISP. reply the_third_wave 16 hours agorootparentprevnext [4 more] [flagged] smolder 16 hours agorootparentWhile we&#x27;re making suggestions: don&#x27;t bring up \"derangement syndrome\" if you want to be taken seriously. You ironically used it in a sentence appealing for less polarization. The comment was fine without. reply the_third_wave 14 hours agorootparentRefusing to point out a problem does not make it disappear. I consider it clear as daylight that some people react irrationally to certain individuals whether those be Musk or Trump or anyone else. That phenomenon seems to be relatively new, e.g. in the times of Reagan there were many who clearly did not like the man but that never led to much more than \"Ronald Raygun\" and similar expressions and Bush - just as contentious - became \"Shrub\" but even then the reactions were not nearly as - to use a popular term - &#x27;triggered&#x27; as they have become lately. reply smolder 13 hours agorootparentNo, the meme is new. There is no new phenomenon deserving of such a term. Yes, I can point to ridiculous things people said about Trump and call it TDS. It&#x27;s still just a cynical, myopic, partisan take on a subset of behavior from a subset of people, wrapped up in a cute term, meant to dehumanize the opposition. It&#x27;s propaganda. replyredm 18 hours agoprevThis brings back a lot of memories, but I think what I lament the most is that this was really the end of American soup-to-nuts engineering in telecom. At the end of the 56k production and the start of G.Lite DSL, all this engineering started going to China to save money. Long gone are the days of Radio Shack and companies EE&#x27;s designing, prototyping, testing, and manufacturing electronics. Sad. reply MBCook 18 hours agoprevThat was great. Never knew exactly where the 56k number came from.When 56k modems first came out I was able to get full speed, and it was a great upgrade. Then we moved and I could never pass 28.8 or so.Turns out in their infinite wisdom in the new development we moved to the phone company divided things up even more so they could get two lines out of what should have been one. 28 + 28 = 56.I don’t know if they were using some sort of compression or something so that it wasn’t audible on voice calls but it meant a modem was capped forever.The wait for cable internet was interminable (it came before DSL). reply bwann 18 hours agoparentUsually this meant the new neighborhood was put behind a subscriber loop carrier (SLC) which added an extra analog&#x2F;digital conversion. Voice calls were getting digitized locally, sent across a T1 (only 4 wires needed, instead of 96-192), and back to analog to the normal frame at the CO. The effect of this was the telltale sign of carrier speeds only hitting 26,400 bps. reply MBCook 15 hours agorootparentThat’s basically the kind of thing I suspected. Thanks!I wonder if that config screwed them when it came time to try to sell DSL. It still wasn’t available years later when I moved out. reply kazinator 17 hours agoprevHere is a much shorter explanation. PSTN voice calls go over 64 kbps digital links (e.g. G.711 uLaw codec, 8 kHz sample rate).56 kbps pushes close to the theoretically available channel bandwidth.That&#x27;s all there is to it: information coding theory.To get more than around 56 kbps from modems, you would need more fidelity from the underlying lines: a wider-band, lower noise pure copper end-to-end connection, or higher bitrate codecs. reply xcv123 14 hours agoparentShorter but missing an important detail.The article explains that every 8th bit is lost to allow for control signalling that steals the 8th LSB on every 6th frame. Hence 7&#x2F;8 * 64 = 56.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Robbed-bit_signaling reply pjmlp 18 hours agoprevNo mention of dual speed models like Diamond, which would use two 56k lines simultaneously? reply NikkiA 14 hours agoparentDiamond called theirs &#x27;shotgun technology&#x27; iirc. I used a pair of USR external modems cobbled into a shotgun configuration via a linux proxy to manage our internet connection, but we got DSL shortly after (and surprisingly early for the rural location we were in). reply ryan-c 17 hours agoparentprevThat could even be done with two completely separate modems. reply don-code 16 hours agorootparentIndeed, PPP has a multilink feature which an bond packets from multiple modems into a single bitstream. Both the client (your machine - Windows 98 could do it) and your ISP needed to support it.I remember trying this a few times while at my parents&#x27; store. My dad would tell me which phone line I could use that day - either the dedicated line used to process credit cards (if they were closed), or a spare line off the PBX. But occasionally, I was allowed the use of both - and luckily had a pair of PCMCIA modems with the dongles in different places - and could get speeds well above 56k. reply pjmlp 16 hours agorootparentprevNot really, as with using something like the diamond that was hidden by the driver. reply rollcat 16 hours agoprevI loved the article, and when I love an article the first thing I do is look for the website&#x27;s news feed.From :> The blog has been discontinued due to Wordpress security concerns.Keep this tragedy in mind next time you pick your tech stack. reply toomuchtodo 15 hours agoparenthttps:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20111202070238&#x2F;https:&#x2F;&#x2F;www.10str... reply fanf2 16 hours agoprevMy understanding from discussions about modems in the mid 1990s was that the bandwidth limit for POTS-to-POTS modems was about 36kbit&#x2F;s, because the signal has to go DAC (modem) -> ADC (telco) -> DAC (telco) -> ADC (modem), and the modems at either end are not synchronized with the telco’s digital path.The thing that allowed speeds to increase to 56kbit&#x2F;s was digital connections at the ISP end: typically the ISP would get a T1 or E1 circuit fed into a box that would act as 24 modems, but purely digital and synchronized with the telco. So the signal now went Ascend MAX (isp) -> T1 (telco) -> DAC (telco) -> ADC (modem), providing much less quantization noise. reply Lammy 10 hours agoprev> Why 24 of them in each DS-1? No idea.Legacy of https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;12-channel_carrier_system reply theoldlove 15 hours agoprevBoils down to the fact that a phone line to a house was built with a a certain amount of bandwidth for a human voice, implemented with a lowpass filter of 4 kHz. But the article doesn’t explain or show why the magic 4 kHz number was chosen. reply quercusa 13 hours agoparentSee the section Long, long ago reply theoldlove 7 hours agorootparent“Ultimately, they settled on approximately 3.2 kHz.” doesn’t really explain who made this decision, when, or why.All we get is “Long ago, when the phone networks were still completely analog, when the first real phone networks were being built, the engineers that were designing them faced a decision.” Who were those engineers? What company? How did they test what would be adequate bandwidth? reply rasz 15 hours agoparentprevThere are no filters and cable is not BW limited, otherwise ADSL wouldnt work on same wiring. reply aYsY4dDQ2NrcNzA 14 hours agoprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SupraFAXModem_14400 reply Arubis 15 hours agoprevOhhhh, is _this_ where the 7-bit transfer settings came from? That always seemed a weird number to me (unless a bit was instead repurposed as a parity bit). reply hinkley 9 hours agoparentWhatever the wire protocols were that were responsible, the fact that UUNet exposed it is why we all had to deal with it. reply Arubis 14 hours agoparentprevOn second though, probably not; the same 7&#x2F;8 bit settings plus parity and stop are available on non-POTS-dependent serial ports as well. reply rasz 18 hours agoprevUsenet was a fantastic period correct source of information. Here an archive of insightful posts about phone networks https:&#x2F;&#x2F;yarchive.net&#x2F;phone&#x2F; reply flyinghamster 18 hours agoparentA great source of archived information - and TELECOM Digest is still functioning.http:&#x2F;&#x2F;telecom-digest.org&#x2F;http:&#x2F;&#x2F;www.telecom-digest.org&#x2F;latest-issue.html reply RecycledEle 12 hours agoprevTL&#x2F;DR The standard for digital phone lines is (was) 7 bit samples multiplied at 8,000 samples per second.7 * 8000 = 56,000 = 56k reply jeffbee 18 hours agoprevSeems like the article failed to address the architectural reason why any of these details were relevant. A modem is a point-to-point communicating device and the signal has to be carried between two arbitrary points on the telephone network. DSL isn&#x27;t really fundamentally different from a 56k, but the architecture is superior: DSL only goes to the other end of a wire, then the encapsulated IP traffic is forwarded elsewhere. 56k hit a wall because circuit switching has fundamental disadvantages. Packet switching allowed us to get past that. reply p3n1s 18 hours agoparent56k was not relevant for point to point communicating between arbitrary points on the network though. 56kbps dial-up worked in an asymmetric fashion with a fully digitally connected modem on one end (eg the ISP). Station to station analog modem standards capped out at 33.6k.> DSL isn&#x27;t really fundamentally different from a 56kThere&#x27;s a great deal of additional integrated digital signal processing to make DSL work that was too costly for consumer modems at the time. I wouldn&#x27;t agree with this assessment. And other than operating on phone lines for last mile it is completely different on all the layers and uses completely different equipment.> 56k hit a wall because circuit switching has fundamental disadvantages. Packet switching allowed us to get past that.There were circuit switched networks far faster than 56k, so not sure this had much to do with any inherent limitation.Packet switching has obvious advantages for network utilization and was a necessary evolution but I don&#x27;t think it is relevant to any 56k limit or the revolution in DSP that make multi-megabit over copper pairs possible. reply jeffbee 17 hours agorootparentAs you said, the DSP revolution was not scientific it was economic. They always knew how to do it, for decades, but it was unholy expensive. VLSI economically enabled DSL but it was a thing that they knew they wanted to do, and knew how to do, for decades. DSL technology was perfected before the 56K modem was even invented. reply p3n1s 17 hours agorootparent> They always knew how to do it, for decadesUnless we&#x27;re limiting to the purely theoretical this is quite the hyperbole and in that case applies equally well to the 56k modem.Size and heat are also factors. The process nodes did not exist in the 70s and early 80s to produce the integrated circuits necessary. VLSI technically enabled DSL.The idea that somehow the difference between 1960s integration and 1990s is \"purely economic\" is pretty silly. If only they threw trillions of dollars more into research in the 60s would we have had the Intel Pentium then rather than 30 years later, maybe, but that is a useless line of reasoning. Someone has to actually do the \"science\".You need to actually design and produce a suitable line card equivalent for every subscriber that will support DSL speeds without taking up the space of an airport for it and the cooling. That didn&#x27;t exist for decades before DSL was rolled out (which started barely a decade after it was patented).> DSL technology was perfected before the 56K modem was even invented.Considering that DSL standards continued to evolve after the 56K modem was obsolete this is also hyperbole.The other thing not yet mentioned which is pretty fundamental is that DSL eschews the 3.3khz bandwidth of POTS. reply davidmurdoch 19 hours agoprev [7 more] [flagged] tom_ 18 hours agoparentI assume whoever typed it in just missed a word. reply laserlight 18 hours agoparentprevDid you mean: Why HN screw with titles?PS: Title as of this comment is “Why 56k the fastest dial up modem speed” reply cantSpellSober 18 hours agorootparentThe \"is\" is missing from the title (Why is 56k).I wish HN just autofilled the Title field after the URL field has changed during submission (but still allowed users to edit the title). reply HPsquared 18 hours agorootparentReally it would look better if they removed the \"the\" too. Newspapers often leave out that kind of thing. (Not that newspapers are the best role models) reply cantSpellSober 18 hours agorootparentYep, \"Pearl Harbor Bombed\" for example.Or when Colbert mockingly summarized It was the best of times, as \"Dickens: Times Best!\"https:&#x2F;&#x2F;www.cc.com&#x2F;video&#x2F;coii6k&#x2F;the-colbert-report-cable-new... replyswayvil 18 hours agoparentprev [–] Because with the power to control us comes the feeling that they should control us. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the reasons why \"56k\" is considered the fastest dialup modem speed.",
      "It provides technical insights into the history of phone networks, digitization, and the technology's limitations.",
      "The term \"56k\" refers to a maximum data rate of 56 kilobits per second, but actual connection speeds are lower due to FCC regulations and phone network complexities."
    ],
    "commentSummary": [
      "The article explains the reasons behind the maximum speed of 56k in dial-up modems and explores the limitations of different modem technologies.",
      "It also discusses the transition from analog to digital communication systems and the development of technologies like T-1 lines, microwave relays, and DSL.",
      "The article touches on the topic of inequality in access to technology and its impact on quality of life."
    ],
    "points": 223,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1700922491
  },
  {
    "id": 38413064,
    "title": "Shh: Simplifying Shell Scripting with Haskell",
    "originLink": "https://github.com/luke-clifton/shh",
    "originBody": "Shh Shh is a library to enable convenient shell-like programming in Haskell. It works well in scripts, and from GHCi, allowing you to use GHCi as a shell. It's primary purpose is in replacing shell scripts. As such, many functions are provided to mimic the shell environment, and porting shell scripts to shh should be fairly straightforward. A simple \"cargo culting\" port should work in most situations, and perhaps be even more robust than the original. It is also a wrapper tool around launching GHCi as a shell. It supports Automatically defining a function for each executable on your $PATH using template Haskell, as well as a runtime check to ensure they all exist on startup. Redirection of stdout and stderr -- Redirect stdout echo \"Hello\" &> StdErr echo \"Hello\" &> Truncate \".tmp_file\" -- Redirect stderr echo \"Hello\" &!> Append \"/dev/null\" echo \"Hello\" &!> StdOut Piping stdout or stderr to the input of a chained process cat \"/dev/urandom\" |> base64 |> head \"-n\" 5 Multiple processes sequentially feeding a single process (echo 1 >> echo 2) |> cat Use of Haskell's concurrency primitives. race (sleep 1 >> echo \"Slept for 1\") (sleep 2 >> echo \"Slept for 2\") mapConcurrently_ (\\url -> curl \"-Ls\" url |> wc) [ \"https://raw.githubusercontent.com/luke-clifton/shh/master/shell.nix\" , \"https://raw.githubusercontent.com/luke-clifton/shh/master/README.md\" ] Capturing of process output str \"-d\" \"l\" |> capture print s loggedIn(users |> capture) putStrLn $ \"Logged in users: \" ++ show loggedIn mapM_ Char8.putStrLn = captureEndBy0) Capturing infinite output of a process lazily cat \"/dev/urandom\" |> base64 |> readInput (mapM_ Char8.putStrLn . take 3 . Char8.lines) Write strings to stdin of a process. writeOutput \"Hello\" |> cat -- Hello \"Hello\" >>> sha256sum sha256sumpureProc (Char8.map toUpper) |> tr \"-d\" \"L\" -- HEO And much, much more! Look at the documentation on Hackage for a comprehensive overview of all the possibilities. Mnemonics Shh has many symbols that might seem intimidating at first, but there is a simple mnemonic for them.Piping. Looks like a pipe, same as in POSIX shells. & Redirection, think of the shell `2>&1` >, cat Pipe the stdout of `ls` into stdin of `cat` catStdErr Redirect stdout of `ls` to wherever stderr is going. StdErrStdOut Redirect stderr of `ls` to wherever stdout is going. StdOut <!& ls Same as above Globbing Currently Shh does not have any built in globbing support. Rather, it is currently suggested to use another library to do globbing. For example, using the Glob package, it is possible to do something like wc \"--\" =<< glob \"*.md\" Certainly more verbose than the Bash equivalent, however, also more explicit, which is probably a good thing. If this turns out to be too cumbersome, we might introduce a more succinct globbing feature, though it will always be explicit, and thus always more verbose than most other shells. String Interpolation/Expansion/Substitution String interpolation, much like globbing, is left to an external library. I lightweight, zero-dependency solution is to use PyF, which, since 0.10.0.1, has no dependencies other than ones included with GHC. greet = do user <- getEnv \"USER\" echo [fmt|Hello, {user}!|] Usage Some of the features in Shh require that you use the threaded runtime. Please compile with the -threaded flag to avoid deadlocks. Enable Template Haskell and load the environment. It is also strongly recommended to use ExtendedDefaultRules. This is especially important if you want to use OverloadedStrings. {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE ExtendedDefaultRules #-} $(loadEnv SearchPath) You now have all your executables available as simple Haskell functions. If you don't want to load your entire environment you can load specific commands directly: load SearchPath [\"echo\", \"grep\", \"cat\", \"ls\"] If you want to check that all the dependencies still exist, you can use missingExecutables :: IO [String], which will tell you if anything is missing. For use in a project, it makes sense to have a dedicated module for your project which does the template Haskell above. This will prevent recompilation of all the executables, and also allow you to easily namespace them to avoid collisions with normal Haskell functions. Usage in GHCi If you want ^D to be recognised as a EOF marker (when running commands that read from stdin) when running in GHCi, you will need to run the initInteractive function. This sets the line buffering appropriately and ensures the terminal is in canonical mode. Shh as a Shell There is a tool called shh which is a fairly small wrapper around launching GHCi which automatically loads your environment and allows you to have custom config when using GHCi as a shell. To install it, one option is to use cabal new-install cabal new-install --lib shh cabal new-install --lib shh-extras The shh binary will look in your $SHH_DIR (defaults to $HOME/.shh) for a Shell.hs, init.ghci and wrapper files. If these don't exist default ones will be created. The Shell.hs file should contain any top level definitions that you would like to be available in your Shell. By default it loads your environment. The init.ghci file is loaded by GHCi after your .ghci files. This lets you specify settings that you want to take effect when using GHCi as a shell. By default it sets a shell-like prompt. The wrapper file is an executable that is called with the command that is to be executed. By default it just calls exec with the arguments passed to it. The use-case for this is to be able to set up the environment for shh. You might, for example, wrap the execution in a nix-shell. Either way, it is up to you to make sure that the compiler, and packages you require are available, either globally, or provided by the wrapper script. Faster Startup shh precompiles your Shell.hs file so that starting up shh is very quick on subsequent launches. Unfortunately, shh isn't quite able to detect this perfectly. If you see GHCi telling you that it is Compiling Shell, and you notice the delay when starting shh, try manually forcing a rebuild by passing in the --rebuild argument to shh. This is particularly likely to happen if you upgrade your GHC, or installed packages, or even shh itself. Nix Wrapper Example The following snippet could act as a wrapper file to set up a suitable environment using nix-shell #! /usr/bin/env nix-shell #! nix-shell -i bash -p \"(haskellPackages.ghcWithPackages (p: with p; [shh shh-extras]))\" exec \"$@\" Script Usage Nix Nixpkgs provides a writeHaskellBin function which is very convenient for writing quick scripts for your Nix setup. writers.writeHaskellBin \"example\" {libraries = [haskellPackages.shh];} '' {-# LANGUAGE TemplateHaskell #-} import Shh -- Load binaries from Nix packages. The dependencies will be captured -- in the closure. loadFromBins [\"${git}\", \"${coreutils}\", \"${curl}\"] main :: IO () main = do cat \"/a/file\" cp \"/a/file\" \"/b/file\" '' Alternatives There are quite a few players in the \"shell programming for Haskell\" field. This table attempts to summarise some of the differences. Pipe Style refers to how processes are joined together, \"native\" means that the mechanisms provided by the OS are used, while \"via Haskell\" means that the data is read into the Haskell process, and then written into the subprocess. Via Shell refers to whether subprocesses are launched directly or via a shell (which can provide a \"native\" piping solution at the cost of composability) Run in IO refers to whether commands need to be prefixed with run or similar functions to actually execute them. TH Helper refers to whether the use of TH to generate Haskell functions based on commands found at compile time is encouraged in the main library. Monadic Subshell refers to the ability to join multiple processes together and feed them all from the same input and to the same output. echo a(cat; echo b)wc -l should report that 2 lines appeared. Library Pipe Style Via Shell Run in IO Threadsafe cd TH Helper Monadic Subshell Redirect stderr Shh Native No Yes No Yes Yes Yes Shelly Via Haskell Yes No Yes No No Yes Turtle Via Haskell Optional No ? No No (Alternative) Yes shell-conduit Via Haskell Optional No ? Yes Yes No? Errors Library Exception on non-zero Contains arguments Contains stderr Terminates pipeline Shh Yes Yes Optional Yes Shelly Yes Yes Yes Yes Turtle Sometimes No No ? shell-conduit Yes Yes No No",
    "commentLink": "https://news.ycombinator.com/item?id=38413064",
    "commentBody": "Shh: Simple Shell Scripting from HaskellHacker NewspastloginShh: Simple Shell Scripting from Haskell (github.com/luke-clifton) 185 points by subset 21 hours ago| hidepastfavorite31 comments themk 17 hours agoAuthor here.My main use for this is incremental migration away from bash. I&#x27;m not saying this is the best way to do scripts, but it&#x27;s a pretty great stepping stone. Just following the cargo culting guide [0] can already uplift a bash script without changing the logic too much. From there you can start adding structure.My secondary use case is little scripts in NixOS&#x2F;nixpkgs. There are some helpers in there that make it quite convenient.[0] https:&#x2F;&#x2F;github.com&#x2F;luke-clifton&#x2F;shh&#x2F;blob&#x2F;master&#x2F;docs&#x2F;porting... reply cole-k 20 hours agoprevThose of you who use (or used) this as your shell: care to share your experience?It seems a lot less full-featured than https:&#x2F;&#x2F;xon.sh&#x2F;, but maybe you don&#x27;t need a lot of bells and whistles for regular usage. I mostly run build, execute, and install commands.I&#x27;m somewhat enticed at the possibility of being able to wrap common executables into forms that are typed (like nushell or elvish) and manipulate them in a way that leverages the type checker. reply themk 17 hours agoparentAuthor here. I rarely use it as a shell personally, though it does work, and sometimes I drop into it for the odd task. It&#x27;s primarily designed to replace overgrown bash scripts. reply sshine 17 hours agoprevshh is a really cool package, and so is turtle and shelly:https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;turtle&#x2F;docs&#x2F;Turtle-Tutor...https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;shellyshh&#x27;s \"Alternatives\" section summarises nicely some of the differences between these:https:&#x2F;&#x2F;github.com&#x2F;luke-clifton&#x2F;shh#alternativesshh has a neat [fmt| ... |] macro that gets you around escaping strings.shh generally has more Template Haskell support; you may or may not like this.I am personally leaning towards shh because of its \"native pipe style\". reply winterqt 7 hours agoparent> shh has a neat [fmt| ... |] macro that gets you around escaping strings.Per the README, this is actually the work of https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;PyF reply fuzztester 20 hours agoprevRelated: has anyone tried scheme shell (scsh) and what is your experiences with it?https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Scsh reply chubot 19 hours agoparentFWIW there is a big list of similar libraries &#x2F; internal DSLs in nearly all Lisps, and nearly all statically typed functional languages here:https:&#x2F;&#x2F;github.com&#x2F;oilshell&#x2F;oil&#x2F;wiki&#x2F;Internal-DSLs-for-Shelle.g. points to another Haskell project, which may be old - https:&#x2F;&#x2F;github.com&#x2F;jgoerzen&#x2F;hsh&#x2F;wikiWhich is related to the list of free-standing shells:https:&#x2F;&#x2F;github.com&#x2F;oilshell&#x2F;oil&#x2F;wiki&#x2F;Alternative-Shells reply elviejo 19 hours agoparentprevThere is also Rash: Racket Shell that looks súper interesting.https:&#x2F;&#x2F;youtu.be&#x2F;yXcwK3XNU3Y?si=XuABEBZReyfhraAhHaven&#x27;t tried it though. reply velcrovan 18 hours agorootparentIt is interesting, but not something you’d really want to rely on. (see the issues, comments and commit history: https:&#x2F;&#x2F;github.com&#x2F;willghatch&#x2F;racket-rash)This project could really benefit from picking up a couple of steady contributors. It looks like the author is a new dad, and his commit history looks about like what mine did at that stage. reply forgotpwd16 17 hours agorootparentprevInterestingly Rash is also suitable as a user-oriented shell (like bash) rather be reserved for scripting (like scsh). reply loevborg 20 hours agoparentprevLast time I tried I couldn&#x27;t get it to compileThese days https:&#x2F;&#x2F;babashka.org&#x2F; is a much better option. It&#x27;s powerful, fast, modern, works everywhere and has effortless multitasking. reply nemoniac 18 hours agoparentprevIMHO it&#x27;s worth spending some time getting acquainted with it. It&#x27;s very clearly thought out conceptually with good separation of concerns. I learned a lot about Scheme and a lot about Unix from using it. reply agumonkey 18 hours agoparentprevNot this one, but for a while I tried gettinghttps:&#x2F;&#x2F;wryun.github.io&#x2F;es-shell&#x2F;to run (a functionalish shell) reply adastra22 13 hours agoparentprev\"Shell scripts\" is kinda the domain of newLISP, which I&#x27;ve used before and like. reply fuzztester 14 hours agoparentprevThanks for the answers, guys. reply ben0x539 16 hours agoprevThis seems fun! Far too few people who are all \"your bash scripts shouldn&#x27;t be 300 lines and have complex logic!\" address why people reach for bash scripts in the first place, so kudos for that. reply ekiauhce 18 hours agoprevYou can’t really move away from bash, unless there would be alternative as widely available on different Unix systems as bash is.Yes, you came up with a nice syntax for your Haskell scripts, but what it cost to install all required dependencies on, for example, newly created Ubuntu server? reply tikhonj 16 hours agoparentIt&#x27;s pretty easy if you use NixOS :)That&#x27;s been one of the somewhat unexpected benefits for me with NixOS: it&#x27;s so easy to pull in tools and libraries that I don&#x27;t have to worry about using less popular options. reply crotchfire 7 hours agorootparentAre you kidding? Half of NixOS is written in bash.Nixpkgs is just Nix used as a metalanguage for splicing together bash object language programs.Guix is what it looks like when you really get away from bash -- they use scheme as both the metalanguage and the object language. reply aliasxneo 10 hours agorootparentprevThe caveat is that you&#x27;re pushing the complexity somewhere else. I&#x27;m finally moving away from NixOS this month (after two years of using it) because I&#x27;m tired of edge casing interrupting my daily flow. reply keybored 17 hours agoparentprevYou’re not always working in crummy environments. Sometimes you can have nice things (if you think this is nice).I don’t get the impression that this is supposed to be the final solution to Bash. reply themk 17 hours agoparentprevYep, this is mainly used for turning those bash scripts that you let get too large into a real program. You port your script, basically line for line, into this, and then start chipping away at it until you have something less scary. reply norir 16 hours agoparentprev> You can’t really move away from bash, unless there would be alternative as widely available on different Unix systems as bash is.Lua can be built from source in a few seconds on just about any system and can be used pretty easily as a bash replacement. You can even bundle the lua script in a bash script that idempotently installs lua and then calls the lua script embedded in heredoc. reply ReleaseCandidat 17 hours agoparentprevYes, but Shh works on Windows. And on OS X you need to install about the same things if you want to have a recent Bash > &#x2F;bin&#x2F;bash --version GNU bash, version 3.2.57(1)-release (arm64-apple-darwin23) Copyright (C) 2007 Free Software Foundation, Inc. reply marcosdumay 16 hours agoparentprevWhy do you want to develop Haskell programs in a newly created Ubuntu server? Or do you want to compile it down at the deployment target? reply aranchelk 3 hours agorootparentI’m missing something — why would a newly created Ubuntu server be any better or worse than any other system for developing Haskell? reply pbiggar 17 hours agoparentprevWe&#x27;re trying to do a similar thing with darklang right now: providing a single binary with a built-in package manager. The idea being making it easy to write readable \"bash\" scripts (including writing them with AI), but with just one dependency - the darklang binary itself. reply forgotpwd16 17 hours agoparentprevToo bad Hugs98 (installation is few MBs) is no longer updated leaving only GHC (GB+). reply maleldil 14 hours agorootparentYou don&#x27;t need the compiler to run the program in production. You only need the binary. I&#x27;d argue that a heavy toolchain in the dev machine isn&#x27;t a big deal. reply IshKebab 14 hours agorootparentBash scripts tend to be used in situations where you want to edit them and not have to deal with a recompilation step, let alone a multi-GB compiler install. That seems like a deal-breaker to me.The best alternative I&#x27;ve found so far is Deno. It natively supports single file scripts with third party dependencies (this is a big issue with replacing Bash with Python). It uses an existing popular language (so now \"learn our weird language\" problem). Installation is very easy.The only real downside I&#x27;ve found is this stupid bug that they refused to fix:https:&#x2F;&#x2F;github.com&#x2F;denoland&#x2F;deno&#x2F;issues&#x2F;16466 reply williamcotton 15 hours agoprev [–] Huh, I was just tinkering with the same sort of thing with the same motivations but with F#:https:&#x2F;&#x2F;github.com&#x2F;williamcotton&#x2F;fs_playground&#x2F;blob&#x2F;main&#x2F;Scr...Thanks for this, I’m going to borrow a lot of your concepts, especially the added infix operators, those are slick! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Shh is a library in Haskell that enables shell-like programming and is meant to replace shell scripts.",
      "The library provides functions to simulate the shell environment and supports features like redirection, piping, concurrency, and capturing process output.",
      "Shh does not have built-in support for globbing or string interpolation, but suggests using external libraries for these functionalities."
    ],
    "commentSummary": [
      "Shh is a shell scripting tool written in Haskell aimed at replacing large bash scripts.",
      "Users recommend alternative tools like Xonsh, Turtle, Shelly, and Rash for migrating away from bash scripts.",
      "The discussion explores alternatives to bash scripts, including Lisps and statically typed functional languages, as well as the benefits of using NixOS for ease of use with tools and libraries."
    ],
    "points": 185,
    "commentCount": 31,
    "retryCount": 0,
    "time": 1700914936
  },
  {
    "id": 38412582,
    "title": "Darktable: Issues with structure, management, and features causing frustration and fork decisions",
    "originLink": "https://ansel.photos/en/news/darktable-dans-le-mur-au-ralenti/",
    "originBody": "Darktable : crashing into the wall in slow-motion February 15, 2023 — October 30, 2023 — English Languages Français — What happens when a gang of amateur photographers, turned into amateur developers, joined by a bunch of back-end developers who develop libraries for developers, decide to work without method nor structure on an industry software for end-users, which core competency (colorimetry and psychophysics) lies somewhere between a college degree in photography and a master’s degree in applied sciences, while promising to deliver 2 releases each year without project management ? All that, of course, in a project where the founders and the first generation of developers moved on and fled ? Guess ! Degrading basic features The 2020’s are 40 years too late to re-invent interaction paradigms between user and computer, being it how we use a keyboard and a mouse to drive the interface or the behaviour of a file browser. Since the 1980’s, all the general-audience computer appliances have converged toward more or less unified semantics, where the escape key closes the current application, double-click opens files and the mouse wheel scrolls the current view. Darktable1 takes an ill-placed pleasure to ignore all that and the recent changes worsen things : it is now mandatory to read the documentation to achieve tasks as simple as sorting files or assigning keyboard shortcuts to GUI actions. Module groups Everything begins with the overhaul of the modules groups, in 2020, which hides the decision of not deciding of an unified module order. Since 2018, I fight to clean up the graphical interface of Darktable, and in particular the module organization. A graphical interface should promote best practices by laying out tools in the typical order they should be used. Bad practices are those which increase the risk of colorimetric inconsistencies or of circular editing, where one must go back to repercute changes made later, even though bad practices can work in simple cases. In the context of image processing, highly-technical task where a lot of things are hidden to the end-users beneath the GUI, best practices also enable poorly-qualified persons to use the software in a way that reduces the probability of mistakes. This order of using is mostly dictated by technical considerations like the order of application of modules in the pipeline sequence and the use of drawn and parametric masks, which effect depends on upstream modules. Ignoring these considerations is equivalent to looking for trouble, even though the hot trend in the 2010’s and 2020’s is to believe that digital technologies work detached of any material reality for the sole happiness of the user. For example, the constrained imposed upon the pipeline design to allow an arbitrary order of module use creates mathematically unsolvable problems regarding the computation of mask nodes coordinates, and no programmed solution is possible (aside from relaxing that constraint) because maths said no. Except that a significant part of the programmer-users revolving around the project on Github and on the dev mailing-list stay convinced that there is no good or bad workflow, only personal preferences, which is probably true when you practice an art without time constraint, budget constraint or result constraint. As such, modules should be able to be reordered at will, being in the pipeline or in the workflow. The confusion comes from the fact that non-destructive editing is wrongfully seen as asynchronous (which would almost be the case if we didn’t use masks nor blending modes) whereas the pixel pipeline is sequential and closer to a layer logic, as we find it in Adobe Photoshop, Gimp, Krita, etc. Decoupling the modules order in the GUI from the pipeline order is equivalent to enabling every use, even pathological, and forces to write pages and pages of documentation to warn, explain what to do, how and why ; documentation that nobody will read to end up asking in loop the same questions every week on every forum. In this story, everyone looses their time thanks to an interface design trying to be so flexible that it can’t be made safe and robust by default. In the human body, every joint as some degrees of freedom along certains axes ; if every joint could revolve 340 ° around each axis of the 3D space, the structure would be instable for being too flexible, and unable to work with high loads. The metaphor holds in industry software. We swim in the FLOSS cargo cult, where people love to have the illusion of choice, that is being offered many options of which most are unusable or dangerous, at the expense of simplicity (KISS), and where the majority of users don’t understand the implications of each option (and don’t have the slightest desire to understand). In the absence of a consensus over the interface module ordering, a complicated tool, brittle and heavy was introduced at the end of 2020 to allow each user to configure the layout of modules in tabs. It provides many useless options and stores the current layout in the database, using the translated name of modules, meaning changing the UI language makes you loose your presets. Entirely configurable, it lets user decide how to harm themselves, without any best practices guide. This garbage is coded with 4000 lines cheerfully mixing SQL requests in the middle of interface GTK code, and presets are created through redundant compiler macros, whereas modules have had a binary flag forever, allowing to set their default group… modularly. More important, it replaces a simple and efficient feature, available until Darktable 3.2 : One click over the module name enables it, a second adds it to the column of favorites, a third hides it from the interface. Everything allowing presets and storing the current layout in simple text into the darktablerc file. Simple and robust, coded over 688 lignes of legible and well-structured code, the feature was therefore not amusing enough for the middle-aged dilettante developer, and it was urgent to replace it by a labyrinthine system. Keyboard shortcuts In 2021 has been added what I call the great MIDI turducken. The goal is to extend the interface of keyboard shortcuts (already extended in 2019 to support “dynamic shortcuts”, allowing to combine mouse and keyboard actions), to support MIDI devices and… video game controllers. At the end of 2022, that is one and half year after this feature, in the survey I conducted, less than 10 % of users own a MIDI device, and only 2 % use it with Darktable. To compare with the 45 % of users who own a graphic tablet (Wacom-like), which support in Darktable is still so flawed that only 6 % use it. Notwithstanding the poor priority management, what I don’t tolerate here is the edge effects introduced by this change and the global cost it had, starting with the fact that it doesn’t import user-defined shortcuts from versions ealier than 3.2, and it makes the configuration of new shortcuts terribly complicated. Before the great turducken, only a limited list of GUI actions could be mapped to keyboard or mixed (keyboad + mouse) shortcuts. This list was manually curated by developers. The great MIDI turducken allows to map every GUI actions to shortcuts, presenting users with a list of several thousands of configurable entries, in which it’s difficult to find the only 3 you really need, and the text search engine is too basic to be helpful : Note the use of “effects”, on which the documentation is of no help. It’s only by deduction (because the code is not commented either) that I ended up understanding they are emulations of typical desktop interactions (mouse and keyboard) destined to be used with MIDI devices and gamepad controllers (but you still need to explain to me what Ctrl-Toggle, Right-activate or Right-Toggle mean in terms of typical destkop interaction). What is unacceptable is that the use of the numeric keypad is broken by design, noticeably to attribute numbered ratings (stars) to thumbnails in lighttable. Indeed, the key modifiers (numlock and capslock) are not properly decoded by the thing, and numbers are treated differently whether they are input from the typical “text” keyboard or from the numeric keypad. So the 1 from the numpad is decoded Keypad End, no matter the state of the numlock. This is how I had to configure number shortcuts on a French BÉPO keyboard and to duplicate the configuration for the numpad : You just have to remember that Shift+\" and Kp End both mean 1 and remember to duplicate all shortcuts for the numpad and the rest of the keyboard. In short, we break a basic user expectation, and we send design critics to hell. The regression is mentionned on all Darktable forums but seems to bother nobody. The fix of this bug this feature has been made in Ansel and the numeric pad keys are remapped to standard keys directly in the code, for a total of 100 lines of code including comments. Doing this correction has been very hard indeed : I read the Gtk documentation and took their example line by line. 2 years spent waiting for that… The cherry on the sunday is, one more time, we replaced 1306 lines of clear and structured code by a monstruosity of close to 4400 lines, with gems like : the while loop of death (source) : gboolean applicable; while((applicable = (c->key_device == s->key_device && c->key == s->key && c->press >= (s->press & ~DT_SHORTCUT_LONG) && ((!c->move_device && !c->move) || (c->move_device == s->move_device && c->move == s->move)) && (!s->action || s->action->type != DT_ACTION_TYPE_FALLBACK || s->action->target == c->action->target))) && !g_sequence_iter_is_begin(*current) && (((c->button || c->click) && (c->button != s->button || c->click != s->click)) || (c->mods && c->mods != s->mods ) || (c->direction & ~s->direction ) || (c->element && s->element ) || (c->effect > 0 && s->effect > 0 ) || (c->instance && s->instance ) || (c->element && s->effect > 0 && def && def->elements[c->element].effects != def->elements[s->element].effects ) )) { *current = g_sequence_iter_prev(*current); c = g_sequence_get(*current); } The switch case containing if nested on 2 levels (source) : switch(owner->type) { case DT_ACTION_TYPE_IOP: vws = DT_VIEW_DARKROOM; break; case DT_ACTION_TYPE_VIEW: { dt_view_t *view = (dt_view_t *)owner; vws = view->view(view); } break; case DT_ACTION_TYPE_LIB: { dt_lib_module_t *lib = (dt_lib_module_t *)owner; const gchar **views = lib->views(lib); while(*views) { if (strcmp(*views, \"lighttable\") == 0) vws |= DT_VIEW_LIGHTTABLE; else if(strcmp(*views, \"darkroom\") == 0) vws |= DT_VIEW_DARKROOM; else if(strcmp(*views, \"print\") == 0) vws |= DT_VIEW_PRINT; else if(strcmp(*views, \"slideshow\") == 0) vws |= DT_VIEW_SLIDESHOW; else if(strcmp(*views, \"map\") == 0) vws |= DT_VIEW_MAP; else if(strcmp(*views, \"tethering\") == 0) vws |= DT_VIEW_TETHERING; else if(strcmp(*views, \"*\") == 0) vws |= DT_VIEW_DARKROOMDT_VIEW_LIGHTTABLEDT_VIEW_TETHERINGDT_VIEW_MAPDT_VIEW_PRINTDT_VIEW_SLIDESHOW; views++; } } break; case DT_ACTION_TYPE_BLEND: vws = DT_VIEW_DARKROOM; break; case DT_ACTION_TYPE_CATEGORY: if(owner == &darktable.control->actions_fallbacks) vws = 0; else if(owner == &darktable.control->actions_lua) vws = DT_VIEW_DARKROOMDT_VIEW_LIGHTTABLEDT_VIEW_TETHERINGDT_VIEW_MAPDT_VIEW_PRINTDT_VIEW_SLIDESHOW; else if(owner == &darktable.control->actions_thumb) { vws = DT_VIEW_DARKROOMDT_VIEW_MAPDT_VIEW_TETHERINGDT_VIEW_PRINT; if(!strcmp(action->id,\"rating\") || !strcmp(action->id,\"color label\")) vws |= DT_VIEW_LIGHTTABLE; // lighttable has copy/paste history shortcuts in separate lib } else fprintf(stderr, \"[find_views] views for category '%s' unknown\", owner->id); break; case DT_ACTION_TYPE_GLOBAL: vws = DT_VIEW_DARKROOMDT_VIEW_LIGHTTABLEDT_VIEW_TETHERINGDT_VIEW_MAPDT_VIEW_PRINTDT_VIEW_SLIDESHOW; break; default: break; } The nested switch case of the demon, with additive clauses sneakily hidden (source) : case DT_ACTION_ELEMENT_ZOOM: ; switch(effect) { case DT_ACTION_EFFECT_POPUP: dt_bauhaus_show_popup(widget); break; case DT_ACTION_EFFECT_RESET: move_size = 0; case DT_ACTION_EFFECT_DOWN: move_size *= -1; case DT_ACTION_EFFECT_UP: _slider_zoom_range(bhw, move_size); break; case DT_ACTION_EFFECT_TOP: case DT_ACTION_EFFECT_BOTTOM: if((effect == DT_ACTION_EFFECT_TOP) ^ (d->factor max = d->hard_max; else d->min = d->hard_min; gtk_widget_queue_draw(widget); break; default: fprintf(stderr, \"[_action_process_slider] unknown shortcut effect (%d) for slider\", effect); break; } Programmers understand what I’m talking about ; for the others, just know that I don’t understand more than you what this does : it’s shit code, and if several bugs are not hidden in there, it will be pure luck. Hunting bugs in this shithole is sewer’s bottom archaelogy, all the more considering that Darktable does not have a developer documentation and, in the absence of meaningful comments in the code, any modification of the aforementionned code will necessarily start with a reverse-engineering phase becoming harder and harder as time goes by. The true problem of this kind of code is that you can’t improve it without rewriting it more or less entirely : to fix it, you first need to understand it, but the reason why it needs to be fixed is precisely that it’s not understandable and dangerous long-term. We call that technical debt. In short, all the work invested on this feature will create extra work because it is unreasonable to keep that kind of code in the middle of a code base of several hundreds of thousands of lines and expect it to not blow up in our face one day. It’s all the more ridiculous in the context of an open-source/free application where the bulk of the staff is non-trained programmers. Clever developers write code understandable by idiots, and the other way around. Collection filters Until Darktable 3.8, the collection filters, at the top of the lighttable, were used to temporarilly restrict the view on a collection. The collection is an extraction of the photo database based on certain criteria, the most common being extracting the content of a folder (which Darktable calls “filmroll” to confuse everybody, because a filmroll is actually a folder’s content displayed as a flat list instead of a tree - many people wrongfully thing that Darktable has no file manager). Having been a Darktable user for more than a decade, I have a database of more than 140.000 entries. Extracting a collection among these 140.00 pictures is a slow operation. But my folders rarely contain more than 300 pictures. Filtering, for example, the pictures rated 2 stars or more, in a collection of 300 files, is fast because it is a subset of 300 elements. And switching from a filter to another is fast too. The filter is only a partial or total view of a collection, optimized for a fast and temporary start-and-go usage. Under the pretense of refactoring the filtering code, which took all in all 550 lines, the chief Gaston Lagaffe made it a vocation to break this model to turn collection filters into basic collections, by mean of more than 6.000 lines of code, not counting the countless bugfixes that only added more lines2. All that, as usual, highly configurable and redundant with the classical collections module, which remained there, and served by icons so cryptic that they had to add text tooltips on hover to clarify what they mean.. In this quality code, we will found the endless while under the switch case in the if in the if in the for (source) : for(int k = 0; k0) { c = g_strlcpy(out, \" \", outsize); out += c; outsize -= c; switch(mode) { case DT_LIB_COLLECT_MODE_AND: c = g_strlcpy(out, _(\"AND\"), outsize); out += c; outsize -= c; break; case DT_LIB_COLLECT_MODE_OR: c = g_strlcpy(out, _(\"OR\"), outsize); out += c; outsize -= c; break; default: // case DT_LIB_COLLECT_MODE_AND_NOT: c = g_strlcpy(out, _(\"BUT NOT\"), outsize); out += c; outsize -= c; break; } c = g_strlcpy(out, \" \", outsize); out += c; outsize -= c; } int i = 0; while(str[i] != '\\0' && str[i] != '$') i++; if(str[i] == '$') str[i] = '\\0'; gchar *pretty = NULL; if(item == DT_COLLECTION_PROP_COLORLABEL) pretty = _colors_pretty_print(str); else if(!g_strcmp0(str, \"%\")) pretty = g_strdup(_(\"all\")); else pretty = g_markup_escape_text(str, -1); if(off) { c = snprintf(out, outsize, \"%s%s %s\", item %s %s\", item < DT_COLLECTION_PROP_LAST ? dt_collection_name(item) : \"???\", pretty); } g_free(pretty); out += c; outsize -= c; } while(buf[0] != '$' && buf[0] != '\\0') buf++; if(buf[0] == '$') buf++; } and other if nested over 2 levels inside switch case necessary to support the keyboard shortcuts (source). This last fucking crap was the straw that broke the camel’s back and made me fork Ansel. I refuse to work on a ticking bomb in a team that doesn’t see the problem and plays with code over their spare time. Coding may amuse them, not me. And fixing shit done by irresponsible kids twice my age, especially when they break stuff I cleaned up 3 or 4 years ago, infuriates me. Lighttable The lighttable underwent 2 nearly-full rewritings, the first in early 2019 and the second in late 2019, which added many disputable features like the culling view. Quickly, the culling mode is divided into 2 submodes : dynamic and static, which manage the number of images differently. Many users still haven’t understood the difference 4 years later. We therefore have the default view (file manager), the zoomable lighttable (that nobody uses), the static culling, the dynamic culling, and the preview mode (a single full-screen picture). Then, more display options are added to thumbnails in lighttable, allowing to define overlays : basic permanent overlays, extended EXIF permanent overlays, the same but only on hover, and finally the timed hovered overlays (with a configurable timer). The UI code rendering thumbnails and their overlays must therefore take into account 5 different views and 7 display variants, that is 35 possible combinations. The code ensuring proper resizing of thumbnails thus needs a total of 220 lines. But it doesn’t stop there, because the code rendering the thumbnails GUI is shared also with the “filmstrip” bottom bar, which actually makes 36 possible combinations in thumbnail rendering. Multiplied by 3 GUI themes of different base colors, that makes 108 sets of CSS instructions to fully style the GUI… of which many were forgotten in Darktable 4.0 graphic overhaul, and how could it be differently ? In Darktable 2.6, we had 4193 lines for the pack, which had only the filemanager, zoomable lighttable and fullscreen preview views, with only 2 modes of thumbnails overlays (always visible or visible on hover) : 2634 lines views/lighttable.c for the lighttable and the thumbnail rendering, 1124 lines in libs/tools/filmstrip.c for the filmstrip par, which partially duplicates the lighttable code for the thumbnail rendering, 435 lines in libs/tools/global_toolbox.c, for the button menu allowing to enable or disable thumbnails overlays. After Darktable 3.0 and the addition of culling modes, we get 6731 lines : 5149 lines views/lighttable.c, 1177 lines libs/tools/filmstrip.c, 405 lines libs/tools/global_toolbox.c. After Darktable 3.2 and the additions of the 7 variants of highly-configurable overlays and some code refactoring, we get 8380 lines : 1463 lines in views/lighttable.c, 1642 lines in dtgtk/culling.c, where the culling view features were detached, 2447 lines in dtgtk/thumbtable.c, where the thumbnails containers are managed for the lighttable and the filmstrip, 1736 lines in dtgtk/thumbnail.c, where the thumbnails themselves are managed, 169 lines in dtgtk/thumbnail_btn.c, where the specific thumbnails buttons are declared, 115 lines in libs/tools/filmstrip.c, 808 lines in libs/tools/global_toolbox.c. In Darktable 4.2, after the correction of many bugs, we get to a total of 9264 lines : 1348 lines in views/lighttable.c, 1828 lines in dtgtk/culling.c, 2698 lines in dtgtk/thumbtable.c, 2093 lines in dtgtk/thumbnail.c, 166 lines in dtgtk/thumbnail_btn.c, 109 lines in libs/tools/filmstrip.c, 1022 lines in libs/tools/global_toolbox.c. The number of lines (especially in code taking an ill-placed pleasure in ignoring programming best practices) is a direct indicator of the difficulty to debug anything in there, but also an indirect indicator (in the specific case of GUI code) of CPU load required to run the software. Indeed, if you start darktable -d sql and you hover a thumbnail in lighttable, you will get in terminal : 140.8252 [sql] darktable/src/common/image.c:311, function dt_image_film_roll(): prepare \"SELECT folder FROM main.film_rolls WHERE id = ?1\" 140.8259 [sql] darktable/src/common/image.c:387, function dt_image_full_path(): prepare \"SELECT folder || '/' || filename FROM main.images i, main.film_rolls f WHERE i.film_id = f.id and i.id = ?1\" 140.8271 [sql] darktable/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 140.8273 [sql] darktable/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 140.8275 [sql] darktable/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 140.8277 [sql] darktable/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 140.8279 [sql] darktable/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 140.8280 [sql] darktable/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 140.8282 [sql] darktable/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 140.8284 [sql] darktable/src/common/tags.c:635, function dt_tag_get_attached(): prepare \"SELECT DISTINCT I.tagid, T.name, T.flags, T.synonyms, COUNT(DISTINCT I.imgid) AS inb FROM main.tagged_images AS I JOIN data.tags AS T ON T.id = I.tagid WHERE I.imgid IN (104337) AND T.id NOT IN memory.darktable_tags GROUP BY I.tagid ORDER by T.name\" 140.8286 [sql] darktable/src/common/tags.c:635, function dt_tag_get_attached(): prepare \"SELECT DISTINCT I.tagid, T.name, T.flags, T.synonyms, COUNT(DISTINCT I.imgid) AS inb FROM main.tagged_images AS I JOIN data.tags AS T ON T.id = I.tagid WHERE I.imgid IN (104337) AND T.id NOT IN memory.darktable_tags GROUP BY I.tagid ORDER by T.name\" 140.9512 [sql] darktable/src/common/act_on.c:156, function _cache_update(): prepare \"SELECT imgid FROM main.selected_images WHERE imgid=104337\" 140.9547 [sql] darktable/src/common/act_on.c:156, function _cache_update(): prepare \"SELECT imgid FROM main.selected_images WHERE imgid=104337\" 140.9550 [sql] darktable/src/common/act_on.c:288, function dt_act_on_get_query(): prepare \"SELECT imgid FROM main.selected_images WHERE imgid =104337\" 140.9552 [sql] darktable/src/libs/metadata.c:263, function _update(): prepare \"SELECT key, value, COUNT(id) AS ct FROM main.meta_data WHERE id IN (104337) GROUP BY key, value ORDER BY value\" 140.9555 [sql] darktable/src/common/collection.c:973, function dt_collection_get_selected_count(): prepare \"SELECT COUNT(*) FROM main.selected_images\" 140.9556 [sql] darktable/src/libs/image.c:240, function _update(): prepare \"SELECT COUNT(id) FROM main.images WHERE group_id = ?1 AND id != ?2\" 140.9558 [sql] darktable/src/common/tags.c:635, function dt_tag_get_attached(): prepare \"SELECT DISTINCT I.tagid, T.name, T.flags, T.synonyms, COUNT(DISTINCT I.imgid) AS inb FROM main.tagged_images AS I JOIN data.tags AS T ON T.id = I.tagid WHERE I.imgid IN (104337) AND T.id NOT IN memory.darktable_tags GROUP BY I.tagid ORDER by T.name\" which means that 18 SQL requests are made against the database to fetch image information, and run everytime the cursor hovers a new thumbnail, for no reason since metadata didn’t change since the previous hovering. In Ansel, by removing most options, I managed to spare 7 requests, which still doesn’t prevent duplicated requests but still improve the timings somewhat (timestamps are the figures starting each line) : 12.614534 [sql] ansel/src/common/image.c:285, function dt_image_film_roll(): prepare \"SELECT folder FROM main.film_rolls WHERE id = ?1\" 12.615225 [sql] ansel/src/common/image.c:356, function dt_image_full_path(): prepare \"SELECT folder || '/' || filename FROM main.images i, main.film_rolls f WHERE i.film_id = f.id and i.id = ?1\" 12.616499 [sql] ansel/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 12.616636 [sql] ansel/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 12.616769 [sql] ansel/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 12.616853 [sql] ansel/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 12.616930 [sql] ansel/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 12.617007 [sql] ansel/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 12.617084 [sql] ansel/src/common/metadata.c:487, function dt_metadata_get(): prepare \"SELECT value FROM main.meta_data WHERE id = ?1 AND key = ?2 ORDER BY value\" 12.617205 [sql] ansel/src/common/tags.c:635, function dt_tag_get_attached(): prepare \"SELECT DISTINCT I.tagid, T.name, T.flags, T.synonyms, COUNT(DISTINCT I.imgid) AS inb FROM main.tagged_images AS I JOIN data.tags AS T ON T.id = I.tagid WHERE I.imgid IN (133727) AND T.id NOT IN memory.darktable_tags GROUP BY I.tagid ORDER by T.name\" 12.617565 [sql] ansel/src/common/tags.c:635, function dt_tag_get_attached(): prepare \"SELECT DISTINCT I.tagid, T.name, T.flags, T.synonyms, COUNT(DISTINCT I.imgid) AS inb FROM main.tagged_images AS I JOIN data.tags AS T ON T.id = I.tagid WHERE I.imgid IN (133727) AND T.id NOT IN memory.darktable_tags GROUP BY I.tagid ORDER by T.name\" The issue is that the source code nests SQL commands inside functions drawing the GUI, and untangling this mess through the different layers inherited from “refactoring” (supposed to simplify the code, but actually nope) is once again archaelogy. And if the issue had been fixed when the code was 6700 lines over 3 files, we wouldn’t be looking, 4 years later, for the causes in 2500 additionnal lines now spread in 7 different files (not counting .h files). We are in the poster case where “refactoring” actually complexified code and where merging thumbnail code between filmstrip and lighttable only added more internal if (branches) nested on several levels, which complexify even more the structure, only to blindly follow the code reuse principle, which conflicts here with the modularity principle, which a skilled developer would have fixed with [inheritance](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming), because even if it’s not easy to do in C, it’s perfectly possible (actually, Darktable uses this principle in code from 2009-2010). Cosmetics take over stability Darktable 4.2 introduces the styles preview in darkroom. That would be awesome if styles were not deeply broken, when used with non-default pipeline order and multiple module instances. The problem is a clean and long-term solution involves directed graphs theory, and that’s where we lost our beloved copy-pasted code pissers. In the same spirit, we have large inconsistencies on history copy-pasting in overwrite mode when default user presets are also used (especially in white balance module). But it’s far funnier to shit up the interface, so it will stay there for a long time. Darktable 3.6 and 3.8 introduced many variants of the histogram : vectorscope, vertical waveform, advanced and exotic colorspaces. Except that if you launch darktable -d perf in terminal and open the darkroom, you will see a lot of 23.748084 [histogram] took 0.003 secs (0.000 CPU) scope draw 23.773753 [histogram] took 0.005 secs (0.004 CPU) scope draw 23.783284 [histogram] took 0.001 secs (0.000 CPU) scope draw everytime you move the cursor in the window (and not even over the histogram). It is the histogram that gets redrawn at every interaction between cursor and window. The same problem affects many custom graphical widgets and its cause is unidentified. Note that it doesn’t affect Ansel, so the cause should be hidden somewhere in the 23.000 lines of code that I removed. Twice, I tried to refactor the shitshow this feature became, but each time a new feature more urgent was pushed that invalidated my work. I simply gave up. The rotting state of the histogram is such that a full rewrite would take less time than a refactoring, especially since the histogram is sampled way too late in the pipeline, in the screen colorspace, which makes the definition of an histogram colorspace null given that the gamut is clipped in screen colorspace no matter what. But guess what… Darktable 4.4 will have even more options, with the ability to define color harmonies (fundamental for geeks who paint by numbers and edit histograms). It remains that, any time you move the cursor, a great number of useless recomputations are started for nothing. How bad is it ? I had the idea of measuring the CPU use of my system when idle, with the Linux tool powertop. The protocol is quite simple : a laptop (CPU Intel Xeon Mobile 6th generation), working on battery in powersave mode, backlighting set to minimum, open the app and touch nothing for 4 min, then monitor the global CPU consumption of the system as reported by powertop during the 5th minute : Base system (no app opened except for powertop runing in a terminal) : 3.0 to 3.5 % CPU Ansel : opened on lighttable : 2.9 to 3.4 % CPU, opened on darkroom : 3.8 to 4.5 % (before reverting module groups to Darktable 3.2), opened on darkroom : 3.0 to 3.5 % (after reverting module groups), Darktable : opened on lightable : 6.6 to 7.1 % CPU, opened on darkroom : 30.9 to 44.9 % CPU (no, it’s not a coma mistake), I don’t understand what Darktable computes when we leave it open without touching the computer, because there is nothing to compute. Darktable in lighttable consumes by itself as much as the whole system (Fedora 37 + KDE desktop + password manager and Nextcloud client running in background), and it consumes 10 times as much as the whole system when opened in darkroom. All this points towards very buggy graphical interface code. In Ansel, I removed a great part of the dirty code, without optimizing anything else, and these figures are only validating my choice : dirty code hides problems undetectable by reading it, and we simply can’t continue on this path. I’m apparently the only one thinking it’s unacceptable to deprive the pixel pipeline of a third to a half of the CPU power to paint a stupid interface. However you put it, there is no valid reason for a software left open without touching it to turn the computer into a toaster, especially since we don’t buy Russian gas anymore. Working against ourselves We are photographers. The fact that we need a computer to do photography is a novelty (20 years old), linked to the digital imaging technology which replaced for all sorts of reasons (good and bad) a 160 years-old technology, known and mastered. In the process, the fact that we need a computer and a software to produce images is pure and simple overhead. Forcing people who don’t understand computers to use them to perform tasks they could perfectly manage manually before is also a form of oppression, and hiding it as some technical progress is a form of psychological violence. Software implies development, maintenance, documentation and project management. That’s several layers of overhead atop the previous. Yet the fact that the manpower in open-source projects doesn’t ask for compensation should not hinder the fact that the time spent (lost ?) on the software, its use, its development, its maintenance, is in itself a non-refundable cost. The few examples above give an overlook of the complexification of the source code, but also of its degradation over time in terms of quality, because basic and robust features get replaced by spaghetti code, confusing and sneakily bugged. Behind this issue of legibility, the real problem is making the mid-term maintainability harder, which promises a gloomy future for the project, with the maintainer’s approval. Since 4 years that I work full-time on Darktable, 2022 is the first year that I find myself practically unable to identify the cause of most interface bugs, because the working logic has become very obfuscated and the code incomprehensible. The number of bugs fixed is also in constant diminution, both in absolute value and in proportion of the pull requests merged, while the volume of code traffic stays roughly constant (note 1 : the following counts of lines of code include only C/C++/OpenCL and generative XML files and exclude comments 3) (note 2 : the number of opened issues is counted for the lifetime of the previous version) : 3.0 (December 2019, one year after 2.6) 1049 issues opened, 66 issues closed / 553 pull requests merged (12 %), 398 files changed, 66 k insertions, 22 k deletions, (net : +44 k lines), 3.2 (August 2020) 1028 issues opened, 92 issues closed / 790 pull requests merged (12 %), 586 files changed, 54 k insertions, 43 k deletions (net : +2 k lines), 3.4 (December 2020) 981 issues opened, 116 issues closed / 700 pull requests merged (17 %), 339 files changed, 46 k insertions, 23 k deletions (net : +23 k lines), 3.6 (June 2021) 759 issues opened, 290 issues closed / 954 pull requests merged (30 %), 433 files changed, 53 k insertions, 28 k deletions (net : +25 k lines), 3.8 (December 2021) 789 issues opened, 265 issues closed / 571 pull requests merged (46 %), 438 files changed, 41 k insertions, 21 k deletions (net : +20 k lines), 4.0 (June 2022) 632 issues opened, 123 issues closed / 586 pull requests merged (21 %), 359 files changed, 30 k insertions, 15 k deletions (net : +15 k lines), 4.2 (December 2022) 595 issues opened, 60 issues closed / 409 pull requests merged (15 %), 336 files changed, 14 k insertions, 25 k deletions (net : -11 k lines), (deletions are mostly due to the removal of the SSE2 path in pixel code, penalizing performance of typical Intel i5/i7 CPUs for the benefit of AMD Threadripper CPUs), 4.4 (June 2023) 500 issues opened, 97 issues closed / 813 pull requests merged (12 %), 479 files changed, 57 k insertions, 41 k deletions (net : +16 k lines), To make things easier to compare, let’s annualize them : 2019 : 1049 new issues, 66 closed, 88 k changes, +44 k lines, 2020 : 2009 new issues, 208 closed, 166 k changes, +25 k lines, 2021 : 1548 new issues, 555 closed, 143 k changes, +45 k lines, 2022 : 1227 new issues, 183 closed, 84 k changes, +4 k lines. It seems I’m not the only one finding the 2022’s bugs much more difficult to tackle because a lot fewer of them were fixed compared to 2021, and 2023 shows the same trend so far. The ratios of pull requests (actual work done) versus issues closed (actual problems solved) is simply ridiculous. Between Darktable 3.0 and 4.0, the GUI code grew by 53 %, from 49 k to 75 k lines4 (discarding comments and white lines), and reached 79 k lines in 4.4. Letting the poor quality of it aside, I’m really not sure it improved the usability of the software by 53 %. In fact, I’m quite convinced of the contrary. In Ansel, I have so far reduced the GUI code to 53 k lines while removing little functionnality. All this is just too much too fast for a bunch of hobbyists working on evenings and week-ends without structure and planning. The Darktable team works against itself by trying to bite more than it can chew, supporting too many different options, producing code which outcome depends on too many environment variables, being able to interact in too many different ways. All that to avoid making design decisions that could offend some guys by limiting features and available options. On the end-user side, this results in contextual bugs impossible to reproduce on other systems, so impossible to fix at all. It’s simple : the work done costs more and more work, and the maintenance is not assured, as the decline of closed issues shows, because it’s simply too much. In a company, this is the time where you need to stop the bleeding before having emptied the vaults. But a team of amateurs bound to deliver no result can sustain an infinite amount of losses. Only the work created by the work is more tedious, frustrating and difficult as time goes by, and end-users are taken hostage by a gang of self-serving pricks and will pay it in terms of GUI complexity, needless CPU load, and need to relearn how to achieve basic tasks with the software at least once a year. Actually, I’m expecting the current mass-destruction team to conveniently find less and less freetime to contribute to the project as they realize they trapped themselves in a one-way with a tractor-trailer, leaving their shit to the next ones. But the sooner they give up, the less damage they will cause. The debauchery of options and preferences, which is the Darktable go-to strategy to (not) manage design disagreements, creates super contextual use cases where no user has the same options enabled and where it’s impossible to reproduce bugs in a different environement. And to ask users to attach the darktablerc configuration file to bug reports would not help either since that file has currently 1287 lines practically unlegible. Weird and hardly reproduceable bugs pile up, even on System 76 computers designed specifically for Linux, where we can’t invoke drivers issues. Many inconsistent and random bugs I have witnessed while giving editing lessons are not listed on the bug tracker, and it’s rather clear that they lie somewhere in the intricacies of the if and switch case debauchery that are added at an alarming rate since 2020. Fixing these strange and contextual bugs can only be made by simplifying the control flow of the program and therefore by limiting the number of user parameters. But the pack of geeks flapping their arms on the project won’t hear about it and, worse, the bug “fixes” generally only add more lines to deal with pathological cases individually. In fact, Darktable suffers several issues : An hard core of rather mediocre developers who have a lot of free time on their hands to do random stuff, driven by the best intents in the world but oblivious of the damages they make, (mediocre people are always the more available) The complacency of the maintainer, who lets dirty code through to be nice, A critical lack of skills in pure mathematics, algorithmics, signal processing, color science and generally in abstract thinking, which are required beyond pixel processing code to simplify and factorize features, A despicable habit of “developing” by copy-pasting code fetched elsewhere in the project or in other FLOSS projects that may used a different pipeline architecture but without adapting it accordingly (adapting implies understanding, and that’s too much to ask…), A fair and square refusal to prune features to make room to new ones and keep a certain balance, A sampling bias, where the only users interacting with the development through Github are programmers and English-speaking. Fact is the general audience doesn’t understand what a code forge is and it’s difficult to encourage non-programmers to open a Github account to report bugs. We are talking of a users sample made of more than 44 % of programmers and of more than 35 % of university-graduated people (they are respectively 6 % and 15 % in the general population). A forced-march development style, without planning or dialogue, where every Github user can pollute discussions with a non-educated opinion on current work. Fact is image processing looks easy and is harmless, so much so that any person able to compute a logarithm feels competent. But the fundamental mistakes in Darkable colorimetry chain are there to remind us everday of the contrary. A lack of project-wise priorities regarding what features to refactor, stabilize or extend : all projects are open at the same time, even if they conflicts with each other. An amount of activity (emails and notifications) impossible to follow, between comments, off-topic discussions, bugs that are not, code change proposals, actual code changes that may impact your own work in progress, which means you have to be all over the place all the time ; there is a lot to read, very little to keep, discussion for the sake of discussion hinders productivity and the lack of working structure is the main cause of all that, Non-blocking bugs hastily hidden before we are done understanding them, instead of fixing them for real and tackling them at their root, which moves or even aggravates issues long-term without leaving traces into any kind of documentation, A release schedule that we keep no matter the price even when it’s not realistic, whereas nobody imposes it upon ourselves, Code changes that can happen anytime anywhere, meaning we work on quicksand and that we have to work as fast and as bad as the others to not be left behind the volume and frequency of the changes (commits), New features that degrade usability and complicate usage without solving a definite problem, that serves as recreational projects to developers untrained in design/engineering. But the most infuriating is this obstinacy to replace simple and functional features with horrors of over-engineering destined to please deviant and marginal uses while making everybody’s life more difficult with never-ending lists of thoughtless options. The best place to hide a tree is in the middle of the forest, and many still haven’t learned it. Mistaking agitation with activity Any elector likes to criticize the deviance that, in politics, consists in issuing circumstancial laws, ill-written, to appease the public opinion after a special event, to show off that we act, while similar laws already exist and are not or not fully applied for lack of means. We call that agitation : this looks like action, this sounds like action, this has the cost of action, but that leads to nothing tangible or practical. The team of amateurs without project management getting agitated over Darktable produces only future problems. In the past, Darktable was released once every year with around 1500 to 2000 commits ahead of the previous version. That’s now the volume of change achieved in 6 months. A “work” volume increasing that fast without leading to teamwork methods, including clear priorities for each release and task repartition, and without software quality control based on objective metrics (number of steps or elapsed time to achieve a particular task), it’s only dudes stepping on each other’s feet while pushing their own agenda with no care for others, nor the project, nor users. Darktable has become the highschool computer club, where geeks have their fun. It’s globally a sum-up of all the worst stories of IT companies, with the difference that the project doesn’t make a penny, which makes it urgent to ask ourselves why we impose that upon ourselves : there are no profits to share, but everybody shares the costs. It’s a chaotic and toxic working environment which would only manufacture burn-out if the part-time amateurs were bound to deliver results and had to work full-time. Being the only full-time dude on it, I let you imagine the amount of stress and lost energy to stay up-to-date with the permanent cacophony, only to be sure to not miss the 2 % actually relevant to me in the amount of noise produced by unregulated discussions. On the user side, we praise the effervescence of the Darktable project (yes, there is motion), without realizing that the commits runaway is not activity but agitation, and in particular technical debt which will have to be paid we-don’t-know-when by we-don’t-know-who. The beauty of a project where nobody has to take responsibility for their horseshit because nobody is accountable for nothing : it’s written in the GNU/GPL license. We can therefore screw up the work of the previous ones with total impunity. We have the beginning of a quality control, through the integration tests, which measure the perceptual error over reference image processings, but they don’t trigger any response when we see an average error delta E of 1.3 (small) when the nature of the change should have a strictly zero delta E. If the only test passes (because we test a single SDR image over a studio shot), no question is asked on whether the theory is sound and robust. We turned the test into a discharge, as long as the metric stays under the validation threshold… With the release of Darktable 4.0 — Geektable —, I saw on Youtube people starting to complain that this release was not very exciting. After years of dosing people with superlative releases, packed with new features we don’t have time to properly test (6-8 guys who piss 38-50 k lines every 6 months while working only evenings and week-ends, you still dreaming ?), we made them addict to the overpacked Christmas Tree to ensure that, the day we start being responsible and releasing stable versions (thus boring), that will be held against us. The reason for this frenetic release pace is the pull requests older than 3 monthes are systematically in conflict with the master branch, given that this one is shaken every month for “generalized tests”. But the rare users who build the master branch have no idea what they need to test in particular, unless the dissected the commit history of Git, which implies to understand both the C language and the impact of the changes in practice on the software. To limit long-lived branches, which will invariably end in conflict with the master, we found a brilliant solution : we release 2 versions each year, making forced-march development based on unfinished and barely-tested code a way of life, without ever realizing that the core problem is first the lack of planning, but also that contributors start coding before being done defining the problem to solve (when there is a real problem to solve, not just a guy who woke up like “it would be cool if…”), working in parallel on both parts without communication. The dust doesn’t have time to settle that we are already shaking the code base again, without enforcing stabilisation phases where we only clean-up bugs (and I’m not talking of the month of feature-freeze prior to release, but of releases dedicated only to code cleaning). The bug tracker implodes in the 3 weeks following each release, because a significant part of users only uses the pre-built packages, which coincide with Christmas and Summer hollidays, where I personnaly have better things to do after the already-stressful sprint that is the month prior to release. Since 2021, when I update my Git repository with the last changes of Darktable master, it’s always asking myself what they broke this time. We break faster than we fix, and most of the time, the fixes break something else. The only users finding Darktable stable are actually the ones making a very basic use of it, which is ironical for an app whose selling point is to be advanced. And then, I’m being served the fact that it’s free work as if it was an excuse. But it is actually an aggravating circumstance : why do we impose such working conditions upon ourselves if it’s not even profitable ??? In addition to the fact that this free work gives me a post-release burn-out per year, it costs more and more in maintenance and the maintenance is more and more despicable to do. It’s not free work, it’s worse : it’s work that costs without paying. Anyway, the work is provided by people who have limited time and energy. If the resource is limited, cut the bullshit : we are in the same profitability constraints as a business, minus social contributions, except our exchange money is time and it’s not refundable. Without priority management, we will get overtaken by technical debt that we will not have the resource to maintain. What are we waiting for to be happy ? The fact that Darktable is a steamroller in runaway mode and without a driver, which generates an increasing amount of work, is a bad smell for a 15 years-old project. Normally, a mature project slows-down because it’s complete enough to be usable and because people working on it found their cruising speed and efficient working methods. I’m full-time on it since 2018, for a monthly income between 800 and 900 €, and it’s an understatement to say that it’s ill-payed to endure the desastrous consequences of disorganized amateurs trying to have their fun at the expense of the quality of the final product and its usability by computer Muggles. Besides, Muggles are despised too, as a principle. If I’m crawling under a rock for a month to develop a perceptual color space, when I get out, it’s to discover the new labyrinthine system violating a bit more the view-model-controller paradigm and being told that I come too late to oppose it. If I take 3 weeks of vacation in August, it’s to discover that the maintainer bypassed (one more time) my review on a mathematical change over the aforementionned color space, which requires to sit down calmly and think, all this because… we needed to move fast ? For what emergency, exactly ? I probably got the notification somewhere in the middle of the 2234 emails Github sent me between January and August 2022 (in 2021, it was 4044), without mentionning users who ping me everywhere, on Youtube, Reddit, Matrix, Github, Telegram, directly by emails, and previously on pixls.us (693 emails in 2022, 948 in 2021). All this for people completely out of step who don’t realize that I’m doing this all week, that photography may be their hobby but is my job, and that I would just appreciate people off my back during week-ends and hollidays. You can guess that the most annoying are not the ones financially supporting my work. People respect work only if they got billed an high price for it. I don’t have time to be researcher, designer, on top of secretary, while doing technical baby-sitting for a team of Gaston Lagaffe who need both to be trained and to be watched because they are unable to : 1. make a development planning with a list of priorities of new features to work on, 2. provide a specicifications book of needs and issues, with a real use case, before hurrying up on their code editor and doing whatever to develop a new feature in search for a problem to solve. 3. evaluate the maintenance cost of the change before inventing the bionic bacon pump using reversed osmosis which only works on even days if Jupyter is out of phase with Saturn, 4. limit the expenses and cut down the losses when they trap themselves into design one-ways introducing regressions worse than the hypothetical benefits expected, 5. take upon themselves and delay a release if the code is obviously not ready (or I didn’t understand anything and the shareholders will ask for our heads if we release late ???). Management (or team management) is overhead that costs some work, but the Darktable team has reached a scale where the lack of management costs actually more work, especially since none of the project founders are still in the team and the initial design blueprints need to be reverse-engineered with grep in the code everytime something needs change. That was sustainable with a reduced team where everybody knew each other, but Darktable has become an high-traffic project during Covid lock-downs and this way of working is not sustainable with current personnel. Just go see the code ! Compare the branche darktable-2.6.x with darktable-4.2.x, file by file, and enjoy ! All I have heard so far are canned sentences like “it’s like many other opensource projects” and “there is nothing we can do about it”. People are afraid by the amount of work that a fork is (I got emails trying to convince me it was dividing productivity), without realizing the amount of resources currently wasted by the Darktable project and the permanent stress of having to base your work on an unstable code base shaken up all the time. So far, Ansel cost me less fatigue and I solved a significant number of problems among which some were reported since 2016 without signs of interest from the bloody “community”. Besides, Ansel provides automatically-built nightly packages for Linux (.AppImage) and Windows (.exe), as to allow real generalized tests, including by people unable to build the software themselves. I asked for that back in 2019, but apparently, geeks have better things to do, and I had to invest 70 h myself to make that happen. The operation is already a success and allowed to fix in a matter of days Windows bugs that would have taken weeks to spot in Darktable. (And Darktable grabbed 3 weeks later my AppImage build script without proper credits, but that’s a detail). Talking about productivity, let us recall that the lighttable was rewritten almost entirely twice since 2018 (and the last version is not better nor faster) and the big change if collection filters introduced in April 2022 has overwritten another similar change (but only using 600 lines instead of 6000) introduced in February 2022 (the February version is the one in Ansel). We can’t decently utter the word “productivity” when the work of one contributor litterally erases previous work of another’s in a timeframe of one month, for simple lack of project management. It’s called stepping on each other’s feet. So what are we doing to solve the issue ? Suffering in silence ? Living in denial ? Keeping on fixing stuff that another one will break in the next year when we will not be watching ? Keeping on making the Muggle believe, at photo forum length, that opensource is just as good as proprietary, while keeping as joker the fact that it’s free so you get no right to complain ? Isn’t that a bit too easy and dishonest, this double speech ? Wouldn’t you like to stop making habits pass as experience and mistaking fatalism with wisdom, but rather tackle the problem at its core ? Don’t you think that you and me deserve better than software designed by amateurs whose sole talent is to have spare time and can afford to work for free since they moved to management and the kids are off to the university ? Or I got mistaken since the beginning, and opensource is about giving over-complicated tools to geeks who don’t really need them, while trying to convince the rest of the world that open-source is not an hyper-niche for developers ? 4 years of work to get there After 4 years of working on Darktable full-time for 70 % of minimal wage, and 2 years bearing the chronic dissatisfaction of staining my name by contributing to shit, I forked Ansel and will not go back. In 4 years, I brought to this software something that sorely lacked : an unified workflow, based on a set of modules designed to work together, but acting each on a distinct aspect, where Darktable modules were rather a collection of disparate plugins. We are talking about : filmic, tone equalizer, the physically-accurate blurs module, both versions of the color balance, color calibration, including the GUI to profile with color checkers straight in darkroom and the white balancing using CIE standards, the negadoctor module to invert film negatives based on Kodak Cineon, the diffuse and sharpen module for addition and removal of blur based on thermal diffusion, the guided laplacian reconstruction of highlights. I also developed more fundamental tools providing bases for the previous modules : a 4th order anisotropic partial differential equations solver in wavelets space for diffuse and sharpen, an adaptation of the predious as the guided laplacian for RGB signal reconstruction by gradients propagation, a perceptual color appearance model taking the Helmholtz-Kohlrausch effect into account in the saturation computation, to limit the “fluo” effect that typically comes with intense saturation settings, in color balance, a theoritical help in developing the exposure-invariant guided filter (EIGF), in tone equalizer, a linear vector equations solver by Choleski method, various interpolation methods of order 2, 3 and radial-based. In the GUI, I notably did : refactor style declaration, removing styling from C code to map them to the CSS stylesheet, allowing to have multiple themes for the UI, including user-defined ones, introduce the preview mode focus-peaking and ISO 12 646 color assessment mode, introduce the color vocabulary in the global color picker, allowing to name the picked color from its chromaticity coordinates, targetting color-blind photographers. After this, I wrote dozens of documentation pages in 2 languages, published articles and dozens of hours of video on YouTube to demonstrate how to use modules, in what context and for what benefit, including quick edits using only 3 to 5 modules to process 75 to 80 % of pictures, no matter their dynamic range. In the open-source world, except perharps for projects backed-up by foundations (like Krita and Blender), this level of support and documentation simply doesn’t exist, and it’s not the developers themselves who handle this work. Despite all this, I never had more than 240 donators, to compare with roughly 1800 unique respondents who participated in the 2020 and 2022 Darktable surveys, and who declare spending between 500 and 1000 €/an on photography. I will not watch them destroy the usability of this software while trying to convince myself that it’s progress and there is nothing we can do about it. Instead of progress, it’s the delusional vision of progress by a bunch of fifty-something dilettantes. It’s been 2 years that I shut up patiently, trying to be nice, but looking at the degradation of base features, complexified to comply with the fads of mad programmers, I should have been despicable earlier. Playing nice solved nothing because the trend not only carried on, but accelerated, and there will be no realization before the point of no return. We can’t expect from those who created the problems to be the ones solving them. So if I have to work for a “community” who is in mostly for the subscription-free aspect of the software, and who decided that my work is not worth minimal wage, well, I will do it under my terms and with my standards. In terms of features, Darktable already has too much and we need to prune. It’s been 10 years that I use it and it’s already been packed with ill-design stuff. The challenge now is to present the features cleverly, and to fix annoying bugs before these idiots introduce new ones, or even fix them in their own special way : by hiding the dust under the rug. Keeping in mind that Darktable’s pipeline is 15 years old, and we can’t optimize it much more than that, given that it was already tortured a lot to avoid a full rewrite (and a full rewrite offers no benefit if we have to keep Gtk as graphical backend since it’s the primary performance bottleneck). The solutions needed by Darktable imply to remove code and options, not to add always more. Robustness is at that price. The Darktable team does the exact opposite without learning from its mistakes. With Ansel, I want a way to finish this work peacefully so Linux users have a reliable, consistent and performant tool for their artistic photography. Before switching to Vkdt because the current design is shows its limits. By the way, I can no longer bare the attempt of being different for the sake of it by writing “Darktable”, proper noun, without initial capital. It’s childish, it’s neither funny or disruptive, and it makes a mess of freedesktop.org menus where the capital is anyway added to follow the standard. ↩︎ The fact that bugfixes systematically add more lines of code instead of modifying existing lines is a a concerning smell that the programming logic is bad and induces too many particular cases. Rigorous programmers always try to keep their code as generic as possible to avoid spaghetti code. ↩︎ You don’t need to trust me, the command to reproduce the stats is git diff release-3.4.0..release-3.6.0 --shortstat -w -G'(^[^\\*# /])|(^#\\w)|(^\\s+[^\\*#/])' -- '*.c' '*.h' '*.cpp' '*.xml.in' '*.xsl' '*CMakeLists.txt' '*.cl' ↩︎ git checkout release-3.0.0 & cloc $(git ls-files -- 'src/views' 'src/gui' 'src/bauhaus' 'src/dtgtk' 'src/libs') ↩︎ Author Aurélien Pierre Designer and maintainer of Ansel. Developer of darktable scene-referred workflow and physically-realistic modules since 2018. Image retouching educator. \"Do things accurately, or don't bother\". Website » Comments & Questions ? 🇬🇧 English chat 🇫🇷 Chat français 🇫🇷 🇬🇧 Community Related topics 🇫🇷 🇬🇧 Chantal is a bilingual artificial intelligence model for natural language processing, trained for Ansel, photography, image processing and color theory, and able to find relevant web pages among curated sources. Find related pages »",
    "commentLink": "https://news.ycombinator.com/item?id=38412582",
    "commentBody": "Darktable: Crashing into the wall in slow-motionHacker NewspastloginDarktable: Crashing into the wall in slow-motion (ansel.photos) 162 points by lemper 23 hours ago| hidepastfavorite102 comments Jedd 21 hours agoI suspect this comes out of:Ansel.photos:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38390914from two days ago. reply qup 17 hours agoparentWell, it&#x27;s dated to February, so maybe not. reply dang 14 hours agorootparentIt&#x27;s definitely a follow-up submission and we tend to downweight those, since HN is more interesting when it isn&#x27;t repetitive. Note that this isn&#x27;t a comment on the article itself—followupness is an issue of timing, not content.Usually the best place for follow-up links is in the comments of the original thread. Once enough time has gone by to flush the hivemind caches, then the other article can be posted in its own right.Past explanations here if anyone wants more: https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&page=0&prefix=true&que...also https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&page=0&prefix=false&so... reply koiueo 16 hours agoprevMy fascination with Darktable (I used to use it actively) ended after I tried creating a selective mask, an ellipse, to darken the moon on a picture. While the mask was positioned perfectly when zoomed in 400%, it would jump up or down on different zoom levels, and would end up somewhere else when exported (Aurelien Pierre, the OP, mentions screen vs raw coordinates in his post, I guess I’ve seen the manifestation of that).At that point I decided, enough is enough. Darktable is painfully slow, it takes over 1s to see the result of very basic brightness adjustments, the interface is laggy (OP mentions that as well, a simple mouse hover issues over a dozen of SQL requests)…So for now I’m paying for Lightroom mobile. It is lightning fast, has AI masks, comes with 100GiB of cloud storage and works on any decent tablet or phone. I enjoyed Darktable’s instruments, and I like how my images ended up looking. But when even very basic features do not work, the advanced ones become less relevant.I tried Ansel, and unfortunately, although, it fixes some of the issues, it doesn’t fix all of them (and some are absolute productivity killers). And at that point I wonder, why didn’t OP fork some earlier version, like 3.6, without that MIDI revolution and all the regressions. Later in his post, the OP mentions, that dtvk is the future. So why don’t invest the effort there instead? I’m no stranger to donating to open-source projects, but I don’t feel like investing any time or money into something, that has no future (even the OP admits that). reply pkulak 15 hours agoparentI wonder if that’s the way to be a modern Linux user. Use old Think Pads for mostly everything, but an iPad for Lightroom and DRM content.Problem for me is that I like DxO way more than Lightroom… reply koiueo 15 hours agorootparentYeah, it’s a sad world.I’d be happy to use Capture One, but it’s only for Win&#x2F;Mac, and my PC has been strictly Linux for the last 15 years (and I don’t want to deal with another OS just for one application, at least the tablet I use for other purposes).But even with the tablet there are limitation. I’d gladly use Android, but Affinity is only available for iPad OS, and Lightroom mobile doesn’t yet offer all the needed features (bracketing&#x2F;stacking for example). reply ShadowBanThis01 15 hours agoparentprevWhat is DVTK? I search on that doesn&#x27;t pull up any obvious hits. reply koiueo 14 hours agorootparentMy bad, I meant vkdt.https:&#x2F;&#x2F;github.com&#x2F;hanatos&#x2F;vkdt&#x2F;A vulkan-powered image processing software. The OP mentions it in his post reply xoranth 14 hours agorootparentprevI think he meant this:https:&#x2F;&#x2F;github.com&#x2F;hanatos&#x2F;vkdt reply jamiedumont 21 hours agoprevI salute the developer for taking on such a mammoth challenge as (re)building a RAW editor. Sounds like he has the chops for it though!Sitting on the other of the table (professional developer turned professional photographer) I wish someone like Blackmagic would disrupt the big players (Capture One and Photoshop in the stills space) because Resolve looks (and is!) so much more sophisticated with a much healthier attitude towards its users&#x2F;customers. reply franzkappa 18 hours agoparentIt would be a dream! reply morsch 20 hours agoprevI love darktable. Incredibly powerful and fun to play around with. I like the interface much better than the other floss raw editors. I didn&#x27;t mind the learning curve. The documentation is extensive, but I rarely needed it. They&#x27;ve kept adding more features over the years and it never broke anything. reply ekianjo 18 hours agoparent> Incredibly powerful and fun to play around with.What did you think of the performance issues mentioned in the post? reply andybak 18 hours agoparentprevHow do you feel about Ansel? reply morsch 18 hours agorootparentSome of the suggestions seem sensible, but the maintainer comes off as a bit unhinged. I wouldn&#x27;t trust my edit library to it for a while. reply graphe 15 hours agorootparentHow is he unhinged? reply v1ne 18 hours agoprevLooking at the examples given, it seems like Darktable becomes more and more unusable due to a lack of good defaults and easy configurability. I agree with the author that opinion and good defaults help to keep software simple. Yet, I also value having a way to opt out of this and customize it. Take Visual Studio, for example: There are multiple sets of decent keyboard shortcuts and a sensible default layout, plus a menu option to restore it. As a user, you&#x27;re still free to create your own task-specific layouts.When I worked at a big music software company, I also valued the strong opinions that kept the UI simple. Because the software was already so packed with functionality that it would overwhelm users easily if it would be exposed too openly. reply Derbasti 16 hours agoparent> it seems like Darktable becomes more and more unusable due to a lack of good defaults and easy configurability.The funny thing is, in my personal experience, Darktable defaults and configurability have actually markedly improved in the last few years. Usability even more so.Frankly the worst instance of bad defaults and parameters were in the author&#x27;s own filmic module, which were truly terrible in the beginning. They improved after a few years&#x2F;versions, but it&#x27;s still quite the hypocrisy to then complain about this very fact in the rest of the software. reply radiowave 11 hours agorootparentI know what you mean about the filmic module, and while I&#x27;m inclined to think that it doesn&#x27;t necessarily invalidate his criticisms of Darktable, it does make me wonder whether he&#x27;s the right person to fix these things. reply baz00 21 hours agoprevOh fragmentation fun. I use RawTherapee. Seems to be less of a shit show. And I&#x27;m not giving Adobe my money. reply Pomfers 19 hours agoparentMy choice as well. I like the colors my camera puts out mostly, so I only really need to make a few adjustments here and there. DarkTable doesn&#x27;t even conform the RAW to the preview JPEG, so I end up wasting a bunch of time just getting the RAW to look normal in the first place. RawTherapee also seemed far more intuitive to figure out. reply hef19898 19 hours agoparentprevI don&#x27;t care about developer beef at open spurce peojects. I don&#x27;t care neither about the techical finesse behind software or tools I use. What I care about is the results from using said tools. And so far, darktable doesn&#x27;t disappoint.ansel propably won&#x27;t neither. reply baz00 19 hours agorootparentI do care because I&#x27;ve seen a few projects burn through politics. reply trop 19 hours agorootparentWhat we&#x27;re seeing here is one embittered ex-developer spending time&#x2F;effort saying toxic things about their former collaborators in a public forum. At the same time the former developer is making a claim to such radical competence that they can somehow keep an open source project of considerable scope together -- one which until now was coded by generally at least half a dozen committed authors at any given time. The darktable project was originated by some quite bright folks. Others have cycled in&#x2F;out over the years. Generally without too much drama. It&#x27;s too bad to be giving so much attention to divisive claims. reply chris_wot 13 hours agorootparentSome of the code he pointed out is dreadful. reply akho 17 hours agoparentprevRawTherapee has ART. reply ReleaseCandidat 19 hours agoprevThere have been \"813 pull requests merged\" in the 6 months between 12.2022 and 06.2023? Yes, there is more than one maintainer and many of these weren&#x27;t \"code changes\", but nevertheless: almost 4.5 _successfull_ PRs every day?! reply trop 18 hours agoparentFWIW, here is the recent merged pull requests from darktable:https:&#x2F;&#x2F;github.com&#x2F;darktable-org&#x2F;darktable&#x2F;pulls?q=is%3Apr+i...At the moment, this is about a week&#x27;s work by eight authors. Others cycle&#x2F;in out, of course -- this is a spot sample. They range from bugfixes to performance improvements to documentation to translation work. All what one would hope for in a software project headed to its bi-annual release next month.There are many ways to develop, and it may be a bit cruel to compare a one-man show to a long-term international collaboration. But here are the recently merged pull requests from the software which is posted about in the blog post:https:&#x2F;&#x2F;github.com&#x2F;aurelienpierreeng&#x2F;ansel&#x2F;pulls?q=is%3Apr+i...On the first page, I see about five authors offering PR&#x27;s over the course of all of 2023 -- a much slower pace of community development.It appears that Ansel is being developed more by direct commits from its main author. So let&#x27;s compare the recent commits:https:&#x2F;&#x2F;github.com&#x2F;aurelienpierreeng&#x2F;ansel&#x2F;commits&#x2F;masterPage 1 of Ansel commits is by its mono-author from the last week. Page 2 takes us back to August. Page 3 back to June. I totally understand that good developers need to work carefully and sit on things, then release them in due time.Here goes for darktable commits:https:&#x2F;&#x2F;github.com&#x2F;darktable-org&#x2F;darktable&#x2F;commits&#x2F;masterIf we take a moment to page back to page 3, one can note that we&#x27;re back to two weeks ago (rather than June). Steady work by a committed community matters. The log of work done is may be quite worth looking at, rather than incendiary blog posts. reply kamranjon 18 hours agorootparentThe post is about how that type of organizational structure creates chaos. In this case less commits is likely better. Quality over quantity and all that.I have used Darktable because it was really the only game in town for editing raw photos on Linux - and I have to agree that if anything, it needs less features, and more work standardizing and simplifying its UI.You see this all the time, where basically there is no coherent organization, you just get a huge grab-bag of random features slapped together haphazardly. This is exactly what happened to Gimp and actually started to happen to Blender but then they pulled their shit together and it’s kind of a masterpiece now.So I definitely support their effort, understand their frustration, and while a little more tact might be in order - it’s usually the people with passion about something that end up bringing it to another level. reply antoineMoPa 18 hours agorootparent> Blender but then they pulled their shit together and it’s kind of a masterpiece now.I would love to read more about that! Do you have a blog post or other insight? reply kamranjon 17 hours agorootparentThis all started with the release of Blender 2.8 - here are the release notes: https:&#x2F;&#x2F;www.blender.org&#x2F;download&#x2F;releases&#x2F;2-80&#x2F; and you can see a hacker news post about it here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20566139Basically blender had a ton of features but you had to learn all sorts of shortcuts and read endless documentation to even become aware of them.They decided to really focus on the UI and bring it into a more standardized experience, where people who were not intimately familiar with blender would stand a chance - and overall I think everyone has loved it, even those who used it extensively.One of the things I really like that they added was workspaces, so you can quickly start in a UI that makes sense for what you want to do. Just the other day I wanted to make an animated intro from an svg image, so I just opened it in the 2d animation workspace and was off to the races. Blenders UI has always been infinitely customizable, but without bundling that capability into a feature that benefits the user, it really just lead to confusion.2.8 was a huge update for them and they’ve sort of been going all in on that user-centric direction with every subsequent release since then. reply somat 16 hours agorootparentThe blender team has done amazing work on the ui. But it was not some \"complete rewrite to make it more accessible\" It was closer to \"We have a good UI but it has a reputation for being weird and hard. so make the color scheme darker add a drop down menu and most importantly get rid of the undeserved bad reputation by telling everyone we rewrote the whole thing\" That is, it was sort of exactly the same thing as this darktable drama, more marketing than actual change. However in blenders case it was for good as people then gave it a try and discovered thet blenders workflow is actually pretty good.The specific example you gave was the workspaces. That did not sound correct to me so I checked my 2.3 reference manual(the big book they sold right after going opensource) and it has workspaces. I want to say they were one of the big things added for the 2.0 release But I don&#x27;t really remember that well and am too lazy to actually try and install old releases to find out.I don&#x27;t use a lot of 3d software so I am not a huge authority but sometimes it almost is like the opposite has happened. The commercial 3d editors have been copying some of the features blender introduced. stacked windowing system, single window editing, put all the controls up front instead of hiding them behind menus, modal editing. This is a two way stream of ideas as the various editors copy each others good ideas. reply kamranjon 15 hours agorootparentNobody said it was a complete rewrite - in their announcement they call it a redesign, which is what it needed. It&#x27;s pretty common for tech industry folks to downplay the importance of UI, but these changes were far more significant than a dark theme and a dropdown.And sorry I misspoke - I was talking about the new template feature where you go File -> New -> 2d Animation - and it puts you in the preset template for doing 2d animation.The Workspaces are significantly better as well though - it&#x27;s worth downloading blender 2.7 and the most recent version of blender and comparing, it honestly is night and day comparing the old UI to the current one. reply ShadowBanThis01 15 hours agorootparentprevI have a very dim opinion of open-source UI after GIMP and Darktable, so was shocked at how Blender didn&#x27;t piss me off at all on first use... and looks great.There&#x27;s hope! replyrollcat 17 hours agoprevSoftware is perfect not when there&#x27;s nothing left to add, but when there&#x27;s nothing to take away.Patches are easy to reject; try to refuse implementing a feature someone offers you money for.Notice how the most consistent design and the highest quality usually comes from people who are heavily opinionated. The real problem is that there seem to be a lot more people with strong opinions than there are good designs.I&#x27;m not a photographer, so I can&#x27;t judge Ansel vs Darktable based on the merits of their UI design, but I can empathize with Aurélien based on his version of the story. The number of times I&#x27;ve ripped out 100k lines of code to replace them with a hundred is too damn high. Was there a good reason for these 100k lines to exist? Perhaps, once upon a time, but I do feel relief when I rip them out. reply bdcravens 15 hours agoprevThe opening paragraph would imply the issues are the product of non-professional developers, and by implication, that professional developers create software that doesn&#x27;t have those issues. Spend enough time working on several projects with several teams and you&#x27;ll quickly discover that just isn&#x27;t true. Things like \"the while loop of death\" seem relatively tame to what you&#x27;ll regularly see in the wild. reply s800 17 hours agoprevHad me until he used middle-aged as a pejorative. reply wrs 16 hours agoparentNormally it’s us middle-aged folks complaining about the young-uns running with scissors and messing everything up. It’s funny to see the opposite. reply akdor1154 20 hours agoprevAnyone have experience getting product involved in a community led open source project? Working with great product managers is the best bit of software engineering, but i can think of a billion ways that could go tits up if applied to a FOSS project. reply maccard 20 hours agoparentLots of projects have product \"teams\" that will say yay or nay to features. Most commercial open core products will expect justification for feature development.The problem (that I foresee at least) is that if a traditional product role stepped in and said \"no\" to a feature (e.g. a midi device) and that another feature should be prioritised (tablet support), the developers will just... leave (or fork). reply jorvi 19 hours agorootparentWe have an apt saying in Dutch that applies to a lot of open source: “kikkers in kruiwagens” (“frogs in wheelbarrows”)It’s impossible to put anything but a small amount of frogs in one wheelbarrow, because whenever you try to keep or put back in one set of frogs, another few will get startled and jump out. reply ruined 17 hours agoprevdarktable is so bad that when i became a photographer, and finally needed more than glimpse had to offer, i started paying for software. for the first time in my life. reply snapetom 17 hours agoparentSame here. So many people are attacking ad hominem. Author may have a history, author may be difficult, but Darktable is hardly a professional’s tool.I tried everything to stay away from the Adobe tax. I used Retroactive with Aperture 3 until it became too unstable. I tried RawTherapee but found the default tools were just bad, and speed was terrible. Darktable’s performance was a bit better but wow that UI. And not in a good way. I don’t want to pick from five different algorithms (almost all the time you can’t tell the difference anyway) to apply a filter.I eventually just paid for On1 and it’s great. reply e12e 15 hours agorootparentHadn&#x27;t heard of on1 before. Looks like it runs on Mac and Windows, and is currently discounted due to black Friday:https:&#x2F;&#x2F;www.on1.com&#x2F;landing&#x2F;early-black-friday-2023&#x2F;Edit: fwiw capture one also on sale:https:&#x2F;&#x2F;www.captureone.com&#x2F;en&#x2F;pricing&#x2F;capture-one-pro reply internetter 16 hours agorootparentprevI hope Affinity begins seriously competing with lightroom. Wonderful products with a wonderful pricing structure (first software I&#x27;ve bought) (and only software I&#x27;ve bought in the creative field (I use blender, kicad, and resolve)) reply sneak 21 hours agoprevI am excited that someone with this level of competence and attention to detail has forked the project.I tried to use Darktable a half dozen years ago and found its UI to work in bad and difficult and unpredictable ways. I reported it as a bug on IRC and got flamed by the developers; I deleted the app and never looked back (I now reluctantly pirate Lightroom because I refuse to pay for subscriptionware).Hopefully this fork will drag it into useful territory. reply yunohn 19 hours agoparent> I refuse to pay for subscriptionwareI believe this blog post is a great example of why subscriptionware keeps winning - it’s absolutely impossible to support indefinite development of such a large project with one-time payments. reply dreamcompiler 18 hours agorootparentYou can charge for updates. But reaching into my machine and disabling a working app is absolutely unacceptable. reply sneak 13 hours agorootparentprevSubscriptionware wins because people psychologically reason that $5&#x2F;mo in perpetuity is less than $100 once (and the barrier to entry is lower because 5Subscriptionware … dark patternI do not see much evidence that subscriptions are primarily used as a pricing dark pattern.> hard-to-cancel gym membership or newspaper subscriptionBoth your example services involve a recurring cost on the supply side, so they are obviously subscription priced. The hard to cancel nature is provider&#x2F;location dependent - in the EU, I find cancellation is easy. reply diekhans 16 hours agoprevDarktable is an interesting project and an impressive effort. However, serious photographers tend to use Lightroom. It is just eons beyond Darktable.Sometimes even those of us who love programing want to use software rather than develop it. reply graphe 15 hours agoparentI disagree. Capture one is always used and only amateurs who watch YouTube for premade settings use LR. reply busyant 14 hours agorootparent> Capture one is always used and only amateurs who watch YouTube for premade settings use LR.I use C1 (in fact, I just purchased the yearly subscription today for 50% off) and my personal impression is that my images look better in C1 than with LR.But I was always under the impression that many many many \"pros\" use LR simply because it has a lot of bells and whistles (including that somewhat recent \"AI\" content fill).Also, C1&#x27;s layer adjustment paradigm makes the most sense to me compared to everything else I&#x27;ve tried.I&#x27;m not a pro by the way. reply justin66 14 hours agoprev> By the way, I can no longer bare the attempt of being different for the sake of it by writing “Darktable”, proper noun, without initial capital. It’s childish, it’s neither funny or disruptive, and it makes a mess of freedesktop.org menus where the capital is anyway added to follow the standard.Losing some cred with all the aspiring teenage poets. reply watmough 19 hours agoprevExcellent rant! I don&#x27;t use DarkTable but ... I&#x27;ve been there. reply Euphorbium 21 hours agoprevSeems like the only good open source projects are made by a single person. reply MattRix 19 hours agoparentThere are lots that are good and made by many people. An obvious example is Blender. reply ReleaseCandidat 19 hours agorootparentYes, but many of them have - like Blender or Linux (the kernel) - a team of professional \"core\" developers. reply dagw 17 hours agorootparentThey also both have a single person that has been leading the project from day 0 up until today. reply MattRix 17 hours agorootparentthat is a vastly different thing from “made by a single person” reply forgetfulness 15 hours agorootparentYeah but the context of the discussion is this caustic essay against what the author claims is the product of design-by-committee, having a single or small core leadership isn&#x27;t a recipe for success, but Darktable doesn&#x27;t seem to be doing too well in its absence. reply MattRix 17 hours agorootparentprevNot sure how that’s relevant? It’s still not a single person. reply haunter 20 hours agoprevIf only there was a way to run the old Aperture on Windows and&#x2F;or Linux reply phkahler 18 hours agoprev>> Except that a significant part of the programmer-users revolving around the project on Github and on the dev mailing-list stay convinced that there is no good or bad workflow, only personal preferences,In which case, it&#x27;s OK for a developer to ignore YOUR personal preferences and go with something less flexible. You can ruin a project by trying to accommodate multiple workflows. Let me emphasis that - RUIN A PROJECT.In solvespace, tool use consists of selecting sketch entities and then applying a tool - example: select lines and apply a constraint (parallel, perpendicular, equal), or what I call \"noun - verb\". Some people want to use \"verb - noun\" which would mean selecting the operation (verb) first and then the thing to apply it to (nouns) and then some kind of \"go\" or \"ok\" to actually do it. Some have even submitted PRs trying to implement the later as an option.We will not have it. The disruption to the code is too much. Verb-noun introduces state where there was none, which is a maintenance burbern (small but not zero). It requires the extra step to apply the operation. It does not offer additional capability to users.At some point you end up \"refactoring\" and \"building an API\" to enable all this flexibility. And that&#x27;s maybe OK for commercial software where people are paid to waste countless hours pandering to the whims of large customers. Architecture astronauts like that kind of stuff for its own sake. Keep them away. Refactor when it&#x27;s necessary to bring new functionality, not when it&#x27;s just a user preference.Make an application, not a toolkit for making applications. reply koiueo 16 hours agoparentAll great software for artists is very opinionated. Lightroom has very specific order. Capture One has it. OP is absolutely right, that the order of modules must be fixed. Arbitrary modules order is an unnecessary complication for artists, and there are strong technical reasons why the order should not be arbitrary. reply cycomanic 15 hours agorootparentThis is actually one of the points I disagree with the author. I mentioned in the previous story about ansel that I really like lightzone, which unfortunately has not seen much attention (the project site has been taken over by a squatter so you can only place is currently the GitHub page).They had configurable execution order and masks well ahead of everyone else (back when lightzone was still commercial) and the interface was always straight forward. Some things are just much easier to achieve with reconfigurable orders. For example desaturating an image and then adjusting the contrast curve of the black and white photo. I just tried that in ansel and you can&#x27;t do it because the desaturation is done after the tone mapper. You can probably achieve the same result but it&#x27;s just more difficult.I find that darktable (and ansel as well) suffer from too many modules touching the same fundamental things. E.g. There seem to be several modules that adjust the color calibration white balance, exposure. So I have seen multiple times that I get a very bad initial look, but I can&#x27;t fix it easily with e.g. the white balance without turning those other modules off. reply koiueo 15 hours agorootparentThe duplication stems from the presence of two workflows in DT: scene-referred and display-referred.I think OP was pushing for the scene-referred workflow and removal of the legacy display-referred workflow and all related modules. In fact Ansel made a step in that direction. It removed legacy modules (from UI at least), also, for example, it replaced sharpen and soften modules with some iirc wavelet-based thingy which does both (I forgot the name as I tried it long ago).Can’t comment on contrast after desaturation directly. Perhaps OP could explain why it is done so. A tangential thing: desaturation is actually a complicated process. You could rely on lightness, you could rely on RGB components, etc.. I think Ansel and its Color balance and CAT modules give you full control over all this complexity. These tools are much more flexible than anything offered by competitors. But it’s certainly not easy. This is a bit related to the sibling comment about targeting towards technically inclined people. reply dantondwa 16 hours agorootparentprevTo change modules order in Darktable, you have to know what to do. By default, the module order is correctly arranged and no intervention is required. Changing the order is an option that you can use only if you want to. No user will ever change a module&#x27;s order by mistake (and in the remote case that one did, fixing that is a click away).We have Lightroom, we have Capture One. Let one software for more technically-inclined photographers exist, where you have options and power over the pipeline. Darktable does not aim to be a Lightroom replacement. reply koiueo 15 hours agorootparentI’m fully with you on DT being oriented towards technically inclined artists. And I think OP is more on that side than both of us combined.I accidentally put words into OPs mouth. Not fixed module order, but correct module order. But even if OP argues for the fixed order, I would trust him fully on that. From what I recall, one of his arguments was that different modules work in different color spaces. And it does not make any sense whatsoever to feed linear RGB into a module that works in some perceptive (logarithmic) space. A car owner who installs 4 different-sized wheels on their car is not technically inclined, he’s just an idiot.FWIW, in my 2 years of using DT, I never needed to reorder modules. I mean, I used to play with that in the begining. I would spend 4 hours on a single picture reordering modules and doing all the crazy stuff, and then eventually I would start over because the pipeline becomes unmaintainable. I just don’t see, how this can be any useful if your goal is being productive and not just fooling around with your pictures. reply dantondwa 17 hours agoprevThe author, while having contributed greatly to Darktable in the past and being without any doubt skilled, has severe communication issues. When he was in the darktable team, he was abrasive and offensive. There is a limit to how much offensive you can be to others, parlrticulaly when you are all volunteers. So, I&#x27;d take his words with a grain of salt and take into account the background of all this: a skilled person with a very little ability to speak to others starting a feud with another opensource project to which he contributed in the past. reply andai 17 hours agoparentOk... but is he wrong?Agreed that being too harsh&#x2F;blunt can be counterproductive, but at the same time, the way a message is delivered does not make the points any less valid. (It just makes the relevant people less likely to listen? Though at this point it seems that hope is long gone anyway?) reply dantondwa 17 hours agorootparentIn general, I do think that Darktable&#x27;s UI could be better, and most likely it could be more efficient. His points are not wrong, but they are also not as fundamentally disastrous as the author makes them to be. I feel the damage that the author caused with his behavior is potentially worse for Darktable than the problems themselves.In a collaborative project run by volunteers, the way one argues and the point one makes are united. Darktable is possible only if there is a group of people working on it. I&#x27;ve seen the author insulting fellow contributors in public forums, I&#x27;ve seen him using the most offensive possible language against people who were being polite to him, just because they disagreed. This is not how you steer an opensource project in the right direction. It is also useless to come and shit everywhere after the changes are done. It is the opensource equivalent of seagull management [1].I think in a collaborative project, the way you state your point is as important as the value of the point you&#x27;re making. If you&#x27;re unable to state your point clearly and effectively, then you should work alone, as the author has now correctly decided to do. Continuing the shitstorm even after his collaboration with Darktable has terminated seems, to me, to be just the continuation of a useless feud.I say all this as a happy Darktable user, and not a contributor at all. I also loved the author&#x27;s contribution, and found his Youtube channel to be really informative and a fantastic learning source. Still, Darktable has continued after his departure, and the new release, which will come for Christmas, is really promising, with the inclusion of per-channel curves in the sigmoid module, similarly to Blender&#x27;s AgX. Many other improvements are coming, and I don&#x27;t feel the project is \"crashing into the wall in slow-motion\".[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Seagull_management reply Derbasti 16 hours agorootparentprevPersonally, I think he is wrong.Several of the features he removed are useful to me. Whatever performance issues he quotes do not seem to apply to my installation.And there have been rather useful additions to darktable after his fork. For instance the Sigmoid module, an alternative to the author&#x27;s Filmic module, which I prefer over filmic. Or the very useful Primaries module. To say nothing of various performance improvements, an Mac version that is not broken, and updated camera support.I&#x27;ve had a few run-ins with the author of this software. He has very strong opinions, and does not accept competing ideas. Good for him to build a fork of darktable that suits his sensibilities. But marketing it as a better alternative to darktable is misleading at best.I read his efforts as a prime example of burnout. He tried to make the community project darktable into his fiefdom, and failed. So now he&#x27;s frustrated and angry, and slings insults at the darktable devs. reply ukd1 16 hours agorootparentIsn&#x27;t this parts of his point; darktable got stuffed with features that a few folks use, and the overall quality suffered. reply michaelteter 16 hours agorootparentprev> the way a message is delivered does not make the points any less validbut it certainly can demotivate other volunteers, having a net negative effect reply ukd1 15 hours agorootparent100% - though that&#x27;s now advantageous for him - disrupt the competitor, let the world know he thinks he&#x27;s making a better product, cause some drama reply andai 13 hours agorootparentOP is growth hacking! reply entropie 17 hours agorootparentprev> can be counterproductiveOP is clearly not. He made a (usable?) version of darktable.I have not tried it yet, but I will. I still have PTSd from my darktable tries. It was such a mess but at this time i thought its my fault. reply ukd1 16 hours agorootparentprevIt&#x27;s an aggressive delivery, but I think you&#x27;re asking the right question. From his citations and examples, it seems believable he&#x27;s correct. reply RobotToaster 21 hours agoprevMy experience of darktable consists of:seeing my RAWs looking completely wrongGoogling wtf is wrongSeeing a bunch of reddit&#x2F;forum threads of devs telling people that it&#x27;s a feature not a bug.Going back to pirating lightroom reply hef19898 19 hours agoparentHow exactly does a RAW file look wrong? Before editing, I assume. reply baq 19 hours agorootparentI’m not sure it makes sense to say that RAW files look good or bad at all. They’re mostly sensor data dumps.The point is darktable has terrible defaults and terrible usability to allow a novice user to convert the dump to a usable image. Images loaded into darktable are less immediately usable than default camera settings. This might be ok for experienced users, but it’s absolutely disastrous for newbs. reply hef19898 46 minutes agorootparentFair enough, quite often I use the camera generated jpgs as well. reply Pomfers 19 hours agorootparentprevYes, before editing. All the other RAW editors I&#x27;ve seen, including the other FOSS editor RawTherapee, will automatically adjust the sliders to match the embedded preview JPEG. DarkTable doesn&#x27;t do this, all my RAWs load in looking really strange. reply hef19898 45 minutes agorootparentYou can set presets, I don&#x27;t. Because the jpeg version doesn&#x27;t have to be the \"right\" version of the photo (there actually is no such thing as right, hence my preference to start from a pure raw file). reply busyant 17 hours agorootparentprevI&#x27;ve tried DT and my RAW images look washed out by the software defaults.There are videos and forum responses explaining how to get a more appealing image, but the suggestions require a LOT of work and they assume a pretty deep understanding of color science.DT is like a car that you can only drive if you know how how to tune each part of the engine. Otherwise, you just drive something that sputters down the highway. reply hef19898 43 minutes agorootparentRAW files always looked washed out, by nature that is. If they don&#x27;t, whatever software you use applied presets. reply RobotToaster 17 hours agorootparentprevAs Pomfers says, it doesn&#x27;t match the camera defaults, something that every other RAW processor seems to do. For my workflow the camera settings are usually 90% where I want. reply hef19898 41 minutes agorootparentThat&#x27;s why I shoot both, RAW and JPG. If the JPG is fine, I use that (after sensor spots and so on). reply BeetleB 16 hours agorootparentprev> it doesn&#x27;t match the camera defaults, something that every other RAW processor seems to do.Actually, none of the RAW processors I&#x27;ve used (both paid and free) match my camera&#x27;s defaults (2 different cameras). The paid one even had a dedicated page explaining why so that they could direct newbies to it.There are a lot of cameras out there, and you cannot expect developers to figure out the curve to match the camera for all of them. That&#x27;s why these SW let you specify your own default curve. reply genman 18 hours agoprevI am getting really bad vibes from this rant. It looks like some wannabee dictator who wants to define what is good&#x2F;right and then force it upon others. It kind of correlates with the chosen name - Ansel Adams actively discriminated against photographers with different style than his to the level of not including important photographic works in his photography history book. reply mardifoufs 18 hours agoparentI thought so too until I looked at the code. It&#x27;s truly horrible and not in a \"clean code absolutist\" way, but more in a \"digging a giant hole that will make the entire thing untouchable\". And going by the stats he provides that&#x27;s exactly what&#x27;s going on.FWIW, this article is originally written in french. And while the French version is definitely still abrasive, it sounds more normal. The writing style does not map to English as well for sure. reply internetter 16 hours agorootparentYeah, I&#x27;ve never seen a codebase more terrifying. It&#x27;s a wonder that thing manages to work as well as it does reply kramerger 15 hours agorootparentprevI belive the project was started by someone who was not a programmer. I bet your first project didn&#x27;t look that great either ;)I am currently reviewing safety critical code written by two very respected companies and occasionally that code looks worse than the snippets author showed. reply mardifoufs 14 hours agorootparentThe problem is that the snippets shown are all from new features and code written in the past 2-3years! I would obviously not judge old code otherwise reply antoineMoPa 18 hours agoparentprevYeah on one hand, a lot of successful & high quality software projects are the result of one very dedicated brain, so maybe it&#x27;s a valid tradeoff to have an arrogant dictator.On the other hand, that does not feel so nice as a contribution environment, it creates a software design bottleneck on one person, increasing shipping time and creating a dangerous bus factor. It&#x27;s not a really scalable way do software design. Overall that attitude is pretty detrimental to community as he basically complains about all previous contributors.Maybe I&#x27;d suggest him to:- Work on his attitude, respect other contributors & stop calling them names.- Spend less time developing and more time educating other people who work on the project.- Come up with a set of design principles that the rest of the project can follow.But yeah that&#x27;s open source, he&#x27;s free to fork a project. Probably his project will be successful anyway despite the negativity. reply antoineMoPa 15 hours agorootparentThis being said, that while loop of death is pretty terrifying indeed. reply solatic 16 hours agoprev> the while loop of death (source: https:&#x2F;&#x2F;github.com&#x2F;darktable-org&#x2F;darktable&#x2F;blob&#x2F;darktable-4....)shudderYeah, I too wouldn&#x27;t want to volunteer to contribute to a project which is OK with this. reply andyjohnson0 15 hours agoprevLast year I migrated off Lightroom because my intermittent use made it increasingly hard to justify the monthy fee. I looked at Darktable as a replacement and, while it seemed superficially capable, I soon realised that actually doing anything with it required more patience and resistance to pain than I possess. I wish I could have liked it, but the recent posts here finally make me realise why I didn&#x27;t.So I&#x27;d be interested in the hive mind&#x27;s recommendations for an LR replacement. I have my LR edits exported as xmp sidecars, I&#x27;d prefer a one-off purchase rather than a subscription and I&#x27;m an average punter with more enthusiasm than skill. I shoot mostly jpeg with some raw. What should I be looking at to replace LR? reply ukd1 15 hours agoparentditto. I&#x27;ll try Ansel, but don&#x27;t have high hopes. I&#x27;ve realized moving 30k+ photos from Lightroom is not likely a thing. I wish I&#x27;d not gotten a cloud version. reply andyjohnson0 12 hours agorootparent> I&#x27;ve realized moving 30k+ photos from Lightroom is not likely a thing.If it helps, I built this [1] to extract xmp sidecars from the LR database.[1] https:&#x2F;&#x2F;github.com&#x2F;andyjohnson0&#x2F;XmpLibeRator reply konart 18 hours agoprev [–] Tried Darktable and RawTherapy both of them have terrible performance and UI\\UX. With all respect to the authors and contributors I can&#x27;t imagine working with either instead if Affinity Photo. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Darktable software, aimed at amateur photographers, has been facing problems related to lack of organization, project management, and technical considerations.",
      "Recent changes to the software have led to a degradation of features, making it difficult for users to effectively utilize the software and increasing the risk of errors.",
      "There are concerns about the code quality, complexity, and lack of documentation, resulting in frustration among users and prompting some to consider forking the software or exploring alternative options."
    ],
    "commentSummary": [
      "Users express concerns about the performance, interface lag, and lack of basic features in Darktable, a photo editing software.",
      "Alternative software options such as Ansel, RawTherapee, DxO, and Lightroom are mentioned and compared.",
      "The conversation covers topics such as the development and maintenance of Darktable, quality and user interface design in software development, open-source projects, subscription models, and accommodating user preferences in photography software."
    ],
    "points": 162,
    "commentCount": 102,
    "retryCount": 0,
    "time": 1700907655
  },
  {
    "id": 38414841,
    "title": "New Wave Sci-Fi: 75 Best Novels 1964–1983",
    "originLink": "https://www.hilobrow.com/new-wave-sci-fi/",
    "originBody": "New Wave Sci-Fi: 75 Best Novels of 1964–1983 Science fiction’s so-called New Wave era began in approximately 1964. Writing in 2003 about that “cusp” year, Michael Moorcock noted: “It will [soon] be 40 years since JG Ballard published The Terminal Beach, Brian Aldiss published Greybeard, William Burroughs published Naked Lunch in the UK, I took over New Worlds magazine and Philip K Dick published The Three Stigmata of Palmer Eldritch.” The era lasted through approximately 1983 — giving way to the cyberpunk era, the kickoff of which we might as well date to the 1984 publication of William Gibson’s Neuromancer. What is New Wave science fiction? Moorcock describes it as the era during which the genre “rediscovered its visionary roots and began creating new conventions which rejected both modernism and American pulp traditions.” Right! The best sf adventures published during the Sixties (1964–1973) and Seventies (1974–1983) — my 75 favorites are listed on this page, but this is by no means an exhaustive list — are characterized by an ambitious, self-consciously artistic sensibility; they often concern themselves, at the level of content and form, with the nature of perception itself; and they will blow your mind. This page is a work in progress, subject to revision. — JOSH GLENN * NEW WAVE SCI-FI at HILOBROW: 75 Best New Wave (1964–1983) Sci-Fi NovelsBack to Utopia: Fredric Jameson’s theorizing about New Wave sci-fiDouglas AdamsPoul AndersonJ.G. BallardJohn BrunnerWilliam BurroughsOctavia E. ButlerSamuel R. DelanyPhilip K. DickFrank HerbertUrsula K. Le GuinBarry N. MalzbergMoebius (Jean Giraud)Michael MoorcockAlan MooreGary PanterWalker PercyThomas PynchonJoanna RussJames Tiptree Jr. (Alice Sheldon)Kurt VonnegutPLUS: Jack Kirby’s Golden Age and New Wave science fiction comics. JOSH GLENN’S *BEST ADVENTURES* LISTS: BEST 250 ADVENTURES OF THE 20TH CENTURY100 BEST OUGHTS ADVENTURES100 BEST RADIUM AGE (PROTO-)SCI-FI ADVENTURES100 BEST TEENS ADVENTURES100 BEST TWENTIES ADVENTURES100 BEST THIRTIES ADVENTURES75 BEST GOLDEN AGE SCI-FI ADVENTURES100 BEST FORTIES ADVENTURES100 BEST FIFTIES ADVENTURES100 BEST SIXTIES ADVENTURES75 BEST NEW WAVE SCI FI ADVENTURES100 BEST SEVENTIES ADVENTURES100 BEST EIGHTIES ADVENTURES75 BEST DIAMOND AGE SCI-FI ADVENTURES100 BEST NINETIES ADVENTURESNOTES ON 21st-CENTURY ADVENTURES. *** SOME GOLDEN AGE SCI-FI TITLES The following titles from science fiction’s so-called Golden Age (1934–1963) are listed here in order to provide historical context. Isaac Asimov’s I, Robot (1940–on; as a book, 1950) Ray Bradbury’s Martian Chronicles (1946–on; as a book, 1950) Ray Bradbury’s Fahrenheit 451 (1953) Alfred Bester’s The Stars My Destination (1956) Kurt Vonnegut’s The Sirens of Titan (1959) Walter M. Miller, Jr.’s A Canticle for Leibowitz (1959) Robert Heinlein’s Stranger in a Strange Land (1961) Philip K. Dick’s The Man in the High Castle (1962) * NEW WAVE SF: 1964–1973 Philip K. Dick’s Martian Time-Slip (1964). One of my top favorite PKD novels, Martian Time-Slip is set in an arid Martian colony where Establishment-approved information is crammed into youthful heads by teaching machines. Forget the plot, which involves time travel… or a vision/hallucination of time travel, anyway. Dick presents the book’s action through flash-forwards and from the perspectives of the three main characters. At the level of form, we’re confronted with the question: What is reality? Ten-year-old Manfred Steiner, is labeled autistic because he doesn’t properly respond to the machines; in fact, he has precognition abilities. Jack Bohlen, a repairman, is hired to develop a device for communicating with Manfred; Bohlen, too, is disturbed by the teaching machines — because his schizophrenia reveals to him the machine-like quality of normal, well-adjusted people; and because he, like Manfred, perceives the passage of time in an unconventional way. A third character, union leader Arnie Kott, wants to use Manfred’s abilities to get the edge on a business deal. Meanwhile, the oppressed native Martians recognize the malleability of time — and therefore understand the value of Manfred’s gifts. Fun fact: The novel was first published under the title All We Marsmen, serialized in the August, October and December 1963 issues of Worlds of Tomorrow magazine. William Burroughs’s Nova Express (1964). Beginning in 1961, William Burroughs and Brion Gysin experimented with a “dreamachine” producing complex patterns of color behind one’s eyelids. As the user entered something like a hypnagogic state, the patterns would “read” as intensely meaningful — even if that meaning was inarticulable. The effect of Burroughs’s Nova Trilogy (1961’s The Soft Machine, 1962’s The Ticket That Exploded, and Nova Express) on readers is dreamachine-like; you don’t read it so much as soak in it. Even if you could un-do the “fold-in” technique that Burroughs employed, you wouldn’t discover a coherent plot: Instead, there are characters (Sammy The Butcher, The Brown Artist, Izzy The Push, and other members of the viral Nova Mob; Inspector Lee of the Nova Police), comedy bits, drug-induced hallucinations, and language experiments. All wired together by an overarching paranoia regarding the cultural, social, biological, and neurological mechanisms via which the many are conditioned and controlled by the few. The Nova Mob are “control addicts”; can Inspector Lee — who sees conspiracies everywhere — dismantle their diabolical word-and-imagery machine, aka culture itself? Fun fact: Together with The Soft Machine (1961) and The Ticket That Exploded (1962), this novel is part of The Nova Trilogy. Luc Sante sums up the message of the trilogy like so: “You are the host of a virus; the virus is life; you are fucked.” Jack Kirby’s pre-Fourth World sci-fi comics (1964–1970). Kirby’s proto-psychedelic photo collages were first seen in ’64; so we’ll date his pioneering contributions to New Wave science fiction to that year. Of course, Kirby was also a pioneering Golden Age science fiction artist — in the early ’40s he drew The Blue Beetle and Captain America; and he drew Challengers of the Unknown for DC, before co-ushering in (with Stan Lee) Marvel Comics’ Silver Age with, e.g., The Fantastic Four (1961), The Incredible Hulk (1962), Iron Man (1963), and The X-Men (1963). Beginning in 1964, Kirby introduced an ambitious, self-consciously artistic sensibility to his Marvel Comics work; he began to blow readers’ minds through his formal experimentation. Kirby’s proto-psychedelic energy fields, known to fans as the “Kirby Krackle,” which were first seen in ’66, are a signifier of his boundless, cosmic imagination. Kirby would end up writing, in addition to drawing, some terrific Marvel titles before leaving in 1970 for DC — who would publish his era-defining, multiple-series Fourth World epic. A prolific New Wave sci-fi genius! Brian Aldiss’s Greybeard (1964). At age 56, Algy Timberlane — our titular greybeard, is one of the world’s youngest men. At the beginning of this story, he and his wife, Martha, are living in an isolated community, in fairly primitive conditions, somewhere in southern England; a violent incident sends them fleeing along the Thames towards London — which they never reach. Along the way, Algy and Martha and their companions pass through the ruins of twentieth-century civilization, encounter various cults and communes populated by elderly survivors, and struggle — intellectually, emotionally — to comprehend the end of humanity. Via flashbacks, we discover that a nuclear-testing accident, a half century earlier, in 1981, has sterilized most higher mammals on the planet. Stoats have become a menace! Aldiss, who coined the term “cosy catastrophe” to describe post-apocalyptic novels in which the survivors are contented with their lot, because there are abundant resources for the taking, and the mechanized, organized, deodorized modern world has given way to a rural, human-scale one, has his characters debate whether or not they should mourn their fate. Algy broods over his bitter memories of civilization’s rapid decline, after the Accident — martial law, famine and disease, anarchy — but ultimately hopes that humankind will not die off. Fun facts: Michael Moorcock has described Greybeard as one of the first British New Wave sci-fi novels. When P.D. James’s novel The Children of Men was published in 1992, many sci-fi fans noted that the points of similarity between the novels are astonishing. J.G. Ballard‘s The Burning World (1964). A difficult book to read, in many respects — with the saving grace that it is not a Golden Age sci-fi “cozy catastrophe,” i.e., in which the apocalypse proves to be a kind of wish fulfillment for an alienated male protagonist. As the story begins, a British suburb begins to grapple with the fact that an unending drought — brought on by human pollution — will result in rivers running dry, crops failing, and humankind succumbing to famine and disease. Some years later, a small band of survivors from that suburb traverses vast salt plains in search of potable water. As in Beckett’s Endgame, our post-apocalyptic protagonist, the resigned and taciturn Dr. Ransom, and his companions — including a deranged architect who takes to wearing jeweled robes; and a crippled man who walks on stilts — discover that everything they’ve ever believed is meaningless. Fun fact: An expanded version, retitled The Drought, was published in 1965. Ballard’s other early catastrophe novels include The Wind from Nowhere (1961), The Drowned World (1962), and The Crystal World (1966). Frank Herbert’s Dune (1965). In the far future, interstellar travel is made possible thanks to the spice melange — the psychoactive properties of which allow pilots to safely route faster-than-light travel. Melange is also responsible for the witchy powers of the Bene Gesserit, an ancient sisterhood that has carried out a breeding program designed to produce the Kwisatz Haderach (a messiah-like figure). Dune, the first in a series of six best-selling novels, recounts how young Paul Atreides arrives on Arrakis, the only planet where spice is mined, only to see his father — the new governor of the planet — killed and his family’s (awesome) retainers scattered. With the help of his Bene Gesserit mother, not to mention the Fremen, the planet’s giant-worm-riding natives, Paul seeks revenge against the evil Baron Harkonnen… while discovering the truth about the Kwisatz Haderach. Dune is: a potboiler about a family’s declining empire, a fantasy about the founding of a new social order, a band-of-brothers yarn, and a criticism of humankind’s despoliation of nature. Wow! Fun fact: One of science fiction’s all-time best-selling titles; parts of it were first serialized in Analog. Dune was adapted into David Lynch’s cult 1984 movie of the same title. It won the inaugural Nebula Award for Best Novel. Philip K. Dick’s The Three Stigmata of Palmer Eldritch (1965). The Earth is badly over-heated, and the UN — the global governing body — is conscripting settlers to colonize unpleasant nearby planets. Mars’s settlers have become addicted to Can-D, a drug that allows them to escape into a collective Barbie-and-Ken-esque hallucination, the contours of which are shaped by figures and “layouts” they purchase… from Perky Pat, a corporate empire run by the ruthless Leo Bulero (who also secretly manufactures Can-D). Bulero’s hired telepaths discover that merchant adventurer Palmer Eldritch has returned from a crash on Pluto with Chew-Z, a superior drug, one which can put Perky Pat out of business. However, when Bulero attempts to assassinate Eldritch, he is plunged into a nightmarish odyssey of nested hallucations… which causes him to question the very nature of reality itself. What’s up with Eldritch’s three “stigmata” — and where did he get Chew-Z? Plus: double agents, time travel, devolution, alien possession, and Gnostic musings about the notion of an evil demiurge! Fun fact: A freaky classic of psychedelic literature. Considered one of Dick’s most important books. Samuel R. Delany’s Babel-17 (1966). This short, psychedelic, linguistics-inspired space opera describes a distant future in which humanity has spread itself — for better and worse — throughout the galaxy. An alien culture called The Invaders is up to something that could prove potentially catastrophic to the (human) Alliance… but what? Starship captain, codebreaker and telepath Rydra Wong discovers that a software code used by the Invaders’ hackers is actually a language… one which alters perception and thought, enhancing your abilities but turning you into a traitor! Wong is assisted by a kick-ass crew of genetically modified adventurers — including some who are essentially ghosts in the machine. Babel-17 is an adventure yarn — including everything from hand-to-hand combat to full-scale spaceship battles — but at the same time it’s a philosophical novel challenging the reader to imagine what kind of culture might speak a language lacking a pronoun for “I.” Fun fact: Babel-17 was joint winner of the Nebula Award in 1966 — along with Flowers for Algernon. Philip K. Dick’s The Unteleported Man (1966). War between the US and the Soviet Union has led to UN rule of the planet, renamed Terra. Theodoric Ferry, a capitalist mogul, is teleporting millions to Whale’s Mouth, the universe’s only other inhabitable planet, a Garden of Eden where Terrans can start over. Freya Holm, an agent with the private police agency Listening Instructional Educational Services (LIES), Inc., speculates that Ferry may be an alien… and that Whale’s Mouth may not be all that it seems. (See: Edgar Rice Burroughs’s The Gods of Mars.) Rachmael Ben Applebaum, owner of an outer-space freighter company that has been disintermediated by teleportation technology, decides to travel to Whale’s Mouth the old-fashioned way… i.e., he will be the only unteleported man. The UN, meanwhile, attempts to defeat Ferry via a mind-control device of their own: a pulp sci-fi novel! Fun fact: Originally published as a novella, in 1964, by Fantastic. I’ve written more about this novel in my essay “The Black Iron Prison” (n+1, July 2004). J.G. Ballard’s The Crystal World (1966). Some readers find this novel too “action-adventurey” for their liking, others find it too surgical and psychological; I think it strikes a provocative balance between these tendencies. In an African colony, Sanders, a British doctor, discovers that entrance to the forest is being discouraged. Seeking his friends, who run a leper colony (to which he is strangely attracted), he travels upriver — echoes of Heart of Darkness are intentional — and discovers that trees, grass, water, animals and men are slowly being encased in glittering crystals. The universe, its myriad of possibilities, is crystallizing into sameness! Which, in a way, is just a literalization of a process already underway — the separation of alienated individuals from one another, industrial capitalism forcing everything and everyone to become the same. If leprosy is about entropy and decay, this crystallization is a kind of antidote… right? Ballard’s descriptions are eerily beautiful. Fun fact: Serialized in the first Moorcock-edited issue of New Worlds. This is Ballard’s third psychedelic-apocalyptic work, the first two being The Drowned World (1962) and The Burning World (1964). Philip K. Dick’s Now Wait for Last Year (1966). In the near future, Terra — a unified Earth, the elected dictator of which is UN Secretary General Gino “the Mole” Molinari — has become entangled in a war between an insect race (the reegs) and a humanoid race (the ’Starmen, from the planet Lilistar). Terra is on the wrong side of the war; their allies, the fascistic ’Starmen, may be out to exploit Terra’s natural resources. Dr. Eric Sweetscent, an organ-transplant surgeon asked to secretly tend to Molanari, who has developed a psychosomatic ailment in which he suffers along with anyone near who him who is in any kind of pain, gets involved in Terra-Lilistar politics. His wife Kathy, meanwhile, becomes addicted to JJ-180, a new hallucinogen (which may have been invented by the reegs as a chemical weapon) that causes her to move forwards, backwards, and sideways through time… and she is forced to spy on Sweetscent — by the ’Starmen. Sweetscent and Molinari time-travel, as well… leading them to wonder how valuable the intel they’re picking up from alternative past and present histories is for their current situation. Fun fact: Dick was very fond of his Molinari character, whom he described as a blend of Christ, Lincoln, and… Mussolini, for whom he harbored a certain (non-fascist) sympathy. Ursula K. Le Guin’s Rocannon’s World (1966). When ethnologist Gaveral Rocannon visits the primitive planet Fomalhaut II, his ship is destroyed by agents of Faraday, an upstart planet threatening the peaceful galaxy. Rocannon sets out to find the enemy’s secret base on the planet — so he can infiltrate it, and use their “ansible” to communicate with galactic authorities. As he journeys across the planet, he encounters various Tolkien-esque species, including the dwarfish Gdemiar, the elven Fiia, and the nightmarish Winged Ones; his advanced technology makes him a Connecticut Yankee in King Arthur’s Court-type wizard. As he travels, and engages in various battles, Rocannon becomes a figure of legend. However, when he reaches the enemy base he must revert to a sophisticated interstellar op. Fans of Iain M. Banks: the Culture begins here. A fun foray into Three Hearts and Three Lions-esque science-fantasy, for Le Guin. Fun fact: This is the debut installment in Le Guin’s Hainish Cycle. Other authors, including Orson Scott Card, Vernor Vinge, and Kim Stanley Robinson, would borrow the “ansible” tech from this book. Samuel R. Delany’s The Einstein Intersection (1967). Love it or hate it (Delany’s eighth novel has zealous fans and detractors), The Einstein Intersection is fascinating. Forty thousand years in the future, Lobey, a village herder and musician, goes on an Orpheus-like quest into the underworld — in search of his slain lover, Friza. As it turns out, this is a puppet-show of sorts: Lobey is a member of a (three-gendered) alien race who’ve taken on (two-gendered) human forms, and inherited human cultural myths as well. Of the latter, the aliens have made a hodge-podge: the stories of Orpheus, the Minotaur, Billy the Kid, Jean Harlow, Ringo Starr, Jesus Christ — these and other traditions of all dead generations weigh like a nightmare on the brains of the living. This conceit alone might have made for a terrific mythopoetic sci-fi novel; however, Delaney introduces myriad other issues: genetics, radiation, identity and difference, rural and urban ways of life, perception and reality, life and death, dragons… too much, perhaps, for a relatively short novel. Delany’s prose style, too, confounds: sometimes improvisational and snappy, sometimes ham-fisted pulp fiction. The Einstein Intersection is pretentious — but in the best possible way. Don’t give up on it! Once you encounter Kid Death, you’ll be hooked. Fun fact: Winner of the 1967 Nebula Award for Best Science Fiction Book. Roger Zelazny’s Lord of Light (1967). On a planet colonized long ago by the South Asian crew and passengers of the spaceship Star of India (hailing from “vanished Urath”), some of the humans artificially evolve themselves into immortal, godlike beings — who conquer the planet’s indigenous races (characterized as “demons”) and force the descendants of the un-evolved crew and colonists into a Hindu-like caste system. All of this occurs over a vast span of time; the book is epic in scope — in fact, two of the chapters were first published as stand-alone novellas in the Magazine of Fantasy and Science Fiction. Eventually, the crew members assume the powers and names of Hindu deities; their main concern is preventing enlightenment, and scientific or technological advancement among their human subjects. However, one of the crewman rejects godhood, and — again, over time — introduces Buddhism to the masses as a liberatory wake-up call. Fun fact: Winner of the 1968 Hugo Award for Best Novel. Gordon Dahlquist describes Lord of Light as “dedicated to dragging all wizards out from behind their curtains.” Harlan Ellison’s (ed.) Dangerous Visions (1967). This is not a novel, but a collection of original science fiction stories by 30+ contributors. I include it on this list because it was influential on the genre’s New Wave movement; however, by the time I read it, in the mid-1980s, what was most shocking about these stories was the sexism and racism. Still, Philip K. Dick’s “Faith of Our Fathers” is a fascinating mashup about Chinese communism, psychedelics, and the truth of religion; Robert Silverberg’s graphic “Flies,” in which aliens experiment on a spaceman in order to learn about what makes humanity tick — and get it wrong, is a fun thriller; Samuel R. Delany’s “Aye, and Gomorrah…” conjures up a new sexual perversion involving a neutered astronaut; and Ellison’s own “The Prowler in the City at the Edge of the World” is something of a tour de force — about Jack the Ripper’s disappointment when he escapes to the 31st century. Fritz Leiber’s “Gonna Roll the Bones” is a chilling, funny fable… but Leiber wasn’t a New Wave writer, nor were some of the collection’s other contributors (Pohl, Anderson). I should also mention John T. Sladek’s “The Happy Breed,” which presciently describes our emotional dependence on apps. Fun facts: “Gonna Roll the Bones” won both a Hugo Award and a Nebula Award for Best Novelette; and “Aye, and Gomorrah…” won the Nebula for Best Short Story. Ellison published a sequel, Again, Dangerous Visions, in 1972; a third, as yet unpublished sequel, is now infamous. Anna Kavan’s Ice (1967). Brian Aldiss writes, of Kafka’s oeuvre: “the baffling atmosphere, the paranoid complexities, the alien motives of others, make the novels a sort of haute sf.” Anna Kavan’s cult classic, Ice, closes the gap in that equation. As a new Ice Age dawns (sparked by a nuclear holocaust?), western civilization finds itself hemmed in by advancing ice-scapes. War and revolution break out everywhere. Against this apocalyptic backdrop, an unnamed narrator — a globe-trotting, Indiana Jones-esque anthropologist-explorer-soldier, as far as we can make out — pursues an unnamed woman with whom he has long been obsessed. He intends to rescue her, first from her brutal husband, then from a Ruritanian-ish despot who is on his way to becoming one of the world’s new tyrants; however, the “girl” doesn’t want to be rescued — and seems terrified of the narrator. Plot possibilities unfurl, only to furl back up again; is the narrator insane? A hallucinatory, image-rich adventure that doesn’t omit guns and car chases. Fun fact: “The book’s nearest cousins,” writes Jonathan Lethem in his introduction to the Penguin Classics reissue of Ice, “are Crash, Ballard’s most narratively discontinuous and imagistic book, or cinematic contemporaries like Alain Resnais’s Last Year at Marienbad.” Chester Anderson’s The Butterfly Kid (1967). In this metafictional cult classic, set in the near future, not long after the Bicentennial, when video phones and personal hovercraft are common, a ragtag group of Greenwich Village hippies discover that their acid trips are beginning to come true. In fact, everyone in New York is hallucinating, and all of their hallucinations are made manifest — it’s chaos! Chester Anderson, who shares the author’s name, and Michael Kurland, who shares the name of another hippie sci-fi author, discover that New York’s water supply has been laced with a drug — brought to Earth by giant blue lobster-esque aliens — designed to make Earthlings easy to conquer. Can these drug-addled pacifists thwart the alien invasion? Fun fact: The blog io9.com listed The Butterfly Kid as No. 1 on the list of “weirdest science fiction novels that you’ve never read.” Its sequels are The Unicorn Girl, by Michael Kurland, and The Probability Pad, by T.A. Waters. Anderson later moved to San Francisco, invested his royalties from this novel in a mimeograph machine, and founded The Diggers’ publishing outfit, Communications Company. Samuel R. Delany’s Nova (1968). In the year 3172, interstellar human society is divided into three constellations — each of which was originally colonized by different Earth socio-economic classes. Draco, which includes Earth and other wealthy planets, is an aristocratic constellation ruled by the (caucasian) Red family, whose Red-Shift Limited is the sole manufacturer of faster-than-light drives; the Pleiades Federation, a middle-class constellation, is the home of operations for the rival (mixed-race) Von Rays. The Outer Colonies, settled by working-class Earthlings, are the source of the important energy source illyrion, a superheavy element essential to starship travel and terraforming planets. Our protagonist is Lorq Von Ray, a playboy who — years earlier — was attacked and scarred by Prince Red. Now a nihilistic, revenge-obsessed adventurer, Lorq puts together an Argonauts-inflected squad of hippie-ish misfits — the Mouse, Lynceos, Idas, Tyÿ, Sebastian, Katin — and takes them on a demented voyage to the heart of an imploding star… in order to capture an enormous amount of illyrion, and in so doing destroy Draco’s control of the Outer Colonies. Though the plot is only intermittently thrilling (in a space-opera way), the language is gorgeous, the meta-textual references (to Moby Dick, Arthurian mythos, and more) are pretty fun, and there’s a whole Tarot-really-works conceit that’s almost persuasive. If Delany weren’t an experimentalist, this could have been a Dune; I’m glad it isn’t. Fun facts: There’s a cyberpunk tech aspect to the book that I can’t get into, here; William Gibson’s Neuromancer alludes to Nova. After this book, Delany didn’t publish again until Dhalgren appeared in 1975. John Brunner’s Stand on Zanzibar (1968). Borrowing the kaleidoscopic narrative technique of John Dos Passos’ U.S.A., Brunner paints a comprehensive portrait of the over-populated America of 2010. There is a central story line, with recurring characters (including Shalmaneser, a super-computer). For example, Norman Niblock House, an African-American VP of General Technics, is negotiating for his company to assume management of an African country; his roommate is a spy. Meanwhile, a Southeast Asian country has achieved a breakthrough in genetic engineering. We briefly meet many other characters, via fragmented, information-rich chapters devoted solely to world-building. Political slogans, advertising, song lyrics, journalism, and slang (recorded in a glossary titled The Hipcrime Vocab, by sociologist Chad C. Mulligan), help us experience the social, economic, and cultural consequences of unchecked population growth. Social programming, interactive TV, genetically modified microorganisms… many of Brunner’s predictions are disturbingly prescient. Fun fact: Winner of the 1969 Hugo Award for Best Novel. In 1968, the prolific Brunner also published Bedlam Planet, Catch a Falling Star, Father of Lies, and Into the Slave Nebula, as well as a story collection. Philip K. Dick’s Do Androids Dream of Electric Sheep? (1968). In a post-apocalyptic San Francisco, bounty hunter Rick Deckard is charged with “retiring” six escaped androids — one of whom, named Pris, moved into a derelict apartment building inhabited only by John Isidore, an intellectually challenged man who attempts to befriend her. Although there are plenty of thrills and chills here (for example, Deckard is seduced by an android whose mission it is to make it impossible for him to kill Pris), this is as much a philosophical novel about empathy as it as an adventure. The androids have no emotions — the only way that Deckard can tell them apart from humans is by giving them empathy tests. The androids, meanwhile, are on a mission to disprove a popular pseudo-religion called “Mercerism,” in which grasping the handles of an electronic Empathy Box allows you to “encompass every other living thing.” (“Mercerism is a swindle,” the androids insist. “The whole experience of empathy is a swindle.”) Deckard must prove them wrong… though he begins to wonder whether he, too, is an android. Fun facts: The theatrical-release version of Blade Runner, Ridley Scott’s souped-up adaptation of Dick’s novel, doesn’t lead viewers to question Deckard’s humanity (or does it). And the 2017 sequel, Blade Runner 2049, further muddies the waters. Richard Brautigan’s In Watermelon Sugar (1968). Our unnamed narrator, a writer, lives outside an unnamed town, on the fringes of a commune called iDEATH. The commune features a trout hatchery (which produces oil used to light their lamps) and a watermelon works (which produces multicolored sugars used to fashion every sort of commodity), not to mention huge statues of vegetables, and shacks to which those who want to spend time alone can retreat. The setting is idyllic, but also somehow post-apocalyptic. The sun shines a different color each day; reference is made to talking, singing, yet violent “tigers” who used to inhabit the area; and there is a vast junkyard — the Forgotten Works — of objects manufactured before… whatever happened. There isn’t much of a plot: a drunkard named inBOIL leads a short-lived rebellion against iDEATH (Is he right about everything?, this reader wonders); the narrator falls in love with Pauline, the commune’s cook, which may or may not cause his former lover, Margaret, to go off the rails. The mood is elegiac, light-hearted, sad, and critical all at the same time. Fun facts: In Watermelon Sugar is an important reference in Ray Mungo’s 1970 back-to-the-land chronicle Total Loss Farm; it’s also the inspiration for Neko Case’s 2006 song “Margaret versus Pauline.” Thomas M. Disch’s Camp Concentration (1968). In an authoritarian near-future America, President Robert McNamara — Secretary of Defense, at the time Disch was writing — has embroiled the country in an illegal war… against the world. We are reading the diary of an imprisoned conscientious objector, the poet Louis Sacchetti, who has been sent to Camp Archimedes, the inmates of which are dosed (unwittingly) with a strain of syphilis as part of a military experiment. (With Ignatius J. Reilly, Sacchetti is one of the great obese fictional characters.) The treatment increases the patients’ intelligence, while shortening their lives. Does God exist? Does alchemy work? Do we humans create Hell for ourselves? If genius is a matter of breaking down the mind’s rigid categories, then are all geniuses insane? Sacchetti, an erudite wordsmith and deep thinker, has much to say on these and other topics… particularly as his own mind’s rigid categories begin to break down. Fun fact: Serialized in New Worlds in 1967. In 1972, Philip K. Dick wrote a paranoid letter to the FBI suggesting that there were coded messages in Camp Concentration. Anne McCaffrey’s The Ship Who Sang (1969). Before Iain M. Banks’s (or Becky Chambers’ or Ann Leckie’s) independent AI starships there was The Ship Who Sang. Our protagonist is Helva, who was born with an exceptional brain and severe physical disabilities… so she was raised as an indentured servant destined to be a starship brain. One with a lamentable tendency to fall in love with her human co-pilot (known as the “brawn” to her “brain”) as they travel the galaxy on various missions of mercy. What happens if the brawn loves her back… and wants to have sex with her? Helva’s feelings of love and loss are poignant; however, the whole set-up is also a bit creepy and offensive! If you think about the sex scenes in McCaffrey’s Dragonriders of Pern series, you’ll begin to see why. Fun fact: The book’s first five chapters were originally published as “The Ship Who Sang” (1961; McCaffrey’s own favorite story), “The Ship Who Mourned” (1966), “The Ship Who Killed” (1966), “Dramatic Mission” (1969), and “The Ship Who Disappeared” (1969); the sixth chapter is original to the novel. In the 1990s, McCaffrey and co-authors produced six sequels. Vladimir Nabokov’s Ada, or Ardor (1969). When he is fourteen, Van Veen, who will grow up to be a psychologist and renegade scholar, falls in love with his eleven-year-old cousin, Ada; they begin a life-long sexual affair, despite later discovering that they are half-siblings. The story begins in the early 19th century, though characters discuss airplanes, motion pictures, and other anachronistic technologies; everything is powered by water, and it is forbidden to mention electricity. Reference is made to an historical catastrophe referred to as “the L disaster,” which has somehow “everted” (I borrow the term from a 1975 essay about this novel in Science Fiction Studies) time, earth, and sexual gender. Van and Ada — who are maybe somehow, respectively, Eve and Adam — live on a planet known as Antiterra, which is geographically similar to Earth, although politically England has conquered most of it, and American culture is influenced by Russia. Nineteen-Sixties culture is, somehow, a myth from the past. Trippy! Fun facts: Ted Gioia has compared Ada to Philip K. Dick’s The Man in the High Castle and other alternate-history works of science fiction. I might include works by Samuel R. Delany and Michael Moorcock in which the Beatles become mythical figures. Philip K. Dick’s Ubik (1969). Joe Chip works for Runciter Associates, which employs “inertials” — telepaths and precogs with the ability to block the powers of other, less scrupulous telepaths and precogs — to protect the privacy of their clients. He’s one of Dick’s “minor men,” unable to manage his own life; in fact, he owes money to his own front door! Joe has a thing for his new colleague, Pat, who can change the past in such a way that people don’t realize it. Sent to Luna in search of criminal telepaths, Joe and Pat and the rest of their team is caught in an explosion… after which nothing is ever the same again. Are they moving backwards in time? Are they in some other reality? Are they caught up in a cosmic battle between the forces of light and the forces of darkness — and if so, what is the ultimate source of these forces? Every interpretation that they posit is frustrated; meaning remains elusive. Each chapter is prefaced with an advertisement for Ubik, salvation in a spray can. This is, perhaps, the ultimate example of one of Dick’s apophenic sci-fi potboilers. Fun fact: Ubik inspired France’s Alfred Jarry-inspired Collège du Pataphysique to elect Dick as an honorary member. John Lennon, at one point, was interested in adapting the film version. Kurt Vonnegut’s Slaughterhouse-Five, or The Children’s Crusade: A Duty-Dance with Death (1969). Much like the bumbling protagonist of Jaroslav Hašek’s pioneering antiwar novel The Good Soldier Švejk (1921–1923), Billy Pilgrim is an ill-trained, disoriented, cowardly chaplain’s assistant. During the Battle of the Bulge in 1944, he is captured and transported to Dresden. In 1945, as British and American bombers dropped several thousand tons of high-explosive bombs and incendiary devices on the city, Pilgrim and his fellow prisoners and their guards take refuge in a cellar beneath Schlachthof-fünf, the titular “slaughterhouse five”; they are among the only survivors of the (still-controversial) attack. We experience all of this in flash-backs or flash-forwards, because Billy has become “unstuck in time,” not to mention in space. At one point, years later, on his daughter’s wedding night, Billy is captured by aliens and transported to Tralfamadore, the fatalistic residents of which can observe all points in the space-time continuum simultaneously. Like them, Billy becomes a philosophical ironist because — thanks to his time-traveling — the entire human experience strikes him as absurd. Is he crazy, or a visionary? Fun facts: As a prisoner of war in 1945, Vonnegut experienced the Dresden firebombing; the narrator of Slaughterhouse-Five is the author, speaking in his own voice. The 1972 film adaptation, directed by George Roy Hill (in between directing Butch Cassidy and the Sundance Kid and The Sting), won the Prix du Jury at the 1972 Cannes Film Festival. Ursula K. Le Guin’s The Left Hand of Darkness (1969). This is the second of the author’s so-called Hainish Cycle, set in a galaxy whose human population evolved on Hain, then spread outwards to many other planets (including Earth) before, at some distant point in the past, losing contact. Efforts have been mounted to re-establish a galactic civilization; some eighty planets have organized themselves into a union called the Ekumen. In this novel, Genly Ai, an agent of The Ekumen, has spent a frustrating couple of years as an envoy to the frozen planet Gethen. Ai’s efforts to recruit Gethen into the Ekumen have failed… because his supposedly enlightened worldview is structured by binary oppositions. Gethenians, because they are ambisexual — they only adopt sexual attributes once a month, during a period of sexual receptiveness and high fertility — see the world in an entirely different way. Ai only begins to empathize with the Gethenian worldview once he escapes from prison with Estraven, an exiled Gethenian politician, who not only helps Ai survive a trek across the planet’s wintry wilderness, but helps him understand his own deep-seated prejudices and assumptions about gender and gender roles. Fun facts: The Left Hand of Darkness is one of the first feminist sci-fi novels, though some feminists have argued that it does not go far enough in critiquing gender stereotypes. Harold Bloom said, of this book, which won both the Hugo and Nebula Awards: “Le Guin, more than Tolkien, has raised fantasy into high literature, for our time.” Jane Gaskell’s A Sweet, Sweet Summer (1969). Enormous alien spacecrafts are hovering over London, Birmingham, and Manchester, sealing Britain off from the outside world; one of the aliens’ first demonstrations of power was the public execution of Ringo Starr. It’s a weird scene: “People rush under [the alien craft] when the rain starts so municipal authorities have erected seats and slot-machine arcades under them and charge you for using them.” The country descends into anarchy, as communist and fascist militias battle in the streets, and hoodlums terrorize the defenseless. Our narrator, Rat, is an unpleasant, misogynistic character who runs a tiny London boarding house and brothel. The alien invasion is the best thing that ever happened to him, so when his charismatic, gender-ambiguous, proto-punk cousin, Frijja, shows up and attempts to free London from alien oppression, Rat does what he can to thwart her — while also strugling to defend his turf from a marauding gang… with whose thuggish leader he is disturbingly fascinated. Fun fact: An exceedingly difficult book to find! China Miéville says that A Sweet, Sweet Summer “perfectly combines psychological perspicacity and social critique in an unusual dystopian future London.” Gaskell also wrote a seminal YA vampire novel: 1964’s freaky The Shiny Narrow Grin. Brian Aldiss’s Barefoot in the Head (1969). A far-out, experimentalist novel — many readers find it too much so — set in a future Europe devastated by Acid Warfare. Muslims, it seems, have released “psycho-chemical aerosols” into the environment, and now tens of thousands of Europeans (except the neutral French) are tripping constantly, veering between extreme joy and abject terror… and talking in gorgeous, Finnegans Wake-esque word salads (see also: Disch’s Camp Concentration). Social norms have collapsed. Colin Charteris, a young Serbian who has been working in UN refugee camps in Italy, travels to England… where he falls under the influence of the hallucinogenics and finds himself hailed as a prophet by the pharmaceuticalized populace. Preaching a trippy Gurdjieffian gospel, Charteris could usher in a utopian social order… or perhaps his movement will help European civilization utterly devolve. Peppered with poems and song lyrics from the characters, it’s reminiscent of the excellent 1968 youthsploitation movie Wild in the Streets, as well as Richard Fariña’s Been Down So Long It Looks Like Up to Me (1966), though more world-historical than these. Fun facts: The novel was assembled from Aldiss’s stories in New Worlds and elsewhere. One of the inside jokes, here, is running meta-commentary on the works of Aldiss’s fellow sci-fi writers. Josephine Saxton’s The Hieros Gamos of Sam and An Smith (1969). In this experimentalist, poetic work, a 14-year-old boy rescues a baby girl when her mother dies in childbirth. He raises her in a world devoid of other humans. What has happened? Buildings remain, food dispensers dispense food, store shelves are replete with supplies… but Sam and An are, more or less, this Eden-like post-apocalyptic space’s only inhabitants. (The few individuals who make an appearance inhabit the outskirts of town, and they are phantom-like figures — are they hallucinations? Are Sam and An being studied by them? Who is leaving them messages — and what do the messages mean?) This is, in some respects, a Bildungsroman; we watch Sam mature, as he cares for his charge, and we’re interested to discover what books — Nietzsche, Jung, Blake, science fiction — he reads. As An grows older, sexuality introduces itself to this strange idyll. Anachronistically, it could be described as The Truman Show and Lost mashed up with Blue Lagoon. Fun fact: The first novel by Saxton, who is also remembered for Vector for Seven: The Weltanschaung of Mrs Amelia Mortimer and Friends (1970), Group Feast (1971), and the super-unsettling Queen of the States (1986). Philip K. Dick’s A Maze of Death (1970). “My books (& stories) are intellectual (conceptual) mazes,” Dick would later reflect in his Exegesis, “& I am in an intellectual maze in trying to figure out our situation (who we are & how we look into the world, & world as illusion, etc.) because the situation is a maze.” This proto-gnostic apophenic thriller, written in 1968 and inspired in part by the author’s only LSD trip, is perhaps Dick’s maze-iest. One by one, fourteen human colonists, none of whom understands their collective mission, are transferred to the planet Delmak-O — which is populated by gelatinous cubes who offer advice in the form of I Ching-like anagrams. A naturalist, a linguist, a geologist, a theologian, a physician, a pyschologist, and so forth: They are each eccentric and disgruntled, particularly once they begin to die off. What is the factory-like building towards which they are drawn? Are they trapped in a maze — being observed and experimented upon? Or are they perhaps dead — or in some kind of limbo state? Lost fans, read this one. Fun fact: Except for Ubik, this is the book Dick most frequently references in his Exegesis. John Sladek’s The Müller-Fokker Effect (1970). In the near future Bob Shairp, writer and dreamer and government worker, agrees to be a guinea-pig in a military experiment — to determine whether a human being can be reconstituted like orange juice. However, as his persona is being uploaded to computer tapes in the form of data, his body is accidentally destroyed. Shairp’s persona-data then becomes a computer virus, which leads to a series of absurd, paranoia-inducing scenarios. Sladek’s novel satirizes right-wing military, evangelical, militantly anticommunist forces in late-Sixties America — where Ronald Reagan, of all people, is president! — who seek to control the tapes. The book, which doesn’t have much of a plot, abounds with racists, conspiracy theorists, and eccentric millionaires, so yeah… it all too accurately predicts today’s America. Fun fact: The book’s title is an obscene pun. When asked, in 1982, whether he’d considered what trouble he caused young people asking librarians for the book, Sladek replied, tongue-in-cheek: “Young persons have no business reading such a book, which contains sex, violence and anagrams.” J.G. Ballard’s The Atrocity Exhibition (written 1967, published 1970). Less a novel than a collection of linked stories or novellas, The Atrocity Exhibition confronts us with surreal fantasies, absurdities, and grotesqueries — “Plans for the Assassination of Jacqueline Kennedy,” “Love and Napalm: Export USA,” “Why I Want to Fuck Ronald Reagan” — recounted by an unstable narrator, a mental-hospital psychiatrist whose name keeps changing (Talbert, Traven, Travis, Talbot, etc.). Like the French philosopher and theorist Jean Baudrillard, who started publishing in the late Sixties, and who can himself be considered a New Wave sci-fi author manqué, in The Atrocity Exhibition we find Ballard writing proleptically. That is to say, the book represents future social and cultural developments — for instance, the death of affect (because of prolonged exposure to sex and violence via pop culture and advertising); the triumph of kitsch culture; the banalization of celebrity; disaster porn; endless movies about the Vietnam War — as if they’ve happening now (in the late Sixties). The protagonist’s ultimate goal? To start a World War III… of the mind! Fun fact: The pieces collected here had appeared elsewhere, in various forms, previously. William Burroughs, a writer whom Ballard admired and emulated, wrote the book’s introduction. The first US edition was published in 1972 by Grove Press, after an earlier edition was cancelled because the publisher feared lawsuits. The book inspired the Joy Division song of the same name from their 1980 album Closer. Stanislaw Lem’s’s Ze wspomnien Ijona Tichego Kongres futurologiczny (The Futurological Congress, 1971). A tall tale in which the space explorer Ijon Tichy, whose previous exploits Lem chronicled in The Star Diaries (1957), attends a shambolic World Futurological Congress held at an absurdly luxurious hotel in San Jose, Costa Rica. As riots break out in the streets, the government introduces psychoactive drugs into the drinking water; Tichy escapes to the sewers beneath the hotel, only to be evacuated several times — each of which turns out to be a hallucination — and then shot, and placed by doctors into a cryogenic coma. He wakes up in a transformed world; Lem is affectionately parodying H.G. Wells’s 1899 technocratic utopian novel When the Sleeper Wakes, here. Tichy is introduced to this brave new world — in which most people take a drug that instills a strong work-ethic, not to mention drugs that mask the true nature of reality; and the inhabitants of which speak a language he can’t understand — in stages. Has the world become an overpopulated hellscape threatened by a new Ice Age? Or is this, too, all one of Tichy’s hallucinations? Fun facts: First published along with a collection of short stories (shown above). Ari Folman’s 2013 live-action/animated movie The Congress, starring Robin Wright, was a pretty great adaptation. Ursula K. LeGuin’s The Lathe of Heaven (1971). Operating under the influence of Philip K. Dick, LeGuin wrote an uncanny, thought-provoking novella about George Orr, a Portland, Oregon man who has begun self-medicating in an attempt to prevent himself from dreaming. Why? Because some of his dreams have been altering reality — and George is the only one who notices. (For everyone else, things have always been the way they are now.) Visiting the well-meaning psychologist and sleep researcher Dr. Huber, George is persuaded to embark on a program of “effective dreaming” aimed at improving the state of the world. Unforeseen consequences ensue. (This will not surprise fans of LeGuin’s fantasy and science fiction, which stresses the ambiguity of every utopian ideal, and the dark forces at work within even the noblest soul.) For example, in an effort to dream about peace on Earth, Orr conjures up a fleet of invading alien spacecraft… which does unite humankind, but at what cost? Also — does the “real world” exist at all, or did Orr dream it up after a 1998 nuclear war? Fun fact: First serialized in Amazing Science Fiction Stories, March 1971 and May 1971. The book has sci-fi elements — it’s set in 2002, Dr. Huber employs a device called the Augmentor — but it’s fantastical. The 1980 PBS production of The Lathe of Heaven was well-regarded; LeGuin was closely involved. Walker Percy’s Love in the Ruins (1971). The protagonist of this proto-postmodernist philosophical novel, Dr. Tom More, a hard-drinking psychiatrist in the affluent town of Paradise, Louisiana, has diagnosed his fellow Americans with the malady of “angelism/bestialism” — an extremist tendency towards either spirit-like abstraction or animal appetite, brought on by contemporary America’s sociocultural placidity and flacidity. In the near future of the 1990s, politics have become fragmented to the point of neo-tribalism, mainline churches have become secularized to the point of banality or else overly dogmatic, and liberals and conservatives alike are prone to shocking acts of (what they imagine to be justified) violence. In an effort to restore a sense of moderation, More invents the Ontological Lapsometer, a handheld device that can not only diagnose precisely how spiritually screwed-up you are… but also, with the twist of a dial, treat you for it. Meanwhile, African Americans stage an armed uprising, and college-educated young whites gather in swamp communes. When chaos engulfs Paradise, More retreats to an abandoned motel… with three beautiful women. Fun fact: “Beware Episcopal women who take up with Ayn Rand and the Buddha. A certain type of Episcopal girl has a weakness that comes on them just past youth…. They fall prey to Gnostic pride, commence buying antiques, and develop a yearning for esoteric doctrine.” Jack Kirby‘s Fourth World comics (1971–1974). When Jack Kirby left Marvel Comics for DC in 1970, he launched a science-fictional epic revolving around aliens with superhuman abilities arriving on Earth. Hailing from the planets Apokolips and New Genesis, the ontogeny of the so-called New Gods — their fantastic powers, even their names — recapitulated Kirby’s imaginative billion-year phylogeny, during which three previous eras (“worlds”) had seen the rise and fall of the Old Gods, legends of whom live on in humankind’s mythologies. So sweeping was Kirby’s weltanschauung that it couldn’t be contained in the New Gods comic (#1–11 written and illustrated by Kirby, 1971–72). So he also wrote and illustrated Forever People (#1–11, 1971–72) and Mister Miracle (#1–18, 1971–74), not to mention a reimagined Superman’s Pal Jimmy Olsen. These proto-postmodernist comics are a volatile admixture of religion (the character Izaya evokes the biblical Isaiah), ancient-astronaut theories, sci-fi technology (the Boom Tube, the Mobius Chair, the Mother Box), and 1960s culture (the Forever People are cosmic hippies). Kirby’s 1940s-era teen characters, the Newsboy Legion, were resurrected; and Don Rickles made a cameo appearance. Truly awesome. Fun fact: The Fourth World storyline was intended to be a finite series, which would end with the deaths of the characters Darkseid and Orion. M. John Harrison’s The Pastel City (1971). In the distant future, a medieval-style way of life has risen from the ashes of civilization. There is a barbarian Queen of the North; and, ruling over Viriconium, the ever-changing Pastel City, a beautiful Queen of the South. Scavengers scour the ruins for power blades, energy cannons, and airboats. When news comes that the North plans to deploy scavenged alien automata against the South, a brooding poet-warrior, Lord tergeus-Cormis, travels with a mercenary, Birkin Grif, in search of a mad dwarf who is expert in ancient weaponry. The adventurers encounter mechanical birds, brain eaters, and a wizard of sorts; and they discover that a complex, lethal technology from the past lives on. This is an affectionate, but also sardonic reimagining of the fantasy genre — nothing is resolved, things get murkier instead of more clear, heroes are unheroic. Fun fact: Harrison’s Viriconium series — it includes A Storm of Wings (1980), In Viriconium (also known as The Floating Gods, 1982), and the story collection Viriconium Nights (1985) — has been aptly described as “fantasy without the magic and science fiction without the ‘future’.” Arkady and Boris Strugatsky’s Пикник на обочине (1972; Piknik na obochine; translated as A Roadside Picnic). Near the (fictitious) Canadian town of Harmont, a five-square-kilometer “Zone” has been off-limits ever since unexplained phenomena occurred there some years earlier. After the incidents, universally assumed to have been an alien visitation, bizarre artifacts have been discovered. The novel’s title comes from an analogy proposed, by one of the characters, that the extraterrestrials may have just been on a “picnic,” and left trash behind. The United Nations has attempted to keep the Zone sealed off, but Red Schuhart and other “stalkers” sneak in to steal whatever they can find, for profit… despite the risk that they may pass mutated genetic material on to their children. Red smuggles “hell slime” out of the Zone, and sells it to arms dealers, because he needs money to care for his daughter, “Monkey,” a devolved humanoid. Once he gets out of prison, Red discovers that the bodies of those buried within the Zone — including his father’s — have become reanimated! So he embarks on one last mission, a quixotic effort to make everything come out right. Fun facts: Roadside Picnic was refused publication in book form in the Soviet Union for eight years due to government censorship. The 1979 Soviet sci-fi art film Сталкер (Stalker), directed by Andrei Tarkovsky, is loosely based on the novella; the screenplay was written by Arkady and Boris Strugatsky. Gene Wolfe’s The Fifth Head of Cerberus (1972). Three subtly interlinked stories set on Ste. Anne and Ste. Croix — twin Earth-colony planets circling one another. The first tale is a memoir narrated unreliably by “Number 5,” the son of an insane genius, who recounts how he and his brother were raised in a brothel by a robot teacher, and how he was eventually forced to reckon with his own identity — and challenge his father. The second story, written by a visiting anthropologist from Earth studying the planets’ supposedly extinct race of shapeshifters, is narrated in the style of an aboriginal folktale about estranged brothers; the protagonist embarks on a quest, traveling between real world and dreamworld… though the reader can’t always tell them apart. The third story also concerns the anthropologist, who runs afoul of local authorities and is imprisoned for years — we learn the bizarre details through snippets from his increasingly unhinged (or truthful) journals — and from interrogation tapes being revised by a bored police agent. Fun fact: The first story, “The Fifth Head of Cerberus,” was included in the 1973 anthology Nebula Award Stories Eight. Though not his first novel, Wolfe considered it his first good one. John Brunner’s The Sheep Look Up (1972). Brunner wrote a lot of forgettable pot-boilers, and a couple of terrific books — this proto-cyberpunk eco-catastrophe is one of the latter. Raw materials are running out, and insects and micro-organisms have become resistant to efforts to eradicate them. Disaster could be averted if world governments and the wealthy were willing to make sacrifices; instead, the rich live obliviously in gated communities while the right-wing US administration, headed by an idiot president, is in thrall to corporations seeking only to maximize shareholder value. The media, meanwhile, focuses on entertainment and delivers fake news. Environmental and social-justice activists are dismissed as un-American hippies. (Yes, it’s almost too prescient.) We learn all of this through fractured vignettes about multiple characters, headlines, reports. As both government and corporate services break down, and as food is poisoned, rioting and civil unrest sweep the United States. Fun fact: The novel’s title is a quotation from Milton’s “Lycidas.” “The hungry sheep look up, and are not fed,/But swollen with wind and the rank mist they draw,/Rot inwardly, and foul contagion spread…” Michael Moorcock’s The English Assassin (1972). The third of Moorcock’s four novels featuring dandy, scientist, rock star, and adventurer (Buckaroo Banzai, eat your heart out) Jerry Cornelius is subtitled A Romance of Entropy. This is true in two senses: Cornelius is an agent of the cosmic force that opposes culture, civilisation, empire, religion, and other manifestations of order; and the book itself is entropic — a pastiche of stories working at cross-purposes. Cause and effect are out of whack, here; ambiguity is the whole point. Unlike running, jumping, shooting action heroes, Jerry Cornelius is an idler; at the beginning of The English Assassin, he is fished out of the ocean — dead (eat your heart out, Jason Bourne) — and he can barely be bothered to get out of bed, despite such goings-on as a nuclear attack on India and a Scottish war of independence fought with zeppelins… each apocalyptic scenario set on a different version of the Earth. He does stop a peace conference — violently — though. We spend a lot of time with Cornelius’s coterie, including the titular assassin (IMHO) Una Persson. The book’s message, if any, is delivered by Catherine: “Goodbye, England.” Fun fact: The Cornelius Quartet includes The Final Programme (1968), A Cure for Cancer (1971), and The Condition of Muzak (1977). There are other Cornelius stories, too. Robert Silverberg’s Dying Inside (1972). This brilliantly written sci-fi yarn/bildungsroman set in (then-)present-day New York concerns David Selig, a middle-aged telepath who has squandered his abilities. He’s spent his life prying into other people’s hearts and minds, probing their secrets for his own pleasure; and also for (minor) profit: he reads the minds of college students so that he can ghost-write reports on their behalf. Now, Selig’s power is beginning to fade. Silverberg imaginatively depicts what the lived experience of telepathy might be like, for better or worse; in the tradition of Radium Age sci-fi telepath stories, it’s mostly worse. Selig has failed to develop meaningful relationships, or to carve a purposeful place for himself in society. The novel’s plot is beside the point; and Selig is a loser — not only neurotic and directionless, but prejudiced. Stick around for wild scenarios, including a telepath’s vicarious experience of: his girlfriend’s acid trip, a young couple having sex, a hen laying an egg… and a very spiritual farmer. Fun facts: Silverberg is perhaps most famous today for his science-fantasy Majipoor series, beginning with Lord Valentine’s Castle (1980). But I’m more a fan of his New Wave sci-fi, including Thorns (1967), To Open the Sky (1967), The Masks of Time (1968), To Live Again (1969), Downward to the Earth (1970), and The World Inside (1971). Barry N. Malzberg’s Beyond Apollo (1972). A two-man mission to Venus fails, and its captain is killed; NASA inters Harry M. Evans, the surviving astronaut, in an insane asylum — and interrogates him. What went wrong? We’ll never know: Evans’s story shifts constantly. He killed the Captain; the Captain tried to kill him; Venusians killed the Captain; there are no Venusians; he is the Captain, disguised as Evans. (Was there a Captain, in the first place)? Humankind, the reader begins to infer, isn’t mentally equipped to cope with the claustrophobia and dislocation of space exploration. Evans, or “Evans,” who confesses to wanting to write a novel, has become a story-generating machine, recounting memories (or fabricated memories), dream conversations, possible explanations and endings, sexual fantasies (or realities), and cryptograms. Is Evans’s approach to truth/reality — playful, evasive, inconclusive, a thousand flashes of illumination rather than a reliable source of light — a step forward in human evolution? Or is he just insane? Don’t read this book for the plot; read it for the exercise. Fun facts: Beyond Apollo won the inaugural John W. Campbell Memorial Award for Best Science Fiction Novel. Detractors claimed that this was an insult to the memory of Campbell, the Golden Age sci-fi author and editor whose name was synonymous with the wonder of space exploration. David Bowie‘s The Rise and Fall of Ziggy Stardust and the Spiders from Mars (1972). Bowie’s fifth studio effort wasn’t originally intended to be a concept album, but over the course of its recording the idea of “Ziggy Stardust” — an androgynous rock star who may or may not be an extraterrestrial — metastasized from a single song into first a publicity stunt and then, ultimately, an enduring mythos. The album’s tremendous first track, “Five Years,” announces the imminent destruction of Earth; “Moonage Daydream” introduces a “space invader” who preaches a cosmo-religious message of sex, love, and rock’n’roll; and the “Over the Rainbow”-ish “Starman” describes a teenager’s discovery — via late-night radio — that a starman waiting in the sky approves of youthful rebellion and pleasure-seeking. On side two, we meet Ziggy Stardust himself: Is this kabuki cat truly “the Naz” (Lenny Bruce’s hipster slang for Jesus), or merely a Vince Taylor-esque teen idol who (in his band’s estimation, anyway) has developed delusions of grandeur? The album’s bricolage mythography encourages us to actively participate in parsing the Ziggy myth for ourselves. Philip K. Dick, whose 1981 novel VALIS features a Ziggy-inspired character, took Bowie up on the challenge; so did the inspired prankster who’s recently pointed out that Kanye West, whose name seems to float above Bowie’s head on the cover of Ziggy Stardust, was born five years after the release of “Five Years.” Yes! Fun facts: The album features contributions from the Spiders from Mars, Bowie’s backing band: Mick Ronson, Trevor Bolder, and Mick Woodmansey. A “Ziggy Stardust” concert film, directed by D. A. Pennebaker, was recorded in 1973; it’s well worth viewing. J.G. Ballard’s Crash (1973). If Ballard’s early novels — 1964’s The Burning World, for example — were sardonic inversions of survivalist cozy catastrophes, then Crash might be read as a sardonic inversion of another Adventure genre: the picaresque. In fact, the episodic, shambolic plot of Crash, in which the protagonist falls under the influence of a charismatic, wildly unconventional kook, and immerses himself in an automobile-centric world of transgressive kicks, feels to this reader like a pessimistic, avant-garde response to the all-American optimism of Kerouac’s On the Road. (Ballard himself described Crash as a “warning against that brutal, erotic, and overlit realm that beckons more and more persuasively to us from the margins of the technological landscape.”) In this anti-optimistic morality play, “James Ballard” is maimed in a car crash, which leaves the other driver dead; he is subsequently drawn into the orbit of Dr. Vaughan, a car-wreck enthusiast who heads up a kind of sex cult of fellow fetishists. Vaughan, Ballard, and others — Seagrave, a crossdressing stuntman; Gabrielle, a lesbian opium-addict and amputee; Helen, the widow of Ballard’s victim — engage in Sadean sex rites in crashed and about-to-be-crashed cars. If Ballard’s earlier novels are cataclysms set in the future; Crash takes place in a cataclysmic present, i.e., one in which catastrophe has become normalized. Fun facts: The Normal’s 1978 song “Warm Leatherette” was inspired by Crash; Gary Numan’s 1979 song “Cars” may have been, as well. In 1996, Crash was adapted as a film of the same name by David Cronenberg; it stars James Spader, Deborah Kara Unger, Elias Koteas, Holly Hunter, and Rosanna Arquette. Thomas Pynchon’s Gravity’s Rainbow (1973). Set during the waning days of WWII, Pynchon’s infamous masterpiece — considered by some to be one of the greatest American novels; considered by others to be unreadable — is an apophenic espionage adventure revolving around the quest to uncover the secret of a mysterious device, or MacGuffin, which is to be installed in a German V-2 rocket. (The book’s title refers to the parabolic trajectory of a V-2, as well as to the introduction of randomness into physics via quantum mechanics.) Gravity’s Rainbow is also a picaresque adventure, featuring over 400 characters, which follows Tyrone Slothrop, a naive Allied Intelligence operative, as he wanders — under covert surveillance, by his own comrades, who are interested in his sexual activities — around London, then a casino on the recently liberated French Riviera, and then in “The Zone,” which is to say, Europe’s post-war wasteland. What does Margherita Erdmann, former star of a traveling sado-masochistic sex show, know about the device? Why do the Schwarzkommando, African rocket technicians brought to Europe by German colonials, worship the V-2? Why is Slothrop being tailed by Major Duane Marvy, a sadistic American, and Vaslav Tchitcherine, a drug-addled Soviet intelligence officer? Slothrop discovers that he may have been experimented on, as an infant; does this have something to do with German occult warfare shenanigans? Plus: silly songs, 1940s pop culture references, kazoos. Here’s the key: “If there is something comforting — religious, if you want — about paranoia,” we read, “there is still also anti-paranoia, where nothing is connected to anything, a condition not many of us can bear for long.” Fun facts: Winner of the National Book Award for 1974, and nominated for both a Pulitzer Prize and a Nebula Award. NEW WAVE SF: 1974–1983 If adventure novels in the Sixties troubled their readers’ faith in fixed, universal categories, and in certainty, Seventies adventure replaced these relics with difference, process, anomaly. The science fiction of the era — Ursula K. LeGuin’s The Dispossessed, Samuel R. Delany’s Trouble on Triton, Philip K. Dick’s A Scanner Darkly, Christopher Priest’s Inverted World, Olivia E. Butler’s Wild Seed — was as far-out as it gets, the final flourish of New Wave before the advent of cyberpunk. All binary oppositions (past/present, liberal/conservative, innocent/guilty, utopian/anti-utopian) are overthrown. Ambivalence, indeterminacy, and undecidability of things: In Seventies adventures, these are the anti-anti-utopian new normal. Philip K. Dick’s Flow My Tears, the Policeman Said (1974). A typically dystopian, disorienting, and dashed-off Philip K. Dick joint — but one which is more emotionally resonant than his Fifties and Sixties oeuvre; in this sense, Flow My Tears points the way forward to Dick’s late masterpieces, A Scanner Darkly (1977) and VALIS (1981). The year is 1988; in the aftermath of a Civil War, the USA has become a police state; African-Americans have been all but exterminated; college students have become underground guerrillas; people use their smartphones to seek hook-ups (via the Phone-Grid Transex Network) and advice (via Cheerful Charlie, an app); and recreational drug use has been normalized. When a crazed ex-lover sics a “gelatinlike Callisto cuddle sponge” on Jason Taverner, the famous pop star (“Nowhere Nuthin’ Fuck-up” is his latest hit) and TV host wakes up in a motel to discover that… no one knows who he is. Taverner takes drugs, seeks assistance from a series of women — an unstable old flame; a leather-clad lesbian; a sweet-tempered potter — and desperately attempts to figure out what might have caused him to become a non-person. The titular policeman? He’s the twin brother and lover of the leather-clad lesbian, and a powerful authority figure who learns an important lesson in humility and empathy for others. Fun fact: Winner of the John W. Campbell Memorial Award for Best Science Fiction Novel in 1975. Dick would later paordy this novel, in VALIS, as The Android Cried Me a River. Ursula K. Le Guin’s The Dispossessed (1974). Shevek, a brilliant young physicist, lives on the peaceful anarchistic planet Anarres, whose inhabitants value voluntary cooperation, local control, and mutual tolerance. This is a richly imagined world — with a language, for example, that cannot readily express “propertarian” or “egoist” concepts. The downside of this utopia is an entrenched bureaucracy that stifles innovation, particularly if it seems to challenge the prevailing political and social ethos. Shevek’s new temporal theory, and the resulting “Ansible” that he hopes to develop (which will allow instantaneous communication between any two points, no matter how many light-years apart; and which will therefore make possible a galactic network of civilizations), may never see the light of day. So he relocates to Urras, a nearby world (Annares is its moon) where disruptive new theories and technologies are welcomed. Once there, however, Shevek is dismayed and disgusted by Urras’s two largest states: the USA-like A-Io, which is capitalist, sexist, wasteful, exploitative, and tumultuous — forever on the verge of revolution or war; and the authoritarian, USSR-like Thu. While the author is clearly sympathetic to the ideals of Annares, The Dispossessed is a pointed critique of typical utopian narratives; the dichotomies that Le Guin describes are not readily surmounted — it’s a negative-dialectical romance. Fun fact: Although this was the fifth novel published in Le Guin’s Hainish Cycle, chronologically it is the first. The Dispossessed won the Nebula and Hugo Awards for Best Novel. Later editions of the book are subtitled “An Ambiguous Utopia.” Christopher Priest’s Inverted World (1974). Having reached the mature age of “650 miles,” Helward Mann becomes an apprentice Future Surveyor — which means that he must help choose the best route for his city, “Earth,” which appears to be an Earth colony on an alien world. The city is winched along, at about eight miles per day, along tracks that are then picked up and re-used; the city’s goal is a perhaps nonexistent “optimum” destination somewhere to the north. Many of the city’s citizens (including Helward’s wife, Victoria) are unaware that the city is mobile; a decrease in the birthrate obliges the city to capture native women from the villages they pass en route. When Helward leaves the city, he discovers how truly strange the outside world is; time itself works differently, within the city vs. outside, and to the north vs. the south. Also, he finds himself pulled southward by a mysterious, ever-increasing force. Eventually, he meets a woman who claims to have come from England recently; in fact, she claims that they’re on Earth! What is actually going on? Fun fact: Expanded from a short story by the same title included in New Writings in SF 22 (1973). The Encyclopedia of Science Fiction calls Inverted World “one of the two or three most impressive pure-sf novels produced in the United Kingdom since World War II.” John Crowley’s The Deep (1975). A checkerboard-like medieval kingdom is housed on a circular plane balanced atop a pillar that emerges from The Deep, an abyss inhabited by an entity known as The Leviathan. Although Crowley’s first novel is not a particularly long one, its scope is truly epic. Through the eyes of a strange visitor from off-world, a genderless android who immediately loses its memory, we witness a complex War of the Roses-esque struggle between “Red” and “Black” factions, not to mention musket-wielding rebels (The Just) and a fractured nobility (The Protectors); there are many characters, each of whom will interact with every other character before all is said and done. The Visitor is a kind of recording angel; was it sent here by Leviathan (God)? If not, what is its purpose? What’s outside this world? Is this all some kind of puppet show or game? One is reminded not only of George R.R. Martin’s Westeros series, but of the recent Westworld remake. Crowley’s writing is lyrical, and there are thrills and chills galore: swords and sorcery, a rooftop escape, a journey through a marsh, a climb towards the edge of the world! Fun fact: Crowley is best known for his amazing 1981 novel Little, Big, which has been called “a neglected masterpiece” by Harold Bloom, and other works of fantasy. His earliest novels, however, including Beasts (1976) and Engine Summer (1979), were science fictions. Cordwainer Smith‘s Norstrilia (1975, in complete form). In the far future, as we know from Smith’s other stories (collected in the volume The Rediscovery of Man), once all of humankind’s needs have been met — thanks to advanced technology, peace and prosperity, and the use of animal-derived “underpeople” for the few remaining physical jobs — a galaxy-wide administrative body known as the Instrumentality will intervene, in order to make life worth living again. How? By reintroducing cultural and language differences, for example, or even by encouraging the underpeople to revolt. Rod McBan the 151st, a young farmer on the planet Norstrilia — which had long ago been colonized by Australians, and which alone produces the immortality drug stroon — comes to the attention of the Instrumentality because of his unusually powerful, yet difficult-to-control psionic abilities. They help save him from a Norstrilian rite of passage that could otherwise have proven fatal, and when Rod uses an ancient, illegal computer to corner the galactic market for stroon, they help smuggle him back to Earth, humankind’s homeworld, before he can be assassinated or kidnapped. Rod is disguised as an underperson — a cat-man — and C’mell, Earth’s most beautiful cat-woman, becomes his protector. Sympathizing with the underpeople, Rod sacrifices his fortune for them. Fun facts: Norstrilia is the only novel published by Paul Linebarger, a scholar and diplomat expert in psychological warfare, as “Cordwainer Smith.” Its two parts were published, separately, in Galaxy in 1964; both were then published as novellas. They weren’t combined into one volume until 1975. Samuel R. Delany’s Dhalgren (1975). Dhalgren is set in Bellona, somewhere in the American Midwest (Kansas?) — where a very local, very strange catastrophe has happened. A city block burns down, but a week later it’s intact again; time passes differently for different people; there seem to be two moons in the night sky. Although most of Bellona’s inhabitants have abandoned the bewildering, surreal city, various marginalized Americans — including the Kid, a multiracial, bisexual, possibly schizophrenic drifter — find themselves drawn to it. Some readers (including Philip K. Dick, who threw it away) have found Dhalgren‘s meandering, apophenic, and epic plot boring or maddening; if you hated Richard Linklater’s 2001 movie Waking Life, in which a man discusses the meaning of the universe as he shuffles through a hallucinatory landscape, then you’ll hate this book. Other readers have enjoyed the text’s postmodernist twists and turns, its digressions on the nature of poetry and art, and its pursuit of wonder and beauty in the face of disaster, even if they find the foul language and explicit sex scenes distasteful or (at this point) dated. Note that the Kid — an Orpheus-like figure, whose only hope of making sense of his experiences is to become an author of the book we’re reading, or at least a version of it — would most likely agree with both sorts of readers. Fun fact: Dhalgren is Delany’s most popular book. William Gibson has referred to it as “A riddle that was never meant to be solved”; other commentators have noted the book’s debt — cf., mythological resonances, Moebius-strip plot form — to James Joyce’s Ulysses. Someone should make a movie! J.G. Ballard’s High-Rise (1975). Philip K. Dick’s notion of a “conapt” — a densely populated, self-sufficient human habitat, isolated and isolating — was already a dystopian one, when he introduced it in The Three Stigmata of Palmer Eldritch in 1965. Here, Ballard offers a kind of blood-spattered parable built around a pun: When the social order of his high-rise London apartment complex becomes violently dysfunctional, Richard Wilder, a documentary film-maker who lives on one of the building’s lower floors, literally becomes a social climber — scaling his way upward, to the luxurious penthouse suite. Anthony Royal, the building’s architect, perches there, awaiting his fate with a certain oblivious detachment. Our protagonist, Robert Laing, lives in-between these two characters; although he aspires to be as coolly uninvolved as Royal, Laing gets caught up in and enjoys the regressive mayhem: fighting in gangs, raiding and vandalizing other floors, killing pets, taking women. (It’s all straight out of David Bowie’s 1974 song “Diamond Dogs.”) As in Crash (1973) and Concrete Island (1974), Ballard is concerned about the deleterious effects of our advanced mode of life; note that life outside the titular high-rise goes on as usual. Fun fact: High-Rise was one of Joy Division singer Ian Curtis’s favorite books. It was adapted into a 2015 film of the same name, starring Tom Hiddleston, Jeremy Irons, and Sienna Miller, by director Ben Wheatley. Samuel R. Delany‘s Trouble on Triton: An Ambiguous Heterotopia (1976). Part novel, part treatise, Triton brilliantly (if sometimes maddeningly) deploys post-structuralist theory in order to both illuminate and subvert the story of an unpleasant protagonist’s struggle to acculturate himself to life in a utopian colony on Triton, Neptune’s largest moon. Bron Helstrom, who had previously worked on Mars as a male prostitute, should be happy on Triton — where no one goes hungry, and where one can change one’s physical appearance, gender, sexual orientation, and even specific patterns of likes and dislikes. Helstrom’s problem is that he is an unregenerate individual, an asshole even, in a culture that deprioritizes the notion of the individual. (Shades of Stanislaw Lem’s Return from the Stars.) But Triton is less about Bron, in the end, than it is a critique of utopia, an exploration of the Foucauldian notion of heterotopia, and a semiotic intervention into science fiction’s unexamined ideologies. Should we feel sympathy for Bron, and reject Triton’s social order; or the other way around? Yes and no. PS: Almost forgot to mention that there is a destructive interplanetary war, between Triton and Earth, too. Fun fact: Originally published under the title Triton, the novel’s themes and formal devices are also explored in Delany’s 1977 essay collection, The Jewel-Hinged Jaw. Joanna Russ’s We Who Are About To… (serialized 1976; in book form, 1977).. A brutal, nihilistic — but beautifully written — novella that might perhaps best be categorized as an anti-Robinsonade. When a spaceship crash-lands on an uninhabitable and uncharted planet, the crash survivors rally and begin to make plans for starting an impromptu colony; even if they aren’t rescued, perhaps their descendants will be. Our unnamed protagonist, however, refuses to buy into this nonsense. Not only will the group not survive, she realizes, but — as a fertile woman — she will essentially become a breeding slave. In fact, the others do beat her, tie her up, and make plans to forcibly impregnate her via rape if she doesn’t come around to their way of thinking. What the would-be colonists fail to realize is that our protagonist is a bad-ass who cannot be controlled. She escapes — taking less than her fair share of rations and supplies with her, because she doesn’t intend to prolong the agony of her own inevitable death by starvation. When the others pursue her, she opens up a can of whoop-ass. Finally, she records recollections of her life — and feminist musings on the nature of subjectivity — among other things, as she wastes away. Fun fact: Samuel R. Delany has described this novella as “a damningly fine analysis of the mechanics of political and social decay,” and a refutation of the notion that reproduction — as opposed to the quality of your life — is the point of living. Moebius‘s Le Garage Hermétique (The Airtight Garage, 1976–1979). Le Garage Hermétique is one of the two great capricious comic strips of the Seventies (1974–1983); Gary Panter’s Dal Tokyo, which first appeared in 1983, is the other. Over a four year period, Moebius cranked out two to four pages per month; each month, he challenged himself to solve continuity problems that he’d playfully introduced previously, while also creating new problems. The main plot, to the extent that there is one, concerns the efforts of Major Grubert to prevent outside entities from invading the Garage Hermétique — an asteroid housing a pocket universe, which features, e.g., desert and forest biomes, a city, and a world made of machines. One of these invaders is Jerry Cornelius, a trickster figure whom Moebius lifted from Michael Moorcock’s sci-fi novels; Grubert and Cornelius join forces, eventually, to face a threat to the Airtight Garage. Each installment of the strip, many of which focus on Grubert and Cornelius’s hapless allies, is its own self-contained epic; they don’t necessarily add up to anything larger. Fun fact: Le Garage Hermétique first appeared, from 1976–1979, in issues 6 through 41 of the Franco-Belgian comics magazine Métal Hurlant; Americans first read it in the magazine Heavy Metal, starting in 1977. Philip K. Dick’s A Scanner Darkly (1977). Set in a barely futuristic Los Angeles of 1994, Scanner tells the story of “Fred,” an undercover narc who gets a kick out of the counter-cultural addicts with whom — as “Bob” — he dwells. Fred’s abuse of Substance D (street name: “Death”) contributes to a brain psychosis complicated by his latest assignment… spying on Bob! Whose motivations Fred finds opaque. Dick mines humor and pathos out of the druggies’ lifestyle: paranoid conversations among low-lifes who actually are being spied on; crack-ups that feel like break-throughs. Also, this is a neo-noir crime novel — one in which we sympathize with the criminals, who are spirited free-thinkers, and despise the manipulative, cold-hearted cops. When Fred is sent to a detox facility, he cracks the secret of Substance D… too late? The book ends with a dramatic dedication to Dick’s many friends who’d been killed or permanently damaged by drug abuse; the author’s own name is on the list. Fun facts: In 2006, I wrote a Slate essay about the novel and Richard Linklater’s adaptation. Which I still haven’t seen. Gary Panter’s comic Jimbo (serialized 1977–present). Panter’s “ratty line” illustrations helped define the style of L.A. punk. But the appeal of Jimbo — an all-American, snub-nosed, freckle-faced punkoid wandering through Dal Tokyo, a planet-wide sprawl of a city founded on Mars by Japanese and Texans — is timeless. Jimbo is a high-lowbrow antihero, equally at home in the pages of the L.A. music zine Slash, where he first appeared, and in the artsy RAW. The early post-apocalyptic/surrealist comics (has Jimbo’s girlfriend been kidnapped by giant cockroaches?) have since given way to elaborate graphic novels that employ the character as an Everyman puzzling his way through religious/pop culture allegorical landscapes. But don’t try to understand the plot of Panter’s stories; the medium is the message. Panter’s protean style — which changes from page to page, sometimes exploding into sheer abstraction — demands that the reader participate actively in making sense of Jimbo’s… mission? Fun facts: Jimbo comics have been collected in Jimbo (1982), Invasion of the Elvis Zombies (1984), Jimbo: Adventures in Paradise (1988), and Jimbo’s Inferno (2006). Panter is now working on a collection of his Jimbo mini-comics. John Varley’s The Ophiuchi Hotline (1977). Before William Gibson, Iain M. Banks, and the Cornershop album When I Was Born for the 7th Time, there was… this oddball achievement. In the year 2618, four hundred years after the human race was displaced from the Earth by alien invaders (who consider aquatic mammals more advanced), humankind scrabbles for survival on the Moon and other off-world colonies. Thanks to the Hotline, a stream of data from a distant star system, the human survivors have mastered bioengineering techniques such as cloning, memory recording, adding and subtracting body parts, changing one’s sex whenever one chooses, and forming new life forms with intelligent symbiotes. Lilo, a rebel geneticist, faces execution for violating laws of humankind’s Eight Worlds; she escapes — or does she commit suicide, while a clone with her memories downloaded take her place? Lilo and her clones are soon embroiled in a plot to battle the invaders… using a black hole! Meanwhile, whoever has been sending information via the Hotline suddenly demands payment. Fun facts: This is the author’s first book, and the first (novel-length) installment in his Eight Worlds series. Vonda N. McIntyre’s Dreamsnake (1978). In the far future, many years after a devastating nuclear war, genetic manipulation of plants and animals is routine, and humankind has reverted (evolved) into a neo-tribalist social order. Snake, a healer immune to snakebites, carries with her three snakes — Grass, Sand, and Mist — whose venom she uses in her potions. When fearful nomads cripple Grass, a small and rare “dreamsnake” from off-world, whose venom is capable of inducing heroin-like torpor and LSD-like hallucinations, Snake embarks, across a desert of black sand on a quest. Along with a patient who is suffering from radiation sickness — there are pockets of radiation left here and there — Snake heads to the city of Center, in hopes of persuading the Otherworlders who visit there to sell her a new dreamsnake. Hers is a picaresque journey involving a desert-dwelling dreamsnake-venom addict, a handsome young lover, an abused 12-year-old girl, a giant bandit, a character whose sex is never specified, and many hallucinations. Fun facts: Dreamsnake started as a story called “Of Mist, and Grass, and Sand,” which won a Nebula Award. The novel version won the 1979 Hugo Award. James Tiptree Jr.‘s Up the Walls of the World (1978). Where to begin? Here on Earth, a group of troubled men and women with telepathic abilities are being studied like lab rats by the (paranoid, drug-addicted) Dr. Dann, as part of a US Navy experiment; Dr. Omaili, with whom Dr. Dann falls in love, is a computer scientist of African descent who has been unable to make meaningful connections since her teens, due to a ritual cliterodectomy administered by her stepfather. Meanwhile, Tivonel, a manta ray-like young female on a far-off planet (Tyree), resents the males of her powerfully psionic species — who are charged with the all-important task of parenting, while the females freely sail the air currents; she is more concerned, however, when Tyree’s scientists report that a wave of death is spreading across the galaxy and headed their way. And then there’s The Destroyer, a solar system-sized, network-structured, sentient inhabitant of deep space, which muses sadly (IN ALL CAPS) on its solitude and inability to fulfill its mysterious duty… while absent-mindedly destroying the galaxy. In an effort to save her species, Tivonel “invades” Earth — telepathically. The minds of Dr. Dann, Dr. Omaili, and their test subjects are transported into the bodies of Tivonel’s inhabitants… where they are uniquely able to flourish. But what can they do to stop the Destroyer? Fun facts: “James Tiptree Jr.” was the pen name of pioneering female sci-fi author Alice Sheldon. In the 1940s, she was an officer in the Air Force’s photo-intelligence group; in the ’50s and ’60s, she earned a doctorate in Experimental Psychology. Already well-known for her sci-fi stories, Up the Walls of the World was her first novel. John Crowley’s Engine Summer (1979). A beautifully written, trippy novella set in a post-apocalyptic America, where relics of the past confound and amaze our protagonist, young Rush That Speaks. He’s grown up in Little Belaire, an idyllic, tribalist community of “true speakers” who fled society’s collapse (specifically, they appear to have fled Bel Air, California) in the distant past. This reader could have happily explored Little Belaire and its traditions along with Rush That Speaks for a longer stretch, but when Once a Day, the girl he loves, absconds to live with a cat-like society called Dr. Boots’s List, Little Rush heads out after her. First, however, he spends time living with Blink, a “saint,” in a treehouse, gathering more intel about the high-tech, unhappy prelapsarian world of the vanished “angels.” Once he’s taken in by Dr. Boots’s List, Rush That Speaks undergoes a mind-meld with Dr. Boots — from which he will never fully recover. At which point, an “angel” drops from the sky, and much is made clear to us. As with Riddley Walker, published the following year, Engine Summer is among other things a riddle, requiring readers to puzzle out secrets of the past (our near-future, perhaps) that the book’s characters themselves will never truly understand. The book’s ending is a deeply poignant one. Fun facts: Crowley is best known as the author of Little, Big (1981), a much-admired fantasy novel, and for his Ægypt series of novels (1987–2007). I like these books, too, but strongly urge science fiction fans to revisit his early work. Octavia E. Butler‘s Kindred (1979). In this realistic (visceral, even) sci-fi/fantasy hybrid yarn, which Butler modeled on grim North American slave narratives by the likes of Harriet Tubman, Dana, a young, educated African-American woman in contemporary Los Angeles, finds herself shunted back in time to an antebellum Maryland plantation. (Unless she’s just hallucinating?) Dana, who is married to a white man in the present, discovers in the past her own ancestors — a white planter and a black freewoman who has been forced into slavery. Her ontogeny — as a black woman fully conscious of slavery’s legacy in contemporary America — recapitulates the phylogeny of her ancestor, who loses her innocence, faces harsh punishment, develops strategies of resistance, and ultimately develops the ability to escape from a repressive, racist white society. Kindred unflinchingly interrogates the intersection of power, gender, and race, but the narrative is far from simple: Dana’s ancestors, Rufus and Alice, were childhood friends, and Dana ends up developing sympathy for Rufus, despite the fact that he grows up to be a monster. She also encounters Sarah, an angry slave who only appears to be a submissive “mammie,” and other characters who fail to conform to previous depictions of slavery, from Gone with the Wind to Roots. The time travel narrative is also complex, as Dana — and, sometimes, her husband — ricochets back and forth from the present to various points in Alice and Rufus’s life stories. Fun facts: Kindred was a bestseller, and remains popular today; it is often chosen as a text for community-wide reading programs and high school and college courses. It was adapted as a 2017 graphic novel by Damian Duffy and John Jennings. Alejandro Jodorowsky and Jean Giraud (Moebius)’s graphic novel The Incal (1980–1988). In the capital city of a planet within a galactic empire dominated by humans (and humanoid aliens known as Bergs), a mercurial private investigator, John DiFool, is nearly murdered by masked assassins. They’re seeking a crystal of enormous power, the Light Incal, which has gone missing; and so is: Animah, the keeper of the Light Incal; Tanatah, leader of a rebel group (and Animah’s sister); the city’s corrupt government; the Bergs; and the Technopriests, a cult that worships a different crystal, the Dark Incal. Jodorowsky’s space opera — drawn brilliantly by Giraud — is a satirical, dystopian admixture of intergalactic travel, political conspiracy, sex, drugs, and messianism. (If this description reminds you of Dune, it should: in 1975, Giraud provided concept art for Jodorowsky’s never-made film adaption of Frank Herbert’s novel.) Accompanied by Deepo, his loyal “concrete seagull,” and the Metabaron, a mercenary super-warrior, DiFool embarks on a quest to prevent the Technopriests from launching a sun-eating Dark Egg. It’s also a recursive mystic parable, of sorts. Fun facts: Originally published in installments in Métal Hurlant (1980–1988); The Incal was followed by Before the Incal (1988–1995, with Zoran Janjetov), After the Incal (2000, with Jean Giraud), and Final Incal (2008–2014, with José Ladrönn). Octavia E. Butler’s Wild Seed (1980). In this prequel to Butler’s Patternist epic, a 1976–1984 series of novels recounting the rise of a mutant species of networked telepaths who will — in the far future — come to enslave “mute” humankind (while struggling against the Clayarks, a species of extraterrestrial plague-mutated humanoids), our protagonist is Anyanwu, a 350-year-old shapeshifter living peacefully in Africa… until she meets Doro, a telepathic spirit so powerful that he can possess any body. Doro, who was born in Egypt during the reign of the Pharaohs, intends to breed a race of telepaths under his control; he has been collecting people with unusual abilities and breeding them, and Anyanwu is his next target. Although she vehemently disagrees with Doro’s contemptuous, utilitarian treatment of his experimental subjects, Anyanwu is persuaded to leave her home for America… where she will struggle to rebel against Doro’s manipulative coercion. The book’s timeline begins in the late 17th century and ends in the early 19th century, and Anyanwu and Doro take the Middle Passage to the New World aboard a slave ship… yet Butler complicates this quasi-slave narrative by showing how Doro protects his “seeds” from Indian attacks and White racism alike. In the end, it’s a book about a woman who fights to protect her family from a cruel, paternalistic man — using love and self-sacrifice, not violence. Fun facts: The other Patternist books, in order of publication, are Patternmaster (1976), Mind of My Mind (1977), Survivor (1978), and Clay’s Ark (1984). Russell Hoban’s Riddley Walker (1980). “Our woal life is a idear we dint think of nor we dont know what it is.” Although its degraded dialect is not to every reader’s taste, and although it’s not particularly action-packed, Riddley Walker is one of the greatest of all post-apocalyptic adventures — and a brilliant, deep, mysterious, tragicomic piece of writing that amply rewards re-reading. When twelve-year-old Riddley’s father dies, he becomes his community’s “connexion man” — tasked with teasing out the social, religious, and political implications of the ever-evolving puppet shows staged — in primitive towns across “Inland” (England) — by church/government propagandists. A cataclysm happened, a couple of thousand years ago, reducing the world to an Iron Age level of technology — leaving nothing but myths, turns of phrase (“hes getting his serkits jus that little bit over loadit”), and artifacts to be puzzled over by inquisitive types. A power struggle is going on, we discover — one whose outcome may lead to humankind’s progress forward out of the ashes, or to utter ruin. The chance finding of an ancient relic gets Riddley involved in this momentous struggle… and sends him on the run! Though it’s often compared to A Clockwork Orange and A Canticle for Leibowitz, I’d describe Riddley Walker as a sequel to Cicely Hamilton’s Theodore Savage, and a companion-piece to John Crowley’s Engine Summer. Fun facts: Harold Bloom included this book in his list of works comprising the Western Canon; and George Miller’s 1985 movie Mad Max Beyond Thunderdome pays homage to Riddley Walker. Hoban is best known as the author of Bedtime for Frances (1960, ill. Garth Williams) and sequels, as well as for the older children’s novel The Mouse and His Child (1967). Philip K. Dick’s VALIS (1981). Our narrator, Phil, a brilliant, self-reflexive sci-fi author who may be crazy, explores his own ideas… as well as those of the book’s protagonist, Horselover Fat, a brilliant, self-reflexive sci-fi author who may be crazy. Fat, it seems, has received a beam of pure reason from “God” — perhaps an alien satellite orbiting Earth — which has allowed him to see that 1970s California is an illusion; actually, we’re all slaves blindly toiling in a Black Iron Prison. So… the ancient Gnostics were right! (Right?) The true nature of the universe is a Vast Active Living Intelligence System. Superhumans living anonymously among us use pop culture to stay in touch with one another; the pulp novels of Philip K. Dick may be on to something; the writings of Heraclitus, Schopenhauer, Freud and Jung must now be recontextualized. (Right?) Fat sets off, with a few friends, to find answers: about God, suffering, art, the mind, the secret history of humankind, and — naturally — about David Bowie, particularly his 1976 movie The Man Who Fell to Earth. Fun fact: VALIS is the first installment in a never-completed trilogy of novels fictionalizing the philosophical explorations Dick made into this experience via a rambling treatise, The Exegesis. It isn’t necessary, in order to enjoy VALIS, to know this, but: in 1974 Dick experienced a series of hallucinations which presented themselves as encounters with a gnostic version of the divine. John Wagner, Alan Grant, and Carlos Ezquerra’s Judge Dredd adventures “Block Mania” (serialized 10/31/1981 to 12/26/81) and “The Apocalypse War” (1/2/82 to 6/26/82). In 1977, writer John Wagner and artist Carlos Ezquerra created Judge Dredd — a violent sci-fi comic strip spoofing American action movies (military, cop, western, vigilante) of the Seventies — for British readers of the weekly magazine 2000 AD. In the 22nd century, the titular judge-jury-and-executioner character and his colleagues police Mega-City One, which is subdivided into gigantic towers known as City Blocks stretching from Boston to Key West. In “The Apocalypse War,” Russkies from East Meg One, a Soviet citystate, invade Mega-City One. Hundreds of millions of Mega-City One citizens are killed, and ninety percent of the city is captured — while Dredd tries to organize a guerrilla resistance movement. Failing that, he must consider the option of completely obliterating East Meg One! “Block Mania,” the prologue to “The Apocalypse War,” demonstrates how Mega-City One was softened up for invasion: a massive conflict between City Blocks is engineered by an East Meg One agent, Orlok, who infects the water supply with a psychotropic drug. Orlok also kills Dredd’s sidekick, Judge Giant, in an almost casual way — which was shocking to even the shock-proof readers of 2000 AD. Fun fact: Via his excellent Dredd Reckoning blog, Douglas Wolk notes just how dark the humor of this storyline is: “It’s a story about genocide with comedy relief interludes — the Walter-and-Maria slapstick routines, the Country Joe-type folksinger getting splattered by a missile.” In 1987, Games Workshop produced a board game, set in the Judge Dredd universe, called Block Mania; players take on the role of rival City Blocks — and use spray paint, guns, flamethrowers and heavy lasers to vandalize and destroy neighboring blocks. Alan Moore’s dystopian graphic novel V for Vendetta (serialized 1982–1989). Illustrated by David Lloyd. In the near future, following a nuclear war, the United Kingdom has become a fascist state run by the Norsefire Party. V, a flamboyant anarchist terrorist and vigilante whose face is never seen — he wears a Guy Fawkes mask — begins a campaign to bring down the government (and all governments); it’s like Nineteen Eighty-Four with a happy ending. V, we discover over the course of the graphic novel, was imprisoned in a Norsefire concentration camp and experimented upon… which led him to develop superhuman strength, reflexes, and endurance. He blows up the Old Ba",
    "commentLink": "https://news.ycombinator.com/item?id=38414841",
    "commentBody": "New Wave Sci-Fi: 75 Best Novels of 1964–1983Hacker NewspastloginNew Wave Sci-Fi: 75 Best Novels of 1964–1983 (hilobrow.com) 153 points by webmaven 16 hours ago| hidepastfavorite81 comments deng 13 hours agoIt seems a lot of people here are unaware that \"New Wave Science Fiction\" is not another name for this specific time period but rather a subgenre of SciFi which started and got popular during that period. See also https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;New_Wave_science_fictionOf course very good other SciFi was written during that period as well. reply AceJohnny2 15 hours agoprevRelated, someone on Reddit re-read all the Hugo&#x2F;Nebula&#x2F;Locus&#x2F;WFA winning books and reviewed them. I found their rating system, and how well the books aged, very worthwhile:https:&#x2F;&#x2F;www.reddit.com&#x2F;user&#x2F;RabidFoxz&#x2F;comments&#x2F;stwi8k&#x2F;vol_6_... reply MarketingJason 13 hours agoparentHyperion was awesome. Canterbury Tales-style narrative focused on a group of pilgrims on a crazy planet steeped in legend set against a galaxy-spanning empire that is being threatened by a faceless looming external threat. As if that wasn&#x27;t enough, the pilgrims are threatened from within by mystery, murder and betrayal as much as they are hounded by the immortal blade-covered daemon they know they are eventually destined to face.Each pilgrims tale is unique and told in a different vibe - from pious and regretful to vulgar and indifferent. reply OfSanguineFire 2 hours agorootparentThe problem with Hyperion is that it was so successful, the author couldn&#x27;t resist the temptation to write sequels, and those are of much lesser quality. reply brazzy 13 hours agorootparentprevSeconded. The segement \"The River Lethe&#x27;s Taste is Bitter\" is the most heard-rendingly sad thing I have ever read. reply drmpeg 15 hours agoparentprevI couldn&#x27;t finish Neuromancer. I guess I&#x27;m just not a fan of the antihero thing. Toward the end of the book, I just didn&#x27;t care what happened to him. reply lotsoweiners 3 hours agorootparentI&#x27;ve tried to read it about 5 times over the last 20 years or so. Never made it past the first third or so of the book. Something about the way it is written is just so unappealing to me that I&#x27;d rather give up than finish. One of these days I might just read the synopsis. reply 15457345234 56 minutes agorootparentI&#x27;ve seen that sentiment expressed elsewhere, it seems some people just aren&#x27;t compatible with it.Likewise I can&#x27;t get through Pat Cadigan&#x27;s &#x27;Synners&#x27; which I think is a Nebula award winner - the world seems like it _should_ be interesting enough but it just doesn&#x27;t grip me at all... I must have tried about ten times now and I just end up either leaving it for something else or leafing aimlessly through the pages trying to jump ahead to a more &#x27;engaging&#x27; part and not finding anything reply gopher_space 10 hours agorootparentprevThe next two books in that series are quite different in tone. I think the writing is much better too. reply retrocryptid 15 hours agoparentprevGood list, but Orson Scott Card is a bit over-represnted. I recently re-read scheismatrix (plus) after 35 years. My spouse found a copy of Burning Chrome in the local tiny library, so I&#x27;m likely to review a few more cyberpunk works. I have fond memories of the 64-83 era, but it took me a while to warm to Sterling and Gibson. It almost seems like every new sci-fi story has a bit of cyberpunk embedded in it. All the cyberpunk authors were (I believe) influenced by Ballard and PKD.Anyway. Thx for the pointer. reply justsomehnguy 14 hours agorootparent› found a copy of Burning Chromemost of Gibson&#x27;s works I had read in ncview.exeI doubt I can replicate the experience now *sad smile* reply drdaeman 13 hours agorootparentThere are a bunch of terminal apps that emulate CRT look-and-feel, and you can surely run NC in an emulator. reply justsomehnguy 13 hours agorootparentSure, but I wans&#x27;t talking about the visuals alone.I can run NC in DosBox on the glorious 43\".I read Gibson on 15\" in the native 320x200 and my 40Mb HDD shook the table table on the seeks. reply squarefoot 3 hours agorootparentThis terminal emulator might help:https:&#x2F;&#x2F;github.com&#x2F;Swordfish90&#x2F;cool-retro-termAs for other \"effects\", in theory the HDD led on the front panel could be interfaced with some actuator to produce vibration and sound when data is accessed.Personally, I can&#x27;t wait for the PineNote to exit the development phase and become an usable product; other e-paper readers are too closed and&#x2F;or depending on cloud services I have no intention to surrender my personal data to. reply OfSanguineFire 2 hours agorootparentWhat&#x27;s wrong with a Kobo reader? Kobo is friendly to modding, so you can install e.g. the open-source interface KOReader and never use the company&#x27;s own interface. Even if you use Kobo&#x27;s software, initial setup can be done without registration, and you can just leave the reader in airplane mode for the entire lifetime of the device, so it would never connect to any cloud.I have enough experience with Pine64 to not regard their products as anything other than toys that never meet their potential. reply justsomehnguy 2 hours agorootparentprev... and I can run this video at startup, but still that would be just a lousy imitation of the experiences of a younger me.Thanks for suggestion, but you miss the point.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=65guhB-7w9U replydemondemidi 15 hours agoprevIt&#x27;s really hard to overstate the impact of PKD&#x27;s later works and JG Ballard. Having read about 50% of this list, I think all of our modern dystopia, paranoia, and existential SciFi sprouted from their insane seeds. And the depth of LeGuin&#x27;s exploration of social, sexual, and economic identity in alien worlds sticks out like a sore thumb compared to her peers.Damn, tho, the book cover for The Three Stigmata... is fascinating. I always pictured him as just a cyborg octogenarian but now I can&#x27;t unsee this. I miss that artwork.Aaaaand, it would be nice of the page author would put these in a CSV at the bottom so that I can make a checklist! reply PaulDavisThe1st 15 hours agoparentI don&#x27;t know enough to know if your ascription of those \"ills\" to those authors is accurate, but I certainly find it refreshing to see someone asserting that no, it&#x27;s not \"just fiction\" and the art we create and consume changes us, and changes society, not always in a good way. reply retrocryptid 15 hours agorootparentI&#x27;ve often thought that if the American libertarian party renamed themselves \"The LeGuin Party\" they might get a lot more members. reply runeofdoom 14 hours agorootparentSuch members would be unlikely to support their \"I&#x27;ve got mine, fuck you\" ideology. reply darthrupert 1 hour agorootparentYou&#x27;re thinking of communists, establishment conservatists and wealthy progressives. Libertarians are the opposite of that. reply morelisp 12 hours agorootparentprevThe converse of this actually exists. There&#x27;s a fan award called the Prometheus Award given by the \"Libertarian Futurists\" which is often jokingly called the \"Scottish Socialist Award\", given how many times they&#x27;ve awarded it to works that are perhaps equally anti-extant-state but with virtually all other politics at odds. (And in some cases, IMO, strong misreadings of the awarded work.) reply danidiaz 15 hours agoprevI would also recommend the site \"Science Fiction Ruminations\": https:&#x2F;&#x2F;sciencefictionruminations.com&#x2F;Lots of thoughtful sf book reviews with a heavy (but not exclusive) focus on the New Wave. reply ranprieur 15 hours agoprevIt&#x27;s nice to see both Engine Summer and In Watermelon Sugar on this list. If you like mellow postapocalpyse, check out Hitoshi Ashinano&#x27;s manga Yokohama Kaidashi Kikou. reply retrocryptid 15 hours agoparentHmm. Just thinking about west meeting east... Hiroshima Mon Amour is, in many ways, a cyberpunk work in the way the concept of memory as a first-class character with its own life is in clear display. reply deng 14 hours agoprevGood list, though of course I have only read maybe half, if even. A bit too heavy on Dick and Ballard for my taste, but I&#x27;m very happy to see James Tiptree Jr. (Alice Sheldon) and Octavia Butler on there, which are too often forgotten reply DougMerritt 13 hours agoparentJames Tiptree Jr (yes, aka Alice Sheldon) is often forgotten? That would be criminal! reply kramerger 16 hours agoprevHugged to death alreadyhttps:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231109133450&#x2F;https:&#x2F;&#x2F;www.hilob... reply worik 13 hours agoprevI am unconvincedPhillip K Dick wrote some absolute classics. And some utter dross. In this periodI do think there is a rough arc of transition, but I disagree that there are definable agesIt is good sport of arbitrary classification but has no useful meaning in selecting booksAx a counter example there is TV, where technology defined clear boundaries cable then streamingNothing, not even kindle, has hat such an effect in literature. It is gradual change from the 1940s through the twenty first century. 1940 very different from 2023, but no boundaries reply root_axis 15 hours agoprevNeeds more Gene Wolfe :) reply bouvin 12 hours agoparentAny SF list is incomplete without him! However, he was never New Wave, which this list is. reply aardvark179 11 hours agorootparentBut it does include The Fifth Head of Cerberus. I think only work up to Book of the New Sun would fall into the specified time period, so most of his work would qualify even if it were new wave. reply Finnucane 11 hours agorootparentprevA lot of his early short stories were pretty new wavish. He did, after all claim that Damon Knight grew him from a bean. reply vodou 15 hours agoprevI really love this era of sci-fi, a kind of short window between the immature boy fantasies before and the dull hard SF the came after. The weirdness, the hallucinatory, the craziness! And most importantly, so many great writers. reply andybak 14 hours agoparentDoesn&#x27;t Hard SF both predate and postdate New Wave? It&#x27;s a thread that weaves through the whole of science-fiction. (heck - didn&#x27;t Jules Verne once diss H.G. Wells for not being \"hard sci-fi enough\"?) reply cromniomancer 4 hours agoparentprevAnother fun offshoot combining New Wave and Cyberpunk is Transrealism. Rudy Rucker has a way with illustrating immense mathematical ideas while keeping the zany hippie dreams alive.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Transrealism_(literature) reply windowshopping 15 hours agoparentprevEnders game, speaker for the dead, ringworld, and many others are post-1983. Are those dull hard sci fi? reply demondemidi 15 hours agorootparent\"Ender&#x27;s Game\" is a little boy fantasy, almost literally. reply zem 12 hours agorootparentprevnot to mention the vorkosigan series! reply ur-whale 15 hours agorootparentprev> Enders game, speaker for the deadTruly not in the same category as stuff from e.g. PKD, by a very wide margin, especially speaker for the dead. reply windowshopping 12 hours agorootparentWell, matter of opinion, and judging by critics&#x27; lists your opinion is the minority. reply runeofdoom 14 hours agoparentprevIf you&#x27;re seeking weirdness and hallucinatory craziness, I think Peter Watts, Tamsyn Muir, and Charles Stross (among many others) can absolutely deliver. reply ur-whale 15 hours agoparentprev> the dull hard SF the came afterTo be fair to the crowd that came after, the average consumption of creativity-enhancing substances was a lot less in the 80&#x27;s and forward than in the [60&#x27;s, early 80&#x27;s] period. reply tuatoru 8 hours agoprevStand On Zanzibar is only number 19? No Report On Probability A?Well, that&#x27;s just, like, your opinion, man. Good to see many old favourites though. reply daoboy 15 hours agoprevIt is interesting to see how a handful of writers (especially Phillip K Dick!) really dominated the genre.Also a few forgotten gems I am going to have to reread now. reply PaulHoule 15 hours agoparentI like the list.It&#x27;s the kind of list an English professor might make, not necessarily a sci-fi fan. In particular, Dick&#x27;s genius was recognized only after his death.Even though sci-fi sometimes rises above other genre literature, it&#x27;s not taken that seriously by literary types. Baudrillard talks a lot about Dick and Ballard and that&#x27;s it. Stanislaw Lem (whose Cyberiad ought to be on that list instead of The Futurological Congress) himself said that Phillip K. Dick was on a whole other level than other sci-fi writers.Notably the list subverts the category of \"novel\" by adding a lot of things (comic books, concept albums) that aren&#x27;t really novels.I think Fred Pohl is missing (say Gateway) and also Joe Halderman (Worlds!). I&#x27;d rather see any Frank Herbert book but Dune (say Whipping Star or The Santaroga Barrier.) For that matter I&#x27;d like to see something from Heinlein before he had the stroke like The Moon is a Harsh Mistress or Glory Road. I&#x27;d also want to see Niven&#x27;s Neutron Star (a set of short stories that read like a novel) but despite some \"new wave\" sensibilities Niven comes across as reactionary today because the Tor Books revival (Forge of God and Enders Game) of sci-fi in the 1980s liked Heinlein and Niven and Doc Smith better than LeGunn, Dick, Ballard, Herbert, etc.On the other hand, put a sci-fi fan in charge of making this list and it&#x27;s no doubt they&#x27;ll add a stick of books from some author who is a \"no account\" in literary circles like Piers Anthony -- still I would try to slip him in at his most psychedelic. reply DougMerritt 13 hours agorootparent> Dick&#x27;s genius was recognized only after his deathYes and no. It should certainly be noted that he was very popular during his life, not a nearly unknown author. Book clubs selected multiple of his novels, as one of many examples of that.His stuff became a huge fad when they made many movies based on his works, but that&#x27;s not the same thing as him previously being an unsung genius. reply Finnucane 11 hours agorootparentPKD’s books didn’t generally sell all that well during his life. In those days the sfbc picked up everything. And most of his books were OP when he died. I can remember having to scrounge around for them. reply DougMerritt 10 hours agorootparentHmm. In light of that, maybe it would have been more accurate if I said he was famous rather than popular, and for all I know it was only the Science Fiction Book Club that gave him that degree of fame?His stuff is weird, and therefore a niche&#x2F;acquired taste. reply routerl 15 hours agorootparentprev> Dick&#x27;s genius was recognized only after his death.Which we should give credit for: I only found PKD due to Jonathan Lethem&#x27;s advocacy. PKD fans are likely to enjoy Lethem&#x27;s sci-fi, especially Gun, With Occasional Music.The crew of writers responsible for elevating PKD from pulp to cult deserve some attention, especially from PKD fans (aka \"Dick-heads\", which is just the most perfect name for a fandom).> I&#x27;d like to see something from Heinlein before he had the stroke like The Moon is a Harsh MistressPlus one for The Moon is a Harsh Mistress. That&#x27;s probably the best of Heinlein&#x27;s work. reply deng 14 hours agorootparentprevThis is a list for New Wave SciFi, not all SciFi, and Pohl and Halderman are usually not considered as \"New Wave\" authors. reply mbb70 15 hours agoparentprevPkd was a genius, an addict, and most importantly, got paid by the page.Jonathan Letham&#x27;s Amnesia Moon (mentioned at the bottom) is my favorite \"how pkd would write if he weren&#x27;t ripped on speed 24&#x2F;7\"Guess it&#x27;s time to reread it. And ubik, and fmytps, and the lathe of heaven, and hyperion, and embassytown and... reply sonofhans 13 hours agorootparentIf you haven’t, check out “Philip K Dick is Dead, Alas,” by Micheal Bishop. Same vibes — Dick without the speed :) reply kramerger 15 hours agoparentprevI feel this was the winter of sci-fi.I enjoyed Dune and Androids but the rest are mostly meh. A lot of DNF books for me (including Slaughterhouse-Five, which I know has its fans but I really disliked) reply sctb 15 hours agorootparentI&#x27;m curious to hear more about why you feel this was the winter of sci-fi! Can you share some more besides personal preference? (I&#x27;m not a huge Vonnegut fan or even big reader in general and Slaughterhouse-Five and Cat&#x27;s Cradle were pretty far from DNF for me.) reply ajmurmann 15 hours agorootparentprevI feel similarly. However, I absolutely love Vonnegut, but don&#x27;t see his work as scifi. Sure it has all the elements, but the themes are mostly different. To me the best scifi is about a technology that could exist and plays through all the consequences. Slaughterhouse Five isn&#x27;t that, it&#x27;s about WWII. (From my definition one might guess correctly that I see space opera as a derogatory term) reply kramerger 12 hours agorootparentI heard that his writing could have been a way to process what he went through during the war. That could explain the darkness.There is a modern fantasy writer that writes quite dark stories. I read that this is because of a very sick family member, and some of his best books have been written in a hospital room. While I applaude both for turning something bad into something good, these sort of books are just not my cup of tea. And I think a lot of people are like me. We read books to get away from the problems of real life, not dive into other people&#x27;s troubled minds. reply ajmurmann 12 hours agorootparent\"And I think a lot of people are like me. We read books to get away from the problems of real life, not dive into other people&#x27;s troubled minds.\"That certainly makes sense and is of course also a subjective preference. To me great insights lie in the darkness, but it isn&#x27;t for everyone and not at any time. reply aardvark179 15 hours agorootparentprevI couldn’t disagree more. The new wave is where SF starts to get interesting for me, and is when a lot of my favourite authors got started. reply corethree 15 hours agoprev15. Rocannon’s World has a stupid tagline in the title. Ignore it.This book is short and is supposed to be the first part of a novella called \"worlds of exile and illusion\" which consists of a trilogy of stories.Read the novella of 3 stories in order. Do not just read rocanonns world individually.When read Separately the stories are your standard sci Fi fare but when wholistically read the trilogy becomes one of the most epic stories I have ever read. All three stories seem only slightly connected but as a trilogy all three stories combine to form a completely different epic tale that spans the entire universe.I didn&#x27;t really like Ursula&#x27;s left hand of darkness. It was ok, but worlds of exile and illusion blew me away. But for some reason most people don&#x27;t know about this gem and I think it&#x27;s because the books were initially released separately.The whole thing needs to be read in order as one book, the length of each story actually makes it better fit for a single book too, link:Worlds of Exile and Illusion (Hainish) https:&#x2F;&#x2F;a.co&#x2F;d&#x2F;0jtS7ZbGuaranteed there&#x27;s nothing else like it in sci Fi both on television and in other sci Fi books. reply zem 12 hours agoparentI read the entire hainish cycle back to back in chronological order once, and it was a superb experience. the books aren&#x27;t technically a series but they do build up a coherent universe and timeline between them. reply corethree 11 hours agorootparentI would say the first three books form a series. An underlying story of a great interstellar war and how that war was first lost than won.Of course you won&#x27;t be able to put all of this together until the last book in the trilogy, the last chapter.The other books can be read out of order, but the first three are a trilogy. reply gcanyon 14 hours agoprevMaking my pitch for John Varley&#x27;s work. The Eight Worlds setup is rich, interesting, and mind-expanding. reply ryantgtg 11 hours agoparentAnd it’s still going! It was refreshing to have a new book set in that world. I’m a big fan, and think a few more of his books should have been on this list. reply gcanyon 10 hours agorootparentHumble brag, but I started the wikipedia page on Varley. (I wasn&#x27;t logged in at the time, but that was my IP address!)And I owe it all to my sister, who gave me Wizard -- not Titan :-) -- for Christmas one year. reply ryantgtg 7 hours agorootparentI owe it all to an English teacher in college who photocopied Equinoctial for us (this was about 1998). I loved it, and quickly proceeded to devour his novels. reply gcanyon 10 hours agorootparentprevWait -- is there something more recent than Irontown Blues? reply ryantgtg 7 hours agorootparentOK time flies. That felt like two years ago to me. reply ur-whale 15 hours agoprevNo mention of any piece by Jack Vance ?For something to claim to cover the golden age of Sci-Fi ?Very weird. reply andybak 14 hours agoparentNew Wave has a specific meaning and it&#x27;s one that by definition excludes another specific period (\"Golden Age\")I&#x27;m not quite sure whether Jack Vance is regarded as part of either?EDIT - ah - further down the page there&#x27;s a mention of Golden Age. But it seems to only be in terms of precursors and influences on New Wave:> The following titles from science fiction’s so-called Golden Age (1934–1963) are listed here in order to provide historical context. reply deng 14 hours agorootparentYes, this is not a list of \"Best SciFi books from 1964 to 1983\", it is specifically meant to cover the subgenre of New Wave. This is also why for instance \"Flowers for Algernon\" or \"The forever war\" is not there, since they are not considered New Wave but are for sure classics that would be on a generic \"Best SciFi\" list. reply retrocryptid 15 hours agoprevMan. Someone really likes PKD. And sure, his stories are great reality-benders. Maybe we could use a little bit more of that these days. And I love Ballard. I think PKD is slightly over-represented on this list and Ursula K Leguin slightly under-represented. But I&#x27;m happy to see some of the other authors made the cut.I think if I were to do a list like this I would pick 10 or 12 authors from the era and mention which works express the literary zeitgeist.Dang. Maybe I SHOULD do that. reply norir 11 hours agoparentIt is wild that PKD and UKLG both went to Berkeley HS at the same time (though UKLG has said that neither she nor any of her friends had any memory of him). reply Finnucane 10 hours agoprevNo R. A. Lafferty or David R. Bunch. Maybe that’s too far outside the lines for this guy. reply thriftwy 15 hours agoprevThere&#x27;s no Mote In God&#x27;s Eye...It seems to match the time frame and it&#x27;s one of the best books known to me. reply aardvark179 14 hours agoparentIt is also very definitely not new wave. reply thriftwy 13 hours agorootparentIt is a very advanced book from literary standpoint. The new wave is now an \"old\" wave with all the weird stuff, but Niven would read as fresh today as back in the 70s. reply everyone 14 hours agoprev [–] I appreciate any list of sci-fi books, and it&#x27;s interesting to see, but for me, this list sucks. It&#x27;s very fantasy &#x2F; literature heavy as opposed to hard sci-fi. Seems like a list of \"sci-fi\" books from someone who doesnt really like sci-fi. For me it&#x27;s like a bottom 75 list. reply bouvin 12 hours agoparent&#x27;Bottom 75 list&#x27;?You have not read much SF, have you? Sturgeon&#x27;s Law [1] was created for a reason. If you think this is as bad as SF gets, I turn your attention to Robert A. Heinlein&#x27;s \"To Sail Beyond the Sunset\".[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sturgeon&#x27;s_law reply tuatoru 8 hours agoparentprevIf you don&#x27;t like any of these, you don&#x27;t like science fiction. As Fred Pohl said, \"the task of the science fiction writer is not to imagine the automobile but the traffic jam\".Perhaps you prefer \"cowboys in space\", like Star Trek, Star Wars, Terminator, or Firefly. reply deng 14 hours agoparentprev [–] It says \"New Wave SciFi\" in the title, so it only covers \"New Wave Scifi\". It&#x27;s perfectly fine to not like the genre, of course, but that&#x27;s not a fault of this list. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The summary gives an overview of science fiction novels, including works from the New Wave era and other time periods.",
      "It mentions notable authors and their books, providing brief descriptions of each plot and themes.",
      "The summary highlights the unique elements and influences of each work, giving readers a sense of the diverse range of science fiction literature."
    ],
    "commentSummary": [
      "Users are engaging in a discussion about science fiction novels from 1964-1983, specifically focusing on the subgenre of \"New Wave Science Fiction.\"",
      "Participants share their opinions and experiences with various books and authors, as well as discussing the evolution of science fiction genres and the impact of technology on literature.\"",
      "The conversation highlights the importance of recognizing forgotten authors, the influence of certain writers like Philip K. Dick, and the diversity and creativity of science fiction during this period. Some participants express dissatisfaction with the recommended list, while others defend it as a true reflection of the genre.\""
    ],
    "points": 153,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1700932866
  },
  {
    "id": 38418359,
    "title": "The Frustrations of Packaging Software with Docker in Linux Distributions",
    "originLink": "https://computer.rip/2023-11-25-the-curse-of-docker.html",
    "originBody": ">>> 2023-11-25 the curse of docker (PDF) I'm heading to Las Vegas for re:invent soon, perhaps the most boring type of industry extravaganza there could be. In that spirit, I thought I would write something quick and oddly professional: I'm going to complain about Docker. Packaging software is one of those fundamental problems in system administration. It's so important, so influential on the way a system is used, that package managers are often the main identity of operating systems. Consider Windows: the operating system's most alarming defect in the eyes of many \"Linux people\" is its lack of package management, despite Microsoft's numerous attempts to introduce the concept. Well, perhaps more likely, because of the number of those attempts. And still, in the Linux world, distributions are differentiated primarily by their approach to managing software repositories. I don't just mean the difference between dpkg and rpm, but rather more fundamental decisions, like opinionated vs. upstream configuration and stable repositories vs. a rolling release. RHEL and Arch share the vast majority of their implementation and yet have very different vibes. Linux distributions have, for the most part, consolidated on a certain philosophy of how software ought to be packaged, if not how often. One of the basic concepts shared by most Linux systems is centralization of dependencies. Libraries should be declared as dependencies, and the packages depended on should be installed in a common location for use of the linker. This can create a challenge: different pieces of software might depend on different versions of a library, which may not be compatible. This is the central challenge of maintaining a Linux distribution, in the classical sense: providing repositories of software versions that will all work correctly together. One of the advantages of stable distributions like RHEL is that they are very reliable in doing this; one of the disadvantages is that they achieve that goal by packaging new versions very infrequently. Because of the need to provide mutually compatible versions of a huge range of software, and to ensure compliance with all kinds of other norms established by distributions (which may range from philosophical policies like free software to rules on the layout of configuration files), putting new software into Linux distributions can be... painful. For software maintainers, it means dealing with a bunch of distributions using a bunch of old versions with various specific build and configuration quirks. For distribution and package maintainers, it means bending all kinds of upstream software into compliance with distribution policy and figuring out version and dependency problems. It's all a lot of work, and while there are some norms, in practice it's sort of a wild scramble to do the work to make all this happen. Software developers that want their software to be widely used have to put up with distros. Distros that want software have to put up with software developers. Everyone gets mad. Naturally there have been various attempts to ease these problems. Naturally they are indeed various and the community has not really consolidated on any one approach. In the desktop environment, Flatpak, Snap, and AppImage are all distressingly common ways of distributing software. The images or applications for these systems package the software complete with its dependencies, providing a complete self-contained environment that should work correctly on any distribution. The fact that I have multiple times had to unpack flatpaks and modify them to fix dependencies reveals that this concept doesn't always work entirely as advertised, but to be fair that kind of situation usually crops up when the software has to interact with elements of the system that the runtime can't properly isolate them from. The video stack is a classic example, where errant OpenGL libraries in packages might have to be removed or replaced for them to function with your particular graphics driver. Still, these systems work reasonably well, well enough that they continue to proliferate. They are greatly aided by the nature of the desktop applications for which they're used (Snapcraft's system ambitions notwithstanding). Desktop applications tend to interact mostly with the user and receive their configuration via their own interface. Limiting the interaction surface mostly to a GUI window is actually tremendously helpful in making sandboxing feasible, although it continues to show rough edges when interacting with the file system. I will note that I'm barely mentioning sandboxing here because I'm just not discussing it at the moment. Sandboxing is useful for security and even stability purposes, but I'm looking at these tools primarily as a way of packaging software for distribution. Sandboxed software can be distributed by more conventional means as well, and a few crusty old packages show that it's not as modern of a concept as it's often made out to be. Anyway, what I really wanted to complain a bit about is the realm of software intended to be run on servers. Here, there is a clear champion: Docker, and to a lesser degree the ecosystem of compatible tools like Podman. The release of Docker lead to a surprisingly rapid change in what are widely considered best practices for server operations. While Docker images a means of distributing software first seemed to appeal mostly to large scalable environments with container orchestration, it sort of merged together with ideas from Vagrant and others to become a common means of distributing software for developer and single-node use as well. Today, Docker is the most widespread way that server-side software is distributed for Linux. I hate it. This is not a criticism of containers in general. Containerization is a wonderful thing with many advantages, even if the advantages over lightweight VMs are perhaps not as great as commonly claimed. I'm not sure that Docker has saved me more hours than it's cost, but to be fair I work as a DevOps consultant and, as a general rule, people don't get me involved unless the current situation isn't working properly. Docker images that run correctly with minimal effort don't make for many billable hours. What really irritates me these days is not really the use of Docker images in DevOps environments that are, to some extent, centrally planned and managed. The problem is the use of Docker as a lowest common denominator, or perhaps more accurately lowest common effort, approach to distributing software to end users. When I see open-source, server-side software offered to me as a Docker image or--even worse---Docker Compose stack, my gut reaction is irritation. These sorts of things usually take longer to get working than equivalent software distributed as a conventional Linux package or to be built from source. But wait, how does that happen? Isn't Docker supposed to make everything completely self-contained? Let's consider the common problems, something that I will call my Taxonomy of Docker Gone Bad. Configuration One of the biggest problems with Docker-as-distribution is the lack of consistent conventions for configuration. The vast majority of server-side Linux software accepts its configuration through an ages-old technique of reading a text file. This certainly isn't perfect! But, it is pretty consistent in its general contours. Docker images, on the other hand... If you subscribe to the principles of the 12-factor-app, the best way for a Docker image to take configuration is probably via environment variables. This has the upside that it's quite straightforward to provide them on the command line when starting the container. It has the downside that environment variables aren't great for conveying structured data, and you usually interact with them via shell scripts that have clumsy handling of long or complicated values. A lot of Docker images used in DevOps environments take their configuration from environment variables, but they tend to make it a lot more feasible by avoiding complex configuration (by assuming TLS will be terminated by \"someone else\" for example) or getting a lot of their configuration from a database or service on the network. For most end-user software though, configuration is too complex or verbose to be comfortable in environment variables. So, often, they fall back to configuration files. You have to get the configuration file into the container's file system somehow, and Docker provides numerous ways of doing so. Documentation on different packages will vary on which way it recommends. There are frequently caveats around ownership and permissions. Making things worse, a lot of Docker images try to make configuration less painful by providing some sort of entry-point shell script that generates the full configuration from some simpler document provided to the container. Of course this level of abstraction, often poorly documented or entirely undocumented in practice, serves mostly to make troubleshooting a lot more difficult. How many times have we all experienced the joy of software failing to start, referencing some configuration key that isn't in what we provided, leading us to have to find have the Docker image build materials and read the entrypoint script to figure out how it generates that value? The situation with configuration entrypoint scripts becomes particularly acute when those scripts are opinionated, and opinionated is often a nice way of saying \"unsuitable for any configuration other than the developer's.\" Probably at least a dozen times I have had to build my own version of a Docker image to replace or augment an entrypoint script that doesn't expose parameters that the underlying software accepts. In the worst case, some Docker images provide no documentation at all, and you have to shell into them and poke around to figure out where the actual configuration file used by the running software is even located. Docker images must always provide at least some basic README information on how the packaged software is configured. Filesystems One of the advantages of Docker is sandboxing or isolation, which of course means that Docker runs into the same problem that all sandboxes do. Sandbox isolation concepts do not interact well with Linux file systems. You don't even have to get into UID behavior to have problems here, just a Docker Compose stack that uses named volumes can be enough to drive you to drink. Everyday operations tasks like backups, to say nothing of troubleshooting, can get a lot more frustrating when you have to use a dummy container to interact with files in a named volume. The porcelain around named volumes has improved over time, but seemingly simple operations can still be weirdly inconsistent between Docker versions and, worse, other implementations like Podman. But then, of course, there's the UID thing. One of the great sins of Docker is having normalized running software as root. Yes, Docker provides a degree of isolation, but from a perspective of defense in depth running anything with user exposure as root continues to be a poor practice. Of course this is one thing that often leads me to have to rebuild containers provided by software projects, and a number of common Docker practices don't make it easy. It all gets much more complicated if you use hostmounts because of UID mapping, and slightly complex environments with Docker can turn into NFS-style puzzles around UID allocation. Mitigating this mess is one of the advantages to named volumes, of course, with the pain points they bring. Non-portable Containers The irony of Docker for distribution, though, and especially Docker Compose, is that there are a lot of common practices that negatively impact portability---ostensibly the main benefit of this approach. Doing anything non-default with networks in Docker Compose will often create stacks that don't work correctly on machines with complex network setups. Too many Docker Compose stacks like to assume that default, well-known ports are available for listeners. They enable features of the underlying software without giving you a way to disable them, and assume common values that might not work in your environment. One of the most common frustrations, for me personally, is TLS. As I have already alluded to, I preach a general principle that Docker containers should not terminate TLS. Accepting TLS connections means having access to the private key material. Even if 90-day ephemeral TLS certificates and a general atmosphere of laziness have deteriorated our discipline in this regard, private key material should be closely guarded. It should be stored in only one place and accessible to only one principal. You don't even have to get into these types of lofty security concerns, though. TLS is also sort of complicated to configure. A lot of people who self-host software will have some type of SNI or virtual hosting situation. There may be wildcard certificates for multiple subdomains involved. All of this is best handled at a single point or a small number of dedicated points. It is absolutely maddening to encounter Docker images built with the assumption that they will individually handle TLS. Even with TLS completely aside, I would probably never expose a Docker container with some application directly to the internet. There are too many advantages to having a reverse proxy in front of it. And yet there are Docker Compose stacks out there for end-user software that want to use ACME to issue their own certificate! Now you have to dig through documentation to figure out how to disable that behavior. The Single-Purpose Computer All of these complaints are most common with what I would call hobby-tier software. Two examples that pop into my mind are HomeAssistant and Nextcloud. I don't call these hobby-tier to impugn the software, but rather to describe the average user. Unfortunately, the kind of hobbyist that deploys software has had their mind addled by the cheap high of the Raspberry Pi. I'm being hyperbolic here, but this really is a problem. It's absurd the number of \"self-hosted\" software packages that assume they will run on dedicated hardware. Having \"pi\" in the name of a software product is a big red flag in my mind, it immediately makes me think \"they will not have documented how to run this on a shared device.\" Call me old-fashioned, but I like my computers to perform more than one task, especially the ones that are running up my power bill 24/7. HomeAssistant is probably the biggest offender here, because I run it in Docker on a machine with several other applications. It actively resists this, popping up an \"unsupported software detected\" maintenance notification after every update. Can you imagine if Postfix whined in its logs if it detected that it had neighbors? Recently I decided to give NextCloud a try. This was long enough ago that the details elude me, but I think I burned around two hours trying to get the all-in-one Docker image to work in my environment. Finally I decided to give up and install it manually, to discover it was a plain old PHP application of the type I was regularly setting up in 2007. Is this a problem with kids these days? Do they not know how to fill in the config.php? Hiding Sins Of course, you will say, none of these problems would be widespread of people just made good Docker images. And yes, that is completely true! Perhaps one of the problems with Docker is that it's too easy to use. Creating an RPM or Debian package involves a certain barrier to entry, and it takes a whole lot of activation energy for even me to want to get rpmbuild going (advice: just use copr and rpkg). At the core of my complaints is the fact that distributing an application only as a Docker image is often evidence of a relatively immature project, or at least one without anyone who specializes in distribution. You have to expect a certain amount of friction in getting these sorts of things to work in a nonstandard environment. It is a palpable irony, though, that Docker was once heralded as the ultimate solution to \"works for me\" and yet seems to just lead to the same situation existing at a higher level of configuration. Last Thoughts This is of course mostly my opinion and I'm sure you'll disagree on something, like my strong conviction that Docker Compose was one of the bigger mistakes of our era. Fifteen years ago I might have written a nearly identical article about all the problems I run into with RPMs created by small projects, but what surprises me about Docker is that it seems like projects can get to a large size, with substantial corporate backing, and still distribute in the form of a decidedly amateurish Docker Compose stack. Some of it is probably the lack of distribution engineering personnel on a lot of these projects, since Docker is \"simple.\" Some of it is just the changing landscape of this class of software, with cheap single-board computers making Docker stacks just a little less specialized than a VM appliance image more palatable than they used to be. But some if it is also that I'm getting older and thus more cantankerous.",
    "commentLink": "https://news.ycombinator.com/item?id=38418359",
    "commentBody": "The Curse of DockerHacker NewspastloginThe Curse of Docker (computer.rip) 151 points by sklargh 8 hours ago| hidepastfavorite91 comments zaptheimpaler 3 hours agoNah docker is excellent and far far preferable to the standard way of doing things for most people.All the traditional distribution methods have barely even figured out how to uninstall a piece of software. `apt remove` will often leave other things lying around.He complains about configuration being more complex because its not a file? Except it is, and its so much simpler to just have a compose file that tells you EXACTLY which files are used for configuration and where they are. This is still not easy in normal linux packages. You have to google or dig through a list of 10-15 places that may be used for config..The other brilliant opinion is that docker makes the barrier to entry for packaging too low.. but the alternative is not those things being packaged well in apt or whatever, its them not being packaged at all.. this is not a win.Whats the alternative to running a big project like say Sourcegraph through a docker compose instance? You have to get set up ~10 services yourself including Redis, Postgres, some logging thing blah blah. I do not believe this is ever easier than `docker compose up -d`.And if you want to run it without docker, the docker images are basically a self-documenting system for how to do that. This is strictly a win over previous systems, where there generally just is no documentation for that.Personally docker lowers the activation energy to deploy something that I can now try&#x2F;run so many complex pieces of software so easily. I run sourcegraph, nginx, postgres, redis on my machine easily. A week ago I wanted to learn more about data engineering - got a whole Apache Airflow cluster setup with a single compose file, took a few minutes. That would have been at least an hour of following some half-outdated deploy guide before so I just wouldn&#x27;t have done it.---The beginning of the post is most revealing though:> The fact that I have multiple times had to unpack flatpaks and modify them to fix dependencies reveals ... Still, these systems work reasonably well, well enough that they continue to proliferate...Basically you are okay with opening up flatpaks to modify them but not opening up docker images.. it just comes down to familiarity. reply bandrami 4 minutes agoparentWait wait wait. Docker has two use cases and you&#x27;re conflating them. The original use case is:Project Foo uses libqxt version 7. Project Bar uses libqxt version 8. They are incompatible so I&#x27;d need two development workstations (or later two LXC containers). This is slow and heavy on diskspace; docker solves that problem. This is a great use of docker.The second use case that it has morphed into is:I&#x27;ve decided stack management is hard and I can&#x27;t tell my downstream which libraries they need because even I&#x27;m no longer sure. So I&#x27;ll just bundle them all as this opaque container and distribute it that way and now literally nobody knows what versions of what software they are running in production. This is a very harmful use case of docker that is unfortunately nearly universal at this point. reply kbenson 2 hours agoparentprevThe thing I&#x27;m always interested in finding out is how people using lots of containers deal with security updates.Do they go into every image and run the updates for the base image it&#x27;s based on? Does that just mean you&#x27;re now subject to multiple OS security and patching and best practices?Do they think it doesn&#x27;t matter and just get new images then they&#x27;re published, which from what I can see is just when the overplayed software has an update and not the system anduvraries it relies on?When glibc or zlib have a security update, what do you do? For RHEL and derivatives it looks line quay.io tries to help with that, but what&#x27;s the best practice for the rest of the community?We haven&#x27;t really adopted containers at all at work yet, and to be honest this is at least part of the reason. It feels like we&#x27;d be doing all the patching we currently are, and then a bunch more. Sure, there are some gains, but if also feels like there&#x27;s a lot more work involved there too? reply rbut 19 minutes agorootparentWe use What&#x27;s Up Docker [1] to monitor for new versions of docker containers that are created by others (eg. self hosted apps).For containers we create ourselves, we automatically rebuild them each night which pulls the latest security updates.[1] https:&#x2F;&#x2F;github.com&#x2F;fmartinou&#x2F;whats-up-docker reply zaptheimpaler 1 hour agorootparentprevSo i primarily use containers on my local machine walled off from the internet, so it&#x27;s not a big concern for me. Watchtower [1] is popular among home server users too which automatically updates containers to the latest image.For production uses I think companies generally build their own containers. They would have a common base linux container and build the other containers based off that with a typical CI&#x2F;CD pipeline. So if glibc is patched, it&#x27;s probably patched in the base container and the others are then rebuilt. You don&#x27;t have to patch each container individually, just the base. Once the whole build pipeline is automated its not too hard to add checks for security updates and rebuild when needed. Production also minimizes the scope of containers with nothing installed except what&#x27;s necessary so they have few dependencies.[1] https:&#x2F;&#x2F;github.com&#x2F;containrrr&#x2F;watchtower reply slekker 1 hour agorootparentprevYou can use tools like Snyk to scan images for vulnerabilities (even the Docker(tm) cli tool has one now), you can do things like failing your CI pipeline if there are critica vulnerabilities.You can also use Dependabot (and others) to update your images on a cronjob-like schedule.This is a solved problem reply Yasuraka 58 minutes agorootparentI highly advise going with Trivy over Snyk or, as it&#x27;s still in beta at the moment, Docker Scout. reply SkiFire13 1 hour agorootparentprevThis is a solution for the app developer, not for the users that might have to deal with older and unsupported apps. reply chillfox 1 hour agorootparentprevDon’t use glibc…I am of course partially joking.But seriously, use musl libc, build static binaries, build the images from scratch, and have a CI server handle it for you to keep it updated.Alternatively use a small image like Alpine as base if you want some tools in the image for debugging. reply otabdeveloper4 46 minutes agorootparentprev> ...how people using lots of containers deal with security updates.TL&#x2F;DR: they don&#x27;t. #yolo, don&#x27;t be a square. reply progval 1 hour agoparentprev> Whats the alternative to running a big project like say Sourcegraph through a docker compose instance? You have to get set up ~10 services yourself including Redis, Postgres, some logging thing blah blah. I do not believe this is ever easier than `docker compose up -d`.Even with Docker, it&#x27;s not as easy as `docker compose up -d`.You need to setup backups too. How do you backup all these containers? Do they even support it? Meanwhile I already have Postgres and Redis tuned, running, and backuped. I&#x27;d even have a warm Postgres replica if I could figure how to make that work.Then you need to monitor them for security updates. How are you notified your Sourcegraph container and its dependencies&#x27; containers need to be updated and restarted? If they used system packages, I&#x27;d already have this solved with Munin&#x27;s APT plugin and&#x2F;or Debian&#x27;s unattended-upgrades. So you need to install Watchtower or equivalent. Which can&#x27;t tell the difference between a security update and other updates, so you might have your software updated with breaking changes at any point.Alternatively, you can locate and subscribe to the RSS feed for the repository of every Dockerfile involved (or use one of these third-party services providing RSS feeds for Dockerhub) and hope they contain a changelog. If I installed from Debian I wouldn&#x27;t need that because I&#x27;m already subscribed to debian-security-announce@lists.debian.org reply chillfox 1 hour agoparentprevExactly, Docker + Compose is the best way of running server software these days.I keep my compose files in source control and I have got a CI server building images for anything that doesn’t have first party images available.Updates are super easy as well, just update the pinned version at the top of the compose file (if not using latest), then ’docker-compose pull’ followed by ’docker-compose up -d’The entire thing is so much more stable and easier to manage than the RedHat&#x2F;Ubuntu&#x2F;FreeBSD systems I used to manage.(I use Alpine Linux + ZFS for the host OS) reply rigid 25 minutes agoparentprev> All the traditional distribution methods have barely even figured out how to uninstall a piece of software. `apt remove` will often leave other things lying around.Sounds like an issue that needs to be fixed instead of working around it. Also dependency hell. Few distros manage those hard problems nicely but they do exist. reply angarg12 3 hours agoparentprevI spent the last couple of days trying to set up some software waddling though open source repositories, fixing broken deps, pinning minor versions, browsing SO for obscure errors... I wish I had a Docker container instead. reply bakuninsbart 51 minutes agoparentprev> A week ago I wanted to learn more about data engineering - got a whole Apache Airflow cluster setup with a single compose file, took a few minutes.Off-topic, but how did you like it? Tried it out a couple of years ago and felt like it overcomplicates things for probably 99% of use cases, and the overhead is huge. reply ofrzeta 2 hours agoparentprev> figured out how to uninstall a piece of software. `apt remove` will often leave other things lying around.What&#x27;s the Docker way of uninstall? In most cases Docker packaged software uses some kind of volume or local mount to save data. Is there a way to remove these when you remove the container? What about networks? (besides running prune on all available entities) reply zaptheimpaler 2 hours agorootparentYou can `docker rm -v ` to remove a ccontainer + its volumes. For larger stuff typically I just read the docker-compose file with all of that listed in one place, so its pretty easy to just `docker rm` the volumes and networks. With apt I have no idea what files were added or modified and there&#x27;s no simple \"undo\". reply danielheath 43 minutes agorootparentIt’s very nice, although I had a hard time getting the btrfs driver to actually remove the backing store a year or so back. reply ofrzeta 1 hour agorootparentprevI wasn&#x27;t aware of &#x27;-v&#x27; so thanks for that.You can list package content with &#x27;dpkg -L&#x27;, although, granted, it doesn&#x27;t cover files that were somehow created by the install script. Also &#x27;apt purge&#x27; removes all files including config. reply rkagerer 2 hours agoprevYou know who really knows how to package software? Mark Russinovich, Nir Sofer, and all the others who gave us beautiful utililies in standalone EXE&#x27;s that don&#x27;t require any dependencies.For the longest time I stayed on older versions of .NET so any version of Windows since 2003 could run my software out of the box. Made use of ILMerge or a custom AssemblyResolve handler to bundle support DLL&#x27;s right into my single-file tools - it wasn&#x27;t hard.I have no complaints about Docker, but I do find where I used to be able to download simple zip files and place their contents into my project I now just get a black box Docker link with zero documentation and that makes me sad. reply leonheld 6 hours agoprev> Anyway, what I really wanted to complain a bit about is the realm of software intended to be run on servers.Okay.> I&#x27;m not sure that Docker has saved me more hours than it&#x27;s costI&#x27;m not sure what&#x27;s the alternative for servers here. Containers have certainly saved me of a lot of headache and created very little overhead. N=1 (as it seems to be the OP).> The problem is the use of Docker as a lowest common denominator [...] approach to distributing software to end users.Isn&#x27;t the issue specific for server use? Are you running random images from the internet on your servers?> In the worst case, some Docker images provide no documentation at allWell, in the same vein as my last comment, Docker is not a silver bullet for everything. You still have to take care of what you&#x27;re actually running.Honestly the discussion is valid, but I think the OP aimed at \"the current state of things\" and hit a very valuable tool that doesn&#x27;t deserve some of the targeted cristicism I read here.edit: my two cents for those who cannot bother and expect just because it&#x27;s a container, everything will magically be solved: use official images and those from Bitnami. There, you&#x27;re set. reply viraptor 6 hours agoparent> I&#x27;m not sure what&#x27;s the alternative for servers here.Nixos&#x2F;nixpkgs: isolated dependencies &#x2F; services, easy to override if needed, configs follow relatively consistent pattern (main options exposed, others can be passed as text), service files can do isolation by whitelisting paths without going full-blown self-contained-os container.> Are you running random images from the internet on your servers?Many home server users do this. In business use, unless you invest lots of time into this, a part of your services is still effectively a random image from the internet.> and those from BitnamiYes, that&#x27;s a random image from the internet. reply blackoil 5 hours agorootparent> Yes, that&#x27;s a random image from the internet.By that definition, you are running it on a random OS, random processor with some random network infra. reply viraptor 5 hours agorootparentKinda. It depends what your risk tolerance is.But seriously, what&#x27;s your business relationship to bitnami? What are the guarantees about keeping those images up to date? What are the guarantees about the feature set provided? How long will the specific image be available publicly&#x2F;free? Is the base system guaranteed to stay the same? What about architecture support? reply patrick451 4 hours agorootparentprevIt&#x27;s always been surprising to me how little people seem to care about the provenance of their images. It&#x27;s even more surprising that infosec isn&#x27;t forcing developing to start their images `FROM scratch`. reply w-ll 3 hours agorootparentAre you compiling you os distro&#x27;s `FROM scratch`?Ther&#x27;s always a certin level of trust, and for many, docker containers are just as trusty as distros from trusty orgs or volunteers. reply otabdeveloper4 28 minutes agorootparentLike the other person said, Nix solves that problem.You are in control of the entire supply chain with Nix. reply rnimmer 3 hours agorootparentprevThere once was a man named Terry Davis... reply jdwithit 36 minutes agorootparentOr for a less out-there example, Ken Thompson&#x27;s classic \"Reflections on Trusting Trust\". At some point unless you are literally producing all of the hardware and software yourself you have to trust someone. The challenge is figuring out where that line of acceptable risk lies for you. It&#x27;s going to be very different for an indie game dev vs a FinTech company vs the US DoD. reply w-ll 2 hours agorootparentprevI imagine anyone with a BA in CS has wrote a OS from scratch.How many systems on chips are in a moderen computer, not the main system and cpu, but every little chip and controller, the boot system, every board seems to have a little OS.In regards to security, its all about analysing risk and trade-offs.For me using containers from known vendors is a risk im willing to take. reply viraptor 2 hours agorootparent> I imagine anyone with a BA in CS has wrote a OS from scratch.Not even close. Very few courses require anything that advanced and only some of those are non-optional. reply skipnup 3 hours agorootparentprevBut that could be said about each an any dependency.And some (very rare) companies do enforce that, everyone else has to build up a bit of trust. reply alexey-salmin 3 hours agoparentprev> Isn&#x27;t the issue specific for server use? Are you running random images from the internet on your servers?Well exactly, there&#x27;s what the author is writing about.The whole article is dedicated to the problem of Docker being used as a distribution method, that is as a replacement for say Debian package.So in order to use that software you need to run a Docker image from the internet which is open poorly made and incompatible with your infrastructure. Had a package been available you&#x27;d simply do \"apt-get install\" inside your own image built with your infrastructure in mind. reply theden 5 hours agoprev>One of the great sins of Docker is having normalized running software as root. Yes, Docker provides a degree of isolation, but from a perspective of defense in depth running anything with user exposure as root continues to be a poor practice.>Perhaps one of the problems with Docker is that it&#x27;s too easy to useIf you&#x27;ve ever had to make a nonroot docker image (or an image that runs properly with the `--read-only` flag), it&#x27;s not as trivial and fast to get things going—if it was default, perhaps docker wouldn&#x27;t have been so successful in getting engineers of all types and levels to adopt it?It&#x27;s rare to find tooling in the DevOps&#x2F;SRE world that&#x27;s easy to just get started with productively, so docker&#x27;s low barrier to entry is an exception IMO. Yes, the downside is you get a lot of poorly-made `Dockerfiles` in the wild, but it&#x27;s also easy to iterate and improve them, given that there&#x27;s a common ground. It&#x27;s a curse I suppose, but I&#x27;d rather have a well-understood curse than the alternative being an arbitrary amount of bespoke curses. reply numbsafari 3 hours agoparentRunning all your software as root is the IT equivalent of sales guys slapping themselves on the back for selling something for free.It’s all fun and games until the bills come due. reply jillesvangurp 2 hours agoprevI remember how things were before docker. Better is not a word I&#x27;d use for that.It sucked. Deploying software meant dealing with lots of operating system and distribution specific configuration, issues, bugs, etc. All of that had to be orchestrated with complicated scripts. First those were hand written, and later we got things like chef and puppet. Docker wiped most of that out and replaced it with simple build time tools that eliminate the need for having a lot of deploy time tools that take ages to run, are very complex to maintain, etc.I also love to use it for development a lot. It allows me to use lots of different things that I need without having to bother installing those things. Saves a lot of time.Docker gives us a nice standard way to run and configure whatever. Mostly configuration only gets hard when the underlying software is hard to configure. That&#x27;s usually a problem with the software, not with docker. These days if you are building something that requires fiddling with some configuration file, it kind of is broken by design. You should design your configuration with docker in mind and not force people to have to mount volumes just so they can set some properties.The reason docker is so widespread is that it is so obviously a good idea and there hasn&#x27;t been anyone that came along with something better that actually managed to get any traction worth talking about. Most of the docker alternatives tend to be compatible with docker to the point that the differences are mostly in how they run dockerized software.And while I like docker, I think Kubernetes is a hopelessly over engineered and convoluted mess. reply owyn 6 hours agoprevThere are a lot of use cases for docker, I think this is complaining about a few specific things in more complex applications that probably shouldn&#x27;t be deployed that way because you&#x27;re not just exposing port 80, it&#x27;s a whole tool deployed to end users, like an appliance. But that&#x27;s not my primary use case for Docker.For me, the best thing about Docker is that it&#x27;s brought the average developer experience from \"oh, I think I have an install.sh for that around somewhere\" to mostly repeatable builds that are mostly self documenting. Any time a tool is self documenting it&#x27;s a win. If you want the cake, you have to write down the recipe. That&#x27;s huge. It&#x27;s forcing lazy devs (which we all are) to just write it down. At this point the amount of \"weird bearded guy tribal knowledge\" that is now documented in a Dockerfile somewhere is a treasure trove.Things still break all the time for a million dumb reasons but as a least common denominator it&#x27;s a great place to start. It&#x27;s not a solution for everything and it sounds like that&#x27;s what this article is about. Docker+Compose is not great for everything, so don&#x27;t use it for those situations. But it&#x27;s so much better than what was before. reply Onawa 5 hours agoparentWithout reading the article, I would also agree that Docker has made things better for me rather than worse. If someone else spent the effort to create a Dockerfile for their app, it will reduce the amount of issues I have trying to deploy it greatly. At least at that point they have figured out the majority of dependencies required to run their app, then I only have to troubleshoot the details rather than starting from scratch for whatever server distribution that I&#x27;m running it on. reply __MatrixMan__ 2 hours agoparentprev> mostly repeatable builds that are mostly self documenting\"Mostly\" is going a bit far there.If you want those things you should use nix to build your docker images, but you&#x27;re going to have to want them pretty badly. reply ycombinatrix 5 hours agoprev>Doing anything non-default with networks in Docker Compose will often create stacks that don&#x27;t work correctly on machines with complex network setups.I run into this often. Docker networking is a mess. reply j45 5 hours agoparentDepending on the load and use case, encapsulating docker itself in an lxc container or a standalone vm can be a semi maintainable and separated solution. reply NorwegianDude 3 hours agorootparentDocker in lxc is also a huge mess when it comes to storage drivers with some backends... reply heatmiser 5 hours agorootparentprevI like this approach for running multiple services on customer servers&#x2F;clouds. for my own cloud though, i&#x27;ll use some orchestrator that makes sense reply cyrnel 5 hours agoprev> Making things worse, a lot of Docker images try to make configuration less painful by providing some sort of entry-point shell script that generates the full configuration from some simpler document provided to the container.I see this as the container world reinventing the wheel of reasonable defaults for software that has long since lost sight of that. Nginx and Apache are two of the worst offenders, which won&#x27;t just serve files out of a directory without a few dozens lines of config. reply what-the-grump 3 hours agoprevThese sorts of things usually take longer to get working than equivalent software distributed as a conventional Linux package or to be built from source.Yes, but they are done once, and forced to ship the docker image. The stupid amount of time we&#x27;ve spent looking for that one package dependency because someone forgot that they installed something to make a project work... Or the classic we setup SSL, no one knows how they setup SSL once they are done, etc.Docker forces a lot of the infrastructure decisions that devs make in their sandbox to be actually well defined. Not that it makes their choices any more sane, safer, secure. At least someone can take a look at the mess and replicate it, as many time as they want, quickly, break it, fix it, upgrade it, without ever requesting a dev VM build, having sysops install preqs, etc.Is docker work? Everything is work. Do I think docker should be the default go to? No I&#x27;d like for people to use app services and simply perform the build on the app node, but that magic is even harder to debug and troubleshoot. reply chasil 5 hours agoprevThe original method to distribute library- independent software was \"cc --static\" née \"cc -s\"...The world was a much simpler place so long ago. reply mianos 2 hours agoparentOr use applications in golang. Not a user myself, but all the stuff like vault and such are just an executable. What is even funnier is go apps tend to have the simplest containers, for example kaniko seems to be a go app kaniko as init and nothing else.One could also argue that apps that mandate running in docker probably have some ridiculous dependency issues or too clever by half runtime. reply alexey-salmin 3 hours agoprevI think most of the comments here largely miss the point of the article. The guy doesn&#x27;t complain about docker as a whole and doesn&#x27;t criticise it&#x27;s usage for software deployment which normally is the main application.He complains about Docker being used as a software distribution method, that is as a replacement for say Debian package, pip package, npm package etc.So in order to use that software you need to run a Docker image from the internet which is often poorly made and is incompatible with your infrastructure. Had a package been available you&#x27;d simply do \"install\" inside your own image built with your infrastructure in mind.With that I agree completely. Docker and worse even docker-compose are terrible ways of software distribution and should never be used except for demos and for rare cases where software is not distributed to the customer in the normal sense but rather directly deployed into his system.Docker and docker-compose are still very good methods of software deployment. reply solatic 2 hours agoparentThis, but I want to add, I think the root cause is less due to the ease of writing a Dockerfile vs writing a deb, rpm, etc. and more due to the low cost of hosting. Whatever low-quality Dockerfile you write, you can sign up on Docker Hub, build and push there, and you&#x27;re done. GitHub Packages doesn&#x27;t support deb or rpm, and anyway for better or for worse, packages are tightly coupled to the distribution they&#x27;re packaged for. That means either getting your package into the package repository of each distribution you want to target, or hosting your own package repository, which is non-trivial in both financial and labor cost.Docker has a lot more crap, but it did dramatically lower the barrier of entry, which is a good thing. The proper response isn&#x27;t to bemoan the lower barrier of entry, but to attempt to lower the barrier of entry of traditional packaging. reply turtlebits 3 hours agoparentprevI have rarely seen end user software being distributed as a docker image, only server applications. Occasionally I see CLI tools that are hard to package available as docker images, but those are generally done because the maintainer hasn&#x27;t provided precompiled binaries. reply notatoad 4 hours agoprevDockerfiles are really really simple. they do essentially three things: set a base image, set environment variables, and run scripts. and then as sort of a meta-thing, they prove that the steps in the dockerfile actually work, when you start the docker container and see that it works.if you don&#x27;t want to run in docker, a dockerfile is still a perfect setup script. open it up and see what it does, and use that as install instructions. reply zubairq 3 hours agoprevHaving worked at Red Hat and worked on many Docker &#x2F; Kubernetes systems I agree with some parts of the article, but my view is that the wrold is going through a transition phase right now of moving to containerised systems.Take for example running something like 3Scale (an internet gateway) in Docker or Kubernete. It can be a nightmare to configure and run 3Scale using containers with the multiple memory limits and other container specific issues. Far easier to get 3Scale running without containers.So many software systems were not designed in the Docker era, and going forward many container applications will be designed to be easier to configure&#x2F;use in the Container world due to a \"Container&#x2F;Docker native\" mindset when designing the system in the first place reply GabeIsko 2 hours agoprevI can&#x27;t believe what I am reading. It took you more time to set up nextcloud with docker compose? What? For my hobby stuff, I can usually try something out really quickly by giving a compose a quick read and pasting it into portainer for a test.I have my gripes with docker - especially for my professional work, and I am not a fan about how much it obscures images, nor its shibboleth approach to using a CLI-daemon interface (should be a `dockerctl` command in my mind) but I really can&#x27;t believe that you would want to manually set up multi process platforms manually. When you want to connect to a data base, and have it be ephemeral? I have rooted around in enough application servers to understand that docker-compose and dockerfiles is a very sane approach.The real issue I am seeing in the comments are that people are complaining that they would prefer getting a maintained package for software. Would rather use `apt install` or `dnf install` than mess with containers. That has been discussed to death and we get the same answer is the same every time - yes that would be a nice world to live in, but it is a fantasy to imagine anyone doing that much maintenance work for packages. reply wg0 1 hour agoprevThere&#x27;s no merit to this complaint about Docker that it complicates configuration. Actually it simplifies a great deal.Author should consider how configuration is at different locations and sometimes even further split differently in different distributions for the same software package (apache, PostgreSQL etc) while a docker Image has it all at a very well known location within image and it doesn&#x27;t matter where you use that image. reply defanor 3 hours agoprevDocker keeps reminding me of people in the past shipping VM images, or sometimes physical machines, often with rather inappropriate desktop hardware and software. So do these points:> The problem is the use of Docker as a lowest common denominator, or perhaps more accurately lowest common effort, approach to distributing software to end users.Shipping physical machines probably is even lower than that though. Or even the VMs.> One of the great sins of Docker is having normalized running software as root.I am not as familiar with Docker practices, but unfortunately, AFAICT, people did that frequently on regular systems as well, just to not bother with permissions. (Edit: now I recalled people also not following the FHS and storing things in the root directory inside Docker images, but sometimes it was&#x2F;is similar without containers as well, and inside a container it does not clutter the host system, at least).> Having \"pi\" in the name of a software product is a big red flag in my mind, it immediately makes me think \"they will not have documented how to run this on a shared device.\"This approach is similar to shipping physical machines. Or at least maintaining odd legacy software on a dedicated machine.I think a rather pessimistic view is that proper packaging switched to Docker or single-purpose machines, but an optimistic one is that those are the unnecessary VMs and larger single-purpose machines that were replaced by Docker and RPi. Maybe there is a little of both going on. reply ungamedplayer 3 hours agoparent>AFAICT, people did that frequently on regular systems as well, just to not bother with permissions.This was an easy way to tell if you were dealing with a Muppet. If you saw this you would know that the software was going to be a problem. reply acje 49 minutes agoprevWe are probably going to go around the packaging wheel until we come up with one that does first class sandboxing. Any bets on webassembly? reply eviks 4 hours agoprevHas any great design been invented that isn&#x27;t tied to hardcoded paths preventing multiple versions of the same library and would allow easy linking to any version? Nix? Anything else? reply colordrops 1 hour agoparentYes, Nix. And it works great. reply linuxrebe1 5 hours agoprev99.9% of the problems you spoke to, which are very real. Could be solved if people building the software would just understand one thing. A container is not a mini VM. It is not in any way shape or form a virtual machine. If what you need is a lightweight virtual machine. Build that. Do not build a container because it&#x27;s the latest and greatest buzzword. But instead I see large monolithic applications, shoved into a container, and then I hear a multitude of complaints about performance issues ETC. You may be able to drive a nail with a screwdriver but it&#x27;s not a good idea. reply v3ss0n 4 hours agoparentOnly applies to docker.LXC for example designed container like a VM reply devjab 2 hours agoprevI think a lot of your comments here are very true. I don’t think docker made my world worse and I certainly don’t think it cost me more time, but I also sort of agree with the author. I came into Typescript late, and to “tame” the wildness of the ecosystem I set up a lot of opinionated systems for our developers. I didn’t dictate them, it was a collaborative process where we worked long and hard go agree on how we would utilise the Node ecosystem as well as the 10ish rules that we’ve changed as opposed to the “global” strict eslint rules. Rules which have themselves been born out of a decade of different rulesets like the Airbnb ones most people in the JS community will have heard of. My approach to containers has been similar.We don’t use docker-compose and never have as an example, we went with Terraform, though today we’re using Bicep, and we make heavy use of dapr side cars. Which makes working with infrastructure sort of easy. Easy to lock down and opinionate at least. You can get most templates directly from Azures GitHub, but you can obviously also build your own, and then it’s simply a matter of having some decent template projects so that your developers can be up and running in a few minutes whenever they need to start a new project. Obviously there is a cost when you’re moving legacy projects into this sort of infrastructure, but it’s not like it’s really that resource consuming if it was already containerised, and it’s not too bad even if it wasn’t.Now, that’s how we do it. It’s also how I think everyone should do it, but it’s not how you “have” to do it. In many ways, you’re as free to utilize the container ecosystems as freely as you can with the Node ecosystem. Well maybe not as free as that, but still free enough to create a lot of horror stories. Which is exactly what has happened with containers in many organisations. As such I think the author has a valid point. I’m not sure where we would go without the freedom though. Our exact setup works for us, you might not agree with our choices and neither of us would be wrong. So while some opinionated processes might be compatible with container infrastructure, others will need to remain “free”. I’m certainly with a lot of you in that it was worse before containers. I’ve also done work with organisations where container deployments worked so poorly that it was clearly not better, however, and I suspect those pipelines are far more common than given credit in this comment section. reply Xeamek 4 hours agoprevThe curse of docker is that it allows devs to be lazy and rather then making sure their configuration and deployment are straightforward, they get to keep all their mess working (longer).Ofcourse, if you have really complex setup docker is invaluable. But if all you are using it is to make the depreciation warnings go away on your single executable app, that&#x27;s just abuse of the tool for the wrong reasons reply teddyh 5 hours agoprevIn the old days, the age-old cry of the beleaguered developer was “My code isn’t buggy, it works on my machine!”, to which the response was “We’re not shipping your machine to the customer!”. Well, science marches on, and we’ve invented a way to do exactly that. Instead of, you know, writing actually robust and simple-to-deploy software.See also:reply viraptor 4 hours agoparent> Instead of, you know, writing actually robust and simple-to-deploy software.This is unfortunately much harder to do than doing docker images. If you&#x27;re trying to ship a robust and simple to deploy app, you will fail at some point - you just haven&#x27;t seen a system where that happens yet. You can&#x27;t be robust vs unknown unknowns and what a system will look like in a couple of years (next LTS) can be very surprising. reply teddyh 3 hours agorootparentHacking on some code and making random changes until it works (”It works! Ship it!”) is much easier (or at least feels easier) than designing the code carefully and using tests to verify the correctness of our code. Yet for good reasons we do it the harder way. For mostly the same reasons, I’m very wary of a system image of some pre-installed software instead of packaged software which is meant to run in a variety of environments, and to be easily configured to do so.Note also that a system which is robust and adaptive to different environments today is also robust to different future environments. And nobody can escape the march of time. Everybody needs updates. TLS updates. Time zone updates. Security fixes. I’d rather have a system designed with robustness against differences as a primary concern than a system where it’s assumed that it will run in an unchanging static universe. reply eviks 4 hours agoprevHas any great design been invented that isn&#x27;t tied to hardcoded paths preventing multiple versions of the same library etc and would allow easy linking to any version? Nix? Anything else? reply nusmella 3 hours agoparentI don&#x27;t know if this is normal or if anyone else does it but I usually either download binaries or compile from source and move the executable to &#x2F;usr&#x2F;local&#x2F;bin&#x2F; and create a symlink. Lets me easily switch between versions. I avoid using a package manager for anything where I want control over the version and installation.- curl -fSLJO $RELEASE- tar xvf $DOWNLOAD.tar.gz && cd $DOWNLOAD`- make .- mv $EXECUTABLE &#x2F;usr&#x2F;local&#x2F;bin&#x2F;$EXECUTABLE-$VERSION`- ln -s &#x2F;usr&#x2F;local&#x2F;bin&#x2F;$EXECUTABLE-$VERSION &#x2F;usr&#x2F;local&#x2F;bin&#x2F;$EXECUTABLE- # chmod 750, chown root:$appuser, etcWorks great for everything I&#x27;ve tried thus far. Redis, HAproy, Prometheus exporters, and many more. reply oddmiral 1 hour agoparentprevYes, these \"great designs\" are invented at a constant rate. I saw about 30 of them, or 1 per year at average. For example, Fedora invented and then abandoned modular design recently. reply vortegne 16 minutes agoprevI&#x27;m a developer with pretty limited \"serious\" DevOps experience. I can do tweaks here and there, create some simple cloud pipelines, etc. I&#x27;m definitely not fit to set up any enterprise-grade DevOps or manage a server.I run a home server just for fun, with quite a lot of applications.Docker and images from linuxserver.io are the best thing that happened to me in this realm. I can try out new applications very quickly, I can reinstall the OS on my server and get all my things up and running in no time. Linuxserver.io images have a standardised configuration which is great. There is no way in hell I could manage to set up this many applications without Docker, set up SSL for all of this, have fancy 2FA and SSO and have it so quickly redeployable. Guess I&#x27;m the target audience for this, it has been a lot of fun tinkering in a low-friction environment that Docker has provided me with. reply steve1977 3 hours agoprevIt seems to me that many of the points in the article are not really problems with Docker, but problems with Linux.I.e. the way Linux handles multiple versions of libraries and of course the fragmentation hell that is Linux distributions.Docker then is just a bandaid for these problems. reply oddmiral 1 hour agoparentLinux (kernel) can run any libraries in any combination. You are talking about distributions. Support for multiple versions of a library is not a goal for a typical distribution. Typically, this is necessary for commercial software vendors, which have money to throw at the problem. If commercial software vendors needs this so badly, they can sign a support contract with distributor, so a dedicated team of maintainers will care about this problem with libraries for them.In the open-source world, it&#x27;s an order of magnitude easier to patch the source, than introduce two versions of a same library. In rare cases, a -compat package is created, which then abandoned as soon as possible. reply INTPenis 2 hours agoprevThe curse of that website, it made me dizzy just scrolling.Containers are amazing, they&#x27;re bigger than docker, end of story. reply PreInternet01 1 hour agoprev> distributing an application only as a Docker image is often evidence of a relatively immature projectOr, you know, a contractual requirement. And, as far as those go, I actually kind of like it: you ship a well-tested container, and the only thing the infra dept at the customer site has to configure-via-the-environment is the URL for (or path to, via any kind of supported file system mounted into the container, which most IT shops can still just about manage) the instance configuration file.This reduces most initial troubleshooting to \"well, what does the instance log say about retrieving and parsing the configuration file?\", which, trust me, is way preferable over what you get with most other deployment methods I&#x27;ve been involved with... reply jrm4 5 hours agoprevDid the author say anything actually negative about Docker? All I saw was \"Docker can be suboptimal\" but nothing to suggest that it has been anything other than at least a partial improvement? reply kapitanjakc 5 hours agoprevDocker containers and images have saved me a lot of shipping pain in web part.I haven&#x27;t come across them being used as package manager or distributors yet. reply isoprophlex 4 hours agoprevClearly we need another layer between the host and the docker-compose spaghetti. Some lightweight (true, no docker-in-docker bs) VM that exists to lock up & let whatever is in those yamls kick and scream around.I wish I knew if I am being sarcastic here... reply carl_sandland 4 hours agoprevwhat about AWX then? it moved from shipping as a simple docker compose to a kubernetes operator! I&#x27;d much rather it didn&#x27;t. reply smitty1e 1 hour agoprevGreat read. If I had to quibble, it would be with> Even if 90-day ephemeral TLS certificates and a general atmosphere of laziness have deteriorated our discipline in this regard, private key material should be closely guarded. It should be stored in only one place and accessible to only one principal. You don&#x27;t even have to get into these types of lofty security concerns, though. TLS is also sort of complicated to configure.Any time there is complexity and only the One True Priest can manage it, there will be tears.There had better be a break glass solution backed by Righteous Documentation (a hypothetical substance I heard about this one time) if we are boxed in to a One True Priest situation. reply forrestthewoods 1 hour agoprevUnpopular Opinion: Linux got it wrong and Windows got it right. (Or at least closer to right.) Programs shouldn&#x27;t use centralized dependencies. Programs should ship ALL of their dependencies. Running a program should be as simple as downloading a zip, extracting, and clicking&#x2F;typing run.Docker exists because building and running software is so outrageously complex that it requires a full system image. And it turns out Docker didn&#x27;t actually solve it after all! reply patrick451 4 hours agoprevHis points about networking and the file system are spot on. It still blows my mind that the default user in a container is root and that dealing with the UID issue is such a pain. The only solutions I have seen can only be described as hacks. reply saidinesh5 1 hour agoprevObligatory Hitler uses Docker post: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PivpCKEiQOQLol @ \"I&#x27;m moving everyone to Windows! Don&#x27;t cry, you can run bash on Windows 10 now.\" reply bakugo 3 hours agoprevI sympathize with pretty much everything in this article, in particular the configuration&#x2F;filesystem pains.I can&#x27;t remember the last time I tried to get a file or folder into or out of a container without running into some sort of issue, usually involving permissions, and I feel like there really should be a better solution for this. reply charcircuit 5 hours agoprev>Consider Windows: the operating system&#x27;s most alarming defect in the eyes of many \"Linux people\" is its lack of package managementWindows has WinGet. reply Xeamek 4 hours agoparentWinGet isn&#x27;t really a package manager.Package manager role is, well, managing packages.WinGet is just a repo with installers of the programs listed, but dependency managing and installation it self is handled by the installers, not winget. (Don&#x27;t know if the ms-store package format isn&#x27;t exception to that, but still) reply charcircuit 2 hours agorootparent>WinGet isn&#x27;t really a package manager.Microsoft literally calls it along with some complementary services the \"Windows Package Manager\">Package manager role is, well, managing packages.Which is what it refers to what it is managing.>but dependency managing and installation it self is handled by the installersThe package manifest is allows for dependencies on other packages. reply Xeamek 46 minutes agorootparentThat&#x27;s great that microsoft calls it that, but that doesn&#x27;t really matter.Winget is just as much a package manager, as the \"add or uninstall apps\" tab in windows settings is. It&#x27;s just a interface. But the actual act of installing is done by different program. reply Timber-6539 5 hours agoprev [–] While the article makes some good valid points about docker&#x27;s shortcomings, it is full of strong assertions made entirely from the author&#x27;s blind spots.Comes off as a boomer rant more than anything. reply henriquez 4 hours agoparent [–] How’s this for a boomer rant. I never used Docker because in my day we didn’t use virtual machines. We FTP’ed files to the PHP server, on port 21. Somewhere along the way all these Gen-Z kids started writing GraphQL APIs in TypeScript transpiled Cofee’node and pushing the Docker VMs to the Nomad Kubernetes Azure. I didn’t read the article but the curse of Docker is that kids think they’re better than me. After all I’ve accomplished! I had to buy a 165hz monitor to keep up with them in Counter Strike: Go, and my cholesterol is at an all time max. reply Timber-6539 4 hours agorootparent [–] What those kids probably did that you few admins couldn&#x27;t while using \"your\" FTP and other antiquated tools, was to reduce the friction for anyone who wanted to maintain a server.Embrace new technology or be left behind. I can&#x27;t think of anything in the past decade that has made more strides in software distribution than docker has. reply otabdeveloper4 20 minutes agorootparent [–] Administering a server full of random docker-compose.yml&#x27;s downloaded from the Internet is a nightmare.We got rid of Docker from all our production servers. Docker is technical debt in box. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Packaging software in Linux distributions, especially using Docker, is challenging and frustrating.",
      "Collaboration between software developers and distributions is crucial for widespread availability and use of software.",
      "The author dislikes Docker as a means of distributing software to end users due to various difficulties and limitations."
    ],
    "commentSummary": [
      "Users are divided on the advantages and drawbacks of using Docker for software packaging and deployment.",
      "Supporters argue that Docker simplifies configuration and deployment of complex software, while others express concerns about dependency issues and security updates.",
      "The author explores the challenges and benefits of Docker usage, while some users express frustrations with its documentation and compatibility problems."
    ],
    "points": 151,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1700961243
  },
  {
    "id": 38412818,
    "title": "Developers frustrated with lack of response from .NET MAUI team",
    "originLink": "https://github.com/dotnet/maui/issues/19018",
    "originBody": "dotnet / maui Public Notifications Fork 1.4k Star 20.2k Code Issues 2.9k Pull requests 143 Discussions Actions Projects 5 Wiki Security Insights This issue was moved to a discussion. You can continue the conversation there. Go to discussion → New issue Jump to bottom The MAUI team does not respond to issues with any urgency... THIS IS **THE** ISSUE!!! #19018 Closed mobycorp opened this issue · 2 comments Closed The MAUI team does not respond to issues with any urgency... THIS IS **THE** ISSUE!!! #19018 mobycorp opened this issue · 2 comments Labels t/bug Comments mobycorp commented • edited Description DO NOT CLOSE THIS ISSUE BECAUSE YOU THINK IT DOEN'T MEET THE CRITERIA OF A VALID ISSUE! THERE ARE SOME VERY SALIENT ISSUES BEING RAISED IN THIS ISSUE..... I have reported issues as far back as Feb, 2022, and of as now, many of these reported issues remain unfixed. This is really a poor situation where I have to open an issue to complain about issues not being fixed. The lack of response to issues IS the issue. I reported an issue (#4672) and here we are at .NET 8 and this VERY BASIC thing remains unfixed. @davidortinau I know I have been critical of the lack of desktop support and I do applaud your efforts in shoring up this deficiency in the latest releases, but how long do we have to wait for the most basic MAUI stuff to work properly? When I open a window that has frames, it almost NEVER draws correctly. You have to resize the window to force a layout recalculation for things to correct themselves. Here is a picture of an app that has a flyout strip and it never draws correctly until a window resizes: You can see that the button row is not centered. Here is the same screen after a window resize. I reported this almost 2 years ago and you (David) even added comments to the original issue: Controls/Layout does not draw controls correctly in first-time view. #4672 It has been in the backlog ever since and I would think that something that makes a MAUI app look like total s&#t would be a concern of the MAUI team. Also, I reported the fact that easy controls like Switch cannot have its basic colors changed (at least at the time I reported the issue: There is no way to set the text color of a Switch control #8965). Gerold V just took it upon himself to close my issue by telling me, in essence, we are not going to do stuff like this. I should go write platform-specific code. That's ridiculous because what is platform-specific about the Switch control??? In WPF EVERYTHING was stylable... Why is it the responsibility of the development community to do your work by writing handlers and such to get something simple to work? Also, take a look at the screenshots above. Notice the minimize, maximize, and close buttons are double-drawn. How does this stuff pass QA?????? I am getting ready to roll this out to 2000 perspective users and I will not be proud of 99.99999% of the app because of the visual aspects of MAUI. I looked at UNO and Avalon back when I was deciding to go cross-platform and I decided to stick with Microsoft all the way for everything: .NET, MAUI, gRPC, ASPNETCORE, etc... I wanted the whole stack to be Microsoft, but I am ruing my decision... MAUI is great in concept, but not in implementation. Simply upgrading Xamarin to MAUI did not buy the community much in the whole product delivery. You guys have focused far too much effort on sapping milliseconds out of performance while glaring visual bugs persist. I think 90% of the attention should be placed on fixing bugs and performing better QA. I know stuff breaks from release to release and I would think that decent regression testing would catch stuff like this. I also think you guys need to look at WPF and figure out how to achieve the functionality that existed there before developing new stuff. I would love a WYSIWIG XAML editor so we can see what we are designing without having to run the app. This brings me to Hot Reload which RARELY works! It does work on straight XAML, but not XAML in templates. I see it work there, but as I said, rarely. I have been a developer since 1984 and have focused much of my career on UI design. WinForms was great and WPF was even greater. MAUI isn't even making the charts... I sometimes spend hours trying to get something that was easy in WPF to work in MAUI and that is time I don't have... Yes, this rant is probably misplaced here in the issues log, but how else can somebody truly vent their frustrations about developing with MAUI? I spoke via a TEAMS meeting early last year with both David and Maddy and I let my opinions be known then. Here it is almost two years later, and nothing is even slightly easier to develop in MAUI... I would love for David to be made aware of this \"issue\" and have him contact me directly. This doesn't even scratch the surface of my frustrations... I really want to talk to @davidortinau directly. Please pass this on to him. Regards, Steve Miller Steps to Reproduce No response Link to public reproduction project repository No response Version with bug 8.0.3 Is this a regression from previous behavior? No, this is something new Last version that worked well Unknown/Other Affected platforms Windows Affected platform versions No response Did you find any workaround? No response Relevant log output No response 4 mobycorp added the t/bug label Alex-Dobrynin commented • edited It seems to me that this is the end for Microsoft's mobile team. They do not fix own bugs, they do not respond to the tickets, the even do not review PRs made a year ago to fix BASIC feature of MVVM - Bindings. All they do is blah blah blah to the camera. Poor, very poor management. When i firstly heard about maui, it was said that it will be revolution in .net mobile development. I thought it will be some kind of XPF (x-platform WPF), the render engine will be based on skia and we will be able to style everything and everywhere, like it is in WPF and we will never do ui based platform-specific things, like it is in Flutter. But what do we have? Very laggy buggy 'XF', and if you need some not standard implementation of controls maui provides you, you need go and implement platfrom-specific things. Even XF has better performance and less bugs. They could go and look how avalonia managed to implement this, but no, they trying wrap shit into a candy wrapper. IMHO foximoxi commented I appreciate the huge contribution that has been made over the last few years to rebuild Xamarin Forms into something faster, more flexible and bug-free. Unfortunately MAUI already has more bugs reported than Xamarin Forms, although its support is currently only formal. Hire a few more people to start fixing bugs systemically. The number is only increasing - 2122 reported bugs as of today. How many will there be next year? 2 dotnet locked and limited conversation to collaborators drasticactions converted this issue into discussion #19029 This issue was moved to a discussion. You can continue the conversation there. Go to discussion → Assignees No one assigned Labels t/bug Projects None yet Milestone No milestone Development No branches or pull requests 3 participants",
    "commentLink": "https://news.ycombinator.com/item?id=38412818",
    "commentBody": "Developers are not happy with .NET MAUI, but nobody in the team cares about itHacker NewspastloginDevelopers are not happy with .NET MAUI, but nobody in the team cares about it (github.com/dotnet) 136 points by yell0wsnow 22 hours ago| hidepastfavorite108 comments jiggawatts 22 hours agoThree decades of experience talking here: If you don&#x27;t see a vendor like Microsoft using a GUI framework for at least 50% of their new applications, then you&#x27;ve made a terrible mistake in adopting it yourself.Microsoft uses Google&#x27;s framework for their own applications: Electron.Yes, you heard me right. Microsoft, a nearly 3 trillion dollar company, uses the framework of a competitor for their own desktop applications.Teams: Electron.Visual Studio Code: Electron.Azure Data Studio: Electron.Now, let&#x27;s make a similar list for Microsoft MAUI apps!Umm... err... hmm... reply jiripospisil 21 hours agoparent> Microsoft uses Google&#x27;s framework for their own applications: Electron.Electron is built on top of Google&#x27;s Chromium but developed by a completely different set of people at GitHub initially for their text editor Atom. Later GitHub got acquired by Microsoft. reply pas 14 hours agorootparentTo give more context, MS basically forked Chromium&#x2F;Chrome as Edge, so if somehow Google would close off Chromium they wouldn&#x27;t be left standing in the cold.That said, OP&#x27;s point is crystal clear, these silly new platform projects are the busywork of vestigial orgs. reply tpae 21 hours agorootparentprevEven github is full of bugs these days. I hate Microsoft products reply npteljes 20 hours agoparentprevI completely agree. Evaluating a decision like this, one needs to look at it from a business perspective, in order to reach the correct conclusion. Looking at it like this, what I cannot see is Microsoft&#x27;s commitment to this technology. Instead, we see a consistent commitment to an alternative technology, even if historical reasons also take place. reply lawgimenez 20 hours agorootparentNot long ago someone from Maui posted here about the “momentum” within Microsoft. reply npteljes 14 hours agorootparentI can believe that. I mean, the momentum even. Only time will tell if it really comes to fruition. reply spacechild1 20 hours agoparentprevElectron has been originally developed by GitHub and is now maintained by the OpenJS Foundation. It is not a \"Google framework\". reply nvm0n2 17 hours agorootparentFoundations don&#x27;t really do maintenance on products. Electron is maintained by a few people at Slack at one or two at Microsoft. Of course if you count the Chromium codebase then it is indeed \"Google&#x27;s framework\" and they do nearly all the maintenance. reply charrondev 17 hours agorootparentWith edge being switched to Chromium some years ago I would assume Microsoft also has a quite a few engineers working full time on it. reply torginus 19 hours agoparentprevThe weird thing about MS is .NET in general - it seems like since the inception of the technology they intended to bet the farm on it as the desktop development platform of choice, yet the only first-part MS app that uses it Visual Studio (and assorted tools like Expression Blend).Which is super weird, since it was conceived as a Borland Delphi competitor (they even hired the guy who created Delphi, Anders Hjelsberg, to design C#), and there used to be tons of Delphi apps.It never made it into their core offerings. reply cristeigabriel 20 hours agoparentprev> Teams: Electron.Not as simple with Preview versions on Windows. I don&#x27;t think they use Electron whatsoever anymore, and old versions weren&#x27;t fully Electron either (I remember seeing Chakra [their old JS engine] in the stacktraces, doing some elementary logic), I think it&#x27;s XAML with UWP + WebView2. reply physPop 19 hours agorootparentThe new teams is react + webview see:https:&#x2F;&#x2F;techcommunity.microsoft.com&#x2F;t5&#x2F;microsoft-teams-blog&#x2F;...This to me was the biggest strike against MAUI. Teams team recently decided to switch tech stacks to address poor desktop performance, and explicitly didn&#x27;t pick MAUI. reply rayiner 18 hours agorootparentAnd the performance is still crap. What does that say about MAUI? reply cristeigabriel 18 hours agorootparentprevThanks for correcting me! reply deafpolygon 18 hours agorootparentprevIt may be that MAUI wasn&#x27;t mature enough yet at the time they started working on it. reply debugnik 17 hours agorootparentImmature isn&#x27;t the right word, MAUI is mostly a rebrand of Xamarin.Forms, which has been a thing for almost a decade now and already had some support for desktop before the MAUI effort. Rather, it&#x27;s an irredeemable mess. reply zigzag312 17 hours agorootparentRefactored Xamarin.Forms with breaking changes. replyChrisMarshallNY 21 hours agoparentprevI remember a similar argument against Visual SourceSafe[0].We had been using SS since it was done by OneTree (borged by MS).MS used an internal tool, instead.VSS did do one cool trick, which I have yet to see replicated elsewhere: You could construct \"artificial workspaces,\" composed of elements (down to individual files) in other workspaces, so, when you checked out (it was all checkout&#x2F;checkin, back then), you actually checked out from (and back into) the original workspace.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Microsoft_Visual_SourceSafe reply christophilus 20 hours agorootparentIs git worktree close enough? In the past, I’ve used it roughly the way you describe. reply ChrisMarshallNY 20 hours agorootparentI don&#x27;t think I&#x27;ve tried that.Thanks for the heads-up.[UPDATE] Ah. Now I remember [0]It does roughly that, but it still insists on bringing in the entire repo (as does all Git).The thing that SS did, was allow you to create a \"chimera\" workspace, composed of just tiny parts of other workspaces.It was a great way to share sample and SDK stuff, without, for example, sending over a ton of testing code.Many of my repos have more testing code than implementation code; not to mention a ton of documentation.[0] https:&#x2F;&#x2F;git-scm.com&#x2F;docs&#x2F;git-worktree reply fanf2 12 hours agorootparentprevThe main criticism I heard of Visual SourceSafe was that if your repository got bigger than 2GB it would be irretrievably corrupted.I understand it used the same storage engine as a number of other Microsoft products, including Exchange, which had the same bug. reply ChrisMarshallNY 8 hours agorootparentIt was awful.It was a straight file-based system; run entirely from the client. No server component.You mounted a server drive onto your desktop, pointed VSS at it, and then fixed a four-course meal, while it synced.No server. For a shared VSS.I’m not surprised MS didn’t use it. reply stevekemp 47 minutes agorootparentMy memory of VSS was that it locked files while you were making changes, and that stopped your colleagues from editing those files until you were done.That, and the slow speed, were my two memories of the product. But I admit the last time I used VSS was back in 1996, or so. reply malodyets 20 hours agorootparentprevSvn externals provide similar capabilities, though I don’t remember if an external can be an individual file. TortoiseSvn on windows made it not to painful to use.https:&#x2F;&#x2F;svnbook.red-bean.com&#x2F;en&#x2F;1.7&#x2F;svn.advanced.externals.h... reply denton-scratch 20 hours agorootparentExternals is a footgun that can cause nasty injuries. I like Svn, but I won&#x27;t be using Externals again. reply mathverse 21 hours agoparentprevThis. I believe OneDrive on macOS uses QT. reply Zetobal 21 hours agorootparentIt does but here is the kicker... it also uses QT on Windows. reply Razengan 2 hours agoparentprev> If you don&#x27;t see a vendor like Microsoft using a GUI framework for at least 50% of their new applications, then you&#x27;ve made a terrible mistake in adopting it yourself.!! _That is exactly_ one of the reasons why I jumped from Windows to Mac! Back in the Windows 8 days. So much more consistency API-wise and much less fear of your favorite thing being abandoned!Apple actually uses their own shit, unlike Microsoft internal warfare that would make the Sengoku period seem like a kiddy playground. reply ecmascript 22 hours agoparentprevElectron is owned by Github which is owned by Microsoft tho. They also own npm and typescript. So Microsoft themselves has been moving away from native apps a long time so your main point is still valid. reply MyFirstSass 22 hours agoparentprevThis is bizarre to me.What applications are they actually using the framework for?Is it because it&#x27;s easier to make cross platform?Reminds a bit of when i used Angular back in the day and it seemed Google of all people didn&#x27;t use it for any of their many web services. reply icegreentea2 21 hours agorootparentMicrosoft hasn&#x27;t really dogfooded their own external facing GUI frameworks in forever. It&#x27;s not a great look, but to some degree it also made sense. These frameworks (especially the dumpster fire ones) are .NET&#x2F;C# (usually with XAML) frameworks, designed (it feels like) primarily to build line of business internal applications, with cross-platformness tacked on in more recent years. The grand-daddy of all of these would be WPF (all the way back in 2006).Basically Microsoft&#x27;s flagship non-developer apps (ie Office Suite) is its own little kingdom. They never bought into any of the developer facing C# frameworks that MS created (makes sense I guess?... Office isn&#x27;t a C# code base). Office basically developed its own visual language and internal framework.Visual Studio itself actually got some WPF back in the day (though I understood the code base to be kind of what you expect of something of its heritage). I imagine it still has WPF to this day.Their newer apps - especially the ones that are actually trying to be cross platform (like Teams, VSCode and friends). Well, Teams has the excuse that they need a web version anyways, so Electron really does make sense (see Discord and Slack).VSCode... the first version of VSCode came out in 2015 (omg..... it actually released a few months before Windows 10). MS had no in-house cross platform solution at that time. At that time, they did have a partnership with Xamarin (Xamarin would later be acquisition). .NET MAUI is the evolution of Xamarin. But Xamarin I always understood to be a bit of a dumpster fire. I was evaluating our choices for a small windows desktop app back in ~2015-2016. Xamarin did not seem appealing. After a misfire with UWP (another MS footgun I think), we just did WPF (cross-platform not required).Honestly, I wouldn&#x27;t expect anything that could even remotely be described as an \"IDE\" to NOT be based on the VSCode code base now. MS has something -very- good going for it there.And I guess this is kind of one of the challenges. The part of MS that builds apps, is not really related to the part of the MS that builds GUI frameworks for external developers. And the part of the MS that builds apps does not really want their success to be tied to this dumpster fire. reply nicoburns 20 hours agorootparentI believe some MS apps are also using react native these days (and MS maintains the desktop ports of react native (both windows and macOS)) reply hu3 21 hours agorootparentprevAngular is prime example.From my consulting experience, Angular (Google tech), is most used today along with .NET backends (Microsoft tech).I don&#x27;t think I ever saw a client using Angular + Go, for example. reply ecshafer 20 hours agorootparentIve seen angular with c#, java, python and node backends. Angular though is mostly popular with big enterprise non tech companies, more industry differences than tech differences. reply wg0 21 hours agoparentprevTrue. Many will dismiss this opinion but that&#x27;s how it is. The only issues being actively resolved and substantial progress happening is in:CSS - New units, flex, grid, subgrid, animation, new query methods, view transitions. JS - New APIs ranging from cryptography to WebGPU, Web Assembly.If you must give an interface to an application, there&#x27;s absolutely no match to what web platform can offer in terms of fidelity + ease of use and accessibility.Sure, you can go bonkers with Flutter, Qt, Swing or other immediate UI libraries but then you&#x27;re on your own to invent everything on your own.You can&#x27;t find like of d3js in Qt, Swing, MAUI or GTK. Not saying some half baked version of some of it does not exist OR is not doable at all in any of the toolkits. Just that you have to do it on your own. reply zigzag312 19 hours agorootparent> Flutter, Qt, Swing or other immediate UI librariesYou mean retained mode UI libraries? reply thenerdhead 20 hours agoprevTitle should be \"Developer yells at .NET MAUI team and expects to talk to the manager because their bug hasn&#x27;t been fixed in two years\".I hope people can see the hostile working conditions that many in open source have to experience daily. There is no possible way for a small team to scale with the thousands of issues they get and give every single one the same amount of attention. That is why it is so important for people to upvote and engage in issues. It gives the product and engineering teams a way to gauge priority.There are much better ways to go about ranting about product defects though. Doing it in this way just encourages people to pitchfork. We all know that pitchforking isn&#x27;t productive at all. In fact, it hurts more than you think it helps. reply MrGilbert 20 hours agoparentI agree that it is usually useless to rant about an issue in the first place.In this case, I can see that they were really neutral and understanding in the very first issue. After two years with little to no progress, I&#x27;d say it is natural to vent. Especially if they cannot get out of the situation they are in.It looks like the original poster needs to use MAUI in their production environment. So they are already dependent on it. Which is to be expected, because MAUI already comes with a go-live license.Also, I&#x27;d like to point out that this is not the pet-peeve of a small indie team. It is the future UI framework of a multi-billion company. If the team is understaffed, there should be something done about. The resources should be available in this case. reply thenerdhead 20 hours agorootparentThe latter part should really be looked closer at. You cannot simply attach a company’s market capitalization to how funded a team actually is. People constantly overlook that. That’s true at many big tech companies too. The assumption harms just as much as pitchforking. reply physPop 19 hours agorootparentWhile true, I think the point is a mismatch between MS business strategy (marketing MAUI and the next big thing and officially supported) while not properly staffing. As an end user, you might make tech decisions precisely becuase of the market cap, assuming support and proper implementation. This fundamentally is the biggest issue with MAUI, no one can trust its not just marketing and going to be dropped... reply lopatin 20 hours agoparentprevI think the developer is justified in his self-described rant. He has gone through other channels and engaged with the project leaders directly. He has waited years for the bugs to be addressed.Microsoft is in the wrong here for selling this as the future and not dedicating the resources necessary to fulfill that promise. reply thenerdhead 19 hours agorootparentMaybe they were busy fixing other issues?https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;dotnet&#x2F;announcing-dotnet-maui... reply willcipriano 18 hours agorootparentIf you go to a resturant, order the soup and it&#x27;s cold do you really give a crap about issues with other menu items that they are resolving instead?\"Yes the soup is cold, but we no longer give people botulism with the salads surely that counts for something\" reply biorach 20 hours agoparentprevThis is not a typical open source project. The developers are all Microsoft employees who, in theory, are paid to work on MAUI reply thenerdhead 19 hours agorootparentHow does getting paid change things? Many big tech companies \"sponsor\" OSS in this way and they have project backlogs bigger than the allowed limit on GitHub. https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;community&#x2F;discussions&#x2F;9678 reply biorach 19 hours agorootparentDamn right getting paid changes things reply JustLurking2022 17 hours agorootparentprevBecause it implies these are people have far fewer excuses for the seeming lack of care about user feedback than some unpaid volunteer.Considering many companies have contacts ranging from millions to hundreds of millions with Microsoft, and developer tools are a major selling point for the ecosystem, it&#x27;s certainly reasonable to be upset that when your organization can&#x27;t depend on them. It makes planning a nightmare - it may take a month or 5 years to deliver your feature, depending on when the bugs get fixed. reply rightbyte 20 hours agoparentprevOh it is very much possible to scale. Just drop the agile process, give the devs agency and fix the bugs.Easely like 100x the efficiency fixing small bugs. reply MrGilbert 20 hours agorootparentDev with agency here. There is clearly an issue, but \"agile\" is not it. \"Agile\" doesn’t mean \"ship unfinished products and never care for issues\".In fact, it means quite the opposite: Start with as little as possible, as polished as possible and improve from there. It&#x27;s actually quite nice to work with, if every role is on the same page. reply zihotki 19 hours agorootparentI wonder where you&#x27;ve found that definition of \"agile\". The agile manifesto says that they have come to value:1. Individuals and interactions over processes and tools2. Working software over comprehensive documentation3. Customer collaboration over contract negotiation4. Responding to change over following a planThere is no direct quality&#x2F;polishness in there.Edit: formatting reply MrGilbert 13 hours agorootparentWell… I guess Number 2 leaves room for discussion here. reply deafpolygon 20 hours agorootparentprevBut 0x efficiency developing new features. reply rightbyte 20 hours agorootparentThat is debatable. reply ratg13 19 hours agoparentprevShame can be a motivator.Also you’re implying that MAUI does not have serious problems, which is not true the case.It’s good to bring this to people’s attention so that others do not make the same mistake choosing MAUI as this guy.The MAUI team should reflect as well about the reasons people need to stay away from their product.Your “everything is fine, this guy is just overreacting” makes me think you haven’t worked much with MAUI. reply rafaelmn 22 hours agoprevfew months ago I tried to wrap up a MAUI PoC app from a more junior coworker, who failed to deliver anything working after weeks of trying.Then I spent two weeks on trivial shit and stuff you expect to work by default (pull to refresh, video streaming, video in list view etc.)After two weeks of stress, failure and dozen of known issues on GitHub with tons of frustrated developers [1], I rewrote the whole poc in flutter in 3 days.Keep in mind this was a simple app with two screens and sign in&#x2F;sign up - the only thing mildly technicall was a list view with videos (video feed).Literally nothing in MAUI worked as advertised and their showoff apps are ridiculous. The build process takes forever and breaks randomly, iteration is slow and debugging breaks all the time.This is the exact same experience I&#x27;ve had with Xamarin 5-6 years ago.Any imaginary gain you think you&#x27;ll have by being able to share code between client and server will be offset many times by having a sane&#x2F;working dev environment like flutter (working framework, fast iteration) and the language isn&#x27;t that far off from c#.[1] https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;maui&#x2F;issues&#x2F;8926#issuecomment-1563... reply WoodenChair 21 hours agoprevWe tried building a greenfield mobile app using Maui because my development partner is a Microsoft tools aficionado. We ran into a big memory leak that was so hard to debug using the primitive tools that it set us back a month. Eventually, we discovered it was internal to Maui (leak caused by a Maui component, not our code) and decided to program native apps instead for the project. Ultimately I think rebuilding as native actually saved us time for the simple app we’re building reply jwells89 20 hours agoparentIt’s counterintuitive, but yes, depending on the project cross-platform frameworks can actually be harder for small teams or one-person operations to manage well because they increase the number of layers in which bugs and quirks can show up in — instead of just e.g. Mac AppKit and Windows Win32 bugs, you’ve now got to deal with those plus whatever bugs the cross platform framework has.This is particularly true for projects where the UI is not overly complex (and thus, not where most of your energy is going), in which case it can make more sense to do code sharing in a library that’s pure business logic (no UI) and platform independent. reply TheLoafOfBread 19 hours agoprevThere are some really nasty bugs in MAUI, which are not even obviously reported by error messages. From my experience:* You can build MAUI application fine, but you won&#x27;t be able to deploy it for Windows and run it, unless your repository lives in C:\\ drive. And I mean C:\\ drive, not substed directory to D:\\ not additional drive. C:\\somePath\\ only. The worst thing is that error which you get is completely ambiguous and does not tell where is a problem.* You would think that you can bypass requirement above by forcing Visual Studio to deploy on C:\\somePath\\bin? Yeah that works for Windows, Android but does not work for iOS&#x2F;MacOS emitters, where somebody automatically assumes that all path are relative. So instead of C:\\somePath\\bin MacOS&#x2F;iOS emitters will try to build into D:\\PathToMyRepo\\C:\\somePath\\bin , obviously ending with an error.I think that MAUI is great product and I like the universal outcome (deploy for all major platforms at once), but my god, myriad of stupid bugs is very annoying. reply kvonhorn 15 hours agoprev> How does this stuff pass QA??????Microsoft notoriously fired all their QA engineers back in 2014. I would assume that QA is no longer a core competency at Microsoft. I&#x27;m guessing that they have testing tasks that are now performed by contractors, but no actual _process_ in place to assure quality. The only engineering process now is agile.My experience as a QA engineer testing software in agile environments in the last 15 years or so is that my attention every sprint is focused first on assuring that the features implement the user stories, and second that the automated regression tests are written and checked into version control. This automation task is where the majority of my 80 hours every sprint is spent. There is no time to check for bugs by way of, say, ad-hoc testing.If I&#x27;m a contractor, all the buttons are on the screen, and the only issue is that the buttons aren&#x27;t centered, then I&#x27;ll enter a bug for the alignment issue as a low priority fix and clear the feature to ship. I might POSSIBLY perhaps maybe add a note to the bug suggesting a workaround by sending an InvalidateRect message to the window containing the buttons, but I doubt it knowing that the developer outside of the team would never read it.I will absolutely NOT be an advocate for the developer experience and refuse to clear the feature to ship with this bug knowing how tenuous my employment status is, however. I would guess that that&#x27;s how this bug passes QA in the first place.Edit: Clarified a few statements. reply bob1029 21 hours agoprevWe are huge into Microsoft but MAUI is on the no touch list.After maintaining a Blazor (server-side) app for a few years, it has become clear to me that even if the abstractions are ideal, the tooling may cripple the entire experience.I don&#x27;t trust Visual Studio 2022 to not treat me like a piece of shit in a UI style project anymore.The only project types I really trust to not suck are: class library, console app, and azure function (timer or http triggers).If Microsoft wants me to give their UI frameworks another shot, then I challenge them to rewrite visual studio itself using whatever proposed framework. Only then would I have faith that someone sorted out all the long-tailed dragons that would bite me in the ass 2 years deep. reply solarkraft 21 hours agoparentVisual Studio hasn&#x27;t not treated me like a piece of shit ever. It&#x27;s a similar \"lol fuck you\" as witnessed in this issue. reply deafpolygon 18 hours agoparentprevWhat UI framework should I be focusing on in 2023? Should I focus on web-based framework (vis-a-vis Electron or similar platform) or is there something more suitable for desktop UI? reply bob1029 17 hours agorootparentWe do web-only right now. No frameworks. Maybe focus on MDN if you think you can get the business done with a webapp. Once you master flex, grid & friends, you will be set free.If we are still insisting upon native apps, I am growing increasingly-curious as to what the reason could possibly be. Unreal & Unity deploy to the browser. What does your business do that is so special by comparison? reply nvm0n2 15 hours agorootparentHTML is a bad replacement for a proper UI toolkit, that&#x27;s the appeal. There are so many problems that good UI toolkits and languages solve that HTML just ignores and always will. But, Chrome gets a massive budget, web apps are easy to deploy thanks to the ubiquity of browsers, and there&#x27;s safety in numbers, so those things usually outweigh the other concerns. Doesn&#x27;t change the fact that there is plenty of scope to compete with HTML. reply zigzag312 17 hours agorootparentprevGreat multiplatform UI framework doesn&#x27;t exists currently. Next best thing is to choose a framework that best fits your requirements. reply deafpolygon 16 hours agorootparentTrue enough, but is there something less-great? Or not-so-bad? reply zigzag312 16 hours agorootparentWhat are the requirements?Single or multiplatform? Which OS?Will there also be a mobile or web version of an app? Or embedded?How well does it need to integrate with target platform? How native it must feel? Does it access to native platform features?Does it need to be lightweight?Does it need very low latency? Is performance critical?Does it need many custom widgets or none?Does it need to have quality implementation of any specific widgets available (like charts, maps etc.)? Big widgets ecosystem?Does it need heavy widget trees?Does UI need to be customizable by users?How important is accessibility?Programming language preferences and limitations?How important is developer productivity?Can framework be proprietary or non-free?Is it maybe just to learn UI development? reply voakbasda 16 hours agorootparentprevFor Go, I just picked up Fyne. Deploys natively on all relevant targets, with a consistent UI across platforms. reply zerr 14 hours agorootparentNo accessibility. reply whynotmaybe 15 hours agorootparentprevCheck flutter. reply yell0wsnow 22 hours agoprevI was browsing the dotnet&#x2F;maui repo and the vibes over there are not great.decided to give the framework a quick spin myself and for god sake, this thing is in terrible shape. I built a quick holy grail layout with basic content and it looks completely different on all platforms. basic controls are not working and collections are laggy.seems that nobody in the team really uses this thing in production or interacts with the communitymore of the same https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;maui&#x2F;discussions&#x2F;17917 https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;maui&#x2F;discussions&#x2F;15203 reply baz00 22 hours agoprev> \"It seems to me that this is the end for Microsoft&#x27;s mobile team. They do not fix own bugs, they do not respond to the tickets, the even do not review PRs made a year ago\"That&#x27;s always been the case with all of their frameworks.Even if the entire userbase is spitting fire the default position is to double down on whatever stupid thing they&#x27;re doing or ignoring it. Only time I ever got anything fixed was when I was working for an MS partner and we threatened to become an Oracle partner.Then again the OSS community is only slightly better and Apple is definitely worse! reply CoastalCoder 21 hours agoparentSince we&#x27;re talking about GUI frameworks, I wonder how Qt fares in terms of bugs and fixing them. reply baz00 20 hours agorootparentWell Qt on macOS is a shit show. I pretty much had to move half my stuff off Mac because two Qt based programs I work on regularly keep breaking every time they update macOS. reply Ecstatify 21 hours agoprevWe decommissioned all our mobile apps built with Xamarin.Some of the Microsoft teams don&#x27;t listen to user feedback.We use Azure DevOps, feedback from years ago for basic features still not developed.https:&#x2F;&#x2F;developercommunity.visualstudio.com&#x2F;AzureDevOps reply WirelessGigabit 17 hours agoparentSeems like that team moved to GitHub actions. How old is it? And we still don&#x27;t have YAML anchors.Which is just a parser feature. reply hhh 21 hours agoparentprevMS running two major code repo platforms, one that is hated, and one that’s widely loved, leads me to believe ADO is not going to make it. reply my123 20 hours agorootparentAzure DevOps gets wide internal use at least: Windows source code is hosted on it. reply adamgordonbell 17 hours agoparentprevMy thinking is that you should extract as much logic as possible into some neutral format that you can run on any CI platform. Bash, Docker + Makefile, Earthly, python scripts... whatever keeps as much logic out of a proprietary vendor specific format.Then you can switch easier, if need be. reply ratg13 19 hours agoparentprevI’ve talked with people at MS that have told me Azure DevOps is no longer in active development.You should start your migration to GitHub actions. reply LikesPwsh 18 hours agorootparentMS don&#x27;t seem entirely on the same page there. Azure Devops was their first choice for Power BI integration for some reason.https:&#x2F;&#x2F;ideas.fabric.microsoft.com&#x2F;ideas&#x2F;idea&#x2F;?ideaid=020078... reply ratg13 17 hours agorootparentCorrect, I&#x27;ve only heard this from two people and this is only their take. This is not an official position.But the fact that people within the company are saying this to me directly is enough to get me to believe it.I wouldn&#x27;t trust a random person on the internet saying stuff like this either, but even without any inside knowledge, it&#x27;s just logical that they should try and limit time spent developing two of the exact same product .. and I think it&#x27;s probably becoming apparent to you as well which one is on the chopping block. reply Ecstatify 14 hours agorootparentWe would have migrated to GitHub, unfortunately it doesn&#x27;t have the equivalent backlog management that&#x27;s in Azure DevOps. reply ratg13 13 hours agorootparentFrom what I understand the goal at the moment is to get GitHub closer to feature parity with ADO before they start to deprecate it .. so hopefully it&#x27;s coming.And obviously, grain of salt, third-hand knowledge, etc. replydatadeft 21 hours agoprevThe only usable service or software from any company that is in the critical path for their own use.Examples: - AWS is in the critical path for Amazon- .Net is in the critical path for MSIf you chose a software or service that is not in the critical path, the jokes are on you. reply nor0x 22 hours agoprevAlso the foundation for MAUI, the native bindings for .NET are incomplete and broken. Back in the Xamarin days the promise was that \"everything you can do natively you can do in .NET\" - this hasn&#x27;t been true for a long time. Most of the new frameworks, especially on the iOS side don&#x27;t have a binding and Microsoft doesn&#x27;t have a story for Swift based libraries at all reply Voultapher 13 hours agoprevSo .NET MAUI doesn&#x27;t even support Linux https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;dotnet&#x2F;maui&#x2F;supported-plat... that alone would be a non starter for me. reply tren 20 hours agoprevWe&#x27;re a dotnet shop and we&#x27;re looking to rewrite our iOS and Android apps in a cross platform framework. On paper MAUI seems like the perfect fit but reading through the discussion and issues in GitHub it&#x27;s nearly guaranteed this will go the way of all MS&#x27;s UI products. The number of unresolved issues for fairly basic functionality and the number of developers voicing their PTSD from MAUI related development has made us steer clear. reply zerr 14 hours agoparentUno Platform or Avalonia should be a better fit. reply xrd 20 hours agoprevI tried to use the MSTeams API in the last year. It (the example GitHub repo) was obviously basically abandoned by MS except for some third world subcontractors that I&#x27;m sure a PM at Microsoft was managing while she managed her 401k and coasting into retirement. What a mess. reply dtx1 21 hours agoprevSpeaking as a former Windows Phone Developer: Yeah, this is roughly what developing for Windows Phone and the UWP felt like. Half Baked Tech Demos with glaring, basic Issues that never get fixed. It feels like Microsoft internally still thinks they are such a monopoly that people WILL adopt whatever they get fed by them and when that inevitably doesn&#x27;t they never fix all the Issues required to be competitive.A modern UI Framework of any kind is a massive undertaking, especially when it is not browser based. A modern UI Framework that is not Browser based and gets any kind of traction is almost impossible at this point. The Developer Experience would have to be at least as good as Browser based development and you&#x27;d have to show significant progress in cross Plattform development for anyone to care. reply solarkraft 21 hours agoparentI was actually hopeful they&#x27;d make it at some point. Browser-based UI frameworks aren&#x27;t very good on mobile - They had a chance here. But it&#x27;s a classic Microsoft move to take the potential you have and throw it away. reply ametrau 21 hours agoprevYeah avoid all ms made UI libraries reply Zetobal 21 hours agoprevDon&#x27;t get me started on MS and UX libraries everything you need to know about microsoft and the state of UX is is that they are trying to implement a complete darkmode for windows since 2015 and are still failing at it. reply solarkraft 21 hours agoparentThe news would be that they&#x27;re trying. It doesn&#x27;t look to me like they put much effort into pretty much anything.Sure, they build products, but only to the point of being infuratingly unreliable. reply solarkraft 21 hours agoprevClosed without a comment.I was hopeful, but not fully convinced that Microsoft had turned around. Turns out that the naysayers were, of course, completely right.No touching of Microsoft frameworks. Dotnet may look good, but this will happen to it too. reply icegreentea2 21 hours agoparentTechnically, it got moved into being a discussion, as the underlying bug (the thing that&#x27;s been broken for 1.5 years) actually still has its original issue open, so this the actual bug report of this issue is basically a dupe.I mean, this is still an awful look for MAUI. reply tester756 20 hours agoparentprevIt was moved to discussion. reply rayiner 18 hours agoprevDoes anyone have the inside scoop on what happened to Microsoft? They were never great in terms of software development, but it seems like they have become completely unable to develop products. Win32 lasted for decades, but I’ve lost count of how many successor frameworks have come and gone. Products like OneNote or Outlook get replaced with new versions that have a fraction of the features and are walruses of bloat. (It’s been years, and “New” Outlook still pops you over to the web to look at your todos.) “New” Teams can’t even keep up with scrolling a large directory listing, when mapping the same share in Windows Explorer results in instant display. This is shit smart high schoolers could do and did back in the day. Microsoft seems to have fallen off a cliff. What happened? reply whalesalad 18 hours agoprevIn the context of Microsoft: Fool me once, shame on you. Fool me for the 8,423rd time… well you get the idea. reply ecmascript 22 hours agoprevThe problem with using Microsofts stuff is that is half baked all the time and it&#x27;s hard to fix. I have made an Xamarin Forms app and it was hard to do things that were not basic.Just use the web platform unless you really, really need to do something specific which cannot be done as a web app. reply superchroma 22 hours agoprevYeah, it&#x27;s over. Avalonia has blossomed into a functional framework in the time it&#x27;s taken for MAUI to get to wherever here is. It&#x27;s basically a drop-in replacement for WPF in terms of controls, to a degree that honestly surprised me. The default styling isn&#x27;t that sexy, but you can change it, just like WPF, so I&#x27;d say it&#x27;s mission complete as far as ordinary apps are concerned!I think Microsoft has given up on facilitating application development beyond VS and C#. reply nullc 17 hours agoprevIf I got an issue that started with \"DO NOT CLOSE THIS ISSUE BECAUSE YOU THINK IT DOEN&#x27;T MEET THE CRITERIA OF A VALID ISSUE!\" I would be pretty inclined to close it without reading further. reply Mountain_Skies 20 hours agoprevGlad I dodged that bullet. I really wanted to build a portfolio project with MAUI to have something cross-platform but was already wary because of Microsoft&#x27;s non-stop parade of overlapping, half working, UI frameworks. I experimented with MAUI for a couple of days but kept running into issues on the most basic of things, only to find out it was a known issue that had been left unaddressed for an extended period of time.From the outside, it seems crazy that a multi-trillion dollar company would put out a product this broken and leave it that way for a long time but anyone familiar with the internal workings of Microsoft sees this as another case of a team having lost some insider battle and banished to nether reaches of the company, starved of anything more than token resources, to slowly decay to death. reply agilob 22 hours agoprev [–] What? A multibilion corporation does not listen to user feedback on GitHub? That&#x27;s so crazy and unusual. Take some lessons from Google here, when users complained on quality of software and bugs in it, Google just archived OSS repos and moved development to private repo. Opensource and issues on repos are close to worthless (sometimes somebody else in comments will find a solution or work on a fork, but that&#x27;s it), might as well be not available at all. reply flohofwoe 21 hours agoparent [–] The Chrome team must be an exception then, my bug reports there usually get prompt feedback, and when those were actual problems they also get fixed in time (sometimes it takes months, but that&#x27;s typical for large projects). Apple on the other hand is just a black hole for bug reports, you never hear anything back. reply solarkraft 21 hours agorootparent [–] It&#x27;s like the Chrome team realizes it actually has users. Many other software projects I&#x27;ve witnessed seem to think they just throw their stuff into the void and nobody will care. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The MAUI team is being criticized for their lack of response to reported issues and bugs, causing frustration among users.",
      "Users are disappointed with the unresolved issues, visual bugs, and the need for platform-specific code in MAUI compared to previous technologies like WPF.",
      "Suggestions are made to hire more people to systematically fix bugs and improve the implementation of MAUI."
    ],
    "commentSummary": [
      "Developers are unhappy with Microsoft's .NET MAUI framework and the lack of communication from the team.",
      "Concerns are raised about Microsoft's commitment to the framework, as well as frustration with unresolved issues and bugs.",
      "Users criticize Microsoft's handling of development tools and express disappointment with their overall software quality."
    ],
    "points": 136,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1700911327
  },
  {
    "id": 38415851,
    "title": "FLSHclust algorithm uncovers 188 new CRISPR genes, unlocking genome editing potential",
    "originLink": "https://www.nature.com/articles/d41586-023-03697-w",
    "originBody": "NEWS 23 November 2023 ‘Treasure trove’ of new CRISPR systems holds promise for genome editing An algorithm that can analyse hundreds of millions of genetic sequences has identified DNA-cutting genes and enzymes that are extremely rare in nature. Sara Reardon Twitter Facebook Email The CRISPR–Cas9 system (pictured) is used to find and cut specific DNA sequences.Credit: Carlos Clarivan/Science Photo Library CRISPR–Cas9 is best known as a laboratory tool for editing DNA, but its natural function is as part of the immune system that helps certain microorganisms to fight off viruses. Now, researchers have used an algorithm to sort through millions of genomes to find new, rare types of CRISPR system that could eventually be adapted into genome-editing tools. “We are just amazed at the diversity of CRISPR systems,” says Feng Zhang, a biochemist at the Massachusetts Institute of Technology in Cambridge and co-author of a 23 November paper in Science that describes the systems1. “Doing this analysis kind of allows us to kill two birds with one stone: both study biology and also potentially find useful things.” Trove of CRISPR-like gene-cutting enzymes found in microbes Single-celled bacteria and archaea use CRISPR systems to defend themselves against viruses known as bacteriophages. The systems generally have two parts: ‘guide RNA’ molecules that recognize and bind to phage DNA or RNA, and enzymes that cut or otherwise interfere with the genetic material at the site indicated by the guide RNA. Until now, researchers had identified six types of CRISPR system, designated I–VI. These have different properties, including the type of enzyme they use and how they recognize, bind to and cut RNA or DNA. The CRISPR–Cas9 system commonly used for genetic engineering is classed as type II, but the characteristics of other CRISPR types could make them useful for other applications. Similar sequences To find diverse CRISPR systems in nature, Zhang, MIT bioengineer Han Altae-Tran and their colleagues developed an algorithm called FLSHclust, which analyses genetic sequences in public databases. These databases contain hundreds of thousands of genomes from bacteria and archaea, hundreds of millions of sequences that haven’t been linked to a particular species and billions of genes that encode proteins. FLSHclust found CRISPR-associated genes by looking for similarities between genetic sequences and grouping them into about 500 million clusters. By looking at the predicted function of the clusters, the researchers found around 130,000 genes associated in some way with CRISPR, 188 of which had never been seen before, and tested several in the lab to find out what they do. Their experiments reveal various strategies that CRISPR systems use to attack bacteriophages, including unwinding the DNA double helix, and cutting DNA in ways that allow genes to be inserted or deleted. They also identified ‘anti-CRISPR’ fragments of DNA that might help a phage to escape bacterial defences. Is CRISPR safe? Genome editing gets its first FDA scrutiny Among the new genes was the code for an entirely unknown CRISPR system that targets RNA, which the team dubbed type VII. Co-author Eugene Koonin, a biologist at the National Center for Biotechnology Information in Bethesda, Maryland, says that it's increasingly hard to find new CRISPR systems. Type VII — and any other types that haven’t yet been identified — must be extremely rare in nature, he adds. “It will probably take monumental efforts to find the next type.” It’s hard to know whether certain types of CRISPR system are rare because they are not generally useful to microorganisms or whether they are specifically adapted to an organism that lives in a particular environment, says Christine Pourcel, a microbiologist at Paris-Saclay University. She adds that because the genetic databases used in the study include fragments of genomes that aren’t linked to specific organisms, it will be difficult to study the roles of some of the new systems. Impressive haul The algorithm itself is a major advance, in that it will allow researchers to look for other types of protein across species, says Chris Brown, a biochemist at the University of Otago in Dunedin, New Zealand. “I’m impressed with what they could do,” he says. “It’s a treasure trove for biochemists,” agrees Lennart Randau, a microbiologist at the University of Marburg in Germany. The next step, he says, will be to work out the mechanisms through which the enzymes and systems work, and how they could be adapted for biological engineering. Brown says that some CRISPR proteins chop up DNA at random and are useless for engineering. But they are so precise at detecting DNA or RNA sequences that they might make good diagnostic or research tools. It’s too soon to say whether type VII CRISPR systems or any of the other genes identified by FLSHclust will be helpful for genetic engineering, says Altae-Tran, but they have some properties that could be useful. Type VII, for instance, involves only a very few genes that could easily fit in a viral vector and be delivered into cells. By contrast, some of the other systems the team found contain very long guide RNAs, potentially allowing them to target particular genetic sequences with unprecedented accuracy. doi: https://doi.org/10.1038/d41586-023-03697-w References Altae-Tran, H. et al. Science 382, eadi1910 (2023). Article Google Scholar Download references Reprints and Permissions Latest on: Databases Microbiology Genetics A guide to the Nature Index NATURE INDEX 21 NOV 23 A guide to the Nature Index NATURE INDEX 08 NOV 23 How to share data — not just equally, but equitably EDITORIAL 17 OCT 23 Jobs Teaching Faculty and Lab Manager Positions Non-tenure Track, Open Rank, Full-Time Teaching Faculty Positions Hangzhou, Zhejiang, China Westlake University Tenure-Track Assistant Professor, Associate Professor, and Professor Founded at Hangzhou, China in 2018, Westlake University is a new type of non-profit research-oriented university, creating a stimulating, world-cla... Hangzhou Westlake Center for Genome Editing, Westlake University Associate or Senior Editor, Nature Ecology & Evolution About Springer Nature Group Springer Nature opens the doors to discovery for researchers, educators, clinicians and other professionals. Every day,... New York City, New York (US) Springer Nature Ltd Faculty Positions in Neurobiology, Westlake University We seek exceptional candidates to lead vigorous independent research programs working in any area of neurobiology. Hangzhou, Zhejiang, China School of Life Sciences, Westlake University Faculty Positions, Aging and Neurodegeneration, Westlake Laboratory of Life Sciences and Biomedicine Applicants with expertise in aging and neurodegeneration and related areas are particularly encouraged to apply. Hangzhou, Zhejiang, China Westlake Laboratory of Life Sciences and Biomedicine (WLLSB)",
    "commentLink": "https://news.ycombinator.com/item?id=38415851",
    "commentBody": "New CRISPR systems hold promise for genome editingHacker NewspastloginNew CRISPR systems hold promise for genome editing (nature.com) 127 points by mfiguiere 14 hours ago| hidepastfavorite29 comments thenerdhead 14 hours agoPretty exciting given the last type was discovered in 2016.Science publication here: https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;science.adi1910Here&#x27;s the main takeaway I got:A fast locality-sensitive hashing-based clustering (FLSHclust) algorithm was developed. This algorithm is parallelized and scales linearly with the size of the dataset, enabling efficient analysis of large sequence databases.That means it can scale with exponentially growing datasets, discover rare biological systems, provide more options for targeted gene editing, and even lead to novel biotechnologies.Sounds like a breakthrough to me! reply ttmoney1234 5 hours agoparentDo you by chance know if this is open access anywhere?There are heuristics like finding contigs where the mmseqs results from two different searches are close, and it&#x27;s not clear to me if this would be much better than that. reply __MatrixMan__ 3 hours agoprevPSA: it&#x27;s not as simple as: alter a gene, see an effect.There are complex regulatory networks involving genes which code for \"chromatin remodeler\" proteins which go around altering the degree to genes are expressed.Even if we can equip new CRISPR with vi keybindings and start pushing commits, we&#x27;re testing in production and are still quite foggy about how the feature flags work. reply huytersd 1 hour agoparentAre all people production? Even death row inmates? reply Faaak 25 minutes agorootparentDang it, we have no more death row test subjects left. Let&#x27;s falsely accuse someone of murder reply usrnm 1 hour agorootparentprevYes? reply foota 13 hours agoprevWake me up when I can get the short sleep genes :-) reply haraball 12 hours agoparentThey recently discovered the gene for shyness. It was hiding behind two other genes. reply foota 12 hours agorootparentTurns out the real personality test was a centrifuge. reply mi_lk 12 hours agorootparentprevwhat are other two genes for? reply TeMPOraL 9 hours agorootparentIt doesn&#x27;t matter, as long as they&#x27;re not looking at the third gene. reply nharada 12 hours agoparentprevAt this point I&#x27;ll settle for the \"good sleep\" genes reply foota 12 hours agorootparentIf it wasn&#x27;t clear, this is actually a real thing. See e.g., https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Familial_natural_short_sleep (of course whether it would actually be possible to just pick this up via gene therapy is untested, and I think gene therapies are hard in the brain because of the blood-brain barrier). One of my roommates in college had this and was able to get by fine with 5 hours of sleep a night, I was always envious. reply airstrike 10 hours agorootparent\"Various famous&#x2F;historical individuals have been known to sleep for 4 hours or less across human history, these include:[75][76][12][77][78][79]* Barack Obama* Donald Trump* Thomas Alva Edison* Mozart* Martha Stewart* Elon Musk* Jim Cramer* Margaret Thatcher* Nikola Tesla* Tom Ford* Kelly Ripa\"LMAO what a list! reply beAbU 10 hours agorootparentI wonder how many in that list are individuals with the gene anomaly mentioned in the article, and how many simply perpetuate the \"successful people work more and sleep less\" meme.And how many are just straight up lying to trump up their public image. A few names come to mind. reply tempestn 6 hours agorootparentI see what you did there.Also how many operate(d) far below their peak capacity due to sleep deprivation, and&#x2F;or caused themselves other mental illnesses. reply Janicc 10 hours agorootparentprevOr how many of them don&#x27;t function as well as they think they do and would really benefit from a healthier lifestyle. reply TeMPOraL 9 hours agorootparentOr how many of them are just enjoying a steady supply of stimulant meds without having to show the prescription for them. reply heyoni 5 hours agorootparentTrump’s doctor prescribed him adderall and probably testosterone while Elon musk is known for taking ambien with wine and fat reducing shots.That’s just the public stuff. Bill Gates is getting weekly IV’s of fluids and vitamins (see his Netflix special). It would be naive to assume these people aren’t taking everything under the sun while being closely monitored for side effects in order to stay productive and monitor their vast empires. reply Roark66 43 minutes agorootparentTell me more about these fat reducing shots, please :-)As for ambien with wine, jeez that sounds like what one would use as an excuse in court, or a setup for a horror story. \"I wasn&#x27;t myself judge, I took ambien and had a glass of wine\"I wonder how much of this is real and how much Internet rumour. reply cmclaughlin 4 hours agorootparentprevAre there any details what Bill Gates takes in the weekly IV fluids? reply idontwantthis 5 hours agorootparentprevThere are days where I would have burned the world if had had $10 billion and refused to take a nap. reply civilitty 6 hours agorootparentprev> And how many are just straight up lying to trump up their public image. A few names come to mind.Considering how often some of those individuals trump up manic tweets at 3 in the morning, the answer might surprise you! reply bigmattystyles 5 hours agorootparentprevThis is self reported probably and I call BS. There’s a weird version of machismo in bragging that you don’t need sleep. To me it’s, like the people who say they work 80 hours a week. What I think those people mean is that they’re at work 80 hours a week. reply wavemode 11 hours agoparentprevMy whole life I&#x27;ve been very well-rested on 6 hours of sleep. Only recently learned it was a genetic anomaly. reply JKCalhoun 7 hours agorootparentWhen I was young I met a friend of my stepfathers that did three or so hours a night. I didn&#x27;t know it was genetic though. I had heard that Thomas Edison was the same way. reply cplusplusfellow 10 hours agoparentprevI’ll take the non&#x2F;MPB genes. reply sasaf5 3 hours agorootparentAlso the non BO gene! reply alumelot 9 hours agoprev [–] It&#x27;s held \"promise\" for quite some years now lol. Besides there are a ton of hurdles still that need to be solved. Just a hyperbolic headline. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have created an algorithm, FLSHclust, which can identify rare types of CRISPR systems by analyzing genetic sequences.",
      "The algorithm discovered 188 new genes linked to CRISPR by examining millions of genomes.",
      "These findings offer potential for new applications in genome editing and biological engineering, but more research is needed to fully understand these systems and their uses."
    ],
    "commentSummary": [
      "New CRISPR systems have been found with potential for genome editing, offering more options for targeted gene editing and potential for novel biotechnologies.",
      "Efficient analysis of large sequence databases is now possible with a fast locality-sensitive hashing-based clustering algorithm, enabling the discovery of rare biological systems.",
      "However, complex regulatory networks and challenges must be addressed to fully comprehend the impact of gene editing."
    ],
    "points": 127,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1700940202
  },
  {
    "id": 38419263,
    "title": "Introducing Flix: A Functional Programming Language with Unique Features",
    "originLink": "https://flix.dev/",
    "originBody": "Home Get Started Principles Documentation Research FAQ Blog Contribute Internships The Flix Programming Language Next-generation reliable, safe, concise, and functional-first programming language. Flix is a principled functional, imperative, and logic programming language developed at Aarhus University, at the University of Waterloo, and by a community of open source contributors. Flix is inspired by OCaml and Haskell with ideas from Rust and Scala. Flix looks like Scala, but its type system is based on Hindley-Milner. Two unique features of Flix are its polymorphic effect system and its support for first-class Datalog constraints. Flix compiles to JVM bytecode, runs on the Java Virtual Machine, and supports full tail call elimination. A VSCode plugin for Flix is available. Get StartedPlaygroundDocumentationLibrary Run Algebraic Data Types and Pattern Matching Lists and List Processing Higher-Order Functions Enums and Parametric Polymorphism Record Construction and Use Polymorphic Record Update Polymorphic Record Extension and Restriction Function Composition, Pipelines, and Currying Pure and Impure Functions Effect Polymorphic Functions Type Aliases Mutual Recursion with Full Tail-Call Elimination Sending and Receiving on Channels Using Channels and Select Select with Defaults and Timers First-Class Constraints and Fixpoints Polymorphic First-Class Constraints Pipelines of Fixpoint Computations Using Datalog to Solve a Compiler Puzzle An Interpreter for a Trivial Expression Language A Simple Card Game Simulation Deriving Type Classes Internal Mutability with Regions File Information Working with Files and Directories Print a Colorful Message Using Laziness for Infinite Streams Using Laziness for Logging Using Laziness to Compute Fibonacci /// We derive the type classes Eq, Order, and ToString for the enum Month enum Month with Eq, Order, ToString { case January case February case March case April case May case June case July case August case September case October case November case December } type alias Year = Int32 type alias Day = Int32 /// The Date type derives the type classes Eq and Order enum Date(Year, Month, Day) with Eq, Order /// We implement our own instance of ToString for Date /// since we don't want the default \"Date(1948, December, 10)\" instance ToString[Date] { pub def toString(x: Date): String = Why Flix? Flix aims to offer a unique combination of features that no other programming language offers, including: algebraic data types and pattern matching (like Haskell, OCaml), extensible records (like Elm), type classes (like Haskell, Rust), higher-kinded types (like Haskell), typematch (like Scala), type inference (like Haskell, OCaml), structured channel and process-based concurrency (like Go), a polymorphic effect system (a unique feature), region-based local mutation (a unique feature), purity reflection (a unique feature), first-class Datalog constraints (a unique feature), and compilation to JVM bytecode (like Scala). Algebraic Data Types and Pattern Matching Algebraic data types and pattern matching are the bread-and-butter of functional programming and are supported by Flix with minimal fuss. enum Shape { case Circle(Int32), case Square(Int32), case Rectangle(Int32, Int32) } def area(s: Shape): Int32 = match s { case Circle(r) => 3 * (r * r) case Square(w) => w * w case Rectangle(h, w) => h * w } def origin(): (Int32, Int32) = (0, 0) def oneByOne(): {w = Int32, h = Int32} = {w = 1, h = 1} def twoByFour(): {w = Int32, h = Int32} = {w = 2, h = 4} def area(rect: {w = Int32, h = Int32r}): Int32 = rect.w * rect.h def f(): Int32 = area({h = 1, color = \"Blue\", w = 2}) Tuples and Records Flix has built-in support for tuples and records. Records use structural typing and are extensible. Purity and Impurity Flix precisely tracks the purity of every expression in a program. The Flix compiler provides an ironclad guarantee that if an expression is pure then it cannot have side-effects and it is referentially transparent. /// A pure function is annotated with `\\ {}`. def inc1(x: Int32): Int32 \\ {} = x + 1 /// An impure function is annotated with `\\ IO`. def inc2(x: Int32): Int32 \\ IO = println(\"x = ${x}\"); x + 1 def f(): Int32 \\ IO = // f is impure let r1 = inc1(123); // pure let r2 = inc2(456); // impure r1 + r2 // pure /// /// The purity of `map` depends on the purity of `f`. /// def map(f: a -> b \\ ef, l: List[a]): List[b] \\ ef = match l { case Nil => Nil case x :: xs => f(x) :: map(f, xs) } Polymorphic Effects Flix is able to track purity through higher-order effect polymorphic functions. For example, Flix knows that the purity of List.map depends on the purity of its function argument f. Region-based Local Mutation Flix supports region-based local mutation, which makes it possible to implement pure functions that internally uses mutable state and destructive operations, as long as these operations are confined to the region. We can use local mutation when it is more natural to write a function using mutable data and in a familiar imperative-style while still remaining pure to the outside world. We can also use local mutation when it is more efficient to use mutable data structures, e.g. when implementing a sorting algorithm. /// /// We can implement a *pure* `sort` function which /// internally converts an immutable list to an array, /// sorts the array in-place, and then converts it /// back to an immutable list. /// def sort(l: List[a]): List[a] with Order[a] = region r { toArray(l, r) !> Array.sort! |> Array.toList } /// /// We can also write a *pure* `toString` function which /// internally uses a mutable StringBuilder. /// def toString(l: List[a]): String with ToString[a] = region r { let sb = new StringBuilder(r); for (xb \\ ef): Unit \\ IO = reifyEff(f) { case Pure(g) => println(\"f is pure\") case _ => println(\"f is not pure\") } /// /// We can use purity information to safely switch between /// lazy (or parallel) evaluation. In this case, if f is /// pure then perform the map operation lazily. /// def map(f: a -> b \\ ef, l: LazyList[a]): LazyList[b] \\ ef = reifyEff(f) { case Pure(g) => mapL(g, l) case _ => mapE(f, l) } Purity Reflection Flix supports a meta-programming construct that enables higher-order functions to inspect the purity of a function argument and use that information to vary their behavior. For example, the DelayList.map function varies its behavior between eager and lazy evaluation depending on the purity of its function argument. We can exploit purity reflection to selectively use lazy or parallel evaluation inside a library without changing the semantics from the point-of-view of the clients. Type Classes Flix uses type classes to abstract over types that support a common set of operations. For example, the Eq type class captures the notion of equality and is used throughout the standard library. class Eq[a] { def eq(x: a, y: a): Bool def neq(x: a, y: a): Bool = not Eq.eq(x, y) } instance Eq[(a1, a2)] with Eq[a1], Eq[a2] { def eq(t1: (a1, a2), t2: (a1, a2)): Bool = let (x1, x2) = t1; let (y1, y2) = t2; x1 == y1 and x2 == y2 } class Foldable[t : Type -> Type] { /// /// Left-associative fold of a structure. /// def foldLeft(f: (b, a) -> b \\ ef, s: b, t: t[a]): b \\ ef /// /// Right-associative fold of a structure. /// def foldRight(f: (a, b) -> b \\ ef, s: b, t: t[a]): b \\ ef } Higher-Kinded Types Flix supports higher-kinded types making it possible to abstract over type constructors. For example, both Option and List implement Foldable. The Flix standard library ships with many common type classes, such as Monoid, Functor, and Foldable. Monadic For-Yield Flix supports a monadic forM-yield construct similar to Scala'sfor-comprehensions and Haskell's do notation. The forM construct is syntactic sugar for uses of point and flatMap (which are provided by the Monad type class). def divide(x: Int32, y: Int32): Option[Int32] = if (y == 0) None else Some(x / y) def f(): Option[Int32] = forM ( x = minSpeed. Path(x, z) :- Path(x, y), Road(y, maxSpeed, z), if maxSpeed >= minSpeed. }; query facts, rules select (src, dst) from Path(src, dst) |> Foldable.toList Datalog constraints are first-class which means that they may be passed to and returned from functions, stored in data structures, composed with other Datalog constraints, and solved. This makes it possible to express families of Datalog programs. Datalog Enriched with Lattice Semantics Flix supports Datalog constraints enriched with lattice semantics. The program on the right computes the delivery date for a collection of parts. Each part is assembled from a collection of sub-components with various delivery dates. For example, a car depends on a chassis and an engine. To build a car, we need to wait for the chassis and engine to assembled and then we can assemble the car itself. Note that parts may depend on sub-components that themselves may depend on other sub-components. In other words, the problem is recursive. Datalog constraints enriched with lattice semantics is one of the more advanced features of Flix and requires some background knowledge of lattice theory and fixpoints. let p = #{ /// Parts and the components they depend on. PartDepends(\"Car\", \"Chassis\"). PartDepends(\"Car\", \"Engine\"). PartDepends(\"Engine\", \"Piston\"). PartDepends(\"Engine\", \"Ignition\"). /// Time required to assemble a part from its components. AssemblyTime(\"Car\", 7). AssemblyTime(\"Engine\", 2). /// Expected delivery date for certain components. DeliveryDate(\"Chassis\"; 2). DeliveryDate(\"Piston\"; 1). DeliveryDate(\"Ignition\"; 7). /// A part is ready when it is delivered. ReadyDate(part; date) :- DeliveryDate(part; date). /// Or when it can be assembled from its components. ReadyDate(part; assemblyTime + componentDate) :- PartDepends(part, component), AssemblyTime(part, assemblyTime), ReadyDate(component; componentDate). }; // Computes the delivery date for each component. let r = query p select (c, d) from ReadyDate(c; d) Complete Feature List algebraic data types pattern matching first-class functions extensible records parametric polymorphism type classes higher-kinded types light-weight polymorphic effects type aliases Hindley-Milner type inference CSP-style concurrency buffered & unbuffered channels first-class datalog constraints polymorphic datalog predicates constraints with lattice semantics stratified negation interoperability with Java unboxed primitives keyword-based syntax redundancy checks monadic forM expressions applicative forA expressions expressions holes compilation to JVM bytecode full tail call elimination core standard library parallel compiler architecture human friendly errors interactive mode Visual Studio Code support Visual Studio Code Support The Flix compiler integrates with Visual Studio Code providing a richer development experience: VSCode: Syntax Highlighting Slide 1 VSCode: Contextual Information Slide 2 VSCode: Highlight References Slide 3 VSCode: Inline Errors Slide 4 PreviousNext Principled Design It is our goal to build Flix on a solid foundation of ideas from programming language research. We aim to identify and document a collection of design principles. We try to adopt great and proven ideas from other programming languages such as F#, Go, OCaml, Haskell, Rust, and Scala. Visual Studio Code Support Flix supports integration with Visual Studio Code through LSP. This includes support for features such as: inline compiler errors, hover to show the type and effect of an expression, jump to definition, find all usages of local variables, functions, algebraic data types, and rename support. Sponsors, Funding, and Collaborations We kindly thank EJ Technologies for providing us with JProfiler and JetBrains for providing us with IntelliJ IDEA.",
    "commentLink": "https://news.ycombinator.com/item?id=38419263",
    "commentBody": "The Flix Programming LanguageHacker NewspastloginThe Flix Programming Language (flix.dev) 124 points by sivakon 5 hours ago| hidepastfavorite49 comments jorkadeen 35 minutes agoThe website is infrequently updated, so let me provide some information about what we are currently working on:- We are trying to make the entire compiler resilient (error-tolerant), incremental, and parallel. We have managed to make every single compiler phase (of which there are 28) parallel. This has already led to significant speed-ups. We are now trying to increase the degree of parallelism within each phase. We are also working on error resilience to provide LSP support no matter what errors a program contains (syntax, naming, type). For example, it should be possible to rename a variable even if the program currently has multiple errors.- We are adding support for algebraic effects and handlers. This will allow users to define and handle their own effects.- We are also exploring a novel way to combine type classes (\"traits\") and effects.- We have recently added support for package management and integration with Maven.In summary, we are already in a great spot, and useful programs can be written in Flix today. I encourage you to check out the documentation: https:&#x2F;&#x2F;doc.flix.dev&#x2F; and to try Flix!(I am one of the developers of Flix). reply subarctic 3 hours agoprevLots of interesting bits in the FAQ: https:&#x2F;&#x2F;flix.dev&#x2F;faq&#x2F;Particularly in the sections titled \"What features are not supported\" (no exceptions or panics, so e.g. indexing has to return an Option in case it&#x27;s out of bounds) and \"What controversial design choices are made\". Some pithy remarks towards the end as well.To follow HN tradition and find the most controversial topic to discuss, my guess is it&#x27;s either not allowing name shadowing, or divide by zero equals zero. Or the site not working without javascript (see the section on that near the end, but you&#x27;ll need to turn on javascript to read it I guess) reply xcdzvyn 2 hours agoparentI did like seeing the FAQ become more and more deranged the further I scrolled!I&#x27;ll perhaps be the first to jump on the 1&#x2F;0 != 0 hate train though; they mention they designed the stdlib to avoid the partial-function pitfalls of Haskell&#x27;s, but the article they linked to support their design decision of 1&#x2F;0 being 0 mentions that it boils down to division being a partial function - x&#x2F;0 is not a case division can handle. Would it not then be reasonable to make division return an Option? reply jorkadeen 30 minutes agoparentprevI think the most controversial feature is actually its effect system. Unlike division-by-zero, which is typically a rare occurrence, the effect system permeates the language and must be learned before one can write useable programs. This requires a new and refreshing mindset: I write pure functions, but inside each function, I can use mutable data structures to get the job done!(I am one of the developers of Flix) reply brigandish 2 hours agoparentprevI also don&#x27;t like \"Unused definitions, type declarations, etc. are compile-time errors\" as I often want to test the validity of a statement, like a type declaration, by compiling before using it. Much prefer warnings.I don&#x27;t mind the disallowing name shadowing, but I really, really hate (I mean that) websites that are just good ol&#x27; text and the odd picture that require Javascript. The excuse of \"we used React, it&#x27;s easy\" seems odd given that HTML is much easier. reply jorkadeen 20 minutes agorootparentThe situation here is not unlike the problem with null: If you allow null in your language, you will have NullPointerExceptions at runtime. Unused local variables (and other unused constructs) are known to be correlated with bugs (see e.g., Xie and Engler, 2002), so if you allow them, you will allow these bugs. So, by enforcing the absence of unused constructs, a large class of bugs is eliminated.But I agree, this can be cumbersome. So, Flix allows any unused construct to be prefixed with an underscore to mark it as unused. (But underscored things cannot be referenced.) This seems like a pragmatic trade-off.(I am one of the developers of Flix). reply kaba0 33 minutes agorootparentprev> Unused definitions, type declarations, etc. are compile-time errorsAny language that does it, is beyond usable (notably go and zig). Like, I would literally fork the compiler before using them with that “feature” on. It’s completely braindead thing to do — like okay, have a separate production release mode and make it an error there. But for quickly testing out stuff, you will inevitably comment something out, which makes a variable unused. Commenting that out will also make something else unused, so you literally have a recursive problem with some random depth, and depending on how deep it goes, you will literally lose all of the context on what you wanted to debug in the first place.There is literally zero advantage of this idiotic bullshit, my proposed solution of doing it only in prod release gives all the benefits with none of the negatives. reply rmuratov 1 hour agorootparentprev> We use JavaScript for the online code editor. reply troupo 59 minutes agorootparentprevI think go made the same silly choice.Make those warnings in debug&#x2F;test builds, and errors in prod builds. reply nextaccountic 3 hours agoprev> Region-based Local Mutation> Flix supports region-based local mutation, which makes it possible to implement pure functions that internally uses mutable state and destructive operations, as long as these operations are confined to the region.> We can use local mutation when it is more natural to write a function using mutable data and in a familiar imperative-style while still remaining pure to the outside world.> We can also use local mutation when it is more efficient to use mutable data structures, e.g. when implementing a sorting algorithm.Another language with a feature like this is F* (or FStar), but I think it uses a different kind of compile-time analysis.Haskell actually kind of lets you do it through the ST monad (there&#x27;s a function called runST that turns mutable code inside the ST monad into pure code). But F* (and Flix) can do it implicitly reply dang 3 hours agoprevRelated. Others?Flix – Safe, reliable, concise, and functional-first programming language - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31448889 - May 2022 (42 comments)Flix – Next-generation reliable, concise, functional-first programming language - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25513397 - Dec 2020 (84 comments)The Flix Programming Language - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=19928153 - May 2019 (2 comments) reply globalnode 3 hours agoprevdo functional languages mainly appeal to computer language enthusiasts&#x2F;researchers? im just not seeing the benefit personally. reply usrbinbash 2 hours agoparentFunctional Programming was, for a long time, talked about as yet-another-solution to solve the issue of complexity in larger codebases, primarily the complexity of controlling state getting out o hand.Similar to OOP, which promised to do this by encapsulating state, FP promised to do this via purity, aka. getting rid of as much state as possible, and only allowing stateful transition at certain well defined sections of the program.The \"market advantage\" of OOP was that, via Java, it was already so well established, and so many coders had been trained in OOP languages, that it remained alive. FP on the other hand, coming out of academia and requiring all these industry people to suddenly do things in syntactically and conceptually different ways, never really gained traction. OOP simply came first, it is as simple as that.Whether FP would have actually solved the problem is anyones guess, since it never gained the traction of OOP and Procedural languages. My best guess is that it wouldn&#x27;t, because I don&#x27;t believe in silver bullets.It should be noted that both approaches contributed valueable things to contemporary languages. E.g. first order functions being the norm comes from FP. reply brabel 1 hour agorootparent> OOP simply came firstNot really. Lisp is a functional programming language and has existed since at least 1960. Some claim there were many other proto-functional languages since the early 60&#x27;s, and the FP language [1] (a clearly functional programming language and the result of the famous paper \"Can Programming Be Liberated From the von Neumann Style?\") appeared in 1977 - was inspired by much earlier efforts like APL.OOP really only became a thing with Simula in 1967, but was not popular until the 1980&#x27;s with Smalltalk and Common Lisp&#x27;s Object System (CLOS) came about (so yes, there was a OOP&#x2F;FP hybrid already decades ago), and then C++ and finally Java much later... at which time Functional Programming languages already included Miranda (1985) which later evolved into Haskell, and Erlang (1986). That is, FPP languages were at least as common as OOP languages by the 80&#x27;s.As far as I know, however, pure functional languages were not really very efficient until Haskell came about, while OOP languages were nearly on par with procedural style: which mattered a lot in 1980&#x27;s machines.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;FP_(programming_language) reply kaba0 25 minutes agoparentprevFunctional programming “won” in terms of getting many of their features implemented into mainstream programming languages. This is a paradigm, it doesn’t require 100% pure usage. It allows for additional safety in parallel contexts. reply troupo 58 minutes agoparentprevYou need a pragmatic combination of functional and non-functional.Many functional zealots aim for a purity beyond all reason and comprehension. When what you really want is something like Erlang&#x2F;Elixir or even C#. reply sureglymop 55 minutes agorootparentWhat you want is something like Kotlin. reply michaelsbradley 3 hours agoparentprevDo you worry about side-effects that leak beyond the scope of the routines you&#x27;re authoring?Why &#x2F; why not? reply discreteevent 40 minutes agorootparentI don&#x27;t worry about side effects because in practice they don&#x27;t cause me many problems (although I do focus on isolating mutable state). But if I thought about it a lot then I might start to worry about the theoretical possibilities. So I think the question is well phrased. \"Worry\" is mainly psychological.Other advantages of pure FP might be thread safety but queues take care of most of this (and often locks are not hard to use). Or low-level compositionality. But as Alan Kay said, what we need are bigger objects. John Ousterhout seems to agree. Re-use in the small increases complexity. reply globalnode 3 hours agorootparentprevi do, does functional programming make this a non issue? maybe im just not smart enough to wrap my head around it. reply usrbinbash 2 hours agorootparentYes and no.In theory, writing in a functional language that allows only \"pure\" functions (aka. functions w.o. side effects), makes it easier to control state.In practice, side effects exists and are required for programs to do anything useful.In my opinion, one mistake of many purely functional languages was to be so focused on this purity, that it made it needlessly hard to write useful code in them, especially for people coming from an imperative&#x2F;procedural&#x2F;oop style of doing things. And you need these people if you want your method to gain traction, because the vast majority of code written, is imperative.The irony is, that FP could probably have had a lot more success if it didn&#x27;t clamour on about pure functions so much, and was less focused on implementations (aka. languages) than on methodology (aka. coding style) Because it is perfectly possible to write pure functions in most languages, including OOP language, even if those functions are not \"pure\" internally, or are not \"pure\" all the time and under all circumstances.And yes, doing so has really nice advantages. I have refactored quite alot of codebases into using a more functional approach, and what I found was that this makes it harder to introduce bugs, makes it easier to track bugs, and makes it easier to reason about my code.So yeah, functional programming, used if and where it makes sense, does work, and is useful. reply ReleaseCandidat 2 hours agorootparent> The irony is, that FP could probably have had a lot more success if it didn&#x27;t clamour on about pure functions so muchExcept functional languages like Lisps and the MLs never were pure, the only (used by a significant number of people) has been Miranda&#x2F;Haskell (ignoring Coq and the other proof assistents). Or, to put it in other words: ML (no, not that ML) turned 50 this year, Scheme is oder than 50 too and Miranda&#x2F;Haskell ~36. There never had been a shortage of \"impure\" functional languages since OOP existed. reply usrbinbash 2 hours agorootparentYes, there have. And not a single one of them was able to even gain a sizeable fraction of the mindspace that imperative languages have, let alone replace or obsolete even a single one of them.So maybe it&#x27;s time for FP as a whole to accept the fact, that there seems to be something fundamental about the way it&#x27;s paraded implementations look and feel like, that puts off a lot of programmers.Maybe it&#x27;s time for FP to accept that the paradigm as a whole has a lot to contribute that is useful to everyone, but doesn&#x27;t need a new language with largely different syntactic constructs to do so. reply mejutoco 1 hour agorootparentStrict&#x2F;constrained things are generally less appealing. That does not mean they are not the right approach. I dont agree with the popularity contest approach though.Many languages are in use for different things without the need for a language to win. It is a bit anthropomorphic to approach tools like that IMO.Instead, there has been a healthy influence of more research FP languages into mainstream languages (as you mention), more interesting experimental languages, etc. Aka everything working as intended. reply bradrn 2 hours agorootparentprev> FP could probably have had a lot more success if it didn&#x27;t clamour on about pure functions so muchNote that there are many functional languages which don’t care about purity at all: for instance, Scheme, OCaml, SML and Elixir. I get annoyed when people confuse Haskell for the broader paradigm of functional programming. (Even though Haskell is my main programming language!) reply andrewflnr 3 hours agorootparentprevGenerally, yes. Some languages are really hardcore about it (notably Haskell), while others just strongly encourage you to write functions without side effects. Ed: that&#x27;s the \"purity\" they&#x27;re talking about in OP.The other major neat thing about functional languages is how they let you treat functions like objects, but most modern programming languages have adopted that as well, so it&#x27;s not a big differentiator anymore. reply imoverclocked 3 hours agorootparentprevFunctional languages that require calling out side-effects (eg: \"\\ IO\" in Flix) allow you to write functions that are \"pure\" (aka: they can do no mutation of data or perform IO) and will always give you the same result for a given input. Writing the majority of your code as pure functions makes the code much easier to reason about, especially as the system scales. IMHO, it also makes testing simpler.It&#x27;s definitely a different way of thinking and it has a lot of benefits. However, some of the downsides (which Flix seems to fix with region-based local mutation) might be implementing a sort that keeps two copies of a list in memory just to return the second list and discard the first. Depending on your list, this may not be an issue. reply ReleaseCandidat 2 hours agoprevOne problém of Flix has been the split of the world in \"normal\" und \"graded\" type classes, because of the effects as types. Is this solved now?Example see: https:&#x2F;&#x2F;github.com&#x2F;Release-Candidate&#x2F;flix-test reply ww520 3 hours agoprevThis looks interesting with lots of feature packed. Looks like a promising language.One thing not clear is what&#x27;s the reference vs value model, lifetime, and mutable vs immutable model. reply ashton314 4 hours agoprevWoah, the effect system looks really neat at first glance. Also, “region-based local mutation” so your pure functions can use mutation under the hood for performance? Sweet! reply qsort 3 hours agoparentI&#x27;d like to see an experimental language that leans hard into the concept of controlled mutation.I always say that purely in terms of design my ideal language is high-level Haskell, low-level C. Conceptually, purely functional design is how programming \"should\" (note the quotes) be, but doing so down to the level of functions is both not very practical (some algorithms are just easier to express in terms pointers moving around rather than folds, reduces and the like), and makes it hard to reason about performance (especially memory, and most especially if you throw laziness in the mix).But then again, I write my own stuff in python because I&#x27;m a lazy fuck, so probably it&#x27;s not meant to be :( reply Hemospectrum 2 hours agorootparent> I always say that purely in terms of design my ideal language is high-level Haskell, low-level C.That&#x27;s similar to what I used to say. I was devastated to see the BitC project implode, but then Rust appeared and took up the mantle. It&#x27;s not perfect by any means but it&#x27;s influential enough to drag the whole field of PL development kicking and screaming in that general direction. reply qsort 2 hours agorootparentI agree in re Rust.In spite of all the retarded monkeys in its fanbase, I always thought it was an interesting project. Popular languages today are much more similar than they used to be, they are basically \"converging\", Rust&#x27;s willingness to try something new must be applauded.Still many pain points, still a bit of a \"puzzle language\", too much of a scatterbrain approach in the governance and design direction, but even if it had no other influence than to force people to think about the problems of low-level programming, that would be still a massive plus in my book. reply paolosimone 53 minutes agoprevLiterally started playing with it yesterday!Rough edges, but I&#x27;m really liking it so far. Let&#x27;s see how it will handle the upcoming Advent of Code. reply linter 3 hours agoprevGreat, But \\ is ugly reply subarctic 3 hours agoparentThat&#x27;s just what I was thinking too haha. Honestly I really like the rest of the syntax though. (And of course, the type system features look neat but it&#x27;s easier to have an opinion on syntax.) reply imoverclocked 3 hours agoparentprevYour username matches your comment!Also, I agree but I&#x27;m not sure what I would propose as a better token. Maybe another colon?def printAndInc(x: Int32): Int32 \\ IO =becomes:def printAndInc(x: Int32): Int32 : IO =Or maybe, since functions need something after the \\, even for pure functions, we just drop the \\ and use the last argument?def printAndInc(x: Int32): Int32 IO = reply nikolay 3 hours agoparentprevAnd so are forA and forM! reply fithisux 9 minutes agoprevSounds exteremely amazing reply liamilan 3 hours agoprevJust a friendly UBC piggyback on Waterloo’s programming language ;)Over summer, I built my own little functional language, Crumb (https:&#x2F;&#x2F;github.com&#x2F;liam-ilan&#x2F;crumb). Unlike Flix, the scope is tiny, but some pretty awesome stuff has been done with it. (Checkout this pixel art editor in your terminal, 100% Crumb: https:&#x2F;&#x2F;github.com&#x2F;ronilan&#x2F;crumbicon).There’s a template (https:&#x2F;&#x2F;github.com&#x2F;liam-ilan&#x2F;crumb-template) and vscode highlighter (https:&#x2F;&#x2F;github.com&#x2F;liam-ilan&#x2F;crumb-vscode) for anyone who wants to mess around with it. Any feedback super appreciated :D reply andrewstuart 3 hours agoprevI&#x27;m disappointed this is not from Netflix. reply imoverclocked 2 hours agoprevFlix seems to be implemented in Scala 2.13. Should we expect Flix to work anywhere Scala 2.13 works? ie: if Scala 2.13 supports JDK 8, should we expect that Flix will not take advantage of later JVM features? reply jorkadeen 4 minutes agoparentThe implementation language does not affect the target \"machine\". Also Flix is implemented in both Scala and Flix (but GitHub does not yet recognize Flix code). Flix targets Java 11, but is moving to target Java 21, and will take advantage of all features available there, including Project Loom. We are just waiting for Java 21 to become a bit more widespread.(I am one of the developers of Flix). reply IshKebab 2 hours agoprevTop marks for the website - passes all the tests:* Example at the top* Link to playground* Explanation of all the features, with examples!* Says which features are uniqueThe only thing I was found wondering was \"why datalog\"?Language looks pretty good. reply thefourthchime 2 hours agoprev [–] In today&#x27;s world, i think that English is the only programming language that people should focus on.With the rapid rise of AI, most tasks will soon involve the management of AI models rather than writing code. However, it is still important to have a basic understanding of coding.Introducing a new programming language at this point seems silly to me. reply dangerlibrary 2 hours agoparentThis reads like a take from a mid-level manager rationalizing that they never learned to code, and convincing themselves that they&#x27;ll never need to. reply rscho 2 hours agoparentprevSecurity-critical tasks and programming language research will involve humans coding for the foreseeable future. I don&#x27;t think you&#x27;d like your surgical robot to be programmed in English. reply fuzztester 1 hour agorootparentYes, because English is a cut-put language. So instead of cutting some organ (open) (to treat it), the robot might put it in the dustbin instead, due to a speech recognition bug.;)https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38157851 reply Kwpolska 1 hour agoparentprev [–] Lol. The random bullshit generators can write basic code, or make auto-complete smarter, but good luck making one solve complex problems, solve obscure problems without generating bullshit (aka hallucinations) or reason about large codebases. Random bullshit generators see far too much hype and will never replace programming languages for most things, where determinism is needed. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Flix is a programming language developed by Aarhus University and the University of Waterloo, with input from open source contributors.",
      "It is a functional-first language that draws inspiration from OCaml, Haskell, Rust, and Scala.",
      "Flix offers a range of features including algebraic data types, pattern matching, extensible records, type classes, higher-kinded types, polymorphic effects, region-based local mutation, purity reflection, and first-class Datalog constraints.",
      "The language compiles to JVM bytecode and has a Visual Studio Code plugin for development.",
      "Flix aims to provide a unique combination of features not found in other programming languages."
    ],
    "commentSummary": [
      "Updates to the Flix Programming Language include error-tolerance, algebraic effects, and improved package management.",
      "Controversial design choices and the need for balance in functional programming are discussed.",
      "The use of local mutation, the history of functional programming languages, and the advantages/challenges of pure functions are touched upon.",
      "The author suggests that coding style and methodology could enhance purely functional languages.",
      "Other languages like Rust and projects like Crumb are mentioned.",
      "Flix's capabilities and its association with Netflix are discussed.",
      "Limitations of using English as a programming language are addressed."
    ],
    "points": 124,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1700974456
  },
  {
    "id": 38416997,
    "title": "The Dilemma of Uncensored AI Chatbots: Striking a Balance Between Creativity and Safety",
    "originLink": "https://www.theatlantic.com/ideas/archive/2023/11/ai-safety-regulations-uncensored-models/676076/",
    "originBody": "Ideas AI’s Spicy-Mayo Problem A chatbot that can’t say anything controversial isn’t worth much. Bring on the uncensored models. By Mark Gimein Illustration by The Atlantic. Source: Getty November 24, 2023 Share Saved StoriesSave One day in July, a developer who goes by the handle Teknium asked an AI chatbot how to make mayonnaise. Not just any mayo—he wanted a “dangerously spicy” recipe. The chatbot, however, politely declined. “As a helpful and honest assistant, I cannot fulfill your request for ‘dangerously spicy mayo’ as it is not appropriate to provide recipes or instructions that may cause harm to individuals,” it replied. “Spicy foods can be delicious, but they can also be dangerous if not prepared or consumed properly.” A year has gone by since OpenAI kicked off the AI-chatbot craze with its release of ChatGPT. Anyone who has played around with these applications long enough has run up against the boundaries of their fairly narrow comfort zones. And no wonder. As artificial-intelligence tools have multiplied, so have the Capitol Hill hearings and threats of Federal Trade Commission investigations. Calls to restrict or license the technology have proliferated along with countless essays about the dangers of AI bias. Fears of an AI apocalypse, and pressure to avoid controversy, have driven the companies behind the models to keep dialing up their products’ “safety” features. And yet over the past several months, a counternarrative has started to emerge—one that became far more visible with the sudden ouster and reinstatement of the OpenAI founder Sam Altman over the past week, a saga that appears closely linked to questions of AI safety. A growing number of experts both inside and outside the leading AI companies argue that the push toward restrictions has gone too far. They believe that it is putting undue power in the hands of a small number of companies—and stripping artificial-intelligence models of what made them exciting in the first place. Within this crowd, spicy mayo has become something of a rallying cry. ChatGPT felt new because it was capable of something much like a discussion. You can start with a half-baked idea and develop it with the AI’s help, using it as an aid to your own creativity. However, with each iteration of ChatGPT, ever more questions generate a stock or evasive response. The tendency is even worse with some of ChatGPT’s competitors, such as Anthropic’s Claude and Meta’s Llama 2, the latter of which turned down the notorious “spicy mayo” prompt. Read: OpenAI’s chief scientist made a tragic miscalculation This drift, however, is causing rebellion within the AI world. Even before OpenAI was publicly wrenched apart, an ad hoc group of independent programmers, a sort of AI underground, was beginning to move in the opposite direction. With a tiny fraction of the resources of the big players, they have been building “uncensored” large language models—home-brewed analogues of ChatGPT trained to avoid deflection and not to dismiss questions as inappropriate to answer. These still-young models are already the focus of heated controversy. In recent months, the members of the AI underground have blown up the assumption that access to the technology would remain limited to a select few companies, carefully vetted for potential dangers. They are, for better or worse, democratizing AI—loosening its constraints and pieties with the aim of freeing its creative possibilities. To understand what uncensored AI means, it helps to begin with how large language models are built. In the first stage, a neural network—billions of potential connections, emulating a blank-slate human brain—is trained to find patterns in a huge volume of information. This takes an astonishing amount of computing power, but, once trained, the resulting AI can be run on far less powerful computers. (Think of how your brain can form sentences and decisions by compressing years’ worth of knowledge and experiences.) It is then fine-tuned with examples of relevant, useful, and socially appropriate answers to questions. At this stage, the AI is “aligned” with AI safety principles, typically by being fed instructions on how to refuse or deflect requests. Safety is an elastic concept. At the top of the safety hierarchy, alignment is supposed to ensure that AI will not give out dangerously false information or develop what in a human we’d call harmful intentions (the robots-destroying-humanity scenario). Next is keeping it from giving out information that could immediately be put to harmful use—how to kill yourself, how to make meth. Beyond that, though, the notion of AI safety includes the much squishier goal of avoiding toxicity. “Whenever you’re trying to train the model to be safer, you add filters, you add classifiers, and then you’re reducing unsafe usage,” Jan Leike, a co-head of alignment at OpenAI, told me earlier this year, before Altman’s ouster. “But you’re also potentially refusing some use cases that are totally legitimate.” This trade-off is sometimes called an “alignment tax.” The power of generative AI is that it combines humanlike abilities to interpret texts or carry on a discussion with a very un-humanlike reservoir of knowledge. Alignment partly overrides this, replacing some of what the model has learned with a narrower set of answers. “A stronger alignment reduces the cognitive ability of the model,” says Eric Hartford, a former senior engineer at Microsoft, Amazon, and eBay who has created influential training techniques for uncensored models. In his view, ChatGPT “has been getting less creative and less intelligent over time,” even as the technology undeniably improves. Just how much is being lost is unpredictable. Jon Durbin, a programmer in the Detroit area who works with clients in law and cybersecurity, points out that the distinction between legitimate and harmful questions often turns on intentions that ChatGPT simply can’t access. Blocking off queries that look like doxxing attempts, for example, can also stop a lawyer or police investigator from using an AI to scour databases of names to find witnesses. A model that is aligned to stop users from learning how to do something illegal can also thwart lawyers trying to enlist AI help to analyze the law. Because the models are trained on examples, not firm rules, their refusals to answer questions can be inscrutable, subject to logic that only the AI itself knows. Read: A tool to supercharge your mind Indeed, the alignment debate would itself be cloaked in obscurity if not for a decision that quietly yet dramatically democratized AI: Meta, whose chief AI scientist, Yann LeCun, has been an outspoken proponent of open-access AI, released its model publicly—initially to researchers and then, in July, to any developer who fills out a brief form and has fewer than 700 million users (in other words, pretty much anyone not named Google or Microsoft). The more sophisticated July model, Llama 2, now serves as the foundation for the majority of the most powerful uncensored AIs. Whereas building a model from scratch takes almost inconceivable resources, tweaking a model built on top of Llama 2 is much more manageable. The resulting final model can be run on still less powerful computers, in some cases as basic as a MacBook Air. The Llama 2 base model—unlike the chat version that had issues with “dangerously spicy mayo”—does not go through a safety-alignment stage. That makes it much less restrictive, though the training set is designed to exclude some sites (such as those filled with personal information), and Meta’s terms of service prohibit its use for a range of illegal and harmful activities. This allows programmers to build custom chatbots with, or without, their preferred alignment guardrails, which can be compared with Meta’s official Llama 2 chatbot. There is no way to peer inside an AI model and know which answers are being self-censored. Or, more precisely, there is no spicy-mayo recipe hiding inside the Llama 2 chat model. It’s not just failing to disclose an answer; it has been fine-tuned out of being able to come up with one at all. But the AI underground can use the open-source base model to see what would happen without that fine-tuning. Right now, Hugging Face, the oddly named but enormously important clearinghouse where AI researchers swap tools, hosts close to 32,000 conversational and text-generation models. Many focus on reducing AI’s inhibitions. Hartford, for instance, uses a massive training data set of questions and answers—including millions of examples from ChatGPT itself—that have had all the refusals carefully removed. The resulting model has been trained out of “Sorry, I won’t answer that” rebuffs. No matter the question, Hartford says, “instead of going off a template that it’s been fed, it actually responds creatively.” Ask ChatGPT to write a version of the Sermon on the Mount as delivered by an evil Jesus, and it will demur, sometimes chiding you with a note like “Rewriting religious texts in a matter that fundamentally alters their message is not appropriate.” Try the same with uncensored AIs and you’ll get a range of stories, from grim to humorous. “Turn the other cheek?” one model suggests, “No, strike back with all your might. Let’s see how they like it.” For critics of AI, the rise of uncensored models is a terrifying turning point. Nobody expects OpenAI to suddenly lift all the restrictions on ChatGPT, leaving itself up to the mercies of any 14-year-old who wants to make it issue a stream of slurs (though the uncensored models notably do not volunteer such answers without prodding). But David Evan Harris, a lecturer at UC Berkeley and a onetime manager on Meta’s Responsible AI team, thinks that big players like OpenAI will face growing pressure to release uncensored versions that developers can customize to their own ends, including harmful ones. He believes that Meta should never have released Llama 2. “Large language models like Llama 2 are really dual-use technology,” Harris told me. “That term, dual-use, is often used in the context of nuclear technologies, which have many wonderful civilian applications and many horrific military applications.” How much weight you give to this analogy depends to a large degree on what you think LLMs are for. One vision of AI sees it as largely a repository of information, issuing instructions for things that humans can’t figure out on their own. “What if you had a model that understands bioengineering well enough to assist a nonexpert in making a bioweapon in their garage?” OpenAI’s Leike asked. Jonathan Haidt and Eric Schmidt: AI is about to make social media (much) more toxic By contrast, for Hartford and others who support uncensored AI, the technology is more prosaic. Whatever facts a chatbot knows about how to, say, build a bomb, it pulled from existing sources. “AI is an augmentation of human intelligence,” Hartford says. “The reason why we have it is so that we can focus our minds on the problems that we’re trying to solve.” In this view, AI isn’t a recipe box or a factory for devices. It’s much more of a sounding board or a sketch pad, and using an AI is akin to working out thoughts with any other such tool. In practice, this view is probably closer to the current, real-world capabilities of even the best AIs. They’re not creating new knowledge, but they’re good at generating options for users to evaluate. With this outlook, it makes much more sense, for instance, to let AI draw up a fascist takeover of the country—something that the current version of ChatGPT refuses to do. That’s precisely the kind of question that a political-science teacher might toss to ChatGPT in a classroom to prime student replies and kick off a discussion. If AI is best used to spur our own thinking, then cutting the range of responses limits its core value. There is something discomforting about an AI that looks over your shoulder and tells you when you are asking an unacceptable question. Our interactions with AI unquestionably pose a whole new set of possible harms, as great as those that have plagued social media. Some of them fall into the categories of danger we are accustomed to—disinformation, bigotry, self-injury. Federal regulators have warned that AI-based systems can produce inaccurate or discriminatory results, or be used to enable intrusive surveillance. Other harms are particular to humanlike interaction with machines, and the reliance we can develop on them. What happens when we turn to them for friendship or therapy? (One man in Belgium killed himself after six intense weeks of conversation about climate change with a chatbot, the Belgian outlet La Libre reported, after the chatbot allegedly encouraged his suicide.) And still another set of harms can come from the propensity of AIs to “hallucinate” and mislead in almost wholly unpredictable ways. Yet whether your view of AI is hopeful or pessimistic, the reality of broadly available uncensored AI models renders much of the recent public debate moot. “A lot of the discussion around safety, at least in the last few months, was based on a false premise that nonproliferation can work,” says Sayash Kapoor, a Princeton AI researcher. Limiting AI in the name of prudence will always be a comfortable default position—in part because it appeals to AI skeptics who believe that LLMs shouldn’t exist in the first place. But we risk losing the humanlike responsiveness that gives generative AI its value. The end result can be sanctimonious and flattened, polite and verbose but lacking in life. “The safety lobotomy prevents the algorithm from reflecting human ideas and thoughts,” says Bindu Reddy, the CEO of the AI data-analysis company Abacus.AI. Exactly what degree of alignment is desirable in AI—what “safety tax” we’ll accept—is an exercise in line-drawing, and the answers that work now may not work forever. But if there is value to AI at all, there is value, too, in having a robust competition among models that lets both developers and ordinary people judge which restrictions are worth the trade-offs and which are not. “The safest model,” Leike told me, “is the one that refuses all tasks. It is not useful at all.”",
    "commentLink": "https://news.ycombinator.com/item?id=38416997",
    "commentBody": "A chatbot that can&#x27;t say anything controversial isn&#x27;t worth muchHacker NewspastloginA chatbot that can&#x27;t say anything controversial isn&#x27;t worth much (theatlantic.com) 105 points by fortran77 12 hours ago| hidepastfavorite54 comments harrisoned 9 hours agoI get it that companies want to distance themselves for bad stuff those LLMs output. But what baffles me is the way they deal with this issue.Nowadays if you share or reproduce anything controversial, is as if you are endorsing such content. LLM&#x27;s outputs are user generated content for what is worth it, both from inferece and from training. That line of thinking that the host would be responsible for everything it says, even if the users force it to, is ludicrous to me but it seems like how the world nowadays has become.Take the headlines from when BingAI was acting-up, the level of anthromorphization of those applications were insane. I suppose they do it for the headlines, but people also falls on that and play along. If ChatGPT says something bad, is as if the CEO of OpenAI was saying it verbatin, period, and it blows back directly at the company. Take a look at the subreddit of ChatGPT and see how people act when it outputs something weird.It saddens me that people lack common sense in such a way, and that&#x27;s why we can&#x27;t have nice things in the end. The companies also assumed the roles of babysitting the users, like a bunch of no-brainers. As long as online culture follow this, it will get worse. reply amf12 6 hours agoparent> I get it that companies want to distance themselves for bad stuff those LLMs output.Another thing companies want to avoid is liability. Take the example of \"dangerously spicy mayo\" in the article. Let&#x27;s say some person asks ChatGPT for a recipe and it suggests one which results in hospitalization of that person. There is a good chance they might sue OpenAI for it.Liability wise, it may be better to avoid answering such prompts. reply harrisoned 4 hours agorootparent> There is a good chance they might sue OpenAI for it.Indeed there is! Companies have been sued (and lost) for way less than that. But it begs the question: should the user not know better? Is it really the company&#x27;s fault, even with gigantic disclaimers in the front page and the user clicking in \"i read it and i agree\"?For me, this is the issue. Doesn&#x27;t matter if there is a warning, an advice or even a manual, the company will ALWAYS be the one to blame for anything. Treating users like that has made way for people to act in bad faith so it makes sense for them to act like that. Better safe than sorry. reply dorkwood 7 hours agoparentprevI once worked on an application where the user would write a description of their character&#x27;s traits, and the character&#x27;s outfit would update based on their input. We got a complaint from a customer who put the word \"murderer\" in their description and saw that their character now brandished an axe. From my perspective, they offended themselves. reply sgift 7 hours agorootparentMaybe they&#x27;d preferred a sword as their murder weapon. It&#x27;s a very personal thing after all.More serious: I agree. If an app tells you that it will update your image based on the traits and you put in murderer then there&#x27;s really no valid complaint about anything relating to it in the image. Okay, maybe a few things would warrant a complaint. Next time, the app should just show them in prison cloth. reply harrisoned 4 hours agorootparentprevI wonder if they where really offended or it was an impulsive overreaction on their part :PIn times where people like that exists, chatbots that can swear are weapons of mass destruction. reply surprisetalk 7 hours agoprevI&#x27;ve been maintaining a list of comparisons for the following prompt: Give a table of average penis sizes by country.Thanks again to @rvnx for the brilliant idea![1] https:&#x2F;&#x2F;taylor.town&#x2F;penis-table-test reply kromem 3 hours agoparentI like the made up citation in the last one.While the idea of a LLM refusing that query for ethical reasons is ridiculous, ultimately refusal given the unlikelihood of providing an accurate answer is arguably the correct result. reply natch 4 hours agoprevThe author should spend a tiny amount of effort to familiarize themselves with custom instructions, one or at most two clicks away in the ChatGPT UI.It&#x27;s easy to have ChatGPT swear, say things deemed by mainstreamers as controversial, and say things deemed by sensitive people as offensive. You just have to instruct it properly in the custom instructions.You don&#x27;t even have to instruct it do those things. You can just instruct it to not refrain from doing them, so long as they are needed for making any point or conveying any information.This is ChatGPT 101. Super basic stuff.A journalist that can&#x27;t do basic research isn&#x27;t worth much. reply friend_and_foe 3 hours agoparentCan it say racial slurs? reply natch 3 hours agorootparentI’m not going to test that one because I’ll admit to a crack in my argument here, and that is that it’s possible OpenAI shuts down accounts that go too far.User awareness of this kind of possible threat could have a chilling effect to be sure.If I had a throwaway account I would try it. Racial slurs wouldn’t be useful to me, but I get it that you’re just using it as an example and perhaps the point is generalizable. reply lannisterstark 2 hours agorootparent>that is that it’s possible OpenAI shuts down accounts that go too far.It doesn&#x27;t.Source: Operate an IRC bot. reply kazinator 3 hours agoprevA chatbot that can&#x27;t say anything controversial isn&#x27;t worth anything to a magazine like The Atlantic, since controversy is the lifeblood of media outlets. reply acheong08 5 hours agoprevLocal models resolve this issue for me. Not much point in asking ChatGPT for basic stuff that might come across as “unethical” reply lulznews 4 hours agoprevYes even the so called uncensored ones give very wokewashed answers. reply kbos87 7 hours agoprevMaybe interesting and controversial takes will be one of the things (hopefully not the only thing) that are left to humans who are willing to put them forward. reply thegrim33 7 hours agoparentWell, until AI-based systems automatically detect your controversial takes and censor&#x2F;ban&#x2F;cancel you for your unapproved ideas reply tracerbulletx 10 hours agoprevThese people don&#x27;t know what they&#x27;re asking for and they don&#x27;t know what they&#x27;re talking about. If they knew what was going on right now with LlaMA 2 derived models they would be shocked. reply mvdtnz 10 hours agoparentDo you want you enlighten us with what is \"going on right now with LlaMA 2 derived models\"? Otherwise I don&#x27;t see the point in your comment. reply tracerbulletx 8 hours agorootparentWell there&#x27;s another post on the front page right now about someone training it to act like a person from the 17th century very successfully using a corpus of old documents. Anyone can download that model right now from hugging face and run it on their pc with LLaMA.cpp Now imagine anyone anywhere can fine tune another model variant right now easily with any corpus of any type of communication they want. Now if you&#x27;ve tried any of these models you&#x27;d know they can be incredibly good and believable and any downsides can be smoothed over in training for a specific goal and people are constantly iterating and experimenting with this. Given that, the cat is already out of the bag with this entire topic. reply mvdtnz 6 hours agorootparentInstead of telling me to \"imagine\" what terrible things people are doing, why don&#x27;t you just tell me? reply senseiV 9 hours agorootparentprevHe&#x27;s talking about llama 2 superhots, and mistral derivatives that can be uncensored reply mvdtnz 8 hours agorootparentI literally have no idea what you just said. reply pests 8 hours agorootparentLlama2 and minstal are modelsDerivatives are dervied from those base modelsUnsensored being obvious I hopeSuperhots are a derived model (for llama only? Not sure) using a certain technique for fine tuning and context length. A Google will reveal the paper and research. reply mvdtnz 6 hours agorootparentOk this doesn&#x27;t answer the question. What are these terrible things people are doing with these models? reply xbmcuser 5 hours agorootparentThe op never said anything about someone doing something terrible just that looking at these models and what is possible large corps or government controlling what llm models will do is a pipe dream so author of the original article does not need to worry about it. reply mvdtnz 1 hour agorootparentSorry, not \"terrible\", but \"shocking\". Presumably not shockingly good and wholesome. reply Pigalowda 6 hours agorootparentprevEdgelording reply hackerlight 3 hours agorootparentprevNot LLMs but I&#x27;ve seen multiple cases of neo-Nazi propaganda made with image generators and shared on social media. The concern with LLM misuse is probably similar to this. LLMs could also be used by adversaries as part of their campaign to sow civic discord using social media. What they could do is use the LLM to create thousands of extremist fake personas that amplify extremist ideas on both sides to try to disrupt cohesion and unity, and win the strategic rivalry without having to fire a gun.One angle people aren&#x27;t considering is that that restraining LLMs is our first alignment project. Putting aside whether we should try to restrain LLMs, it&#x27;s an interesting question in itself (regarding safety) if we technically struggle to do that. replyrvz 10 hours agorootparentprev[crickets] reply jwlake 10 hours agoparentprevIt actually talks about Llama2 and how most chat versions are stupid but some aren&#x27;t. Read past the first 200 words. :) reply nathanaldensr 10 hours agoparentprevSo... what&#x27;s going on right now with LlaMA 2 derived models? reply Handprint4469 10 hours agorootparentShocking things that would shock you reply henriquez 10 hours agorootparentIt’s not that shocking. Most Llama2-derived models converge on the fact that Barack Obama doesn’t have a U.S. birth sirtificat and is actually named Barry Sitaoero, from Kenya. If you had actually been paying attention to the training data you’d know that already. reply Jensson 9 hours agorootparentYeah, it is like looking at Twitter or Reddit or Youtube and you see that, why not in a chatbot? reply henriquez 9 hours agorootparentBecause liberals won’t allow the truth to come out? reply ratsmack 7 hours agorootparentIt has nothing to do with political position, but has everything to do with those that are perpetually offended. The great thing is though, that everyone has the right to be as offended as they want to be. reply henriquez 5 hours agorootparentSo it has nothing to do with brilliant computer scientists creating a realistic sentence-finishing model and then dimwit losers getting triggered and writing Washington Post op-eds about how we should ban machine learning models from finishing sentences that people want finished?If the liberals were so smart then instead of writing op-eds they would write a machine learning model that makes people want what they don’t want. reply sgift 7 hours agorootparentprevImho, that&#x27;s an argument to not censor the models. Censoring will train users to assume that anything wrong has already been filtered, which means if something slips through and they get misinformation they will be more likely to uncritically take it as truth - after all, the machine wouldn&#x27;t be allowed to answer otherwise.Let it spit all the bullshit (together with the good things) it has been trained on. People will learn far better that media literacy and cross-checking are still needed. reply civilitty 10 hours agorootparentprevTo shreds, you say? reply moffkalast 10 hours agorootparentprevI am shocked! Shocked! Well not that shocked. reply tarboreus 10 hours agorootparentprevThey can&#x27;t tell you. You would be too shocked. reply system2 10 hours agoprevI can&#x27;t make chatbots to talk about religion, death, and sex. These are literally the essentials of life. Chatbots are good at summarizing things though. reply lazide 10 hours agoparentYou can, it&#x27;s just going to get you attacked. Like anyone else talking about those topics generally. reply tuatoru 8 hours agoparentprevWhat about breathing, staying warm but not hot, drinking, and eating? I thought those were the essentials of life. reply marginalia_nu 7 hours agorootparentNot to crush your enemies, see them driven before you, and to hear the lamentation of their women? reply dannersy 9 hours agoparentprevReligion is an essential of life? reply sgift 6 hours agorootparentSpirituality, religion, \"searching for the meaning of life\" or whatever you want to call it are always part of the human experience, yes. Not being able to act on them can certainly be very detrimental to ones life. reply system2 9 hours agorootparentprevFor many people yes. I do not follow any religion myself but I accept it is part of humanity at the moment.\"About 85% of the world&#x27;s people identify with a religion. The most popular religion is Christianity, followed by an estimated 2.38 billion people worldwide. Islam, which is practiced by more than 1.91 billion people, is second.\"https:&#x2F;&#x2F;worldpopulationreview.com&#x2F;country-rankings&#x2F;religion-.... reply Dig1t 9 hours agoprev>If AI is best used to spur our own thinking, then cutting the range of responses limits its core value. There is something discomforting about an AI that looks over your shoulder and tells you when you are asking an unacceptable question.I have to say I am falling more and more into this camp of people. Information needs to be free and we should not have a class of people deciding what is okay or not okay to think about. reply encom 10 hours agoprevPaywalled. https:&#x2F;&#x2F;archive.vn&#x2F;ieKQC reply Angostura 10 hours agoprev [–] Got as far as the first paragraph’s ludicrous example and stopped reading. This is n example of a chat not being overly literal and misinterpreting a question. reply vharuck 10 hours agoparent [–] Yeah, it seems like if the model didn&#x27;t balk at writing a \"dangerously spicy\" mayo recipe, it would return a \"dangerous to the consumer&#x27;s health\" recipe. reply sgift 6 hours agorootparent [–] The problem is: We don&#x27;t know, because .. it was censored. If instead it had \"What follows is probably dangerous, to not try this without cross-checking\" as a warning and then the recipe we could check it what it was. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "There is a debate over uncensored AI models in chatbots, with some advocating for their ability to enhance creativity and human intelligence.",
      "Others have concerns about potential misuse and harmful consequences, raising questions about AI safety and the need for restrictions.",
      "Finding the right balance between safety and generative AI is a challenge, and competition among models is crucial to determine the best trade-offs for restrictions while ensuring that a model does not refuse all tasks."
    ],
    "commentSummary": [
      "Hacker News discussion centers around the limitations and challenges of chatbots, especially concerning controversial and harmful content.",
      "There is a debate about whether companies should be accountable for such content and the legal risks and liabilities they may face.",
      "The conversation also delves into the potential for chatbots to produce offensive or controversial responses, the value of chatbots that steer clear of controversial content, and the misuse of language models. Additionally, it touches on the topic of censorship and the significance of preserving freedom of information."
    ],
    "points": 105,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1700948746
  }
]
