[
  {
    "id": 39746468,
    "title": "YouTube Enforces Disclosure for AI-Generated Content",
    "originLink": "https://blog.google/intl/en-in/products/platforms/how-were-helping-creators-disclose-altered-or-synthetic-content/",
    "originBody": "YouTube How we're helping creators disclose altered or synthetic content 18 Mar, 2024 min read Share Twitter Facebook Linkedin Mail Copy link The YouTube Team Share Twitter Facebook Linkedin Mail Copy link Generative AI is transforming the ways creators express themselves – from storyboarding ideas to experimenting with tools that enhance the creative process. But viewers increasingly want more transparency about whether the content they’re seeing is altered or synthetic. That’s why today we’re introducing a new tool in Creator Studio requiring creators to disclose to viewers when realistic content – content a viewer could easily mistake for a real person, place, or event – is made with altered or synthetic media, including generative AI. As we announced in November, these disclosures will appear as labels in the expanded description or on the front of the video player. We’re not requiring creators to disclose content that is clearly unrealistic, animated, includes special effects, or has used generative AI for production assistance. The new label is meant to strengthen transparency with viewers and build trust between creators and their audience. Some examples of content that require disclosure include: Using the likeness of a realistic person: Digitally altering content to replace the face of one individual with another's or synthetically generating a person’s voice to narrate a video. Altering footage of real events or places: Such as making it appear as if a real building caught fire, or altering a real cityscape to make it appear different than in reality. Generating realistic scenes: Showing a realistic depiction of fictional major events, like a tornado moving toward a real town. Example of a label on the video player Of course, we recognize that creators use generative AI in a variety of ways throughout the creation process. We won’t require creators to disclose if generative AI was used for productivity, like generating scripts, content ideas, or automatic captions. We also won’t require creators to disclose when synthetic media is unrealistic and/or the changes are inconsequential. These cases include: Clearly unrealistic content, such as animation or someone riding a unicorn through a fantastical world Color adjustment or lighting filters Special effects like background blur or vintage effects Beauty filters or other visual enhancements You can see a longer list of examples in our Help Center. For most videos, a label will appear in the expanded description, but for videos that touch on more sensitive topics — like health, news, elections, or finance — we’ll also show a more prominent label on the video itself. Example of a label in the expanded description You’ll start to see the labels roll out across all YouTube surfaces and formats in the weeks ahead, beginning with the YouTube app on your phone, and soon on your desktop and TV. And while we want to give our community time to adjust to the new process and features, in the future we’ll look at enforcement measures for creators who consistently choose not to disclose this information. In some cases, YouTube may add a label even when a creator hasn't disclosed it, especially if the altered or synthetic content has the potential to confuse or mislead people. Importantly, we continue to collaborate across the industry to help increase transparency around digital content. This includes our work as a steering member of the Coalition for Content Provenance and Authenticity (C2PA). In parallel, as we previously announced, we’re continuing to work towards an updated privacy process for people to request the removal of AI-generated or other synthetic or altered content that simulates an identifiable individual, including their face or voice. We’ll have more to share soon on how we’ll be introducing the process globally. Creators are the heart of YouTube, and they’ll continue to play an incredibly important role in helping their audience understand, embrace, and adapt to the world of generative AI. This will be an ever-evolving process, and we at YouTube will continue to improve as we learn. We hope that this increased transparency will help all of us better appreciate the ways AI continues to empower human creativity. Posted in: YouTube",
    "commentLink": "https://news.ycombinator.com/item?id=39746468",
    "commentBody": "YouTube now requires to label their realistic-looking videos made using AI (blog.google)668 points by marban 17 hours agohidepastfavorite398 comments jjcm 15 hours agoI think it's smart to start trying things here. This has infinite flaws with it, but from a business and learnings standpoint it's a step toward the right direction. Over time we're going to both learn and decide what is and isn't important to designate as \"AI\" - Google's approach here at least breaks this into rules of what \"AI\" things are important to label: • Makes a real person appear to say or do something they didn't say or do • Alters footage of a real event or place • Generates a realistic-looking scene that didn't actually occur At the very least this will test each of these hypotheses, which we'll learn from and iterate on. I am curious to see the legal arguments that will inevitably kick up from each of these - is color correction altering footage of a real event or place? They explicitly say it isn't in the wider description, but what about beauty filters? If I have 16 video angles, and use photogrammetry / gaussian splatting / AI to generate a 17th, is that a realistic-looking scene that didn't actually occur? Do I need to have actually captured the photons themselves if I can be 99% sure my predictions of them are accurate? So many flaws, but all early steps have flaws. At least it is a step. reply OscarTheGrinch 1 hour agoparentPorn classification / regulation boils down to: \"I'll know it when I see it.\" Implying the existence of some hyper vigilant seer who can heroically determine what we should keep behind the video storre curtain of dencey, as if no grey areas exist. This also has the problem of requiring actual unbiased humans to view and accurately assess everything, which of course does not scale. Perhaps AI classification is the mirror opposite to porn, using the test: \"I'll know it when I don't see it\", ie, if an average user would mistake AI generated content for reality, it should be clearly labeled as AI. But how do we enforce this? Does such enforcement scale? What about malicious actors? We could conceivably use good AI to spot the bad AI, an endless AI cat and AI mouse game. Without strong AI regulation and norms a large portion of the internet will devolve into AI responding to AI generated content, seems like a gigantic waste of resources and the internet's potential. reply Intralexical 2 hours agoparentprevI think the real benefit for this is that probably that it establishes trust as the default, and acts as a discriminator for good-faith uses of \"AI\". If most non-malicious uses of ML are transparently disclosed, and that's normalized, then it should be easier to identify and focus on bad-faith uses. reply lysp 6 hours agoparentprevI think it also gives them a legal / business remedy if people fail to label their content. If someone for example makes a political video and fails to label it, they can delete the video/terminate the account for a breach of service. reply sverhagen 4 hours agorootparentGiven the regular stories posted on HN about folks who've had some aspect of their social or other media canceled by any some SaaS company, are these companies having many (legal) qualms as it is about canceling people without providing a good reason for it? Would be nice if they did, though... reply YetAnotherNick 2 hours agorootparentAt the very least it wouldn't be bad for PR if Google bans someone for specifically breaking a clear TOS. reply victorbjorklund 1 hour agorootparentAlways gonna be greyzones. Someone with a 60 min video where everything is real except 10 sec of insignificant b-roll footage reply GauntletWizard 3 hours agorootparentprevI'd much prefer Google cancel capriciously with solid TOS backing to it than without, but I'll complain about their double standards about what they choose to censor... Not regardless, but without a doubt, because Google will choose to selectively enforce this rule. reply chii 3 hours agorootparentprevbut this gives room to also abuse the uncertainty to censor anyone without recourse - by arguing such and such video is \"AI\" (true or not), they have a plausiblely deniable reason to remove a video. Power is power - can be used for good or bad. This labelling is a form of power. reply tsimionescu 2 hours agorootparentThis is no different from their existing power. They can already claim that a video contained copyright infringement, and you can only appeal that claim once, or try to sue Google. reply jjcm 15 hours agoparentprevOne black hat thing I'm curious about though is whether or not this tag can be weaponized. If I upload a real event and tag it as AI, will it reduce user trust that the real event ever happened? reply AnthonyMouse 15 hours agorootparentThe AI tags are fundamentally useless. The premise is that it would prevent someone from misleading you by thinking that something happened when it didn't, but someone who wants to do that would just not tag it then. Which is where the real abuse comes in: You post footage of a real event and they say it was AI, and ban you for it etc., because what actually happened is politically inconvenient. And the only way to prevent that would be a reliable way to detect AI-generated content which, if it existed, would obviate any need to tag anything because then it could be automated. reply mazlix 13 hours agorootparentI think you have a bit backwards. If you want to publish pixels on a screen there should be no assumption that they represent real events. If you want to publish proof of an event, you should have some pixels on a screen along with some cryptographic signature from a device sensor that would necessitate atleast a big corporation like Nikon / Sony / etc. being \"in on it\" to fake. Also since no one likes RAW footage it should probably just be you post your edited version which may have \"AI\" upscaling / de-noising / motion blur fixing etc, AND you can post a link to your cryptographically signed verifiable RAW footage. Of course there's still ways around that like your footage could just be a camera being pointed at an 8k screen or something but at least you make some serious hurdles and have a reasonable argument to the video being a result of photons bouncing off real objects hitting your camera sensor. reply AnthonyMouse 12 hours agorootparent> If you want to publish proof of an event, you should have some pixels on a screen along with some cryptographic signature from a device sensor that would necessitate atleast a big corporation like Nikon / Sony / etc. being \"in on it\" to fake. At which point nobody could verify anything that happened with any existing camera, including all past events as of today and all future events captured with any existing camera. Then someone will publish a way to extract the key from some new camera model, both allowing anyone to forge anything by extracting a key and using it to sign whatever they want, and calling into question everything actually taken with that camera model/manufacturer. Meanwhile cheap cameras will continue to be made that don't even support RAW, and people will capture real events with them because they were in hand when the events unexpectedly happened. Which is the most important use case because footage taken by a staff photographer at a large media company with a professional camera can already be authenticated by a big corporation, specifically the large media company. reply miki123211 11 hours agorootparentalso the three letter agencies (not just from the US) will have access to private keys of at least some manufacturers, allowing them to authenticate fake events and sow chaos by strategically leaking keys for cameras that recorded something they really don't like. reply eek2121 10 hours agorootparentFor all the folks that bash the United States for \"reasons\" this one gave me a chuckle. Our handling of privacy and data and such is absolute ass, but at least we *can* hide our data from big government with little repercussion in most cases (translation: you aren't actively being investigated for a crime that a judge isn't aware of) Of course that says nothing about the issues of corruption of judges in the court system, but that is a \"relatively\" new issues that DOES absolutely need to be addressed. (Shoot one could argue that the way certain folks are behaving right now is in itself unconstitutional and those folks should be booted) Countries all over the world (EVEN IN EUROPE WITH THE GDPR) are a lot less \"gracious\" with anonymous communication. The UK actually has been trying to outlaw private encryption, for a while now, as an example, but there are worse examples from certain other countries. You can find them by examining their political system, most (all? I did quit a bit of research, but also was not interested in spending a ton of time on this topic) are \"conservative leaning\" Note that I'm not talking just about existing policy, but countries that are continually trying to enact new policy. Just like the US has \"guarantees\" on free speech, the right to vote, etc. The world needs guaranteed access to freedom of speech, religion, right to vote, healthcare, food, water, shelter, electricity, and medical care. I don't know of a single country in the world, including the US, that does anywhere close to a good of job with that. I'm actually hoping that Ukraine is given both the motive and opportunity to push the boundaries in that regard. If you've been following some of the policy stuff, it is a step in the right direction. I 100% know they won't even come close to getting the job done, but they are definitely moving in the right direction. I definitely do not support this war, but with all of the death and destruction, at least there is a tiny little pinprick of light... ...Even if a single country in the world got everything right, we still need to find a way to unite everyone. Our time in this universe is limited and our time on earth more-so. We should have been working together 60 years ago for a viable off-planet colony and related stuff. If the world ended tomorrow, humanity would cease to exist. You need over 100,000 people to sustain the human race in the event a catastrophic event wipes almost everyone out. Even if we had 1,000 people in space, our species would be doomed. I am really super surprised that basic survival needs are NOT on the table when we are all arguing about religion, abortion, guns, etc. Like really? reply tsimionescu 2 hours agorootparent> We should have been working together 60 years ago for a viable off-planet colony and related stuff. If the world ended tomorrow, humanity would cease to exist. You need over 100,000 people to sustain the human race in the event a catastrophic event wipes almost everyone out. We are hundreds of years away from the kind of technology you would need for a viable fully self-sustainable off-world colony that houses 100k or more humans. We couldn't even build something close to one in Antarctica. This kind of colony would need to span half of Mars to actually have access to all the resources it needs to build all of the high-tech gear they would require to just not die of asphixiation. And they would need top-tier universities to actually have people capable of designing and building those high-tech systems, and media companies, and gigantic farms to make not just food but bioplastics and on and on. Starting 60 years earlier on a project that would take a millennium is ultimately irrelevant. Not to mention, nothing we could possibly do on Earth would make it even a tenth as hard to live here than on Mars. Nuclear wars, the worse bio-engineered weapons, super volcanoes - it's much, much easier to create tech that would allow us to survive and thrive after all of these than it is to create tech for humans to survive on a frozen irradiated dusty planet with next to no atmosphere. And Mars is still the most hospitable other celestial body in the solar system. reply LtWorf 12 minutes agorootparentprev> but at least we can hide our data from big government with little repercussion They come and ask. You say no? They find cocaine in your home. You aren't in jail because you refused to hand out data. You are in jail because you were dealing drugs. shafyy 2 hours agorootparentprev> I am really super surprised that basic survival needs are NOT on the table when we are all arguing about religion, abortion, guns, etc. Like really? Most people in the world struggle to feed themselves and their families. This is the basic survival need. Do you think they fucking care what happens to humantiy in 100k years? Stop drinking that transhumanism kool-aid, give your windows a good cleaning and look at what's happening in the real world, every day. reply robertlagrant 12 hours agorootparentprevI think at minimum YouTube could tag existing footage uploaded before 2015 as very unlikely to be AI generated. reply bandrami 8 hours agorootparentThe first (acknowledged) deepfake video is from 1997 reply wpietri 9 hours agorootparentprevI think doing this right goes the other direction. What we're going to end up with is a focus on provenance. We already understand that with text. We know that to verify words, we have to trace it back to the source, and then we evaluate the credibility of the source. There have been periods where recording technology ran ahead of faking technology, so we tended to just trust photos, audio, and video (even though they could always be used to paint misleading pictures). But that era is over. New technological tricks may push back the tide a little here and there, but mostly we're going to end up relying on, \"Who says this is real, and why should we believe them?\" reply miki123211 11 hours agorootparentprev> that would necessitate atleast a big corporation like Nikon / Sony etc. being \"in on it\" to fake Or an APT (AKA advanced persistent teenager) with their parents camera and more time than they know what to do with. reply mr_toad 8 hours agorootparentprevSo you could never edit the video? reply teaearlgraycold 11 hours agorootparentprevI worked in device attestation at Android. It’s not robust enough to put our understanding of reality in. Fine for preventing API abuse but that’s it. reply dataflow 9 hours agorootparent> I worked in device attestation at Android. It’s not robust enough to put our understanding of reality in. I don't follow. Isn't software backward compatibility a big reason why Android device attestation is so hard? For cameras, why can't the camera sensor output a digital signature of the sensor data along with the actual sensor data? reply qbit42 7 hours agorootparentI am not sure how verifying that a photo was unaltered after capture from a camera if very useful though. You could just take a photo of a high-resolution display when an edited photo on it reply dataflow 7 hours agorootparentThat wouldn't look nearly realistic. And it would be significantly harder to achieve for most people anyway. reply teaearlgraycold 9 hours agorootparentprevThere's been a slow march to requiring hardware-backed security. I believe all new devices from the last couple of years need a TEE or a dedicated security chip. At least with Android there are too many OEMs and they screw up too often. Bad actors will specifically seek out these devices, even if they're not very technically skilled. The skilled bad actors will 0-day the devices with the weakest security. For political reasons, even if a batch of a million devices are compromised it's hard to quickly ban them because that means those phones can no longer watch Netflix etc. reply dataflow 9 hours agorootparentBut you don't have to ban them for this use case? You just need something opportunistic, not ironclad. An entity like Google could publish those devices' certificates as \"we can't verify the integrity of these devices' cameras\", and let the public deal with that information (or not) as they wish. Customers who care about proving integrity (e.g., the media) will seek the verifiable devices. Those who don't, won't. I can't tell if I'm missing something here, but this seems much more straightforward than the software attestation problem Android has been dealing with so far. reply johnny22 7 hours agorootparentWoudln't that prevent most folks from being able to root their devices without making the camera lesser than everyone else's camera? reply dataflow 7 hours agorootparentWhat does this have to do with root? The camera chip would be the one signing the data flowing through it, not the Android kernel. reply 8note 6 hours agorootparentIf you do a jpeg compression, or crop the file, then does that signature matter anymore? reply dataflow 6 hours agorootparentNot if you do it, only if the chip also gives you a signed JPEG. Cropping and other simple transformations aren't an issue, though, since you could just specify them in unsigned metadata, and people would be able to inspect what they're doing. Either way, just having a signed image from the sensor ought to be adequate for any case where the authenticity is more important than anesthetics. You share both the processed version and the original, as proof that there's no misleading alteration. reply chii 3 hours agorootparent> You share both the processed version and the original, as proof that there's no misleading alteration so you cannot share the original if you intend to black out something from the original that you don't want revealed (e.g., a face or name or something). The way you specced out how a signed jpeg works means the raw data _must_ remain visible. There's gonna be unintended consequences from such a system. And it aint even that trustworthy - the signing key could potentially be stolen or coerced out, and fakes made. It's not a rock-solid proof - my benchmark for proof needs to be on par with blockchains'. reply dataflow 2 hours agorootparent> The way you specced out how a signed jpeg works means the raw data _must_ remain visible. There's gonna be unintended consequences from such a system. You can obviously extend this if you want to add bells and whistles like cropping or whatever. Like signing every NxN sub-block separately, or more fancy stuff if you really care. It should be obvious I'm not going to design in every feature you could possibly dream of in an HN comment... And regardless, like I said: this whole thing is intended to be opportunistic. You use it when you can. When you can't, well, you explain why, or you don't. Ultimately it's always up to the beholder to decide whether to believe you, with or without proof. > And it aint even that trustworthy - the signing key could potentially be stolen or coerced out, and fakes made. I already addressed this: once you determine a particular camera model's signature ain't trustworthy, you publish it for the rest of the world to know. > It's not a rock-solid proof - my benchmark for proof needs to be on par with blockchains'. It's rock-solid enough for enough people. I can't guarantee I'll personally satisfy you, but you're going to be sorely disappointed when you realize what benchmarks courts currently use for assessing evidence tampering... reply johnny22 2 hours agorootparentprevah. I thought it'd be more in the vein of safetynet, but guess not. reply makeitdouble 7 hours agorootparentprevAI tags are to cover issues in the other direction: you publish an event as real, but they can prove it wasn't. If you didn't put the tag on it, malice can be inferred from your post (and further legal proceeding/moderation can happen) It's the same as paid reviews: tags and disclaimers exist to make it easier to handle cases where you intentionally didn't put them. It's not perfect and can be abused in other ways, but at least it's something. reply kube-system 12 hours agorootparentprev> The premise is that it would prevent someone from misleading you by thinking that something happened when it didn't, but someone who wants to do that would just not tag it then. And when they do that, the video is now against Google's policy and can be removed. That's the point of this policy. reply MBCook 13 hours agorootparentprevThat’s what I was thinking. Why don’t we just ask all scam videos to label themselves as scams while we’re at it? It’s nice honest users will do that but they’re not really the problem are they. reply makeitdouble 7 hours agorootparent> Why don’t we just ask all scam videos to label themselves as scams while we’re at it? We do, we ask paid endorsements to be disclaimed. reply anigbrowl 14 hours agorootparentprevNot convinced by this. Camera sensors have measurable individual noise, if you record RAW that won't be fakeable without prior access to the device. You'd have a straightforward case for defamation if your real footage were falsely labeled, and it would be easy to demonstrate in court. reply AnthonyMouse 14 hours agorootparent> Camera sensors have measurable individual noise, if you record RAW that won't be fakeable without prior access to the device. Which doesn't help you unless non-AI images are all required to be RAW. Moreover, someone who is trying to fabricate something could obviously obtain access to a real camera to emulate. > You'd have a straightforward case for defamation if your real footage were falsely labeled, and it would be easy to demonstrate in court. Defamation typically requires you to prove that the person making the claim knew it was false. They'll, of course, claim that they thought it was actually fake. Also, most people don't have the resources to sue YouTube for their screw ups. reply anigbrowl 10 hours agorootparentMoreover, someone who is trying to fabricate something could obviously obtain access to a real camera to emulate. Yes, but not to your camera. Sorry for not phrasing it more clearly: individual cameras have measurable noise signatures distinct from otherwise identical models. On the lawsuit side, you just need to aver that you are the author of the original footage and are willing to prove it. As long as you are in possession of both the device and the footage, you have two pieces of solid evidence vs. someone elses feels/half-assed AI detection algorithm. There will be no shortage of tech-savvy media lawyers willing to take this case on contingency. reply nomel 13 hours agorootparentprevMost consumer cameras require access menus to enable raw because dealing with RAW is a truly terrible user experience. The vast majority of image/video sensors out there don't even support raw recordings, out of the box. reply anigbrowl 10 hours agorootparentAnyone with a mid-to-upper range phone or better-than-entry level DSLR/bridge camera has access to this, and anyone who uses that camera to make a living (eg shooting footage of protests) understands how to use RAW. I have friends who are complete technophobes but have figured this out because they want to be able to sell their footage from time to time. reply fennecbutt 9 hours agorootparentprev\"Dealing with raw\" is one of the major reasons to use an actual camera these days. reply VelesDude 13 hours agorootparentprevUnfortunately video codecs love to crush that fine detail. reply Gregaros 14 hours agorootparentprevDMCA abuse begs to differ. reply anigbrowl 10 hours agorootparentThat's because of safe harbor provisions, which don't exist in this context. reply mr_toad 8 hours agorootparentprev> The AI tags are fundamentally useless. To the extent that they allow Google to exclude AI video from training sets they’re obviously useful to Google. reply towelpluswater 9 hours agorootparentprevI mean they’re building the labeled dataset right now by having creators label it for them. I would suspect this helps make moderation models better at estimating confidence levels of ai generated content that isn’t labeled as such (ie for deception). Surprised we aren’t seeing more of this in labeling datasets for this new world (outside of captchas) reply cottsak 7 hours agorootparentprevagreed! this is another censorship tool. reply Cthulhu_ 42 minutes agorootparentprevIf you upload a real event but you're the only source, it'll be doubted anyway; see also, most UFO sightings. reply JohnFen 12 hours agorootparentprevI fear that we're barrelling fast toward a future when nobody can trust anything at all anymore, label or not. reply Cthulhu_ 40 minutes agorootparentAnd this isn't new. A fad in films in the 90's was hyper-realistic masks on the one side, and make-up and prosthetics artists on the other, making people look like other people. Faking things is not new, and you've always been right to mistrust what you see on the internet. \"AI\" technology has made it easy, convenient, accessible and affordable to more people though, beforehand you needed image/video editing skills and software, a good voice mod, be a good (voice) actor, etc. reply sverhagen 4 hours agorootparentprevI was immediately thinking that the #AI labels are going to give people a false sense of trust, so that when someone posts a good-enough fake without the #AI label, it can do damage if it goes viral before it gets taken down for the mislabeling. (Kudos for the effort, though, YouTube.) reply Cthulhu_ 39 minutes agorootparentBehind the scenes, I'm 99% confident that Google has deployed AI detection tools and will monitor for it. That said, unless all the AI generators agree on a way to add an unalterable marker that something is generated, at one point it may become undetectable. May. reply Gigachad 5 hours agorootparentprevIt just goes back to trusting the source. If 5 media orgs post different recordings of the same political speech, you can be reasonably sure it actually happened, or at least several orders of magnitude more sure than if it's one blurry video from a no name account. reply pojzon 3 hours agorootparentAnd then you learn all of those media orgs are owned by the same billionare. There will be no way to say something is true beside seeing it with own eyes. reply nonethewiser 10 hours agorootparentprevWe have to expect people to think for themselves. People are flawed and will be deceived but trying to centralize critical thinking will have far more disastrous results. Its always been that way. Im not saying Youtube shouldn’t have AI labels. Im saying we shouldn’t assume they’re reliable. reply Barrin92 8 hours agorootparent>but trying to centralize critical thinking will have far more disastrous results No. Having sources of trust is the basis of managing complexity. When you turned the tap water on and bought a piece of meat at the butcher you didn't yourself verify whether its healthy right? You trust the medicine you buy contains exactly what is says on the label and didn't take a chemistry class. That's centralized trust. You rely on it ten thousand times a day implicitly. There need to be measures to make sure media content is trustworthy, because the smartest person on the earth doesn't have enough resources to critically judge 1% of what they're exposed to every day. It is simply a question of information processing. It's a mathematical necessity. Information that is collectively processed constantly goes up, individiual bandwith does not, therefore you need more division of labor, efficieny and higher forms of social organisation. reply xarope 5 hours agorootparentI'm probably paraphrasing Schneier (and getting it wrong), but getting water from the tap and having it polluted or poisonous, has legal and criminal consequences. Similarly getting meat from a butcher and having it tainted. Right now, getting videos which are completely AI/deepfaked to misrepresent, are not subject to the same consequences, simply because either #1 people can't be bothered, #2 are too busy spreading it via social media, or #3 have no idea how to sue the party on the other side. And therein lies the danger, as with social media, of the lack of consequences (and hence the popularity of swatting, pretexting etc) reply nonethewiser 7 hours agorootparentprev> Having sources of trust is the basis of managing complexity. This is a false equivalence that I’ve already addressed. > When you turned the tap water on and bought a piece of meat at the butcher you didn't yourself verify whether its healthy right? To a degree, yeah, you do check. Especially when you get it from somewhere with prior problems. And if you see something off you check further and adjust accordingly. Why resort to anology? Should we blindly trust YouTube to judge whats true or not? I stated that labeling videos is fine but what’s not fine is blindly trusting it. Additionally, comparing to meat dispenses with all the controversy because food safety is a comparatively objective standard. Compare, “is this steak safe to eat or not?” To “is this speech safe to hear or not?” reply knowaveragejoe 9 hours agorootparentprevThis bodes well for autocracies and would-be autocrats. It's the logical extreme of what they've been trying to do on social media over the last decade or so. https://en.wikipedia.org/wiki/Firehose_of_falsehood reply Aeolun 10 hours agorootparentprevHaving the tag weaponizes it by itself, because people will now consider any content without the tag real, whether it actually is or not. reply sangnoir 12 hours agorootparentprevI suspect we're headed into a world of attestation via cryptographically signed videos. If you're the sole witness, then you can reduce the trust in the event, however, if it's a major event, then we can fall back on existing news-gathering machinery to validate and counter your false tagging (e.g. if a BBC camera captured the event, or there is some other corroboration & fact checking). reply smt88 9 hours agorootparentHow does the signature help? It only proves that the video hasn't been altered since [timestamp]. It doesn't prove that it wasn't AI-generated or manipulated. reply leoqa 6 hours agorootparentIf I have a CCTV camera that is in a known location and a TPM that signs its footage, I could probably convince a jury that it’s legit in the face of a deepfake defense. That’s the bar- it’s not going to be infallible but if you don’t find evidence of tampering with the hardware then it’s probably going to be fine. reply Gigachad 5 hours agorootparentThis might be worse than nothing. It's exactly the same tech as DRM, which is good enough to stop the average person, but where tons of people have private exploits stashed away to crack it. So the judge and general public trust the system to be basically foolproof, while criminals can forge fake signatures using keys they extracted from the hardware. reply drexlspivey 9 hours agorootparentprevThe labels collected by google will certainly be used to train classifiers to detect AI created content so I think that’s a legit concern. reply yosito 7 hours agoparentprevAs someone who studied video production two decades ago, regarding the criteria you mentioned for AI: - Makes a real person appear to say or do something they didn't say or do - Alters footage of a real event or place - Generates a realistic-looking scene that didn't actually occur These are things that have been true of edited video since even before AI was a thing. People can lie about reality with videos, and AI is just one of many tools to do so. So, as you said, there are many flaws with this approach, but I agree that requiring labels is at least a step in the right direction. reply randmeerkat 9 hours agoparentprev> Alters footage of a real event or place I wonder if this will make all forms of surveillance, video or otherwise, inadmissible in court in the near future. It doesn’t seem like much of a stretch for a lawyer to make an argument for reasonable doubt, with any electronic media now. reply dragonwriter 1 hour agorootparent> I wonder if this will make all forms of surveillance, video or otherwise, inadmissible in court in the near future. No, it won't. Just as it does now, video evidence (like any other evidence that isn't testimony) will need to be supported by associated evidence (including, ultimately, testimony) as to its provenance. > It doesn’t seem like much of a stretch for a lawyer to make an argument for reasonable doubt, “Beyond a reasonable doubt” is only the standard for criminal convictions, and even then is based on the totality of evidence tending support or refute guilt, its not a standard each individual piece of evidence must clear for admissibility. reply Intralexical 2 hours agorootparentprevBy that rationale, all witness testimony and written evidence should already be inadmissible. This website focuses too much on the technical with little regard for the social a bit too often. Though in general, videos being easily fakable is still scary. reply Eiim 9 hours agorootparentprevBad evidence is not the same thing as inadmissible evidence. Evidence is admitted, and then the fact finder determines whether to consider it, and how much weight to give it. It is likely that surveillance video will be slightly less credible now, but can still be part of a large, convincing body of evidence. reply tsimionescu 2 hours agorootparentprevVideo evidence already requires attestation to be admissible evidence. You need a witness to claim that the footage comes from a camera that was placed there, that it was collected from the night of, etc. It's not like the prosecutor gets a tape in the mail and they can present it as evidence. reply leoqa 6 hours agorootparentprevThere will be a new side gig for ‘experts’ to explain deepfakes to a jury. reply albert_e 4 hours agoparentprevAnd some of these questions apply to more traditional video editing techniques also. If someone says \"I am not a crook\" and you edit out the \"not\", do you need to label it. What if it is done for parody. What if the edit is more subtle - where a 1 hour interview is edited into 10 minute excerpts. Mislabeled videos for propaganda. Or simply date or place incorrectly stated. Dramatic recreations as often done in documentaries. Etc reply 4ndrewl 15 hours agoparentprevIt's to comply with the EU AI regulatory framework. This step is just additional cost they wouldn't have voluntarily burdened themselves with. reply mjevans 3 hours agoparentprevTruth in 'advertising' is important. >> 17th angle - AI Generated vantage point based on existing 16 videos (reference links).keep holding ourselves back Not everyone works in ML. > with poorly written legislation This is a company policy, not legislation. > designed to garner votes Represent the will of the people? > while rival companies take strides Towards? > in the technology at rapid rates YouTube's \"Trending\" page isn't a research lab. Even if it was, why would honesty slow it down? reply tkiolp4 11 hours agoparentprevBasically, Google decides what’s real and what’s not. Cool. reply pizzafeelsright 11 hours agorootparentFor at least a dozen years it would seem. reply smt88 9 hours agorootparentprevOn their own platforms, yes. We need to break up their monopolies so that their choices don't matter as much. reply barfbagginus 9 hours agorootparentprevDon't worry Google has become incompetent - it is in the \"Fading AOL\" portion of its life cycle. Do you remember how incontinent AOL became in the final minutes before it became completely irrelevant? Sure it's taking longer with Google. But it's not a process that they can reverse. That means the system will be really really awful. So challengers can arise - maybe a challenger that YOU build, and open source! reply surajrmal 7 hours agorootparentI think you're living in a bubble if you believe that. reply okdood64 14 hours agoparentprevI know people on HN love to hate on Google, but at least they're a major platform that's TRYING. Mistakes will be made, but let's at least attempt at moving forward. reply summerlight 15 hours agoprevLooks like there is a huge grea area that they need to figure out in practice. From https://support.google.com/youtube/answer/14328491#: Examples of content creators don’t have to disclose: * Someone riding a unicorn through a fantastical world * Green screen used to depict someone floating in space * Color adjustment or lighting filters * Special effects filters, like adding background blur or vintage effects * Production assistance, like using generative AI tools to create or improve a video outline, script, thumbnail, title, or infographic * Caption creation * Video sharpening, upscaling or repair and voice or audio repair * Idea generation Examples of content creators need to disclose: * Synthetically generating music (including music generated using Creator Music) * Voice cloning someone else’s voice to use it for voiceover * Synthetically generating extra footage of a real place, like a video of a surfer in Maui for a promotional travel video * Synthetically generating a realistic video of a match between two real professional tennis players * Making it appear as if someone gave advice that they did not actually give * Digitally altering audio to make it sound as if a popular singer missed a note in their live performance * Showing a realistic depiction of a tornado or other weather events moving toward a real city that didn’t actually happen * Making it appear as if hospital workers turned away sick or wounded patients * Depicting a public figure stealing something they did not steal, or admitting to stealing something when they did not make that admission * Making it look like a real person has been arrested or imprisoned reply Lerc 9 hours agoparent> * Voice cloning someone else’s voice to use it for voiceover This is interesting because I was considering cloning my own voice as a way to record things without the inevitable hesitations, ums, errs, and stumbling over my words. By this standard I am allowed to do so. But then I thought what does it even mean \"someone else's\" when multiple people can make a video, if my wife and I make a video together can we not then use my recorded voice because to her my voice is someone else. I suspect all of these rules will have similar edge cases and a wide penumbra where arbitrary rulings will be autocratically applied. reply exodust 7 hours agorootparent> ...to her my voice is someone else. To her, your voice is your voice not someone else's voice. If you share a Youtube account with your wife, \"someone else\" means someone other than you or your wife. The more interesting and troubling point is your use of \"synthetic you\" to make the real you sound better! reply alt227 1 hour agorootparentIs this really any different to say using makeup, cosmetic plastic surgery, or even choosing to wear specific clothes? reply rrr_oh_man 3 hours agorootparentprev> The more interesting and troubling point is your use of \"synthetic you\" to make the real you sound better! Why? reply Aardwolf 15 hours agoparentprev> Synthetically generating music (including music generated using Creator Music) What about music made with a synthesizer? reply zuminator 15 hours agorootparentIn one of the examples, they refer to something called \"Dream Track\" > Dream Track in Shorts is an experimental song creation tool that allows creators to create a unique 30-second soundtrack with the voices of opted-in artists. It brings together the expertise of Google DeepMind and YouTube’s most innovative researchers with the expertise of our music industry partners, to open up new ways for creators on Shorts to create and engage with artists. > Once a soundtrack is published, anyone can use the AI-generated soundtrack as-is to remix it into their own Shorts. These AI-generated soundtracks will have a text label indicating that they were created with Dream Track. We’re starting with a limited set of creators in the United States and opted-in artists. Based on the feedback from these experiments, we hope to expand this. So my impression is they're talking about labeling music which is derived from a real source (like a singer or a band) and might conceivably be mistaken for coming from that source. reply GuB-42 15 hours agorootparentprevEven if it is fully AI-generated, this requirement seems off compared to the other ones. In all of the other cases, it can be deceiving, but what is deceiving in synthetic music? There may be some cases where it is relevant, like when imitating the voice of a famous singer, but other than that, music is not \"real\", it is work coming from the imagination of its creator. That kind of thing is already dealt with with copyright, and attribution is a common requirement, and one that YouTube already enforces (how it does that is different matter). reply slowfox 15 hours agorootparentFrom a Google/Alphabet perspective it could also be valuable to distinguish between „original“ and „ai generated“ music for the purpose of a cleaner database to train their own music generation models? reply kmeisthax 8 hours agorootparentAlternatively they want to know who to ban when the RIAA inevitably starts suing the shit out of music generators. reply Jensson 15 hours agorootparentprevIf you manually did enough work have the copyright it is fine. But since AI can't legally have copyright to their music Google probably wants to know for that reason. reply dragonwriter 15 hours agorootparent> If you manually did enough work have the copyright it is fine. Amount of work is not a basis for copyright. (Kind of work is, though the basis for the “kind” distinction used isn't actually a real objective category, so its ultimately almost entirely arbitary.) reply anigbrowl 14 hours agorootparentThat could get tricky. A lot of hardware and software MIDI sequencers these days have probabilistic triggering built in, to introduce variation in drum loops, basslines, and so forth. An argument could be made that even if you programmed the sequence and all the sounds yourself, having any randomization or algorithmic elements would make the resulting work ineligible for copyright. reply Gormo 15 hours agorootparentprevIt goes without saying that a piece of software can't be a copyright holder. But the person who uses that software certainly can own the copyright to the resulting work. reply Jensson 15 hours agorootparentIf someone else uses the same AI generator software and makes the same piece of music should Google go after them for it? I don't think that would hold in court. Hopefully this means that AI generated music gets skipped by Googles DRM checks. reply Aardwolf 15 hours agorootparentprevI hope there is some kind of middle ground, legally, here? Like say you use a piano that uses AI to generate artificial piano sounds, but you create and play the melody yourself: can you get copyright or not? reply jprete 14 hours agorootparentIANAL. I think you'd get copyright on the melody and the recording, but not the sound font that the AI created. reply BlueGh0st 4 hours agoparentprev>Showing a realistic depiction of a tornado or other weather events moving toward a real city that didn’t actually happen A bit funny considering a realistic warning and \"live\" radar map of an impending, major, natural disaster occurring in your city apparently doesn't violate their ad policy on YouTube. Probably the only time an ad gave me a genuine fright. reply kazinator 15 hours agoparentprev> Synthetically generating music Yagoddabekidding. That could cover any piece of music created with MIDI sequencing and synthesizers and such. reply kevindamm 6 hours agorootparentI think there's a clear difference between synthesizing music and synthetically generating music. One term has been around for decades and the other one is being confused with that. reply xarope 5 hours agoparentprev* Digitally altering audio to make it sound as if a popular singer missed a note in their live performance Does all the autotuning that singers use in live performance counts? /j reply dheera 15 hours agoparentprev> * Showing a realistic depiction of a tornado or other weather events moving toward a real city that didn’t actually happen > * Making it appear as if hospital workers turned away sick or wounded patients > * Depicting a public figure stealing something they did not steal, or admitting to stealing something when they did not make that admission Considering they own the platform, why not just ban this type of content? It was possible to create this content before \"AI\". reply dotnet00 15 hours agorootparentThere are many cases where such content is perfectly fine. After all, YouTube doesn't claim to be a place devoted to non-fiction only. The first one is an especially common thing in fiction. reply ipaddr 14 hours agorootparentDoes that mean movies clips will need to be labeled? reply lesostep 1 hour agorootparentOr video game footage? I explicitly remember people confusing Arma footage with real war footage. reply samatman 14 hours agorootparentprevThe third one could easily be satire. Imagine that a politician is accused of stealing from the public purse, and issues a meme-worthy press statement denying it, and someone generates AI content of that politician claiming not to have stolen a car or something using a similar script. Valid satire, fair use of the original content: parody is considered transformative. But it should be labeled as AI generated, or it's going to escape onto social media and cause havoc. It might anyway, obviously. But that isn't a good reason to ban free expression here imho. reply kmeisthax 8 hours agorootparentFor what it's worth, this is already a genre of YouTube video, and I happen to find it absolutely hilarious: https://youtu.be/3oWFFAVYMec https://youtu.be/aL1f6w-ziOM reply perihelions 10 hours agorootparentprevRespectfully disagree. Satire should not be labelled as satire. Onus is on the reader to be awake and thinking critically—not for the entire planet to be made into a safe space for the unthinking. It was never historically the case that satire was expected to be labelled, or instantly recognized by anyone who stumbled across it. Satire is rude. It's meant to mock people—it is intended to muddle and provoke confused reactions. That's free expression nonetheless! reply dinkleberg 6 hours agorootparentSo when we have perfect deep fakes that are indistinguishable from real videos and people are using it for satire, people shouldn’t be required to inform people of that? How is one to figure out what is real and what is a satire? Times and technologies change. What was once reasonable won’t always be. reply perihelions 2 hours agorootparent- \"How is one to figure out what is real and what is a satire?\" Context, source, tone of speech, and reasonability. - \"Times and technologies change.\" And so do people! We adapt to times and technology; we don't need to be insulated from them. The only response needed to a new type of artificial medium, is, that people learn to be marginally more skeptical about that medium. reply nashashmi 15 hours agoparentprevThose voice overs on tiktok that are computer generated but sound quite real and often are reading some script. Do they have to disclose that those voices are artificially produced? reply astro- 13 minutes agoprevI’m wondering whether another motivation for this could be trying to keep the data set as clean as possible for future model training. Creating videos takes quite a bit of time. If AI video generation becomes widely available, pretty soon, there could be more AI content being uploaded to YouTube than human-made stuff. Presumably, training on AI generated stuff magnifies any artefacts/hallucinations present in the training set, reducing the quality of the model. reply the_duke 15 hours agoprevThey don't bother to mention it, but this is actually to comply with the the new EU AI act. > Providers will also have to ensure that AI-generated content is identifiable. Besides, AI-generated text published with the purpose to inform the public on matters of public interest must be labelled as artificially generated. This also applies to audio and video content constituting deep fakes https://digital-strategy.ec.europa.eu/en/policies/regulatory.... Some discussion here: https://news.ycombinator.com/item?id=39746669 reply supriyo-biswas 15 hours agoparentIndia is considering very similar laws as well (though not implemented at this time)[1], so it’s not just the EU. Also, if every applicable regulation had to be mentioned, it’d be a very long list. [1] https://epaper.telegraphindia.com/imageview/464914/53928423/... reply hoffs 15 hours agorootparentConsidering is different from actually something that should be enforced reply alphazard 13 hours agoparentprevIs anyone else worried about how naive this policy is? The solution here is for important institutions to get onboard with the public key infrastructure, and start signing anything they want to certify as authentic. The culture needs to shift from assuming video and pictures are real, to assuming they are made the easiest way possible. A signature means the signer wants you to know the content is theirs, nothing else. It doesn't help to train people to live in a pretend world where fake content always has a warning sticker. reply pjc50 23 minutes agorootparentPKI has been around for, what, 30 years? Image authentication is just not going to happen at this point, because everyone's got too used to post-processing and it's a massive hassle for something that ultimately doesn't matter because real people use other processes to determine whether things are true or not. reply RandallBrown 13 hours agorootparentprevOne of Neal Stephenson's more recent novels deals with this concept. Fake news becomes so bad that everyone starts singing everything they create. reply pixl97 12 hours agorootparentThis is about as realistic as the next generation of congress people ending up 40 years younger. We literally have politicians talking about pouring acid on hardware and expect these same bumbleheads to keep their signing keys safe at the same time. The average person is far too technologically illiterate to do that. Next time you go to grandmas house you'll learn she traded her signing key for chocolate chip cookies. reply RandallBrown 9 hours agorootparentI imagine it would be something handled pretty automatically for everyone. If Apple wanted to sign every photo and document the iPhone they could probably make the whole user experience simple enough for most grandmas. Some people will certainly give away their keys, just like bank accounts and social security numbers today, but those people probably aren't terribly concerned with proving the ownership of their online documents. reply thwarted 12 hours agorootparentprevI see a lot of confusing authenticity with accuracy. Someone can sign the statement \"Obama is white\" but that doesn't make it a true statement. The use of PKI as part of showing provenance/chain of trust doesn't make any claims about the accuracy of what is signed. All it does is assert that a given identity signed something. reply airspresso 12 hours agorootparentIt's not about what is being signed, it's about who signed it and whether you trust that source. I want credible news outlets to start signing their content with a key I can verify as theirs. In that future all unsigned content is by definition fishy. PKI is the only way to implement trust in a digital realm. reply The_Colonel 12 hours agorootparentprev> The culture needs to shift from assuming video and pictures are real, to assuming they are made the easiest way possible. That sounds like a dystopia, but I guess we're going into that direction. I expect that a lot of fringe groups like flat-earthers, lizard people conspiracy, war in Ukraine is fake, will become way more mainstream. reply orbital-decay 14 hours agoparentprevLabeling AI-generated content (assuming it works) is beneficial for Google, as they can avoid some dataset contamination. reply airspresso 12 hours agorootparentExcellent point. With more and more AI-generated content it will be key to be able to tell it apart from the human-generated content. reply machinekob 15 hours agoparentprevOfc. they don't mention it for big tech companies EU = Evil reply sylware 15 hours agorootparentnext [4 more] [flagged] apwell23 15 hours agorootparentI make good pay from that scam so screw EU trying to steal our wealth. How is this shit different that useless and annoying cookie popups. reply bgdam 14 hours agorootparentSo you making good pay by enabling a scammer makes it totally okay for the scammer to operate? By extension of that logic, hitmen should no longer be persecuted provided they make good pay from it. reply anigbrowl 14 hours agorootparentprevI love cookie popups, I get to reject all kinds of marketing and tracking cookies. reply duringmath 15 hours agorootparentprevYou'd think they're evil too if they let a bunch of middlemen and parasitic companies dictate how the software you invested untold sums and hours developing and marketing should work. reply damiankennedy 9 hours agorootparentWhy should software be any different from aircraft? reply ysofunny 14 hours agoparentprevI have a more entertaining: \"typical google, getting somebody else to give them training data in exchnage for free hosting of some sort\" reply cmilton 12 hours agoparentprevI would take this a step further and make it required that companies create an easy way for users to opt-out of this type of content. reply pier25 12 hours agoparentprevThank you EU! reply Karellen 13 hours agoparentprevWhat if a real person reads a script that was created with an LLM? Does that count? Should it? reply airspresso 12 hours agorootparentBlog post specifically mentions that using AI to help writing the script does not require labeling the video. reply Karellen 11 hours agorootparentSorry, I wasn't entirely clear that I was specifically responding to the GP comment referencing the EU AI act (as opposed to creating a new top-level comment responding to the original blog post and Google's specific policy) which pointed out: > Besides, AI-generated text published with the purpose to inform the public on matters of public interest must be labelled as artificially generated. This also applies to audio and video content constituting deep fakes Clearly \"AI-generated text\" doesn't apply to YouTube videos. But, it is interesting that if you use an LLM to generate text and present that text to users, you need to inform them it was AI-generated (per the act). But if a real person reads it out, apparently you don't (per the policy)? This seems like a weird distinction to me. Should the audience be informed if a series of words were LLM-generated or not? If so, why does it matter if they're delivered as text, or if they're read out? reply hnbad 12 hours agoparentprevUsually when a big corporation gleefully announces a change like this it's worth checking whether there's any regulations on that topic taking effect in the near future. On a local level, I recall how various brands started making a big deal of replacing disposable plastic bags with canvas or paper alternatives \"for the environment\" just coincidentally a few months before disposable plastic bags were banned in the entire country. reply ajross 15 hours agoparentprevSeems like this is sort of a manufactured argument. I mean, should every product everywhere have to cite every regulation it complies with? Your ibuprofen bottle doesn't bother to cite the FDA rules under which it was tested. Your car doesn't list the DOT as the reason it's got ABS brakes. The EU made a rule. YouTube complied. That changes the user experience. They documented it. reply hnlmorg 15 hours agorootparentIf the contents of my ibuprofen bottle changed due to regulatory changes, then it wouldn’t be weird to have that cited at all. reply LudwigNagasena 15 hours agorootparentprevCertain goods sold in the EU are required to have CE marking to affirm that they satisfy EU regulations. reply nlehuen 14 hours agorootparent+1 in France at least, food products must not suggest that mandatory properties like \"preservative free\" is unique. When they advertise this on the package, they must disclose it's per regulation. Source: https://www.economie.gouv.fr/particuliers/denrees-alimentair... reply contravariant 15 hours agorootparentprevDoesn't seem that out of place for a blog post on the exact change they made to comply though. I mean you'd expect a pharmaceutical company to mention which rules they comply with at some point, even if not on the actual product (though in the case of medicine, probably also on the actual product). reply yoavz 15 hours agoprevMost interesting example to me: \"Digitally altering audio to make it sound as if a popular singer missed a note in their live performance\". This seems oddly specific to the inverse of what happened recently with Alicia Keys from the recent Superbowl. As Robert Komaniecki pointed out on X [1], Alicia Keys hit a \"sour note\" which was silently edited by the NFL to fix it. [1] https://twitter.com/Komaniecki_R/status/1757074365102084464 reply elpocko 13 hours agoparentDigitally altering audio to make it sound as if a popular singer hit a lot of notes is still fine though. reply yoavz 12 hours agorootparentCorrect, it's the inverse that requires disclosure by Youtube. Still, I find it interesting. If you can't synthetically alter someone's performance to be \"worse\", is it OK that the NFL synthetically altered Alicia Key's performance to be \"better\"? For a more consequential example, imagine Biden's marketing team \"cleaning up\" his speech after he has mumbled or trailed off a word, misleading the US public during an election year. Should that be disclosed? reply hackernewds 2 hours agorootparentI don't understand the distinction. if the intent is to protect the user, then what if I make the sound better for rival contestants on American idol and don't do it for singers of a certain race. seems to comply? reply post_break 12 hours agoparentprevOh no, is that going to mess up my favorite genre called shreds? https://www.youtube.com/watch?v=1nAhQOoJTIA reply frays 14 hours agoparentprevThis is a great example as a discussion point, thank you for sharing. I will be coming back to this video in several months time to check whether the \"Altered or synthetic content\" tag has actually been applied to it or not. If not, I will report it to YouTube. reply ryandrake 13 hours agorootparentYea, it’s a really super example! However autotune has existed for decades. Would it have been better if artists were required to label when they used autotune to correct their singing? I say yes but reasonable people can disagree! I wonder if we are going to settle on an AI regime where it’s OK to use AI to deceptively make someone seem “better” but not to deceptively make someone seem “worse.” We are entering a wild decade. reply JadedBlueEyes 10 hours agorootparent> I say yes but reasonable people can disagree! A lot of people do! Tone correction [1] is a normal fact of life in the music industry, especially in recordings. Using it well takes both some degree of vocal skill and production skill. You'll often find that it's incredibly obvious when done poorly, but nearly unnoticeable when done well. [1] AutoTune is a specific brand reply sigmoid10 16 hours agoprev>Some examples of content that require disclosure include: [...] Generating realistic scenes: Showing a realistic depiction of fictional major events, like a tornado moving toward a real town. This sounds like every thumbnail on youtube these days. It's good that this is not limited to AI, but it also means this will be a nightmare to police. reply nosvince 15 hours agoparentExactly, and many have done exactly the same kind of video using VFX. What's the difference? These kind of reactions remind me of the stories of the backlash following the introduction of calculators in schools... reply DylanDmitri 15 hours agorootparentUsing VFX for realistic scenes is more involved. VFX requires more expertise to do convincingly and realistically, in the thousands of hours of experience. More involved scenes require multiple professionals. The tooling and assets costs more. An inexperienced person, in a hundred hours of effort, can put out 10ish realistic scenes with leading edge AI tools, when previously they could do 0. This is like regulating handguns differently from compound bows. Both are lethal weapons, but the bow requires hours of training to use effectively, and is more difficult to carry discreetly. The combination of ease, convenience, and accessibility necessitates new regulation. This being said, AI for video is an incredibly promising technology, and I look forward to watching the TV shows and movies generated with AI-powered tooling. reply alickz 12 hours agorootparentWhat if new AI tools negate the thousands of hours experience to generate realistic VFX scenes, so now realistic scenes can be made by both non-AI VFX experts and AI-assisted VFX laymen? Do we make all usages of VFX now require a warning, just in case the VFX was generated by AI? I think this is different to the bow v gun metaphor as I can tell an arrow from a bullet, but I can foresee a future where no human could tell the difference between AI-assisted and non-AI-assisted VFX / art I believe this is evidenced by the fact that people can go around accusing any art piece of being AI art and the burden of proving them wrong falls on the artist. Essentially I believe we are rapidly approaching the point of it not mattering if someone uses AI in their art because people won't be able to tell anyway reply nomel 13 hours agorootparentprev> Using VFX for realistic scenes is more involved. This really depends on what you're doing. There are some great Cinema 4d plugins out there. As the plethora of YouTube tutorials out there clearly demonstrate, multiple professionals, and vast experience, are not required for some of the things they have listed. Tooling and assets costs are 0, in the high seas. Until Sora is widely available, or the open source models catch up, at this moment it's easier to use something like Cinema 4d than AI. reply hackernewds 2 hours agorootparentprevso if I used Blender it's banned? it's very tough to draw that line in the sand reply mazlix 13 hours agorootparentprevWhat if i use an LLM powered AI to operate VFX software to generate a realistic looking scene? ;) reply GolDDranks 15 hours agorootparentprev> What's the difference? The ease and lack of skill required. That brings whole another set of implications. reply dylan604 15 hours agorootparentprevI'm sorry, but using a calculator to get around having to learn arithmetic is not even close being the same thing. Prove to me that you can do basic arithmetic, and then we can move on to using calculators for the more complex stuff where if you had to could at least come to the same value as the calculator. People using VFX aren't trying to create images in likeness of another existing person to get people to buy crypto or other scams. Comparing the two is disingenuous at best. reply skybrian 15 hours agoprevI’m reminded of how banks require people to fill out forms explaining what they’re doing, where it’s expected that criminals will lie, but this is an easy thing to prosecute later after they’re caught. Could a similar argument be applied here? It doesn’t seem like there is much in the way of consequences for lying to Google. But I suppose they have other ways of checking for it, and catching someone lying is a signal that makes the account more suspicious. reply danpalmer 1 hour agoparentYeah I think it’s a very similar approach to what you’ve described. The scale of YouTube, I don’t think you can just start banning content you don’t like. Instead you have to have a policy, clearly documented, and then you can start enforcing based on that policy. The other thing is that they don’t necessarily want to ban all of this content. For example a video demonstrating how AI can be used to create misinformation and showing examples, would be fairly clearly “morally” ok. The policy being that you have to declare it allows for this sort of content to live on the platform, but allows you to filter it out in certain contexts where it may be inappropriate (searches for election coverage?) and allows you to badge it for users (like Covid information tags). reply antoniojtorres 9 hours agoparentprevIt’s a compliance checkbox for the most part I think. They can stay on top of new legislation by claiming they are providing tools to deal with misinformation, whereas it’d be easier to say that they are encouraging the proliferation of misinformation by not doing anything about it. It certainly shifts the legal question in the way you described it would seem. reply idatum 14 hours agoprev> Altering footage of real events or places: Such as making it appear as if a real building caught fire, or altering a real cityscape to make it appear different than in reality. What about the picture you see before clicking on the actual video? This article of course is addressing the content of the videos, but I can't help but look at the comically cartoonish, overly dramatic -- clickbait -- picture preview of the video. For example, there is a video about a tornado that passed close to a content author and the author posts video captured by their phone. In the preview image, you see the author \"literally getting sucked into a tornado\". Is that \"altered and synthetic content\"? reply NoPicklez 6 hours agoparentI don't think they need to be treated the same. The thumbnail isn't the content itself necessarily. reply ziofill 8 hours agoprevRather than tagging what’s made up, why not tag what’s genuine? There’s gonna be less of it than the endless mountain of generated stuff. I’m thinking something as simple as a digital signature that certifies e.g. a photo was made with my phone if I want to prove it, or if someone edits my file there should be a way of keeping track of the chain of trust. reply asadalt 8 hours agoparentyeah expect this to flip. i am guessing this will go like “https” path. first we will saw green lock for https enabled sites, later we saw insecure for http sites. reply unwind 58 minutes agoprevMeta: the word \"creators\" is sorely missing from the title, it should read something like \"[...] now requires creators to label [...]\" reply rchaud 15 hours agoprevGoogle of yore would have offered a 'not AI' type of filter in their advanced search. Present day Google is too busy selling AI shovels to quell Wall St's grumbling, to even consider what AI video will to do to the already bad 'needle in a haystack' nature of search. reply exodust 7 hours agoparentA \"not AI\" filter is an excellent idea. reply primeradical 10 hours agoprevDoes YouTube know that the Google Photos team actively encourages altering your videos and photographs to represent scenes that never happened? https://blog.google/products/photos/google-photos-features-p... https://blog.google/products/photos/google-photos-magic-edit... reply pompino 1 hour agoprev>in the future we’ll look at enforcement measures for creators who consistently choose not to disclose this information. Nothing of use here. As per the usual MO of tech companies they throw the responsibility back on the user. Sounds like yet another bullshit clause that they can invoke when they want to cancel you. reply zuppy 1 hour agoparentcall me cynic but i share the same thought. plus... unless we figure out a way to detect it, which we can't reliably do now at scale, this will be pretty useless. the ones who want to use it for profit will do whatever it takes, just the honest people will label it. i believe that this is even worse than to assume that everything is ai generated, as people without technical knowledge will trust that the labeling works. reply arduanika 15 hours agoprevIs there a word missing from the title here? Requires whom? reply jbiason 15 hours agoparentSame. For a second, I thought YouTube made a rule that YouTube is now required to flag the AI videos created by YouTube. reply samatman 14 hours agoparentprevThe title was editorialized, which people do far more often than they should. The original title, with the domain name next to it, would have been fine. reply cpncrunch 10 hours agorootparentEditorializing is fine, as long as it's done properly and the headline makes sense. reply samatman 8 hours agorootparentIt is not, except under certain circumstances which this case does not meet. > Otherwise please use the original title, unless it is misleading or linkbait; don't editorialize. reply yoavz 15 hours agoprevI am not envious of the policy folks at Youtube who will have to parse out all the edge cases over the next few years. They are up against a nearly impossible task. https://novehiclesinthepark.com/ reply rchaud 15 hours agoparentIt's not like there are any real consequences if they don't get it right. Deepfake ads already exist on YT. reply asadotzler 9 hours agorootparentCertainly there are if YouTube wants to continue to do business in the EU. reply micheljansen 11 hours agoprevThe cynic in me thinks this is just Google protecting their precious training data from getting tainted but I’m glad their goals align with what’s better for consumers for once. reply dotnet00 15 hours agoprevWithout enforceability it'll go the same way as it has on Pixiv, the good actors will properly label their AI utilizing work, while the bad actors will continue to lie to try to maximize their audience until they get caught, then rinse and repeat. Kind of like crypto-scammers. For context, Pixiv had to deal with a massive wave of AI content being dumped onto the site by wannabe artists basically right as the initial diffusion models became accessible. They responded by making 'AI-generated' a checkbox to go with the options to mark NSFW and adding an option for users to disable AI-generated content from being recommended to them. Then, after an incident of someone using their Patreon style service to pretend to be a popular artist, selling commissions generated by AI to copy the artist's style, they banned AI-generated content from being offered through that service. reply dotancohen 15 hours agoparentI think that the idea is mostly to dictate culture. And I like the idea, not only for preventing fraud. Ever since the first Starship launches, the reality looks more incredible than the fiction. Go look up the SN-8 landing video, tell me that does not look generated. I just want to know what is real and what is generated, by AI or not. I think that this policy is not perfect, but it is a step in the right direction. reply jtriangle 15 hours agoparentprevAlso remains to be seen if labeling your content as containing AI-generated work will help or hurt you in your viewership. My guess is that youtube is going to downrank this content, and may be trying to crowdsource training data in order to do this automatically. reply dotnet00 15 hours agorootparentI think that for now they're just going to use it as a means of figuring out what kind of AI-involved content people are ok with and what kind they react negatively to. Personally, I've developed a strong aversion to content that is primarily done by AI with very little human effort on top. After how things went with Pixiv I've come to hold the belief that our societies don't help people develop 'cultural maturity'. People want the clout/respect of being a popular artist/creator, without having to go through the journey they all go through which leads to them becoming popular. It's like wanting to use the title of Doctor without putting in the effort to earn a doctorate, the difference just being that we do have a culture of thinking that it's bad to do that. reply russdill 12 hours agoparentprevI think one of the bigger issues will be false positives. You'll do an upload, and youtube will take it down claiming that some element was AI generated. You can appeal, but it'll get automatically rejected. So you have to rework your video and figure out what it thought might be AI generated and re-upload. reply Devasta 14 hours agoprevI hope this allows me to filter them entirely. If it wasn't worth your time creating it, its not worth my time looking at it. I am generally very skeptical of these tags though, I suspect a lot of them are in place to stop an AI consuming its own output rather than any concern for the end user. reply sumedh 37 minutes agoparent> If it wasn't worth your time creating it Once something like Sora is available to the public, its going to be game over. A new bunch of creators will use it to \"create\" videos and I am sure you will change your mind then. reply munificent 11 hours agoparentprev> If it wasn't worth your time creating it, its not worth my time looking at it. God, I wish I could beam this sentence directly into the brain of every single person breathlessly excited about using gen AI to be \"a creative\". reply danlugo92 8 hours agorootparentThe fact that it cannot self-feed will take care of this anyways. reply RyEgswuCsn 13 hours agoprevThis is somewhat expected to be honest. I am rather pessimistic on the future solutions to such issues though. I can see only one possibility going forward: camera sensor manufactures will either voluntarily or forcibly implement hardware that inject cryptographic \"watermarks\" to the videos produced by their cameras. Any videos that do no bear valid watermarks are considered potentially \"compromised\" by GenAI. reply IlPeach 2 hours agoprevOh that's great so we can finally train AI on what human generated content is Vs content to discard. reply NoPicklez 6 hours agoprevAfter reading it I think it's a good approach, whilst not perfect it's a good step. Interestingly it isn't just referring to AI but also \"other tools\", used to make content that is \"altered, synthetic and seems real\". Fair amount of ambiguity in there but I see what they're getting at when it comes to the bigger fish like the president being altered to say something they didn't. reply omoikane 13 hours agoprev> Creators must disclose content that [...] Generates a realistic-looking scene that didn't actually occur This may spoil the fun in some 3D rendered scenes. For example, I remember there was much discussion on whether a robot throwing a bowling ball was real or not[1]. Part of the problem has to do with all the original tags (e.g. \"#rendering3d\") being lost when the video spread through various platforms. The same problem will happen with Youtube -- creators may disclose everything, but after a few rounds through reddit and back, whatever disclosure and credit that was in the original video will be lost. [1] https://twitter.com/TomCoben/status/1146431221876105216 https://twitter.com/TomCoben/status/1147870621713543168 reply sheepscreek 11 hours agoprevWhile their intentions are good, the solution isn’t. There’s a lot that they have left to the subjectivity of the creators. Especially for what is “clearly unrealistic”. reply airspresso 12 hours agoprevNo mention of clearly labeling ads made using AI. The deepfake Youtube ads are so annoying. Elon wants to recruit me to his new investment plot? Yeah right. reply cottsak 7 hours agoprev> Risks of not disclosing > It can be misleading if viewers think a video is real, when it's actually been meaningfully altered or synthetically generated to seem realistic. > When content is undisclosed, in some cases YouTube may take action to reduce the risk of harm to viewers by proactively applying a label that creators will not have the option to remove. Additionally, creators who consistently choose not to disclose this information may be subject to penalties from YouTube, including removal of content or suspension from the YouTube Partner Program. https://support.google.com/youtube/answer/14328491 This is censorship. That's all. reply TobTobXX 2 hours agoprevTikTok added the same switch a few days ago. reply Tioka001 5 hours agoprevNot bad? it helps users distinguish between reality and virtuality, the world is becoming more virtual though... reply RobotToaster 15 hours agoprevIf it's realistic who will know? reply simion314 15 hours agoparent>If it's realistic who will know? Look at \"realistic\" photos , it is easy for someone with experience to spot issues, the hangs/fingers are wrong, shadows and light are wrong, hair is weird, eyes have issues. In a video there are much more information so much more places to get things wrong, making it pass this kind of test will be a huge job so many will not put the effort. reply duxup 15 hours agoprevGoing to be a long road with this kinda thing but forums and places I visit often already have \"no AI submissions\" type rules and they have been received pretty well that I've seen. Are they capable of enforcing it? I don't know, but it's clear users understand / don't like the idea of being awash in a sea of AI content at this point. If they can actually avoid it remains to be seen. reply barfbagginus 9 hours agoprevActually love AI content! Been a member of the Cursed AI group on Facebook, and now non-AI images just look boring to me! So this announcement begs the question: is there any way to search for just AI content? reply whoopdedo 12 hours agoprevI'd like a content ID system for AI generated media. If someone tries to pass an image to me as authentic I can check its hash against a database that will say \"this was generated by such-and-such LLM on 18 Mar 2024.\" Maybe even add a country of origin. reply zhoujianfu 12 hours agoparentThese guys are doing something sort of in that vein.. https://wolfsbane.ai/ reply asciimov 14 hours agoprevWill this cover all those product \"review\" videos that are clearly reading some copy or amazon reviews? reply strangescript 15 hours agoprevThis a pointless nearly unenforceable rule to make people feel better. Sure, if you generate something that seems like a real event that is provably false you can be caught, but anything mundane is not enforceable. Once models reach something like Sora 1.5 level of ability, we are kind of doomed on knowing whats real in video. reply gloosx 14 hours agoparentnaah, there still will be certain patterns and they will be recognisable. once something sora 1.5 level of ability is there – definitely reverse-sora model which can recognise ai-made videos should be possible to train as well reply supertrope 12 hours agoparentprev>This a pointless nearly unenforceable rule to make people feel better. Pretty much. If Google says \"Swiper no swiping\" they can point at their policy when lobbying against regulations or pushing back against criticism. Before surveillance capitalism became the norm, web services told users to not share personal information, and to not trust other users they had not met in real life. reply qwertox 15 hours agoprevHow about first removing those crypto-scam channels which pop up whenever something big happens at SpaceX. reply bhasi 3 hours agoprevHow do they plan to enforce this? reply lawlessone 13 hours agoprevSo how do I report the ones that don't? I have a whole lot of shorts content to report.. reply dorkwood 6 hours agoprevThis isn't fair. If I make a video using AI, who is to say whether it's anymore real than a video taken with a camera? You think what a camera captures is reality? reply twodave 12 hours agoprevI've said before that we're entering an age where no online material is truly verifiable without some kind of hardware signing (and even that has its flaws). Public figures will have to sort out this quagmire before things get even uglier than they are. And I really hope that's the biggest problem of the next decade or so, rather than that we achieved AGI and it decided we were inferior. reply danlugo92 8 hours agoparentI guess the first AGI will not be connected to anything thus it will be shut down right away if it wants to kill us? I think... reply romanovcode 2 hours agoprevIt would be amazing if users could opt-out of any videos that use AI content. The whole short-form farm is incredibly annoying to sift through. reply creatonez 8 hours agoprevAny links to videos that currently have this status? reply dmje 14 hours agoprevI suspect it’ll get me downvoted but this newish trend of using this grammar syntax drives me nuts. It’s “YouTube now requires YOU to” not “YouTube now requires to”. It’s lazy, it’s grammatically incorrect and it doesn’t scan. reply dwighttk 14 hours agoprevTitle seems to be missing “creators” reply xyst 15 hours agoprevSelf reporting. How useless. Wonder what legislation they are minimally complying with reply bandrami 8 hours agoprevThis has definite RFC 3514 energy reply codedokode 10 hours agoprevAnd what if someone doesn't label the video? What if someone has drawn a fake video in Photoshop? The whole requirement to label AI-generated videos is dumb. Typical decision from some old politician who doesn't understand anything in AI or video editing and who should have retired 20 years ago instead of making such dumb rules. Why movies are not labeled but AI video must be labeled? What about comedians impersonating politicians? If Google or govt is afraid that someone will use AI-generated videos for bad purposes (e.g. to display a candidate saying things that he never said) then they should display a warning above every video to educate people. And popular messengers like Telegram or video players must do the same. At least add a warning above every political and news videos. reply cush 15 hours agoprevAh it’s better than nothing! reply paulpauper 14 hours agoparentScammers have been making fake content on youtube since its founding. And youtube has never even pretended so much as to care about doing anything about it. reply Joel_Mckay 8 hours agoprevOr more likely, google wants a labeled dataset to improve their own training projects. =) reply wslh 15 hours agoprevELI5: what would be the difference if you use AI or it is a new release of Star Wars? I understand that AI does not need proof-of-work and that is the difference? reply mvdtnz 15 hours agoparentWas this comment generated by the world's worst LLM? No idea what you're asking. reply wslh 15 hours agorootparentI am not an AI, I am a person and in HN I look for being well treated. reply stevage 12 hours agoprevI predict that this kind of labelling will disappear before long and in a couple of years will look ridiculous. reply armatav 5 hours agoprevcollecting training data reply acituan 6 hours agoprevIt might take 50 years to awaken to the abuse of power going on here. Forget individual videos for a second and look at youtube-the-experience as a whole. The recommendation stream is the single most important \"generative AI\" going on ever, using the sense of authenticity, curiosity and salience that comes from the individual videos themselves, but stitching them together in a very particular way. All the while the experience of being recommended videos being almost completely invisible. Of course this is psychologically \"satisfying\" to the users - in the shortest term - because they keep coming back, to the point of addiction. (Especially as features like shorts creep in). Allowing the well of \"interesting, warm, authentic audio & videos having the secondary gains of working on your psychological needs\" being tainted with the question of generated content is a game changer because it breaks the wall of authenticity for the entire app. It brings the whole youtube-the-experience into question, it reduces its psychological stand-in function for human voice & likeness, band-aiding the hyper-individualized lonely person's suffering based content consumption habits. I know this is a bit dramatic, and for sure videos can be genuinely informative, but let's be honest, neither that is the entirety of your stream, nor that is the experience for the vast majority of the users. It will get worse as long as there is a mathematical headroom of making more money out of making it worse, that's what the shareholder duty is about. When gen-AI came about I was naively happy about the fake \"authenticity\" wall of the recommended streams breaking down thanks to the garbage of generated sophistry overtaking and grossing out the users. Kind of like super delicious looking cakes turning out to be made of kitchen sponges turning people off of cakes all together. I was wrong to think AI oligopoly would let the opportunity of having a chokehold on the entire \"content\" business, and here we are. (Also this voluntary tagging will give them the perfect live training set, on top of what they have.) Once the tech is good enough to generate video streams on the fly, so that all you need is a single livestream, that you won't even have a recommendation engine of videos and instead a team of virtual personas doing everything you could ever desire on screen, it is game over. It might already be game over. To get out of this the single most important legislative maneuver is being able to accept and enforce the facts that a) recommendation is speech b) recommendation is also gen-AI, and should be subject to same level of regulatory scrutiny. I don't care if it generates pixels or characters at a time, or slaps together the most \"interesting\" subset of videos/posts/users/reels/shorts out of the vast sea of the collective content-consciousness, they are just one level of abstraction apart but functionally one and the same: look at me; look at my ads; come back to me; keep looking at me. reply CatWChainsaw 10 hours agoprev\"Requires\". It will rely on the honor system, which sleazy assholes won't honor; or the report system, which people will abuse. AI detectors aren't reliable; GenAI is basically tautologically defined as hard/impossible to detect and we keep getting reminded that \"it will only get better\". Everyone calls this a problem, but it's a predicament because it has NO solution, and I have nothing but contempt for everyone who made it reality. reply scotty79 11 hours agoprevThis label will be mostly misleading. Absence of the tag will give false sense of veracity and presence of it on non-ai generated materials will discredit them. Fact checking box like on twitter would be better and if you can't provide it, don't pretend you know anything about the content. reply meindnoch 12 hours agoprevOr else? reply dbg31415 13 hours agoprevThis will just result in a pop-up before every video, like the cookie warnings, “Viewers should be aware that this video may contain AI-generated or AI-enhanced images.” And it’ll be so annoying… reply thomastjeffery 13 hours agoprevIt's as if everyone in the world just forgot that fraud has been illegal the whole time. reply paulddraper 11 hours agoparentFraud is a very specific thing. Namely it requires payment under false pretense reply paul7986 14 hours agoprevAll websites and all for profit AI companies must add and then display AI watermarks otherwise nothing can truly believed online and offline too reply 111111101101 15 hours agoprevWe can't have the proles misrepresenting reality the same way that the rich have been doing for the last century. Rules for thee but not for me. reply paulpauper 14 hours agoparentWe cannot have fake content on youtube now! No way. reply 148 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "YouTube launches a new tool in Creator Studio for creators to disclose altered or synthetic media produced with generative AI to enhance transparency and trust with viewers.",
      "Required disclosure includes altered footage, synthetic voices, and digitally modified faces, displayed in video descriptions and on the video player.",
      "The platform plans to enforce disclosure regulations and update privacy policies to remove AI-generated or synthetic content mimicking real individuals, aiming to clarify the use of generative AI in content production."
    ],
    "commentSummary": [
      "The debate centers on the necessity of labeling AI-generated content on platforms like YouTube to distinguish between real and AI-generated content.",
      "Concerns revolve around potential misuse of AI, enforcement challenges, and the impact on free expression and trust in online content.",
      "Participants discuss verifying content authenticity, copyright ownership in AI-generated content, regulating synthetic alterations in audio and video, and leveraging technology like PKI for authentication, expressing skepticism about the effectiveness of such labeling due to misinformation and user manipulation in the digital realm."
    ],
    "points": 668,
    "commentCount": 398,
    "retryCount": 0,
    "time": 1710778798
  },
  {
    "id": 39749312,
    "title": "Stability.ai Unveils SV3D: Cutting-Edge 3D Video Generation",
    "originLink": "https://stability.ai/news/introducing-stable-video-3d",
    "originBody": "Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images Product 18 Mar Key Takeaways: Today we are releasing Stable Video 3D (SV3D), a generative model based on Stable Video Diffusion, advancing the field of 3D technology and delivering greatly improved quality and view-consistency. This release features two variants: SV3D_u and SV3D_p. SV3D_u generates orbital videos based on single image inputs without camera conditioning. SV3D_p extends the capability by accommodating both single images and orbital views, allowing for the creation of 3D video along specified camera paths. Stable Video 3D can be used now for commercial purposes with a Stability AI Membership. For non-commercial use, you can download the model weights on Hugging Face and view our research paper here. SV3D takes a single object image as input and output novel multi-views of that object. We can then use those novel-views and SV3D to generate 3D meshes. When we released Stable Video Diffusion, we highlighted the versatility of our video model across various applications. Building upon this foundation, we are excited to release Stable Video 3D. This new model advances the field of 3D technology, delivering greatly improved quality and multi-view when compared to the previously released Stable Zero123, as well as outperforming other open source alternatives such as Zero123-XL. This release features two variants: SV3D_u: This variant generates orbital videos based on single image inputs without camera conditioning. SV3D_p: Extending the capability of SVD3_u, this variant accommodates both single images and orbital views, allowing for the creation of 3D video along specified camera paths. Stable Video 3D can be used now for commercial purposes with a Stability AI Membership. For non-commercial use, you can download the model weights on Hugging Face and view our research paper here. Advantages of Video Diffusion By adapting our Stable Video Diffusion image-to-video diffusion model with the addition of camera path conditioning, Stable Video 3D is able to generate multi-view videos of an object. The use of video diffusion models, in contrast to image diffusion models as used in Stable Zero123, provides major benefits in generalization and view-consistency of generated outputs. Additionally, we propose improved 3D optimization leveraging this powerful capability of Stable Video 3D to generate arbitrary orbits around an object. By further implementing these techniques with disentangled illumination optimization as well as a new masked score distillation sampling loss function, Stable Video 3D is able to reliably output quality 3D meshes from single image inputs. See the technical report here for more details on the Stable Video 3D models and experimental comparisons. Novel-View Generation Stable Video 3D introduces significant advancements in 3D generation, particularly in novel view synthesis (NVS). Unlike previous approaches that often grapple with limited perspectives and inconsistencies in outputs, Stable Video 3D is able to deliver coherent views from any given angle with proficient generalization. This capability not only enhances pose-controllability, but also ensures consistent object appearance across multiple views, further improving critical aspects of realistic and accurate 3D generations. Stable Video 3D is able to generate novel multi-views that are more detailed, faithful to the input image, and multi-view consistent compared to existing works. 3D Generation Stable Video 3D leverages its multi-view consistency to optimize 3D Neural Radiance Fields (NeRF) and mesh representations to improve the quality of 3D meshes generated directly from novel views. For this, we have designed a masked score distillation sampling loss to further enhance 3D quality in regions not visible in the predicted views. Additionally, in order to reduce the issue of baked-in lighting, Stable Video 3D employs a disentangled illumination model that is jointly optimized along with 3D shape and texture. Sample 3D mesh generations with our 3D optimization using SV3D model and its outputs. Example of 3D mesh results obtained using SV3D compared to outputs generated from EscherNet and Stable Zero123. Stable Video 3D can be used now for commercial purposes with a Stability AI Membership. For non-commercial use, you can download the model weights on Hugging Face and view our research paper here. To stay updated on our progress, follow us on Twitter, Instagram, LinkedIn, and join our Discord Community. 3D Anel Islamovic",
    "commentLink": "https://news.ycombinator.com/item?id=39749312",
    "commentBody": "Stability.ai – Introducing Stable Video 3D (stability.ai)562 points by ed 13 hours agohidepastfavorite97 comments thrdbndndn 8 hours agoThe emphasis here is Single Image, but can this model generate with multiple images too? We know that a single image of an object physically can't cover all the sides of it, so it's all guesswork in AI. This is totally fine for certain scenario, but in lots of other cases, it's trivial to have multiple images of the same object, and if that offers higher fidelity, it's totally worth it. I'm aware there are many algorithms or AI models that already do that. I'm asking about Stability's one specifically because if they have impressive Single Image result, surely their multi-image results would also be much better than state-of-the-art? reply pksebben 6 hours agoparentIf it's not there yet, I'm willing to bet it will be soon enough given folks hacking it apart and injecting their own solutions. reply kouteiheika 11 hours agoprevJust tried to run this using their sample script on my 4090 (which has 24GB of VRAM). It ran for a little over 1 minute and crashed with an out-of-memory error. I tried both SV3D_u and SV3D_p models. [edit]Managed to generate by tweaking the script to generate less frames simultaneously. 19.5GB peak VRAM usage, 1 min 25 secs to generate at 225 watts.[/edit] reply GistNoesis 11 hours agoparentI managed to get it working with a 4090. You need to adjust the parameter decoding_t of the sample function in simple_video_sample.py to a lower value (decoding_t = 5 works fine for me). I also needed to install imageio==2.19.3 and imageio-ffmpeg reply kouteiheika 11 hours agorootparentAh, yep! You're right! It works now! reply ganeshkrishnan 11 hours agoparentprev4090 is in weird spot. High speed but low RAM. Theoretically everything should run in ai but practically nothing runs reply karolist 3 hours agorootparentYou can add multiple, but practically speaking you're better off with used 3090s which you get 2 for the price of one 4090. I have 3090 Ti and I can run Q4 quant 33b models at 30t/s with 8k context. A 4090 would allow me to do the same but with ~45t/s, both inference speeds are more than fast enough for people so 3090 is the usual choice. In my tests on runpod, H100 with 80GB memory is around the same speed as 3090, so slower than a 4090. reply jug 53 minutes agorootparentprevAlmost sounds like a GPU vendor who isn't seeing enough competition. reply ImHereToVote 49 minutes agorootparentAlmost like the only competition of Nvidia is the niece of the CEO. reply Sohcahtoa82 10 hours agorootparentprevThey don't want to cannibalize sales of the super-expensive GPUs dedicated to ML/AI. 5090 likely won't have more than 32 GB, if even that much. reply Tenoke 9 hours agorootparentI made a Manifold market[0] on the amount of ram a 5090 will have, and while pretty much nobody has participated, I just checked and the market is amusingly at the 32GB you've also quoted. Just like you, I hope it will be more but I fear it will be even less. 0. https://manifold.markets/Tenoke/how-much-vram-will-nvidia-50... reply karolist 3 hours agorootparentprevEven 32GB would be great for a gaming card, any more and you're never seeing on sale as it will be bought by truckloads for AI, so of course they're not gonna balloon the VRAM. I suspect we'd still be at 16GB but they launched 3090 on Sep 2020 with 24GB, before all this craze really, and lowering is bad optics now. reply chaxor 2 hours agorootparentprev\"Theoretically everything should run in ai\" Odd statement. I don't really know what you mean by that. Perhaps 'math _works_, code should too' ? I would definitely agree that it _should_ work. I'm of the belief that no one should _have to_ publish (e.g. to graduate, get promotions, etc) in academia, and that publications should only occur if they're believed to be near Novel prize worthy, and fully reproducible by code with packaging that should last and work in 10 years, from data archives that will exist in 10 years. But it seems I have been outvoted by the administration in academia. Hence, we get this \"ai that doesn't run\" phenomenon reply KeplerBoy 2 hours agorootparentWhat's the point of academia if not to publish? Do you want to publicly fund researchers only for the industrial research partner's benefit? reply Zenst 9 hours agorootparentprevPerhaps NVIDIA or somebody could invent a RAM upgrade via NVLINK? Seems plausible and not every problem would want to add another GPU when the ability to add the extra memory alone is all they need. reply wongarsu 9 hours agorootparentBut why would NVIDIA do that when they can just sell you an A100 for ten times the price of a 4090? reply margorczynski 8 hours agorootparentWe need AMD to compete, but from what I know their software is subpar to NVIDIA's offering and most of the current ML stacks are built around CUDA. Still there's a lot of money to be made in this area now so competition big and small should pop up. reply idonotknowwhy 6 hours agorootparentI'd love it if AMD and Intel teamed up to make a wrapper layer for CUDA. Surely they'd both benefit greatly. reply versteegen 4 hours agorootparentFirst Intel and then AMD funded a wrapper, yes. Unfortunately the new version supports AMD but no longer Intel. https://github.com/vosen/ZLUDA That's a binary level wrapper. Of course there's also ROCm HIP at the source level, and many other things, such as SYCL reply dragonwriter 8 hours agorootparentprevIn a hypothetical near-future world, competition? reply chaostheory 8 hours agorootparentprevYeah, I’m still debating whether I go with a Mac Studio with the RAM maxed out (approx $7500 for 192 GB) or a PC with a 4090. Is there a better value path with the Nvidia A series or something else? (I’m not sure about tibygrad) reply karolist 3 hours agorootparentI have an M1 Max with 64GB and 3090 Ti. M1 Max is ~4x slower at inference for the same models than 3090 (i.e. 7t/s vs 30t/s), which depending on the task can be very annoying. As a plus you get to run really large models, albeit very slowly. Think if that will bother you or not. I will not give up my 3090 Ti and am rather waiting for 5090 to see what it can do because when programming, the Mac is too slow to shoot of questions. I use it mostly to better understand book topics now and 3090 Ti to do fast chat sessions. reply Oioioioiio 41 minutes agorootparentprevJust don't max out the Mac Studio and get both... reply chaxor 2 hours agorootparentprevGroq may be an option? reply jokethrowaway 11 hours agorootparentprevWhat can't you run? Unquantised large text models are the only thing I can't run Stable diffusion, stable video, text models, audio models, I never had issues with anything yet reply LoganDark 11 hours agorootparentprev4090 has more VRAM than most computers have system RAM. Surprised this is considered \"low RAM\" in any way except for relative to datacenter cards and top-spec ASi. reply samplatt 6 hours agorootparentYou're comparing RAM amounts to other RAM amounts without considering requirements. 24GB is more than (most) current games would ever require, but is considered a uncomfortably-constrictive minimum for most industrial work. Traditional CPU-bound physics/simulation models have typically wanted all the RAM they could get; the more RAM the more accurate the model. The same is true for AI models. I can max out 24GB just using spreadsheets and databases, let alone my 3D work or anything computational. reply idonotknowwhy 6 hours agorootparentprevDidn't know 24GB was considered low lol. reply Hikikomori 10 hours agorootparentprevMaybe dont use a gaming card for ai then? 24 is plenty as most games dont use more than half in 4k. reply smcleod 10 hours agorootparentMaybe give me lots of money to give Nvidia for a card with more memory then? Nvidia have held back the majority of their cards from going over 24GB for years now. It's 2024 and my laptop has 96GB of RAM available to the GPU but desktop GPUs that cost several thousands just by themselves are stuck at 24GB. reply dannyw 10 hours agorootparentThey don’t get their absurd profit margins by cannibalising their data centre chips. This is like Intel and their refusal to support ECC memory; when AMD does on nearly all Ryzens. — Note: your laptop is probably using a 64-bit memory bus for system RAM. For GPUs, the 4090 is 384-bit. That takes up a lot more die area for the bus and memory controller. reply versteegen 4 hours agorootparentBut GP's laptop with 96GB of unified memory would be a M2 Max Macbook or better. The M2 Max has a 4 x 128-bit memory bus (410GB/s) and the M2 Ultra is 8 x 128bit (819GB/s), versus a 4090 at 1008GB/s. But see here for caveats about Mac bandwidth: https://news.ycombinator.com/item?id=38811290 reply DSMan195276 9 hours agorootparentprevIsn't there the risk that if they give the gaming cards enough RAM for such tasks then they'll get bought up for that purpose and the second-hand price will go even higher? I guess my point is, rather than give the cards more RAM, the gaming cards should just be priced cheaper. reply Hikikomori 10 hours agorootparentprevWhy would they do that with a gaming card? If you want more you can rent in Aws etc. reply smcleod 7 hours agorootparentIt wouldn’t be a local model if it has to work on AWS. reply chaostheory 7 hours agorootparentprevWhich laptop models share system RAM with an Nvidia RTX cards? reply stygiansonic 7 hours agorootparentOp probably referring to an M series MacBook since it has a unified memory architecture and the same memory space used by both cpu and gpu reply karolist 3 hours agorootparentprevThis is unfairly downvoted. They launched 3090 on Sep 2020 with 24GB which was more than AMD's 16GB 6900XT launched on that same month. Maybe before blaming Nvidia, blame AMD for lack of trying to compete with them? Of course they're not gonna release a gaming card with loads more VRAM because a) competition doesn't exist nor has gaming cards with more VRAM b) it would all be bought up for AI workloads c) games don't really need more as parent said. reply Filligree 13 hours agoprevIf the animations shown are representative, then the mesh output may very well be good enough to use in a 3d printer. Looking forward to experimenting with this. reply jsheard 13 hours agoparentWith previous attempts at this problem the shaded examples could be quite misleading because details that appeared to be geometric were actually just painted over the surface as part of the texture, so when you took that texture away you just had a melted looking blob with nowhere near as much detail as you thought. I'd reserve judgement until we see some unshaded meshes. What they show in the demo: https://i.imgur.com/9bZNTcd.jpeg What comes out of the 3D printer: https://i.imgur.com/MZrzsfh.png reply Oioioioiio 40 minutes agorootparentThere are AI models who can create proper meshes though. reply strich 9 hours agorootparentprevThere exists software to reproject texture normals back on to a high poly model. So this problem does have a solution for anyone interested. reply jsheard 9 hours agorootparentThat's assuming your generator produces a normal map, the ones I've seen do not, the only texture channel they output is color. That being the one channel that a model trained on images is naturally equipped to produce. reply pksebben 6 hours agorootparentI may be speaking out of ignorance here, but couldn't you use photogrammetry techniques to translate these to a higher resolution mesh? reply huytersd 5 hours agorootparentprevYou can generate pretty reliable texture depth maps from just an image. It’s going to be trash if you’re trying to generate the depth for the entire 3D model but I presume it’s going to go a good job with just texture. Then you just use a displacement based on the depth map. reply euazOn 11 hours agorootparentprevTherefore, what is the main usecase of this model? Generating cheap 3D assets for videogames? reply jsheard 10 hours agorootparentI don't think they have a specific use-case for this model, they're throwing ideas at the wall again in the hopes some of them stick and eventually turn into another product. The paper doesn't discuss any of the problems that would need to be solved in order to easily generate game-ready assets so I think it's safe to assume that it currently doesn't. For games at the very least you need to consider polygon budget, getting reasonably good UVs, and generating materials which fit into a PBR shader pipeline, at least if it's going to work with rendering pipelines as we know them today (as opposed to rendering neural representations directly, which is a thing people are trying to do but is totally unproven in production). reply pksebben 6 hours agorootparentI'd be willing to bet you could create a diffusion model to map unrefined meshes to UV-fixed and remeshed surfaces. If you had a large enough library of good meshes you just programmatically mess 'em up and use that as the dataset. reply SV_BubbleTime 12 hours agorootparentprevIt’s always been this. None of these ever show the untextured model. When I see a demo where they are showing wireframes I know it’ll be good enough. reply jsheard 12 hours agorootparentSeems like a tougher nut to crack than image generation was, since there isn't a bajillion high quality 3D models lying around on the internet to use as training data, everyone is trying to do 3D model generation as a second-order system using images as the training data again. The things that make 3D assets good, the tiny geometric details that are hard to infer without many input views of the same object, the quality of the mesh topology and UV mapping, rigging and skinning for animation, reducing materials down to PBR channels that can be fed into a renderer and so on aren't represented in the input training data, so the model is expected to make far more logical leaps than image generators do. reply wincy 6 hours agorootparentI know where I could get several hundred terabytes (maybe an exabyte? It’s constantly growing) of ultra high quality STL files designed for 3D printing. I just don’t have the storage or the knowledge of how to turn those into a model that outputs new STL files. I’d imagine it’d require a ton of tagging, although I have a good idea of how I could leverage existing APIs to tag it mostly automatically by generating three still image thumbnails of the content, then feeding that through CLIP, and verifying that all two or three agree on what it’s an STL of, and manually tag the ones that fail that test. reply supermatt 5 hours agorootparentThere’s a pretty big difference between hundreds of terabytes and an exabyte. Maybe you meant petabyte? reply derefr 8 hours agorootparentprev> since there isn't a bajillion high quality 3D models lying around on the internet to use as training data There aren't a bajillion high-quality 3D models of everything, but there are an unbounded number of high-quality 3D models of some things, due to the existence of procedural mesh systems for things like foliage. You could, at the very least, train an ML model to translate images of jungles into 3D meshes of the trees composing them right now. Although I wonder if having a few very-well-understood object types like these, to serve as a base, would be enough to allow such a model to deduce more generalized rules of optics, such that it could then be trained on other object categories with much smaller training sets... reply refulgentis 12 hours agorootparentprevIt almost seems easier, in that you have an arbitrary # of real world objects to scan and the hardware is heavily commoditized (IIRC iPhones have this built in at highres now?) reply polygamous_bat 11 hours agorootparentHow is building a dataset easier than using a prebuilt dataset? reply refulgentis 10 hours agorootparentIn context, the conversation was beyond a dichotomy - thankfully. Having only 2 choices leaves conversation at people insisting one is better, and becomes an argument about definitions where people take turns alternating being \"right\" from the viewpoint of a neutral observer. It's proposing a solution to the author's observation that everyone is doing it in second order fashion and missing a significant amount of necessary data. The implication is that rather than doing it the hard way via the already-obtained 2nd order dataset, it'll be easier to get a new dataset, and getting that dataset will be significantly easier that it was to get the second-order dataset, as you don't need to worry about aesthetic variety as much as teaching what level of detail is needed in the mesh for it to be \"real\" reply bobba27 11 hours agorootparentprevYes. But it is still promising. Things are getting incrementally better. (I dream of the day when this can be used to automatically create paper-craft templates.) reply neom 13 hours agoparentprevI don't know much about 3D printing, would be very interested in learning more about this idea if you'd be so kind as to expand on it. Could I have AI spend all day auto scanning what teens are doing on instagram, auto generate toys based on it, auto generate advertisements for the toys, auto 3D print on demand? reply CobrastanJorji 13 hours agorootparentI think their suggestion was more \"I have a photo of a cool horse, and now I would like a 3D model of that same horse.\" Another way of looking at it, 3D artists often begin projects by taking reference images of their subject from multiple angles, then very manually turning that into a 3D model. That step could potentially be greatly sped up with an algorithm like this one. The artist could (hopefully) then focus on cleanup, rigging, etc, and have a quality asset in significantly less time. reply bobba27 11 hours agorootparentThe question is whether this actually \"creates a 3d model based on the picture\", or if it \"finds an existing model that looks similar to the picture and texture map it\". reply SirSourdough 13 hours agorootparentprevHypothetically, sure, assuming the parent comment that these meshes are sufficient for modelling is correct and that you can find any teens who want a non-digital toy. I think a good hobbyist application for this would be something like modelling figurines for games, which is already a pretty popular 3D printing application. This would allow people with limited modelling skills to bring fantastical, unique characters to life “easily”. reply Filligree 13 hours agorootparentPretty much. We're already generating images of monsters and characters for a D&D campaign; being able to print those in 3D would be pretty amazing. reply maicro 13 hours agorootparentprevOP is suggesting that this (AI model? I honestly am behind on the terminology) could replace one of the common steps of 3D printing - specifically, the step where you create a digital representation of the physical object you would want to end up with. There are other steps to 3D printing in general, though; a super rough outline: - Model generation - \"Slicing\" - processing the 3D model into instructions that the 3D printer can handle, as well as adding any support structures or other modifications to make it printable - Printing - the actual printing process - Post-processing - depending on the 3D printing technology used, the desired resulting product, and the specific model/slicing settings, this can be as simple as \"remove from bed and use\" to \"carefully snip off support structures, let cure in a UV chamber for X minutes, sand and fill, then paint\" As I said before, this AI model specifically would cover 3D model generation. If you were to use a printing technology that doesn't require support structures, and handles color directly in the printing process (I think powder bed fusion is the only real option here?), the entire process should be fairly automatable - a human might be needed to remove the part from the printer, but there might not be much post-processing to do. The rest of your desired workflow is a bit more nebulous - I don't know how you would handle \"scanning what teens are doing on instagram\", at least in a way that would let you generate toys from the information; generating and posting the advertisement shouldn't be too hard - have a standardish template that you fill in with a render from the model, and the description; printing on demand again is possible, though you'll likely need a human to remove the part, check it for quality and ship it. You could automate the latter, but that would probably be more trouble than it's worth. reply neom 12 hours agorootparentInteresting, to be clear I don't think this is a good idea and it's kinda my nightmare post capitalism hell. I just think it's interesting this could be done now. On finding out what teens want, that part is somewhat easy-ish, I guess you'd need a couple of agents, one that is scanning teen blogs for stories and then converting them to key words, then another agent that takes the key words (#taylorswift #HaileyBieberChiaPudding #latestkdrama etc) into Instagram, after a while your recommend page will turn into a pretty accurate representation of what teens are into, then just have an agent look at those images and generate difs of them. I doubt it would work for a bunch of reasons, but it's an interesting thought experiment! Thanks! reply issung 9 hours agoprev> Stable Video 3D (SV3D) is a generative model based on Stable Video Diffusion that takes in a still image of an object as a conditioning frame, and generates an orbital video of that object. So can it actually output a 3d model? Or just images of what it thinks the object would look like from other angles? reply krebby 9 hours agoparentThe reference video (https://youtu.be/Zqw4-1LcfWg) says they use a NeRF / structure from motion and then create a mesh with marching cubes from the generated radiance field. This is how most soa text-to-object generators work now as well reply 2StepsOutOfLine 7 hours agoparentprevI'm also struggling to find any examples of how to actually get a 3D model output. Very few references to this capability outside of the blog post. reply ionwake 13 hours agoprevIm sorry for dumb lazy question. But would the input require more than one image? Is there a demo url to test this? I think it might jsut be time to buy a 3d printer. EDIT> Does \"single image inputs\" mean more than one image? reply simonw 12 hours agoparentIt's just a single image. It guesses the shape of the bits it can't see based on vast amounts of training data. reply ionwake 12 hours agorootparentAmazing! Thank you reply exodust 2 hours agoparentprevI have an even lazier question after failing to speed-read the article. Does this output an actual 3D mesh? Or does it only output a 3d-looking rendered animation? reply kylebenzle 12 hours agoparentprevSingle image means one image. reply dartos 12 hours agorootparentCan confirm the word single means 1 reply ionwake 12 hours agorootparentprevlol cmon guys don't be too hard on me it does say \"inputs\" reply stavros 12 hours agorootparentI do see how \"single image inputs\" can be conflated with \"multiple inputs of a single image each time\", as opposed to \"video\". reply ionwake 11 hours agorootparentTBH I always look at the worst case scenario. I was worried it meant it need 3 images inputted as a single image at direct steps of the process, so requiring different angles. I wasn't sure, but thought best to check. I feel like it would have been clearer to have said something like \" generates a 3d models from a single image\". ( not exact wording but you catch my drift ). Sorry I am over analysing but all feedback is good right? reply ganeshkrishnan 11 hours agorootparentprevDescribe in single words only the good things that come into your mind about... your mother. reply ddtaylor 12 hours agoprevDoes anyone know what hardware inference can run on or memory requirements? reply Mathnerd314 11 hours agoparentIn the repo the model weights file is 9.37GB, whereas sdxl turbo is 13.9GB, and I don't see any mention of huge context windows, so probably it just needs a decent graphics card. reply kouteiheika 11 hours agoparentprevIt crashes with an out-of-memory error on my 24GB 4090, so at least when it comes to their sample script the answer is \"a lot\". Maybe it's just an inefficient implementation though. reply dragonwriter 9 hours agorootparentPretty much every initial Stability release has been inefficient and has resources drop a lot when optimized for real consumer hardware community engines appeared for running the model. OTOH, with their shift to a less open licensing structure, community tooling probably won’t emerge with the same level of energy. reply dubin 8 hours agoprevI'd like to play around with something like this, but from my understanding my machine (Macbook, 2021 M1) isn't nearly powerful enough (right?). Are there remote/cloud environments where I can run models like this? reply ilaksh 7 hours agoparentI suggest just using Stability's API. You aren't allowed to use it locally for commercial use anyway. You could set something up on RunPod or AWS, but I doubt it's worth the effort. reply airstrike 13 hours agoprevthat demo animation is so clever and satisfying reply amelius 12 hours agoparentBut it doesn't look very realistic, tbh. reply dreadlordbone 11 hours agorootparentit doesn't break Euclidian space at least reply itsgrimetime 9 hours agoparentprevI can’t get them to play reply londons_explore 10 hours agoprevAll the examples resemble plastic children's toys... How would it handle other objects? (People, fabrics, buildings, plants, mountains, mechanical parts, etc) reply programjames 10 hours agoparentIt's hard to get camera position tracking for random objects, so it looks like they used simulations. There's probably a lot more plastic children's toy models in Blender than people, fabrics, buildings, &c. reply abdellah123 6 hours agoprevDid you write the blog post using AI ? reply throwaway743 4 hours agoprevAnyone know of anything that'll auto rig/add weights? reply ImHereToVote 8 minutes agoparentThere are numerous tools that auto-rig humanoid figures. The obvious one: https://www.mixamo.com/#/ reply canadiantim 11 hours agoprevI can't wait until we can use something like this for architectural design reply dheera 6 hours agoprevThey compare against Zero123-XL, but they should compare against MVDream instead. MVDream is quite good. If you fiddle with the loss you can get even better results. reply leesec 7 hours agoprev [–] I wonder when Emad will be outed as a fed or a fraud. He's certainly leaving a trail of nasty behavior in the industry. reply preommr 7 hours agoparent [–] elaborate? reply esafak 6 hours agorootparent [–] https://en.wikipedia.org/wiki/Emad_Mostaque#Controversy_and_... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SV3D is a new advanced generative model that produces high-quality, view-consistent 3D videos from single images, offering two variants: SV3D_u for orbital videos and SV3D_p for 3D videos along defined camera paths.",
      "Outperforming previous models like Stable Zero123 and Zero123-XL, SV3D excels in multi-view consistency and novel view synthesis, available for both commercial use with a Stability AI Membership and non-commercial use with weights on Hugging Face.",
      "Utilizing video diffusion models, SV3D improves disentangled illumination optimization and introduces masked score distillation sampling loss for superior outputs, enhancing 3D Neural Radiance Fields and detailed mesh representations."
    ],
    "commentSummary": [
      "Stability.ai has introduced Stable Video 3D, comparing the effectiveness of their Single Image model with models using multiple images.",
      "Discussions revolve around technical issues on specific GPUs, limitations, and future advancements in GPU models for high-performance computer setups.",
      "Topics cover generating high-resolution 3D models from images, automating 3D model creation and printing, potential AI use in toys and figurines for games, and challenges in obtaining high-quality 3D files, showcasing both potential and limitations of these technologies."
    ],
    "points": 562,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1710792377
  },
  {
    "id": 39742422,
    "title": "Firebase Misconfigurations Expose 125M User Records",
    "originLink": "https://env.fail/posts/firewreck-1/",
    "originBody": "TLDR: - Firebase allows for easy misconfiguration of security rules with zero warnings - This has resulted in hundreds of sites exposing a total of ~125 Million user records, including plaintext passwords & sensitive billing information After the initial buzz of pwning Chattr.ai had settled down, we set to work on scanning the entire internet for exposed PII via misconfigured Firebase instances. Attempt 1 MrBruh wrote up a rudimentary scanner in Python that would check for Firebase configuration variables in websites or their loaded .js bundles. It worked... until it didn't. Turns out that a Python program with ~500 threads will start to chew up memory over time. For us that meant it was OOM'ing within an hour of starting it. Attempt 2 Logykk rewrote the scanner in Go, unlike the prior it didn't seem to leak memory. Our initial estimate was that this was going to take ~11 days to scan through the 5 1/2 Million domains, but it turned out to be closer to 2-3 weeks which made this very much a waiting game. Manually checking every domain We first went and started manually looking at each entry in the 550k line text file, seeing if we could find anything interesting, this was time consuming and very repetitive. In the end, we went through quite a lot of it and had 136 sites and 6.2 million records, but we all knew that we needed to do something fully automated, because this was taking way too long. Catalyst Once we had acquired our shortlist of potentially affected sites we ran it through a secondary scanner that Eva had made called Catalyst. This would take the site (or .js bundle) we found, then automatically check for read access to common Firebase collections & any that were explicitly mentioned in the JavaScript itself. When it found read access to a collection it would attempt to calculate the impact of the exposed data by gathering a sample of 100 records, checking the type of information contained and extrapolating that by the total size of the collection. We then, decided the database to use to store all of these results, and we chose Supabase (which uses PostgreSQL under the hood) a open source Firebase competitor, just for the irony. Once all that had been completed the data was formatted and uploaded to a private database table. The Numbers All (records): 124,605,664 Names: 84,221,169 Emails: 106,266,766 Phone Numbers: 33,559,863 Passwords: 20,185,831 Billing Info (Bank details, invoices, etc): 27,487,924 These numbers should be taken with a grain of salt. They are likely larger than shown here. The Shortlist (of affected sites) 1. Silid LMS Learning management system for students & teachers Most total exposed user records, at 27 Million affected users. (Names, Emails & Phone numbers) 2. Online gambling network Comprised of 9 sites which are all reskins of each other. Some spins are rigged to have a 0% chance of winning. Customer support tried to flirt with me when attempting to report the issue. Contains the most amount of exposed bank account details (logins) out of all of our records. (8 Million) Also contains the most plaintext passwords out of all of the affected websites, at 10 Million passwords being exposed. 3. Lead Carrot Online “lead” generator for cold calling Top 3 in total amount of exposed user information, siting at 22 Million affected people. 4. MyChefTool Business management app & Point of Service application for restaurants 1st place for most exposed Names & 2nd place for exposed Emails. (14 Million & 13 Million respectively) The aftermath 842 Emails sent over 13 days 85% Emails delivered 9% Emails bounced 24% of Site owners fixed the misconfiguration 1% of Site owners emailed us back 0.2% (2) Sites owners offered a bug bounty",
    "commentLink": "https://news.ycombinator.com/item?id=39742422",
    "commentBody": "900 Sites, 125M accounts, 1 Vulnerability (env.fail)550 points by MrBruh 23 hours agohidepastfavorite133 comments abeisgreat 21 hours agoI worked at Firebase for many years and the concerns with security rules have always plagued the product. We tried a lot of approaches (self expiring default rules, more education, etc) but at the end of the day we still see a lot of insecure databases. I think the reasons for this are complex. First, security rules as implemented by Firebase are still a novel concept. A new dev joining a team adding data into an existing location probably won’t go back and fix rules to reflect that the privacy requirements of that data has changed. Second, without the security of obscurity created by random in-house implementations of backends, scanning en masse becomes easier. Finally, security rules are just hard. Especially for realtime database, they are hard to write and don’t scale well. This comes up a lot less than you’d think though, as any time automated scanning is used it’s just looking for open data, anything beyond “read write true” as we called it would have prevented this. Technically there is nothing wrong with the Firebase approach but because it is one of the only backends which use this model (one based around stored data and security rules), it opens itself up to misunderstanding, improper use, and issues like this. reply supriyo-biswas 20 hours agoparentTo be honest I've always found the model of a frontend being able to write data into a database highly suspect, even with security rules. Unlike a backend where where the rules for validation and security are visible and part of the specifications, Firebase's security rules is something one can easily forget as it's a separate process, and has to be reevaluated as part of every new feature developed. reply Kiro 19 hours agorootparentYeah, I've never understood how this concept can work for most applications. In everything I build I always need to do something with the input before writing it to a database. Just security rules are not enough. What kind of apps are people building where you don't need backend logic? reply amadeuspagel 19 hours agorootparentMany apps where every user has his own data, which just needs to be synced between devices. reply Falimonda 17 hours agorootparentCurious as to which apps if there are any you can point to? reply amadeuspagel 17 hours agorootparentA typical notes app. reply cbsmith 19 hours agorootparentprevI think I missed where writing to the database precludes backend logic. Databases have triggers and integrity rules, but beyond that, why can't logic execute after data is written to a database? reply withinboredom 18 hours agorootparentBecause once it is written to the database, it can be output somewhere before you execute your logic. IE, explicit language, child porn, etc. You generally want to check for that BEFORE you write the data. reply cbsmith 18 hours agorootparentYou're saying it's impossible to have public write access to a table without also providing public read access? \"it can be output somewhere before you execute your logic\" is a design choice that is orthogonal from whether you execute your logic before or after input into the database. reply withinboredom 18 hours agorootparentYou generally don't want to write child porn to disk, if you can help it. reply cbsmith 18 hours agorootparentFirst of all, most database records couldn't fit child porn, unless it was somehow encoded across thousands of records, in which case you couldn't realize it was child porn until after you've stored 99% of it. Sure though, by putting \"child porn\" in a sentence, you can make anything seem bad. Tell me this, would you rather your application middleware was in the \"copying child porn\" business? ;-) Actually, the more I think about it, the crazier this seems. You're going to store all the \"child porn\" you receive in RAM until you've validated that it is child porn? reply withinboredom 18 hours agorootparentI don’t get your tone or why you seem shocked that binary data can be stored in a database. Postgres and MySQL both have column sizes for binary data that can hold gigabytes. Second, you generally need to hold the entire image in RAM to create the perceptual hash needed to check that the image is/isn’t child porn. reply cbsmith 17 hours agorootparent> I don’t get your tone or why you seem shocked that binary data can be stored in a database. Postgres and MySQL both have column sizes for binary data that can hold gigabytes. My tone is shocked, because what you're describing seems totally removed from any system I've seen, and I've implemented a ton of systems. For performance reasons, you want to stream large uploads to storage (web servers, like nginx, are typically configured to do this even before the request is sent to any application logic). You invariably want to store UGC data that conforms to your schema, even if you're going to reject it for content. There's a whole process for contesting, reviewing and reversing decisions that requires the data be in persistent storage. I think you misunderstood what I said. Yes, Postgres, MySQL and a variety of other databases have column sizes for binary data that can hold gigabytes. What I wouldn't agree with is that most database records can hold gigabytes, binary or otherwise. Heck, most database records aren't populated from UGC sources and not UGC sources where child porn is a risk. But okay, let's assume, for arguments sake, most database records are happily accepting 4TB large objects, and you're accepting up to 4TB uploads (where Postgres' large objects max out). Do all your web & application servers have 4TB of memory? What if you're processing more than one request at once, do you have N*4TB of memory? At least all the systems I've implemented that receive data from users enforce limits on request sizes, and with the exception of file uploads, which are typically directly streamed to the filesystem before processing, those limits tend to be quite small, often less than a kilobyte. Maybe someone could write some really terse child porn prose and compress it down to fit in that space, but pretty much any image would have to be spread across many records. By design, almost any child porn received would be put in persistent storage before being identified as such. > Second, you generally need to hold the entire image in RAM to create the perceptual hash needed to check that the image is/isn’t child porn. This is one of many reasons that you generally want to stream file uploads to storage before performing analysis. Otherwise you're incredibly vulnerable to a DoS attack on your active memory resources. Even without a DoS attack, you're harming performance by unnecessarily evicting pages that could be used for caching/buffering for bytes that won't be served at least until you've finished receiving all the file's data. [Note: Many media encodings tend to store neighbouring pixels together, so you can, conceptually, compute a perceptual hash progressively, without loading the entire file into active memory, which is often desirable, particularly with video content.] reply cbsmith 16 hours agorootparentThought about it some more... this whole scenario makes sense in only the narrowist of contexts. Very few applications directly serve UGC to the public, and a lot of applications are B2B. You're authenticated, and there's a link to your employer (or you if you're self-employed). Uploaded data isn't made visible to the public. Services are often limited to a legal jurisdiction. If you want to upload your unencrypted child porn to a record in Google's Firebase database, you go ahead. The feds could use some easy cases. reply abeisgreat 18 hours agorootparentprevWriting directly to Firebase is rarely done past the MVP stage. Normally it's the reading which is done directly from the client. Generally writes are bounced through Cloud Functions or a traditional server of some form. Some also \"fan out\" data, where a user has a private area to write to (say a list of tweets) then they get \"fanned out\" to follower's timelines via an async backend process which does any verification / cleansing as needed. reply xyzeva 10 hours agorootparentSadly, most developers don't know this and continue to write from frontend, almost all of the apps and websites we found did this. reply refulgentis 19 hours agorootparentprevIt's a really good question context: I have a near-100% naive perspective. Mobile dev whose built out something approximating Perplexity on Supabase. I have to use edge functions for ex. CORS, but by and large, logic is all in the app. Probably because the client is in Flutter, and thus multiplatform & web in one, I see manipulating the input on both the client and server as code duplication and error prone. I think if I was writing separate native apps, I'd push everything through edge functions, approximating your point: better to have that sensitive logic of what exactly is committed to the DB in one place. reply piotrkaminski 13 hours agorootparentprevOur experience has been very different. Our Firebase security rules are locked down tight, so any new properties or collections need to be added explicitly for a new feature to work — it can't be \"forgotten\". Doing so requires editing the security rules file, which immediately invites strict scrutiny of the changed rules during code review. This is much better than trying to figure out what are the security-critical bits in a potentially large request handler server-side. It also lets you do a full audit much more easily if needed. reply winwang 20 hours agorootparentprevAre you suggesting that it's essentially too easy for a dev to just set and forget? That's a pretty interesting viewpoint. Not sure how any BaaS could solve that human factor. reply btown 19 hours agorootparentSay you add a super_secret_internal_notes field. If you're writing a traditional backend, some human would need to explicitly add that to a list of publicly available fields somewhere (well, hopefully). For systems like Firebase, it's far too easy to have this field be created by frontend code that's just treating this as another piece of data in a nested part of a payload. But this can also happen on any system, if you have any JSON blob whose implicit schema can be added to by frontend development alone. IMO implicit schema updates on any system should be consolidated and lifted to an easily emailed report - a security manager/CSO/CTO should be able to see all the super_secret_internal_notes as they're added across the org, and be able to immediately rectify security policies (perhaps even in a staging environment). AFAIK Firebase doesn't do this - while there are pretty audit logs, there's not an automatic rollup of implicit schema changes: https://firebase.google.com/support/guides/cloud-audit-loggi... (Also, while tongue in cheek, the way that the intro to a part of Firebase's training materials https://www.youtube.com/watch?v=eMa0hsHqfHU implicitly centers security as part of the launch process, not something ongoing, is indicative of how pervasive the issue is - and not at all something that's restricted to Firebase!) reply abeisgreat 18 hours agorootparentGenerally agreed on improved audit logs of some formed helping. Re training materials, this is one of the mitigations we launched to attempt to pull security to front of mind. I do not really think this is a Firebase problem, I think average developers (or average business leaders) just don't, in general, think much about security. As a result, Firebase materials have a triple burden - they need to get you to think about security, they need to get you to disrupt the most \"productive\" flow to write rules, and they need to get you to consistently revisit your rules throughout development. This is a lot to get into someone's head. For all the awesomeness of Firebase's databases, they're both ripe footgun territory (Realtime Database specifically). Our original goal was to make the easiest database to get up and running with, which I think we did, but that initial ease comes with costs down the road which may or may not be worth it, that's a decision for the consumer. reply supriyo-biswas 19 hours agorootparentprevYou could either do away with the model of the frontend writing to the DB and ask customers to implement a small backend with a serverless component like AWS Lambda or Google Cloud Functions. Barring that, perhaps Firestore could introduce the concept of a \"lightweight database function hook\" akin to Cloudflare workers that runs in the lifecycle of a DB request, thus formalizing the security requirements specific to the business requirement and causing the development organization to allocate resources to its upkeep. So while a security rule usually gets tested very lightly, you'd see far more testing in a code component like the one I'm suggesting. reply cbsmith 19 hours agorootparent> Barring that, perhaps Firestore could introduce the concept of a \"lightweight database function hook\" akin to Cloudflare workers that runs in the lifecycle of a DB request, thus formalizing the security requirements specific to the business requirement and causing the development organization to allocate resources to its upkeep. Firebase has triggers. reply chavesn 19 hours agorootparentprevI think it's more like there's more surface area to forget when you have humans handling so many concerns, and it's not likely the part that's changed the most so it's a likely candidate for being \"pushed out of the buffer\" (of the human). In a more typical model, backend devs focus more on security, while not needing to know the frontend, and vice versa. reply bruce343434 19 hours agorootparentprevEventually, humans will forget, set or not. reply xyzeva 10 hours agorootparentI agree, but I also disagree. The concept with firebase DB's is flawed IMO, I never got the point of directly accessing a DB in the frontend, or allowing that even with security rules, it just seems like it would cause problems. reply xyzeva 21 hours agoparentprevWe tried to contact google, via support to try to help or for them to help disclose the issues to the websites. We got no response other then a response telling us that they will be creating a feature request on our behalf if we wanted instead of helping us, which is fair as I think we'd have to escalate pretty far up in Firebase to get the attention of someone who could alert project owners. reply abeisgreat 21 hours agorootparentOne of the things we fought for, for years after acquisition was to maintain a qualified staff of fulltime, highly paid support people who are capable of identifying and escalating issues like this with common sense. This is a battle we slowly lost. It started with all of support being the original team, then went to 3-4 fulltime staff plus some contracts, to entirely contractors (as far as I’m aware). This was a big sticking point for me. I told them I did not believe we should outsource support, but they did not believe we should have support for developer products at all, so I lost to that “compromise.” After that I volunteered myself to do the training of the support teams, which involved traveling to Manila, Japan and Mexico regularly. This did help but like support as whole, it was a losing battle and quality has declined over time. Your experience is definitely expected and perhaps even by design. Sadly this is true across Google, if you want help you’d best know a Googler. reply GuB-42 20 hours agorootparentI suspect it is going to end up being Google's downfall, or at least, be part of it. They simply don't know humans. Their repeated failures at building social networks is good enough evidence. They always try to have the human out of the loop, which, to be fair, worked for them in the early days, as their search engine was better than those that relied on human-made directories. But now it is becoming ridiculous. It is a company of bots, for bots. And when they need humans for some reason, they take away most of the value they can add with rigid frameworks, basically treating them like bots. They pay hundreds of thousands not for people who are competent and trustworthy to provide the best service, but instead, to people who write bots to provide mediocre service. I believe that at some point, a startup who understand humans will eat them up, bit by bit, by feeding on dissatisfied customers who don't want to deal with stupid bots. reply xyzeva 19 hours agorootparentI really doubt that this will be google's downfall, theyre too big to fall right now. I think it will be laws. reply zackmorris 16 hours agorootparentThank you, and good job for isolating the root cause and solution. Deregulation is an op designed to prevent the people from toppling dragons. reply withinboredom 18 hours agorootparentprevThe bigger they are, the harder they fall; is a saying for a reason. There is no such thing as “too big to fail” otherwise the East India Trading Company would still be in operation. reply sillyfluke 16 hours agorootparentSometimes. IBM was still considered big when Buffet invested in them early in the 2010s. And it took almost a decade worth of bad performance for him to finally exit. It might be slowly sliding into irrevelance but its stock hasn't completely tanked -- during or after that period. reply andybak 20 hours agorootparentprev> they did not believe we should have support for developer products at all That explains a lot. reply xyzeva 10 hours agorootparentWhat'd you expect, its google! reply 0xdeadbeefbabe 18 hours agorootparentprev> which is fair as I think we'd have to escalate pretty far up in Firebase to get the attention of someone who could alert project owners. This begs the question, isn't this a security vulnerability after all? reply xyzeva 10 hours agorootparentIt is for the sites, not Firebase. reply seanwilson 20 hours agoparentprevLooking at https://firebase.google.com/docs/rules/basics, would it be practical to have a \"simple security mode\" where you can only select from preset security rule templates? (like \"Content-owner only\" access or \"Attribute-based and Role-based\" access from the article) Do most apps need really custom rules or they tend to follow similar patterns that would be covered by templates? A big problem with writing security rules is that almost any mistake is going to be a security problem so you really don't want to touch it if you don't have to. It's also really obvious when the security rules are locked down too much because your app won't function, but really non-obvious when the security rules are too open unless you probe for too much access. Related idea: force the dev to write test case examples for each security rule where the security rule will deny access. reply piotrkaminski 13 hours agoparentprevOne simple trick helped us a lot: we have a rules transpiler (fireplan) that adds a default \"$other\": {\".read\": false, \".write\": false} rule to _every_ property. This makes it so that any new fields must be added explicitly, making it all but impossible to unknowingly \"inherit\" an existing rule for new values. (If you do need a more permissive schema in some places you can override this, of course.) Our use of Firebase dates back 10+ years so maybe the modern rules tools also do this, I don't know. What would really help us, though, would be: 1. Built-in support for renaming fields / restructuring data in the face of a range of client versions over which we have little control. As it is, it's really hard to make any non-backwards-compatible changes to the schema. 2. Some way to write lightweight tests for the rules that avoids bringing up a database (emulated or otherwise). 3. Better debugging information when rules fail in production. IMHO every failure should be logged along with _all_ the values accessed by the rule, otherwise it's very hard to debug transient failures caused by changing data. reply mh8h 18 hours agoparentprevIt's not that difficult to build the scanner into the firebase dashboard. Ask the developer to provide their website address, do a basic scanning to find the common vulnerability cases, and warn them. reply abeisgreat 17 hours agorootparentFirebase does that, the problem is \"warning them\" isn't as simple as it sounds. Developers ignore automated emails and they rarely if ever open the dashboard. Figuring out how to contact the developers using the platform (and get them to care) has been an issue with every developer tool I've worked on. reply nness 20 hours agoparentprevI've been an advocate for Firebase and Firestore for a while — but will agree to all of these points above. It's a conceptual model that is not sufficiently explained. How we talk about it on own projects is that each collection should have a conceptual security profile, i.e. is it public, user data, public-but-auth-only, admin-only, etc. and then use the security rule functions to enforce these categories — instead of writing a bespoke set of conditions for each collection. Thinking about security per-collection instead of per-field mitigates mixing security intent on a single document. If the collection is public, it should not contain any fields that are not public, etc. Firestore triggers can help replicate data as needed from sensitive contexts to public contexts (but never back.) The problem with this approach is that we need to document the intent of the rules outside of the rules themselves, which makes it easy to incorrectly apply the rules. In the past, writing tests was also a pain — but that has improved a lot. reply andenacitelli 21 hours agoparentprevIt also makes portability a pain. Switching from an app with Firebase calls littered through the frontend and data consistency issues to something like Postgres is a lengthy process. reply ben_jones 17 hours agoparentprevFirebase attracts teams that don’t have the experience to stand up a traditional database - which at this point is a much lower bar thanks to tools like RDS. That is a giant strobing red light of a warning for what security expectations should be for the average setup. No matter what genius features the Firebase team may create this was always going to be a support and education battle that Google wasn’t going to fully commit to reply markhalonen 19 hours agoparentprevat Steelhead we use RLS (row level security) to secure multi-tenant Postgres DB. Coolest check we do is create a new Tenant and dbdump with RLS enabled and ensure the dump is empty. Validates all security policies in 1 fell swoop. reply kjuulh 19 hours agoparentprevThe security rules where I fell off my love with Firebase, not that there is anything wrong with the security, but until the point of having to write those security rules, the product experience felt magical, so easy to use, only one app to maintain pretty much. But with the firebase security rules, I now pretty much have half of a server implemented to get the rules working properly, especially for more complex lookups. And for those rules, the tooling simply wasn't as great as using typescript or the likes. I haven't used firebase in years tho, so I don't know if it has gotten easier. reply cryptonector 16 hours agoparentprevFirebase needs something like RLS (row-level security). It needs to be real easy to write authorization rules in the database, in SQL (or similar), if you're going to have apps that directly access the database instead of accessing it via a proxy that implements authorization rules. reply xyzeva 10 hours agorootparentI agree! Supabase does it pretty good. reply mistrial9 21 hours agoparentprevvery well spoken arguments for a fundemental need for structural diversity, not monoculture, on the net reply brazzy 21 hours agorootparentI don't see the comment arguing for that at all, and I don't think the analogy to crop monocultures being more vulnerable to pests really holds. There are good reasons we deride \"security through obscurity\" as valid, and just because \"structural diversity\" makes automated scanning harder doesn't mean it can't be done. See Shodan. reply mananaysiempre 20 hours agorootparentThe idea as I (who is not GP) see it is not that diversity makes scanning harder, it’s that it makes the blast radius smaller. Notably, though, that means we have to be talking about diversity of implementations, not just deployments—numerous deployments of just a few pieces of software can be problematic in their own ways, and of course there have been bugs with huge consequences in Apache, MSRPC, or—dare I say it—sendmail since the very earliest days. reply 0xdeadbeefbabe 17 hours agorootparentprev\"security through obscurity\" is red team trash talk mostly. reply Logykk 20 hours agoparentprevI view the issue as more of a poor UX choice than anything else. Firebase's interface consists entirely of user-friendly sliders and toggles EXCEPT for the security rules, which is just a flimsy config file. I can understand why newer devs might avoid editing the rules as much as possible and set the bare minimum required to make warnings go away, regardless of whether they're actually secure or not. There should be a more graphical and user-friendly way to set security rules, and devs should be REQUIRED to recheck and confirm them before any other changes can be applied. reply begueradj 21 hours agoprevThis reminds me of \"How I pwned half of America’s fast food chains, simultaneously.\" https://mrbruh.com/chattr/ HN: https://news.ycombinator.com/item?id=38933999 reply MrBruh 21 hours agoparentThat's because I wrote both of those articles, and this is the sequel to that blog post. :P reply begueradj 21 hours agorootparentIndeed :) Same authors ... referring to each others :) reply xyzeva 21 hours agorootparentWe decided to make a shared blog because we will likely have other projects we will do together, so all of us posting on our personal blogs on the same topic would be counterproductive reply begueradj 21 hours agorootparentThat's a good thing. There is nothing wrong with your approach. Glad you are sharing what you do because what you do interests many people. reply hk__2 20 hours agoparentprevThis is normal, the 6th word of the blog post is a link to it. > After the initial buzz of [pwning Chattr.ai] had settled down, […] reply rjbwork 21 hours agoprevCorrect me if I'm wrong, but 75% of sites with these vulns are still just hanging out there ready to be dumped, according to the end of this post? Insane. Some days I think one ought to be licensed to touch a computer. reply Aurornis 17 hours agoparentMany businesses don’t have full time developers. They contract out to agencies who build the website for them. The agencies have a rotating cast of developers and after the initial encounter with their good devs they try to rotate the least experienced developers into handling the contract (unless the company complains, which many don’t). The vulnerability emails probably got dismissed as spam, or forwarded on and ignored, or they’re caught in some PM’s queue of things to schedule meetings about with the client so they can bill as much as possible to fix it. > Some days I think one ought to be licensed to touch a computer. There are plenty of examples of fields where professional licensing is mandatory but you can still find large numbers of incompetent licensed people anyway. Medical doctors have massive education and licensing requirements, but there is no shortage of quack doctors and licensed alternative medicine practitioners anyway. reply xyzeva 20 hours agoparentprevSadly, this is true, and theres probably much more. We did our best, sent customized emails to each of them, telling what was affected, how to fix it, and how to get in contact. reply MrBruh 20 hours agoparentprevCorrect, that's why we couldn't post a list of affected sites or malicious actors would immediately abuse it :/ reply wavemode 20 hours agorootparentIt seems reasonable to assume that the exposed information has already fallen into the wrong hands. Might as well post the list at this point (or at some point, at least) so that any users of those sites can become aware, no? reply prmoustache 18 hours agorootparentprevShouldn't encrypting all databased records be the only sane, safe and legal solution with decryption key sent to local (to the website owner) law enforcement when site owners aren't responsive? Not saying you should do that given the current state of the laws. reply tgv 20 hours agorootparentprevIt'll now take them 2-3 weeks to get the details. reply bsder 19 hours agoparentprevUntil PII compromise puts a company out of business, it's just a cost (to you) of doing business (for them). reply xyzeva 10 hours agorootparentTrue, but also better than threat actors getting to it and dumping the DB, causing more problems for the customers. reply itqwertz 20 hours agoprevThis is the inevitable outcome of picking cheap-fast from the cheap-fast-good PM triangle. Unfortunately for some customers/users, their concerns were left out of the conversation and their PII is the cost. I’d be wary of any company listed here that made that decision and hasn’t changed leadership, as it has been proven time and time again that many companies simply don’t care enough about customers enough to protect them. History repeats itself. reply xyzeva 10 hours agoparentI agree for the most, but there was some good apples (even though very few) that were very thankful and fixed it fast. reply simonw 19 hours agoprevI have a very basic Firebase question: are most of the apps described in this post implemented entirely as statically hosted client-side JavaScript with no custom server-side code at all - the backend is 100% a hosted-by-Google Firebase configuration? If so, I hadn't realized how common that architecture had become for sites with millions of users. reply evantbyrne 19 hours agoparentYeah. Either entirely client-side or passing through a server naively. This is the inevitable result of having an \"allow by default\" security model in an API. Unfortunately, insecure defaults are a common theme with libraries targeted at JavaScript developers. GraphQL is another area I would expect to see these kinds of issues. reply hazelnut 19 hours agoparentprev> with no custom server-side code at all Could be a mix. Firebase also offers Firebase Functions which are callable functions in the cloud. That code is not public. However, Firestore or Firebase realtime database both require the user to setup security rules. Otherwise all data can be read by anybody. reply cryptonector 16 hours agoparentprevThat's a pretty crazy set-up, but it can work if appropriate authorization rules are coded into the SQL schema on the backend. Writing appropriate authz rules on the backend has to be made easy. reply johnnyAghands 21 hours agoprev900 Sites, 125 million accounts, 1 vulnerability, 0 Girlfriends. reply voidUpdate 21 hours agoparentApart from the customer service agent that tried to flirt with them :P reply HaZeust 16 hours agoparentprevGreat comment, you really showed them! reply MrBruh 21 hours agoparentprevBro woke up on the wrong side of the bed reply throwaway984393 21 hours agorootparentBro woke up with no gf :'( reply maipen 20 hours agoprevStuff like this, makes me thankful to have chosen password managers and virtual cards a long time ago... Still this makes the interent scarier. Most people don't have a clue how fragile the web is and how vunerable they are. reply user90131313 19 hours agoparentSomehow my assumption is it will only get worse from here, with AI agents looking for exploits etc with much more efficiently than bots? weird future is waiting reply tamimio 18 hours agoparentprev> chosen password managers It’s not enough; make sure to use a unique email for each service you sign up for. This limits the damage in case of an incident and protects your privacy, as no one can perform OSINT on you to cross reference other services. Additionally, I’ve found that sometimes you can detect a site breach before the owners do when you receive a malicious email sent to that unique address. reply maipen 17 hours agorootparent> make sure to use a unique email for each service you sign up for. Unfortunatly that's a big hassle that I am not willing to go through. Apple's approach to pseudo emails was very nice and in my experience, works very well, but as mainly PC user I can't take advantage of this. Do you know or recommend a service for this thats easy and fast to use? reply LtdJorge 8 minutes agorootparentFirefox Relay reply tamimio 17 hours agorootparentprevIt isn’t a hassle, you won’t create a separate email, but an aliases. > Do you know or recommend a service for this thats easy and fast to use? Honestly I don’t like to promote any commercial services, but there are few out there that automate it, simplelogin (I believe you can host it yourself too), anonaddy, or fastmail that has an integration with 1password, so your password manager generate a random pass and an email alias automatically for you. There are more and it’s better to research it yourself to find the best solution for you, again, my post sounds like shilling for these products but a good start. reply faet 14 hours agorootparentprev>Do you know or recommend a service for this thats easy and fast to use? I use a google account with a catch all domain. {Website/Store} @ short domain that goes to the same mailbox. I reply from name @ different domain, though. The benefit is when I give it over the phone it's easy to say \"StoreName\"@ than spelling my name or using another longer email. reply YoshiRulz 15 hours agorootparentprevDuckDuckGo and Mozilla have similar products. And if your email provider allows the \"plus trick\", as Gmail does, you should at the very least use that. reply xyzeva 19 hours agoparentprevYeah, funny how that works. Services as time goes on makes making websites easier, and abstracts more stuff, which makes devs oblivious to what they have to configure. reply rfl890 21 hours agoprevThe customer support gave me a good laugh. Thanks reply suddenclarity 20 hours agoprev> Turns out that a Python program with ~500 threads will start to chew up memory over time. Anyone have more info about this issue? I've got a scraper myself in Python with a few hundred threads which seems to eat a lot of memory. Any workarounds or is the only solution to rewrite in another language? reply belorn 20 hours agoparentYou can do it in python but one has to dig into how python do reference counting and how that interacts with threads. Personally I prefer using processes rather than threads, with a worker pool and a message bus rather than shared memory. That solution has its own drawbacks (and a bit more overhead), but you don't need to worry so much about memory issues. Processes also seems a better match for crawlers since the number of processes will be fairly constant and the work the processes do is fairly independent. reply teddyh 19 hours agoparentprevimport multiprocessing as threading reply xyzeva 20 hours agoparentprevPython just isn't the language for this, really. Rewriting it is the only real solution, I don't know your exact problem. reply dist-epoch 20 hours agoparentprevYou really should be using ayncio. It's a perfect fit for your usecase. reply _tk_ 20 hours agoprevGood job! I’d be interested to know how you’re coming to the conclusion that the amount of affected users is likely higher. From the looks of it, I’d suspect that at least some of the sites you mention (gambling, lead carrot) to be littered with fake account data. reply MrBruh 20 hours agoparentWhen manually reviewing a lot of these sites it was not identifying PII that were in non-english since the automated scanner checks the variable name for known data types (e.g phone) but that would only work for English sites. reply xyzeva 20 hours agoparentprevWe confirmed that the gambling site is not fake data, I dont know about the lead one. Why we are saying its more is there is likely other services not in our scan list that could be vulnerable. reply tamimio 18 hours agoprevThat customer support looked like an automated AI response.. But I’m not surprised of the scale, years ago same thing happened with AWS cloud XY service, and you would find the token literally in plaintext in millions of smartphones apps. reply ddtaylor 21 hours agoprevGreat work and an awesome write up. reply dazh 20 hours agoprevWhat would be the correct way to set up security to prevent this? reply xyzeva 20 hours agoparentSetting up firebase security rules: https://firebase.google.com/docs/rules/ reply lawgimenez 21 hours agoprevCan you migrate data from Firebase database to PostgreSQL or similar? reply mooreds 21 hours agoparentDefinitely. They even let you export the password hashes (which you should do carefully). You can then import them into any identity provider that supports modified scrypt[0]. Your users will continue to be able to log in without a password reset. 0: https://firebase.google.com/docs/reference/admin/java/refere... reply xyzeva 21 hours agoparentprevOn certain databases, yes We only scanned for firestore, which is a NoSQL database, conversion tools may still be possible, a good firebase alternative would be https://supabase.com, but please set up RLS, its IMO much easier then Firebase. reply necovek 21 hours agorootparentWouldn't it be easy to migrate a NoSQL database to Postgres without any adaptation? Postgres is an \"object database\", so you could use Array, JSON or JSONB fields wherever necessary, and you shouldn't introduce any foreign key relations or such. reply xyzeva 21 hours agorootparentIt would, except if you have a f'ed up schema like most of these companies had. reply marcosdumay 20 hours agorootparentThe point is that Postgres has no problem at all with a fucked-up schema. You just tell it not to check a thing, and it won't check a thing. But then, I may be underestimating how fucked-up things are. reply robmccoll 21 hours agoparentprevThe closest setup off the top of my head might be something like meteor.js on top of mongo DB. reply xyzeva 20 hours agorootparentWould work! If you're willing to write something, go for it. I'm personally way too exhausted right now. reply andersa 17 hours agoprevI've never really understood how Firebase makes any sense. Why would you let the frontend access the database directly? reply bruce343434 13 hours agoparentBecause writing the boilerplate that hooks up the DB to an ORM Model to a ViewModel to a Router and then back via a Controller to the Model to the DB again sucks? A lot of the times it's equivalent to writing manual getters and setters except it's many many lines of code over many files... No wonder people are trying to cut corners! reply hubraumhugo 19 hours agoprevHas Supabase learned from this and done a better job? reply pizzafeelsright 21 hours agoprevGo for the win. And we're still in the Wild West when it comes to internet business even after 20 years of \"verified\" domains. reply cozzyd 18 hours agoprevSomeone should develop a browser plugin to warn you if a site is using firebase... reply joshxyz 21 hours agoprevdoing gods work here man, thank you for your work! reply MrBruh 21 hours agoparentNo worries, support like this really helps to make it feel worthwhile. :) reply balder1991 21 hours agoprevLaughed out loud with the “Customer support tried to flirt with me when attempting to report the issue”, “I want to be your gf, you very smart” lol the print looks like Kik Messenger? reply MrBruh 21 hours agoparentGlad to hear you enjoyed it, it was a messenger called \"Line\" reply vishnugupta 21 hours agorootparentAh nice! This brings back memories. I think this is a very popular messenger app in SEA region? reply phantomathkg 21 hours agorootparentTaiwan, Japan and Hong Kong mostly. SEA used a mix of messaging app, where Whatsapp > Line. reply xyzeva 19 hours agorootparentWe believe the gambling ring is based in Indonesia, which is uncommon to use Line, but they seem to be using it here for all of their customer support across all sites. reply robjan 19 hours agorootparentprevNot really used in Hong Kong that much anymore apart from a small subset of people (WhatsApp is king). It's also heavily used in Thailand. reply kijin 21 hours agorootparentprevThey must have trained their customer support AI with a database stolen from dating sites. reply MrBruh 21 hours agorootparentCan't blame a company for trying to make their AI charismatic :D reply pksebben 19 hours agorootparentprevI feel like we need more followup, so come on, dish! Where are you taking her for that first date? reply denysvitali 19 hours agoparentprevThis is the best reply to a vulnerability disclosure I have ever seen so far. Wow. \"Your firebase allows anyone to read information\" \"i want to be your gf\" reply golergka 19 hours agoparentprevTBH, that's still better than most of my interactions with customer support when I tried to report bugs. In most cases, they just don't have a script for this and have no idea what to do about it, even if I manage to convince them it's the app that's wrong, not me. reply dtx1 20 hours agoparentprevyeah that must be some weird ai thats misused here reply xyzeva 19 hours agorootparentMust've used the twitch chat dataset reply WarOnPrivacy 20 hours agoparentprevI'm either very old or very western. What is duckling with bat communicating here? edit: oh wait. Did it hit itself with the bat? reply komali2 20 hours agorootparentIt has an \"angry vein\" on its head, it's threatening (someone) with the bat. I have no idea what they're trying to communicate lol. In my experience Line stickers are often used the same way \"huh\" is, when you don't know wtf else to say but you'd like to terminate the conversation somehow. reply bluelightning2k 22 hours agoprevnext [2 more] [flagged] xyzeva 19 hours agoparentThank you! Means a lot, helps us keep going. reply zelon88 20 hours agoprev [–] biggest threat on the web is Google. They lower the bar so low that people who have no business collecting user information are collecting user information, then they host it insecurely for you with no liability to the end user whatsoever. Not only that but they provide the same crappy services to schools and scummy gambling websites alike. It frustrates me watching people who believe they are professionals flock to these services. Honestly, if you can't roll your own you probably shouldn't let someone roll this for you. But Google won't say no and none of you cloud devs can help yourself. So we have this race to the bottom in cost and first to market and all the products are least common denominator shit that gets built in 6 hours by copy pasting as many GH repositories together as possible on rented infra. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Firebase misconfigurations led to the exposure of 125 million user records, exposing sensitive data like passwords and billing details.",
      "Manual review of 550k entries transitioned to automated scanning using a tool named Catalyst to identify the data stored in a Supabase database.",
      "Despite the report, only 24% of affected site owners rectified the misconfigurations, with a minimal 0.2% offering bug bounties."
    ],
    "commentSummary": [
      "The discussion explores security vulnerabilities in Firebase, emphasizing complex security rules and potential platform misuse.",
      "Participants debate issues such as storing large binary data, memory limits, and the importance of request size restrictions.",
      "Concerns include setting authorization rules, customer security, privacy, and critiques about Google's support and cost prioritization in cloud development."
    ],
    "points": 550,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1710759690
  },
  {
    "id": 39742578,
    "title": "Misconception of EU Cookie Banner Law",
    "originLink": "https://www.amazingcto.com/cookie-banners-are-not-needed/",
    "originBody": "CTO Coaching Get in touch ↗ Articles Dear Paul Graham, there is no cookie banner law Trust is Not a One-Way Street Procrastination Does Not Exist Let New Hires Write A Todo App Learn from Success Not From Failure How To Succeed With A Rewrite I love Unsubscribes ✨ Min vs Max Problem Solving Books on HackerNews for CTOs - Reviewing 2023 The Luck Formula Development Speed: From Idea to Release in One Day Keyboard with Display for Developers - Kwumsy K3 Too Many Developers Get Refactoring Wrong Tests Are Bad For Developers Three Fundamentals of Software Estimation Hosting a Website on BunnyCDN - Radical Simplicity Our Fetish with Failover and Redundancy The 5 Reasons Not to Use Scrum Best Books for a CTO Musings about error handling mechanisms in programming languages ManyCam dishonored my lifetime license What Startups and Managers can learn from Brexit About Pay Rises And Customers CTO vs CEO - how cooperation can work Remote Work and Fair Developer Salaries How many developers do you need? How to Outsource Development Successfully Just Use Postgres for Everything Scrum is no longer fit with remote work Selfhealing Code for Startup CTOs and Solo Founders 🦹 We see the AI Endgame for Software Engineering Comparing SQL, SQL JSON, ORM and GraphQL performance in Golang Startup CEOs learned Engineering Management from Captain Kirk CTO vs VP of Engineering ⌚ Why Everything Takes Longer and Longer in Growing Startups Why we always end up with waterfall 😊 One Thing A Day For Happiness Automatic Management to Save Time The AI Manager - The End of Programming The Mysterious Case of Lost Developer Productivity The ZigZag Model of Vision and Strategy 💌 Newsletter for CTOs CTO Coaching 💌 Newsletter for CTOs Dear Paul Graham, there is no cookie banner law On why you keep seeing these cookie banners I’m not a lawyer and this is not legal advice. Ask your data protection specialist. https://twitter.com/paulg/status/1769663627739443661 Recently, Paul Graham came up with the thought, that the EU forces companies to have cookie banners. There is no law for cookie banners. Indeed, as an American, there is no need to force them onto you. But perhaps of bad IT departments or bad outsourcing, or “Look, Why take a chance?” (Remo Gaggi), Americans see them too. What the EU is saying, you need my consent when you want to track me, profile me and sell my behavior off to ad companies. Companies could easily avoid any cookie banner. Just don’t track. Or they could avoid banners for people who don’t want to be tracked, just listen to “Do Not Track” headers (it’s deprecated because companies did hate this). Browsers could have a tracking icon like that SSL icon years ago, with the SSL information, you click it, information pops up, and you give consent. Or cookie banners could be this small line at the top of a site, that you sometimes see for notifications like coupons, and which says “Give Cookie consent YESNO” (and open all details on YES). I would just ignore that small banner, no need to decline consent, I just don’t need to give it. Or they could put a small button at the bottom of the page to the right, like those support buttons, which says “I want to be tracked to support you”, and you click there, and it pops up all the ways the company wants to track you and sell your data. And you give consent. But of course, this is not what companies want. You don’t want to be tracked, but they want to track you. So they force half-page-size banners on you, grey out the content and prevent you reading their site, in the hope you say yes. Or you are worn down and no longer care after twenty banners and say yes. Or you misclick and say yes. Or you are confused and say accidentally yes. Or they nudge you with Dark UI Patterns into saying yes. “Companies are making your life hard by choice. They got told by the EU they could not be secret abusers anymore, so now they decided to be irritating on top.” https://www.bitecode.dev/p/there-is-no-eu-cookie-banner-law It’s like a child throwing a tantrum “But I want the toy!”. This is why we have cookie banners. The EU does not mandate cookie banners. Companies do. PS: Having written this, I don’t think EU regulation is a good thing per se, especially if we don’t know about things to be regulated, like the current AI regulation. But I think data privacy is a good thing, I fought for PGP 30 years ago and will keep doing so. CTO Newsletter Join more than 2500 CTOs and Engineering Managers Get Weekly Insights By signing up I agree to receive your newsletters and accept the terms and conditions and data protection More Stuff from Stephan The Luck Formula The Mysterious Case of Lost Developer Productivity The AI Manager - The End of Programming How To Succeed With A Rewrite 😊 One Thing A Day For Happiness Trust is Not a One-Way Street Other interesting articles for CTOs • Best books for CTO • The CTO Book • Experienced CTO Coach • CTO Coaching • CTO Mentor • CTO Mentoring • CTO Newsletter • How many developers do you need? • Postgres for Everything Product Roadmaps for CTOs • How to become a CTO in a company - a career path Books By Stephan Amazing CTO Book Other Articles Three Fundamentals of Software Estimation CTO vs CEO - how cooperation can work ⌚ Why Everything Takes Longer and Longer in Growing Startups Startup CEOs learned Engineering Management from Captain Kirk Just Use Postgres for Everything Other Articles Three Fundamentals of Software Estimation Keyboard with Display for Developers - Kwumsy K3 How many developers do you need? How to Outsource Development Successfully Best Books for a CTO",
    "commentLink": "https://news.ycombinator.com/item?id=39742578",
    "commentBody": "Dear Paul Graham, there is no cookie banner law (amazingcto.com)547 points by KingOfCoders 22 hours agohidepastfavorite574 comments nolok 22 hours agoImagine a market in which companies charge a lot of hidden fees behind their customers' back, and users are not happy when they realize after the fact. The law is updated to say you are not allowed to charge the user a fee unless you tell him in advance. Companies with tons of hidden fees decide to keep them but force you to read all the fees on every page of the menu before you can see the rest of the text, in the most annoying way possible, and promote the idea that the issue is not the extravagant fees, nor the fact that the companies hide them before and had to be forced by law to warn you about them, no the problem is a law that force them to tell you what you're getting into before it's too late ! That's, essentially, what's happening. And we have people complain that companies need to display their fees. On this issue in the group that complain about the cookie law there are some people who are very wrong on purpose because it's in their interest, and some people who are very wrong because they genuinely don't understand the position they're defending, complaining about being made aware of the fee, instead of the fees themselves or the fact that the companies hide them if not forced by law. To each their own belief about which category PG fits into. reply CipherThrowaway 21 hours agoparentAgree. How much corporate propaganda are people consuming that legislators are seen as wholly responsible for the bad behavior and malicious compliance actions of corporations? What does it say about the relationship between businesses and consumers that the first response to this bad behavior is to shout \"look what you made them do!\" Seemingly it is everyone's fault except the bad actors themselves. reply ragnese 21 hours agorootparentIt's so depressing. Many of the people who are pointing the finger at the regulators for the annoying cookie banners don't actually see the web site/app *as* a bad actor. The fact that they had been tracking tons of extra data via cookies without their consent or knowledge was totally fine to them as long as it wasn't inconveniencing them in any way. The cookie banner is an inconvenience to their mindless consumption, so NOW it's a problem and they just don't care what the solution actually is as long as the thing goes away. I've seen this attitude from tech people, too, so it's not just a matter of tech ignorance or illiteracy. reply unmole 9 hours agorootparent> don't actually see the web site/app as a bad Some of these bad actors actors with annoying cookie banners: https://gdpr.eu/ https://european-union.europa.eu/ https://www.europarl.europa.eu/portal/en reply depressedpanda 8 minutes agorootparentI only got a cookie banner on the first of those links, and as far as cookie banners go it wasn't very annoying; I clicked \"no\" and it went away immediately. reply kortilla 20 hours agorootparentprev> The cookie banner is an inconvenience to their mindless consumption, It’s an inconvenience to people who care about privacy and use browser configurations that don’t store state between visits. So now in an attempt to protect regular users, the law ended up hurting users that already cared. Additionally, the shadiest and incompetent sites still just track people with no cookie banner. So the law doesn’t really provide protection against uncooperative parties, whereas privacy technology does. reply ragnese 20 hours agorootparent> It’s an inconvenience to people who care about privacy and use browser configurations that don’t store state between visits. > > So now in an attempt to protect regular users, the law ended up hurting users that already cared. Fair point about the banners mostly \"hurting\" users who care about privacy (but, really though- how much does it really \"hurt\" you? I'm \"hurt\" more by the fact that I have to fold laundry several days a week). But, I take major issue with you saying that the LAW ended up hurting users. Companies are under no legal obligation to make those banners as obnoxious as they are or with so many dark patterns (I sometimes don't know if I'm even enabling or disabling tracking with the way they word it). That's squarely on the web site owners pulling that nonsense. > Additionally, the shadiest and incompetent sites still just track people with no cookie banner. So the law doesn’t really provide protection against uncooperative parties, whereas privacy technology does. I agree that the only/best way to protect yourself is via technology and not by relying on people obeying the law. However, if this is also an argument against having the law, it's an incredibly weak one. You can apply that logic to argue that NO laws are effective. People still murder even though it's illegal- must be a bad law, no? reply nickpp 15 hours agorootparent> Companies are under no legal obligation to make those banners as obnoxious as they are Actually every single lawyer we asked about implementing GDPR advised us to have one of those obnoxious banners. Because the law is so ambiguous and the penalties so high that is better to play it safe. And we have no ads nor tracking at all on our product website. You can ignore your lawyer's advice if you want, but it's a bit like a lawyer office ignoring my data security and backup advice: assuming a huge amount of risk. reply account42 19 hours agorootparentprevThe GDRP is about all kinds of tracking, of which things you can block locally at the browser level is only one part. So yes, even those users that already cared enough to block/discard cookies benefit. reply webworker 18 hours agorootparentprevAgain, this should have been a >browser feature I call upon all German users of this website to write to their legislators! Obviously the German civil service is a bad actor! The German deep state must be shut down! I understand the joke you're trying to make but you clearly don't understand the relation between germans and privacy/tracking regulation to think this makes sense. reply username332211 20 hours agorootparentIt's not supposed to make sense, it's supposed to show the absurd position of the post I'm replying to. The less it makes sense, the better. And I only picked Germany, because it's one of the few EU countries where stuff like that is rigorously enforced. In the rest of the EU, everything unrelated to the common market and/or getting money from the EU is at best haphazardly enforced. If you want to, check out france.fr, a website maintained by an agency of the French tourism ministry. (After disabling the 3 dozens of annoyance blocking extensions everyone must use nowadays, of course.) What do you see? A giant cookie overlay. Égoutter le marais! reply musiciangames 19 hours agorootparentI get no overlay on france.fr reply username332211 19 hours agorootparentMe neither, unless I disable ad blockers and anti-annoyance extensions. reply astura 19 hours agorootparentprevShows for me - https://i.imgur.com/c8c3MDk.png reply account42 19 hours agorootparentprev> the German state railways No such thing anymore, unfortunately. reply lynx23 21 hours agorootparentprevIf it only were that simple. When the GDPR came out, a lot of confusion and misunderstanding ensued. Not only regarding the damn cookie banner. Even totally legitimate health-care providers started to collect signatures to be on the safe side. I still rememeber receiving a basic GDPR training where we were told that opt-out/signing is only necessary if the entity is planning to do weird stuff with your data. IOW, if someone wants you to sign, they plan a bad move. Then my bank wanted a signature. And a month later, one of my healthcare providers wanted a signature. After a chat with him, I learnt that his lawyer told him to collect the signatures just in case, and made him believe that if someone doesn't sign, that is a problem. So now we have this situation where providers were trained to play the GDPR in such a way that they will never have a problem, no matter what they actually do with the data. And consumers are pissed because they are made to sign things which essentially reduce their rights... And if someone (like me) thinks the EU did a half-assed job there, the downvotes rain in. reply HelloNurse 20 hours agorootparentIncompetent lawyers and managers did a half-assed job, and some exemplary fines will motivate them with respect to the other half. reply lynx23 20 hours agorootparentThat is so wonderfully naiv that I had to laugh out loud. The fairytale of the manager who suddenly is fined big-time for his/her decisions is just that, a fairytale to pacify critics. reply soco 20 hours agorootparentprevThe same people also complain they cannot use by default said websites unless they share all their personal data with them. Half-assed, indeed the measure is. But it also reflects the majority thinking, unfortunately. So unless there's some popular pressure to full-ass the measure, we will still have banners and misused personal data. reply ambichook 7 hours agorootparentprevwould an adjustment of GDPR that forces websites to respect DNT solve the problem? am i missing something obvious? (aside from the pain in the ass that is amending laws) /gen reply orwin 20 hours agorootparentprevI kinda hate saying this, but Microsoft (or at least github) got it right in a week. Some OSS publishers also got it right, like nexedi, and some i'm sightly upset with (gitlab) but it is true that for the commercial internet it seems to be invasive. I do not use the commercial internet much, and like any person with greasemonkey, i took a rainy afternoon to remove the most annoying banners (i think now i use a plugin that does it for me). reply lynx23 20 hours agorootparentThe fact that you have to use a plugin or other thecnical remedies to fix the cookie banner situation is all the proof we need to see that the EU totally fucked up. It is easy to declare that you just need to install this or that to get a obstruction-free internet again. But it is also very very elitist. Not even 1% of the population is truly capable of handling that. reply orwin 12 hours agorootparentI use a plugin to block ads. That it also block most cookie consent banner is a positive side effect. It does not block all of them. It doesn't block gitlab banner, or EU website banners, because i think it automagically block cross domain js injection, which, you know, is good because less attack surface, and every browser should do the same without plugin (CORS do not go far enough imho). reply wizzwizz4 20 hours agorootparentprev> they are made to sign things which essentially reduce their rights... But not as much as you might think. Consent under GDPR only applies to what you were informed of when you consented, and you're allowed to revoke consent (with prospective effect) at any time. reply lynx23 19 hours agorootparentYeah, but these are rather theoretical practicalities. In the majority of cases, consent is coaxed out of the consumer. If you show up for a MRI, and you get a piece of paper with the comment \"It is for data protection\", almost nobody has the time or nerve to actually read the text, and even less people have the inclination to decline to sign. After all, they (sometimes desperately) need the service. Let alone that the accompanying comment is deliberately phrased such that some people will believe they need to sign in order for their data to be protected. Dark patterns all over the place. My bank implemented the consent (for a while) as a reoccuring pop-up after login. Yes, you get the popup as long as you decline to sign it, over and over again. I think they gave up on that practice, and it was partly a dark pattern (IOW, there were two buttons to decline to sign, and one would result in the popup reoccuring). Examples are all over the place if you walk an EU country with open eyes. reply wizzwizz4 9 hours agorootparentUnder GDPR, your MRI example and your bank example do not qualify as consent. (For the MRI example, they might be able to claim basis b, but only if they're doing stuff that you could actually have requested.) reply throwawaysleep 21 hours agorootparentprevMany of us had no real problem with the ad-supported web in the first place. I was happy with the status quo. So yes, I do blame the government as I would be fine returning to the prior state. reply alistairSH 20 hours agorootparentA site can serve ads without tracking (and the banner) - the ads just couldn't be targeted at individuals. Instead they'd have to guess what ad was appropriate (\"Rolling Stone\" could serve everybody ads for Taylor Swift's latest album without a banner, etc). reply phkahler 19 hours agorootparent>> A site can serve ads without tracking (and the banner) - the ads just couldn't be targeted at individuals. The biggest problem with online advertising is not tracking users. It's a lack of trust between advertisers and pretty much everyone else. If you're going to pay for an ad, you want to be sure it was seen by a real person. I'm not sure that's the concern any more because click-through is more important than \"seeing\" an ad. Regardless, the goals are to make sure it's easy for a given advertiser to get on many web sites, easy for a site to get ads, and also possible to prevent fraud since there will obviously be multiple parties involved. I suspect tracking users was an offshoot of just verifying that users were real to prevent fraud in the ad world. Not saying any of it is OK, but it seems like the way to prevent tracking is to find a way to verify authenticity while also preserving privacy. reply actionfromafar 19 hours agorootparentEmbedded banner ads with a third party sampling the site to see that ads are fairly displayed according to paid quota. Maybe something like that? reply Cthulhu_ 19 hours agorootparentprev> (\"Rolling Stone\" could serve everybody ads for Taylor Swift's latest album without a banner, etc). And that would be fine, as long as Swift was willing to pay for it. But the tracking and personalized ads thing was a numbers game; personalized ads have a higher conversion rate, thus are more valuable, thus we need data to personalize ads. reply LunaSea 19 hours agorootparentprevThis means you get less money for it and can't survive due to the lesser revenue. reply actionfromafar 19 hours agorootparentThis is contrafactual. Many things survived on exact that model before hyper targetted ads. And besides, with targetted ads the middle men take most of the cut. reply Cthulhu_ 19 hours agorootparentprevThat was your choice in the end, but this was the problem - people didn't have the choice, or the awareness. The EU law fixed this, but instead of corporations going \"Hmm, maybe we shouldn't track users\", they instead went with malicious compliance and implemented annoyances - because data is more valuable for a lot of websites than whatever said website is peddling. reply MattHeard 21 hours agorootparentprev\"I would like website operators to assume that I consent to being tracked, so I'm annoyed that website operators are not allowed to assume that everybody consents to being tracked.\" reply Cthulhu_ 19 hours agorootparentprevApple is doing the same thing, passive-aggressively doing things like removing support for pinning webapps / PWAs / whatever they're called to your home screen, then backtracking after backlash. Or Microsoft with their browser choice screen or Windows releases without media player. And even those aren't as bad as the malicious compliance of cookie banners. reply raydev 19 hours agorootparentprev> How much corporate propaganda are people consuming that legislators are seen as wholly responsible for the bad behavior and malicious compliance actions of corporations Why do I need to be \"consuming corporate propaganda\" when I just hate that I need to dismiss banners on every news website, when I didn't have to before the regulation? I don't care about being tracked. But now that all websites need to cover their asses in response to regulation, I'm forced to figure out which button I need to click on to read content, and these websites don't even appear to save my preferences whether I agree to be tracked or not. Objectively, the outcome of this regulation is that my experience is worse. Are the companies bad actors? Sure! Sounds like the EU should account for companies' bad behavior instead of forcing the internet to be more annoying. reply Jensson 18 hours agorootparentGet a plugin to click that button for you, I got one and haven't seen such a banner in a really long time now. reply aurareturn 5 hours agorootparentAny iOS plugins that work? reply ImPostingOnHN 18 hours agorootparentprevThe experience you describe is the fault of websites which chose to make things that way. The article goes into more detail on this point: There Is No Cookie Banner Law. It's important to note that we didn't have to go through the banners after the law, either. We only had to go through them after website operators intentionally picked the most disruptive and annoying popup to serve us. We can blame them. They chose to add it when they could have legally not added anything at all. It's like the situation described here: https://news.ycombinator.com/item?id=39742766 reply raydev 18 hours agorootparentAgain, from the perspective of users, the experience got worse post-regulation. > The experience you describe is the fault of websites which chose to make things that way. I don't disagree. But they were less annoying before. So make them go back to being less annoying. reply rstuart4133 12 hours agorootparent> Again, from the perspective of users, the experience got worse post-regulation. Your right, the experience got worse. But the underlying point is there two ways this could have gone. The GDPR simply mandated that if companies track you they have to get your informed consent. So one way it could have gone is companies didn't track anonymous users. Notice this doesn't apply to non-anonymous users. By definition once you've logged in you've revealed who are and agreed to a far more onerous privacy statement. So one way companies could comply is just to make you log in to see some content (and track you that way), and not bug you otherwise. But they didn't go that way. They insist on tracking you regardless. Perhaps you don't agree, but I find this even more annoying because I install tracking blocking extensions and that breaks some sites. To me the world would have been a much better place if they had just gone along with the intent of the damned law and not tracked people who are try to remain anonymous. To be fair it's not so bad. Firefox dismisses the cookie banners for you [0], and I have extensions that block the worst of their effects. If you are using a browser from an ad company and are complaining about cookie banners (which almost to the man use a deceptive UI to encourage you to accept them all so the ads work better), then I don't have a lot of sympathy. Me rejecting as many cookies as I can then blocking their trackers the worst possible outcome for the web sites trying to garner some ad revenue of course, but shrug, the industry could have acted in good faith, and didn't. [0] https://community.mozilla.org/en/campaigns/firefox-cookie-ba... reply ImPostingOnHN 18 hours agorootparentprevAgain, from the perspective of users, the experience got worse only after websites decided for themselves to add annoying cookie banners. Not after the regulation. > make them go back to being less annoying That is a request between you and them (the websites), unless you're talking about legislating a banner-less opt-out, or maybe just willing to file a complaint against the website with a data protection authority, if the banner is already illegally annoying. Websites have the right to annoy their users with cookie popups, with or without the GDPR (ironically , the GDPR actually has some protections here, websites simply break the law). Unfortunately, it seems many are choosing to exercise that right because they make money doing so. reply soerxpso 10 hours agoparentprevThe flaw in your analogy is that the modal consumer cares about hidden fees, wants to be made aware of them, and might even make a different decision with that knowledge. The modal consumer does not care about cookies. Imagine you walk into a restaurant and they hand you a paper that details full allergy information for all of the foods they serve, and then they wait for you to say, \"I consent to these ingredients being in the food,\" before they can seat you. I think that's a closer analogy. We can all agree that the restaurant shouldn't hide that information from you, and that some minority of people might want the information, but do we really have to add this inconvenient step to the process for all people? The current real-world system, where allergy information is available upon request, was working fine. There are some things that everyone cares about and would be appalled by, that businesses should have to inform people about, and many things that a small minority of people care about. Why stop at cookies? Maybe we should mandate a popup if the website's server infrastructure was manufactured abroad, and another popup if the company that runs the website has higher than average carbon emissions, and another popup if the food in the food court that serves the headquarters of the company that runs the website is not kosher. The lobby of people who care about cookies is of similar size to the lobby of people who care deeply about binary size and about running JavaScript. Should there be mandatory popups to execute JavaScript? If the website is >10MB, should I have to consent on a lightweight page before downloading it? How do you determine which activities warrant a popup warning and which do not? reply AbrahamParangi 21 hours agoparentprevThis is a bad example because the market usually fixes this problem. The reason why the market doesn’t fix the cookie banner problem and the reason why this is bad law is because users defacto do not care, it is merely annoying. There’s a law in California that says that businesses which have chemicals that might cause cancer on the premises need to let people know. That’s great but the levels they set turn out to be lower than what you can feasibly test for and as a result all properties pretty much just put up the signs that say “there might be chemicals here”. The warning is useless and annoying because of market forces which is another way of saying the law incentivized the behavior that occurred. reply pornel 20 hours agorootparentThe market is working perfectly here, if you remember that users are not the customers. Users are the product sold to adtech, data brokers, law enforcement, etc. For data-harvesting companies users are like livestock, and nobody cares about livestock's opinion. It only matters how much value can be extracted from users, even if it's annoying, misleading, and relies on dark patterns. reply sanderjd 14 hours agorootparentYes, but that \"livestock\" can vote with their feet. If people cared about this, it would be a good opportunity for new entrants to take market share from incumbents by not using tracking cookies and thus not needing to have the cookie banners. But (to the parent comment's point) that isn't happening because this is not a compelling feature to offer, because people do not care about this, no matter how much we want them to. I used to think this was just an education issue, that people just didn't understand the implications of privacy concerns on the web. But I no longer think this is the case. I think people do mostly understand, and just do not consider this a priority. reply Aeolun 21 hours agoparentprevI don’t think this is strictly accurate. There’s nothing about cookies themselves that makes them a problem. It’s the way they are used. Needing to inform people you are using cookies for sessions is like needing to inform people you are using a fork to eat. The problem is that some people are using the fork to stab people, so now we require everyone to say how they’re going to use it in advance. Instead of just prohibiting stabbing people. reply nine_k 21 hours agorootparentA few places allow you to opt for a spoon instead, or drink right from the bowl without utensils. Note that it's not the customers who use the forks for stabbing; it's the restaurants themselves. To show their goodwill to a customer who does not trust them with a fork, they can offer a spoon. The further we take this analogy, the more strained it becomes. Yes, it's natural to use a cookie to track a session; this is a mechanism invented for that purpose. It's much less natural to share this tracking information with third parties, especially along with a record of your purchases or other interesting actions. But ad revenue is much harder to obtain without targeting and thus tracking. And a lot of places depend mostly on ad revenue. This is another case of \"buy now, pay later\" pattern, stretched to \"take for free now, pay in loss of your privacy later\". In a funny enough way, many people don't value the information they get on many ad-supported sites as highly as the marketers paying to grab their attention, so simply compensating by adding a subscription or one-time payment to go ad-free sometimes does not even work; the more generic / \"doom-scrollalbe\" the content is, the worse it works. reply proto-n 21 hours agorootparentprevSee for example GitHub's statement [1] about no longer displaying a cookie banner. While ironically the blog still does display them, the main site doesn't. [1] https://github.blog/2020-12-17-no-cookie-for-you/ reply bazoom42 21 hours agorootparentprevYou don’t need to inform people you are using cookies. It is not about cookies. reply vaylian 20 hours agorootparentAs long as those cookies are only used for making the core functionality of the website work (i.e. login sessions, user preferences) reply nickpp 15 hours agorootparentprev> You don’t need to inform people you are using cookies. Are you a lawyer? Are you willing to assume the liability I may incur if I follow your advice? reply pas 11 hours agorootparent\"\"\" Cookies that do not require consent [...] or authentication cookies (when users authenticate themselves on your web site to log in in order to check online services such as their bank account). \"\"\" https://europa.eu/youreurope/business/dealing-with-customers... reply nickpp 10 hours agorootparentAgain: are you a lawyer?! If not, in what quality are you advising businesses how to implement such a dangerous law? Have you seen the magnitude of the potential punishment? Will you cover my fines if you're wrong? reply bazoom42 3 hours agorootparentNote that putting a “we use cookies” banner on your website will not absolve you from GDPR fines anyway. You still need to adher to the law about informend consent, storing safely etc. If you are worried about GDPR, by far the safest is to just not collect personal information. reply nolok 21 hours agorootparentprevYou don't need a cookie banner for session cookie, not in eprivacy nor in gdpr, same applies for all cookies that are \"strictly necessary\" for the functionnal operation of the website on the technical level. Language selection cookie, \"remember me\" cookie, etc ... Are all perfectly fine. reply greggsy 21 hours agorootparentI’ve often wondered if necessary cookies could just be carved out and designed (and named) differently to improve handling. You could then just configure your browser to inherently accept the benignfrom a site, which would then only ask for non-essential ones. The real nirvana, IMO, would be better sandboxing between sites. reply nolok 21 hours agorootparentBrowser based solution not mandated by law but made by the industry wouldn't work, because all 4 major browser vendor makes significant revenues from Ads. At a time a solution appeared with \"do not track\", and we ended up with the industry making sure it was as toothless as possible, opt-in, and google pushing hard to control the browser market. reply aurareturn 22 hours agoparentprevUsing your analogy, I think what ends up happening is that even companies that don't collect hidden fees will put up a banner just in case. Not only that, I'm not an EU citizen and I'm not browsing websites based in EU but I'm still bombarded with cookie banners non-stop. reply gizmo 21 hours agorootparentDo you have /any/ examples of websites that don't have a bunch of 3rd party cookies that still have a cookie banner? Middle managers absolutely love anything with charts and graphs because it makes their decisions feel more scientific. That's why they want tracking software included on their websites. And if the law requires disclosure then a cookie popup is the solution. reply maccard 21 hours agorootparentMy company recently announced a game, and we launched a website for the game. There's no ̶t̶h̶i̶r̶d̶ ̶p̶a̶r̶t̶y̶ e:tracking cookies (I didn't make the site, but I do run it). Our US based legal team told us we needed a cookie banner if we were going to have visitors in the EU. I pushed back, but I lost, and ultimately it's not my fight. reply account42 19 hours agorootparentSounds like your US legal team is covering their asses on topics they are not familiar with instead of acquiring the neccessary competences. reply qarl 20 hours agorootparentprevYour legal team is holding the door open for the day they decide to start tracking. They probably won't tell you that, tho. reply maccard 19 hours agorootparentOur legal team is following the checklist that they have that they know is pre-approved reply qarl 19 hours agorootparentOK? Does that contradict what I said? reply p_l 16 hours agorootparentprevWhich was probably written (even if not by the legal team, but someone they consulted) with an eye towards keeping more data than legitimate interest allows under GDPR. reply almostnormal 21 hours agorootparentprevIt is not about third party or not, but what it is used for. Consent may be required even if there are no cookies at all. reply maccard 20 hours agorootparent> It is not about third party or not you're right, I said third party, but I actually meant tracking. I actually went and checked, and our only cookie is the cookie for if you've seen the cookie banner or not... > Consent may be required even if there are no cookies at all. For what? reply akvadrako 20 hours agorootparentIt's not about cookies. Tracking without cookies also requires consent. reply maccard 20 hours agorootparentSee my original post. Our US legal team said that we need the banner if we have visitors from the EU, not if we're tracking them. reply astura 20 hours agorootparent>Our US legal team said that we need the banner if we have visitors from the EU, not if we're tracking them. This actually makes sense - because if you didn't have the cookie banner then some fucking weirdo would come to Hacker News and make a self righteous post about how you're \"tracking residents of the EU without their consent and abusing them\" (even though you're not). Instant karma. Next thing you know these weirdos and their mob are reporting you to their government and you're dealing with government inquiries and more legal expenses trying to prove your cookie-less web 1.0 site doesn't \"abuse people.\" The banner placates them. reply bmicraft 14 hours agorootparentDo you have any basis at all for such an absurd claim? The law actually works in the opposite direction (kinda): You may use \"legitimate interest\" cookies/tracking without saying so, but as soon as you show a privacy dialog you actually have to disclose everything you're doing including legitimate interest. Basically by having a list of what youre're doing with your user's data you're giving up your right to do anything not listed. reply paulmd 14 hours agorootparentprev> For what? GDPR actually doesn't specifically mention cookies at all. Tracking is what's illegal, not cookies. Let's say you keep website logs with IPs on them, and you do analytics for non-essential purposes. You can do this under GDPR, but you must gain consent from the user before logging this PII. It actually is completely and totally orthogonal to cookies. Some cookies are fine without consent. Some things that are not cookies are illegal without consent. reply raverbashing 21 hours agorootparentprevThanks for this, it seems a lot of cookie popups are there just due to cargo culting reply maccard 20 hours agorootparentI don't quite think Cargo Culting is the right label for it. It's not just because everyone's doing it. My experience when legal meets code is that common sense, intent and what is actually allowed go out the window, and cover-your-ass wins. My experience with Legal has been that they default to no \"just in case\" for every question you come to them with. It's a battle to get them onboard to not taking the safest possible approach, so you only want to fight that battle when it's a kingmaker of an opportunity. reply rcxdude 20 hours agorootparentYeah, people often approach legal in the wrong way: people often want to ask \"is this OK?\" and have the lawyers say \"yes\", but basically no lawyer is going to say that for almost anything. Instead you need to ask them to explain what the risks of different courses of action are and take a view as to whether they are important or not. reply maccard 20 hours agorootparentThat's been my experience, but unfortunately _that's_ where cargo culting comes in. As part of $NEW_WEBSITE_CHECKLIST we have to \"check with legal\" which inevitably involves a laundry list of stuff like this, and the default is to accept what legal says, unless we _really_ don't like the answer at which point we're going to do it anyway... reply ryandrake 19 hours agorootparentLegal counsel is there to advise, not to design product UX. Some companies have bonehead policies like “you must develop whatever Legal advises” but that’s a choice the company is making. Sensible companies treat their in house counsel as advisory, and weigh the risks like they would weigh any other risks. reply p_l 16 hours agorootparentprevThe funny thing is that most of the CYA cookie banners... are in themselves GDPR violations reply PennRobotics 21 hours agorootparentprevIt only took two minutes to find at https://www.schwarzkuenstler.com/ and I'm sure I can find a dozen more in half an hour. Germany is a bit litigious w.r.t. internet or privacy, so the combination---cookie consent---is a doozy. Nearly every German website that does anything will have a consent notification, and the slightest misstep (e.g. using Google Fonts without asking permission) can be punishable. reply gizmo 21 hours agorootparentTheir privacy policy states they use Google reCAPTCHA, which requires disclosure. reply rcxdude 20 hours agorootparentprevI think a heck of a lot of smaller sites just cargo-cult the pop-up. Either because they misunderstand the law or because of overly cautious lawyers. reply account42 19 hours agorootparentOr because of FUD from people interested in undermining privacy protections. reply jlokier 20 hours agorootparentprevAggregate data is not considered personal data by the GDPR. Managers and everyone else can have charts and graphs without retaining personal data. The processing of personal data prior to anonymisation to turn it into aggregate data, that part needs protection. But you can do it in a variety of ways that don't require personally invasive tracking. reply nolok 21 hours agorootparentprevFor your first point I disagree, my companies don't track and we don't have banner cookies. On your second point, that is again a choice of said companies, not a problem with the law. The GDPR has proven very well that if they cared, they can segment who is affected or not, and not just big tech lots of random local news site and the likes are doing it just fine. So again, you're aiming at the wrong culprit. reply piva00 21 hours agorootparentprev> Not only that, I'm not an EU citizen and I'm not browsing websites based in EU but I'm still bombarded with cookie banners non-stop. Again, that's the fault of the companies putting those up, they could make it opt-in to collect your data, they could just put a small notice on the footer with 2 simples links \"Accept all/Reject all\". But they chose, they decided to pester you with those banners as annoyingly as possible to make you have exactly the reaction you're having. reply aurareturn 21 hours agorootparentThe fact that companies are doing that says more about the bad law than the companies which is exactly Paul Graham's point. reply toyg 21 hours agorootparentSo the problem is that the legislator did not expect companies to be even worse assholes than they already were...? Laws are not borne in a perfect state; very much like programs, sometimes you need a few versions to see how the system actually works in practice and fix a few bugs. The fact that v1.0 has such bugs is not a good reason to just give up, nor it's an indication that the programmer is bad at programming. reply nickpp 15 hours agorootparent> companies to be even worse assholes All companies? Every single company with a website even if without any trackers or ads?! All companies are evil and the single law that triggered their evil behaviour is good. Sure. Ever heard of Occam's razor? reply toyg 12 hours agorootparentCompanies who don't track don't need any banner or popup. > All companies are evil No, but many, many companies are sociopathic assholes that exist just to make a buck. Otherwise we wouldn't need these laws. reply nickpp 10 hours agorootparentAll companies exist just to make a buck. And in the process they serve us with literally every single product and service we are using every second of every day. They play by the rules we put on them, making their life easier or harder, and in the process making our life easier or harder. Like this bad cookie law. reply layer8 21 hours agorootparentprevThe law is pretty crystal clear. In many cases the issue is that websites are outsourcing their tracking to ad companies, which in turn apply those banners indiscriminately because that's in their interest. That being said, all the dark-pattern banners actually break the law. The problem, if anything, is lack of enforcement of the law. reply aurareturn 6 hours agorootparentThe law is pretty crystal clear. In many cases the issue is that websites are outsourcing their tracking to ad companies, which in turn apply those banners indiscriminately because that's in their interest. You think a small company should roll their own tracking software? A mom and pop website that wants to track its conversion rate on its little eCommerce site? Come on. Be realistic. reply rvense 20 hours agorootparentprevIt is the companies that suck, and Paul Graham is (quite literally) invested in the suckage, wherefore this dumb tweet. Which, if one wanted to create an ad campaign for that eternal Upton Sinclair quote, couldn't have been done much better. (Thanks for the site though, Paul) reply kortilla 20 hours agorootparentPaul is very unlikely to be invested in tracking unless he has some shares in Google/Facebook. Startups in tracking aren’t really a thing reply toyg 18 hours agorootparentI expect most startups \"integrate\" their regular revenues (if they have any) with some sort of adtech deal. reply nickpp 15 hours agorootparentAny source for those allegations or is it only your imagination? reply paulryanrogers 21 hours agorootparentprevOr it says more about the manipulative intentions of the companies than anything about a good law. reply piva00 21 hours agorootparentprevWhat exactly is bad about the law that allows companies to do the annoying cookie banner? reply bananapub 21 hours agorootparentprevwhat a ridiculous point of view. do you think the same thing about laws against murder? about fraud? reply card_zero 21 hours agorootparentI've got to admit, I'm unclear what the equivalent of a cookie banner for murder would be. This criminal uses murder! If you continue to interact, you consent to being murdered. Murders you anyway reply paulmd 14 hours agorootparentprev\"this is what you get for trying to handle us with kid gloves\" is an understandable reaction/blowback from advertisers but I don't think it's something we should be giving any real weight or merit. it's also generally an indictment of the modern neoliberal regulatory approach in general. asking people nicely to follow train safety regulations etc isn't going to get you results. even fines/penalties largely end up just as cost-of-doing-business (even in the EU). if you really want behavior to go away, make it illegal and give people at the top Sarbanes-Oxley-style legal culpability if it happens on their watch. again, if you want to know the right way to set incentives so that people don't do a thing, you need only look at the way rich people want their money handled. you can bet that ripping off rich people is an ultra-mega-crime and doesn't just get a 1%-of-the-takings slap on the wrist. And lo and behold SOX does actually hold important people accountable as a result, not just some fall guy at the bottom. reply kibwen 21 hours agorootparentprevnext [2 more] [flagged] throw10920 20 hours agorootparent> Please stop defending the behavior of shitty companies. At the very least, I hope you're getting paid for these comments. Please don't do this on HN. reply pokot0 21 hours agorootparentprevJust been in Europe last week (I live in US): you have no idea what a nightmare internet is in Europe. You are only seeing a side effect here. reply denton-scratch 21 hours agorootparent> what a nightmare internet is in Europe I live in Europe; I don't experience this \"nightmare\". Would you care to expand? reply pokot0 20 hours agorootparentSure. The nightmare is that every single time you open the browser on a website you have to go through the data tracking preference for that website. It's a lot of work to avoid being tracked (companies are obviously using dark patterns there) and when you do it 20 times a day it gets frustrating quickly and collectively a big waste of human time. Now I am not saying the US doesn't have a problem. They just don't have GDPR and most website don't ask you for any permission to track you. So the experience is generally smoother (with the occasional tracking popup). Ideally there should be a way for me to broadcast my willingness to share my data and not allow dark patterns to try to change my opinion. But the GDPR does not cover that and allows websites to drive you crazy until you click \"YES, Track me\" reply tremon 20 hours agorootparentI think your problem is that you're accessing US websites from Europe, since those are what you know. European websites are a lot less annoying, they actually care about the customer base here. reply rstuart4133 11 hours agorootparentprevIf you are using a browser provided by an ad company surely being nagged to death to provide data they can sell is the expected outcome? You could use Firefox, which can disable most cookie banners [0]. [0] https://community.mozilla.org/en/campaigns/firefox-cookie-ba... reply denton-scratch 18 hours agorootparentprev> every single time you open the browser on a website you have to go through the data tracking preference for that website This is not my experience. Perhaps the websites you favour are exceptionally abusive. > a way for me to broadcast my willingness to share my data That's the opposite of what most people want to broadcast. > But the GDPR does not cover that and allows websites to drive you crazy Apparently your view is that GDPR should not allow that, i.e. it isn't strict enough. I'm inclined to agree. reply yau8edq12i 21 hours agorootparentprevAs someone who's lived in both the US and Europe during the past few years... GP is full of shit. reply diordiderot 20 hours agorootparentprevIt's really not much different. Source: Living in Europe reply overstay8930 21 hours agorootparentprevIt's crazy how censored the internet is too, you need a VPN to access even piracy adjacent sites in Germany. Unheard of that an ISP would block a website in the US without the FBI itself taking it down. reply MaKey 21 hours agorootparentYou don't need a VPN, just a different DNS server. reply onel 17 hours agoparentprevI think this is a good analogy and I agree that the intent of the law was not to force websites to have a cookie banner, it was just the side effect. What I think we are missing is a browser option/API that lets the user choose the acceptable tracking level. Similar to the do not track header but more fine grained. As we are missing that, extensions are doing a good job ATM https://chromewebstore.google.com/detail/consent-o-matic/mdj... https://addons.mozilla.org/ro/firefox/addon/consent-o-matic/ I found pretty late about Consent-o matic and it saved me a ton of time handling banners. It's exactly what we should have built-in the browser. reply bmicraft 14 hours agorootparentWe have that, it's called DNT. Unfortunately companies argue that: - it's not really a user choice when some browsers set it by default and therefore ignore it - it's set globally for a browser but a user might want to give away their privacy to my specific site ... and show the banner anyway reply rglullis 19 hours agoparentprev> The law is updated to say you are not allowed to charge the user a fee unless you tell him in advance. Why not a real regulation then to get rid of hidden fees and heavy fines/jail time for companies that are found to be doing it? PG's argument (I hope) is that there is no point in talking about \"regulation\" and \"customer protection\" if companies STILL get away with their ridiculous and hostile practices. There is no customer benefit in having user data collection and tracking. Companies do it only to exploit you. Even the usual BS excuses (\"oh, we need user data to customize the experience\") could be done completely in-device. I don't want regulatory bodies to just give more hoops for other companies to jump. They will jump it anyway, because it is profitable to do so. What I want is for regulatory bodies to effectively stop predatory practices. reply danaris 19 hours agorootparentI mean, that would be great, but I suspect that even just here on HN you'd get a lot of people strongly disagreeing with you. Because that would infringe upon the companies' \"freedom\" to profit in whatever way they see fit—and the people's \"freedom\" to let their data be vacuumed up and sold for massive profits. reply rglullis 18 hours agorootparentWhether they agree or not is irrelevant. I think that PG's argument is that all the \"regulation\" and \"strength of the EU\" amounts to nothing. It's just people pretending to play power games, doing privacy theater and solving absolutely zero problems. reply paulmd 14 hours agorootparenthave him register a new, anonymous HN account and go say that in one of our weekly \"apple bad\" threads reply rglullis 13 hours agorootparentTwo wrongs don't make a right. Apple is bad and the EU bureaucracy is ineffective. reply whazor 20 hours agoparentprevAirbnb used to hide their total price until EU started requiring them to do so in 2019, whereas USA only had this requirement from December 2022. reply clktmr 21 hours agoparentprevI don't want to be tracked either. But if companies can play the law this easily, I think it's a pretty bad law. reply ragnese 21 hours agorootparentAre we all such spoiled brats that some cookie banners interrupting our web browsing is all it takes for us to give up and call the malicious companies the winners and the law(s) trying to protect our privacy \"bad\"? We're a pathetic lot. reply miracle2k 22 hours agoparentprev> On this issue in the group that complain about the cookie law there are some people who are very wrong on purpose because it's in their interest, and some people who are very wrong because they genuinely don't understand the position they're defending, complaining about being made aware of the fee, instead of the fees themselves or the fact that the companies hide them if not forced by law. The reality is that I (and others who are complaining, as well as many who have resigned themselves to their fate) are happy to have a website \"track me\", certainly if the cost of non-tracking are having to click away an annoying popup, and think that people who compare a website wanting to know the number of their visitors to \"hidden fees\" are kind of being ridiculous. reply isodev 21 hours agorootparent\"Number of visitors\" does not constitute tracking. The tracking in question here is to discover who you are specifically and the absurd amount of detail about your online activities collected and shared with data brokers for aggregation and resale. A few of these cookie prompts during the day and they'd be able to tell everything from where your kids go to school to the kind of prn you prefer to watch on weekdays and everything in between. reply rnts08 21 hours agorootparentI used to work at an online video advertisement company, you'd be horrified how much information we tracked across all the ads, especially since the ad was played with a special media player \"plugin\" loaded inside the other media player. This is how ad companies can sell premium views, don't show cosmetics to men, increase car related ads to people who has watched other car related ads and so on. There's no such thing as server-side \"private browsing\". reply tremon 20 hours agorootparent> This is how ad companies can sell premium views, don't show cosmetics to men, increase car related ads to people who has watched other car related ads and so on. It's really not. They already could do all that before cyberstalking was normalized. It's called content-based profiling, and it doesn't require any GDPR consent. reply p_l 16 hours agorootparentThe ad companies wanted to aggregate information across multiple channels. The example about \"show more car ads to someone who watched other car ads\"? It's not about showing car ad on a site whose content is about cars (or where the site owner decided they like that kind of thing). It's about knowing you have wandered over to car comparison site recently so they can show you car advertisements when you look up sports news, show car-related merchandise when you're browsing some shopping site, show you insurance ads, etc. reply joenot443 21 hours agorootparentprev> where your kids go to school Is this something that's kept secret in European society? If someone told me they knew where my kids went to school I wouldn't be surprised, it's sort of dependent on our address which is in the phone book. reply yard2010 21 hours agorootparentprevHonestly I don't mind them collecting this data, what is really infuriating is the fact they won't share it with me. I would love to know what kind of porn I prefer on weekdays. I think they shouldn't be allowed to track anything with consent or without it unless they share all the data with the subject of spying. And aside from that, I think it should be much more expensive to say sorry than ask for permission. In my world a firm like facebook should not have any right to exist, they earned it. Fine them to oblivion just like I would get a long time behind bars if I wouldn't do my taxes right. reply ragnese 21 hours agorootparentI call BS. Give me your email password and your browser history and I'll share everything I learn about you with you. I'll also keep it and share it with whomever else I want to, but I'll definitely share it with you, too. reply catapart 21 hours agorootparentprevThis is addressed in the article. They could track you, with your consent, in many different ways. The fact that they are choosing to force this cost upon you is what is ridiculous. reply pera 21 hours agorootparentprevThe reality is that most people don't want to be tracked: https://arstechnica.com/tech-policy/2021/07/facebook-adverti... reply madmoose 21 hours agorootparentI've stopped going to Ars Technica exactly because their cookie pop-up lets me know that Condé Nast wants to share my data with at least (according to the popup) 159 partners. They have so many \"partners\" that their cookie popup comes with a search bar. 56 of their \"partners\" want my precise geolocation data! 16 \"partners\" want to actively scan my device! 101 \"partners\" want to \"match and combine data from other data sources\" (I can't disable or object to this) 102 \"partners\" want to identify my device. I also can't object to this. The only way I can really object is to close the tab, so that's what I do. reply stefanka 20 hours agorootparent> The only way I can really object is to close the tab, so that's what I do. Isn't it too late by then? reply Jensson 18 hours agorootparentLegally no, they can't store his data if he doesn't click yes. reply p_l 16 hours agorootparentConsidering their consent banner isn't legal under GDPR anyway, I'd be wary of expecting them to be compliant with that either. reply caskstrength 20 hours agorootparentprevThe problem is that most people don't want to pay for any of the internet services they use either. reply account42 18 hours agorootparentAny internet services that are unable to secure funding without abusing their users are welcome to stop existing. reply dwaltrip 20 hours agorootparentprevGreat, then maybe we can all finally go outside and smell the damn roses. reply geysersam 21 hours agorootparentprev> The reality is that I (and others who are complaining, as well as many who have resigned themselves to their fate) are happy to have a website \"track me\", certainly if the cost of non-tracking are having to click away an annoying popup, and think that people who compare a website wanting to know the number of their visitors to \"hidden fees\" are kind of being ridiculous. I agree that wanting to know the number of visitors is benign and it is not abuse. But saying companies should be allowed to track me (for whatever purpose) across the web without my consent is also pretty ridiculous. reply paulryanrogers 21 hours agorootparentprevDoes it become less ridiculous when your browsing history is sold to insurers, who use it to raise your rates. reply suslik 21 hours agorootparentprevWell, different people want different things - I'd rather spend a millisecond to click 'refuse' rather than let them track me - out of spite if nothing else. Yes, cookie banners are annoying; the dark patterns within cookie banners (you need multiple clicks to get to the 'refuse' button while the 'accept' button is right there in your face) are even more so. But honestly - screw them. reply nolok 21 hours agorootparentprev> The reality is that I (and others who are complaining, as well as many who have resigned themselves to their fate) are happy to have a website \"track me\", certainly if the cost of non-tracking are having to click away an annoying popup The you should doubly blame the companies, because that's what do not track was for, they're the one who decided to make it not work that way and instead being ignored and not considered a valid option for the law. > think that people who compare a website wanting to know the number of their visitors to \"hidden fees\" are kind of being ridiculous. You don't need a cookie for that, and what GDPR has told us is that we're not talking of that but about dozens or hundreds on every major sites so trying to frame it that way is disingenuous. reply leereeves 21 hours agorootparentprev> people who compare a website wanting to know the number of their visitors to \"hidden fees\" are kind of being ridiculous Is counting visitors all that sites are doing with tracking info? They're not selling it to ad brokers, insurance companies, governments? They're not matching your name, address, and phone number with your web activity (including sexual interests, \"anonymous\" embarrassing stories, health concerns, etc)? reply foobarian 21 hours agoparentprevThe fees example is maybe apples to oranges. The fees are a problem because they subvert the pricing information signals needed for the free market. The problem is not the fact that they are charged, the problem is that they are not included in an upfront price display. Were they included in the total upfront price and never specified the users should not care - it's not their business how a company spends their money. But I suppose that was just an example you picked to illustrate the industry's malicious compliance, and not the main point, in which case fair enough. :-) reply ImPostingOnHN 19 hours agorootparentThe use of secret tracking also subverts the pricing signals needed for the free market. Users aren't informed that the website is subsidized by the sale of the users' information, much less the details of the arrangements and monetary amounts. If the total price of the website without the secret costs of tracking were presented upfront, it would be less of an issue. reply haitchfive 19 hours agoparentprevI agree with almost everything you said, except for one thing: I don't believe the euphemism \"hidden fees\" helps to clarify the fact that these people are taking money away from people without their knowledge or explicit consent. We have other more precise words to describe that action. I asked ChatGPT what those could be, here's its answer: Q: What are some english words meaning \"taking money away from people without their knowledge or explicit consent\"? ChatGPT: There are several words and phrases in English that convey the idea of taking money away from people without their knowledge or explicit consent: Embezzlement: This refers to the act of dishonestly withholding assets for the purpose of theft. It often involves someone in a position of trust, such as an employee, misappropriating funds entrusted to them. Misappropriation: Similar to embezzlement, misappropriation involves taking something (usually money) for one's own use without permission or legal right, often in a breach of trust. Theft: Theft is the generic term for taking someone else's property without permission, including money. Fraud: Fraud involves intentional deception for personal gain, which can include financial deception or stealing. Swindling: This term implies deceitful behavior to cheat or defraud someone, often involving trickery or manipulation. Skimming: Skimming refers to the illegal practice of taking cash \"off the top\" of the proceeds of a business or other source of income without recording it. Extortion: While not always directly related to taking money without explicit consent, extortion involves obtaining money, property, or services from an individual or entity through coercion or threat. Pilfering: Pilfering involves stealing small amounts or petty theft, often done stealthily or without detection. Conning: This refers to the act of deceiving or tricking someone, often for financial gain, through manipulation or persuasion. Clandestine withdrawals: This phrase specifically refers to taking money from someone's account without their knowledge or consent, typically in a secretive or unauthorized manner. reply ImPostingOnHN 19 hours agorootparentWe here are all interested in hearing your thoughts, so please filter raw chatbot output through them, rather than pasting the output verbatim, which isn't value-added, and can even be negative value, given chatbots' penchant for hallucinating information. reply c22 20 hours agoparentprevExcept you could always just \"turn off fees\" in the browser, so the whole conflict seems kind of superfluous. reply account42 18 hours agorootparentExcept you can't because the in-browser fees are only one of many possible fees you could be charged. reply AlchemistCamp 21 hours agoparentprevThe \"fee\" isn't the cookie. It's the obnoxious popup. reply roenxi 21 hours agoparentprevHidden fees are bad because of the specific combination - the hiding, and the fees. Since tracking isn't hidden and isn't a fee, the analogy doesn't help to justify the EUs law. People should have a default expectation that if they give their personal data to companies then it will be recorded. And if they don't want cookies then they should disable cookies. The EU's regulation hasn't revealed anything that is useful to know about. reply AlexandrB 21 hours agorootparentPeople don't \"give\" their information to trackers, it's collected without their knowledge. I don't think most people expect the kind of things trackers collect is being collected. reply paulryanrogers 21 hours agorootparentprevTracking is certainly hidden if you're not a programmer, and is certainly a fee if you value your time. Not all people live in low-trust societies or desire to. reply jasode 22 hours agoprev>, Paul Graham came up with the thought, that the EU forces companies to have cookie banners. There is no law for cookie banners. [...] Companies could easily avoid any cookie banner. Just don’t track. KingOfCoders/amazingcto, of course you are technically correct but Paul Graham wasn't talking about the letter of the law. Instead, you have to interpret his complaint with the lens of game theory. I.e. The Law of Unintended Consequences that takes into account what companies actually do in response to laws instead of what we hope they will do. Your blog post focused on good intentions of the law. PG's tweet focused on actual outcome. reply n4r9 21 hours agoparentDoesn't that argument work both ways? If you interpret the EU's regulation with the \"lens of game theory\", it is an unintended consequence of aggressive corporate data collection. Not sure why it makes sense to complain about the EU and not the companies. reply CipherThrowaway 20 hours agorootparentOf course not. Only titans of industry and the landed gentry of the executive class are allowed to \"move fast and break things\", \"ask for forgiveness rather than permission\" and take \"imperfect action rather than perfect action.\" It's more morally permissible for corporate decision makers to install a global surveillance complex than for civil servants to attempt to regulate it. reply sshine 15 hours agorootparent> It's more morally permissible for corporate decision makers to install a global surveillance complex No, it's more transparent. Unlike cookie banners. If only cookie banners protected the consumer, but shadow cookies work fine. reply edanm 19 hours agorootparentprevBecause the companies are getting what they want (data on users), but the regulation is not getting what it wants (no tracking or informed tracking). I don't know if this mini-competition between regulators and companies is truly zero-sum, there could be some way to get everyone something they want. But with the current regulation, it is zero-sum, and the companies are winning and the EU is losing. And the EU \"works for you\", so of course you can complain to them. reply n4r9 18 hours agorootparent> the regulation is not getting what it wants (no tracking or informed tracking) That's an overstatement of the purpose of the regulation IMO. The purpose is to give the user control over the tracking of their data. reply edanm 18 hours agorootparentOK, fair enough. pg's point still stands I think - I believe that most users have zero idea what that popup is and don't bother doing anything but clicking on it immediately even if they do have some idea. reply polygamous_bat 21 hours agorootparentprev> Not sure why it makes sense to complain about the EU and not the companies. Unfortunately a non-negligible number of people in tech also have libertarian leanings, with a default “gubmint bad!” position, which makes them easy prey for adtech propaganda. reply FredPret 21 hours agorootparentHow arrogant to assume your position should be the default one, and people who don’t agree with you are - of course - easy prey for propaganda. reply hallway_monitor 21 hours agorootparentSeems to be a common tactic from a certain faction currently in power in the United States. reply FredPret 17 hours agorootparentNot just a US phenomenon - it's gone global reply fauigerzigerk 21 hours agorootparentprevThat's beside the point. If you are in favour of government intervention you should be all the more interested in good policies that have the intended effect. Bad laws boost libertarianism. reply ZeroGravitas 21 hours agorootparentAlso, lying about good laws boosts libertarianism. At least until you realise what they're doing, then you think they're skeevy corporate toadies with no morals. reply gred 21 hours agorootparentprev> Unfortunately a non-negligible number of people in tech also have libertarian leanings Why is this unfortunate? Because you don't agree with us? The \"they would agree with me if they were smarter\" trope is tired and gets us nowhere. reply Kbelicius 21 hours agorootparent> Why is this unfortunate? GP answered your question, for some reason you decided to cut the quote right before the answer. Here is the part that is missing from your quote which answers your question: '[...]with a default “gubmint bad!” position' reply gred 21 hours agorootparentPretty clearly implying the diminished mental capacity which prevents us from agreeing with him, no? I addressed this above: > The \"they would agree with me if they were smarter\" trope is tired and gets us nowhere. reply Gormo 19 hours agorootparentprevPerhaps you should consider the possibility that the reason why libertarians assume a default \"gubmint bad!\" reaction to new policy interventions is that they are sensitized due to decades of experience seeing multitudes of government interventions both (a) to achieve their intended outcomes and (b) create unintended consequences, often worse than the problems they are meant to solve, instead. Personally, I find it very very strange that many of the people who call for regulation as a remedy to perverse incentives manifest in commercial markets seem unwilling to recognize the existence of even more perverse incentives in the political realm. If people seeking profit sometimes do bad things to get it, why would people seeking political power be expected to behave differently? reply FredPret 20 hours agorootparentprevSome people have a default “gubmint gud business bad” position and assume that disagreement is only possible if you’re a brainwashed bootlicker. I say that’s unfortunate reply fauigerzigerk 21 hours agorootparentprevNo, it does not work both ways. The roles of governments and corporations are not symmetric. Good regulation is regulation that has good outcomes. If a law has bad outcomes it is a bad law. You can separately complain about what companies are doing but that doesn't change the fact that it's a bad law. It is of course debatable whether GDPR as a whole has bad outcomes, but if we're talking about cookie banners in isolation then it certainly does. reply ragnese 21 hours agorootparent> No, it does not work both ways. The roles of governments and corporations are not symmetric. > > Good regulation is regulation that has good outcomes. [...] You don't seem to explain what the role of corporations is or what a good corporation looks like. If these things are not symmetric, you need to finish your explanation of why or how they aren't. Corporations and the whole of property rights only exist because of government protection, so it would be pretty audacious--in my opinion--to assert that corporations have no duty to behave to the benefit of society. I'm not saying that's your claim, but I'm curious as to how close you're willing to get to that claim... reply fauigerzigerk 20 hours agorootparentGovernments are supposed to represent the whole of society. The justification for their policies is ideally based on democratic legitimacy. No entity outside of government can possibly have that legitimacy. In my opinion it is not audacious at all to reject the idea that corporations should intentionally pursue societal goals or claim to act out of a sense of duty. Of course we want the effect of what corporations do to be of net benefit to society as a whole. But this cannot be based on their intentions or sense of duty. It has to be based on the systemic effects of them pursuing their own (possibly enlightened) self interest within the framework of the law. It is for governments to make sure that these effects are beneficial and to intervene when they are not. So the asymmetry I see is that capitalism is a tool of society, not the other way around. reply n4r9 20 hours agorootparentI would regard it as a duty of government to ensure that using the internet is safe and respects user privacy, but not to ensure that the internet has a clean UI. To that extent, I'd argue that the EU is achieving good outcomes. Ensuring a clean UI and smooth user experience is one of those things that should manifest as a result of market economics, but does not manifest because markets don't really work that way. reply fauigerzigerk 19 hours agorootparentI see absolutely no reason why clean UX should manifest as a result of market forces. Media consumers, on average, are clearly unwilling to pay for ad-free experiences. reply n4r9 18 hours agorootparentI won't pursue that assertion as it's tangential. I shouldn't have brought it up. The main point is that clean UX is not the purpose of the EU's data laws. reply ragnese 18 hours agorootparentprev> In my opinion it is not audacious at all to reject the idea that corporations should intentionally pursue societal goals or claim to act out of a sense of duty. I still find it somewhat silly to reject the idea that a corporation (run by human beings) shouldn't intentionally be evil for the sake of maximizing profit, but I do understand that this is a fairly common Friedman-esque point of view. But, even so, I guess \"duty\" was the wrong word for me to use. I more meant that if a corporation does NOT benefit society, we should expect the corporation to stop existing. So, in that sense, there's a \"duty\" (existential requirement) to benefit society. > Of course we want the effect of what corporations do to be of net benefit to society as a whole. But this cannot be based on their intentions or sense of duty. It has to be based on the systemic effects of them pursuing their own (possibly enlightened) self interest within the framework of the law. > > It is for governments to make sure that these effects are beneficial and to intervene when they are not. So the asymmetry I see is that capitalism is a tool of society, not the other way around. I feel like you're circling back around to almost disagree with yourself. Several comments back in this thread someone made a point about \"unintended consequences\" of the law and applying \"game theory\" logic to it, and another commenter replied that the companies in question could also have seen the law coming if they misbehaved too badly. That commenter asked if the \"game theory\" logic shouldn't go both ways, and that we should then blame the corporations for the regulation because the government is just doing what governments do. You replied that the argument does NOT go both ways because the roles of government and corporations are not symmetric. But, what you're arguing here seems to be consistent with the view that the \"unintended consequences\" and \"game theory\" logic DOES go both ways. You acknowledge that it is a government's duty to intervene when corporations are not benefiting society, and you also say that corporations will pursue their own self-interest within the framework of the law. I don't mean to put words in your mouth, but the only way I can resolve this asymmetry in my mind is to have a framework where corporations doing things that are bad for society is okay, because the government is supposed to stop them; but if the government is unable to fully stop them from being bad, then it's STILL not the corporation's fault, but the government's... It just sounds like we've gotten lost in the abstractions of corporations and governments. At the end of the day, these are decisions being made by fellow sentient human beings, and if a corporation's humans make some evil decision, I refuse to let them off the hook with \"well, free markets\" and \"they have no choice but to maximize profits\". reply fauigerzigerk 15 hours agorootparent>I still find it somewhat silly to reject the idea that a corporation (run by human beings) shouldn't intentionally be evil for the sake of maximizing profit, but I do understand that this is a fairly common Friedman-esque point of view. On a very general level, the idea is that not every part of a complex system has to incorporate all the principles of the system as a whole. Individual parts of the system can have limited roles and responsibilities. That's fine and it has nothing to do with being evil. Defense lawyers must defend their clients to the best of their ability whatever horrible things they may have done. Juries, judges, prosecutors, they all have their specific roles to play. It's the justice system as a whole that should result in justice being done. If everyone involved tried to pursue their own interpretation of generally desirable societal outcomes, the justice system would be unfit for purpose. And here's the asymmetry again. Those designing the system as a whole have to think about societal outcomes as part of their job (as does every citizen). Those acting in a specific defined role as part of the system can only do that in limited ways or under exceptional circumstances. Corporations are run by people, but these people act in a limited role that is defined in such a way that pursuing specific societal outcomes does not necessarily boost the likelihood of their personal success or the success of the corporations they run. If there is a conflict between certain societal outcomes and making a profit then those executives willing to prioritise profits will be the ones running the successful corporations. That's why it's so futile to bet on corporations acting against their self-interest in significant ways. They are systemically incapable of doing that (on average - exceptions are always possible). That's why I'm saying that if we want to make corporations act in desirable ways, we have to make laws rather than appealing to the conscience of those running the corporations. >I don't mean to put words in your mouth, but the only way I can resolve this asymmetry in my mind is to have a framework where corporations doing things that are bad for society is okay, because the government is supposed to stop them; but if the government is unable to fully stop them from being bad, then it's STILL not the corporation's fault, but the government's... The question I'm asking is who can fix a particular issue, and if the issue isn't getting fixed then I'm assigning blame to those whose job it is to fix it. Corporations collectively can't fix an issue when the only fix is not exploiting a particular economic opportunity. If one corporation stops exploiting the opportunity, another one will. That said, of course I do blame corporations for stuff all the time. There's nothing wrong with that. Blaming them is sometimes effective consumer power. It can take away the economic opportunity as the reputational damage may outweight the benefits. Blame can also help build momentum for a change in the law. But if laws are made and they have giant loopholes in them, then I blame lawmakers for doing a shoddy job. reply zanellato19 21 hours agorootparentprevExposing the fact that the entire internet is tracking is actually a good outcome. reply hikingsimulator 21 hours agoparentprevThe blog clearly works from the actual outcome lense. It's repeated. Several times. The companies could just not track. The actual outcome is that they do want to track, and use adversarial patterns and malicious compliance to twist your arm and \"force consent.\" Paul Graham is still wrong. reply jasode 21 hours agorootparent>The blog clearly works from the actual outcome lense. [...] The companies _could_ just not track. No, you've inadvertently stated a contradiction. Your use of the word _\"could\"_ is literally a hope/wish/intention of the law. In contrast, the actual outcome is that the companies didn't stop tracking. We _wish_ they would stop tracking. (I.e. \"The companies _could_ just stop tracking us!\") But that hope still doesn't change the observation of reality. reply hikingsimulator 21 hours agorootparentThe law is not code. Equating hope with the intention of the law is a poor way to think about it. The law is to protect users against opaque companies and to enable them making informed choices. If companies act maliciously to contort around the law and force users back to making uninformed choices, it is the companies' fault and not the law's. Companies could have followed the interpretation of the law unobstrusively. But they didn't. Invoking \"reality,\" semanticking a position, do not make Graham's position justified. Neither does it make the blog wrong. reply itishappy 19 hours agorootparentprevBut companies have stopped tracking (or they've started lying). I can now opt out. I could not before. reply voxic11 20 hours agorootparentprevCan you really say that confidently? I think a lot of these companies would go out of business if they didn't track users so it seems like under the law they have no option but to show cookie banners. Or are you claiming the law exempts companies in such circumstances? reply itishappy 19 hours agorootparentI'm sure illegal/unethical actions would help a lot of struggling companies. reply KingOfCoders 21 hours agoparentprev(author here) I'm a fan of second-order thinking and unintended consequences, so I'm with you there. How would you frame a \"don't track people without consent\" without unintended consequences? The article tries to make the point (perhaps fails), that companies do this intentionally to get the \"consent\" of people against their will, therefor running the tight line of breaking the law without breaking it. reply denton-scratch 21 hours agorootparent> How would you frame a \"don't track people without consent\" without unintended consequences? Drop the consent requirement? I.e. just don't track people. No third-party cookies, first-party only, and only for the correct operation of the site. It's not the cookies that people object to, it's the tracking. Tracking provides no benefits to visitors. If there were no tracking risk, there would be no need to require consent. reply caskstrength 20 hours agorootparent> It's not the cookies that people object to, it's the tracking. Tracking provides no benefits to visitors Sure it does. Visitors get to use all those great sites and apps without paying for the services directly. reply account42 18 hours agorootparent\"directly\" does the heavy lifting here. Users (on average) still end up paying for the services in the end. reply danaris 19 hours agorootparentprevThat's not a benefit of the tracking. That's a benefit of the advertising dollars. I have yet to see any kind of meaningful study showing that tracking improves the ROI on advertising by anything remotely resembling enough to justify it. reply mlrtime 21 hours agorootparentprevProbably the same way most laws end up. We see the unintended consequences, then revise the law to counter the consequences. Thus the cat/mouse game continues. An idea could be that the tracking has to be opt-in AND the webpage cannot stop critical use of the page as part of the opt-in process. Then another round of consequences.. rinse repeat... reply denton-scratch 21 hours agorootparent> An idea could be that the tracking has to be opt-in Why would anyone opt-in? Tracking provides zero benefits to the site visitor. reply ImPostingOnHN 12 hours agorootparentWhy does there need a reason for them to opt-in? Maybe they just don't. That's okay. Like you say, zero direct benefits. reply gizmo 21 hours agorootparentprevThe problems with the current law are: - no fines for non-compliance (or malicious compliance) - no legal liability for data leaks of PPI When businesses believe (correctly or incorrectly) that the benefit of tracking outweighs the cost (annoying users, regulatory noncompliance) they will do it. The fix is to make tracking too costly for businesses. reply pella 21 hours agorootparent> - no fines for non-compliance (or malicious compliance) \"The Biggest GDPR Fines of 2023\" 1. Meta – €1.2 billion (Ireland) 2. Meta – €390 million (Ireland) 3. TikTok – €345 million (Ireland) 4. Criteo – €40 million (France) 5. TikTok – €14.5 million (UK) 6. Axpo Italia Spa – €10 million (Italy) 7. Tim S.p.A. – €7.6 million (Italy) 8. WhatsApp – €5.5 million (Ireland) 9. EOS Matrix – €5.5 million (Croatia) 10. Clearview AI – €5.2 million (France) \"GDPR fines are designed to make non-compliance around data security a costly mistake and they can be separated into two tiers. Less severe infringements can result in a fine of €10 million or 2% of a firm’s annual revenue from the preceding financial year, depending on which amount is higher. More serious violations can result in a fine of up to €20 million or 4% of a firm’s annual revenue from the preceding year, depending on what is higher.\" via https://www.eqs.com/compliance-blog/biggest-gdpr-fines/ reply gizmo 20 hours agorootparentWhich ones of those fines were because of inappropriate use of cookie consent popups? You just copy-pasted a list of GDPR fines. reply pella 20 hours agorootparent> fines were because of inappropriate use of cookie consent popups? see: \"8 companies that faced cookie consent fines\" https://www.cookieyes.com/blog/cookie-consent-fines/ \"In January 2023, France’s data protection watchdog, CNIL, fined TikTok €5 million ($5.4 million) for making it difficult to refuse cookies on its website. The CNIL found that TikTok manipulated consent by discouraging users from rejecting cookies. They required multiple clicks to refuse cookies, but only one click to accept them. TikTok resolved the issue by adding a “Refuse all” button to its site.\" reply kevmo314 21 hours agorootparentprevFines for data breaches is one idea? If we want to disincentivize data hoarding, the main cost to data hoarding is data breaches, so we could perhaps penalize that. This would have a different issue, specifically companies would no longer self-report data breaches, but it's just an idea. There are alternative approaches to getting to \"don't track people without consent\" that aren't a toothless stick by making it more expensive to track. reply falcor84 21 hours agorootparentHere's my idea - no data collection without compensation. For example, you must pay me in advance 1 cent for the permission to access 1 byte of my personally identifiable information for the following month, whether that's stored in a cookie or in your own database or you access it via a third party (e.g. Meta). So instead of a \"cookie consent\" pop-up, I want a \"cookie payment\" pop up where the site will ask me for my payment details and say how much they'll pay me (again, in advance) for each of the options I can toggle. reply Sponge5 21 hours agorootparentprev> the main cost to data hoarding is data breaches ... the main cost at the moment. I think we as a society are very close to a tracking/data tax. reply caskstrength 20 hours agorootparentprev> The article tries to make the point (perhaps fails), that companies do this intentionally to get the \"consent\" of people against their will, therefor running the tight line of breaking the law without breaking it. That X button is right there at the top near the tab name. Not sure how a user could be forced against their will into staying on the site presenting them with a cookie banner. reply CipherThrowaway 21 hours agoparentprevEveryone knows that bad actors will continue to behave badly in the face of the law. This isn't the insight you seem to think it is. Really, PG's tweet has little to do with game theory or anything else. It is a first-world-problem whinge about having to click through cookie banners. Assessing the \"actual outcome\" of complex regulation and legislation is a task beyond the scope of a single tweet. It might be useful for Graham to determine what claim he is trying to make in the first place. Is he rebutting a particular EU representative for boasting about how good they are at regulation? Or is the idea that the EU shouldn't have the audacity to attempt to regulate in the first place? reply semi-extrinsic 21 hours agoparentprevI consider it a good outcome when I can clearly identify shitty websites and just click the back button. reply gizmo 21 hours agoparentprevExcept many companies respond to the cookie law with a cookie consent popup that violates the law (by making opt-out harder than opt-in). Could we really have predicted from the \"Law of Unintended Consequences\" that companies would respond not by tracking less nor by giving people an easy way to opt out, but with a cookie consent popup that is not compliant and also really annoying to their visitors? This is better explained by business operators being ignorant of the actual law and being ignorant of the UX impact. reply ryandrake 19 hours agoparentprevThere are a lot of ridiculous things a company can choose to do in response to any given law. Those choices are not mandated by the law. Horrible consent UX is not the only option to choose from. Government can, and should, analyze likely (or unlikely) unintended consequences and use those to further shape the law, but at the end of the day, those consequences come from choices that people who are subject to the law make. I think the big mistake the EU made is they probably thought: “Surely no company would choose to abuse their customers with horrible UI just because they don’t like the law and want to take their collective frustration out on their users!” The EU was obviously wrong about the extent to which companies would throw their users under the bus while maliciously complying. reply GTP 21 hours agoparentprevI see your point, but then to have a constructive conversation Paul Graham should also give his two cents about how the law could be improved. I don't know him, so I'll ask here: did he do that? reply karmakaze 15 hours agoparentprevThe outcome would be much better if the law explicitly stated that the initial cookie banner must have a \"Necessary cookies only\" opt-out one-click option. And that this option means truly necessary, not the Internet Explorer is needed by the operating system 'necessary'. reply jmathai 21 hours agoparentprevIt’s unfortunate, if companies are okay not tracking you, that they care little enough about their user experience to use cookie banners. reply VeejayRampay 21 hours agoparentprevPaul Graham focused on whining about regulation as he always does reply sputr 21 hours agoparentprevThe actual outcome is, from my experience, that tracking has reduced, a lot. When this law was enacted, *we all removed \"like on Facebook\"* buttons. Remember those? Yeah, we don't see them anymore. Google Analytics also was forced to change, at least a little. Is there still tracking? Sure. But it's not so blatant anymore. There are hoops one needs to jump through. And that was the point - to make tracking a harder. None of my projects have cookie banners. Why? Because I use a first party tracking system (Matomo), I anonymize all visits and I respect DNT. It's that easy. reply FredPret 21 hours agorootparentIt’s not the difficulty level that people object to. It’s a combination of two things: 1) the law comes to the rest of the world from Europe. We (rest of the world) didn’t vote in the people who brought it. We’ve had quite enough of Europeans making rules for the rest of the world in the past few centuries thank you very much. 2) GDPR encodes an expectation that may or may not be common in the EU, but certainly isn’t common elsewhere. I don’t have any expectation of privacy when I walk in public or when I give any information at all to a business. My solution to this is: a) I wear pants outside, and b) I don’t give out private information. Whether the business ecosystem knows their age and purchasing patterns is largely immaterial to virtually everyone I’ve ever met. And don’t show me a survey showing people don’t like it - if you prime people with the question, of course they will respond that way. They know their info is being gathered, and they just don’t think it’s as big a deal as GDPR would like it to be. reply sputr 17 hours agorootparentSo, I get your point. I can see how (1) can be aggravating. Can't really say anything to defend it, that's the Brussels effect for you. From the point of view of your own sovereignty, it's a bad thing, period. From the point of view of an effect on the lives of average people, I'm not so sure, it's so cut and dry. Now, point (2) is, unfortunately, in the same vein as smoking, pollution, seat belts etc. Uninformed people (uninformed because they have better things to do) are not protected from their lack of knowledge. They suffer the consequences just the same. And while I agree that and informed person, making a self-destructive choice has (in most cases) the right to do so, there is something to be said about the very, very powerful exploiting the uninformed. And this is where GDPR comes into play. It's protecting normal people, from a very, very big threat, that is not that obvious and is being wielded by the powerful. GDPR is one of those laws restraining western corporations from going full dystopian future on us all. I said restraining, to be honest, I think it's just slowing them down. And as far as surveys go - it used to be the same here. Europeans didn't care and said exactly the same things (i.e. the famous \"i didn't do anything wrong, so I have nothing to hide\") and then activists worked for years to educate them that, at the very least, it's leading them to buy things at higher prices. Now most people are extremely sensitive to their data. reply FredPret 16 hours agorootparentI get it - what you're saying is a very common-sense regulation. Reasonable people can disagree about this. But different societies prefer a different balance here. Americans are used to a more caveat emptor situation. Europeans want more regulation. Which one to choose is a political choice. What's happening is that the political choice that the EU went with is being forced on the rest of us, whether we like it or not. reply ImPostingOnHN 12 hours agorootparentWith all due respect, you're speaking on behalf of 1 person here, not an entire country of people, and certainly not the entirety of the non-E.U. world. \"We\" can speak for ourselves, and don't all agree with the views you're ascribing to us. And \"I\" don't agree with the sort of stereotyping I'm responding to. I'm personally glad someone is doing something for my privacy here. My own government, due to regulatory capture, is unlikely to act in my best interests here. reply FredPret 8 hours agorootparentUh, what? Because the EU is forcing you to do something that you want to do anyway, you now like it? If you want cookie banner laws in your non-EU country, vote for it. I don’t want some bureacrat I didn’t vote for issuing diktats that affect how I build my business and my websites. The entire point is that we all need representatives in government. reply ImPostingOnHN 4 hours agorootparentThe EU isn't forcing me or you to do anything. The article elaborates on this point: There Is No Cookie Banner Law. Only bad website operators choosing to abuse their users with annoying consent dialogs. Nobody in Europe is issuing \"diktats\", meaning citizen-supported legislation I guess, or affecting your business, unless you're trying to deal with their citizens' data. Just don't process EU citizen data and it's not an issue. Better yet, just don't track users. In any case, your disagreement only serves to underscore that you were speaking on behalf of 1 person, not any country or countries. Otherwise, we wouldn't be disagreeing! reply FredPret 13 minutes agorootparentI actually self-host all my web assets and use Matomo already. So I do actually agree with the premise. What I object to strenuously is someone dictating terms to the world from distant shores, especially since they seem not to get how the internet works (it’s all funded by ads, online sales, and ads for online sales, all of which involve metrics and tracking!) The diktats I mentioned include a ruling that Google Fonts are illegal now. So if I’m using those, or I’m using Google Analytics, and a European happens across my site, I’m now a criminal? Fuck that. The consequences of contravening the GDPR are uncertain but sounds scary. This is terrible for the open and free internet. klabb3 21 hours agoparentprevI’m not surprised. This is a “hot take-centric” platform issue, and a laziness in trying to understand him too. Or.. two people on the street yelling at each other but not listening. reply tlb 22 hours agoprevPart of what it means to be \"good at regulation\" is to anticipate the likely consequences of regulations. So a regulation that says that \"businesses must now give away their products for free, unless they honk each customer's nose\" will result in a lot of sore noses. Which is basically the case here. Almost all websites make money through ads, or at least keep logs of user activity to help them optimize their website, and that's not going to change, so the EU's boneheaded regulations make the customers suffer a little extra. reply IanCal 22 hours agoparent> Almost all websites make money through ads, Doesn't require tracking of individuals. > or at least keep logs of user activity to help them optimize their website Doesn't require tracking of individuals. reply rchaud 20 hours agorootparent> Doesn't require tracking of individuals. Only if you maintain your own ad inventory, instead of using Google/Facebook ads like 90% of online advertisers do. And neither of those platforms work without installing their scripts on your site. reply IanCal 17 hours agorootparentSure, lots of people want to sell my data. That's a choice. You don't need to do that for advertising - it's a pretty recent invention having fully personalised adverts. reply thejohnconway 18 hours agorootparentprevAnd they did it that way because they could. It could be done a different way. reply rchaud 17 hours agorootparentIt would be like opening an independent video store when the entire market has moved to streaming. Yeah you could try it, but there are good reasons not to. reply IanCal 2 hours agorootparentThe only change needed is to let people default to no. After all, Google and Facebook still show ads if a user doesn't consent right? I bet they'd add that option in a heartbeat if people would leave them otherwise. The scale of this kind of thing is ridiculous. Opening a basic news site and I'm asked to consent to my data being taken and used by 750 companies. reply soerxpso 10 hours agorootparentprev> Doesn't require tracking of individuals. Building a house doesn't require powertools, but if your company tries to do it with handtools we'll see who goes bankrupt first. reply IanCal 10 hours agorootparentBuilding a house doesn't require using cheaper but more dangerous materials either but people try to, that's why we have regulations. Analogies can be pithy but are rarely useful as an argument. Talk about reality. reply madsbuch 3 hours agorootparentanalogies are even less useful when you make it to a widely regulated sector and seek anti-regulation analogies. I am sure the construction sector is overflowing with grumpy people who feel like aspestos is the best form for isolation. reply throw10920 21 hours agorootparentprevCorrect me if I'm wrong, aren't but IP addresses are considered to be \"personal information\" and therefore collecting them is \"tracking\" under the GDPR? reply IanCal 21 hours agorootparentYes but it depends what you're doing with them as to whether you need consent. If you're keeping a record of my IP address and what I do on your site to sell me stuff then yes you're tracking me and need my consent for that. If you've got my IP address in your logs because you keep security logs for reasonable timeframes then you don't need my consent - though you do need to handle them appropriately because it's my personal data. reply oneeyedpigeon 21 hours agorootparentprevMy guess is that they are because ISPs may keep records of them—I think they are required to in some jurisdictions. But you don't have to store IPs in your server logs. reply IanCal 21 hours agorootparentYou're also allowed to store IP addresses in your logs, you just have to take care with the data and the reason you're storing them needs to be justified - either because you have a legitimate interest in doing so (e.g. security) or because you have my explicit consent. If I order something from an online shop, they don't need to have a banner in order to take my name and address to post the item to me - that's fully expected and reasonable. They do need my consent if they want to use that to post adverts to me though. reply thomastjeffery 20 hours agorootparentprevWhat else would you need my IP address for? reply throw10920 20 hours agorootparentUh...DDoS and spam protection? reply yau8edq12i 18 hours agorootparentThen store it for that purpose, don't use it for anything else, and delete it when it's not useful anymore (realistically, for these purposes, after a few minutes to an hour?). reply tekknik 4 hours agorootparentOk so now a criminal just needs to avoid being detected for an hour and the logs are self wiping. Sounds like a feature they’ll love! reply yau8edq12i 2 hours agorootparentI thought this was about DDOS and spam protection. If you want to move the goalposts, state that explicitly. reply IanCal 2 hours agorootparentprevKeep them as long as is reasonable, then delete them. reply rkangel 22 hours agoparentprevYour point is well made, and this is an unfortunate consequence of the regulation (and I enjoyed the analogy). But it isn't necessary to have cookie banners on every website. Github is a moderately complex, user-optimised website, right? https://github.blog/2020-12-17-no-cookie-for-you/ reply hgomersall 21 hours agorootparentI clicked on that link and immediately got a cookie banner. Am I missing something? reply rkangel 21 hours agorootparentInteresting. Clearly I am providing out of date information. reply cseleborg 21 hours agorootparentMore interestingly, that article says: > We are also committing that going forward, we will only use cookies that are required for us to serve GitHub.com. A few pixels further down, on the cookie banner: > We use optional cookies to improve your experience on our websites and to display personalized advertising based on your online activity. I guess now we finally have a rule-of-thumb figure for what \"going forward\" means: 3-4 years, tops. reply hgomersall 19 hours agorootparentOTOH, github.blog is not github.com. reply denton-scratch 20 hours agorootparentprevI don't know; I clicked on the link and saw no cookie banner. reply siva7 17 hours agorootparentIt's probably only displayed to EU users. I saw the cookie banner and it left a bad impression on me with such an blog article. reply isodev 22 hours agoparentprev> Almost all websites make money through ads The EU regulation does not prevent ads from being shown, it specifically targets tracking. No tracking > no banner > everyone is happier > go ahead and show all the ads that are required. reply Xelbair 21 hours agorootparentAnd all that tracking comes down with inability to take risk on business side. Ad company wants to be 100% sure that ads are shown to humans, and pay only for those shown to humans(going deeper - to specific cohorts of humans, whi",
    "originSummary": [
      "The EU does not require cookie banners on websites; companies use them to track users for ads, as discussed in the article.",
      "Alternative, less invasive ways for obtaining user consent for tracking are proposed by the author.",
      "The article emphasizes the significance of data privacy, EU regulations safeguarding it, and offers additional resources for CTOs on software development and management."
    ],
    "commentSummary": [
      "Discussion revolves around companies concealing fees and tracking user data without consent, especially concerning cookie banners and GDPR rules.",
      "Debate focuses on the effectiveness of laws safeguarding privacy, impact on user experience, and responsibilities of corporations and government in upholding consumer protection.",
      "Potential solutions discussed include browser plugins and the necessity for stricter enforcement to tackle online tracking and privacy issues."
    ],
    "points": 547,
    "commentCount": 574,
    "retryCount": 0,
    "time": 1710761116
  },
  {
    "id": 39742692,
    "title": "Rust introduces Cranelift for faster code generation",
    "originLink": "https://lwn.net/SubscriberLink/964735/8b795f23495af1d4/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us User: Password:| Subscribe / Log in / New account Cranelift code generation comes to Rust [LWN subscriber-only content] Welcome to LWN.net The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider accepting the trial offer on the right. Thank you for visiting LWN.net! Free trial subscription Try LWN for free for 1 month: no payment or credit card required. Activate your trial subscription now and see why thousands of readers subscribe to LWN.net. By Daroc Alden March 15, 2024 Cranelift is an Apache-2.0-licensed code-generation backend being developed as part of the Wasmtime runtime for WebAssembly. In October 2023, the Rust project made Cranelift available as an optional component in its nightly toolchain. Users can now use Cranelift as the code-generation backend for debug builds of projects written in Rust, making it an opportune time to look at what makes Cranelift different. Cranelift is designed to compete with existing compilers by generating code more quickly than they can, thanks to a stripped-down design that prioritizes only the most important optimizations. Fast compiler times are one of the many things that users want from their programming languages. Compile times have been a source of complaints about Rust (and other languages that use LLVM) for some time, despite continuing steady progress by the Rust and LLVM projects. Additionally, a compiler that produces code quickly enough is potentially viable in applications where it currently makes more sense to use an interpreter. All of these factors are cause to think that a compiler that focuses on speed of compilation, rather than the speed of the produced code, could be valuable. Cranelift's first use was as the backend of Wasmtime's just-in-time (JIT) compiler. Many languages now come equipped with JIT compilers, which often use specialized tricks to quickly compile isolated functions. For example, Python recently added a copy-and-patch JIT that works by taking pre-compiled sections of code for each Python bytecode and stitching them together in memory. JIT compilers often use techniques, such as speculative optimizations, that make it difficult to reuse the compiler outside its original context, since they encode so many assumptions about the specific language for which they were designed. The developers of Cranelift chose to use a more generic architecture, which means that Cranelift is usable outside of the confines of WebAssembly. The project was originally designed with use in Wasmtime, Rust, and Firefox's SpiderMonkey JavaScript interpreter in mind. The SpiderMonkey project has since decided against using Cranelift for now, but the Cranelift project still has a design intended for easy incorporation into other programs. Cranelift takes in a custom intermediate representation called CLIF, and directly emits machine code for the target architecture. Unlike many other JIT compilers, Cranelift does not generate code that relies on being able to fall back to using an interpreter in case an assumption is invalidated. That makes it suitable for adopting into non-WebAssembly-related projects. Cranelift's optimizations Despite its focus on fast code generation, Cranelift does optimize the code it generates in several ways. Cranelift's optimization pipeline is based on equality graphs (or E-graphs), a data structure for representing sets of equivalent intermediate representations efficiently. In a traditional compiler, the optimizer works by taking the representation of the program produced by parsing and then applying a series of passes to it to produce an optimized version. The order in which optimization passes are performed can have a large impact on the quality of code produced, since some passes require simplifications made by other passes in order to apply. Choosing the correct order in which to apply optimizations is called the phase-ordering problem, and has been the source of a considerable amount of academic research. In Cranelift, the part of each optimization that recognizes a simpler or faster way to represent a particular construct is separated from the part that chooses what representation should ultimately be used. Each optimization works by finding a particular pattern in the internal representation, and then annotating it as being equivalent to some simplified version. The E-graph data structure represents this efficiently, by allowing two copies of the internal representation to share the nodes that they have in common, and to allow nodes in CLIF to refer to equivalency classes of other nodes, instead of referring to specific other nodes. This produces a dense structure in which adding an alternate form of a particular section of the program is cheap. Because optimizations run on an E-graph only add information in the form of new annotations, the order of the optimizations does not change the result. As long as the compiler continues running optimizations until they no longer have any new matches (a process known as equality saturation), the E-graph will contain the representation that would have been produced by the optimal ordering of an equivalent sequence of traditional optimization passes — along with many less efficient representations. E-graphs are more efficient than directly storing every possible alternative (taking O(log n) space on average), but they still take more memory than a traditional intermediate representation. Depending on the program in question and the set of optimizations employed, a fully saturated E-graph could be arbitrarily large. In practice, Cranelift sets a limit on how many operations are performed on the graph to prevent it from becoming too large. E-graphs pay for this simplicity and optimality when it comes time to extract the final representation from the E-graph to use for code generation. Extracting the fastest representation from an E-graph is an NP-complete problem. Cranelift uses a set of heuristics to quickly extract a good-enough representation. Trading one NP-complete problem (selecting the best order for a set of passes) for another may not seem like a large benefit, but it does make sense for a smaller project. The order of optimization passes is largely set by the programmers who write the optimizations, because it requires domain knowledge to pick a reasonable sequence. Extracting an efficient representation from an E-graph, on the other hand, is a generic search problem that can have as much or as little computer time applied to it as the application permits. Cranelift's heuristics don't extract the most efficient representation, but they do a good job of quickly extracting a decent one. Representing optimizations in this way also makes it easier for Cranelift maintainers to understand and debug existing optimizations and their effects, and makes writing new optimizations somewhat simpler. Cranelift has a custom domain-specific language (ISLE) that is used internally to specify optimizations. While Cranelift does not organize its optimizations in phases, it does have ten different sets of related optimizations defined in their own ISLE files, which allows for a rough comparison with GCC and LLVM. LLVM lists 96 optimization passes in its documentation, while GCC has 372. The optimizations that Cranelift does have include constant propagation, bit operation simplifications, vectorization, floating-point operation optimizations, and normalization of comparisons. Dead-code elimination is done implicitly by extracting a representation from the E-graph. A paper from 2020 showed that Cranelift was an order of magnitude faster than LLVM, while producing code that was approximately twice as slow on some benchmarks. Cranelift was still slower than the paper's authors' custom copy-and-patch JIT compiler, however. Cranelift for Rust Cranelift may have been designed with the aim of being an alternate backend for Rust, but actually making it usable has taken significant effort. The Rust compiler has an internal representation (IR) called mid-level IR that it uses to represent type-checked programs. Normally, the compiler converts this to LLVM IR before sending it to the LLVM code-generation backend. In order to use Cranelift, the compiler needed another library that takes mid-level IR and emits CLIF. That library was largely written by \"bjorn3\", a Rust compiler team member who contributed more than 3,000 of the approximately 4,000 commits to Rust's Cranelift backend. He wrote a series of progress reports detailing his work. Development began in 2018, and kept pace with Rust's own rapid development. In 2023, the backend was considered stable enough to ship as part of Rust nightly as an optional toolchain component. People can now try the Cranelift backend using rustup and cargo: $ rustup component add rustc-codegen-cranelift-preview --toolchain nightly $ export CARGO_PROFILE_DEV_CODEGEN_BACKEND=cranelift $ cargo +nightly build -Zcodegen-backend The given rustup command adds the Cranelift backend's dynamic library to the set of toolchain components to download and keep up to date locally. Setting the CARGO_PROFILE_DEV_CODEGEN_BACKEND environment variable instructs cargo to use Cranelift for debug builds, and the final cargo invocation builds whatever Rust project lives in the current directory with the alternate code-generation backend feature turned on. The latest progress report from bjorn3 includes additional details on how to configure Cargo to use the new backend by default, without an elaborate command-line dance. Cranelift is itself written in Rust, making it possible to use as a benchmark to compare itself to LLVM. A full debug build of Cranelift itself using the Cranelift backend took 29.6 seconds on my computer, compared to 37.5 with LLVM (a reduction in wall-clock time of 20%). Those wall-clock times don't tell the full story, however, because of parallelism in the build system. Compiling with Cranelift took 125 CPU-seconds, whereas LLVM took 211 CPU-seconds, a difference of 40%. Incremental builds — rebuilding only Cranelift itself, and none of its dependencies — were faster with both backends. 66ms of CPU time compared to 90ms. Whether Cranelift will ameliorate users' concerns about slow compile times in Rust remains to be seen, but the initial signs are promising. In any case, Cranelift is an interesting showcase of a different approach to compiler design. Did you like this article? Please accept our trial subscription offer to be able to see more content like it and to participate in the discussion. (Log in to post comments) Cranelift code generation comes to Rust Posted Mar 15, 2024 22:29 UTC (Fri) by willy (subscriber, #9762) [Link] So what you're saying is that Cranelift is less parallel than LLVM? ;-) Yes, I'm being facetious, but the metric that is important to me as a programmer is \"How many seconds am I waiting for the test to run\" (edit/compile/debug cycle). I don't really care how many CPU-seconds are consumed. Amdahl's Law applies, of course, but if a compiler wants to take advantage of 15 extra cores to improve the quality of the code, I have no problem with this. Cranelift code generation comes to Rust Posted Mar 16, 2024 10:51 UTC (Sat) by HadrienG (subscriber, #126763) [Link] If I understand the article correctly, the cranelift design is actually more amenable to parallelization than the LLVM one. In a fixed pass order design like LLVM, there is an inherent sequential dependency chain, where each pass must run to completion before another pass can start. Each pass can, in principle, use parallelism internally, but usually parallelizing tiny workloads with running times in milliseconds leads to disappointing results due to task spawning and synchronization overheads dwarfing all the benefits of extra parallelism. In cranelift's E-graph based design, on the other hand, it is in principle possible to repeatedly run all passes in parallel on the current E-graph[1] until a fixed point is reached. This will not use CPU time as efficiently as running them sequentially because each optimization pass will see less new input E-graph nodes on each run, and more pass runs will be needed to reach the fixed point, which will increase the costs associated with starting/ending passes. But if you are latency bound (no other compilation unit is being built concurrently), using CPUs inefficiently can be better than not using them at all. Assuming this pass parallelization scheme works well, the running time would eventually be bottlenecked by the final fastest e-graph representation selection step, but if this pass were parallelizable too (and it is if it works by assigning each node a score and searching for the lowest score, or by comparing nodes with each other), that's not an issue. Ultimately, assuming sufficient L3 cache capacity, larger builds will probably get better overall performance by using coarser-grained compilation unit based parallelism. I wonder how well build systems will cope with the mixing of different levels of parallelism that combining multiple compilation units with parallel compilation of individual compilation units produces. --- [1] Fine-grained E-graph write synchronization can be used to ensure that each pass sees as many E-graph nodes from other passes as possible on startup, reducing the number of times each pass needs to run and the number of global joins (wait for all passes to finish before moving on) at the expense of a more complex synchronization protocol that will slow down individual accesses to the E-graph. Cranelift code generation comes to Rust Posted Mar 16, 2024 1:01 UTC (Sat) by jalla (subscriber, #101175) [Link] why are there ads on lwn? Cranelift code generation comes to Rust Posted Mar 16, 2024 1:43 UTC (Sat) by mattdm (subscriber, #18) [Link] > why are there ads on lwn? https://lwn.net/op/FAQ.lwn#textads Cranelift code generation comes to Rust Posted Mar 16, 2024 2:05 UTC (Sat) by jalla (subscriber, #101175) [Link] Ah ha! So someone ordered some banners and they're ruining the subscriber experience. thank you for your help Cranelift code generation comes to Rust Posted Mar 16, 2024 16:03 UTC (Sat) by ghodgkins (subscriber, #157257) [Link] > LWN subscribers at the \"professional hacker\" level and above can disable ads by going into the account management area and selecting \"Customization.\" Cranelift code generation comes to Rust Posted Mar 16, 2024 12:34 UTC (Sat) by swig-flail-tricky-sterling (subscriber, #170190) [Link] C'mon, the LWN authors gotta eat like the rest of us Cranelift code generation comes to Rust Posted Mar 17, 2024 0:57 UTC (Sun) by intelfx (subscriber, #130118) [Link] > C'mon, the LWN authors gotta eat like the rest of us I mean, this is a subscription-only article that the readers are _paying_ to see… Cranelift code generation comes to Rust Posted Mar 16, 2024 16:24 UTC (Sat) by zdzichu (subscriber, #17118) [Link] Are you implying this article is an ad? LLVM ist a mess Posted Mar 16, 2024 16:03 UTC (Sat) by Curan (subscriber, #66186) [Link] Almost OT here, but LLVM is a real mess in my opinion. I have to use it for Mesa and the whole GPU/ML ecosystem and every other week something breaks. The packages from apt.llvm.org almost always need hand-holding and fixups. The amount of time I put into keeping my CI/CD pipelines vs. \"latest\" LLVM running is insane. LLVM ist just not a stable platform you can develop against. So many of my patches for various projects, including Mesa, are just fixups to make the builds work again with recent LLVM versions (internal/private stuff is even crazier, but I can't show, of course). I can accept that major versions break things, but if you need to break core concepts this often, you probably made a lot of mistakes in the past (and seeing how this kind of breakage is not slowing down, the project doesn't seem to wise up either). A lot of LLVM feels like it is a test environment to try out new things for the compiler space (which is great, don't get me wrong), but then it shouldn't be the basis of anything else. The one thing I'll never understand is how so many parts of the Khronos/Mesa ecosystem (and others, including Rust and WebAssembly) can depend on such an unstable platform. And before anybody says \"just stick with some stable LLVM version\": you really don't want to be stuck on an old LLVM version, because it almost always will hold your workload back. Which means you have to update. I really do see major upgrades in performance here. Most of it is backend-work, that would be possible without the breakage. Long story short: as a non-compiler developer I hate LLVM with a vengeance. On the other hand I do understand what LLVM is offering. clang is sometimes producing better binaries than GCC and – depending on what you want – it is easier to experiment with LLVM than GCC. GCC is a classic GNU project with all of the baggage that entails (really wish they would abandon at least their GNU Make system and move to something sensible like CMake), but at least you don't have to worry about breakage. Even with libgccjit. LLVM ist a mess Posted Mar 16, 2024 21:13 UTC (Sat) by Wol (subscriber, #4433) [Link] I think part of the problem with llvm may *still* be that it was written for C/C++. Aiui, they had to re-write large chunks because of all the C assumptions that weren't true of Rust, and it kept breaking Rust's guarantees. That might still be crawling out the woodwork. Cheers, Wol LLVM ist a mess Posted Mar 17, 2024 10:01 UTC (Sun) by khim (subscriber, #9252) [Link] It's just simply research projects acts like a research project. As you were advocating in other discussion that means everyone who uses it just have to get their act together and fork (or write something from scratch). LLVM ist a mess Posted Mar 17, 2024 18:57 UTC (Sun) by Wol (subscriber, #4433) [Link] > that means everyone who uses it just have to get their act together and fork Drop the \"and fork\", please. Okay, you can if you want, but the BEST approach (which is what the Rust guys did, I assume) is to *branch* it, fix it, and send pull requests upstream. If upstream ignores them, then you have to decide what you want to do about it, but what you do NOT do is launch a self-entitled petulant shit-storm at upstream because their priorities are different from yours. If it ends with a full fork, then that's sad, but then you have two - hopefully friendly - projects sharing code, but with different priorities and aims. So be it ... Cheers, Wol LLVM ist a mess Posted Mar 17, 2024 9:59 UTC (Sun) by khim (subscriber, #9252) [Link] > if you need to break core concepts this often, you probably made a lot of mistakes in the past Nope. Linux kernel breaks internal interfaces pretty often, too. The only problem of LLVM is that it's advertised as something of a separate project while in reality it's only half of many projects. If all these projects would have lived in one repo and people would have changed everything in sync it wouldn't have even been visible. > LLVM ist just not a stable platform you can develop against That's the core issue: it was never designed as such. Clang/LLVM developers even explicitly said that you shouldn't try to use it as a stable platform. But lots of companies wanted stable compiler platform and they decreed that LLVM is it against developer's wishes and insistence. > a lot of LLVM feels like it is a test environment to try out new things for the compiler space Which is precisely what LLVM was designed for. Just open Wikipedia and read: LLVM was originally developed as a research infrastructure to investigate dynamic compilation techniques for static and dynamic programming languages. From what you are saying LLVM works and acts like it was designed to work and act so why is that an issue? > but then it shouldn't be the basis of anything else Build “better basis for anything else”, isn't that the right solution? Maybe as LLVM fork or write from scratch. I was told in no-uncertain terms in somewhat tangetially related discussion just over there that you have zero right to complain since LLVM is free. > The one thing I'll never understand is how so many parts of the Khronos/Mesa ecosystem (and others, including Rust and WebAssembly) can depend on such an unstable platform. License. Writing compilers is hard and time-consuming process. Thus there are, realistically, only two choices: LLVM and gcc (via libgccjit). And pointy-haired-bosses out there don't like GPL so LLVM was chosen. Initially they even mandated the use bitcode which produced many stillborn projects (pNaCl, RenderScript and bitcode iOS apps, to name a few), after they realized that developers weren't joking and they couldn't force them to do what they never promised to do bitcode use was abandoned, but since no replacement was available LLVM use continued. LLVM ist a mess Posted Mar 17, 2024 16:03 UTC (Sun) by jem (subscriber, #24231) [Link] >LLVM was originally developed as a research infrastructure to investigate dynamic compilation techniques for static and dynamic programming languages. Note the word \"originally\". That was 21 years ago, and the sentence does not imply it still is nothing more than a research project. On the official LLVM web site we can read \"LLVM began as a research project[...] Since then LLVM has grown to be an umbrella project consisting of a number of subprojects, many of which are being used in production by a wide variety of commercial and open source projects[...]\" LLVM ist a mess Posted Mar 17, 2024 16:20 UTC (Sun) by khim (subscriber, #9252) [Link] Beyond certain size it's incredibly hard to change the nature of a project. Like Windows 11, which includes certain design decisions which may be traced back to design decisions made more than half-century ago when TOPS-10 was designed many things in LLVM are still in the shape needed to be a research project. LLVM ist a mess Posted Mar 17, 2024 16:32 UTC (Sun) by tialaramex (subscriber, #21167) [Link] In particular though LLVM's IR has a lot of places where either they say \"We do X\" but actually \"Oh, we just assumed C++ even though that's not what we wrote\" or they just don't explain and when you ask \"Um, I guess it's whatever C++ requires\". Part of this leaks from Clang and has been significantly corrected by the competing requirements from the Rust compiler team but part of it is clearly more intrinsic to LLVM as a project - they have people who think in C++ when they're supposed to be thinking in terms of LLVM's IR. There's a sloppiness that I'd expect from C++ people and I think is less prevalent for Cranelift. I think \"We assumed C++\" is a problem for a research project too. Lots of interesting new work from the last few decades can't happen if you're just \"assuming C++\" everywhere, what you get out is \"Oh well, apparently it's impossible to do better than C++\" because you've assumed that's all that's possible. LLVM ist a mess Posted Mar 17, 2024 17:49 UTC (Sun) by farnz (subscriber, #17727) [Link] And even when they didn't assume C++, there's often been bugs that boil down to \"Clang doesn't use this much, therefore it's not routinely tested and there's lot of lurking bugs\"; see the fun Rust has had trying to use noalias on references, where because the matching Clang feature (the C99 restrict type qualifier) is rarely used correctly, miscompilations by LLVM traceable to noalias in LLVM IR kept blocking Rust from using it for Rust references (which definitionally can't alias each other). LLVM ist a mess Posted Mar 17, 2024 19:09 UTC (Sun) by Wol (subscriber, #4433) [Link] Is LLVM written in C++? Should they re-write it in Rust? :-)) Cheers, Wol LLVM ist a mess Posted Mar 17, 2024 19:53 UTC (Sun) by farnz (subscriber, #17727) [Link] Implementation language isn't at the root of this; the underlying issue is that LLVM IR's semantics aren't (yet) formally defined, but instead rely on informal reasoning throughout LLVM. As a consequence, it's intractable to verify that LLVM actually implements the claimed semantics, and it's not reasonable to write test cases that validate the LLVM IR semantics are met in edge cases, since we don't actually know what the edge cases are. There's efforts afoot to fully define LLVM IR semantics formally, and one of the biggest outputs those efforts are having (at the moment) is finding bugs in existing LLVM functionality, where existing LLVM code assumes opposing meanings (that both fit the informally defined semantics) for the same LLVM IR construct in different places. LLVM ist a mess Posted Mar 17, 2024 17:06 UTC (Sun) by willy (subscriber, #9762) [Link] Writing a compiler is not a hard problem. Evidence: https://bellard.org/tcc/ (and many other compilers). What is hard is creating a thriving project that has many people who are dedicated to finding & fixing the glass jaws. There's also a question of how much optimisation you really need; TCC takes that to an extreme, but maybe it's a useful extreme. LLVM ist a mess Posted Mar 17, 2024 19:56 UTC (Sun) by roc (subscriber, #30627) [Link] \"Writing a compiler is not a hard problem\" is too ambiguous to be useful. The gulf between a minimal C compiler (TCC) and an optimizing cross-platform C++ compiler with all the bells and whistles (good error messages, sanitizers, etc etc etc etc) is so vast you're not talking about the same thing at all. LLVM ist a mess Posted Mar 17, 2024 20:25 UTC (Sun) by tialaramex (subscriber, #21167) [Link] One compiler application which feels intuitively useful to me (but I'm not a language designer) would be to have a _non-optimising_ compiler which can translate from a suitable language to dependable constant time machine code for some N architectures where N > 1 The purpose would be to wean ourselves off machine code for writing core cryptographic libraries. It would be nice if the sort of people who enter NIST competitions could write this rather than C but it's not crucial. In this application we actually don't want ordinary optimisation, so I suspect some (many?) optimisation strategies are invalid and it may be faster to begin from almost nothing. LLVM ist a mess Posted Mar 17, 2024 22:07 UTC (Sun) by khim (subscriber, #9252) [Link] > One compiler application which feels intuitively useful to me (but I'm not a language designer) would be to have a _non-optimising_ compiler which can translate from a suitable language to dependable constant time machine code for some N architectures where N > 1 You do realize that for modern CPUs “architecture”, here, would include not just CPU vendor, but stepping, version of microcode, etc? One trivial example: when Intel implemented BMI instructions in 2013 they had nice, constant, execution time, but AMD turned them into nice let's leak all your data to everyone to see version after four years and every microcode update (on both AMD and Intel) may do the same to any instruction — to patch some other vulnerability. > In this application we actually don't want ordinary optimisation, so I suspect some (many?) optimisation strategies are invalid and it may be faster to begin from almost nothing. Before you may even begin attempting something like this you would need to define what do you want in the end. Given the fact that give enough samples you may even distinguish between (xor %eax,%eax and mov $1,%eax (they affect flags and one is 2bytes while other is is 5bytes) first you would need to define some metric which would say if timings are “sufficiently similar” or not. The whole thing looks like an incredible waste of manpower: instead of trying to achieve something that's not possible to, realistically, achieve on modern CPUs we should ensure that non-ephemeral keys are generated on dedicated core. Adding tiny ARM core (Cell-style) would be much easier and more robust than attempts to create such compiler. Constant-time cryptography Posted Mar 18, 2024 7:05 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] This is not the first time I have seen this suggestion. It is also completely non-viable in practice. The security core will be much slower than the other cores, which will ruin performance. One can avoid that by using a hardware accelerator instead of a slow core, but then one needs to (a) patch all of the existing applications and libraries to use the accelerator and (b) deal with the fact that hardware accelerators, especially for symmetric cryptography, require an asynchronous API to get good performance. That requires application changes, not just library ones. Hardware crypto engines are nice, but they are not at all a substitute for constant time guarantees for software operations. Constant-time cryptography Posted Mar 18, 2024 8:55 UTC (Mon) by khim (subscriber, #9252) [Link] > Hardware crypto engines are nice, but they are not at all a substitute for constant time guarantees for software operations. Oh, sure. Hardware works. “Constant time guarantees” are a snake oil you may lucratively sell. Completely different products with different properties and target audience. > That requires application changes, not just library ones. So you can't even change apps, yet, somehow, pretend that they are not leaking your precious key in some other way except for operations being of different speeds depending on source? You keys are not leaking (or maybe leaking but you just don't know that) because nobody targets you. It's as simple as that. LLVM ist a mess Posted Mar 18, 2024 9:01 UTC (Mon) by pm215 (subscriber, #98099) [Link] Modern CPUs, at least for Intel and Arm, have an architecturally defined data independent timing mode that you can enable in a status register bit when you want to execute this kind of crypto code, and which then guarantees that execution timing of a specified subset of instructions is not dependent on the data they are operating on. So I think the situation is not so bleak as you suggest: there's now a defined set of \"stay within these boundaries and things won't change in future designs or microcode updates\" rules. LLVM ist a mess Posted Mar 18, 2024 9:08 UTC (Mon) by khim (subscriber, #9252) [Link] > Modern CPUs, at least for Intel and Arm, have an architecturally defined data independent timing mode that you can enable in a status register bit when you want to execute this kind of crypto code, and which then guarantees that execution timing of a specified subset of instructions is not dependent on the data they are operating on. They still would depend on alignment of you data and code, on speculative properties of code which was executed before and after you call that “well crafted” code and so on. Just look on continuous struggle to guarantee that SGX is useful for something. With another vulnerability revealed less than week ago. Ultimately the solution would be the same as with memory security in C: solution that was obvious on the day one would be applied… but only after everything else would be unsuccessfully tried. LLVM ist a mess Posted Mar 18, 2024 15:18 UTC (Mon) by paulj (subscriber, #341) [Link] \"writing a [blah] is not a hard problem\" -> links to something written by one of the most prodigious, genius authors of Free Software. Hmm... ;) LLVM ist a mess Posted Mar 18, 2024 16:28 UTC (Mon) by willy (subscriber, #9762) [Link] You've completely missed the point of that comment, but to refute the uninteresting part you're quibbling with: https://student.cs.uwaterloo.ca/~cs444/ Team of four students builds a compiler in three months. LLVM ist a mess Posted Mar 18, 2024 18:15 UTC (Mon) by paulj (subscriber, #341) [Link] It was mostly humour. I too, like most students who've done a CS degree, have had to write some kind compiler for a class assignment (a toy front-end with type-checker for a subset of a typed JS-ish language). I've written compilers for simple DSLs - in AWK even ;). A compiler itself is not /that/ hard - completely agreed. I was just pointing out the humour in the irony of making that point via an example written by an author who has a (prodigious) habit of solving difficult problems. ;) I agree though that, even if a basic compiler is simple, there is a /lot/ more to making a _good_ C/C++ compiler. LLVM ist a mess Posted Mar 18, 2024 23:48 UTC (Mon) by NYKevin (subscriber, #129325) [Link] > I too, like most students who've done a CS degree, have had to write some kind compiler for a class assignment (a toy front-end with type-checker for a subset of a typed JS-ish language). Fun fact: If your professor is sufficiently insane, it is possible that you will end up having to write an interpreter for the (untyped) lambda calculus. So count yourself lucky that you got a language that actually looked vaguely modern. OTOH, I must admit that the lambda calculus is much, *much* easier to implement than most real languages. It only has 2½ rules, or 1½ if you use De Bruijn indexing. But I would've liked to do a real language, or at least something resembling a real language. I often feel that the most difficult courses were the only ones that actually taught me anything useful. LLVM ist a mess Posted Mar 19, 2024 4:42 UTC (Tue) by buck (subscriber, #55985) [Link] > I was just pointing out the humour in the irony of making that point via an example written by an author who has a (prodigious) habit of solving difficult problems. ;) Well, I'm with you: a compiler written by Fabrice Bellard is not your run-of-the-mill hobby project. But, I'm really more just pointing out that this still blows me away: https://bellard.org/jslinux/index.html which i think saves this comment from being (rightly) criticized for being OT, since it's more Linux-y than the article, and this is LWN after all, dang it. Well, to get this right back on topic, i can actually just point out that JSLinux features tcc and gcc but not clang: localhost:~# cat readme.txt Some tests: - Compile hello.c with gcc (or tcc): gcc hello.c -o hello ./hello - Run QuickJS: qjs hello.js - Run python: python3 bench.py localhost:~# b/c the performance win: [`time gcc -o hello -c hello.c -O0` output elided to spare my old laptop's feelings] LLVM ist a mess Posted Mar 19, 2024 4:47 UTC (Tue) by buck (subscriber, #55985) [Link] Sorry. Let me stand corrected by myself: Quoth https://bellard.org/jslinux/news.html: 2020-07-05: Added the Alpine Linux distribution. Many packages are included such as gcc, Clang, Python 2.7 and 3.8, Node.js, Ruby, PHP, ... The more adventurous (and patient) people can also try to run Wine or Firefox. Added SSE2 support to the x86 emulator Added dynamic resizing of the terminal LLVM ist a mess Posted Mar 18, 2024 18:02 UTC (Mon) by farnz (subscriber, #17727) [Link] I've written more than one compiler, and I would not count myself as a Fabrice Bellard level developer. You should be able to write a simple optimizing C compiler following a book like this in about 3 months full-time effort if you're a competent developer (less if you're willing to reuse tools like gas and ld rather than doing everything yourself). LLVM ist a mess Posted Mar 18, 2024 19:57 UTC (Mon) by Cyberax (✭ supporter ✭, #52523) [Link] Writing a \"toy C\" compiler had been a course project in my university. It was C with most of its functionality, except things that are baffling, like syntactic ambiguity between function pointers and definitions, dangling else, etc. It really is not a hard problem. Annoying and somewhat long, but not hard. LLVM ist a mess Posted Mar 19, 2024 5:40 UTC (Tue) by adobriyan (subscriber, #30858) [Link] And you'll learn operator precedence table for FREE! LLVM ist a mess Posted Mar 19, 2024 5:56 UTC (Tue) by Cyberax (✭ supporter ✭, #52523) [Link] I actually promptly forgot it. I always just spam the code with braces to disambiguate anything that is more complex than 2*2+1. LLVM ist a mess Posted Mar 18, 2024 6:55 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] I think part of the problem is that distributions and downstream try to use LLVM as a system library, instead of having each downstream bundle a known-good version. If each user of LLVM bundled it, these problems would go away, at the expense of significantly longer build times. I know that Rust has a fork of LLVM. I believe this is partly because sometimes they need to fix miscompiles and can’t wait for upstream to take the patch. LLVM ist a mess Posted Mar 18, 2024 12:43 UTC (Mon) by intelfx (subscriber, #130118) [Link] > If each user of LLVM bundled it, these problems would go away Ah yes. I believe it is time for a revised version of the \"layers of abstraction\" maxim: every releng problem may be solved by pinning and bundling, except for the problem of too much pinning and bundling. LLVM ist a mess Posted Mar 18, 2024 23:49 UTC (Mon) by NYKevin (subscriber, #129325) [Link] Meh, just use block-layer deduplication and/or a content-addressed filesystem, and then all of the logical duplication works out to relatively little physical duplication. Cranelift code generation comes to Rust Posted Mar 16, 2024 20:17 UTC (Sat) by willy (subscriber, #9762) [Link] I came across this blog entry which seems relevant to people who are interested in this topic: https://kobzol.github.io/rust/rustc/2024/03/15/rustc-what... Cranelift code generation comes to Rust Posted Mar 17, 2024 17:39 UTC (Sun) by rvolgers (subscriber, #63218) [Link] Other Rust code generation backends include the GCC one, which is nearing some kind of usable status and hopes to become installable via rustup within the next months (https://blog.antoyo.xyz/rustc_codegen_gcc-progress-report-31), as well as an extremely experimental one to compile using Microsoft's .Net framework (since apparently .Net bytecode is also capable of representing unsafe operations). Much faster! Posted Mar 18, 2024 21:01 UTC (Mon) by proski (subscriber, #104) [Link] I tried to compile alacritty (a terminal program) using Cranelift. It compiles much faster. It really makes the difference for the development, when I want to test my changes quickly. The resulting binary would panic on startup. The backtrace points to some unsafe code in `fontconfig-rs`. It could actually be a bug in that code. It's nice to have another tool that can find issues in unsafe code. Miri is a tool specifically for that purpose, but it has too many limitations. Copyright © 2024, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=39742692",
    "commentBody": "Cranelift code generation comes to Rust (lwn.net)369 points by ridruejo 22 hours agohidepastfavorite95 comments CodesInChaos 18 hours agoYou can use different backends and optimization for different crates. It often makes sense to use optimized LLVM builds for dependencies, and debug LLVM or even Cranelift for your own code. See https://www.reddit.com/r/rust/comments/1bhpfeb/vastly_improv... reply zozbot234 16 hours agoparentI would not expect this to work without issues, as Rust does not support a stable binary ABI across different compiler versions. How can we be sure that the two \"codegen backends\" will always be implementing the same binary ABI? reply aseipp 14 hours agorootparentBecause the ABI is defined and implemented by shared code in the runtime and compiler. There is no need for cross version compatibility between the two backends, only compatibility within the same version. This isn't particularly new either, FWIW. The Glasgow Haskell Compiler also has an unstable ABI that is not standardized, but LLVM compiled code interoperates seamlessly with code generated by the non-LLVM compiler backend (the mechanism by which that is achieved is likely different though.) reply sanxiyn 16 hours agorootparentprevIn theory yes, this could be problematic with alternative Rust implementations, but it works fine and many people are using it in this case. We can be sure LLVM and Cranelift codegen backends will be implementing the same binary ABI, because binary ABI decisions are made in the shared code. reply devit 14 hours agorootparentprevPresumably the LLVM and Cranelift backends in the same rustc version generate code with the same ABI, and it would be a bug if that wasn't the case. reply aw1621107 16 hours agorootparentprevI suppose that might depend on how the ABI is represented internally. If the ABI is fully described by (one of?) the lowered IR the backends consume (e.g., does MIR fully describe struct layouts/etc.) then I wouldn't expect there to be any issues outside \"regular\" bugs since all the relevant information would be contained in the inputs. reply sanxiyn 15 hours agorootparentIt is done in the shared code, see https://rustc-dev-guide.rust-lang.org/backend/backend-agnost... for details. reply CodesInChaos 14 hours agorootparentStruct layout happening in generic code makes sense (and is actually required since you can reference it in `const`s). It seems unlikely that they made function calling fully backend agnostic, since it'd require assigning the registers for parameters and result in generic code, and not in the backend. I'd expect the generic code to lower the function parameters to primitive types (pointers, ints, floats, etc.), but the backend would then distribute those over registers and/or stack. Keeping that compatible would still require an (unstable) specification implemented by all compatible backends. Unwinding might be tricky as well. reply vlovich123 9 hours agorootparent> I'd expect the generic code to lower the function parameters to primitive types (pointers, ints, floats, etc.), but the backend would then distribute those over registers and/or stack Not sure about that. Calling conventions are defined by the platform ABI which is what the backend implements, so any conforming backend should still be mutually invokable. That's why a Rust program can emit an ABI-stable C API and why you can call GCC built libraries from an LLVM built executable. The lowering of parameters to registers is constrained by this because the intermediate representation understands that what are parameters to functions & then follows the platform ABI to lower it to function calls. reply aw1621107 12 hours agorootparentprevAh, I think that's about what I had in mind. Just didn't know what to look for. Thanks! reply Ar-Curunir 16 hours agorootparentprevThis is for local work, so you use the same compiler for your dependencies and for your own code. Only the codegen backend differs. reply PartiallyTyped 14 hours agorootparentprevIf you do the compilation locally, you know the structure of the ABI for the compiled code. You know that regardless of the backend you are using. At this stage, no functions with generics are compiled, only what is non-generic. What is left is to account for the ABI in any monomorphizations that occur at the boundary, i.e. when your own structures monomorphize generic functions in the dependency. When the compiler creates this monomorphic variant, and lowers down, it can provide the necessary details of the ABI. reply chrisaycock 21 hours agoprevThis article provides an excellent overview of the latest in speed of optimizer vs quality of optimization. In particular, copy-and-patch compilation is still the fastest approach because it uses pre-compiled code, though leaves little room for optimization. Cranelift uses e-graphs to represent equivalence on the IR. This allows for more optimizations than the copy-and-patch approach. Of course, the most optimized output is going to come from a more traditional compiler toolchain like LLVM or GCC. But for users who want to get \"fast enough\" output as quickly as possible, newer compiler techniques provide a promising alternative. reply weinzierl 21 hours agoparentFrom what I understand, the big advantage of the e-graphs approach is, that the quality of the output is (within limits) a function the time and memory given. The more memory, the more nodes can be generated in the e-graph and the more time for search, the better the selected node. It might never be as fast as copy-and-patch or as good as LLVM or GCC, but this flexibility is a value in itself. reply cfallin 8 hours agorootparentWe actually take a fairly unconventional approach to e-graphs: we have a few linear passes and we do all rewrites eagerly, so we use them to provide a general framework for the fixpoint problem into which we plug in all our rewrites, but we don't have the usual \"apply as much CPU time as you want to get better results\" property of conventional equality saturation. I gave a talk about this approach, aegraphs (acyclic e-graphs), here: slides (https://cfallin.org/pubs/egraphs2023_aegraphs_slides.pdf), video (https://vimeo.com/843540328) (disclosure: Cranelift tech lead 2020-2022 and main author of the e-graphs mid-end, as well as regalloc, isel and its custom DSL, and other bits, along with the excellent team) reply thechao 13 hours agorootparentprevWhen we first reviewed the equality saturation paper, we thought there was one major pro, and one major con: [pro] phase-ordering invariant; [con] at the time there was no (believable) way to extract the transformed graph in a non-hand-wavy-way. Personally, I think e-graphs should be combined with Massalin superoptimization — they're natural \"duals\" — and just turn the whole exercise into a hill-climbing process. You can tune the total effort by the set of passes used, the amount of time to drive the graph to saturation, and the method (and time) for graph extraction. reply vlovich123 9 hours agorootparentCan you memoize across invocations so that the time spent optimizing is cumulative across all builds? reply vsnf 19 hours agoparentprevIs there any literature or guidelines on what to do if I'm willing to spend effectively unlimited CPU cycles in return for a more optimized final output? reply thesz 19 hours agorootparentSuperoptimizers: https://en.wikipedia.org/wiki/Superoptimization Also, program distillation: https://www.researchgate.net/publication/220989887_Distillat... reply mort96 14 hours agorootparentIsn't it a bit weird that this isn't just the standard? Like imagine if Chrome was optimized with such a superoptimizer; let the optimizer spend a couple hours every month or so when cutting a new release. Surely that has to be worth it? reply MaxBarraclough 12 hours agorootparentI'm not a compiler engineer, but I think it's a diminishing returns issue. Modern optimising compilers are already impressive, and they get more impressive year on year, but only quite slowly. We don't see compilers producing code that runs 30% faster than the code generated last year, for instance. Such improvements can happen with compiler updates, but not when the starting point is a compiler that's already of decent quality. I recall some GPU driver updates from ATi (years ago) delivered pretty drastic performance improvements, but I believe that's because the original drivers were rather primitive. (Perhaps a drastic improvement to autovectorisation could give a 30% boost, or better, but this would apply only to certain programs.) You could grant a compute budget 100x the typical build set-up, but no one has built a production-ready compiler to take advantage of that, and if they did I suspect the improvements would be unimpressive. They may also run into a 'complexity ceiling' issue, as the compiler would presumably be even more complex than today's ordinary optimising compilers, which are already enormous. As Filligree says, superoptimisers tend to only be practical for very short programs. They can't be applied to monstrous codebases like Chromium. reply thesz 11 hours agorootparentThe ability of compilers to make code faster is 12 (twelve) times slower than Moore's law: they double the program's speed in 18 years [1]. [1] https://proebsting.cs.arizona.edu/law.html This might seem discouraging, but it is not - one can still reap the benefits of code optimization twelve times as long after Moore's law stops working. reply alserio 11 hours agorootparentprevBut maybe you can superoptimize some hot sections, and encode the superoptimizer findings somewhere. Then the compiler can validate the optimizations and apply them to the particular piece of code for the rest of the program life, untill the preconditions hold. reply kibwen 13 hours agorootparentprevDoing a full-fledged release-this-to-millions-of-users build of Chrome or Firefox already takes on the order of 24 hours (or at least it did when last I checked a few years ago). reply Filligree 12 hours agorootparentprevIs it just a couple hours? Even just ordering the optimisation passes is already an NP-complete process, so I could easily imagine superoptimisers would take trillions of years for a large program... reply kccqzy 8 hours agorootparentprevChrome already uses PGO: https://blog.chromium.org/2020/08/chrome-just-got-faster-wit... reply remorses 13 hours agorootparentprevI am pretty sure the existing release process already takes much more than 2 hours. reply sanxiyn 19 hours agorootparentprevHave a look at Unison which uses constraint programming to do optimal code generation. https://unison-code.github.io/ reply kapilsinha 21 hours agoparentprevAgree, especially when developing, I'd assume speed of optimizer matters more than quality of optimization. I wonder though if LLVM spent less time on that \"phase ordering\" problem, what is the tradeoff between these two factors? reply boomanaiden154 18 hours agorootparentLLVM doesn’t spend really any runtime solving the phase ordering problem since the pass pipelines are static. There have been proposals to dynamically adjust the pipeline based on various factors, but those are a ways out from happening. reply ad-ops 18 hours agoprevI see that there are many comments on full debug builds, but for me the most important difference are incremental build times when making minor changes. In my opinion this is what speeds up the development iterations. Here are my build times when making a trivial change to a print-statment in a root function, comparing nightly dev vs adding cranelift + mold for rust-analyzer[0] (347_290 LoC) and gleam[1] (76_335 LoC): $ time cargo build Compiling rust-analyzer v0.0.0 (/home/user/repos/rust-analyzer/crates/rust-analyzer) # nightly Finished `dev` profile [unoptimized] target(s) in 6.60s cargo build 4.18s user 2.51s system 100% cpu 6.650 total # cranelift+mold Finished `dev` profile [unoptimized] target(s) in 2.25s cargo build 1.77s user 0.36s system 92% cpu 2.305 total Compiling gleam v1.0.0 (/home/user/repos/gleam/compiler-cli) # nightly Finished `dev` profile [unoptimized + debuginfo] target(s) in 4.69s cargo build --bin gleam 3.02s user 1.74s system 100% cpu 4.743 total # cranelift+mold Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.99s cargo build --bin gleam 0.71s user 0.20s system 88% cpu 1.033 total For me this is the most important metric and it shows a huge improvement. If I compare it to Go building Terraform[2] (371_594 LoC) it is looking promising. This is a bit unfair since it is the release build for Go and this is really nice in the CI/CD. Love Go compilation times and I thought it would be nice to compare with another language to show the huge improvements that Rust has made. $ time go build go build 3.62s user 0.76s system 171% cpu 2.545 total I was looking forward to parallel front-end[3], but I have not seen any improvement for these small changes. [0] https://github.com/rust-lang/rust-analyzer [1] https://github.com/gleam-lang/gleam [2] https://github.com/hashicorp/terraform [3] https://blog.rust-lang.org/2023/11/09/parallel-rustc.html *edit: code-comments & links + making it easier to see the differences reply ad-ops 17 hours agoparentFor completion here is the compilation time for a small project from the axum/examples[0] (125 LoC), also comparing nightly dev vs adding cranelift + mold: $ time cargo build Compiling example-todos v0.1.0 (/home/user/ws/rust/example-todos) Finished `dev` profile [unoptimized + debuginfo] target(s) in 1.65s cargo build 1.49s user 0.58s system 123% cpu 1.685 total Compiling example-todos v0.1.0 (/home/user/ws/rust/example-todos) Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.55s cargo build 0.47s user 0.13s system 102% cpu 0.590 total [0] https://github.com/tokio-rs/axum/tree/main/examples/todos reply kapilsinha 14 hours agoparentprevThis is most definitely self-promotion (I am the author), but also extremely relevant if you are looking at incremental build speed-ups: https://news.ycombinator.com/item?id=39743431 reply cube2222 20 hours agoprevSlightly off-topic, but if you fancy writing compilers in your free time, Cranelift has a great Rust library[0] for doing code generation - it’s a pleasure to use! [0]: https://docs.rs/cranelift-frontend/0.105.3/cranelift_fronten... reply loeg 18 hours agoparentI've seen it used for really simple JIT in Advent of Code puzzles. reply PartiallyTyped 2 hours agorootparentWhy am i not surprised and even find this amusing? :D reply diggan 20 hours agoprevTried out the instructions from the article on a tiny Bevy project, and compared it to a \"normal\" build: > cargo build --release 23.93s user 22.85s system 66% cpu 1:09.88 total > cargo +nightly build -Zcodegen-backend 23.52s user 21.98s system 68% cpu 1:06.86 total Seems just marginally faster than a normal release build. Wonder if there is something particular with Bevy that makes this so? The author of the article mentions 40% difference in build speed, but I'm not seeing anything near that. Edit: just realized I'm caching my release builds with sccache and a local NAS, hence the release builds being as fast as Cranelift+debug builds. Trying it again with just debug builds and without any caching: > cargo +nightly build 1997.35s user 200.38s system 1878% cpu 1:57.02 total > cargo +nightly build -Zcodegen-backend 280.96s user 73.06s system 657% cpu 53.850 total Definitely an improvement once I realized what I did wrong, about half the time spent compiling now :) Neat! reply naasking 20 hours agoparentMaybe your build is not limited by code generation, which seems like the only thing that changed here. There was a good thread recently about the variation in what slows down compilation: https://news.ycombinator.com/item?id=39721922 reply diggan 20 hours agorootparentEdited my comment now, forgot I was caching the release builds with sccache so wasn't actually compiling all the units, but fetching a lot of them instead :/ reply vlovich123 16 hours agorootparentWhich speaks to maybe that cargo should just ship with sccache turned on by default so that the “normal” experience for developing Rust has this seamless experience. Intelligent caching should always win. I’d like to see rustc get sccache integration that works at the MIR level too so that changing something that doesn’t change the code gen meaningfully still gets a cached response (e.g. changing some comments/whitespace, moving functions around, etc). Even cooler would be if LLVM itself could also cache internal expensive parts of compilation and optimization across process instances. That would make a huge impact in cutting down incremental builds. reply SkiFire13 14 hours agorootparent> I’d like to see rustc get sccache integration that works at the MIR level too so that changing something that doesn’t change the code gen meaningfully still gets a cached response (e.g. changing some comments/whitespace, moving functions around, etc). Isn't incremental compilation already like that? reply kapilsinha 13 hours agoparentprevIs that Bevy project open-source by any chance? I'd love to it out myself reply doctor_phil 13 hours agorootparentBevyengine is open source and very findable by most search engines. Here is a direct link: https://github.com/bevyengine/bevy reply kapilsinha 13 hours agorootparentAh yep I'm aware of that. Bevy is a framework/engine that devs built on top of. I was referring to the project that OP has built on top of Bevy reply metadat 18 hours agoprevThe Equality Graphs link [0] led me to discover ESC/Java [1] [2]. Has anyone actually tried or had any success with ESC/Java? It's piqued my curiosity to compare with Spot bugs (formerly known as Findbugs). [0] https://en.wikipedia.org/wiki/E-graph [1] https://en.wikipedia.org/wiki/ESC/Java [2] https://www.kindsoftware.com/products/opensource/escjava2/ reply Tehnix 19 hours agoprevVery excited for Cranelift for debug builds to speed up development iteration - in particular for WASM/Frontend Rust where iteration speed is competing with the new era of Rust tooling for JS which lands in the sub 1 second builds sometimes (iteration speed in Frontend is crucial). Sadly, it does not yet support ARM macOS, so us M1-3 users will have to wait a bit :/ reply kapilsinha 13 hours agoparentBut usually, at least I don't build an executable while iterating. And if I do, I try to set up the feature flags so the build is minimal for the tests I am running reply Deukhoofd 20 hours agoprevDoes anyone by chance have benchmarks of runtime (so not the compile time) when using Cranelift? I'm seeing a mention of \"twice as slow\" in the article, but that's based on data from 2020. Wondering if it has substantially improved since then. reply cfallin 8 hours agoparentThere are some benchmarks of Cranelift-based Wasm VMs (Wasmtime) vs. LLVM-based Wasm VMs here: https://00f.net/2023/01/04/webassembly-benchmark-2023/ The (perhaps slightly exaggerated but encouraging to me at least!) money quote there is: > That’s right. The cranelift code generator has become as fast as LLVM. This is extremely impressive considering the fact that cranelift is a relatively young project, written from scratch by a very small (but obviously very talented) team. In practice anywhere from 10%-30% slower maybe is reasonable to expect. Compiler microbenchmarks are interesting because they're very \"quantized\": for any particular benchmark, often either you get the right transforms and achieve the correct optimized inner loop, or you don't. So the game is about getting more and more cases right and we're slowly getting there. (disclosure: I was tech lead of Cranelift in 2020-2022) reply Someone 8 hours agoprevFTA: “Because optimizations run on an E-graph only add information in the form of new annotations, the order of the optimizations does not change the result. As long as the compiler continues running optimizations until they no longer have any new matches (a process known as equality saturation), the E-graph will contain the representation that would have been produced by the optimal ordering of an equivalent sequence of traditional optimization passes […] In practice, Cranelift sets a limit on how many operations are performed on the graph to prevent it from becoming too large.” So, in practice, the order of optimizations can change the result? How easy is it to hit that limit? reply namuol 10 hours agoprevIs there no native support for M1-M3 Macs currently, and no Windows support either? Unclear what the roadmap is there, as this update from the most active contributor is inconclusive: > Windows support has been omitted for now. And for macOS currently on supports x86_64 as Apple invented their own calling convention for arm64 for which variadic functions can’t easily be implemented as hack. If you are using an M1 processor, you could try installing the x86_64 version of rustc and then using Rosetta 2. Rosetta 2 will hurt performance though, so you will need to try if it is faster than the LLVM backend with arm64 rustc. Source is from Oct 2023 so this could easily be outdated, but I found nothing in the original article: https://bjorn3.github.io/2023/10/31/progress-report-oct-2023... reply sanxiyn 9 hours agoparentWindows is supported. See https://github.com/rust-lang/rustc_codegen_cranelift/issues/.... reply bhaney 10 hours agoparentprevhttps://github.com/rust-lang/rustc_codegen_cranelift#platfor... reply posix_monad 18 hours agoprevCan anyone explain why Cranelift is expected to be faster than LLVM? And why those improvements can't also be applied to LLVM? reply Filligree 17 hours agoparentIt uses E-graphs, as explained in the article. That’s a completely different approach to compilation, and to use it in LLVM you’d have to rewrite LLVM. It’s also unlikely that the resulting code will ever be as fast as a traditional compiler’s output. It’s great for development, but I wouldn’t use it in a release build. reply Rusky 15 hours agorootparentI don't believe the performance difference between Cranelift and LLVM really has much to do with E-graphs. Cranelift had this same performance profile (faster than LLVM but generating slower output) before they switched to E-graphs. Rather it's about how much effort Cranelift puts toward optimizing its output- it has fewer, less involved passes (regardless of whether those \"passes\" are expressed as part of the E-graph framework). More subtly, this means it is also written to generate \"okay\" code without as much reliance on those passes- while LLVM on the other hand generates a lot of naive code at -O0 which contributes to its slower compile times in that mode. reply cfallin 8 hours agorootparentRight, it's about algorithmic tradeoffs throughout. A good example I wrote about is here: https://cfallin.org/blog/2021/01/22/cranelift-isel-2/ where we use a single-pass algorithm to solve a problem that LLVM has a multi-part fixpoint loop to solve. Most CPU time during compile is in the register allocator and I took a really careful approach to optimization when I rewrote it a few years ago (more details https://cfallin.org/blog/2022/06/09/cranelift-regalloc2/). We generally try to pay close attention to algorithmic efficiency and avoid altogether the fixpoint loops, etc that one often finds elsewhere. (RA2 does backtrack and have a worklist loop, though it's pretty minimal, and also we're planning to add a single-pass mode.) (disclosure: I was Cranelift tech lead in 2020-2022) reply norman784 17 hours agoparentprev> why those improvements can't also be applied to LLVM? LLVM is a huge and bloated ecosystem (it has tons of tools and millions of LoC), also the code base itself is pretty old or rather the project has his age, so there's a lot of legacy code, other aspect is that is hard to try new/radical things because how big the project itself is. reply posix_monad 16 hours agorootparentX is too bloated! We need Y, which is leaner and more tailored to our use-case. (years pass...) Y is too bloated! We need Z... reply sanxiyn 9 hours agorootparentAfter years of trying (IIRC there was funded effort from Intel), LLVM still can't parallelize code generation at function level. Cranelift had function level code generation parallelization from day 1. reply azakai 6 hours agorootparentI have always been told that LLVM doesn't parallelize codegen because the right place for that is in the build system: just do make -j N (or use ninja, which parallelizes automatically). And when you're building lots of source files in a normal project that's definitely true. It's different when you're a wasm VM that receives big chunk of wasm that contains many source files, and you get it all at once. And for that reason pretty much every wasm compiler parallelizes codegen: Cranelift as you said, and also V8, SpiderMonkey, JSC, and not just VMs but also optimizers like Binaryen. It's a crucial part of their design. For LLVM, the right design may be what it has today: single-core codegen. (LTO is the main issue there, but that's what thin LTO is for.) reply dymk 14 hours agorootparentprevI don't get it. Is your point that new software shouldn't be written to fulfill new usecases? We should always bolt new patterns and architecture onto existing codebases? reply alserio 12 hours agorootparentprevYes. But in the meantime one can experiment with new things, void some assumptions. And I don't know about the LLVM project, but its quite more difficult to try different ideas in a big and old organization and codebase than it is in a greenfield project. Maybe they won't be successful, maybe they will move the whole field ahead. reply papruapap 17 hours agorootparentpreviirc Zig want to replace LLVM for similar reasons, also hard to debug. reply mort96 13 hours agoparentprevI've heard compiler writers complain that LLVM's API is inherently slow. Something about using runtime polymorphism everywhere. I don't know how much there is to this though, I haven't investigated it myself. reply mmoskal 17 hours agoprev> JIT compilers often use techniques, such as speculative optimizations, that make it difficult to reuse the compiler outside its original context, since they encode so many assumptions about the specific language for which they were designed. > The developers of Cranelift chose to use a more generic architecture, which means that Cranelift is usable outside of the confines of WebAssembly. One would think this has more to do with Wasm being the source language, as it's fairly generic (compared to JS or Python), so there are no specific assumptions to encode. Great article though. It's quite interesting to see E-matching used in compilers, took me down a memory lane (and found myself cited on Wikipedia page for e-graphs). reply rayiner 17 hours agoprevVery interesting article. I had not heard of equality graphs before. Here's some pretty good background reading on the subject: https://inst.eecs.berkeley.edu/~cs294-260/sp24/2024-03-04-eq... reply k_bx 21 hours agoprevAny fresh compilation time benchmarks and comparisons to LLVM? reply nindalf 20 hours agoparentFew reports comparing Cranelift to LLVM from this day old reddit thread [1] - 29.52s -> 24.47s (17.1%) - 27s -> 19s (29.6%) - 11.5s -> 8.4s (26.9%) - 37.5s -> 29.6s (28.7%) - this measurement from TFA. To put these numbers in context, all the perf improvements over the last 4 years have helped the compiler become faster on a variety of workloads by 7%, 17%, 13% and 15%, for an overall speed gain of 37% over 4 years. [2] So one large change providing a 20-30% improvement is very impressive. When you add that to the parallel frontend [3] and support for linking with LLD [4], Rust compilation could be substantially faster by this time next year. [1] - https://old.reddit.com/r/rust/comments/1bgyo8a/try_cranelift... [2] - https://nnethercote.github.io/2024/03/06/how-to-speed-up-the... [3] - https://blog.rust-lang.org/2023/11/09/parallel-rustc.html [4] - https://github.com/rust-lang/rust/issues/39915 reply k_bx 17 hours agorootparentThanks, that's really great news. The only thing I wish for now is aarch64 support on Apple Silicon reply boxed 21 hours agoparentprevThe very last paragraph: > A full debug build of Cranelift itself using the Cranelift backend took 29.6 seconds on my computer, compared to 37.5 with LLVM (a reduction in wall-clock time of 20%). Those wall-clock times don't tell the full story, however, because of parallelism in the build system. Compiling with Cranelift took 125 CPU-seconds, whereas LLVM took 211 CPU-seconds, a difference of 40%. Incremental builds — rebuilding only Cranelift itself, and none of its dependencies — were faster with both backends. 66ms of CPU time compared to 90ms. reply AlexErrant 21 hours agoparentprevNot quite fresh, but > A paper from 2020 [0] showed that Cranelift was an order of magnitude faster than LLVM, while producing code that was approximately twice as slow on some benchmarks. [0] https://arxiv.org/pdf/2011.13127.pdf reply Findecanor 20 hours agorootparentThe changes in Cranelift since 2020 have been quite significant so I would not put any trust in those benchmarks. reply zokier 21 hours agoparentprevFrom the article: > A full debug build of Cranelift itself using the Cranelift backend took 29.6 seconds on my computer, compared to 37.5 with LLVM (a reduction in wall-clock time of 20%) That seems much smaller difference than what I would have expected reply __s 21 hours agorootparentThose numbers are the entire compile time, parsing/typecheck/MIR+MIR optimization/linking Don't have numbers handy, so hard to say how much faster Cranelift is making the codegen portion, but gets into Amdahl's Law reply Filligree 17 hours agorootparentAnd there are faster linkers than the default. …why is it still the default? reply kibwen 13 hours agorootparentBecause the linker provided by the system itself is the one that's guaranteed to be the most broadly compatible. For a toolchain to begin distributing an alternative linker is a lot of work on every supported platform (future versions of Rust may sidestep this issue by making LLD the default on certain platforms, which they get \"for free\" by dint of shipping alongside LLVM). But also, because there are about ten people on planet Earth qualified to write a production-grade linker, which is why LLD still isn't broadly recommended yet. reply 3836293648 16 hours agorootparentprevCompatibility with niche usecases reply Filligree 13 hours agorootparentThat sounds like a good reason to keep the existing one as an opt-in possibility. reply Dowwie 20 hours agoprevWould it be naive to assume a general compile-time reduction of 20% for all Rust projects by swapping llvm with cranelift? reply mrklol 19 hours agoparentThere are still projects out there which won’t hit 20% or even some %, if the bottleneck isn’t code gen (for example). So the part with\"all\" can be wrong, but beside that 20% is a good number. reply Ericson2314 19 hours agoprevReally looking forward to the death of non-e-graph-based compilation :) reply convolvatron 13 hours agoparentI've tried to look into this a couple times, including today. To me this looks alot like unification? but I don't really understand how operationally one gets from equivalence classes to instructions. is there an e-graphs for dummies writeup? reply cfallin 6 hours agorootparentThe approach that Cranelift uses is what we call the \"aegraph\" (talk I gave about it: slides https://cfallin.org/pubs/egraphs2023_aegraphs_slides.pdf, video https://vimeo.com/843540328). The basic idea is that eclasses hold sets of operator expressions (think sea-of-node IR), and we keep that alongside the CFG with a \"skeleton\" of side-effecting ops. We build that, do rewrites, then extract out of it back to a conventional CFG-of-basic-blocks for lowering. The \"equivalence\" part comes in when doing rewrites: the main difference between an egraph and a conventional sea-of-nodes IR with rewrite rules is that one keeps all representations in the equivalence class around, then chooses the best one later. We had to solve a few novel problems in working out how to handle control flow, and we're still polishing off some rough edges (search recent issues in the repo for egraphs); but we're mostly happy how it turned out! (Disclosure: tech lead of CL for a while; the e-graphs optimizer is \"my fault\") reply tekknolagi 13 hours agorootparentprevIf by instructions you mean machine instructions, you don't; it's used for internal optimization passes only. reply convolvatron 12 hours agorootparentthat helps, so this is really an alternative to pushing analysis attributes aronud a dataflow graph reply rishav_sharan 18 hours agoprevIt sucks that there is no way to use cranelift from outside of rust to create your own toy language. I would have loved to use cranelift in a toy compiler, but I am not ready to pay the Rust price of complexity. reply kibwen 17 hours agoparentAs far as I know, you should be able to generate textual CLIF files and feed them to Cranelift: https://github.com/bytecodealliance/wasmtime/blob/main/crane... reply sigsev_251 17 hours agoparentprevThere is always qbe if you need something simpler reply pjmlp 20 hours agoprevFinally, looking forward for wider adoption. reply Monalisa12 17 hours agoprevnext [5 more] [flagged] ecmascript 17 hours agoparentSeriously, spam at HN? That is pretty unusual to see. reply dredmorbius 12 minutes agorootparentnext [–](spam, trolls, same thing) reply orthecreedence 16 hours agorootparentprevIf you have showdead set to \"yes\" in your settings, you'll see a lot more spam like this. You also get to see comments that add to discussion but were downvoted to oblivion because people don't like certain things being questioned. reply ecmascript 3 hours agorootparentIt wasn't dead when I replied. I know this, I show dead since I often find my comments to be flagged or killed since I have some unpopular opinions. Just watch the my history and view the sea of down votes I've collected. ;) reply tsegratis 18 hours agoprev [–] I feel like im reading advertising blurb reading that article I wish them every success, but i hope for a more balanced overview of pros and cons rather than gushing praise at every step... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cranelift, a code-generation backend for WebAssembly, is now an optional component in Rust's nightly toolchain starting October 2023, focusing on fast code generation for debug builds and optimizations to rival existing compilers.",
      "Its versatile architecture extends its use beyond WebAssembly, providing quicker compilation for interpreter-reliant applications.",
      "Discussions highlight Cranelift's advantages over LLVM, concerns about LLVM's stability, compiler development hurdles, and the potential for enhancing Rust development through optimized code generation."
    ],
    "commentSummary": [
      "The focus is on integrating Cranelift code generation in Rust, examining its compatibility with LLVM, optimization strategies, and the promise of quicker output generation.",
      "Users are assessing build times, exploring e-graph optimizations, and comparing Cranelift's performance with LLVM, highlighting the potential for faster compilation and hurdles in optimizing extensive programs.",
      "Discussions also touch on addressing spam and downvoted comments in online forums, stressing the importance of balanced perspectives and critical analysis in tech articles."
    ],
    "points": 369,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1710761982
  },
  {
    "id": 39746806,
    "title": "EPA Imposes Ban on Asbestos, a Lethal Carcinogen in Use",
    "originLink": "https://apnews.com/article/epa-asbestos-cancer-brakes-biden-72b0fa8b36adedaff6000034d35c2acd",
    "originBody": "1 of 2FILE - Asbestos Removal Technologies Inc. staff work on asbestos abatement in Howell, Mich., Oct. 18, 2017. The Environmental Protection Agency on Monday, March 18, 2024, announced a comprehensive ban on asbestos, a carcinogen that kills tens of thousands of Americans every year but is still used in some chlorine bleach, brake pads and other products. (AP Photo/Paul Sancya, File) Read More 2 of 2FILE - Environmental Protection Agency administrator Michael Regan speaks, Feb. 16, 2024, in East Palestine, Ohio. The EPA announced Monday, March 18, a comprehensive ban on asbestos, a carcinogen that is still used in some chlorine bleach, brake pads and other products and kills thousands of Americans every year. Regan called the final rule a major step to protect public health. (AP Photo/Andrew Harnik, File) Read More By MATTHEW DALY Updated 10:56 PM UTC, March 18, 2024 Share Share Copy Link copied Email Facebook X Reddit LinkedIn Pinterest Flipboard Print WASHINGTON (AP) — The Environmental Protection Agency on Monday announced a comprehensive ban on asbestos, a carcinogen that kills tens of thousands of Americans every year but is still used in some chlorine bleach, brake pads and other products. The final rule marks a major expansion of EPA regulation under a landmark 2016 law that overhauled regulations governing tens of thousands of toxic chemicals in everyday products, from household cleaners to clothing and furniture. The new rule would ban chrysotile asbestos, the only ongoing use of asbestos in the United States. The substance is found in products such as brake linings and gaskets and is used to manufacture chlorine bleach and sodium hydroxide, also known as caustic soda, including some that is used for water purification. EPA Administrator Michael Regan called the final rule a major step to protect public health. “With today’s ban, EPA is finally slamming the door on a chemical so dangerous that it has been banned in over 50 countries,’' Regan said. “This historic ban is more than 30 years in the making, and it’s thanks to amendments that Congress made in 2016 to fix the Toxic Substances Control Act,’' the main U.S. law governing use of chemicals. READ MORE Half of US states join GOP lawsuits challenging new EPA rule on deadly soot pollution EPA delays rules for existing natural gas power plants until after the November election EPA approves year-round sales of higher ethanol blend in 8 Midwest states Exposure to asbestos is known to cause lung cancer, mesothelioma and other cancers, and it is linked to more than 40,000 deaths in the U.S. each year. Ending the ongoing uses of asbestos advances the goals of President Joe Biden’s Cancer Moonshot, a whole-of-government initiative to end cancer in the U.S., Regan said. “The science is clear: Asbestos is a known carcinogen that has severe impacts on public health. This action is just the beginning as we work to protect all American families, workers and communities from toxic chemicals,’' Regan said. The 2016 law authorized new rules for tens of thousands of toxic chemicals found in everyday products, including substances such as asbestos and trichloroethylene that for decades have been known to cause cancer yet were largely unregulated under federal law. Known as the Frank Lautenberg Chemical Safety Act, the law was intended to clear up a hodgepodge of state rules governing chemicals and update the Toxic Substances Control Act, a 1976 law that had remained unchanged for 40 years. The EPA banned asbestos in 1989, but the rule was largely overturned by a 1991 Court of Appeals decision that weakened the EPA’s authority under TSCA to address risks to human health from asbestos or other existing chemicals. The 2016 law required the EPA to evaluate chemicals and put in place protections against unreasonable risks. Asbestos, which was once common in home insulation and other products, is banned in more than 50 countries, and its use in the U.S. has been declining for decades. The only form of asbestos known to be currently imported, processed or distributed for use in the U.S. is chrysotile asbestos, which is imported primarily from Brazil and Russia. It is used by the chlor-alkali industry, which produces bleach, caustic soda and other products. Most consumer products that historically contained chrysotile asbestos have been discontinued. While chlorine is a commonly used disinfectant in water treatment, there are only eight chlor-alkali plants in the U.S. that still use asbestos diaphragms to produce chlorine and sodium hydroxide. The plants are mostly located in Louisiana and Texas. The use of asbestos diaphragms has been declining and now accounts for less than one-third of the chlor-alkali production in the U.S., the EPA said. The EPA rule will ban imports of asbestos for chlor-alkali as soon as the rule is published but will phase in prohibitions on chlor-alkali use over five or more years to provide what the agency called “a reasonable transition period.’' A ban on most other uses of asbestos will effect in two years. The National Association of Clean Water Agencies, which represents 350 publicly owned wastewater treatment agencies, said in a statement that it supports “EPA’s efforts to move away from asbestos products’’ and will work with the agency to track implementation of the rule. The association warned before the final rule was announced that an immediate ban on asbestos would “almost certainly cause shortages and price increases for chlorine and other disinfection and treatment chemicals used by the water sector.’' The American Chemistry Council, the chemical industry’s largest lobbying group, said a 15-year transition period is needed to avoid a significant disruption of chlorine and sodium hydroxide supplies. A ban on asbestos in oilfield brake blocks, aftermarket automotive brakes and linings and other gaskets will take effect in six months. The EPA rule allows asbestos-containing sheet gaskets to be used until 2037 at the U.S. Department of Energy’s Savannah River Site in South Carolina to ensure that safe disposal of nuclear materials can continue on schedule. Scott Faber, senior vice president of the Environmental Working Group, an advocacy group that pushed to ban asbestos, hailed the EPA action. “For too long, polluters have been allowed to make, use and release toxics like asbestos and PFAS without regard for our health,’' Faber said. “Thanks to the leadership of the Biden EPA, those days are finally over.” Separately, the EPA is also evaluating so-called legacy uses of asbestos in older buildings, including schools and industrial sites, to determine possible public health risks. A final risk evaluation is expected by the end of the year. Sen. Jeff Merkley, D-Ore., said the United States was “finally starting to catch up” with the rest of the world on asbestos. “However, it cannot be the end of the road when it comes to phasing out other dangerous asbestos fibers,’' he said. “Congress has a role to play here when it comes to providing stronger protections for our health.’' ___ Follow the AP’s coverage of the U.S. Environmental Protection Agency at https://apnews.com/hub/us-environmental-protection-agency. MATTHEW DALY Matthew Daly covers climate, environment & energy policy twitter mailto",
    "commentLink": "https://news.ycombinator.com/item?id=39746806",
    "commentBody": "EPA bans asbestos, a deadly carcinogen still in use decades after partial ban (apnews.com)353 points by anigbrowl 17 hours agohidepastfavorite340 comments KingOfCoders 2 hours agoSometimes the US is suprising to me. The EU has done that 2005, getting 25 countries on board. Germany in 1993. Then again, in a free market, why regulate at all? I understand the regulation in the EU, but with all the free market fundamentalism, why is the US regulating this? Shouldn't people decide if they want to buy stuff with asbestos or live in an asbestos home? Or get a job with asbestos exposure? If you believe in free market fundamentalism, why regulate anything? Just label things. reply the_other 2 hours agoparentIf government isn’t protecting the people it represents, why have one? reply coldtea 1 hour agorootparentIt's not like the people are asked, or have a choice on the matter of having a government reply otikik 1 hour agoparentprevCan’t tell whether this is ironic or not. In case it isn’t: the US has the best politicians money can buy. So things that are bad for the general public but good for a certain minority of wealthy individuals take longer to take effect than in other countries. But most eventually do pass. reply 0dayz 21 minutes agorootparentEPA isn't run by politicians but by normal workers also known as buerocrates. reply blitzar 2 hours agoparentprevForcing companies to label things is government over-reach. reply hackernewds 2 hours agoparentprevWho said the US is a free market? you consistently have Biden playing God and distributing EV tax credit, $10k for first home buying, student loan forgiveness randomly in an election year. There is consistent interference with the free market as it is politically favorable in the US. reply gruez 2 hours agorootparentAnd this isn't even limited to Biden. The Trump presidency enacted a variety of protectionist measures. reply YeBanKo 1 hour agorootparentProtections is not the same as buying votes by paying off someones student debt with tax payers money. reply cqqxo4zV46cp 21 minutes agorootparentYou could classify any government action that actually benefits people as “buying votes”. This a thought-terminating do-nothing argument. reply zachmu 8 hours agoprevMesothelioma from household exposure is very rare, nearly all cases come from industrial exposure, involving chronic exposure to concentrations of airborne particles millions of times higher than you would ever find in a residential setting, even during a renovation that disturbs asbestos. I looked into this when I discovered some old asbestos paper in my basement that I had abated by professionals (which I always recommend). I was freaking out about my family being exposed to the fibers for years before that, so I went digging for hard numbers. To my surprise, there is almost zero good quantitative data on the risk of mesothelioma from residential asbestos exposure. The best info we have suggests about a doubling of the risk of MM from residential exposure, from about 1 per million person-years to 2.5 per million person years: https://www.thelancet.com/journals/lanpub/article/PIIS2468-2... > However, the chief interest from this study will be in the more than doubling of risk of mesothelioma in men who had lived in an affected house, compared with unexposed males (SIR 2·54, 95% CI 1·02–5·24). ... The background incidence of mesothelioma without exposure to asbestos is very low (highly age-dependent and roughly one case per million person-years), so any rise would be indicative of previous asbestos exposure Other studies indicate that home renovations that disturb asbestos could increase the risk by about a factor of 5 from baseline, which sounds high until you realize that it means about 5 per million person-years. The base rate is very low! And there is no such thing as zero exposure to asbestos: there are a couple fibers, on average, in every liter of air you breathe. Everybody is constantly exposed to a very small amount of these fibers, and it's not the case that it gives everybody MM: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7721955/#:~:tex.... So the bottom line is: get your asbestos abated by professionals, but don't freak out if you have been living with the stuff. It's not the same as being an asbestos miner, your risk is higher but still very low. reply HdS84 2 hours agoparentWhile true, domestic use is super problematic and balloons any bill. E.g. our house in germany is from 1968. We where lucky and the builder did not use any asbestos so I could do most renovations myself. If you have asbestos, you need specialized equipment and procedures or involve experts. That is really expensive and can easily double or triple your estimated cost. You can ofc ignore it, but especially with small children it seems ill advised to do so. NTM there are fines if you expose the public. reply wizerdrobe 29 minutes agorootparentMeanwhile in America I am allowed to self-remove asbestos as a homeowner no verifications required. Whereas a commercial builder is required to have certifications. Another “fun fact,” when the asbestos ban went into place builders were still allowed to use any remaining stock they held in their warehouses. No new product could be purchased. So homes could still be built years later with a now banned substance. Fascinating bit of law there. reply speleding 1 hour agorootparentprevIf you involve a specialised removal firm it's super expensive and stalls your project by weeks. I was advised by the builder I could just remove a few plates of asbestos myself, about half a day of work, most of which was spent getting the right plastic bags and then driving them to the correct dump, the actual removal of the asbestos plates took minutes. Used mask etc, but really a single exposure was a very acceptable risk for me to save on cost and time. (There's different types of asbestos, some more dangerous than others, so you should do a bit of research into what you're doing) reply ro-ka 2 hours agoparentprevMy parents house (in the EU) was covered in plates of asbestos. As far as I know it only is dangerous when you install / remove it as these plates break and release chemicals to the air. I’m no expert though. reply pcchristie 5 hours agoparentprevThis is really interesting, thanks. reply hanniabu 6 hours agoparentprev> Mesothelioma from household exposure is very rare > To my surprise, there is almost zero good quantitative data on the risk of mesothelioma from residential asbestos exposure Love it when comments state something so confidently, yet contradict themselves a few sentences later reply zachmu 5 hours agorootparentThere is no contradiction there reply leke 5 hours agorootparentThere isn't? reply YeBanKo 1 hour agorootparentI think the point that are trying to make is that if it was more common, it would be noticeable and more quantifiable. Otherwise, most of the cases seems to have been tracked to industrial exposure. reply zachmu 5 hours agorootparentprevRare but difficult to precisely quantify. We know the rate of mesothelioma in people with no known occupational exposure, but we have very little quantitative data about how much a given amount of asbestos exposure might increase one's risk. The disease takes decades to develop after exposure. reply roenxi 5 hours agorootparentprevDeducing that something is rare from limited evidence is really easy. If there is no good evidence regarding something, that default position should be that the thing is rare. reply happytiger 12 hours agoprevLeading the world in protecting Americans by following the lead of more than 50 countries. That’s my EPA. Good job! reply mandevil 9 hours agoparentIn July 1989 the EPA issued the Asbestos Ban and Phase-Out Rule, which would have been a total ban on asbestos. The incredibly conservative 5th Circuit Court of Appeals ruled in 1991 that ban was invalid in Corrosion Proof Fittings v. Environmental Protection Agency, because the EPA failed to show that it was the \"least burdensome alternative\" in regulating asbestos. The George HW Bush administration chose not to appeal the ruling. Over the past three decades the EPA believes it has finally done the work to show that this ban is the \"least burdensome alternative,\" but there is no guarantee that the courts will agree this time either. The politicization of the judiciary is not new, certainly, but seems to have gotten worse over the past three decades. reply Thorrez 7 hours agorootparentAccording to the article, this ban is \"thanks to amendments that Congress made in 2016 to fix the Toxic Substances Control Act\". Wouldn't a change in law override the judges? (Unless the judges based their ruling on the constitution, which doesn't sound like the case here.) reply sidewndr46 7 hours agorootparentIn this case, likely yes reply hackernewds 2 hours agorootparentprevwhat the hell is wrong with specifically the 5th Circuit Court? they're in the news now too because the SC is fed up with their right leaning rules reply rodgerd 8 hours agorootparentprev> 5th Circuit Court of Appeals Ah yes. Well-known for its general view that the United States is a diktat-by-judge, and that mere laws are irrelevant. reply EasyMark 5 hours agorootparentThe 5th circuit has become a joke over the past decade or so. I'm sure all the other circuits refer to it as a way on how not to conduct themselves. reply emeril 10 hours agoparentprevyeah, we vote in people, regularly, whose mission it is to dismantle the EPA/FDA/CDC/etc. in the name of \"progress\" reply lokar 10 hours agorootparentThe basic premise of political conservatism is not progress, it is literally to “conserve” the current status or restore the recent past. reply voisin 9 hours agorootparentIsn’t the basic premise of political conservatism is to minimize public intrusion in private lives? Conserving the primacy of the individual rather than the collective? reply lokar 9 hours agorootparentWith any global term/movement there will be variations, but it generally started with support for maintaining monarchy and aristocracy. reply roenxi 4 hours agorootparent2 points to make on that: 1) \"Conservatism\" these days would arguably be a US phenomenon as leading democracy in the Anglophone world. They certainly didn't get started supporting a monarchy. 2) And it is really interesting to note that, while I think monarchies are stupid, it was a remarkably good strategy. As far as I know (my history might be about to betray me) the UK didn't have an equivalent of the Terror after the French revolution or the period where the French killed off people like Lavoisier. To say nothing of the debacles in places like Russia (Communists) or Germany (Nazis) when they moved away from monarchism. The UK probably should get rid of the King; but in hindsight a slow transition is arguably the cleverer path. It is a complex topic; the aristocrats in Europe are systematically underwhelming. reply gattilorenz 2 hours agorootparentItaly and Spain kept their king while being a fascist and sort-of-fascist country respectively, so I’m not sure the theory is correct. The UK didn’t have the Terror, but afaik it did have at least a civil war because of the monarchy, a few centuries ago. Rather than the monarchy in itself, I suspect it’s the monarch(s) that make it or break it… reply fuzztester 24 minutes agorootparent>The UK didn’t have the Terror, but afaik it did have at least a civil war because of the monarchy, a few centuries ago. Cromwell. https://en.m.wikipedia.org/wiki/Oliver_Cromwell https://en.m.wikipedia.org/wiki/First_English_Civil_War https://en.m.wikipedia.org/wiki/Second_English_Civil_War reply Georgelemental 4 hours agorootparentprevNo, that's libertarianism. Libertarianism and conservatism often overlap, especially in the US (which has a long tradition of limited gov't to conserve), but are not the same thing. As a self-identified conservative, if I had to give as short a summary as possible of what it means to me: we (US, other developed nations) have a pretty good thing going. The least well off 10% here live better lives than the top 10% of a large number of places. That didn't happen by magic or accident, it happened because the people who preceded ud, over centuries of history, made some very good choices. We should figure out what they did right, and then keep doing it. reply yau8edq12i 2 hours agorootparent> The least well off 10% here live better lives than the top 10% of a large number of places. You're absolutely blind if you think that. Tell me, in what place do the top 10% live worse life than the bottom 10% in the USA? Keep in mind, the bottom decile of income in the USA is $10k/year - think about how a person that earns that lives. reply bruce511 8 hours agorootparentprevAt this point its hard to understand the premise of conservatism. It seems to just be outrage and pretty much anything and everything. I'll avoid suggesting the root is racism, but more and more its becoming harder to do that. Between the pretty blatant racist rants of the leader, to the embrace of far-right fundamentalists, it seems like a common thread. As for public intrusion of private lives, even that seems hazy. Where does public intrusion end and private life start? For example are private medical choices a matter of public policy? Are the choices about which books to read at the library public or private? In a two party system many voters can be left without a home when their preferred party swings off in a different direction. And while no voter is going to always be happy with their chosen group, the risk of homelessness goes up as the preferred choice swings away from traditional premises. For many conservatives (small c) the current direction of Conservative Leaders (big C) is not ok. But changing allegiance is mentally traumatic. I think your premise can be both accurate, and currently invalid. It sure doesn't seem like \"minimal public intrusion\" right now. If you had to decide which party represents \"live and let live\", which party wants personal freedoms, which party prioritizes individual choices, well, I'd argue it's not the nominally \"conservative\" one. reply voisin 6 hours agorootparentI concur with everything you said, and to be clear, I wasn’t specifically referring to big C Conservatives, GOP or otherwise, which I agree are straying from small c conservative values. I was referring specifically to political conservatism which I understood to be about minimal intrusion in private affairs. It would appear no mainstream political parties still stand for this. reply bruce511 4 hours agorootparentIm thinking Libitarianism is perhaps more into \"individual freedoms\" than conservatism, but there's certainly overlap. If I had to put a fence around it, I'd suggest the root of conservatism is more \"govt working in the interests of the rich/aristocracy/establishment.\" The \"people in power before there was a vote\". Charitably described as \"keep things the way they are\", or less charitably as \"return to when our group had riches and power\". So \"labor\" is getting govt to work for the masses, \"conservative\" us getting govt to work for those already established, and libitarianism is scaling govt back, and letting people do whatever they like. Political parties try and be all things to all men. In some countries (where there are more than two parties) there's more likely to be different parties to cover these bases. reply waveBidder 9 hours agorootparentprevEstablishment Democrats and Republicans are the conservative parties in this sense. Tea party and their anarcho-capitalist descendants aren't really conservative. reply boplicity 8 hours agorootparentEither party is not a monolith -- this has become increasingly clear. We can and should support candidates in primaries who are more aligned with us. reply notfed 7 hours agorootparentprevI suspect that a lot of people who say they want to do this have a delusion that at some point in the past the whole country/world was aligned with the fantasy envisioned in their head. reply refurb 6 hours agoparentprevThe US is actually the leader here, but typical of the discourse on HN, people love to promote their own ideas above facts. This ban eliminates immediately all use in the US, including chlorine plants. The 2005 EU “ban” on asbestos has an exception for chlorine plants. This will eventually be phased out in 1 July 2025. Jeeze EU, get with the times! Why does the US have to demonstrate the right path before you take it? /s https://chemycal.com/news/e6b71d43-1892-454c-80a9-740207f556... reply Skgqie1 6 hours agorootparentAsbestos has been completely banned in Australia since 2003. reply stevejb 3 hours agorootparentAustralia also probably has some of the worst deployments of asbestos in the developed world. Drive around even nice neighborhoods in Sydney and you’ll see plenty of cracking and breaking “fibro”, a cement asbestos sheet. Canberra is full of asbestos. They had to completely remove an asbestos mining town (Wittenoom) from the map because it was so contaminated. There is a ton of asbestos currently in Australian households. Plenty of aussies drink water collected in tanks off of asbestos cement roofs. reply naet 5 hours agorootparentprevThe new ban does not immediately eliminate all use in the US, it allows up to twelve years from now (depending on number of facilities) for companies to phase out their usage, including chlorine plants. reply refurb 3 hours agorootparentprevNot that I care about downvotes, but it’s interesting that posting actual facts, correcting a highly upvoted false comment, gets you down-votes. reply fastball 12 hours agoparentprevNot sure what purpose the snark serves – the way you phrased your comment seems to imply someone involved claimed they are \"leading the world\", but that isn't the case. reply spiderfarmer 11 hours agorootparentHe’s making fun of the fact that a majority of the US population assumes the US is leading in, quite simply, any area. reply deepsun 9 hours agorootparentI don't think that majority assumes that. Quite contrary, majority believes US falls back behind on almost everything (as it should be statistically). reply bruce511 8 hours agorootparentYou're right, if you include people living outside the US. To those outside its easy to see the flaws. But I belive the original snark, and explanation, wete referring to the majority of people living inside the US. From that perspective there's an implicit understanding that the US leads the way (in every field) and their approach to society is best (in all contexts.) This is of course evidenced by the number of people who desire to live there, and hence the immigration issues. (Immigration issues being a uniquely US phenomenon.) reply ninjanomnom 2 hours agorootparentI'd wager the majority inside the US knows that it's behind in many fields but don't know exactly which ones. We learn to disregard people speaking blanket praise but don't learn enough about other countries from our media and schooling to argue against them. From the outside then this looks like most of our population thinks we're the best in every respect, but on the inside there's a very large and growing sentiment of dissatisfaction and pessimism. reply pb7 8 hours agorootparentprevDo you really believe the US doesn’t lead in any area? reply eyelidlessness 8 hours agorootparentTheir usage of “any” is clearly different than yours: “any [and all]” versus “any [at all]”. reply pb7 7 hours agorootparentTheir usage is improper. If that was their intention, it should have read “every area”. reply eyelidlessness 6 hours agorootparentTheir usage is fine, and I couldn’t have imagined interpreting it any other way until I saw your misunderstanding. But if you’re more interested in correcting arbitrary grammar rules than gaining a better understanding of the conversation you joined, that’s my cue to drop it and go do something better with my time. reply akira2501 11 hours agorootparentprevIs he making fun of the actual fact or just the majority of the US population? If it's the latter, wouldn't it be more worthwhile to make fun of whatever institutions cause them to believe this simple flattery? reply avery17 11 hours agorootparentChuckle and move on. :) reply akira2501 6 hours agorootparentDoesn't really fit in with the \"hacker\" vibe, though, does it? reply hexo 11 hours agorootparentprevNot sure what or who your comment serves, at all. reply fastball 10 hours agorootparentTrying to nudge the conversation into a more productive direction than misleading snark. First rule[1] of the HN comment guidelines. [1] https://news.ycombinator.com/newsguidelines.html reply Freedom2 7 hours agorootparentYet regardless of your nudging, the conversation moved into a productive and importantly curious discussion, which is one of the most important things when it comes to HN comment sections. reply hexo 9 hours agorootparentprevThis helps nothing and you started whining about it. reply GenerWork 15 hours agoprevI'm curious as to why they banned the use of asbestos by the chlor-alkali industry. It seems that the asbestos is relatively inert (i.e. it's not going anywhere), perhaps they had some evidence that there were asbestos fibers getting into the final products? reply whatshisface 15 hours agoparentThat stuff almost always gets into the lungs of the workers making it. Oftentimes stuff gets banned because nobody can handle it properly. Its actually kind of like nuclear power. The electricity isn't more dangerous to consumers, but observation of the mortal nature of man lead to the conclusion that it'd be better to keep corporations away from it. reply Reason077 14 hours agorootparentVastly more people have died from hydro accidents, let alone coal pollution, than have ever died from all the world’s nuclear accidents combined. reply whatshisface 14 hours agorootparentThe very same meticulous regulatory oversight that make plants safe makes them impossible to get through permitting cost-effectively. The result of the \"soft ban\" are a number of extremely safe plants that it's not economical to make more of. You could hypothetically enforce a similar \"soft ban\" for leaded gasoline or asbestos - by requiring elaborate filtration, containment and disposal procedures, plus monitoring and redundant process oversight. They'd never show up on consumer vehicles or in homes, but they may show up in a handful of specialty applications, complete with extraordinary amounts of paperwork and large government departments. Some people might start talking about \"clean fiber,\" or \"safe leaded.\" P.S. if you are wondering what's wrong with allowing corporations to take on endeavors with a risk of causing large amounts of damage, it's because the value of a company isn't allowed to go negative due to bankruptcy and the corporate veil. If you're able to incorporate, you can profit from a series of \"-$1,000 if I lose, +$1 if I win,\" bets, because creating $1000 liabilities on an entity with no money in the bank costs little more than the equipment you'd be forced to liquidate. reply smallmancontrov 13 hours agorootparentIn the US we get 20% of our power from nuclear, we have for 40 years, and most of those plants were built before we invented safety. How much risk for 100%?Better late than never. Most likely too late as we kept burning fossil fuels too long and we are now facing the danger of civilization-ending climate change. All thanks to doomer propaganda like whatshisface's. Amazing how powerful fear is - to manipulate and keep people down. reply blkhawk 2 hours agorootparentI come to the conclusion that the results will most likely be \"civilisation supressing\" worst case. It won't be fun but we won't end up in caves or extinct. reply rsynnott 12 hours agorootparentprevThe tricky bit about nuclear construction today isn’t really _safety_ so much as public acceptance; you’ll get tied up for decades in disputes which are nothing in particular to do with the safety regulation, and everything to do with public opposition. reply Georgelemental 4 hours agorootparentprevChina and Korea manage to build nuclear plants under budget with no significant accidets to date. So it's empirically possible reply SnorkelTan 13 hours agorootparentprevChina seems to be building next gen reactors in reasonable amounts of time. reply whatshisface 13 hours agorootparentLet's check again in 50 years, after the safety systems have had time to deteriorate, and we've had Chinese Glasnost for a look into the archives from 2056. (The Soviet Union would have kept Chernobyl a secret if Sweden hadn't detected the radiation. If the accident had been limited to the boundaries of Ukraine, it might never have been revealed as long as the regime lasted.) reply rsynnott 12 hours agorootparentVirtually all plants being built in China today are third-gen evolutions of proven second-gen PWRs, themselves very safe. Hiding a Chernobyl-scale nuclear disaster would be far more difficult today; even in China, total news blackouts are a lot more difficult to pull off than they used to be. And the designs in use are inherently a lot safer and more conservative than the RBMK type (which was really pretty innovative and impressive for the time, if you ignore the unfortunate tendency to explode, but certainly _not_ conservative). reply YetAnotherNick 11 hours agorootparentIt doesn't have to be Chernobyl level which are unlikely. It could just be radioactive waste contamination in ground water or things like that. reply pdonis 11 hours agorootparentprevChernobyl was not a result of safety systems deteriorating over a long period of operation. We have plenty of reactors in the world that have operated for decades and their safety systems still work just fine. Chernobyl was the result of an insane reactor design (positive void coefficient of reactivity and no secondary containment) which then had uncontrolled experiments run on it during operation. Nobody is going to repeat that bonehead move. reply arcticbull 5 hours agorootparentFun fact, there were four reactors at Chernobyl. Only one - Unit 4 - was destroyed in the flagship incident. The other three were retrofitted after the incident and continued to produce power (relatively) safely for many years. Unit 2 was shut down in 1991 after a turbine fire. Unit 1 was decommissioned in 1996. Unit 3 was decommission in 2000 after the international community forced their hand in exchange for funding the New Safe Containment sarcophagus over Unit 4. I believe there are still 7 (retrofitted) RBMK reactors in use today. Kursk-3 and 4, Leningrad-3 and 4, and Smolensk 1-3. Last one won't be shut down until 2034. reply carlmr 1 hour agorootparentprev>Nobody is going to repeat that bonehead move. Maybe not that exact one, but curious humans continue to do boneheaded stuff. reply _visgean 12 hours agorootparentprev> The Soviet Union would have kept Chernobyl a secret if Sweden hadn't detected the radiation they would try but it would leak one way or another. Simply too many people involved... reply rlpb 13 hours agorootparentprev> ...it'd have to involve a serious overhaul of the very concepts of corporations and liability in America Don't insurance companies solve this problem? Require the nuclear corporation to have liability insurance by regulation. Then the liability is shifted to an underwriter who has much more diversified risk. reply whatshisface 12 hours agorootparentThat sounds like a great idea, if you can get it implemented. Policymakers seem to be going in the opposite direction with things like tort reform (dollar value limitations on damages), and insurers who make exceptions for \"acts of God,\" like major natural disasters. Hundred-billion-dollar insurance policies on the habitability of areas several counties in expanse, along with courts willing to resettle entire cities as part of treble damages is far from the system we have! These astounding things would be necessary to fully replace regulation with liability, even for the often-seen-as-less-apocalyptic chemical industry, which is nonetheless capable of killing thousands in extremis, and that would have to be insured. reply jltsiren 12 hours agorootparentprevThey would solve the problem by making nuclear power too expensive for anyone to consider. Insurance companies don't like risks they can't quantify reliably. They either won't underwrite such risks at all, or they charge unreasonable premiums. The risks of nuclear power are still largely unknown, as there have been only two major disasters over a few decades. Both of them cost ~$200 billion, at least according to some estimates. We don't know how much worse the reasonable worst case could be. Chernobyl and Fukushima were also one-off disasters with specific causes. Insurance companies would also have to be prepared for multiple disasters in a short period of time caused by systemic issues in the design of a particular reactor type. reply pdonis 11 hours agorootparent> The risks of nuclear power are still largely unknown, as there have been only two major disasters over a few decades. No, the risks of nuclear power are very well known, as there have been only two major disasters over quite a few decades, and the root causes of both of them are well known and easily avoidable. > We don't know how much worse the reasonable worst case could be. Yes, we do. We know that Chernobyl was worse than any reasonable worst case for any other reactor, because nobody is going to build another reactor with a positive void coefficient of reactivity and with no secondary containment, and then run uncontrolled experiments on it. And we know that Fukushiman was a reasonable worst case, because it subjected a just shut down reactor to zero decay heat removal for an extended period, and that is going to be the consequence of the worst possible accident that could happen to a running reactor, since any such accident will shut the reactor down. In other words, in a sane world, the risks of nuclear power would be more insurable than the risks of, say, coal power, precisely because the nuclear risks are contained. No insurance company is going to sign up to liability for deaths due to respiratory failure from breathing coal dust over a period of many years. But in our insane world, we don't hold anyone accountable for such risks, so we end up treating nuclear, which is far safer per unit of energy generated than any source except solar, as if it were the riskiest of all sources. reply jltsiren 7 hours agorootparentYou are talking about risks in a narrow technical sense. Insurance companies are more interested in the consequences of the disaster. A reasonable worst case is something that could plausibly happen once in a century in the entire world with 10x more reactors than today. Maybe the reactor is located close to a major city. Maybe the company operating it has become incompetent and corrupt. Maybe the weather conditions are particularly unfavorable and most of the fallout goes towards the city. Maybe the meltdown was caused by a natural disaster that also makes evacuating the city harder. Maybe there are also widespread protests happening for an unrelated reason. Or maybe the worst case is the incompetent and corrupt power company upgrading its reactors in a way that leads to a large number of meltdowns. Remember that we are talking about what happens when the power company cannot pay and we don't want to socialize the risks. This is not something where the insurance company is allowed to set the terms. If courts and/or politicians ultimately determine that the power company is liable for $1 trillion, but its assets were only worth $100 billion, the insurance company must cover the rest. reply roenxi 4 hours agorootparentThat is a mistaken view; he's actually talking about liability. The risk of nuclear is smaller than the damage of the status quo, but it is so unexpected that we hold people liable when it materialises. The damage from coal (not risk because the risk is technically 0 - the bad outcome is guaranteed by design) is substantially greater than the risk of a nuclear disaster. We just don't hold anyone liable for fossil fuel damage because then society would collapse. It is pretty stupid that we don't move to nuclear and highlights the inability of the anti-nuclear crowd to do cost-benefit analysis. reply II2II 10 hours agorootparentprev> No, the risks of nuclear power are very well known, as there have been only two major disasters over quite a few decades, and the root causes of both of them are well known and easily avoidable. I think the statement that you were responding to would be better phrased as: \"The risks of nuclear power are still largely unknown, since there have been only two major disasters over a few decades.\" Insurance companies are typically okay with insuring against high cost, high frequency occurrences as long as they can quantify it. If they can quantify it, they can set a price on it. It is also worth noting that insurance companies were among the first adopters of digital computers because the better they can quantify frequencies and costs, the better they can set rates. reply pdonis 9 hours agorootparent> I think the statement that you were responding to would be better phrased as: \"The risks of nuclear power are still largely unknown, since there have been only two major disasters over a few decades.\" That was how I interpreted the statement when I responded to it. > Insurance companies are typically okay with insuring against high cost, high frequency occurrences as long as they can quantify it. Insurance companies are also good at specifying insurable risks and providing incentives to the insured to manage them properly, even for rare but high cost events. That includes refusing to insure people who the insurance companies judge to not have good judgment. For example, no sane insurance company would ever have insured Chernobyl given its insane reactor design, nor would any insurance company have agreed to pay a claim when they found that the reactor operator ran an uncontrolled experiment on the reactor and that was what caused the accident. As for Fukushima, as I commented elsewhere upthread, an insurer covering a reactor in an earthquake/tsunami zone would probably either not include tsunami inundation coverage in the policy, or would require a separate rider that would be priced separately. And such a rider would also have a list of requirements for coverage, which would include things like \"don't site your backup power and switchgear where a tsunami might inundate it\". In other words, as I pointed out, the risks of nuclear power are well known, because we understand how reactors work and what the failure modes are and insurers therefore know how to write policies that set the right incentives for managing risk. That is true even though we have had only two major disasters over quite a few decades; in fact, as I pointed out, the fact that we have had only two such disasters, and both of them were due to root causes that are easily avoidable in the future, is a reason to be more confident that the risks are well understood and insurable. reply bluejekyll 9 hours agorootparentprevThree Mile island was very close to being a major disaster, and upon further review had more radiation escape than was initially disclosed. The list of all of these nuclear incidents is interesting, many of them seem to be various components of the system exploding for various reasons, which could have led to horrible outcomes but luckily did not: https://en.m.wikipedia.org/wiki/Nuclear_and_radiation_accide... reply pdonis 6 hours agorootparent> Three Mile island was very close to being a major disaster No, it wasn't. In fact, the plant operators made just about every possible mistake they could have made--and still no harm to the general public. TMI was actually an illustration of how good design of safety systems can protect the general public even when reactor operators make multiple bonehead mistakes. Unfortunately the media narrative, as in so many other cases, drastically misrepresented what actually happened. > and upon further review had more radiation escape than was initially disclosed. Yes, but still zero harm to the general public. > many of them seem to be various components of the system exploding for various reasons, which could have led to horrible outcomes but luckily did not I do not see any incidents with commercial power reactors that fit this description. reply seanp2k2 10 hours agorootparentprevYep, it's a perception issue that isn't helped by the fact that to normies, \"nuclear\" means \"bomb\", and the oil and gas industries have massively more lobbying dollars to get politicians to push their solutions above all others. reply londons_explore 10 hours agorootparentprev> Insurance companies don't like risks they can't quantify reliably. They either won't underwrite such risks at all, or they charge unreasonable premiums. I believe there is a gap in the market for a new type of insurance company which regulates/inspects the insured to reduce the risks (and hence increase their profit margins while premiums get lower). reply bbarnett 12 hours agorootparentprevThere'd just be a run down to lowest policy price, and then after a disaster the government would have to bail out. Which it does with glee. reply wpietri 12 hours agorootparentprevAnd I'd add that nuclear power is worse here than even standard capitalism. In the US, the federal government provides a massive subsidy in the form of free insurance: https://en.wikipedia.org/wiki/Price%E2%80%93Anderson_Nuclear... And that's before we get to all of the other subsidies, cataloged here: https://www.ucsusa.org/resources/nuclear-power-still-not-via... I'm a big fan of free and fair markets, and think that they have been woefully underapplied in recent decades. Happily there's an uptick in antitrust regulation, so maybe we're moving in the right direction. reply whatshisface 12 hours agorootparentAll regulated industries receive liability breaks as \"compensation\" for being regulated. For example, many classes of product approved by the FDA are considered immune from liabilities related to dangers discovered in application[0], in particular, medical devices[1]. I agree that it is likely to distort the markets and incentivize regulatory capture. [0] https://en.wikipedia.org/wiki/FDA_preemption [1] https://en.wikipedia.org/wiki/Riegel_v._Medtronic,_Inc. reply wpietri 12 hours agorootparentDo you have more examples for this \"all\"? There are a lot of regulated industries out there, and I haven't heard of liability exemptions for them. E.g., food is highly regulated, but there are plenty of lawsuits over that. Ditto cars. Or banks. Or airplanes. From what I've seen, I think things like the Price-Anderson Act are less about universal liability protections for regulated activities, and more about special pleading by people with money. It seems to me that the correct response for \"this is so dangerous nobody will insure it\" is not \"well fuck it, go ahead and the government will cover your losses.\" reply pdonis 11 hours agorootparentprev> In the US, the federal government provides a massive subsidy in the form of free insurance While I'm no fan of this act on general principles and would be fine with seeing it repealed, to call it a \"massive subsidy\" ignores the fact that, as the Wikipedia article you link to notes, the secondary insurance provided under it has never been used. Every claim has been within the amount of primary insurance coverage, which is bought on the open insurance market at normal rates. This is strong evidence that nuclear power is in fact an ordinary insurable risk and the government does not need to take measures like this act. reply wpietri 10 hours agorootparentNo, insurance doesn't work like that. The cost of the Fukushima disaster is something like $200 billion. An insurer isn't going to say, \"Well gosh, no US plant has ever had a problem, so we're happy to write a $250 billion policy for the same price as a $15 billion policy.\" ($15 billion being the liability cap beyond which taxpayers are on the hook.) The subsidy here is the price difference between having full insurance and price of only having to insure to the cap, cumulatively since 1957, for every nuclear plant in the US. It's hard to put a number on that, but it's wildly larger than zero. reply pdonis 9 hours agorootparent> The cost of the Fukushima disaster is something like $200 billion. Yes, and the root cause is well known and easily avoidable, so any insurer can simply require that it is not present in an insured system--in this case, that backup generators and switchgear for decay heat removal are sited where they can't be inundated by a tsunami. Exactly as the backup generators and switchgear for all other reactors at the same site were. In fact, that very observation highlights a problem with the absence of insurance: nobody was independently checking for exposure to risk. An insurer would have asked why the backup generators and switchgear for that one reactor were sited differently from all the others at the site before agreeing to provide coverage. Also, insurance is not necessarily blanket coverage for everything at a single price. An insurer of a reactor might not include tsunami inundation in the policy--or might require a separate rider, priced separately, for that particular risk, which will have a different risk profile than other risks associated with the reactor. > The subsidy here is the price difference between having full insurance and price of only having to insure to the cap While this was true back in 1957, when there was no real data on commercial nuclear reactor operational risks, that is not the case now and has not been the case for quite some time. I would expect the fact that no claims have gone over the cap for decades to be reflected in pricing for insurance up to the cap being basically the same as it would be for insurance to a higher maximum liability. OTOH, if insurance for this particular risk is not a very competitive market--which it might well not be precisely because of government interference in this area--then the \"subsidy\" the government is providing is just canceling out what would otherwise be an unwarranted increase in premiums by insurers who effectively have monopoly pricing power. All that said, I'll reiterate that I would be fine with having this law repealed and letting nuclear compete in a free market with other energy sources--of course provided that we treat all those other energy sources the same and the government does not favor or penalize any of them. I just think that nuclear would do better in such a regime than many others appear to think. reply wpietri 9 hours agorootparent> root cause is well known and easily avoidable Your belief is that American insurers carefully check not only that particular risk but all other potentially catastrophic risks for every reactor they insure? I'd like to know the basis for that belief. > I would expect the fact that no claims have gone over the cap for decades to be reflected in pricing for insurance up to the cap being basically the same as it would be for insurance to a higher maximum liability. I see. And what's your expertise such that your personal belief is relevant to what actually happens here? reply bruce511 7 hours agorootparentprevWhile you are correct, this is one of those cases where statistics can be misleading. \"The other source which is heavily influenced by a few large-scale accidents is hydropower. Its death rate since 1965 is 1.3 deaths per TWh. This rate is almost completely dominated by one event: the Banqiao Dam Failure in China in 1975. It killed approximately 171,000 people. Otherwise, hydropower was very safe, with a death rate of just 0.04 deaths per TWh – comparable to nuclear, solar, and wind.\" [1] While there are reasons for and against nuclear and hydro, their safety difference is not material. [1] https://ourworldindata.org/safest-sources-of-energy reply bawolff 7 hours agorootparentI mean, you could probably say the same thing about nuclear - its dominated by a small number of very bad accidents like chernobyl. reply bruce511 5 hours agorootparentYes, both are inherently safe with only a small number of mishaps. From a safety point of view they are intrinsically \"the same\". reply legulere 11 hours agorootparentprevDeath rate is just one measure for safety, there are also other ways of harm like evacuation. Another way risks of nuclear are underestimated are compound effects over long durations. A million years and a thousand years are easily grouped together under very long, but at a incidence of once per thousand years it's the difference between 1 and 1000. reply usrusr 10 hours agorootparentprevVastly more electricity has been supplied by hydro. Nuclear is tiny. reply arcticbull 4 hours agorootparent... no, it's not. They're similar order of magnitude. Nuclear provides about 10% of global electricity, and hydro around 17%. In the US nuclear is about 18% while hydro is around 6%. [1] [1] https://www.eia.gov/tools/faqs/faq.php?id=427&t=3 reply Retric 10 hours agorootparentprevHydro actively saves lives by reducing flooding. It’s net death toll is negative. reply smallmancontrov 15 hours agorootparentprevIf we had kept up the pace of nuclear construction, we would have finished decarbonizing our electric grid by now. Instead, we've barely started. Was it worth the extra 100Gt of CO2 in the atmosphere? reply Filligree 14 hours agorootparentYes. Extra CO2 and global warming is far less likely to scuttle your reelection attempts, and voter deaths don't matter if they're diffuse enough no-one will point their finger at you. reply contravariant 13 hours agorootparentDiffuse isn't the most important bit, the main point is that they happen during the next election cycle. reply legulere 11 hours agorootparentprevYearly added renewable generation (not capacity, but actual generation) is already much larger than added nuclear power ever was. reply jajko 14 hours agorootparentprevPeople clearly prefer slow silent mass death (and messing up world for good for grandchildren) over few-per-century bigger accidents (when literally everybody in the world takes notes and massively improves). Similar to car massive amount of death (not even going into secondary and tertiary effects) and nobody bats an eye, even if they know personally somebody who died like that. Yet every single plane crash and tons of folks are getting panic attacks. Human psychology fascinates me, but at the same time makes me outright sad. So much potential often wasted on utter stupidities, and even worse - its trivial for skilled people to manipulate masses into literally shooting off their own feet with 12 gauge shotgun buckshot, just play the fear tune well enough for long enough. reply rurp 12 hours agorootparentI think you're right to be sad about it, and I would add \"deeply concerned\" as well. Our society is at a point where the most dangerous risks are ones that the human mind is pretty bad at reasoning about. That's a big problem, to put it mildly. Climate change, nuclear weapons, bio threats, and runaway AI all pose immense risks, at scales that are hard to reason about intuitively. Hopefully we develop ways to better manage current and future risks before a big one blows up. reply tacocataco 14 hours agorootparentprevI think you might be interested in Adam Curtis' \"century of the self\" if long documentaries are your thing. reply HPsquared 13 hours agorootparentprevSee also, economic policy of the last 100 years or so. reply faeriechangling 5 hours agorootparentprevNuclear power is safely handled by humans, even accounting for every accident. Humans just have a lot easier time dealing with deaths that happen gradually for no definitive cause, like that caused by say burning coal, than sudden dramatic events which leave no doubt as to what killed people. If a nuclear accident killed 10k people tomorrow that still wouldn’t really change my mind. reply unglaublich 14 hours agorootparentprevStrange, that same observation and the corresponding 3 million worldwide deaths yearly didn’t change our attitude towards fossil fuel combustion and its air pollution. Even more remarkable, the fact that nuclear is kept out of the hands of corporations, but the oil industry is not might have been the cause that the latter put so much money and effort into the nuclear fear campaign. More people die of fossil fuel air pollution per year than have died of COVID. We might have saved more lives during the lock down by the reduction in pollution than the actual virus containment. reply AlexandrB 14 hours agorootparent> Strange, that same observation and the corresponding 3 million worldwide deaths yearly didn’t change our attitude towards fossil fuel combustion and its air pollution. Stranger still, alcohol is still legal even though there's \"no safe level of consumption\"[1]. [1] https://www.who.int/europe/news/item/04-01-2023-no-level-of-... reply admax88qqq 13 hours agorootparentIt's different when someone wants to put something into their own body vs someone wants to put something into the air and water we all share. Stop pretending it isn't. reply HPsquared 13 hours agorootparentprevEvery activity carries risk of one form or another. reply oblio 12 hours agorootparentTrue, but as we remove various forms of risks and prolong life, for sure there will be a point in the future where our 200-year-living descendants will say: why were those idiots actively, massively and collectively poisoning themselves? reply stevejb 3 hours agorootparentProbably not. They’ll understand it was the best we could do at the time, just like we understand that about our ancestors. reply asdff 9 hours agorootparentprev>the fact that nuclear is kept out of the hands of corporations American nuclear plants are ran by corporations. reply akira2501 11 hours agorootparentprev> 3 million worldwide deaths yearly Is this a count or a statistic? reply asdff 9 hours agorootparentprevAmerican nuclear plants are ran by corporations though reply briandear 13 hours agorootparentprevMore people have died from wind turbines than in all the nuclear accidents combined. Here’s some data from just Scotland: https://scotlandagainstspin.org/turbine-accident-statistics/ reply tolien 11 hours agorootparent> Here’s some data from just Scotland: No it's not (putting aside that they seem to be a bunch of anti-wind cranks). > Total number of accidents: 3493 If you drill into the PDF they link to: > 3493 Miscellaneous 31/12/2023 Whispering Willow North windpower facility, Franklin County, Iowa USA Last I looked, the USA hasn't been part of the UK in almost 250 years. But also their data is nonsense - they've counted one turbine catching fire as two separate incidents, and conveniently ignore that it was during a named storm when the turbine was exposed to winds gusting >70 knots and thousands of people lost power due to downed power lines [0]. Even if you take their number as gospel, which seems unwise given they're counting this as an accident: > \"World's biggest wind farm makes bid to force land sales\". Report that SSE Renewables, owners of the Berwick Bank wind farm project off the cost of East Lothian, Scotland, are seeking to force landowners to sell land to operators of the Berwick Bank project. > Their application to Scottish Ministers to use a Compulsory Purchase Order for lands has brought a stream of objections from private land owners, Network Rail, and EDF who operate Torness nuclear plant. The application will now go to public enquiry. The FT [1] claims 2202 deaths due to Fukushima: > There were 2,202 disaster-related deaths in Fukushima, according to the government’s Reconstruction Agency, from evacuation stress, interruption to medical care and suicide 0: https://www.metoffice.gov.uk/binaries/content/assets/metoffi... 1: https://www.ft.com/content/000f864e-22ba-11e8-add1-0e8958b18... reply patall 13 hours agorootparentprevThe wind number seems to include everything, i.e transport etc. To have a fair comparison, one would need the number of premature deaths in uranium ore mining etc. Without that, it is kind of apples to oranges. (And I don't want downplay wind turbines, I fully aware that maintanance on a 200m high pole is dangerous) reply pfisch 13 hours agorootparentprevIt is hard to say how many people died from chernobyl. Some accounts say more than 30k(https://en.wikipedia.org/wiki/List_of_nuclear_and_radiation_...) Your link shows ~200 from wind turbines. reply arcticbull 4 hours agorootparentAbsolutely not, the USCEAR report is pretty much the gold standard, and the number is no more than 4,000. I've read the report and I'd suggest if you're curious that you do the same. The 4,000 number includes fully realized expected number of people who would contract cancer in their lifetimes, etc. And it uses the contested and very pessimistic linear no-threshold dose response model for very low doses. It also includes increase in suicide risk due to people believing they're 'contaminated.' As your link shows the direct deaths were around 78. Everything else beyond that is a statistical estimate, and the more anti-nuclear the group running the numbers is, the exponentially larger the \"death toll.\" The largest estimate came in at a staggering 985,000 -- despite there only being 350,000 people in what is now the CEZ. reply amluto 12 hours agoparentprevI recall reading some rather concerning accounts from plant maintenance workers in chlor-alkali plants. It wasn’t some nicely contained chunk of asbestos — it sounded like fluffy asbestos got everywhere. reply adrr 13 hours agoparentprevSomeone still has to mine it. reply Gibbon1 13 hours agoparentprevI think they now use a polymer membrane that works better at keeping the two products separate. reply ars 14 hours agoparentprevThis has an overview: https://www.etui.org/sites/default/files/ez_import/Asbestos_... Nothing is in the final product, and there is no worker contact except around every 10 years as the cell is replaced. reply GenerWork 13 hours agorootparentThanks for the link. Seems strange that this industry would be singled out considering that asbestos exposure seems really limited. reply cqqxo4zV46cp 12 minutes agoprevI don’t know what I’m more surprised-but-not-actually-surprised by: the fact that the US is this far behind the rest of the world, or that the comments are packed full of American debating the merits of this ban as if they’re in some sort of vacuum. When it comes down to it, the typical born-and-bred American truly cant help but engage in some good old fashioned American exceptionalism. This is great news, worldwide. Presumably the US was a major buyer of some of the asbestos products that were up-til-now legal to sell. As that market gets smaller, there’ll be less of a chance of asbestos-laden products fraudulently making their way into other markets with more same asbestos bans. As a resident of one of those countries, thank you for getting your act together, I guess. reply cat_plus_plus 6 hours agoprevI would rather not categorically ban anything. I don't want lots of arsenic in my food, but arsenic also has unique industrial uses where it can be prevented from leaching into the environment. Maybe asbestos turns out to be the only thing we can think of to enable exploration of other planets, or heat storage of zero carbon energy. Or maybe in time asbestos can be treated to prevent release of airborne fibers. I certainly wouldn't use asbestos in residential buildings or consumer products barring unforeseen breakthroughs to make it safe. But treat each use case separately, there could be rare ones where it's worth the risk. reply mkl95 14 hours agoprevMy building still has some asbestos pipes. Removing that stuff can be expensive and annoying, so it is usually delayed again and again. reply Sharlin 14 hours agoparentThey're essentially harmless as long as they're in there. Removing the stuff is the dangerous part. reply elromulous 13 hours agorootparentHarmless, until some contractor accidently disturbs it, and people have to have their home vacated (just happened to a friend of mine). That story might just be an anecdote, but the US has really normalized the \"it's ok to live with poison, just don't awaken it\". Same with lead pipes, lead paint, etc. Government remediation is long overdue. This is a price we pay as a society, we should treat it as a society. reply anonuser123456 12 hours agorootparentPretty harmless to the building occupants. The contractor is at risk it they are doing it everyday for a decade. reply wtfwhateven 11 hours agorootparentno, it's dangerous for everyone, not just contractors. reply alex_lav 11 hours agorootparentCare to explain? reply usrusr 10 hours agorootparentIf it's dangerously unknown to the contractor, chances are inhabitants won't know better. The contractor only exposed for a few hours, inhabitants live there. reply anonuser123456 8 hours agorootparentAverage construction zone concentrations of ACM during demolition hover around 1fcc. It can be higher for some specific cases (zonolite or high concentration asbestos insulation) but for general things like contaminated drywall, duct wrapping etc it is generally low. This is during actual demolition of ACM mind you, not just disturbing a little drywall here and there. When disturbance is completed, concentrations fall of with the air exchange rate and clear usually with 24-48 hours back to background levels. Diluting that by the volume of the entire building not under construction would generally put that at or below the 0.1fcc OSHA limit without PP. The OSHA limits assume 1 extra cancer death/300 workers for someone with maximum exposure limits for 8 hours per day, 250 days / year for 30 years. That is to say… A contractor spending a day kicking up asbestos results in 1/7500th ish of the dose required to cause an excess of 1/300 deaths. And this assumes that acute low exposures hold the same linear dose response rate as high dosage chronic exposure. That is a controversial assumption in the literature. My information is based on reading over 50 asbestos related journal articles, EPA and OSHA policy documents etc. From all the informarion available, it’s just not that big a deal for the average building occupant. reply dangus 9 hours agorootparentprevContractor disturbs aesbestos, HVAC systems could spread the particles around the whole building. That's a simple example. reply alex_lav 9 hours agorootparentExcellent. It’s nice to see a comment with more substance than “nu uh”. reply zipping1549 11 hours agorootparentprevIt's as harmless as a defective WW2 grenade in a garage. reply anonuser123456 11 hours agorootparentSaid no epidemiologist ever. reply akira2501 11 hours agorootparentprev> Harmless, until some contractor accidently disturbs it, and people have to have their home vacated (just happened to a friend of mine). That's inconvenience. Which was systematically invoked because the harms are well understood and tended to. reply faeriechangling 5 hours agorootparentprevThis is a community based out of San Francisco, the once city levelled by the San Andreas Fault. Accepting low probability dangers to reduce costs is a perfectly reasonable tradeoff. reply andbberger 12 hours agorootparentprevidk \"it's in a bucket forever\" sounds like a pretty good remediation strategy to me. dollar-for-dollar that government money probably has more impactful things to do. eg plastering public spaces in cities with bollards (see the awful awful west portal crash) reply nicup12345689 13 hours agorootparentprevnext [2 more] [flagged] fckgw 13 hours agorootparentWhat? reply Reason077 14 hours agoparentprevThe hazard with most asbestos is when it’s disturbed: usually by construction, repairs, renovation work, etc. As long as it doesn’t need to be disturbed by such work, and the hazard is well known/well marked to anyone contemplating it, in many cases the lowest-risk option is just to leave it be. reply alistairSH 8 hours agorootparent…the hazard is well known/well marked to anyone contemplating it But is it always marked? Or can a homeowner demo a wall and find themselves with lung cancer? reply 1123581321 7 hours agorootparentYes and no are essentially the answers to your questions. reply Dalewyn 9 hours agorootparentprevAlso known by this age old wisdom: If it ain't broke, don't fix it. For another point of comparison: Asbestos has been banned in new construction in Japan for a long time, but asbestos in old construction are all still there. They are only dealt with when the building is demolished, because the risk of just leaving it there is close to nil. reply seanp2k2 10 hours agoprevGiven sentiments about regulations and government agencies these days, I fully expect a new TikTok trend of snorting lines of asbestos to pop up very soon. reply fransje26 45 minutes agoparentIt would be a bit like washing your hands in tetraethyllead to show that it's totally safe. reply ourguile 15 hours agoprevGreat news, friend is currently dealing with asbestos remediation headaches. Can't imagine having to deal with someone so dangerous every day. reply azinman2 15 hours agoparentGoogling this it seems it doesn’t cause headaches. Also if you’re doing the remediation, you should have PPE and good protocols to protect you. reply hokumguru 14 hours agorootparentI think they meant metaphorical headaches dealing with asbestos remediation. As in difficulties. reply inglor_cz 15 hours agoprevAsbestos is a two-faced material. On one hand, it really helped to reduce frequency of devastating fires in the late 19th and early 20th century. Many lives saved. On the other hand, especially for people working with it, it is a harbinger of doom. reply timr 15 hours agoparent> On the other hand, especially for people working with it, it is a harbinger of doom. That's overstating it. We know how to work with the stuff safely, but yeah, you don't want to put it in places where unskilled people have access to it. If we can safely work with trans-uranic compounds and things like hydrofluoric acid, we can safely work with asbestos. reply stephen_g 8 hours agorootparentThe question though is do we need to safely work with asbestos? I've seen it removed properly by professionals, but I've personally witnessed too many times of people who should know better (mostly tradesmen who are working around this stuff frequently and should have proper training) doing dumb things and exposing themselves and others (one instance needing our whole 1960s era office building vacated for decontamination). This is in a country with a complete ban on new asbestos products since the '90s and strong building regulation, so it'd be even worse in countries with weaker regulation. And why use it? My house had a lot of asbestos-cement wall sheeting (fibro), I had much of it professionally removed and the safe plaster (for general rooms) and cellulose fibre-cement sheeting (for kitchen, bathroom) is just as good or better (the new fibre cement can be jointed nicely whereas the old asbestos cement sheeted rooms had visible seams). For insulation etc., fibreglass, rockwool, etc. can work just as well, maybe you need slightly thicker insulation but who really cares? There are safer alternatives for basically every application of asbestos, so why bother using something that is dangerous at every step (mining, transport, processing, manufacture, modifications (drilling, cutting, etc.), disposal) instead of alternatives? reply dredmorbius 7 hours agorootparentprevThe US produced ~31.5 megatons of asbestos between 1900 and 2003,[1] where it was widely used in construction, manufacturing, industrial processing, clothing, and even film sets and Christmas decor.[2] The US produced less than 112 tons of plutonium,[3] the principle trans-uranium element used industrially or militarily in the US, between 1944 and 1994.[4] This was, absent nuclear testing and grossly misguided military handling, not casually spread throughout the environment and most especially not in manufactured and constructed artefacts. And where plutonium contamination did occur it's been a multi-billion-dollar, multi-century environmental catastrophe. Your argument is specious and utterly disconnected from reality in the context of asbestos's past and present uses and distribution. ______________________________ Notes: 1.2. Snow & film sets: , cigarette filters: . 3. Others include curium, used in RTG \"nuclear batteries\"; americium-241, used in smoke detectors; and californium-252. All are effectively produced in trace quantities, on the order of kilograms at best, rather than tonnes. 4.reply inglor_cz 15 hours agorootparentprevWe know, as a civilization, yes, but that doesn't mean we do it. Especially in the developing world, manual workers' rights aren't very strong and plenty of people working in shipbreaking or material recycling don't have any PPEs, or barely any. People working with trans-uranic compounds are usually lab employees/scientists, whose employers value them higher and are willing to spend more money to protect them. reply XorNot 11 hours agorootparentprevExcept you really can't, for the same reason in both cases. Asbestos is only useful if you can use it everywhere, that's it's whole value - cheap and available. But it's literally worse then radiation because it's inert: you can't detect it easily, the dust persists and goes everywhere, and there's no way to reliably know without expensive testing if it's in a place. And once contaminated, you probably can't get rid of all of it. My backgarden is filled with the products of people demolishing asbestos containing fiber-cement board from some time in the 70s where evidently they just tossed it all into a section of retained wall and buried it (guess how I found out? Because I had to dig up the sewer pipe, and then discovered the reason the whole area is subsiding because it didn't magically compact itself over time either). reply akira2501 11 hours agorootparent> asbestos containing fiber-cement board from some time in the 70s where evidently they just tossed it all into a section of retained wall and buried it I wonder if they did this out of habit or because regulations made it difficult to dispose of otherwise? reply sidewndr46 6 hours agorootparentprevdid it wind up being like a big prominence in the backyard? I've found where the builders of my home buried tons of stuff. I have had to haul it all off myself. Thankfully most of it is just plaster and stuff. Oddly enough I think at least one employee was committing some form of sabotage, as I found a pit of never used materials like nails at one point. reply ClumsyPilot 8 hours agorootparentprev> If we can safely work with trans-uranic compounds and things like hydrofluoric acid, we can safely work with asbestos. 'We'? Certainly some highly-competent people can be trusted. But 'we'? Have you seen an average Builders in Britain? Have you witnessed the level of training of the tradesmen, the level of not not giving a flying **, and the level of cost cutting and skirting of regulations from the management? I've seen walls that aren't straight, windows installed upside down, insulation that it outright missing - the wall was just empty. Just this week my wife was closing 'professionally installed\" blinds and the mechanism fell off the ceiling and hit her on the head - the moron was too lazy to install the retaining screws. These are not people that should be trusted with anything more dangerous than a nail gun. maybe not even that. They behave like Javascript developers! reply rsynnott 11 hours agorootparentprevI mean, we can work with, say, plutonium, in extremely small amounts, in extremely controlled circumstances, and there are still problems. That’s very different to how asbestos was used, though. It used to be used _everywhere_. reply mschuster91 15 hours agorootparentprev> That's overstating it. We know how to work with the stuff safely, but yeah, you don't want to put it in places where unskilled people have access to it. Which includes everyone renovating their home. I recently backed out of a purchase because the sellers couldn't find out if there was asbestos used in the tile glue. If there were asbestos, it would have added a significant cost and especially >> 3 months of delay in moving in because people certified to work on that shit are more rare than gold. > If we can safely work with trans-uranic compounds and things like hydrofluoric acid, we can safely work with asbestos. The compounds you list are generally highly regulated, very difficult to get your hands on if you can't prove why you have a legitimate need for it, pretty expensive, and you won't find them outside of places that need to have it. As for asbestos, there are enough \"jack of all trades\" type handymen who don't give a fuck about safety - neither their own nor those of their client. That's why it got banned in the EU even for theoretically harmless usages. reply monknomo 15 hours agorootparentto be perfectly honest, we can't really work safely with trans-uranic compounds, just look at rock mountain flats reply babypuncher 15 hours agorootparentprevWe should consider ourselves lucky that it is not somehow cheaper for low cost homebuilders to fill their drywall with plutonium. reply simonw 15 hours agoparentprevThe R101 was a British hydrogen airship with 50 passenger cabins and a smoking room (despite being filled with hydrogen) - but the smoking room was lined with asbestos for safety. https://en.m.wikipedia.org/wiki/R101 reply asdff 15 hours agoparentprevOn yet another hand, house fires wouldn't be so devastating if they weren't making multifamily balloon framed timber structures. reply dukeofdoom 14 hours agoprevIt's still in schools in Canada, especially older grade schools, many of which still don't have AC. I sometimes wonder where all the taxes went to. reply asdff 9 hours agoparentIt's not an issue until you demo the building reply morkalork 11 hours agoparentprevI'd wager it's in most institutional sorts of buildings built in the 50s and 60s. reply grogenaut 14 hours agoprevAre they banning it in safety gear like bunker gear? reply shmerl 9 hours agoprevFinally. Corruption prevented it from happening more than 30 years ago. reply fransje26 39 minutes agoparentHere here. Lobbying has nothing to do with corruption. It's here to counterbalance over-zealous regulators who are detached from business reality. Please think of the shareholders. reply bilsbie 15 hours agoprevWere these remaining uses harmful? Is this just a PR victory? reply kube-system 15 hours agoparentBrake linings in particular were a harmful use, because they are worn away into dust during use, and mechanics in particular are exposed to a brake dust as an occupational hazard. reply Sharlin 14 hours agorootparentIt's crazy that this particular use case wasn't already banned. I mean, the stuff is only dangerous in particulate form – so why on Earth was it permitted to be used for things that by their very nature wear down to particulates during use? reply kube-system 14 hours agorootparentThere were previous attempts to ban it but they were overturned by courts in the 90s reply anonuser123456 12 hours agorootparentprevBecause the dose makes the poison. Low levels of asbestos exposure is statistically unlikely to harm you, and the concentrations from brake dust are relatively low. As long as a brake shop ventilates its work spaces it’s a negligible risk to workers. reply ramblenode 12 hours agorootparent> Low levels of asbestos exposure is statistically unlikely to harm you This is not correct. One could say that low levels of asbestos have not statistically been shown to cause harm, but that is quite different from statistically showing evidence of no harm. Harm may very well be occuring, but it is below the sensitivity threshold of our instruments to detect it. reply anonuser123456 11 hours agorootparentWhat ever harm it might be causing, is below the detection threshold, and thus meaningful risk tolerance of everyday life. Living near a freeway for instance is substantially more dangerous to your health than occasional incidental exposure to asbestos. You are breathing asbestos right now. In every breath. reply ramblenode 9 hours agorootparent> What ever harm it might be causing, is below the detection threshold, and thus meaningful risk tolerance of everyday life. The sensitivity of an instrument to detect harm has no relationship to whether there is some ground truth harm. In many cases, like smoking cigarettes or getting hit on the head repeatedly, harm is incremental and compounding---a single event may not be detectable, but it is almost certainly still harmful. reply anonuser123456 8 hours agorootparentEverything is harmful, from sunlight to food to driving. At issue is cost benefit. reply rodgerd 8 hours agorootparentprev> Because the dose makes the poison. This is a thought-terminating cliche, not a useful position. reply cjensen 15 hours agorootparentprevNormally modern brakes do not contain asbestos. All major car manufacturers have not used it in decades. Why was it still being used at all? reply kube-system 15 hours agorootparentGlobal OEMs haven't used asbestos for a while. They don't want the liability, and it makes logistical sense for them to use brakes that are legal in all markets. But some low-cost aftermarket replacement brakes imported from countries where it is still legal to manufacture them, have contained asbestos. Asbestos is cheap and functional. reply robocat 14 hours agorootparentAn acquaintance was trying to argue that Lamborghini brakes contained asbestos. There are places where asbestos is the ideal engineering material but my guess is that brakes is not one of them. reply kube-system 14 hours agorootparentThe phrase \"ideal engineering material\" doesn't make any sense. You can't define what is ideal until you define the design goals. If you want to optimize price and fire resistance, and you have no other goals, asbestos is ideal. But yes, VAG quite likely has other goals when designing a Lamborghini. reply robocat 13 hours agorootparentYou are being uninformatively pedantic, \"There are places\" is enough context for a throwaway sentence. Analysing every word or phrase is not productive and I think my meaning was clear enough. https://news.ycombinator.com/newsguidelines.html doesn't specifically mention nitpicking: I admit I have the same fault (Edit: rephrased this). Edit 2: perhaps I should have added that I think engineering is the art of making good compromises. reply kube-system 12 hours agorootparentI'm not being pedantic, as far as I understand the meaning of your comment. Asbestos is a very good material for making very cheap brakes, and in developing countries brakes are sometimes still made with asbestos for that reason. reply rsynnott 12 hours agorootparentprevI mean… it seems incredibly unlikely, given that the EU banned the manufacture, import and use of asbestos items, except for one extremely narrowly defined case where no acceptable substitute existed at the time (electrolysis diaphragms for chlorine production plants), about 25 years ago. Is their theory that Lamborghini ships the US models to the US brake-less, then adds special US-only brakes, for some reason? reply robocat 10 hours agorootparentThanks - I'm in New Zealand where we only banned it in 2016 and I didn't know about EU rules. It seems unlikely that Lambo would use asbestos. I had made an off-the-cuff bet against a pig-headed acquantance that asbestos brakes were banned here before I knew anything. He rang up his mate with a Lambo who had some other story! NZ has some import exceptions but it looks to me that the exceptions are mostly for something like QC training kits for recognising asbestos: https://www.worksafe.govt.nz/topic-and-industry/asbestos/imp... reply askvictor 10 hours agorootparentprevAbout 5 years ago so Chinese auto manufacturers had to recall some models as they had asbestos in the brakes, contrary to Australia's laws. reply asdff 15 hours agorootparentprevMechanics should be wearing sufficient PPE if they are working with particulate pollution. Take asbestos out of the brake dust but they still do things that generate plenty of particulate, which is only an occupational hazard when coupled with improper ppe. reply kube-system 15 hours agorootparentUnfortunately PPE is not always taken seriously by automotive tradespeople (and their employers). It is common to see both professional mechanics and home mechanics doing brake jobs without PPE, in many places. Also, smaller amounts of brake dust are distributed everywhere in the environment where vehicles are operated. reply IntrepidWorm 15 hours agorootparentprevSure, but ensuring all mechanics everywhere have access to use PPE, and then enforcing its correct use is way less effective in practice than removing the hazardous material from the workplace entirely. Being exposed to any fine particulate matter over long periods will be detrimental to health (we aren't evolved to breath large amounts of dust), but not every particulate is an acute carcinogen. reply robocat 14 hours agorootparent> we aren't evolved to breath large amounts of dust We are doing that evolution at present! The best way to do evolution is something that kills children before they breed. Second best for evolution is killing adults before they breed. Mostly ineffective is killing adults after they breed: although in theory loss of an adult can affect the population breeding chances downwards for children. Just a reminder that evolution is about breeding children and not so much about death. reply IntrepidWorm 12 hours agorootparentI'm just spitballing here, but I'd wager that the human toll required to evolve effective resistance to regular and prolonged asbestos exposure is more than most of us would be willing to pay. We already evolved these massive craniums filled with (to date) the most intricate and powerful general computers in the world - it seems like the solution to asbestos exposure is simply engineering a way to avoid it. No evolution necessary. reply asdff 14 hours agorootparentprevWe can play whack a mole getting every source of particulate out of the garage (can we even do that? consider sanding, grinding, painting, etc, not just the brakes are making this), or we can use our existing workplace safety enforcement mechanisms to enforce ppe in this industry like they've enforced ppe in many other industries. reply ziddoap 14 hours agorootparent>_or_ we can Or, and hear me out because I know this is crazy... we can do both! By trying to get rid of the bad stuff from the workplace and get better at enforcing ppe use. reply greenavocado 15 hours agorootparentprevIndependent mechanics never wear PPE and they are covered in dust and automotive fluids daily. reply asdff 14 hours agorootparentSo take out the asbestos and they are still at risk because the fundamental issue is a lack of PPE. You can regulate PPE. Many industries have done it. Shadetree work, sure, people do dumb things like cook hotdogs in shopping carts and sell that too, that will always happen. But for a brick and mortar mechanic they are already going though many regulations (e.g. how they deal with oil). PPE requirements are nothing in comparison to having to deal with things of that nature, or even just general small business requirements. reply kube-system 14 hours agorootparentHow? Staff an OSHA inspector to stand around in every garage? If Jim at Jim's Auto Service doesn't want to wear a mask, he's not going to wear one. It's not like oil where there's an evidence trail and a big mess if he decides to dump 50 gallons in the back lot. Yes, there's more that could be done in regards to PPE enforcement, but I think that's really an orthogonal issue. Asbestos isn't necessary in brakes, and can be banned regardless of PPE enforcement. reply asdff 10 hours agorootparentThe same way they manage to conduct health inspections even in the most random hole in the wall greasy spoon locations. You have an expectation for certain things to be there. You have an expectation that procedures are followed. When an incident occurs or an inspection happens where its clear that certain protections were not in place or certain procedures were not followed, you drop the hammer on them. Its not hard and we already have the bureaucratic apparatus to do this. Saying we can't do it for mechanics while we simultaneously do it for other industries like farming, construction, manufacturing, or food service, is just lazy. reply kube-system 8 hours agorootparentWhat you are describing is exactly how it is already enforced today. But an \"incident\" regarding asbestos (or other particle) inhalation might not appear until decades after exposure. Many tradespeople, automotive or otherwise, use the amount of PPE they personally are comfortable with. Construction is notoriously similar. You will see more compliance at larger organizations where there is more governance, but at smaller orgs, people often do as they please. reply robocat 14 hours agorootparentprev> shadetree https://shadetreehq.com/what-is-a-shadetree-mechanic/ Not a term I've heard in New Zealand. reply Scoundreller 15 hours agorootparentprevSounds like my shadetree approach means I’m steps ahead of the pros. reply azinman2 15 hours agorootparentprevHow often do you see mechanics with any kind of PPE? reply asdff 14 hours agorootparentHow is this an argument to them not needing to wear PPE? Asbestos isn't the only particulate risk they face. PPE on the other hand would solve the others. You can say you don't see mechanics wearing PPE but thats because its not presently regulated. How many kitchens today lack a handwashing sink? Zero, if they have passed a health inspection. reply kube-system 14 hours agorootparentHealth inspection failures are very common. Health inspections also only capture a specific a point in time. I have personally eaten at restaurants that initially passed a health inspection, yet failed later for not having a functional handwashing sink. e.g. https://medium.com/@michaelkduchak/predicting-chicago-health... Ensuring complete compliance at this level is very difficult. reply azinman2 13 hours agorootparentprevIt’s not an argument, but an observation. They deal in all kinds of chemicals and other nastiness, and I’ve never seen one wear PPE. I’ve even talked to mechanics about it and they don’t seem to care. reply asdff 10 hours agorootparentService your car at a dealer instead of the cheapest shop in town and you will see people gladly using ppe. reply toast0 14 hours agorootparentprevAlmost all professional mechanics are wearing coveralls. That's PPE. I've certainly seen a lot that wear gloves. That's PPE. Goggles or safety glasses are common but not ubiquitous in the shops I've seen. That's PPE. Ignoring masks that seem responsive to COVID, I haven't seen a lot of masks or respirators outside of shops where they're doing paint work, but I also don't get to see all the shop space and masks are intrusive, so probably if mechanics wear them, it's only when they perceive an accute risk. You don't usually keep your welding mask on all day, unless there's a lot of welding. reply lbotos 13 hours agorootparentprevMy younger brother worked at an independent BMW shop for a bit and they wore gloves all day. Anyone actively using airtools also had on ear muffs. Grinding, they'd wear a face shield. Yes, often ppl are dumb but they don't have to be. reply biscuits1 15 hours agorootparentprevRIP, McQueen :pray: reply adrr 13 hours agoparentprevSomeone has to mine it, pack it/assemble it. reply op00to 14 hours agoparentprevCheck back in 30 years. reply chucksta 15 hours agoparentprevTalc, ie baby powder. Its still too common in cosmetics too afaik reply kube-system 15 hours agorootparentTalc has sometimes been contaminated with asbestos, but it isn't a use of asbestos. reply black6 14 hours agorootparentprevOne tainted batch of talcum powder from J&J and now I have to use much less effective corn starch to powder my nether regions. reply gtvwill 14 hours agoprevProduct shouldn't be on market and should have global bans on its sale and installation/use. It's everywhere in building, and because it has dangers when removed or worked on, nobody wants to pay to get that work done correctly. Resulting in situations like we now have asbestos fibers found in mulch distributed across sydney. From kids playgrounds to the local council garden bed. All because someone wanted to avoid a fee whilst holding the perspective of \"It's not that dangerous\". Have pulled raw asbestos when drilling from like 300m+ down. Stuff is crazy pretty but is a pita to handle and keep safe. Looks almost like spicy fairy floss. reply anonuser123456 12 hours agoparentThe reason no one wants to pay the fee is because the exposure standards are so unreasonably low, that remediation costs huge $$$. And since we’ve made everyone terrified of the stuff, only specialized dumps handle it. Reasonable precautions that prevent 95% of exposure could be had at a fraction of the cost if people were more reasonable about the stuff. reply gtvwill 9 hours agorootparentYou ah wanna go roll that dice for that 5 percent exposure risk? Or are you just willing to pay me to roll that dice? Because if it's the latter we'll guess what I value my life fairly highly so you can pay up the high rates. I find most of the folks whinging about the costs are rarely involved in doing the work or taking the risk. reply anonuser123456 8 hours agorootparentYeah, I will roll those dice. I am steeped in the literature of asbestos demolition, exposure, health hazards etc. The current policy around ACM and its handling makes you _LESS_ safe because everyone looks the other way. It could be easily, cheaply disposed of by relaxing a number of protocols on how it’s handled during demolition. But because everyone has to follow the crazy moonsuit protocols, which improves the safety of the general public by absolutely zero, contractors just go find unknowing immigrant labor to blitz in and tear shit up. reply cqqxo4zV46cp 9 minutes agorootparent“The current policy around ACM and its handling makes you _LESS_ safe because everyone looks the other way.” is your own personal view, based on a hypothetical. You can be as steeped in the literature as you want. If you’re unable to separate the “I think”s from the “I know”s then you’re untrustworthy, no matter how knowledgeable you are. reply timr 15 hours agoprev> “For too long, polluters have been allowed to make, use and release toxics like asbestos and PFAS without regard for our health\" Criticizing politican-speak about science is almost pointless, but this is a ridiculous conflation of two wildly different things: asbestos, which is a specific mineral, and for which the link to lung cancer in humans is indisputable; and \"PFAS\", which is a hazy conglomeration of things for which the scientific link to harm is weak, and mostly based on animal studies or bad observational data. reply es7 15 hours agoparentI'm not really clear on the asbestos risk. Everything I've read and heard seems to indicate that the panic around asbestos might be overblown. Asbestos is unsafe but it is a matter of degrees. In certain cases like loose-fill insulation or certain situations where workers grind/cut asbestos regularly, it seems to cause a meaningful level of risk. Especially for those who are already smokers. But having gone through a remodel in a house with asbestos, I have been blown away at the extreme level of regulations, the meticulous procedures that remediation companies have to follow, the tens-of-thousands spent on remediation and repeated testing, and the tens-of-millions being thrown around in courts whenever Asbestos comes up. As best as I can tell, the risk is close to zero for minor and occasional exposure in otherwise healthy individuals. I'm open to seeing hard evidence to convince me otherwise. reply nostrademons 14 hours agorootparent> As best as I can tell, the risk is close to zero for minor and occasional exposure in otherwise healthy individuals. I'm open to seeing hard evidence to convince me otherwise. Asbestos is interesting in that the mechanism of carcinogenicity is very well-studied and well-understood. The fibers get into the lungs; the body can not get them out of the lungs; they cause persistent cell-damage as they mechanically rupture lung cells; and then the resulting chronic inflammation eventually causes cancer. Because it's so well understood, we also know how to protect against asbestos. If the fibers are never airborn, they can't get into the lungs. If you're wearing an N95 mask or respirator, they can't get into the lungs. If you can cough them out in the moment, they don't stay in the lungs. Once they're in the lungs, you're pretty well screwed. It's a sliding scale of how screwed, with more exposure causing more cancer risk, but the fibers are not coming out and will continue rolling cancer dice while they're in there. Having asbestos in your walls or ductwork is not going to kill you - the asbestos fibers aren't in the air. Doing a DIY reno on your asbestos-containing walls absolutely can kill you, and there have been cases of mesothelioma linked exactly to that. reply adriand 13 hours agorootparent> the fibers are not coming out and will continue rolling cancer dice while they're in there This is rather alarmist. The truth is more nuanced. This resource [1] lists a variety of biological mechanisms that work to remove asbestos fibers from the lungs beyond simply coughing them out, such as via \"alveolar macrophages\". > Doing a DIY reno on your asbestos-containing walls absolutely can kill you This is true, but if this made any readers anxious, it's important to note that \"light, short-term exposure rarely causes disease\" and that it is \"not uncommon for homeowners to do a renovation and then realize afterward that they disturbed asbestos products. Fortunately, the risk from this is low.\" [2] My advice is that if you are going to renovate your home, unless it is quite new and you have good reason to believe there is no risk of asbestos contamination, you should assume that materials like tiles, plaster, drywall, insulation, etc., may contain asbestos, and get them tested before commencing. However, if you have renovated in the past and are anxious about exposure, chill out. You can't change anything now, and unless you were renovating regularly, you'll very likely be fine. Remember that if you live in a rural area, you can be exposed to asbestos via natural weathering of rock. If you live in an urban area, you have likely been exposed to asbestos via construction and demolition work taking place nearby. [1] https://www.atsdr.cdc.gov/csem/asbestos/biological_fate_of_a... [2] https://www.asbestos.com/exposure/short-term/ reply anonymousiam 14 hours agorootparentprevLots of non-banned substances are more dangerous to breathe than asbestos. I understand the risks because I spent 20 years working in a building containing asbestos, and received annual notifications and warnings. It's been banned for use in construction for over 30 years, so I don't see how the EPA ban will make much difference. reply epicureanideal 14 hours agorootparentprevIs there any way to wash them out of the lungs? At the same time could smokers get a lung cleaning treatment? reply jijijijij 13 hours agorootparentNo. Because of their needle shape, they wander deep into the tissue. The notoriously associated cancer is found in the mesothelium, a layer around the lungs. reply thereisnospork 14 hours agorootparentprevIt probably could[0] be done, but good luck getting an FDA approval. [0]Speaking of smokers specifically, it is entirely possible to 'breathe' oxygenated liquid per fluorocarbons ('PFAS') which would very likely dissolve and 'wash out' tar from the lungs. reply ClumsyPilot 8 hours agorootparentprev> If you're wearing an N95 mask or respirator, they can't get into the lungs A random N95 mask is not gonna save a random Joe from asbestos. If you study how masks and respirators work, you will find that you need to be clean-shaven, the respirator must match the shape of your face and have a good seal, etc. People who work with hazardous substances spend a good amount of time on this. NHS had to discard huge number of masks during COVID because they weren't the right shape and weren't forming a seal. In some cases, that's good enough - general dust, woodcutting, etc. For highly toxic substances, it won't save you. reply AlexandrB 14 hours agorootparentprevIt's also under-appreciated how risky many common substances are when ground or cut. Cutting concrete, for example, can cause silicosis of the lungs[1] if precautions aren't taken. Wood dust is also potentially carcinogenic[2]. Then there's stuff like metal fume fever[3], which seems to be temporary but who knows what long term effects we'll discover in the future. [1] https://www.elcosh.org/document/1930/d000852/Dry+Cutting+%25... [2] https://www.ccohs.ca/oshanswers/chemicals/wood_dust.html [3] https://en.wikipedia.org/wiki/Metal_fume_fever reply nostrademons 14 hours agorootparentThe general rule of thumb should be \"don't breathe dust\", not regulating specific types of dust. Almost all forms of dust are bad for you. reply sidewndr46 6 hours agorootparentprevBerylliosis is the truly unexpected one for me. Something as harmless looking as a piece of ceramic can basically wipe out your lungs reply nxobject 12 hours agorootparentprevQuartz has a similar effect to concrete, too. reply timr 15 hours agorootparentprev> Everything I've read and heard seems to indicate that the panic around asbestos might be overblown. Asbestos is unsafe but it is a matter of degrees. Oh definitely. Like, if you ask the EPA, they'll tell you that there's \"no safe level of exposure\"...which is true at a population level (and completely understandable for a regulatory body to say), but terrorizes the kind of people who panic at the idea of chemicals. You don't want to be breathing the stuff when it's floating in the air, but people absolutely freak out over the idea of being near anything containing asbestos, even if the stuff is sealed in plastic or ceramic -- tons of old floor tiles contained it, for example. That's pretty obviously harmless, unless you grind it up and aerosolize it, but it triggers the same level of response as fraying asbestos pipe insulation. reply cmrdporcupine 14 hours agorootparentPart of what we're dealing with here is that asbestos present in a home harms not only your health but... potentially the perceived $$ value of your home. Your biggest financial investment. reply timr 9 hours agorootparentI'm not just talking about homes. Plenty of schools, museums and other public places spend huge amounts of money removing otherwise undisturbed asbestos. That said, it's more-or-less the same thing with old homes -- the \"we found asbestos; give us a discount\" thing is not really about rational perceptions of risk. If you buy an old home, you basically have to assume that it's going to have asbestos in it. reply hedora 8 hours agorootparentBanning asbestos prevents people from using asbestos in new houses / remodels. We bought a house, and only later found out that the materials used in its post-2000 remodel contained asbestos. reply sidewndr46 6 hours agorootparentBelieve it or not, that isn't even correct. The original ban allowed the usage of existing stocks of materials in all forms of construction until depleted. There is no tracking of all materials containing asbestos that were ever imported. Somewhere today there are still new builds going in with asbestos in them as a result. reply timr 8 hours agorootparentprevYeah, I get it. I'm not making an argument about banning the stuff in new construction. reply cmrdporcupine 7 hours agorootparentprev> spend huge amounts of money removing otherwise undisturbed asbestos. I mean, the legitimate concern here is that someone accidentally disturbs it, and in a stupid way, and now your kids are exposed. Even floor tiles, I assume if you cut into them with the wrong kind of saw in the wrong conditions could be spreading fine particles. Maybe. In any case, people don't behave rationally around risk. Nor do they understand statistics (evidence: lottery tickets get sold). But at the same time, overall it doesn't really hurt to get rid of this stuff, if done properly. reply Scarblac 14 hours agorootparentprevMy dad died of it some years ago, and we never knew where his lungs came into contact with the stuff. He never worked in construction, but sometimes near it. There is typically about 30 years between the contact with asbestos and getting ill. It's a very depressing diagnosis, there is no remedy and you just get gradually worse over a year or so until you die. reply uptown 12 hours agorootparentI’m very sorry about your dad and how his death must have pained your family. My dad also died from asbestos exposure leading to mesothelioma. His final months were a lot like what you described. reply throwitaway222 15 hours agorootparentprevIn 100 years I bet those same stringent policies will exist for fiberglass batt reply rayiner 13 hours agorootparentProbably not. Fiberglass has been studied for carcinogenicity specifically based on the experience with asbestos: https://connect.mayoclinic.org/discussion/fiberglass-insulat... (\"Fibers deposited in the deepest parts of the lungs where gas exchange occurs are removed more slowly by special cells called macrophages. Macrophages can engulf the fibers and move them to the mucous layer and the larynx where they can be swallowed. Swallowed fibers and macrophages are excreted in the feces within a few days. Synthetic vitreous fibers deposited in the gas exchange area of the lungs also slowly dissolve in lung fluid. Fibers that are partially dissolved in lung fluid are more easily broken into shorter fibers. Shorter fibers are more easily engulfed by macrophages and removed from the lung than long fibers.\"). We also have been unable to find clear evidence of health harms in longitudinal studies of fiberglass manufacturing workers: https://www.atsdr.cdc.gov/ToxProfiles/tp161-c2.pdf (\"Studies of workers predominantly involved in the manufacture of fibrous glass, rock wools, or slag wools have focused on the prevalence of respiratory symptoms through the administration of questionnaires, pulmonary function testing, and chest x-ray examinations. In general, these studies reported no consistent evidence for increased prevalence of adverse respiratory symptoms, abnormal pulmonary functions, or chest x-ray abnormalities; however, one study reported altered pulmonary function (decreased forced expiratory volume in 1 second) in a group of Danish insulation workers compared with a group of bus drivers.\"). reply throwup238 14 hours agorootparentprevThey'll exist for a lot of nanotech like carbon nanotubes too. Pretty much any rigid nanostructure has potential for same effect on the lungs as asbestos since it's caused by mechanical damage. reply observationist 13 hours agorootparentAsbestos repeats the injury, endlessly, with near immunity to any chemical decomposition. Mechanical decomposition just makes it more dangerous, as it cleaves into sharper, tinier needle like structures. Other nanostructures aren't nearly as chemically stable, especially inside the body, and can be metabolized or expelled from the body through natural processes.",
    "originSummary": [
      "The Environmental Protection Agency (EPA) has announced a sweeping ban on asbestos, a known carcinogen still present in some products, citing its severe health risks.",
      "The ban, a crucial measure for public health protection, aligns with efforts to regulate hazardous substances according to a 2016 law and will notably affect the chlor-alkali industry, with a gradual phase-out",
      "Certain industries have raised concerns about supply disruptions, while the EPA is assessing the use of asbestos in older buildings, receiving accolades from advocacy groups and legislators for safeguarding public health from toxic substances."
    ],
    "commentSummary": [
      "The discussion highlights the risks and regulations associated with asbestos exposure, political ideologies' impact on government decisions, and challenges in nuclear power liability.",
      "It explores debates on energy sources, safety regulations and insurance in high-risk industries, and the comparison of health risks posed by various substances.",
      "The importance of personal protective equipment in handling hazardous materials, societal reliance on industries like fossil fuels, and proper risk management across sectors are also addressed."
    ],
    "points": 353,
    "commentCount": 340,
    "retryCount": 0,
    "time": 1710780458
  },
  {
    "id": 39745993,
    "title": "Comparing Real-Time Server Communication Technologies",
    "originLink": "https://rxdb.info/articles/websockets-sse-polling-webrtc-webtransport.html",
    "originBody": "Articles WebSockets vs Server-Sent-Events vs Long-Polling vs WebRTC vs WebTransport On this page WebSockets vs Server-Sent-Events vs Long-Polling vs WebRTC vs WebTransport For modern real-time web applications, the ability to send events from the server to the client is indispensable. This necessity has led to the development of several methods over the years, each with its own set of advantages and drawbacks. Initially, long-polling was the only option available. It was then succeeded by WebSockets, which offered a more robust solution for bidirectional communication. Following WebSockets, Server-Sent Events (SSE) provided a simpler method for one-way communication from server to client. Looking ahead, the WebTransport protocol promises to revolutionize this landscape even further by providing a more efficient, flexible, and scalable approach. For niche use cases, WebRTC might also be considered for server-client events. This article aims to delve into these technologies, comparing their performance, highlighting their benefits and limitations, and offering recommendations for various use cases to help developers make informed decisions when building real-time web applications. It is a condensed summary of my gathered experience when I implemented the RxDB Replication Protocol to be compatible with various backend technologies. What is Long Polling? Long polling was the first \"hack\" to enable a server-client messaging method that can be used in browsers over HTTP. The technique emulates server push communications with normal XHR requests. Unlike traditional polling, where the client repeatedly requests data from the server at regular intervals, long polling establishes a connection to the server that remains open until new data is available. Once the server has new information, it sends the response to the client, and the connection is closed. Immediately after receiving the server's response, the client initiates a new request, and the process repeats. This method allows for more immediate data updates and reduces unnecessary network traffic and server load. However, it can still introduce delays in communication and is less efficient than other real-time technologies like WebSockets. // long-polling in a JavaScript client function longPoll() { fetch('http://example.com/poll') .then(response => response.json()) .then(data => { console.log(\"Received data:\", data); longPoll(); // Immediately establish a new long polling request }) .catch(error => { /** * Errors can appear in normal conditions when a * connection timeout is reached or when the client goes offline. * On errors we just restart the polling after some delay. */ setTimeout(longPoll, 10000); }); } longPoll(); // Initiate the long polling Implementing long-polling on the client side is pretty simple, as shown in the code above. However on the backend there can be multiple difficulties to ensure the client receives all events and does not miss out updates when the client is currently reconnecting. What are WebSockets? WebSockets provide a full-duplex communication channel over a single, long-lived connection between the client and server. This technology enables browsers and servers to exchange data without the overhead of HTTP request-response cycles, facilitating real-time data transfer for applications like live chat, gaming, or financial trading platforms. WebSockets represent a significant advancement over traditional HTTP by allowing both parties to send data independently once the connection is established, making it ideal for scenarios that require low latency and high-frequency updates. // WebSocket in a JavaScript client const socket = new WebSocket('ws://example.com'); socket.onopen = function(event) { console.log('Connection established'); // Sending a message to the server socket.send('Hello Server!'); }; socket.onmessage = function(event) { console.log('Message from server:', event.data); }; While the basics of the WebSocket API are easy to use it has shown to be rather complex in production. A socket can loose connection and must be re-created accordingly. Especially detecting if a connection is still usable or not, can be very tricky. Mostly you would add a ping-and-pong heartbeat to ensure that the open connection is not closed. This complexity is why most people use a library on top of WebSockets like Socket.IO which handles all these cases and even provides fallbacks to long-polling if required. What are Server-Sent-Events? Server-Sent Events (SSE) provide a standard way to push server updates to the client over HTTP. Unlike WebSockets, SSEs are designed exclusively for one-way communication from server to client, making them ideal for scenarios like live news feeds, sports scores, or any situation where the client needs to be updated in real time without sending data to the server. You can think of Server-Sent-Events as a single HTTP request where the backend does not send the whole body at once, but instead keeps the connection open and trickles the answer by sending a single line each time an event has to be send to the client. Creating a connection for receiving events with SSE is straightforward. On the client side in a browser, you initialize an EventSource instance with the URL of the server-side script that generates the events. Listening for messages involves attaching event handlers directly to the EventSource instance. The API distinguishes between generic message events and named events, allowing for more structured communication. Here's how you can set it up in JavaScript: // Connecting to the server-side event stream const evtSource = new EventSource(\"https://example.com/events\"); // Handling generic message events evtSource.onmessage = event => { console.log('got message: ' + event.data); }; In difference to WebSockets, an EventSource will automatically reconnect on connection loss. On the server side, your script must set the Content-Type header to text/event-stream and format each message according to the SSE specification. This includes specifying event types, data payloads, and optional fields like event ID and retry timing. Here's how you can set up a simple SSE endpoint in a Node.js Express app: import express from 'express'; const app = express(); const PORT = process.env.PORT || 3000; app.get('/events', (req, res) => { res.writeHead(200, { 'Content-Type': 'text/event-stream', 'Cache-Control': 'no-cache', 'Connection': 'keep-alive', }); const sendEvent = (data) => { // all message lines must be prefixed with 'data: ' const formattedData = `data: ${JSON.stringify(data)}`; res.write(formattedData); }; // Send an event every 2 seconds const intervalId = setInterval(() => { const message = { time: new Date().toTimeString(), message: 'Hello from the server!', }; sendEvent(message); }, 2000); // Clean up when the connection is closed req.on('close', () => { clearInterval(intervalId); res.end(); }); }); app.listen(PORT, () => console.log(`Server running on http://localhost:${PORT}`)); What is the WebTransport API? WebTransport is a cutting-edge API designed for efficient, low-latency communication between web clients and servers. It leverages the HTTP/3 QUIC protocol to enable a variety of data transfer capabilities, such as sending data over multiple streams, in both reliable and unreliable manners, and even allowing data to be sent out of order. This makes WebTransport a powerful tool for applications requiring high-performance networking, such as real-time gaming, live streaming, and collaborative platforms. However, it's important to note that WebTransport is currently a working draft and has not yet achieved widespread adoption. As of now (March 2024), WebTransport is in a Working Draft and not widely supported. You cannot yet use WebTransport in the Safari browser and there is also no native support in Node.js. This limits its usability across different platforms and environments. Even when WebTransport will become widely supported, its API is very complex to use and likely it would be something where people build libraries on top of WebTransport, not using it directly in an application's sourcecode. What is WebRTC? WebRTC (Web Real-Time Communication) is an open-source project and API standard that enables real-time communication (RTC) capabilities directly within web browsers and mobile applications without the need for complex server infrastructure or the installation of additional plugins. It supports peer-to-peer connections for streaming audio, video, and data exchange between browsers. WebRTC is designed to work through NATs and firewalls, utilizing protocols like ICE, STUN, and TURN to establish a connection between peers. While WebRTC is made to be used for client-client interactions, it could also be leveraged for server-client communication where the server just simulated being also a client. This approach only makes sense for niche use cases which is why in the following WebRTC will be ignored as an option. The problem is that for WebRTC to work, you need a signaling-server anyway which would then again run over websockets, SSE or WebTransport. This defeats the purpose of using WebRTC as a replacement for these technologies. Limitations of the technologies Sending Data in both directions Only WebSockets and WebTransport allow to send data in both directions so that you can receive server-data and send client-data over the same connection. While it would also be possible with Long-Polling in theory, it is not recommended because sending \"new\" data to an existing long-polling connection would require to do an additional http-request anyway. So instead of doing that you can send data directly from the client to the server with an additional http-request without interrupting the long-polling connection. Server-Sent-Events do not support sending any additional data to the server. You can only do the initial request, and even there you cannot send POST-like data in the http-body by default with the native EventSource API. Instead you have to put all data inside of the url parameters which is considered a bad practice for security because credentials might leak into server logs, proxies and caches. To fix this problem, RxDB for example uses the eventsource polyfill instead of the native EventSource API. This library adds additional functionality like sending custom http headers. 6-Requests per Domain Limit Most modern browsers allow six connections per domain () which limits the usability of all steady server-to-client messaging methods. The limitation of six connections is even shared across browser tabs so when you open the same page in multiple tabs, they would have to shared the six-connection-pool with each other. This limitation is part of the HTTP/1.1-RFC (which even defines a lower number of only two connections). Quote From RFC 2616 – Section 8.1.4: \"Clients that use persistent connections SHOULD limit the number of simultaneous connections that they maintain to a given server. A single-user client SHOULD NOT maintain more than 2 connections with any server or proxy. A proxy SHOULD use up to 2*N connections to another server or proxy, where N is the number of simultaneously active users. These guidelines are intended to improve HTTP response times and avoid congestion.\" While that policy makes sense to prevent website owners from using their visitors to D-DOS other websites, it can be a big problem when multiple connections are required to handle server-client communication for legitimate use cases. To workaround the limitation you have to use HTTP/2 or HTTP/3 with which the browser will only open a single connection per domain and then use multiplexing to run all data through a single connection. While this gives you a virtually infinity amount of parallel connections, there is a SETTINGS_MAX_CONCURRENT_STREAMS setting which limits the actually connections amount. The default is 100 concurrent streams for most configurations. In theory the connection limit could also be increased by the browser, at least for specific APIs like EventSource, but the issues have beem marked as \"won't fix\" by chromium and firefox. Lower the amount of connections in Browser Apps When you build a browser application, you have to assume that your users will use the app not only once, but in multiple browser tabs in parallel. By default you likely will open one server-stream-connection per tab which is often not necessary at all. Instead you open only a single connection and shared it between tabs, no matter how many tabs are open. RxDB does that with the LeaderElection from the broadcast-channel npm package to only have one stream of replication between server and clients. You can use that package standalone (without RxDB) for any type of application. Connections are not kept open on mobile apps In the context of mobile applications running on operating systems like Android and iOS, maintaining open connections, such as those used for WebSockets and the others, poses a significant challenge. Mobile operating systems are designed to automatically move applications into the background after a certain period of inactivity, effectively closing any open connections. This behavior is a part of the operating system's resource management strategy to conserve battery and optimize performance. As a result, developers often rely on mobile push notifications as an efficient and reliable method to send data from servers to clients. Push notifications allow servers to alert the application of new data, prompting an action or update, without the need for a persistent open connection. Proxies and Firewalls From consutling many RxDB users, it was shown that in enterprise environments (aka \"at work\") it is often hard to implement a WebSocket server into the infrastructure because many proxies and firewalls block non-HTTP connections. Therefore using the Server-Sent-Events provides and easier way of enterprise integration. Also long-polling uses only plain HTTP-requests and might be an option. Performance Comparison Comparing the performance of WebSockets, Server-Sent Events (SSE), Long-Polling and WebTransport directly involves evaluating key aspects such as latency, throughput, server load, and scalability under various conditions. First lets look at the raw numbers. A good performance comparison can be found in this repo which tests the messages times in a Go Lang server implementation. Here we can see that the performance of WebSockets, WebRTC and WebTransport are comparable: note Remember that WebTransport is a pretty new technologie based on the also new HTTP/3 protocol. In the future (after March 2024) there might be more performance optimizations. Also WebTransport is optimized to use less power which metric is not tested. Lets also compare the Latency, the throughput and the scalability: Latency WebSockets: Offers the lowest latency due to its full-duplex communication over a single, persistent connection. Ideal for real-time applications where immediate data exchange is critical. Server-Sent Events: Also provides low latency for server-to-client communication but cannot natively send messages back to the server without additional HTTP requests. Long-Polling: Incurs higher latency as it relies on establishing new HTTP connections for each data transmission, making it less efficient for real-time updates. Also it can occur that the server wants to send an event when the client is still in the process of opening a new connection. In these cases the latency would be significantly larger. WebTransport: Promises to offer low latency similar to WebSockets, with the added benefits of leveraging the HTTP/3 protocol for more efficient multiplexing and congestion control. Throughput WebSockets: Capable of high throughput due to its persistent connection, but throughput can suffer from backpressure where the client cannot process data as fast as the server is capable of sending it. Server-Sent Events: Efficient for broadcasting messages to many clients with less overhead than WebSockets, leading to potentially higher throughput for unidirectional server-to-client communication. Long-Polling: Generally offers lower throughput due to the overhead of frequently opening and closing connections, which consumes more server resources. WebTransport: Expected to support high throughput for both unidirectional and bidirectional streams within a single connection, outperforming WebSockets in scenarios requiring multiple streams. Scalability and Server Load WebSockets: Maintaining a large number of WebSocket connections can significantly increase server load, potentially affecting scalability for applications with many users. Server-Sent Events: More scalable for scenarios that primarily require updates from server to client, as it uses less connection overhead than WebSockets because it uses \"normal\" HTTP request without things like protocol updates that have to be run with WebSockets. Long-Polling: The least scalable due to the high server load generated by frequent connection establishment, making it suitable only as a fallback mechanism. WebTransport: Designed to be highly scalable, benefiting from HTTP/3's efficiency in handling connections and streams, potentially reducing server load compared to WebSockets and SSE. Recommendations and Use-Case Suitability In the landscape of server-client communication technologies, each has its distinct advantages and use case suitability. Server-Sent Events (SSE) emerge as the most straightforward option to implement, leveraging the same HTTP/S protocols as traditional web requests, thereby circumventing corporate firewall restrictions and other technical problems that can appear with other protocols. They are easily integrated into Node.js and other server frameworks, making them an ideal choice for applications requiring frequent server-to-client updates, such as news feeds, stock tickers, and live event streaming. On the other hand, WebSockets excel in scenarios demanding ongoing, two-way communication. Their ability to support continuous interaction makes them the prime choice for browser games, chat applications, and live sports updates. However, WebTransport, despite its potential, faces adoption challenges. It is not widely supported by server frameworks including Node.js and lacks compatibility with safari. Moreover, its reliance on HTTP/3 further limits its immediate applicability because many WebServers like nginx only have experimental HTTP/3 support. While promising for future applications with its support for both reliable and unreliable data transmission, WebTransport is not yet a viable option for most use cases. Long-Polling, once a common technique, is now largely outdated due to its inefficiency and the high overhead of repeatedly establishing new HTTP connections. Although it may serve as a fallback in environments lacking support for WebSockets or SSE, its use is generally discouraged due to significant performance limitations. Known Problems For all of the realtime streaming technologies, there are known problems. When you build anything on top of them, keep these in mind. A client can miss out events when reconnecting When a client is connecting, reconnecting or offline, it can miss out events that happened on the server but could not be streamed to the client. This missed out events are not relevant when the server is streaming the full content each time anyway, like on a live updating stock ticker. But when the backend is made to stream partial results, you have to account for missed out events. Fixing that on the backend scales pretty bad because the backend would have to remember for each client which events have been successfully send already. Instead this should be implemented with client side logic. The RxDB replication protocol for example uses two modes of operation for that. One is the checkpoint iteration mode where normal http requests are used to iterate over backend data, until the client is in sync again. Then it can switch to event observation mode where updates from the realtime-stream are used to keep the client in sync. Whenever a client disconnects or has any error, the replication shortly switches to checkpoint iteration mode until the client is in sync again. This method accounts for missed out events and ensures that clients can always sync to the exact equal state of the server. Company firewalls can cause problems There are many known problems with company infrastructure when using any of the streaming technologies. Proxies and firewall can block traffic or unintentionally break requests and responses. Whenever you implement a realtime app in such an infrastructure, make sure you first test out if the technology itself works for you. Follow Up Check out the hackernews discussion of this article Shared/Like my announcement tweet Learn how to use Server-Sent-Events to replicate a client side RxDB database with your backend. Learn how to use RxDB with the RxDB Quickstart Check out the RxDB github repo and leave a star ⭐",
    "commentLink": "https://news.ycombinator.com/item?id=39745993",
    "commentBody": "WebSockets vs. Server-Sent-Events vs. Long-Polling vs. WebRTC vs. WebTransport (rxdb.info)320 points by bubblehack3r 18 hours agohidepastfavorite160 comments kellengreen 16 hours agoI've always had a bit of a soft spot for Server Sent Events. Just simple and easy to use/implement. reply shams93 15 hours agoparentWith ipv6 they can now be fully scaled easily but they are absolutely awesome, much easier to scale because you can give your client a simple list of sse services and its essentially stateless if done right. Websockets get really complex to scale past a certain level of use. reply apitman 13 hours agorootparent> With ipv6 they can now be fully scaled easily Any day now: https://www.google.com/intl/en/ipv6/statistics.html reply bawolff 7 hours agorootparentprevWhat does ipv6 give you that virtual hosts don't? reply cushpush 4 hours agorootparentwhy dropbox when rsync\" reply freedomben 9 hours agorootparentprevis anybody actually able to disable ipv4? maybe if you only serve vpn or internal users? This might be the best thing about Elixir/Phoenix LiveView. I haven't actually had to care in quite some time :-) (though to be fair, I keep things over the websocket pretty light) reply ajvpot 16 hours agoparentprevI agree. Unfortunately you can only have 6 SSE streams per origin per browser instance, so you may be limited to 6 tabs without adding extra complexity on the client side. https://crbug.com/275955 reply TheP1000 16 hours agorootparentIs above still an issue with http2/3? edit: From the article: To workaround the limitation you have to use HTTP/2 or HTTP/3 with which the browser will only open a single connection per domain and then use multiplexing to run all data through a single connection. reply xialvjun 21 minutes agorootparentprevjust one tab use SSE and others use storage event. reply andrewmutz 15 hours agorootparentprevYou can get around that limit using domain sharding, although it feels a bit hacky. reply ravxx 16 hours agorootparentprevjust use a service worker to share state, you would be much better off doing this anyways. saves a ton and is performant. reply simonw 16 hours agorootparentI think you need a SharedWorker for that rather than a service worker https://developer.mozilla.org/en-US/docs/Web/API/SharedWorke... reply jedschmidt 7 hours agorootparentA service worker would work fine; the connection would be instantiated from the SW and each window/worker could communicate with it via navigator.serviceWorker. reply esprehn 5 hours agorootparentThat doesn't work because browsers have duration limits on ServiceWorkers: https://github.com/w3c/ServiceWorker/issues/980#issuecomment... Also unfortunately Chrome doesn't keep SharedWorker alive after a navigation (Firefox and Safari do): https://issues.chromium.org/issues/40284712 Hopefully Chrome will fix this eventually, it really makes it hard to build performant MPAs. reply nine_k 15 hours agorootparentprevCan the tabs share a background worker that would handle that? reply merb 15 hours agorootparentYou can use https://www.npmjs.com/package/broadcast-channel which creates a tab leader, no need for a background worker Edit: of course you could use: https://caniuse.com/sharedworkers but android does not support it. We migrated to the lib because safari took its time… so mobile was/is not a thing for us reply simonw 16 hours agorootparentprevIs that true if you are using HTTP/2? reply hobobaggins 9 hours agorootparentYes, but the limit is different (usually much higher) and negotiated, up to maximum SETTINGS_MAX_CONCURRENT_STREAMS (which is fixed at 100 in Chrome, and apparently less in IOS/Safari.) reply cassepipe 10 hours agorootparentprevNope. That's only a problem with HTTP/1.1 reply paulddraper 10 hours agorootparentprevHTTP 2/3 doesn't have they limitation. For HTTP 1, simply shard the domain. reply djbusby 16 hours agoparentprevWorks with bog-standard Apache prefork and PHP. reply paulddraper 10 hours agoparentprevThe downside is that you have to base64 payloads or otherwise remove newlines. I wonder why they didn't just a multipart streamed response. Supports my metadata, very commonly implemented format reply fswd 9 hours agoparentprevdon't forget the timeout reconnect! reply marban 16 hours agoparentprevAbsolutely underrated. reply spintin 13 hours agoparentprevSSE are really a subset of Comet-Stream (eternal HTTP response with Transfer-Encoding: chunked) only they use a header (Accept: text/event-stream) and wraps the chunks with \"data:\" and \"\". But yes it's the superior (simplest, most robust, most performant and scalable) way to do real-time for eternity. The browser is dead, but SSE will keep on doing work for native apps. reply tdudhhu 2 hours agoprevNot in the article by also relevant is short polling. While this does not send messages from a server to a client it can still be useful when all other options are not available (on shared hosting for example). In my experience it even works great when the poll interval is long (for example 20 seconds) but when you also include the message list in each response. That way the client will be up to date when it interacts with the server: user presses a button -> the client sends a request to the server -> the server reponds with data and also a list of the latest messages. reply kybernetikos 21 minutes agoparentIt's also applicable for fast changing data where the proportion of polls that gets an update is high too. reply apitman 15 hours agoprevA few additional cons to be aware of: WebSockets lack flow control (backpressure) and multiplexing, so if you need them you either roll your own or use something similar to RSocket. Also SSE can't send binary data directly. You have to base64 encode it or similar. WebTransport addresses these and also solves head of line blocking. But I'm concerned that we might run into a similar problem as we had with going from Python2 to Python3 and IPv6. Too easy for people to keep using the old version, and too little (perceived) benefit to upgrading. As long as browsers still work with TCP, some networks will continue to block UDP (and thus HTTP3/WebTransport) outright. reply cbsmith 15 hours agoparent> WebSockets lack flow control (backpressure) and multiplexing, so if you need them you either roll your own or use something similar to RSocket. Yes, head of line blocking is an issue, but TCP provides flow control, and if you're not using that, you're going over HTTP3. > WebTransport addresses these and also solves head of line blocking. But I'm concerned that we might run into a similar problem as we had with going from Python2 to Python3 and IPv6. Too easy for people to keep using the old version, and too little (perceived) benefit to upgrading. At one time or another, one could have said the same thing about TLS transport, HTTP3, or XHR itself. Because of the comparatively huge domination of a few key browser engines, it's much easier to roll out new browser capabilities & protocols. > As long as browsers still work with TCP, some networks will continue to block UDP (and thus HTTP3/WebTransport) outright. By that logic, as long as browsers still work with HTTP 1.1 without TLS, some networks will continue to block HTTP 2 and TLS. While that's not entirely incorrect, the broad adoption of HTTP2 and TLS in particular suggests it's less of a problem than you think. reply apitman 14 hours agorootparent> Yes, head of line blocking is an issue, but TCP provides flow control Unfortunately, the way browsers implement WebSocket it undermines TCP's flow control. It's trivial to crash a browser tab by opening a (larger than RAM) file and trying to stream it to a server using a tight loop sending on a WebSocket. WebSocket.bufferedAmount exists, but as of 2019 I failed to use it to solve this problem and had to implement application-level backpressure. > At one time or another, one could have said the same thing about TLS transport, HTTP3, or XHR itself. Because of the comparatively huge domination of a few key browser engines, it's much easier to roll out new browser capabilities & protocols. > By that logic, as long as browsers still work with HTTP 1.1 without TLS, some networks will continue to block HTTP 2 and TLS. While that's not entirely incorrect, the broad adoption of HTTP2 and TLS in particular suggests it's less of a problem than you think. HTTP3 actually falls under my concern. There are still networks that block HTTP3, because it has really nice fallback to HTTP2/1.1, so there's no obvious impact on users. So I guess the real question is will QUIC be an HTTP/2 or an IPv6, or something in between? Was HTTP/2 ever actively blocked the way UDP is? If so that certainly gives us hope. The reason I care is that I'm currently developing a protocol that WebTransport is an excellent fit for. But I can't assume WebTransport will work because UDP might be blocked, so I'm having to implement WebSocket support as well, which is a lot more work. reply cbsmith 12 hours agorootparent> Unfortunately, the way browsers implement WebSocket it undermines TCP's flow control. It's trivial to crash a browser tab by opening a (larger than RAM) file and trying to stream it to a server using a tight loop sending on a WebSocket. WebSocket.bufferedAmount exists, but as of 2019 I failed to use it to solve this problem and had to implement application-level backpressure. Before you were saying, \"WebSockets lack flow control (backpressure) and multiplexing, so if you need them you either roll your own or use something similar to RSocket.\", and now you're saying you can't roll your own? ;-) > HTTP3 actually falls under my concern. There are still networks that block HTTP3, because it has really nice fallback to HTTP2/1.1, so there's no obvious impact on users. Erm, HTTP2 & HTTP 1.1 have their own problems, some of which you yourself have identified. We actually rolled back from HTTP2 to HTTP 1.1 because of problems with HTTP2, particularly with mobile performance. Our migration to HTTP3 has been all win so far. While UDP might be blocked for security reasons, there are security reasons to move to HTTP3. That said, there are cases where only HTTP/1.0 is supported. > The reason I care is that I'm currently developing a protocol that WebTransport is an excellent fit for. But I can't assume WebTransport will work because UDP might be blocked, so I'm having to implement WebSocket support as well, which is a lot more work. I feel your pain. I've been using WebRTC, and the vast majority of the time UDP doesn't seem to be blocked anymore. That said, adoption of HTTP3 seems to be about half that of HTTP2 right now. Not bad for a brand new protocol, but I'd say we still have a significant amount of time to go before HTTP3 is the dominant protocol. I think the path forward is going to require some toil by the likes of you to support both, but no reason you can't support HTTP3 better! ;-) reply apitman 10 hours agorootparent> Before you were saying, \"WebSockets lack flow control (backpressure) and multiplexing, so if you need them you either roll your own or use something similar to RSocket.\", and now you're saying you can't roll your own? ;-) You can, but you need to do it at the application level, which is a lot more involved. It would be nice if it were as simple as checking if bufferAmount > some threshold and then waiting on a promise before attempting to send again. That's essentially what you get with ReadableStream and WritableStream, which are provided by WebTransport. > Erm, HTTP2 & HTTP 1.1 have their own problems, some of which you yourself have identified. We actually rolled back from HTTP2 to HTTP 1.1 because of problems with HTTP2, particularly with mobile performance. Not sure if we're actually disagreeing here. In any case, we can both agree HTTP2 is not a panacea, and actually worse than HTTP/1.1 in some cases. > That said, adoption of HTTP3 seems to be about half that of HTTP2 right now. Not bad for a brand new protocol, but I'd say we still have a significant amount of time to go before HTTP3 is the dominant protocol. Here here. Overall I'm bullish on HTTP3 in the long run. I really just hope random enterprise networks don't decide to block it. In any case, it's going to be a big win for the places where it works. reply 0x457 14 hours agorootparentprevHTTP/2 traffic was looking essentially the same as HTTP/1.1 + TLS. It wasn't particularly interesting to block to begin with. UDP if blocked, then it's blocked in a name of Security. reply laurencerowe 11 hours agoparentprev> As long as browsers still work with TCP, some networks will continue to block UDP (and thus HTTP3/WebTransport) outright. HTTP2 should still work in that scenario then you don't need to worry about multiplexing. reply apitman 10 hours agorootparentYou can't use HTTP2 directly in the browser. Or are you referring to something else? reply laurencerowe 4 hours agorootparentAll browsers released since the end of 2015 support HTTP2. If the server supports it the browser will use it. Unlike HTTP3 this is all TCP. If you connect to a WebSocket over an HTTP2 connection then you don't need to worry about multiplexing since you can rely on the browser doing it for you - HTTP2 connections support over 200 concurrent streams. reply 0x457 14 hours agoparentprev> As long as browsers still work with TCP, some networks will continue to block UDP (and thus HTTP3/WebTransport) outright. I keep hearing it, but I've never actually seen such a network. There are many things that run on UDP. I can see it being closed in some tiny offices (but those usually lack brain power to accomplish it) or some dystopian corporate offices you can only see in a movie. I really don't see how the fact that some networks might ban UDP has anything to do with it. Some networks ban google.com and wikipedia.com, you don't see them failing. reply apitman 14 hours agorootparentIn their excellent article on NAT traversal[0], Tailscale mentions that the UC Berkeley guest wifi blocks all outbound UDP except DNS. EDIT: According to this[1] issue, Berkeley guest wifi allows port 443, which would solve HTTP3/WebTransport. That's certainly hopeful, and I hope all networks are like this in the future. It's just not a forgone conclusion yet. [0]: https://tailscale.com/blog/how-nat-traversal-works#have-you-... [1]: https://github.com/danderson/natlab/issues/1 reply CodesInChaos 14 hours agorootparentprevNot a traditional network, but Tor doesn't support UDP. reply rvanmil 16 hours agoprevOr, if you’re building for clients with a traditional “enterprise” and “secure” IT infrastructure: add refresh buttons and call it a day. If there’s one thing in my experience that consistently fails in these environments and cannot be fixed due to endless red tape, it’s trying to make real-time work for these type of clients. reply palmfacehn 5 hours agoparentJetty/CometD will fall back to long polling if other transports are not available. reply cbsmith 15 hours agoparentprevHonestly, all the techniques for this stuff have their problems, including the refresh button. reply rvanmil 15 hours agorootparentTrue, though when carefully implemented it’s the most reliable option I guess. reply cbsmith 15 hours agorootparentI've switched to using SSE to get around problems with the refresh button. It's pretty simple and reliable. reply rvanmil 14 hours agorootparentThat’s great. Unfortunately I’ve seen SSE fail quite a few times in the scenario I described. reply chgs 15 hours agoparentprevMy browser has a refresh button. Alas your application likely breaks when I use it. reply rvanmil 15 hours agorootparentNot entirely sure why it would break…? reply chgs 11 hours agorootparentClearly I don’t know what your application is, but many heavyweight “web apps” don’t cope with a simple refresh, kicking back to a default screen or even login screen in some cases. reply hobobaggins 9 hours agorootparentSometimes this is to scope logins to single tabs for security reasons (I think that's why Userify does it that way). It's annoying but for infrequently used apps, no worse than getting logged out every three minutes. reply klysm 12 hours agorootparentprevclient side state that isn't in the URL or local storage reply skybrian 15 hours agoprevThis is probably naive, but it seems like assuming HTTP/2 or better, an EventSource combined with fetch() for sending messages should be just as good as any other protocol that uses a single TCP connection? And HTTP/3 uses UDP, so even better. (This all assumes you only care about maintaining a connection when the tab is in the foreground.) I’m wondering what problems people have run into when they tried this. reply apitman 13 hours agoparentOne limitation is SSE is text-only, so you can't efficiently send binary data. You have to encode it as base64 or similar. reply ksec 9 hours agoparentprevWas thinking exactly the same thing. H2 with SSE solves 99% of problems? I was wondering if we could push SSE even further along with lower latency, memory usage and CPU resources than doing something completely different. reply jacobr1 14 hours agoparentprevThis presumes the majority of your use case is server-client, but otherwise yes. reply Fabricio20 15 hours agoprevTo this day I still dont know why WebSockets and SSE dont support sending headers on the initial request, such as Authorization. Leaving authentication on realtime services entirely up to whoever is implementing the service. I may be wrong here and the spec suggests a good way to do it, but i've seen so many different approaches that at this point might as well say there's none. reply rexxars 10 hours agoparentThe EventSource API (the browser \"client API\" for Server-Sent Events) leaves a lot to be desired. While I am a maintainer of the most used EventSource polyfill[1], I've recently started a new project that aims to be a modern take on what an EventSource client could be: https://github.com/rexxars/eventsource-client. Beyond handling the custom headers aspect, it also supports any request method (POST, PATCH..), allows you to include a request body, allows subscribing to any named event (the EventSource `onmessage` vs `on('named event')` is very confusing), as well as setting an initial last event ID (which can be helpful when restoring state after a reload or similar). And you can use it as an async iterator. I love the simplicity of Server-Sent Events, but the `EventSource` API seem to me like a rushed implementation that just kinda stuck around. [1]: https://github.com/eventsource/eventsource reply martypitt 3 hours agorootparentNice work! This addresses many of the issues I've had with SSE. Another problem we've never worked out the solution to, is how to send a termination - signalling \"there are no more events coming\". We always end up having to roll our own, though it felt like something that should've been handled at the protocol layer. reply skinner927 3 hours agorootparentJust read the spec Clients will reconnect if the connection is closed; a client can be told to stop reconnecting using the HTTP 204 No Content response code. reply michaelt 15 hours agoparentprev> To this day I still dont know why WebSockets and SSE dont support sending headers on the initial request Doesn't the initial request get to send a full set of standard HTTP headers, cookies and all? reply paulgb 15 hours agorootparentIt does, but if you're calling it from the browser you can't add arbitrary data to them (the way you can in e.g. a `fetch`) reply makkesk8 13 hours agorootparentSomeone at Azure thought of this[1] [1] https://github.com/Azure/fetch-event-source reply pier25 11 hours agorootparentprev> you can't add arbitrary data to them What about intercepting the request with a service worker? reply dist-epoch 15 hours agoparentprevThey do send cookies. reply Sammi 1 hour agorootparentI do login the bog standard way with a regular old http request and the server responds with setting an http only cookie. Then I reconnect the websocket, which will then provide the cookie to the server on reconnect. reply cbsmith 15 hours agoparentprevThere's always TLS certificates... ;-) reply omgtehlion 15 hours agoparentprevWait, what?? Been using these for years. Am I missing something? reply tytho 15 hours agorootparentThe browser EventSource constructor does not have options to pass in your own headers. You can pass an option to have it use the cookies for the domain you’re using. There are libraries that allow you to pass in additional HTTP options, but they essentially reimplement the built-in EventSource object in order to do so. Not terribly difficult, fairly simple spec. reply omgtehlion 15 hours agorootparentWell, that constructor by default sends all the headers you have for your own domain and auth you are entitled to. This is how all other APIs in browsers work due to security and privacy concerns. If you call to other domains, then this problem is no different to what we had with CORS years ago. reply apitman 13 hours agorootparent> This is how all other APIs in browsers work due to security and privacy concerns They're probably comparing it to the fetch and XHR APIs, which both allow custom headers. reply apitman 15 hours agorootparentprevThey're probably referring to browsers specifically. The WebSocket constructor doesn't allow for headers reply paol 15 hours agoparentprevOooh boy you touched a pet peeve. I mean who needs authentication on the modern Web right? /s The even more irritating thing is that there is nothing preventing this, and every server I've tried supports it. It's only the browser WebSocket API that was designed without this. Cookies are the only thing browsers will deign to send in the initial request. reply eightnoteight 14 hours agoprevwebsockets and sse are a big headache to manage at scale, especially backend, requires special observability, if not implemented really carefully on mobile devices its a nightmare to debug on frontend side devices switch off network or slow down etc,... for battery conservation, or when you don't explicitly do the I/O using a dedicated API for it. new connection setup is a costly operation, the server has to store the state somewhere and when this stateful layer faces any issue, clients keep retrying and timing out. forever stuck on performing this costly operation. it's not like there is an easy way to control the throughput and slowly put the load on database reliability wise long polling is the best one IME, if event based flow is really important, even then its better to have a 2 layer backend, where frontend does long polling on the 1st layer which then subscribes to websockets to the 2nd layer backend. much better control in terms of reliability reply pornel 9 hours agoparentSSE supports long polling. You can make the server close the connection whenever you want. SSE supports automatic reconnection, and will even include the last ID seen to let the server continue seamlessly. reply hobobaggins 9 hours agorootparentIt's important to remember that SSE won't automatically reconnect for quite a few HTTP status codes (i.e., upstream proxy outages like 50x error codes) reply debarshri 14 hours agoparentprevI cannot agree more with you. I have seen people shot themselves on foot with Websockets and SSE. Long Polling even though is expensive, is it most explainable and scalable approach in my opinion. reply cbsmith 16 hours agoprevI always find articles like this amusing, because I designed an online auction system back in the late 90's. No XHR requests at all. Real-time updates were all handled with server-push/HTTP streaming. It wasn't easy to handle all the open connections at the time, but it could be done to an acceptable scale with the right architecture. reply switchbak 15 hours agoparentI've spent so many hours trying to communicate to folks the importance of HTTP streaming ... it's an uphill challenge for sure. Yes, all the benefits of http/2 (or 3) are great, but we should also be aware of what we can take advantage of in http 1.1, especially since it's effectively universally supported. reply merb 15 hours agorootparentActually what he meant is not supported anymore: https://en.m.wikipedia.org/wiki/Push_technology#HTTP_server_... Comet/sse/chunked transfer needs xhr to work. x-mixed-replace was würd back in the days and still is. Edit: maybe you could also use an iframe/frame which holds a chunked connection but that will only give you text. reply cbsmith 15 hours agorootparentChunked transfer encoding (which is indeed the underlying mechanism behind server-push) predates XHR, and AFAIK is still supported and will continue to be as long as HTTP/1.1 is still supported. You can use a frame/iframes, but you can also just have content that is updated with multi-part MIME that doesn't cause the page layout to be redone. reply merb 14 hours agorootparentx-mixed-replace Was the only mechanism that did that and chrome removed it and it is not standard and never was reply contravariant 14 hours agorootparentI'm so confused. Was x-mixed-replace the only way to use chunked transfer encoding (which is part of the HTTP standard) or are you talking about something else entirely? reply merb 14 hours agorootparentactually you can use transfer encoding chunked to stream data , but it will get appended and the page would never finish loading, that worked back than it works today but that is not useful for several reasons and wasn’t back than. with x-mixed-replace (as the name implies x-: experimentell) you can stream the page over and over again and the browser would change to the new version. (chrome still supports that for images, cheap webcams) Tbf without frames neither mechanism made much sense, (even back than) because it would be horrible to use with form fields. I started to play with the web in the early 2000s where xhr/long-polling/comet(via iframes, later it used xhr onreadystatechange with chuncked encoding, without sse, which basically was created because of that) started to gain traction and x-mixed-replace was extremely niche even back than because of the limitations it had on the page Edit: comet (streaming script tags, inside an iframe) of course worked, back then. But I never heard of an implementation before 2006 (maybe a few years earlier like 2-3)or so, would’ve been worth a Wikipedia change if you would have old entries. Also http/1.1 was 97 reply cbsmith 10 hours agorootparent> Tbf without frames neither mechanism made much sense, (even back than) because it would be horrible to use with form fields. Frames were the common way to deal with that, but I even did stuff with having a separate \"named\" browser window. > Also http/1.1 was 97 IIRC there was support for it in Netscape before it was really a standard. reply merb 2 hours agorootparent> IIRC there was support for it in Netscape before it was really a standard. That is probably true, since x-mixed-replace started the whole chunked transfer encoding. It’s probably also what really triggered the invention of comet and later xhr. Netscape was way ahead and Microsoft just pushed it out with money, integration, activex? and of course unfair advantage. reply cbsmith 12 hours agorootparentprev> actually you can use transfer encoding chunked to stream data , but it will get appended and the page would never finish loading, that worked back than it works today but that is not useful for several reasons and wasn’t back than. Oh Server Push has all kinds of issues with it. There are lots of good reasons to prefer the newer protocols for a lot of use cases. reply cbsmith 15 hours agorootparentprevHello fellow traveler. ;-) Nobody reads the specs anymore, and to a certain degree I can't blame them, as the protocols/standards have become quite complicated. reply kevmo314 16 hours agoprevI kind of miss long polling. It was so stupidly simple compared to newer tech, and that's coming from someone who thinks WebRTC is the best thing since sliced bread. reply mmis1000 15 hours agoparentSSE isn't really more complex than long polling. The only difference is the server don't close the connection immediately after sent the response. Instead, it wait for data again and send more response using the same stream. reply apitman 14 hours agorootparentOne limitation of SSE compared to long polling (and WebSockets etc) is you can't efficiently send binary data such as cbor, protobuf, etc. Though if your long polling is chatty enough eventually the HTTP overhead will kill your efficiency too. reply jallmann 14 hours agorootparentprevLong polling is more amenable than SSE for most HTTP tools out of the box, eg curl. The SSE message body is notably different from plain HTTP responses. To the OP, you can still build APIs with long polling. They are uncommon because push patterns are difficult to design well, regardless of protocol (whether long-polling, SSE, websockets, etc). Whiteboarding a push API is a good exercise. There is a lot of nuance that gets overlooked in discussions whenever these patterns come up. reply kevmo314 13 hours agorootparentWell I know I can write applications use it, but I don't often write code outside the context of a team that has other opinions anymore :) reply jacobr1 14 hours agorootparentprevAgreed - I see SSE as basically a standardized approach to modern long polling reply Animats 12 hours agoparentprevOh, if only it were that simple. The networking that makes Second Life go uses long polling HTTPS for an \"event channel\", over which the server can send event messages to the clients. Most messages go over UDP, but a few that need encryption or are large go over the HTTPS/TCP event channel. At the client end, C++ clients use \"libcurl\". Its default timeout settings are not compatible with long polling. Libcurl will break connections and make another request. This can result in lost or duplicated messages. At the server end, Apache front-ends the actual simulation servers, to filter out irrelevant connection attempts (Random HTTP attacks that try any open port, probably). Apache has its own timeouts, and will abort connections, forcing the client to retry. There's a message serial number to try to prevent this mechanism from losing messages. The Second Life servers ignore the serial number the client sends back as a check. Some supposedly compatible servers from Open Simulator skip sequential numbers. The end result is an HTTPS based system which can both lose and duplicate what were supposed to be reliable messages. Some of those messages, if lost, will stall out the user's activity in the game. The people who designed this are long gone. The current staff was unaware of how bad the mess is. Outside users had to find the problem and document it. The company staff has been trying to fix this for months. It seems to be difficult enough to fix that the current action is to defer work on the problem. So, no, long polling is not \"stupidly simple\". The right way to do this is probably to send a keep-alive message frequently enough that the TCP and HTTPS levels never time out. This keeps Apache and libcurl on their \"happy paths\", which work. reply jallmann 11 hours agorootparentMy solution to broken connections has actually been to have relatively short timeouts by default, eg 10 seconds. That guarantees we have a fresh connection every so often without any assumptions about liveness. You can even overlap the reconnects a bit (eg 10 second request timeouts, but reconnect every 8 seconds) as long as the application can reconcile duplicated messages - which it should be able to do anyway, for robustness reasons. Really, anytime there is any form of push (whether SSE, long polling, etc) then you need another way to re-hydrate to the full state. In which case you are nearly at the point of doing plain old polling to sidestep the complexity of server-driven incremental updates and all the state coordination problems that entails. Of course with polling, you lose responsiveness. For latency-sensitive applications (like an interactive mmorpg!) then HTTP is probably not the correct protocol to use. It does sound like Second Life has its own special blend of weirdness on top of all that. Condolences to the engineers maintaining their systems. reply mjevans 11 hours agorootparentI've seen a bunch of timeouts / heartbeat / keep alive durations. I think it might have been Wireguard, but 25 seconds seems like a good number. Usefully long, most things that break are more likely to do it at ~30 seconds, and if there's an active activity push at 15 or 20 seconds with device wakeup then the keep alive / connection kill might not even happen. Full Refresh; yes please, in the protocol, with a user button, with local client state cached client code and reloaded state on reconnect. Maybe even a configurable polling period; some services might offer shorter poll as a reason to pay for a higher tier account. reply Animats 6 hours agorootparent> with a user button If the user ever has to push a \"retry\" button, the networking levels are very badly designed. Just because some crappy web sites work that way does not mean it's OK. reply mjevans 2 hours agorootparentThe user shouldn't _have_ to. However, a 'refresh state' (and validate state, more gracefully than a full kill and reload) button can be both helpful and psychologically reassuring. It can also be very helpful for out of band issues, like ISP hiccups, random hardware failures, bitflips, etc. reply Animats 6 hours agorootparentprev> Of course with polling, you lose responsiveness. No, that's the whole point of long polling. The server delays the reply until it has something to say. Then it sends it immediately. The trouble here is middleware which does not comprehend what's going on and introduces extraneous retry logic. reply jallmann 3 hours agorootparentSorry, that part of the comment was probably not clear - I was comparing \"plain old polling\" (stateless request-reply with no delay) with \"push\", ie long polling reply ramesh31 14 hours agoparentprev>I kind of miss long polling. It was so stupidly simple compared to newer tech, and that's coming from someone who thinks WebRTC is the best thing since sliced bread. I still use it all the time. There are plenty of applications where the request overhead is reasonable in exchange for keeping everything within the context of an existing HTTP API. reply tschellenbach 13 hours agoprev(I work at Stream, we power activity feeds, chat and video calling/streaming for some very large apps) You should in most cases just use websockets with a keep-alive ping every 30 seconds or so. It's not common anymore to block websockets on firewalls, so fallback solutions like Faye/Socket.io are typically not needed anymore. WebTransport can have lower latency. If you're sending voice data (outside of regular webrtc), or have a realtime game its something to consider. reply ambigious7777 11 hours agoparentI'm making a WASM browser dungeon crawler game using WebTransport. It currently does not have great support -- namely Safari -- but because of other API incompatibilities I'm not planning on supporting Safari :P WebTransport is a bit more work than other ones, like SSE, but the flexibility and performance make it work it IMO. reply mirekrusin 16 hours agoprevJsonrpc over websockets is underrated tech. Simple, easy to implement, maps to programming language constructs (async functions, events, errors) which means code looks natural as any library/package/module usage devs are used to, can be easily decorated with type safety, easy to debug, log, optimize etc, works on current browsers and can be used for internal service to service comms, it's fast (ie. nodejs is using simdjson [0]), can be compressed if needed (rarely the need), we built several things on top of it ie. async generators (avoids head of line blocking, maps naturally to typescript/js async generators). [0] https://github.com/simdjson/simdjson reply cbsmith 15 hours agoparentIt's been a while since I've used websockets, but at least the last time I did, \"simple\" wouldn't be the word I'd have used. All kinds of annoying issues between different browsers. SSE was generally much simpler. reply mirekrusin 15 hours agorootparentMust have been time when spec wasn't stabilized and browsers have been introducing it. Those times are long gone. reply cbsmith 15 hours agorootparentI don't know about long gone. I still support a websocket solution that needs to have server-push as a fallback because of browser incompatibility issues. reply mirekrusin 13 hours agorootparentIt shouldn't be a problem for about a decade [0]? What's your hit rate for the fallback? [0] https://developer.mozilla.org/en-US/docs/Web/API/WebSocket reply cbsmith 12 hours agorootparentThe hit rate for fallback is pretty low. It was less than 5% the last I checked. reply mirekrusin 4 hours agorootparentThis 5% includes or excludes non-human traffic? reply apitman 14 hours agorootparentprevDo you have some specific examples? reply cbsmith 12 hours agorootparentOld Android handsets seem to be the big ones. reply londons_explore 10 hours agoprevA decent number of corporate firewalls still don't support web sockets... That means if you build something that requires web sockets, prepare to have a deluge of support/refund requests from the most valuable clients who think your site is broken. I suggest just having a once-per-second polling fallback, perhaps with an info bar saying 'the network you are connected to is degrading your experience'. reply cpursley 10 hours agoparentCertainty this can’t be true? I believe you but do you have any actual examples? reply paddybyers 2 hours agorootparentYes it's true. At Ably we support websockets, SSE and comet fallbacks (simple long-polling and streamed long-polling). It's less and less common but there are firewalls that fail to handle websockets correctly, or simply block them. I can't name specific companies/examples, but call centers are one example - the network and desktop environments are fully locked down. We also see in these cases that streamed HTTP can also be broken by the firewall - for example a chunked response can be held back by the firewall and only forwarded to the client when the request ends, as a fixed-length response. Obviously that breaks SSE and means you can't just use streamed comet as a fallback when websockets don't work. reply londons_explore 10 hours agorootparentprevAll UK government offices doesn't seem to allow it... That's a couple of million potential users right away. reply lxe 11 hours agoprevI love how SSE is just a \"don't close the connection and just keep flushing data\". I bet IE6 supports this. reply hobobaggins 9 hours agoparentThere are at least two polyfills for it, but I think most of them require at least IE10. (but IE6 has so many other issues that probably 90% of your other JS won't work anyway... so glad it's gone!) reply atum47 11 hours agoprevI did some testing with using SSE to send push notifications to my phone if someone set off a sensor, worked kinda good, but the browser had to be running in the background in order for it to work. After that i implemented a chat for a meme app that I've created to share memes with my friends, using websocket (Open Swoole) it is working nicely also. Never tested to see how many clients it can handle at once, but i guess the bottleneck would be in my server, not the software. Open Swoole is very easy to setup and there's lots of tutorials online. Got my ass kicked a little bit trying to making my websocket secure (wss) but I'm the end it worked fine. reply btown 14 hours agoprevIs there a modern open-source solution for bridging a traditional stateless web application to real-time notifications - one that's implemented all the best practices from the OP? Something like pusher.com but on self-hosted infrastructure/k8s, where messages from clients are turned into webhooks to an arbitrary server, and the server can HTTP POST to a public/private channel that clients can subscribe to if they know the channel secret. I've come across https://github.com/soketi/soketi and https://centrifugal.dev/ but not sure if there are more battle-tested solutions. reply kdunglas 5 hours agoparentI maintain the Mercure protocol (built on SSE) and the reference implementation (written in Go, available as a standalone binary and a Caddy module) which does exactly that: https://mercure.rocks In addition to the free and open source server, we also provide a cloud offering and on-premises versions that support clustering using Redis Streams, Kafka, Pulsar or Postgres LISTEN/NOTIFY as backends. The solution is used by many big actors in production for years: How Raven Controls uses Mercure to power big events such as Cop 21 and Euro 2020: https://api-platform.com/con/2022/conferences/real-time-and-... Pushing 8 million Mercure notifications per day to run mail.tm: https://les-tilleuls.coop/en/blog/mail-tm-mercure-rocks-and-... 100,000 simultaneous Mercure users to power iGraal: https://speakerdeck.com/dunglas/mercure-real-time-for-php-ma... reply roopakv 13 hours agoparentprevAnother one to check out https://partykit.io They are building on top of Cloudflare, and getting started is a breeze. That being said they are also fairly new, but based on everything I have seen, I am a fan reply jkarneges 12 hours agoparentprevThere's also Pushpin, if you want the API to blend with your existing app. Disclosure: Pushpin lead dev. reply bterlson 16 hours agoprevGreat comparison. Would love to see http response streaming added to the mix. I think a lot of use cases involving finite streams sent from server to client can be handled by the server streaming JSONL in the response. I tend to prefer this over SSE for finite data streams. reply mmis1000 16 hours agoparentServer send event IS http response streaming. It is a standardized way to implement response streaming. reply bterlson 15 hours agorootparentI agree, but SSE is a more complex protocol that requires additional handling on both client and server, with capabilities that may not be relevant for your use case (multiple event types, event id). For JS clients and JS servers it is not particularly onerous to implement, but for other ecosystems can require a fair bit of code. JSONL streaming is very easy to implement on both ends, so all else aside I think would be preferred if all you really want to do is stream JSON values. reply apitman 14 hours agorootparentI've implemented SSE from scratch on the server and XHR streaming/parsing from scratch on the client side (which would be necessary for JSONL), and SSE was way simpler. Unless there's another way to do JSONL in a browser that I'm not aware of? reply bterlson 12 hours agorootparentIf you use the fetch API you can get a readable stream and party on without too much difficulty. You can also implement a transform stream in ~10SLOC that will make the reader vend parsed JSON objects and can be reused easily. reply apitman 10 hours agorootparentThis is a good point. In fact, you just helped me realize that I can probably replace this[0] at work with a fetch implementation. ReadableStreams weren't generally available across browsers when I wrote that. This would also allow us to return binary data if we so desired (XHR can handle binary but it can't stream it chunk by chunk). Thanks! [0]: https://github.com/anderspitman/xhr-stream-dl reply jallmann 14 hours agoprevBrowser push APIs are hard to design well regardless of the underlying protocol (SSE, Long Polling, Websockets, etc). There are a bunch of things to consider: - Is this a full or partial state update? - What if the client misses an update? - What if the client loses connectivity? - How can the server detect and clean up clients that have disappeared? How those are answered in turn raise more questions. SSE or long polling or even WebSockets is a relatively unimportant implementation detail. IMO the bigger consideration should probably be ease-of-use and tooling interoperability. For that, I would say that long polling (or even just polling) is the clear winner. reply akira2501 13 hours agoparent> Is this a full or partial state update? It's a message. Mapping protocol units to messages was always your business. > What if the client misses an update? Sequence numbers on updates combined with a \"fill in\" mechanism through a separate request. > What if the client loses connectivity? Then more important things won't work either. > How can the server detect and clean up clients that have disappeared? The SSE client will restart dropped connections. You can have the server opportunistically close connections that haven't received messages recently. The browser will automatically reconnect if the object is still alive on the client side. > For that, I would say that long polling (or even just polling) is the clear winner. Coordinating polling intervals while simultaneously avoiding strong bursting behavior is genuinely not fun. reply jallmann 12 hours agorootparentAll that speaks to the point that the underlying protocol does not matter that much. The larger design is a more important consideration than the specific push tech. None of this favors SSE, or anything else for that matter. You still have to think about bursting / thundering herd behavior after server restarts, which would affect any protocol. The browser auto reconnecting doesn't mean you can assume the connection remains alive indefinitely if a reconnect hasn't happened; you may want periodic notifications as a keepalive to enable more immediate recovery. Long-lived server state introduces its own distributed coordination and cleanup problems, which strict request-reply polling sidesteps entirely. Etc. There is no silver bullet, only tradeoffs in the design space that need to be matched to your application's requirements. reply akira2501 11 hours agorootparent> the underlying protocol does not matter that much Then why is long polling the \"clear winner?\" reply jallmann 11 hours agorootparentFor interoperability with HTTP tooling (eg, curl) it is the clear winner reply akira2501 6 hours agorootparentSSE, which is just an HTTP request, works perfectly fine with curl or any other library which makes HTTP requests. reply apitman 14 hours agoparentprevThis is one of my favorite articles along these lines: https://blog.sequin.io/events-not-webhooks/ reply jallmann 13 hours agorootparentYeah that is a great article, and I agree that long polling an events endpoint along with a cursor gets you most of the way there for browser clients. At that point, things start to resemble the Kafka protocol. reply elwell 6 hours agoprevMan... I was trying to use WebRTC over ten years ago to implement livestreaming from your phone's camera *within a PhoneGap app*! Didn't work too well. reply lgrapenthin 10 hours agoprev> Long-Polling: The least scalable due to the high server load generated by frequent connection establishment, making it suitable only as a fallback mechanism. That makes no sense. Long-polling scales linearly like all the other ones as well. reply hnav 10 hours agoparentthere are a few ways that long-polling is \"least scalable\" - other approaches give you locality without sticky load-balancing: let's say your application server needs to subscribe to a topic once a connection is established, with long polling you need to setup and teardown that subscription every time, other approaches let you keep that HTTP stream alive and periodically send some stuff on it resulting in mostly just memory overhead. - each returned payload will result in at least 1 extra packet (the initial request headers, assuming the response headers and the payload fit into a packet) and at least 1/2 RTT delay. reply ascii78 14 hours agoprevI've been reading about WebRTC, does anyone actually know if browser to browser communication actually works reliably in practice ? Specifically NAT traversal, been hesitant to research it further because of this issue, seems that most of the connection parts seem to be legacy voip related protocols. reply Sean-Der 14 hours agoparentWorks well! Lots of developers/companies use WebRTC with NAT Traversal. You can also use it in a client/server setup. Check out 'WebRTC SFU' I wrote a little bit about the different topologies in [0] [0] https://webrtcforthecurious.com/docs/08-applied-webrtc/#webr... reply ravxx 16 hours agoprevWhoever made this article is pretty clueless. The article states using WebRTC would be pointless since you need to run it over websockets, sse, or webtransport, but forgets to mention the main benefit of WebRTC is that it can be done with UDP. Which is why I expect it was faster in the results... I am shocked to not even see UDP mentioned in this article. reply paulgb 15 hours agoparent> Whoever made this article is pretty clueless This genre of HN comment irks me. Technical writing is an exercise in taking a lot of material and distilling it down to an audience. Writing that does a good job this will often look basic to someone with a deep understanding of the tech. It doesn't mean the author is clueless, it just means they decided they made different decisions on what to cut than you would have. reply kordlessagain 13 hours agorootparentThat the author didn't mention UDP \"leaves out\" the vs. objective, however. If it were a fair contest, the article would mention the use of WebRTC over UDP, which isn't just for streaming media (an assumption I had before doing a bit more research on it). > This article aims to delve into these technologies, comparing their performance, highlighting their benefits and limitations, and offering recommendations for various use cases to help developers make informed decisions when building real-time web applications. So, the article's intent was to help others. When we do things like this, we should ensure all technologies being evaluated will be covered exhaustively. Otherwise, you risk leaving out an important part of the puzzle and then assumptions kick in which ignore a possible better solution for a given use case. It looks like UDP use is possible between a browser and a server, and that connection has to have components that deal with dropout, given it's UDP. There is a LOT to consider and deal with implementing UDP over WebRTC, so I put a dump of this up here: https://pastebin.com/xgA78dky reply realPubkey 15 hours agoparentprevAuthor here. The article is mostly about web apps. How would your signaling server emit new connection updates to clients in the scenario you describe? reply andreigheorghe 15 hours agorootparentI think what they mean is that yes, the signalling needs to be done over \"traditional\" web APIs (websockets, etc), but that's just for discovering/negotiating the p2p connections. The actual data transfer between the peers then happens over UDP which can have a bunch of advantages over TCP for some scenarios. reply cbsmith 15 hours agoparentprevThe WebRTC section of the article seemed weird in general. The reason WebRTC doesn't specify signaling requirements is that clients can use any communications mechanism they'd like for signaling, and in the case where you are using WebRTC as sever-push mechanism, the \"signaling\" server and the server you want to receive pushes from could be the same server, allowing the use of \"regular\" HTTP as the transport for your \"signaling\" data. With QUIC & HTTP/3, and things like RFC 8441 & 9220, you could well be using UDP with non-WebRTC protocols, and TCP stacks & routers tend to be pretty well tuned these days, so UDP doesn't necessarily have much of an advantage in this kind of use case. If you check out the benchmark the article uses, it specifically breaks out using \"unreliable WebRTC/WebTransport\". The \"unreliable\" looks to be referring to UDP (I despise how people misleadingly associate UDP with \"unreliable\"). They also have \"reliable WebRTC/WebTransport\", which appears to be using TCP. In the latter case, they actually found in some cases WebTransport doing a tad better in the face of packet loss, which is interesting. I haven't looked at the details of the tests, but in my experience benchmarching WebRTC is not as straightforward as one might expect; it's entirely possible the nature of the benchmark itself is leading to WebRTC's better & worse performance. reply arendtio 16 hours agoprevI wonder why mobile push notifications are just a side-note in this article as mobile clients are responsible for a large part of the global traffic. reply nine_k 15 hours agoparentAren't mobile push notifications an entirely different tech, one that significantly uses the capabilities of mobile carriers? reply arendtio 15 hours agorootparentI don't know about the implementation details, but even the use case is slightly different, as the discussed techs are also meant to transfer larger amounts of data. At the same time, mobile push is more about triggering synchronization than transferring large amounts of data. However, since the discussed techs all have major problems with mobile connections, I still think it should have been discussed in more depth. reply cbsmith 15 hours agorootparentprevYou can do mobile push notifications to a phone even if they aren't connected to a mobile carrier. That said, I'd argue SSE is the browser equivalent of mobile push notifications. reply jallmann 12 hours agorootparentSSE generally requires the page to be active in the foreground, which is not a viable solution for mobile. Web Push is a distinct API for browsers to emulate native push notifications via service workers. https://developer.mozilla.org/en-US/docs/Web/API/Push_API reply cbsmith 11 hours agorootparentI thought it was more that the Document object had to stay alive. ...although, these days a lot of mobile browsers will kill that thing off if the page is not in the background, so yeah... reply slt2021 15 hours agorootparentpreviOS and android push does not rely on mobile carrier capability afaik, it has its own service reply psnehanshu 13 hours agorootparentYes, iOS has APNS and Android has FCM. One can use FCM for both Android and iOS as FCM is capable of abstracting away APNS. If you are building on React Native with Expo, you can also use Expo Push, which abstracts away both APNS and FCM, albeit it is not fully featured. reply cookiengineer 12 hours agoprevNote that the author misses an essential point: custom compression dictionaries, which can only be used by WebSockets (WS13) and hardly by the others, as that would break compatibility. I'd argue that you can push websocket data usage way lower than the other protocols, if you use a binary compression based on a predefined schema. In WebSockets, you can use plug&play extensions which can modify the payload on both the client and the server, which make them also ideal for tunneling and peer-to-peer applications. I've written an article from an implementer's perspective a while ago, in case you are interested [1] [1] https://cookie.engineer/weblog/articles/implementers-guide-t... reply EGreg 15 hours agoprevWhat I want to know is, on iOS can we have Web Workers running for a few hours when the browser is in the background, with setInterval communicating w the network or will they get suspended? reply psnehanshu 13 hours agoparentDon't assume reliability reply lakomen 10 hours agoprev [–] SSE in gin (go) is broken and has been for years. No one uses it and no one bothers to fix it. reply hobobaggins 9 hours agoparent [–] r3labs/sse seems pretty good, but doesn't seem to handle backpressure or defend against slow-client attacks. (actually, I haven't seen any libs that do.. maybe this isn't a problem for anyone?) You could roll your own, since the protocol is extremely simple. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article evaluates various real-time server-to-client communication methods, including WebSockets, Server-Sent Events, Long-Polling, WebRTC, and WebTransport, highlighting their performance and scalability differences.",
      "WebSockets enable bidirectional communication, Server-Sent Events are suitable for frequent updates, Long-Polling is outdated, and WebTransport excels in low latency and high throughput.",
      "Recommendations are given for selecting the appropriate method based on the specific use case, along with suggestions for enhancing HTTP response times and addressing connection limitations, while promoting the use of the RxDB replication protocol for client synchronization."
    ],
    "commentSummary": [
      "The discussion delves into different web communication technologies like WebSockets, Server-Sent Events, Long-Polling, WebTransport, and WebRTC, emphasizing their pros, cons, and challenges.",
      "It stresses the significance of adapting to new protocols and aligning them with application needs when selecting a push technology for better real-time data transmission and improved connectivity.",
      "Overall, the conversation highlights leveraging modern web technologies for dependable and effective real-time communication solutions."
    ],
    "points": 320,
    "commentCount": 160,
    "retryCount": 0,
    "time": 1710776217
  },
  {
    "id": 39749646,
    "title": "Nvidia unveils Blackwell AI chips for enhanced performance",
    "originLink": "https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV INVESTING CLUB PRO MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN TECH Nvidia CEO Jensen Huang announces new AI chips: 'We need bigger GPUs' PUBLISHED MON, MAR 18 20244:28 PM EDTUPDATED MOMENTS AGO Kif Leswing @KIFLESWING KEY POINTS Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The new AI graphics processors are named Blackwell and are expected to ship later this year. The announcement comes as companies and software makers still scramble to get their hands on the current generation of H100s and similar chips. In this article NVDA Follow your favorite stocks CREATE FREE ACCOUNT Nvidia CEO Jensen Huang delivers a keynote address during the Nvidia GTC Artificial Intelligence Conference at SAP Center on March 18, 2024 in San Jose, California. Justin SullivanGetty Images Nvidia on Monday announced a new generation of artificial intelligence chips and software for running artificial intelligence models. The announcement, made during Nvidia's developer's conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia's share price is up five-fold and total sales have more than tripled since OpenAI's ChatGPT kicked off the AI boom in late 2022. Nvidia's high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of \"Hopper\" H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company's developer conference in California. Nvidia shares fell more than 1% in extended trading on Monday. The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. \"Blackwell's not a chip, it's the name of a platform,\" Huang said. \"The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,\" said Nvidia enterprise VP Manuvir Das in an interview. \"Of course, we still do that. But what's really changed is, we really have a commercial software business now.\" Das said Nvidia's new software will make it easier to run programs on any of Nvidia's GPUs, even older ones that might be better suited for deploying but not building AI. \"If you're a developer, you've got an interesting model you want people to adopt, if you put it in a NIM, we'll make sure that it's runnable on all our GPUs, so you reach a lot of people,\" Das said. Meet Blackwell, the successor to Hopper Nvidia's GB200 Grace Blackwell Superchip, with two B200 graphics processors and one Arm-based central processor. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company's Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a \"transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC . It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Nvidia CEO Jensen Huang compares the size of the new \"Blackwell\" chip versus the current \"Hopper\" H100 chip at the company's developer conference, in San Jose, California. Nvidia Amazon , Google , Microsoft , and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That's much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn't provide a cost for the new GB200 or the systems it's used in. Nvidia's Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia will also sell B200 graphics processors as part of a complete system that takes up an entire server rack. Nvidia inference microservice Nvidia also announced it's adding a new product named NIM, which stands for Nvidia Inference Microservice, to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. \"In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,\" Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=39749646",
    "commentBody": "Nvidia CEO Jensen Huang announces new AI chips: ‘We need bigger GPUs’ (cnbc.com)311 points by tiahura 13 hours agohidepastfavorite241 comments __mharrison__ 4 hours agoMy take from being at the keynote and the content I've seen so far at the conference is that Nvidia's is moving up the stack (like all good hardware vendors are prone to do). Obviously they are going to keep doing bigger. But the takeaway for me is that they are building \"docker for llms\" - NIM. They are building a container system where you can download/buy(?) NIMs and easily deploy them on their hardware. Going to be fun to watch what this does to all the AI startups... reply flessner 3 hours agoparentWon't do anything to most consumer facing AI, the UI & convenience is already a major selling point. A bigger threat is that the feature the business is built around makes it into mainline software... there is no demand for (paid) background removal anymore as every iPhone can do it nowadays. Generally if whatever AI product you have can easily just be a feature in whatever application businesses already use, then you are running a business on borrowed time. reply Kinrany 44 minutes agorootparentOh no, turns out the thing people really want from AIs is generality because everything else can be done in stupid software! reply ixaxaar 1 hour agoparentprevThere are also open source equivalents btw - https://github.com/geniusrise reply djtango 3 hours agoparentprevI'm not that abreast of all the developments in the AI space. What specific class of AI startups do you have in mind here? AI-aaS startups who provide the \"infra\"? reply cebert 6 hours agoprev> “Nvidia … is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. I can understand from a growth perspective why it’s more profitable for Nvidia if it can become more of a platform service for AI. However, that’s difficult to balance that and partnerships the company already has with AWS and Microsoft. I’d expect to see some acquisitions or competing custom solutions in the future. Fortunately for Nvidia, a lot of AI is still dependent on CUDA. I’m interested to see how this plays out. reply wmf 6 hours agoparentI think they're planning for a world where half their customers (hyperscalers) just use GPUs and CUDA while the other half (the long tail) use more profitable, higher-level parts of the platform. They don't have the leverage to force customers one way or the other. It would be easier to just sell GPUs, but they know that sophisticated customers can switch to other chips while the platform provides lock-in for smaller customers. reply neverokay 4 hours agorootparentDoesn’t matter how good your in-house tech team was, your company still outsourced it to cloud infra. That’s what Nvidia faces. Doesn’t matter how good the current in-house teams are using direct hardware, the trend in corporate is shift to a vendor (Google/AWS). Nvidia can watch this inevitable shift or get ready to offer itself as a platform too. reply joshellington 4 hours agorootparentI get it, the service model always shines the brightest in the eye of the revenue calculator. But I have immediate skepticism they’ll be able to execute at a competitive level. Their core competency has always been manufacture and production, not service-based things. It’s a big rock to push up a tall hill. reply bboygravity 1 hour agorootparentGenuine question (I don't know much about cloud stuff): how is providing a cloud service/platform (at scale) even remotely as hard as designing, manufacturing and selling GPU's (including drivers and firmware) at massive scale? It feels like reading that setting up something like Facebook would be extremely challenging for a company like SpaceX. reply komadori 8 minutes agorootparentI share your intuition, perhaps unfairly, that it's indeed not as hard in absolute terms. However, it certainly requires a different set of skills and organisational practices. Just because an organisation is extremely good at one thing doesn't mean it can easily apply that to another field. I would guess that SpaceX probably does have the talent on hand to throw together a Facebook clone, but equally I think they would struggle to actually complete with Facebook as a business at scale. reply omnimus 2 hours agorootparentprevThis doesnt seem to be true. They have been running Geforce Now for a long time and its one of the best gaming streaming services. It seems they are doing it in partnership with other regional companies but nobody says they cant use same partners. Running games with small latency seems more complicated than llms on cuda. reply neverokay 2 hours agorootparentprevHey, if it doesn’t work out they can always return back to what they are, a consumer first company but now with world class hardware/software. AI graphics service that renders games in the cloud (photorealistic), concluding its epic journey of being an amazing graphics card company. Kinda cool when you think about it. reply bayindirh 1 hour agorootparentprevThere are already platforms and projects which provide resources for long tail of science. EU is supporting projects which brings together resource providers with researchers and private companies in some cases. NVIDIA is not entering an empty marketplace here. Also there's already enough know-how to make things run on cloud, OpenStack, HPC, etc. Unless they make things very difficult for independent platforms, they can't force their way in that much, from my perspective. reply dheera 6 hours agoparentprevMy prediction is eventually there will be anti-trust ligitation, they will be required to open the CUDA standard, after which AMD will become a competitor. NVIDIA could voluntarily open the standard to avoid this ligitation if they wanted to, though, and IMO it would be the smart thing to do, but almost every corporation in history has chosen the ligitation instead. reply nemothekid 5 hours agorootparent>My prediction is eventually there will be anti-trust ligitation, they will be required to open the CUDA standard, after which AMD will become a competitor. If AMD isn't a competitor before government intervention, I don't the government forcing nvidia to open up CUDA changes much. CUDA's moat isn't due to some secret sauce - nvidia put in the developer hours; and if AMDs CUDA implementation is still broken, people will continue to buy nvidia. There has been a lot of trying to get AMD to work - Hotz has been trying for a while now[1] and has been uncovering a ton of bugs in AMD drivers. To AMD's credit, they have been fixed, but it does give you a sense of how far behind they are in regards to their own software. Now imagine them trying to implement a competitor's spec? [1] https://twitter.com/__tinygrad__/status/1765085827946942923 reply AYBABTME 5 hours agorootparentI don't understand AMD in this. Isn't it insanity that they're not throwing all they've got at their software stack? reply roenxi 3 hours agorootparentYou know what happens to companies that panic and throw all their resources into knee-jerk software projects? I don't, but I'd predict it is ugly. Adding more people to a bad project generally makes it worse. The issue that AMD has is they had a long period where they clearly had no idea what they were doing. You could tell just from looking at websites, CUDA pretty much immediately gets to \"here is a library for FFT\", \"here is a library for sparse matricies\". AMD would explain that ROCM is an abbreviation of the ROCm Software platform or something unspeakably stupid. And that your graphics card wasn't supported. That changed a few months ago; so it looks like they have put some competent PMs in the chair now or something. But it'll take months for the flow on effects to reach the market. They have to figure out what the problems are which takes months to do properly; then fix the software (1-3 months more minimum); then get it into the open and the foundational libraries like PyTorch pick it up (might take another year). You can speed that up, but more cooks in the kitchen is not the way. Bandwidth use needs to be optimised. It isn't like ROCm seems lacks key features; it can technically do inference and training. My card crashes regularly though (might be a VRAM issue) so it is useless in practice. AMD can check boxes but the software doesn't really work and grappling with that organisationally is hard. Unless you have the right people in the right places, which AMD didn't have up to at least mid 2023. reply Certhas 1 hour agorootparentLook at AMDs vs Intel. They have now surpassed Intel in terms of CPUs sold and market cap. That was unthinkable even six, seven years ago. It makes perfect sense that, organisationally, they were focused on that battle. If you remember the Athlon days, AMD beat Intel before, but briefly. It didn't last. This time it looks like they beat Intel and have had the focus to stay. Intel will come back and beat them some cycles, but there is no collapse on the horizon. So it makes sense that they started looking at nVidia in the last year or so. Of course nVidia has amassed an obscene war chest in the meantime... reply nicoburns 34 minutes agorootparentAMDs graphics was an acquisition though (ATI), and I understand that the company culture of that division might still be quite different. reply elcomet 1 hour agorootparentprevPytorch has been supporting rocm for all last 2 years reply tempaccount420 4 hours agorootparentprevHardware people don't get along very well with software people. reply elbear 3 hours agorootparentWhy's that? reply rapsey 3 hours agorootparentBecause it is a different type of engineering. If you manage software development like you manage hardware development your software is going to be bad. That has always been AMD's problem and it is not likely to get fixed. reply imtringued 2 hours agorootparentprevBecause they didn't go to uni when hardware-software-codesign was being taught. reply logicchains 55 minutes agorootparentprevIt's a political problem. Good software engineers are paid more than good hardware engineers, but AMD management is unwilling to pay up to bring on good software engineers because then they'd also need to pay their hardware engineers more, otherwise the hardware engineers would be unsatisfied. If you check NVidia salaries online you'll see NVidia pays significantly more than AMD for both hardware and software engineers; it's a classic case of AMD management being penny-wise, pound-foolish. reply imtringued 1 hour agorootparentprevYou have to remember that this only applies to cheap consumer GPUs, they tend to support their datacenter GPUs better. When you consider that Ryzen AI already eats the AI inference lunch, having better GPUs with better software only threatens to cannibalize their data center GPU offering. Given enough time nobody will care about using AMD GPUs for AI. reply blitzar 1 hour agorootparentprevGetting this working might be worth a trillion $ to AMD - they should be doing more than just waiting for a bootstrapped startup to debug their drivers for them. reply wmf 6 hours agorootparentprevIt would be kind of genius for Nvidia to \"open\" the CUDA APIs (which have already been unofficially reverse engineered anyway) but not the code. Maybe they'd also officially support HIP and SYCL. Maybe they could open SXM after all competitors have already committed to OAM. They'd create the appearance of opening up while giving up very little. reply sitkack 4 hours agorootparentBy \"Opening Up\" they cement their leadership position. AI frameworks are already targeting CL, SPIR-V, etc. The low level details will fade and so will Nvidias api dominance. The MI300 smokes the H100 yet here we are. reply incrudible 1 hour agorootparentJust because they are a target doesn’t mean things just work. Historically, AMD hardware for GPGPU becomes obsolete well before the software landscape catches up. I am not going to risk my time and money finding out whether history repeats itself, just for a few potential FLOPS per dollar. reply anon291 6 hours agorootparentprevThe cuda api is essentially open... Hip is basically a copy. CUDA is such a misnomer. Amd doesn't have tensorRT, cuDNN, cutlass, etc. Forcing Nvidia to make these work on AMD is like forcing Microsoft to make windows work on apple hardware... Not going to happen. reply alphabeta567 56 minutes agorootparentCUDA is not open. See what happened with ZLUDA. reply coryrc 36 minutes agorootparentI'm not sure your implication. My understanding of the project is AMD didn't want to invest in it anymore. reply wmf 5 hours agorootparentprevThey did force Microsoft to make Office work on Mac though... (Office for Mac already existed but I think MS agreed to not cancel it.) reply pjmlp 2 hours agorootparentIt was more like Microsoft had the anti-trust stuff going on, and Apple was on the verge of going bankrupt. reply gammalost 29 minutes agoprevI wonder when we as an industry will start to address the scaling issues in LLMs.It is obviously in Nvidias interest to keep pushing out bigger and better GPUs, but what is the collective interest? It is already proven that good language models are possible given enough resources. The challenge now is to put these models in a solution which do not require unfathomable amounts of resources for the average use cases. reply bayindirh 26 minutes agoparentWasteful software development is easy and keeps momentum for development. As long as growth is king, quick and dirty will always beat well optimized and smaller systems. This is not a problem with AI only, but with every software we use. Only two groups try to optimize things and try to fit into smaller systems. Passionate programmers and people who is paid to do this (e.g.: phone manufacturers' software teams, etc.). reply qwertox 11 hours agoprevWhat is FP4, 4 bit floating point? If so, the comparison graph [0] with 30x above Hopper was a bit misleading. [0] https://youtu.be/Y2F8yisiS6E?t=4698 reply wongarsu 9 hours agoparentIt's 4 bit floating point, at twice the speed of 8 bit floating point. There's also FP6, doesn't offer faster compute than FP8 but manages to take advantage of the better memory bandwith and cache use of the 6 bit format. Apparently some people are drawing connections to this paper [1] on 4 bit LLMs, which has one NVIDIA employee among its contributors 1: https://arxiv.org/pdf/2310.16836.pdf reply Havoc 10 hours agoparentprev>bit misleading. Only partially, because in LLMs FP4 isn't half as useful as FP8. So if you have gear that crushes at FP4 then that's what you use and you benefit from that increased speed (at minimal accuracy loss). Definitely some marketing creativity in there, but its not entirely wrong as a measure of real world usage reply jxy 6 hours agorootparentcuriously, what real world usage actually uses FP4? AFICT, most of the LLMs still use BF16, and even the quantizations down to 4bits and 2bits end up back to 16bit or INT8 for actual computations. reply creshal 2 hours agorootparentHalf the reason why they move back up to 8/16 bit is that current hardware doesn't properly support 4 bit floats, and you get better performance from the conversion. I think once this hardware hits, most of the computation will shift to native 4 bit just for efficency's sake. ...assuming the recent 1.58b paper doesn't render the entire float quantization approach obsolete by then. reply imtringued 1 hour agorootparentThe 1.58b approach is good for everyone including for quantization. It means that current quantization schemes have room for improvement. reply buildbot 5 hours agorootparentprevLlama.cpp and many others support 4 bit weights and lower reply opcode84 8 hours agoparentprevhttps://arxiv.org/pdf/2310.10537.pdf Discussed in a previous post https://news.ycombinator.com/item?id=37930663 reply buildbot 5 hours agorootparentYes this is it! reply s_m_t 10 hours agoparentprevHow can 4 bits possibly be enough? Are intermediate calculations done at a higher width and then converted down back to FP4? reply WhitneyLand 9 hours agorootparent- Training isn’t done at 4-bits, to date this small size has only been for inference. - Research for a while now has been finding that smaller weights are surprisingly effective. It’s kind of a counterintuitive result, but one way to think about it is there are billions of weights working together. So taken as a whole you still have a large amount of information. reply tmalsburg2 1 hour agorootparent> - Training isn’t done at 4-bits, to date this small size has only been for inference. Wasn't there a paper from Microsoft two weeks ago or so where they trained on log₂(3) bits? Edit: https://arxiv.org/pdf/2402.17764.pdf reply acchow 9 hours agorootparentprevIntuitively, there is a ton of redundancy and we still have a long way we can still compress things. reply imtringued 1 hour agorootparentEach token is represented by a vector of 4096 floats. Of course there is redundancy. reply coffeebeqn 8 hours agorootparentprevMaybe the rounding errors are noise that is somewhat useful in a big enough neutral net. Image generators also generate noise to work on reply yalok 6 hours agorootparentprevThere are research papers where even 1 bit (not floating point) was enough, with some quality loss. 4 bits is effectively 16 different float point numbers - 8 positive, 8 negative, no zero and no NaN/inf. 1 bit for sign and 3 bits for exponent, 0 bits for mantissa, mantissa is implied to be 4. It’s logarithmic - representing numbers in the range from -4^3 to 4^3, smallest numbers are 4^-3. reply phh 1 hour agorootparentThanks. First source i see for what fp4 is. Gotta say I'm surprised: I would have chosen to lose one value, but have a zero. (though I have no doubt those people are much more clever and knowledgeable than I am) reply carlmr 1 hour agorootparentprev>1 bit (not floating point) I like how you specified that it's not floating point. reply s_m_t 6 hours agorootparentprevThanks, I was thinking that zero, negative zero, inf, negative inf, and the NaN's were included like in IEEE 754 reply anon291 6 hours agorootparentprevThe fundamental 'unit' of NN computation is not an individual vector element but rather an entire vector. One of the first results you often learn about in linear algebra is that some axes are more important than others (principal components, singular value decomposition). Thus, it totally stands to reason that the underlying field of the vector is inconsequential but rather the entire vector machinery. All you have to do is make sure that there are enough elements in the vector to get the job done for whatever bit size of element. reply s_m_t 6 hours agorootparentI see, so the idea is that enough of the quantization errors are sort of averaged out across the dimensions of the vector space to still be useful? reply singularity2001 3 hours agorootparentThe way I think about it is finally it will end in a binary feature vector similar to 20Questions (male or female, alive or dead ...) just with 100s of dimensions reply wongarsu 9 hours agorootparentprevFor training FP4 sounds pretty niche, but for inference it might be very useful. reply CamperBob2 6 hours agorootparentprevThe various sigmoid activation functions have the effect of keeping bit growth under control, by virtue of clamping to the +/- 1 range. reply fancyfredbot 11 hours agoparentprevThat's right.There was mention of a precision aware transformer engine which might make it easier to use fp4, but it's not 30x faster in a like for like way. This shouldn't be surprising since it's more or less two hoppers next to one another on a slightly improved process node. 2.5x seems more likely in cases where you don't exploit a new feature like that or the increased memory. reply sipjca 11 hours agoparentprevyes reply bluedino 12 hours agoprevThey acquired Bright Cluster Manager a few years ago, who would be next on their list to acquire? It seems like they want to provide customers with the whole stack. reply shiftpgdn 12 hours agoparentCanonical is a ripe target. Canonical has been trying to grow Ubuntu and other tools in the enterprise world for the last few years without significant success, and much of the Nvidia devkit stuff is built around Ubuntu. reply hipadev23 10 hours agorootparentCanonical’s culture [1] is the antithesis of what nvidia wants. [1] https://www.reddit.com/r/recruitinghell/comments/1bec2zk/lit... reply riffic 10 hours agorootparentThat's not culture, that's Shuttleworth. reply ethbr1 7 hours agorootparent\"We hire only the best who can fully recall the intimate nuances of their high school experience?\" reply pjmlp 2 hours agorootparentprevI would rather bet Microsoft doing that, given their cozy relationship for .NET and main WSL distribution. At least I would finally get to buy MS Ubuntu PCs at the shopping mall. reply echelon 11 hours agorootparentprevPlease do not give them this idea. Ubuntu is actually a pretty great daily driver desktop Linux, and I'd hate for that to lose priority and disappear. I'm not a fan of what happened to the Red Hat ecosystem for exactly the same reasons. reply xmprt 11 hours agorootparentAs someone who has used Ubuntu in the past and has now moved onto greener pastures, I appreciate everything Canonical and Ubuntu have done for the Linux community but there are many better options today and Canonical is already far from the company it once used to be. reply hnlmorg 10 hours agorootparentThere have always been better distributions than Ubuntu. That isn’t something new. What Canonical did better than anyone else was mass market appeal. Or at least appeal to a wider market than Linux traditionally had. But as someone who’s used Linux since the 90s, I was always underwhelmed by Ubuntu as a distribution. That all said, I have to work with a lot of CentOS and Rocky workstations for VFX and I enjoy those for desktop Linux even less than Ubuntu. reply solumunus 4 hours agorootparentprevI hate it when people say stuff like this and then don’t express their opinions on the better alternatives. reply sitkack 4 hours agorootparentThey mean Nix and Arch. reply dgfitz 11 hours agorootparentprevWhenever I see an open job req for canonical I run for the hills. reply mianos 9 hours agorootparentThey must have a fast revolving door. I have know so many people who worked there in the last few years but seem to have moved in. Probably depends on the team and the worst ones have a lot of churn. reply margorczynski 8 hours agorootparentFrom what I see on Glassdoor they have bad, toxic management so that would explain a lot. reply xarope 5 hours agorootparentprevwhat would your top suggestions be for server or desktop, instead of ubuntu? Arch (too unstable for server?), Silverblue? reply rompledorph 3 hours agorootparentRecently installed PopOS on my desktop. That is currently my top suggestion as an Ubuntu alternative reply unmole 2 hours agorootparentprevDebian for server, Fedora or openSUSE Tumbleweed for desktop. reply riffic 10 hours agorootparentprevthe desktop Linux ecosystem can survive w/o Ubuntu. Silverblue / Universal Blue for instance is quite compelling. reply pm90 5 hours agorootparentprevWhat happened to Red Hat? As far as I see they’re continuing to invest in linux. Im glad they are keeping CoreOs around as FCOS. reply greggsy 11 hours agorootparentprevTbh, Ubuntu’s only pull is the support and breadth of users. As a desktop, it’s let down by Unity, which IMHO is basically a port of Windows 8 tablet UI. If they defaulted back to a menu and taskbar-based WM, it might actually be more approachable to users who are more familiar with macOS and Windows. reply kcb 10 hours agorootparentMain Ubuntu hasn't shipped with Unity for like 7 years. reply az226 1 hour agoparentprevAnthropic or Mistral and build AGI/ASI. reply kflansburg 11 hours agoparentprevRun:AI https://news.ycombinator.com/item?id=39738342 reply herecomethefuzz 11 hours agoprev\"Platform company\" means multi-chip in this case? Seems logical since it's becoming impractical to cram so many transistors on a single die. reply dweekly 10 hours agoparentIt means all the main chips required for a large-scale datacenter. And many of the layers of software on top of it. Hardware: * The GPU * The GPU-GPU Fabric (NVLINK) * The CPU * The NIC * The Network Fabric (infiniband) * The Switch And that's not even starting to get into the many layers of the software stack (CUDA, Riva, Megatron, Omniverse) that they're contributing and working to get folks to build on. reply 0xcde4c3db 10 hours agoparentprevI don't really understand the bird's-eye view of the product line, but judging by some of the raw physical numbers and configurations Jensen was bragging about, it means that they want to basically play the mainframe game of locking high-end applications into proprietary middleware running on proprietary chassis with proprietary cluster interconnect (hello, Mellanox acquisiton). reply wtallis 10 hours agorootparentThe lock-in is more of a bonus for them. The underlying problem is that it's impossible to build a chip big enough, or even a collection of chiplets big enough. Training LLMs requires more silicon than can fit on one PCB, so they need an interconnect that is as fast as possible. With interconnect bandwidth as a critical bottleneck, they're not going to wait around for the industry to standardize on a suitable interconnect when they can build what they need to be ready to ship alongside the chips they need to connect. reply l33tman 26 minutes agorootparentCerebras: -Hold my beer reply anon291 5 hours agorootparentprevIn this case the interconnects are also doing compute. reply 1oooqooq 11 hours agoparentprevno it means rent seeking. imagine aws if they also sold all computers in the world, now you can only rent from them reply maximus-decimus 6 hours agorootparent\"For only 100$ a month, you'll be able to turn on the gpu you already paid for\" --Nvidia, pretty soon reply bpye 3 hours agorootparentThis is sort of already a reality. Their vGPU functionality (partitioning a single physical GPU into multiple virtual GPUs) is already separately licensed - https://www.nvidia.com/en-us/data-center/buy-grid/ And that's once you've bought an expensive Tesla/Quadro GPU too. reply throwaway11460 10 hours agorootparentprevSo like IBM at the beginning of computers reply jakobov 6 hours agoprevThey are claiming a 25x reduction in power consumption. That can't be right. Anyone understand where this number is coming from? reply LTL_FTC 6 hours agoparentDid you read that in the linked article? I couldn’t find it. But maybe due to the better efficiency with regard to the performance boost (5x) and the ability to now use 27 trillion parameters versus 1.7 Trillion, one can presumably finish the same amount of work in 1/25th of the time and bam, reduction in power consumption. As you say, I’m skeptical the max power draw itself is 25x lower. reply wmf 6 hours agorootparentI think Jensen said something like needing 25x fewer GPUs (vs. A100) to get the same performance, which amounts to essentially the same thing. reply creshal 2 hours agorootparentIt doesn't imply a full 25x reduction in power consumption though, that might \"only\" go down by 10x. reply Deasiomlo 10 hours agoprevDouble digit peta flop mass produced. \"The computing power needed to replicate the human brain’s relevant activities has been estimated by various authors, with answers ranging from 10^12 to 10^28 FLOPS.\" Petaflop is 10^15 Crazy times. reply teaearlgraycold 10 hours agoparentI’ll be happy with this if we use it to design viable fusion power plants. And I’ll be severely disappointed if it’s mostly used for ad targeting. reply sbstp 3 hours agorootparentYou are about to be severely disappointed. reply steelframe 10 hours agorootparentprev> I’ll be severely disappointed if it’s mostly used for ad targeting Obligatory Rick and Morty: https://www.youtube.com/watch?v=xerLPWdyX-M reply tzm 10 hours agoprevPlatform co seems fitting, considering Nvidia's data center revenue in the fourth quarter of 2023 was a record $18.4 billion, which is 27% higher than the previous quarter and 409% higher than the previous year. Seems revenue from inference is growing at a significant clip. reply ec109685 9 hours agoparentData center revenue includes sales to companies like Meta that run their chips in their own data centers. reply dagmx 13 hours agoprevFP8 being 2.5x Hopper is kind of disappointing after such a long time. Since its 2 fused chips, that means it’s 25% effective delta. though it seems most of the progress has been on memory throughput and power use which is still very impressive. I wonder how this will trickle down to the consumer segment. reply azeirah 12 hours agoparentJensen revealed later that the LLM inference is 30x due to architectural improvements, it's massive. I don't know if it's latency or just 2-3x performance boost with 30x more customers served in the same chip. Either way, 30x is massive. reply kkielhofner 10 hours agorootparentThe other big announcement here is NIM - Nvidia Inference Microservice. It's basically TensorRT-LLM + Triton Inference Server + pre-build of models to TensorRT-LLM engines + packaging + what appears to be an OpenAI compatible API router in front of all of it + other \"enterprise\" management and deployment tools. This software stack is extremely performant and very flexible, I've noted here before it's what many large-scale hosted inference providers are already using (Amazon, Cloudflare, Mistral, etc). From the article: 'Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said.' The dead giveaway is \"I changed one line of code in my OpenAI code\" which means \"I pointed the OpenAI API base URL to an OpenAI compatible API proxy that likely interfaces with Triton on the backend via its gRPC protocol\". I have a lot of experience with TensorRT-LLM + Triton and have been working on a highly performant rust-based open source project for the OpenAI compatible API and routing portion[0]. On this hardware (FP4) with this software package 30x compared to other solutions (who knows what - base transformers?) on Hopper seems possible. TensorRT-LLM and Triton can already do FP8 on Hopper and as noted the performance is impressive. [0] - https://github.com/toverainc/ai-router reply modeless 12 hours agorootparentprevHe always does that. They stack up a bunch of special case features like sparsity that most people don't use in practice to get these unrealistic numbers. It'll be faster, certainly, but 30x will only be achievable in very special cases I'm sure. reply cma 11 hours agorootparentIsn't sparsity almost always a win at this point? Making everything fully connected is a major waste. reply modeless 11 hours agorootparentThe kind of sparsity that the hardware supports is not fully general. I'm not aware of any large models trained using it. Maybe they are all leaving 2x perf on the table for no reason, but maybe not. I don't think sparsity is really proven to be \"almost always a win\" for training. reply cma 7 hours agorootparentTo train well with it I think you still need to store all the optimizer state (derivatives and momentum or whatever) if not all the weights (for RigL), so maybe not nearly as much memory bandwidth advantage as you get in inference? reply ephemeral-life 12 hours agorootparentprev30x is the type of number that when you see it in a generational improvement, you should ignore it as marketing fluff. reply azeirah 12 hours agorootparentFrom how I understood it, it means they optimised the entire stack from CUDA to the networking interconnects specifically for data centers, meaning you get 30x more inference per dollar for a datacenter. This is probably not fluff, but it's only relevant for a very very specific use-case, ie enterprises with the money to buy a stack to serve thousands of users with LLMs. It doesn't matter for anyone who's not microsoft, aws or openai or similar. reply misterdabb 10 hours agorootparentIt's a weird graph... It's specifically tokens per GPU but the x-axis is \"interactivity per second\", so the y-axis is including Blackwell being twice the size and also the increase from fp8 -> fp4, note it will needs to be counted multiple time as half as much data is needed to be going through the networks as well. reply acchow 11 hours agorootparentprevThey showed 30x was for FP4. Who is using FP4 in practice? reply KaoruAoiShiho 11 hours agorootparentBut maybe you should. Once the software stack is ready for it there'll be more people since the performance gains are so massive. reply dagmx 7 hours agorootparentIt would depend highly on the model though. Some stuff will generalize better to FP4 than others. reply jimmySixDOF 6 hours agorootparentprevThis is also the only place Nvidia are getting competitive pressure - from the likes of Groq (and likely but less published from Cerebras) with higher inferance T/s and concurrency utilization/batching [1] so if this proves to be the true then the case for big chip systems (on todays specs) will be harder. [1]https://twitter.com/swyx/status/1760065636410274162?t=rpbcr8... reply my123 12 hours agorootparentprevThe 30x number is for a really narrow scenario tbh. Running a GPT 1.8T parameters (w/ MOE) on one GB200 reply huac 12 hours agorootparent'narrow scenario,' perhaps, but one that also happens to closely match rumors for GPT4's size reply dagmx 12 hours agorootparentprevYeah and the 30x is largely due to the increase in factors like packaging and throughput. It's not indicative of general purpose performance which is what I was talking about. Again, I do think the throughput and energy efficiency gains are impressive, but the raw performance gain is lower than I'd have expected for the massive leap in node size etc reply qwertox 11 hours agorootparentprevBut Blackwell in the graph is FP4 whereas Hopper is FP8. reply YetAnotherNick 11 hours agoparentprevHow is 2.5x disappointing in one generation? reply dagmx 10 hours agorootparentDid you skip the sentence immediately after that one? It’s two fused chips. So 1.25x per chip. 25% uplift. Not 2.5x uplift. The 2.5x is for the whole package. reply downvotetruth 8 hours agorootparent> two fused chips Jensen's comment about being first was such a dig to Emerald Rapids. reply dagmx 7 hours agorootparentIs it the first? The Apple Ultra series chips are two Max’s fused with an interconnect. In which case it’s both CPU and GPU. I believe this is just the first for a GPU only product. reply throwaway11460 8 hours agorootparentprevIs that how it works? Why don't we just put many chips in one computer? reply dagmx 7 hours agorootparentthe massive blackwell SoC he showed is two Blackwell dies with an interconnect. It’s very similar to what Apple does with their Ultra series. Then the B200 package is 2 of these plus a CPU. So a total of 4 GPU dies in each unit. reply abhinavk 6 hours agorootparent> Then the B200 package is 2 of these plus a CPU. That's GB200. reply chimney 11 hours agorootparentprevCompare to the 10x that was Hopper uplift. reply YetAnotherNick 11 hours agorootparentBecause it involved scaling in chip area needed for FP8. AI community realized that FP8 training is possible few years back so the transistors given for FP8 was scaled. Overall I think transistors grow just by ~50% per generation so most of the gains comes from removing FP32/FP64 share which were dominant 10 years back, but there is only some point it could go to. reply wmf 6 hours agoprevMore technical details: https://news.ycombinator.com/item?id=39752500 reply paulpauper 13 hours agoprevStock unchanged in afterhours. A lot of people were hoping for a big pop on some big development. reply TheAlchemist 12 hours agoparentWell, stock price is not a good short term indicator about Nvidia developments, nor any company for that matter. Nvidia is doing a very good job. That being said, their stock is absolutely and hilariously overvalued. reply Blammar 10 hours agorootparentNVDA's forward PE is ~37, about what it has been for the past ~5 years I've been tracking that. So it's not overpriced based on that metric. If you're convinced the stock is that overvalued, go short some or, if you like to live dangerously, buy some long-term put options (don't be an idiot and buy short-term options.) I have no idea if NVDA is like Cisco Systems in 2000, or if it's something unique. What I am aware of is that there's around 5-7 trillion that were moved from stocks to t-bills since the Fed raised rates in March 2022. If and when they drop their rates back to the historical ~2.5%, it's reasonable to predict these funds will go back into stocks, which will presumably drive up prices. reply TheAlchemist 10 hours agorootparentThat's exactly what I'm saying below - PE is still very high, hence projecting the past growth into the future. But the scale changed a bit. A 37 PE ratio is extremely high by historical standards - this was reserved for very promising, small startups. Not for 2T companies. I know this got distorted in the past 15 years by abnormally low interest rates, but sooner or later it will come back to something that makes sense. Buying long-term put options on Nvidia now is extremely expensive - the stock was so volatile that the price you pay for those options almost annihilate any gains you could expect, even if the stock losses 50% in 12 months. You got me curious about those 5-7 trillions. Where these numbers come from ? reply rytill 7 hours agorootparentThen be the options seller. You can sell cash secured puts, or a put credit spread, or a call credit spread. Calls are even more expensive than puts right now. reply TheAlchemist 7 hours agorootparentSelling options is an even worse idea. Frankly, I don't understand why we made it possible for individuals to gamble by selling options. As Charlie Munger used to say, Wall Street will sell shit as long as shit can be sold. reply rytill 2 hours agorootparentYou would be right about selling naked options. But call/put credit spreads have bounded downside, just like buying options. Selling cash secured puts or selling covered calls would be less risky than just holding stock. reply solumunus 3 hours agorootparentprevPeople need to stop focusing on “historical standards”. For better or worse, retail entering the market en masse (often with options trading) has created a new standard. The market over the last 10 years is the new normal, the smart people have worked this out and are making huge returns. TSLA at its peak had a WAY higher PE than NVDA does now, and NVDA is just as popular, with even stronger fundamentals. reply TheAlchemist 1 hour agorootparentFor better or worse, human psychology doesn't change that much - those historical standards very much apply today. For us, older folks, we've seen this 'new normal' several times already - it will end up as usual. There are no free lunches and as it appears to me that have not entered any permanently high plateau. It's even quite funny that ~100 years ago, we've had the previous big pandemic, and the biggest stock market crash. Epidemic of this century is done, now waiting for the second part ! reply solumunus 1 hour agorootparentNo, that’s objectively wrong. You simply haven’t seen retail involvement in the market on this level before, not even close. It’s a new precedent, the market has changed. To clarify, I’m not saying NVDA won’t crash from here or bear markets no longer exist. I’m simply saying that historic PE valuations are a poor metric for assessing the potential of a stock in todays market conditions. reply TheAlchemist 50 minutes agorootparentWell, if you use the word \"objectively\", I would expect some numbers to back it up. Yes, it's probably the first time that retail is allowed to trade options. But it's not the first time that retail is all in in stocks. I've tried to find a funny number to back it up - just check the Wiki on 1929 crash - https://en.wikipedia.org/wiki/Wall_Street_Crash_of_1929 - there was more money lent to 'small investors' so they can buy on margin ... than the entire amount of currency circulating at the time. On average, all those retail guys will loose money - that's the sad truth. In the long term, the stocks simply follow the earnings - all other movements around this trends are pretty much a zero sum game - and most skilled operators are not loosing money in that game. Price to earning ratio is just the number of years the company 'pays for itself' if you buy it. PER at 40s for big chunks of main indexes mean that either there will be tremendous progress in the economy that will boost the earnings or people are hoping to resell to a bigger fool. Note: I work in finance, and I very much see the retail involvement in stocks. Hedge Funds and banks, are making a ton of money out of them, that's for sure. reply dkrich 7 hours agorootparentprevWe are at a very unique time. The stock market has basically been in a bull market for 15 years with some very short-lived sell-offs along the way. During that time we've had some incredible innovations such as the iPhone, FANG stock dominance and unprecedented profitability for years. You've also had three or four bona fide bubbles in that span, starting around 2017. First was Bitcoin along with the stock market as a whole (with Nvidia being one of the leading stocks of that bull market advance). Then you had Tesla go parabolic and lots of people become rich. Then you had the whole post-COVID speculative mania. The result of this has been extreme credulity by the average person. Today's keynote is the perfect summation of this phenomenon. I saw multiple people who almost certainly couldn't explain in any level of detail how Nvidia GPUs are used for training and inference, but rather rely on the secondhand talking points like CUDA that they've learned by watching Jim Cramer, watching this keynote with excitement and anticipating how much it would pump their shares or call options. Contrast this with Steve Jobs keynotes from 15 years ago when Apple's best days were well ahead of them. Most keynotes were questioned, in some cases even mocked. When Tesla stock broke out, many people couldn't make sense of it. Ditto for cryptocurrencies. But now, taking their cues from those cycles, the average person wants to ride the next bubble to riches and is trying to catch the wave and so now believes every story attached to a rising asset price. CEO's aren't blind to this and are using every opportunity to create favorable storylines. The leadup to a keynote like this carries with it an enormous amount of pressure to deliver. Hence a company like Nvidia leaning into generative artwork and straight up made up storylines like robot development. At the end of the day, I'm afraid that there likely isn't all that much substance and the evidence is beginning to pile up that the megacap tech stocks have run out of ideas which is why they are laying off people en masse and appealing to the AI hype cycle to carry their stocks higher. Consider that Nvidia has gone up 8x- 800%!- in just over a year. The cycles are moving faster and faster. I remember just a year ago when lots of people said Nvidia at $250 was insane. Now here we are with the stock at more than three times that level and most people are calling it cheap. The stock market seems to have in certain areas like semis, completely disconnected from the fundamentals and taken flight. Yes, Nvidia earnings have grown. But understand that this is all part of a positive feedback loop where tech CEO's are pressured by their competitors and shareholders to show that they are investing in AI. Thus they all talk about it on their earnings calls and spend massively. All of their stocks rise in unison as you have a market that increasingly looks like its chasing momentum stock trends up. Nvidia's moves of late have almost nothing to do with any fundamental developments in the company. It has been routinely trading upwards of $45 billion a day. The Friday before last that number was over $100 billion. These are absolutely insane figures. Compare that to Microsoft, the largest company by market cap in the world, which trades on average around $8 billion per day. I think this is generally how bull markets end and I think we may be actually forming the top of the great bull market for the megacaps that began around 2010 but really hit its stride starting in 2017. reply costcofries 12 hours agorootparentprevTell me more about why you believe their stock is hilariously overvalued. reply TheAlchemist 12 hours agorootparentTheir market cap is 2.2T $. In the past year, they had a revenue of 60B $ and net income of 30B $. Absolutely amazing numbers, I agree. The year before they had a revenue of 30B $ and a net income of 4.5B $ - and it was a rather good year. What happens next of course depend of how you judge the situation - was it a peak hype demand ? Will it stabilize now ? Grow at current extraordinary rates ? Scenario 1 - margins get back to normal due to hype going down, competition improving etc - in this case the company is worth at best ~200B $ - or 1/10 of what it is now. Scenario 2 - they maintain current revenue and the exceptional margins - the company would be worth ~1T - or 1/2 of what it is now. Scenario 3 - they current growth rate (based on past 12 months) continue for ~5 years. This is the case the company is worth ~2T $. But they are in a business where most money come from a handful of customers, all of which are working on similar chips - and given the sums in play now, the incentives are *very* strong. My opinion, is that the company is already priced for perfection - basically the current price reflects the perfect scenario. I struggle to see any upside, unless we have AGI in the next 5 years and it decides it can only run on Nvidia chips. All of this is akin to Tesla in the past years. They grew from a small startup to a medium car maker - the % growth rate was huge of course - an amazing achievement in itself. But people projected that the % growth rate would continue - and the stock was priced accordingly. Reality is catching up on Tesla, even if some projections are still absolutely crazy. reply CamperBob2 6 hours agorootparentIt does no good to design similar or even superior chips if you can't get them fabbed. How much of the world's fab capacity has Nvidia already reserved? reply Workaccount2 12 hours agorootparentprevThey are priced as if they are the only ones who are capable of creating chips that can crunch LLM algos. But AMD, Google, Intel, and even Apple are also capable. Apple is in talks with Google to bring Gemini to the iPhone, and it will obviously also be on android phones. So almost every phone on earth is poised to be using Gemini in the near future, and Gemini runs entirely on Google's own custom hardware (which is at parity or better than nVidia's offerings anyway). reply jerf 12 hours agorootparentThis seems as good a place as any to be Corrected by the Internet, so... correct me if I'm wrong. Making a graphics chip that is as good as Nvidia: Very difficult. Huge moat, huge effort, lots of barriers, lots of APIs, lot of experience, lots of decades of experience to overcome. Making something that can run a NN: Much, much easier. I'd guess, start-up level feasible. The math is much simpler. There's a lot of it, but my biggest concern would be less about pulling it off and more around whether my custom hardware is still the correct custom hardware by the time it is released. You'd think you could even eke out a bit of a performance advantage in not having all the other graphics stuff around. LLMs in their current state are characterized by vast swathes of input data and unbelievably repetitive number crunching, not complicated silicon architectures and decades-refined algorithms. (I mean, the algorithms are decades refined, but they're still simple as programs go.) I understand nVidia's graphics moat. I do not understand the moat implied by their stock valuation, that as you say, they are the only people who will ever be able to build AI hardware. That doesn't seem remotely true. So... correct me Internet. Explain why nVidia has persistent advantages in the specific field of neural nets that can not be overcome. I'm seriously listening, because I'm curious; this is a deliberate Cunningham's Law invocation, not me speaking from authority. reply smallmancontrov 12 hours agorootparentI agree with you, but let me devil's advocate. After 10 years of pretending to care about compute, AMD has filled the industry with burned-once experts who, when weighing nvidia against competitors, instinctively include \"likely boondoggle\" against every competitor's quote because they've seen it happen, possibly several times. Combine this with nvidia's deep experience and and huge rich-get-richer R&D budget keeping them always one or two architecture and software steps ahead, like it did in graphics, and their rich-get-richer TSMC budget buying them a step ahead in hardware, and you have a scenario where it continues makes sense to pay the green tax for the next generation or three. Red/blue/other rebels get zinged and join team \"just pay the green tax.\" NV continues to dominate. Competitors go green with envy, as was fortold. reply htrp 11 hours agorootparent> burned-once experts More like burned 2x / 3x / 4x of this time it's different people. Looking at you Intel reply lmm 10 hours agorootparentprev> So... correct me Internet. Explain why nVidia has persistent advantages in the specific field of neural nets that can not be overcome. I'm seriously listening, because I'm curious; this is a deliberate Cunningham's Law invocation, not me speaking from authority. To become a person who writes driver infrastructure for this sort of thing, you need to be a smart person who commits, probably, several of their most productive years to becoming an expert in a particular niche skillset. This only makes sense if you get a job somewhere that has a proven commitment of taking driver work seriously and rewarding it over multiple years. NVidia is the only company in history that has ever written non-awful drivers, and therefore it's not so implausible to believe that it might be the only company that can ever hire people who write non-awful drivers, and will continue to be the only company that can write non-awful drivers. reply bgnn 11 hours agorootparentprevCUDA is/was their biggest advantage to be honest, not the HW. They saw the demand to super high-end GPUs driven by Bitcoin mining craze thanks to CUDA, and it transitioned gracefully to AI/ML workloads. Google was much more ahead to see the need and develop TPUs for example. I don't think they have a crazy advantage HW wise. Couple of start-ups are able to achieve this. If SW infrastracture end is standardized, we will have a more level playground. reply __mharrison__ 3 hours agorootparentprevAnecdata... one of the folks sitting in front of me at a session at GTC claimed the be an AMD employee who also claimed to previously work on cuda. He seemed skeptical that AMD would pull this off. This is the sort of fun stuff that you hear at a conference and aren't sure how much of it is just technical bragging/oneupmanship. reply elorant 11 hours agorootparentprevCUDA is a big reason for their moat. And that's not something you can build in a couple of years no matter how money you can throw on it. Without CUDA you have a chip that runs on premise without anyone having a clue how good that is which is supposedly what Google does. Your only offering is cloud services. As big as this is, corporations would want to build their own datacenters. reply sottol 11 hours agorootparentSure, CUDA has a lot of highly optimized utilities baked-in (CUDNN and the likes) and maybe more importantly, implementors have a lot of experience with it but afaict everyone is working on their own HAL/compiler and not using CUDA directly to implement the actual models. It's part of the HAL/framework. You can probably port any of these frameworks to a new hardware platform with a few man-years worth of work imo if you can spare the manpower. I think nobody had the time to port any of these architectures away from CUDA because: * the leaders want to maintain their lead and everyone needs to catch up asap so no time to waste, * and progress was _super_ fast so doubly no time to waste, * there was/is plenty of money that buys some perceived value in maintaining the lead or catching up. But imo: 1. progress has slowed a bit, maybe there's time to explore alternatives, 2. nvidia GPUs are pretty hard to come by, switching vendors may actually be a competitive advantage (if performance/price pans out and you can actually buy the hardware now as opposed to later). In terms of ML \"compilers\"/frameworks, afaik there's: * Google JAX/Tensorflow XLA/MLIR, * OpenAI Triton, * Meta Glow, * Apple PyTorch+Metal fork. reply sangnoir 9 hours agorootparentprev> CUDA is a big reason for their moat. Zen 1 showed that absolute performance is not the end-all metric ( Zen lost on single-core performance vs Intel). A lot of people care for bang-for-buck metric. If AMD can squeak out good-enough drivers for cards with good-enough performance for a TCO[1] significantly lower than NVidia, they break Nvidia's current positive feedback cycle. 1. Initial cost and cooling - I imagine for AI data center usage, opex exceeds capex. reply imtringued 1 hour agorootparentprevIt doesn't. If NVIDIA doesn't work with SK Hynix to integrate PIM GDDR into their products they are going to die, because processing in memory is already a thing and it is faster and more scalable than GPU based inference. reply belter 11 hours agorootparentprevGood luck with that. Gemini Advanced is simply unusable right now....It's so bad its hard to believe nobody picked up on that yet. reply belter 10 hours agorootparentGo to Gemini Advanced and try a common programming task in Parallel with Claude and ChatGPT4. Within 2 prompts Claude and ChatGPT4 will give nice working code you can use as a basis while Gemini Advanced will ignore your prompts, provide partial code and quickly tell you it can do more, until you tell it exactly what you want. It will go from looking usable to stuck on \"I can do A or I can do B you tell me what you prefer hell\" in less than 2 or 3 prompts...Unusable. And I say that as paying customer that will soon cancel the service. reply Workaccount2 10 hours agorootparentYou're not wrong, but it wouldn't be surprising if Google irons things out with a few more updates. The point is that it would be foolish to write off Gemini right now, and Gemini is totally independent of Nvidia's dominance. reply swalsh 12 hours agorootparentprev72 P/E ratio while they have a mere monopoly on one the most valuable resource in the world. Competition WILL come. Maybe it's Groq, maybe AMD, maybe Cerebras. Maybe there's a stealth startup out there. Point is, they're going to be challenged soon. reply htrp 11 hours agorootparentYou and what fab? It's almost impossible to manufacture at scale with good yields and leading edge fabs are almost all bought out. reply smallmancontrov 12 hours agorootparentprevNo moat. Yes, CUDA, but CUDA is maaaaaybe a few tens of billion USD deep and a few (more) years wide. When the rest of the industry saw compute as a vanity market, that was sufficient. Now, it's a matter of time before margins go to, uhhh, less than 90%. Does that make shorting a good idea? I wouldn't count on it. The market can always remain irrational longer than you can remain solvent. reply yen223 9 hours agorootparentI used to think that CUDA was something that would get commoditised real fast. How hard could building it be? However, given that the nearest competitor AMD has basically given up on building a CUDA alternative, despite the fact that this could grow the company by literal trillions of dollars, I suspect the CUDA moat is much bigger than I give it credit for. reply tiahura 11 hours agorootparentprevAnd MS and everyone else have plenty of interest in helping AMD commodify CUDA compatibility. reply stefan_ 10 hours agorootparentIt's so weird it's taking them so long, because as far as anyone can tell AMD is mostly competent enough to make GPUs within some percentage points of Nvidia, the \"breadth of complexity\" in what these things do at the end of the day is ... rather underwhelming, the software stack may appear to be changing all the time but is also distinctly JavaScript-frotend-esque... is there an insider that knows what the holdup is? Is AMD just averse to making a ton of money? At this point AMD investors should be rebelling, it's pissing money out there but they are not getting wet, and management might have doubled the stock price but that's little consolation if \"order of magnitude\" is what could have been. reply sangnoir 8 hours agorootparent> At this point AMD investors should be rebelling Looking at the chart for $AMD over the past 5 years gives plenty od reasons to be happy, and no reason to rebel. A rational AMD investor should not be Jonesing Nvidia's catching lightning in a bottle via crypto + AI. The Transformers paper was published a few months before AMD released Zen 1 chips - they did not have a lot of money for GPU R&D then. The timing of the LLM-craze was very fortuitous for Nvidia. reply wkat4242 7 hours agorootparentprevIt's kinda great for those of us wanting GPUs though. Nvidia might eventually decide it's not worth their time to bother with. reply cma 11 hours agorootparentprevThey also bought infiniband which has played a big role in being the best at clustering, though Google's TPU reconfigurable topology stuff seems really cool too. Tesla went after them with Dojo and has still ended up splurging on big H100 clusters. reply xyst 12 hours agorootparentprevBecause their stock value is highly coupled with crypto mining and AI craze. The move from PoW to PoS for most crypto networks in combination with bust of ‘22. NVDA slid down in value. OpenAI debuts ChatGPT in late 2022 and now it’s suddenly bumping in price as the hype and rush for GPUs from companies of all types buys up their stock of GPUs. Demand is far outpacing the supply. Nvda can’t keep up. Thus, share price is brittle. Competition in the GPU market is dominantly owned by Nvidia. That can change, but so far openai loves using nvidia for some reason. reply ryandrake 12 hours agorootparentIf you are a true believer that AI is not a craze, then the stock can only go up from here. If you think there is a chance that everyone gets bored of AI and moves on to some other fad that is not in Nvidia’s wheelhouse, then it’s probably down from here. I’m staying out of this bet: don’t have the stomach for it. reply throw0101b 9 hours agorootparent> If you think there is a chance that everyone gets bored of AI and moves on to some other fad that is not in Nvidia’s wheelhouse, then it’s probably down from here. You may wish to look at history to see how things can work out: Cisco had a P/E ratio of 148 in 1999: * https://www.dividendgrowthinvestor.com/2022/09/cisco-systems... The share price tanked, but that does not mean that people got bored of the Internet and the need for routers and switches. QCOM had a P/E of 166: did people decide that mobile communications was a fad? The connection between technological revolutions and financial bubbles dates back to (at least) Canal Mania: * https://en.wikipedia.org/wiki/Canal_Mania * https://en.wikipedia.org/wiki/Technological_Revolutions_and_... It is possible for both AI to be a big thing and for NVDA to drop. reply oblio 5 hours agorootparentRegarding bubbles: https://en.m.wikipedia.org/wiki/South_Sea_Company https://en.m.wikipedia.org/wiki/Tulip_mania reply AlexandrB 11 hours agorootparentprevThere's another case for pessimism as well: cost. It's possible that many AI applications aren't worth the money required for the extra compute. AI-enhanced search comes to mind here: how is Microsoft going to monetize users of Copilot in Bing to justify the extra cost? Right now a lot of this stuff is heavily subsidized by VCs or the MSFTs of the world, but when it comes time to make a profit we'll see what actually sticks around. reply _factor 7 hours agorootparentBetter question: why does a simple search for “What color is a labrador retriever” require any compute time when the answer can be cached? This is a simple example, but 90% of my searches don’t require an llm to process a simple question. reply jazzyjackson 2 hours agorootparentOne time I came across a git repo that let me download a gigabyte of prime numbers and I thought to myself, is that more or less efficient than me running a program locally to generate a gigabyte of prime numbers? The compute for a direct answer like that is fractions of a penny, it might be better to create answers on the fly than store an index of every question anyone has asked (well, that's essentially what the weights are after all) reply jacobr1 11 hours agorootparentprevThis seems true as far as incentives go. But how much of that cost driver will be due to efficiencies driven by companies like NVIDIA? They seem well poised to benefit from a lot of the increased (non-hype) use of AI. Seems like we spent a decade or more of stalled CPU performance gains chasing better energy efficiency in the data center, same story could play out here. reply partiallypro 11 hours agorootparentprevAI is obviously the future, though current iterations will probably die at some point. but the dot com bubble ended up with the internet being more pervasive than may have even been thought of at the time, but regardless even the likes of Amazon's stock went bust before it recollected itself. Not a perfect comparison given Nvidia has really good revenue growth, but the point still stands. reply Takennickname 12 hours agorootparentprevBecause he missed the train. My guess. reply swalsh 12 hours agoparentprevAt 2 trillion, it's all baked in already reply _factor 7 hours agoparentprevNvidia is no secret. Whatever hidden value is in the stock, is likely already represented. reply synergy20 13 hours agoparentprevnot only that, it lost steam during the day, maybe it was overheated too much and no more news can pump it up any further. reply rvz 12 hours agoparentprev> A lot of people were hoping for a big pop on some big development They are waiting for earnings projections for such a pop since right now it is extremely overbought and struggling to move past >$1,000 per share. For now, Microsoft and OpenAI will use these chips, but in the long term they are just looking at this and plotting to build their own chips and reducing their dependence on Nvidia and will be ready to switch once their contracts have run out. reply dagmx 13 hours agoparentprevI imagine it’ll pop in the morning reply dxbydt 12 hours agoparentprevguy is messing it up bigtime and in real-time as well. sheesh. none of his jokes are landing. “we had 2 customers. we have more now”. long pause. screen behind him covered with logos of all his customers. pause. pause. finally applause. ok on to the next tidbit. whole conference has been proceeding like this now. look if you invite cramer and the wall street crowd, you should throw in some dollar figures. like - who is paying for all this. how much. and why. talk is entirely about token generation bandwidth, exaflops and petaflops, data parallel vs tensor parallel vs pipeline parallel - do you honestly think cramer knows the difference between an ml pipeline and an oil pipeline ? i am watching this conf with my kid - proper GenZ member - who got up after 5 mins and said man who is this comedian, his jokes are so bad, and left :( reply gmerc 12 hours agorootparentNah, wallstreet doesn’t understand what it’s looking at. That’s fine, it’s a developer conference for a founder lead company that hasn’t reached the “stock price is the product” state. He’s not trying to optimize the next 5 days of stock. There’s a full ecosystem grab with Nim there, a new GPU that forces every major datacenter to adopt (or their competitors will massively increase their compute density) reply smallmancontrov 12 hours agorootparentprevYou might not like it, but this is what peak performance looks like. reply Deasiomlo 10 hours agorootparentprevWhy would a Gen z kid care about this conference? It's not a hipster event. And the pauses are clearly his style of presenting. Having a artificial pause to tell the audience it would be a good time to react. You clearly don't like it, I don't mind it. But just to be clear: we see peak human ingenuity. This right now will be history on how we as humans build AGI, full human robots etc (in case we don't nuke ourselves). You can read the conference results easily at any it news portal. There is no requirement from Nvidia entertaining you or your kid. reply anon291 12 hours agorootparentprevThis is a developer's conference, not a financial one. reply Takennickname 12 hours agorootparentprevCramer is an entertainer. Not a developer or an investor. reply tamimio 11 hours agoprevI think at this point, they should stop making it video “cards” but rather video “stations”, a full tower station with power supply and one giant “card” inside with proper cooling, etc., might also justify the crazy prices anyway. reply jedberg 11 hours agoparenthttps://lambdalabs.com/gpu-workstations/vector reply justinclift 9 hours agorootparentHeh Heh Heh \"Ultimate AI Workstation\". Pricing starts at US$83,549: https://shop.lambdalabs.com/gpu-workstations/vector/customiz... Adding every option only adds $2,100 to the price too (totalling $85,649). They should probably just include everything as standard. ;) reply jazzyjackson 2 hours agorootparentprevI love that \"we installed a working python environment for you\" is a front-page value-add reply ufocia 11 hours agoparentprevProbably better to stick to the GPUs. Integration is a low margin game. reply georgyo 11 hours agorootparentI'd prefer they stick to GPUs, but I think you're over simplifying. Dell proves that selling complete units is very profitable. Apple shows that owning the entire stack is immensely profitable. Nvidia already has significant hardware and software investment. They very well could fully integrate and grab larger slices of the pie. In fact, Nvidia already has complete appliance like fully integrated machines. But enterprises like to install their own OS and run their own software stack. These appliances have not caught on, at least not yet. reply zer00eyz 9 hours agorootparent>> Apple shows that owning the entire stack is immensely profitable. Apple shows no such thing. Apple, sells pretty, reliable and safe. A car is a car, but apple is a sports car, or a saloon. Vertical integration is the way they chose to deliver that, and pretty and reliable are all normal people care about. Nvidia is gonna have to think long and hard about the \"whole stack\". 20 years ago they might have been able to pull a next, but right now anything that isnt LINUX is a rounding error, and they dont want to turn into SUN (no one at nividia is smart enough to make them the next sun). Nvidia architecture + Nvidia os is not something that I see them pulling off for the datacenter. reply __mharrison__ 3 hours agorootparentNvidia is moving up the stack. They announced NIMs today. I liken it to Docker for AI. reply caycep 11 hours agorootparentprevgranted, at this point we plug the computer into the GPU, so it might not make a difference... reply acchow 10 hours agoparentprevThat’s the DGX GB200 they announced today, with liquid cooling. reply wmf 10 hours agoparentprevSXM for desktop would be great but it won't happen. The PC industry can't even adopt things like 12VO. reply ribosometronome 11 hours agoparentprevIsn't that just a computer? Or an eGPU, if it doesn't contain the rest of the computer? reply adamnemecek 6 hours agoprevWhich Blackwell is it named after? reply wmf 6 hours agoparenthttps://en.wikipedia.org/wiki/David_Blackwell reply jairuhme 12 hours agoprevI haven't listened to Jensen speak before, but am I the only one who thought the presentation wasn't very polished? Not a knock on anything he has accomplished, just an observation that sorta surprised me reply azeirah 12 hours agoparentHe said he didn't rehearse well. I think it makes him come across very genuinely, not some dumb hyperpolished corporate blabla reply CaptainFever 2 minutes agorootparentIn comparison to Apple keynotes, precise, polished, practiced and pre-recorded. reply sumedh 30 minutes agoparentprevIf I am not mistaken he has said in some interviews he is not a big fan of public speaking. reply swalsh 12 hours agoparentprevHe's selling water in a desert, kind of doesn't matter how polished his presentation is. reply erupt7893 11 hours agoparentprevI've been watching his keynotes for as long as I can remember, this is how it's always been reply Havoc 9 hours agoparentprevPrevious ones have been similar. Awkward “you may clap now” pauses etc. Don’t think anyone cares as long as the company keeps getting the bets right reply cableshaft 12 hours agoparentprevThe beginning in particular seemed pretty rough, but he seemed to mostly get into a groove about halfway into it. At least he started talking a lot smoother around then. reply jefozabuss 12 hours agoparentprevWhen he had that slide up with generating everything I was kind of expecting that he'd say this whole keynote is generated including him. That'd have been crazy. reply caycep 11 hours agoparentprevI remember his opening line at NEURIPS 2017, to an audience of grad students and postdocs, \"only Nvidia would unveil their most expensive product to an audience who's completely broke\" Then he went into a comedic monologue about GANS. But hey, at least that meant that the CEO was reading the actual conference proceedings... reply acchow 12 hours agoparentprevHe has more important things to do than perfecting a presentation. He likes his employees to message him freely with things they think he can help with. reply PartiallyTyped 3 hours agorootparentI read about this around and tbh I respect it a lot. Somehow Jensen has managed to scale the company and still keep it focused on the product with high eng practices. reply modeless 12 hours agoparentprevIt's typical. He isn't a great public speaker IMO. Not terrible but not great. reply angm128 12 hours agoparentprevThe products, animations and slides are doing some heavy lifting. Most jokes don't land and his presentation is somewhat confusing at times (e.g. star trek intro token count) reply sct202 12 hours agoparentprevI think it's a good reminder that objectively great CEO's and leaders can be kind of cringe when presenting. A lot of times people like that get passed up in promotions in favor of smooth talkers. reply wmf 10 hours agorootparentIt's been said that founders are people who can't get hired. reply ipsum2 12 hours agoparentprevYeah he said he didn't rehearse and it really shows. reply CoachRufus87 7 hours agoparentprevIts refreshing. Another tech keynote that I regularly watch (Apple) is far too polished these days. reply geor9e 10 hours agoprevLike a lot of the commenters here, I have a problem with this headline. They don't \"seek to become a platform company\", as in making their own cloud platform where they rent GPU time, meaning they stop selling GPUs to other cloud platforms. That easy misinterpretation makes good clickbait, but no, that's not what the article says - the article has Huang bragging that CUDA already is a parallel computing platform, for a decade or more, and Blackwell Architecture is so integrated and customizable with CUDA (with all its user-extendable kernels and community) that it's thought of as a platform rather than just chip architecture. reply basiccalendar74 9 hours agoparentthey announced a cloud service NIM that hosts LLMs using Nvidia software stack. https://developer.nvidia.com/blog/nvidia-nim-offers-optimize... so there is a push towards a platform company, but probably not well explained in this article. reply baobabKoodaa 9 hours agoparentprevSomeone please \"chip in\" and confirm or deny if this interpretation is correct? reply wmf 8 hours agorootparent\"Platform\" can have several different meanings and some people in this thread are picking the most evil meaning as an excuse to shit on Nvidia. It's true that Nvidia is providing the GPU, the CPU, the server, the rack, the network, the drivers, and the orchestration software to run AI training/inference. (If you want that stuff. You could just buy GPUs if you want.) It's fair to call that a platform. Nvidia is not becoming a cloud provider (beyond a small eval environment perhaps). reply lvl102 12 hours agoprevSeems Nvidia is going for maximum margin as they see competition ahead. reply theGnuMe 11 hours agoparentAnd they can build a big moat with cuda. reply stevethomas 12 hours agoprevTime to sell. When they start becoming a platform, it means they have nothing more concrete in the near future. Sell now and buy again later once the price corrects. reply belter 12 hours agoparentDon't bet against a CEO who knows what is talking about, has 80% market share and an arm tattoo of his own company logo.... :-) So far the short sellers, learned that bitter lesson. reply hsuz 2 hours agoparentprevSelling is a bit exaggerated. Scaling up is almost always non-detrimental. But I do feel that NVDA is slowly falling to a stall because there are no exciting new modes of business coming out. reply pvg 12 hours agoparentprevIf you held and then sold Nvidia stock when they announced CUDA or GeForce Live, you'd now be now a big pile of negative money richer. reply Keyframe 11 hours agoparentprevThey still have to announce you can send an email through their platform. reply golergka 12 hours agoparentprevDoes a company like Nvidia has to have anything more concrete than newer, bigger and faster chips? reply mark336 8 hours agoparentprevI wouldn't be 100% against them, but it is disappointing that they don't have any better ideas. reply amelius 10 hours agoprev [–] \"platform co.\" Platform company, as in they're allowing developers on their AI platform and opening an app store? reply fnordpiglet 10 hours agoparent [–] The headline is editorialized by the submitter, the actual headline is “Nvidia CEO Jensen Huang announces new AI chips: ‘We need bigger GPUs’” which is arguably worse. The article doesn’t discuss becoming a platform co but instead discussed ways their existing platform subscription model is evolving to add backwards compatibility testing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nvidia CEO Jensen Huang unveiled new AI chips called Blackwell, including the GB200 Grace, with 20 petaflops AI performance and a \"transformer engine\" for AI models at Nvidia's developer's conference.",
      "The introduction of NIM software aims to simplify AI deployment on Nvidia GPUs, including older models, signaling Nvidia's transformation into a platform provider similar to Microsoft or Apple.",
      "Nvidia plans to collaborate with major cloud service providers like Amazon, Google, Microsoft, and Oracle to offer GB200 access, aiming to solidify its standing in the competitive AI market."
    ],
    "commentSummary": [
      "Nvidia CEO Jensen Huang introduces new AI chips and a container system for easy deployment, marking a transition to becoming an AI platform provider like Microsoft and Apple.",
      "The rise of companies resorting to cloud services presents a challenge for Nvidia, sparking discussions on potential anti-trust issues and the opening of Nvidia's CUDA standard.",
      "While emphasizing Nvidia's strong position in the GPU market, the discourse delves into concerns about competition, market shifts, and the company's strategic trajectory, including hardware-software integration hurdles and the use of 4-bit floating point calculations in inference tasks."
    ],
    "points": 313,
    "commentCount": 242,
    "retryCount": 0,
    "time": 1710793966
  },
  {
    "id": 39742114,
    "title": "Elegant self-hosted project tracker: Planka",
    "originLink": "https://github.com/plankanban/planka",
    "originBody": "Planka Elegant open source project tracking. Client demo (without server features). Features Create projects, boards, lists, cards, labels and tasks Add card members, track time, set due dates, add attachments, write comments Markdown support in card description and comments Filter by members and labels Customize project backgrounds Real-time updates Internal notifications Multiple interface languages Single sign-on via OpenID Connect How to deploy Planka There are many ways to install Planka, check them out. For configuration, please see the configuration section. Contact If you want to get a hosted version of Planka, you can contact us via email contact@planka.cloud For any security issues, please do not create a public issue on GitHub, instead please write to security@planka.cloud We do NOT offer any public support via email, please use GitHub. Development See the development section. Tech stack React, Redux, Redux-Saga, Redux-ORM, Semantic UI React, react-beautiful-dnd Sails.js, Knex.js PostgreSQL License Planka is AGPL-3.0 licensed. Contributors",
    "commentLink": "https://news.ycombinator.com/item?id=39742114",
    "commentBody": "Elegant open source project tracking, Trello like but self-hosted (github.com/plankanban)268 points by thushanfernando 23 hours agohidepastfavorite87 comments bachmeier 19 hours agoFor someone that's not a web developer, I found Kanboard to be the easiest to set up, and it has all the basic features you'd expect. It's a traditional PHP app where you copy the files to your web server and set a few configuration options and you're good. If you want to use it locally, you download it, run php -S localhost:8080, and start using it. https://kanboard.org/ Note: The project is in maintenance mode, it hasn't shut down or been abandoned. reply KronisLV 18 hours agoparentAlso available as a Docker image, for example: docker run -p 80:80 -t kanboard/kanboard:v1.2.8 https://docs.kanboard.org/v1/admin/docker/#running-the-conta... (that page has info about persistent storage, configuration and so on) Honestly one of the fastest and least \"bloated\" pieces of software in recent memory, way more responsive than something like OpenProject (which I use as a self-hosted Jira replacement for my personal needs), as long as the feature set is enough for you. I did rather enjoy the cost reports of OpenProject, as well as having all of my usual epics and whatnot, but kanban works better for smaller projects than scrum. reply littlestymaar 16 hours agorootparentWhat's the point of using Docker for PHP apps? The main appeal of PHP for me has always been it's ability to work as a “serverless” execution environment, long before this marketing concept even existed, so hosting your own PHP on a cloud machine with Docker sounds really backward to me. reply tunesmith 16 hours agorootparentFor me, I have a cheap cloud server that handles multiple low-traffic personal websites, side projects, etc. Each project has a different tech stack and it can be months or years before I circle back to one to bring it up to date. I don't want to wrestle with making sure that I have the right versions of php and apache for my ubuntu. Having them all as docker containers makes it a lot easier, and a lot easier to move to new servers, too. reply KronisLV 15 hours agorootparentTo add to this, for me it really helps to look at any piece of software that I want to run pretty much the same way, as a self-contained bundle, not unlike an app installed on a phone. I can give them resource limits the same way (CPU/memory limits, except easier than cgroups), as well as set restart policies and have a clear look at what's executing where, with something like Docker Swarm it becomes like systemd across multiple nodes and scaling up/down becomes easy, especially with load balancing for network calls. Software like Portainer also has pretty nice discoverability. Speaking of networking, I don't have to worry about tunnels or firewall configuration myself, can just expose a web server that acts as a reverse proxy and give everything else custom private networks that span across nodes (with something like Docker Swarm again, though Consul and Kubernetes have the same functionality, details aside). I can have custom port mappings (regardless of what the software uses, I might not even care about digging in some configuration file to change it), which is especially useful when running multiple separate instances on the same machine (like different versions of PostgreSQL, or separate instances for different projects), or hostnames in case I don't want to expose ports. I can easily have custom persistent/transient storage paths or even in memory storage (tmpfs), when I have persistent storage then suddenly backups become easy to do and I can be very clear about all other directories being wiped and being in a known state upon startup/restart. It's also immensely useful for me to escape the sometimes weird ways how software on *nix uses the file system, I can just mount my persistent files in /app/my-app/database/var/lib/postgresql/data or /app/my-app/web-server/etc/apache2/sites-enabled and know that I don't care about anything outside of /app. I can also treat Docker as lightweight VMs, except a bit more stateless, in that I can have container images that I base on a version of Debian/Ubuntu/Alpine or whatever, ship them, and then don't have to worry about a host OS update breaking something, because only Docker or another runtime like Podman is the actual dependency and most of the other software on the node doesn't come in contact with what I'm running. With rootless containers, that also improves the separation and security there a little bit. With all of that in place, suddenly I can even move apps and all of their data across nodes as necessary, load balance software across multiple nodes, be able to easily tell people how to run what I have locally and store and later use these images very easily. Are there pieces of software or alternatives (e.g. jails) that do a lot of the same? Sure, but Docker essentially won in ease of use. reply btbuildem 16 hours agorootparentprevFor me, it's the simplicity. I don't have to care whether a project is super basic, or a thorny hairball from hell. Whatever it is, \"docker run\" is how I spin it up. It doesn't infect my local. I can have three differently hobbled versions of it side by side. Virtualization makes it simple, conceptually - and for me that's more precious than it being actually technically simple. reply littlestymaar 18 minutes agorootparent> I don't have to care whether a project is super basic, or a thorny hairball from hell. That's the biggest problem I see with Docker: nobody has an incentive to make well structured software with a lean dependency chain and a straightforward installation process… These used to be good proxy of the overall software quality of the project, but now Rube Goldberg projects that just happen to work by luck are routinely distributed and the user has no idea of how big of a mess it is internally. reply lolinder 15 hours agorootparentprevI self-host a dozen or so different web apps locally on an old PC, and containers are what makes that feasible to do in my very limited spare time. If I tried to run all of these directly on the hardware with whatever minimal non-Docker setup each uses, I'd have a dozen update processes, a dozen different ways to start the server, and a dozen log files following a dozen different conventions for storage. I'd also have to be sure that each app I add either uses a different database and language runtime than the ones I've installed already or is compatible with the versions of those that I already installed. Instead, with Docker/Podman, I can use the same tool (compose files stored in a git repo) to manage all of the apps and their dependencies with zero risk of weird dependency issues across app boundaries. reply diggan 16 hours agorootparentprev> What's the point of using Docker for PHP apps? Same reason you'd use Docker for anything, why would it matter if it's Python, PHP or Rust? Is there something specific about the language that makes Python (or other language) more suitable with Docker for you, compared to PHP? (Personally I only use Docker when I start to deal with multiple hosts/distributed architecture, which doesn't happen a lot tbh) reply littlestymaar 16 minutes agorootparent> Same reason you'd use Docker for anything, why would it matter if it's Python, PHP or Rust? I would ask the same question if you were using docker for a Rust project btw. IMHO Docker mostly make sense when you have projects that require globally installed dependencies (like C or Python). reply lmm 9 hours agorootparentprev> Is there something specific about the language that makes Python (or other language) more suitable with Docker for you, compared to PHP? Python has notoriously awful dependency management. One of the biggest appeals of Docker is that it lets you build the equivalent of a \"fat jar\" so that you get at least somewhat reproducible versions of your dependencies at runtime. For a language with decent dependency management the value proposition is much weaker. reply happymellon 2 hours agorootparentI can't think of anything that Docker would give Python that pyenv and venv wouldn't already. Not that I wouldn't just use Docker for this, but those two help a lot when dealing with multiple clients who have different versions requirements. reply lnxg33k1 12 hours agorootparentprevI always keep the host clean of any language, interpreter, tool, except for docker, and everything I run is ran within docker, I have multiple clients with multiple level of support and PHP versions needed, each project lives in its container reply magicalhippo 11 hours agorootparentprevFor me the point of using Docker is that it's a unifies configuration and backups, and makes installation easier. I can easily see which directories or files to back up, and it's fairly explicit which knobs I've tweaked or config files I've changed, regardless of what stack the app relies on. It's also makes it much easier to roll back a version. Just take zfs snapshots of relevant directories before pulling new image, if it goes south just roll back snapshots and use the old image. reply pmontra 10 hours agorootparentprevWith docker you can self host on your dev machine and for a solo developer it's the end of all problems. reply beretguy 16 hours agorootparentprevProgrammers these days like to overcomplicate things for some reason. I’m as puzzled as you are. reply lolinder 15 hours agorootparentThis comment shows a remarkable lack of curiosity. You're not the least bit interested to know why so many people find tools like Docker to be valuable? reply smokel 11 hours agorootparentIt does show empathy, though. Docker has its advantages, but the approach also has a lot of disadvantages which are not so obvious to junior developers. Isolation seems fun, but the interfaces (Unix sockets where anything goes) are extremely brittle. Version management seems simple at first, but will become horrible once old containers offer no upgrade path in the future, or when the free hubs from today will become tomorrow's subscription model. I'm not advocating for PHP, but it sure made deployment of several websites on one machine extremely simple. Eventually version management destroyed some of the fun, which will probably happen with Docker containers as well, given enough time. Java's application servers were initially also hailed with similar enthusiasm as Docker containers, and look at the complicated mess that has become. To some, all that is old is new again. reply anonymous_union 12 hours agorootparentprevbecause raccoons like shiny things reply diggan 15 hours agorootparentprevOne person's over-complication is another person's simplification, it's only \"hacked together\" if someone else wrote it, etc, etc reply samstave 18 hours agorootparentprevTangent - is there a Docker Wherehouse where I can find dockers to DL an run, that HN would suggest and some use cases of \"pull a docker from here to do X - super cool\" reply Elidrake42 17 hours agorootparentThe awesome selfhosted* list is a pretty good resource. While it does mention if there's a Docker container, I've found a few of the services without one listed do actually offer one, just have to search for it. * https://github.com/awesome-selfhosted/awesome-selfhosted reply tomrod 18 hours agorootparentprevDocker hub has been one of the primary registries. Each of the cloud providers typically have their own concept for docker or image repositories, and you can build docker files locally of you have a docker file in source code. reply samstave 17 hours agorootparentThank you, I was more looking for use-case as opposed to a barf of all dockers.... \"I want to do TASK so here are all the dependencises for you to do TASK and how they will link\" --- And yeah; how do you tink a 3-year old in 2050 is going to be able to setup his dev env? Do you want him to learn binary. reply oarsinsync 14 hours agorootparent> And yeah; how do you tink a 3-year old in 2050 is going to be able to setup his dev env? Dunno about 2050, but it wasn’t particularly difficult in the 1980s. reply paledot 5 hours agorootparentI guarantee it will be harder in 2050 than it was in 1980. reply colonelpopcorn 17 hours agoparentprevIt's plug-in system is quite comprehensive. I just finished writing a note taking plug-in and the source code itself was a great reference for developing a plug-in. reply nbbaier 4 hours agorootparentI'm also interested in the link! reply raphman 13 hours agorootparentprevMind sharing a link if it is public? reply vrinsd 12 hours agoparentprevReally great project, just wish nested tasks or sub-tasks was easier to interact with. reply kioshix 17 hours agoparentprevI also use Kanboard, it's pretty decent. reply 3abiton 8 hours agoparentprevDoes it offer Agile-like integration? reply muppetman 17 hours agoparentprevI didn't realise it'd moved into maintenance mode, where abouts is that detailed? reply kioshix 17 hours agorootparenthttps://github.com/kanboard/kanboard This application is in maintenance mode. What does it mean? Citing Wikipedia: In the world of software development, maintenance mode refers to a point in a computer program's life when it has reached all of its goals and is generally considered to be \"complete\" and bug-free. The term can also refer to the point in a software product's evolution when it is no longer competitive with other products or current with regard to the technology environment it operates within. - The author of this application is not actively developing any new major features (only small fixes) - New releases are published regularly depending on the contributions made by the community - Pull requests for new features and bug fixes are accepted as long as the guidelines are followed reply loganmarchione 19 hours agoprevI switched to Planka after Focalboard went community-supported[1], but failed to appoint any community leaders. So far, I'm very happy with Planka for my needs at home. There are more self-hosted options in this link[2]. [1] https://github.com/mattermost/focalboard [2] https://awesome-selfhosted.net/tags/task-management--to-do-l... reply alberth 13 hours agoparentSince you use to use Focalboard, do use your Mattermost (chat)? Curious what your experience has been like using it? reply vrinsd 12 hours agorootparentI'm not the parent-level poster but I've stood-up a Mattermost instance a few times and it's really easy to get going and is good for a text-IM/DM channel/group service. The desktop app or web-based interface work quite well and the architecture is pretty sane, Javascript front-end, golang-based \"backend\", Postgres database. But, there are some frustrating aspects. LDAP is only available in the \"enterprise\" edition which is kind of crazy and there is no price-break for < 10 users. So for personal / non-commercial usage if you want LDAP you're placed into an enterprise bucket. I reached out to Mattermost and pointed this out and even said \"Hey, what about offering a 10-user license for some reasonable fee?\" No response. reply lionkor 19 hours agoprevIve had great success with Kanboard, but at BeamMP we use plane[0], self-hosted. Apart from the lack of github integration, it does the job for our small team. [0]: https://plane.so/ reply remram 17 hours agoparenthttps://github.com/makeplane/plane, AGPL reply drunkan 10 hours agoparentprevThis is the closest looking open source one I’ve seen to linear which is by far the best kanban I’ve used maybe one inspired the other, either way looks good though linear is so polished. Introduced it to a client and it gets heavy use and the cycles concept is well liked reply remram 8 hours agoparentprevWhat is this monstrosity. This is the first time I see software that runs in docker-compose but has to be installed with a setup.sh run as root. What the hell is wrong with those people? Whatever setup steps are required, put them in the container! reply rbut 4 hours agorootparent100% agree. When we installed it we looked at the setup.sh and extracted what was needed from it. It was as simple as: curl -o docker-compose.yaml https://raw.githubusercontent.com/makeplane/plane/master/deploy/selfhost/docker-compose.yml curl -o .env https://raw.githubusercontent.com/makeplane/plane/master/deploy/selfhost/variables.env vim .env # adjust for your environment docker compose up -d I really don't understand how the above is too complex that it required the creation of a bash script. Some other notable docker-based projects that I've seen require an .sh are Sentry [1] and Postal [2]. [1] https://develop.sentry.dev/self-hosted/ [2] https://docs.postalserver.io/getting-started/prerequisites reply huhtenberg 12 hours agoprevUsing the \"client demo\" [1] - once a new project is added and a new board is added to that project, how do you add a list? There seems to be just a blank canvas, basically [2]. [1] https://plankanban.github.io/planka [2] https://i.imgur.com/6OPyn9W.png reply __fst__ 14 hours agoprevIf you use kanban as your personal to-do list I can recommend this Obsidian plugin: https://github.com/mgmeyers/obsidian-kanban reply rpigab 19 hours agoprevLooks nice, I selfhosted https://github.com/wekan/wekan for a while, which is a MIT licensed heavily Trello-inspired alternative, does someone know both Wekan and Plankanban and can tell their differences? reply xet7 17 hours agoparentWeKan has MIT license. As maintainer of WeKan, I'm adding major new features to WeKan. WeKan features are listed at https://github.com/wekan/wekan/wiki/Deep-Dive-Into-WeKan Planka changed from MIT license to AGPL-3.0 license https://github.com/plankanban/planka There is Planka fork 4gaBoards with MIT license at https://github.com/RARgames/4gaBoards , newest change one hour ago. reply liotier 17 hours agoprevGitlab's kanban board is very nice, integrated with its ticketing and, if you are on Gitlab, it is there already ! reply coldblues 13 hours agoprevI'm surprised https://vikunja.io/ wasn't mentioned. reply winrid 10 hours agoparentIt looks like this has a public view too you can share to customers? reply vrinsd 12 hours agoparentprevYes! And it supports sub-tasks! reply jms703 18 hours agoprevAnother open source project tracking alternative: https://kanboard.org/ reply dools 8 hours agoprevI did some work about a year ago modifying Trellinator to support WeKan, and would like to do the same here. When I did that I made a comprehensive list of things that were missing from the WeKan API to provide a fully functional drop-in replacement for existing Trellinator code. I can't see any API documentation, is it somehow Trello-like? reply johnchristopher 2 hours agoparentThere's an API, a year ago I had a nifty little script that would pull all my incomplete tasks for the day. reply scubbo 17 hours agoprevI tried installing OpenProject on my homelab (for tracking tasks related _to_ my homelab), only to find that it was missing the one feature I really wanted - identifying dependencies and blockers (i.e. \"I can't install X until I install Y, but Y needs a feature that requires an update to Z, and updating Z requires I tweak config in A\" - where I'm perfectly happy to manually write out X/Y/Z/A as tickets myself, but I want a tool to tell me that \"A\" is an unblocked task I can pick up). Any suggestions for a tool that can do that? reply nico 19 hours agoprevLooks really good, great work! Anyone knows of something like this but for the terminal? I’m building a job searching app for the terminal and a main upcoming feature is to have application tracking within the app. It would be great to use a kanban system for it Thank you! reply xet7 17 hours agoparentFor terminal, for example this, made with Rust: https://github.com/yashs662/rust_kanban reply nico 12 hours agorootparentReally like the look of this one, super cool reply bachmeier 19 hours agoparentprevThis one got a lot of attention a while back: https://github.com/smallhadroncollider/taskell reply khqc 3 hours agoprevAnyone using icescrum? I like how it can be self-hosted reply h1fra 18 hours agoprevCongrats on shipping. Elegant could be removed from title though @dang reply zettabomb 17 hours agoparentIt's written that way at the top of the README. reply rjzzleep 18 hours agoprevEven though it's not open source, just free(for very small projects), I have been really liking kitemaker.co I'm curious what other people think of their approach, and whether that should be a model for open source kanban boards to follow. It's not Trello, which is way to flexible turning work items into a mess, but it's not Jira either. For me it seems to nicely fit the sweet spot of structure and ease of use. reply TomasEkeli 17 hours agoparenti love kitemaker, and they're really responsive on their slack reply poidos 19 hours agoprevLooks nice! An alternative is https://www.openproject.org/ reply johnchristopher 2 hours agoparentThey really aren't the same beast, though. It's like people are posting their preferred project manager without even checking out what planka brings to the table. So many kanboard comments. reply eps 18 hours agoparentprev\"Start free trial\" reply zettabomb 17 hours agorootparentThe base functionality is free (and this is far more than a basic kanban). If you want more enterprisey features it's paid, although I don't believe it's too difficult to bypass the license check. reply latchkey 18 hours agoprevI personally would like to see less Trello and more Pivotal Tracker. Especially now that they've changed their pricing model. reply ensocode 1 hour agoprevCame here to say the same. Kanboard is great and has a nice plugin concept. But I think this one is indeed more elegant and looks very modern. Good job! reply orblivion 20 hours agoprevUI seems smoother than Trello or Wekan (on my rather slow machine). Though maybe it's because the demo board doesn't have very much data compared to what I have on those other two. And maybe it has fewer features thus far to bloat the frontend. reply loganmarchione 19 hours agoparentI believe the demo doesn't connect to a database backend. Their GitHub says \"without server features\". https://github.com/plankanban/planka reply orblivion 18 hours agorootparentMy sense is that the thing that makes the other sites laggy on my computer is UI stuff. Maybe not though. reply satellite2 14 hours agoprevWith the demo client, I don't manage to create a list on an empty board on FF on Android. reply applied_heat 11 hours agoprevI’m still using redmine, it doesn’t look fancy but it works ! reply winrid 10 hours agoprevI've been using Asana for years but there have been a lot of quality issues with the software... and various bugs that just don't get fixed. They also put columns and some sorting options behind a paywall. Are there any similar OSS tools? I just need task tracking that works offline, on mobile, let's me filter/sort, and creating public shareable links for customers would be a nice bonus. It's probably something I could whip up in an hour with Django but open to options. reply sneak 17 hours agoprevIf you are already selfhosting Gitea, it has a nice kanban-style project board view that works similarly. This is what I use, and find it to be pretty good. It’s not as good as a dedicated solution but it’s one less app I have to tend to, and the Gitea backups are already mega mission critical so the PM stuff (and issues and wikis) get this vigilance baked in for free. reply maayank 17 hours agoprevJust use taskwarrior + git reply LAC-Tech 11 hours agoparentManually having to sync with stuff like: git commit -am \"update\" is very tedious. reply winrid 10 hours agorootparentSetup a cron! reply otabdeveloper4 1 hour agorootparentThanks but no thanks. reply alchemist1e9 15 hours agoparentprevI use both also but not together. Can you elaborate on any synergy or connection you are referring to? reply colordrops 19 hours agoprevI'm frankly not a fan of the monolithic NextCloud, but the \"Decks\" feature has good UX and a mobile app on Android, which pretty much nothing else in the open source community has pulled off. reply npteljes 57 minutes agoparentThat's what I'm switched to, especially since I already use Nextcloud as a cloud backend for my phone. From the kanban suggestions, I tried kanboard in the past, but really disliked the mobile experience. In this regard, Deck is much better, and it has at least two ways to access the boards; one is the Nextcloud Deck companion app, and the other is the jtx board, which stores its tickets in a way that they can be synced with CalDav. So by using Nextcloud Deck, one is not even locked in into one application / provider. reply lolc 14 hours agoparentprevThanks for the hint! A friend recently asked about a kanban-style app, and they're already using Nextcloud. reply npteljes 55 minutes agorootparentI suggest to actively keep up with Nextcloud project. They have a lot of things going on, and add nice things over time. \"Notes\" for example is also useful. reply ulrischa 15 hours agoprevAnother cool tool for this is: https://kanboard.org/ reply ImHereToVote 20 hours agoprev [–] Looks really nice. Is there a roadmap Planka board? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Planka is an open-source project tracking tool enabling project, board, card, and task creation, time tracking, due dates setting, and attachment adding.",
      "It supports markdown, filtering by members/labels, real-time updates, and internal notifications, deployable in various methods with single sign-on via OpenID Connect.",
      "Tech stack: React, Redux, PostgreSQL; licensed under AGPL-3.0. For the hosted version, email contact@planka.cloud, and for security problems, reach out to security@planka.cloud. Development details and support available on GitHub."
    ],
    "commentSummary": [
      "The focus is on self-hosted project tracking tools like Kanboard, highlighting the use of Docker for simple configuration and administration.",
      "Users discuss their experiences and suggest various task management tools, such as Planka and Redmine, along with comparisons to software like OpenProject.",
      "Considerations for hosting PHP apps with Docker, managing multiple projects with different technology stacks, and recommendations for tools like Nextcloud Deck and Taskwarrior are also featured."
    ],
    "points": 268,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1710756976
  },
  {
    "id": 39744932,
    "title": "Paris Sees Cycling Numbers Double Due to Infrastructure Investment",
    "originLink": "https://momentummag.com/paris-cycling-numbers-double/",
    "originBody": "} Paris cycling numbers double in one year thanks to massive investment and it’s not stopping News, Advocacy March 14, 2024 Paris cycling numbers double in one year thanks to massive investment and it’s not stopping In the heart of Paris, amidst the historic boulevards and bustling streets, a revolution is taking place—one fueled not by engines, but by pedals. A report from French newspaper Le Monde paints a vivid portrait of the city’s evolving cycling culture, revealing a surge in two-wheeled activity that has captured the attention of cyclists and […] Written by: Ron Johnson In the heart of Paris, amidst the historic boulevards and bustling streets, a revolution is taking place—one fueled not by engines, but by pedals. A report from French newspaper Le Monde paints a vivid portrait of the city’s evolving cycling culture, revealing a surge in two-wheeled activity that has captured the attention of cyclists and urban enthusiasts alike. The article dives deep into the data, uncovering a remarkable doubling in bicycle usage on Parisian streets between October 2022 and October 2023. But this is more than just a statistic—it’s a reflection of a broader societal shift towards sustainable mobility and urban renewal. As Deputy Mayor David Belliard of Europe Ecologie-Les Verts (EELV) aptly notes, cycling in Paris has transcended mere trendiness; it’s become a fundamental aspect of the city’s identity. Despite challenges like inclement weather, cyclists continue to flock to the streets, setting new attendance records and reshaping the urban landscape in the process. “We already knew that the use of bicycles had gone far beyond the simple fashion effect. (…) While the weather has deteriorated in Paris in recent weeks, we have still recorded attendance records,” said Belliard. Rue de Rivoli in Paris, circa 2020 “In mobility more than elsewhere, the central issue is supply rather than demand,” says the elected official, who assures that the municipality has “massively amplified the supply in infrastructure.” But it’s not just about numbers, the report delves into the nuances of Parisian cycling culture, exploring the vibrant community of riders who navigate the city’s streets with grace and determination. From leisurely rides along the Seine to daily commutes through bustling boulevards, cycling has become an integral part of Parisian life. It’s not surprising, as the success has come after significant investment in cycling infrastructure dating back to 2015 when Mayor Anne Hidalgo first started advancing Paris’s cycling plan with 150 million euros invested to double the number of cycling lanes and create a critical mass of infrastructure. This, followed by even greater investment to fill in the gaps and make Paris a “100 percent cycling city.” And it is unlikely to slow down anytime soon. It more likely to spread throughout France. It was just a year ago that Momentum reported a huge $2 billion investment to promote cycling throughout the country over the next four years. And as the city continues to invest in cycling infrastructure, with expanded bike lanes and innovative public bike-sharing programs, the future of cycling in one of the world’s most iconic cities looks brighter than ever. More from Momentum Mag Seattle Mother Anne Phyfe Palmer 88Bikes Foundation Check out these 5 all-road folding bikes Folding bike brand DAHON shows off new models at China Cycle trade show Mon dieu, France invests cool $2 billion to promote cycling Montreal’s car-free evolution continues with Mount Royal announcement A Guide To Biking In Santa Fe Cycle Portland’s Seasonal Tasting Tour: Breweries By Bike Leave a comment Save my name, email, and website in this browser for the next time I comment. Autumn Gear Guide Find inspiration in our Gear Guide that will keep you out on your bike through wind or rain. Download Now Signup to Weekly Newsletter Subscribe Now Editor’s Choice Cargo bike delivery is much faster than polluting vans and trucks with massive benefits January 28, 2024 Toronto Study Highlights Underreporting of Cyclist and… January 15, 2024 Yes, Paris has its own bicycle hearse January 5, 2024 How to Find a Bike for Short… October 2, 2023 Pedaling Art: Taliah Lempert’s Passion for Bicycles… September 27, 2023 How to ride your bicycle slower and… September 24, 2023 Previous Article Portland gets its bike-friendly swagger back with spike in cycling growth and new infrastructure Next Article Toronto mom inspired by her own kids launches Bold Helmets Recent News London just Quadrupled its Bicycle Network in Eight… Read More Features The Beauty of Upright Bikes: Sit Up and… Read More Travel On St. Patrick's Day Explore One of these… Read More Here is Why Governments Paying People to Bike… Read More Stay Connected! Subscribe to our free newsletter Sign Up Company About Contact Us Subscribe Connect Terms & Conditions Privacy-Policy ©2010-2015 All Rights Reserved Momentum Magazine Ltd. momentummag.com ShareThis Copy and Paste",
    "commentLink": "https://news.ycombinator.com/item?id=39744932",
    "commentBody": "Paris cycling numbers double in one year thanks to investment (momentummag.com)256 points by Timothee 19 hours agohidepastfavorite199 comments darkamaul 18 hours agoAs a daily biker in Paris (and for years), I think it is hard to grasp the changes Paris has seen in the last years. You now even get to see some congestion on bike lanes (Bd Sebastopol for instance), because they are too many cyclist on an older (and thus too narrow) bike lane. This change, the new subway lanes, and the reduction of car usage in the city make the city much more enjoyable on a day to day basis, especially for pedestrians as cyclists are less annoying than drivers. reply chopsuey5540 17 hours agoparentI've also been a daily cyclist for over 10 years in Paris and paradoxically, I feel less safe now when I'm riding on shared infrastructure (that is roads without a separated cycling lane). I attribute it to the fact that people driving, and especially people who HAVE to drive (taxis, deliveries, etc...) are facing insane levels of congestion and are lashing out on cyclists because they blame them for being stuck in traffic for hours every day. Now of course as a cyclist, I applaud all this new infrastructure but I'm wondering if there's a way to appease this growing atmosphere of violence (and I'm not exagerating, I've seen several fights break out between motorists and cyclists). Given the very limited space available in Paris centre, I don't really see an easy way unfortunately... Curious if that's also something you noticed / experienced ? reply tetris11 17 hours agorootparentI had the same reaction when bike lines were rolled out over London. The influx of slower and less inexperienced riders with the (correct) mentality of \"I have the right to cycle as slowly as I feel safe to\", vs the previous (incorrect, but accepted) mentality of \"keep up with traffic or pull over\" amongst riders, made my cycle commute less efficient. But it's better now. People learn, the city adjusts, attitudes change. reply noio 17 hours agorootparentprevI guess people aren’t always rational, but do those who blame the cyclists not realize how bad things would be if those cyclists were also in cars? reply watwut 16 hours agorootparentPreviously, some were in cars, but others in public transport. reply yakireev 17 hours agorootparentprev> Given the very limited space available in Paris centre, I don't really see an easy way unfortunately... Bicycles take less space than cars, both on road and when parked, don't they? A four-lane bicycle highway is as wide as one-lane car road. If anything, people switching from cars to bicycles should produce more free space for the city, not less. Am I missing something? reply touisteur 17 hours agorootparentMany streets have been transformed to one way or made cycle-only so some critical arteries are getting congested a lot. I feel the violence and anger too, you don't even need to go to Paris, the suburbs are becoming a mess of seething drivers. I get a lot more of very dangerous behaviour, insults from drivers on shared infrastructure as I did 5 years ago, even though I have my kids in a cargo and I cycle around the max assisted (25km/h) speed or higher if my legs work, most of the time. I see shouting matches at least once a week, angry drivers honking on streets they can't even pass a bicycle... And isolated infrastructure is not always possible... I feel this has become another part of some culture war, where I just don't have a license and drive my kids around in a bicycle (I don't want to drive a car) so I'm some angry green extremist out to annoy every driver out there... reply chopsuey5540 17 hours agorootparentprevIn Paris, most people who are now biking are people who would have taken public transportation before, so the amount of cars on the roads is roughly the same as before. reply alistairSH 17 hours agorootparentDo you have a citation for that? Is car ownership and usage for city residents really the same? reply 6bb32646d83d 16 hours agorootparentI don't live there anymore but I grew up in Paris - I knew absolutely nobody living in Paris driving to another location in Paris. It's always been metro first, bicycle sometime. Almost all of the passenger car you see in Paris are people driving from the suburb. reply adrianN 5 hours agorootparentThis is why this whole change was even politically possible. reply prmoustache 16 hours agorootparentprevMany cars in Paris are driven by people commuting to Paris from outside. There is a real fracture between suburbans (who don't vote for those changes happening in Paris) and city residents who votwd for them. reply getwiththeprog 8 hours agorootparentprevPossibly the dis-ingenuity of vocal posters? The continuing anti-rational love of the most expensive form of entranched transport? reply CalRobert 17 hours agoparentprevWe were trying to decide where to live and visiting Paris for Christmas in 2022 was a sheer joy. We ended up in the Netherlands but biking along Rue de Rivoli, the Seine, etc. were a delight for myself and my daughters (we were in a cargo bike) and we hope to return for many visits. I visited in 2008 and while it was a great city then I definitely wouldn't have been as quick to take kids along for a bike ride. reply jcul 2 hours agorootparentWere you able to rent a cargo bike in Paris? Or did you somehow bring one with you? reply Fradow 17 hours agoparentprevSame here. I'm pleasantly surprised when I try a new itinerary, and now it's mostly bike lanes. Even 5 years ago, it was 50/50 whether there was a proper separated bike lane you could use. I also remember 20 years ago, when a car was a normal way to move around in Paris. It hasn't been the case for several years for me now. Around 2019/2020, something important happened: the critical mass for bicycle infrastructure was crossed, and nearly overnight a lot of people started cycling (no doubt helped by 2019 strikes and 2020 covid lanes). Since then, bicycles are a common sight everywhere. reply bsaul 18 hours agoparentprevThe right bank is going to be really overcrowded this spring, for sure. Left bank have wider roads and less bikes, but right bank is insane. It's now up to the point that i rather wait for commute hours to be over before i take my bike to that side. reply k99x55 17 hours agoparentprevHow far outside central Paris does the cycle infra reach? Are the suburbs still very car-centric and/or hostile bicycles? reply getwiththeprog 8 hours agorootparentGood question, and I do not know. However, I suspect that looking at the OpenStreetMap of cycling, there are trunk cycling lines, which feed to lower car density backstreets - so certainly one can cycle the suburbs, but without the excellent ammenities of cycle-lanes on every street over 30km/h like in Holland or Belgium. reply lucsky 17 hours agoparentprev> cyclists are less annoying than drivers I had to laugh out loud at this, please define \"less annoying\". As a daily cyclist in Paris my very own statistics say that about 95% of them (yes, that's NINETY FIVE percent) are either color blind, suicidal or willfully ignore the most basic rule, aka red lights. And yes that include M12 signs which have arrows indicating directions in which running the red light is authorized, that is ignored as well. Breaking these 95% down into categories, the vast majority would be delivery guys - you can easily imagine their \"excuse\" - and (usually young) idiotic morons on bikeshares (Vélib, Dott, Lime, etc) who therefore do not own the bikes and therefore care even less. I'm especially angry at parents on cargo bikes who run red lights with kids with them, this is not only suicidal, it's borderline criminal. But, you know, they are saving the planet, or something. So I kinda disagree with the \"less annoying\", cyclists in Paris are the reason why everybody in Paris hates cyclists. reply Affric 11 hours agorootparentThis is not just Paris but drivers are the same on the inside. The in my experience difference is enforcement. A higher proportion of cyclists believe they will get away with breaking rules and so you see much more of it. Once it’s a norm to sneak around that corner or cross that road then other rule breaking follows. This can only be fixed with enforcement. It can be ameliorated with infrastructure and more permissive rules where appropriate but the only way to make cyclists less annoying is enforcement. EDIT: I say this as someone who very rarely drives and even then only outside of the city. reply k99x55 17 hours agoparentprevHow far outside central Paris does the cycle infra reach? Are the suburbs still very car-centric? reply Ylpertnodi 18 hours agoparentprevIs Paris still full of dog poop? reply Retric 18 hours agoparentprev> especially for pedestrians as cyclists are less annoying than drivers As an American pedestrian I find cyclists far worse on average than drivers. They far more often ignore traffic signs and I have seen multiple collisions including one that looked quite serious, though luckily no fatalities. On a per person basis my personal annoyance scale goes subway riders have least impact, followed by busses, normal pedestrians, cars, joggers, then cyclists at the top. Obviously cars are quite dangerous but they are more predictable. Bikes and joggers seem to spend a lot of time looking at traffic not who they are going to run into. Edit: To be clear I think this would improve as more average people start cycling. reply EraYaN 18 hours agorootparentThat no fatalities bit is the crucial part. And for quality of life the amount of noise is just much better for bikes, which if you live somewhere with roads and bicycle lanes, you'd love for all the cars to go away and just have bicycles. You just need separates infrastructure, as a pedestrian you should almost never have to share a space with bicycles (or cars for that matter). And if you are on a shared space any sane design will make all cars and bicycles wait and slow down. And at that point they can't be annoying cause you don't have to go out of the way for them ever, they can wait (and seethe I suppose). reply Retric 18 hours agorootparent> as a pedestrian you should almost never have to share a space with bicycles These collisions are mostly at intersections, which aren't avoidable on a flat surface you would need to go 3D. I'm in favor of raised sidewalks for pedestrians, but they are relatively pricey infrastructure. Edit: > That no fatalities bit is the crucial part. Globally I think the rates are quite a bit lower. But in the US it's closer than it might seem based on how many more drivers there are than cyclists. Cyclist only kill about 4 pedestrians per year in the US, but cyclists are also vastly less common especially on a per mile basis. https://www.nationalworld.com/news/politics/pedestrians-kill... To be clear I don't this says as much about bikes as it does the biased population of people currently using them. reply latortuga 18 hours agorootparentIf we are comparing against cars though this is a no brainer. Cars and trucks are extremely hard on infrastructure while people and bikes are basically free. Go 3D! reply cycomanic 14 hours agorootparentprevYour personal observation does not match statistics. Most statistics show that in the majority of bike car crashes the driver has been at fault. The second link which is for Berlin where cyclist have a very bad reputation it's 77% the drivers fault, in contrast for car pedestrian accidents it was 50/50. I also find these complaints interesting, considering that it's generally considered the norm to drive about 10% above the speed limit, and especially pedestrians are happy to cross traffic lights at red all the time. https://www.quadlockcase.com.au/blogs/news/8-key-facts-about... https://www.udv.de/resource/blob/79324/c290c92fc2f0712187c0d... (sorry German) reply CalRobert 17 hours agorootparentprevCars (drivers, really) are very, very dangerous. They are the leading cause of dead children. They are also one of the reasons kids don't play in the street. I find that far less annoying than cyclists. reply Retric 17 hours agorootparentDrivers are a lot more common than cyclists in the US. So yes cars kill ~7,000 pedestrians per year vs ~4 (2013-2020) for bikes, but normalize the numbers and bikes are also very dangerous in the US especially on a per mile basis. https://www.nationalworld.com/news/politics/pedestrians-kill... https://www.statista.com/chart/17194/pedestrian-fatalities-i... reply CalRobert 15 hours agorootparentyour first link is for the UK, I think? That seems like a separate issue in a place with very different urban design and habits. What's remarkable is that cars kill thousands of pedestrians _even after we've greatly reduced the frequency of walking_. If the pool were full of sharks then deaths by shark would quickly fall to 0 of course since nobody would swim anymore! reply Retric 12 hours agorootparentOps, can’t find national numbers for the US. NYC alone had at least 7 pedestrians killed by cyclists from 2011-2019 plus several more recent deaths not counted by this article. That works out to ~10x safer per cyclist and fairly close mile. (As a side note NYC cabs are driving 100k miles per year!) https://nypost.com/2019/08/31/nyc-bicyclists-are-killing-ped... reply jobs_throwaway 15 hours agorootparentprevSeems like much of the danger of cycling is due to being near cars though, while the reverse isn't true. Better infrastructure and less car subsidy is likely to make cycling a lot safer, whereas cars already have ~all the infrastructure and subsidy and are still enormously dangerous reply Retric 12 hours agorootparentThe danger for cyclists is drivers, the danger for pedestrians is both. reply jobs_throwaway 16 hours agorootparentprevLaughable opinion as a fellow American and New Yorker. A bike crash is an unfortunate, and potentially scary event. A car crash is often life threatening or fatal. You're just not weighing the cost of each type of incident properly reply rrrrrrrrrrrryan 18 hours agorootparentprevWay less noise though reply vanilla_nut 18 hours agorootparentI don't think London has made quite the strides in bikeability that Paris has since 2020, but when I visited Hackney last year I was astonished to see more bikes during rush hour than cars. Walking down a road to a coffee shop, I actually had to wait a few seconds to cross at the intersection between two bike highways. Hundreds of people riding bikes to work. And as quiet as the wilderness behind my house in the rural USA. I could hear the wind in the trees, and that was it. Maybe a little drivetrain noise from a poorly maintained bike here or there. I desperately want to live in a city that quiet. Back when I lived in NYC practically every environment was an assault on the ears. reply Mawr 3 hours agorootparentA treat for you: https://youtu.be/CTV-wwszGw8?t=289 :) reply ninininino 17 hours agorootparentprevAnd way less particulate pollution. Automobile drivers are actively hurting you everytime you breathe their exhaust fumes, tire wear dust, or brake pad dust. Of course the same can be said for bicycle tires or brakes on a far smaller scale, or probably even pedestrian plastic shoe soles. reply Retric 18 hours agorootparentprevSure, that's fair. reply lupusreal 17 hours agorootparentprevSeconding this. American cyclists are extremely rude to pedestrians. Whenever somebody brings this up, cyclists dismiss it by pointing out that cars kill more pedestrians than bikes. Well that's certainly true, but I've never had cars yell at me for walking on the sidewalk or over a crosswalk, while cyclists have hurled abuse and (deliberate) near misses at me more times than I can count. reply alistairSH 17 hours agorootparentAnd a driver assaulted me with his car last week (I was on a bike in a \"sharrow\" lane). Almost put me into the ditch, and when I didn't fall, he proceeded to brake-check me. I've had food thrown at me by motorists when I'm in dedicated bike lanes. I've been buzzed by trucks and buses while in bike lanes. I see cars regularly use the bike lane outside my house to make aggressive passes of slower auto traffic. Pedestrians have been killed crossing the streets near my house. All by cars, not bicycles. Cars regularly kill animals. And not just squirrels, but raptors and fox and other predators we don't want to lose. And on and on. We all have our anecdotes. The answer is dedicated infrastructure for a range of transportation. reply SamBam 17 hours agorootparentprev> but I've never had cars yell at me for walking ... over a crosswalk You don't live in Boston, I see. reply lupusreal 15 hours agorootparentTrue, I don't. I recognize that driver and pedestrian culture are very different indifferent cities. However a significant number of cyclists seem to hate pedestrians everywhere I've gone. To the other responder, I agree that cars are frequently assholes to cyclists, but how is that relevant to cyclists abusing pedestrians? reply h1fra 18 hours agoprevPeople not living in Paris can't even grasp how much the city has changed in just 6 years. Starting a bit before 2020 but massively accelerated by the covid situation (and public transport saturation). You could barely cycle safely before and now it's hard to not find a bike lane. Because of the fast pace, of course not everything is perfect but we have to celebrate this as much as we can. reply mattcantstop 18 hours agoparentThat's a problem I desperately hope the US has to encounter soon. Too much demand and we have to accelerate our plans to satisfy all those people on bikes! reply tremon 18 hours agorootparentToo much demand That's interesting because the article says the opposite, that supply (mostly) proceeds demand when it comes to mobility infrastructure. The fact that demand is showing so readily indicates there is much unrealized potential there. reply bombcar 17 hours agorootparentIt's called \"induced demand\" when people don't like it - but for almost all transportation you need the supply before the demand because there's not really any other way to go about it. You can try to work out where people are currently traveling and build on that, but it's such a feedback loop it may not work. One of the most common examples I know of is building a metro line to \"nowhere\" (e.g. a few stops past where it ends) - in 5-10 years that will be a bustling area because now it's connected. reply mupuff1234 18 hours agorootparentprevUS cities should probably first start focusing on pedestrians. reply al_borland 18 hours agorootparentWith the way many areas of the US are laid out (being car-centric), a bike bias may be needed. Bikes to get to where you want to go, then once you’re there, walk around. Currently, a lot of people need to drive to get to an area they would walk. Everything is very spread out. reply ThunderSizzle 17 hours agorootparentI can walk with my toddler and infant and post c-section wife. I can't bike ride with them. Of course, I can also drive almost anywhere with them. reply InitialLastName 15 hours agorootparentGreat! if most other people are walking and biking, that's less traffic on the road to slow you down and endanger those young children. reply al_borland 15 hours agorootparentprevThere are various solutions for biking with an infant, and from what I can tell, a c-section takes 6-8 weeks for recovery. No one ever side bikes should be the only possible form for transport. Use the car for 2 months while your wife recovers, for the trips she needs to take. If you live in a walkable area, go for a walk. A bike path is easier to drop in than rebuilding entire cites and relocating the entire population around those walkable areas. Bike paths take years, consolidating urban sprawl would take decades, even if the political will existed. reply getwiththeprog 8 hours agorootparentprevThe best way to help pedestrians is to start removing cars. The best way to do this is to install public transport and facilitate bike use. reply jahnu 18 hours agoparentprevI'm hoping Vienna will learn from this and accelerate things here a bit. reply littlestymaar 16 hours agoparentprevI concur: I used to ride a bike in Paris during the mid 2010 and I eventually left the city in 2017 in part because I was fed up with the difficulty of the endeavor. And now everytime I travel back there I'm amazed at how far it's gone already, and the number of cyclists keep rising every time! This year I started seeing a significant amount of cargo bikes, the things that I had only seen in the Nederland before. I don't know when the growth will stop, but it's already beyond what I could even imagine back then. reply vladms 19 hours agoprevWhile its true that compared to 10 years ago cycling is much better served by the infrastructure, there are still large challenges ahead, and I would mention only two of them: - many intersection are very poorly signaled if you are riding a bike (not sure for cars is much better) and old bike lanes setups put you in weird situations (ex: bike lane changing from \"outside\" of the road to \"inside\" of the road, without clear way to change from one to another) - there is quite few temporary parking space for personal bikes. Velib works great until there is a rush hour and you either can't find bikes or a parking spot. reply consp 18 hours agoparent> While its true that compared to 10 years ago cycling is much better served by the infrastructure, there are still large challenges ahead It takes decades (emphases on plural) in most places. Having seen it happen to my hometown from the start of the '90s until about halfway though the '10s as an example. And that was with a head start since in some places it was already done well. And that was in the often mentioned cyclist's Valhalla. reply throwup238 18 hours agorootparent> And that was in the often mentioned cyclists Valhalla. I’m confused by this analogy. Are cars Valkyries in this case, taking the souls of bikers off to Valhalla? reply consp 18 hours agorootparentProbably lost in translation, should have phrased differently. AValhalla is seen as an alternative phrasing toparadise but probably does not translate well into English and might be a specific cultural thing (and not even Nordic in use, since I'm talking about the Netherlands). It has a bit more \"promised land\" tone than paradise without the baggage of a large faith behind it. reply throwup238 18 hours agorootparentNo that part translated, I was just under the impression that one only got to Valhalla by dying in glorious battle. reply arethuza 17 hours agorootparentI commuted 15km each way for a few years through heavy traffic and had a few close calls - would they count as \"dying in glorious battle\". e.g. I was cycling home at night once when kids threw a brick at my head... reply kindatrue 18 hours agoprev\"I really hope US cities head in this direction\" The US is headed on the path of autonomous vehicles as the solution. It's the perfect combination of things our society loves: 1) No new public investment. 2) Continuous, end-to-end air conditioning reply HEmanZ 18 hours agoparentI suspect the US is going to get a super bifurcated urbanist result across cities in the next 20 years. Places like NYC, Boston, Chicago, Seattle, Portland, and San Francisco (maybe not these exact cities but you get the gist) are going to move strong towards European style urban infrastructure. It seems to be one topic that really does mobilize voters, especially younger educated voters, in these cities. It will in turn attract a lot more of these kinds of people, accelerating the change. The rest of the US is just going to double down on sprawl. reply lotsoweiners 17 hours agorootparentThose cities you mentioned already have European style urban infrastructure. In my opinion the big difference is that the drivers in the European cities seemed crazier than their US counterparts. reply HEmanZ 16 hours agorootparentThat’s interesting. I’m a born and raised American, but I lived in two different European countries (Oslo and Geneva) between 2014 and 2017. During that time I traveled all over Western Europe and the near east for fun and business, and besides 2020-21 I have vacationed in Europe once or twice per year when not living there. Old Soviet Europe drives pretty crazy, but other than that I have not encountered anywhere else in Europe with drivers as crazy as the US. They have very different norms in Europe that Americans might confuse as crazy but are actually better habits IMO. E.g drivers in Florence and Rome followed few rules and basically drive by the “if it fits, it fits” manta. But they were always keenly aware of pedestrians and other drivers. They drive very light cars in streets where they can’t get above 15 mph. Even if they do get into more fender-benders, I’m not very concerned about a fiat hitting my car at 15mph. In short, their way of driving is safer and I feel much more comfortable around it. Americans on the other hand seem to prefer deadly speeds in giant vehicles even in neighborhoods. My current neighborhood regularly sees F150s and King Ranches going 55+mph, on neighborhood streets. I saw a driver last year hit a dog they should have clearly seen and avoided because they were going about 45 mph and on their phone, they didn’t even slow down after they hit the dog. Americans regularly pretend pedestrians and obstacles don’t exist at all, and have way worse reaction times than what I saw living in Europe. Combined with the high speeds and heavy cars, driving is comfortable for the driver and terrifying for everyone else on and around the roads. I also wouldn’t compare these American cities infrastructure to most European cities of the late 2010s. Excepting possibly NYC, which is sort of its own league in the US. Maybe they are comparable to the European cities of the 90s, but things have moved on a lot since then. reply mbs159 3 hours agorootparentprev>Those cities you mentioned already have European style urban infrastructure. I hope you're joking. it's very rare to see a highway wider than 6 lanes in Europe, especially that runs through cities, whilst the aforementioned cities have a abundance of such monstrosities. reply twoWhlsGud 18 hours agoparentprevDone properly (I know…) autonomous vehicles could be a godsend for cycling in the US. If vehicles behaved properly you wouldn't theoretically need cycling infrastructure at all – the dream of vehicular cycling would actually start working and you could share the road anywhere. reply beppuboy 18 hours agorootparent+100...I love driving and will miss it with autonomous cars but I like being able to ride my bike without being nearly killed in the city every week. Autonomous cars (assuming they're trained to deal with bikes) would be a massive benefit I'm keeping my fingers crossed for. reply trgn 17 hours agorootparentprevthe aspirational future of AI-traffic is kei cars and vans puttering around. Hope to see it in my lifetime still. reply al_borland 18 hours agoparentprevAutonomous vehicles will make traffic worse, not better. Now we won’t even need people in cars to create traffic. reply NegativeLatency 16 hours agoparentprev3) A technical solution to a societal problem reply rangestransform 14 hours agoparentprevI would definitely prefer end to end AC over cycling in >30c heat reply doctor_phil 18 hours agoparentprevI don't get the American fixation of cars. Isn't public roads a mild form of communism? reply NegativeLatency 18 hours agorootparentLots of money has been successfully spent on convincing Americans that car ownership directly makes them more free. Car == freedom This is sorta true in part (in some places) because other options like walk/bike/bus/train have been so disinvested that in many places you really do need to own a car to get around with any level of dignity. It does seem to be changing on larger US cities though, so I’m hopeful. reply ThunderSizzle 17 hours agorootparentprevFreedom of movement. Cars represent freedom of movement akin to a horse with a trailer, except you have an engine instead of a horse. I dont think Americans care for the current road monopoly states have, or the monopolistic tolls and enforcement. The idea of freedom of movement isn't really a European-centric ideal - its more uniquely American and derived from being a nation derived from those escaping injustices of Europe and searching freedom and liberty. With the history of Europe being based in serfdom - peasants being forced to work their lord's land for protection - the idea of freedom of movement never really seemed to be of importance. Those who deemed it important probably emigrated from Europe to America. reply mbs159 3 hours agorootparentI disagree that it provides any freedom at all, since you are limited by roads. If certain roads are cut off by sabotage or other means, you lose your imaginary \"freedom of movement\". Regarding it being an \"American thing\", Germany was the first country to build highways, and currently the highways there (Autobahn) don't even have a speed limit, so you could say that they provide more freedom of movement than the roads in the US. reply milkytron 11 hours agorootparentprevI agree, but for those that truly value freedom, I would think that they would also value the freedom of choice. Car dependent societies don't allow many choices, you are restricted to only choosing a car for transportation. reply pandaman 6 hours agorootparent\"Freedom of choice\" is literally that, a freedom to make a choice, nobody is restricting you from it. Having many choices to make is not a freedom, it's power. reply mbs159 3 hours agorootparentCar infrastructure taking up unnecessary parts of cities limits the freedom of transportation by other means, like walking or cycling. reply carlosjobim 18 hours agorootparentprevImperialism rather. reply zahma 18 hours agoprevThe city started during the pandemic and has moved toward making these bike lanes more permanent. There is also a deep dissatisfaction with public transportation here. While comprehensive throughout the entirety of the Paris metropolitan, it is expensive, unreliable (especially the regional RER trains that connect the inner city to suburbs and vital for commuters), often filthy, and at times unsafe (depending on the line). The other issue is the traffic above ground. I've heard reports that there are fewer cars, but it doesn't feel this way. If anything, there are more delivery trucks. So while infrastructure investment has helped (not to mention some nice tax write-offs and discounts on e-bikes), there are some other issues here. I like bikeable Paris a lot, but the ecological goals of this city's mayor demand much more investment in the city and regional authority's mass transit. It is unreasonable to expect commuters to bike in from many kilometers away. Biking inside Paris upon arrival should be the desired end result. Ultimately to get the cars off the roads, people need to feel good on mass transit so that they want to use it more than a car, and the impression I have right now is that many people are biking because it's better than sweating in filthy, delayed trains. To that effect, riding a bike is and should be a delight -- not a mere alternative to defunct metro transit, but I know many who are terrorized or disgruntledby biking in Paris. To solve this intractable issue of getting more riders, cars have to be removed from the streets which need to be progressively closed to make room for more bike infrastructure. These kinds of plans well face strident criticism and backlash from the suburbanite commuters. In that vein, I am totally for the Grand Paris Express plan. Make Paris great for Parisians again! https://en.wikipedia.org/wiki/Grand_Paris_Express reply offsign 18 hours agoprevTitle is misleading... the source Le Monde article states that use of certain bicycle paths has doubled (or tripled) in certain places, as investment has gone into improving these paths. That's great, but kind of obvious that if you build out dedicated bike lanes, cyclists are more likely to prefer them to alternate routes. reply digging 18 hours agoparent> kind of obvious that if you build out dedicated bike lanes, cyclists are more likely to prefer them to alternate routes. That's not obvious at all; it's not even true. It's not uncommon in US cities to install long, wide bike lanes on major roads which see close to 0 daily users. Significant problems include: - complete lack of physical barriers between cars and bikes - bike lanes terminating at dangerous roads - density is still low and there are dangerous parking lots at every destination - bike lanes are exposed to direct sunlight in 100F+ - a non-trivial number of American drivers need extremely little push to intentionally hurt or kill bibcyclists reply ThunderSizzle 17 hours agorootparentWhile those are \"dedicated bike lanes\", it wasn't infrastructure built for bikes. Typically those are existing road safety shoulders converted to a bike lane. I dont count that as dedicated bike infrastructure. reply digging 17 hours agorootparentIf your definition of bicycle infrastructure excludes anything insufficient to facilitate increased usage, then yes we can agree it is obvious that building such will facilitate increased usage, but that's a useless statement. But if we talk about all bicycle infrastructure, which is a conversation useful to have, it is clear from the multiple issues I pointed out (not limited to lack of physical separation) that simply building bike infrastructure ad-hoc and without holistic change is not useful. reply ThunderSizzle 16 hours agorootparentAlmost all of your points are because what you are describing isn't bike infrastructure, it's a line of paint on a highway's shoulder that use to be a pull-over safety shoulder - that's why they terminate randomly, don't have any barriers, there's random/low density, and direct sunlight). It's literally a political line in the road to get federal money from the DOT, which is why it shouldn't be counted as bike infrastructure. It's a literal line on a state highway. Any real conversation about bike infrastructure would need to start with recognize a political line in the road is not real bike infrastructure any more than Amtrak using freight lines is a real passenger rail route. reply digging 15 hours agorootparentOne of those points relates directly to paint-only bike lanes. None of the others do. Any conversation about expanding bike infrastructure needs to acknowledge existing bad bike infrastructure and common bad techniques in order to explain why we can't expect results if we use them again. Otherwise, they'll just get used again and waste more money. If all you say is \"You literally have no bike infrastructure\" to a city that literally has spent money and effort creating (bad) bike infrastructure, I don't see how that's helpful. reply kfarr 18 hours agoparentprevEven though it seems obvious you will find the majority of Americans fighting against bike lanes because they think nobody will use it. Having data to show causation like this is genuinely helpful for other countries and cities to follow their lead. reply al_borland 18 hours agorootparentI think a lot of the push back in the US against bike lanes comes from bad bike lanes and a lack of a wholistic solution. In most places I’ve been, the city will paint a line on a road where cars are going 50mph and call it a bike lane. There is no chance that will get me to start riding a bike. All it does it make the road worse for cars, by making it more narrow, or more likely, losing a lane. One place I lived did get a protected bike lane going right in front of the building. I still didn’t use it, as it wasn’t really connected to anything else. Everywhere along the route I’d go, I’d simply walk. It wasn’t that far. Everything has to start somewhere, and I hope they build more, but so far drivers see problems without any payoff. The worst of it was during the pandemic. There were construction barrels all over the city. It was hell to get around by car. I figured they were preparing for construction and ripping up the road. I found out a year later that the barrels were meant to create temporary protected bike lanes so people could get out and ride around to places during the pandemic. Cool… if there has been a single sign to tell people that’s what it was. Instead, it just made drivers mad, and the lanes weren’t used, because people didn’t know what they were for. More space taken from cars with no payoff in terms of reducing traffic through increased biking. I want good bike infrastructure, but the plans and efforts I keep seeing still don’t seem that good. The useful paths are dangerous and the safe paths aren’t that useful. reply krowek 18 hours agoparentprev> That's great, but kind of obvious that if you build out dedicated bike lanes, cyclists are more likely to prefer them to alternate routes. Not really, here in Poland there are new bike lanes, but they go far from the city, so if you need to commute you end up going around the city to finish in a bottleneck when you are approaching the center. So, want it or not, you end up using the alternate routes. reply agumonkey 18 hours agoparentprevBased on some videos, some paths usage has way more than doubled. It's getting super messy.. a sad side effect of popularity. reply bsaul 18 hours agoparentprevnot sure about the title, but i guarantee that usage has exploded. It's really obvious (and starts to become a problem in certain areas). reply adrianN 19 hours agoprevGiven safe infrastructure cycling is one of the fastest most reliable forms of transportation in cities. No wonder more people ride if you improve the experience. reply milkytron 11 hours agoparentIt truly is. I moved to a city across my country that has pretty good bike infrastructure. I had never even considered riding a bike for transportation, but after experiencing somewhat good infrastructure that got me to my office faster than driving, I was blown away by how enjoyable, sustainable, and economical it can be. reply timy2shoes 19 hours agoprevMy wife and I visited Paris last summer and biked everywhere. It felt very safe, not as safe as Copenhagen but still safe. I hope that with more time, as more drivers get used to bikes on the road it will get safer and more people will bike, creating a positive feedback loop. reply jauntywundrkind 19 hours agoparentI'm a strong believer in road culture change happening. Biking in DC changed enormously when our bike share program showed up in 2008. Seeing casual chill people on slow bikes, often a little lost or uneasy, forces a bit of a reset from the classic fast cars vs spandex wheel-stander bikes dynamic. reply ckemere 19 hours agoprevHaving just moved from Houston to Manhattan , I have been really impressed by the number of bikes I see. One factor, which this article doesn’t seem to discuss, is the e-bike. In Paris, is it mostly classical bikes, or are the electronics becoming widespread there too? reply beej71 18 hours agoparentOn that topic, I ride a regular bicycle but I love that there are so many e-bikes on the roads these days. The city where I live is blessed with pretty good weather for a lot of the year and e-bikes enable people who live out towards the edge of town to get downtown with relative ease. Even though they lack some of the benefits of a simple bicycle, they drive the same infrastructure development that I need. reply plants 18 hours agoparentprevI imagine that’s just about the starkest within-US move you could make with regard to bike friendliness. I once made the mistake of visiting Houston without a rental car, and it was bleak. Welcome to NYC! reply EricAski 18 hours agoparentprevI would say there is a 80/20 ratio between mechanical and e-bikes. The regional council offers ebikes for 40€/month[1] which boosted ebike adoption. The service is very popular with both office workers and food delivery workers. [1]: https://www.iledefrance-mobilites.fr/en/the-network/mobility... reply wnolens 18 hours agoparentprevI bike in Manhattan and it's a dream. A literal grid of 20-30mph traffic, with a greenway up and down in case you want to avoid traffic entirely. reply gniv 18 hours agoparentprevI'm in a Paris suburb. Lots of e-bikes and lots of electric scooters. reply himinlomax 18 hours agoparentprevNote that ebikes in the EU are limited to 25 km/h and 250 W. Thus there is little difference in practice with regular bikes ... except when climbing hills. reply Ylpertnodi 17 hours agorootparentMax 25kmh is stupid. 5km slower than the traffic I'm trying to keep up with, so being constantly close-shaved is an e-bike pain. reply Symbiote 14 hours agorootparentIt's the perfect speed. It's around the upper speed of cycling somewhere like Copenhagen, so the people on e-bikes by too annoying for the people on bicycles. reply davidw 19 hours agoprevUsing 'induced demand' for good - I love it. People are under the impression that things \"are just the way they are\" in the US, but it is very much a political choice to not dedicate more funding to keeping people on bicycles safe from automobiles. reply throw0101d 18 hours agoparent> Using 'induced demand' for good - I love it. Oh The Urbanity had a good video on this, \"What People Get Wrong About Induced Demand\": > In this video we explain why induced demand does apply to transit, walking, and cycling infrastructure, but with different consequences. * https://www.youtube.com/watch?v=8wlld3Z9wRc Transit, for example, can get better with more ridership: more train capacity is added to a line, and more frequent service may occur for better throughput. Both actions would improve service for each individual rider, so while while one may not want it \"crowded\", having it busy is good. While with roads, an individual driver may want enough traffic so that a street exists, any traffic beyond that would probably only slow you down. There are also externalities: more walking, cycling, transit reduces pollution and can improve individual health and the collective health of the population. Driving does the opposite. reply giraffe_lady 19 hours agoparentprevI'm sure some people do but I know on HN specifically in most threads around american urban cycling you will see some extremely clear-eyed car advocates. They (rightly) recognize that better prioritizing cycling requires, to some extent, fewer resources allocated to car use and oppose it on those grounds. Some of it is ignorance but a lot of it is not, and a continuing car-centric world is just their well understood and sincerely held value. reply dublinben 18 hours agorootparentEven as a driver, if you think about the second-order effects of getting more (other) people out of their cars, it becomes obvious that it would improve traffic for you. An enlightened self-interested driver would want to increase investment in alternatives to driving, since that is the only proven solution for congestion. As a video: https://www.youtube.com/watch?v=d8RRE2rDw4k reply milkytron 11 hours agorootparentGetting people out of cars also reduces tax spend on road costs. Bike infrastructure lasts longer and is cheaper to maintain than a road, which can result in improved municipal finances. reply gambiting 18 hours agorootparentprevThe thing that is somewhat counterintuitive is that countries with amazing cycling infrastructure are also very good for drivers because you just get less congestion on the roads. There are some great youtube videos about it - how driving across Amsterdam takes less time than covering 3 miles in LA, that kind of thing. Like, if you like driving cars, a place like the Netherlands is great for it because you spend more time actually driving and not standing in traffic(because most people are out on bikes and not in cars). reply pchristensen 18 hours agorootparentprevIn the later chapters of The Power Broker, there are stories of transit advocates and driving advocates begging Robert Moses to preserve right of way for transit when building the Van Wyck Expressway and the Long Island Expressway. The expected traffic demand (~9k cars/hr) the day they opened was far beyond the capacity (1500 cars/hour/lane * 3 lanes). A transit line takes about the same space as one traffic lane, with a capacity of 30-40k/hour. Without provision for transit, the road wouldn't even be a good road for drivers, because everyone with no choice but to drive would overcrowd the road and create the congestion the road was supposed to alleviate. This is setting aside all other considerations - urban design, air pollution, land and property value, tax revenue, health, neighborhood cohesion, etc. Roads without provision for transit and non-motorized travel don't even do the job of a road well. reply bryanlarsen 18 hours agorootparentprevAlmost every additional cyclist is one less car. And since bicycle infrastructure is a lot cheaper than vehicle infrastructure, spending a dollar on bicycle infrastructure can improve conditions for car drivers more than spending that dollar on car infrastructure. reply the_snooze 18 hours agorootparentWhen I bike and have to wait at a busy crossing alongside other cyclists and cars nearby, I take note of how vastly different the two situations are. Each car (often an SUV or a pickup truck) typically only has one passenger. Each bike also typically only has one passenger. But in the space of one car at a stoplight, you can fit 3 to 5 cyclists. reply IllIlIIllI 18 hours agorootparentprevIt's a real \"you hate traffic without realizing that you _are_ traffic\" situation. reply frozencell 2 hours agoprevThat’s written by gpt (bustling, at the heart of, …) reply ryandrake 18 hours agoprevI don’t get how you can make bicycling your only form of transportation, even if the bike travel infrastructure and parking is adequate. When I bicycle, even for a short amount of time like 15 minutes, I sweat, and end up a stinky wet mess at my destination. Especially in the summer. My shoes and pants get dirty from the road and my hair gets wet and messed up especially if it’s raining. So if I’m going anywhere with even a moderate expectation of personal hygiene, I need to 1. Make sure there is a shower at my destination, 2. Haul a change of clothes with me (necessitating a backpack or bag), 3. Hang on to my dirty sweaty clothes in that bag the whole time. Fine if I’m commuting to work where they have a shower and storage. What about going to a restaurant or business meeting or a museum or an appointment with a professional? I honestly don’t know but there are so many cities where people bike everywhere so there must be a solution. EDIT: ok, ok, I guess e-bikes it is. Still there are cities like Copenhagen and Amsterdam where everyone bikes everywhere and they don’t all have e-bikes. Do they just bring multiple changes of clothes everywhere? reply posix_monad 18 hours agoparentA large part of that is cycling on roads with automobile traffic. Dodging cars, taking gaps, sprinting at lights, constantly looking around... are all things that make you sweaty. Plus cars kick up a load of road grime and water (when it rains). Comparing cycling on pleasant infrastructure to cycling on roads is a bit like comparing walking and running. They're similar activities, but the level of exertion is quite different! In NL, they cycle with umbrellas. In Tokyo, rain ponchos are a common choice. There's also this mentality in the anglo-sphere of commuting on a racing bike with a backpack. This choice is somewhat driven by the infrastructure, since you need to be fast and agile around traffic. However, in countries with developed cycle infrastructure, people tend to use upright bikes with baskets, racks and fenders. This makes one much less sweaty when riding! reply ryandrake 17 hours agorootparentFor sure. I guess I’ve never bicycled anywhere where I wasn’t in constant fear for my life with cars whizzing past me at 45-55 miles per hour. The anxiety alone makes ya sweat! reply IllIlIIllI 18 hours agoparentprevThe answer is probably just a lot (most?) people don't sweat as much as you do. It's a solution that works for many but not all. There's also a huge uptake for ebikes, which remove a lot of the exertion component. As an aside, when I started cycling I sweat a lot more than I do now. It is a mild form of exercise, after all. Once you get used to it, cycling at a leisurely pace takes less energy than walking. reply c0nfused 18 hours agoparentprevI think the key here is mostly to go slow. Right now,if I'm bike commuting I need to keep up with cars or risk getting rear ended or tailgated or shouted at. Setting things up so you don't have to worry about cars while on your bike means you can bike at a more reasonable speed instead of 25mph. Think beach cruiser bikes not guys in spandex on race bikes reply wongarsu 18 hours agorootparentAlso infrequent cyclists are often overdressed. You don't need to get out the spandex, but if the weather permits dress so that you are slightly cold while standing still. That gives you the thermal budget to heat up from exercise without instantly sweating. reply _whiteCaps_ 16 hours agorootparentBe bold, start cold! reply random_kris 18 hours agoparentprevAfter biking for some months a 15min ride shouldn't get you too sweaty reply johnmaguire 18 hours agorootparentYeah, I disagree. During the pandemic I did a year of heavy cycling, including a 525-mile trip over 7 days. While cycling got a lot easier, I didn't stop sweating. My family is full of heavy sweaters, and I live in states where it gets into the 80s-90s in the summer. My girlfriend however seems to never sweat, even when pushing herself. Admittedly, I struggle to maintain a \"casual speed.\" reply Tade0 18 hours agorootparentprevWhy should that be the case? The amount of sweat is only correlated to the amount of heat generated. Only way to counter sweating is to dress appropriately and/or ride slower. reply l1tany11 18 hours agorootparentCycling economy increases naturally with increased fitness. As with virtually all forms of fitness training. It along with many other factors is why someone can quadruple their possible power output with training. All without melting. Look up cycling economy. Also how much you sweat is a trained response. This is why athletes do heat training before hot events. There is more to it than some oversimplified physics based equation. It’s a biological system. All these things are much more significant when going from someone who basically never cycles or exercises, to someone who does. It is less significant in pros, so keep that in mind if you are looking at studies of “trained cyclists”. Law of diminishing returns. reply saberience 17 hours agorootparentSweating is absolutely not a “trained” response. I’ve always been an athlete from when I was a competitive athlete in my college days (cross-country running) but I’ve always sweated a lot more than average and it’s not correlated with my fitness. I’ve always been slim and fit and I’ve often noticed that I sweat a lot more than people whom I’m beating in races! That is, I can be fitter and faster than someone and still sweat more. On the flip side, I am extremely cold resistant and when others are chilly and need to wear a sweater or coat, I don’t need it. My body just seems to run hotter than others, for better or worse. reply l1tany11 14 hours agorootparentIt absolutely is. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9460081/ Adaptations include decreases in HR, internal body temperature, skin temperature, rating of perceived exertion (RPE), and sweat sodium and chloride concentrations, as well as increases in plasma volume and sweat rate. Sweat rate. Sweat rate is a training adaptation. Thus we would say sweating is a trained response. Yes, I have no doubt you are very sweaty. That doesn’t mean that sweatiness isn’t a trained response though. reply Tade0 18 hours agorootparentprev> All these things are much more significant when going from someone who basically never cycles or exercises, to someone who does. You could say that I do that every season(well, I walk all year round at least) and my observation is that there's maybe a short period of increased unnecessary movements that I do which produce more sweat, but after two weeks, when I relearn the right movements, it plateaus and sweating is just proportional to energy used - it's lower, but still there. In any case I noticed that I would need to go frustratingly slow to be appropriately fresh for an office. Even at my leisurely pace of 15km/h I change my shirt when I get back from a ride, because it's simply uncomfortable otherwise. reply mikestew 18 hours agorootparentprevThe amount of sweat is only correlated to the amount of heat generated. I’m no expert, but that’s demonstrably not true. Evidence being two fit cyclists riding the same route, and one is visibly more sweaty than the other. reply Shrezzing 18 hours agorootparentprevAfter a few months of regular training, your fitness will invariably improve, even if that \"training\" is just cycling to/from work each day. The higher your fitness level, the less heat you generate for the same physical output. Lower heat generation requires lower heat dissipation, which requires less sweating. reply bsaul 18 hours agorootparentprevPeople that exercise regularly sweat less for the same amount of effort than when they weren't exercising. I have no idea what the biological / thermodynamics explanation is, but it's a quite basic fact. reply saberience 17 hours agorootparentprevYeah this is absolutely not accurate, some people just sweat more and it’s not correlated with fitness. I’ve noticed that I sweat a “lot” more than people who are much less fit than I am. reply itishappy 18 hours agorootparentprevAirflow drastically alters cooling. reply seer 18 hours agoparentprevBest solution is of course “shower at the office”. I worked for a company about 1h cycling away, and Sofia, Bulgaria is a hilly city, so cycling would involve quite a lot of exertion. After some experimenting I found that my condition would be acceptable if I just change my t-shirt when I arrive. My role was just your average dev so grooming requirements are a bit less than customer facing roles. Also its all about temp management. I’d ware clothes that can be easily removed mid-journey so I can manage my temperature. Airy exercise t-shirts can cool you so well that if you don’t strain yourself too much you can avoid sweating altogether. reply triceratops 18 hours agorootparent> cycling would involve quite a lot of excretion. I think you meant \"exertion\". I did giggle like a child though (very sorry). reply seer 17 hours agorootparentthanks! reply romafirst3 18 hours agoparentprevSo this is a real problem especially in hot climates like the US can be. My advice would be slow down your cycling speed to your appointment. I found myself having to keep reminding myself to slow down because my natural cycling speed generates a sheen of sweat. Of course you can open up on the way home. This strategy along with suitable clothes will help for moderate climates. If you are heading out into 95 and up weather there will be sweat though. I’ve also had the benefit of a shower at work which really changed the game, but that is a luxury. reply NegativeLatency 18 hours agoparentprev> shoes and pants get dirty from the road Most bikes sold in the US are historically not sold or marketed for transportation, they’re aimed at fitness/sport users. So they don’t come with fenders that would keep your pants clean, and racks you could put your bag on. To keep from getting sweaty you’ll need to intentionally ride slower, again US bikes are not designed for this. The classic Dutch bike has a much more relaxed and comfortable posture that helps remind you not to work too hard. reply patall 18 hours agoparentprev1. get fit 2. get an ebike 3. don't live that far away from everything (I do everything by bike, but all daily things are max. 15 min away 4. don't live in Mexico 5. don't race / take your time. Average (non-sporty) cyclist goes 15 kph, i.e your do not get more exhausted as you would be when walking 6. for me: do not wear a backpack (i.e bike bag or basket) 7. have proper equipment (i.e I have a poncho. I get a little wet in the rain though because it does get warm underneath) reply sloowm 18 hours agoparentprevI think there are a lot of suggestions already given like lower the pace, ebikes, bring a fresh shirt, there are rain clothes you can put over your clothes. But it's also about your destination and what is expected. Most of my destinations are people I like to meet or places I like to be at. A fancy restaurant or business meeting doesn't happen that often. Have you seen people at museums ;) The Netherlands and Denmark are generally not that hot. I also think Dutch working culture has been influenced by people cycling to work. Clothes that fit the weather are more accepted. There might also be some bias to what kind of work you do opposed to what type of work is actually happening. A lot of people do work that will make you sweat more over the day than a short bike ride. reply FinnKuhn 18 hours agoparentprevFor most day-to-day things you don't need to cycle longer than 15 minutes in most bike friendly cities. In addition to recommendation by others I would also add that for journeys significantly longer than 15 Minutes you can take public transport for part of it. reply ryandrake 18 hours agorootparentInteresting! I picked 15 minutes because that’s the shortest possible distance I could imagine having to go to do anything (yes I’m in the USA). Once in my life, very briefly, I managed to afford to live a 15 minute bike ride from my work, and I did it, but (like I said) with the sweaty consequences. Usually it’s 1-2 hours by car. reply bombcar 17 hours agorootparentI realized part of the problem recently; if you can afford to live in a 15 minute bike city, you're also living in a 5 minute car city. (Or more precisely, if something is a 10 minute walk away, it's about a minute or two by car.) Until you go from one car to zero, the car is a damn good temptation. I just did a round trip to pick up something from the library, a mile away. Could have walked, but couldn't have walked and got back in time before my meeting. reply handity 18 hours agoparentprevIf you ride at a reasonable pace and shift gears frequently to adapt to the terrain, you should never need to exert yourself so much that you'd break out in sweat. A gentle bike ride should feel pretty similar to a walk. reply Tade0 18 hours agoparentprev> Do they just bring multiple changes of clothes everywhere? Worse: from an early age they dress considerably lighter than in other countries. A friend of mine lived there for the first couple years of her life so she, ahem, adopted the tradition and now carries it over to the next generation with her two sons, who are essentially immune to cold already as by local standards they were always missing a layer or two. Also 30°C in Amsterdam amounts to a massive heat wave, so there's little opportunity to sweat if you dress light enough. reply unculture 18 hours agoparentprevIt's quite cold a lot of the time in Northern Europe, and people who are cycling to commute there often really aren't going very fast so won't sweat much. You can also get \"moisture wicking\" clothes, that might help with a certain amount of sweat. There is quite a lot of e-bike use in Amsterdam and Copenhagen. Google \"bakfiets\" - it's a whole \"suburban parents with a family size e-bike\" cliche (but a good one!). reply battery_glasses 10 hours agoparentprevEbike is a good solution. I ride 20 minutes to work and never show up sweaty. The breeze works better than AC to keep me cool even on 85F+ days. I invested in a good rain jacked with helmet compatible hood and a pair of rain pants and I am able to ride in all but the heaviest of PNW rains. I do end up nearly always carrying a pannier with me wherever I bike (usually to hold the extra layers I need while biking) but that has turned out to rarely ever be an issue. I have been able to replace ~80% of my car trips the bike, it could be 99%+ if I were a little less lazy. reply cjf101 18 hours agoparentprevRe: Copenhagen + Amsterdam. These cities are relatively flat, relatively cool (watch videos of cycling in Copenhagen and you'll see a lot of people in coats and hats), and you don't have to bike very far to get somewhere interesting. There's also the fact that there's a lot of bikes on the paths (which, for me, caused me to slow down considerably from my typical bike pace when I visited). This all combines to mean you aren't putting anymore effort in than a short walk: you just end up going a little farther in the same amount of time. Re: the rest. When I was a regular cycle commuter (in a Canadian city) I did pack a change of clothes, spare deodorant and/or wore a removable outer layer that resisted road gunk. Road gunk is _much worse_ on any route you share with cars. Dedicated bike paths tend to be quite clean. Paniers are far better than a backpack, since they sit on the bike frame, you barely notice the weight. But as other commenters noted: a little fitness goes a long way. After a year of commuting, my regular route wouldn't even cause me to break a sweat. reply extr 18 hours agoparentprevE-bikes solve this, they're already basically mandatory for cities with a lot of grade. Personally I run 100% of my errands on a jack rabbit e-bike [0] which isn't even really a bike (no pedals) but a sit down scooter. [0] https://jackrabbit.bike/ reply adverbly 18 hours agoparentprev> What about going to a restaurant or business meeting or a museum or an appointment with a professional? Depends on the restaurant obviously. Client meetings and appointments obviously depends on the location. If they're coming to your office then you would probably have already showered if needed. If you have the kind of job where you need to go to other client locations to meet with them then obviously you want a car because you're going to be driving a lot across clients. I imagine such people have cars in Amsterdam as well. For museums, nobody gives a damn. I've never dressed up for a museum in my life. reply Symbiote 14 hours agorootparentIn Copenhagen and Amsterdam, the people meeting clients probably choose their transport based on each day's needs. Several clients in the city centre? Bike, or even on foot. Spread around the suburbs? Car, or maybe bike+train if the offices are near stations. (More likely for office jobs, less likely if your clients are manufacturing/industry.) It is rare for people visiting us to ask for a parking permit. (Drinking alcohol with the clients in the evening? Then it can't be car.) reply sod 18 hours agoparentprevCycling 15 km/h requires just as much power as walking. reply glennpratt 17 hours agoparentprevI pack some backup clothes (undershirt, underwear, socks), they mostly stay in my bike bag because I rarely use them. If I do need them, I just go to the bathroom, then change back into the bike clothes for the ride home. If it's really hot, I might switch it up and put the office clothes in the bag from the start - meaning I know I'll need a change. Really not that complicated. reply teemur 18 hours agoparentprevWon't help on dirt and rain (and limited help on upwind and uphill) but using a low enough gear you should be able to stroll around with similar strain as walking. And you need to be able to accept a slightly slower speed, which is often challening, at least for me. reply lm28469 18 hours agoparentprev> I honestly don’t know but there are so many cities where people bike everywhere so there must be a solution. It depends how tolerant you are, personally I consider a little sweat to be a fairly small thing in regard to the other hardships life will throw at you, and a very small price to pay to save thousands of euros of a lot of stress. There is no magic bullet but make sure you eat clean and stay hydrated it helps with sweat smell, dress for the ride and not for the destination, a well ventilated outfit does wonder. I personally don't sweat much so it's ok, my gf gets more sweaty and it can definitely be annoying reply piva00 18 hours agoparentprev> EDIT: ok, ok, I guess e-bikes it is. Still there are cities like Copenhagen and Amsterdam where everyone bikes everywhere and they don’t all have e-bikes. Do they just bring multiple changes of clothes everywhere? These cities are almost 100% flat, it doesn't take much effort to bike in a leisure pace anywhere, you won't be sweating if you are riding at 12-15km/h. You don't need changes of clothes to bike as people do there. I bike in Stockholm, in a longer route these days since I live in a house some 15km away from the city center, it is quite hilly and I ride a fixed gear bike so I do sweat, I do not carry multiple changes of clothes, I have my clothes for riding depending on the season (staples are a wind jacket, wool underlayers due to their good wicking for sweat), carry a small towel with me, clothes I'd like to wear when I arrive at my destination, and wet wipes. If I'm too sweaty I will wait a few minutes until I cool down, use the towel to dry the sweat and the wet wipes in case it's needed. It takes almost no effort after you are used to the routine, like anything you just adapt to it and becomes the new normal. The resistance you feel is just the fear of the unknown, it's absolutely doable even in places not as perfect for biking like Denmark, the Netherlands, Berlin, etc., in my case I just love biking and the added routine I might need after biking 15km is totally fine. Others will use an e-bike around here if total practicality is needed. reply calcifer 18 hours agoparentprevThat sounds like a fitness issue. Millions of people cycle everywhere in Netherlands, in regular clothes, without needing showers in between. reply onlyrealcuzzo 18 hours agorootparentOkay - but Netherlands is not as sunny, hot, and swampy as Houston and Orlando in the Summer. I've never biked anywhere that's super hot and humid and sunny like Orlando in the Summer. Theoretically, the wind from biking should help evaporate your sweat before you get \"sweaty\" - but it's got to be easier to get swamp ass in Houston than Amsterdam. I think the climate would matter somewhat. reply extr 18 hours agorootparentprevIt's not a fitness issue. The netherlands is fairly flat and has a cooler climate than any major US city. It's just not possible to be pedaling up/down hills in any amount of heat/humidity and not get sweaty. reply patall 18 hours agorootparentThat is an excuse. There are many people in hilly cities that cycle. Not as many as in the Netherlands or in Denmark, but they exist. And even Norther Europe gets days of 30 °C (~90F), and we do not stop cycling, its rather the other way around. So just start cycling in winter, if you want to try. reply extr 18 hours agorootparentNorthern European climates are not comparable to even mid-atlantic US cities. Take a look at a comparison between Amsterdam and Washington DC. https://weatherspark.com/compare/y/51381~20957/Comparison-of.... Pay special attention to the \"Chance of Muggy Conditions\" chart. reply patall 17 hours agorootparentAs I said, just cycle in winter. Your chart says that it does not get muggy from November to April, i.e half of the year. And yes, I do not mind the cold, and do not need it flat, I live in Stockholm and not in Amsterdam. reply extr 17 hours agorootparentI think you should come live for a year in DC and see what you think about the winters. Notice that the comparison also shows it snows quite a bit more, ~~and is on average colder during core winter months~~! Weather is just more of a \"thing\" for the average US city. And when you start by subtracting half the year for heat, at least a month for snow, more days for harsh freezing conditions (not uncommon to have some weeks in winter that dip to -10C) etc etc... (edit: forgot you lived in stockholm where it's quite cold, I admit) The point is not that cycling is impossible but that it's attractiveness is highly variable depending on local/daily conditions, since you are exposed to the elements. Less consistency in conditions means the entire form of transportatin is percieved as unreliable. reply the_snooze 14 hours agorootparentI commute by bike in my US city, and the weather concerns are alleviated 90% of the time just by having the right clothing. If you have gym clothes, that's what you wear during hot/warm weather. If you have cold weather hiking clothes, that's what you wear during cold/cool weather. Pack a small towel and a change of clothes for your destination, and you're all set. It's actually easier to do this during the warm months because you wear fewer layers and the clothes aren't as bulky. I use a rack-mounted bag more often in winter to carry those extra layers. The remaining 10% of the time is for just straight up unpleasant weather that catches you by surprise. For example, if I bike to work in the morning and there's a huge rainstorm when I would bike home in the afternoon. When that happens, I just leave my bike at the office and catch a bus home. Or Lyft if I'm feeling especially impatient. reply patall 12 hours agorootparentprevYes, of course cycling is most popular in the Netherlands because geography and climate are most ideal there. But the same is true about infrastructure. If it exists, any form of transportation becomes widely more popular. Just think of Oulu :) https://www.bbc.com/future/article/20231220-why-oulu-finland... reply watwut 15 hours agorootparentprevIt is as possible as walking in the same place, unless we talk about some kind of super steep hill where would just push the bike. Very likely issue here is that people who are used to cycle for sport only are not used to bike for transport. They don't do equivalent of walk, they do equivalent of run. So they get sweaty, because they race. reply ekiwi 18 hours agoparentprevOne solution is e-bikes which are getting better every year and will allow you to go quite far without breaking a sweat. reply antisthenes 18 hours agoparentprevAn e-bike changes this math considerably. You get: 1. Faster trip, so less time to sweat 2. Faster speed, so more cooling, so less sweating overall. 3. Less exertion, so less generated heat by body. Although if you live in a hot AND humid climate, I'm afraid there's just not much you can do. There are physical limits after all. reply felixge 18 hours agoparentprevI think e-bikes are a good solution for this problem. reply 11235813213455 18 hours agoparentprevsweat is the least of concerns, and by biking regularly you get quickly fit and don't sweat too much reply javiramos 18 hours agoparentprevCould an eBike help? reply taminka 18 hours agoparentprevskill issue i’m afraid, when you cycle a lot a 15min bike ride is like a 3min walk, and most of the time if the temp is high you’ll sweat regardless in that case, get an electric bike :) reply carlosjobim 18 hours agoparentprevThey're living in places where the weather is cool, so they're not getting sweaty on their bikes. reply TacticalCoder 18 hours agoparentprev> What about going to a restaurant or business meeting or a museum or an appointment with a professional? I also like to find my bicycle where I left it. Which is not an option in many cities. > I honestly don’t know but there are so many cities where people bike everywhere so there must be a solution People have less and less money to spend on a car and fuel/electricity. Many are using bicycles not by choice but because they're broke. And it shows when you see the kind of bicycle they ride. My car is an extension of my house and although I'll take public transports once in a while (public transports are free in my country), there's no way I'll bicycle everywhere. Add to that that I do actually bicycle with a MTB, from point A to point A, with a 5-digit $$$ bicycle and there's no way I could possibly enjoy riding a shittier bike. And due to the insecurity (thieving scums), there's no way I can leave my bicycle anywhere in the city (gone in 60 seconds would be an understatement). Same for my neighbor: he'll do 50 kilometers+ a day on his bicycle, but from point A to point A, with a roadbike. Don't get me started as to when I was living in a rural area, with the closest highway being a 45 minutes (car) drive: I'd rant about how practical bicycles are there. I just don't get it and I don't think I can get along with people who wants to put us all on bicycles and I don't think people who want to get us all on bicycles could get along with me. And I'm fine with that. reply diggan 18 hours agorootparent> Many are using bicycles not by choice but because they're broke. And it shows when you see the kind of bicycle they ride. I'd be careful to assume people's social/financial status based on the bicycle they ride. Where I'm from, you get the cheapest bike possible not because you cannot afford something else, but because eventually it'll get vandalized or stolen, so if you only spent 20 EUR on the bike, it won't hurt as much to replace it with another 20 EUR bike. reply patall 18 hours agorootparentprevI do have a quite nice (and growing) amount of personal assets, yet I bike everywhere on my now 17 year old bike. My grandpa still has his bike of 60 years (and grandmas of 55). Of course will a $10k bike more likely be stolen over a $100 bike. Though proper bike infrastructure also lessens that risk quite a bit. Or where you personally live is an international crime hotspot. reply NegativeLatency 18 hours agorootparentprevNobody’s going to make you bike. What people want is the option to choose, which is not possible for safety reasons in many cities. Personally I’d rather spend my money on more interesting things than car ownership and maintenance. reply seszett 17 hours agorootparentprev> Many are using bicycles not by choice but because they're broke. And it shows when you see the kind of bicycle they ride. Maybe in your place. In my place (a big city in Belgium) people just use whatever bike they have that works and fix it up from time to time, but as long as it rides few people care how it looks. Especially since less shiny bikes don't attract thieves as much (although this isn't a big problem here). I use my bike because driving in traffic is longer and the trip to school with the kids and back home would take 20-30 minutes depending on traffic, while it takes almost exactly 15 minutes by bike (with the kids in a trailer). I use one of two bikes depending on where I parked my car (which I use once or twice a week to go to places with bad public transit. Otherwise it's parked somewhere within a 10 minutes walk radius, wherever I found space) and one of them looks like I fished it out of the river, the paint is gone in many places, but it's lighter. The other looks nice but it's quite old and heavy steel so I tend to use it less. I also use my bike (the heavy one, I figure if I want to do exercise, the whole point is to do some actual efforts) for 50 to 100 kilometre round-trips, although less so these days, it was nicer when I lived closer to the countryside between France and Belgium. But the most important thing is I hate the noise of cars in the city. I want to be able to walk with someone and talk in the street without having to shout or stop while a loud vehicle passes. I don't care about cars themselves, after all they're on their streets doing their things, if they take twice as long as I do to get somewhere it's not my problem. I just want their noise to be gone. reply piva00 18 hours agorootparentprev> I just don't get it and I don't think I can get along with people who wants to put us all on bicycles and I don't think people who want to get us all on bicycles could get along with me. And I'm fine with that. You are creating a strawman though, people like me want urbanites/city dwellers to become cyclists because it's better for everyone living in a city. If you live in a rural area where you need to go 10-20km to find a grocery store, yeah, it's not that practical to only rely on a bike. If you live in a dense-ish city where you can bike some 15 minutes to get to almost all of your daily needs, and with a 30 min ride you can get to 80% of your needs in a year then yeah, I will try to convince you that a bike is more practical for your day-to-day than driving the odd 3-4 km to get somewhere (and paying for parking, fuel, polluting the city, etc.), it's just stupid to drive this short if you don't really need it (e.g.: carrying big/heavy loads, being disabled, etc.). It's odd to see this kind of overreaction to something that simply is better for your health, for the other residents of the city. It's odd to see how defensive car-centric people get by the suggestion that maybe more people cycling, and governments taking action to create infrastructure for it, will be overall better for a city. Even if you prefer to drive it'll be better for you. Just don't create a strawman to distill your reactionary take, it always sounds like a whining child feeling their toy will be taken away... That's not the point. reply cycomanic 13 hours agorootparent> > I just don't get it and I don't think I can get along with people who wants to put us all on bicycles and I don't think people who want to get us all on bicycles could get along with me. And I'm fine with that. > You are creating a strawman though, people like me want urbanites/city dwellers to become cyclists because it's better for everyone living in a city. In my experience what people like the OP really mean by \"I don't like that cyclist want to put us all on bikes\", is that they want infrastructure to be catered for their car use (paid by the everyone), e.g. they say they can't use a bike because they (want to) live rural (rural infrastructure is heavily subsidised by city dwellers btw), but want to be able to drive into the city (despite city dwellers wanting less cars and more bikes). Thus they complain about \"cyclist forcing everyone to bike\". reply lotsoweiners 17 hours agoparentprevIn my experience the Venn diagram of people who bike as their main transportation source and people who care about hygiene and overall appearance doesn’t overlap very much at all. reply tin7in 18 hours agoprevCovid really accelerated this. Friends visiting in Paris are also surprised how fast the city changed in just 2-3 years. reply EVa5I7bHFq9mnYK 18 hours agoprevIsn't it rather thanks to the banning of electric scooters? reply favflam 19 hours agoprevI switched to cycling for transportation. Benefits: 1. way more access to the city 2. eliminated need to go to gym; transport time doubles as cardio time 3. freed up time to see people; 15 minutes door to door is hard to beat I really hope US cities head in this direction. It is a no brainer for cities: 1. higher density -> lower taxes per person, higher tax rev per sqm 2. lower cost of living -> lower inflation 3. quieter; cars on streets are noisy 4. people see each other more; better social cohesion 5. kids are free from the house to do things reply lkbm 19 hours agoparentI biked for transportation (and fun) for years and it was great. In 2022 I moved and had no bike for eight months and hurt my back ~four times in that period. I got a new bike about a year ago and have had no back pain since (until randomly yesterday, but I'm 41, I guess it's expected that it'll happen occasionally). Biking doesn't seem like a back exercise, but you are using your core and flexing it a lot, and for longer than you would on any standard core exercise. Building exercise into your day to day life is incredible valuable. If you're lucky enough to be in a place where biking is viable, I highly recommend giving it a try. It's not for everyone, but given the right situation, I suspect the majority of people would find it a positive change to start biking more. (I suspect this is true for a lot of exercise, but biking has the benefit of being a productive workout—similar to chopping wood vs. lifting weights, except few of us need much wood chopped.) reply JAlexoid 18 hours agorootparentCycling is fairly heavy on your lower back and the need to balance yourself means that your core gets a huge hit a lot of the time. Unless you have one of those lounger bikes, you're getting an exercise that hits almost everything below your ribs. reply teemur 18 hours agoparentprevThere is also a somewhat amusing argument that cycling does not waste your time. you get every hour cycled back as hour added to your expected lifetime: https://www.huffingtonpost.co.uk/2015/10/12/cycling-add-year... reply triceratops 18 hours agorootparentYeah but it's more time when you're older instead of right now /s reply nradov 18 hours agoparentprevCycling is great! But it doesn't eliminate the need to go to the gym. People who rely on cycling as their only form of exercise tend to suffer from low bone density and unbalanced muscle development. Everyone should do resistance training as well. reply a_gnostic 19 hours agoparentprevThe first US asphalt roads were built by cycling clubs (1). 1) https://www.vox.com/2015/3/19/8253035/roads-cyclists-cars-hi... reply VeejayRampay 18 hours agoprevbut do note that car owners in Paris (where I live) are reaching never-seen-before levels of whining, complaining about the fact that their little selfish privilege is not as cushy as it used to be and that only 80% of the public road infrastructure is dedicated to their idiotic 2 miles journeys on board of their oversized SUVs now reply richrichie 18 hours agoprevThe second order effect is that tax rates will increase to cater to increase in social security payment and medical care thanks to increased life expectancy. That might also divert money from other public spending that could mean living longer but with lower quality of life. reply gregoriol 18 hours agoprev [–] It must also has be taken into account that driving has become annoying to the point that many people from suburbs don't come anymore to Paris for shopping, theatres, exhibitions, ... and many living inside or nearby are really annoyed by the public transports which are dirty, not on time, too crowded, beggars every few minutes, ... so biking is like a forced solution that helps with some things but isn't always a choice of preference. It's easier to see this in the winter, which is not cold/wet enough in Paris to make cycling difficult, but there are clearly much less cyclists then. reply sloowm 18 hours agoparentYou describe Paris as if it's a shopping/entertainment center for the suburbs. It also looks like you're implying that Paris needs the economic activity generated by the suburbs. For that I would investigate what a suburb is by definition. reply gregoriol 14 minutes agorootparentIn the 70s-90s, it was! A lot of shops and movie theatres have closed in some areas like Champs-Elysées, Montparnasse, Saint-Germain, ... where people used to come by car from suburbs on the week-ends because they could, because it was practical, because they could park. Places for shopping and entertainment have been created in the suburbs since then, which is a win for the suburbs but not for Paris which has become depressing. reply trgn 17 hours agoparentprev [–] it's a win win win! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Paris has experienced a doubling in cycling numbers within a year due to substantial investments in cycling infrastructure, aiming to become a \"100 percent cycling city.\"",
      "The surge in cycling aligns with a growing trend towards sustainable mobility and urban revitalization, reflecting a broader societal shift.",
      "The significant funding allocation by the French government indicates the likelihood of this trend extending nationwide, promising a flourishing cycling culture in Paris with expanded bike lanes and public bike-sharing programs."
    ],
    "commentSummary": [
      "Investment in cycling infrastructure in Paris doubled cycling numbers but led to congestion on bike lanes, raising safety concerns amid tension between motorists and cyclists.",
      "The discussion underlines benefits of biking infrastructure, challenges like sweating, and solutions such as fitness, clothing, and pacing to address the issue.",
      "Biking's impacts on health, transportation, and urban planning are stressed, with e-bikes emerging as a practical commuting solution in cities, promoting cycling as a sustainable transportation mode."
    ],
    "points": 256,
    "commentCount": 199,
    "retryCount": 0,
    "time": 1710771621
  },
  {
    "id": 39742916,
    "title": "Developing pg-copyjit: A New JIT Compiler for PostgreSQL",
    "originLink": "https://www.pinaraf.info/2024/03/look-ma-i-wrote-a-new-jit-compiler-for-postgresql/",
    "originBody": "Posted on March 18, 2024March 18, 2024 by pinaraf Look ma, I wrote a new JIT compiler for PostgreSQL Sometimes, I don’t know why I do things. It’s one of these times. A few months ago, Python 3.13 got its JIT engine, built with a new JIT compiler construction methodology (copy-patch, cf. research paper). After reading the paper, I was sold and I just had to try it with PostgreSQL. And what a fun ride it’s been so far. This blog post will not cover everything, and I prefer other communication methods, but I would like to introduce pg-copyjit, the latest and shiniest way to destroy and segfault speed up your PostgreSQL server. Before going any further, a mandatory warning: all code produced here is experimental. Please. I want to hear reports from you, like “ho it’s fun”, “ho I got this performance boost”, “hey maybe this could be done”, but not “hey, your extension cost me hours of downtime on my business critical application”. Anyway, its current state is for professional hackers, I hope you know better than trusting experimental code with a production server. In the beginning, there was no JIT, and then came the LLVM JIT compiler In a PostgreSQL release a long time ago, in a galaxy far far away, Andres Freund introduced the PostgreSQL world to the magics of JIT compilation, using LLVM. They married and there was much rejoicing. Alas, darkness there was in the bright castle, for LLVM is a very very demanding husband. LLVM is a great compilation framework. Its optimizer produces very good and efficient code, and Andres went further than what anybody else would have thought and tried in order to squeeze the last microsecond of performance in his JIT compiler. This is a wonderful work and I don’t know how to express my love for the madness this kind of dedication to performance is. But LLVM has a big downside : it’s not built for JIT compilation. At least not in the way PostgreSQL will use it: the LLVM optimizer is very expensive, but not using it may be worse than no compilation at all. And in order to compile only the good stuff, the queries that can enjoy the performance boost, the typical query cost estimation is used. And that’s the PostgreSQL downside making the whole thing almost impossible: costs in PostgreSQL are not designed to mean anything. They are meant to be compared to each other, but do not mean anything regarding the real execution time. A query costing 100 may run in 1 second, while another costing 1000 may run in 100 milliseconds. It’s not a bug, it’s a design decision. That’s why a lot of people (including me) end up turning off the JIT compiler: most if not all queries on my production system will not get enough from the performance boost to compensate the LLVM optimizer cost. If I can run the query 10ms faster but it needed 50ms to be optimized, it’s pure loss. There is one way to make the LLVM JIT compiler more usable, but I fear it’s going to take years to be implemented: being able to cache and reuse compiled queries. I will not dig further into that topic in this post, but trust me, it’s not going to be a small feat to achieve. And in 2021, copy-and-patch was described… So, what can we do? We need fast enough code generated the fastest way possible. Fast enough code mean at least a bit faster than the current interpreter… But writing a compiler is painful, writing several code generators (for different ISAs for instance) is even worse… This is where the innovation of copy-and-patch comes into play and saves the day. With copy-patch, you write stencils in C. These stencils are functions with holes, and they are compiled by your typical clang compiler (gcc support pending, too complicated to explain here). Then when you want to compile something, you stitch stencils together, fill in the gaps, and jump straight into your brand new “compiled” function. And this is it. This is the magic of copy-and-patch. You only copy the stencils in a new memory area, patch the holes, and voilà. Of course, you can go further. You can figure out what computation can be done at compilation time, you can split loops in several stencils to unroll them, you can merge several stencils together to optimize them in one go (creating kind of meta-stencils…) This paper caught the eyes of the Faster-CPython team, they implemented it in CPython 3.13, and this is when more people (including me) discovered it. Bringing copy-and-patch to PostgreSQL So, what does it take to build a new JIT engine in PostgreSQL? Hopefully, not that much, otherwise I would likely not be blogging about this. When JIT compilation was introduced, it was suggested on hackers to make LLVM a plugin, allowing future extensions to bring other JIT compilers. Back then, I was quite skeptical to this idea (but never expressed this opinion, I did not want to be wrong later), and it turned out I proved myself wrong… The interface is really simple, your .so only needs to provide a single _PG_jit_provider_init function, and in this function initialize three callbacks, named compile_expr, release_context and reset_after_error. The main one is obviously compile_expr. You get one ExprState* parameter, a pointer to an expression, made of opcodes. Then it’s “only” a matter of compiling the opcodes together in any way you want, mark this built code as executable, and changing the evalfunc to this code instead of the PostgreSQL interpreter. This is easy, and you have an automatic fallback to the PostgreSQL interpreter if you encounter any opcode you’ve not implemented yet. The copy and patch algorithm (implemented with only a few small optimizations so far) is so easy I can explain it here. For each opcode, the compiler will look into the stencil collection. If the opcode has a stencil, the stencil is appended to the “built” code. Otherwise, the compilation stops and the PostgreSQL interpreter will kick in. After appending the stencil, each of its holes are patched with the required value. For instance, let’s consider this basic unoptimized stencil, for the opcode CONST. Datum stencil_EEOP_CONST (struct ExprState *expression, struct ExprContext *econtext, bool *isNull) { *op.resnull = op.d.constval.isnull; *op.resvalue = op.d.constval.value; NEXT_OP(); } op is declared as extern ExprEvalStep op; (and NEXT_OP is a bit harder to explain, I won’t dig into it here). When building this to a single .o file, the compiler will leave a hole in the assembly code, where the address for op will have to be inserted (using a relocation). When the stencil collection is built, this information is kept and used by the JIT compiler to use the current opcode structure address in order to get a working code. The build process for the stencils is quite fun, not complicated, but fun. The first step is to build the stencils to a single .o file, and then extract the assembly code and relocations from this .o file into C usable structures, that the JIT compiler will link to. And that’s about all there is. At first, I was extracting the assembly code manually. Using that way, I managed to get the three needed opcodes for SELECT 42; to work. And there was much joy. After this first proof of concept (and I guess some disturbed looks a few days ago at PgDay.Paris when people saw me happy with being able to run SELECT 42, that may have sound weird), I wrote a DirtyPython (unofficial variant) script to automate the assembly code extraction, and in a few hours I implemented function calls, single table queries, more complicated data types, introduced a few optimizations… Current state It works on my computer with PostgreSQL 16. It should be fine with older releases. It only supports AMD64 because that’s what I have and I can not target everything at once. Later I will add ARM64, and I would love to have some time to add support for some interesting targets like POWER64 or S390x (these may require some compiler patches, sadly, and access to such computers, nudge nudge wink wink)… Performance-wise, well, keeping in mind that I’ve spent almost no time optimizing it, the results are great. Code generation is done in a few hundreds microseconds, making it usable even for short queries, where LLVM is simply out of the game. On a simple SELECT 42; query, running with no JIT takes 0,3ms, with copyjit it requires 0,6ms, LLVM with no optimizations goes to 1,6ms and optimizing LLVM will require 6,6ms. Sure, LLVM can create really fast code, but the whole idea here is to quickly generate fast enough code, and thus comparing both tools won’t make much sense. But still, you are all waiting for a benchmark, so here we go, benchmarking two queries on a simple non-indexed 90k rows table. This benchmark is done on a laptop and my trust in such a benchmark setup is moderate at best, a proper benchmark will be done later on a desktop computer without any kind of thermal envelope shenanigans. And I have not optimized my compiler, it’s still quite stupid and there is a lot of things that can and must be done. Query Min/max (ms) Median (ms) and stdev select * from b; — no JIT 10.340/14.046 10.652/0.515 select * from b; — JIT 10.326/14.613 10.614/0.780 select i, j from b where i < 10; — no JIT 3.348/4.070 3.7333/0.073 select i, j from b where i < 10; — JIT 3.210/4.701 3.519/0.107 Stupid benchmark on a laptop running non-optimized code, don’t trust these… As you can see, even in the current unfinished state, as soon as there is CPU work to do (here it’s the where clause), performance relative to the interpreter get better. It’s only logical, and what is important here is that even if the JIT is an extra, slightly time consuming step, it takes so little time even these queries can go a few percents faster. Note that even if I’ve implemented only a small handful of opcodes, I can run any query on my server, the JIT engine will only complain loudly about it and let the interpreter run the query… For the more curious, the code is dumped there on github. I said dumped because I focus only on the code and not on the clarity of my git history nor on wrapping it in a nice paper with flying colors and pretty flowers, that’s what you do when the code is done, this one isn’t yet… If you want to build it, the build-stencils.sh file must be run manually first. But again, I do not document it yet because I simply can not provide any support for the code in its current state. TODO… This is a proof of concept. I’ve not worked on making it easy to build, on making it possible to package it… The build scripts are Debian and PostgreSQL 16 specific. And, well, to be honest, at this point, I don’t care much and it will not trouble me, my focus is on implementing more opcodes, and searching for optimizations. I really hope I will reach a point where I can safely package this and deploy it on my production servers. This way, I’ll keep using the LLVM JIT on the server that can use it (a GIS server where queries are worth the optimization) and use this JIT on my web-application databases, where short query time is a must have, and the LLVM optimizations end up being counter-productive. I am also dead serious on porting this to other architectures. I love the old days of Alpha, Itanium, Sparc, M68k and other different architectures. I am not going to use this kind of system, but I miss the diversity, and I really don’t want to be a part of the monoculture issue here. Thanks First, huge thanks to my current day-job employer, Entr’ouvert. We are a small french SaaS company, free-software focused, and my colleagues simply let me toy on this between tickets and other DBA or sysadmin tasks. I would like to thank my DBA friends for supporting me and motivating me into doing this (won’t give their names, they know who they are). BTW: use PoWA, great tool, tell your friends… Also, quick question: they suggest I shall go to PGConf.dev to show this, but it’s too late for the schedule and since I live in France I did not intend to go there. If you think it’s important or worth it, please, please, say so (comments below, or my email is p@this.domain), otherwise see you in future european PG events 🙂 CategoriesPostgreSQLTagsJIT, PostgreSQL",
    "commentLink": "https://news.ycombinator.com/item?id=39742916",
    "commentBody": "I wrote a new JIT compiler for PostgreSQL (pinaraf.info)252 points by mattashii 22 hours agohidepastfavorite30 comments weliveindetail 20 hours ago> There is one way to make the LLVM JIT compiler more usable, but I fear it’s going to take years to be implemented: being able to cache and reuse compiled queries. Actually, it's implemented in LLVM for years :) https://github.com/llvm/llvm-project/commit/a98546ebcd2a692e... reply pinaraf 19 hours agoparentYeah, well, sorry, I should have been more explicit here: the issue is with PostgreSQL, not LLVM. The JIT compiler has to inject direct memory addresses, making the generated code specific to your query and process. reply weliveindetail 12 hours agorootparentInteresting, because we store relocatable objects. And process symbols can be resolved by name if you really want. It might be yet another performance trade-off though. reply SigmundA 18 hours agoparentprevSince PG uses one process per connection and the LLVM JIT code is process specific the code can't be shared amongst all connections to the DB. Plans themselves suffer from this since they are in memory data structures not designed to be shared amongst different processes. DB's like MSSQL don't have this issue since they use a single process with threads which is also why it can handle more concurrent connections without an external pooler. Although MSSQL can also serialize plans to a non process specific representation and store them in the DB for things like plan locking. reply hinkley 13 hours agorootparentWay back on Oracle 9i, we had a mystery stall problem. We couldn’t saturate the network, the CPU, or the fiber channel links. We were stuck at ~50% and stumped. Some fuckery was going on and we had to call in a professional. Turned out 9i could only run queries that currently resided in the query cache, and some idiot (who was now my boss) had fucked up our query builder code so that we were getting too many unique queries. Not enough bind variables. So it’s clear Oracle was using a shared cache back then, but like other people here, I’m scratching my head how this would work with Postgres’s flavor of MVCC. Maybe share query plans when the transaction completes? I feel like that would get you 90% of the way but with some head of queue nastiness. reply hans_castorp 18 hours agorootparentprev> Plans themselves suffer from this since they are in memory data structures not designed to be shared amongst different processes. Oracle uses a process-per-connection model as well (at least on Linux), and they are able to share execution plans across connections. They put all the plans into the \"global\" shared memory. reply SigmundA 17 hours agorootparentLooks like you can change that with THREADED_EXECUTION to make it act like it does on Windows with a single process and threads: >On UNIX, starting with Oracle Database 12c Release 2 (12.2), Oracle Database can use an operating system process or an operating system thread to implement each background task such as database writer (DBW0), log writer (LGWR), shared server process dispatchers, and shared servers. The use of operating system threads instead of processes allow resource sharing and reduce resource consumption. On Windows, each background process is implemented as a thread inside a single, large process. https://docs.oracle.com/en/database/oracle/oracle-database/1... Processes in Windows are much more expensive than Unix typically so using threads has always been preferred to multi process, perhaps thats why MSSQL only has that option with an almost fully recreated internal process model that you can list and kill etc. Even Oracle says it helps with resource usage, even on Unix/Linux. Also looks like Oracle has had some kind shared mode for a long time where it basically has a built in pooler to keep actual OS process count down, not 1:1 like PG. Sharing plans can obviously be done using shared memory but it's not a simple as just creating some C++ object model (which I believe is what PG has internally) for the plan it must have a process agnostic data format that is then executed probably by deserializing into a executable model from shared memory. Fully jitted code is even trickier vs just a set of logical plan operations. With threads you just share executable code. reply pinaraf 20 hours agoprevAuthor here. Thanks for submitting my article on hackernews. I'll do my best to answer any question. reply o11c 3 hours agoparentIs copy-and-patch really a new idea, or just a new name for an old idea? When I learned programming (and interpreters particularly) around 2010, I thought it was well-known that you could memcpy chunks of executable code that your compiler produced if you were careful ... the major gotcha was that the NX bit was just starting to take off at the time (Even on Linux, most people still assumed 32-bit distros and might be surprised that their CPUs even supported 64-bit. At some point I ended up with a netbook that didn't support 64-bit code at all ...). Unfortunately I ended up spending too much time on the rest of the code to actually look deeply enough into it to build something useful. reply winternewt 17 hours agoparentprevIs there a fundamental difference between copy and patch with C and what compilers do when they target intermediate representations? It seems to me that traditional compilation methods are also \"copy and patch\" but with another intermediate language than C. reply tetha 16 hours agorootparentI think conceptually, there is no real difference. In the end, a compiler outputting machine code uses very small stencils, like \"mov _ _\", which are rather simple to patch. Practically though, it's an enormous difference, as the copy and patch approach re-uses the years of work going into clang / gcc supporting platforms, optimizations for different platforms and so on. The approach enables a much larger pool of people (\"People capable of writing C\" vs \"People capable of writing assembly / machine code\") to implement very decent JIT compilers. reply pinaraf 11 hours agorootparentThe real difference is in the possible optimizations. If you consider the full scope of JIT compilation in for instance a web browser or the JVM, you could use copy and patch as a tier 0 compiler, and once really hot paths are identified, trigger a complete compiler with all the optimizer steps. Some optimizations are more complicated to implement with copy-patch, esp. if you can't use all the tricks described in the paper (for instance they use the ghccc calling convention to get a much finer register allocation, but from the documentation I don't think it's going to make it for PostgreSQL). But as you say, yes, this enables people capable of writing C and reading assembly (or you have to be perfect and never have to go into gdb on your compiled code), and it makes the job so much faster and easier... Writing several machine code emitters is painful, and having the required optimization strategies for each ISA is quickly out of reach. reply pgaddict 20 hours agoparentprevWould be a great topic for pgconf.eu in June (pgcon moved to Vancouver). Too bad the CfP is over, but there's the \"unconference\" part (but the topics are decided at the event, no guarantees). reply mattashii 19 hours agorootparentDid you mean pgconf.dev in May (which has the unconference), or pgconf.eu in October (which doesn't have an unconference, but the CfP will open sometime in the - hopefully near - future)? reply pgaddict 17 hours agorootparentYeah, I meant May. Sorry :-( Too many conferences around that time, I got confused. That being said, submitting this into the pgconf.eu CfP is a good idea too. It's just that it seems like a nice development topic, and the pgcon unconference was always a great place to discuss this sort of stuff. There are a couple more topics in the JIT area, so having a session or two to talk about those and how to move that forward would be beneficial. reply frizlab 20 hours agoparentprevNot a question, but I love this. I’m eager to see its evolution. reply can3p 20 hours agoparentprevNice post, thanks! Do I read it right that using jit results in the worst max times? What could be a reason in your opinion? reply pinaraf 20 hours agorootparentTwo parts: I did the benchmark on a laptop and didn't spend enough time forcing its runtime PM in a fixed state, I'll run a real pgbench on my desktop once I implement all required opcodes for it. And since JIT requires a minimum amount of time (about 300us on my tests), on such small runtimes this can quickly overcome the benefits. reply adzm 20 hours agoprevI'm still surprised there isn't a query/plan cache for PostgreSQL. I could easily see these two approaches working in harmony once it does, as frequent queries could end up being cached and more aggressively optimized with a cache to offset the compilation cost. Of course that adds a whole new layer of complexity and trouble. reply williamdclt 20 hours agoparent(The article goes a bit above my head so my excuses if I am a bit off-topic) There is a form of query plan caching in PG: for prepared statements, if PG determines that the actual value of parameters won't affect the query plan much, it uses a \"generic plan\" so that it reuses the same query plan for every execution of the prepared statement (https://www.postgresql.org/docs/current/sql-prepare.html, see \"notes\") reply pinaraf 20 hours agorootparentIndeed, and right now it's the only possible way since it remains in a single session, doing otherwise would be very hard. reply cbsmith 17 hours agorootparentUnless you count stored procs... reply SigmundA 18 hours agorootparentprevYes its manual and per session, DB's like MSSQL have that was well but are very rarely used anymore because it got automatic plan caching about 20 years ago which basically eliminates any advantage to manually preparing. Its actually better since it can be shared across all sessions reply pinaraf 20 hours agoparentprevHonestly I thought the same as you, then I wrote this, and I now understand it's going to be really hard to do. To make it very simple: there are pointers to query parts \"leaking\" everywhere across the execution engine. Removing them will require a significant overall of the execution engine, the planner and who knows what else. Even in a single session, two compiled queries will have different compiled code because of that (both llvm and my copyjit have to inject the adresses of various structs in asm code) reply adzm 20 hours agorootparentJust going to say, I'm blown away by how simple this JIT is though. Really quite a beautiful JIT approach. reply pinaraf 19 hours agorootparentSame for me, that's why I did this after finding out this research paper. With the proper compiler settings and small tricks you can remove some parts and already end up faster than the interpreter (because you remove some branches and a few memory accesses) and it's even possible to create \"super-stencils\" covering typical opcodes series and optimizing them further. Or the opposite, \"sub-stencils\" in order to do some loop unrolling for instance. reply aeyes 19 hours agoparentprevThe plan cache on Oracle in combination with prepared statements where the optimizer can't peek into the parameters has been really problematic for me in the past. I usually had to go in and either add hints or force a plan. Even simple queries like SELECT * FROM t WHERE x = TRUE; could turn into a nightmare depending on the distribution of the x values in the table. With Postgres I rarely encountered such problems but I must admit that I haven't used Postgres with prepared statements. I have seen some queries with slow planning time (>100ms) where a cache could have been useful but I don't remember ever really needing to optimize one. reply SigmundA 18 hours agorootparentIs x a parameter because it doesn't look like it? MSSQL has parameter sniffing and will make multiple plans based on incoming parameters I would be surprised if Oracle does not do the same. It can actually be problematic to sniff parameters sometimes and it can be disabled with a hint! reply aeyes 18 hours agorootparentI just made up something simple but yes, I had these problems with bind variables in prepared statements. > SELECT * FROM t WHERE x = :var; But I haven't used Oracle in years and back in the day, there was no bind variable peeking. reply miohtama 12 hours agoprev [–] Back in 386 era, there was a concept of self-modifying code (assembly). A similar like stencils presented here, but because code was a singleton, rarely a copy was made. E.g. Doom on DOS used this optimisation techique, because otherwise you could not cram out enough performance from tight rendering loops on old CPUs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author introduces pg-copyjit, a new JIT compiler for PostgreSQL, inspired by a novel construction methodology, highlighting challenges with using LLVM for JIT compilation in PostgreSQL.",
      "They propose the copy-and-patch algorithm as an alternative due to cost estimation challenges, detailing the process of implementing a new JIT engine in PostgreSQL.",
      "Performance benchmarks of pg-copyjit versus LLVM are shared, with ongoing optimization efforts aiming for potential deployment on production servers despite being in progress."
    ],
    "commentSummary": [
      "The author introduced a new JIT compiler for PostgreSQL, highlighting the difficulties of caching compiled queries and the JIT code's process-specific nature.",
      "A comparison was made on how PostgreSQL, Oracle, and MSSQL manage query plans, emphasizing potential optimizations in JIT compilation.",
      "The discussion included insights on copy-and-patch in compilers and query plan caching in various database systems."
    ],
    "points": 252,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1710763044
  },
  {
    "id": 39743594,
    "title": "Lotilaner Pill Shows Promise in Tackling Tick-Borne Diseases",
    "originLink": "https://arstechnica.com/science/2024/03/tick-killing-pill-shows-promising-results-in-human-trial/",
    "originBody": "Ticked off — Tick-killing pill shows promising results in human trial Should it pan out, the pill would be a new weapon against Lyme disease. Emily Mullin, wired.com - 3/16/2024, 11:16 AM Enlarge Ladislav Kubeš reader comments 217 If you have a dog or cat, chances are you’ve given your pet a flavored chewable tablet for tick prevention at some point. What if you could take a similar pill to protect yourself from getting Lyme disease? Tarsus Pharmaceuticals is developing such a pill for humans—minus the tasty flavoring—that could provide protection against the tick-borne disease for several weeks at a time. In February, the Irvine, California–based biotech company announced results from a small, early-stage trial showing that 24 hours after taking the drug, it can kill ticks on people, with the effects lasting for up to 30 days. “What we envision is something that would protect you before the tick would even bite you,” says Bobby Azamian, CEO of Tarsus. Lyme disease is a fast-growing problem in the United States, where approximately 476,000 people are diagnosed and treated for it each year, according to the most recent data from the Centers for Disease Control and Prevention. That number is likely an overestimate, because many patients are treated after a tick bite even if an infection isn’t confirmed, but it underscores the burden of Lyme disease on the health care system—which researchers at the CDC and Yale University put at nearly $1 billion per year. The disease is caused by the bacteria Borrelia burgdorferi, which gets passed to humans through the bite of an infected tick. In most cases, a tick has to be attached for around 36 to 48 hours before the bacteria can be transmitted. Symptoms include fever, headache, fatigue, and a characteristic skin rash that looks like a bullseye. Advertisement Without a vaccine for Lyme disease on the market, current prevention includes using insect repellents such as DEET and permethrin and wearing closed shoes, long pants, and long sleeves when in a tick-infested area. “We’ve seen increasing rates of tick-borne diseases over the years, despite being told to do tick checks, use DEET, and impregnate your clothes with permethrin,” says Paul Auwaerter, a professor of medicine at the Johns Hopkins University School of Medicine who studies Lyme disease. A more effective treatment strategy would be welcome, Auwaerter says, especially because Lyme disease can sometimes cause serious health issues. Antibiotics are usually effective when taken early, although about 5 to 10 percent of patients can have lingering symptoms for weeks or months. If left untreated, the infection can spread to the joints and cause arthritis. It can also become established in the heart and nervous system, causing persistent fatigue, numbness, or weakness. The experimental pill that Tarsus Pharmaceuticals is testing is a formulation of lotilaner, a drug that paralyzes and kills parasites by interfering with the way that signals are passed between their nerve cells. Lotilaner is already approved as a veterinary medicine under the brand name Credelio to control fleas and ticks in dogs and cats. “Our animals have better options than we do for tick prevention,” says Linden Hu, a professor of immunology at Tufts Medical School who led the Tarsus trial. “There are quite a few drugs and vaccines available for dogs and cats, but there's nothing for us.” Tarsus first developed lotilaner for human use as an eye drop to treat blepharitis, or inflammation of the eyelid, which is caused by tiny mites. That drug, Xdemvy, was approved by the US Food and Drug Administration in July 2023. It stuns and kills mites present in the eyelid. Azamian and his team had the idea to test it against ticks in people. The oral version of the drug enters the bloodstream and is passed to a tick when it bites and starts sucking blood. Advertisement “A lot of drugs are tested in animals, but very few are commercialized for animal use and then go to human use,” Azamian says. In a Phase II trial, 31 healthy adults took either a low or high dose of the Tarsus pill, or a placebo. Researchers then placed sterile ticks on participants’ arms and, 24 hours later, measured how many died. They also observed tick death 30 days after a single dose of the pill. At day one, 97 percent of ticks in the high-dose group and 92 percent in the low-dose group had died, while only 5 percent of ticks in the placebo group had. One month out, both doses of the pill killed around 90 percent of ticks. The company reported no serious adverse events from the pill, and none of the participants dropped out due to side effects. “The takeaway is that it killed the ticks really quickly,” Hu says. “And the effect lasted for a long time.” The fact that the drug targets ticks, rather than the bacteria that causes Lyme disease, means that it could protect against other tick-borne diseases that are spreading in the US, including babesiosis and anaplasmosis. Thanks to climate change and exploding deer populations, ticks are expanding their ranges—and carrying diseases with them. Tarsus has not proven that its pill can actually prevent Lyme disease. That will require testing the drug in hundreds of people who are at high risk of contracting the disease. But Hu is cautiously optimistic: “This pill is potentially a pre-exposure prophylaxis that you don’t have to think about.” Azamian imagines it as something people would take before going hiking or on a camping trip or just going outside in any tick-infested area. “There is that subset of people that truly have persistent symptoms after Lyme disease that can really be devastating,” Auwaerter says, “so preventing that would be an amazing opportunity.” This story originally appeared on wired.com. reader comments 217 WIRED Wired.com is your essential daily guide to what's next, delivering the most original and complete take you'll find anywhere on innovation's impact on technology, science, business and culture. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39743594",
    "commentBody": "Tick-killing pill shows promising results in human trial (arstechnica.com)245 points by ludovicianul 21 hours agohidepastfavorite176 comments nightowl_games 20 hours agoI've lived in an area that has dog ticks (not the Lyme carrying ones). I've pulled dozens of ticks off myself. I've never once had one attach, always caught them in time. I got into archery during covid and would haul a target out onto some grasslands and shoot. My buddy and I would pull half a dozen ticks off ourselves at the end. I'd find ticks half dead on my car dash the next day. We bought \"tick pants\". Pants coated in some chemical that does something like this. Tuck them into our boots. No more ticks. Clothes coated in this drug seems like a good alternative to actually consuming the drug in my opinion. Also I've been trying to spot a tick in the wild before it grabs me and have never done it. I want to see a \"questing\" tick, ie one that is holding on to the edge of a blade of grass with its arms out trying to grab something. reply mike_red5hift 19 hours agoparentWhen you know what to look for, a questing tick is not too hard to spot. If you are in an area that you suspect might have a lot of ticks. Look at the tips of any grass stems for a little brown/black spot. They climb as high as they can and then kinda \"stand up\" and put their \"arms\" up and out and wait for something to touch them. Then it's off to the races to climb as high as they can... on you. They stop once they hit an obstacle they can't easily get around and dig in. I spray my shoes with permethrin. That seems to almost totally eliminate climbers as I'm not walking through tall grass. You know they've gotten a deadly dose when they start walking in circles. I don't feel bad for them. My doctor told me that 85% of ticks in my area are positive for Lyme. reply DowagerDave 18 hours agorootparenttrying to spot ticks in heavy brush as you are walking seems like a inefficient approach. They take a while to attach so if you do a thorough review after your activity you should be good. It's also a lot easier to strip down to your birthday suit in the privacy of your own home. reply transitionnel 18 hours agorootparentYeah, still gotta enjoy nature without a crick in the neck. reply jajko 16 hours agorootparentprevStripping down has massive 'corner' cases. If you are alone, good luck find one on your back, between ass cheeks and few other places. Also if you do full day activity (or god forbid multiday out in the wild), not so good. Once they reach your hair (unless you shave clean), even good luck may not be enough, very hard to spot and almost impossible alone reply zikduruqe 14 hours agorootparentWhere I grew up we called that Redneck Foreplay. When you'd get home and have the significant other do a tick check. reply giantg2 16 hours agorootparentprev\"Stripping down has massive 'corner' cases. If you are alone, good luck find one on your back, between ass cheeks and few other places.\" Mirrors or camera phones are handy. reply jajko 14 hours agorootparentYes of course but not good enough, imagine darker not ultra short hair, how do you spot one on the back of your greasy hair. I live in places where lyme is barely 5% incidence, and even that not everywhere. If it would be like 80% like some mention, doing week-long hikes alone would be outright suicidal. Better chances for retirement in base jumping. reply giantg2 13 hours agorootparentYou can feel your head for them. Even if you have one attach, Lyme is really only concern after 72 hours of attachment (other things can be an immediate concern). You should be quite itchy and notice it before then. A single dose of antibiotics is a highly effective post exposure prophylactic. Lyme is a concern, but not as bad as many in the media make it out to be. reply zx8080 7 hours agorootparentLyme is not the worst thing from ticks. Tick-borne encephalitis is much worse and can effectively leave a person disabled. There's just no treatment for it. reply acomjean 6 hours agorootparentprev“permethrin“ is the stuff that kills ticks. My brother works outside and got Lyme disease. It was awful so his crew wear permethrin soaked clothing. I think they buy it pre treated. I found two deer ticks (one attached) last fall. I always wear long pants and they’d just climb up till past my waist. They’re quite small. They gave me one dose of antibiotics as a preventative. I worry about our dog. She got the vaccine and has some medicine that kills then, but I still found three on her. reply SamBam 18 hours agoparentprevI once rented a cabin on Cape Cod, right next to the beach. The path to the beach had big clumps of grass growing at the sides, with overhanging blades of grass. The first day we realized that every single clump of grass had numerous questing ticks on them. You could see them clearly, all just waiting to latch on to you as you walked by. Honestly, it was kind of terrifying. reply kjhughes 17 hours agoparentprevAlso I've been trying to spot a tick in the wild before it grabs me and have never done it. I want to see a \"questing\" tick, ie one that is holding on to the edge of a blade of grass with its arms out trying to grab something. Interestingly, a \"questing\" tick has been found to be able to cross air gaps by leveraging static electricity: “We have now discovered that ticks can be lifted across air gaps several times larger than themselves by the static electricity that other animals naturally build up. This makes it easier for them to find and attach onto animals that they want to latch onto and feed from. https://www.bristol.ac.uk/biology/news/2023/httpswwwbristola... reply Workaccount2 19 hours agoparentprevIt's called permethrin. Kills ticks but also kills cats, so be careful. I have heard that once it is dry, it isn't a threat to your little furry friend, but I still won't risk it. reply diggan 19 hours agorootparent> It's called permethrin. Seems there are a couple of chemicals that act as anti-tick. We're using Seresto collars for our two dogs (no ticks in a decade of usage), which seems to be using Imidacloprid (anti-fleas) and Flumethrin (anti-ticks). So at least there seems to be two ingredients, there are probably more. reply Scoundreller 18 hours agorootparentI always wonder if we could have some human collar that absorbs & oozes out insecticide and keeps mosquitoes and blackflies away. But nice to see we’re finally getting for humans what pets have had for a while for ticks. reply doodlebugging 17 hours agorootparentA long time ago when I worked on a seismic crew we found ourselves cutting lines through the brush in a river bottom. Millions of ticks would swarm up your pant legs in a red-brown cloud looking for any opening to your skin. It was not unusual to see someone strip down in the field and burn the ticks off of their clothes. I carried a large knife and used the blade to peel them off my pants by the hundreds. It was crazy how thick they were. They seemed to be hanging on every pecan leaf and on the grass under the trees. We began treating our boots with diesel to try to repel them. That worked to a point. It didn't take long to reveal the shortcoming in that method - diesel fuel soaks through the leather and easily enters the skin. It ruins the boot leather and burns your skin so that wearing anything on your feet is very painful so we abandoned that method. After looking at options we decided to use cattle ear tags because each cow only uses one and it keeps them tick-free. After reading all the warnings about safe handling - wear gloves, avoid skin contact, potential reproductive harm, cancer risk, etc. - we decided that it made no difference. We realized that this had to be absorbed through the cow's bloodstream from the puncture on the ear and since every cow we had ever seen had an ear tag, this meant that this chemical was present in every steak or burger we had ever eaten so wearing one long enough to complete the part of the survey with the tick problem wasn't going to increase our risk of any adverse effects. We bought a pack of tags and each hung one on a boot and never had another tick. I don't have any known health issues to report here 40 years later. Totally anecdotal information from what is likely to have been a low-dose, short duration exposure event. reply Varriount 6 hours agorootparentWell, there's a difference between something entering your body through your digestive tract, and something entering your body directly through the bloodstream. Additionally, different substances have different bioaccumulation characteristics - some substances may accumulate in only in specific tissues (fat, muscle, etc.), a combination of tissues, or none. How an insecticide bioaccumulates in cattle would affect their suitability for beef. reply DowagerDave 18 hours agorootparentprevYou don't usually get bitten by mosquitoes on your neck though, how would this help with the bites you DO get all over your legs and arms? Or are you suggesting something so potent it absorbs into your bloodstream and then prevents bug bites? no thanks. reply Scoundreller 17 hours agorootparent> Or are you suggesting something so potent it absorbs into your bloodstream and then prevents bug bites That’s what the medicated pet collars do. Part of the advantage of collar-absorption is that you can use a molecule that readily breaks down/inactivates in digestion. And avoid sharp peaks in blood levels. reply Turing_Machine 17 hours agorootparentI knew a woman who was highly amused that the cancer drug she was on caused mosquitoes to die instantaneously upon biting her. Tough woman. She is missed. reply diggan 17 hours agorootparentprev> how would this help with the bites you DO get all over your legs and arms? Not sure how true it is, but the marketing for the collars we use claims that eventually the chemicals spreads to protect the full body, even if the collar is just around the neck. reply dylan604 16 hours agorootparentprev> You don't usually get bitten by mosquitoes on your neck though, and how do you manage this? reply afandian 19 hours agorootparentprevAnd lots of other wildlife. https://theriverstrust.org/about-us/news/flea-mergency-pet-t... reply gryzzly 12 hours agorootparentIn Germany there is a spray based on eucalyptus oil and it works very well. it is marketed as “sensitive”. we have a dog that licks stuff and wanted to avoid the use of the acaricides reply LinuxBender 18 hours agorootparentprevKills ticks but also kills cats AFAIK there has only been one documented case of it killing a kitten and it drank the liquid form, unknown quantity. Baby animals do not yet have fully formed livers to metabolize the sodium channel blocker. Insects on the other hand do not have a liver. They use body fat to detoxify but it isn't fast enough or volumetric enough to handle the sodium channel blocker in large amounts relative to their size. reply joecool1029 17 hours agorootparentPeople put dog tick/flea stuff on cats all the time by accident containing it (I've personally seen it twice now), it usually doesn't kill them but it's absolutely horrible to witness, cats will get seizures and treatment is only supportive, either they live or they don't. Both the cases I saw were brought to the vet and the cats were messed up pretty bad for a few days. reply LinuxBender 15 hours agorootparentI could imagine if a cat was being covered in it often enough and they lick it up their glutathione may be depleted then their liver won't keep up. I will have to dig into the stats iskander provided to see pre-existing condition the cats were at. i.e. elderly, kittens, sickly Skimming over it I am not seeing stats on age or liver health. reply iskander 15 hours agorootparentprev#1 on ASPCA's list of common toxicoses in cats: https://www.aspcapro.org/sites/default/files/zl-vetm0606_339... Study of 286 cat/permethrin cases in London: https://journals.sagepub.com/doi/full/10.1016/j.jfms.2007.05... Study of 750 cases in Australia: https://journals.sagepub.com/doi/full/10.1016/j.jfms.2009.12... ...we don't track this stuff nearly as well for pets as we do for humans but seems to have pretty robust evidence of toxicity. reply LinuxBender 15 hours agorootparentThankyou for that. I had not found the Sage journal when I was looking for this in the past. reply melvyn2 17 hours agorootparentprevHere's a case study of 42: https://journals.sagepub.com/doi/10.1016/j.jfms.2009.09.018 And there's certainly many more cases than just those at a \"referral hospital in Sydney, Australia.\" reply kamarg 19 hours agorootparentprevIt's also highly toxic to aquatic animals as well and does come off a bit when put in the wash. reply nucleardog 17 hours agorootparentprevI use permethrin around my yard as well as have some clothing I’ve treated myself. I also use flea and tick treatment on my two dogs which is primarily permethrin. I have two cats and we haven’t had any issues that I could see. I make 100% sure they can never be exposed to it when it’s wet, but they’ve been around me in treated clothing, escaped the house and walked through treated (dry) grass, been around dogs that have been treated with it, etc. Given the number of people that have cats and dogs living together who are treating their dogs with it monthly, it seems relatively safe if used properly and cautiously. reply Tiktaalik 14 hours agorootparentprevYeah it's a serious enough chemical that I think it's only legally usable in Canada by the military. reply CatWChainsaw 10 hours agorootparentI find this surprising, are you sure it's not based on the concentration of the permethrin? Here in the US, I was able to buy a clothing spray of 0.5% permethrin from Home Depot. I'm also reasonably certain that at least some lice shampoos I've seen at my nearby CVS use permethrin, although I checked the one I bought and it doesn't have it. reply bethekind 8 hours agorootparentCan confirm above comment. Only thing close we can buy is permethrin impregnated clothes for mosquitoes. No permethrin anything otherwise. I have some, but only because I had my sister ship it from the states. Blows my mind that you can buy up to 36% permethrin by the jug down there reply delfinom 18 hours agorootparentprevThe problem is ticks are also slowly becoming immune to it. reply transitionnel 18 hours agoparentprevThanks for this. I just pulled one [dug in] that I assume got on me meters away from a fully eaten deer carcass :/ Tweezed it out best I could, H2O2'd and lightly cauterized it then Neosporin'd, but still gonna have to play the bullseye waiting game AFAIK... Got dang ticks! reply pabl8k 16 hours agorootparentIt takes time for a tick to transmit lyme disease, but you can also take a single dose of doxycycline as post-exposure prophylaxis if you think it was attached for a while and ask a doctor reply acomjean 6 hours agorootparentThey recommend that dose if it’s been attached more the 36 hours and it’s less than 72 hours since removed. I fell into this slot last fall and got a single dose of doxycycline. https://www.cdc.gov/lyme/resources/FS-Guidance-for-Clinician... reply switchbak 16 hours agorootparentprevOdd how the sibling comment got downvoted into oblivion for this, but you're right - prophylaxis is an option here, and last I checked it was the recommended course of action if you develop the bullseye ring. Note that the tests have high false negative rates, so still might be sensible if you have a negative test. Of course, these antibiotics are not without risk too. reply lyme-educator 12 hours agorootparentSome points correcting what's been posted here and above. A single dose of Doxycycline is not an effective prophylactic. It failed me and other people I know (after my doctor and I read over the CDC guideline that was misinformation). The original study supporting it was poor regarding how it was established as effective (required the bullseye rash). The bullseye rash does not appear for everyone. Depending on the attach point/strength, ticks pass along bacteria and/or viruses in minutes. Testing the tick and monitoring yourself for symptoms is always a good practice. I prefer clothing & topical solutions vs ingested chemicals. It would be nice to see support for natural consumption of ticks by fowl (chickens, turkeys, quail, etc), marsupial, etc. reply nick7376182 17 hours agorootparentprevYou can take a prophylactic dose of doxycycline for 1-2 days I believe. If you have a lot of ticks in your area a general physician should be familiar with that protocol. reply zikduruqe 14 hours agorootparentprevPut it in the freezer as you wait the bullseye game. If you do think you have Lyme's disease then bring the tick with you to the doctor, so that they confirm it. reply neoromantique 10 hours agorootparentFirst symptom of Lyme is very distinct red oval on a place that got bit, if you see it then evaluating the tick is kinda pointless at that point. reply tohnjitor 8 hours agorootparentNot every Lyme infection has a corresponding bullseye, or it will be in a location that you can't see easily. Therr are also other tick borne illnesses that can be quite dangerous. reply vanilla_nut 18 hours agoparentprevWorks well until you end up with temperatures too hot to wear pants. Or you exercise someplace with ticks and get too sweaty for pants. Hell, I live as far north as you can go in New England, and I've found ticks on my feet after grilling dinner in my backyard. At some point I just do not care to don my anti-tick hazmat suit every time I do anything outside. reply IG_Semmelweiss 8 hours agoparentprevI can relate to this. My poor dog god bless her soul would collect ticks like a bandit in our backyard (not the Lyme ones). I enjoyed removing the little devils off my beloved dog and clear her suffering (and, killing ticks, which is surprisingly hard). I also gave her regular antitick soap baths, but it was not super effective. Not one bit or attached to any member of our family. And, we had plenty of kids in the low teens around the house. In fact, it was so \"OK\" to see ticks, that none of the adults made a fuss about them - so we grew up never being paranoid about them - in contrast to most of my east coast friends with kids I have a hard time understanding how a tick bites a fully grown adult in summer weather. Of course if you are wrapped in massive layers of clothing and you can't see it coming at you, its something different. Protip: If you take your city dog abroad -particularly a tropical area outside of the country- make sure your dog is very well tick-protected. East/West coast dogs can't handle foreign tick diseases. They don't have built-in inmunities. It only takes 1 tick from a farm/field you visited to give a deadly disease to your best friend. reply chankstein38 15 hours agoparentprevI agree I'd be more happy to use permethrin than ingest something that gets into my blood. I find a ton of ticks around where I live and have experienced a questing tick by pulling them off of my dog before they attach and then carrying them on something. Sure you're not spotting them naturally but they will quest when given the freedom to do so. If you don't kill them immediately but put them on a leaf or something they will crawl to an end of it and start! If you're just interested in seeing how it looks reply shafyy 17 hours agoparentprevWe usually use anti-tick spray on our skin and clothes when hiking in tick-infested areas, and then check for ticks in the evening. Also important to long pants and shirts if possible. reply 83 19 hours agoparentprevAlmost all coated clothes use permethrin or a similar pyrethroid. It's fairly effective but I often have issues with ticks coming off my dog much later and getting on me. I look forward to this (or a lyme vaccine) making it to market - the ticks are already pretty bad this year in the midwest. Watch for long grass along deer trails. During a particularly bad tick year (Wisconsin) I could walk along the deer trail by our swamp and just count the ticks hanging out on the ends of the green swamp grass. I did not spend much time outside that spring. reply tdb7893 18 hours agorootparentAre there already ticks? I haven't seen them yet in Illinois (though my last hike was a week and a half ago) and it's been below freezing the last few days reply 83 15 hours agorootparentBetween my wife, dog, and I we've gotten about a dozen in the last three weeks (we are frequent visitors of forests/fields around here). One of them was in February which I've never seen before. Apparently it takes several days of 10 degree weather to kill them off so I doubt it will slow down much from the last couple days of cold. Unfortunately one of them was a deer tick that dug into my armpit, so now I'm playing the \"wait and see if it's Lyme\" game :( reply alyandon 18 hours agoparentprevWhen I was a kid, my mother would lightly apply sevin dust on the lower part of my jeans before I spent a lot of time outside. Seemed to work really well at reducing the incidence of flea/tick bites and the stuff washes off quite readily. reply hammock 18 hours agorootparentI didn't know what Sevin dust is. It's the pesticide that the plant in the Bhopal disaster was making (MIC, precursor to carbaryl). Today's Sevin doesn't have the same chemicals in it that it used to though https://en.wikipedia.org/wiki/Bhopal_disaster reply stevenae 19 hours agoparentprevI saw a questing tick once, it was actually kind of adorable. They wave their arms in a gathering motion, probably helps keep them balanced. reply cmrdporcupine 19 hours agorootparentkill. reply projektfu 15 hours agoparentprevI don't know if the ticks would consume the drug on clothing. It is meant to be consumed in a blood meal. That said, it is largely well tolerated in dogs and cats. reply oblib 15 hours agoprevI live in rural area of the Ozarks where there are a lot of ticks. Treating shoes and clothes with permethrin is the most effective way to keep them off you. And doing \"tick checks\" when you come inside your home is a habit here. Getting them off you fast is the best way to keep from getting lyme disease. And either burning off or grinding up and piling up all the leaves to make compost this time of year is the best way to get them out of your yard. I learned this the hard way. Here's a video of how bad they can get here and one of my first attempts at trying to get rid of them. It didn't really work very well: https://youtu.be/TFVDv8swzxQ?si=C4R064iTgRjdvSwv reply ben7799 14 hours agoparentYour video is crazy. I have never seen anything like that up here in New England. That towel has more ticks on it than I've seen in my life, and we're not exactly a low-Lyme area. Good luck! reply below43 14 hours agoparentprevI know it's not the intent, but this video is a very good advert for pesticides. reply oblib 12 hours agorootparentI did end up nuking the yard with pesticide that year. Since then I've ground up the leaves this time of year and pile them up to compost them. In spring and summer I bag grass clippings and stuff those into the pile of composting leaves and it's amazing how hot those clippings get. Hot enough to roast any ticks in there for sure. So for the past few years we've had almost no ticks at all during the warm season. And I haven't had to use any pesticides. reply nicoburns 6 hours agorootparentHave you considered keeping some tick predators (e.g. chickens)? reply kstrauser 6 hours agorootparentAnd possums! When I learned 1) how utterly harmless possums are and 2) how voraciously they love to eat ticks, they became my favorite backyard visitors. If you find a possum in your yard, leave ‘em be and count yourself lucky. reply kstrauser 6 hours agoparentprevSeconded. I grew up in southwest Missouri. When we came in from the woods or fields, the question was how many ticks you’d have to pull off you. It was guaranteed you’d have at least a couple. I’m glad Lyme disease wasn’t as pervasive when I was a kid. At 2+ tick bites a day, every day, all summer, we couldn’t have beat those odds. reply nazca 19 hours agoprevI have hunting dogs that will pickup hundreds of ticks if not protected. I use Bravcto (Fluralaner) on them now, which is extremely effective. Hopefully lotilaner is similar with humans. Most of the older generation of anti-tick meds have pretty substantial side effects and poor efficacy. reply cptaj 19 hours agoparentI've been using Bravecto and Nexgard for the past 4 years and it really should be emphasized how much of a seismic shift these meds have been. I've had dogs all my life and live in a very tick-prone area. Nothing ever really worked, to be honest. It was a constant battle of attrition. I had to spray the house with nasty chemicals every few months cause that was pretty much the only thing that kind of tipped the balance against the ticks. I frankly don't know how we didn't get Lyme disease, we were exposed for decades. Since these new meds appeared, ticks have completely disappeared from our property. They only provide 1-3 months of protection, but they're so effective at eradicating the parasite population that I've only had to use 3 pills in the past 4 years. reply diggan 18 hours agorootparentThere might be external effects happening at the same time here. I grew up on a island with a lot of ticks. Being a kid and spending time on fields and in forests, we constantly had to remove ticks before heading home. But now 2 decades later, visiting the island again as an adult and expecting having to do the same after walking around the forest, we didn't find a single tick on ourselves, when it would easily have been a couple of ticks each in the before-days. So many parameters being different though, so hard to reach any conclusion, maybe my blood is less attractive, maybe we weren't physically intensive enough, maybe the wrong season, but maybe there are other chemicals at play too that wasn't there before. reply oldstrangers 19 hours agoparentprevIsoxazoline class drugs still come with a lot of risks for pets. https://www.fda.gov/animal-veterinary/animal-health-literacy... reply pgcudahy 18 hours agorootparentNot at normal doses. The therapeutic window is narrow, but they're used in millions of animals and we're not seeing an epidemic of seizures. https://www.merckvetmanual.com/toxicology/insecticide-and-ac... reply oldstrangers 18 hours agorootparentMerck, makers of Bravecto, say isoxazoline class drugs are safe... That's probably not a great source. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7738705/ The consolidated FDA, Project Jake and EMA findings (Table 8) showed notable differences between survey populations regarding the percentage of neurological toxicity and serious AE, and fatal effects. Statistical analysis of these serious AE showed highly significant differences between the findings of the Project Jake survey and those reported by the FDA and EMA. While the number of death and seizure AE reported by the EMA was 7 to 10 times higher than those reported to the FDA, the reported responses for the Project Jake survey for death and seizures fell in between those of the FDA and EMA but aligned more closely with the EMA results. Furthermore, the number of reported death and seizure AE for lotilaner and spinosad were considerably higher than suggested with respect to their product labelling for potential neurological effects (Table 2 and and8).8). But yes, the drug is \"generally\" safe for use. It's still worth being aware of the potential risks. reply ahepp 16 hours agorootparentThe link you posted has a notice right at the top saying the article has been corrected. It links to a corrigendum adding a conflict of interest disclosure stating > A Class Action lawsuit related to the use and safety of isoxazoline parasiticides was filed on December 27, 2019 in New Jersey, while this manuscript was undergoing peer review. One of the article's co‐authors, Valerie Palmieri, is the Plaintiff. [PALMIERI, et al. v. INTERVET INC, Case No. .2:19‐cv‐22024‐JMV‐AME (D. N. J.).]” reply oldstrangers 16 hours agorootparentCorrect, a conflict of interest for sure, but it's still a peer reviewed paper regardless. reply projektfu 15 hours agorootparentIt has several red flags, IMO. #1: Authorship is a big one, and the conflict of interest developing during the review process shows that these are not disinterested researchers. #2: The type of study is a nearly worthless type, in that it has no real statistical control and is just asking people to report on things on the internet. What ends up happening with these studies is that people self-recruit by word of mouth. Survey respondents may have been asked by other participants to register adverse events, and survey respondents may have never given the medication to a pet or seen an adverse event. There is no controlling for that apparent in the study. #3: The survey instrument is supposed to be in the appendix and it is not, yet it is not described. Their recruitment process is described only as \"distributed electronically by mail throughout the United States to veterinarians, veterinary clients, pet caregivers/owners, kennel club groups and on social media sites between August 1 and 31, 2018.\" There are lots of complaints about the adverse-event reporting system, the worst is that adverse events are merely enumerated from reports and there is no real way to put them into a statistical study. This is just getting another enumeration of events and putting a different denominator under them. In my prescribing I have seen only one adverse event worth reporting: a dog was heartworm positive and received ivermectin, doxycycline and afoxolaner at the same time. It had a transient episode of low blood pressure treated with fluids. For a drug that, in their denominator, gives 80+% adverse reactions, that is very surprising. So, the study doesn't really pass the smell test. There have been other drugs that have caused adverse reactions in significant numbers of patients. We stop using them immediately. The difference is very noticeable. reply pgcudahy 13 hours agorootparentprevOMG, I went down the rabbit hole on this paper and it is bonkers. Their methodology is amateur hour. It's just an uncontrolled non-validated survey sent out by an activist group. What is their list of survey recipients based on? What were the demographic differences between responders and nonresponders? It's the science equivalent of a political push-poll (https://en.wikipedia.org/wiki/Push_poll) The first author is a business executive who launched a huge class-action against Merck without disclosing it in this paper, the only veterinarian in the authors has been cited for practicing without a license (https://www.ocregister.com/2021/10/26/founder-of-hemopet-in-...) and the senior author is an orthopedic surgeon with no relation to the field and has maybe one other publication. Come on, this is not a serious paper. reply rozap 11 hours agorootparentprevThey are absolutely not safe at normal doses. They killed my 2 year old dog. There are thousands of dogs out there that experienced seizures from this class of drugs. She had seizures within hours of giving her the stuff, and was dead within 48 hours. I'll happily die on this hill of looking like an internet crank ranting about drug companies, but it was a traumatizing experience for my wife and I, and even worse for our dog. Fuck Merck, they can rot in hell. reply oldstrangers 8 hours agorootparentIt's weird that regardless of how many different ways you try to tell this to people they're still more than happy to give the benefit of the doubt to $307 billion dollar pharma company. reply kstrauser 6 hours agorootparentIt’s about risk assessment. It would be awful to have one of the small percent of dogs that have bad reactions to those medicines. It’s also bad to have dogs crawling with ticks. Where I grew up, having dogs get sick from tick bites was fairly common. Ever seen a collie with its ears packed with swollen ticks? That has to be miserable. The risk of the medicine was less than the risk of the bites in our case. reply cmrdporcupine 18 hours agorootparentprevIf you browse around pet forums, or go wild with Google, you'll find all sorts of anecdotal reports of people who claim their pets died (with all sorts of terrifying symptoms etc) soon after taking these kinds of drugs. Take from that what you want. It's anecdotal and non-scientific. But it concerns me enough that we keep our dogs on Nexgard only during the peak of tick season and not year round like the vets seem to want to push. There are also known genetic differences in dogs that cause some to find various anti-parasiticals (esp heartworm) to be toxic to some herding breeds (border collies, aus. sheperds). We had our border collies tested for this before putting them on medications. (https://vcacanada.com/know-your-pet/multidrug-resistance-mut...) That said, genetic diversity is higher in dogs than it is in humans. reply zdragnar 15 hours agorootparentI was the same way, until one of my dogs got Lyme's disease. It is so, so bad. We've also had a few warmer than usual weeks this winter, and deer ticks have been out for a month now, if not longer. I'm at the point where I'm probably going to have them on something year round. reply cmrdporcupine 15 hours agorootparentSorry to hear. Yeah this weird winter is super unusually warm where I am too, though we've fallen back into seasonal normals in the last week. I've been keeping an eye out for ticks, but so far nothing. I've only ever seen deer ticks here, never black legged, but they're definitely in the area. reply DowagerDave 17 hours agoprevWhat's super interesting about Lyme disease is that the cycle is larval and nymphal ticks get infected from (typically) a rodent, then if they feed on a deer, not only does the deer not get infected, it appears to cure the tick. Also female ticks do not pass this on to their offspring. reply faitswulff 20 hours agoprev> Thanks to climate change and exploding deer populations, ticks are expanding their ranges—and carrying diseases with them. We’ve had our first run ins with ticks in our suburban Chicago backyard recently. They were never an issue before, and never literally just outside the door. I’m sure the unseasonably warm start to the year is to blame. reply op00to 19 hours agoparentLook into \"tick tubes\", which you scatter around your yard. Tick tubes have permethrin impregnated cotton inside them, which mice collect and use as bedding. Mice are a critical part of the tick life cycle. The permethrin kills ticks that feed on the mice, and over a season or two you can seriously eliminate infestations of ticks without indiscriminate pesticide spraying. The good thing is, we're just approaching the best time to apply tick tubes, early spring to summer. reply daemonologist 18 hours agorootparentInteresting, especially since it seems this product could be DIYed for _very_ cheap. I do wonder if we're going to end up with permethrin-resistant super ticks though. reply eindiran 17 hours agorootparenthttps://en.wikipedia.org/wiki/Knockdown_resistance I don't know if this has appeared in ticks but it has appeared in mosquitos. reply pavel_lishin 19 hours agorootparentprevI've used this in the suburbs, and anecdotally it doesn't seem like any small critters are picking up the cotton at all. I guess I should be happy that I don't have any mice or rats in the area? reply op00to 18 hours agorootparentI don’t think rats will touch the tubes, and it’s totally possible there are no mice around. I had the same experience the first season I tried. I moved the location around the second season and noticed they were empty after a few months. I hide some near a wood pile, along fences underneath plants. I just ordered more. It makes me feel better even if it’s just placebo. reply pavel_lishin 14 hours agorootparentTrue enough - they're cheap enough that I'm not worried about wasting money. reply bmitc 19 hours agoparentprevI was under the understanding that any animal with fur or feathers can carry ticks. It's probably also an issue that smaller animals that eat ticks, like possums, are getting squeezed out. Ever since I found out that ticks carry a disease that makes you allergic to meat, I became extra worried about them. However, despite living in a major tick area, I've luckily never seen one. I just make it a point to wear long pants and sleeves when working in the yard and then immediately shower afterwards. reply nucleardog 17 hours agorootparentIf you don’t mind some chemical treatments, a really effective combo is permethrin and picaridin. Pick a set of outerwear to be your yard work clothing. I’ve got a pair of overalls, a light long sleeve shirt, and a hat. Treat them with permethrin. This doesn’t repel ticks and mosquitos, but can actually incapacitate or kill them on contact. Tuck your pants in your boots, and sleeves in your gloves if you’re wearing any. Every time before you go out, spray yourself (particularly any exposed skin) with the picaridin repellent. I live in the middle of a forest that’s sitting on wetlands. The bugs are out of control here. I haven’t found a tick on me (not in me, on me at all) in years. I also get very, very few mosquito bites. (If you don’t want to go this crazy… just long pants tucked in and some picaridin repellent before you go out is all my wife does and she’s had similar results, though spends less time outside and less wandering through dense forest and stuff.) reply alphazard 14 hours agoprev> The experimental pill that Tarsus Pharmaceuticals is testing is a formulation of lotilaner, a drug that paralyzes and kills parasites by interfering with the way that signals are passed between their nerve cells. https://en.wikipedia.org/wiki/Lotilaner Couldn't find a good description of mechanism of action on Wikipedia. I also have nerve cells that need to communicate and damaging them just to spite a tick seems counterproductive. reply rozap 11 hours agoparentNot sure about the effects of lotilaner as described in the article, but flurolaner is the active ingredient in bravecto, a flea and tick medicine for dogs. There's a class action against the manufacturer as thousands of dogs have gotten seriously ill or killed from the drug. My 2 year old collie (no previous health issues) was poisoned and died after we gave her bravecto. She had several seizures within hours of giving her the dose and was dead within 48 hours. I'm surprised that similar sounding drugs are making their way to humans. I'm not a chemist and I'm not sure how similar lotilaner and flurolaner are, but damn if I'm not suspicious. reply tazu 9 hours agorootparent> My 2 year old collie (no previous health issues) was poisoned and died after we gave her bravecto. She had several seizures within hours of giving her the dose and was dead within 48 hours. That's awful and way too young. I hope the class action has a good outcome. reply quatrefoil 11 hours agoparentprevI mean, we had this stuff for pets for a long time. The basic idea is that yes, it's a lipid-soluble neurotoxin. Because it is fat-soluble, it spreads throughout various tissues and lingers for a good while, which is these pet formulations last for weeks or months. And yes, it is technically a mammal neurotoxin too, although our bodies are better at compartmentalizing and managing the risk. Similarly, you don't drop dead if exposed to caffeine or nicotine, even though they are insect neurotoxins produced by plants to kill bugs. But it cracks me up that we are flipping out about herbicides such as glyphosate or 2,5-D - both of which are plant growth regulators that have no real mechanism to cause obvious harm in animals - but we're a lot more flippant with actual animal neurotoxins such as permethrin. reply hollerith 11 hours agorootparentA lot of the worry about glyphosate is about its effect on microbes in the human gut. reply mattwdelong 17 hours agoprevI bought property in 2021 with significant tick pressure. I’ve pulled dozens of ticks off me in the first couple years. The past two years I’ve been spraying Heterorhabditis bacteriophora and have seen significant improvement in that time. I have a tractor with a boom sprayer, which helps with large coverage. Obviously this is not a variable you can control in the wild, but for me, it has been super helpful on private land. reply test1235 17 hours agoparenthttps://en.wikipedia.org/wiki/Heterorhabditis_bacteriophora \"Heterorhabditis bacteriophora is a species of entomopathogenic nematode known commonly as beneficial nematodes. They are microscopic and are used in gardening as a form of biological pest control.\" That's fascinating. Have you noticed any knock-on effects? E.g. fall in the number of other wildlife which might normally feed on those insects? reply mattwdelong 17 hours agorootparentNothing of note. I still see plenty of turkeys, frogs and possums. I spray the beneficials so that it does not harm other beneficial insects and I assume if there is a healthy population of insects in general, there is still enough food source for wildlife. I can’t think of anything that prefers ticks as their main diet. reply uticus 16 hours agorootparent> I can’t think of anything that prefers ticks as their main diet. not 'main diet,' but can confirm free range guineas, chickens, etc help with the tick pressure. reply nntwozz 12 hours agoprevI'm still waiting for nootkatone products to take off after reading about it in The Ultimate Hang back around 2011. \"The active ingredient, nootkatone, is found in Alaska yellow cedar trees (also known as the Nootka cypress), some herbs, and citrus fruits. Biologists in CDC's Division of Vector-Borne Diseases have found nootkatone to be an effective repellent and insecticide for use against ticks and mosquitoes.\" It's super effective and completely natural without any drawbacks, but apparently hard to synthesize on a commercial scale. reply coderenegade 11 hours agoprevI grew up in an area with pervasive cattle ticks. I've pulled them off of myself, and pulled thousands from our dogs. My father claims to have once seen a lizard with a tick on its eyeball. Our solution was a bit of sulfur powder in the dog's food, and around the edges of the verandah, which was a home remedy suggested by the grey beards in the area. We never had any problems after that. reply h00k 19 hours agoprevThere was a vaccine on the market but was discontinued in 2002 by its manufacturer. I don't know reasons why. Pfizer currently has a Lymes vaccine study that I have some personal knowledge on that should be wrapped up in 2025: https://www.pfizer.com/news/press-release/press-release-deta... > About the VALOR trial VALOR is an ongoing randomized, observer-blind, placebo-controlled Phase 3 trial which has enrolled 9,437* participants 5 years of age and older to receive VLA15 or a saline placebo (1:1 ratio). As part of the primary series, participants receive three doses of VLA15 within the first year at months 0, 2 and 5-9, and one booster dose 9-12 months after completion of the primary immunization.5 The final primary series vaccination for participants occurs just before the peak Lyme disease season for the region. Participants will be followed for the occurrence of Lyme disease. The trial is conducted at sites located in areas where Lyme disease is highly endemic across the U.S., Canada and Europe and has enrolled volunteers with a cleared past infection with Borrelia burgdorferi as well as Borrelia burgdorferi naïve volunteers. reply icegreentea2 18 hours agoparentLYMErix was withdrawn because there were concerns it was inducing arthritis. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2870557/ There is lab data suggesting an interaction between LYMErix and certain genotypes potentially inducing arthritis, but the initial trial and post market surveillance data did not show any meaningfully elevated arthritis rates. However, post market surveillance was definitely hampered by the limited market uptake, fueled partly by the side effect concern. Genetic testing/screening should be significantly cheaper today, and the higher prevalence of ticks as a concern would probably mean that LYMErix could probably be a viable product today, even if its arthritis side effect profile was real, as long as it was combined with screening. reply SigmundA 18 hours agoparentprevIt was around the time when vaccine and Autism mania was at its peak: > Despite the lack of evidence that the complaints were caused by the vaccine, sales plummeted and LYMErix was withdrawn from the U.S. market by GlaxoSmithKline in February 2002, in the setting of negative media coverage and fears of vaccine side effects. The fate of LYMErix was described in the medical literature as a \"cautionary tale\"; an editorial in Nature cited the withdrawal of LYMErix as an instance in which \"unfounded public fears place pressures on vaccine developers that go beyond reasonable safety considerations.\" The original developer of the OspA vaccine at the Max Planck Institute told Nature: \"This just shows how irrational the world can be ... There was no scientific justification for the first OspA vaccine LYMErix being pulled. https://en.wikipedia.org/wiki/Lyme_disease#LYMErix reply bilsbie 19 hours agoprevHas anyone looked into giving these to mice and deer? That would kill off deer ticks really quickly. reply sunshine_reggae 19 hours agoprevHow do you get the tick to swallow the pill? reply sideshowb 17 hours agoparentI read the headline as implying the humans died... reply Pikamander2 18 hours agoparentprevVery carefully. reply bee_rider 14 hours agoprevIt is really impressive that they managed to get the ticks to eat the pills. reply kstrauser 6 hours agoparentYou have to put it in their little mouths and then rub their throats until they swallow it. reply shrubble 20 hours agoprevIvermectin has been tested against certain ticks as well: https://pubmed.ncbi.nlm.nih.gov/34656045/ reply salad-tycoon 17 hours agoparentBeen trying to figure out what to do with my liter of ivermectin. Thanks! Supposedly essential oils also kill ticks. I bought some yard sprays in addition to getting some tick hunting birds. reply joe8756438 13 hours agoprevI’m outside a lot. Live on a farm in mid-atlantic US. I’m not opposed to chemical solutions, but I have a pretty effective mechanical approach to ticks [1]. Basically it takes advantage of a tick’s inclination to walk upward. Tuck your pants into socks and shirt into pants. Wear colors that contrast with the tick body and CHECK YOURSELF! 1. https://hedgerider.farm/blog/ticks-go-up/ reply projektfu 15 hours agoprevI tried to interest any researcher in using an isoxazoline as a method of malaria control by offering it to people infected with malaria to protect their villages. The idea would be that mosquitoes that bite infected persons would die, reducing the population of infected mosquitoes. Nobody wrote me back. If you are studying malaria and are interested in this approach, please run with it. I won't be sore. reply mwpmaybe 12 hours agoparentThey've been doing that with ivermectin in some parts of the world. reply nyjah 20 hours agoprevI’m in a rural area. For the last 3 years, we’ve had ticks. It was bad last year. Supposed to be really bad this year. I’m spraying permethrin this year. Unless someone here has a better idea.. reply sarchertech 19 hours agoparentTick tubes. Permethrin sprayed directly will kill every beneficial insect as well and will just wash away after the first rain. Tick tubes are toilet paper rolls filled with permethrin soaked cotton balls. Rodents collect the cotton balls to use as nesting material which kills the nymph ticks that live on them before they can mature into adults. You put then out in early spring and again in summer. You can either make them yourself (much cheaper) or buy them ready made. reply xofer 18 hours agoparentprevChickens and other fowl, if allowed to free range for a couple of hours a day, will completely solve the problem. reply ecommerceguy 15 hours agoprevIts so crazy, and I've lived in rural areas, been hunting in Ozark woods, I've never gotten bit by a tick. A friend of mine was literally covered in them one day after shooting stuff, he put one on my hand and it jumped off. Needless to say he was jealous. reply syedkarim 14 hours agoparentI thought ticks could not jump or fly. reply whyenot 13 hours agoprevSign me up! As a botanist I used to work in some areas where it would not be unusual to remove hundreds of ticks from my field clothes in a day. Inevitably one or two would slip through. reply deanc 19 hours agoprevThis will be game-changing if it works for the Nordics. Here in Finland, ticks are an ongoing nuisance carrying lyme and also TBE (which can be vaccinated against). They are almost unavoidable if you go into nature during the warmer months and walk past any bushes, or long grass. Finland is also covered in forest too, and they can often be found falling from trees on top of hammocks and tents. reply e40 14 hours agoprevAre ticks a new phenomenon? What about in the 1800s? Did explorers get them? How did they deal with them? reply flurdy 14 hours agoparentI would suspect in earlier times they were not aware of why they got sick or had a fever. I think people are just not aware of how frequently ill the population was in general several hundred years ago, especially in cities, though not tick related. reply cmrdporcupine 18 hours agoprevIt seems to me that what we want is something like this but spread through the deer and mouse populations instead. Because in the case of lyme spreading ticks, their (complicated) lifecycle involves both of those populations. Deer carry huge populations of ticks. If drops of food laced with this kind of thing were given to deer, I'd expect it to drop the tick populations significantly. Without risk of human side effects. That and we need to cull deer populations and encourage predators of mice generally. reply BinaryBuddha 13 hours agoprevAs in... it also kills humans??? reply cyberax 13 hours agoprevI grew up in an area where tickborne encephalitis is endemic. For those who don't know, tickborne encephalitis is a delightful viral disease, with 5% lethality rate and a real chance of neurological damage in case of recovery. Oh, and the virus also persists throughout the whole life and can re-activated by antibiotics or other kinds of immune stress. Our trips to the woods looked like a visit to a BSL-4 lab. Tightly buttoned shirts, long sleeves, checking each other after a visit, etc. Fuck ticks. reply inglor_cz 19 hours agoprevThere is natural human immunity to ticks, though fairly rare. In those individuals, the immune system detects a biting tick and kills it. https://www.businessinsider.com/acquired-tick-resistance-why... https://www.statnews.com/2023/07/31/acquired-tick-resistance... That would be a superpower worth having. Perhaps a vaccine could confer this sort of immunity to us non-resistant folks? reply voisin 19 hours agoprevHaving to take a pill every few weeks seems like a major impediment to use. I could see certain hikers or forestry workers taking these pills but for your average few weekends a season camper, this isn’t the right solution given the cadence with which you need to take the pill. What about the vaccine that was near approval a few decades ago? reply alt227 19 hours agoparentSounds about the same as needing to take Malaria pills when going near the equator. reply voidmain0001 18 hours agorootparentI live in an hot spot of tick borne Lyme disease (had Lyme once so far) so I would have to take these constantly as I'm constantly outside and I'm not hiking or in the woods. Ticks are everywhere here and with the lack of snow this winter they were active during the winter too. reply voisin 19 hours agorootparentprevRight, but the average person would go on a trip requiring these pills very, very sporadically. Imagine going camping or hiking irregularly throughout the year but needing to take these pills 20+ times? Seems crazy to me when there was an effective vaccine at one point. reply happypumpkin 18 hours agorootparentTicks carry a lot more than Lyme though, and many of the diseases they carry can have long-term or life-long effects. Even with a Lyme vaccine I think it'd be worth it to take these if you're gonna be in a tick-heavy area. https://en.wikipedia.org/wiki/Powassan_virus https://en.wikipedia.org/wiki/Rocky_Mountain_spotted_fever https://en.wikipedia.org/wiki/Tularemia https://en.wikipedia.org/wiki/Babesiosis https://en.wikipedia.org/wiki/Alpha-gal_syndrome reply resoluteteeth 16 hours agorootparentprev> Right, but the average person would go on a trip requiring these pills very, very sporadically. Imagine going camping or hiking irregularly throughout the year but needing to take these pills 20+ times? Couldn't you just take it a few days before you go camping or hiking each time? reply objektif 19 hours agoparentprevTaking a pill day before you hike does not sound like too bad imo. I would looove to be able to not care about getting red meat allergy while hiking. reply 83 19 hours agoparentprevI would love to have this pill available and don't see this as an issue. The ticks numbers really spike for about a month in the spring when they come out in force which is when I'd probably try to be on something like this. When spring dries out and turns to summer ticks are far less common here (Wisconsin) so you probably wouldn't be on it. Sometimes you'll get a little uptick in the fall but spring is the only time I'm really concerned about it. reply cmrdporcupine 19 hours agoparentprevYeah I live on a rural property and I'd be uncomfortable taking something like this constantly through the season. So far I have success keeping the grass short, discouraging deer on the property (tick magnets), and keeping the (two) dogs on Nexgard. If they got worse, I'd get myself some guinea fowl to annoy the neighbours with. reply 1letterunixname 15 hours agoprevGood start as another layer of defense with DEET, clothing, and such. The next iteration should be a pill that causes ticks to avoid humans altogether. Bonus points if there were a pill that also causes mosquitos to avoid humans. reply verisimi 16 hours agoprev> Tick-killing pill shows promising results in human trial Surely they could have given the treatment to ticks rather than humans? :) reply lupusreal 20 hours agoprev> In most cases, a tick has to be attached for around 36 to 48 hours before the bacteria can be transmitted That's news to me. reply thinkingtoilet 20 hours agoparentI live in Western Massachusetts and I can confirm this is objectively false. We have a ton of ticks here. You won't get lyme if it's on you for a few hours but if you get one on you and go to sleep you can get it by the time you wake up. It's more the longer it's on the more likely it is, but you absolutely can get it if it's been attached for only 12-18 hours. reply thehappypm 19 hours agorootparentHow do you know? I also live in a tick area and typically when i have one tick on me, there are more if i look around for them. reply xkcd-sucks 17 hours agorootparentI realize there is confirmation bias here, but ticks always seem to cause pain and inflammation after being attached for a bit reply thinkingtoilet 16 hours agorootparentprev>How do you know? I've seen it happen multiple times, once to myself. reply pwenzel 19 hours agoparentprevChronic lyme patient here, been bit twice. The first time a deer tick was attached to me for roughly 6-8 hours. I got really sick. Here's a couple pictures next to a ruler and a penny to show how tiny the tick was: https://imgur.com/7YpvzRb.jpg https://imgur.com/b5sUUO3.jpg reply bilsbie 19 hours agorootparentAny treatments you’ve found to help? It’s a disaster trying to find info online. reply voidmain0001 18 hours agorootparentDoxycycline worked for my Lyme infection. reply pwenzel 14 hours agorootparentprevI have a local doctor in my area that specializes in Lyme care. Doxycycline, Minocycline, herbs, supplements, and sleep. I have also taken the path of acupuncture and medical cannabis for supplemental pain management. Overall I'm doing better now! reply goda90 20 hours agoparentprevTick gut bacteria takes awhile, but tick borne viruses that multiply in their salivary glands, like Powassan virus, are much faster. 15 minutes transmissions. reply TheCraiggers 20 hours agoparentprevIt's one of those \"the risk goes up the longer it's attached\" type things. I can't find studies, but I presume it looks like a bell curve. I know there's also a much higher risk of contracting diseases if you can't remove the tick in a careful way. If you smoosh it, or otherwise cause it enough distress that it regurgitates while still attached to you, that will also increase your chances. If you live in tick areas, get a tick kit, people. They're cheap. Don't do what my dad did and use pliers. reply bigbillheck 20 hours agorootparent> It's one of those \"the risk goes up the longer it's attached\" type things. I presume it looks like a bell curve. These two statements contradict each other. reply mjhay 19 hours agorootparentNot if the GP is describing the rate of change of the probability of transmission as a bell curve. We can assume 0% transmission when the bite first occurs, and assume that chance gradually plateaus at some point. That would look like an S-shaped logistic curve, whose derivative is bell-shaped. reply thehappypm 19 hours agorootparentprevIt doesn’t— Most tick bites take 36-48 hours before you get infected (bell hump). But sometimes you get infected quickly (left tail) and sometimes it takes longer (right tail). reply happyopossum 13 hours agorootparentThat'd be an s-curve, as it's always going up - a bell curve goes up then down. reply thehappypm 9 hours agorootparentThey’re the same thing; the bell curve is probability of being infected at time X specifically, and the S curve is the cumulative sum, that is, the probability you’ll have been infected by day X reply macintux 11 hours agorootparentprevI suppose it depends on whether you’re charting the cumulative or the “instant” odds. reply CharlesW 20 hours agoparentprevI assume they're citing the CDC: https://www.cdc.gov/lyme/transmission/index.html reply xkcd-sucks 17 hours agoparentprevCentral MA, also can confirm this is false. Possibly RMSF or something other than Lyme, but you can definitely get sick from non engorged nymphs. reply jajko 20 hours agoparentprevIts well known here for past decade. Maybe having wife working as a doctor helps, but this info is freely available. Also, if you ever interacted with any doctor with suspicion of a tick bite, this is first thing they ask. If you are outdoorsy type, this is should be around top of your list, just like knowing inside & out of say various mosquito-borne diseases and their spread vectors when traveling around. reply zzzeek 17 hours agoprevthere's a vaccine for Lyme. I'd like that. not a pill that is literally poison, putting deet on our skin is bad enough. reply josephcsible 15 hours agoparentThere are a lot more tick-borne diseases than just Lyme. reply zzzeek 7 hours agorootparentof course, but Lyme is most prevalent and, I'd like vaccines for rocky mountain etc. as well, and there are efforts towards each (Well probably not for the alpha gal sensitivity caused by lone star ticks). there was an actual ready to go vaccine for Lyme, LYMERix, which was pulled because it was not profitable. That's the problem there, which is that vaccines should be made available even if they are not \"profitable\". reply amelius 19 hours agoprev> The experimental pill that Tarsus Pharmaceuticals is testing is a formulation of lotilaner, a drug that paralyzes and kills parasites by interfering with the way that signals are passed between their nerve cells. Who knows what effect this has on signals between our own nerve cells. Yikes. reply pgcudahy 18 hours agoparentThey target gamma-aminobutyric acid chloride channels (GABACl) that aren't present in mammals. There is a relatively narrow therapeutic index in animals, but generally felt to be safe at approved doses. They're given to millions of animals, so we'd probably know if there was a problem. reply mtlmtlmtlmtl 18 hours agoparentprevLotilaner is selective for mite GABA receptors. No activity has been found for mammalian receptors even at doses far above the ones in clinical use. reply xkcd-sucks 17 hours agoparentprevIt's easier and more plausible than a rat poison that doesn't hurt people; harder and less plausible than an antibiotic that doesn't hurt people; ultimately the proof is in the pudding. Personally I would not try this in a long lasting formulation, only something oral that washes out completely in a few hours, unless it had been on the market for quite a long time reply amelius 17 hours agorootparentPeople also believed that pesticides like Roundup only affect insects. However, after a few decades it is becoming clearer that there is a link with Parkinson's. See e.g.: https://parkinsonscare.org.uk/pesticides-and-herbicides/ reply diggan 19 hours agoparentprevI think existing anti-tick chemicals works the same way, at least from my amateur understanding. Flumethrin (which is used as anti-tick in the collar we use for our dogs) targets the nervous system of parasites, which I understand to be the same approach as what Tarsus Pharmaceuticals is testing. reply seattle_spring 13 hours agoparentprevWe probably know a lot about how it does or does not affect humans. \"Just asking questions\" from an unscientific perspective is why we have people saying things like \"GMOs eat the lining of your stomach!\" reply beefman 19 hours agoprevWe should think bigger: gene drive to eradicate ticks. reply jncfhnb 19 hours agoparentThe effort with mosquitos didn’t work. Forcing a gene with negative impact on survivability is touch reply coryrc 52 minutes agorootparentWhere has it actually been tried? reply ed_balls 11 hours agorootparentprevmosquitos are hard, but with ticks there are a few different vectors of attack. My understanding is that ticks first need to bite a rodent that is infected to spread Lyme, so maybe gene editing on rodents? reply w10-1 16 hours agoprev [–] No, no, no. Why bother killing the tick after a tick bite has delivered Lyme's Borrelia burgdorferi spirochete? Even if it's effective at killing ticks, it won't stop Lyme transmission. And for it to be effective at killing ticks, the human has to be completely steeped in this GABA channel blocker paralytic. The eye drops offer no real safety guarantees because the dosages are no where near comparable. The human study of surface skin had no data on bites and a short observation period of 6 weeks (roughly the 11-day half-life x 5 half-lives to wash out the dose?). ---- The reported cost for ophthalmic 10ml solution of 0.25% lotilaner/Xdemvy for mites is ~$2k. Reported adverse effects in dogs of lotilaner from 2013-2018 were only ~250, compared to 4k-18K for other drugs (no data on prevalence/usage). It works as a GABA channel inhibitor (neurologic paralytic), specific to mites and perhaps ticks. No effect found in mammalian cells at ~11K/daily dosage for the ophthalmic drops. Human blepharitis study had ~400 people in treatment arm over 6 weeks, ~50% effective. Half-life of 11 days. No reported reversal agent. So if it is toxic, you're out of luck. The optical solution is a tiny amount in the eyes, so little systemic exposure risk. The tick pill involves much more systemic exposure via blood, so the eye drop experience says almost nothing about risk to humans. I imagine the exposure to the tick from a blood bite is 10-1000x what it is from walking on the skin, so it's not clear to me this would stop lyme's Borrelia burgdorferi from being transmitted with the initial bite. So: no. reply streptomycin 15 hours agoparent [–] https://www.uptodate.com/contents/what-to-do-after-a-tick-bi... > Even if a tick is attached, it must have taken a blood meal to transmit Lyme disease. At least 36 to 48 hours of feeding is typically required for a tick to have fed and then transmit the bacterium that causes Lyme disease. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tarsus Pharmaceuticals is working on a pill that kills ticks in humans, displaying positive outcomes in initial trials with the ability to guard against Lyme disease for around a month.",
      "The pill includes lotilaner, focusing on ticks instead of the Lyme disease-causing bacteria, indicating possible defense against other tick-borne illnesses.",
      "Although further studies are necessary to validate its efficiency in preventing Lyme disease, the pill exhibits fast tick-killing capabilities and might be crucial in addressing the escalating problem of tick-borne diseases in the United States."
    ],
    "commentSummary": [
      "Methods of tick prevention and identification are discussed, such as permethrin-treated clothing and Seresto collars for pets.",
      "Concerns regarding the toxicity of chemicals like permethrin to cats are addressed.",
      "The risks of tick-borne diseases like Lyme, prevention strategies like tick tubes and new medications, a new Lyme disease vaccine, and gene editing to eradicate ticks are all covered in the discussion."
    ],
    "points": 245,
    "commentCount": 176,
    "retryCount": 0,
    "time": 1710765949
  },
  {
    "id": 39742188,
    "title": "The Evolution of AI in Software Development",
    "originLink": "https://www.sheshbabu.com/posts/thoughts-on-the-future-of-software-development/",
    "originBody": "Thoughts on the Future of Software Development Large Language Models (LLMs) caused a huge stir in the creative circles when they were able to generate images, text and code. Initially the results were quite hilarious with drawings of people with messed up hands, hallucinating incorrect facts and code. But things are slowly and steadily getting better. Before the advent of these models, the main argument against automating these tasks was that machines can’t think creatively. Now this argument gets weaker by the day. Where do we go from here? The downside of trying to think about some vague problem like predicting future is that your thoughts get muddled and it’s hard to think clearly. So we need to come up with frameworks and analogies for us to lean on. Framework: Software Development Capability Level Software development is not just about writing code. The image people have of programmers is a person sitting in a dark room looking at computers and furiously typing code. Though coding all day sounds very appealing, most of software development time is spent on communicating with other people or other admin work instead of just writing code: Gathering requirements from business users Refining these requirements so they can be modeled as code Talking with other team members like Designers and Product Managers to visualize the solution and coming up with a plan of attack Working with other developers to come up with a technical design and refining it Setting up infrastructure, configuration, boilerplate etc. Actually writing some code Debugging, trying to understand other people’s code, writing documentation, etc. Deploying to production Firefighting production issues … and so many more tasks So saying things like “AI will replace Developers” requires “AI” to be competent in all the above tasks and not just writing code. So saying things like “AI will replace Developers” requires “AI” to be competent in all the above tasks and not just writing code. But looking at the list above, it looks like some of these tasks can also be automated in future but not yet. How do we frame this thought? The world of self-driving cars come up with a way to classify the level of automation. It has discrete levels and goes all the way from no automation to partial automation to full automation. I find this very useful for many reasons: It clearly describes what the current technology is capable of It prevents us from thinking in black and white - it’s not about human driver vs AI driver where AI fully replaces human drivers, it’s possible to have gray areas where human drivers are assisted by AI in things like emergency braking, lane centering etc. How does such classification look like to AI driven software development? The lowest tier would be what we had before - no AI involved in work. Of course we had other types of automations like compilers, build processes, etc., but these are not AI, these were human written deterministic automation. The next level is what we have now - developers using ChatGPT or GitHub Copilot to assist them. They use it for things like writing tests, boilerplate code, refactoring, understanding code/errors etc. It’s like talking with a fellow developer over chat whom you can ask questions and get some help from, but they don’t have access to your machine so they can’t create files, run build commands or deploy to production. The highest level would be like delegating part of your project or the entire project to a developer. These “AI coders” would take in the requirements, write the code, fix errors and deploy the final product to production. I assumed were still many months away from this happening, but was proved wrong with the Devin demo - even though it can only perform simple development tasks now, there’s a chance that this will improve in future. Apart from what the AI model is capable of, we should ask think in terms of how accurate the solutions are. Initially these models were prone to hallucinations or you need to prompt them in specific ways to get what you want. This adds friction to adoption and most people give up on AI assistants at this point. But this is also improving, the newer models don’t need that level of prompt engineering. Also, the models should be able to “learn” by browsing the web instead of relying on their training data. This is important as new versions of libraries and programming languages get introduced. Framework: Outsourced Software Development Now that we’ve established the capabilities, how would these influence the team or organizational structure? Companies do software development in multiple ways: Fully inhouse Mostly inhouse with few vendors Mostly vendors with few inhouse Fully vendors In a way, we can think of AI coders as outsourced software vendors/consultants. Some companies use them a lot and some don’t as much. Irrespective of the composition, I believe it’s always important to have an inhouse team oversee their work. This is to make sure vendor’s output is aligned with your organization’s long term goals. Of course, you can solve this via contracts, but they usually only apply to a specific vendor or a project, and you can’t enforce long terms goals using this method. It’s always better to have at least a small inhouse team who can guide the vendors. Similarly, even when AI coders can be rented out like EC2 instances, it will be beneficial to have an inhouse team of Software Developers to oversee their work. Framework: Software Development Is Modeling Complexity If we’re talking about solving business problems, let’s take sometime to talk about the elephant in the room - Excel. It’s a well known secret that the world runs on Excel, and more than 1 Billion people use it. It provides a very low barrier to entry for business users who want to organize data, perform data analysis, or automate some process. However, we can’t use Excel for complex business workflows as it doesn’t have features like granular access control, ability to integrate with unsupported systems, testability, reusability, or just vendor lock-in etc. The same can be said for “Low Code” solutions like Power Automate, etc. Coming back to the original question, would business users be able to use AI coders to create these complex workflows without the help of software developers? Would business users be able to use AI coding tools to create these complex workflows without the help of software developers? If you think about it, Excel and Low Code tools have existed for many decades, so why does the Software Development profession still exist? It goes back to thinking of Software Development as just writing code. For complex problems, we need people who can effectively manage these complexity and translate the business problems from real world domain to digital models. In other words, if you’re able to build a wooden shed from YouTube tutorials without the help of a Civil Engineer, doesn’t mean you can/should do the same for a 10 story building. If you go about learning how to do this properly, then you slowly become a Civil Engineer! It’s just a matter of whether you’re willing to put in the time to learn this properly or hire an experienced Engineer to do it for you. So whether these people are using Excel or the latest AI coder, if they’re modeling complex logic, they’re still software developers in my opinion! They’re just using different tools to express the business requirements - spreadsheet formulas vs code vs prompts. Framework: Size Of The Pie Most of the anxiety surrounding this topic assumes the size of the market for Software Development remains the same - AI coders will slowly take “market share” away from humans. From the previous section, we know that the market size of “solving business problems” is much much bigger than just Software Development. So there’s no reason to believe that Software Development will disappear anytime soon. Framework: Formal Business Logic Definition Business logic must always be defined in an unambiguous format. This is why programming languages, even though they use English words like “if”, “switch” etc., are very particular about what these words mean and won’t work if you use the wrong words. If you think about it, the same goes for Excel formulas or Low Code flows. In future, even if the AI coders could generate a software product from instructions given in conversational English, I believe there would still be an underlying formal definition of the business logic generated in the backend. It might look very different from the languages and frameworks we use today, but a formal definition of business logic sounds a lot like “code”. Until AI coders can start generating these business logic from conversational English in a deterministic manner, there would still be a need for people who can understand the code it generates in the backend and make changes if necessary. These people would be Software Developers. Conclusion In summary, I believe there would still be a market for Software Developers in the foreseeable future, though the nature of work will change and the tools we would use might be very different from what we have now. Tagged under: Development, LLM Published on: 18 Mar 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39742188",
    "commentBody": "Thoughts on the Future of Software Development (sheshbabu.com)226 points by rkwz 23 hours agohidepastfavorite352 comments JohnFen 18 hours ago> In summary, I believe there would still be a market for Software Developers in the foreseeable future, though the nature of work will change This is precisely what I dread. When it comes to software development specifically, the parts that the AI cheerleaders are excited about AI doing are exactly the parts of the job that I find appealing. If I wanted to be a glorified systems integrator, I would have been doing that job already. The parts that the author is saying will still exist are the parts I put up with in order to do the enjoyable and satisfying work. So this essay, if it's correct, explains the way that AI threatens my career. Perhaps there is no role for me in the software development world anymore. I'm not saying that's bad in the big picture, just that it's bad for me. It increasingly appears that I've chosen the wrong profession. reply warbled_tongue 18 hours agoparentThis resonates strongly with me. I don't want to describe the painting, I want to paint it. If this is indeed where we end up, I don't know that I'll change professions (I'm 30+ years into it), but the joy will be gone. It will truly become \"just a job\". reply sanity 17 hours agorootparentI remember back in the 80s I had friends who enjoyed coding in assembly and felt that using higher-level languages was \"cheating\" - isn't this just a continuation of that? reply falcor84 17 hours agorootparentYeah, that's a good way of looking at it. We gradually remove technical constraints and move to a higher level of abstraction, much closer to the level of the user and the business rather than the individual machine. But what's the endpoint of this? There will probably always be a need for expert-level troubleshooters and optimizers who understand all the layers, but for the rest of us, I'm wondering if the job wouldn't generally become more product management than engineering. reply rented_mule 16 hours agorootparentI'm not sure that there is an endpoint, only a continuation of the transitions we've always been making. What we've seen as we transitioned to higher and higher level languages (e.g., machine code → macro assembly → C → Java → Python) on unimaginably more powerful machines (and clusters of machines) is that we took on more complex applications and got much more work done faster. The complexity we manage shifts from the language and optimizing for machine constraints (speed, memory, etc.) to the application domain and optimizing for broader constraints (profit, user happiness, etc.). I think LLMs also revive hope that natural languages (e.g., English) are the future of software development (COBOL's dream finally be realized!). But a core problem with that has always been that natural languages are too ambiguous. To the extent we're just writing prompts and the models are the implementers, I suspect we'll come up with more precise \"prompt languages\". At that point, it's just the next generation of even higher level languages. So, I think you're right that we'll spend more of our time thinking like product managers. But also more of our time thinking about higher level, hard, technical problems (e.g., how do we use math to build a system that dynamically optimizes itself for whatever metric we care about?). I don't think these are new trends, but continuing (maybe accelerating?) ones. reply diego_sandoval 2 hours agorootparent> machine code → macro assembly → C → Java → Python The increase in productivity, we can all agree on, but a non-negligible portion of HN users would say that each one of those new languages made programming progressively less fun. reply flohofwoe 1 hour agorootparentI think where people will disagree is how much productivity those steps brought. For instance I think the step from machine code to macro assembler is bigger than the step from a macro assembler to C (although still substantial), but the step from C to anything higher level is essentially negligible compared to the massive jump from machine code to a 'low level high level' language like C. reply nopinsight 15 hours agorootparentprev> But also more of our time thinking about higher level, hard, technical problems (e.g., how do we use math to build a system that dynamically optimizes itself for whatever metric we care about?). It’s likely that a near-future AI system can suggest suitable math and implement it in an algorithm for the problem the user wants solved. An expert who understands it might be able to critique and ask for a better solution, but many users could be satisfied with it. Professionals who can deliver added value are those who understand the user better than the user themselves. reply rented_mule 12 hours agorootparentThis kind of optimization is what I did for the last few years of my career, so I might be biased / limited in my thinking about what AI is capable of. But a lot of this area is still being figured out by humans, and there are a lot of tradeoffs between the math/software/business sides that limits what we can do. I'm not sure many business decision makers would give free rein to AI (they don't give it to engineers today). And I don't think we're close to AI ensuring a principled approach to the application of mathematical concepts. When these optimization systems (I'm referring to mathematical optimization here) are unleashed, they will crush many metrics that are not a part of their objective function and/or constraints. Want to optimize this quarter's revenue and don't have time to put in a constraint around user happiness? Revenue might be awesome this quarter, but gone in a year because the users are gone. The system I worked on kept our company in business through the pandemic by automatically adapting to frequently changing market conditions. But we had to quickly add constraints (within hours of the first US stay-at-home orders) to prevent gouging our customers. We had gouging prevention in before, but it suddenly changed in both shape and magnitude - increasing prices significantly in certain areas and making them free in others. AI is trained on the past, but there was no precedent for such a system in a pandemic. Or in this decade's wars, or under new regulations, etc. What we call AI today does not use reason. So it's left to humans to figure out how to adapt in new situations. But if AI is creating a black-box optimization system, the human operators will not know what to do or how to do it. And if the system isn't constructed in a mathematically sound way, it won't even be possible to constrain it without significant negative implications. Gains from such systems are also heavily resistant to measurement, which we need to do if we want to know if they are breaking our business. This is because such systems typically involve feedback loops that invalidate the assumption of independence between cohorts in A/B tests. That means advanced experiment designs must be found that are often custom for every use case. So, maybe in addition to thinking more like product managers, engineers will need to be thinking more like data scientists. This is all just in the area where I have some expertise. I imagine there are many other such areas. Some of which we haven't even found yet because we've been stuck doing the drudgery that AI can actually help with. [cue the song Code Monkey] reply SgtBastard 8 hours agorootparent> AI is trained on the past. Yes, unless models are being live fine-tuned, but generally yes. >What we call AI today does not use reason I don’t think this is correct- I think it’s more accurate to say it reasons on its priors rather than from first principles. For the most part, I agree with the rest of your post. reply ptx 9 hours agorootparentprevI don't think COBOL's dream was to generate enormous amounts of assembly code that users would then have to maintain (in assembly!) and producing differently wrong results every time you ran it. reply bitwize 3 hours agorootparentIt may not have been the dream, but the reality is many COBOL systems have been binary-patched to fix issues so many times that the original source may not be a useful guide to how the thing actually works. reply vczf 16 hours agorootparentprevThe endpoint is that being a programmer becomes as obsolete as being a human \"calculator\" for a career. Millions, perhaps billions of times more lines of code will be written, and automated programming will be taken for granted as just how computers work. Painstakingly writing static source code will be seen the same way as we see doing hundreds of pages of tedious calculations using paper, pencil, and a slide rule. Why would you do that, when the computer can design and develop such a program hundreds of times in the blink of an eye to arrive at the optimal human interface for your particular needs at the moment? It'll be a tremendous boon in every other technical field, such as science and engineering. It'll also make computers so much more useful and accessible for regular people. However, programming as we know it will fade into irrelevance. This change might take 50 years, but that's where I believe we're headed. reply withinboredom 14 hours agorootparentYet, we still have programmers writing assembly code and hand-optimizing it. I believe that for most software engineers, this will be the future. However, experts and hobbyists will still experiment with different ways of doing things, just like people experiment with different ways of creating chairs. An AI can only do what it is taught to do. Sure, it can offer unique insights from time to time, but I doubt it will get to the point where it can craft entirely new paradigms and ways of building software. reply vczf 14 hours agorootparentYou might be underestimating the potential of an automated evolutionary programming system at discovering novel and surprising ways to do computation—ways that no human would ever invent. Humans may have a better distribution of entropy generation (i.e. life experience as an embodied human being), but compared to the rate at which a computer can iterate, I don't think that advantage will be maintained. (Humans will still have to set the goals and objectives, unless we unleash an ASI and render even that moot.) reply withinboredom 11 hours agorootparentAI, even in its current form can provide some interesting results. I wouldn’t underestimate an AI, but I think you might be underestimating the ingenuity of a bored human. reply ta1243 11 hours agorootparentHumans aren't bored any more [0]. In the past the US the US had 250 million people who were bored. Today it has far more than than scrolling through instagram and tiktok, responding to reddit and hacker news, and generally not having time to be bored Maybe we'll start to evolve as a species to avoid that, but AI will be used to ensure we don't, optimising far faster than we can evolve to keep our attention [0] https://bigthink.com/neuropsych/social-media-profound-boredo... reply nradov 12 hours agorootparentprevPerhaps, but evolutionary results are difficult to test. They tend to fail in bizarre, unpredictable ways in production. That may be good enough for some use cases but I think it will never be very applicable to mission critical or safety critical domains. Of course, code written by human programmers on the lower end of the skill spectrum sometimes has similar problems... reply vczf 11 hours agorootparentIt doesn't seem like a completely different thing to generate specifications and formally verified programs for those specifications (though I'm not familiar with how those are done today). reply jimbokun 6 hours agorootparentprevI mean, I don’t even like programming with Spring because what all of those annotations are doing is horribly opaque. Let alone mountains of AI generated code doing God knows what. I mean Ken Thompson put a back door into the C compiler no one ever found. Can you imagine what an AI could be capable of? reply beacon294 12 hours agorootparentprevASI? reply vczf 12 hours agorootparentArtificial Super-Intelligence reply jimbokun 6 hours agorootparentprevIt will equally eliminate the need for all scientists and engineers. And every other human occupation. reply goatlover 5 hours agorootparentI don't believe that's going to happen. If it were, humans would have stopped playing chess. But not only do lots of people still play chess, people making a living playing chess. There are YT channels devoted to chess. The same thing will be true of almost all sports, lots of entertainment, and lots of occupations where people prefer human interaction. Bar tenders and servers could be automated away, but plenty like to sit at a bar or table and be served by someone they can talk to. I have a hard time seeing nurses being replaced. Are people going to want the majority of their care automated? I also don't know what it means to completely remove humans from all work. Who is deciding what we want done? What we want to investigate or build? The machines are just gong to make all work-related decisions for us? I don't believe that. It would cease being our society at that point. Which brings up the heart of the matter. Why are we trying to replace ourselves? It's our civilization, automation are just tools we use to be more productive. It should make our lives better, not remove us from the equation. My guess is the real answer is it will make some people obscenely rich, and give some governments a significant technical advantage over others. reply Aerbil313 2 hours agorootparentChess is not something you make new discoveries in anymore, not something that results in a product that people pay for. Poor analogy. reply kaba0 14 hours agorootparentprev> The endpoint is that being a programmer becomes as obsolete as being a human \"calculator\" for a career. Yeah, the same time the singularity happens, and then your smallest problem will be eons bigger than your job. But LLMs can’t solve a sudoku, so I wouldn’t be too afraid. reply jacobr1 14 hours agorootparentThey are pretty close. LLMs can write the code the solve a sudoku, or leverage an existing solver, and execute the code. Agent frameworks are going to push the boundaries here over the next few years. reply kaba0 13 hours agorootparent> LLMs can write the code the solve a sudoku It’s literally part of its training data. The same way it knows how to solve leetcode, etc. reply thfuran 11 hours agorootparentprev>There will probably always be a need for expert-level troubleshooters and optimizers who understand all the layers There's already so many layers that essentially no one knows them all at even a basic level, let alone expert. A few more layers and no one in the field will even know of all the layers. reply financypants 13 hours agorootparentprevIs a more generic version of this argument be that there will always be a need for smart/experienced people? reply tejohnso 16 hours agorootparentprevSeems so. Those friends did have to contend with the enjoyable part of their job disappearing. Whether they called it cheating or not is doesn't diminish their loss. reply jjmarr 13 hours agorootparentIt didn't; there are still many roles for skilled assembly programmers in performance-critical or embedded systems. It's just their market share in the overall world of programming has decreased due to high-level programming languages; although better technology has increased the size of the market that might have demands for assembly. reply bugbuddy 10 hours agorootparentI am not skilled in these areas so I am very scared. I am going to go back to school to get a nursing degree because it is guaranteed to not be disrupted by the disrupters like now where the disrupters are disrupting themselves. Despite the personal risks of a healthcare job, it will bring me so much more peace of mind. reply wyclif 8 hours agorootparentI'm afraid it's naive to think that nursing is not going to get disrupted by AI. Seems like robotics is going to massively impact medical caregiving in the near future. reply grugagag 8 hours agorootparent> robotics is going to massively impact medical caregiving in the near future Not in the near near future. Do you know anything about nursing? The field will require some hard changes for robots to replace nurses, and the robots will need licenses reply xnx 8 hours agorootparentprevEven without robotics, many jobs like nursing (or construction) that require training will be able to be accomplished with much less training + a live computer coach that can give context-specific directions. reply bugbuddy 7 hours agorootparentThis is how we get to Idiocracy. Everyone is now relying on AI and become stupid because of it. reply xnx 7 hours agorootparentDefinitely a risk. There's also an upside to having a 1-on-1 tutor with limitless patience and knowledge. reply jimbokun 6 hours agorootparentprevThey could move into compilers or VMs or low level performance profiling, where those skills are still very important. reply elicksaur 17 hours agorootparentprevThe difference is that your friend has a negative view of others than the OP is not presenting. They’re just stating their subjective enjoyment of an activity. reply spit2wind 15 hours agorootparentprevDavid Parnas has a great take on this: \"Automatic programming always has been a euphemism for programming with a higher level language than was then available to the programmer. Research in automatic programming is simply research in the implementation of higher-level languages. Of course automatic programming is feasible. We have known for years that we can implement higher-level programming languages. The only real question was the efficiency of the resulting programs. Usually, if the input 'specification' is not a description of an algorithm, the resulting program is woefully inefficient. I do not believe that the use of nonalgorithmic specifications as a programming language will prove practical for systems with limited computer capacity and hard real-time deadlines. When the input specification is a description of an algorithm, writing the specification is really writing a program. There will be no substantial change from out present capacity. The use of improved languages has led to a reduction in the amount of detail that a programmer must handle and hence to an improvement in reliability. However, extant programming languages, while far from perfect, are not that bad. Unless we move to nonalgorithmic specifications as an input to those systems, I do not expect a drastic improvement to result from this research. On the other hand, our experience in writing nonalgorithmic specifications has shown that people make mistakes in writing them just as they do in writing algorithms.\" Programming with AI, so far, tries to specify something precise, algorithms, in a less precise language than what we have. If AI programming can find a better way to express the problems we're trying to solve, then yes, it could work. It would become a matter of \"how well the compiler works\". The current proposals, with AI and prompting, is to use natural language as the notation. That's not better than what we have. It's the difference between Euclid and modern notation, with AI programming being like Euclidean notation and current programming languages being the modern notation: \"if a first magnitude and a third are equal multiples of a second and a fourth, and a fifth and a sixth are equal multiples of the second and fourth, then the first magnitude and fifth, being added together, and the third and sixth, being added together, will also be equal multiples of the second and the fourth, respectively.\" a(x + y) = ax + by You can't make something simpler by making it more complex. https://web.stanford.edu/class/cs99r/readings/parnas1.pdf reply nonethewiser 10 hours agorootparentprevI do think it’s basically the same. Its further on the same continuum of: Natural language/Machine code. reply jgwil2 8 hours agorootparentI don't really think it's a continuum. There is a continuum of abstraction among programming languages, from machine code to Java/Python/Haskell or whatever, but natural language is fundamentally different: it's ambiguous, ill-defined. Even if LLMs generate a lot of our code in the future, somebody is going to have to understand it, verify its correctness, and maintain it. reply nonethewiser 7 hours agorootparentNatural language, python, c, assembly The distance isn’t the same between them, but each one is more abstracted than the next. Natural language can be ambiguous and ill defined. Because the compiler is smarter. Just like you don’t have to manage memory in Python, except it abstracts a lot more. The fact is that this very instant you can compile from natural language. reply jimbokun 6 hours agorootparentThere is a vast gulf between natural language and the other 3, which are fundamentally very similar to each other. reply goatlover 5 hours agorootparentprevLLMs can generate code, but they still need to be prompted correctly, which requires someone who knows how to program beyond toy examples, since the code is going to have to be tested and integrated into running code. The person will need to understand what kind of code they're trying to generate, and whether that meets the business requirements. Python is closer to C (third generation programming language). Excel is a higher level example. It still takes someone who knows how to use Excel to do anything meaningful. reply __loam 15 hours agorootparentprevI think it's a fundamentally different thing, because AI is a leaky abstraction. I know how to write c code but I actually don't know how to write assembly at all. I don't really need to know about assembly to do my job. On the other hand, if I need to inspect the output of the AI to know that it worked, I still need to have a strong understanding of the underlying thing it's generating. That is fundamentally not true of deterministic tools like compilers. reply MisterTea 17 hours agorootparentprevBoiler plate being eliminated by syntactic sugar or runtime is not the same thing. Sure that made diving in easier but it didn't abstract away the logic design part - the actual programming part. Now the AI spits out code for you without thinking about the logic. reply xandrius 17 hours agorootparentprevGreat point. I think this will weed out the people doing tech purely for the sake of tech and will bring more creative minds who see the technology as a tool to achieve a goal. reply brailsafe 17 hours agorootparentIndeed, can't wait for the day when technical people can stop relishing in the moments of intimate problem solving between stamping out widgets, and instead spend all day constantly stamping out widgets while thinking about the incredible bullshit they'll be producing for pennies. Thanks boss! reply xandrius 6 hours agorootparentIt feels that people commenting on this post are forgetting that tools have evolved since the times of punch cards or writing only in pure assembly. I personally wouldn't have enjoyed being that kind of programmer as it was a tedious and very slow process, where the creativity of the developer was rather low as the complexities of development would not allow for just anyone to be part of it (my own assumption). Today we have IDEs, autocomplete, quick visual feedback (inspectors, advanced debuggers, etc.) which allow people who enjoy creating and seeing the results of their work as opposed to purely be typing code for someone else. So, I don't get why people jump straight to thinking that adding yet another efficiency tool would destroy everything. To me it seems to make developing simpler applications something which doesn't require a computer science degree, that's all. reply jimbokun 6 hours agorootparentThat’s like saying Shakespeare couldn’t be productive as a writer because he didn’t have a word processor. reply xandrius 4 hours agorootparentSo are you saying that you would rather live in a society where only lucky people could participate in a given field than making it accessible to more people? reply Aperocky 16 hours agorootparentprevExcept that's what low code is today. You'll have to describe it in such detail that you might as well as paint it yourself. Maybe it will abstract away setting up the paint and brush and the canvas, that part I'm fine with though. reply hnick 10 hours agorootparentFrom the perspective of the programmer, true. Not necessarily from the perspective of the manager/customer, who can say in broad terms what needs to be done, and the programmer-black-box spits something out. reply PlunderBunny 9 hours agorootparentThe manager/customer is going to be very disappointed that the computer can't just \"do what I ask it to\". reply karmakaze 15 hours agorootparentprevFunny you should phrase it this way. I know you mean prompts as description, but I would currently prefer declaring/describing what I want in a higher-level functional way rather than doing all the stateful nitty-gritty iterations to get it done. Some folks want to do manual memory management, or even work with a borrow checker, I'm good for most purposes with gc. The question is always what's your 'description' language and what's your 'painting' language? I see the same in music: DJ's mix and apply effects to pre-recorded tracks, others resample on-the fly, while some produce new music from samples, and others form a collage from generated soundscapes, etc. It's all shades of gray. reply the_arun 10 hours agorootparentprevAgreed. But isn't this what is happening over a period for all manual jobs? I mean people used to carve wood. Now, machines do that with more precision & speed. The same goes for laying roads, construction & other professions. All niche jobs will become mundane chores. I don't know if it is good or bad. Because humans always find a way to cultivate something new. reply diego_sandoval 2 hours agorootparentYeah, but those were boring jobs. Programming is fun. I'm only half joking. reply jimbokun 5 hours agorootparentprevOnly until machines are better at everything humans can do. reply ActionHank 18 hours agoparentprevAlso if there are fewer humans involved in the code production there is a lot of room for producing code that \"works\", but is not cohesive or maintainable. Invariably there will be a point at which something is broken and someone will need to wade through the mess to find why it's broken and try to fix it. reply Taylor_OD 17 hours agorootparentThis is the future imagined by A Fire Upon the Deep and its sequel. While less focused on the code being generated by ai, it features seemingly endless amounts of code and programs that can do almost anything but the difficulty is finding the program that works for you and is safe to use. To some extent... This is already the world we live in. A lot of code is unreadable without a lot of effort or expertise. If all code was open sourced there would almost certainly be code written to do just about anything you'd like it to. The difficulty would be finding that code and customizing it for your use. reply Normalcy294812 16 hours agorootparentThanks for the Book Title. It looks like an interesting read. reply scubbo 10 hours agorootparentCaution - lots of people like to talk about this \"code archeology\" idea as if it's a central driving point of the book, whereas in fact it's mentioned once in passing in the prologue and is never again relevant to the story. Don't get me wrong, it's still a decent book on its own merits - but don't go into it expecting that to be the main point of the book (I did, and disappointed as a result). reply broast 17 hours agorootparentprevCertainly many of us here already have a good amount of experience debugging giant legacy spaghetti code-bases written by people you can't talk to, or people who can't debug their own code. That job may not change much. reply bluefirebrand 18 hours agorootparentprevNah, you just throw it out and have the AI generate an all new one with different problems! reply Jensson 17 hours agorootparentI really look forward to all programs now having new strange bugs every release. They already do, but I expect AI to do that more at first. reply ipaddr 16 hours agorootparentThe hacking opportunities will be endless. Feeding AI the exploit will be new. reply falcor84 17 hours agorootparentprevI don't know if AIs will ever get really good at QA in general, but I do think that AIs can get quite good quickly at regression testing. reply CuriouslyC 16 hours agorootparentprevThat's how bads use GPT to code. The right way is to ask GPT to break the problem down into a bunch of small strongly typed helper functions with unit tests, then ask it to compose the solution from those helper functions, also with integration tests. If tests fail at any point you can just feed the failure output along with the test and helper function code back in and it will almost always get it right for reasonably non-trivial things by the second try. It can also be good to provide some example helper functions/tests to give it style guidelines. reply bluefirebrand 14 hours agorootparentIf you're already doing all of this work then it's trivial to actually type all the stuff in yourself Is GPT actually saving you any time if it can't actually do the hard part? reply CuriouslyC 13 hours agorootparentIt's not really \"all this work,\" once you have good prompts you can use them to crank out a lot of code very quickly. You can use it to crank out thousands of lines of code a day that are somewhat formulaic, but not so formulaic that a simple rules based system could do it. For example, I took text document with headers for table names and unordered lists for table columns, and had it produce a database schema which only required minor tuning, which I then used to generate sqlmodel classes and typescript types. Then I created an example component for one entity and it created similar components for the others in the schema. LLMS are exceptionally good at this sort of domain transformation, a decent engineer could easily crank out 2-5k lines/day if they were mostly doing this sort of work. reply c-cube 9 hours agorootparentNow your description of \"good prompts\" to reuse has created an abomination in my mind. I blame you. The abomination: prompts being reused by way of yaml templating, a Helm chart of sorts but for LLM prompts. The delicious combination of yaml programming and prompt engineering. I hope it never exists. reply croo 12 hours agorootparentprevYou know with GPT you can do these steps in a language you are not familiar with and it will still work. If you don't know some aspect of the language or it's environmental specifics you can just chat until you find out enough to continue. reply thetacitman 13 hours agorootparentprevHow do I know if a problem needs to be broken down by GPT, and how do I know if it broke the problem down correctly? What if GPT is broken or has a billing error, how do I break down the problem then? reply CuriouslyC 10 hours agorootparent1. Intuition built by trial and error 2. Domain expertise backed by automated checks 3. The old fashioned way, and if your power is out you can even bust out a slide rule reply mbil 10 hours agorootparentprevMaybe I'm being overly optimistic but in a future where a model can digest hundreds of thousands of lines of code, write unit tests, and do refactors, will this even be a problem? reply gitfan86 16 hours agoparentprevI'm the opposite. I enjoy engineering and understanding systems. Manually coding has been a necessary to build systems up until now. AWS similarly was great because it provided a functional abstraction over the details of the data center. On a personal level I feel bad for the people who enjoyed wiring up small data centers or enjoyed writing GitHub comments about which lint rules were the best. But I'm glad those are no longer necessary. reply bamboozled 12 hours agorootparentPeople still wire up data centres. reply Towaway69 17 hours agoparentprevBeing a developer, I heartily agree with you. Being a human, I realise that I as a developer have put a lot of people out of a job. Those folks have had to adapt to that change. I guess now it's our time to adapt to change. At least it keeps me on my feet! reply quonn 1 hour agorootparent> I realise that I as a developer have put a lot of people out of a job. For most developers this will not be true. Most apps, websites, compilers, desktop software etc. will not have put anyone out of a job. I certainly never ever put someone out of a job. I made some peoples live easier, but their total working hours didn't shorten and they certainly did not change profession or were replaced. In fact the majority of tasks that my software was applied to would simply have been deemed impossible to do and not have been done and that would have been all there was to it. reply JohnFen 13 hours agorootparentprev> I guess now it's our time to adapt to change. I'm just saddened by the prospect that, for me, \"adapting to change\" would mean \"no longer being able to make a living doing what I actually enjoy\". that's why if this is the future, it's a career-killing one for me. Whether or not I stay in the industry, there is no future in my chosen career path, and the alternative paths that people keep bringing up all sound pretty terrible to me. My only hope is that AI will not achieve the heights that its proponents are trying to reach (I suspect this is the case). I see no other good outcome for me. reply Towaway69 13 hours agorootparentIf AI does achieve the hyped heights then we're all out of a job, regardless of what we do. Many people suffer through bullshit jobs[1] so we are privileged to have - at least for a time - done what we really enjoy and got paid for it. [1] David Graeber father of the term \"bullshit jobs\" reply ta1243 11 hours agorootparentprevWe're all sorry for the other guy when he loses his job to a machine. When it comes to your job, that's different. And it always will be different. That was 1968. Plus ca change... reply theshackleford 12 hours agorootparentprev> I'm just saddened by the prospect that, for me, \"adapting to change\" would mean \"no longer being able to make a living doing what I actually enjoy\". that's why if this is the future, it's a career-killing one for me. Ok and? You don’t think any of the others put out of their work by other forms of computing like you might’ve enjoyed their jobs? You don’t think it might have been career ending for them? reply imbnwa 17 hours agorootparentprevThe catch is that those people could, barring the AI advances we seem to be seeing, could retrain for an SWE labor market that lagged demand; that wont even be possible for devs put out of work in the future. reply cjbgkagh 15 hours agorootparentThose people who did retrain are the same devs being put out of work - which means they got hit with the setback twice and are worse off than people who started off as devs and thus only got hit once. Like the allied bomber pilots in WWII looking down below at the firestorm knowing that there is a good chance (~45%) that they too will join their fate only later. reply jacobr1 14 hours agoparentprevI suspect this is the wrong take. AI can only perform integrations when there are systems to integrate. The frontier of interesting work to be done isn't supervising an integration AI, but building out the hard components that will be integrated. Integration work itself already has been moving up the stack to low-code type tools and power-user like people over the past decade even before LLMs become the new thing. reply jayd16 17 hours agoparentprevI understand your feelings but I do also wonder if its not similar to complaining about compilers or garbage collection. I'm sure there are people that love fiddling with assembly and memory management by hand. I assume there will be plenty of interesting/novel problems no matter the tooling because, fundamentally, software is about solving such problems. reply imbnwa 17 hours agorootparentSoftware engineering as an occupation grew because of static analysis and GCs (literally why the labor market is the size that it is as we speak); the opposite appears to be the outcome of AI advances. reply glasss 17 hours agorootparentThe same happened with accountants and spreadsheet software, the number of accounting jobs grew. The actual work they performed became different. I think a similar thing is likely to happen in the software world. reply imbnwa 17 hours agorootparentTech has already learned there’s not enough real frontier left to reap the bounty of(removing zero interest rates that incentivize mere flow of capital). This stuff is being invested in to yield the most productivity at the least cost. There will either be a permanent net decrease in demand or, being so high level, most openings will pay no more than 60-70K in an America (likely with reduced benefits) where wages are already largely stagnant. reply glasss 16 hours agorootparentI think there is definitely merit to your statements. I believe the future of the average software developer job involves a very high level language, API integration, basic full stack work with a lot of AI assistance. And those roles will mostly be at small to medium businesses who can't afford the salaries or benefits that the industry has standard in the US. Almost every small business I know has an accountant or book keeper position which is just someone who had no formal education and the role is just managing QuickBooks. I don't think the need for formally educated accountants who can handle large corporate books decreased significantly, but I don't have any numbers to back that up. Just making the comparison to say I don't think the hard / cool stuff that a lot of software developers love doing is going away. But these are just my thoughts. reply samatman 17 hours agorootparentprevI'll take the other side of that bet. It's reasonable to expect that sometime relatively soon, AI will be a clear-cut aid to developer productivity. At the moment, I consider it a wash. Chatbots don't clearly save me time, but they clearly save me effort, which is a more important resource to conserve. Software is still heavily rate-limited by how much of it developers can write. Making it possible for them to write more will result in more software, rather than fewer developers. I've seen nothing from AI, either in production or on the horizon, that suggests that it will meaningfully lower the barrier to entry for practicing the profession, let alone enable non-developers to do the work developers do. It will make it easier for the inexperienced to do tasks which need a bit of scripting, which is good. reply aibot923 13 hours agorootparent> Software is still heavily rate-limited by how much of it developers can write Hmm. We have very different experiences here. IME, the vast majority of industry work is understanding, tweaking, and integrating existing software. There is very little \"software writing\" as a percentage of the total time developers spend doing their jobs across industry. That is the collective myth the industry uses to make the job seem more appealing and creative than it is. At least, this is my experience in the large FAANG type companies. We already have so much code. Just figuring out what that code does and what else to do with it constitutes the majority of the work. There is a huge legibility issue where relatively simple things are obstructed by the morass of complexity many layers deep. A huge additional fraction of time is spent on deployments and monitoring. A very small fraction of the work is creatively developing new software. For example, one person will creatively develop the interface and overall design for a new cloud service. The vast majority of work after that point is spent on integration, monitoring, testing, releases, and so on. The largest task of AI here would be understanding what is going on at both the technical layer and the fuzzy human layer on top. If it can only do #1, then knowledge workers will still spend a lot of effort doing #2 and figuring out how to turn insights from #1 into cashflow. reply Clubber 9 hours agorootparent>At least, this is my experience in the large FAANG type companies. We already have so much code. Just figuring out what that code does and what else to do with it constitutes the majority of the work. That sounds horrible. I've always sought out smaller companies that need stuff built. It certainly doesn't pay as much as SV companies but it's pretty stimulating. Sometimes being a big fish in a small pond is pretty nice. IMO, maintaining someone else's code is probably the worst type of programming job there is, especially if it's bad /disjointed code. A lot of people can make a good living doing it though. It would be nice if AI could alleviate the pain of learning and figuring out a gnarly codebase. reply visarga 17 hours agorootparentprev> I've seen nothing from AI, either in production or on the horizon, that suggests that it will meaningfully lower the barrier to entry for practicing the profession, let alone enable non developers to do the work developers do. Good observation. Come to think of it, all examples of AI coding require a competent human to hold the other end, or else it makes subtle errors. reply imbnwa 17 hours agorootparentHow many humans do you need per project though? The number can only lower as AI tooling improves. And will employers pay the same rates when they’re already paying a sub for their AI tools and the work involved is so much more high level? reply doktrin 16 hours agorootparentI don’t claim to have any particular prescience here, but doesn’t this assume that the scope of “software” remains static? The potential universe of programmatically implementable solutions is vast. Just so happens that many or most of those potential future verticals are not commercially viable in 2024. reply ElevenLathe 13 hours agorootparentExactly. Custom software is currently very expensive. Making it cheaper to produce will presumably increase demand for it. Whether this results in more or fewer unemployed SWEs, and if I'll be one of them, I don't know. reply bitwize 3 hours agorootparentprev> Making it possible for them to write more will result in more software, rather than fewer developers. Goddamnit, software developers are already writing more software than we need. I wish they'd stop. Or redirect all that energy to new problems to solve. Instead we're seeing cloud-deployed microservice architecture CRUD apps that do what systems built for mainframes with kilobytes of RAM do, only worse. We're in a glut of bad software, do you think that AI accelerating production of more of the same will make things better? reply CuriouslyC 16 hours agorootparentprevIf chatbots aren't saving you time you need to refine what you choose to use them for. They're absolutely amazing at refactoring, producing documentation, adding comments, translating structured text files from one format to another, implementing well known algorithms in newer/niche languages where repository versions might not exist, etc. On the other hand, I've mostly stopped asking GPT4 to write quickstart code for libraries that don't have star counts in the high thousands at least, and while I'll let it convert css/style objects/etc into tailwind I it's pretty bad at styling in general, though it is good at suggesting potentially problematic styles when debugging layout. reply samatman 15 hours agorootparent> you need to refine what you choose to use them for This is making assumptions about the work I do which don't happen to be valid. For example: > libraries that [...] have star counts in the high thousands at least Play little to no role in my work, and > I'll let it convert css/style objects/etc into tailwind Is something I simply don't have a use for. Clearly your mileage varies, and that's fine. What I've found is that for the sort of task I farm out to the chatbots, the time spent explaining myself clearly, showing it counterexamples when it gets things wrong, and otherwise verifying that the code is fit to purpose, is right around the time I would spend on the task to begin with. But it's less effort, which is good. I find that at least as valuable if not more so. > producing documentation Yikes. Not looking forward to that in the future. reply dartos 14 hours agorootparent> producing documentation I remember watching this really funny video where a writer, by trade, was talking about recent AI products they were exploring. They saw a \"Make longer\" button which took some text and made it longer by fluffing it out. He was saying that it was the antithesis of his entire career. As a high schooler who really didn't care, I would've loved it, though. reply ponector 13 hours agorootparentI've heard one CEO been asked about gen-ai tools to be used in the company. The answer was vague, like they are evaluating the tooling. However one good example was made: chatgpt is really good in writing mails, and in summarizing text as well. He said they don't want to have situation when sender is using chatgpt to write a fancy mail and recipient is using chatgpt to read it. However I think that it is the direction where we are going right now. reply int_19h 13 hours agorootparentThis sort of thing is already being rolled out for emails and even pull requests in some large companies. reply dartos 12 hours agorootparentprevYeah it’s good for the kinds of emails that people don’t really read or, at best, just skim over reply jprete 8 hours agorootparentIf people would just stop loading those emails up with bullshit then we wouldn't have any reason to put AI on either end of the transaction. reply CuriouslyC 14 hours agorootparentprevI was giving examples, in the hopes that you could see the trend I was pointing towards for your own benefit. You can take that and learn from it or get offended and learn nothing, up to you. Not sure why you are scared of GPT assisted documentation. First drafts are universally garbage, honestly I expect GPT to produce a better and more accurate first draft in a fraction of the time, which should encourage a lot of people who otherwise wouldn't have documented at all to produce passable documentation. reply jimbokun 6 hours agoparentprevIt’s interesting how all of these articles implicitly assume AI keeps getting more intelligent then at some point just…stops. There’s no reason to think AI won’t also take over all the parts you don’t find appealing, too. The whole point of the Singularity, is that no aspect of human work cannot be performed better by superhumanly intelligent machines. reply goatlover 5 hours agorootparentThe point of the Singularity is that it's a futurist prediction, and we all know how often people are wrong about the future. reply Braini 12 hours agoparentprevThese are exactly my thoughts. I comfort myself by thinking that it is still a while away and also not certain, but this might just be willful ignorance on my side. Because TBH, no clue yet what else I would like to (or even could) do. reply bamboozled 11 hours agorootparentWhatever you decide to do next will be automated soon after anyway…career change ? Don’t bother. Jump on the progress train reply pksebben 18 hours agoparentprevAt the risk of exposing my pom-poms, it's not the writing of the code or the design of the systems that I find the current batch of AI useful for. Probably the biggest thing that GPT does for me these days is to replace google (which probably wouldn't be necessary if google hadn't become such hot garbage). As I say this, I'm made aware of the incoming rug-pull when the LLMs start spitting SEO trash in my face as well, but right now they don't which is just the best. A close second is having a rubber duck that can actually have a cogent thought once in a while. It's a lot easier to talk through a problem when you have something that will never get tired of listening - try starting with a prompt like \"I don't want advice or recommendations, but instead ask me questions to elaborate on things that aren't completely clear\". The results (sometimes) can be really, really good. reply tomjakubowski 14 hours agorootparentFor me the principal benefit of ChatGPT is it helps me to maintain focus on a problem I'm solving, while I wait for a slow build or test suite or what ever. I can bullshit about it without annoying my coworkers with Slack messages. And sometimes I'll find the joy reveling in the chatbot's weird errors and hallucinations. I suppose my lunch is about to be eaten by all these people who will use it to automate the software engineer job away. So it goes reply ToucanLoucan 18 hours agoparentprevCall me a cynic (many have, especially on this topic) but I can't help but think that the majority of what AI will \"successfully\" replace in terms of craftsmanship is going to be stuff that would've never been produced the \"correct\" way if you will. It's going to be code created for and to suit the interests of the business major class. Just like AI art isn't really suitable for anything above hobby fun stuff like generating your D&D character's avatar, or product packaging stock photo junk or header images for LinkedIn blog posts. Anything that's actually important is going to still need to be designed, and that goes for creative work like design, and proper code-work for development too, IMO. Like sure, these AI's can generate code that works. Can they generate replacement code when you need to change how something works? Can they troubleshoot code that isn't doing what it's meant to? And if you can generate the code you want but then need to tweak it after to suit your purpose, is that... really that much faster than just writing the thing in your style, in a way you understand, that you can then change later as required? I dunno. I've played with these tools and they're neat, and I think they can be good for learning a new language or framework, but once I'm actually ready to build something, I don't see myself starting with AI generation for any substantial part of it. reply Oioioioiio 17 hours agorootparentThe question is not about what AI can do today but what we assume AI will be able to do tomorrow. All of what you wrote in your second paragraph will become something AI will be doing better and faster than you. We never had technology which can write code like this. I prompted ChatGPT to write a very basic java tool which renders an image from an url and makes it bigger on a click. It just did it. Its not hard to think further and a lot of technology is already going into this direction. Alone last week devin was showne. Gemini has a window token of 1 Million tokens. Groq shows us how it will feel to have instant response. Right now its already good enough that people with Copilot like to keep it when asked. We already now pay billions for AI daily. This means the amount of research, business motivation and money flowing into it now is probably staggering in comparision to what moved this field a few years ago. Its not clear at all how fast we will progress but i'm pretty sure, we will hit a time were every junior is worse than AI which will force people of rethinking what they are going to do. Do i hire an junior and train him/her? Or do i prefer to invest more into AI? The gap will widen and widen, a generation or a certain amount of people will stay longer and might be able to stay in development but a lot of others might just not. reply dartos 14 hours agorootparent> We never had technology which can write code like this. I prompted ChatGPT to write a very basic java tool which renders an image from an url and makes it bigger on a click. It just did it. It's worth noting, that it can do things like that because of the large amount of \"how to do simple things in java\" tutorials there are on the internet. Ask an AI to _make_ java, and it won't (and will continue to not) be able to. That's the level that AI will fail at, when things aren't easily indexed from the internet and thus much harder / impossible to put into a training set. I think the technology itself (transformers and other such statistical models) have exhausted most of their low hanging fruit by now. Sora, for example, isn't a grand innovation in the way latent space models, word2vec, or transformers are, it's just a MUCH larger model than DALLE-3. which is great! but still has the limits inherit to statistical models. They need the training data. reply ToucanLoucan 12 hours agorootparent> It's worth noting, that it can do things like that because of the large amount of \"how to do simple things in java\" tutorials there are on the internet. Much like the same points made elsewhere with regard to AI art: It cannot invent. It can remix, recombine, etc. but no AI model we have now is anywhere close to where it could create something entirely new that's not been seen before. reply Oioioioiio 43 minutes agorootparentI have not seen crabs made out of food before. What level do you think 'invention' have to be to count as something AI can't do? The only thing AI needs is a feedback loop and a benchmark/cost function. If the cost function is page impressions, thats easy. If its running unit tests based from business requirements, thats easy too. reply ToucanLoucan 12 hours agorootparentprev> The question is not about what AI can do today but what we assume AI will be able to do tomorrow. And I think many assumptions on this front are products of magical thinking that are discarding limitations of LLMs in favor of waiting for the intelligence to emerge from the machine, which isn't going to happen. ChatGPT and associated tech is cool, but it is, at the end of the day, pattern recognition and reproduction. That's it. It cannot invent something not before seen, or in our case here, it cannot write code that's never been written. Now that doesn't make it useless, there's tons of code that's being written all the time that's been written thousands of times before. But it does mean depending what you're trying to build, you will run into it's limitations pretty quickly and have to start writing it yourself. And that being the case... why not just do that in the first place? > We never had technology which can write code like this. I prompted ChatGPT to write a very basic java tool which renders an image from an url and makes it bigger on a click. It just did it. Which it did, because as the other comment said, tons of people already have. > Its not clear at all how fast we will progress but i'm pretty sure, we will hit a time were every junior is worse than AI which will force people of rethinking what they are going to do. Do i hire an junior and train him/her? Or do i prefer to invest more into AI? The gap will widen and widen, a generation or a certain amount of people will stay longer and might be able to stay in development but a lot of others might just not. I mean, this sounds like an absolute crisis in the making for software dev as a profession, when the entire industry is reliant on a small community of actual programmers overseeing tons of robot junior devs turning out mediocre code. But to each their own I suppose. reply Oioioioiio 45 minutes agorootparentMost of the time i'm not 'inventing' anything new too. I get a requirement, find a solution and the solution is 99,99999% not a new algorithm. I actually believe i never invented a new algorithm. Besides the next step is reasoning in GPT-5 and devin shows that GPTs/LLMs can start breaking down tasks. I don't mind being wrong tbh, there is no risk in it for me if AI will not take my job but i don't believe it. I do believve the progress will be better and better and AI will do more and more reasoning. It can easily try and do things 1000x fater than us, including reasoning. Its not hard to see that it will also be able to create its own examples and learn from them. reply __loam 14 hours agorootparentprevI think the question is whether we're going to plateau at 95% or not. It's possible that we just run into a wall with transformers, or they do iron it out and it does replace us all. reply rzzzt 14 hours agoparentprevRyan D. Anderson's \"We Wanna Do the Fun Stuff\" captures this very concisely: https://www.instagram.com/itsryandanderson/p/BrY0N-lH31p/ reply godelski 12 hours agoparentprevSorry, can you clarify more? I don't think I understand. The part you enjoy the most is the integrating of systems, right? If that's really your passion, I'm not sure you're in danger of losing your job to AI. AI is not great at nuance and this is exponentially more challenging than what we've done so far. I'm just assuming that since this is your passion (if I'm understanding correctly) that you see it as the puzzle it is and the complexities and uniqueness of each integration. If you're the type of person that's frustrated by low quality or quick shortcuts and not understanding the nuances actually involved, I think you're safe. I don't see AI pushing out deep thinkers and the \"annoying\" nuance devs anytime soon. I'm that kinda person too and yeah, I'm not as fast as my colleagues. But another friend (who is similar) and I both are surprised how often other people in our lab and groups we work with (we're researchers) talk about how essential GPT and copilot are to their workflows. Because neither of us think this way. I use GPT(4) almost every day, but it's impossible for me to get it to write good quality code. It's great at giving me routines and skeletons, but the real engineering part takes far more time to talk the LLM into than it does to write it (including all the time to google or even collaborate with GPT[0]). LLMs can do tough things, but their abilities are clearly directly proportional to the frequency of the appearance of those tasks. So I think it is the coding bootcamp people that are in the most danger. There are expert people that are also at risk though. These are the extremely narrow expertise people. Because you can target LLMs for specific tasks. But if your skills are the skills that define us as humans, I wouldn't lose too much sleep. I say this as a ML researcher myself. And I highly encourage everyone to get into the mindset of thinking with nuance. It has other benefits too. But I also think we need to think about how to transition into a post scarce world, because that is the goal and we don't need AGI for that. [0] A common workflow for me is actually due to the shittiness of Google. Where it overfits certain words and ignores the advanced things like quotes or NOTs. Or similarly broaching into a new high level topic. I can't trust GPT's answer, but it will sure use keywords and vernacular I don't know that enable me to make a more powerful search. (But google employees should not take away that they should push LLMs into google search but rather that search is mostly good but that same nuance is important and being too forceful and repeating 5 pages of essentially the same garbage is not working. The SEO people attacked you and they won. It looks like you let them win too...) reply maxrecursion 9 hours agorootparentThis is pretty muchy experience as well. AI is a fantastic helper, and it will make devs more productive, but it is not going to put all software devs out of work. Probably a tiny fraction of them at best. However, with recent grads flooding into IT for remote work and high pay, they could be hurting as AI reduces the need for entry level roles. Entry level was already saturated, and now it will be more saturated, with AI reducing the need for jobs. reply godelski 5 hours agorootparentYeah I remember seeing someone try to measure it and they saw improvements in productivity on all experience levels but they found that it helped novices the most and experts only a little. But this is actually something I worry about. The best way to become an expert is to do things the hard way. I've talk a lot of people linux over the years and only 3 have really learned it. Every time I teach people I give them two options: the easy way, which is just how to use it and use the gui and a bit of terminal or the hard way, which hand them the arch wiki, tell them to come back after their third failed attempt to install. Those 3 people came back, but usually did more than 3 installs, but had all been successful at some point. All 3 mentioned they understood why and then we could really talk about how to use linux, make scripts (and that scripts aren't aliases...), and so on. All 3 still are terminally terminal, years later. The thing is that humans (and even machines) learn by struggling, getting things wrong, and learning from mistakes. The struggle is part of the learning process. I've found this both in myself and whenever I teach anything, that if I just feed someone the answer (or look it up and nothing more for myself) they (I) don't end up remembering, they don't end up playing, they don't end learning how to learn. reply dist-epoch 16 hours agoparentprevMost knifes today are mass produced. But there are still knife craftsman. You could become a software craftsman/artist if you enjoy writing software. reply galdosdi 10 hours agorootparent> Most knifes today are mass produced. But there are still knife craftsman. But are there more or less knife craftsmen today than in the old days? How about more or less knife craftsmen per capita? Finally, and most importantly, if you are a budding knife crafstman -- is it easier or harder to get a job that pays the bills of a contemporary average lifestyle, today than in the old days (ie, what is the balance of supply and demand) reply javajosh 16 hours agorootparentprevThe market is different, and so is the supply. The market for artisanal cutlery is basically an art market. The programmer supply today is an approaching-standardization factory worker. There IS an art market for software, in the indie gaming space, so perhaps that will survive (and AI could actually really help individual creators tremendously). But the work-a-day enterprise developer's days are numbered. The great irony being that all the work we've done to standardize, framework-ize the work makes us more fungible and replaceable by AI. The result I foresee is a further concentration of power into the hands of those with capital enough to own data-centers with AI capable hardware; the petite bourgeoisie will shrink to those able to maintain that hardware and (perhaps) as a finishing interface between the AI's output and the human controlling the capital that placed the order. It definitely harms the value proposition of people who's main talent is understanding computers well enough to make useful software with them. THAT is rapidly commoditizing. reply mattgreenrocks 12 hours agorootparent> The great irony being that all the work we've done to standardize, framework-ize the work makes us more fungible and replaceable by AI. I mean, at some level, this is what frameworks were meant to do: give you a loose outline and do all that messy design stuff for you. In other words: commodify some amount of software design skill. And I’m not saying that’s bad. Definitely puts a different spin on the people that get mad at you in the comment section when you suggest it’s possible to build something without a framework though! reply exceptione 15 hours agorootparentprevSince AI has been trained on the generous gifts of the collective (books, code repos, art, ..), it begs the question why normal societies would not start to regulate them as a collective good. I can foresee two forces that will work against society to claim it back: - Dominance of neoliberalism thought, with its strong belief that for any disease markets will be the cure. - Strong lobby from big corporates. You don't want to intervene to early, but you have to make sure you have at least some limits before you let the winners do too much damage. The EU has to be applauded for having a critical look on what effects these developments might have, for instance which sectors will face unemployment. That is in the interest of both people and business, because the winner takes it all means economic and scientific stagnation. I fear that 90% of the worlds' data is already in the hand of just a few behemots, so there is already no level playing field (which is btw caused by aforementioned dominance of neoliberalism). reply stereolambda 14 hours agorootparentThe sectors of work that have been largely pushed out of economy in recent decades have not been defended by serious state policy. In fact there are whole groups of crucial workers, like teachers or nurses, who are kept around barely surviving in many countries. The groups protected by the state tend to be heavily organized and directly related to exploitation of natural strategic resources, like farmers or miners. There is no particular sympathy towards programmers in society, I don't think. Based on what I observe calling the mood neutral would be fair, and this is mostly because the group expanded, and way more people have someone benefiting from IT in their family. I don't see why there would be a big intervention for programmers. Artists maybe, but these are proverbially poor anyway, and the ones with popular clout tended to somehow get rich despite the business models of culture changing. I am all for copyright reform etc., but I don't see making culture public good, in a way that directly leads to more artisanal creators, as anything straightforward. This would have to entail some heavier and non-obvious (even if desirable) changes to the economic system. It's debatable if code is culture anyway, though I could see an argument for software, like Linux and other tools. > I fear that 90% of the worlds' data Don't wanna go into a tangent in this already long post, but I'd dispute if these data really reflect the whole knowledge we accumulated in books (particularly non-English) and otherwise not put into reachable and digestible formats. Meaning, sure, they have these data, they can target individual people with private stuff they have on them, but this isn't full accumulation of human knowledge that is objectively useful. reply exceptione 13 hours agorootparent> There is no particular sympathy towards programmers in society, I don't think. The concern policy makers have is not about programmers, but about boatloads of other people having no time to adapt to the massive wave these policymakers see coming. There a strong signals that anyone who produces text, speech, pictures or whatever is going to be affected by it. If the value of labor goes down, if a large part of humanity cannot reach a level anymore to meaningfully contribute, if productivity eclipses demand growth, you simply will see lots of people left behind. Strong societies depend on strong middle classes. If the middle class slips, so will the economy, so no good news for blue collar as well. AI has the potential to suffocate the organism that created it. reply javajosh 14 hours agorootparentprev>AI has been trained on the generous gifts of the collective Will be interesting to see how various copyright lawsuits pan out. In some ways I hope they succeed, as it would mean clawing back those gifts from an amorphous entity that would displace us (all?). In some ways I hope that we can resolve the gift problem by giving every human equity in the products produced by the collective value of the training data they produced. >winner takes it all means economic and scientific stagnation Given the apparent lack of awareness or knowledge of philosophy, history, or current events, it seems like a tough row to hoe getting the general public on board with this (correct) idea. Heck, we can't even pass a law overturning Citizens United, the importance of which is arguably even less abstract. When the tide of stupidity grows insurmountable, and The People cannot be stopped from self-harm, you get collapse, and the only way to survive it is to live within a pocket of reason, to carry the torch of civilization forward as best you can. reply exceptione 13 hours agorootparent> When the tide of stupidity grows insurmountable, and The People cannot be stopped from self-harm, you get collapse, Yes, people are unfortunately highly unaware of what societal ecosystem they depend on, and so cannot prioritize on what is important. These topics don't sell in media shows. reply shrimp_emoji 18 hours agoparentprevIn the limit, it's all professions. :p Software development tomorrow, $other_profession the day after tomorrow. But same; if AI starts writing code and my job becomes tweaking pre-written code, I'm planning an exit strategy. :D reply barfbagginus 17 hours agoparentprevYou will be able to speak what human needs must be fulfilled. Then the code will appear and you'll be able to meet those human needs. You, not a boss, will own the code. And if you have any needs, you will be able to speak the word and those needs will be met. You will no longer need to work at all. Whatever you want to build, you will be able to build it. What about that frightens you? reply JohnFen 13 hours agorootparentNone of that frightens me, but I also think that none of that is in the realm of reasonable possibility. reply aibot923 12 hours agorootparentprev> You, not a boss, will own the code. Developers can already deploy code on massive infrastructure today, and what do we see? Huge centralization. Why? Because software is free-to-copy winner-takes-most game, where massive economies of scale mean a few players who can afford to spend big money on marginal improvements win the whole market. I don't think AI will change this. Someone will own the physical infrastructure for economies-of-scale style services, and they will capture the market. reply Pugpugpugs 16 hours agorootparentprevWhat if we don’t hit AGI and instead the tools just get pretty good and put lots of people out of work while making the top 0.1% vastly richer? Now you’ve got no prospects, no power, and barely any money. reply galdosdi 9 hours agorootparentThe only real wealth have, at the end of the day is: your health, your relationships, and your ability and willingness to eat beans. reply empath-nirvana 20 hours agoprevI don't really think that thinking of LLMs and related technologies as \"Artificial Humans\" is the right way to think about how they're going to be integrated into workflows. What is going to happen is that people are going to be adopt these tools to solve particular tasks that are annoying or tedious for developers to do, in a way similar to the way tools like Ansible and Chef replaced the task of logging into ssh servers manually to install stuff, and aws replaced 'sending a guy out to the data center to setup a server' for many companies. And it's going to be done piecemeal, not all-at-once. Someone will figure out a way to get an AI to do _one_ thing faster and cheaper than a human and sell _that_. Maybe it's automatic test generation, maybe it's automatically remediating alerts, maybe it's code reviews. The scope of work of what a software developer does will be reduced until it's reduced to two categories: 1) Those tasks that it is still currently only possible for a human to do. 2) Those tasks which are easier and cheaper for a human to do. You don't even really need to think about LLMs as AIs or conscious or whether they pass the turing test or not, it's just like every other form of automation we've already developed. There are vast swathes of work that software developers and IT people did a few decades ago that almost nobody does any more because of various forms of automation. None of that has reduced the overall amount of jobs for software developers because there isn't a limited amount of software development to do. If you make software development less expensive and easier than people will apply it to more tasks, and software developers will become _more_ valuable and not _less_. reply visarga 19 hours agoparent> 1) Those tasks that it is still currently only possible for a human to do. 2) Those tasks which are easier and cheaper for a human to do. I agree, but \"1\" must include all tasks where a mistake could lead to liabilities for the company, which is probably most tasks. LLMs can't be held responsible for their fuckups, they can't be punished, they have no body. It's like the genie from the bottle, it will grant your three wishes, but they might turn out in a surprising way and it can't be held accountable. The same will apply for example for using LLMs in medicine. We can't afford to risk it on AI, a human must certify the diagnosis and treatment. In conclusion we can say LLMs can't handle accountability, not even in principle. That's a big issue in many jobs. The OP mentioned this as well: > even when AI coders can be rented out like EC2 instances, it will be beneficial to have an inhouse team of Software Developers to oversee their work Oversight is basically manual-mode AI alignment. We won't automate that, the more advanced an AI, the more effort we need to put in overseeing its work. reply wongarsu 17 hours agorootparent> I agree, but \"1\" must include all tasks where a mistake could lead to liabilities for the company, which is probably most tasks If you hire a junior programmer and they make a mistake, they aren't held liable either. Sure, you can fire them, but unless there's malice or gross negligence the liability buck stops at the company. The same can be said about the wealth of software currently involved in producing software and making decisions. The difficulty of suing Microsoft or the llvm project over compiler bugs hasn't stopped anyone from using their compilers. I don't see how LLMs are meaningful different from a company assuming liability for employees they hire or software they run. Even if they were AGI it wouldn't meaningfully change anything. You make a decision whether the benefits outweigh the risks, and adjust that calculation as you get more data on both benefits and risks. Right now companies are hesitant because the risks are both large and uncertain, but as we get better at understanding and mitigating them LLMs will be used more. reply robryan 13 hours agorootparentEven with a junior there is generally a logic to the mistake and a fairly direct path to improving in future. I just don't know if the next token was chosen to be x statistically is going to be able to get to that level. reply estebank 18 hours agorootparentprev\"A COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION\" — IBM slide from 1979. reply visarga 18 hours agorootparenthahaha funny Let me tell you a story - a company was using AI for invoice processing, and it misread a comma for a dot, so they sent a payment 1000x larger than expected, all automated of course because they were very modern. The result? they went bankrupt. \"Bankrupted by AI error\" might become a thing reply chromanoid 11 hours agorootparentOf course. It is like when a company goes bankrupt because they didn't establish good fire protection in their factory. Using AI automation has its risks that have to be mitigated appropriately. reply esafak 17 hours agorootparentprevSome might consider that a plus in the same way that \"you can't get fired for choosing IBM\" -- it's a way to outsource blame. reply jazzyjackson 8 hours agorootparentted nelson calls it \"cybercrud\", blaming the machine as if it has the final say on the matter, \"the system won't let me...\" reply bee_rider 18 hours agorootparentprevI think there is some under-explored issue in the liability, but I don’t know enough about business law to have a useful opinion on it. It seems interesting, though. Even if an LLM and a human were equally competent, the LLM is not a living being and, I guess, isn’t capable of being liable for anything. You can’t sue it or fire it. Doctors have to carry insurance to handle their liability. I can see why it would be hard to replace a doctor with an LLM as a result. Typically engineers aren’t personally liable for their mistakes in a corporate setting. (I mean, there’s the whole licensed Professional Engineer distinction, but I don’t feel like dying on that hill at the moment). So where does the liability “go?” I think it just gets eaten by the company somehow. They might fire the engineer, but that doesn’t make the victim whole or benefit society, right? Ultimately we’d expect companies that are so bad at engineering to get sued so often that they implement process improvements. That could be wrapped around AI’s instead of people, right? But we’re not using the humans’ unique ability to bear liability, I think? reply empath-nirvana 17 hours agorootparentprevIt's vanishingly rare that individuals have any liability or are punished for software fuckups. Maybe if someone is completely incompetent, they'll get fired, but I'm not sure that's meaningfully different than cancelling a service that doesn't work as advertised. reply politician 18 hours agorootparentprevHow do you negotiate for a salary when the role is to be ablative armor for the company? \"I'm excited to make myself available to absorb potential reputation damage for $CORP when the AI goes off the rails.\" reply cogman10 13 hours agoparentprevExactly my take. I'd further state that LLMs appear to do ok with generating limited amounts of new code. Telling them \"Here's a class, generate a bunch of unit tests\" works. However, telling them \"Generate me a todo application\" will give mixed results at best. Further, it seems like updating and changing code is simply right out of their wheelhouse. That's where I think devs will be primarily valuable. Someone needs to understand the written code for when the feature request eventually bubbles through \"Now also be able to do x\". I don't think you'll be able to point an LLM at a code repository and instruct it \"Update this project so that it can do feature X\" reply karmakaze 12 hours agoparentprevIt's also paints a narrow picture of how results are described. I suspect that there will be a lot more by example like this, and iteration on the output, except that... where the inputs/outputs are multi-modal. Everything is going to be close enough, not fully spec'ed out. Full-self-driving is only the beginning of everything. reply joenot443 13 hours agoparentprevThis is really well put and level-headed, I particularly like the comparison to AWS and in-field ops work. reply gtirloni 18 hours agoparentprevI think this is the sanest comment I've seen about LLMs. reply clktmr 20 hours agoprevAs long as there is no AGI, no software engineer needs to be worried about their job. And when there is, obviously everything in every field will change and this discussion will soon be futile. reply pizzafeelsright 19 hours agoparentI would argue future engineers should be worried a bit. We no longer need to hire new developers. I was not trained professionally yet I'm writing production code that's passing code reviews in languages I never used. I will create a prompt, validate it compiles, passes tests, have it explain so I understand it was written as expected and write documentation about the code, write the PR, and I am seen as a competent contributor. I can't pass leet code level 1 yet here I am being invited to speak to developers. Velocity goes up and cost of features will drop. This is good. I'm seeing at least 10 to 1 output from a year ago based upon integrating these new tools. reply visarga 19 hours agorootparentYeah, it sounds to me your teammates are going to pick up the tab at the end, when subtle errors will be 10x harder to repair, or you are working on toy projects where correctness doesn't really matter. reply nyrikki 18 hours agorootparentTo add to this. I was going through devin's 'pass' diffs from SWE bench. Every one I ended up tracing to actual issues caused changes that would reduce maintainablity or introduced potential side effects. I think it may be useful as a suggestion in a red-green-refactor model, but will end up producing hard to maintain and modify code. Note this one here that introduced circular dependencies, changed a function that only accepted points to one that appears to accept any geometric object but only added lines. Domain knowledge and writing maintainable code is beyond generative transformers. https://github.com/CognitionAI/devin-swebench-results/blob/m... You simply can't get past what Gödel and Rice proved with current technology. It is like when visual languages were supposed to replace programmers. Code isn't really the issue, the details are. reply ekidd 18 hours agorootparentThank you for reading the diffs and reporting on them. And to be fair, lots of humans are already at least this bad at writing code. And lots of companies are happy with garbage code so long as it addresses an immediate business requirement. So Devin wouldn't have to advance much to be competitive in certain simple situations where people don't care about anything that happens more than 2 quarters into the future. I also agree that producing good code which meets real business needs is a hard problem. In fact, any AI which can truly do the work of a good senior software engineer can probably learn to do a lot of other human jobs as well. reply nyrikki 17 hours agorootparentArchitectural erosion is an ongoing problem for humans, but they don't produce tightly coupled low cohesion code by default at the SWE level the majority of the time. With this quality of changes it won't be long until violations stack up to where further changes will be beyond any algorithms ability to unravel. While lots of companies do only look out in the short term, human programers are incentivized to protect themselves from pain if they aren't forced into unrealistic delivery times. At&t wireless being destroyed as a company due to a failed SAP migration that was largely due to fragile code is a good example. But I guess if the developer jobs that will go away are from companies that want to underperform in the market due to errors and a code base that can't adapt to changing market realities, that may happen. But I would fire any non intern programmer if they constantly did things like removing deprecation comments and introduced circular dependencies with the majority of their commits. https://github.com/CognitionAI/devin-swebench-results/blob/m... PAC learning is powerful but is still probably approximately correct. Until these tools can avoid the most basic bad practices I don't see any company sticking to them in the long term, but it will probably be a very expensive experiment for many of them. reply falcor84 17 hours agorootparentCan't we just RLHF code reviews? reply nyrikki 16 hours agorootparentRLHF works on problems that are difficult to specify yet easy to judge. While RLHF will help improve systems, code correctness is not easy to judge outside of the simplest cases. Note how on OpenAI's technical report, they admit performance on college level tests is almost exclusively from pre-training. If you look at LSAT as an example, all those questions were probably in the corpus. https://arxiv.org/abs/2303.08774 reply falcor84 15 hours agorootparent>RLHF works on problems that are difficult to specify yet easy to judge. But that's the thing, that it seems that everyone here on HN (and elsewhere) finds it easy to judge the flaws of AI-generated code, and they seem relatively consistent. So if we start offering these critiques as RLHF at scale, we should be able to bring the LLM output to the level where further feedback is hard (or at least inconsistent), right? reply ogogmad 9 hours agorootparentprev> You simply can't get past what Gödel and Rice proved with current technology. Not this again. Those theorems tell you nothing about your concerns. The worst case of a problem is not equal to its usual case. reply barrell 18 hours agorootparentprevAgreed. I use LLMs quite extensively and the amount of production code I ship from an LLM is next to zero. I even wrote a majority of my codebase in Python despite not knowing Python precisely because I would get the best recommendations from LLMs. As a frontend developer, with no experience in backend engineering in the last decade, and no Python experience, building an app where almost every function has gone through an LLM at some point, for almost 8 months — I would be extremely surprised if some of the code it generated landed in production. reply csomar 15 hours agorootparentprevMost software is already as bad as this, though. And managers won't care (maybe even shouldn't?) if the execution fairly delivers. Think of this as Facebook page vs. WordPress website vs. A full custom website. The best option is to have a full custom website. Next, is a cheaper option from someone who can put a few lines together. The worst option is a Facebook page that you can create yourself. But the Facebook page also does the job. And for some businesses, it's fairly enough. reply packetlost 15 hours agorootparentprev> I'm writing production code that's passing code reviews in languages I never used Your coworkers likely aren't doing a very good job at reviewing, but also I don't blame them. The only way to be sure code works is to use it for its intended task. Brains are bad interpreters, and LLMs are extremely good bullshit generators. If the code makes it to prod and works, good. But honestly, if you aren't just pushing DB records around or slinging HTML, I doubt it'll be good enough to get you very far without taking down prod. reply acedTrex 17 hours agorootparentprevI have yet to see either copilot or gpt4 generate code that I would come close to accepting in a PR from one of my devs, so I struggle to imagine what kind of domain you are in that the code it generates actually makes it through review. reply codeyperson 8 hours agorootparentThat you know of reply jackling 16 hours agorootparentprevWhat's your domain? reply ipaddr 16 hours agorootparentprevHonestly that sounds like a problem with the way you are managing prs. The PRs are too big or you are overly nitpicking prs on unimportant things reply supriyo-biswas 19 hours agorootparentprevTo be fair, Leetcode was never a good indicator of developer skills, though primarily because of the time pressure and the restrictive format that dings you for asking questions about the problem. reply politician 18 hours agorootparentSpeaking of Leetcode... is anyone selling a service to boost Leetcode scores using AI yet? It seems like that's fairly low hanging fruit at this point. reply ioblomov 18 hours agorootparentBased on their demos, HackerRank is doing this as part of their existing products. Which makes sense since prompt engineering will soon become a minimum requirement for devs of any experience level. reply robotnikman 15 hours agorootparentprevI have accepted using these tools to help when it comes to generating code and improving my output. However when it comes to dealing with more niche areas (in my case retail technology) it falls short. You still need that domain knowledge of whatever you are writing code for or integrating with, especially is the technology is more niche, or documentation was never made available publicly and scraped by the AI But when it comes to writing boilerplate code it is great, or when working with very commonly used frameworks (like front end javascript frameworks in my case) reply pphysch 15 hours agorootparentprev> passes tests Okay, so you are just kicking the can down the road to the test engineers. Now your org needs to spend more resources on test engineering to really make sure the AI code doesn't fuzz your system to death. If you squint, using a language compiler is analogous to writing tests for generated code. You are really writing a spec and having something automatically generate the actual code that implements the spec. reply jayd16 17 hours agorootparentprevI wonder if the reviewers are just using GPT as well. reply lainga 16 hours agorootparentprevWho's \"we\"? reply l3mure 15 hours agorootparentprevPost some example PRs. reply doktrin 16 hours agorootparentprevThis doesn’t vibe with my experience at all. We also use LLMs and it’s exceedingly rare that a non-trivial PR/MR gets waved through without comment. reply kaba0 14 hours agorootparentprevMeanwhile I’m paid for editing a single line of code in 2 weeks, and nothing less than singularity will replace me. But sure, call me back when AI will actually reason about possible race conditions, instead of spewing out the definition of one it got from wikipedia. reply adrianN 20 hours agoparentprevYou don’t have to completely replace people with machines to destroy jobs. It suffices if you make people more effective so that fewer employees are needed. reply 63 19 hours agorootparentThe number of people/businesses that could use custom software if it were cheaper/easier to develop is nearly infinite. If software developers get more productive, demand will increase reply Workaccount2 19 hours agorootparentprevOr lower the bar of successfully doing such work so that the field opens up to many more workers. Many software devs will likley have job security in the future, however those $180k salaries are probably much less secure. reply MrBuddyCasino 19 hours agorootparentprevIf software developers become more effective, demand will also rise, as they become profitable in areas where previously they weren't. The question then becomes which of those two effects outpaces the other, which is an open question. reply zeroonetwothree 19 hours agorootparentprevJust like when IDEs made programmers more effective so that fewer were needed. Oh wait, the opposite happened. reply Toutouxc 20 hours agoparentprevThis has been my cope mantra so far. I don't mind if my job changes a lot (and ideally loses the part I dislike the most — writing the actual code), and if I find myself in a position where my entire skillset doesn't matter at all, then well a LOT of people are in trouble. reply tomashubelbauer 19 hours agorootparentI have seen programmers express that they dislike writing code before and I wonder what the ratio of people who dislike it and people who like it, is. For me, writing code is one of the most enjoyable aspects of programming. reply hnick 10 hours agorootparentIt's my favourite part, except maybe debugging. I really like getting into the guts of an issue and working out why it happens which I suppose will be around for a while yet with AI code. It's a lot less fun with transient network issues and such though. reply Vinnl 19 hours agorootparentprevThe worst future is where there still are plenty of jobs, but all of them consist of talking to an AI and hoping you use the right words that gets them to do what you need it to. reply weweweoo 13 hours agorootparentNot really. As long as there is no universal basic income, any job with decent salary beats unemployment. The job may suck, but the money allows you to do fun stuff after work. reply kossTKR 18 hours agorootparentprevIf you dislike writing code were you pushed into this field by family, education or because of money? Because not liking code and being a dev is absolutely bizarre to me. One of the most amazing things about being able to \"develop\" in my view is exactly in those rare moments where you just code away, time flies, you fix things, iterate, organise your project completely in the zone - just like when i design, paint or play music, do sports uninterrupted, it's that flow state. In principle i like the social aspects but often they are the shitty part because of business politics, hierarchy games or bureaucracy. What part of the job do you like then? reply Toutouxc 14 hours agorootparentI enjoy the part where I'm putting the solution together in my head, working out the algorithms and the architecture, communicating with the client or the rest of the team, gaining understanding. I do not enjoy the next part, where I have to type out words and weird symbols in non-human languages, deal with possibly broken tooling and having to remember if the method is called \"include\" or \"includes\" in this language, or whether the lambda syntax is () => {} or -> () {}. I can do this second part just fine, but it's definitely not what I enjoy about being a developer. reply kossTKR 14 hours agorootparentInteresting, i also like the \"scheming\" phase, but also very much the optimisation phase. I completely agree that tooling, dependencies and syntax / framework github issue labyrinths have become too much and GPT-4 already alleviates some of that but i wonder if the scheming phase will get eaten too very soon from just a few sentences of business proposal - who knows. reply zeroonetwothree 19 hours agoparentprevI still think it’s much more an “if” than a “when”. (Of course I am perhaps more strict with my definition) reply dgb23 20 hours agoparentprev> this discussion will soon be futile Yes we could simply ask the AGI what to do anyways. I hope it's friendly. reply acchow 9 hours agoparentprevDepends how expensive the AGI is. If it requires $1M of electricity per year to run, it will for sure not replace human jobs paying only $100k. The highest paying jobs will probably get replaced first. reply schaefer 15 hours agoparentprevIf AGI and artificial sentience comes hand in hand, I fail to see how our plans to spin up AGI's as a black box to \"do the work\" is not essentially a new form of slavery. Speaking from an ethics point of view: at what point do we say that AGI has crossed a line and deserves self autonomy? And how would we ever know when the line is crossed? reply hathawsh 14 hours agorootparentHumans can't be copied. It seems like the inability to copy people is one of the pillars of our morality. If I could somehow make a perfect copy of myself, would I think about morality and ethics the same way? Probably not. AGI will theoretically be able to create perfect copies of itself. Will it be immoral for an AGI to clone itself to get some work done, then cause the clone to cease its existence? That's what computer software does all the time. Keep in mind that both the original and the clone might be pure bits and bytes, with no access to any kind of physical body. Just a thought. reply voxl 8 hours agorootparent> Humans can't be copied. There is no reason to believe this, and every reason to believe that humans can, in fact, be cloned/copied/whatever. It may not be an instant process like copying a file, but there is nothing innately special about the bio-computers we call brains. reply hathawsh 4 hours agorootparentI'm not disagreeing. The point I'm trying too make is that humans can't be copied today, yet when AGI arrives, it will be copyable on day one. That difference means that current human morals and ethics may not be very applicable to AGI. The concepts of slavery, freedom, death, birth, and so on might carry very different meanings in a world of easily copyable intelligences. reply goatlover 4 hours agorootparentprevOther than it might be too complex and costly to do so. Just because something is physically possible, doesn't mean we'll find it feasible to do so. Take building a transatlantic high speed rail under the ocean. There's no reason it can't be done. Doesn't mean we'll ever do it. reply int_19h 13 hours agorootparentprevIf humans fundamentally work in the same way as any such hypothetical AGI, then they can be copied in the same way. reply hathawsh 11 hours agorootparentIf we ever do find a way to copy humans (including their full mental state), I suspect all law and culture will be upended. We'll have to start over from scratch. reply brailsafe 16 hours agoparentprevsoftware engineers already need to be worried about either losing their current job or getting another one. The market is pretty much dead already unless you're working on something AI reply mixmastamyk 9 hours agorootparentDo you know how many hype trains I’ve seen leave the station? :-D reply pydry 20 hours agoparentprevMarket consolidation (Microsoft/Google/Amazon) might cause a jobpocalypse, just as it did for the jobs of well paid auto workers in the 1950s (GM/Chrysler/Ford). GM/Chrysler/Ford didn't have to be better than the startup competition they just had to be mediocre + be able to use their market power (vertical integration) to squash it like a bug. The tech industry is headed in that direction as computing platforms all consolidate under the control of an ever smaller number of companies (android/iphone + aws/azure/gcloud). I feel certain that the mass media will scapegoat AGI if that happens, because AGI will still be around and doing stuff on those platforms, but the job cuts will be more realistically triggered by the owners of those platforms going \"ok, our market position is rock solid now, we can REALLY go to town on 'entitled' tech workers\". reply geodel 19 hours agorootparentSeems about right to me. Hyper-standardization around few architecture patterns using Kubernetes/Kafka/Microservice/GraphQL/React/OTelemetry etc can roughly cover 95-99% of all typical software development when you add a cloud DB. Now I know there are ton of different flavors in each of these tech but they will be mostly distraction for employers. With heavy layer of abstraction of above pattern and SLAs by vendors as you say Microsoft/Google/Amazon etc employers will be least bothered vast variety of software products. reply quonn 12 hours agorootparentThe technologies you mentioned are merely the framework in which the work is done. 25 years ago none of that was even needed to create software. Now they are needed to manage the complexity of the stack, but the actual content is the same as it used to be. reply pydry 10 hours agorootparentprevI've noticed over the years that those abstractions have moved up a step too. E.g. we used to code our own user auth with OSS, now we use cognito. At some point it'll become impossible to build stuff off platform because it'll have to integrate to stuff on platform to be viable. Your startup might theoretically be able to run on 3 servers but your customers' first question will be \"does it connect to googazure WS?\" and googazure WS is gonna be like \"you wanna connect to your customers' systems? Pay us. A lot.\". There goes your profit margins. Then, if your startup is really good googazure WS will clone it. There goes your company. reply fullstackchris 19 hours agoparentprevand does even AGI change the bigger picture? we have 26.3 million AGIs currently working in this space [1]. I've never seen a single one take all the work of the others away... [1] https://www.griddynamics.com/blog/number-software-developers.... reply threecheese 7 hours agorootparentPresumably, the same ability we have to scale software which drives the marginal cost of creating it down will apply to creating this kind of software. The difference here though is the high compute cost might upset this ability to scale cheaply enough to make it worthwhile economically. We won’t know for a while IMO; new techniques could make the algorithms more efficient, or new tech will make the compute hardware really cheap. Or maybe we run out of shit to train on and the AI growth curve flattens out. Or an Evil Karpathy’s Decepticons architecture comes out and we’re all doomed. reply zeroonetwothree 19 hours agorootparentprevWhat do you think the “A” stands for? reply hintymad 13 hours agoprev> Though coding all day sounds very appealing, most of software development time is spent on communicating with other people or other admin work instead of just writing code This sounds very...big corp, which inevitably needs many professional box drawers, expert negotiators, s",
    "originSummary": [
      "Large Language Models (LLMs) are significantly evolving software development, expanding AI's role beyond coding tasks.",
      "In-house supervision of AI developers is crucial to align with overarching objectives, as AI aids in complex workflow creation for business users.",
      "Despite AI advancements, software developers will continue to play a vital role in managing complexity, translating business issues, and defining business logic for generating software products."
    ],
    "commentSummary": [
      "The article examines the impact of Artificial Intelligence (AI) on the future of software development, addressing concerns about job displacement and the evolving nature of programming languages.",
      "It discusses the potential for AI to suggest and implement solutions, emphasizing the importance of human intervention and adaptability in AI systems.",
      "There is a debate on the balance between automation and manual input in programming, highlighting risks, benefits, and concerns about power concentration in software development, as well as the future role of human programmers and the implications of automation and AI on the job market and society."
    ],
    "points": 226,
    "commentCount": 352,
    "retryCount": 0,
    "time": 1710757784
  },
  {
    "id": 39744989,
    "title": "DIY Guide: Safely Replace Garage Door Torsion Springs",
    "originLink": "https://www.truetex.com/garage.htm",
    "originBody": "They said it was a job for \"professionals only\". They said you'd wind up in the emergency room, or worse. But I had the Web. I took the ultimate do-it-yourselfer's risk. How I Replaced Deadly Garage Door Torsion Springs And lived to tell the tale. Have a comment or question on my garage door repair? Email me at: kinch@truetex.com Richard J. Kinch Back to Home page Do-it-yourselfer. Still living. Updated July, 2015. This page is a description of how I replaced torsion springs on a garage door. You may find that my experience either frightens you from trying such a stunt yourself, or encourages you to give it a try. You may curse me for revealing the techniques supposedly known only to the trade, or perhaps thank me for explaining how it's done. All the secrets are revealed below. Even if you hire this work out, just knowing how it is done will help you shop for the best deal and avoid falling prey to overcharging tricks. Torsion springs are devices that lift most of the weight of a garage door so that the door can be opened manually or by an electric opener. The torsion principle is applied via an efficient and economical apparatus consisting of a torsion shaft under spring torsion, which turns lift drums, which wind cables attached to the door near the bottom. All of this apparatus is mounted over the top of the door. The energy stored is sufficient, in an uncontrolled release, to break things, hurt you, or perhaps even kill you. The same could be said of jacking up your car to change a tire, or mowing your lawn, or raising children, so it is not crazy to want to do this yourself. This page does not discuss what are called \"extension springs\", which are involved in a different door lift design that use springs running above the horizontal portion of the door track, perpendicular to the door. Replacement of extension springs is an easier task that is more amenable to do-it-yourself (DIY) effort. Hey, Isn't This Dangerous or Illegal for Us Do-It-Yourselfers? If you've researched this subject at all, you will no doubt have heard that you shouldn't be attempting torsion spring replacement as a do-it-yourselfer. That is generally good advice, so if you have any doubts about your abilities to do risky physical work on your own, hire the job out like everyone else. I found I was capable of doing this work with acceptable risk, because I intelligently understood the techniques, paid careful attention to methods and safety, knew how to use common tools in good condition, properly improvised the special tools I didn't have, and diligently attended to correctly performing a few moments of hazardous manipulation. I learned to do it purely on my own based mostly on bits of advice reluctantly given in Internet forums such as the Usenet newsgroup alt.home.repair. When I first wrote this page in 2002, there was no other do-it-yourself information available on the Web, and it was not until 2005 that reliable information disclosing the techniques started to appear elsewhere (see links below). I am also an engineer, and have always done a lot of mechanical repairs around the house. I also operate my own laboratory machine shop, and do some mechanical design work there. This background helped me figure things out, but nothing that involved is critical to repairing a standard garage door. This work is risky, but the risk is comparable to doing your own car repairs, or climbing on the roof of your house to clean your gutters. These are dangerous things that many people can do safely, but that safety depends on intelligent understanding and application of proper techniques. Professional door repair technicians, who are fully knowledgable, skilled, and experienced, report that they nevertheless are injured from time to time, despite their best efforts. Coldly evaluate your abilities and motivations, to judge whether you can manage the risks of this work for the benefit of the money and time you might save. The right to evaluate risk for oneself is part of what it means to be a functioning human being. —Mark Steyn Obtaining Parts May Be the Hardest Part of the Job Even if you are intelligent and skilled enough to do this repair, and as much as it may frustrate your instincts for self-sufficiency, you may be prevented from doing this work yourself by a system of commerce that refuses to take your money for the parts. Manufacturers and distributors of torsion springs believe they are better off not retailing their product directly to the public. They believe they are maintaining higher prices for their product by restricting sales \"to the trade.\" One brochure for parts even flatly stated, \"We do not sell to the end user. We protect our dealers,\" which would seem to be prima facie evidence of an illegal restraint-of-trade scheme. But this is an old story which is true of virtually every product and service, going back to medieval guilds and before. Repair of garage doors is a licensed trade in many jurisdictions, and manipulation of the market inevitably follows. Look in your phone book yellow-pages under \"garage doors\" and you'll find a lot of big, costly ads for door service. The profits are quite juicy, I'm sure. The customers need service urgently, and this need will typically arrive suddenly and at a busy time when shopping for prices is not convenient. A few dollars in parts, an hour of labor and travel, and a $150 invoice (assuming the outfit is charging fairly, some are not). Lately (2006) I hear of outfits charging $200 or $300 for this work, and occasionally a story of a $500 or $800 service call. You'll also find the phonebook advertisers waiting eagerly for your call, because artificially high prices inevitably lead to an oversupply of service firms working under capacity. Those who benefit from this anti-competitive behavior have many excuses for it. They claim that the product is too dangerous and infrequently needed for the public to purchase directly. They say the job is hazardous and requires techniques and tools with which the amateur will rarely practice, which is true. They write me in anger saying I don't understand how expensive it is to put a truck on the road with a technician and parts and salary and benefits and insurance and advertising and every other common business expense. They say it will take the homeowner hours of effort to do this safely and correctly, while they can do it in a fraction of that time and at a price that isn't worth the effort to save. But should they set themselves up as the judge of what you can and cannot do? Of what is an economical use of your time? Imposing minimum order quantities or charging retail prices for small orders is legitimate; inquiring into the purchaser's background is not. If this manipulative, we-are-your-nanny business approach is truly in our best interests as consumers, then we shouldn't be allowed near ladders, lawn mowers, or power tools. Those products are just as hazardous and prone to misuse as torsion springs, yet no one thinks of them as forbidden. The only genuine difference is that torsion springs are a hazardous thing you need only rarely, while a lawn mower is a hazardous thing you need all the time. The one excuse that makes the most sense is, \"if we sell springs to a do-it-yourselfer, and he gets hurt installing it, we could get sued.\" I can sympathize with someone who wants to sell only to the trade and not bother with the risk of a spurious product liability lawsuit from an ignorant member of the public. But the lawn-mower dealers have figured out how to manage that kind of exposure, so this is not an absolute barrier to retailing garage door parts to the public. It doesn't explain why torsion springs at retail are virtually non-existent. These are just my observations as a consumer; I am not on a crusade to change the garage door industry. But I will observe that the Web is the innovation that can finally give intelligent consumers the advantage in these commercial games. Trade restraints work only when all sellers in the market collude in and agree to the scheme. If anyone, anywhere is selling freely, then the Web can help you find them. In the years since I have first published this information, a number of reputable Web-based merchants have appeared to supply the parts you need to repair your garage door as a do-it-yourselfer (and I have linked many of them below). For the industry point of view, see: Door and Access Systems Manufacturers Association (DASMA), http://www.dasma.com. Also has excellent technical data sheets on garage doors. This trade group consolidated the older Door Operator and Remote Controls Manufacturers (DORCMA) and National Association of Garage Door Manufacturers (NAGDM) The International Door Association, http://www.doors.org. This is chiefly a dealers' and contractors' group, with a strong trade-protection attitude. Institute of Door Dealer Education And Accreditation, http://www.dooreducation.com. The Life and Death of Torsion Springs The most common grade of torsion springs have an expected life of about 10,000 cycles. The hardened and tempered steel experiences tremendous forces each time the door opens or closes. Gradually, the steel fatigues with each flexure, and eventually cracks and breaks, usually releasing its stored energy in an instant with a horrific \"sproing\" noise or bang. If you average about two car trips per day, opening and closing the door a total of 4 times daily when you come and go, then that expected life becomes 2500 days, or only about 7 years. If you have an automatic opener, then if you're like me, you tend to cycle the door even more frequently, and can expect the need for spring replacement even sooner. Moreover, my three-car garage has three doors, so on average I can expect a repair job every few years. Over a lifetime, it is very economical to do these repairs myself. One of these \"sproing\" events at our house finally motivated me to research how these repairs are done. This happened in 2002, when my wife parked the chariot and shut the door. After the door closed, there was a horrific noise that she could only astutely describe as, \"a big spring snapping and vibrating\". Although I have hired professionals several times in the past to install or repair garage doors, the difference this time was the innovation of Google and newsgroups like alt.home.repair. I was determined to learn the process and to search for online parts vendors. Since the springs are winding \"up\" when the door is closing and going down, the fully closed position is the most stressful on the steel and thus the most likely position at the moment of failure. This is a good thing, because failure near the top-of-travel means that you suddenly have a large, increasing weight falling. Thus we see the principle that you should never be standing or walking under the door when it is opening or closing, especially if you do so manually instead of with an electric opener. When the springs are working correctly, the door appears nearly weightless, but this is an illusion that turns into a calamity when the springs suddenly fail. This is your last chance to turn back! I will now describe how I replaced torsion springs. Avert your gaze and read no further, lest you be tempted to do likewise! Here's a view of my door and its broken torsion spring. This door is 10 feet wide and 7 feet high, constructed of 3/4 thick hollow wood panels inside with 3/4 inch plywood siding outside to match the house exterior. This is original to the house which was constructed in 1978, and is much heavier (238 pounds, as I measured later as described below) than the steel doors most common today in new construction. The 10-foot width is a little larger than usual for a one-car garage; such doors are typically only 7 or 9 feet wide. The ceiling height is 9 feet, providing 18 inches clearance above the torsion shaft. This is in a 3-car garage with 3 separate extra-wide doors. Every man's dream! ('cept when the door is broke.) The torsion shaft with lift drums on the ends is above the door. The standard residential door shaft is a 1-inch outside diameter hollow steel tube. The inside diameters of the bearings, drums, and winding cones are sized to loosely fit that 1-inch diameter shaft. At the center is a bearing plate, on either side of which are the torsion springs, or in some cases just one larger spring. The spring pictured on the left in the photo is broken about 1/4 of the way in from its left end. The black shaft with dangling rope and door bracket is the track for the electric opener. In a standard residential door, the axial degree of freedom of the torsion shaft is constrained by the lift drums being set up against the end plates just inside the ends of the shaft. The rotating shaft assembly is thus captured closely between the end plates, but still loose so it is free to turn. The inside of the door panels bear a few dings, since filled with Bondo, from my teenage boys shooting hockey pucks. The streaks on the right that look like dinosaur claw scratches came from operating the door inadvertently when the rear doors of a van were open. The responsibility for these scars is all mine; they have also been Bondo-filled. We tend to use a lot of Bondo around our house. The crud on the walls and ceiling are cobwebs and dust infiltrates. Here's a close-up of that left spring showing how it broke. The winding cone has 1/2-inch round sockets every 90 degrees for the insertion of round winding bars. Two 3/8\" square-head set-screws fasten the cone to the torsion shaft. Note the left winding cone with red spray paint. This shpritz of paint is applied to create fear and doubt in the mind of the do-it-yourselfer. Sometimes it is a color code for the wire size (using a DASMA standard, red indicating 0.2253 inch diameter wire). Sometimes it indicates the winding direction: red may indicate right-hand winding, but don't rely on that; do you own independent analysis. Sometimes it is a manufacturer's private code for another dimension than wire size. This color code is for the installer's information when the spring is new; I would not depend on interpreting the color code properly on an old spring, since one can't be certain of a correct interpretation without documentation from the original supplier. The wire size and winding direction are easily discovered and proved, as I will explain below. You absolutely must know and understand the critical measurements of your old springs to order replacements. This assumes that the old springs were the correct to begin with; it is not uncommon to have incorrectly sized springs on a door due to a previous sloppy installation, or a significant change in weight of the door. You must use springs that are matched to the weight of the door. You cannot compensate for the wrong size spring by adjusting the number of winding turns. If you do not know a proper spring size, then you or your spring supplier must calculate a proper size (see below) based on an accurate weight (within 5 pounds) of the door. So you must then in turn have an accurate weight. When ordering springs, be aware that a number of different sizes of springs will make proper replacements, not just the specific size being replaced. The wire size, winding diameter, and length can be traded off to make springs of varied geometry but equivalent torque characteristics. This will also affect the expected lifetime (in cycles) for the spring(s). Since the critical specification for a replacement is the weight it is designed to bear, not the sizes per se, there are likely several stock sizes that replace a given old spring. The spring distributor's inventory may happen to offer only a different size with an equivalent weight-bearing specification. One has to judge whether to trust the advice of the seller in such situations. The seller should have the data to know what substitutions are proper. The right side of the photo shows the center bearing plate where the stationary cones attach with two bolts. Some doors may have only one spring rather than two equal ones as shown here (indeed, old marks on the shaft show that this door originally had one spring about twice as long on one side). Above the center bearing plate is the bracket and track from the electric opener. Critical measurements: Torsion springs come a variety of standardized sizes, so you have to carefully measure the old springs to know what to order for proper replacements. Tables of standard sizes and designs are on the Web, such as here [www.industrialspring.com]. The four critical measurements (all in inches) are: (1) the wire thickness (which I'm measuring here with a dial caliper; you can also measure the length of a number of closely stacked turns with a ruler and divide by the number of turns in the stack, measuring 10 turns this way makes the math easy), (2) the inside diameter (not outside!) of the relaxed (not wound!) coil, (3) the overall length of the relaxed (not wound!) spring coils, not including the winding cones, and (4) the right- or left-hand winding of the spring. One must glibly quote those figures to the spring supplier, otherwise one's lack of expertise will be obvious, and one will not be worthy of buying the parts. Measure springs only when relaxed: Measurements must be taken on a relaxed spring because the winding adds significant overall length while reducing the coiled diameter. If you have a paired design, and one is broken and one is intact, then don't try to measure the length of the intact spring with the door down. A wound spring has 7 or 8 turns adding to the overall length, and will therefore be about 2 inches longer than when relaxed. Measure the lengths of the pieces of the broken spring, which will be unwound, and add them together. As a check, one can measure the length of the intact spring after it is unwound in the procedure to follow below. Be sure also to observe whether the springs are originally of equal sizes, because it is quite possible that they are not. The various increments of standard wire sizes differ by only about 0.010 inch, so calipers or a micrometer would be the tool to use to be certain of the stepped size you have, or else a trustworthy ruler marked in tenths of an inch to use the measure-10-turns-and-divide-by-10 trick. The most common wire sizes in the US are 0.207\", 0.218\", 0.225\", 0.234\", 0.243\", 0.250\", and 0.262\". Note that I am measuring a spring that is fully relaxed because it is broken!. The length of the relaxed, unbroken spring is the specification of interest. It is harder to measure unbroken springs on an intact door because the springs should not fully unwind, even at the top-of-travel. If you can't be certain of the spring diameter from indications on the cones, then you have to go through an unwinding procedure to relax them fully for measurement, or perhaps reckon the size from measuring the somewhat smaller diameter at the nearly unwound condition when the door is at its top-of-travel (although one should not attempt to raise a door with a broken spring). Right-hand versus left-hand winding: Springs are chiral, that is, wound or \"laid\" in either a left- or right-hand orientation. This is a critical property of their design and specification; you cannot substitute a left for a right or vice versa. If you were to grasp the spring in your hand, and if your right hand orients the tips of your fingers like the ends of the coiled wire when your thumb points \"out\" of the core of the spring, then you have a right-hand spring; likewise left (which end you grasp does not matter). (This also happens to match the \"right hand rule\" of magnetic polarity, if you happen to be knowledgeable in such esoteric subjects.) Another way to identify the winding is to examine the spring vertically in front of you; if the coils facing you rise going to the right, it is right-hand (thus you can remember, \"rise to the right is right-hand\"), and likewise left indicates left-hand. Another way is to view the coil axially; a right-hand spring winds in a clockwise direction as it recedes away, and a left-hand spring counter-clockwise. Yet another way, not so easy to remember, is to hold the spring vertically and compare the coil shape to the letter \"Z\" (indicates right-hand lay) or the letter \"S\" (indicates left-hand lay). Confused? A last resort is to compare the winding of the spring coils to the threads on an ordinary screw or bolt, which threads lay in a right-handed winding along the axis. An enantiomorphic (mirrored) pair of springs, such as my standard door uses, will consist of one left-hand and one right-hand spring. Note that this \"right\" and \"left\" has nothing necessarily to do with whether the spring is mounted on the left or right of the center bearing plate. Indeed, with my standard door, if you stand inside the garage, facing out, then the spring to the left is a right-hand-wound spring, and the spring to the right is a left-hand-wound spring. The photos above and below of the broken spring show that it is a right-hand-wound spring. End treatments: Torsion springs also are made in a variety of end treatments. The \"standard torsion end\" is most common, as is pictured in my examples, consisting simply of a short, straight length of wire projecting tangentially. Various non-standard end treatments have longer \"ears\", U-turns, ends bent in toward the center or along the axis, or even loops. Non-standard ends are used in end fasteners peculiar to various manufacturers, which would seem to serve mostly as a guarantee that you buy overpriced replacements from that one source. The replacement springs in my case proved to be 0.2253 wire size, 2.0 inch (inside) diameter, and 24 inches long, in a pair of one left- and one right-hand winding. Actually, the old springs in these pictures were a slightly smaller size, but another similar door on this garage was better balanced by that size. Whoever installed the old springs didn't quite get the weight and size just right; it is not unusual to find a repair service installing a slightly off-balance spring size that happened to already be on the truck during the service call. My electric opener had no trouble handling the small imbalance. But since it is safer to reduce the electric operating force as much as possible through careful balancing, I chose the size that was working better on the other door. The Chamberlain brand electric openers (also sold by Sears) I have incorporate a plastic worm gear that tends to wear out after some years of use, requiring a disassembly and installation of a $20 repair kit; this wear is minimized by a properly balanced door. Correct spring size is determined by factors such as the weight and height of the door. You cannot substitute a different spring and just tighten or loosen the winding to make it balance the load. Why? To maintain cable tension under all operating conditions, the spring must retain about one turn of unspent wind-up at the top-of-travel position, which with the lift drum size and door height predetermines the number of turns of winding at the bottom-of-travel; and furthermore the torsion of the fully-wound spring at the bottom-of-travel must be slightly less than that needed to lift the weight of the door when translated by the lift drums. Although the door weight and drum size determine the maximum torque (termed MIP, maximum inch-pounds) needed from the fully-wound spring(s), the spring selection for a given door can still be varied to adjust the cycling stresses. A heavier wire on a larger diameter or longer length will produce the same torque as a lighter wire on a smaller diameter or shorter length, while undergoing less stress and therefore increasing expected cycle lifetime. The heavier spring will cost more but last longer, so this is another design trade-off. Calculating these spring sizes in the field is done using a book of tables (or the software equivalent) that we cannot provide here, although you will find the formulas to estimate spring properties below. If you can accurately provide the weight of the door, or the size(s) of the old spring(s) (assuming they were well-matched to balance the door), then a spring dealer should be able to tell you which spring sizes will work for you. A spring design manual, also called a rate book, gives tables that relate the torque constant (\"rate\") and maximum turns for springs of given wire size, diameter, and length. For example, a typical page in a rate book would show a table for a given wire size and inside diameter, the maximum inch-pounds (MIP) of torque available for a standard lifetime of 10,000 cycles in that size, the weight of the spring per linear inch, and the rates of the spring (as IPPT, inch-pounds per turn) for each of various lengths. From these figures one can calculate the lifting capacity, substitutions, conversions, and cycle life upgrades for a door of given weight and drum geometry. The weight-lifting capacity of a given spring is calculated based on its torque constant (IPPT, or inch-pounds per turn), which is the rotational version of the spring constant that characterizes the spring. The IPPT constant is found from tables giving IPPT for given spring dimensions (wire-size/diameter/length). The same tables may indicate the maximum number of turns for various expected lifetimes in cycles. The torque required to balance a given door can be calculated from the weight of the door times the moment arm of the drums (as we do below under \"Calculating the Forces We Will Be Handling\"). The ultimate torque of the spring in the fully-wound condition is the number of turns (when fully-wound) times the IPPT constant. Choosing a spring to balance the door then simply requires matching the ultimate torque of the spring to the balancing torque. Beware of improprer prior installations: Sometimes the existing door installation is not correct, and the old springs should not be used as a specification for replacements. For example, the old springs might have been replaced with incorrect sizes because the last repairman didn't have the right one on his truck. If your door has never worked quite right, something like this might be the cause. To correct this, you must use the weight of the door to specify the spring, either from a spring rate manual giving spring torque constants, or from the formulas below. Unmatched or mismatched spring pair: You may find that you have a pair of springs that are different sizes. This mismatch may be a normal application, since the total torque on the torsion shaft is simply the sum of the torque contribution of each spring (indeed, very large doors can be lifted with 4 or more springs along the torsion shaft). The sum of the torque rates determine the lift; and dividing the torque among multiple springs does not change this. Some repair shops even apply mismatched pairs deliberately, since a few stock sizes of springs can be combined to fit a wider range of door weights than only matched pairs. For example, a technician may carry springs in increments of 20 lbs of lift, and when using pairs this allows a 20 lb increment in possible choices instead of 40 lb increments. Or, one spring from a pair may have broken and been replaced with a spring of equal torque rate but different size than the original. Having a mismatched pair makes it difficult to specify the correct matched-pair replacements. To obtain replacement springs for a mismatched pair, you can either specify the same odd pair, try to calculate the equivalent matched pair sizes, or (this is the best method:) measure an accurate door weight and calculate the right spring size(s) \"from scratch\". The spring seller should be able to do the calculations from your accurate measurements of weight, height, and drum size; or you can attempt the calculations yourself using my engineering formulas below. Uncentered center bearing plate: The center bearing plate need not actually be in the center. It doesn't much matter where it is, since the purpose of the bracket is to anchor the spring ends. This anchoring must be secure, since all the torsion is held together at that point. On a stud-framed wall, this bracket may be placed over the stud closest to the center rather than exactly at the center of the door opening. This is the electric opener which operates this door. I'm picturing it here because you pull the rope to disconnect the trolley, run the trolley under power to the fully-open position, and then disconnect power before working on the door. Then you should lock the door down with either the security lock or with Vise-Grips or C-clamps. This avoids the door lifting when you don't expect it as you are applying spring adjustments. It you were to foolishly overwind the spring without the door locked down, you could possibly find the door trying to leap up to the raised position when you aren't prepared. That would likely knock your grip off the winding rods, with potentially disastrous results. I like to work under the safety principle that serious accidents should be physically possible only when you make two or more stupid mistakes at the same time. A tape measure is fine for learning the overall length of the relaxed spring. The gap where it is broken shouldn't be counted in the length, nor should the winding cones, just the wire part as if it were relaxed and unbroken. This length for standard springs is commonly an even number of inches. Here are the new replacement springs I ordered from a distributor, which I found using a Google search for \"garage door supply\" (search that phrase now). You certainly won't find these at Home Depot or Lowe's (although last I checked Lowe's does carry the less daunting extension spring replacements). I also have a list of some suppliers at the end of this page. Cost was $88 for 2 pairs of springs, plus $21 shipping. (I had to order 2 pairs to meet the $50 minimum order.) They came with new cones inserted as shown at that price, so I didn't bother trying to remove and reuse the old cones to save a few dollars. The cones are quite difficult to remove from old springs and to insert in new ones, and the spring supplier will have the right tooling to do that easily. That was the best price I could find on the Web at the time, and didn't seem out of line with what parts like this might cost at at the building supply (if they only sold them). Contractors buy these much cheaper in quantities; they're just an ordinary high-carbon steel wire turned on a winding machine. I also found Web sites asking a lot more money, obviously trying to cash in on search-engine traffic from do-it-yourselfers. Others report that some local dealers sell springs at retail, but at a high price that eliminates any economy versus having them installed. Spring pairs should be replaced together, since the mate is likely to fail soon after the first, and any possible savings in parts isn't worth the extra effort to repeat the work later. These springs weighed in at just over 9 pounds each, including the winding cones. They are covered with a light film of oil, and at this point the job starts to get messy. The manufacturer has painted them with a \"225B24\" part number, which no doubt indicates the 0.2253 inch wire size, 24 inches long. Perhaps the \"B\" indicates the 2-inch inside diameter. Both the left- and right-hand springs have this same number on them. I repeat my caution about the uncertainty of interpreting color codes. A professional installer reading this page emailed me to say that the red color indicates the springs are right-hand windings, not the DASMA color code for the wire size. But this photo shows both a right- and a left-hand spring, and both have the red paint on the cones and set-screws. I conclude it is prudent to make your own measurements and analysis. Do not rely on the colors on old installations. The only time I would respect them would be on new parts that carried documentation giving the code. Yet another professional wrote me to say that the red paint on certain components of these assemblies is an \"industry standard\" that signals, \"Danger! Part under hazardous tension.\" Other items under tension like the bolts on the cable attachment plate on the door should therefore also be painted red. If so, then this is a new, ambiguous, unreliable, and little-publicized standard, because none of my old hardware shows it, red paint also means other things, and searching the Web does not readily turn up references to this practice. Springs should be handled with some care so as not to nick or abrade the wire. Such flaws can weaken or stress the wire, leading to premature failure. This caution applies to every step in the process of shipping, storage, installation, and usage. Removing winding cones from an old broken spring for reuse in a new spring: Springs without the winding cones installed are a little cheaper than with the cones. Twisting the old cones into a new spring is easy with a vise and pipe wrench, but it can be tricky removing old cones from a broken spring for reuse. To remove old cones, mount the cone in a vise such that the spring portion is free. Grab the last few turns of the spring in a pipe wrench, engaging the teeth of the wrench into the end of the spring wire. Turn the wrench against the end of the spring wire, releasing the end of the spring from its clamping onto the cone, as you twist the loosened spring off the cone. Another more certain if not brutal method is to use an angle grinder with a thin metal-cutting disk to cut through the loops of spring wire where the loops wrap around the cones, being careful not to nick the cone itself too much. You could also cut into the old spring loops with a just hacksaw and break off the loops with hand tools, but this will require a lot of effort. Here is what a winding cone looks like without the spring. The threads that grip the inside of the spring coils are ambidextrous, so you can use the same part on either right- or left-hand-wound springs. The cone size is specific for a certain inner diameter of springs, so if you have the wrong size, the cone will slip inside the spring (cone too small), or not fit (cone too big). The two set-screws in the winding cones have a 3/8-inch square head, which fits a 3/8-inch open-end wrench or 8-point socket, or a 7/16-inch 12-point socket or 12-point closed-end wrench. I carried an extra wrench in my pocket while winding, since I didn't want to be holding a wound spring that I couldn't set because I had dropped the wrench (although one could rest the winding rod against the door in this case while picking up a dropped tool). I decided later to use an ordinary open-end wrench rather than this socket or a box-end wrench to turn this fastener. The extra play fits the crude square heads better, and if the cone were to accidentally let loose, an open-end wrench would be the least likely tool to stay on the head and turn into a flying hazard. The standard winding tools are simply a pair of 18-inch lengths of mild steel rod, 1/2-inch diameter. Winding cones can have different socket sizes (such as 5/8 inch instead of 1/2 inch), so it is important to measure the socket and select a matching rod diameter. Also beware that poor-quality cones may have a sloppy fit to the winding bars, and a loose fit presents a severe hazard of slipping at the worst moment; anything more than about an inch or two of play at the handle end is too loose for safety. I bought a 3-foot length of zinc-plated 1/2-inch diameter steel rod from Home Depot for about $3, which conveniently cuts into two halves of just the right length (the store might even cut it for you if you ask). A steel supplier selling at commodity prices might charge about 50 cents or so for such a piece that weighs about 2 lbs. Drill rod would work if used in the annealed condition in which it is originally sold, but the added expense provides no benefit and the brittleness (if it had been hardened and not annealed) would worry me a bit. Rebar, threaded rod, screwdrivers, etc., are absolutely foolish as they will not fit the socket snugly. Aluminum rod is definitely too weak, and will bend under the torque that must be applied. Longer rods would make for more leverage but unwieldly swing; shorter rods make for uncontrollable swing. As we'll calculate below, the 18-inch standard tool length is an appropriate compromise. Note that you do not need 18 inches of ceiling clearance above the torsion shaft to use an 18-inch rod, since you need not swing the rods above horizontal when winding. Clamping the rod stock in the vise and cutting it into two 18-inch winding tools is a quick job with a sharp hacksaw blade. I filed the ends of the rod to remove burrs and square the tip. You don't want a sharp edge cutting your hand at a critical moment. The finished tools each have one end shear-cut from the factory, and one end that I saw-cut and filed. These are the two saw-cut ends. Here are the winding rods inserted in the winding cone of the unbroken old spring, posed just for a picture. Note that I have carefully placed a sturdy, steady ladder just clear of the swing of the rods, such that when I am standing on the lower rungs to reach the rods, that my head and body are clear of the \"kill zone\" around the spring and cone. You must have a trustworthy platform to stand on, because a slip or shake of the ladder while you are winding can cause you to lose your socketed attachment to the cone, letting loose the spring. I would not trust an ordinary household step ladder for this purpose. One might stack lumber or arrange some other low platform for a steady footing, instead of the ladder. The aluminum ladder shown here is the splendid 16-foot Krause Multimatic, which carries a Type 1A Industrial rating (300 pound working load); I highly recommend it. However, product liability apparently forced this company into bankruptcy in September 2000 and the company ceased operations in June 2001; see http://www.krauseladders.com (this Web site went dead sometime in mid-2002). The world is a dangerous place. This picture shows the upper rod pointing at an upward angle, even though you don't need to wind it that high. Winding is accomplished by swinging the rod from pointing down to pointing horizontal. This pose was just to allow me to take a photo with the rods in the cones. Glass windows are another hazard with doors so equipped. The glass can be broken by a slip of your tool, a spring breaking during winding, or other mishap. So one should take extra care when working near glass. Calculating the Forces We Will Be Handling Does winding these springs require an herculean effort from a muscular pair of arms? Does it start out easy, but in the end exhaust and trap you where you have no strength to either finish or back off? Let us apply a little engineering discipline to such questions. \"Felix qui potuit rerum cognoscere causus.\" —Virgil, Georgics Translation: Happy is he who knows the cause of things. \"Superbus est, qui loquitur in prouerbiis Latinis.\" —Kinch, Metallum Flexu pro Ostium Currum Translation: Snobby is he who quotes Latin proverbs. To estimate the maximum physical force required to wind these springs, consider that they are balancing the weight of the door with a torque applied to a lift drum on each end of the torsion shaft. The lift drums have a 2-inch radius, which is the standard residential size, and corresponds conveniently to about a 1-foot circumference. If we pessimistically assume the 10-by-7-foot door has a weight of 350 pounds, this implies a torque of 350 pounds on a 2-inch radius, that is, 700 inch-pounds, or 58 foot-pounds. Each of the two springs should be exerting slightly less than half of the balancing torque, or 29 foot-pounds. Compare this to, say, the bolts in an automobile, which are typically torqued to values of about 50 foot-pounds, or tire lug nuts, which may be torqued to well over 100 foot-pounds. Since we're using an 18-inch (1.5 feet) winding lever to wind each spring up to 29 foot-pounds, we must apply a maximum tangential force to the end of the winding lever of about 20 pounds. Later we'll see that the actual weight of this rather dense door is 238 pounds, implying a maximum tangential force on the winding levers of only about 13 pounds! Now 13 pounds of force must be respected when backed by many hundreds of foot-pounds of stored energy, waiting to be released. Holding this torque is equivalent to stalling a 3 horsepower DC motor. But holding and turning these handles does not require extraordinary human strength. Note that this maximum tangential force depends only on the weight of the door, and the radius of the drums, and is divided by the number of springs (some designs have only one longer spring, as mine did originally, instead of two shorter ones). Higher or lower lift distances imply more or less turns to wind the spring (and thus a different spring geometry), but not more force on each turn. Considering energy instead of force: Work or energy can be measured as foot-pounds, which is to say, the product of the weight lifted times the distance lifted. The energy stored in the spring is equivalent to the weight of the door times the distance it lifts, since these two are approximately in balance. A standard residential door raises 7.5 feet, but since the door goes horizontal this is equivalent to raising the whole door for half that distance, or about 3.75 feet. So if the door weighs, say, 150 lbs, then the energy supplied by the springs is 3.75 * 150, or about 563 foot-pounds. This is like throwing a 50-lb sack of cement up a 11-foot flight of stairs. Or catching a 50-lb sack of cement dropped from 11 feet up. Not something you want to be catching. Interestingly, this 563 ft-lbs is about equal to the muzzle energy of a .45 pistol bullet at point-blank range. Speed of a falling door:: Physics tells us that the transit time of a free-falling body is sqrt(2x/g), where x is the length of the fall and g is the acceleration due to gravity (32.2 ft/sec^2). If this typical 150 lb door were to fall an equivalent of 3.75 feet, this falling time would be sqrt(2*3.75/32.2) = 0.48 seconds (480 milliseconds). The terminal velocity is gt, or 0.48 seconds * 32.2 ft/sec^2 = 15.5 ft/sec = 10.6 mph. Dodging a falling door:: Reversing this equation gives us x=gt^2/2 as the fallen distance x for a given time t. How much time would you have to dodge a falling door if the spring were to suddenly break at the top of travel? Let us assume you are 5.5 feet tall, so the door will hit your head after falling 2 feet from its 7.5 foot fully-raised height. This 2-foot fall takes sqrt(2*2/32.2) = 0.35 seconds (350 milliseconds). The quickest human response time is about 200 milliseconds, so even if you are alert to the hazard, this leaves you only about 150 milliseconds to accelerate and move your noggin out of the way. If you are an Olympic gold medalist in the 100 meter dash, you can accelerate (horizontally) about 10 feet/second^2, and your 150 milliseconds of wide-eyed panic will move you all of 10*0.15^2/2 = 0.11 foot = 1.35 inch. Thus it is humanly impossible to dodge a falling door. If the spring happens to break when the door is moving up or down somewhere in the middle of travel, as is more likely, then you'll have even less time. Hence it is not prudent to stand or walk beneath a moving garage door. Of course people often do, and the only reason this does not frequently kill people is that springs typically break at the bottom of travel, where they are stressed the most. Speed of a thrown winding bar:: The springs, being in balance with the door, effectively are able to launch a typical 150 lb door at 10.6 mph speed. An 18-inch long by 1/2-inch diameter steel winding bar happens to weigh about 1 pound. Since momentum is conserved, this 150:1 ratio in weight of the door to the winding bar means the fully-wound springs could potentially throw a winding bar at 10.6 mph * 150 = 1590 mph = 2332 ft/sec, assuming the energy were perfectly coupled and transferred. If the energy transfer were only 1/3 efficient, this would still be the 800 ft/sec speed of a typical pistol bullet. Except it is a foot-and-a-half metal spear, not a bullet. Again, not something you want to be catching. With the rods and other tools at hand, I am ready to begin. The first task is to remove the broken spring and its unbroken mate from the torsion shaft. To remove and disassemble the shaft and lift drums, the torsion on the unbroken spring must first be released. I used a ratcheting box-end wrench to loosen the set-screws while pushing the rod against the force I knew would be released when the screws let go. Later I switched to an open-end wrench for the set-screws, since some of the square screw heads were too rough to fit in the box-end wrench. It is prudent to be prepared for torque from either orientation when loosening the set-screws. This protects you from a miscalculation of the force orientation or from an attention lapse. If it isn't marked already, you should run a chalk line (as described below for the new springs) down the length of the old spring before unwinding, so you know how many turns it took to unwind. This will give you a starting value for the number of turns in the new springs. By each \"turn\" is meant a full revolution of the winding cone. Each full turn requires four quarter-turn manipulations of inserting and switching the winding rods. My door is a very common height of 7 feet, which with the 4-inch drums requires about 7-1/4 or 7-1/2 full turns on the springs. This is the moment of truth for the beginner, as you will be holding the full force of a fully-wound torsion spring for the first time. It is time to adopt a calm, quiet, deliberate, careful attitude of concentration. Make sure that the telephone, a bystander, or other distraction is not going to startle you or make you lose your concentration. Loosening or tightening the set-screws is the moment of most risk, since the end-wrench is a potential missile if you slip, and your hand is close to the cone. When the wrench is removed and only the rods are in place, it would seem that the worst that could happen is that the rod is flung out and the captive spring and cone rattle around, assuming you are keeping yourself clear of the rod's radial disk of rotation, and not leaning on the rod such as to fall into the apparatus were the rod to slip out of your grasp. The torsion shaft design has the virtue of capturing the mass of the spring and cones reliably on the shaft, preventing these parts from launching themselves as projectiles, even in an accident. The prior clamping of the set-screws tends to have pressed a dimple into the hollow shaft and to have distorted the shaft's roundness into an eccentric shape. While releasing the set-screws, I was careful to loosen them enough to let the cone swing around any such distortions. I was also careful to observe any binding of the old cones on the eccentricity or burring on the shaft. The fit of the cone on the shaft is supposed to be loose enough to avoid binding, but if it were to occur one would have to be careful not to assume the spring was unwound when in fact the cone was just stuck on the shaft. If I had a stuck cone that I could not unwind with a little extra force, then I would have called in a technician to deal with it. In the worst case, I suppose the spring must be deliberately broken with some hazard, thus releasing it for a forceful disassembly, and the shaft and some other parts replaced. But this is an unlikely situation and in this case was not necessary. The winding technique is simply to (un)wind as far as one rod will go, where it is pressed against the top of the door, or nearly so, by the unwinding torsion. You insert the other rod in the next socket, remove the first rod, and continue. At any point you can stop and rest by leaving the active rod pressed against the door, where it will be held by the unwinding force. I would make a quarter-turn increment that way, and let go for a moment to collect my attention for the next increment, almost in a quiet, meditative alertness. While you can go from one quarter-turn and rod-swap to the next continually without letting go, working fast against the steady tension seemed to invite a kind of shakiness in my arms that was a bit unsettling. It isn't that there is much physical exertion, it is more that the tension is unrelenting, like peering over a precipice. While winding or unwinding, one must be mindful of the possibility that the spring could break during winding process itself. If that should happen while the spring is significantly torqued, hazardous forces on the winding bar will suddenly become unbalanced, and the bar will take an unexpected jump, possibly injuring your hand or anything else in its path. At the same time, the spring remnants, although captured on the torsion bar, will create a frightening racket that would give the bravest soul a start. So your winding technique should be firmly in control of the rods, and you should not be so delicately perched on a ladder such that a startle will result in a fall. Depending on the design, you can know in advance how many turns are going to have to be unwound. Lifting a 7-foot door by winding a cable on a 4-inch diameter (about 1 foot circumference) drum requires about 7 turns, plus one extra turn to maintain cable tension at the top-of-travel. Maintaining tension at the top-of-travel is critical; without it the cable will jump off the drum, requiring a serious repair. Why Can't You Install the Springs Unwound, with the Door in the Up Position? You might be thinking: Aha! Why don't we lift the door, clamp it in place, and install the springs while they are thus safely unwound, rather than deal with all that accumulation of hazardous torque? The answer: At the top-of-travel, the unwound springs are not fully relaxed; they are still clamped to the torsion shaft with a significant stretch along the shaft axis, plus about a half-turn to keep the door snug at the top. This extra length amounts to the stacking of extra turns that accumulate from winding, also termed \"spring growth\" in the business. In my case this is about 7 turns of 0.2253 wire, or about 2 inches. Stretching the spring that much and clamping it with a half-turn or so of twist is not feasible. Even if one could somehow stretch and clamp the springs to the proper extra length, the process would still be more trouble, and there would be little or no reduction of risk. Lifting the full weight of the unsprung door by hand and clamping it in the raised position is dangerous in itself, and creates the same amount of stored energy as winding the springs, ready to slip out of your hands. Many doors won't travel far enough up the track to provide clearance to access the springs. You're also going to have to deal with winding stiff steel cables onto both lift drums at once without any resistance to maintain tension. Finally, even if you managed to complete the installation with the door raised, you then have to lower the massive door against an untested balancing torque. If you've made a mistake, then that massive door has nothing but your skeletal force applied through your meat clamps (hands) to prevent it from falling down and crushing whatever is in the way (perhaps your feet?). Other Bad Ideas for Saving Money or Time If you're clever and equipped with a welder, you might think you could get away by welding a broken spring back together. At least two factors make this extremely risky. First, the weld itself may fail, either due to insufficient basic strength, or weakening of the nearby parts of the spring. Second, the fact that the spring was old and fatigued enough to break once, means that it is likely to break again soon at other location(s). If the spring is broken near a winding cone, you might think you can remove and discard the short broken piece of spring from the cone, clean up the end of the long remaining spring, and insert that end into the cone. This is another extrememly risky improvisation. The shortened spring is not going to have the correct weight-bearing characteristics for the door, so you will not be able to balance the door properly. The shortened spring will be proportionately overwound, resulting in extra stress that will increase the expectation of another fracture. And the aging and history of the original spring being broken greatly increases the likelihood of another fracture at other locations. So discard your old springs and replace them with new. Don't try to repair or reuse them when they're broken. Phew! The hazardous torsion is all removed from the old spring. Now the disassembly can begin, with our old friend gravity as the only acceleration threatening personal safety. Most situations allow you to replace spring(s) without removing the assembly from the wall, if there is enough clearance in the surrounding garage structure at the ends of shafts. By unbolting the end bearing plates and removing the drums, you can run the springs down to the ends of the shaft to remove and replace the springs. This avoids the balancing act of holding a long, wobbly, heavy shaft while climbing up and down a ladder. This is how the professionals get the job done in a few minutes. In my case, removing and replacing the relaxed springs required that I take down the assembly: torsion shaft, lift drums, and bearings. Doing that requires unbolting the center bearing plate from the wall, removing the drums from the shaft, and finally sliding the shaft back and forth out of the end bearings to remove the whole assembly off the wall. I am fortunate to have a lot of clearance in this garage to make the disassembly simpler. Tighter clearance to walls or ceiling would make disassembly a more difficult manipulation. Once the springs are relaxed and loose on the torsion shaft, the lift drums lose their tension on the lift cable, and the cable comes loose. The end of the cable is terminated by a press sleeve, which locks into a ramp on the drum. Different drum styles have a bolt or other method to fix the cable end to the drum. These steel cables are springy and won't stay in place without tension. If my pre-inspection had showed that these cables were worn or frayed, then I would have ordered proper replacements ahead of time from the spring distributor, since this is the opportunity to replace them. Standard hardware-store cable and fittings are not appropriate. Once the shaft, springs, and center bearing plate come down and lay on the floor, the old springs should be easy to get off and new ones slid on and assembled. Two bolts hold the center cones to the center bearing plate. The bearing on the center plate can be oiled while it is exposed from having the springs off. At this point it is a relief to be working with inert parts while standing on the floor rather than energized parts while up on the ladder. You might think it would save a little time to replace the spring with the shaft left up on the wall, but I found it was easier and safer to lower the works down to the garage floor first. As noted above, set-screw clamping may have distorted the cross-section of the shaft and made it difficult to slide off all the hardware. With the shaft on the floor, it may be possible to restore enough roundness to proceed, using compensating clamping force to the distorted area via a machinist's vise, an arbor press, a hydraulic shop press, etc., on the shaft body. Burrs and other slight distortions on the shaft can be filed off with a hand file or touched with an abrasive wheel on an angle grinder. At some point, the condition of the shaft may just be degraded enough that it ought to be replaced. I mentioned earlier that this apparatus had at least one prior spring replacement, with a single longer spring having been replaced by two shorter springs. The clamping of the original spring had pressed dimples and an eccentric distortion into the hollow shaft. While this distortion was large enough to block the old cones from sliding across, I was able to remove the old hardware by just sliding them in the other direction. I did not have to bother trying to press out this distortion, since I could just work around it. I was careful not to assume that the previous installation correctly oriented the right- and left-hand springs on the correct sides of the center bearing plate. They could have been installed backwards by an incompetent installer, resulting in them having been wound looser (larger diameter coil) instead of tighter (smaller diameter coil) than when in their relaxed state, and if so I would have corrected them on the new installation. The proper orientation of the springs applies their reaction torque from tighter winding such that it turns the drums to lift the door. Verifying this is a rather simple exercise in mechanical visualization, but does require some care to be certain of correctness. If you were to install the springs backwards, and then start to wind them in the wrong direction, then the torsion bar will start winding the drums backwards, and not holding against the vise pliers, which should be obvious to inspection. Winding a spring backwards also tends to screw the spring off the cones; this error cannot proceed too far before the spring slips off the cones. At this point I weighed the unlifted door to confirm and fine-tune my calculations. This is not strictly necessary, but it makes the adjustments easier to perform, if you happen to have a scale with the requisite capacity. With some helpers, we first lifted the door a few inches and rested it on blocks of wood to provide clearance underneath. Then I slid a 400-pound-capacity freight scale under the center of the door, we lifted again to remove the blocks, and lowered the door gently onto the scale. This door weighed in at 238 pounds, which is very heavy for a single-car door. Since the outside of the door carries the 3/4-inch plywood paneling to match the house, and that plywood weighs about 2 lbs/sq-ft, I estimate the door weight to be about 7 x 10 x 2 = 140 lbs of paneling with the rest 238 - 140 = 98 lbs the interior panels, hardware, and cobwebs. Knowing this total weight will help later in adjusting the torsion on the springs. After weighing, we removed the scale and blocks, leaving the door fully lowered again. Had I not had a high-capacity freight scale, I might have improvised a crude weighing device from levers and smaller weights of known mass, or a lever arm pressing a reduced proportion of the full weight onto a lower-capacity scale. Another factor to remember is that The weight of a wood door can vary with humidity. The door and tracks at this stage of the repair are in a minimum-energy condition. This is a good opportunity to work on any hinges, bearings, rollers, cables, or tracks that need tightening, repair, lubrication, or replacement. Again, these parts should be available from the spring source, and should be ordered based on a pre-inspection. Home-improvement stores carry some of these parts, but the type and quality may not be the best. Next, the torsion shaft is reassembled with the new springs, the drums repositioned loosely on the shaft, this whole assembly slid back into the end bearings, and the drum set-screws tightened down. I tightened the set-screws about 1/2 or 3/4 of a turn after contact with the shaft, which provides a good grip, but does not distort the shaft. The drums can be set on their old positions, if they were correctly installed, which is snug up against the end bearings to remove any longitudinal play in the torsion shaft. Now the lift cable can be reattached to the drums, and a slight temporary torque applied to the shaft to keep the cable taut while the first spring is wound. This temporary torque is conveniently applied with a pair of locking pliers clamped on the shaft, positioned such that they hold the torque by pressing lightly against the wall above the door, before you start the spring winding, The locking pliers stay on the torsion shaft until you have finished the spring winding locked down the spring cone(s) with the setscrew(s), and removed the winding bars. Then you simply remove them with the release on the wrench handle. I feel that any job that doesn't require a trick manipulation with either locking pliers or duct tape (or in the ultimate case, both!) is just too boring. My trusty pliers look a trifle rusty ever since I used them to clamp something on my outdoor TV antenna \"temporarily\" and left them out in the weather for, oh, several years. The white stuff on the drum is paint overspray from the original painting of the garage interior. If you have paired springs, you can take a shortcut here instead of using locking pliers. Simply apply a slight torsion to the bar by clamping one of the springs with an easy half-turn or so applied. This will hold the lift cables in slight tension while you wind the other spring. If you have a single spring design, you can't use this trick, and have to use the locking pliers. Checking if the lift drums need resetting: The old position of the lift drums on the shaft may have slipped or otherwise lost the the proper position, requiring a reset of the drum position on the torsion shaft. You will also reset the drums if you are replacing the lift cables, since the new cables will not exactly match the length of the old ones. Problems like uneven tension on the cables, or a tilted door, or a door that doesn't easily stay aligned with the tracks, can be due to an improper \"set\" of the drums on the shaft. So one shouldn't assume the old positions are correct. Setting the drums on a \"fresh\" part of the shaft will avoid the possibility of damaging the shaft from retightening in the same dimples. Resetting the drums, if needed: If the drums were incorrectly set in their old positions, one must reset both drums in new positions on the shaft. This is complicated by the presence of old dimples in the torsion shaft from previous setting(s), which must be avoided lest they improperly influence the new setting of the drums. To begin this process of resetting the drums, the door must first be lowered and resting level on the floor, the spring(s) must be in the unwound condition with their set-screws loosened, and the lift cables wrapped around the drums. If for some reason the door does not rest level on the floor, such as the floor being uneven, then insert temporary shims between the door bottom and the floor to bring the door up to level. Loosen the set-screws on the drums, and turn the torsion shaft to avoid the old dimples from the set-screws in the old drum position. Tighten the set-screw on the left drum (that is, on your left as you face the door from in the garage), creating a new dimple, and apply tension to its cable with the locking-pliers technique, enough tension to keep the cable taut but not enough to start to move the door up. Attach and wind the cable on the opposite (right) drum by hand until the cable is similarly taut, and set the screw, remembering that tightening the screw will tend to add a bit of extra tension to the cable. Both drums should now be fixed on the torsion shaft, with the cables about equally taut (listen to the sound when you pluck them like a guitar string) and the door still level on the ground. Setting the left drum first, and the right drum second, will allow you to take up any slack in the cable introduced by the left drum rotating slightly with respect to the torsion shaft as you tighten the set screws. This alignment and balance of the cables, drums, and door is critical to smooth operation and proper closing. If you have a single-spring assembly, the distance along the torsion tube from the spring cone to one drum is longer than to the other drum, which allows a bit more twist to one side than the other, and you may have to compensate with the setting of the drums. Lift cable placement: On the standard residential door mechanism, the loops at the lower ends of the two lift cables loop over the two bottom roller shafts which project from the bottom bracket on the door. The upper cable ends fasten to the drums using one of the methods described above. The drums are positioned along the torsion shaft such that the inner edge of each drum is approximately over the edge of the door. The cable winds onto the drum from outside in, so at the top of travel the cable is winding onto the inner edge of the drum, vertical from the edge of the door where it is looped over the roller shaft. As the door is lowered, the cable winds out to the outer edge of the drum, and thus is a bit out from the vertical, but the cable still falls in the gap between the guide rails and door edge. My cables rub and slap on the rails a bit, but after 30 years and many 10,000s of cycles, they don't seem to have worn at all. Cable fail-safe redundancy: Based on the proper setting of the drums on the torsion shaft, the two lift cables divide the lifting force equally to keep the door level as it rises. This not only levels the door, but also provides a fail-safe mechanism. If one of the cables should fail, such as from breaking or losing its end attachment, the other cable will then carry the full weight of the door. This will pull the door up on one side with twice the normal force, while the other side falls from its now unsupported weight, tending to make the door bind in its tracks and jam. Although not foolproof, this is a safety feature of the design which keeps the door from falling catastrophically if a cable were to fail while the door was traveling. The jammed condition also prevents a lowered door from opening with the hazard of a single broken cable, further minimizing the chances of both cables failing. Since if one cable fails the other must sustain the full weight of the door, the cables and attachments are rated many times the normal working load of half the door weight. A proper safety inspection of the door should include a critical look at the cables and their attachments. Leveling the door: Before commencing the spring winding, to check that you have the door properly leveled on the cables, considering all the factors above that make this a tricky adjustment, apply the winding cone setscrew lightly to lock the (unwound) spring cone temporarily on the torsion shaft, and momentarily lift the door slightly off the floor. Adjust the drum set as needed to level the door, repeating this slight lift test. Loosen the cone setscrew before winding the spring(s). Here is the center bearing plate with the new springs in place. While everything is loose, it is time to check the soundness of the mountings to the building frame and repair anything that was incorrectly installed or has deteriorated. In the photo, you can see an extra lag screw on the bottom of the plate, reinforcing weak anchoring from the original two screws (hidden behind the springs). This extra screw was applied by a professional garage door technician on a previous repair visit. I have since replaced the backing with a sound plywood panel anchored into the concrete door header. Since these bolts clamp all the opposing torsion to the wall, the attachment must be sturdy. The end bearings near the drums also get a few drops of oil. The last step before winding is to run a piece of chalk down the length of both relaxed springs. This lets you observe the number of turns as you wind. You don't want to be busy counting turns when you should be paying attention to the winding rods. The new springs are chalked and ready to wind. Time for one last check of fasteners and cables before applying torque. Another moment of truth arrives as the winding-up of the springs begins. (I am just posing here with the camera held out in my left hand. Except for lifting the door onto the scale, this was a one-man job, including the photography.) The position of the bars in this photo was necessary to take the photo, and does not show a correct winding technique, You should not have to swing the bar up as high over the top as shown. The lower bar during winding should swing from pointing down to pointing a little past horizontal. Then you hold that bar horizontal while socketing the other bar pointing down, apply force to that (now) lower bar, then remove the (now) upper bar, wind one-quarter turn, and repeat. You can also see I am wearing eye protection. It would be foolish to go bare and risk an eye injury. Somehow I came out looking like Elmer Fudd in this photo. And somebody oughta sweep out those cobwebs! Winding \"up\" starts out easy. It finishes at the proper number of turns, by which time you are pushing against the maximum torque. Count the turns of spring winding from when the springs are slack. To be sure you're winding the right direction, all you have to remember is that proper winding makes the spring smaller in diameter and longer in length as it twists \"in\". On the standard door (most common), this means you push the winding bars up to wind up the spring, which is an easily remembered rule. This is very apparent and should be verified during the first few easy turns. You can also think about the correct winding direction in mechanical terms, namely which way the reaction of the spring will torque the shaft and drums, which in turn will lift the cable. This should all make perfect sense before you attempt the manipulations. If you were to install the two springs backwards in the wrong sides of the center plate, the winding direction that acts to lift the door will be twisting the springs \"out\", which is backwards. By watching the chalk mark while winding, you can count the number of turns applied, and confirm the number later. My standard-size door (7 foot height) with 4-inch drums has a nominal wind of 7-1/4 or 7-1/2 turns, which leaves 1/4 or 1/2 turn at the top-of-travel to keep the lift cables under tension. After 7 turns on the first spring, I clamped down the set-screws, weighed the door again, and found a lift of about 100 pounds in reduced weight. As expected, this wasn't quite half of the full 238 pounds, nor would it leave any torsion at the top-of-travel, so I added an 8th turn. The door now weighed 122 pounds on one spring, which was ideal. After winding the other spring, the door lifted easily, with only a few pounds apparent weight. This confirmed that the spring choice was properly matched to the door design. I engaged the electric opener trolley, and adjusted the opener forces down to a safer level suitable for the new, improved balance. The door was now ready for return to service. As with the drums, I tightened the winding cone set-screws about 1/2 or 3/4 of a turn after contact with the shaft, which provides a good grip, but does not distort the shaft. One can overwind the springs slightly, up to about 8 turns on a standard residential door (that is, 1/2 or 3/4 extra turns), to compensate for undersized or fatigued springs, or increased door weight from painting or humidity, but this results in more stress on the springs and therefore decreased lifetime. If the door is too heavy for that slight tweak, then different springs are needed. The usually recommended rule for a door being properly balanced is that it should lift \"easily\" through all its travel. The door may also remain stationary if let go somewhere around the middle of the travel, but a smoothly rolling door many not show this behavior (while a sticky track will!), so easy travel is the only reliable test for proper balance. A difficult door may be due to stiff bearings or rollers in the mechanism, tracks out of alignment, etc., not necessarily the torsion spring adjustment. Another trick for fine-tuning the balance of spring turns versus door weight is to adjust one side of a duplex pair slightly more or less than the other. That way you can have an exact weight balance while retaining some desirable torsion at the top. Once the springs are torqued, the setscrews tightened, and the locking pliers and winding rods removed, do not play with turning the torsion bar using the winding rods. Doing so even momentarily can relieve the tension on the lift cables, which then easily slip off the drums. Replacing the cables on the drums can be difficult without repeating the entire spring unwinding-winding procedure again, and the cables can be damaged if tension is applied while they are off the drums. Total time for me to complete this work the first time was 3.5 hours, including cutting the winding tools and the photography. I've completed subsequent repairs in less than an hour. Hundreds of people have written me to say they did it safely in a few hours after studying this essay. I'd like to hear how it goes for you. Spring Supplier Referral List I make no recommendations of, and have no connection to, any of the following suppliers. These are just those whom I have learned about from my Web searches, from correspondence with those sending email about their experiences, or directly from the suppliers. I have removed several contacts that were generating complaints to me from dissatisfied customers. I have added this list to this page due to all the email queries I was receiving daily asking where to obtain parts. If you are a supplier and would like to be added to the list, see my email address at the end of this page. Current wholesale pricing (2005) for a typical spring (2\" ID, 0.2253 wire, 24\" long) is less than $10, including cones. Other sizes are proportionately priced by weight. Expect to pay up to three times that for a reasonable retail source, plus shipping. DDM Garage Doors in Chicago offers an online catalog, including torsion spring sizes and prices, and \"ships to all 50 states\": http://garagedoor.org/. I have received numerous favorable (even glowing) customer evaluations. Their Web site added a thorough instructional essay on the repair process, much like this one, in 2005. Dixie Door Inc. in Tennessee offers springs and hardware at their Web site www.dixiedoorinc.com. Ordering is via an online shopping cart, except for springs which require a phone call. I have received favorable customer evaluations. Luke Stelmack at SuperiorDoorParts.com in New Jersey wrote me to say he is offering springs and parts to knowledgeable do-it-yourselfers. Springs must be ordered via a phone call, however. James Bertschi at Garage Doors Only in Utah wrote me to say he is offering springs and parts to knowledgeable do-it-yourselfers. This is a new referral (3/2008) for which I have no customer evaluations yet. Search eBay for items titled, \"garage torsion\". This will typically bring up more than a few dealers who are now selling springs on eBay, as well as odd surplus items. Although Service Spring Corp (http://www.sscorp.com/) \"sells only to professional garage door dealers, distributors, and manufacturers\", download their catalog for a detailed reference of parts and supplies. Fehr Brothers Industries, Inc., of Saugerties, NY (http://fehr.com/) is a distributor of garage door hardware to the trade. A catalog with prices is available at http://www.fehrbros.com/gdsnp(web).pdf (42-page PDF file). Online ordering for the trade is provided at http://www.garagedoorsupplies.com/. This installer-dealer has corresponded with me offering to sell springs to do-it-yourselfers: Robert Young, email: Crndg945@aol.com. I obtained my parts, as described on this page, from American Garage Door Supply Inc. (http://www.americandoorsupply.com/) They may have raised their small-order prices or imposed a minimum order since I ordered from them in 2002 at the prices in my essay above. The Web site offers a free catalog by PDF download or mail. Springs in 2005 were priced at $2.25/lb. What Do the Professionals Charge for This? A fair price for a professional repair, including the service call, parts, labor, and warranty, in most US locales is $100 to $150. This was the case in the early 2000s. Times change. What Do the Professionals Overcharge for This? I have received many tales by email of various swindles for upwards of $800 involving garage door repairs, or $1000s in needless door replacements. Beware of various tricks. Overcharging doesn't announce itself with a big \"OVERCHARGE\" stamped on the invoice. You have to know and recognize the angles. The parts, parts, parts trick: You might be told you need new rollers, cables, drums, bearings, etc., when you don't, or at highly inflated prices. Good questions to ask when first calling for service include, \"How do I know you will only charge me for the parts I actually need?\", and \"If you don't have all the parts I need, what will you charge me to come back?\" The open-ended work-order trick: You may be very surprised if you allow work to proceed without signing a work order with a specified price. Or, you may sign a work order, and think you're protected against open-ended wallet-reaching, only to find a much higher price due at the finish than you expected, because you signed a \"parts as needed\" order that got loaded up with a long list of parts (that likely were still in serviceable condition). You might have been quoted a price, but then get a bill for that price plus a lot more added for the \"service call\" and the \"parts\", and be told the quote was just for the labor. While this is the normal way of abusing your finances down at the hospital, you shouldn't agree to it for a garage door service call. These guys are not doctors. The not-so-competitive advertising trick: How about this racket: We have all heard how you should get at least three bids for any home improvement jobs, right? In some areas the largest ads in the yellow pages are from a single business using various names and phone numbers, masquerading as independent competitors. When you call asking for prices, \"they\" all quote you very high numbers. You are tricked into thinking you have shopped around for the prevailing price. (The more modern version of this is multiple Web sites that all direct you to call the same person.) If you've ever had to call a plumber you may have unknowingly been a victim of a similar trick. The optician's trick: The serviceman looks over your door with lots of scowling, chin-scratching, and tsk-tsking. You ask, \"how much?\" He replies with the fair price. If you don't flinch at that price, he says, \"for the parts\", while quoting a large additional cost for the labor. If you still don't flinch, he adds, \"each,\" while pointing back and forth to your pair of springs. (I hope none of you service people are reading this!) I call this the \"optician's trick\" after the old vaudeville joke about lenses, frames, and left/right. The lucky-for-you-I-found-another-problem trick: Another trick is to suggest your automatic opener was damaged (or \"compromised\") by the additional load or shock presented by the failure of the spring(s). The plastic worm gear used in the most common openers (see above) wears normally over the years and tends to leave a lot of debris inside the opener housing. Removing the opener cover reveals a lot of plastic shavings that may be cited as \"evidence\" you need a new opener, when the gears are actually still serviceable. Nevertheless, you may have indeed worn out the gear if you repeatedly cycled the door with the opener despite having broken springs, which is possible if you have a very lightweight door. The classic telephone bait-and-switch: This time-honored swindle, also called \"false advertising\", can show up in the garage door business. Here's how this scam works: When you call and say you have a broken spring, and ask for a repair price, you are told over the phone that the price is X dollars, which typically might seem a little better than the competition. When the repairman shows up, after looking at your broken door, he will casually and matter-of-factly tell you it will cost 2X. If they told you over the phone that it would cost X, well, that was only for one spring, and he must (he must, mind you) install not just one, but two. He will act surprised if you object, as if you should have known that from what you were told over the phone. If you expected to really pay just X, it was your fault for misunderstanding because you don't know anything about how garage doors should be repaired. (You will feel intimidated at this, since you honestly don't know anything, else why would you have called a repairman? Intimidation is a powerful tool against customer resistance.) If your door used only one spring to start with, he will insist on converting yours to two, telling you it is safer the next time a spring breaks. If you originally had two springs, he will tell you that he must replace both springs, and you must therefore pay double what was quoted. While it is true that converting or replacing both springs is a good idea, the bait-and-switch pricing is not. The \"bait\" is the low price quoted to you over the phone, which they never intended to honor, and the \"switch\" is switching the price to something higher on a pretense. This method of selling is literally criminal (for example, see Florida Statutes 817.44, all states have similar laws), but a service business can usually avoid detection or prosecution because there are no printed advertisements or other tangible evidence, just one-on-one phone calls. If you find yourself in the middle of this trap, then the proper response is to dismiss the repairman without paying a nickel. Don't expect that you will be able to negotiate a fair price with someone who is using criminal business methods. Certainly don't expect that he will accept a lower price because you accuse him of false advertising. If you absolutely cannot wait for another service call, then you'll have to accept the fraud, in which case you should do so quietly. People that use these methods typically have ways to mentally justify their behavior to themselves as a reasonable business practice, and won't react well to your suggesting otherwise, even though they are in fact small-time criminals, not shrewd businessmen. The special-price don't-tell-my-boss trick: In this scheme, after the technician has worked on your door for a bit, he will grimly notify you that he has discovered an additional repair needed, not just the spring(s). He will offer to do the work at a \"special price\" if you agree not to tell his boss. This air of conspiracy to get a bargain distracts and disarms you from critically thinking whether you really needed the repair in the first place (likely you don't), and whether the price is really a bargain (likely it isn't). Before I give any more examples of many deceptive practices in this business, let me say two things. First, I don't know exactly how prevalent these practices are, I just know that they are not uncommon. I'm sure it varies from place to place and time to time. Don't feel that you'll necessarily get taken, and have a chip on your shoulder when you call someone for service. Second, there are a lot of honest people in this business who will give you good value for your money. While it can be hard to distinguish the honest ones from the opportunists and the outright crooks, being aware of the tricks is the best way to make such a choice from limited information. The home-warranty trap: I cannot adequately express how worthless home \"warranties\" are in general. When it comes to garage doors, if you bought a home warranty thinking that it would fix your broken springs or other major garage door problem, then you are in for a costly disappointment. The \"safety issue\" trick: Another tip-off is the use of language like \"safety issue\". This is meant to trump any objections you might have to a costly repair bill. Don't be manipulated by the suggestion that you are risking disaster if you don't buy something expensive. Even if you think the risk is genuine, get another estimate, and tell the second repairman you are skeptical; every technician loves to prove the competition made a mistake. The salesman-disguised-as-technician trick: In this trick, you arrange for a service call to your home, perhaps paying a small fee up-front, and a neatly uniformed man arrives in a very technical-looking truck, carrying an impressive tool kit. He carefully examines your door, perhaps using some impressive testing devices to lend weight to his expertise. He then condemns your door as not worth repairing, and tells you, to his sincere regret, that you must have a new one. In fact, this technician is not a technician, but a salesman who only sells, and does not repair, doors. Even if he doesn't sell you, he is doing well just collecting fees for service calls that are no more than sales visits. He doesn't actually have to ever fix anything, and he may not even be capable of doing so himself. He's an expert at selling, which genuine technicians are not. In the worst case, when you refuse to buy a whole new door, he might refuse to follow up with a visit from an actual technician, either outright, or only with an unacceptable delay (\"we're too busy to get a guy out until next week\", when your car is trapped). If you find yourself closing in on this situation, then politely invite him to leave, and try someone else. That is your right, and in fact the only power you have to bargain in such circumstances. At that point, he may offer to promptly bring in his competent colleague, who will turn up lacking charm and looking awful, but might actually do the work, possibly at a fair price. If so, you will have beaten a legal variation of the classic illegal bait-and-switch (see below). The switch was attempted, but not required, which makes this legal. This is a hazard of any direct-sales situation. Because it rarely appears in everyday retail sales, it can surprise the unwary. The \"liability\" angle: The flip side of \"safety issue\" is \"liability\". This is not used to directly sell you something; it is used to demean the cheaper alternative and prod you into buying a more expensive (and profitable) option. For example, you may be pressured into buying a whole new door, when you just need a new spring, by the salesman telling you he can't just replace the spring due to \"liability\" issues. Since product liability is a big burden on the garage door industry, and so many old doors (and especially automatic openers) are dangerous, this may be a genuine reason to accept a higher price. The disaster-is-nigh technique: As he inspects your door, the serviceman grimly calls your attention to \"cracks\" in your garage door. These appear very faintly in the middle of the door where the panels bow under their own weight when the door is up. This is normal, but the type of thing you wouldn't casually observe yourself. This surprising revelation disarms you, and you may find yourself strangely susceptible to the pitch for an entire new door. Not-quite-honest advertising: Advertising on the Web and yellow-pages is the most handy source of information when you urgently need a door repair, but it is noisy and deceptive, especially when it comes clueless-consumer items like garage door repairs. I've prepared following table to guide you through the exegesis: Understanding Yellow-Pages Advertising The Advertising The Meaning Multiple-name-multiple-ads All one operation. See explanation in text above. Business name starts with \"A\", \"AA\", etc. Any other random choice is statistically better. Full-page ad. Family owned. No physical address listed. One man, one truck, works out of home. More than one or two brand names or logos displayed No affiliation, but we can get import parts. 24-hour or emergency service Call us, we might feel like working late if you take the bait and pay the rate. Senior citizen discount, AAA discount, discount for any trivial affiliation you might have. Starting prices adjusted accordingly. Low prices! You have no idea what a low price is. $49 (etc.) service call. For $49, we'll send a salesman (disguised as a technician, see above) to sell you a new door, not fix your old one. or We'll fix it, but the real price will have to wait until we get there, when it will cost you $49 just to say no. Multiple locations, multiple phone numbers. I work out of home, all numbers forward to my cellphone. Ranked tops in customer service survey. My mom was the only one voting. Member, Better Business Bureau. Paid dues to a business in the business of collecting dues. Certified technicians. Paid certification fee to a business in the business of collecting certification fees. Picture of broken spring. That's what I have! How did you know!? I'm calling you! Factory authorized. What \"factory\"? Is there such a thing as factory-unauthorized? Free safety inspection. You just pay for the service call. All major brands. We train ourselves doing your job. Lifetime warranty. Does not apply to service call. You'll forget about it in a year or two, anyway Free estimates. Wonder why nobody just publishes a price schedule? Photos of clean-cut men (or cartoon figures) in crisp uniforms and new trucks in front of palatial homes. Did you want your door fixed, or to look at a stock-photo model? You might genuinely need some extra parts when you thought you simply needed a broken spring replaced, and a good serviceman will perform a simple inspection to identify such parts. Nor is it unreasonable for a business to charge separately for a service call versus repair work actually performed. But the best protection for you as a buyer, being somewhat at the mercy of whomever you decide to bring on site, is to understand what is being done, and ask intelligently for a clear explanation or demonstration of why extra parts are required. Insist on having any old parts returned to you, and have that noted on the written order before work begins. An honest and competent technician will not object to this. The evidence will establish whether you really needed the parts or not, and you can consult another opinion if you have any doubts. This tends to deter the parts-upsell scam, and protects you even if you know nothing about what you're buying while under the pressure of making a costly decision. If the old parts mysteriously \"disappear\", then you have reason to be suspicious. The deluxe-model upsell trick: Don't you want the best? Don't you want to protect your family? Galvanized springs may be offered to you at extra expense as \"longer lasting\". Although bare springs (also called \"oil tempered\") can develop a light film of rust, the eventual failure is due to fatigue and not corrosion. The use of coated springs (whether galvanized, painted, powder-coated, or surface-converted) is mostly about appearance: the customer likes his new door to look shiny, and the customer doesn't like repair parts that show superficial rust from storage. Paying for an \"extended warranty\" is a poor gamble. A knowledgeable installer with good inventory can offer you upgraded spring lifetimes by using longer, heavier springs than were originally installed. For example, you may be offered more expensive springs with expected lifetimes of 15, 25, or even 100 thousand cycles, instead of the standard 10 thousand. The difference in labor to substitute this upgrade is nil. Since the dealer's cost of springs is proportional to weight, and typically a small part of the job price anyway, the dealer's cost for this upgrade is slight. This would seem to be a excellent option to offer every customer, and if correctly calculated and reasonably priced, one that you should take as cost-effective. Yet you may not be offered such an upgrade, if the installer is not adept at making the rather simple calculations, or if the optional springs are not on his truck, or if you're not around to be asked, or if the installer just doesn't like selling or taking time to discuss such details. I'm not sure why most doors are installed with 10-thousand cycle springs instead of 25- or 50-thousand, since the difference in cost is minimal. I suspect most people do not want to pay today for extra years of service far in the future. And cynically speaking, the repair biz as a whole is more profitable when the cheapest parts are used. Other Helpful Links Clopay, a garage door manufacturer, provides online installation manuals for their products. These include excellent mechanical diagrams and brief instructions for winding torsion springs on their doors. Of course, this is specific to their product designs, which may or may not match what you have. Note that some of their products involve the \"EZ-Set assembly option\" mechanisms that use a geared housing for winding (instead of standard winding cones) and non-standard geometry for the drums. Clopay should get an award for at least acknowledging in their instructions that you might be able to install your own new door (although they insist you must not take out an old one if it has torsion springs). See the Sectional Overhead Garage Door catalog (PDF file) from the Prime-Line replacement hardware company. Their brochure Sectional Garage Door Torsion Spring Installation Instructions (PDF file, part number GD-12280) is brief but informative. Apparently you can only obtain their torsion springs as special order items through mom-and-pop type hardware stores like Ace and True Value, and not the big-box Home Depot and Lowes. DASMA publishes a detailed glossary in Sectional Garage Door Terminology [PDF document], Two of the spring references specifically for the garage-door industry are the APCO Spring Manual by Bill Eichenberger, and the Torsion Spring Rate Book by Clarence Veigel; these give tables of spring sizes and torque constants. Spring engineering principles in general are described in the Handbook of Spring Design published by the Spring Manufacturer's Institute; the formulas allow you to calculate torque constants knowing only the geometry and the Young's modulus of the material. You can also find some brief spring information in standard references like Machinery's Handbook and Marks' Standard Handbook for Mechanical Engineers. Calculating Spring Properties Dangerous Bend Caution: Advanced material follows. This section is only for the mathematically-inclined and engineering-minded! Given the material property and design geometry of a torsion spring, we can use engineering principles to calculate the spring's mechanical properties using the following formulas (after Wahl, A.M., Mechanical Springs, 2nd Edition, 1963; Machinery's Handbook, 26th edition): Property Formula or Constant Mean coil diameter (inches) D = (OD+ID)/2 = ID+d = OD-d Wire size (inches) d Young's modulus of the material (psi) E = 28.5*10^6 psi (ASTM A229 oil-tempered wire) Density of the material (lb/cu-in) rho = 0.283 lb/cu-in (ASTM A229 oil-tempered wire) Number of (active) coils N = L/d - \"dead coils\" Dead Coils 5 to 10, depending on winding cones Spring index C = D/d Poisson ratio nu Shear modulus (psi) G = E / (2(1+nu)) Wahl correction factor for stress Kw = (4C-1)/(4C-4) + 0.615/C Mass (weight) of spring (lbs) M = rho * L/d*pi*D * pi*d^2/4 Spring rate (IPPT) K = π * E * d^4 / (32*D*N) Number of turns applied F = 7.5 for a standard residential garage door Torque (in-lbs) T = K*F Bending stress from torsion, per turn (psi) S = 32*T/(π*d^3) Recommended stress limit (after Wahl correction)S * Kw < = 242 Kpsi (10,000 cycle life)< = 200 Kpsi (25,000 cycle life)< = 175 Kpsi (50,000 cycle life)< = 150 Kpsi (100,000 cycle life) (The Wahl correction factor accounts for additional stress in the material due to shear forces, although these forces do not contribute to the spring's torque. These shear forces become significant in designs using a low spring index, which is to say, a relatively thick wire for the coil diameter. The correction factor is applied to scale up the stress S to better predict the fatigue lifetime of the spring.) Excel spreadsheets: Several readers of this page have contributed Excel spreadsheets they wrote to calculate torsion spring properties based on the formulas above. Download Jay Hahn's spreadsheet. Download Gary Schrock's spreadsheet. Download Bowyer's spreadsheet. Download Ben Franske's spreadsheet. Note that there is nothing special about torsion springs used for garage doors; the formulas and spreadsheets apply generally to any torsion spring. Spring properties vary slightly by temperature, since Young's modulus is temperature-dependent. For example, a torsion spring at 0 deg F is about 4 percent stronger than the same spring at 100 deg F. Example 1:: Let's plug some numbers into the formulas above, using as an example the pair of replacement springs I described above. Spring rate and torque: In my spring replacement above, the wire size was d = 0.2253 inches, and the ID was 2 inches, giving a mean diameter D = 2.2253 inches. The number of coils is L/d = 24 inches / 0.2253 = 107, less about 5 dead coils on the winding cones, or 102 active coils. Thus the spring rate is K = (π*28.5*10^6 * (0.2253)^4) / (32 * 102 * 2.2253) = 31.8 in-lb/turn (IPPT). Winding 7.5 turns * 31.8 in-lb/turn yields a torque of 238 in-lbs per spring. Lift: The 4-inch lift drums have a 2-inch radius, so the tangential lift of one spring is 238/2 = 119 lbs. There are two such springs, so the total lift is twice that, or 238 lbs. This agrees with with my weighing of the door at, remarkably, 238 lbs. Stress and lifetime: Calculating the maximal stress in the wire is useful for estimating the lifetime. Using the formula above, the bending stress S in the spring wire is 32*238/(π*0.2253^3) = 212 Kpsi. The spring index C is D/d = 2.23 / 0.225 = 9.88. The Wahl correction factor is Kw = (4C-1)/(4C-4) + 0.615/C = 1.15. The Wahl-corrected stress is Kw * S = 1.15 * 212 Kpsi = 244 Kpsi. This predicts about a 10,000-cycle lifetime, which is the standard \"cheap spring\" configuration originally installed. Note that while this stress is proportional to the torque being applied, it is also in proportion to the inverse third-power of the wire size. Thus slightly heavier wire sizes (and suitably adjusted D and/or L) radically improve the expected cycle lifetime of the spring. Weight and cost: The 24-inch-long spring has a calculated weight of 8.4 lbs, not counting the winding cones. At less than $1/lb wholesale, and $3/lb retail for fabricated steel products, this spring should sell for about $8 to $25 (2005 prices) each, depending on the market and source. Since a pair is required, the expected cost for a pair is $16 to $50. Spring stretch: When the door is at the top of travel, the spring(s) are hardly wound, but are stretched, so on a single-spring installation this stretch tends to pull the shaft towards the non-spring side. With two springs, the stretch tends to cancel out. This top-of-travel spring stretch, being about 7 or 8 turns of the wire, will thus amount to upwards of about 2 inches on a typical size spring of 0.253 wire. This spreads out to a gap of about 0.020\" per coil on a typical 100-coil spring, so the stretch is not very visible. Example 2:: As another example, let us analyze the 25-year-old original spring that is still intact on the least-used of my three garage doors. This is a single spring instead of a duplex pair such as has since replaced the originals on the other doors. Measurements: With the door in the down position, I measure a wire size of 0.273 inches, outside diameter of 2.0 inches, and overall length of 41.5 inches. Relaxing the spring shortens the length by about 7.5 coils of wire, so to estimate the relaxed length, we deduct the wire diamter of 0.273 inches times 7.5 from the 41.5 inch wound length, yielding an estimated relaxed length of 39.5 inches. The mean coil diameter is 2.0 - 0.273, or 1.73 inches. Perhaps this was actually a 40-inch-long spring with a 1.5 I",
    "commentLink": "https://news.ycombinator.com/item?id=39744989",
    "commentBody": "How I replaced deadly garage door torsion springs (2002) (truetex.com)203 points by bronzekaiser 16 hours agohidepastfavorite267 comments levocardia 15 hours agoI had a teacher in high school who was blinded (I think only in one eye) trying to do this repair. You could not pay me enough money to do this repair. >This work is risky, but the risk is comparable to doing your own car repairs, or climbing on the roof of your house to clean your gutters. Notably, \"unintentional fall\" is the #1 cause of emergency department visits for adults [1], which is why I'd hire a professional to work on my roof too (and wince when they don't wear safety equipment). I'm not sure where \"crushed by your own car\" falls on the list, but while I'm perfectly comfortable digging around in the engine bay, you also could not pay me enough to crawl around under a poorly jacked-up car. 1. https://wisqars.cdc.gov/lcnf/ reply batch12 8 hours agoparent> Notably, \"unintentional fall\" is the #1 cause of emergency department visits for adults Yes. Until it happens to you, it seems like someone else's problem. I say this as someone who found himself staring at the sky with a concussion and whiplash after the ladder decided to walk out from under me on my concrete walkway. (I'll admit that it was probably my error, but I'll never really know since the ladder took a trip too). Now all trips up the ladder are treated with the gravity my younger self disregarded. I'll add that it's very disconcerting to be unable to move or breathe as everything fades out... reply amluto 11 hours agoparentprevI don’t understand why it’s even considered safe to have one of these springs in one’s garage without a solid shield of some sort. While the energy involved is small on the scale of energy storage devices ( I don’t understand why it’s even considered safe to have one of these springs in one’s garage without a solid shield of some sort. The shield is the steel tube that runs down the middle of the spring. When the spring breaks, it's got an integral safety containment, similar to the aircraft cable that is often run through extension style springs (which are probably far more dangerous when they break, though less dangerous to service). > I’m honestly not sure why springs are common at all in this application. They're nicely matched to the load profile of the door. As the door is fully closed, the spring is exerting maximum upward force. As sections of the door transition to the horizontal track, the spring is simultaneously relaxing some, meaning it exerts around half the force when the door is halfway up and very little force as the last segment hits the turn in the track. It's a pretty elegant match of mechanism to the load profile. reply imoverclocked 11 hours agorootparentprev> A rather small motor with a lot of mechanical advantage can easily open a close a garage door, and if a DC or low voltage AC motor is used, it could do so quite a few times even if the power goes out. Garage doors are actually fairly heavy. By balancing the weight with a spring, you do a lot of favors to your mechanical advantage systems. (Eg: less wear, less advantage needed, the door can open in a reasonable amount of time…) You also do yourself a few favors for when things fail. (Eg: you pull the safety release and it’s possible to open the door without the motor.) Finally, power isn’t free and motors aren’t either; A spring is a really cheap way to reduce the cost of both of these things. reply BizarroLand 10 hours agorootparentI imagine that garage door openers would either need to be a lot beefier or would work a lot slower without the springs. reply codethatwerks 9 hours agorootparentprevWhat about a counterweight or a piston instead (pistons are used in pull down beds which might be a similar load) reply MadnessASAP 8 hours agorootparentStill need to store the same amount of energy. A counter weight or piston also has more opportunities for uncontained failure. At least when a garage door spring fails it is still constrained by the shaft running through the middle. For what it's worth, I have also replaced my own garage door spring, and I really have no desire to do that again. reply Retric 8 hours agorootparentThe risk isn’t just what the spring does but what happens to something caught in it. Springs can release energy a lot more quickly than a counterweight which can only accelerate a 9.8m/s/s, and therefore more risky to disable. reply rob74 20 minutes agorootparentPlus a counterweight is a big fat weight hanging in midair, so it's pretty obvious what will happen if you let it fall (and where it will land), whereas with a spring it's much harder to tell whether it's under tension or relaxed, and what it might do when the tension is released... reply MadnessASAP 7 hours agorootparentprevA few hundred points accelerating at 1g is still plenty of energy. It also comes with it's own challenges when it comes to releasing it. It also provides a constant force, whereas the force required to raise a garage door linearly decreases the higher it gets. With the consequence that a garage door with a failed motor would slam into its stops over your head rather then into the ground. Followed shortly thereafter by the now liberated counterweight slamming into the ground. https://www.physics.smu.edu/scalise/www/misc/bricks.html *Wrong link reply ethbr1 7 hours agorootparentprevSee sibling comment above about the variable counterbalance needed for a normal right-angle tracked garage door. You'd have to match the counterweight to the varying load (i.e. garage door effectively gets \"lighter\" the more its raised). reply mhb 6 hours agorootparentCounterweight = a length of chain that lands in a bucket as the door goes up. reply ethbr1 5 hours agorootparentThat'd be feasible for new-build uninsulated steel doors, but 200+ lbs of chain for wood doors is a lot. reply nradov 9 hours agorootparentprevSure, but then you have to put the counterweight somewhere which would occupy garage floor or wall space. And the counterweight track would have to be enclosed for safety. reply shiroiushi 8 hours agorootparentIt's easy: you dig a deep shaft under the garage and put the counterweight in there. Of course, there may be some cost issues with this solution... reply kelnos 10 hours agorootparentprevIf the garage door opener breaks, or the power is out, many people might have a really tough time opening the door manually without those springs. They might not be able to even do it at all. It is still, of course, a valid question to ask if it's worth having dangerous springs up there all the time for the rare event of the opener being inoperative. Or maybe we just shouldn't have doors that are that heavy? reply QuercusMax 10 hours agorootparentThey are surprisingly heavy, which is why you need such a beefy spring. When my spring broke a decade or so ago, I wasn't able to lift the door on my own, and with the assistance of my wife we were just barely able to lift it. Looks like they can easily be 200-500 lbs depend on materials (steel isn't light, but it's lighter than wood for this application), and I guess the issue is that you have to lift hundreds of pounds straight up against gravity. reply rdtsc 6 hours agorootparentI lifted mine but probably shouldn't have. I kept sticking stuff under it, a crate, a stool, until I was able to put two step ladders under. The higher it goes, the lighter it becomes since now the horizontal roller starts to take on some of the weight. But it's super dangerous. When the spring broke, one of them snapped, and the quickly dropping door snapped the other one as well. Surprisingly it didn't damage the door itself. It's a double door so had two springs. I toyed with the idea of replacing the springs myself but after asking around doing a few searches decided it's best I pay a professional to do it. reply hnick 7 hours agorootparentprevI guess it depends what you make it of, doesn't it? Growing up I had a typical corrugated roller door, it was very easy for me as a kid to open it with some of the weight supported by the roller. I think they have springs but they're not as extreme as the tilt/section doors. Apparently you can throw a motor on the end similar to roller blind kits, and they wouldn't have to deal with as much weight as a spring-based door. Of course, I do not live in an extreme weather area (yet). reply codethatwerks 9 hours agorootparentprevCould it be made of lighter material. Ironic how the garage door is perhaps the most armoured part of a house reply tomoyoirl 8 hours agorootparentIt’s less ironic in areas subject to hurricanes. They have a very large area. One of the most damaging possible sequences in a hurricane is when the garage door blows in, then the wind just blows the whole roof off. reply nradov 9 hours agorootparentprevLighter materials that would provide the same level of durability, security, insulation, and fire resistance would be significantly more expensive. No one wants to pay for that. reply LoganDark 10 hours agorootparentprev> many people might have a really tough time opening the door manually without those springs I think that would be most people. Who can lift over 450lbs from the ground? reply codethatwerks 9 hours agorootparent450lbs but because it is not straight up but up and back that probably gives a 2-1 leverage so 225lb or 100kg. using your legs to lift it would be in the realm of a regular male weight trainer but not the average jo or jane. reply manfre 9 hours agorootparentYour 2-1 leverage assumption is wrong. The first 1' is full door weight plus all the friction in the system. The effective weight will decrease as the panels transition to horizontal, but deadlifting 400 lbs is easier than overhead pressing 200 lbs. reply notact 9 hours agorootparentprevSure it gets progressively easier as more and more of the door rides horizontally on its rails. But those first few inches where the door is nearly entirely vertical, must be some significant fraction of the total weight, probably >90%. reply LoganDark 9 hours agorootparentNot to mention you have to get your fingers or something underneath the closed door in order to begin lifting it in the first place. So the \"from the ground\" part is not insignificant. reply ComputerGuru 6 hours agorootparentAll garage doors I’ve seen had a heavy yank handle on the inside. reply 7thaccount 10 hours agorootparentprevMy garage door is no longer connected to the opener, I just swing it up manually twice a day. My kid can do so as well without issue. Maybe mine is lighter. reply rdtsc 6 hours agorootparentThat's why it's surprising how heavy the doors are. Springs make it very easy open and close the garage doors with one hand, so we get used to it. When springs snap, (they all have a limited lifetime) the doors suddenly feel surprisingly heavy. Don't know your situation but I'd dare to suggest fix the opener, simply because it means not standing right underneath, you or your kid, when the springs snap. It was quite startling when that happened. reply cwillu 9 hours agorootparentprevThe opener doesn't support the weight of the door, that's the springs. reply selcuka 9 hours agorootparentprevAre the springs still in place? If yes, then they are still doing most of the work. reply 7thaccount 7 hours agorootparentThey exist, but are not connected to anything. It just swings on the rollers. reply ianburrell 7 hours agorootparentThe springs are different than the opener. I don’t think the springs can be disconnected, they have to be carefully removed. You would know if they were or were broken, because you couldn’t open the garage door. reply sokoloff 7 hours agorootparentSide-mounted extension springs can be easily disconnected with the door up (and ideally clamped). GP describing them as \"not connected to anything\" makes me suspect that they're extension springs, which can be easily/casually inspected and determined to be not hooked up to anything. Torsion springs you probably need to look to see the marks on the spring wind up (or make a straight chalk mark on it and see if that twists uniformly). reply itsgrimetime 9 hours agorootparentprevI’m not an expert in how springs break but wouldn’t an ejection of material require the spring breaking in at least 2 places that are close enough to not loop around the rod? I’m curious if anyone’s ever been hurt by one breaking (other than when actively working on it). reply spdustin 9 hours agorootparentI’ve also never heard of a shrapnel-based injury. Only door falls or torsion bars breaking arms. I’ve actually replaced these in two houses, and yes, they’re dangerous, but honestly a little common sense and a straightforward (printed, with a buddy double-checking) list of steps you take with each quarter turn. It’s super important to have the right torsion bars. Don’t try to invent your own. reply outop 1 hour agorootparentRealistically, any household maintenance task which needs a checklist and a second pair of eyes and arms to be done safely, is often going to be carried out by someone who feels that they can manage without either. reply ethbr1 6 hours agorootparentprevI am a terrible person and have done this twice with a couple angle-ground pieces of rebar (almost perfect diameter) + a pair of vice grips. On the one hand, nothing went wrong because I was careful. And rebar actually works really well. On the other hand, it was pretty dicey, especially near the maximally-wound state when you're putting serious muscle into another quarter turn. 110% agreed on it being a 2 person job though. Reason #1: so someone is there to call 911. Reason #2: you'll need someone to cable up the drums while you're adjusting the spring, and it's way easier with 2 pairs of hands. reply gh02t 7 hours agorootparentprevI'm far from an expert but many (most?) kinds of spring are hardened steel. It's not hard to imagine how they might shatter under the right stress. reply jakogut 6 hours agorootparentprev> I’m honestly not sure why springs are common at all in this application. Properly adjusted springs effectively cancel out the weight of the door. This makes it easy to open and close with and without an opener, and greatly limits the danger of a door falling closed under it's own weight, which can seriously hurt someone. reply kmoser 5 hours agorootparentOne of my garage door springs snapped unexpectedly while my housemate was opening the garage door. I'm glad nobody was under the door at the time since it slammed so hard it put a bend in the bottom section. reply cj 15 hours agoparentprevMy father is a handyman for multiple large apartment buildings. Climbing onto a roof and changing garage springs are 2 things he says he'll never do. I was surprised how specific he was about it. He knows lots of other handymen so I'm guessing he has heard lots of horror stories. Garage springs are no joke. reply shiroiushi 8 hours agorootparentI've changed garage door springs myself; it was pretty easy. However, there are different types. The torsion ones are the scary ones, and I wouldn't touch those either. Other (generally older) doors have longitudinal springs. When the door is open, the spring is uncompressed, and very easy to change. Just make sure to block the door so it can't close while you're working, and install the safety wire (it goes through the spring so if they spring breaks, it doesn't hit something). Modern doors usually don't have this kind because they take up space in the garage. The torsion kind are on top of the door and stay out of the way. reply vrc 7 hours agorootparentOh! This is such a lightbulb comment for me. I distinctly remember growing up with the wobbly side springs, and then after the motor died switching to a “screw driven” system and those going away. Ever since I’ve only seen the “screw driven” ones and when I hear stories about garage door springs I wonder why I’ve never seen them… Now I know where the springs on my current garage door are. reply giantg2 15 hours agoparentprev\"Notably, \"unintentional fall\" is the #1 cause of emergency department visits for adults\" Most of these are not from DIY, but age realted reply michaelt 15 hours agorootparentAmong the workplace population, if you look at fatal workplace accidents [1] you'll find that falls are the second-most-common cause of death (deaths on the road are the most common) Construction workers have the second-highest number of workplace deaths (transportation workers are the highest) and among construction workers, falls are the most common cause of death. Transport and construction are second and third in fatalities per worker-hour, with 'farming, fishing and forestry' the most dangerous per worker-hour. So ladders aren't the most dangerous thing out there - but falls are pretty near the top of the list when it comes to workplace deaths. That's not to say people can't use ladders safely - just if you're hauling a heavy ass drill up a ladder and it's stopping you using both hands, the cable's flapping around your legs, your pockets are full of sharp pointy tools, and it's raining - maybe think twice :) [1] https://www.bls.gov/news.release/pdf/cfoi.pdf reply treflop 3 hours agorootparentI guess the problem I find with using these statistics is that there are a whole lot of people who are... I guess to put it frankly... doing things completely wrong. For example, I regularly use my table saw but and I see accident videos on Instagram/YouTube where people are doing something inherently dangerous, dumb, and obviously wrong on a table saw that I'd never imagine even trying to do, and then I read replies and everyone is like \"oh that's an honest mistake and it could have happen to anyone,\" which is a complete farce because they should have NEVER even attempted doing it that way to begin with. And if you attempt to explain why this method is dangerous and say something like \"oh, you shouldn't do it this way because of the rotation of the blade's spin and the direction that you pushing the piece in means that you pulling your fingers into the blade,\" they'll attack you and tell you that it's a mistake anyone could make. I guess it bothers me that people don't try to learn how something works, get into accidents, write it off as honest mistakes, and then thinks everyone makes mistakes as often as them. It's like a major disconnect between learning how to do something and learning how to be good at it and not having imposter syndrome. reply sbelskie 15 hours agorootparentprevHow is it age related if it is the number one cause for all ages? Am I misreading the chart? reply mbreese 15 hours agorootparentLook at the spread from #1 to #2. At young ages, the difference is pretty high, suggesting that young kids falling from things is common. In the middle age ranges, #1 and #2 are pretty close, suggesting that falling isn’t that different (from a statistical frequency POV) from the #2 option. But then look at the split at the older age groups. Falling is by and far the biggest cause. By a ton. Starting at 45, the split just keeps growing. And these numbers are higher in general than any other age group, so they skew the “all ages” numbers by a lot. So, while it is a major cause for all age groups, the effect size is very age related. reply csande17 15 hours agorootparentI would suspect that \"unintentional fall\" also covers falling over (like, the kind of fall that would happen if you tripped over a garden hose), not just falling off something. The former is the most common cause of emergency visits among the older people I know. reply mbreese 15 hours agorootparentI figured for all ages it would also encompass falling off something. A chair, bed, etc… one type is more predominant for the younger ages (falling from something), the other is likely more predominant for the older ages (falling down, tripping, stairs, etc). reply Projectiboga 9 hours agorootparentFalls hit 50% fatal above a twelve foot drop. Remember kids gravity is an accelerative force. It isn't a static force. You fall faster and faster every second you fall. reply Projectiboga 6 hours agorootparentUntil you hit 'terminal velocity'. reply nighthawk454 11 hours agorootparentprevmeaning elderly people falling due to mobility/balance issues vs some guy falling off his roof reply lttlrck 11 hours agoparentprevWhen my spring broke I called a someone local (maybe via Yelp), he came out within 30 minutes, replaced it in 20 and charged me $75. This was only a couple of years ago. Likewise with brake fluid changes. I can do it, but the risk/hassle just ain't worth it for the cost of a pro. reply j-a-a-p 2 hours agorootparentI never replace the brake fluid. I do flush each caliper when changing summer and winter tyres. This means a selected offspring presses the brake twice for every tyre and the caliper has fresh fluid in it. The fluid only degrades in the caliper, no need to replace it entirely, and no need for a computer when you do it like I described it. reply mnw21cam 0 minutes agorootparentIt depends. Brake fluid absorbs moisture, which lowers its boiling point. You don't want it boiling from brake heat when you're really trying to stop. If you live somewhere humid then the whole reservoir can slowly absorb too much moisture and make the fluid unsafe. shiroiushi 8 hours agorootparentprevAfter seeing the quality of work of \"pros\" with simple oil changes, I wouldn't trust one of them to properly change and bleed brake fluid, unless I was very familiar with their work. reply pard68 10 hours agorootparentprevWhat are the risks of changing brakes? Never thought of that as a particularly risky endeavor before. reply JoeMattiello 10 hours agorootparentThe most common risk is not properly protecting against contamination of the reservoir or not bleeding the system and having air or water in the line. Air will compress and you could have brake failure. Also water, less common but happens, as the brakes heat up, can boil into gas and then also again you have loss of pressure and brake failure. This is why I comment technique for race cars, which always have the risk of possibly boiling their brake fluid, will give a slight pump to the brake before coming up to a corner to pressurize the master cylinder and make sure that it does have a little bit of pressure pushback While you still have a little bit of space to react and ditch if something goes wrong reply bisby 10 hours agorootparentprevI would assume \"if you don't put them back together right, you can't stop, and cars are deadly enough with functioning brakes\" reply bagels 8 hours agorootparentprevIn addition to the other answers, brake fluid is toxic and removes paint. reply porkbeer 4 hours agorootparentOnly the old dot3 does that. reply HeyLaughingBoy 15 hours agoparentprevRight? One of my garage door lifting cables broke, and although I knew how to replace it, the $150 or so that it cost to have an actual professional fix it was money well spent. I've done car repairs for decades, and climbed up on roofs when I had to, but I'm not dicking around with garage door springs. reply grepfru_it 11 hours agorootparentIt’s not that it’s a hard job, the labor cost comes in at around $100-$150/hr as you said. It takes about an hour to do.. But why am I going to do this when my time could be better spent elsewhere? Saving $1000 tiling your shower? That makes sense. Saving $100 and potentially maim yourself in the process? Hello garage door repairman. reply Scoundreller 15 hours agoparentprev> you also could not pay me enough to crawl around under a poorly jacked-up car. Indeed, but it’s not difficult to set yourself up with multiple failsafes. By the time I’m set up to get underneath a car, I’m in a safer situation than driving the car given how many things would have to go wrong for the car to fall on me. reply WalterBright 9 hours agorootparentI use truck stands that will hold up several times the weight of the car. After I put it on the stands, I also give it several hard shoves to make sure it is secure. reply epcoa 9 hours agoparentprev> Notably, \"unintentional fall\" is the #1 cause of emergency department visits for adults [1] The vast majority of those are the mundane falling down the stairs or the elderly in bathtubs, not anything to do with roofs. reply nullhole 13 hours agoparentprevI knew someone in high school who had started work at an auto repair place, and he described the process - and danger - of replacing car springs to me once. I'm sure there are various types of car spring, the kind he was talking about was the stereotypical coil-of-metal spring. There's so much energy in there that a small slip up can be deadly. reply rootusrootus 12 hours agorootparentIt used to be worse. Lots of cars today use a coil-over-strut design that captures the spring. There's still a lot of energy, but it's much easier to manage safely, by the time the nut at the top runs out of threads there's generally little to no force left in the spring. Way back (like... 30 years ago) when I was working on my Mustang, the spring was separate from the strut. You had to drop the control arm enough to unload it and remove it, and there wasn't anything to contain it. I always tied it with a chain or a seat belt, and tried to not be directly in front of it during the unloading process. I knew a kid who got hit in the chest by a spring popping out; he did not make it. Removing it or installing a new one were both quite dangerous for the careless. reply sokoloff 7 hours agorootparent> by the time the nut at the top [of a McPherson strut] runs out of threads there's generally little to no force left in the spring This might be true on heavily lowered cars, but cars at stock ride height [with stock springs IOW] this is not my experience at all. I sure wouldn't risk my life or limb on it and spring compressors are quite cheap compared to an ER co-pay. reply the__alchemist 15 hours agoparentprevThank you. I agree on all examples. Absolutely not worth the risk. Not even close. I wince thinking about when I changed my own oil in my 20s... Potentially throwing away so many good years, for so little gain. The car danger can be mitigated if the work is done under a professional lift. reply cityofdelusion 15 hours agorootparentThere is pretty much zero risk if the car is either jacked correctly or put on ramps correctly. Dummies that rely on a single 1.5mm o-ring not blowing out on their hydraulic floor jacks are gambling with their life. Always operate on flat terrain, always chock the wheels, always use the parking brake, always use jack stands, always verify no movement in the vehicle by rocking it, never rely on the floor jack. reply cruffle_duffle 9 hours agorootparentIt’s amazing and honestly kind of sad how risk averse some people are. The reason I don’t change my oil anymore is it just isn’t worth the time and hassle. The “unsafety” of such a task doesn’t even register on my things to be worried about: at all. Jack it up, remember to put on gloves(!!!!) put the fucker on stands and go. The actual scary thing is I’m pretty sure way back when I was a kid we’d just dump the oil down the storm drain. I hate to think what that was doing to the environment around us. reply BenFranklin100 6 hours agorootparentI’m baffled too. Changing oil in a car is dead simple and effectively zero risk with a proper pair of jack stands and a modicum of common sense and mechanical aptitude. reply ryanjshaw 2 hours agorootparentNot everyone is born with common sense. I sure wasn't. I'll find some way to do something that will leave the people with common sense scratching their heads, saying \"but why wouldn't you just...\". reply outop 1 hour agorootparentThere's a kind of meta-common sense which allows people to say \"I don't have the ability to perform this task reliably\". reply rpmisms 15 hours agorootparentprevRamps are very good. I don't know why anyone would use a jack for an oil change... reply jtriangle 15 hours agorootparentA jack is basically suicide tbh. Jackstands are fine, provided they're used correctly. Ramps are nice, provided you don't need to take a wheel off, then you're using jackstands anyway, so, if you're buying one or the other, jackstands are a good option. Personally, if you have the space for it, a trench is the best option, I had one previously and I miss it alot. Nothing leaves the ground, car just pulls in and is ready to go. Mine was like 2ft deep, so, maybe a little cramped, but very comfortable once you're in there working. You couldn't make a standable trench at home without engineering it anyway, so, a crawlable trench is really the way to go. reply darkr 11 hours agorootparentAnother option, if you’ve got the ceiling height is a proper 4 post lift. Expensive, but you can justify it by doubling your car storage space. reply the__alchemist 13 hours agorootparentprevIMO a trench mitigates the danger effectively. reply darkr 12 hours agorootparentUntil you fall into it when you’re not paying attention! reply Arrath 11 hours agorootparentprevMake sure it is shored correctly! reply WalterBright 8 hours agorootparentprevThey're against the building code around here. reply thimkerbell 10 hours agorootparentprevBut fumes can collect in a trench, is that right? reply dylan604 12 hours agorootparentprevDepends on the jack. The people I know that do this use a jack and jack stands rather than just a jack. The jack is just the tool to go up/down. reply shiroiushi 7 hours agorootparent>The jack is just the tool to go up/down. This should be drilled into the heads of anyone who touches a jack. It's only a tool for lifting the car onto jackstands. reply class3shock 9 hours agorootparentprevLazyness. I've been under a car dozens of times with them, never had an issue, but know it's not the best practice and try to use jackstands... but it's just so much quicker! reply Scoundreller 15 hours agorootparentprevI’ll also put tires (on rims) underneath in addition to jack stands/ramps. reply the__alchemist 15 hours agorootparentprevValid, re ramps > jack stands > floor jack. I always used ramps, but still... won't do it again. I'll change plugs + coils etc because you don't need to get under the car. reply sokoloff 12 hours agorootparentprevI do my oil changes top-side now, using an air-powered Mityvac (MV7300; no affiliation). That's the factory-recommended procedure for several brands now and works well on my Mercedes and Honda. I was skeptical at first, but did the extraction and then removed the drain plug and the oil I got from the bottom wouldn't have half-filled a shot glass, so that's good enough for me. As long as the filter is accessible from the top or side, I don't see the need to jack up most wet-sump cars (which is almost all of them). reply cnasc 12 hours agorootparentRecently got one of these myself. Didn’t even get my hands dirty doing the change, wish I knew about fluid extractors years ago (though not all of my cars have had filters so easily accessible) reply robocat 10 hours agorootparentprevThanks for the oil change recommendation. Video review that seemed legit if anyone interested: https://m.youtube.com/watch?v=xuYVe-mPsxw reply sokoloff 10 hours agorootparentThat seems like a fair review. The only things I've needed to do in about a decade was to replace the nylon hose (readily available from many suppliers and therefore cheap). It cracked after a half-decade of sitting outside with corrosive used oil sitting it. I also got a somewhat longer piece so I could have the tool sitting firmly on the ground and still reach the bottom of the sump. It seems like they could have given an extra couple feet without breaking the bank. One thing to note: you'll need about 5 CFM @ 90 psi to run the venturi, which is more than a little pancake compressor can give, but otherwise isn't crazy for even a medium portable compressor. If you don't have an air compressor and weren't planning to get one, I think they probably make a powered pump version and that would be a better choice. reply porkbeer 4 hours agorootparentprevNOt all crankcases can be drained fully this way. Please test using the drain plug the first few times. Budfy locked up his benz with this shortcut. reply giantg2 15 hours agorootparentprev\"Potentially throwing away so many good years, for so little gain. The car danger can be mitigated if the work is done under a professional lift.\" I use ramps I built - solid 2x12. Those are extremely safe as there's nothing that can break, bend, or disintegrate. It's way more dangerous to drive than to work on the vehicle. reply paradox460 6 hours agorootparentprevI bought an oil/fluid extractor off Amazon. It's basically a bicycle pump sized tank, with a long hose you stuff down the dipstick hole. You then pump the unit up, which creates a vacuum, and pulls the oil up and out of the car. Takes about 5 minutes to get 5 quarts out, and then you just refill, pour the used oil into the now empty bottles the new oil came in, and take it to your oil recycling center reply YeBanKo 12 hours agorootparentprev> Potentially throwing away so many good years How much time did you spend changing oil?? It's a 20-30 min job once you have done it coupe of time and know the ropes. I sometimes do it myself not because I want to save few bucks, but because it's sometimes faster than going somewhere. reply sokoloff 12 hours agorootparentI read that as \"I risked 50+ remaining years of my life (or serious life-changing injury) with low-probability\" not \"it took me so many wasted hours with probability 1.0\" reply dylan604 12 hours agorootparentprevI'm assuming you're leaving out the time it takes to drive somewhere to dispose of the used oil and driving back. I grew up in the boonies, and my dad was one to dispose of oil \"out back\". It was such a shock the day he brought home a proper catch can to have it disposed of properly reply giobox 10 hours agorootparentThis isn't that bad for most people living in the US, given every major auto part chain in the country accepts the fluids for free (o'reilly, autozone, etc). There's often an auto parts store next to the supermarket or close by. Besides, for most cars we are talking about a once every 12-24 month event for only ~5 quarts of old fluid. You can show up with the used oil in any receptacle you like, there's no such thing as a proper container beyond not leaking, they will take the contents off your hands. reply dylan604 10 hours agorootparentthere's a caveat to \"most people living in the US\" regarding what their lease/deed/HOA contracts/etc say restricting the working on one's car on premises. you'd be amazed at how restrictive they can be. in an apartment, you'll be lucky if they allow you to jack up your car to change a tire. opening the hood for anything more than adding fluids is also a no-no. even single family homes with strict HOAs do not want a car in the front drive actively being worked on (sometimes that comes with \"for extended period\" caveats, but not always). I gues they figure if you can afford to live there, you can afford to have someone else work on your car at their place not in their neighborhood. reply sokoloff 10 hours agorootparent\"Don't buy in an HOA if you like to use your hands\" is probably pretty fair advice. HOAs serve a pretty useful purpose: to coarsely/imperfectly segregate people who want to live in an HOA away from people who would never want to live in an HOA and vice-versa. Those two groups are somewhat prone to having conflicts with each other and HOAs let both be happier than if they were to be annoyed by living next to each other. reply YeBanKo 12 hours agorootparentprevPretty much any big chain auto parts shop accepts used oil. With a proper catch can you still have to bring in for disposal. reply gnicholas 9 hours agorootparentprevWe just put it out with the weekly recycling. reply supportengineer 11 hours agorootparentprevI started changing my own oil when I was 16. We only used ramps, never jacks. reply jakogut 6 hours agoparentprev> you also could not pay me enough to crawl around under a poorly jacked-up car Absolutely, I agree. However, like any work, there's a safe way and a million unsafe ways to perform it. Crawling under a car on a jack is unsafe. Crawling under a car properly supported by jack stands is perfectly safe. I replaced my own garage doors, not just because of the cost, but also because I could not find a contractor to do it inside of four months. I did my research, took my time, and got two 9x7' doors replaced in a weekend and a half at a leisurely pace for a third of the quotes I received from the pros. The FUD spread online around garage doors is unreal. If you don't know how to do it safely, don't attempt it. If you can spend a few hours learning how to do things properly and safely, go for it. It's not black magic. Garage door installers are people too, and it's not like the training is some dark arts ritual imbued upon them by shadowy figures. It's basic physics, geometry, and hand tools. reply willcipriano 13 hours agoparentprevLooks like you have to use a bolt to deload, replace and then load a spring. With the risk being you overload it and it snaps in your face? A chainsaw is scarier to me. reply timschmidt 10 hours agorootparentBattery electric chainsaws are a game changer. I have a little 20v 12 inch that I've felled and limbed 2ft thick trees with. Doesn't jump around, stops the moment you let go of the trigger, isn't noisy or hot. Still have to respect it, but it's a much tamer beast than the two cycle monsters of yore. reply JKCalhoun 15 hours agoparentprevYeah, I'm a DIY'er and I noped-out of this one too. reply linsomniac 15 hours agorootparentDitto, but after he charged me $550 it and used bars rather than a worm-drive \"winder\", I'm thinking that next time I'm going to either get a winder and do it myself (parts were sub $100) or at least get 3 quotes. I went with someone a friend liked, so I didn't do any shopping around, mea culpa. But, I guess he had to pay for his brand new F-250 somehow. :-) I was half expecting him to say I needed other maintenance including new rollers, at which point I was going to call BS, because I replaced those rollers a couple three years ago and I'm about a million uses away from their rated million open/closes. Double sealed ball bearings, ask for them by name. reply wigglewoggle 9 hours agoprevIt's interesting whenever this comes up on Reddit and there are a hundred comments parroting how dangerous this is, but on closer inspection you can see the comments cannot discriminate between important details - eg. talking about how dangerous extension springs are, having no clue that they are different than torsion springs. Always fun to see how many people will pretend they have knowledge on a subject they have no experience in. reply MichaelMug 8 hours agoparent> hundred comments parroting how dangerous this is And every time a similar post or question is made the same responses are parroted again. I’ve read so much Reddit in college that I would play a game where id try to guess what the top voted comments would say. After a while it becomes tiring to read the same things over and over again. reply winkelmann 3 hours agoparentprevSomething I've noticed is that, when under the guise of safety/\"don't do this thing\", netizens seem to have few inhibitions sharing information from topics they barely understand or have experience in. Some caution is always good, but I feel like people are too comfortable using it as an excuse for sharing unsubstantiated information as fact. reply arh68 13 hours agoprev> but the risk is comparable to doing your own car repairs \"Comparable\" in that it's way, way greater, sure. Unless we're talking dangerous car repairs, like using a spring compressor to disassemble a strut &c. Or using harbor freight jack stands.. When my spring broke, I thought someone crashed their car into my house. It's an incredible amount of energy. Garage door springs are up there with lathes & table saws, in terms of danger (IMO). reply user3939382 9 hours agoparentI’m really scared of my angle grinder. Checkout on YT what happens when one of those discs comes apart. reply userbinator 8 hours agoparentprevOr using harbor freight jack stands.. Maybe in 2002? They seem decent these days. There are far worse: https://www.youtube.com/watch?v=hXzusz_eUy8 reply pavel_lishin 10 hours agoparentprevI once bought a table saw, then watched a YouTube safety video about table saws, and then sold my table saw. I wouldn't even consider owning a lathe. I know my limits, and even the chop saw in the garage gives me the shivers. reply WalterBright 8 hours agorootparentI used a Craftsman radial arm saw for a while. One day, I rotated the head 90 degrees to do some ripping. The saw grabbed the wood, and climbed up on it, and fired it at unbelievable speed at the wall. It punched a neat hole in the wall the exact size of the board, and the board disappeared into the next room. I got rid of the saw after that, and bought a sweet Makita cutoff saw instead, and a table saw for ripping. I also bought one of those full face shields people use with chain saws. I like these better than the usual shields because they are a wire mesh, and screens don't fog up. reply rainbowzootsuit 3 hours agorootparentFor what it's worth the 3M full-face respirators are very well designed in that the inlet air passes over your face and then into a sub mask where it is inhaled. Exhaled air is directed out of the front and down. I spent hundreds of hours with one (the FF400) in unconditioned spaces running power tools in fairly strenuous ways and never had it fog at all. reply elzbardico 11 hours agoprevAt my home country we used counter-weights, pulleys and cables. Took more space, it is arguably uglier (but who cares how it looks on the inside?) but the cable usually lasts for ages and fixing it is not a life-threatening procedure. reply LeifCarrotson 10 hours agoparentHow does a counterweight work? A torsion spring is great because as it unwinds, the tension decreases. As the garage door transitions from 200 lbs of vertical load in the closed position to almost 0 lbs when it's mostly parallel to the ceiling, a properly sized and wound spring reduces the force in synchrony with the door position. Wouldn't a counterweight provide insufficient force at the bottom and/or excess force at the top? reply cyberax 6 hours agorootparentYou use multiple weights that telescope into each other ( https://www.hermco.ca/products/2000-series/ ), or a segmented counterweight (Kelley E-Systems one company that does this). But they require space and a sturdy housing. Springs are as compact as you can get. There are also some safety concerns, a cable connecting the door to the counterweight can break, and/or slip. This can result in pieces of metal flying everywhere. A broken spring typically stays contained. Personally, I would do one of the counterweight systems for my new house. reply Horffupolde 9 hours agorootparentprevIt’s like for an elevator. The motor is there for the friction but the potential energy of the system is approximately constant. reply sokoloff 7 hours agorootparentAn elevator on the ground floor and an elevator at the top floor have the \"same\" weight. Not so for a 100%, 50%, and 0% open garage door. The 0% open door is the full weight; the 100% open door has very little weight (as most of the door is on the horizontal track). reply elzbardico 11 hours agoparentprevTorsion springs are BMWs, the expensive, over-engineered, beautiful, high performance style of doing this. Most of use would be just fine with a Toyota instead. reply alliao 5 hours agoparentprevoh that is cool what country is that? I'd like to check them out. Honestly sounds really good. reply LeifCarrotson 10 hours agoprevThe reason it's not expensive to have someone else do it is that if done properly it's not dangerous. Otherwise culture would be lumping garage door repairmen with emergency responders who take dangerous jobs but keep society functioning. You couldn't get it done for $200 if the career killed people frequently. The \"if done properly\" is critical, yes, if done wrong it's incredibly dangerous, but if you understand and respect it there's minimal chance of harm. DIYed it three times now (on different doors of course) without incident. reply alliao 5 hours agoparentif you know it well enough to keep yourself safe then I'd usually say yes. The thing that differs between say... dealing with residential mains power vs spring is I can measure and have quite a bit of trust in materials used. springs I just don't know enough, if it's of good material, metal fatigue? and a mishap means a good chance of disablement. I just don't like springs. Same thing with car suspension and those spring clamps? No SIR. reply KennyBlanken 7 hours agoparentprevOn what planet do you live where dangerousness of work has anything to do with pay? EMTs, who are basically doing the bulk of \"saving people's lives\" - are paid the lowest wages, worse than firefighters, police, and often even the dispatchers. Why? Because the market is totally saturated with former armed services folks. And in many states, assaulting a cop will get you in a fuckton of trouble, assaulting a firefighter will get you in a fair bit of trouble, but assaulting an EMT will (depending on the state) often get you...an ambulance ride, which you were probably getting anyway. Retail workers, transportation workers, field workers...all very dangerous jobs and bottom-barrel pay. Same for people in meat processing; just look up the articles about migrant children working in meat packing plants (or don't if you have kids, because there are some incredibly horrific things happening to them.) reply fragmede 7 hours agorootparentYeah. Pizza delivery is surprisingly dangerous and they're paid very little. Then again, underwater welding and helicopter chainsaw pilot pays quite well. So sometimes it does, but it's not correlated. reply Horffupolde 9 hours agoparentprevFamous last words. reply Loughla 8 hours agorootparentThat makes no sense. I'm with OP. You don't see mass casualties among overhead door installation crews. So, there must be a safe way to do it. I've also installed around 6 overhead doors, and followed all safety instructions that I learned from an overhead door installer. I've never once been killed. reply Horffupolde 7 hours agorootparentIt’s the arrogance that kills. Not the spring. reply lebuffon 11 hours agoprevReminds me of the old engineer joke: \"Did you know you can save $27.00 if you build your own refrigerator?\" reply fbdab103 7 hours agoparentHow is this the first time I have heard this? Just golden wisdom that is broadly applicable to so much DIY. reply nkurz 9 hours agoparentprevI don't know it, and searching got me nowhere. Could you expand? reply xattt 7 hours agorootparentThe BOM of a DIY refrigerator (or anything of this sort), built using retail price components, is almost as much as a manufactured fridge that was built using components bought at a volume discount. reply rootusrootus 12 hours agoprevI've had springs replaced in the past, and that's one of those jobs I'm perfectly willing to pay someone else to do. The cost is insignificant. I'll happily wire up my own 240V appliances, work underneath my properly supported car, etc. But garage door springs? Nah, go ahead, you do it, send me the bill. reply dave78 12 hours agoparentThe cost really is the key for me. We had one replaced a few months ago and it was a few hundred dollars and was done the same day. I usually prefer to DIY, but it would have taken me a while just to measure everything, round up the right spring and acquire it, get the correct rods and cut them to size, etc. I'm sure that would consume at least an afternoon for me and it would still cost something to acquire the parts. The savings just isn't worth it. If the cost to have it done professionally was $3000, it might be a different story. reply MarkusWandel 15 hours agoprevI did one on the \"do it with the spring relaxed and the door up\" plan. The door isn't that heavy, you can just lift it with your hand to help the opener lift it up, and done. But cramped!! Getting the lift cable nicely wound on the lift pulley, giving it a bit of preload (pull) and getting it attached to the bottom of the door, all with the raised door in the way, and doing both sides, let's just say my supply of swear words was severely depleted by the end. But I did get it done, and no dangerous forces were involved. Whereas I saw a professional do it in about 5 minutes flat on the \"wind the spring and block it and then hook everything up with the door down\" plan. But that can kill you if you do it wrong. reply yard2010 39 minutes agoparentThis reminded me of so many childhood memories with my dad.. reply WesleyJohnson 15 hours agoparentprev16' x 7' metal garage doors are like 150lbs or more. Maybe I'm a weak old man, but I wouldn't say that \"isn't that heavy\". reply MarkusWandel 15 hours agorootparentMine is 8', uninsulated, and lifting was with the aid of the opener (which stalled without help, with the broken spring). reply ARandomerDude 7 hours agoprevI attempted an almost identical repair like this once and ended up in the ER. Don't do it. Call a pro. reply j-a-a-p 2 hours agoprevThese doors are what we call a 'luxury problem': just more trouble because the premium door type was chosen. The single pane type door I know [0] are perhaps uglier, but they will last decades and do not need maintenance. They use normal springs. [0] https://www.novoferm.nl/producten/garagedeuren/kanteldeuren/ reply dang 13 hours agoprevRelated: I Replaced Deadly Garage Door Torsion Springs and Lived to Tell the Tale - https://news.ycombinator.com/item?id=28419196 - Sept 2021 (10 comments) reply linsomniac 15 hours agoprevNobody seems to mention keeping the springs lubed for longevity. What's the groups thinking on that? I keep a spray bottle of lube by the door and shoot it every few months, which is probably overkill, but it's so easy. Then just wipe off the excess and grime maybe yearly. reply bumby 12 hours agoparentAt least according to the article, the main failure mode is fatiguing, so I’m not sure lube would help with that, unless the corrosion is really bad. Most materials have a finite amount of cycles in them (although I’m a bit skeptical of the authors 7 year estimate…it seems low to me) reply linsomniac 11 hours agorootparentDoesn't the rubbing of the coils against each other contribute to fatigue? reply manfre 8 hours agorootparentYes, it does. reply mildchalupa 8 hours agorootparentprevRoughly 30k cycles is typical. You can certainly design a spring to have infinite life. But it requires reducing average deflection, aka more spring with less deflection per turn. reply zdragnar 15 hours agoparentprevMight help with corrosion, too. My grandfather had a very old garage door and opener, and one day something gave and it blew apart. It sounded very much like a shotgun had been fired inside the garage. Fortunately, nobody was in or near the garage when it happened, but I'll remember it every time \"garage door spring\" comes up in conversation. reply ToucanLoucan 12 hours agorootparentWe had a corroded one that still functioned but I finally buckled under the RACKET the thing made every time the door opened and closed and got it replaced anyway. Every single time you opened or closed it, it sounded like a goddamn car crash going on out front. Ugh. reply Scoundreller 10 hours agoparentprevDon't just lube the springs, take the cover off the opener and grease up the nylon gears. The grease eventually dries out and then the gears turn into dust. Here's the DIY video I followed to overhaul it when that eventually happened: https://www.youtube.com/watch?v=qE3GTc1h5N0 reply winrid 11 hours agoprevMy grandfather is in his 70s and does this as his \"retirement job\", it's generally around $500 last I asked him... reply Scoundreller 10 hours agoprevOne scam/scheme not mentioned in the article is calling the phone number on the sticker of the vendor that installed your opener. Those are very valuable advertisement slots. Call anyone but them. There's not a ton of specialization in residential garage door systems. reply stavros 10 hours agoparentCan you clarify a bit? Why are they valuable ad slots? Why wouldn't I call the vendor that installed my opener to repair it? reply lolinder 9 hours agorootparentPresumably because when they see the sticker they'll know that you called them without shopping around, so they'll quote you something ridiculous and bank on you not realizing you could easily get a second opinion. reply stavros 9 hours agorootparentAh, the vendor doesn't run a generic repair shop? This makes sense if the only people that will call the vendor for a repair are the people who had the vendor install the thing in the first place. reply lolinder 9 hours agorootparentThey don't have to know the sticker is there while on the phone, only by the time they give you the quote. Once they show up at the house to diagnose the problem they'll be able to see the sticker or not. To be clear, this is just me speculating about how such a scam would run, I've never seen it or heard of it. reply Scoundreller 9 hours agorootparentprevIt's often the builder's contractor. Years/decades down the line, that phone number is worth $$$. reply linsomniac 6 hours agoprevI think the next time I have to replace my torsion springs, I'm going to install the Coplay Ez-Set system, anyone have experience with it? It has a built in tensioning system. https://www.homedepot.com/p/Clopay-EZ-Set-Torsion-Conversion... It has a built-in winder that uses a power drill. There are tools that will attach to the traditional winders and you can use a power drill, those tools are $800, which is a bit hard to swallow. reply ehaskins 4 hours agoparentI've installed doors with that system, or a very similar Menard's carries. Given the choice, I'd opt for it since it is easier to fine tune, but I'm not convinced it's safer than the traditional way. You still have to secure the door against unintentional movement and ensure everything is assembled correctly before tensioning. The only \"safety\" feature I see is it eliminates the appeal to use an inadequate tool to tension the spring, so resist the urge to use a flimsy old screwdriver and you'll probably have similar risk either way. reply mildchalupa 8 hours agoprevPeople think garage door springs are like diffusing a bomb. It's not that bad if you are competent and have the correct equipment. reply toomuchtodo 15 hours agoprevI do not recommend DIYing this work, but I do recommend springing for the high cycle (20k cycles) torsion springs if offered by your installer (TLDR larger wire size and a few inches longer to optimize for longevity). They are a bit more in material cost, but will save you labor costs, which will only go up in the future. Also, depending on your living arrangement and situation, an unexpected spring failure can be annoying AF if you can't get vehicles in and out of the structure. reply ianschmitz 15 hours agoparentI recently had mine serviced and the spring I have is original (~30 years old) and looking a little old. I asked the installer if I should replace it and he said “no. They don’t make them like they used to. They don’t last nearly as long. Wait until this one gives up” reply galdosdi 11 hours agoprev> Copyright 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2014, 2015 Richard J Kinch Why is 2013 missing? reply valleyer 11 hours agoparentPresumably because the document was not updated in 2013. So no part of the document was first published in that year. reply m3kw9 5 hours agoparentprevHe needs all the luck with that job or paying it forward for next time reply irjustin 8 hours agoprevI'll agree with Reddit and most of the posts here. It's too dangerous to be worth it. Once every 3-7 years replacement at ~US$150-350 YMMV is honestly not that much money saved vs the risk. reply kstrauser 7 hours agoparentOnce every 3-7 years? I've heard of them breaking before but I've never had it happen in a place I lived, including my parents' house growing up, and the houses I've personally owned. I've never had the spring worked on other than to have it installed in the first place. Again, I know they break sometimes. It should not be happening every few years. (But if/when it does, take my money. I'll replace my car's brakes and wire a new ceiling fan but I'm not touching that spring for love nor money.) reply theogravity 12 hours agoprevI had mine fail on me a few years ago. Mine was a single spring, and after researching on how to replace, I decided that I'd rather pay someone else to take the risk. It was around $700 at the time and the installer installed two springs (side by side) to handle a failure event with one of them. reply iancmceachern 11 hours agoprevMy dad would do anything, with confidence. This is the one thing he wouldn't touch. Me too. reply uticus 12 hours agoprevIf they are this bad to replace, should I be worried about them randomly failing? I mean, I drive right underneath them like twice a day backing in and out. reply buescher 12 hours agoparentThey typically fail when wound (closed), not unwound (open), because that's when they're stressed. Cables can fail when winding the drum (open), but you have two. reply therein 11 hours agorootparentThat doesn't instill any peace of mind. So they are normally under stress and might explode at some random moment? Edit: Okay, that [0] doesn't look too violent. [0] - https://www.youtube.com/watch?v=F4qTIpspUik reply mzs 11 hours agorootparentThere is a rod that contains the broken spring. If an installed spring were to break when the door was open the most likely cause of danger is a few hundred pounds of garage door coming down on whatever was under it, so wait for the door to fully open before driving through. reply NegativeLatency 10 hours agorootparentprevDriving is probably the more dangerous activity there, if it helps you take your mind off the springs. reply XorNot 11 hours agorootparentprevWhat gets you as I understand it is that the turning handles for the springs. If you lose your grip on them its a rod of steel that's going to flung around at high speed with enough force to counter-weight an entire garage door. reply userbinator 8 hours agoparentprevThe roof of your car can likely survive the garage door falling on it. Unless you're in a convertible. reply tonetegeatinst 15 hours agoprevSo I recently helped my dad replace 2 of these springs. He is no engineer but used to be a firefighter and straight up told me \"you don't respect the power of the spring....you will die\" I don't think he ever had to respond to a call where someone died trying to do this....but I bet he had some local story of the dangers these pose growing up. TLDR treat dangerous things with respect and you vastly increase your odds. Be it a firearm or a chemical or a fast spinning lathe.....danger can be lurking anuwhere and when you recognise it a d stop and develop a aproach to mitigate risks.....you can help elimate stupid mistakes that could really cost you. TLDR its better to have annoyingly boring procedures than just wing it and hope it goes well. reply mikewarot 9 hours agoprevMine is broken and I'm waiting until I have enough savings to pay someone experienced to do it for us. I've worked on all sorts of things, but I don't want to risk my health that recklessly. reply crmd 15 hours agoprevThe author's intentionally verbose/non-linear storytelling technique, for example the two paragraph detour that describes his ladder and the ladder company's bankruptcy, and the 19 paragraph deep dive on garage door repair business marketing, makes it very hard for my brain to absorb the core information I was looking for (the procedure to replace the springs). I know I'm kind of missing the point/creative intent of this essay, and I appreciate the non-linear full-of-detours style in other genres. For example I'm a huge fan of Norm MacDonald whose long, impossible-to-follow stories would often drive unaware audiences and talkshow hosts crazy. But for technical things I personally find the style super annoying and feeling like the author is trying to flex on how much engineering, business, and trivia knowledge he has in many adjacent topics. I actually get anxiety thinking about getting trapped at a bar or party interacting with somebody who talks like this :-) Curious how other readers feel about this, especially those who have the exact opposite reaction! reply mauriciolange 58 minutes agoparentAt first I find it difficult to concentrate, but then I started really enjoying the article, and in way I think it is much better than current low-content-big-font websites. But yes, I was raised with this kind of content, so it seems that when I started enjoying was because I synced with a style that I already knew. reply hvs 15 hours agoparentprevThis is basically what the \"early Internet\" looked like that people often write paeans to. Lots of pages written by people very dedicated to a weirdly specific thing while also peppering it with nonsequiturs and their crazy theory about who runs the \"real\" government. It was fine, but hardly the glorious wonderland often portrayed. (Not that the current Internet is without its flaws, but if I want to learn something from someone it is a million times easier today than it was in the late-90's and early-00's. reply Scoundreller 10 hours agorootparentModern day recipe and gardening websites sound like your description of the \"early internet\". reply crmd 15 hours agorootparentprevYeah, that's a good point and it makes me wonder if the modern web has shrunk my attention span. I used to often meet this exact type of engineer early in my career as an enterprise data storage consultant in the 90s and early 2000s. I would say the most common \"character\" I would run into at a customer site was \"UNIX libertarian hippie guy\" who would love to weave politics, especially about privacy, freedom of speech, government overreach, new world order, esoteric obsessive hobbies, etc into technical discussions. I feel like the typical tech worker today either has very different socio-political views, or keeps their politics out of our workplace interactions. reply ToucanLoucan 12 hours agorootparentprev> It was fine, but hardly the glorious wonderland often portrayed. Probably down to personal taste, but I would happily take a thousand of these websites with strange, esoteric folk sharing knowledge in unconventional ways than another subreddit that's 70% non-sequiturs by volume, or a Stack Exchange thread that's just the same quesiton asked 400 times in broken english. reply thimkerbell 10 hours agorootparentYoutube has the esoteric folk people and I love it. Not big digressors, just really diverse and into their thing. reply neilv 15 hours agoprev> This work is risky, but the risk is comparable to doing your own car repairs, or climbing on the roof of your house to clean your gutters. I wonder what the net impact of an article like this is: * How much money is saved by garage door owners? * How much money is lost by professionals? * How many people are injured/killed because this article made them think this was doable at acceptable risk, when they otherwise wouldn't have attempted it? * How many people would've attempted it anyway, and would've been injured, but this article helped them not to be injured? Modern bonus: * How many people are injured because a YouTube/TikTok/etc. DIY influencer is informed by this article, makes a YouTube video that muddies the information, and people are inspired by the influencer video to attempt it? reply mildchalupa 8 hours agoparentLife's dangerous. reply wackget 15 hours agoprevIs there something I'm missing? A quick search of YouTube reveals tons of garage door spring replacement videos. It doesn't seem like the knowledge or parts are difficult to obtain. reply TheGRS 15 hours agoparentIts widely known as one of the most dangerous DIYs you can do. I think typically people working on them will get pushed off their ladder suddenly from the tension in the spring releasing. reply derekp7 13 hours agorootparentHappened in my family a number of years ago. The instructions said to crank the spring 7 1/4 turns. So it was really make a 1/4 turn, repeat 7 times. Not 7 full turns plus 1/4 additional. After pulling the vice grips off, the door shot up with such force that it knocked the ladder over, causing a broken collar bone in the process. reply jollyllama 12 hours agorootparentI tried and failed to repair before, but didn't get injured. I had to take great care with the vice grips and other implements I was using; my intuition was they could become missiles. reply Brian_K_White 10 hours agorootparentprevHoly crap what a terrible instruction! reply JKCalhoun 15 hours agorootparentprevI worry more about a steel bar being thrown out with ample centrifugal force or coming around and taking a whack at your jaw. reply bluGill 15 hours agoparentprevThere are several different springs, some are more dangerious than others. It isn't hard to safely replace any spring. However it isn't always obvious what is safe or unsafe and mistakes can kill. reply fencepost 10 hours agoparentprevThe page in question predates the founding of YouTube by ~3 years. It's in the middle of the Geocities time period, back when video distribution on the Internet was probably via Usenet and digital photography was probably on a Sony Mavica writing to floppy disks - or maybe to compact CDR if you were lucky. reply camtarn 9 hours agorootparentSort of. It was a different world, but not quite that different! Geocities was really dying by 2002. It had been bought by Yahoo! a few years previously, and they were intent on driving it into the ground. Video distribution was indeed fairly rare, but Flash sites were very widespread, sometimes including video, but more often vector animations. The Mavica was still around in 2002, although mostly in CD-R form. But cameras taking flash memory cards were also around, using a wild mix of CompactFlash, SmartMedia, Memory Stick, xD-Picture Card and MMC. SD had just been invented, and a year or so later I had my own cheap digital camera that took SD cards. reply 8organicbits 15 hours agoparentprev> If you've researched this subject at all, you will no doubt have heard that you shouldn't be attempting torsion spring replacement as a do-it-yourselfer. reply glasss 15 hours agoparentprevProbably the year this was published, the blog said the first version was in posted in 2002 reply mhuffman 15 hours agoparentprevI suspect that a lot or all of those videos are about \"extension spring\" replacements and not \"torsion spring\" replacements. Probably shouldn't do either, but the torsion ones can have instant and catastrophic failure modes during installation or removal. reply 14 9 hours agoparentprevyes this is one of those things that is scaring people but if you pay $500 some other guy can magically do it for you. The fact is that it is not that hard of a job and the dangers can easily be mitigated if you have some basic common sense and knowledge. Unfortunetly common sense is not that common and many people have been hurt doing this job. I myself would not hesitate to do this job like you say lots of information now days on how to do it this site is 22 years old in this post so now days you can see exactly what to do in many videos way before even attempting the job and that alone can take away most of the risk since you get hand held the entire process. I have to say I am a little disappointed in the HN crowd at how shy they are to do this job. All the handy men that do this job now are just humans like you and me and they all had to learn from scratch at some point. Again this is not that hard or dangerous but like the article says it has by shrouded in stories of danger and the trades people would not share their knowledge but told everyone how dangerous it would be so pay them money. reply giantg2 10 hours agoprevI wonder if longer life springs could be made from something like chrome silicon, or if the application of force won't work that way for that material. reply 1970-01-01 10 hours agoprevExcellent example of safety third. If you know how to be careful, don't be afraid to do the job. reply Waterluvian 10 hours agoprev> so if you have any doubts about your abilities to do risky physical work on your own, hire the job out like everyone else. The problem I witness is that the people who kill or maim themselves will tell you they were confident. When people aren’t confident, their brains are responding properly to the danger and it generally leads them to making good decisions. reply throwaway2037 15 hours agoprevThis blog feels like something from Internet hyperspace: Informative, but shitty formatting / layout. I say the latter with genuine love -- no trolling / hate. This blog post could have been posted in 1994! Can we please get more of these on HN!? reply userbinator 7 hours agoprevI wonder if the claims of danger are exaggerated to keep those in the business making $$$, and spread like rumours aided by the lack of information in that era. Now that there's YouTube, and an abundance of information (and accompanying misinformation, of course), the truth can come to light. I even found a supposedly scary warning video of a spring deliberately let go, and it's... underwhelmingly tame: https://www.youtube.com/watch?v=hrUIN6hClB4 Just remember to keep your hands away from it, which is easy to do with long winding rods that should be grasped at the ends for best leverage anyway. reply adrr 13 hours agoprevWhy are we still using springs? reply itishappy 12 hours agoparentGarage doors weigh from 80 to 200 lb (37 - 90 kg)[0], so you'd have to hang one of these from it[1]. Hmm, that doesn't sound too bad. Here's some smarter people than me discussing it[2]. Also, here's a company that sells them[3]. Looks like they use a pully system to double the weight and half the distance. Neat! I feel like I may have seen one of these before... [0] https://veterangaragedoor.com/faq/how-much-do-garage-doors-w... [1] https://upload.wikimedia.org/wikipedia/commons/1/15/Heavy_Du... [2] https://engineering.stackexchange.com/questions/8692/why-are... [3] https://www.hermco.ca/products/2000-series/ reply elzbardico 11 hours agoparentprevThe people answering apparently don't know that you can use counter-weights, cables and pulleys instead of a torsion spring. It takes more space, but it rock solid and safer to fix. It is very common in some other countries. reply Obi_Juan_Kenobi 9 hours agorootparentCounterweights provide a constant force, whereas springs can be precisely balanced to the decreasing weight as the overhead door becomes more horizontal. It is inherently related to the design of segmented overhead doors. reply BizarroLand 10 hours agorootparentprev300lbs of counterweights strapped to the doors would take up a good bit of space. Pulley systems would need to either be 1 to 1 or would need some space to fall below the garage floor or to be stored above garage door height. They seem far safer, longer lasting, and cheaper and easier to repair than springs, but are more expensive to install, take up more space, and probably cost more all in all compared to springs. I found this that also goes into it: https://engineering.stackexchange.com/questions/8692/why-are... reply Lammy 9 hours agorootparentThat looks super cool actually, like passing the Pillars of Hercules every time I drive https://hermco.ca/wp-content/uploads/img_14.jpg reply anamexis 13 hours agoparentprevWhy not? reply adrr 12 hours agorootparentSeems like there should be something better that can last longer and not kill you if you try to fix it. reply selcuka 9 hours agorootparentThere are safer spring systems, such as TorqueMaster that goes into a hollow torque tube. reply riversflow 12 hours agorootparentprevWhy does it seem like there should be something better? reply adrr 11 hours agorootparentOur garage doors openers are better. No more loud chain and they last longer too. reply unglaublich 12 hours agoparentprevDid you even read the article? Why are we still using passive, reliable mechanisms to reduce the force needed to move heavy objects? reply hoppyhoppy2 10 hours agorootparent>Please don't comment on whether someone read an article. \"Did you even read the article? It mentions that\" can be shortened to \"The article mentions that\". https://news.ycombinator.com/newsguidelines.html reply xyst 12 hours agoprevi thought there were alternates to torsion spring as the mechanism to open and close garage doors by now. I guess it’s limited for residential installs reply RockRobotRock 7 hours agoprevthis article was very effective at convincing me I would never try this myself. reply porkbeer 4 hours agoprevI have replaced these and am still alive. Apparently thats a big deal? When did we become so afraid of being responsible for our own safety? reply mistrial9 10 hours agoprevyes this is dangerous ! first hand experience with a commercial rollup door. A small crowbar is used to tighten the final install, about four meters up off the ground, too. reply VincentEvans 15 hours agoprevReading some of the comment section here makes this veteran DIYer sneer. Ya’ll need to “grow some” and unlearn helplessness. Pay someone else who learned the simple steps to do the simple work and save yourself the time for more productive activities, but stop congratulating yourself on “avoiding a disaster” - too much drama for the task at hand here. reply dang 13 hours agoparent\"Please don't sneer, including at the rest of the community.\" - https://news.ycombinator.com/newsguidelines.html If you know more than others, that's great, but in that case please share some of what you know, so the rest of us can learn: https://hn.algolia.com/?dateRange=all&page=0&prefix=true&sor... Putdowns don't help anybody, and make less interesting reading. reply TheGRS 15 hours agoparentprevIts not a regular thing to work on for the majority of people. What's the point of manning up for this particular DIY if the danger is high? Is it really that big of an ego hit to pay someone for a job once in awhile? reply happytoexplain 14 hours agoparentprevI'm sorry, but this \"sneering\" attitude is hideous. It's OK to do this job yourself if you have enough experience. It's also OK to not do it yourself because it is, in fact, dangerous. It's also OK to suggest that other DIY'ers err on the side of caution if they aren't 100% confident. What's not OK is insisting broadly to man up and do it yourself, stop being helpless, etc. You could encourage an innocent person to severely injure themselves. It's pointless and irresponsible. reply bitfilped 12 hours agoparentprevIt also only takes one mistake to severely injure yourself and permanently reduce your quality of life, it's worth considering the value you put on that when deciding these things. I like DIYing as well, but sometimes the risk vs reward isn't worth some extra money. People who get injured doing DIY work often have a similar attitude as yours until they make a critical mistake taking on something they \"understand\" and haven't done before. reply MiguelHudnandez 15 hours agoprevI was hoping this was going to be a new technology making these springs obsolete. reply ajb 15 hours agoparentThe alternative is counterweights. Not exactly new, but there looks like there is enough space for them in this case. The ones in the house I grew up in were a pair of concrete cylinders on either side of the door, maybe 10-15 cm in radius and 1m in length, mounted vertically. Not sure why they're not more common - seemed a completely reliable system. reply derekp7 13 hours agorootparentThe door needs less counter weight as it goes up. In order for weights to work you either need several weights, with different weights bottoming out as the door rises. Or have the cable on a spiral pulley may work. reply ajb 11 hours agorootparentI don't think that's true of the mechanism I saw, which I believe was quite common, but it's 20 years since I looked at the mechanism. Ok I've found a video with one in: https://m.youtube.com/shorts/zLuJYlHBSkU The counter weight is always taking the full weight of the door, there's just a linkage which rotates the door on the way up. So, not the same as the spring ones, which follow a curved channel. This makes me less keen on them as in theory it's going to fall when it wears out, albeit with only 50% of the weight. Unless there's another failsafe. reply ajb 41 minutes agorootparentThinking about it, the failsafe should be that if one side fails first, it gets stuck against the track. Although I don't think the one in the vid would, because the track points inward reply scotty79 11 hours agorootparentprevSome gas piston would be cool. reply shiftpgdn 15 hours agoparentprevI don't entirely understand why you can't just have a motor move the door for newer aluminum doors. They aren't that heavy. reply birken 11 hours agorootparentCoincidentally I just had a professionally done garage door spring replacement today, and I asked the repairman this question, and here is what he said: 1. The springs lift the door from the bottom, and from each side, which puts less load on the door itself as compared to if the entire weight were being lifted from the top middle every time. 2. The motors can be smaller, quieter and use less power 3. In case of power failure, the door is much more functional and safer the less apparently weight it has. Also the springs themselves are very unlikely to be dangerous (as long as you don't try to replace them yourself), because he said they almost always break when the door is at the closed state, because that is when they are under the most tension. Therefore on the whole, the springs in practice offer no practical safety risk, while greatly increasing the safety of the door in it's normal operation while also reducing wear and tear on the door. They also allow people to have heavier types of doors if they want them. reply jtriangle 15 hours agorootparentprevBecause if someone needs to open the door and the power's out, or the motor has failed, many people won't be able to do it. That might be acceptable day-to-day, but, if opening the door is what's required to escape, say, a house fire, it's very much not acceptable. reply Red_Leaves_Flyy 15 hours agorootparentShould be a door or egress window in any garage. reply toast0 11 hours agorootparentKind of hard to get a car out the door or window. Usually getting a car out of a garage isn't a big deal, because who actually puts their car in there? But also, you don't need to drive your car out if your structure is on fire ... OTOH, you may want to drive somewhere else if your power is out, especially if it's out for an extended period of time. reply ok123456 14 hours agorootparentprevThey do. My gym has a garage door like this. Because of the lack of springs, I didn't realize it had a door opener until the owner went to lower it. reply hk1337 15 hours agoprev> Usenet newsgroup alt.home.repair This dude is old school. reply superkuh 15 hours agoprevnext [7 more] What an incredible web page design. It loads and paints instantly on any browser in existence and reflows faster than any site I've seen. The content is front and center and there's a ton of it. This is what peak website performance looks like. reply martinky24 15 hours agoparentYeah but how will I get ~customized ads~ served to me? What if big-ad-tech doesn't know I'm tangentially interested in garage door opener content?? reply abracadaniel 15 hours agorootparentYou will miss out on having your YouTube feed filled with garage door videos for the next 3 years as well. reply xnx 15 hours agoparentprevStill too much design. The centering is unnecessary. reply suddenclarity 15 hours agoparentprevNot sure if satire. It's a terrible user experience for me. For future readers: https://motherfuckingwebsite.com http://bettermotherfuckingwebsite.com https://perfectmotherfuckingwebsite.com reply JKCalhoun 15 hours agoparentprevHa ha, and the compression artifacts on the images: how you know it's gonna load fast. reply gear54rus 15 hours agoparentprevrandomly changing content width and slapping random tables where they don't belong doesn't do it for me, sorry reply 12 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares a DIY project replacing torsion springs on a garage door, stressing safety, the right tools, and discusses industry challenges.",
      "Detailed instructions on measurement, installation, balancing, and potential dangers are provided, alongside advice on buying from reliable suppliers and avoiding repair scams.",
      "The text highlights the importance of proper spring properties for safety and functionality, cautioning against deceptive service provider tactics."
    ],
    "commentSummary": [
      "The article highlights the risks and significance of proper maintenance in replacing garage door torsion springs to ensure safety and smooth operation.",
      "It emphasizes the dangers of DIY repairs, stresses the importance of safety precautions, and explains how springs balance the garage door's weight.",
      "Participants engage in discussions about personal experiences, debate the safety of various mechanisms, and underscore the value of professional expertise in handling garage door springs."
    ],
    "points": 203,
    "commentCount": 267,
    "retryCount": 0,
    "time": 1710771872
  }
]
